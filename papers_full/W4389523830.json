{
  "title": "GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings",
  "url": "https://openalex.org/W4389523830",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2390014234",
      "name": "Fu Bang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3093739590",
    "https://openalex.org/W2131571251",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4310509694",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2077866080",
    "https://openalex.org/W4367858557",
    "https://openalex.org/W4312056202",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W4364385701",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4313484599",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "The rise of ChatGPT1 has led to the development of artificial intelligence (AI) applications, particularly those that rely on large language models (LLMs). However, recalling LLM APIs can be expensive, and the response speed may slow down during LLMs' peak times, causing frustration among developers. Potential solutions to this problem include using better LLM models or investing in more computing resources. However, these options may increase product development costs and decrease development speed. GPTCache2 is an open-source semantic cache that stores LLM responses to address this issue. When integrating an AI application with GPTCache, user queries are first sent to GPTCache for a response before being sent to LLMs like ChatGPT. If GPTCache has the answer to a query, it quickly returns the answer to the user without having to query the LLM. This approach saves costs on API recalls and makes response times much faster. For instance, integrating GPTCache with the GPT service offered by OpenAI can increase response speed 2-10 times when the cache is hit. Moreover, network fluctuations will not affect GPTCache's response time, making it highly stable. This paper presents GPTCache and its architecture, how it functions and performs, and the use cases for which it is most advantageous.",
  "full_text": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 212–218\nDecember 6, 2023 ©2023 Association for Computational Linguistics\nGPTCache: An Open-Source Semantic Cache for LLM Applications\nEnabling Faster Answers and Cost Savings\nBang Fu, Di Feng\nZilliz Inc.\nAbstract\nThe rise of ChatGPT 1 has led to the develop-\nment of artificial intelligence (AI) applications,\nparticularly those that rely on large language\nmodels (LLMs). However, recalling LLM APIs\ncan be expensive, and the response speed may\nslow down during LLMs’ peak times, caus-\ning frustration among developers. Potential\nsolutions to this problem include using better\nLLM models or investing in more computing re-\nsources. However, these options may increase\nproduct development costs and decrease devel-\nopment speed. GPTCache 2 is an open-source\nsemantic cache that stores LLM responses to\naddress this issue. When integrating an AI ap-\nplication with GPTCache, user queries are first\nsent to GPTCache for a response before be-\ning sent to LLMs like ChatGPT. If GPTCache\nhas the answer to a question, it quickly returns\nthe answer to the user without having to query\nthe LLM. This approach saves costs on API\nrecalls and makes response times much faster.\nFor instance, integrating GPTCache with the\nGPT service offered by OpenAI can increase\nresponse speed 2-10 times when the cache is\nhit. Moreover, network fluctuations will not\naffect GPTCache’s response time, making it\nhighly stable. This paper presents GPTCache\nand its architecture, how it functions and per-\nforms, and the use cases for which it is most\nadvantageous.\n1 Introduction\nSince OpenAI released ChatGPT, large language\nmodels have impressed many people and have been\nfrequently integrated into our daily work and lives.\nAt the same time, more open-source enthusiasts\nand tech companies have invested time and effort\ninto developing open-source LLMs, such as Meta’s\nLLama (Touvron et al., 2023a,b), Google’s PaLM\n(Chowdhery et al., 2022), Stanford’s Alpaca (Wang\n1https://openai.com/chatgpt\n2https://github.com/zilliztech/GPTCache\net al., 2023; Taori et al., 2023), and Databrick’s\nDolly (Conover et al., 2023).\nThere are two ways to use large language mod-\nels: online services provided by companies like\nOpenAI, Claude, and Cohere or downloading open-\nsource models and deploying them on your servers.\nBoth methods require payment. Online services\ncharge you based on tokens, while deploying mod-\nels on your own server requires purchasing specific\ncomputing resources. The choice depends on indi-\nvidual needs.\nWhile online services are more expensive, they\nare more convenient and effective and provide a bet-\nter user experience than deploying models yourself.\nCosts and user experience are two critical consid-\nerations for building LLM applications. As your\nLLM application gains popularity and experiences\na surge in traffic, the cost of LLM API calls will\nincrease significantly. High response latency will\nalso be frustrating, particularly during peak times\nfor LLMs, directly affecting the user experience.\nGPTCache is an open-source semantic cache\ndesigned to improve the efficiency and speed of\nGPT-based applications by storing and retrieving\nthe responses generated by language models. Un-\nlike traditional cache systems such as Redis, GPT-\nCache employs semantic caching, which stores and\nretrieves data through embeddings. It utilizes em-\nbedding algorithms to transform the queries and\nLLMs’ responses into embeddings and conducts\nsimilarity searches on these embeddings using a\nvector store such as Milvus. GPTCache allows\nusers to customize the cache to their specific re-\nquirements, offering a range of choices for embed-\nding, similarity assessment, storage location, and\neviction policies. Furthermore, GPTCache sup-\nports both the OpenAI ChatGPT interface and the\nLangchain interface, with plans to support more\ninterfaces in the coming months.\nThrough experiments using the\nparaphrase-albert-small-v2 model (Reimers\n212\nand Gurevych, 2019) to embed input in the onnx\nruntime environment and running it on a local\nMac with i7, 4CPU, and 32G memory, the time\nconsumed when hitting the cache is approximately\n0.3 seconds. Compared to accessing OpenAI\nChatGPT with an average response latency of\n3 seconds, the time consumed is only 1/10.\nFurthermore, no tokens are consumed when\nhitting the cache. Different embedding models\nand similarity evaluation algorithms must be\nselected in real development scenarios based on\nthe tolerance for cache errors. Even so, the entire\nconsumption time is about 3-4 times faster.\n2 Related Works\n2.1 Accelerating LLM Inference\nAccelerating LLM Inference. Large language mod-\nels (LLMs) typically take seconds to infer an-\nswers, prompting researchers to explore ways to\nreduce inference time and resource consumption.\nOne approach is quantization (Dettmers and Zettle-\nmoyer, 2023), which decreases the number of bits\nneeded to represent each parameter and, therefore\nreduces the model size. However, this can result\nin a trade-off between accuracy and memory foot-\nprint. Another method is pruning, which can spar-\nsify large-scale generative pre-trained transformer\n(GPT) models without retraining, as demonstrated\nby SparseGPT (Frantar and Alistarh, 2023). Ad-\nditional methods include Compressing (Xu et al.,\n2020) and Inference with Reference (Yang et al.,\n2023).\n2.2 Widespread application of Caching\nWidespread application of Caching. Caching is a\ncommonly used technique to reduce frequent and\ncomputationally expensive data accesses, which\ncan improve system query performance. Many dif-\nferent caching schemes have been proposed for\nvarious scenarios. For example, semantic knowl-\nedge extracted from data can convert cache misses\nto cache hits, avoiding unnecessary access to web\nsources (Lee and Chu, 1999). Another example is\nin querying multiple databases with sensitive in-\nformation, where a differentially private cache of\npast responses can answer the current workload at a\nlower privacy budget while meeting strict accuracy\nguarantees (Mazmudar et al., 2022). In addition,\na cached memory architecture for new changes to\nembedding tables has been proposed during embed-\nding. In this architecture, most rows in embeddings\nare trained at low precision, while the most fre-\nquent or recently accessed rows are cached and\ntrained at full precision (Yang et al., 2020). As\ndemonstrated, caching is applied in a variety of\nreal-world development processes.\n2.3 Embedding Models\nEmbedding models (Almeida and Xexéo, 2023) are\na type of machine learning model that map discrete\nsymbols or objects (such as text, images, audio,\netc.) to continuous vector spaces. These vectors\nare called embedding vectors and are indispensable\nin many natural language processing (NLP) and\ncomputer vision (CV) tasks.\nIn NLP tasks, embedding models aim to map text\ninto a low-dimensional continuous vector space.\nThis makes it easier for machine learning models\nto process the text. The vectors can capture seman-\ntic information about the text, such as its meaning\nin context. In CV tasks, embedding models can\nmap images, videos, or objects into a vector space.\nThis approach allows them to be processed by com-\nputer vision algorithms, such as image search and\nidentification. Common text embedding models\ninclude BERT (Devlin et al., 2019), GloVe (Pen-\nnington et al., 2014), and Word2Vec (Goldberg and\nLevy, 2014; Mikolov et al., 2013). These models\ngenerate embedding vectors by processing large\namounts of text data. They can also perform well\nin many NLP tasks, such as semantic similarity\ncalculation, part-of-speech tagging, named entity\nrecognition, and sentiment analysis.\n2.4 Vector Store\nA vector database is designed for storing and man-\naging vector data. Vector data consists of sequences\nof numbers commonly used to represent objects or\nfeatures in high-dimensional spaces. For exam-\nple, data types such as images, audio, and natural\nlanguage text can be represented as vector data.\nVector databases improve the efficiency and ac-\ncuracy of vector data retrieval by using vector simi-\nlarity measures to index and query the data. This\nindexing technique allows the database to quickly\nfind vectors most similar to a query vector, making\nit useful for various applications such as sentiment\nanalysis, image search, speech recognition, and\nrecommendation systems.\n213\nFigure 1: GPTCache: The architecture comprises six\ncore components: adapter, pre-processor, embedding\ngenerator, cache manager, similarity evaluator, and post-\nprocessor.\n3 GPTCache: Semantic Cache for LLMs\nThe overall workflow of GPTCache follows the\ngeneral cache pattern - attempting to obtain results\nfrom the cache before fetching data or processing\nrequests. If successful, the process terminates im-\nmediately. Otherwise, the processing path is the\nsame as if the cache did not exist. However, before\nreturning, the corresponding results are stored in\nthe cache so that repeated actions will retrieve re-\nsults directly from the cache next time. Using the\ncache significantly reduces workflow time, which\nexplains why cache designs are ubiquitous in our\nlives, such as multi-level caches in computers, DNS\ncaches in networks, and Redis/Memcache in man-\nagement systems.\n3.1 Adapter\nThe adapter serves as the interface for GPTCache\nto interact with the outside world. It is responsible\nfor converting LLM requests into cache protocols,\ncontrolling the entire cache workflow, and trans-\nforming cache results into LLM responses. For\neasy integration of GPTCache into our systems\nor other ChatGPT-based systems without extra de-\nvelopment effort, the adapter should be easy to\nintegrate with all LLMs and flexible enough to in-\ntegrate more multimodal models in the future.\n3.2 Pre-Processor\nThe pre-processor handles the input of LLM re-\nquests primarily by formatting the information as\nthe primary key for the cache data. This includes\nremoving prompt information from inputs, com-\npressing input information, and only retaining the\nlast certain words for long texts or the last round\nin a multi-round conversation. These operations\nmake the request data more distinguishable from\neach other and remove redundant and irrelevant\ninformation from the requests.\nPre-processing is a critical factor affecting the\nperformance of the cache. For example, suppose\nboth inputs contain a large portion of prompt in-\nformation, where the key part of the information\nis only a small portion of the entire input. In that\ncase, the cache cannot obtain the key information\nwithout eliminating the prompt. This can result in a\nhigh probability that all requests hit the cache. The\npreprocessed results are passed to the Embedding\ncomponent for vector conversion.\n3.3 Embedding Generator\nThe embedding generator can convert user queries\ninto embedding vectors for later vector similar-\nity retrieval. There are two methods to achieve\nthis functionality. The first method generates em-\nbedding vectors through cloud services (such as\nOpenAI, Hugging Face, Cohere, etc.). The sec-\nond method involves generating embedding vectors\nusing local models that can be downloaded from\nsources such as HuggingFace or GitHub.\n3.4 Cache Manager\nThe cache manager is the core component of GPT-\nCache and has three functions:\n• Cache storage: stores user requests and corre-\nsponding LLM responses.\n• Vector storage: stores embedding vectors and\nretrieves similar results.\n• Eviction management: controls cache capac-\nity and clears expired data according to LRU\nor FIFO policy when the cache is full.\nBefore a piece of data is stored, an id will be\ngenerated. The id and scalar data will be stored in\ncache storage, and the id and vector data will be\nstored in vertor storage. In this way, cache storage\nand vertor storage are associated. Eviction man-\nagement also records these IDs. When cache data\n214\nneeds to be cleared, the data corresponding to cache\nstorage and vertor storage will be deleted based on\nthe id.\nThe eviction manager releases the cache space\nby deleting data that has been unused for a long\ntime or is furthest away from using in the GPT-\nCache. If necessary, it removes data from both the\ncache and vector store. However, frequent deletion\noperations in the vector store can lead to perfor-\nmance degradation. Therefore, GPTCache only\ntriggers asynchronous operations (e.g., index build-\ning, compression, etc.) upon reaching deletion\nthresholds.\n3.5 Similarity Evaluator\nGPTCache retrieves the Top-K most similar an-\nswers from its cache and uses a similarity evalu-\nation function to determine if the cached answer\nmatches the input query. The similarity evalua-\ntion module is also crucial for GPTCache. After\nresearch, we eventually adopted the fine-tuned AL-\nBERT model. Of course, there is still room for\nimprovement here, and other language models or\nLLMs (such as LLaMa-7b) can also be used.\n3.6 Post-Processor\nThe post-processor is responsible for preparing the\nfinal response to be returned to the user. It can\neither return the most similar response or adjust\nthe response’s randomness based on the request’s\ntemperature parameter. If a similar response is not\nfound in the cache, the LLM will handle the request\nto generate a response. The generated response will\nbe stored in the cache before being returned to the\nuser.\n3.7 Key GPTCache Use Cases\nNot all LLM applications are suitable for GPT-\nCache, as the cache hit rate is a crucial factor for\nthe cache’s effectiveness. If the cache hit rate is\ntoo low, the return on investment cannot balance\nthe input, and there is no need to spend effort on\nthis feature. This is similar to traditional caching\nscenarios, where caching is usually done only on\nfrequently accessed public nodes to maximize re-\nsource utilization and system performance and im-\nprove user experience.\nThis paper introduces three critical practical sit-\nuations where GPTCache is most beneficial:\n1. LLM applications designed for specific do-\nmains of expertise, such as law, biology,\nmedicine, finance, and other specialized\nfields.\n2. LLM applications applied to specific use\ncases, such as internal company ChatBots\nor personal assistants like chat-pdf and chat-\npaper. These applications can be enhanced\nwith a cutting-edge AI technology stack called\nCVP3 (ChatGPT+Vector DB]+prompt engi-\nneering). This combination overcomes the\nlimitations of knowledge bases and enables\nfurther expansion and innovation.\n3. LLM applications with large user groups can\nbenefit from using the same cache for user\ngroups with the same profile if user profiling\nand classification can be done. This approach\nyields good returns.\n4 Experiments\nTo evaluate GPTCache, we randomly scrape some\ninformation from the webpage, and then let chatgpt\nproduce a corresponding data (similar or exactly\nopposite). And then we created a dataset consisting\nof three types of sentence pairs:\n• Similar sample pairs: two sentences with iden-\ntical semantics\n• Opposite sample pairs: two sentences with\nrelated but not identical semantics\n• Unrelated sample pairs: two sentences with\ncompletely different semantics\nThen we evaluate the effectiveness of cache\nthrough five indicators, which are:\n1. Cache Hit, which successfully finds similar\nvalues based on the input, which consists of\nPositive Hits and Negative Hits.\n2. Cache Miss, no similar value was found based\non the input\n3. Positive Hits, the obtained cache value is con-\nfirmed to be similar to the input value\n4. Negative Hits, the obtained cache value is\nfound to be not similar through inspection.\n3https://zilliz.com/blog/ChatGPT-VectorDB-Prompt-as-\ncode\n215\nCache Cache Positive Negative Hit\nHit Miss Hits Hits Latency\n876 124 837 39 0.20 s\nTable 1: Results for Caching Hit and Miss Samples,\nCaching Mixed Positive and Negative Queries, and Hit\nLatency\n5. Hit Latency, it includes pre-processing time,\ncache data search time, similarity calcula-\ntion time and post-processing time. The pre-\nprocessing and post-processing do not use the\nmodel during the test process, and are just\nsimple character or number comparisons.\nIn addition, we tried different similarity algo-\nrithms and found that they had no impact on the\nresults, so we used the common cosine similarity.\nFirst, we cached the keys of all 30,000 positive\nsample pairs. Next, we randomly selected 1,000\nsamples and used their peer values as queries. Table\n1 presents the results.\nWe found setting the similarity threshold of GPT-\nCache to 0.7 achieves a good balance between hit\nand positive ratios. So we used this for subsequent\ntests.\nTo determine if a cached result is positive or\nnegative to the query, we used the similarity score\nfrom ChatGPT with a positive threshold of 0.7. We\ngenerated this by prompting:\nPlease rate the similarity of the follow-\ning two questions on a scale from 0 to 1,\nwhere 0 means not related and 1 means\nexactly the same meaning. And ques-\ntions, \"Which app lets you watch live\nfootball for free?\" and \"How can I watch\na football live match on my phone?\" The\nsimilarity score is.\nWe issued 1,160 queries with 50% positive and\n50% unrelated negative samples. Table 2 presents\nthe results. The hit ratio was about 50%, and the\nnegative hit ratio was similar to Experiment 1, in-\ndicating GPTCache successfully distinguished re-\nlated and unrelated queries.\nNext, we tried to also cache all negative samples\nand queried with their peers. Surprisingly, despite\nhigh ChatGPT similarity scores (over 0.9) for some\npairs, none hit the cache. The cause of the cache\nerror could be the similarity evaluator’s fine-tuning\non this dataset correctly undervalued the similarity\nof negative pairs.\nCache Cache Positive Negative Hit\nHit Miss Hits Hits Latency\n570 590 549 21 0.17 s\nTable 2: Results for Caching Hit and Miss Samples,\nCaching Mixed Positive and Negative Queries, and Hit\nLatency\nThe initial experiments demonstrate that GPT-\nCache can effectively utilize semantic similarity\nto cache LLM query-response pairs and achieve\nsignificant speedups. We plan to conduct more\nrigorous evaluations on larger and more diverse\ndatasets. When tuning the similarity threshold, fur-\nther investigation is required to balance cache hits\nversus false positives.\n5 Future Challenges\nOne core factor affecting GPTCache’s caching\neffectiveness is the choice of embedding model.\nCompared to other component selections, the\nchoice of embedding model is crucial because sub-\nsequent vector database retrieval relies on the em-\nbedding vectors. If the vectors cannot adequately\ncapture the features of the input text, the retrieval\nresults will be very noisy or even counterproduc-\ntive, returning completely irrelevant cached data.\nOur testing has shown that even the best cache hit\nrates do not exceed 90% with current embedding\nmodels. This means that negative cache hits are\nnoticeable during use. While this may not greatly\nimpact individual users, it would be unacceptable\nin real production scenarios. Although other meth-\nods, like more strict similarity evaluation, could\nimprove positive cache hit rates, this would also\ndecrease the overall hit rate. Most current embed-\nding models are likely optimized for search scenar-\nios but may not work as well for cache matching.\nFor example, results with semantics opposite to\nthe input text are acceptable in search since they\nhave structural similarity, but this is unacceptable\nin caching scenarios. Naturally, how to obtain em-\nbeddings suitable for caching is an open area for\nexploration.\nEven with a suitable embedding model, positive\nhit rates are unlikely to reach production require-\nments, such as 99%, without decreasing cache hits.\nThe similarity evaluation module plays a core role\nin improving positive cache hit rates by filtering\nincorrect hits. Our current implementations include\nvector distance, retrieving distance, cohere rerank\n216\nAPI, and sbert cross-encoder. However, testing\nshows these methods do not sufficiently distinguish\nbetween positive and negative cache hits. To ad-\ndress this, we are using large models to judge sen-\ntence similarity and distill them into a small model\nto obtain a specialized model for textual similarity.\nAs large language models are widely adopted,\ntheir supported token counts have increased from\n2k initially to 100k. However, if a single input\nexceeds the LLM’s token count limit, it cannot\nprocess the request. Similarly, conversations with\ntotal tokens exceeding the limit must drop some\ninformation. Large token counts from long texts or\nconversations pose a challenge for caching, making\nit difficult to identify key information and gener-\nate representative vectors. Currently, we utilize\nsummary models to pre-process and shorten long\ninputs, but this approach increases cache instability,\nand its effectiveness is not optimistic. Therefore,\nspecial cache lookup methods may be needed for\nlong texts.\nAs mentioned earlier, is there any alternative\nto retrieving cache data using vector databases?\nFor example, can we use traditional databases like\nMySQL, PostgreSQL, SQL Server, or Oracle to\nstore cache data, with textual pre-rocessing to stan-\ndardize user inputs? For instance, when the inputs\nare \"tell me a joke\" and \"I want to get a joke\",\ncan we convert them to a certain string, like \"tell a\njoke\" , or a same number? Cache hits could then\nutilize string matching or numeric ranges instead\nof vectors.\n6 Conclusion\nGPTCache is a caching solution tailored for LLM\napplications. It brings the following benefits to the\nLLM app developers:\n• Less costs: Most LLM services charge fees\nbased on a combination of the number of re-\nquests and token count. GPTCache can effec-\ntively minimize expenses by caching query re-\nsults, thereby reducing the number of requests\nand tokens sent to the LLM service.\n• Faster response times: LLMs utilize gener-\native AI to produce responses in real-time,\nwhich can be time-consuming. However,\nwhen a similar query is cached, the response\ntime greatly improves, as the result is retrieved\ndirectly from the cache without interaction\nwith the LLM service. In most cases, GPT-\nCache can also offer better query throughput\nthan standard LLM services.\n• More scalable and available: LLM services of-\nten impose rate limits on the number of access\nrequests within a given timeframe. If these\nlimits are exceeded, additional requests are\nblocked until a cooldown period has elapsed,\nleading to service outages. GPTCache al-\nlows you to easily scale and handle increas-\ning query volumes, ensuring consistent per-\nformance as your application’s user base ex-\npands.\nBy utilizing semantic similarity search and vec-\ntor embeddings, GPTCache provides an effective\ncaching solution that enhances performance, re-\nduces costs, and improves scalability for applica-\ntions that use large language models. Our initial\nexperiments have shown great potential, and we\nplan to conduct more comprehensive evaluations\non diverse real-world datasets and application sce-\nnarios.\nReferences\nFelipe Almeida and Geraldo Xexéo. 2023. Word em-\nbeddings: A survey.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\n217\nTim Dettmers and Luke Zettlemoyer. 2023. The case\nfor 4-bit precision: k-bit inference scaling laws. In\nProceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings\nof Machine Learning Research, pages 7750–7774.\nPMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nElias Frantar and Dan Alistarh. 2023. SparseGPT: Mas-\nsive language models can be accurately pruned in\none-shot. arXiv preprint arXiv:2301.00774.\nYoav Goldberg and Omer Levy. 2014. word2vec ex-\nplained: deriving mikolov et al.’s negative-sampling\nword-embedding method. CoRR, abs/1402.3722.\nDongwon Lee and Wesley W. Chu. 1999. Semantic\ncaching via query matching for web sources. In\nProceedings of the Eighth International Conference\non Information and Knowledge Management, CIKM\n’99, page 77–85, New York, NY , USA. Association\nfor Computing Machinery.\nMiti Mazmudar, Thomas Humphries, Jiaxiang Liu,\nMatthew Rafuse, and Xi He. 2022. Cache me if\nyou can: Accuracy-aware inference engine for differ-\nentially private data exploration.\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space. In 1st International Conference\non Learning Representations, ICLR 2013, Scottsdale,\nArizona, USA, May 2-4, 2013, Workshop Track Pro-\nceedings.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. BERT-of-theseus: Com-\npressing BERT by progressive module replacing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7859–7869, Online. Association for Computa-\ntional Linguistics.\nJie Amy Yang, Jianyu Huang, Jongsoo Park, Ping\nTak Peter Tang, and Andrew Tulloch. 2020. Mixed-\nprecision embedding using a cache.\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\nJiang, Linjun Yang, Rangan Majumder, and Furu Wei.\n2023. Inference with reference: Lossless accelera-\ntion of large language models.\n218",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7989104986190796
    },
    {
      "name": "Cache",
      "score": 0.7095578908920288
    },
    {
      "name": "Speedup",
      "score": 0.4410059154033661
    },
    {
      "name": "Service (business)",
      "score": 0.4139469563961029
    },
    {
      "name": "World Wide Web",
      "score": 0.38480401039123535
    },
    {
      "name": "Computer network",
      "score": 0.2156590223312378
    },
    {
      "name": "Operating system",
      "score": 0.14757364988327026
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    }
  ]
}