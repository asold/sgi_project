{
  "title": "MalBERT: Using Transformers for Cybersecurity and Malicious Software Detection",
  "url": "https://openalex.org/W3133566923",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Rahali, Abir",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Akhloufi, Moulay A.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963759070",
    "https://openalex.org/W3117169309",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2973430807",
    "https://openalex.org/W2998961572",
    "https://openalex.org/W2789326120",
    "https://openalex.org/W3091477168",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3100013573",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3086651087",
    "https://openalex.org/W2215444025",
    "https://openalex.org/W3036603968",
    "https://openalex.org/W3037037884",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3011624874",
    "https://openalex.org/W2999309192",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3088714212",
    "https://openalex.org/W3091722391",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2407313496",
    "https://openalex.org/W3103472728",
    "https://openalex.org/W2974072230",
    "https://openalex.org/W3104008133",
    "https://openalex.org/W2945289329",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "In recent years we have witnessed an increase in cyber threats and malicious software attacks on different platforms with important consequences to persons and businesses. It has become critical to find automated machine learning techniques to proactively defend against malware. Transformers, a category of attention-based deep learning techniques, have recently shown impressive results in solving different tasks mainly related to the field of Natural Language Processing (NLP). In this paper, we propose the use of a Transformers' architecture to automatically detect malicious software. We propose a model based on BERT (Bidirectional Encoder Representations from Transformers) which performs a static analysis on the source code of Android applications using preprocessed features to characterize existing malware and classify it into different representative malware categories. The obtained results are promising and show the high performance obtained by Transformer-based models for malicious software detection.",
  "full_text": "MALBERT: U SING TRANSFORMERS FOR CYBERSECURITY AND\nMALICIOUS SOFTWARE DETECTION\nA PREPRINT\nAbir Rahali\near4587@umoncton.ca\nMoulay A. Akhlouﬁ\nmoulay.akhloufi@umoncton.ca\nPerception, Robotics, and Intelligent Machines Research Group (PRIME)\nDept of Computer Science\nUniversité de Moncton\nMoncton, NB, E1A 3E9, Canada\nMarch 8, 2021\nABSTRACT\nIn recent years we have witnessed an increase in cyber threats and malicious software attacks on\ndifferent platforms with important consequences to persons and businesses. It has become critical to\nﬁnd automated machine learning techniques to proactively defend against malware. Transformers,\na category of attention-based deep learning techniques, have recently shown impressive results in\nsolving different tasks mainly related to the ﬁeld of Natural Language Processing (NLP). In this paper,\nwe propose the use of a Transformers’ architecture to automatically detect malicious software. We\npropose a model based on BERT (Bidirectional Encoder Representations from Transformers) which\nperforms a static analysis on the source code of Android applications using preprocessed features to\ncharacterize existing malware and classify it into different representative malware categories. The\nobtained results are promising and show the high performance obtained by Transformer-based models\nfor malicious software detection.\nKeywords Malware detection · Cybersecurity · Transformers · Attention models · Android\n1 Introduction\nDue to the exponential growth of digitalization usage and easy access to internet technology, cyber threats on information\nsystems such as computers and smartphones increased. Malicious software (malware) is the primary tool used\nby attackers to perform cyber attacks. Different malware categories such as Trojans, Adwares, and Risktools are\nexpeditiously developed and updated with recent encryption and deformation technologies to become a more severe\nthreat to cyberspace. This rapid development incident often with harmful consequences to different data users at the\nlevel of both persons and businesses. For example, IBM reported that the average cost of a data breach is $3.86 million\nas of 2020, however, the average time to identify a breach was 207 days [1]. As result, IT security professionals and\nresearchers need to update the tools available to automatically detect new malware attacks.\nA series of studies were conducted to solve this issue. Different analysis experiments were used on the static, dynamic\nand hybrid level [2] to extract various types of features such as binaries, permissions, and API calls. The features\ncollected then are passed through detection models developed using machine learning and deep learning tools. The use\nof deep learning algorithms, for example, helped security specialists to analyze the most complex and targeted attacks.\nThe wildly used type is static analysis [ 3]. It is a known way of identifying malicious applications among benign\napplications, and this analysis focuses on the source code of software components that may be affected by malware. It is\nless expensive in terms of resources and time since no need to activate the malware by executing the code to capture the\nfeatures, it can identify the maliciousness at the code level. For static analysis, there are mainly three practices to detect\narXiv:2103.03806v1  [cs.CR]  5 Mar 2021\nA PREPRINT - MARCH 8, 2021\nand classify malware: Permission-based (verify if the requested permissions are needed for the app to ensure the normal\nbehavior on the app access and usage of the user data), Signature-based (identify if the app signature matches one of the\nmalware signatures among the library collected in advance), and speciﬁcation-based (verify if the app violates the rules\nset by experts to decide the maliciousness of a program under inspection).\nRecent research in deep learning, focus mainly on Transformer-based models such as BERT [4] and XLNet [5]. These\napproaches clearly showed impressive results in various state-of-the-art natural language processing (NLP) [6] and\ncomputer vision [7] tasks. Thanks to the attention mechanisms layers [8] added to the encoder-decoder architecture,\nTransformers can focus on the most important patterns in the data, leading to a remarkable boost in performance.\nIn this paper, we propose a malware detection approach using a Transformer-based algorithm. We experiment with\ndifferent Transformer model architectures on our data. The dataset includes 11 different malware categories namely\nadware, spyware, ransomware, clicker, dropper, downloader, riskware, SMS-sender, horse-trojan, backdoor, and banker\n[9]. Our methodology focuses on the static analysis level on the source code of Android applications, to identify\ndifferent categories of malware. Indeed, we did not limit the features to permission-based only but considered the\nwhole software code as an important set of feature representation for the analysis. We started with training the model\nwith the features after preprocessing, then a binary classiﬁcation of the apps to malicious and benign, and ﬁnally a\ncross-category classiﬁcation at the malware level.\nOur main contributions include:\n1. We propose a novel approach for malware detection using a Transformer-based model. According to our best\nknowledge, this is the ﬁrst Transformer-based method used for malware classiﬁcation.\n2. We model Android malware detection as a binary and a multi-label text classiﬁcation problem and propose a\nnovel feature representation by considering the software applications’ source code as a set of features. We\napply text preprocessing on these features to keep the important information like permissions, intents, and\nactivities.\n3. We conduct extensive experiments on our preprocessed Android dataset collected from public resources\nwith different category-annotated labels. This preprocessed dataset will be released publicly for the research\ncommunity.\n2 Background\nGenerally, sequence-to-sequence tasks are performed using an encoder-decoder model. And the RNN (Recurrent and\nRecursive Nets) architectures were the most widely-used architecture for both the encoder and decoder. But these\narchitectures have some limitations.\n2.1 RNN\nRNN or also called sequence modeling like LSTM [10], is a family of neural networks for processing sequential data.\nA recurrent network that maps an input sequence of xvalues to a corresponding sequence of output ovalues [10].\nA loss Lmeasures how far each O is from the corresponding training target y[10]. The loss Linternally computes\ny= softmax(O) and compares this to the target y[10]. In terms of limitations, RNN based architectures are hard to\nparallelize because the forward propagation graph is inherently sequential each time step may be computed only after\nthe previous one, where both the runtime and the memory cost are O(t) and cannot be reduced since states computed in\nthe forward pass must be sorted until they are reused during the backward pass.\n2.2 Encoder-Decoder\nThe seq2seq model normally has an encoder-decoder architecture [11], composed of an encoder that processes the input\nsequence and compresses the information into a context vector of a ﬁxed length. This representation is expected to be a\ngood summary of the meaning of the whole source sequence. And a decoder that is initialized with the context vector to\nemit the transformed output. The mean limitation of the encoder-decoder model is the disadvantage of this ﬁxed-length\ncontext vector design is the incapability of remembering long sentences. While the decoder needs different information\nat different time steps.\n2.3 Transformer\nThus, in order to solve the limitations of both RNN and endocer-decoder architectures, the authors of Transformers [8]\nhave proposed a solution. They rely on the seq2seq encoder-decoder by replacing RNN with attention mechanisms.\n2\nA PREPRINT - MARCH 8, 2021\nThe attention mechanism allows the Transformers to have a very long term memory. A Transformer model can \"attend\"\nor \"focus\" on all the previous tokens that have been generated. The attention mechanism allows the decoder to go back\nover the entire sentence and selectively extract the information it needs during decoding. Attention gives the decoder\naccess to all the hidden states of the encoder. However, the decoder still has to make a single prediction for the next\nword, so we can not just pass a whole sequence to it (we have to pass it some kind of synthesis vector). So it asks the\ndecoder to choose which hidden states to use and which to ignore by weighting the hidden states. The decoder then\nreceives a weighted sum of hidden states to use to predict the next word.\nIn this section, we deﬁne the context of the Transformer-based approach by presenting most of the approaches related\nto the architectures of the approaches. The Transformer in NLP is a new architecture that aims at solving sequence\nto sequence tasks while easily managing long-range dependencies. The Transformer has been proposed in [ 8]. A\nTransformer is an architecture that avoids recurrence and relies entirely on an attention mechanism to draw global\ndependencies between input and output. Prior to Transformers, dominant sequence transduction models were based on\ncomplex recurrent or convolutional neural networks that include an encoder and a decoder.\nTransformers also use an encoder and a decoder, but the elimination of recurrence in favor of attention mechanisms\nallows for much greater parallelization than methods such as RNNs and CNNs. The transformation is undoubtedly a\nhuge improvement over the seq2seq models based on RNN. But it has its own set of limitations. Attention can only\nbe paid to text strings of ﬁxed length. The text must be divided into a number of segments or pieces before being\nintroduced into the system as input and this causes context fragmentation. For example, BERT [4], a new linguistic\nrepresentation model from Google AI, uses pre-training and ﬁne-tuning to create state-of-the-art models for a wide\nrange of tasks. These tasks include question answering systems, sentiment analysis, and linguistic inference. Transfer\nlearning has been used in NLP using pretrained language models that have transformative architectures.\nFigure 1: Transformer Encoder Architecture\nBERT or Bidirectional Encoder Representations from Transformers. Figure 1 shows the different layers of the\nTransformer encoder implemented by BERT. This implementation improves standard Transformers by removing the\nconstraint of unidirectionality through the use of a pre-formed Masked Language Model (MLM) objective [12]. The\nMasked Language Model randomly masks certain elements of the input, and the objective is to predict the original\nvocabulary identiﬁer of the masked word based solely on its context. Unlike pre-training the left to the right language\n3\nA PREPRINT - MARCH 8, 2021\nmodel, the objective of the MLM allows the representation to merge the left and right context, which allows us to\npre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a Next Sentence\nPrediction task that jointly pre-trains the text pair representations.\nAlso, RoBERTa [13] an extension of BERT with some modiﬁcations in the pre-training procedure of the architecture,\nuses the same architecture. The modiﬁcations include training the model longer, with larger batches, and on a larger\namount of data. Also, the predictive objective of the following sentence was deleted. And in order to on longer\nsequences, the authors applied dynamic modiﬁcation on the masking scheme [12] applied to the training data. They also\ncollect a new dataset of comparable size to other privately used datasets to better control the effects of training set size.\nIn order to reduce the computation and training time of BERT, [14] proposed DistilBERT, a small, fast, cheap, and\nlightweight Transformer model based on the BERT architecture. This new extension uses knowledge distillation [15]\nduring the pre-training phase in order to reduce the size of a BERT model by 40 %. To exploit the inductive biases\nlearned by larger models during pre-training, the authors introduce a triple loss combining namely; language modeling\nloss, distillation loss, and cosine-distance loss.\nAnother approach based on the Transformer architecture is the autoregressive Transformer. XLNet [5], an example\nof this new implementation, exploits the best of modeling and automatic language encoding while trying to avoid its\nlimitations. Instead of using a ﬁxed forward or backward factorization order as in classical autoregressive models,\nXLNet maximizes the expected log probability of a sequence with all possible permutations of the factorization order\n[6]. Also, thanks to the permutation operation usage, the context of each position can be made up of both left and right\ntokens, which enable XLNet to capture the bidirectional context information of all positions.\nIn this paper, we used the presented models in order to evaluate our proposed approach, by conducting a list of\nexperiments on both binary and cross-category malware detection task.\n3 Related works\nOver the past years, various approaches were proposed in deep learning methods to detect malware. While these existing\nmethods are mainly divided between; on one hand the training of recurrent neural networks (RNN), and convolutional\nneural networks (CNN), on a different set of extracted features, and on another hand the usage of attention mechanisms\ncharacteristics for malware classiﬁcation.\n3.1 Neural network-based approaches\nAmong these approaches this study [16] builds AMalNet, a DN framework to learn multiple integration representations\nand family assignment with Graph CNN (GCNs) to model high-level graph semantics and use an Independent RNN\n(IndRNN) to decode deep semantic information. SeqMobile [ 17], is a behavior-based sequence approach. it uses\ndifferent recurrent neural networks (RNN). It extracts the semantic feature sequence, which can provide information\nof certain malicious behaviors, from binary ﬁles under a certain time constraint. This paper [ 18], presents a new\napproach based on OpCode-level FCG. The FCG is obtained through static analysis of Operation Code (OpCode)\nusing a Long Short-Term Memory (LSTM). the authors conduct experiments on a dataset on 1,796 Android malware\nsamples classiﬁed into two categories and 1,000 benign Android apps. The authors of [ 19] focused on step size as\nan important factor in relation to input size using RNN. They tested the model with three different feature vectors\n(hot-coding feature vector, random feature vector and Word2Vec feature vector) using hyper parameters. [20] transform\nthe android package kit (APK) ﬁle into a lightweight RGB image using a predeﬁned dictionary and intelligent mapping,\nthen apply a CNN on the obtained images for malware family classiﬁcation. Multiple other examples of DNN based\napproach [21] and [22], have been developed, with varying the feature extraction, selection, and representation methods\nin the aim of boosting the detection results.\n3.2 Approaches using Attention Mechanisms\nWhile our approach is, to the best of found knowledge, the ﬁrst study to implement Transformers directly on software\napplication and preprocessing features like text to detect malware, few approaches have attempted ﬁrst steps towards\nusing new methods such as attention mechanisms.\nFor example, [23] propose SLAM a malware detection framework build based on the characteristics of the attention\nmechanism and the sliding window method. It use a feature extraction method of the API execution sequence according\nto its semantics. [ 24] uses of an residual attention based mechanism, based on this study the proposed method\noutperformed traditional CNN models. Similarly, [25] propose a static analysis framework based on a set of N-gram\nopcodes sequence patches with a self-attention based CNN named SA-CNN. Another example proposed by [ 26],\n4\nA PREPRINT - MARCH 8, 2021\napplies a CNN with an attention mechanism to images converted from binary datasets, by calculating an attention map\nto extract characteristic byte sequence. The distinction of regions in the attention map shows regions having higher\nimportance for classiﬁcation in the image.\nWhile in our work, we propose a noval approach, we used BERT to better detect malware, we reﬁned the pre-trained\nmodel to efﬁciently learn representations of source code language syntax and semantics. Our Context-Aware network\nlearns contextual characteristics from a natural language sentences perspective thanks to the attention mechanism layers\nin the Transformer-based architecture.\n4 Methodology\nThis section explains the overall process of malware detection. The core idea of this work is to create, a malware\ndetection framework using a Transformer-based approach. To reach this goal, we conducted a static analysis on the\ncollected corpora from a natural language sentences perspective. So, we need a dataset including source code ﬁles\nand different categories of malware types. Figure 2 explains the logical ﬂow of our Android malware detection. This\nProcess is mainly divided into 4 phases. First, the Android ﬁles collection, then the Decompilation phase of the APK\nﬁles, Feature Mining, and ﬁnally Deep Learning (DL) models training experiments.\nFigure 2: Overview of the Methodology Steps\n4.1 Data Collection\nWe collected the Android applications from the Androzoo public dataset. Androzoo, one of the stae of the art android\nmalware dataset [27], is a growing collection of Android applications from several sources, including the ofﬁcial Google\nPlay app market. It currently contains 13,320,014 different APKs, each of which has been analyzed by dozens of\ndifferent antivirus products to ﬁnd out which applications are detected as malware. This public data is up to date with\nweekly analysis on the samples [28]. The data is labeled based on these analyses into malware and benign, and different\nmalware categories and families.\nBased on state-of-the-art taxonomies for Android malware categories [29] & [30] We selected 11 categories 3 namely;\nadware (displays advertising and entice a user to install it on their device), spyware (installs itself on the user device\nwith the aim of collecting and transferring information without the user is aware of it), ransomware (takes personal data\nhostage), clicker (a type of trojan that performs a form of ad fraud. These “clickers” continuously make connections to\nwebsites, consequently awarding threat actors with revenue on pay-per-click bases), dropper (a syringe program or\ndropper virus, is a computer program created to install malicious software on a target system), downloader (a type of\nTrojan horse that downloads and installs malicious ﬁles), riskware (a software whose installation can represent a risk\nfor the security of the computer, but however, not inevitably), SMS-sender (presents itself as a regular SMS messaging\napplication and uses its basic permissions to send/receive short messages), horse-trojan (is designed to damage, disrupt,\nsteal, or in general inﬂict some other harmful action on your data or network), backdoor (when introduced into the\ndevice, usually without the user’s knowledge, turns the software into a Trojan horse, and banker (is designed to steal\ndata from users’ online bank accounts as well as data from online payment systems and plastic card systems). We select\nthe list of APKs to download based on the recent creation and analysis date, then re-analyze this list with VirusTotal\n[31], to ﬁnally create our dataset list including 12,000 benign apps and 10,000 malware apps.\n5\nA PREPRINT - MARCH 8, 2021\nFigure 3: Android Malware Categories\n4.2 Preprocessing and Feature representations\nOnce the list of APKs is deﬁned, we write a script to download the ﬁles. Then, we decompiled the downloaded APKs\nusing Jdax [32], which creates folders of the apps’ ﬁles [33]. We extracted the AndroidManifest.xml ﬁle from each\nsample. The manifest presents essential information about the application to the Android system, information the\nsystem must have before it can run any of the application’s code, including the list of permissions, the activities, services,\nbroadcast receivers, content providers, the version, and the meta-data. These ﬁles are then treated as text ﬁles and passed\nthrough the preprocessing phase, in this step and to conserve the important information about the features, we apply\nspeciﬁc cleaning of the not important, mostly repeated words, in the code. We manually analyzed different examples\nand created a list of words and expressions that do not provide additional info, so the cleaning included lexicon removal,\npunctuation removal and we conserved the digits and the cases of the characters. The purpose of the preprocessing is to\nreduce the size of the input. The ﬁnal dataset format has 4 columns, the ID column, represented by the APK hash name,\nthe Text column representing the Manifest ﬁles after preprocessing, the Label column, a binary format equal to 1 if the\napp is malware and 0 if not, and ﬁnally the Category column representing the malware type name (exp: adware).\n4.3 Proposed Approach\nOnce the data is created and annotated in the right format, we split the data into train and test. We conduct all\nour experiments using BERT, we ﬁne-tuned it on our train dataset. We ﬁxed the hyperparameters based on each\nclassiﬁcation type. We train BERT to predict Malware/Benign (i.e., binary classiﬁcation) for each sample, then, to\npredict the categories of malware (i.e, multi-classiﬁcation). The Transformer architecture has speciﬁc input formatting\nsteps including the creation of special tokens and ids. We use the Transformers implementation of the hugging face\nlibrary [34] for the binary classiﬁcation of Android applications. Only the Transformer architecture, layers, and weights\nare implemented, while all data formatting must be done beforehand to be compatible with the Transformer. While\nmost pre-trained Transformers have essentially the same steps. Here we test this approach with BERT. Figure 4 gives a\ndetailed overview of our approach.\n5 Experimental results\nTo test the proposed approach, we evaluate it in terms of three main aspects: (1) the proﬁtability on large and recent\ncategorical datasets, (2) the feature representation ability for information context extraction from android apps, and (3)\nthe performance compared to the state-of-the-art approaches.\n6\nA PREPRINT - MARCH 8, 2021\nFigure 4: BERT model ﬁne-tuning process\n5.1 Experiments setup\nIn our model implementation, we used Adam optimizer and set its hyperparameters as follow; the learning rate αto\n2e-5 for binary classiﬁcation and 1e-3 for multi-classiﬁcation, α1=0,9 and α2=0,999. We set the max sequence size to\n512 and the training batch size to 32.\n5.2 Baselines\nWe compare the proposed model with several state-of-the-art methods including LSTM [35]. We selected this sequence\nmodel based on its performance in the reviewed studies. And, XLNet [ 5], RoBERTa [13], and DistilBERT [14] as\nTransformer based baselines to compare their performance to BERT.\n5.3 Evaluation Metrics\nTo evaluate the effectiveness of the model and avoid the contingency caused by the partitioning of the train and test sets,\nwe select three metrics to evaluate the model (Accuracy, MCC, and Loss), which are commonly used in classiﬁcation\nproblems. We deﬁne the following numbers; TP (true positives), FN (false negatives), FP (false positives), and TN (true\nnegatives).\n1. ACC: The accuracy is a metric for evaluating classiﬁcation models. It is equal to the number of correct\npredictions divided by the total number of predictions, as equation 1 present:\nAccuracy = TP + TF\nTP + TF + FP + FN (1)\n2. MCC: The Matthews Correlation Coefﬁcient (MCC) is bast used for binary classiﬁcation with an unbalanced\ndataset. It has a range of -1 to +1. We chose MCC over F1-score for binary classiﬁcation as recommended in\nthis study [36]. MCC equation is deﬁned as fellow :\nMCC =\nTP × TN − FP × FN\n√\n(TP + FP)(TP + FN)(TN + FP)(TN + FN)\n(2)\n3. F1: The formula for the standard F1-score is the harmonic mean of the precision and recall. A perfect model\nhas an F-score of 1. We used macro averaging in the results we presented in this paper. The formula of\nF1-score is deﬁned as follow:\nF1 = 2 ∗ TP\n2 ∗ TP + FP + FN (3)\n4. Loss: We used the cross-entropy loss, or log loss [ 37]. It measures the performance of a classiﬁcation\nmodel whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted\n7\nA PREPRINT - MARCH 8, 2021\nprobability diverge from the actual label. The cross-entropy H(p,q) of the probability distribution prelative\nto a probability distribution qis given as:\nH(p,q) =−\n∑\nx∈X\np(x) logq(x) (4)\n5.4 Results and Analysis\nWe conducted the experiments on the preprocessed dataset. Fine-tuning the pre-trained models, clearly gave the highest\naccuracy results for this classiﬁcation task compared to the LSTM baseline. The best classiﬁcation model is BERT. The\ntest metrics results of Table 1 show that each Transformer learns differently depending on each architecture. The results\nin Table 1 and Table 2 prove that BERT outperformed the other baseline models in both binary and multi-classiﬁcation\nmalware detection. For BERT, the best learning rate shows that only two epochs are required before the loss starts to\nincrease. Our ﬁne-tuning with the training set included changing the hyperparameters to boost the results. To evaluate\nthe ﬁnal results, we used different evaluation metrics. The pretrained models achieved good results overall, but BERT\nobtained the best performance in both tasks.\nAlgos ACC F1 Loss MCC\nLSTM 0.9405 0.9382 0.1521 0.9077\nXLNet 0.9579 0.9549 0.1461 0.9164\nRoBERTA 0.9533 0.9499 0.1430 0.9070\nDistilBERT 0.9542 0.9542 0.1305 0.9087\nBERT 0.9761 0.9547 0.1274 0.9559\nTable 1: Detection results using the feature representation approach across difference networks on the test dataset for\nboth binary classiﬁcation.\nAlgos ACC F1 Loss\nLSTM 0.8507 0.7816 0.3521\nXLNet 0.6232 0.7448 0.8106\nRoBERTA 0.6491 0.7670 0.7611\nDistilBERT 0.5981 0.7274 0.8505\nBERT 0.9102 0.7804 0.2214\nTable 2: Detection results using the feature representation approach across difference networks on the test dataset for\ncross-category malware classiﬁcation.\n6 Conclusion and future works\nThis paper studies the challenge of malware classiﬁcation using our novel approach for Transformer-based malware\ndetection. We detailed the malware text classiﬁcation methodology and used it for feature representation. The BERT\nbased model achieved high accuracy results for both binary and cross-category classiﬁcation, compared to the other\nbaseline pre-trained language models. The results from the experiments show that the best binary accuracy is 0.9761 and\nfor the multi-classiﬁcation, it is 0.9102. We can conclude that the proposed approach’s results of feature representation\nas text input for a Transformer-based model, are very good. So, the implementation of pre-trained linguistic models\nbased on Transformer architectures in cybersecurity tasks can outperform standard RNN models like LSTM, when\napplied on a state-of-the-art dataset like Androzoo. Future work will therefore consist of testing other models, testing\nother types of data, and creating an API to detect malware in new applications.\nReferences\n[1] IBM. Cost of a data breach report 2020. 2020.\n[2] Anusha Damodaran, Fabio Di Troia, Corrado Aaron Visaggio, Thomas H Austin, and Mark Stamp. A comparison\nof static, dynamic, and hybrid analysis for malware detection. Journal of Computer Virology and Hacking\nTechniques, 13(1):1–12, 2017.\n[3] Ya Pan, Xiuting Ge, Chunrong Fang, and Yong Fan. A systematic literature review of android malware detection\nusing static analysis. IEEE Access, 8:116363–116379, 2020.\n8\nA PREPRINT - MARCH 8, 2021\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[5] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in neural information processing\nsystems, pages 5753–5763, 2019.\n[6] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv preprint\narXiv:2009.06732, 2020.\n[7] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah.\nTransformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021.\n[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008, 2017.\n[9] kaspersky. Android mobile security threats. kaspersky, 2012.\n[10] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n[11] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances\nin neural information processing systems, pages 3104–3112, 2014.\n[12] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450, 2019.\n[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[14] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[15] Mary Phuong and Christoph Lampert. Towards understanding knowledge distillation. In International Conference\non Machine Learning, pages 5142–5151, 2019.\n[16] Xinjun Pei, Long Yu, and Shengwei Tian. Amalnet: A deep learning framework based on graph convolutional\nnetworks for malware detection. Computers & Security, page 101792, 2020.\n[17] Ruitao Feng, Jing Qiang Lim, Sen Chen, Shang-Wei Lin, and Yang Liu. Seqmobile: A sequence based efﬁcient\nandroid malware detection system using rnn on mobile devices. arXiv preprint arXiv:2011.05218, 2020.\n[18] Weina Niu, Rong Cao, Xiaosong Zhang, Kangyi Ding, Kaimeng Zhang, and Ting Li. Opcode-level function call\ngraph based android malware classiﬁcation using deep learning. Sensors, 20(13):3645, 2020.\n[19] Sudan Jha, Deepak Prashar, Hoang Viet Long, and David Taniar. Recurrent neural network for detecting malware.\nComputers & Security, 99:102037, 2020.\n[20] Farid Naït-Abdesselam, Asim Darwaish, and Chaﬁq Titouna. An intelligent malware detection and classiﬁcation\nsystem using apps-to-images transformations and convolutional neural networks. In 2020 16th International\nConference on Wireless and Mobile Computing, Networking and Communications (WiMob)(50308), pages 1–6.\nIEEE, 2020.\n[21] Zhiqiang Wang, Qian Liu, and Yaping Chi. Review of android malware detection based on deep learning. IEEE\nAccess, 8:181102–181126, 2020.\n[22] Francesco Mercaldo and Antonella Santone. Deep learning for image-based mobile malware detection. Journal of\nComputer Virology and Hacking Techniques, pages 1–15, 2020.\n[23] Jun Chen, Shize Guo, Xin Ma, Haiying Li, Jinhong Guo, Ming Chen, and Zhisong Pan. Slam: A malware\ndetection method based on sliding local attention mechanism. Security and Communication Networks, 2020, 2020.\n[24] Shamika Ganesan, Moez Krichen, Roobaea Alroobaea, Soman KP, et al. Robust malware detection using residual\nattention network. TechRxiv, 2020.\n[25] Bin Zhang, Wentao Xiao, Xi Xiao, Arun Kumar Sangaiah, Weizhe Zhang, and Jiajia Zhang. Ransomware\nclassiﬁcation using patch-based cnn and self-attention network on embedded n-grams of opcodes. Future\nGeneration Computer Systems, 110:708–720, 2020.\n[26] Hiromu Yakura, Shinnosuke Shinozaki, Reon Nishimura, Yoshihiro Oyama, and Jun Sakuma. Malware analysis\nof imaged binary samples by convolutional neural network with attention mechanism. In Proceedings of the\nEighth ACM Conference on Data and Application Security and Privacy, pages 127–134, 2018.\n9\nA PREPRINT - MARCH 8, 2021\n[27] Kevin Allix, Tegawendé F. Bissyandé, Jacques Klein, and Yves Le Traon. Androzoo: Collecting millions of\nandroid apps for the research community. In Proceedings of the 13th International Conference on Mining Software\nRepositories, MSR ’16, pages 468–471, New York, NY , USA, 2016. ACM.\n[28] Pei Liu, Li Li, Yanjie Zhao, Xiaoyu Sun, and John Grundy. Androzooopen: Collecting large-scale open source\nandroid apps for the research community. In Proceedings of the 17th International Conference on Mining Software\nRepositories, pages 548–552, 2020.\n[29] Najiahtul Syaﬁqah Ismail, Halizah Saad, Robiah Yusof, and Mohd Faizal Abdollah. General android malware\nbehaviour taxonomy. SCIENCE & TECHNOLOGY RESEARCH INSTITUTE FOR DEFENCE (STRIDE), page\n160, 2017.\n[30] Rashmi Rupendra Chouhan and Alpa Kavin Shah. A preface on android malware: Taxonomy, techniques and\ntools. International Journal on Recent and Innovation Trends in Computing and Communication, 5(6):1111–1117,\n2017.\n[31] Virus Total. Virustotal-free online virus, malware and url scanner. Online: https://www. virustotal. com/en, 2012.\n[32] jadx. jadx - dex to java decompiler. Online: https://github.com/skylot/jadx, 2012.\n[33] Nicolas Harrand, César Soto-Valero, Martin Monperrus, and Benoit Baudry. Java decompiler diversity and its\napplication to meta-decompilation. Journal of Systems and Software, page 110645, 2020.\n[34] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language\nprocessing. ArXiv, pages arXiv–1910, 2019.\n[35] Ralf C Staudemeyer and Eric Rothstein Morris. Understanding lstm–a tutorial into long short-term memory\nrecurrent neural networks. arXiv preprint arXiv:1909.09586, 2019.\n[36] Davide Chicco and Giuseppe Jurman. The advantages of the matthews correlation coefﬁcient (mcc) over f1 score\nand accuracy in binary classiﬁcation evaluation. BMC genomics, 21(1):6, 2020.\n[37] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy\nlabels. Advances in neural information processing systems, 31:8778–8788, 2018.\n10",
  "topic": "Malware",
  "concepts": [
    {
      "name": "Malware",
      "score": 0.8427561521530151
    },
    {
      "name": "Computer science",
      "score": 0.7759639620780945
    },
    {
      "name": "Software",
      "score": 0.6069046258926392
    },
    {
      "name": "Transformer",
      "score": 0.5616001486778259
    },
    {
      "name": "Computer security",
      "score": 0.5173144340515137
    },
    {
      "name": "Android malware",
      "score": 0.5140458345413208
    },
    {
      "name": "Android (operating system)",
      "score": 0.47804123163223267
    },
    {
      "name": "Encoder",
      "score": 0.4378418028354645
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42953336238861084
    },
    {
      "name": "Machine learning",
      "score": 0.3967767059803009
    },
    {
      "name": "Software engineering",
      "score": 0.3472415804862976
    },
    {
      "name": "Operating system",
      "score": 0.21375933289527893
    },
    {
      "name": "Engineering",
      "score": 0.14087116718292236
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154799132",
      "name": "Université de Moncton",
      "country": "CA"
    }
  ]
}