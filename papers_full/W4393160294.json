{
    "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models",
    "url": "https://openalex.org/W4393160294",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2097816882",
            "name": "Heng Wang",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2116903207",
            "name": "Jianbo Ma",
            "affiliations": [
                "Dolby (Netherlands)"
            ]
        },
        {
            "id": "https://openalex.org/A2521279359",
            "name": "Santiago Pascual",
            "affiliations": [
                "Dolby (Netherlands)"
            ]
        },
        {
            "id": "https://openalex.org/A1967255775",
            "name": "Richard Cartwright",
            "affiliations": [
                "Dolby (Netherlands)"
            ]
        },
        {
            "id": "https://openalex.org/A2098283194",
            "name": "Weidong Cai",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2116903207",
            "name": "Jianbo Ma",
            "affiliations": [
                "Dolby (Netherlands)"
            ]
        },
        {
            "id": "https://openalex.org/A2521279359",
            "name": "Santiago Pascual",
            "affiliations": [
                "Dolby (Netherlands)"
            ]
        },
        {
            "id": "https://openalex.org/A1967255775",
            "name": "Richard Cartwright",
            "affiliations": [
                "Dolby (Netherlands)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6608052519",
        "https://openalex.org/W6776074830",
        "https://openalex.org/W2912947616",
        "https://openalex.org/W6736965957",
        "https://openalex.org/W6781345847",
        "https://openalex.org/W3207290297",
        "https://openalex.org/W4387969125",
        "https://openalex.org/W3105017218",
        "https://openalex.org/W2768355722",
        "https://openalex.org/W6779823529",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W6802805937",
        "https://openalex.org/W2994728585",
        "https://openalex.org/W6845479124",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W4225432580",
        "https://openalex.org/W3134307371",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W3122887982",
        "https://openalex.org/W2796992393",
        "https://openalex.org/W6687537536",
        "https://openalex.org/W4321392342",
        "https://openalex.org/W3024559545",
        "https://openalex.org/W3150406472",
        "https://openalex.org/W4308613617",
        "https://openalex.org/W3110257065",
        "https://openalex.org/W6803340982",
        "https://openalex.org/W3197358873",
        "https://openalex.org/W4286588216",
        "https://openalex.org/W6801155572",
        "https://openalex.org/W4388118236",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W6746008334",
        "https://openalex.org/W4309802099",
        "https://openalex.org/W3197209004",
        "https://openalex.org/W4367359628",
        "https://openalex.org/W3207498282",
        "https://openalex.org/W4372348103",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3046890131",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W2963807156",
        "https://openalex.org/W4382132560",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3094550259",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4281736089",
        "https://openalex.org/W3036167779",
        "https://openalex.org/W4386764084",
        "https://openalex.org/W4385327621",
        "https://openalex.org/W3035626590",
        "https://openalex.org/W4296001058",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3015371781",
        "https://openalex.org/W4300980117",
        "https://openalex.org/W4318718996",
        "https://openalex.org/W2963663420",
        "https://openalex.org/W3168053944",
        "https://openalex.org/W4224035735",
        "https://openalex.org/W4321472314",
        "https://openalex.org/W4292958273",
        "https://openalex.org/W2964345931",
        "https://openalex.org/W2963799213",
        "https://openalex.org/W4308349017",
        "https://openalex.org/W4385775276",
        "https://openalex.org/W4385004765",
        "https://openalex.org/W4323706279",
        "https://openalex.org/W4307106676",
        "https://openalex.org/W197081157",
        "https://openalex.org/W4375869211",
        "https://openalex.org/W2963066677",
        "https://openalex.org/W4383045354",
        "https://openalex.org/W2526050071",
        "https://openalex.org/W4286336838",
        "https://openalex.org/W3167602185",
        "https://openalex.org/W4284898017",
        "https://openalex.org/W4376481237"
    ],
    "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively. Supplementary materials such as audio samples are provided at our demo website: https://v2a-mapper.github.io/.",
    "full_text": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by\nConnecting Foundation Models\nHeng Wang1*, Jianbo Ma2, Santiago Pascual2, Richard Cartwright2, Weidong Cai1\n1University of Sydney\n2Dolby Laboratories\n{heng.wang, tom.cai}@sydney.edu.au, {jianbo.ma, santiago.pascual, richard.cartwright}@dolby.com\nAbstract\nBuilding artificial intelligence (AI) systems on top of a set\nof foundation models (FMs) is becoming a new paradigm\nin AI research. Their representative and generative abili-\nties learnt from vast amounts of data can be easily adapted\nand transferred to a wide range of downstream tasks with-\nout extra training from scratch. However, leveraging FMs\nin cross-modal generation remains under-researched when\naudio modality is involved. On the other hand, automati-\ncally generating semantically-relevant sound from visual in-\nput is an important problem in cross-modal generation stud-\nies. To solve this vision-to-audio (V2A) generation problem,\nexisting methods tend to design and build complex systems\nfrom scratch using modestly sized datasets. In this paper, we\npropose a lightweight solution to this problem by leverag-\ning foundation models, specifically CLIP, CLAP, and Audi-\noLDM. We first investigate the domain gap between the la-\ntent space of the visual CLIP and the auditory CLAP mod-\nels. Then we propose a simple yet effective mapper mech-\nanism (V2A-Mapper) to bridge the domain gap by translat-\ning the visual input between CLIP and CLAP spaces. Con-\nditioned on the translated CLAP embedding, pretrained au-\ndio generative FM AudioLDM is adopted to produce high-\nfidelity and visually-aligned sound. Compared to previous ap-\nproaches, our method only requires a quick training of the\nV2A-Mapper. We further analyze and conduct extensive ex-\nperiments on the choice of the V2A-Mapper and show that\na generative mapper is better at fidelity and variability (FD)\nwhile a regression mapper is slightly better at relevance (CS).\nBoth objective and subjective evaluation on two V2A datasets\ndemonstrate the superiority of our proposed method com-\npared to current state-of-the-art approaches - trained with\n86% fewer parameters but achieving 53% and 19% improve-\nment in FD and CS, respectively. Supplementary materials\nsuch as audio samples are provided at our demo website:\nhttps://v2a-mapper.github.io/.\nIntroduction\nFoundation models (FMs), trained on large-scale data and\noften making use of self-supervised learning, offer problem-\nagnostic representative or generative capabilities to down-\nstream tasks via adaptation (Bommasani et al. 2021). They\n*Work done during an internship at Dolby.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nvision encoder\nmel-spectrogram generatorvocoder\nlatent code generator\nvision encoder\nInference\nTrainingvocoder\nmel-spectrogram generatorlatent code generator\nmel-spectrogram encoder mel-spectrogram encoder\n(a) Previous V2A generation approaches.\nTraining\nInferencetrain from scratch separately\npretrained & frozensupervision\nVisual FM\n Audio Generator FM\nV2A-Mapper\nAudio FM\n(b) Our lightweight solution to V2A generation.\nFigure 1: Schematic illustrations of training and inference\npipelines from previous V2A algorithms and our lightweight\nsolution, respectively. Leveraging foundation models (FMs),\nwe only require the training of a single V2A-Mapper while\ncurrent works involve multiple modules to train.\nhave demonstrated robust generalization and knowledge\ntransfer ability across a broad spectrum of tasks in recent\nAI research (Zhou et al. 2023; Cao et al. 2023; Yin et al.\n2023). Despite success in many uni-modal tasks spanning\nlanguage (Paa√ü and Giesselbach 2023), vision (Awais et al.\n2023), and audio (Li et al. 2023), the adaptation of FMs\nin problems involving multiple modalities such as cross-\nmodal generation is greatly dominated by vision-language\nresearch (Du et al. 2022). Although attempts (Ao et al. 2022;\nKreuk et al. 2023; Yang et al. 2023; Huang et al. 2023; Liu\net al. 2023a,b; Yuan et al. 2023; Ghosal et al. 2023) have\nbeen made lately to bring FMs into text-to-audio genera-\ntion and achieved remarkable performance, the viability of\nadopting FMs in vision-to-audio generation is still unclear.\nVision and audio are two essential and correlated sources\nthrough which people perceive the world. Humans have the\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15492\nability to imagine the corresponding sound when just ob-\nserving a visual event (Owens and Efros 2018). Mimicry\nof this human-like cross-modal generation ability is appli-\ncable to various scenarios such as enhancing the experience\nof immersion in virtual reality, automating video editing for\ncontent creators, and assisting people with visual impair-\nment (Ghose and Prevost 2020; Luo et al. 2023). Such rich\nvisual-audio consistency and wide application have drawn\nconstant interest in vision-to-audio (V2A) generation (Wei\net al. 2022). Not restricted to a specific in-domain sound type\n(e.g., background music (Di et al. 2021), dance music (Zhu\net al. 2023b), speech (Prajwal et al. 2020)), in this paper, we\naim to generate natural sound from visual input in more di-\nverse real-world scenarios, a V2A task that poses a markedly\nelevated level of difficulty (Zhou et al. 2018).\nTo solve this open-domain V2A generation problem, cur-\nrent methods (Iashin and Rahtu 2021; Sheffer and Adi 2023;\nDong et al. 2023) often involve a complex system of sep-\narately optimized submodules trained with limited size of\ndatasets as illustrated in Fig. 1(a). It could be cumbersome\nand resource-intensive to train each module individually and\nthe generalization capability of each module could be re-\nstricted due to the lack of sufficient training data.\nIn this work, we explore the feasibility of adopting foun-\ndation models in open-domain vision-to-audio generation\ntask. As shown in Fig. 1(b), our lightweight method only re-\nquires the training of a V2A-Mapper to bridge the domain\ngap between the vision representative FM CLIP (Radford\net al. 2021) and the audio generative FM AudioLDM (Liu\net al. 2023a). The V2A-Mapper is supervised by the au-\ndio representative FM CLAP (Wu et al. 2023) to learn the\ntranslation from visual space to auditory space. Leveraging\nthe generalization and knowledge transfer ability of foun-\ndation models, the V2A-Mapper is trained with the same\nmodestly sized dataset but the overall system can achieve\nmuch better performance. Our contribution includes: 1) in-\nvestigating the potential of bringing FMs into the field of\nvision-to-audio generation; 2) proposing a simple but effec-\ntive V2A-Mapper to connect visual and auditory FMs; 3)\ninvestigating both generative and regression strategies of the\nV2A-Mapper; 4) both subjective and objective evaluation on\ntwo V2A datasets demonstrate the efficiency and effective-\nness of our method - it is trained with 86% fewer parameters\nbut can achieve up to 53% and 19% improvement in fidelity\n(FD) and relevance (CS).\nRelated Works\nVision-to-Audio Generation. Earlier V2A works (Owens\net al. 2016; Chen et al. 2017; Hao, Zhang, and Guan\n2018) deal with limited sound in controlled environments.\nVEGAS (Zhou et al. 2018) for the first time introduced\nopen-domain sound generation from in-the-wild visual in-\nput. But VEGAS and later works (Chen et al. 2018,\n2020b) had to train a separate model for each sound type\nwhich is hard to scale up. To solve this issue, SpecVQ-\nGAN (Iashin and Rahtu 2021) designed the first label-free\napproach where a single model can produce diverse sound\ntypes. SpecVQGAN used a pretrained image classifier net-\nwork to extract visual features from which a Transformer-\nbased (Vaswani et al. 2017) autoregressive model synthe-\nsizes the mel-spectrogram. Upgrading this label-free ap-\nproach, Im2Wav (Sheffer and Adi 2023) used the vision\nfoundation model CLIP (Radford et al. 2021) to get vi-\nsual features of multimodal semantic information. Instead\nof predicting the mel-spectrogram directly, Im2Wav au-\ntoregressively generates its latent code based on the vi-\nsual prompt and a VQ-V AE (Van Den Oord, Vinyals et al.\n2017) is trained to encode and decode between the mel-\nspectrogram and the latent space as shown in Fig. 1(a).\nSimilar to Im2Wav, CLIPSonic-IQ (Dong et al. 2023) also\nadopted CLIP but they trained a diffusion model (Nichol and\nDhariwal 2021) to directly generate the mel-spectrogram as\nin SpecVQGAN. All of these attempts train multiple mod-\nules with limited amount of data from scratch. In this pa-\nper, we propose to utilize FMs to inherit their generaliza-\ntion ability obtained from large-scale training. Optimizing a\nV2A-Mapper to connect FMs with the same modestly sized\ndataset, our method is lightweight in the training phase and\neffective in the generalization capability.\nFoundation Model Adaptation. Adapting FMs to down-\nstream tasks has been actively explored in NLP for\nuni-modal tasks, which can be categorized into prompt-\nbased (Le Scao and Rush 2021), fine-tune-based (Za-\nken, Goldberg, and Ravfogel 2022; Hu et al. 2021),\nand lightweight adapter-based methods (Houlsby et al.\n2019). When introducing this new paradigm into multi-\nmodal domain, pioneering works in the vision-language\n(VL) field follow the third strategy and freeze FMs to\navoid catastrophic forgetting (McCloskey and Cohen 1989).\nPICa (Yang et al. 2022) considered language FM GPT3\nas a knowledge base for visual question answering tasks\nwhile ClipCap (Mokady, Hertz, and Bermano 2021) and\nFlamingo (Alayrac et al. 2022) learnt auxiliary modules (i.e.,\ninterleaving new layers or tokens) to utilize vision and lan-\nguage FMs for image captioning. Compared to VL field,\nthere is much less research on FM adaptation in vision-\naudio domain. In this paper, we propose a simple yet ef-\nfective V2A-Mapper to connect visual and auditory FMs for\nopen-domain V2A generation task. In line with VL works,\nwe keep our FMs frozen but, unlike previous attempts, we\ndo not change the inner architecture of FMs. Our method\nkeeps FMs completely intact and only adds a mapper, which\nguarantees easy deployment and updating.\nMethod\nOur lightweight solution includes a visual encoder FM\n(CLIP), an audio encoder FM (CLAP), an audio genera-\ntor FM (AudioLDM), and a trainable V2A-Mapper. Fig. 2\npresents how we train the V2A-Mapper with frozen CLIP\nand CLAP models and how we incorporate it with frozen\nCLIP and AudioLDM models to produce high-fidelity and\nvisually-aligned sound. In this section, we first revisit the\nadopted foundation models. We then analyze the domain gap\nbetween visual and auditory spaces and introduce how we\ntrain the V2A-Mapper to bridge the gap. Lastly, we present\nthe details of our generative diffusion-based V2A-Mapper.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15493\nCLAP Audio Encoder\nCLIP embeddingPredicted CLAP embedding\nMSE loss\nCLIP Image Encoder\nV2A-Mapper\nTarget CLAP embedding\nCLIP Image Encoder\nPretrained & FrozenAggregator\n*train from scratchImage sequence\nùëâ! ùê∏\"Audio\nCLIP embeddingùëâ\nAudioLDMcondition\ntext-audio spacetext-image spaceV2A-Mapper\nCLIP Image Encoder\nThe only trainable partùê¥\nùê∏# ùê∏\"!\nùúé\nùúé\n Training\nInference\nvideo\nimage\nùëâ$\n(ùê∏!)#\n(ùê∏$)#\nFigure 2: Top: Our lightweight V2A-Mapper training process. We first extract the source visual embedding Ev and the target\naudio embedding Ea with frozen pretrained foundation models CLIP and CLAP. We explore different aggregatorsœÉ to project\nthe video data into a single feature vector. We then train the proposed V2A-Mapper using the audio-visual pair {Ev, Ea} with\nMSE loss. Bottom: The compact inference pipeline of our method for vision-to-audio generation. We first adopt pretrained CLIP\nimage encoder to project video/image into text-image space (the aggregation process for video input is omitted for brevity) and\nthen use the trained V2A-Mapper to translate the visual embedding into CLAP text-audio space. Conditioned on the pseudo\nCLAP audio embedding, AudioLDM can be utilized to produce the sound waveform.\nSelected Foundation Models\nWe choose the following foundation models because they\nare currently the state-of-the-art FMs for vision represen-\ntation, audio representation, and audio generation, respec-\ntively. They can be replaced given better alternatives.\nCLIP. As our V2A generation task spans across two\nmodalities, adapting multimodal FMs is a natural way of\nutilizing their semantic features for tasks involving multi-\nple domains (Lu et al. 2021). CLIP (Radford et al. 2021) is\na text-image representation model which is trained to max-\nimize the similarity between 400M paired text and image\ndata via contrastive learning. Since the vision space learnt\nby CLIP is guided by language supervision which is of high-\nlevel semantic meaning, the visual feature is rich in semantic\ninformation. Therefore, we use a pretrained CLIP model to\nextract the features of visual prompts.\nCLAP. CLAP (Wu et al. 2023) is currently the largest au-\ndio representation FM trained with 2.5M text-audio paired\ndata. Similar to CLIP, CLAP learns a joint text-audio em-\nbedding space via contrastive learning under the language\nsupervision. A critical reason we choose CLIP and CLAP is\nthat they both share the text modality as a common domain\nduring their training. We assume text could serve as a bridge\nwhich makes the translation from vision to audio easier.\nAudioLDM. AudioLDM (Liu et al. 2023a) is a continuous\nlatent diffusion model (LDM) trained in a self-supervised\nway with 3.3M 10-second audio clips. Conditioned on\nCLAP audio embedding, it generates the latent code of au-\ndio mel-spectrogram which can be decoded and converted\ninto audio waveform. The original work only explores the\nusage of the LDM part in text-to-audio (T2A) generation\ntask. Since CLAP represents text and audio jointly, Audi-\noLDM can directly take text as input when being adapted to\nT2A task. We note that despite being proposed specifically\nfor T2A generation task, AudioLDM is expected to adapt\nmore naturally to audio features. This inspires us to ponder\nif we could translate a vision feature into its corresponding\naudio embedding in CLAP space, then we could keep Au-\ndioLDM completely intact and utilize it as an off-the-shelf\naudio generator FM.\nBridge the Domain Gap Between Vision and Audio\nWe first investigate if there exists a domain gap between the\nvision and audio spaces learnt by CLIP and CLAP respec-\ntively. Following (Liang et al. 2022), we measure it by ran-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15494\n(a)\n(b)\nFigure 3: We visualize the domain gap between CLIP im-\nage space and CLAP audio space in (a). In (b), we present\nthe process of closing the domain gap during the training of\nthe V2A-Mapper. The accompanying histograms display the\ncosine similarity between paired embeddings from two do-\nmains. The larger the value is, the closer two domains are.\ndomly selecting 5000 samples from the video dataset VG-\nGSound (Chen et al. 2020a) to get an estimation of both the\nvisual and auditory feature distributions. Specifically, we en-\ncode the video frames into 512-d feature vectors with pre-\ntrained CLIP image encoder and average them along the\ntime axis to get one single embedding for each video. For\naudio data, we project each audio sample into 512-d fea-\nture vector with pretrained CLAP audio encoder. We then\nuse UMAP visualization (Sainburg, McInnes, and Gentner\n2021) to project 5000 CLIP image embeddings and 5000\nCLAP audio embeddings into the same 2-d space. As shown\nin Fig. 3(a), the average cosine similarity between paired vi-\nsual (CLIP) and audio (CLAP) features is near 0 and there\nindeed exists a considerable gap between CLIP image do-\nmain and CLAP audio domain.\nTo bridge the domain gap, we propose to train a mapper,\nnamely V2A-Mapper, between CLIP and CLAP so that the\nvisual embedding could be translated into the CLAP space.\nThe upper part of Fig. 2 shows the training pipeline. A video\nVi is a sequence of n images\n\b\nV 1\ni , ..., Vn\ni\n\t\n. To get the vi-\nsual embedding for a video, we use frozen CLIP model to\nencode each frame into 512-d feature vector to get a set of\nframe features\n\b\n(E1\ni )v, ...,(En\ni )v\t\n. We then use an aggrega-\ntor function œÉ to get a single vector Ev\ni as the visual feature\nfor the video input. The aggregator function could be: 1) ran-\ndomly picking one vector; 2) picking the vector of the mid-\ndle frame; 3) averaging along time axis. According to the\nexperiments, the third option obtains the best performance\nin both fidelity and relevance. Similarly, for the paired au-\ndio data Ai, we encode it into 512-d feature vector Ea\ni with\nfrozen CLAP model. Once we have paired visual features\nEv\ni and auditory features Ea\ni , we can train the mapper to\nconvert the CLIP embedding Ev\ni into a pseudo CLAP em-\nbedding Ea‚Ä≤\ni . We use Mean Square Error loss to guide the\ntraining. The training process can be formulated as below:\nL = Ei‚àº[1,K]\nh\n‚à• Ea\ni ‚àí Ea‚Ä≤\ni ‚à•2\ni\n, (1)\nwhere K is the batch size and Ea‚Ä≤\ni is from mapper(Ev\ni ).\nFig. 3(b) visualizes the domain shift after the training.\nSince the mapper is randomly initialized at the beginning,\nthe translated embedding cluster is still far from the target\nCLAP space as displayed in Fig. 3(b)(i). When training fin-\nishes, the translated space and the target CLAP space be-\ncome overlapped as suggested in Fig. 3(b)(ii) indicating the\nmapper is optimized successfully.\nDiffusion-based V2A-Mapper\nSince the mapper is expected to project the embedding from\nvisual space to audio space, a natural way to implement\nthe mapper is a stack of multilayer perceptrons (MLPs) as\na one-to-one regression task. Inspired by DALLE2‚Äôs prior\nmodel (Ramesh et al. 2022), we consider the projection pro-\ncess as a conditional generation task, which models a one-to-\nmany mapping ensuring the diversity and generalization of\nthe target audio distribution. Specifically, we train the map-\nper as a diffusion model (Ho, Jain, and Abbeel 2020; Song\net al. 2020). It includes a forward process where Gaussian\nnoises are gradually added to the target audio embedding\nEa\ni,0 until it approaches to a standard Gaussian distribution\nEa\ni,T (i.e., completely random) for T timesteps and a re-\nverse process where the target is gradually recovered from\nthe noisy distribution by canceling the added noises with a\nnetwork in a recursive manner. Following DALLE2, instead\nof predicting the intermediate noises added at each step (Ho,\nJain, and Abbeel 2020), we directly predict the target audio\nembedding. Therefore, we train the mapper network fŒ∏ to\npredict audio embedding Ea\ni,0 based on the timestep t, the\nnoisy audio embedding Ea\ni,t at timestep t, and the condition\nvisual embedding Ev\ni . Hence, the training objective in Eq. 1\ncan be formulated as:\nL = Ei‚àº[1,K],t‚àº[1,T]\n\u0002\n‚à• Ea\ni,0 ‚àí fŒ∏\n\u0000\nt, Ea\ni,t, Ev\ni\n\u0001\n‚à•2\u0003\n. (2)\nWe experiment with two different architectures for the\nmapper network fŒ∏ - simple MLPs and Transformer. For\nthe Transformer variant, we craft a learnable token of 512-\nd whose output from the Transformer is considered as the\nrecovered audio embedding. We then take the time embed-\nding, noisy audio embedding, as well as the visual condition\nas three other tokens of the same shape (i.e., 512-d) to the\nTransformer encoder to obtain the recovered audio embed-\nding. For the simple MLP variant, we concatenate all the\nthree tokens as input and output the final 512-d vector as the\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15495\nMethod\nVGGSound ImageHear Infer. Time (s) ‚Üì #Trainable Param. (M) ‚Üì\nFD ‚Üì FAD ‚Üì CS ‚Üë CS ‚Üë\nReference 0 0 8.925 - - -\nIm2Wav 51.500 6.005 7.827 9.843 864.12 360.40\nCLIPSonic-IQ 27.124 3.495 7.251 11.392 53.94 142.58\nOurs 24.168 0.841 9.720 11.950 35.45 48.83\nTable 1: Objective comparison with SOTA methods on VGGSound (video-to-sound generation) and ImageHear (image-to-\nsound generation). The inference time is measured as the average time spent for 100 samples through the whole pipeline from\ninput visual prompts to output waveforms on one NVIDIA RTX A6000 GPU. Our method achieves the best on all the objective\nmetrics.\nMethod\nVGGSound ImageHear\nFidelity ‚Üë Relevance ‚Üë Fidelity ‚Üë Relevance ‚Üë\nReference 3.580¬±0.455 4.178¬±0.533 - -\nIm2Wav 1.838¬±0.511 2.415¬±0.645 1.840¬±0.502 2.705¬±0.398\nCLIPSonic-IQ 2.533¬±0.522 2.140¬±0.551 2.888¬±0.502 3.215¬±0.291\nOurs 2.845¬±0.491 2.808¬±0.651 3.425¬±0.459 3.310¬±0.295\nTable 2: Subjective comparison with SOTA methods on VGGSound (video-to-sound generation) and ImageHear (image-to-\nsound generation). Our lightweight solution outperforms previous methods in both sound quality and the relevance to visual\nprompt from a human perception perspective.\npredicted audio embedding via fully-connected network. We\nfind the Transformer is a better way to incorporate the con-\ndition compared to simple concatenation in MLPs.\nExperiments\nExperimental Setup\nDatasets. We train our V2A-Mapper and all the vari-\nants on VGGSound video dataset (Chen et al. 2020a). VG-\nGSound contains 199,176 10-second video clips extracted\nfrom videos uploaded to YouTube with audio-visual corre-\nspondence. Note that VGGSound has never been used as\ntraining data for the foundation models we adapt. Following\nthe original train/test splits, we train on 183,730 videos and\nevaluate on 15,446 videos. To testify the generalization abil-\nity of our V2A-Mapper, we also test on out-of-distribution\ndataset ImageHear (Sheffer and Adi 2023) which contains\n101 images from 30 visual classes (2-8 images per class).\nWe generate 10-second audio samples for all the evaluations.\nMetrics. We measure the performance on two aspects, fi-\ndelity and the relevance to the visual prompt. Specifically,\nwe use Fr ¬¥echet Distance (FD) to measure the overall qual-\nity and variability of generated audio clips. FD computes the\ndistance of embedding distributions between the synthesized\nand the real samples. To compare with previous methods, we\nalso compute the Fr ¬¥echet Audio Distance (FAD) (Kilgour\net al. 2019). FD and FAD differ at the embedding extractor -\nFD uses PANNs (Kong et al. 2020) while FAD adopts VG-\nGish (Hershey et al. 2017). Similar to (Liu et al. 2023a),\nwe choose FD as our main evaluation metric regarding the\nsound quality since PANNs is superior to VGGish by con-\nsidering long distance temporal change. For the relevance\nevaluation, we use CLIP-Score (CS) (Sheffer and Adi 2023)\nto get the cosine similarity between the CLIP embedding of\nthe visual input and the Wav2CLIP (Wu et al. 2022) embed-\nding of the generated sound. As Wav2CLIP learns an audio\nencoder via contrastive loss on VGGSound with the guid-\nance of frozen CLIP image encoder, if the generated sound\nmatches the visual input, the Wav2CLIP embedding is ex-\npected to be similar to its paired CLIP embedding.\nSubjective Testing. To complement the objective metrics,\nwe also conduct a listening test to measure the fidelity of the\ngenerated audio clips and their relevance to visual prompts\nfrom a human perception perspective. We ask 20 listeners to\nrate audio clips of 20 randomly selected visual samples on a\ndiscrete 5-point scale in terms of fidelity and relevance, re-\nspectively. The average rating across all listeners for each al-\ngorithm is computed as Mean Opinion Score (MOS) (Inter-\nnational Telecommunication Union 1996). We also re-code\nthe responses into paired comparisons and infer the relative\nstandings via indirect scaling (Agresti 1992). We calculate\nthe degree by which other approaches exceed our method as\nthe Just Meaningful Difference (JMD) score (e.g. a negative\nvalue indicates inferiority of other algorithms compared to\nours). More details of our human evaluation are provided in\nthe supplementary.\nImplementation Details. We use ‚ÄúViT-B/32‚Äù version for\nCLIP model 1. For CLAP model and audio generator, we\nuse pretrained models from AudioLDM2. For the diffusion-\n1https://github.com/openai/CLIP\n2https://github.com/haoheliu/AudioLDM\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15496\n253035404550\nFidelity (FD) \n7.5\n8.0\n8.5\n9.0\n9.5Relevance (CS) \nIm2Wav\nCLIPSonic\nOurs\nFigure 4: Our method achieves better results on both metrics\nand contains fewer trainable parameters (smaller circle size).\nbased V2A-Mapper, we use a cosine noise schedule with\n1000 diffusion steps during training and 200 steps at infer-\nence time. We use AdamW with a learning rate of 1.1e-\n4, a batch size of 448 visual-audio embedding pairs, and\na dropout rate of 0.1 in classifier-free guidance. We pro-\nvide more implementation details including datasets used in\nadopted FMs, the full experiments of architecture hyperpa-\nrameter tuning, and the guidance scale tuning in the supple-\nmentary.\nCompare with SOTA\nIm2Wav (Sheffer and Adi 2023) is the current state-of-the-\nart method in open-domain vision-to-audio generation. It\ninvolves training of two Transformer decoders of differ-\nent scales for latent code generation, a VQ-V AE for au-\ndio mel-spectrogram encoding and decoding, and a vocoder\nfor waveform conversion. CLIPSonic-IQ (Dong et al. 2023)\nis a concurrent work to ours and they train a diffusion\nmodel to directly generate mel-spectrogram conditioned on\nvisual representation. They also require the training of a\nBigVGAN (Lee et al. 2022) to convert the generated mel-\nspectrogram into audio waveform. Compared to these meth-\nods, our approach only requires the training of a single V2A-\nMapper. Trained with the same modestly sized VGGSound\ndata, our method achieves better performance as a result of\nthe knowledge transfer from foundation models.\nObjective Results. Tab. 1 shows that the proposed method\nachieves superior performance in all objective metrics. We\nalso plot the comparison on VGGSound in Fig. 4 to show-\ncase our method achieves better results on both relevance\nand fidelity and contains fewer trainable parameters. Com-\npared to Im2Wav, our method trains with 86% fewer param-\neters but achieves 53% and 19% improvement in FD and\nCS, respectively. It is also noticeable that our method is sig-\nnificantly faster than Im2Wav (x24 faster) during inference.\nOur method also outperforms CLIPSonic-IQ in all the met-\nrics with fewer parameters and faster inference speed. Note\nthat our method exceeds even the reference for the relevance\nmetric (CS). We conjecture that this is because VGGSound\ncontains noisy data whose audio and visual streams might\nnot be highly-relevant, which could suggest the proposed\nmethod is robust to noisy training data. We recommend read-\ners to watch the sports live video in our demo website to\nIm2Wav CLIPSonic-IQ Ours Reference\n4\n3\n2\n1\n0\n1\nJust Meaningful Difference (JMD)\nSubjective Fidelity\nVGGSound\nImageHear\n(a) Fidelity.\nIm2Wav CLIPSonic-IQ Ours Reference\n1\n0\n1\n2\nJust Meaningful Difference (JMD)\nSubjective Relevance\nVGGSound\nImageHear\n(b) Relevance.\nFigure 5: The Just Meaningful Difference steps of current\nmethods relative to our algorithm with 95% bootstrap confi-\ndence intervals.\nobserve this phenomenon.\nSubjective Results. As shown in Tab. 2 and Fig. 5, our\nmethod exceeds previous works in both fidelity and rele-\nvance. We notice that the usage of diffusion model could\nespecially boost the audio quality as indicated by the im-\nprovement achieved by both CLIPSonic-IQ and our method.\nWhile CLIPSonic-IQ fails at the relevance aspect when tak-\ning videos as input, our method consistently outperforms the\nSOTA method on both videos and images. However, we note\nthat there is still a gap between our performance and the\nground truth. Empirically, we find temporal alignment to be\na major issue that leads to unsatisfactory relevance rating,\nwhich we will attempt to address in our future work.\nAblation Study\nDifferent Ways of Utilizing FMs. Since AudioLDM is\nproposed for text-to-audio generation, the naive way of uti-\nlizing it for V2A synthesis is interleaving a captioning model\nto generate text input as shown in Fig. 6(a). To verify this\nvision-txt-audio idea, we adopt SOTA captioner BLIP (Li\net al. 2022) to generate descriptions for images. For video-\nto-audio generation, we use the tag information provided in\nVGGSound. As reported in Tab. 3, although using text as\nbridge could mitigate the gap to some extent, it is still in-\nferior to our method with the V2A-Mapper in both fidelity\nand relevance. This result indicates that the captioner is ac-\ntually a bottleneck whose performance would directly affect\nwhat audio is to be generated from AudioLDM. We provide\ntwo examples where the captioner fails at predicting correct\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15497\n‚Äúthe fire is glowing brightly in the brick fireplace‚Äù\nCaptioner\nbottleneckAudioLDM\ntext-audio space\nCLAP Text Encoder\n(a) vision-text-audio.\nCLIP \nImage \nEncoder\nAudioLDM\ntext-image space\n(b) w/o mapper.\nFigure 6: Different ways of using FMs. (a) adopts a cap-\ntioner to generate text description as a bridge to use Audi-\noLDM while (b) directly puts the visual features from CLIP\nimage encoder as the condition to AudioLDM.\nMethod\nVGGSound ImageHear\nFD ‚Üì CS ‚Üë CS ‚Üë\nvision-txt-audio 56.397 6.672 7.310\nw/o mapper 72.527 5.258 4.026\nw/ mapper 24.168 9.720 11.950\nTable 3: Ablation study with different ways of using FMs\nfor vision-to-audio synthesis.\nobject category in ‚ÄúWhy Vision-Text-Audio is Bottlenecked\nby the Captioner‚Äù section of our demo website. We suggest\nreaders to check the audio results to examine the bottleneck\nchallenge. Instead of decoding the visual condition into text\nformat, our V2A-Mapper keeps the visual information as its\nlatent code form and explicitly translates it from CLIP‚Äôs vi-\nsual space to CLAP‚Äôs audio space, which could avoid infor-\nmation loss occurred during vision-txt-audio conversion. If\nthe V2A-Mapper is skipped as illustrated in Fig. 6(b), the\ndomain gap between vision and audio space prevents Au-\ndioLDM from generating high-fidelity and visually-relevant\nsound. Audio examples showcasing the difference are pre-\nsented in ‚ÄúDomain Gap Bridging Process‚Äù section of our\ndemo website. The recent text-to-audio generation work\nMake-An-Audio (Huang et al. 2023) trained their audio gen-\nerator with CLIP text embedding and adopted CLIP image\nembedding as input to handle vision-to-audio generation.\nSimilar to the ‚Äúw/o mapper‚Äù strategy, the domain gap be-\ntween the visual condition and the target embedding space\nwhich their audio generator works on is not addressed. We\nrefer readers to our demo website to observe the comparison\nwith Make-An-Audio.\nInside the Mapper: Generative vs. Regression. The\nV2A-Mapper can be implemented in a generative or a re-\ngression strategy. A generative V2A-Mapper learns a one-\nto-many mapping while a regression one builds a one-to-\none projection. As displayed in Tab. 4, although regression\nmodel could learn a slightly better relevance due to the one-\nto-one mapping, the generated sound lacks diversity and fi-\nArch. of the V2A-Mapper\nVGGSound ImageHear\nFD ‚Üì CS ‚Üë CS ‚Üë\nRegression MLPs 35.059 9.927 12.048\nTransformer 29.378 10.076 12.317\nGenerative diff. w/ MLPs 28.803 8.685 10.449\ndiff. w/ Transformer 24.168 9.720 11.950\nTable 4: Ablation study with different V2A-Mapper strate-\ngies (regression vs. generative) and architectures (MLPs vs.\nTransformer).\nAggregation\nVGGSound ImageHear\nFD ‚Üì CS ‚Üë CS ‚Üë\nrandom 24.826 9.200 11.465\nmiddle 25.569 9.192 11.901\naverage 24.168 9.720 11.950\nTable 5: Ablation study with different ways of aggregation\nfor video feature representation during training.\nMethod\nVGGSound ImageHear\nFD ‚Üì CS ‚Üë CS ‚Üë\nw/o mapper BLIP 53.621 4.948 4.314\nCLIP 72.527 5.258 4.026\nw/ mapper BLIP 24.788 9.402 10.836\nCLIP 24.168 9.720 11.950\nTable 6: Ablation study with different vision-language mod-\nels.\ndelity as suggested by much worse FD scores. A generative\nmapper is critical to ensure the variability as also observed\nin text-to-image synthesis (Ramesh et al. 2022). To show-\ncase the diversity of our method, we provide three samples\nfor each visual input in the ‚ÄúVariability of Our V2A Gen-\neration Model‚Äù section of our demo website. And compared\nto linear projections, the attention mechanism used in Trans-\nformer could integrate the visual condition in a better way.\nDifferent Aggregator MethodsœÉ. We explore three dif-\nferent ways of aggregating visual information of videos: 1)\nrandomly select one frame as the key frame to represent the\nvideo; 2) instead of using random frame, choose the middle\none; 3) average the CLIP features of all the frames along the\ntime axis. Tab. 5 shows the performance of models trained\nwith different aggregation methods. Since the task is to gen-\nerate a large time-span (10 seconds) of a highly dynamic sig-\nnal (audio), having time-related information in the condition\ncould help. The average of abstract frame embeddings with\nrich semantic contents throughout the temporal dynamics is\na better summary of the video than a single frame.\nDifferent Pretrained Vision FMs.Our V2A-Mapper can\nbe generalized to other vision-language models such as\nBLIP (Li et al. 2022). As shown in Tab. 6, the proposed\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15498\nMethod\nVGGSound ImageHear Time (s) ‚Üì #Param. (M) ‚Üì\nFD ‚Üì CS ‚Üë CS ‚Üë\naudioldm-s 25.635 9.547 11.586 9.33 185.04\naudioldm-s-v2 24.168 9.720 11.950 9.33 185.04\naudioldm-l 25.130 9.531 12.016 11.58 739.14\nTable 7: Ablation study with different pretrained Audi-\noLDM models.\n(a) Guided by image.\n‚Äúa woman is playing guitar in a rock-n-roll style‚Äù \n (b) Guided by text.\nFigure 7: Our V2A-Mapper enables interpolation guided by\nboth image and text. Audios are provided in demo website.\nV2A-Mapper can boost the performance of both CLIP-\nand BLIP-based systems. We also note that no matter what\nvision-language model is used and how big the domain gap\nbetween the vision and audio spaces is, the proposed V2A-\nMapper can bridge the gap and translate visual information\ninto audio space - two systems achieve similar performance\nwith the proposed V2A-Mapper.\nDifferent Pretrained Audio FMs. We ablate with dif-\nferent pretrained audio generators from AudioLDM: 1)\naudioldm-s is the base model; 2) audioldm-s-v2 is the base\nmodel but trained with more steps; 3) audioldm-l is the\nmodel with larger architecture. As shown in Tab. 7, either\nscaling the model up or optimizing its training for longer\nsteps can help enhance the performance to some extent.\nTherefore, we hypothesize that a better audio generator FM\ncould further improve the quality and relevance in the future.\nLatent Space Interpolation\nAs the visual condition is translated into the CLAP latent\nspace, we could interpolate audio embeddings by either vi-\nsual or textual guidance. For simplicity, we perform linear\ninterpolation between two embeddings. As shown in Fig. 7,\nthe interpolation can happen from a frog sound to a sound\nindicated by an image of a man playing flute, or to a target\nspecified by a description. It is noticeable that vision, text,\nand audio are semantically gathered to the same space with-\nout actual training with three modalities. We hear a relatively\nsmooth transition during the interpolation, which indicates\nauditorily that the V2A-Mapper does learn the translation\nfrom CLIP space to CLAP space. Examples are provided in\n‚ÄúLatent Space Interpolation‚Äù section of our demo website.\nLimitation and Future Work\nWhile our approach has achieved considerable success, it\nis important to acknowledge several limitations. First, the\nsystem can not achieve finer control. The generated sound\nexhibits semantic relevance in a general sense, but it lacks\ncontrollability over specific details. Second, the system fails\nwhen the visual cues involve unclear subjects (e.g., multiple\nobjects, blurry/damaged images). Third, the system does not\nexplicitly handle the temporal alignment between audio and\nvisual signals. All of these could be interesting future di-\nrections in this area. Enforcing text into the condition could\nbe a starting point for explicit controllability by consider-\ning both visual and textual features. To incorporate the lan-\nguage information of high-level semantic meaning into the\nsystem, recent multimodal foundation models such as Meta-\nTransformer (Zhang et al. 2023) and V ATLM (Zhu et al.\n2023a) could be taken into consideration. They learn the rep-\nresentation across vision, language, and audio which could\nshape a common space for different modalities.\nConclusion\nIn this paper, we explore the feasibility and efficiency of\nadapting foundation models (FMs) in the challenging open-\ndomain vision-to-audio generation task. We propose a sim-\nple yet effective mapper mechanism (V2A-Mapper) to con-\nnect the representative visual FM CLIP and the generative\nauditory FM AudioLDM. Learning to translate visual fea-\ntures from CLIP space to the auditory CLAP space, the\nV2A-Mapper successfully passes visual information to its\nauditory counterpart from which the AudioLDM can synthe-\nsize high-fidelity and visually-aligned sound. Our method is\nrelatively lightweight to train because it only requires op-\ntimization of the V2A-Mapper. Despite this simplicity, it\nachieves superior performance compared to current state-of-\nthe-art approaches with far more complex training regimes\nas demonstrated by both subjective and objective evaluation.\nEthical Statement\nOur method aims to leverage foundation models for effi-\ncient vision-to-audio generation. It can be used to enhance\nthe immersion of human experience, such as video editing\nand foley design. Nevertheless, the application of this tech-\nnology poses a risk if being maliciously misused on social\nplatforms, potentially resulting in negative outcomes for so-\nciety. Although significant strides have been made in audio\ndeepfake detection research to mitigate such concerns (Yam-\nagishi et al. 2021), the availability of ample datasets remains\npivotal for improving detection accuracy. In light of this, we\nare committed to presenting our synthesized audio samples,\nintending to contribute to the advancement and fine-tuning\nof existing detection algorithms.\nAcknowledgments\nWe extend our sincere appreciation to Xiaoyu Liu, Hannes\nMuesch, Adam Mater, Chunghsin Yeh, Michael Eckert, and\nLie Lu for their valuable comments, corrections and inspira-\ntion. In addition, we thank Xiaoyu for providing the code of\nCLIPSonic-IQ and we are grateful to Hannes and Adam for\ndiscussing, designing, and analyzing the subjective testings.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15499\nReferences\nAgresti, A. 1992. Analysis of ordinal paired comparison\ndata. Journal of the Royal Statistical Society: Series C (Ap-\nplied Statistics), 41(2): 287‚Äì297.\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\nson, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\net al. 2022. Flamingo: a visual language model for few-shot\nlearning. In NeurIPS, volume 35, 23716‚Äì23736.\nAo, J.; Wang, R.; Zhou, L.; Wang, C.; Ren, S.; Wu, Y .;\nLiu, S.; Ko, T.; Li, Q.; Zhang, Y .; et al. 2022. SpeechT5:\nUnified-modal encoder-decoder pre-training for spoken lan-\nguage processing. In ACL (Long Papers), 5723‚Äì5738.\nAwais, M.; Naseer, M.; Khan, S.; Anwer, R. M.; Cholakkal,\nH.; Shah, M.; Yang, M.-H.; and Khan, F. S. 2023. Foun-\ndational models defining a new era in vision: A survey and\noutlook. arXiv preprint arXiv:2307.13721.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258.\nCao, Y .; Li, S.; Liu, Y .; Yan, Z.; Dai, Y .; Yu, P. S.; and Sun,\nL. 2023. A comprehensive survey of AI-generated content\n(AIGC): A history of generative AI from GAN to ChatGPT.\narXiv preprint arXiv:2303.04226.\nChen, H.; Xie, W.; Vedaldi, A.; and Zisserman, A. 2020a.\nVGGSound: A large-scale audio-visual dataset. In ICASSP,\n721‚Äì725. IEEE.\nChen, K.; Zhang, C.; Fang, C.; Wang, Z.; Bui, T.; and Neva-\ntia, R. 2018. Visually indicated sound generation by percep-\ntually optimized classification. In ECCV Workshops.\nChen, L.; Srivastava, S.; Duan, Z.; and Xu, C. 2017. Deep\ncross-modal audio-visual generation. In Thematic Work-\nshops of ACM Multimedia, 349‚Äì357.\nChen, P.; Zhang, Y .; Tan, M.; Xiao, H.; Huang, D.; and Gan,\nC. 2020b. Generating visually aligned sound from videos.\nIEEE Transactions on Image Processing, 29: 8292‚Äì8302.\nDi, S.; Jiang, Z.; Liu, S.; Wang, Z.; Zhu, L.; He, Z.; Liu,\nH.; and Yan, S. 2021. Video background music generation\nwith controllable music transformer. In ACM Multimedia,\n2037‚Äì2045.\nDong, H.-W.; Liu, X.; Pons, J.; Bhattacharya, G.; Pascual,\nS.; Serr `a, J.; Berg-Kirkpatrick, T.; and McAuley, J. 2023.\nCLIPSonic: Text-to-audio synthesis with unlabeled videos\nand pretrained language-vision models. arXiv preprint\narXiv:2306.09635.\nDu, Y .; Liu, Z.; Li, J.; and Zhao, W. X. 2022. A sur-\nvey of vision-language pre-trained models. arXiv preprint\narXiv:2202.10936.\nGhosal, D.; Majumder, N.; Mehrish, A.; and Poria, S. 2023.\nText-to-Audio Generation using Instruction Guided Latent\nDiffusion Model. In ACM Multimedia, 3590‚Äì3598.\nGhose, S.; and Prevost, J. J. 2020. AutoFoley: Artificial syn-\nthesis of synchronized sound tracks for silent videos with\ndeep learning. IEEE Transactions on Multimedia, 23: 1895‚Äì\n1907.\nHao, W.; Zhang, Z.; and Guan, H. 2018. CMCGAN: A uni-\nform framework for cross-modal visual-audio mutual gener-\nation. In AAAI, volume 32.\nHershey, S.; Chaudhuri, S.; Ellis, D. P.; Gemmeke, J. F.;\nJansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous,\nR. A.; Seybold, B.; et al. 2017. CNN architectures for large-\nscale audio classification. In ICASSP, 131‚Äì135. IEEE.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. NeurIPS, 33: 6840‚Äì6851.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nDe Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and\nGelly, S. 2019. Parameter-efficient transfer learning for\nNLP. In ICML, 2790‚Äì2799. PMLR.\nHu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, S.; Wang,\nL.; Chen, W.; et al. 2021. LoRA: Low-rank adaptation of\nlarge language models. In ICLR.\nHuang, R.; Huang, J.; Yang, D.; Ren, Y .; Liu, L.; Li, M.;\nYe, Z.; Liu, J.; Yin, X.; and Zhao, Z. 2023. Make-An-\nAudio: Text-to-audio generation with prompt-enhanced dif-\nfusion models. In ICML.\nIashin, V .; and Rahtu, E. 2021. Taming visually guided\nsound generation. In BMVC.\nInternational Telecommunication Union. 1996. ITU-T Rec-\nommendation P.800: Methods for Subjective Determination\nof Transmission Quality.\nKilgour, K.; Zuluaga, M.; Roblek, D.; and Sharifi, M. 2019.\nFr¬¥echet audio distance: A metric for evaluating music en-\nhancement algorithms. Interspeech, 2350‚Äì2354.\nKong, Q.; Cao, Y .; Iqbal, T.; Wang, Y .; Wang, W.;\nand Plumbley, M. D. 2020. PANNs: Large-scale pre-\ntrained audio neural networks for audio pattern recognition.\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 28: 2880‚Äì2894.\nKreuk, F.; Synnaeve, G.; Polyak, A.; Singer, U.; D ¬¥efossez,\nA.; Copet, J.; Parikh, D.; Taigman, Y .; and Adi, Y . 2023.\nAudioGen: Textually guided audio generation. In ICLR.\nLe Scao, T.; and Rush, A. M. 2021. How many data points\nis a prompt worth? In ACL, 2627‚Äì2636.\nLee, S.-g.; Ping, W.; Ginsburg, B.; Catanzaro, B.; and Yoon,\nS. 2022. BigVGAN: A universal neural vocoder with large-\nscale training. arXiv preprint arXiv:2206.04658.\nLi, B.; Hwang, D.; Huo, Z.; Bai, J.; Prakash, G.; Sainath,\nT. N.; Sim, K. C.; Zhang, Y .; Han, W.; Strohman, T.; et al.\n2023. Efficient domain adaptation for speech foundation\nmodels. In ICASSP, 1‚Äì5. IEEE.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BlIP: Boot-\nstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In ICML, 12888‚Äì\n12900. PMLR.\nLiang, V . W.; Zhang, Y .; Kwon, Y .; Yeung, S.; and Zou,\nJ. Y . 2022. Mind the gap: Understanding the modality\ngap in multi-modal contrastive representation learning. In\nNeurIPS, volume 35, 17612‚Äì17625.\nLiu, H.; Chen, Z.; Yuan, Y .; Mei, X.; Liu, X.; Mandic, D.;\nWang, W.; and Plumbley, M. D. 2023a. AudioLDM: Text-\nto-audio generation with latent diffusion models. In ICML,\n21450‚Äì21474. PMLR.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15500\nLiu, H.; Tian, Q.; Yuan, Y .; Liu, X.; Mei, X.; Kong, Q.;\nWang, Y .; Wang, W.; Wang, Y .; and Plumbley, M. D. 2023b.\nAudioLDM 2: Learning holistic audio generation with self-\nsupervised pretraining. arXiv preprint arXiv:2308.05734.\nLu, K.; Grover, A.; Abbeel, P.; and Mordatch, I. 2021. Pre-\ntrained transformers as universal computation engines. In\nAAAI, 7628‚Äì7636.\nLuo, S.; Yan, C.; Hu, C.; and Zhao, H. 2023. Diff-Foley:\nSynchronized video-to-audio synthesis with latent diffusion\nmodels. arXiv preprint arXiv:2306.17203.\nMcCloskey, M.; and Cohen, N. J. 1989. Catastrophic inter-\nference in connectionist networks: The sequential learning\nproblem. The Psychology of Learning and Motivation, 24:\n109‚Äì165.\nMokady, R.; Hertz, A.; and Bermano, A. H. 2021. Clip-\nCap: Clip prefix for image captioning. arXiv preprint\narXiv:2111.09734.\nNichol, A. Q.; and Dhariwal, P. 2021. Improved denois-\ning diffusion probabilistic models. In ICML, 8162‚Äì8171.\nPMLR.\nOwens, A.; and Efros, A. A. 2018. Audio-visual scene anal-\nysis with self-supervised multisensory features. In ECCV,\n631‚Äì648.\nOwens, A.; Isola, P.; McDermott, J.; Torralba, A.; Adelson,\nE. H.; and Freeman, W. T. 2016. Visually indicated sounds.\nIn CVPR, 2405‚Äì2413.\nPaa√ü, G.; and Giesselbach, S. 2023. Foundation models for\nnatural language processing: Pre-trained language models\nintegrating media. Springer Nature.\nPrajwal, K.; Mukhopadhyay, R.; Namboodiri, V . P.; and\nJawahar, C. 2020. Learning individual speaking styles for\naccurate lip to speech synthesis. In CVPR, 13796‚Äì13805.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. In ICML, 8748‚Äì8763. PMLR.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\n2022. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125.\nSainburg, T.; McInnes, L.; and Gentner, T. Q. 2021. Para-\nmetric UMAP embeddings for representation and semisu-\npervised learning. Neural Computation, 33(11): 2881‚Äì2907.\nSheffer, R.; and Adi, Y . 2023. I hear your true colors: Image\nguided audio generation. In ICASSP, 1‚Äì5. IEEE.\nSong, Y .; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Er-\nmon, S.; and Poole, B. 2020. Score-based generative mod-\neling through stochastic differential equations. In ICLR.\nVan Den Oord, A.; Vinyals, O.; et al. 2017. Neural discrete\nrepresentation learning. In NeurIPS, volume 30.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, volume 30.\nWei, Y .; Hu, D.; Tian, Y .; and Li, X. 2022. Learning in\naudio-visual context: A review, analysis, and new perspec-\ntive. arXiv preprint arXiv:2208.09579.\nWu, H.-H.; Seetharaman, P.; Kumar, K.; and Bello, J. P.\n2022. Wav2CLIP: Learning robust audio representations\nfrom CLIP. In ICASSP, 4563‚Äì4567. IEEE.\nWu, Y .; Chen, K.; Zhang, T.; Hui, Y .; Berg-Kirkpatrick, T.;\nand Dubnov, S. 2023. Large-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-\ncaption augmentation. In ICASSP, 1‚Äì5. IEEE.\nYamagishi, J.; Wang, X.; Todisco, M.; Sahidullah, M.;\nPatino, J.; Nautsch, A.; Liu, X.; Lee, K. A.; Kinnunen, T.;\nEvans, N.; and Delgado, H. 2021. ASVspoof 2021: acceler-\nating progress in spoofed and deepfake speech detection. In\nProc. ASVspoof Challenge Workshop, 47‚Äì54.\nYang, D.; Yu, J.; Wang, H.; Wang, W.; Weng, C.; Zou, Y .;\nand Yu, D. 2023. DiffSound: Discrete diffusion model for\ntext-to-sound generation. IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing.\nYang, Z.; Gan, Z.; Wang, J.; Hu, X.; Lu, Y .; Liu, Z.; and\nWang, L. 2022. An empirical study of GPT-3 for few-shot\nknowledge-based VQA. In AAAI, 3081‚Äì3089.\nYin, S.; Fu, C.; Zhao, S.; Li, K.; Sun, X.; Xu, T.; and Chen,\nE. 2023. A survey on multimodal large language models.\narXiv preprint arXiv:2306.13549.\nYuan, Y .; Liu, H.; Liang, J.; Liu, X.; Plumbley, M. D.; and\nWang, W. 2023. Leveraging pre-trained AudioLDM for\nsound generation: A benchmark study. In EUSIPCO.\nZaken, E. B.; Goldberg, Y .; and Ravfogel, S. 2022. Bit-\nFit: Simple parameter-efficient fine-tuning for transformer-\nbased masked language-models. InACL (Short Papers), vol-\nume 2, 1‚Äì9.\nZhang, Y .; Gong, K.; Zhang, K.; Li, H.; Qiao, Y .; Ouyang,\nW.; and Yue, X. 2023. Meta-Transformer: A Unified\nFramework for Multimodal Learning. arXiv preprint\narXiv:2307.10802.\nZhou, C.; Li, Q.; Li, C.; Yu, J.; Liu, Y .; Wang, G.; Zhang, K.;\nJi, C.; Yan, Q.; He, L.; et al. 2023. A comprehensive survey\non pretrained foundation models: A history from BERT to\nChatGPT. arXiv preprint arXiv:2302.09419.\nZhou, Y .; Wang, Z.; Fang, C.; Bui, T.; and Berg, T. L. 2018.\nVisual to sound: Generating natural sound for videos in the\nwild. In CVPR, 3550‚Äì3558.\nZhu, Q.; Zhou, L.; Zhang, Z.; Liu, S.; Jiao, B.; Zhang, J.;\nDai, L.; Jiang, D.; Li, J.; and Wei, F. 2023a. V ATLM: Visual-\naudio-text pre-training with unified masked prediction for\nspeech representation learning. IEEE Transactions on Mul-\ntimedia.\nZhu, Y .; Wu, Y .; Olszewski, K.; Ren, J.; Tulyakov, S.; and\nYan, Y . 2023b. Discrete contrastive diffusion for cross-\nmodal and conditional generation. In ICLR.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n15501"
}