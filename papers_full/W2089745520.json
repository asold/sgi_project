{
    "title": "OxLM: A Neural Language Modelling Framework for Machine Translation",
    "url": "https://openalex.org/W2089745520",
    "year": 2014,
    "authors": [
        {
            "id": "https://openalex.org/A2900068394",
            "name": "Baltescu, Paul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2900299007",
            "name": "Blunsom, Phil",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2900382797",
            "name": "Hoang, Hieu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2158195707",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6681435938",
        "https://openalex.org/W6632248436",
        "https://openalex.org/W1970689298",
        "https://openalex.org/W6674650171",
        "https://openalex.org/W2100714283",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2083545877",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2251071050",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W932413789",
        "https://openalex.org/W1965154800",
        "https://openalex.org/W1505680913",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2120861206",
        "https://openalex.org/W2096175520",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W2950075229",
        "https://openalex.org/W2155607551",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2158049734",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2148708890",
        "https://openalex.org/W36903255",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2474824677"
    ],
    "abstract": "Abstract This paper presents an open source implementation 1 of a neural language model for machine translation. Neural language models deal with the problem of data sparsity by learning distributed representations for words in a continuous vector space. The language modelling probabilities are estimated by projecting a word's context in the same space as the word representations and by assigning probabilities proportional to the distance between the words and the context's projection. Neural language models are notoriously slow to train and test. Our framework is designed with scalability in mind and provides two optional techniques for reducing the computational cost: the so-called class decomposition trick and a training algorithm based on noise contrastive estimation. Our models may be extended to incorporate direct n-gram features to learn weights for every n-gram in the training data. Our framework comes with wrappers for the cdec and Moses translation toolkits, allowing our language models to be incorporated as normalized features in their decoders (inside the beam search).",
    "full_text": null
}