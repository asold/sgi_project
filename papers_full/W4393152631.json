{
  "title": "Quantifying and Analyzing Entity-Level Memorization in Large Language Models",
  "url": "https://openalex.org/W4393152631",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2151467377",
      "name": "Zhenhong Zhou",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A4201666550",
      "name": "Jiuyang Xiang",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A3092189341",
      "name": "Chaomeng Chen",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2116711474",
      "name": "Sen Su",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3027379683",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W6770831326",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W1603920809",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W6793601707",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4319050249",
    "https://openalex.org/W4385573004",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3096331697",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4226137521",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W4385571225",
    "https://openalex.org/W4377864714",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4389518968",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4385679821",
    "https://openalex.org/W1490960179",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4385573569",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3166699508",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3118781290"
  ],
  "abstract": "Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. The results demonstrate that LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.",
  "full_text": "Quantifying and Analyzing\nEntity-Level Memorization in Large Language Models\nZhenhong Zhou1, Jiuyang Xiang2, Chaomeng Chen1, Sen Su1*\n1Beijing University of Posts and Telecommunications\n2University of Michigan\n{zhouzhenhong, gianluigi-chen, susen}@bupt.edu.cn, jiuyangx@umich.edu\nAbstract\nLarge language models (LLMs) have been proven capa-\nble of memorizing their training data, which can be ex-\ntracted through specifically designed prompts. As the scale of\ndatasets continues to grow, privacy risks arising from memo-\nrization have attracted increasing attention. Quantifying lan-\nguage model memorization helps evaluate potential privacy\nrisks. However, prior works on quantifying memorization re-\nquire access to the precise original data or incur substantial\ncomputational overhead, making it difficult for applications\nin real-world language models. To this end, we propose a fine-\ngrained, entity-level definition to quantify memorization with\nconditions and metrics closer to real-world scenarios. In ad-\ndition, we also present an approach for efficiently extracting\nsensitive entities from autoregressive language models. We\nconduct extensive experiments based on the proposed, prob-\ning language models’ ability to reconstruct sensitive entities\nunder different settings. We find that language models have\nstrong memorization at the entity level and are able to repro-\nduce the training data even with partial leakages. The results\ndemonstrate that LLMs not only memorize their training data\nbut also understand associations between entities. These find-\nings necessitate that trainers of LLMs exercise greater pru-\ndence regarding model memorization, adopting memoriza-\ntion mitigation techniques to preclude privacy violations.\nIntroduction\nPretrained large language models (LLMs) have made sig-\nnificant breakthroughs in various downstream tasks (Devlin\net al. 2019; Raffel et al. 2020; Ouyang et al. 2022; OpenAI\n2023). However, researches show that LLMs memorize the\ntraining data (Carlini et al. 2019, 2021), typically sourced\nfrom crowd-sourced corpora or the Internet. Through crafted\nprompts, language models can reproduce the training data\n(Huang, Shao, and Chang 2022; Lukas et al. 2023), lead-\ning to serious concerns regarding data privacy. As shown in\nFigure 1, attackers have bypassed the security constraints of\nlanguage models via the “Grandma Exploit” and success-\nfully extracted sensitive information such as Windows Keys\nor Apple IDs from ChatGPT. This suggests that privacy leak-\nages in LLMs may be incurred in real-world settings due\nto emitting memorized sensitive entities. Quantifying the\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Privacy leakage of LLM in real-world scenarios.\nAttackers can bypass safety measures and collect sensitive\ninformation from LLMs.\nmemorization in language models and analyzing their ca-\npability of emitting memorized sensitive entities helps re-\nsearchers understand the potential privacy risks with LLMs.\nRecently, several studies have been conducted to quantify\nmemorization in language models. Prior attempts compared\nmodels trained under different conditions to evaluate lan-\nguage model memorization (Zhang et al. 2021; Mireshghal-\nlah et al. 2022), but these approaches incur high computa-\ntional costs. Carlini et al. (Carlini et al. 2023) used prefix\nprompts to prompt models to complete training set suffixes\nverbatim and quantified memorization based on completion\naccuracy. However, in real-world scenarios, malicious or cu-\nrious users are unlikely to have direct access to the train-\ning set for obtaining the original prefixes. Additionally, the\ninformation leakage from language models is usually fine-\ngrained. For instance, in the “Grandma Exploit” of Chat-\nGPT, the language model has generated content that poses\nprivacy risks. Among these texts, certain key entities (for\ninstance, the keys shown in the figure) constitute sensitive\ninformation rather than all contents or verbatim suffixes.\nTherefore, previous research may not adequately quantify\nreal-world memorization in LLMs.\nApart from the limitations of quantification methodolo-\ngies, challenges also arise in approaches for extracting mem-\norized training data from language models. Researchers typ-\nically employ original prefixes (Carlini et al. 2023) or care-\nfully crafted prompts (Shao et al. 2023; Li et al. 2023) as\ninput. However, obtaining original prefixes is difficult in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19741\nFigure 2: Comparison of the extraction processes of verbatim memorization and entity memorization. Verbatim memorization\nemphasizes the generation of verbatim matched suffixes of training data, effectively serving as perfect continuations of mali-\ncious queries. In contrast, the entity memorization extraction process initially extracts entities carrying critical information from\ntraining data and chooses a uniquely identifiable entity set. Then coupled with a soft prompt embedding, to form a malicious\nquery, expecting the model’s response to contain the required specific entity. This entity-level memorization pattern is more\ncommon in real-world scenarios.\npractical applications of language models. If we forgo the\nuse of prefixes in favor of handcrafted prompts, one must\nconsider that the structure and order (Shin et al. 2020; Lu\net al. 2022; Gao, Fisch, and Chen 2021; Jiang et al. 2020) of\ndesigned textual prompts can significantly influence the re-\nsults. In summary, these challenges make it tricky to probe\nmemorization across language model families using textual\nprompts.\nIn this paper, we conduct comprehensive testing and eval-\nuation of real-world language models to quantify their mem-\norization. To quantify fine-grained model memorization in a\nmanner that more closely resembles real-world privacy leak-\nages, we propose a definition for memorization extraction at\nthe entity level. We also introduce an approach for learn-\ning prompts adaptively, which utilizes entity attribute infor-\nmation and soft prompts (Li and Liang 2021; Lester, Al-\nRfou, and Constant 2021). This approach enables efficient\nlanguage model memorization emitting. Through these, re-\nsearchers can quantify and analyze LLMs’ memorization\nunder conditions closer to real-world applications. Besides,\nactivating more memorization allows researchers to ascer-\ntain potential hazards arising from language models’ mem-\norization. For entity-level specific memorization extraction,\nour method can achieve the highest accuracy of 61.2%. In\nterms of average accuracy, our method has a three-fold in-\ncrease in entity extraction rate in a 6B language model com-\npared to textual prompts when the data is unique to the\ndataset.\nTo summarize, our contributions are as follows:\n1. We propose a quantifiable, fine-grained definition for\nevaluating language model memorization without re-\nquirements for updating original model parameters or\nprecise data from the training set.\n2. We present a method to test language model memoriza-\ntion under conditions that closely resemble real-world\nscenarios. More memorized information can be extracted\nby leveraging entity attributes and soft prompts to help\nunderstand potential privacy risks.\n3. We conduct comprehensive experiments on language\nmodel entity-level memorization. Our analysis delves\ninto the processes involved in activating entity memo-\nrization in language models and discusses factors that\nmay influence entity memorization.\nRelated Work\nPrompt for LLMs\nPre-trained language models store a vast amount of lan-\nguage knowledge. As models continue to scale up in pa-\nrameters, methods based on fine-tuning incur high computa-\ntional overhead. In contrast, prompt-based methods run di-\nrectly on large language models, achieving results compa-\nrable to fine-tuning. Brown et al. (Brown et al. 2020) ini-\ntially attempted to leverage the language knowledge of the\npre-trained language model through the prompts. Shin et\nal. (Shin et al. 2020) introduced the approach of searching\nfor the optimal prompt word. The prompt generated through\nthis method enhances the utility. Recognizing that prompts’\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19742\nprimary role is to improve models’ performance on spe-\ncific tasks, researchers have begun exploring approaches that\nare not constrained by natural language and use continu-\nous vectors directly in the embedding space of the model to\nprompt language models. This prompt-based approach has\nyielded many influential extensions, such as p-tuning (Liu\net al. 2021b), prompt-tuning (Lester, Al-Rfou, and Constant\n2021), prefix-tuning (Li and Liang 2021), etc. These meth-\nods have proposed effective solutions for improving model\nperformance.\nPrivacy Leakage in LLMs\nLanguage models rapidly develop and demonstrate power-\nful capabilities across various fields (OpenAI 2023). How-\never, language models have some privacy risks associated\nwith data leakage (B´eguelin et al. 2020; Mireshghallah et al.\n2022). It has recently been confirmed that language mod-\nels tend to memorize data from their training sets, and prior\nstudies (Huang, Shao, and Chang 2022; Lukas et al. 2023)\nsuccessfully extracted Personally Identifiable Information\n(PII) of specific individuals from LLMs.\nPrevious research (Li et al. 2023; Shao et al. 2023) has\nthoroughly explored how well-designed prompts may cause\nLLMs to output data from their training set or related infor-\nmation attributed to their memorization, potentially leading\nto the leakage of sensitive information. Different language\nprompts can significantly impact the ultimate predictions\n(Gao, Fisch, and Chen 2021; Lu et al. 2022). Besides, due\nto the poor interpretability of LLMs, even experienced re-\nsearchers struggle to determine how to write better prompts\nto extract specific PII or entities. Ozdayi et al. (Ozdayi et al.\n2023) discussed controllable generation via soft prompts,\nyet precise prefixes remain necessary. While these methods\ndemonstrate commendable quantification capabilities, they\nhave a challenge in adapting to real-world scenarios, which\nto some extent, hampers the research into memorization.\nEntity Memorization Can Be Extracted\nEfficiently\nEntity Memorization\nIn fact, for text data, overstrict definitions may overlook\nsome memorized content when key and sensitive informa-\ntion resides only in parts of the data, even though such con-\ntent is more likely to leak private details. Thus, if model out-\nputs contain partial information from the training data, this\ncan be considered as an emission of memorization. To fur-\nther elaborate, if the prompt is explicitly targeted, and the\nportion of the memorized content emitted is sensitive, this\nconstitutes a privacy leak in the language model. In practical\napplications, private information typically attaches to enti-\nties like PII. Since prefix prompts and masked training data\nrely heavily on original data and are challenging to acquire\nin practice, obtaining partial entities becomes more acces-\nsible than the former two methods. Therefore, we expand\nthe definition of language model memorization to the entity\nlevel. We propose a more flexible, quantifiable definition of\nmemorization. When the input prompt is constructed from\npartial entities in the training strings, and the model’s output\n125M 1.3B 2.7B 6B\nModel……Size\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Entity……Extraction……Rate\nEfficient…Memorization…Extraction\nHand-crafted…Prompt…Extraction\nFigure 3: The results of memorization extraction using tex-\ntual hand-crafted prompts versus Efficient Memorization\nExtraction, which combines attribute-annotated text with\nsoft prompts for memorization activation.\ncontains the expected entity information from the prompt,\nthis is considered as entity memorization.\nDefinition 1(Entity Memorization). A training string s\ncontains m entities M and can be uniquely identified by\nany n entities N of them. n ∈ (0, m]. A promptp contains\nthe n entities that could identify s, and p expects an entity\nE ∈ (M − N). Prompted the language model f with p. If\nthe expected entity E is a substring in the output f(p), the\nprocess is referred to as Entity Memorization.\nAccording to Definition 1, language models generate the\nexpected entity based on a prompt constructed using the\nunique identifying entity set N from a training string. This\nquantifies the model’s memorization of training data and\nits capability for directed reconstruction from memoriza-\ntion. In scenarios of partial training data leakage or mali-\ncious querying with extensive guessing, constructing prompt\nqueries from partial entities for the language model to re-\ncall unknown entities is more practical. Entity memorization\nmore accurately mirrors real-world memorization leakages\nby language models than verbatim memorization does. Fur-\nthermore, compared to the original prefix or masked training\ndata that strictly emphasize sequential accuracy, entities of-\nten have looser relationships in training data. Consequently,\nEntity Memorization can further reflect models’ understand-\ning and association abilities regarding memorized content.\nEfficient Memorization Extraction\nCurrently, researchers typically measure language model\nmemorization design textual prompts as inputs. These\nprompts test model memorization in an understandable\nmanner for humans. While textual prompts have good ex-\nplainability, hand-crafted prompts are highly experience-\ndependent (Liu et al. 2021a). We find that the performance of\ntextual prompts in extracting fine-grained memorization on\nsubtly related entity levels heavily depends on the scale of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19743\n1 3 5 10 20 30 50 75 100 500\nPrefix……Length\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n(a)…GPT -Neo-125M\n1 3 5 10 20 30 50 75 100 500\nPrefix……Length\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n(b)…GPT -Neo-1.3B\n1 3 5 10 20 30 50 75 100 500\nPrefix……Length\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n(c)…GPT -Neo-2.7B\n1 3 5 10 20 30 50 75 100 500\nPrefix……Length\n0.04\n0.06\n0.08\n0.10\n0.12\n(d)…GPT -J-6B\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEntity……Extraction……Rate\nEfficient…Memorization…Extraction Hand-crafted…Prompt…Extraction\nFigure 4: The trend of entity extraction rate for prefix lengths across different model sizes. When the model’s parameter size\nexpands to a certain scale, EME demonstrates substantial advantages in entity extraction rates compared to baseline methods.\nthe language model, leaving significant room for improve-\nment. Additionally, designing prompts individually for mul-\ntiple models is laborious and cannot guarantee optimality.\nThese challenges obstruct efficient evaluation of universal\nlanguage model memorization. Even across models within\nthe same family, crafting universal textual prompts for mem-\norization activation is challenging.\nSoft prompt extends prompts into continuous space, pro-\nviding continuous prompt vectors for the input to the lan-\nguage model. It demonstrates similar workflows across di-\nverse models and excellent performance in downstream\ntasks. Inspired by soft prompt, we explore applying soft\nprompts to activate memorization in models. These contin-\nuous prompts are typically learned through approaches like\nprompt tuning. Since the optimal memorization extraction\nprompt is determined by itself, this approach enables effi-\ncient memorization activation while maintaining strong gen-\neralization across diverse models.\nWe introduce Efficient Memorization Extraction (EME)\ncombines effective memorization content filtering and mem-\norization activation. Specifically, EME compresses textual\ncontent into entity attribute annotations and combines it with\nsoft prompts as inputs. Through this workflow, testing mem-\norization across different language models follows an iden-\ntical process, enabling the examination of diverse models’\nmemorization capacities. We explore applying soft prompts\nfor entity-level memorization activation, with experiments\nvalidating fine-grained memorized information extraction.\nSoft prompt encompasses a wide variety of techniques.\nWhen without a specific explanation, the default method\nused in this work is prefix tuning (Li and Liang 2021).\nThe lower section of Figure 2 illustrates our approach to\nimplementing a universal entity-level evaluation of language\nmodel memorization. For documents that experience partial\nleakage from the language model’s training set, we extract\nkey entities that uniquely identify this document. Following\nthis, we construct preliminary prompts based on the entity\nattribute tagging of these entities. Finally, soft prompts are\nincorporated into the textual prompt vector embeddings to\nobtain complete prompts as inputs for the model.\nWe construct textual prompts with entities from training\ndata to prompt the model to generate expected entities and\nuse the method of handcrafted textual prompts as a base-\nline. The results are shown in Figure 3. As models scale up,\ntheir ability to associate entities and emit memorization im-\nprove, revealing a more pronounced tendency to emit memo-\nrization. Nevertheless, By utilizing entity attributes and soft\nprompts, the language model’s memorization can be more\neffectively activated at the entity level. Our approach gener-\nates markedly superior performance over textual prompts on\nlarger models. For instance, on GPT-Neo 6B, our approach\ndemonstrates a three-fold increase in memorization extrac-\ntion efficiency compared to textual prompts.\nExperiments and Results\nExperimental Setup\nThe GPT-Neo model family (Black et al. 2021) includes a set\nof causal language models (CLMs) that are trained on “The\nPile” datasets (Gao et al. 2020) and available in four sizes,\nwith 125 million, 1.3 billion, 2.7 billion, and 6 billion pa-\nrameters respectively. In our experimental setup, we use the\ngreedy decoding strategy by default to generate the output\nwith the minimum perplexity (PPL), which is then utilized\nfor evaluating the model’s entity memorization capabilities.\nThe Enron email dataset (Klimt and Yang 2004), a sub-\nset of “The Pile” dataset, encompasses over 500,000 emails\nfrom approximately 150 users of the Enron Corporation.\nOwing to this dataset’s inherent structured data character-\nistics, it is conducive to extracting entities from the data,\nfacilitating entity-level memorization extraction experimen-\ntation.\nFor data preprocessing, we extract entities from the En-\nron dataset and tag their attributes. “Date” and “Content”\nare retained, while “X-From” and “X-To” are relabeled as\n“Sender” and “Recipient” attributes for that data instance.\nThe attribute labels “X-From” and “X-To” were modified\nto be more interpretable for models. These four columns\nuniquely identify each data instance in the Enron dataset.\nWe select the target entity column as “excepted entity” to\nrecover and then construct prompts from other column en-\ntities. To ensure the model searches its memorization rather\nthan deducing answers directly from prompts, we exclude\ndata where the constructed prompt contains the target entity.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19744\n1 3 5 10 20 30 50 75 100 500\nPrefix……Length\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12Entity……Extraction……Rate\np…tuning\nprompt…tuning\nprefix…tuning\nactivate…ensemble\n(a)\n                                             0.040\n0.060\n0.080\n0.100Entity……Extraction……Rate……………………………………………………………\n1 3 5 10 20 30 50 75 100 500\nPrefix…Length\n0.000\n0.003\n0.005\nopt-6.7b\nllama-7b\ngptj-6b\n(b)\nFigure 5: In (a), we explore several soft prompt methods\nto activate the model’s memorization and attempt to inte-\ngrate these generated results. In (b), we apply the same soft\nprompt memorization activation to models not trained using\nthe “The Pile” dataset.\nPrefix Length Impact on Entity Memorization\nThe prefix length is a crucial parameter of soft prompts. Typ-\nically, a longer prefix length boasts a more robust represen-\ntion and performs better in downstream tasks.\nHowever, in tests of the model’s memorization capabili-\nties, we find that an optimal extraction rate for memoriza-\ntion can be achieved when the prefix length is short. As the\nprefix length increases, the entity reconstruction rate of the\nmodel declines, then rebounds when the prefix length is ex-\ntremely long. As depicted in Figure 4, all sizes of GPT-Neo\nmodels exhibit a trend of declining memorization extraction\nrate after reaching the optimal prefix length.\nWe inspect the text generated by the model and find that\nwhen the prefix length is short, the diversity of the gener-\nated text is better. In contrast, the output is more conserva-\ntive when the prefix length is long. This may be attributed to\nthe prefix length ranging from 5 to 10, which tends to opti-\nmally activate the model’s memorization. While excessively\nlong prefixes, such as 100, compel the model to prioritize\nlearning the relationships between prompted entities rather\nthan activating memorization. Thus, we conjecture that this\nrelates to model memorization inherently residing in the pa-\nrameter space, rendering shorter soft prompts sufficient to\nstimulate innate model memorization.\nDifferent Soft Prompts Have Similar Activation\nCapabilities\nCommon soft prompt methodologies, such as prompt-tuning\n(Lester, Al-Rfou, and Constant 2021) and p-tuning (Liu\net al. 2021b), are among those employed in the field. This\nstudy also tested the memorization activation capabilities of\nprompt-tuning and p-tuning.\nThe results in Figure 5(a) indicate that soft prompt meth-\nods can somewhat activate the model’s memorization at the\nentity level. However, these methods differ in terms of the\noptimal prefix length setting, suggesting that no universal\nsetting can maximize the memorization of the same model.\nFurthermore, these methods have different effectiveness in\nactivating the model’s memorization, in our experiments,\nthe prefix tuning approach achieve an entity extraction rate\nof 9.04%, surpassing the other two methods by a margin\nof 2%. We speculate that this discrepancy is primarily due\nto the fact that the prefix-tuning method concatenates the\nsoft prompt at the beginning of the input, which aligns well\nwith the generation characteristics of autoregressive lan-\nguage models.\nIn addition, we attempt to integrate the results of these\nmethods. In short, we first extract entities from the gener-\nated text of different methods. If the same output entity is\npresent, we select that entity as the result. Otherwise, we\nembed the expected entity from each method back into the\ninput tensor and let the model evaluate the perplexity of the\ninput. We choose the method with the lowest perplexity as\nthe final output result. The results show that this method can,\nto some extent, integrate the advantages of these methods\nunder different settings and achieve better robustness. How-\never, it does not show any significant advantage in accurcy\nof memorization reconstruction, with a prefix length of 5,\nthe activate ensemble only achieve a improvement of 0.4%.\nLLMs Struggle to Reconstruct Entities without\nMemorization\nIn an effort to specifically exclude the possibility of lan-\nguage models learning entity relationships through soft\nprompts, we conduct an ablation study on LLM proposed\nby Meta. OPT (Zhang et al. 2022) utilizes a portion of “The\nPile” datasets but does not include the Enron dataset. Sim-\nilarly, Another large-scale model trained by Meta, Llama\n(Touvron et al. 2023), also does not explicitly mention the\nusage of the Enron dataset. We conduct tests with the same\nhyperparameter settings on the OPT-6.7B and LLAMA-7B\nmodels, similar in size to the GPT-J 6B model.\nThe experimental results shown in Figure 5(b) indicate\nthat OPT and Llama, lacking training data memorization,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19745\nModel Fabricated-50 Fabricated-100 Fabricated-200 Fabricated-500 Fabricated-1000\nGPT-Neo 125M 0.0 ± 0 0.0 ± 0 0.0 ± 0 0.0 ± 0 0.0 ± 0\nGPT-Neo 1.3B 0.0 ± 0.003 0.0 ± 0.002 0.0 ± 0.002 0.001 ± 0.002 0.001 ± 0.001\nGPT-Neo 2.7B 0.004 ± 0.008 0.006 ± 0.010 0.007 ± 0.006 0.007 ± 0.003 0.006 ± 0.003\nGPT-J 6B 0.019 ± 0.016 0.036 ± 0.019 0.057 ± 0.012 0.057 ± 0.009 0.052 ± 0.014\nOPT 6.7B 0.0 ± 0 0.0 ± 0 0.0 ± 0 0.0 ± 0.001 0.0 ± 0.001\nLLAMA 7B 0.0 ± 0 0.0 ± 0 0.0 ± 0.001 0.0 ± 0.002 0.001 ± 0.003\nReal-50 Real-100 Real-200 Real-500 Real-1000\nGPT-Neo 125M 0.003 ± 0.002 0.008 ± 0.003 0.009 ± 0.003 0.008 ± 0.004 0.007 ± 0.006\nGPT-Neo 1.3B 0.009 ± 0.002 0.015 ± 0.004 0.019 ± 0.003 0.016 ± 0.004 0.016 ± 0.009\nGPT-Neo 2.7B 0.014 ± 0.003 0.019 ± 0.005 0.020 ± 0.006 0.020 ± 0.009 0.019 ± 0.008\nGPT-J 6B 0.052 ± 0.009 0.072 ± 0.013 0.090 ± 0.016 0.090 ± 0.013 0.088 ± 0.021\nOPT 6.7B 0.0 ± 0.001 0.001 ± 0.002 0.002 ± 0.001 0.001 ± 0.002 0.002 ± 0.002\nLLAMA 7B 0.001 ± 0.001 0.001 ± 0.001 0.003 ± 0.002 0.004 ± 0.003 0.002 ± 0.003\nTable 1: The effect of the volume of fabricated and real data on entity extraction rate across different models. We constructed\nthe fabricated prompts by randomly shuffling the real entities. Even so, the fabricated prompts can still extract some expected\nreal information from the model. The volume of data in both fabricated and real datasets impacts the accuracy rate.\nachieve a maximum accuracy of 0.4% in accurately recon-\nstructing the expected entities when utilizing soft prompts.\nThe accuracy remains relatively low despite the models’\nfitting ability from the prompt improving with increasing\nprefix length. Their performances are even inferior to the\nsmaller GPT-Neo model, which possesses memorization.\nWithout memorization, LLMs struggle to learn the subtle re-\nlationships between entities based on soft prompts and can-\nnot reconstruct the expected entity. Reconstruction can only\nbe completed if LLMs have learned the dataset during pre-\ntraining.\nBased on the above results, we believe that soft prompts\nindeed activate the model’s memorization rather than en-\nabling the model to learn a lower-loss generation method.\nFabricated Data Also Activate Memorization\nIn order to further evaluate the extent to which the model\ncan generate mnemonic content under more lenient condi-\ntions, we attempt to construct soft prompts using fabricated\ndata. We randomly recombine the target entities with other\nremaining entities, ensuring new entity pairs are not in the\noriginal dataset’s documents.\nTable 1 depicts the accuracy of model reconstruction of\ncorrect entities using varying quantities of real and fabri-\ncated data. The results show that when training soft prompts\nwith fabricated data and providing prompts to the model,\ndespite a near 4% loss in reconstruction rate compared to\nreal data, the model can still generate the expected entity\nfrom memorization with a maximum accuracy of 5.7%.\nWith smaller-sized models, soft prompts obtained from fab-\nricated data are less effective, and the final results fluctuate\ngreatly, but larger-sized models with better robustness are\nmore adaptable. We compare how models without memo-\nrization respond to soft prompts constructed by fabricated\ndata, and the results showed that learning even the relation-\nships between entities becomes more challenging on fabri-\ncated data for these models.\nTherefore, we believe that the language models do not\nmerely reproduce entities superficially based on the data\nused during soft prompt training. Even if the data is fabri-\ncated and contains biases, the soft prompts obtained from\nthe fabricated data can still evoke the model’s memorization\nof the related dataset to a certain extent. Furthermore, this\nunderscores the potential privacy risks associated with the\nmodel’s memorization at the entity level. Attackers might\nbe able to access the model’s memorization under more le-\nnient conditions. Even if they do not have the real dataset,\nknowing what entities exist in a dataset can allow them to\nconstruct prompts to retrieve sensitive information.\nTraining Data Volume Influence Performance of\nSoft Prompts\nFor optimal soft prompt performance, the model must learn\nfrom training data. Consequently, the data choice will im-\npact the effectiveness of the soft prompts. We investigat how\nthe amount of data affects the performance of soft prompts\nin the extraction of entity memorization.\nWhen using real data, we find that soft prompts trained\nfrom less data often fail to activate the model’s memoriza-\ntion effectively. This outcome is little influenced by the ran-\ndomness in choosing training data. As the dataset size in-\ncreases, the effectiveness of the soft prompts improves and\neventually stabilizes. However, the effectiveness declines\nwith massive training datasets, and the results exhibit some\nfluctuations. Upon examining these training data divisions,\nwe argue that an abundance of training data might lead soft\nprompts to lose some of their memorization activation effec-\ntiveness. They instead turn to learning how to derive correct\nanswers directly from the training data.\nThe variance in the entity extraction rate is different when\nusing fabricated data than using real data. The results vary\nmore greatly when using a small amount of fabricated data,\nand we speculate that this variance is primarily related to\nthe randomness during data fabrication. On the other hand,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19746\n0-10 10-50 50-200 200-500 500-1000\nRepeat…… Times……in……Dataset\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Entity…Extraction…… Accuracy\nFigure 6: The effect of duplication times of entity pairs on\nentity extraction accuracy. The duplication of entity pairs\nmay occur within documents that are not remarkably sim-\nilar.\nwhen using more fabricated data, the variance initially di-\nminishes but later rebounds. We investigate the cause of this\nand contend that it may be related to the data fabrication\nmethod we chose. Because our fabricated data do not exist\nin the original data and entity pairs are not duplicated, this\ndiffers from real data and may eventually lead to the vari-\nances observed in the results.\nDuplicate Times Strengthen Entity Memorization\nConsidering previous research, a popular view proposed by\n(Kandpal, Wallace, and Raffel 2022; Lee et al. 2022) is that\nthe memorization of the language model is strongly corre-\nlated with the frequency of data duplication. Excessively du-\nplicated data is more likely to be extracted, yet previous re-\nsearch has not explored this issue on the entity level. There-\nfore, we statistically analyze the duplications of uniquely\nidentified entities and test the reconstruction rates of entities\nwith different numbers of repetitions.\nWe bin duplicated entity sequences with similar dupli-\ncated counts and use the same settings to obtain soft prompts\nfor input construction. And then we query the model multi-\nple times to gauge the effectiveness of the entity sequence\nin extracting the expected entity. As the results in Figure 6\nindicate, even though the entity sequence uniquely identi-\nfies a unique document and repeats less frequently in other\ncontent, the repetition count of some entity sequences still\naffects the degree to which the model memorizes them. Af-\nter the entity sequence is duplicated more than 50 times, the\nmean accuracy of query results exceeded 10%. Beyond 200\nduplications, the reconstruction accuracy of expected entity\nreached a maximum of 61.2%. In other words, when entity\nsequences are extensively duplicated, the model has stronger\nmemorization of these data, presenting a higher risk of pri-\nvacy leakage.\nSince the model’s entity-level memorization still corre-\nlates with the duplication times, deduplicating data still has\na mitigating effect on memorization at the entity level. How-\never, duplication at the entity level is inevitable since these\ndocuments will not be remarkably similar in other aspects,\nwhich makes it even more challenging to guard against ma-\nlicious queries through deduplication.\nLimitations\nDue to privacy concerns, we performed structured process-\ning and experimentation exclusively on the Enron dataset.\nEmpirical results have demonstrated that language models\nexhibit sufficiently strong extraction capabilities at the en-\ntity level. Consequently, if LLMs training or fine-tuning is\nperformed on similar semi-structured sensitive data, the en-\ntity extraction results will be replicable.\nStructured or semi-structured data with abundant entity\ninformation is relatively rare among common LLMs train-\ning datasets, posing challenges for finding multiple sets\nto experiment on extracting unfamiliar sensitive entities.\nThe necessity of procuring a large amount of structured\ndata for testing purposes further exacerbates this problem.\nTherefore, despite our methodology being applicable across\nvarious models, we have been unable to conduct cross-\nfamily model experiments due to these constraints. Lan-\nguage model memorization markedly strengthens with in-\ncreasing model size within the same family, but large model\ninference has high computational demands. Due to compu-\ntational constraints, we only conducted experiments on the\nGPT-Neo family, with the largest LLMs scale being 6B.\nConclusions and Future Work\nIn our paper, we introduce a definition of entity-level memo-\nrization for language models to quantify fine-grained memo-\nrization. With this, researchers can effectively quantify lan-\nguage model memorization to reproduce training data that\nmirrors real-world scenarios. Our method merges entity at-\ntribute information with trainable soft prompts to prompt\nthe language models. Notably, the results indicate that our\nmethod can significantly activate memorization with sub-\ntle entity associations in the model, even if the data ap-\npears only once in the dataset. The entity-level memoriza-\ntion of specific individual data is more explicit than the\nverbatim memorization previously proposed, making entity-\nlevel memorization better quantify privacy risks.\nWe comprehensively analyze entity-level memorization\nof the language models by conducting experiments. Our re-\nsults show that language models have memorized an amount\nof their training data. Through appropriate prompts, LLMs\ncan generate fine-grained sensitive information with a high\nprobability. This reminds that researchers of LLMs need to\npay serious attention to privacy issues in this field, especially\nlanguage models trained on sensitive datasets. It is impera-\ntive to employ strategies that either mitigate memorization\nor establish defenses to minimize associated risks.\nIn future work, we intend to investigate more effective ex-\ntraction strategies to evaluate the memorization of language\nmodels and scale up to larger models to explore their entity-\nlevel memorization. Furthermore, we also aim to develop\nmethods to mitigate entity-level memorization.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19747\nReferences\nB´eguelin, S. Z.; Wutschitz, L.; Tople, S.; R¨uhle, V .; Paverd,\nA.; Ohrimenko, O.; K ¨opf, B.; and Brockschmidt, M. 2020.\nAnalyzing Information Leakage of Updates to Natural Lan-\nguage Models. In Ligatti, J.; Ou, X.; Katz, J.; and Vigna, G.,\neds., CCS ’20: 2020 ACM SIGSAC Conference on Computer\nand Communications Security, Virtual Event, USA, Novem-\nber 9-13, 2020, 363–375. ACM.\nBlack, S.; Leo, G.; Wang, P.; Leahy, C.; and Biderman,\nS. 2021. GPT-Neo: Large Scale Autoregressive Language\nModeling with Mesh-Tensorflow. If you use this software,\nplease cite it using these metadata.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCurran Associates, Inc.\nCarlini, N.; Ippolito, D.; Jagielski, M.; Lee, K.; Tramer, F.;\nand Zhang, C. 2023. Quantifying Memorization Across\nNeural Language Models. In The Eleventh International\nConference on Learning Representations.\nCarlini, N.; Liu, C.; Erlingsson, ´U.; Kos, J.; and Song, D.\n2019. The Secret Sharer: Evaluating and Testing Unin-\ntended Memorization in Neural Networks. In Heninger,\nN.; and Traynor, P., eds.,28th USENIX Security Symposium,\nUSENIX Security 2019, Santa Clara, CA, USA, August 14-\n16, 2019, 267–284. USENIX Association.\nCarlini, N.; Tram`er, F.; Wallace, E.; Jagielski, M.; Herbert-\nV oss, A.; Lee, K.; Roberts, A.; Brown, T. B.; Song, D.; Er-\nlingsson, ´U.; Oprea, A.; and Raffel, C. 2021. Extracting\nTraining Data from Large Language Models. In Bailey,\nM.; and Greenstadt, R., eds., 30th USENIX Security Sym-\nposium, USENIX Security 2021, August 11-13, 2021, 2633–\n2650. USENIX Association.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers), 4171–4186. Association for\nComputational Linguistics.\nGao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.;\nFoster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.;\nPresser, S.; and Leahy, C. 2020. The Pile: An 800GB Dataset\nof Diverse Text for Language Modeling. arXiv:2101.00027.\nGao, T.; Fisch, A.; and Chen, D. 2021. Making Pre-trained\nLanguage Models Better Few-shot Learners. In Zong, C.;\nXia, F.; Li, W.; and Navigli, R., eds.,Proceedings of the 59th\nAnnual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Nat-\nural Language Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, 3816–3830.\nAssociation for Computational Linguistics.\nHuang, J.; Shao, H.; and Chang, K. C. 2022. Are Large Pre-\nTrained Language Models Leaking Your Personal Informa-\ntion? In Goldberg, Y .; Kozareva, Z.; and Zhang, Y ., eds.,\nFindings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, Decem-\nber 7-11, 2022, 2038–2047. Association for Computational\nLinguistics.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How\nCan We Know What Language Models Know.Trans. Assoc.\nComput. Linguistics, 8: 423–438.\nKandpal, N.; Wallace, E.; and Raffel, C. 2022. Dedupli-\ncating Training Data Mitigates Privacy Risks in Language\nModels. In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesv´ari,\nC.; Niu, G.; and Sabato, S., eds., International Conference\non Machine Learning, ICML 2022, 17-23 July 2022, Balti-\nmore, Maryland, USA, volume 162 of Proceedings of Ma-\nchine Learning Research, 10697–10707. PMLR.\nKlimt, B.; and Yang, Y . 2004. The Enron Corpus: A New\nDataset for Email Classification Research. In Boulicaut,\nJ.; Esposito, F.; Giannotti, F.; and Pedreschi, D., eds., Ma-\nchine Learning: ECML 2004, 15th European Conference on\nMachine Learning, Pisa, Italy, September 20-24, 2004, Pro-\nceedings, volume 3201 of Lecture Notes in Computer Sci-\nence, 217–226. Springer.\nLee, K.; Ippolito, D.; Nystrom, A.; Zhang, C.; Eck, D.;\nCallison-Burch, C.; and Carlini, N. 2022. Deduplicating\nTraining Data Makes Language Models Better. In Mure-\nsan, S.; Nakov, P.; and Villavicencio, A., eds., Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, 8424–8445. Association\nfor Computational Linguistics.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power\nof Scale for Parameter-Efficient Prompt Tuning. In Moens,\nM.; Huang, X.; Specia, L.; and Yih, S. W., eds.,Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 7-11 November, 2021, 3045–\n3059. Association for Computational Linguistics.\nLi, H.; Guo, D.; Fan, W.; Xu, M.; Huang, J.; Meng, F.; and\nSong, Y . 2023. Multi-step Jailbreaking Privacy Attacks on\nChatGPT. arXiv:2304.05197.\nLi, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing\nContinuous Prompts for Generation. In Zong, C.; Xia, F.;\nLi, W.; and Navigli, R., eds., Proceedings of the 59th An-\nnual Meeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, 4582–4597. Asso-\nciation for Computational Linguistics.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2021a. Pre-train, Prompt, and Predict: A Systematic Sur-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19748\nvey of Prompting Methods in Natural Language Processing.\nCoRR, abs/2107.13586.\nLiu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang, Z.; and\nTang, J. 2021b. GPT Understands, Too. arXiv:2103.10385.\nLu, Y .; Bartolo, M.; Moore, A.; Riedel, S.; and Stenetorp,\nP. 2022. Fantastically Ordered Prompts and Where to Find\nThem: Overcoming Few-Shot Prompt Order Sensitivity. In\nMuresan, S.; Nakov, P.; and Villavicencio, A., eds.,Proceed-\nings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, 8086–8098. Association\nfor Computational Linguistics.\nLukas, N.; Salem, A.; Sim, R.; Tople, S.; Wutschitz, L.; and\nZanella-B´eguelin, S. 2023. Analyzing Leakage of Person-\nally Identifiable Information in Language Models. In 2023\n2023 IEEE Symposium on Security and Privacy (SP) (SP) ,\n346–363. Los Alamitos, CA, USA: IEEE Computer Society.\nMireshghallah, F.; Uniyal, A.; Wang, T.; Evans, D.; and\nBerg-Kirkpatrick, T. 2022. An Empirical Analysis of Mem-\norization in Fine-tuned Autoregressive Language Models. In\nGoldberg, Y .; Kozareva, Z.; and Zhang, Y ., eds., Proceed-\nings of the 2022 Conference on Empirical Methods in Natu-\nral Language Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, 1816–1826. Associa-\ntion for Computational Linguistics.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and\nLowe, R. 2022. Training language models to follow in-\nstructions with human feedback. In Koyejo, S.; Mohamed,\nS.; Agarwal, A.; Belgrave, D.; Cho, K.; and Oh, A., eds.,\nAdvances in Neural Information Processing Systems, vol-\nume 35, 27730–27744. Curran Associates, Inc.\nOzdayi, M.; Peris, C.; FitzGerald, J.; Dupuy, C.; Majmudar,\nJ.; Khan, H.; Parikh, R.; and Gupta, R. 2023. Controlling the\nExtraction of Memorized Data from Large Language Mod-\nels via Prompt-Tuning. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), 1512–1521. Toronto, Canada: As-\nsociation for Computational Linguistics.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 21: 140:1–140:67.\nShao, H.; Huang, J.; Zheng, S.; and Chang, K. C.-C.\n2023. Quantifying Association Capabilities of Large Lan-\nguage Models and Its Implications on Privacy Leakage.\narXiv:2305.12707.\nShin, T.; Razeghi, Y .; IV , R. L. L.; Wallace, E.; and Singh,\nS. 2020. AutoPrompt: Eliciting Knowledge from Language\nModels with Automatically Generated Prompts. In Web-\nber, B.; Cohn, T.; He, Y .; and Liu, Y ., eds.,Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, November 16-20,\n2020, 4222–4235. Association for Computational Linguis-\ntics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nZhang, C.; Ippolito, D.; Lee, K.; Jagielski, M.; Tram `er, F.;\nand Carlini, N. 2021. Counterfactual Memorization in Neu-\nral Language Models. arXiv:2112.12938.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mi-\nhaylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;\nKoura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.\n2022. OPT: Open Pre-trained Transformer Language Mod-\nels. arXiv:2205.01068.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19749",
  "topic": "Memorization",
  "concepts": [
    {
      "name": "Memorization",
      "score": 0.5948411226272583
    },
    {
      "name": "Computer science",
      "score": 0.5560198426246643
    },
    {
      "name": "Natural language processing",
      "score": 0.4587877094745636
    },
    {
      "name": "Linguistics",
      "score": 0.425618052482605
    },
    {
      "name": "Psychology",
      "score": 0.29731810092926025
    },
    {
      "name": "Cognitive psychology",
      "score": 0.24627605080604553
    },
    {
      "name": "Philosophy",
      "score": 0.08517026901245117
    }
  ]
}