{
  "title": "Text-based adventures of the golovin AI agent",
  "url": "https://openalex.org/W2614188047",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A4302683183",
      "name": "Kostka, Bartosz",
      "affiliations": [
        "Institute of Computer Science",
        "University of Wrocław"
      ]
    },
    {
      "id": null,
      "name": "Kwiecien, Jaroslaw",
      "affiliations": [
        "University of Wrocław"
      ]
    },
    {
      "id": "https://openalex.org/A2313869130",
      "name": "Kowalski, Jakub",
      "affiliations": [
        "Institute of Computer Science",
        "University of Wrocław"
      ]
    },
    {
      "id": null,
      "name": "Rychlikowski, Pawel",
      "affiliations": [
        "Institute of Computer Science",
        "University of Wrocław"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1580222235",
    "https://openalex.org/W6678911119",
    "https://openalex.org/W2490603777",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2604908377",
    "https://openalex.org/W6683910466",
    "https://openalex.org/W6686267929",
    "https://openalex.org/W2604468927",
    "https://openalex.org/W1968791887",
    "https://openalex.org/W6607584435",
    "https://openalex.org/W6600219672",
    "https://openalex.org/W1934909785",
    "https://openalex.org/W6606082447",
    "https://openalex.org/W6675434749",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W4300188727",
    "https://openalex.org/W1551447511",
    "https://openalex.org/W32403112",
    "https://openalex.org/W2125693710",
    "https://openalex.org/W2344251421",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2569196563",
    "https://openalex.org/W2911296969",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6687630728",
    "https://openalex.org/W6680532216"
  ],
  "abstract": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.",
  "full_text": "Text-based Adventures of the Golovin AI Agent\nBartosz Kostka, Jarosław Kwiecie´n, Jakub Kowalski, Paweł Rychlikowski\nInstitute of Computer Science\nUniversity of Wrocław, Poland\nEmail: {bartosz.kostka,jaroslaw.kwiecien}@stud.cs.uni.wroc.pl, {jko,prych}@cs.uni.wroc.pl\nAbstract—The domain of text-based adventure games has been\nrecently established as a new challenge of creating the agent that\nis both able to understand natural language, and acts intelligently\nin text-described environments.\nIn this paper, we present our approach to tackle the problem.\nOur agent, named Golovin, takes advantage of the limited game\ndomain. We use genre-related corpora (including fantasy books\nand decompiled games) to create language models suitable to\nthis domain. Moreover, we embed mechanisms that allow us\nto specify, and separately handle, important tasks as ﬁghting\nopponents, managing inventory, and navigating on the game map.\nWe validated usefulness of these mechanisms, measuring\nagent’s performance on the set of 50 interactive ﬁction games.\nFinally, we show that our agent plays on a level comparable to the\nwinner of the last year Text-Based Adventure AI Competition.\nI. I NTRODUCTION\nThe standard approach to develop an agent playing a given\ngame is to analyze the game rules, choose an appropriate AI\ntechnique, and incrementally increase the agent’s performance\nby exploiting these rules, utilizing domain-dependent features\nand ﬁxing unwanted behaviors. This strategy allowed to beat\nthe single games which were set as the milestones for the AI\ndevelopment: Chess [1] and Go [2].\nAn alternative approach called General Game Playing\n(GGP), operating on a higher level of abstraction has recently\ngained in popularity. Its goal is to develop an agent that can\nplay any previously unseen game without human intervention.\nEquivalently, we can say that the game is one of the agent’s\ninputs [3].\nCurrently, there are two main, well-established GGP do-\nmains providing their own game speciﬁcation languages and\ncompetitions [4]. The ﬁrst one is the Stanford’s GGP, emerged\nin 2005 and it is based on the Game Description Language\n(GDL), which can describe all ﬁnite, turn-based, deterministic\ngames with full information [5], and its extensions (GDL-II\n[6] and rtGDL [7]).\nThe second one is the General Video Game AI framework\n(GVGAI) from 2014, which focuses on arcade video games\n[8]. In contrast to Stanford’s GGP agents are provided with the\nforward game model instead of the game rules. The domain is\nmore restrictive but the associated competition provides multi-\nple tracks, including procedural content generation challenges\n[9], [10].\nWhat the above-mentioned approaches have in common is\nusually a well-deﬁned game state the agent is dealing with.\nIt contains the available information about the state (which\nmay be partially-observable), legal moves, and some kind of\nscoring function (at least for the endgame states). Even in a\nGGP case, the set of available moves is known to the agent,\nand the state is provided using some higher-level structure\n(logic predicates or state observations).\nIn contrast, the recently proposed Text-Based Adventure AI\nCompetition, held during the IEEE Conference on Computa-\ntional Intelligence and Games (CIG) in 2016, provides a new\nkind of challenge by putting more emphasis on interaction\nwith the game environment. The agent has access only to the\nnatural language description about his surroundings and effects\nof his actions.\nThus, to play successfully, it has to analyze the given\ndescriptions and extract high-level features of the game state\nby itself. Moreover, the set of possible actions, which are also\nexpected to be in the natural language, is not available and an\nagent has to deduct it from his knowledge about the game’s\nworld and the current state.\nIn some sense, this approach is coherent with the experi-\nments on learning Atari 2600 games using the Arcade Learn-\ning Environment (ALE), where the agent’s inputs were only\nraw screen capture and a score counter [11], [12]. Although\nin that scenario the set of possible commands is known in\nadvance.\nThe bar for text-based adventure games challenge is set\nhigh – agents should be able to play any interactive ﬁction\n(IF) game, developed by humans for the humans. Such envi-\nronment, at least in theory, requires to actually understand the\ntext in order to act, so completing this task in its full spectrum,\nmeans building a strong AI.\nAlthough some approaches tackling similar problems exist\nsince early 2000s ([13], [14]), we are still at the entry point\nfor this kind of problems, which are closely related to the\ngeneral problem solving. However, recent successes of the\nmachine learning techniques combined with the power of\nmodern computers, give hope that some vital progress in the\ndomain can be achieved.\nWe pick up the gauntlet, and in this work we present our\nautonomous agent that can successfully play many interactive\nﬁction games. We took advantage of the speciﬁc game domain,\nand trained agent using matching sources: fantasy books and\ntexts from decompiled IF games. Moreover, we embed some\nrpg-game-based mechanisms, that allow us to improve ﬁghting\nopponents, managing hero’s inventory, and navigating in the\nmaze of games’ locations.\narXiv:1705.05637v1  [cs.AI]  16 May 2017\nWe evaluated our agent on a set of 50 games, testing\nthe inﬂuence of each speciﬁc component on the ﬁnal score.\nAlso, we tested our agent against the winner of the last year\ncompetition [15]. The achieved results are comparable. Our\nagent scored better in 12 games and worse in 11 games.\nThe paper is organized as follows. Section II provides back-\nground for the domain of interactive ﬁction, Natural Language\nProcessing (NLP), Text-Based Adventure AI Competition,\nand the related work. In Section III, we presented detailed\ndescription of our agent. Section IV contains the results of the\nperformed experiments. Finally, in Section V , we conclude and\ngive perspective of the future research.\nII. B ACKGROUND\nA. Interactive Fiction\nInteractive Fiction (IF), emerged in 1970s, is a domain of\ntext-based adventure or role playing games, where the player\nuses text commands to control characters and inﬂuence the\nenvironment. One of the most famous example is the Zork\nseries developed by the Infocom company. From the formal\npoint of view, they are single player, non-deterministic games\nwith imperfect information. IF genre is closely related to\nMUDs (Multi-User Dungeons), but (being single-player) more\nfocused on plot and puzzles than ﬁghting and interacting\nwith other players. IF was popular up to late 1980s, where\nthe graphical interfaces become available and, as much user\nfriendlier, more popular. Nevertheless, new IF games are still\ncreated, and there are annual competitions for game authors\n(such as The Interactive Fiction Competition).\nIF games usually (but not always) take place in some fantasy\nworlds. The space the character is traversing has a form of\nlabyrinth consisting of so called rooms (which despite the\nname can be open areas like forest). Entering the room, the\ngame displays its description, and the player can interact with\nthe objects and game characters it contains, or try to leave\nthe room moving to some direction. However, reversing a\nmovement direction (e.g. go south ↔go north) not necessarily\nreturns the character to the previous room.\nAs a standard, the player character can collect objects from\nthe world, store them in his equipment, and combine with\nother objects to achieve some effects on the environment (e.g.\nput the lamp and sword in the case ). Thus, many games\nrequire solving some kind of logical puzzle to push the action\nforward. After performing an action, the game describes its\neffect. Many available actions are viable, i.e. game engine\nunderstands them, but they are not required to solve the game,\nor even serve only for the player amusement.\nSome of the games provide score to evaluate the player’s\nprogress, however the change in the score is often the result of\na complex series of moves rather than quick “frame to frame”\ndecisions, or the score is given only after the game’s end.\nOther games do not contain any scoring function and the only\noutput is win or lose.\nB. Playing Text-Based Games\nAlthough the challenge of playing text-based games was\nnot take on often, there are several attempts described in the\nliterature, mostly based on the MUD games rather than the\nclassic IF games.\nAdventure games has been carefully revised as the ﬁeld of\nstudy for the cognitive robotics in [14]. First, the authors iden-\ntify the features of “tradition adventure game environment” to\npoint-out the speciﬁcs of the domain. Second, they enumerate\nand discuss existing challenges, including e.g. the need for\ncommonsense knowledge (its learning, revising, organization,\nand using), gradually revealing state space and action space,\nvague goal speciﬁcation and reward speciﬁcation.\nIn [13], the agent able to live and survive in an existing\nMUD game have been described. The authors used layered\narchitecture: high level planning system consisting of reason-\ning engine based on hand-crafted logic trees, and a low level\nsystem responsible for sensing the environment, executing\ncommands to fulﬁll the global plan, detecting and reacting\nin emergency situations.\nWhile not directly-related to playing algorithms, it is worth\nto note the usage of computational linguistics and theorem\nproving to build an engine for playing text-based adventure\ngames [16]. Some of the challenges are similar for both\ntasks, as generating engine requires e.g. object identiﬁcation\n(given user input and a state description) and understanding\ndependencies between the objects.\nThe approach focused on tracking the state of the world\nin text-based games, and translating it into the ﬁrst-order\nlogic, has been presented in [17]. Proposed solution was able\nto efﬁciently update agent’s belief state from a sequence of\nactions and observations.\nThe extension of the above approach, presents the agent that\ncan solve puzzle-like tasks in partially observable domain that\nis not known in advance, assuming actions are deterministic\nand without conditional effects [18]. It generates solutions by\ninterleaving planning (based on the traditional logic-based ap-\nproach) and execution phases. The correctness of the algorithm\nis formally proved.\nRecently, an advanced MUD playing agent has been de-\nscribed in [19]. Its architecture consists of two modules. First,\nresponsible for converting textual descriptions to state repre-\nsentation is based on the Long Short-term Memory (LSTM)\nnetworks [20]. Second, uses Deep Q-Networks [11] to learn\napproximated evaluations for each action in a given state.\nProvided results show that the agent is able to to successfully\ncomplete quests in small, and even medium size, games.\nC. Natural Language Processing\nNatural Language Processing is present in the history of\ncomputers almost from the very beginning. Alan Turing in\nhis famous paper [21] state (approximately) that “exhibit\nintelligent behavior” means “understand natural language and\nuse it properly in conversations with human being”. So, since\n1950 Turing test is the way of checking whether computer has\nreached strong AI capability.\nFirst natural language processing systems were rule based.\nThanks to the growing amount of text data and increase of\nthe computer power, during last decades one can observe the\nshift towards the data driven approaches (statistical or machine\nlearning). Nowadays, NLP very often is done “almost from\nscratch”, as it was done if [22] where the authors have used\nneural network in order to solve many NLP tasks, including\npart-of-speech tagging, named entity recognition and semantic\nrole labeling. The base for this was the neural language model.\nMoreover, this systems produced (as a side effect) for every\nword in a vocabulary a dense vector which reﬂected word\nproperties. This vectors are called word embeddings and can\nbe obtained in many ways. One of the most popular is the one\nproposed in [23] that uses very simple, linear language model\nand is suitable to large collections of texts.\nLanguage models allow to compute probability of the sen-\ntence treated as a sequence of items (characters, morphemes\nor words). This task was traditionally done using Markov\nmodels (with some smoothing procedures, see [24]). Since\npredicting current words is often dependent on the long part\nof history, Markov models (which, by deﬁnition, looks only\nsmall numbers of words behind) are outperformed by the\nmodern methods that can model long distance dependencies.\nThis methods use recursive (deep) neural networks, often\naugmented with some memory.\nWe will use both word embeddings (to model words similar-\nity) and LSTM neural networks [25] with attention mechanism\n(see [26] and [27]). We are particularly interested in the\ninformation given from the attention mechanism, which allows\nus to estimate how important is each word, when we try to\npredict the next word in the text.\nD. The Text-Based Adventure AI Competition\nThe ﬁrst Text-Based Adventure AI Competition 1, organized\nby the group from the University of York, has been announced\nin May 2016 and took place at the 2016 IEEE CIG conference\nin September. The second, will be held this year, also co-\nlocated with CIG.\nThe purpose of the competition is to stimulate research\ntowards the transcendent goal of creating General Problem\nSolver, the task stated nearly six decades ago [28]. The orga-\nnizers plan to gradually increase the level of given challenges,\nwith time providing more complex games that require more\nsophisticated approaches from the competitors. Thus, ﬁnally\nforce them to develop agents that can autonomously acquire\nknowledge required to solve given tasks from the restricted\ndomain of text-based games.\nThe domain of the competition is speciﬁed as any game that\ncan be described by the Z-machine, the classic text adventuring\nengine. Interactive Fiction games are distributed as compiled\nbyte code, requiring the special interpreter to run them. The\nﬁrst Z-machine has been developed in 1979 by Infocom,\nand supports games developed using a LISP-like program-\nming language named Infocom’s ZIL (Zork Implementation\n1http://atkrye.github.io/IEEE-CIG-Text-Adventurer-Competition/.\nLanguage). The Text-based AI Competition uses Frotz 2, the\nmodern version of the Z-machine, compatible with the original\ninterpreter.\nThe competition organizers provide a Java package man-\naging the communication between a game ﬁle and an agent\nprocess. Also, example random agents in Java and Python 3 are\navailable. The interpreter is extended by the three additional\ncommends, allowing players to quit the game, restart it with\na new instance of the agent, and restart without modifying\nthe agent. Given that, the text-based AI framework supports\nlearning and simulation-based approaches.\nLittle details about the competition insides are available. In\nparticular, the number of participants is unknown, and the test\nenvironment game used to evaluate agents remained hidden,\nas it is likely to be used again this year. The game has been\ndeveloped especially for the purpose of the competition and\nsupports graduated scale of scoring points, depending on the\nquality of agent’s solution.\nThe winner of the ﬁrst edition was the BYU-Agent 3 from\nthe Perception Control and Cognition lab at Brigham Young\nUniversity, which achieved a score 18 out of 100. The idea\nbehind the agent has been described in [15]. It uses Q-learning\n[29] to estimate the utility of an action in a given game state,\nidentiﬁed as the hash of its textual description.\nThe main contribution concerns affordance detection, used\nto generating reasonable set of actions. Based on the word2vec\n[30], an algorithm mapping words into a vector representations\nbased on their contextual similarities, and the Wikipedia as the\nword corpus, the verb-noun affordances are generated. Thus,\nthe algorithm is able to detect, for an in-game object, words\nwith a similar meaning, and provide a set of actions that are\npossible to undertake with that object.\nProvided results, based on the IF games compatible with Z-\nmachine interpreter, shows the overall ability of the algorithm\nto successfully play text-based games. Usually, the learning\nprocess results in increasing score, and requires playing a game\nat least 200 times to reach the peek. However, there are some\ngames that achieve that point much slower, or even the score\ndrops as the learning continues.\nIII. T HE GAME PLAYING AGENT\nA. Overview\nOur agent is characterized by the following features.:\n• it uses a huge set of predeﬁned command patterns,\nobtained by analyzing various domain-related sources; the\nactual commands are obtained by suitable replacements;\n• it uses language models based on selection of fantasy\nbooks;\n• it takes advantage of the game-speciﬁc behaviors, natural\nfor adventure games, like ﬁght mode, equipment manage-\nment, movement strategy;\n2http://frotz.sourceforge.net.\n3The agent is open source and available at https://github.com/danielricks/\nBYU-Agent-2016.\n• it memorizes and uses some aspects of the current play\nhistory;\n• it tries to imitate human behavior: after playing several\ngames and exploring the game universe it repeats the most\npromising sequence of commands. We treat the result\nreached in this ﬁnal trial as the agent’s result in this game.\nThe agent was named “Golovin”, as one of the ﬁrst answers\nit gives after asking Hey bot, what is your name? , was your\nname is Golovin , a phrase from the game Doomlords.\nB. Preprocessing\n1) Language Models: We used language models for two\npurposes. First, they allow us to deﬁne words similarity (which\nin turns gives us opportunity to replace some words in com-\nmands with their synonyms). For this task we use word2vec\n[30] (and its implementation in TensorFlow [31]). Secondly,\nwe use neural network language models to determine which\nwords in the scene description plays more important role than\nother (and so are better candidates to be a part of the next\ncommand). We use the LSTM neural networks operating on\nwords [25], augmented by the attention mechanism ([26] and\n[27]). This combination was previously tested in [32].\nSince the action of many games is situated in fantasy\nuniverse, we decided to train our models on the collection\nof 3000 fantasy books from bookrix.com (instead of using\nWikipedia, as in [15]).\n2) Commands: In order to secure out agent against overﬁt-\nting, we ﬁx the set of games used in tests (the same 50 games\nas in [15]). No data somehow related to this games were used\nin any stage of preprocessing.\nWe considered three methods to gather commands:\n• walkthroughs – for several games, sequence of com-\nmands from winning path can be found in the Internet.\nThis source provides raw valid commands, that are useful\nin some games.\n• tutorials – on the other hand, some games have tutorials\nwritten in natural language. Analyzing such tutorials 4\nseemed to be a promising way of getting valid command.\nConcept of reading manuals has been successfully used\nto learn how to play Civilization II game [33].\n• games – at the end, there are many games that don’t have\ntutorials nor walkthroughs. We downloaded a big collec-\ntion of games, decompiled their codes, and extracted all\ntexts from them.\nThe last two sources required slightly more complicated\npreprocessing. After splitting texts into sentences, we parsed\nthem using PCFG parser from NLTK package [34]. Since\ncommands are (in general) verb phrases, we found all minimal\nVP phrases from parse trees. After reviewing some of them,\nwe decided not to take every found phrase, but manually\ncreate the list of conditions which characterizes ’verb phrases\nuseful in games’. In this way we obtained the collections of\n4This tutorials were downloaded from the following sites: http://www.\nifarchive.org/, https://solutionarchive.com/, http://www.gameboomers.com/,\nhttp://www.plover.net/~davidw/sol/\napproximately 250,000 commands (or, to be more precisely,\ncommand patterns). We also remember the count of every\ncommand (i.e. the number of parse tree it occurs in).\nSome of the commands have special tag: “useful in the\nbattle”. We have manually chosen ﬁve verbs, as the most\ncommonly related to ﬁghting: attack, kill, ﬁght, shoot, and\npunch. Approximately 70 most frequent commands containing\none of these verbs received this tag.\nThe commands used by our agent were created from these\npatterns by replacing (some) nouns by nouns taken from the\ngame texts.\nC. Playing Algorithm\nThe algorithm uses 5 types of command generators: battle\nmode, gathering items, inventory commands, general actions\n(interacting with environment), and movement. The generators\nare ﬁred in the given order, until a non-empty set of commands\nis proposed.\nThere are multiple reasons why some generator may not\nproduce any results, e.g. the agent reaches the predeﬁned limit\nof making actions of that type, all the candidates are black-\nlisted, or simply we cannot ﬁnd any appropriate command in\nthe database. We will describe other special cases later.\nWhen the description of the area changes, all the command\nlists are regenerated.\n1) Generating Commands: Our general method to compute\navailable commands and choose the one which is carried out,\nlooks as follows:\n1) Find all nouns in the state description and agent’s\nequipment. (We accept all type of nouns classiﬁed by\nthe nltk.pos_tag function.)\n2) Determine their synonyms, based on the cosine similar-\nity between word vectors. (We use n-best approach with\nn being subject to Spearmint optimization; see IV-A.)\n3) Find the commands containing nouns from the above\ndescribed set. If a command contains a synonym, it is\nreplaced by the word originally found in the description.\n4) Score each command taking into account:\n• cosine similarity between used synonyms and the\noriginal words\n• uniqueness of the words in command, computed as\nthe inverse of number of occurrences in the corpora.\n• the weight given by the neural network model\n• the number of words occurring both in the descrip-\ntion and in the command\nThe score is computed as the popularity of the command\n(number of occurrences in the command database) mul-\ntiplied by the product of the above. The formula uses\nsome additional constants inﬂuencing the weights of the\ncomponents.\n5) Then, using the score as the command’s weight, ran-\ndomly pick one command using the roulette wheel\nselection.\n2) Battle Mode: The battle mode has been introduced to\nimprove the agent’s ability to survive. It prevents from what\nhas been the main cause of agent’s death before – careless\nwalking into an enemy or spending too much time searching\nfor the proper, battle-oriented and life-saving, action.\nThe agent starts working in battle mode after choosing\none of the “ﬁght commands”. Being in this mode, the agent\nstrongly prefers using battle command, moreover it repeats\nthe same command several times (even if it fails), because\nin many games the opponent has to be attacked multiple\ntimes to be defeated. Therefore, between the consecutive\nﬁghting actions we prevent using standard commands (like\nlook, examine), as wasting precious turns usually gives the\nopponent an advantage.\n3) Inventory Management (gathering and using items):\nIn every new area, the algorithm searches its description for\ninteresting objects. It creates a list of nouns ordered by the\nweight given by the neural network model and their rarity.\nThen, the agent tries take them.\nIf it succeeds (the content of the inventory has changed), a\nnew list of commands using only the newly acquired item is\ngenerated (using the general method). The constant number of\nhighest scored commands is immediately executed.\n4) Exploration: The task of building an IF game map is\ndifﬁcult for two reasons. One, because a single area can be\npresented using multiple descriptions and they may change as\nthe game proceeds. Two, because there may be different areas\ndescribed by the same text. Our map building algorithm tries\nto handle these problems.\nWe have found that usually the ﬁrst sentence of the area\ndescription remains unchanged, so we use it to label the\nnodes of the graph (we have tried other heuristics as well\nbut they performed worse). This heuristic divides all visited\nnodes into the classes suggesting that corresponding areas may\nbe equivalent. The edges of the graph are labeled by the move\ncommands used to translocate between the nodes (we assume\nthat movement is deterministic).\nWe initialize the map graph using the paths corresponding\nto the past movements of the agent. Then, the algorithm takes\nall pairs of nodes with the same label and considers them in\na speciﬁc, heuristic-based, order. For every pair, the MergeN-\nodes procedure (Listing 1) is ﬁred. The procedure merges two\nstates joining their outcoming edges and recursively proceeds\nto the pairs of states that are reachable using the same move\ncommand. If the procedure succeeds, we replaces current map\ngraph with the minimized one, otherwise the changes are\nwithdrawn.\nWe use a small ﬁxed set of movement commands ( south,\nnorthwest, up, left, etc.) to reveal new areas and improve the\nknowledge about the game layout. When the agent decides\nto leave the area, it tries a random direction, unless it al-\nready discovered all outgoing edges – then it uses map to\nﬁnd a promising destination. We evaluate destination nodes\nminimizing the distance to that node plus the number of\ntested commands divided by the node’s curiosity (depending\non scores of available commands and untested movement\ncommands). Then, the agent follows the shortest path to the\nbest scored destination.\nAlgorithm 1 MergeNodes(A, B)\n1: if A = B then return True end if\n2: if label(A)̸=label(B) then return False end if\n3: mergelist ←{}\n4: for all m ∈MoveCommands do\n5: if A.moveby(m)̸=None ∧B.moveby(m)̸=None then\n6: mergelist.append((A.moveby(m), B.moveby(m)))\n7: end if\n8: end for\n9: JoinIncomingAndOutgoingEdges(A, B)\n10: for all (A′, B′) ∈mergelist do\n11: if ¬MergeNodes(A′, B′) then return False end if\n12: end for\n13: return True\n5) Failing Commands: When, after executing a command,\nthe game state (description) remains unchanged, we assume\nthe command failed. Thus, the algorithm puts it on a blacklist\nassigned to the current location. The command on the loca-\ntion’s black list is skipped by the command generators.\nAfter any change in the agent’s inventory, all blacklists are\ncleared.\n6) Restarts: The Frotz environment used for the contest\nallows to restart the game, i.e. abandon current play and start\nagain from the beginning.\nMe make use of this possibility in a commonsense imitating\nof the human behavior. When the agent dies, it restarts the\ngame and, to minimize the chance of the further deaths, it\navoids repeating the last commands of his previous lives. The\nagent also remembers the sequence of moves that lead to the\nbest score and eventually repeats it. The ﬁnal trial’s result is\nused as the agent’s result in the game.\nIV. E XPERIMENTS\nOur experiments had two main objectives: creating the most\neffective agent, and analyze how some parameters inﬂuence\nthe agents performance.\nThe most natural way to measure the agent performance is\nto use the score given by the game (divided by the maximum\nscore, when we want to compare different games). However,\nthere are many games in which our agent (as well as BYU-\nAgent) has problems with receiving non zero points. So, we\nhave decided to reward any positive score and add to the\npositive agent result arbitrarily chosen constant 0.2. Therefore,\noptimal (hypothetical) agent would get 1.2 in every game.\nWe selected 20 games for the training purposes, for all\nof them the maximum score is known. The performance of\nthe agent is an averaged (modiﬁed) score computed on these\ngames.\nA. Creating The Best Agent\nThe agent’s play is determined by some parameters, for\ninstance:\n• the set of command patterns,\n• the number of synonyms taken from word2vec,\n• the number of items, we are trying to gather, after visiting\nnew place,\n• the number of standard command, tried after gathering\nphase,\n• how to reward the commands containing many words\nfrom description (the actual reward is bk, where k is the\nnumber of common words, and b is a positive constant),\n• how to punish the commands containing words with no\ngood reason to be used (neither in state description nor in\ngenerated synonyms), the score is divided by pk, where\nk is the number of such words, and p is a constant,\n• how many command should be done before trying move-\nment command.\nFurthermore we wanted to check, whether using battle mode\nor a map has an observable effect on agent performance. The\nnumber of parameter combinations was too large for grid\nsearch, so we decided to use Spearmint 5.\nWe started this optimization process with (total) score equal\nto 0.02, and after some hours of computation we end with\n0.08 (which means that the score has been multiplied 4 times).\nFrom now all parameters (if not stated otherwise) will be taken\nfrom the ﬁnal Spearmint result.\nB. Evaluation of Domain-based Mechanisms\nWe wanted to check whether some more advanced features\nof our agent give observable inﬂuence on agent performance.\nWe checked the following 4 conﬁgurations with battle-mode\nand map turned on or off. The results are presented in Figure\n1. One can see that map is useful (but only to some extent),\nand battle mode is undoubtedly useful.\nFig. 1. Comparison of agent version with and without map and battle mode.\nBest variant scaled to 100%.\nC. Evaluation of Language Model Sources\nCommands were taken from 3 sources: tutorials (T), walk-\nthroughs (W), and state description from games (G). We\ncompared the agents used command from all combination\nof these sources. The results are presented in Figure 2. The\n5Spearmint is a package which performs Bayesian optimization of hyper-\nparameters. It allows to treat the optimized function as a black-box, and tries\nto choose the parameters for the next run considering the knowledge gathered\nduring previous runs. See [35].\noptimal conﬁguration uses only two sources: T and W 6. We,\nhowever, still believe that decompiled games can be a useful\nsource for game commands. But they cannot be found in\ndescriptions, but in command interpreter – which requires\nmore advanced automated code analysis. We left it as a future\nwork.\nFig. 2. Comparison of agent using different sources of commands. Best\nvariant scaled to 100%.\nD. Gameplay Examples\nWhile playing detective, our agent ﬁnds himself in a\ncloset. We get the following state description:\nGame: You are in a closet. There is a gun on the\nﬂoor. Better get it. To exit, go east.\nOur agent determines items: closet, gun, ﬂoor, exit. Our agent\nis choosing from the commands listed in Table I. We see that\nwhat is the most important for the agent is gun and how to\ntake it, which is reasonable and even suggested by the game.\nMoreover, the agent also tries to search using synonyms of\nthe word gun in order to ﬁnd proper commands (for instance:\nwe can see that he recognizes gun as some kind of weapon,\nand some weapons, as for example knife, can be sharpened).\nFortunately, after using a command get gun, Golovin obtains\na black little pistol.\nTABLE I\nBEST 10 ( OUT OF 25) COMMANDS PROPOSED BY OUR AGENT FOR THE\nSITUATION DESCRIBED IN THE D E T E C T I V EEXAMPLE (SECTION IV-D)\naction score\nget gun 0.1736\ndrop gun 0.1129\ntake gun 0.0887\nopen closet 0.0557\nexamine gun 0.0309\nﬁre gun 0.0252\nload gun 0.0237\nexamine closet 0.0128\nbuy gun 0.0042\nsharp gun 0.0006\nAnother example comes from zork3. We start with the\nfollowing location description:\n6The difference between T+W and G+T+W is not very big. In the previous\nversion of this experiment the winner was G+T+W.\nGame: You are in a shadowy land of low, rolling\nhills stretching out to the west and south. The\nland is bordered to the north by a massive stone\nwall. Ancient and weathered, the wall has crumbled\nenough at one point to permit passage. A cloaked\nand hooded person, carrying a sword not unlike your\nown, is standing blocking the way to the northeast.\nThe hooded ﬁgure has a great deal of strength,\nperhaps matching your own.\nGolovin: examine ﬁgure\nGame: There is nothing special to be seen. Your\nquick reﬂexes cannot stop the hooded ﬁgure’s stroke!\nYou are hit!\nIn this very moment, in the agent’s inventory are: a sword,\na lamp . In the command list we can ﬁnd for instance kill\nman with sword . As the word man is close enough to ﬁgure,\nand Golovin also have sword, he chooses to use command\nkill ﬁgure with sword . Because the ﬁghting command was\nchosen, the battle mode is turned on, and the game continues\nas follows:\nGolovin: kill ﬁgure with sword\nGame: The hooded ﬁgure is hit with a quick slash!\nThe ﬁgure has a light wound which hasn’t affected\nits seemingly great strength.\nthe agent’s score improves and the ﬁght continues. . .\nGolovin: kill ﬁgure with sword\nGame: You charge, but the hooded ﬁgure jumps\nnimbly aside. The hooded ﬁgure stabs nonchalantly\nwith its sword and misses.\nE. The Comparison with BYU-Agent\nFinally, we validate our approach by comparing it with the\nBYU-Agent. We were using the same set of 50 Z-machine\ngames7 as in [15].\nThe results of the comparison are presented in Table II.\nThe BYU agent was trained for 1000 epochs (each epoch\ncontaining 1000 game steps), and its score was noted after\neach epoch. Because the learning curves vary depending on\nthe game, including degeneration of the results (see [15,\nFigure 5]), as the main measure we took the maximum score\nachieved over all epochs.\nAs for the Golovin, we restricted his playing time to\n1000 steps (i.e. an equivalent of one epoch) and use our\ncommonsense restarting mechanism.\nThe BYU-Agent results are obtained using the verb and\naction space reduction algorithm, except the games marked\nwith an asterisk, where the verb space reduction experienced\nerrors, so we present scores obtained by the action space\nreduction variant instead.\nEventually, there are 24 games, out of 50, where some of\nthe agents received any positive reward. Golovin scored better\nin 12 games, including 7 games where BYU-Agent received\nno reward. BYU-Agent scored better in 11 games, including\n7The game set is available at https://github.com/danielricks/textplayer/tree/\nmaster/games.\n6 games where Golovin scored no points. One game is a non-\nzero tie.\nThus, despite signiﬁcantly shorter learning time (i.e. avail-\nable number of steps), our agent is able to outperform BYU-\nAgent on a larger number of games than he is outperformed on.\nOn the other hand, BYU-Agent gains in the games where the\nQ-learning is effective and gradually increases score through\nthe epochs, e.g. curses, detective or Parc.\nLast observation concerns the number of games where only\none of the agents scored 0, which is surprisingly large. This\nmay suggest that the two compared approaches are effective\non a different types of games, and may, in some sense,\ncomplement each other.\nTABLE II\nAVERAGE SCORES FOR 10 RUNS OF EACH GAME . FOR BYU-A GENT WE\nTOOK THE MAXIMUM ACHIEVED SCORE DURING THE 1000 EPOCHS\nTRAINING . GOLOVIN PLAYS FOR ONE EPOCH . IN THE GAMES THAT ARE\nNOT LISTED BOTH AGENTS GAIN NO REWARD . THE ASTERISK MARKS\nGAMES THAT USES OTHER VERSION OF BYU-A GENT\ngame Golovin BYU-Agent max score\nbalances 9.0 0 51\nbreak-in 0 0.3 150\nbunny 2.7 2.0 60\ncandy 10.0 10.0 41\ncavetrip 15.0 10.5 500\ncurses 0.4 1.9 550\ndeephome 1.0 0 300\ndetective 71.0 213.0 360\ngold 0.3 0 100\nlibrary 5.0 0 30\nmansion 0.1 2.2 68\nMurdac 10.0 0 250\nnight 0.8 0 10\nomniquest 7,5 5.0 50\nparallel 0 5.0 150\nParc 1.6 5.0 50\nreverb 0 1.8 50\nspirit 3.2 2.0 250\ntryst205 0.2 2.0 350\nzenon 0 2.8 20\nzork1 13.5 *8.8 350\nzork2 -0.1 * 3.3 400\nzork3 0.7 *0 7\nztuu 0 0.5 100\nbetter in: 12 11 games\nV. C ONCLUSIONS AND FUTURE WORK\nWe have presented an agent able to play any interactive ﬁc-\ntion game created for human players, on the level comparable\nto the winner of the last year Text-Based Adventure AI Com-\npetition. Due to the number of domain-based mechanisms, our\nagent can successfully handle the game in a limited number\nof available steps. The results of the presented experiments\nshow that the mechanisms we embed (battle mode, mapping)\nand a choice of learning sources, indeed improves the agent’s\nperformance.\nAlthough the results are promising, we are still at the\nbeginning of the path towards creating the agent that can\nreally understand the natural language descriptions in order\nto efﬁciently play the text-based adventure games.\nThere are multiple future work directions we would like to\npoint out. First, and one of the most important, is to embed\na learning mechanisms: the in-game learning, that uses restart\nfunctionality to improve player efﬁciency in one particular\ngame; and preliminary learning, that is able to gain useful\nknowledge from playing entire set of games. Also, we plan to\ntake a closer look at the decompiled game codes, as we believe\nthat analyzing them may provide very useful knowledge.\nWe would like to improve the battle mode behavior, es-\npecially mitigate the agent and make it more sensitive to the\nparticular situation. We hope that the mapping mechanism can\nbe further extended to allow the casual approach, where the\nagent travels to distant locations for some speciﬁc reason (e.g.\nitem usage), instead of a simple reactive system that we have\nnow.\nLastly, we would like to continue the domain-based ap-\nproach, and so focus our efforts on discovering the subgames\n(like we did with ﬁghting and exploring) that we are able to\nproperly detect, and handle signiﬁcantly better than the general\ncase.\nACKNOWLEDGMENTS\nThe authors would like to thank Szymon Malik for his\nvaluable contribution in the early stage of developing Golovin.\nWe would also like to thank Nancy Fulda for helpful\nanswers to our questions and providing up-to date results of\nthe BYU-Agent.\nREFERENCES\n[1] M. Campbell, A. J. Hoane, and F. Hsu, “Deep Blue,” Artiﬁcial intelli-\ngence, vol. 134, no. 1, pp. 57–83, 2002.\n[2] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den\nDriessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanc-\ntot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever,\nT. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis,\n“Mastering the game of Go with deep neural networks and tree search,”\nNature, vol. 529, pp. 484–503, 2016.\n[3] M. Genesereth and M. Thielscher, General Game Playing . Morgan &\nClaypool, 2014.\n[4] M. Genesereth, N. Love, and B. Pell, “General Game Playing: Overview\nof the AAAI Competition,” AI Magazine, vol. 26, pp. 62–72, 2005.\n[5] N. Love, T. Hinrichs, D. Haley, E. Schkufza, and M. Genesereth,\n“General Game Playing: Game Description Language Speciﬁcation,”\nStanford Logic Group, Tech. Rep., 2006.\n[6] M. Thielscher, “A General Game Description Language for Incomplete\nInformation Games,” in AAAI Conference on Artiﬁcial Intelligence ,\n2010, pp. 994–999.\n[7] J. Kowalski and A. Kisielewicz, “Towards a Real-time Game Descrip-\ntion Language,” in International Conference on Agents and Artiﬁcial\nIntelligence, vol. 2, 2016, pp. 494–499.\n[8] D. Perez, S. Samothrakis, J. Togelius, T. Schaul, S. Lucas, A. Couëtoux,\nJ. Lee, C. Lim, and T. Thompson, “The 2014 General Video Game\nPlaying Competition,”IEEE Transactions on Computational Intelligence\nand AI in Games , vol. 8, no. 3, pp. 229–243, 2015.\n[9] D. Perez, S. Samothrakis, J. Togelius, T. Schaul, and S. M. Lucas,\n“General Video Game AI: Competition, Challenges and Opportunities,”\nin AAAI Conference on Artiﬁcial Intelligence , 2016, pp. 4335–4337.\n[10] A. Khalifa, D. Perez, S. Lucas, and J. Togelius, “General Video\nGame Level Generation,” in Genetic and Evolutionary Computation\nConference, 2016, pp. 253–259.\n[11] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al. , “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[12] S. Kelly and M. I. Heywood, “Emergent Tangled Graph Representations\nfor Atari Game Playing Agents,” in EuroGP 2017: Genetic Program-\nming, ser. LNCS, 2017, vol. 10196, pp. 64–79.\n[13] M. A. DePristo and R. Zubek, “being-in-the-world,” in Proceedings\nof the 2001 AAAI Spring Symposium on Artiﬁcial Intelligence and\nInteractive Entertainment, 2001, pp. 31–34.\n[14] E. Amir and P. Doyle, “Adventure games: A challenge for cognitive\nrobotics,” in Proc. Int. Cognitive Robotics Workshop , 2002, pp. 148–\n155.\n[15] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate, “What can you do with\na rock? Affordance extraction via word embeddings,” in International\nJoint Conference on Artiﬁcial Intelligence , 2017, (to appear).\n[16] A. Koller, R. Debusmann, M. Gabsdil, and K. Striegnitz, “Put my\ngalakmid coin into the dispenser and kick it: Computational linguistics\nand theorem proving in a computer game,” Journal of Logic, Language\nand Information, vol. 13, no. 2, pp. 187–206, 2004.\n[17] B. Hlubocky and E. Amir, “Knowledge-gathering agents in adventure\ngames,” in AAAI-04 workshop on Challenges in Game AI , 2004.\n[18] A. Chang and E. Amir, “Goal Achievement in Partially Known, Partially\nObservable Domains,” in Proceedings of the Sixteenth International\nConference on International Conference on Automated Planning and\nScheduling, 2006, pp. 203–211.\n[19] K. Narasimhan, T. Kulkarni, and R. Barzilay, “Language Understanding\nfor Text-based Games using Deep Reinforcement Learning,” inProceed-\nings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, 2015, pp. 1–11.\n[20] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[21] A. M. Turing, “Computing machinery and intelligence,” Mind,\nvol. 59, no. 236, pp. 433–460, 1950. [Online]. Available: http:\n//www.jstor.org/stable/2251299\n[22] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P.\nKuksa, “Natural language processing (almost) from scratch,” CoRR, vol.\nabs/1103.0398, 2011. [Online]. Available: http://arxiv.org/abs/1103.0398\n[23] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation\nof word representations in vector space,” CoRR, vol. abs/1301.3781,\n2013. [Online]. Available: http://arxiv.org/abs/1301.3781\n[24] S. F. Chen and J. Goodman, “An empirical study of smoothing\ntechniques for language modeling,” in Proceedings of the 34th\nAnnual Meeting on Association for Computational Linguistics , ser.\nACL ’96. Stroudsburg, PA, USA: Association for Computational\nLinguistics, 1996, pp. 310–318. [Online]. Available: http://dx.doi.org/\n10.3115/981863.981904\n[25] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural prob-\nabilistic language model,” JOURNAL OF MACHINE LEARNING RE-\nSEARCH, vol. 3, pp. 1137–1155, 2003.\n[26] C. Raffel and D. P. W. Ellis, “Feed-forward networks with attention can\nsolve some long-term memory problems,” CoRR, vol. abs/1512.08756,\n2015. [Online]. Available: http://arxiv.org/abs/1512.08756\n[27] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks\nfor machine reading,” CoRR, vol. abs/1601.06733, 2016. [Online].\nAvailable: http://arxiv.org/abs/1601.06733\n[28] A. Newell, J. C. Shaw, and H. A. Simon, “Report on a general problem\nsolving program,” in Proceedings of the International Conference on\nInformation Processing, 1959, pp. 256–264.\n[29] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no.\n3-4, pp. 279–292, 1992.\n[30] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” 2013, arXiv:1301.3781 [cs.CL].\n[31] M. A. et al., “TensorFlow: Large-scale machine learning on\nheterogeneous systems,” 2015, software available from tensorﬂow.org.\n[Online]. Available: http://tensorﬂow.org/\n[32] S. Malik, “Application of artiﬁcial neural networks with attention mech-\nanism for discovering distant dependencies in time series,” Bachelor\nThesis, University of Wrocław, 2016.\n[33] S. Branavan, D. Silver, and R. Barzilay, “Learning to Win by Reading\nManuals in a Monte-Carlo Framework,”Journal of Artiﬁcial Intelligence\nResearch, vol. 43, pp. 661–704, 2012.\n[34] S. Bird, E. Klein, and E. Loper, Natural Language Processing with\nPython, 1st ed. O’Reilly Media, Inc., 2009.\n[35] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-\ntion of machine learning algorithms,” in Advances in Neural Information\nProcessing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.\nWeinberger, Eds. Curran Associates, Inc., 2012, pp. 2951–2959.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I4210087266",
      "name": "Institute of Computer Science",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I219388962",
      "name": "University of Wrocław",
      "country": "PL"
    }
  ]
}