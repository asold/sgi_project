{
    "title": "On the Multilingual Capabilities of Very Large-Scale English Language Models",
    "url": "https://openalex.org/W3197510562",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4222692909",
            "name": "Armengol-Estapé, Jordi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226782779",
            "name": "Bonet, Ona de Gibert",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226782777",
            "name": "Melero, Maite",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3095645723",
        "https://openalex.org/W3085332162",
        "https://openalex.org/W3132736064",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3014718583",
        "https://openalex.org/W3112103703",
        "https://openalex.org/W3086249591",
        "https://openalex.org/W2982180741",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W3035390927"
    ],
    "abstract": "Generative Pre-trained Transformers (GPTs) have recently been scaled to unprecedented sizes in the history of machine learning. These models, solely trained on the language modeling objective, have been shown to exhibit outstanding few-shot learning capabilities in a number of different tasks. Nevertheless, aside from anecdotal experiences, little is known regarding their multilingual capabilities, given the fact that the pre-training corpus is almost entirely composed of English text. In this work, we investigate the multilingual skills of GPT-3, focusing on one language that barely appears in the pre-training corpus, Catalan, which makes the results especially meaningful; we assume that our results may be relevant for other languages as well. We find that the model shows an outstanding performance, particularly in generative tasks, with predictable limitations mostly in language understanding tasks but still with remarkable results given the zero-shot scenario. We investigate its potential and limits in extractive question-answering and natural language generation, as well as the effect of scale in terms of model size.",
    "full_text": "On the Multilingual Capabilities of Very Large-Scale English Language\nModels\nJordi Armengol-Estapé, Ona de Gibert Bonet, and Maite Melero\nText Mining Unit\nBarcelona Supercomputing Center\n{jordi.armengol,ona.degibert,maite.melero}@bsc.es\nAbstract\nGenerative Pre-trained Transformers (GPTs)\nhave recently been scaled to unprecedented\nsizes in the history of machine learning. These\nmodels, solely trained on the language mod-\neling objective, have been shown to exhibit\noutstanding few-shot learning capabilities in a\nnumber of different tasks. Nevertheless, aside\nfrom anecdotal experiences, little is known re-\ngarding their multilingual capabilities, given\nthe fact that the pre-training corpus is almost\nentirely composed of English text. In this\nwork, we investigate the multilingual skills of\nGPT-3, focusing on one language that barely\nappears in the pre-training corpus, Catalan,\nwhich makes the results especially meaning-\nful; we assume that our results may be relevant\nfor other languages as well. We ﬁnd that the\nmodel shows an outstanding performance, par-\nticularly in generative tasks, with predictable\nlimitations mostly in language understanding\ntasks but still with remarkable results given the\nzero-shot scenario. We investigate its poten-\ntial and limits in extractive question-answering\nand natural language generation, as well as the\neffect of scale in terms of model size.\n1 Introduction\nImproving Natural Language Understanding\n(NLU) and Generation (NLG) by pre-training au-\ntoregressive language models based on the Trans-\nformer (Vaswani et al., 2017) decoder architec-\nture has been commonplace since the original GPT\n(Generative Pretrained Transformer) (Radford and\nNarasimhan, 2018) ﬁrst appeared. In the race to\nscale up these language models (Radford et al.,\n2019), the arrival of GPT-3 (Brown et al., 2020)\nhas changed the rules of the game. As claimed by\ntheir creators, its ability to learn from a few exam-\nples \"via text interaction\" makes it stand out from\nthe rest. Its impressive generative capabilities have\ncaused a big sensation, not only at research level\nbut also in the mainstream media.\nA particular feature of GPT-3 is, besides the\nsheer size of the data it has been trained on, the\nfact that, although the data is generally of good\nquality, it has not been ﬁltered for language (in\npurpose). Therefore, although GPT-3 is in prin-\nciple a language model for English, its training\ndata contains many other languages,1 even if they\naccount for a small portion of the dataset in com-\nparison to English (93% by word count). Intu-\nitively, one would expect that this quantity would\nnot be enough to obtain a high-quality language\nmodel in these other languages, especially in the\nlow-resource ones. Some evidence in this regard\nis provided by the large amount of data required\nto train language-speciﬁc models (Nozza et al.,\n2020). Even the multilingual ones2 such as mBERT\n(Devlin et al., 2018) or XLM-R (Conneau et al.,\n2019) employ large multilingual datasets based\non Wikipedia or CommonCrawl. A very recent\nwork trained a language-speciﬁc Catalan model\nwith around 1.7B tokens (Armengol-Estapé et al.,\n2021), but it was published after the elaboration of\nthis article and thus is not included in our compar-\nisons. The code for reproducing the GPT-3 API\nqueries and the results we obtained is openly avail-\nable.3\n2 Related Work\nIn Brown et al. (2020), the authors of GPT-3 al-\nready conducted a thorough evaluation in many dif-\nferent benchmarks, including question-answering,\ncloze tasks, and Natural Language Inference (NLI),\namong many others. Crucially, they train and eval-\nuate models of different sizes, and ﬁnd that by\nsimply scaling up the exact same architecture, the\ndiminishing returns that one would expect are not\n1https://github.com/openai/gpt-3/tree/\nmaster/dataset_statistics\n2Note that both mBERT and XLM-R are encoder-based\nmodels, unlike GPT, but the point still holds.\n3https://github.com/TeMU-BSC/\ngpt3-queries\narXiv:2108.13349v1  [cs.CL]  30 Aug 2021\nobserved. Recently, some works have estimated the\nincrease in performance of autoregressive models\nin terms of model size, data, and compute (Kaplan\net al., 2020; Henighan et al., 2020). Also in Brown\net al. (2020), and relevant to our work, authors eval-\nuate GPT-3 in machine translation, both in zero and\nfew-shot settings, and ﬁnd that in the latter, GPT-3\noutperforms previous unsupervised NMT models\nby 5 BLEU in some pairs. Speciﬁcally, this success\nis observed in the evaluated pairs in which English\nis the target language and not in the ones in which\nEnglish is the source one, being GPT-3 an English\nlanguage model. No other analysis involving lan-\nguages other than English was conducted.\nSince the original article of GPT-3, several works\nhave investigated the capabilities and limits of the\nmodel in English (Zhao et al., 2021). Moreover,\nwith the possibility of querying the model via API,\nhundreds of researchers, journalists and curious\nalike have embarked on all sorts of experiments,\nincluding automatic programming or solving arith-\nmetic operations (Floridi and Chiriatti, 2020). The\nInternet is full of examples of the amazing genera-\ntive capabilities of the model, from poetry, news or\nessay writing (Elkins and Chun, 2020).\nFurthermore, many researchers are interested in\nthe ethical concerns regarding such a capable gen-\nerative model and studying the impact it may had if\nit was released to the public (Dale, 2021; McGufﬁe\nand Newhouse, 2020). In a more consequential\napproach, with the purpose of harnessing the full\nlearning potential of GPT, we are seeing the emer-\ngence of a new line of research exploring optimal\nways to \"prompt\" the model (Liu et al., 2021).\nNevertheless, to our knowledge, no work has\nstudied its potential for solving tasks in languages\nother than English, aside from machine translation.\nIn this work, we investigate the multilingual skills\nof GPT-3, focusing on Catalan, a language barely\nappearing in the pre-training corpus.\n3 Methodology\nIn this work we have explored how good GPT-3\nis at generating natural text in Catalan and solving\none NLU task, speciﬁcally extractive Q&A. Cata-\nlan only accounts for the 0,01798% of words in the\ntraining corpus, that is around 35M words. Lan-\nguage models, even if in a considerably smaller\nscale than GPT-3, are usually trained on corpora\nwith a number of tokens in the billions as can be\nseen in Table 1. Even considering the effect of\nModel Words (M) Catalan words (M)\nmBERT Unclear4 ~200\nXLM-R 295,0085 1,752\nGPT-3 196,7556 35\nTable 1: Pre-training word count in some models\ncertain factors particular to each language, such as\nlinguistic proximity to English (e.g. being an Indo\nEuropean language), afﬁliation to well-populated\nfamilies (e.g. Romance), number of tokens in the\ntraining corpus, etc. we can assume that our results\nmay be relevant for other languages as well.\n3.1 Question-answering\nTo evaluate GPT-3 in question-answering, we use\na Catalan translation (introduced in Armengol-\nEstapé et al. (2021), Rodriguez-Penagos and\nArmentano-Oller (2021b)) of XQuAD (Artetxe\net al., 2019), a cross-lingual question-answering\ndataset consisting of 240 paragraphs and 1,060\nquestion-answer pairs. We focus on the zero-shot\nsetting, in which the model is not given any exam-\nple. GPT-3 is asked to answer one question at a\ntime, pieced with its context as prompts as shown\nbelow (in bold, GPT-3’s answer):\nAixò és un sistema de resposta de pre-\nguntes en català.\nContext: La defensa dels Panthers va\ncedir només 308 punts [...]\nPregunta: Quants punts va cedir la de-\nfensa dels Panthers?\nResposta: 308 punts\nThe whole prompt, including the instruction to\nanswer the question (the ﬁrst sentence), the con-\ntext, the question ( Pregunta), and the ﬁnal word\n(Resposta, \"Answer\") are given in Catalan, with the\nhope that this will further condition the model to an-\nswer in Catalan. To study the effect of scale, we run\nthe model with the 4 engines provided in OpenAI’s\n3mBERT was trained with the top 100 largest Wikipedias,\nbut there are no details on the exact amount of tokens. For\nCatalan, we estimate the size in 200M tokens from a dump\nfrom January 2020.\n4Summing up tokens from all languages from Table 6 in\nConneau et al. (2019).\n5In the dataset statistics in Github, OpenAI claims that\nEnglish, with around 181B tokens, accounts for about 93% of\nthe dataset. This implies a total size of around 197B tokens,\nthe one we use in the table. However, in the article authors say\nthe model was trained with a total of 300B tokens. We have\nnot been able to clarify this apparent inconsistency.\nAPI,7 in increasing size 8 (in parameters): Ada,\nBabbage, Curie, and Davinci, using the default\nsampling parameters9 except for max_tokens,\nwhich we set to 64 to allow the longest answers.\nAs a reference, we include the results of\nwhat should be considered state-of-the-art, the\nones obtained by ﬁne-tuning mBERT and XLM-\nRoBERTa (base size for both models) in a Catalan\nquestion-answering dataset (Rodriguez-Penagos\nand Armentano-Oller, 2021a) using the script from\nthe Huggingface library (Wolf et al., 2019) used for\nﬁne-tuning on the SQuAD dataset. For all models\n(including GPT-3), we apply the same evaluation\nscript as in SQuAD.10\n3.2 Natural Language Generation\nIn order to evaluate the generative capabilities of\nGPT-3 in Catalan, we want to assess how “natural”\nthe generated text is to Catalan natives. For this, we\ncreate a synthetic set of 60 sentences and mix them\nrandomly with 60 control sentences coming from a\nnews corpus,11 and ask our evaluators to score each\nsentence based on their overall ﬂuency and correct-\nness. To obtain the synthetic sentences, we ﬁrst\nquery GPT-3 with a set of 20 headlines extracted\nfrom the same news corpus, and then sample 60\nsentences from the generated output. For this eval-\nuation we only use the output of the largest version\nof GPT-3 (i.e. Davinci). We manually checked that\nthe sentences did not appear in the Internet, 12 to\navoid sentences that could have been directly mem-\norized in training. As in question-answering, we\nused the default sampling parameters of OpenAI’s\nAPI, this time, setting max_tokens to 1024, for\ngenerating more sentences to sample from. For the\nhuman evaluation, similarly to (Casas et al., 2020),\nsentences were evaluated by a pool of 9 annota-\ntors, who were requested to rate the sentence in\nan integer scale from 1 to 5. Each sentence, ran-\n7https://beta.openai.com/\n8To the best of our knowledge, OpenAI has not clariﬁed\nthe exact size of each of the models in the API. However,\nsome evaluations results seem to suggest that Ada, Babbage,\nCurie and Davinci would correspond to 350M, 1.3B, 6.7B,\nand 175B, respectively. See: https://blog.eleuther.\nai/gpt3-model-sizes/.\n9A temperature of 0.7, a frequency penalty of 0, a presence\npenalty of 0, and with top_p = 1.\n10https://github.com/allenai/\nbi-att-flow/blob/master/squad/\nevaluate-v1.1.py\n112021 crawling from https://www.acn.cat/ in\nCatalan\n12By searching them on Google. None of the sentences\nappeared verbatim although we removed a similar one.\nModel F1 EM\nGPT-3: Ada 5.26 0.38\nGPT-3: Babbage 10.08 1.13\nGPT-3: Curie 16.66 5.00\nGPT-3: Davinci 38.43 17.74\nXLM-RoBERTa 67.10 46.42\nmBERT 67.15 46.51\nTable 2: Question answering results for XQuAD-ca\nFigure 1: Question-answering results for GPT-3 sizes\ndomly distributed among the pool of evaluators,\nwas scored by 3 different evaluators; this redun-\ndancy accounts for the variance and subjectivity in\nhuman scores.\n4 Results\nQuestion-answering The results obtained by\nGPT-3 in this task are reported in table 2, show-\ning the F1 score and the Exact Match value for\nXQuAD-ca, for the different GPT-3 model sizes.\nWe also include the results of two supervised, ﬁne-\ntuned models considered state-of-the art as a refer-\nence. Note that this is not a direct comparison,\nsince for GPT-3 it is a zero-shot setting. GPT-\n3 Davinci obtains a F1 score that is more than\n50% the punctuation obtained by the SOTA mod-\nels, which is remarkable being a pure zero-shot\nsetting. Figure 1 shows the scaling curves of the\ndifferent model-sizes of GPT-3.\nNatural Language Generation Table 3 shows\nthe results of the human evaluation. The sentences\ngenerated by GPT-3 obtain an average score of 3,89,\ncompared to 4,49 of the control.13 As can be seen\n13The difference is statistically signiﬁcant. With a t-test, we\nobtain a p-value of 0.00026 < 0.001.\nSource Average\nRating St. Dev. % >\nHuman Av.\nHuman 4.49 0.57 53.33\nGPT-3 3.83 1.05 33.33\nTable 3: Human evaluation (for GPT-3, Davinci)\nFigure 2: Distribution of Human Evaluation ratings\nby the difference between the standard deviations\nand the distribution of scores in Figure 2, GPT-3 is\nless consistent than the control in quality, however\nmost of the sentences are rated between 4 and 5 by\nthe evaluators. In fact, a third of the sentences is\nabove the average of the control, versus half of the\nones generated by humans.\n5 Discussion\nQualitative analysis A closer inspection of the\nresults shows some surprising abilities of GPT-3\nin addition to the naturalness of most of the sen-\ntences. An interesting example is that following\nthe prompt of a headline about Valencia, GPT-3 is\nable to write using the Valencian variant of Catalan,\nwhich is truly remarkable. An analysis of the errors\nshows that those with score of 2 or less (13% of the\nsample) contain gibberish fragments, often mixing\nCatalan and English, and in fact no control sen-\ntence has received such low scores. On the other\nhand, sentences with score 3 (21,6%) are mostly\nsyntactically impeccable but with some peculiari-\nties in the meaning, as for example: \"La IV Mostra\nde Patrimoni Cultural de Bétera ha comptat amb\nuna participació de 15.000 persones, que han pogut\ngaudir d’un espai on diversos grups han mostratels\nseus valors patrimonials.\"\nScaling As shown in Figure 1, there is a steep\ncurve of F1 score in terms of model size, while\npre-training data (and, thus, the amount of Catalan)\nremains the same. This shows that transfer learning\nbetween English and the other languages in zero-\nshot settings scales with model size in a very steep\ncurve. This is coherent with Figure H.11 in Brown\net al. (2020), where zero-shot translation in which\nEnglish is the target language reaches a plateau, but\nwhen the target languages are languages other than\nEnglish, the curves keep climbing.\nUsability in practice We believe the model can\nbe useful in multilingual applications (at least, in\na degree not far from the one for English), espe-\ncially since we used the model in zero-shot set-\ntings and without any effort in prompt design. We\nexpect the model to perform considerably better\nin few-shot settings, and even better in languages\nwith more data in GPT-3’s corpus. Nevertheless, a\ncaveat, at least for Catalan, is that smaller versions\nof GPT-3 aren’t usable, and because the vocabu-\nlary was trained fundamentally on English, Catalan\nsentences are tokenized into considerably long se-\nquences, which makes them expensive to compute.\nLimitations of our studyWe have restricted our\nanalysis to the case of Catalan, and to two speciﬁc\ntasks, even if we believe them to be relevant, and\nreasonably representative of the NLP scenario. We\nhave constrained the analysis to the zero-shot set-\nting, which we believe to be the most interesting\none. For the human evaluation, we have tried to\nmake it as balanced as possible by using a redun-\ndancy of 3 evaluators, but human ratings can be\nbiased. Regarding the relevance to other languages,\nas already mentioned, Catalan probably beneﬁts\nfrom linguistic similarities with Romance and Indo\nEuropean languages at large (including English).\n6 Conclusions and Future Work\nWe have seen that GPT-3 does, indeed, exhibit re-\nmarkable zero-shot NLU and NLG capabilities in\nCatalan. This is surprising in view of the tiny pro-\nportion of Catalan in the training corpus. Our re-\nsults show that GPT-3 can be useful not only for\nEnglish but for many other languages present in\nthe corpus as well. Nevertheless, some practical\nconcerns (the needed model scale and sub opti-\nmal tokenization) make it less computationally ef-\nﬁcient than for English. On the overall, this is a\nvery interesting exercise of how linguistic struc-\ntures (universals) transfer across languages. Given\nthe large amount of tasks GPT-3 has been implicitly\nexposed to during the training procedure, handling\na different language can be considered as work-\ning on yet another domain. As future work, we\nsuggest extending the study of the scaling laws of\nlanguage models (Kaplan et al., 2020) in terms of\ncross-lingual transfer, similarly to Hernandez et al.\n(2021).\nReferences\nJordi Armengol-Estapé, Casimiro Pio Carrino, Carlos\nRodriguez-Penagos, Ona de Gibert Bonet, Carme\nArmentano-Oller, Aitor Gonzalez-Agirre, Maite\nMelero, and Marta Villegas. 2021. Are multilin-\ngual models the best choice for moderately under-\nresourced languages? A comprehensive assessment\nfor Catalan. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages\n4933–4946, Online. Association for Computational\nLinguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. 2019. On the cross-lingual transferabil-\nity of monolingual representations. arXiv preprint\narXiv:1910.11856.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. CoRR, abs/2005.14165.\nNoe Casas, José AR Fonollosa, and Marta R Costa-\njussà. 2020. Syntax-driven iterative expansion lan-\nguage models for controllable text generation. arXiv\npreprint arXiv:2004.02211.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nRobert Dale. 2021. Gpt-3: What’s it good for?Natural\nLanguage Engineering, 27(1):113–118.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nKatherine Elkins and Jon Chun. 2020. Can gpt-3 pass\na writer’s turing test. Journal of Cultural Analytics,\n2371:4549.\nLuciano Floridi and Massimo Chiriatti. 2020. Gpt-3:\nIts nature, scope, limits, and consequences. Minds\nand Machines, 30(4):681–694.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B. Brown, Prafulla Dhariwal, Scott Gray, Chris\nHallacy, Benjamin Mann, Alec Radford, Aditya\nRamesh, Nick Ryder, Daniel M. Ziegler, John Schul-\nman, Dario Amodei, and Sam McCandlish. 2020.\nScaling laws for autoregressive generative modeling.\nCoRR, abs/2010.14701.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and\nSam McCandlish. 2021. Scaling laws for transfer.\nCoRR, abs/2102.01293.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt-3? CoRR,\nabs/2101.06804.\nKris McGufﬁe and Alex Newhouse. 2020. The radical-\nization risks of gpt-3 and advanced neural language\nmodels. arXiv preprint arXiv:2009.06807.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020.\nWhat the [mask]? making sense of language-speciﬁc\nBERT models. CoRR, abs/2003.02912.\nA. Radford and Karthik Narasimhan. 2018. Improving\nlanguage understanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nCarlos Gerardo Rodriguez-Penagos and Carme\nArmentano-Oller. 2021a. ViquiQuAD: an extractive\nQA dataset from Catalan Wikipedia.\nCarlos Gerardo Rodriguez-Penagos and Carme\nArmentano-Oller. 2021b. Xquad-ca.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\narXiv preprint arXiv:2102.09690."
}