{
  "title": "Bootstrapping BI-RADS classification using large language models and transformers in breast magnetic resonance imaging reports",
  "url": "https://openalex.org/W4409148764",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2095666381",
      "name": "Yu-Xin Liu",
      "affiliations": [
        "University of Science and Technology of China",
        "Suzhou Institute of Biomedical Engineering and Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2100623889",
      "name": "Xiang Zhang",
      "affiliations": [
        "Sun Yat-sen Memorial Hospital",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2132178226",
      "name": "Wei-Wei Cao",
      "affiliations": [
        "University of Science and Technology of China",
        "Chinese Academy of Sciences",
        "Suzhou Institute of Biomedical Engineering and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2150965028",
      "name": "Wenju Cui",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Science and Technology of China",
        "JW Medical Systems (China)",
        "Suzhou Institute of Biomedical Engineering and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099896429",
      "name": "Tao Tan",
      "affiliations": [
        "Macao Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2499800121",
      "name": "Yuqin Peng",
      "affiliations": [
        "Sun Yat-sen University",
        "Sun Yat-sen Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098512778",
      "name": "Jiayi Huang",
      "affiliations": [
        "Sun Yat-sen University",
        "Sun Yat-sen Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2114710752",
      "name": "Zhen Lei",
      "affiliations": [
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1988806642",
      "name": "Jun Shen",
      "affiliations": [
        "Sun Yat-sen Memorial Hospital",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2102526341",
      "name": "Jian Zheng",
      "affiliations": [
        "University of Science and Technology of China",
        "Chinese Academy of Sciences",
        "JW Medical Systems (China)",
        "Suzhou Institute of Biomedical Engineering and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095666381",
      "name": "Yu-Xin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100623889",
      "name": "Xiang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132178226",
      "name": "Wei-Wei Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150965028",
      "name": "Wenju Cui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099896429",
      "name": "Tao Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2499800121",
      "name": "Yuqin Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098512778",
      "name": "Jiayi Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114710752",
      "name": "Zhen Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988806642",
      "name": "Jun Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102526341",
      "name": "Jian Zheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4383822665",
    "https://openalex.org/W3193581212",
    "https://openalex.org/W4292722411",
    "https://openalex.org/W2056499382",
    "https://openalex.org/W4311282448",
    "https://openalex.org/W4206967425",
    "https://openalex.org/W3121120936",
    "https://openalex.org/W4281725311",
    "https://openalex.org/W4390097263",
    "https://openalex.org/W2809857565",
    "https://openalex.org/W2901643192",
    "https://openalex.org/W3135534730",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4282983782",
    "https://openalex.org/W3022710784",
    "https://openalex.org/W4403601500",
    "https://openalex.org/W3206086363",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4399854538",
    "https://openalex.org/W4391126287",
    "https://openalex.org/W6605502078",
    "https://openalex.org/W4387559805",
    "https://openalex.org/W4386836900",
    "https://openalex.org/W3205656744",
    "https://openalex.org/W4211187366",
    "https://openalex.org/W4362522726",
    "https://openalex.org/W4399781895",
    "https://openalex.org/W2339073577",
    "https://openalex.org/W4387225871",
    "https://openalex.org/W4386322180",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4392618992",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W2740721704",
    "https://openalex.org/W6929135807",
    "https://openalex.org/W2952370363",
    "https://openalex.org/W3160843001",
    "https://openalex.org/W2062069575",
    "https://openalex.org/W2237958539",
    "https://openalex.org/W4396913589",
    "https://openalex.org/W4404367676",
    "https://openalex.org/W4404988471",
    "https://openalex.org/W4386780386",
    "https://openalex.org/W2777280533",
    "https://openalex.org/W2794680924",
    "https://openalex.org/W3019533076",
    "https://openalex.org/W4403618367"
  ],
  "abstract": "Abstract Breast cancer is one of the most common malignancies among women globally. Magnetic resonance imaging (MRI), as the final non-invasive diagnostic tool before biopsy, provides detailed free-text reports that support clinical decision-making. Therefore, the effective utilization of the information in MRI reports to make reliable decisions is crucial for patient care. This study proposes a novel method for BI-RADS classification using breast MRI reports. Large language models are employed to transform free-text reports into structured reports. Specifically, missing category information (MCI) that is absent in the free-text reports is supplemented by assigning default values to the missing categories in the structured reports. To ensure data privacy, a locally deployed Qwen-Chat model is employed. Furthermore, to enhance the domain-specific adaptability, a knowledge-driven prompt is designed. The Qwen-7B-Chat model is fine-tuned specifically for structuring breast MRI reports. To prevent information loss and enable comprehensive learning of all report details, a fusion strategy is introduced, combining free-text and structured reports to train the classification model. Experimental results show that the proposed BI-RADS classification method outperforms existing report classification methods across multiple evaluation metrics. Furthermore, an external test set from a different hospital is used to validate the robustness of the proposed approach. The proposed structured method surpasses GPT-4o in terms of performance. Ablation experiments confirm that the knowledge-driven prompt, MCI, and the fusion strategy are crucial to the model’s performance.",
  "full_text": "Liu et al. \nVisual Computing for Industry, Biomedicine, and Art             (2025) 8:8  \nhttps://doi.org/10.1186/s42492-025-00189-8\nORIGINAL ARTICLE Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nVisual Computing for Industry,\nBiomedicine, and Art\nBootstrapping BI-RADS classification using \nlarge language models and transformers \nin breast magnetic resonance imaging reports\nYuxin Liu1,2†, Xiang Zhang3,4†, Weiwei Cao1,2, Wenju Cui1,2,5, Tao Tan6, Yuqin Peng3,4, Jiayi Huang3,4, Zhen Lei7, \nJun Shen3,4* and Jian Zheng1,2,5*   \nAbstract \nBreast cancer is one of the most common malignancies among women globally. Magnetic resonance imaging \n(MRI), as the final non-invasive diagnostic tool before biopsy, provides detailed free-text reports that support clini-\ncal decision-making. Therefore, the effective utilization of the information in MRI reports to make reliable decisions \nis crucial for patient care. This study proposes a novel method for BI-RADS classification using breast MRI reports. Large \nlanguage models are employed to transform free-text reports into structured reports. Specifically, missing category \ninformation (MCI) that is absent in the free-text reports is supplemented by assigning default values to the missing \ncategories in the structured reports. To ensure data privacy, a locally deployed Qwen-Chat model is employed. Fur-\nthermore, to enhance the domain-specific adaptability, a knowledge-driven prompt is designed. The Qwen-7B-Chat \nmodel is fine-tuned specifically for structuring breast MRI reports. To prevent information loss and enable comprehen-\nsive learning of all report details, a fusion strategy is introduced, combining free-text and structured reports to train \nthe classification model. Experimental results show that the proposed BI-RADS classification method outperforms \nexisting report classification methods across multiple evaluation metrics. Furthermore, an external test set from a dif-\nferent hospital is used to validate the robustness of the proposed approach. The proposed structured method \nsurpasses GPT-4o in terms of performance. Ablation experiments confirm that the knowledge-driven prompt, MCI, \nand the fusion strategy are crucial to the model’s performance.\nKeywords Large language model, Structured report, Missing category information, Radiology report\n†Yuxin Liu and Xiang Zhang contributed equally to this work.\n*Correspondence:\nJun Shen\nshenjun@mail.sysu.edu.cn\nJian Zheng\nzhengj@sibet.ac.cn\n1 School of Biomedical Engineering (Suzhou), University of Science \nand Technology of China, Division of Life Sciences and Medicine, \nHefei 230026, Anhui, China\n2 Medical Imaging Department, Suzhou Institute of Biomedical \nEngineering and Technology, Chinese Academy of Sciences, \nSuzhou 215163, Jiangsu, China\n3 Department of Radiology, Sun Yat-Sen Memorial Hospital, Sun Yat-Sen \nUniversity, Guangzhou 510120, Guangdong, China\n4 Guangdong Provincial Key Laboratory of Malignant Tumor Epigenetics \nand Gene Regulation, Medical Research Center, Sun Yat-Sen Memorial \nHospital, Sun Yat-Sen University, Guangzhou 510120, Guangdong, China\n5 Shandong Laboratory of Advanced Biomaterials and Medical Devices \nin Weihai, Shandong University, Weihai  264200, Shandong, China\n6 Faculty of Applied Sciences, Macao Polytechnic University, Macao, China\n7 Institute of Automation, Chinese Academy of Sciences, Beijing 100190, \nChina\nPage 2 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nIntroduction\nBreast cancer is one of the most prevalent malignant \ntumors in women worldwide and imposes a significant \nhealth burden [1]. In the diagnostic pathway, magnetic \nresonance imaging (MRI) represents the final non-\ninvasive diagnostic method before considering a biopsy, \nwhich may present risks such as bleeding and compli -\ncations [2, 3]. Computer-aided decision support assists \nless-experienced specialists while reducing unnecessary \nbiopsies and minimizing the pathologists’ workload [4–\n7]. Considering their comprehensive medical information \ncontent, breast MRI reports play a crucial role in clini -\ncal decision-making. Consequently, developing effective \nmethods to extract and learn key features from these \nreports shows significant potential to improve the accu -\nracy of decision-making in breast BI-RADS classification, \nparticularly in differentiating between malignant (sugges-\ntion for biopsy) and benign (suggestion for follow-up).\nAdvancement of radiology report classification \nthrough natural language processing (NLP) approaches \nhas become increasingly important [8, 9]. Traditional \nmachine learning methods [10], such as the support vec -\ntor machine (SVM), k-nearest neighbor (KNN), Naive \nBayes (NB), and maximum entropy classifier, although \nwidely used in report classification, face challenges in fea-\nture extraction, particularly when dealing with the high-\ndimensional and sparse nature of text representations. \nThese limitations impede the accurate capture of intri -\ncate inter-feature relationships. In contrast, deep learning \nmethods enable direct extraction of high-level features \nfrom data. Convolutional neural network (CNN), recur -\nrent neural network (RNN), and bidirectional long short-\nterm memory network have achieved significant success \nin classifying radiology reports [11, 12]. However, these \nmodels may encounter difficulties in handling long-dis -\ntance dependencies and capturing global semantic infor -\nmation. To address these limitations, the bidirectional \nencoder representations from transformers (BERT) \n[13] model has emerged as a breakthrough technology, \ndemonstrating remarkable success in clinical text clas -\nsification through variants such as ClinicalBERT [14], \nBioBERT [15], and RadBERT [16]. However, the effec -\ntiveness of these models depends heavily on high-quality \n[17–19] and large-scale domain-specific corpora, and \nlimitations in data quality and evaluation methods can \nsignificantly compromise model effectiveness. Recently, \nlarge language models (LLMs) have demonstrated revo -\nlutionary potential in the medical field, particularly in \ndiagnostic assistance, personalized treatment planning, \nclinical decision support, and risk prediction [20]. For \nmedical text classification tasks, researchers have exten -\nsively explored the application of advanced models such \nas ChatGPT and GPT-4 in zero-, one-, and few-shot \nlearning scenarios [21–23]. These models demonstrate \nrapid adaptation to new tasks with limited data, substan -\ntially reducing dependence on manual annotation. How -\never, general-purpose LLMs face challenges because of \ntheir domain-specific accuracy. Their black-box nature \nmakes identifying parts of the data that are crucial for \nclassification tasks challenging, potentially limiting their \nreliable application in clinical settings.\nInformation extraction encompasses the process of \nidentifying entities, relationships, and events in unstruc -\ntured text [24]. This process organizes various data attrib-\nutes, providing a foundation for recognizing and utilizing \nkey information in radiology report classification. How -\never, variations in radiologists’ writing styles and educa -\ntional backgrounds result in inconsistencies in structured \ndata attributes, which can cause patient confusion and \nimpede effective physician communication [25].\nTo extract information from radiology reports, \nresearchers have explored various approaches. Although \nrule-based NLP methods have shown effectiveness in \ncertain scenarios, they remain language-dependent \nwith limited generalizability [26]. The adoption of deep-\nlearning techniques has led to significant performance \nimprovements [27, 28]. However, these techniques \nrequire substantial amounts of manually annotated data. \nLLMs offer a promising solution for automatic infor -\nmation extraction, leveraging their advanced seman -\ntic understanding. Studies have demonstrated that the \nGPT-4 model successfully converts free-text reports \ninto structured reports [29, 30]. However, the use of the \nGPT-4 model requires rigorous privacy measures to safe-\nguard sensitive medical data. Furthermore, the preva -\nlence of medical terminology in radiology reports poses \nsignificant challenges for general LLMs when performing \ninformation extraction tasks in this domain.\nTo address these challenges, a novel computer-aided \nBI-RADS classification method based on breast MRI \nreports is proposed, designed to assist less experienced \nspecialists in accurately assessing the severity of breast \nlesions. The proposed approach converts free-text \nreports into structured reports and enhances their com -\npleteness by supplementing missing category information \n(MCI) with default values. By providing richer contextual \ninformation for model training, this approach improves \nthe model’s ability to differentiate between the nature and \nseverity of lesions. To ensure data privacy and strengthen \nthe domain-specific applicability of the model, Qwen-\n14B-Chat was deployed locally, and a knowledge-driven \nprompt was developed, incorporating the fifth edition of \nthe MRI imaging lexicon [31]. Subsequently the Qwen-\n7B-Chat model was fine-tuned to optimize its perfor -\nmance in structuring breast MRI reports. To mitigate \npotential information gaps during the structuring process \nPage 3 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nof LLMs, a fusion strategy was designed that combines \nfree-text and structured reports for joint training, \nthereby optimizing the model’s performance.\nThe main contributions of this study are as follows. \n(1) Development of privacy-preserving LLMs for Chi -\nnese breast MRI report structuring through knowl -\nedge-driven prompt and domain-specific model \nfine-tuning.\n(2) Enhancement of the learning capabilities of the \nmodel by incorporating MCI from free-text reports \ninto structured reports.\n(3) Introduction of an innovative fusion strategy that \nsynthesizes free-text and structured reports for \ncomprehensive information processing.\nMethods\nThis section presents a novel computer-aided BI-RADS \nclassification method based on breast MRI reports. The \nmethodology comprised two main stages: first, the reports \nwere structured using LLMs, with MCI integration. Sec -\nond, to mitigate potential information gaps during the \nstructuring process, a fusion framework was developed to \ntrain the classification model, as illustrated in Fig. 1.\nBreast MRI report structuring\nTo ensure patient information privacy, this study utilized \nthe locally deployed first version of the Qwen-Chat model \n[32], released by Alibaba in 2023 for the inference and fine-\ntuning experiments. This model demonstrated exceptional \nperformance in terms of text comprehension and informa-\ntion extraction.\nFig. 1 Main architecture of the proposed method. Examples of the report shown in this figure are the English translations of the original Chinese \nreports\nPage 4 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nKnowledge‑driven instruction tuning\nAccording to research by Heston and Khun [33], generative \nlanguage models (GLMs) possess the capability for per -\nsonalized learning and timely feedback. Within the medi-\ncal domain, effective utilization of GLMs requires carefully \nconstructed task-specific prompts to generate accu -\nrate inferences. This study designed a knowledge-driven \nprompt that integrates the fifth edition of the MRI imaging \nlexicon [31] to enhance the model’s comprehension, learn-\ning, and reasoning abilities. Figure 2a illustrates the knowl-\nedge-driven prompt designed in this study, which consists \nof three main parts: system description, instruction, and \ninput. The system description defines the model’s identity \nand behavior. The instruction provides guidance for struc-\ntured information extraction, including a task descrip -\ntion, a structured report template with the MRI imaging \nlexicon, and five example reports with expected responses. \nThe input section contains the “radiological description” \ncontent of the breast MRI reports. The response section \nconsists of structured reports generated by the model. Fig-\nure 2b highlights the key distinction between knowledge-\ndriven and default prompts, which lies in the incorporation \nof the MRI imaging lexicon within the structured report \ntemplate.\nLow‑rank adaptation\nFull-parameter fine-tuning presents challenges for cur -\nrently popular LLMs. Low-rank adaptation (LoRA) [34] \nfine-tuning method addresses modifications to the origi -\nnal weight matrix within the self-attention module. It \nemploys low-rank decomposition optimization during the \nweight update process for downstream tasks. As illustrated \nin Fig.  3, during implementation, the optimized low-rank \ndecomposition matrix is combined with the self-atten -\ntion weight matrix to adjust the weights [35]. For the pre-\ntrained weights W 0 ∈ Rd×k of the original language model, \nthe weight update can be expressed as the following addi-\ntion of the original weights and low-rank updates:\nHere, A and B are the matrices of the low-rank \ndecomposition with B ∈ Rd×r and A ∈ Rr×k , where \nrank r ≪ min (d ,k) . During training, W0 remains \n(1)W 0 + /Delta1W= W 0 + BA\nfrozen, whereas A and B contain trainable parameters. \nFor h= W 0x , the formula for forward propagation is as \nfollows:\nMatrix A is initialized with random Gaussian values, \nwhereas B was initialized with zeros. At the beginning of \ntraining, the initialization of /Delta1W= BA is zero.\nMCI\nThis study employed the Qwen-Chat model to convert \nfree-text reports into structured reports. As shown in \nFig. 4, the model extracts information from the free-text \nreport following predefined templates and categorizes \nit within the corresponding attributes of the structured \nreport. The model incorporates MCI to address features \nthat are absent in the original free-text reports. Follow -\ning established practices in medical text analysis [30, 36], \nthese missing categories are automatically assigned “not \nmentioned” as the default value, ensuring consistent han-\ndling of undocumented features.\nIntegration models\nThis study proposes a novel fusion strategy based on a \ntransformer model engineered to embed and integrate \nfeatures from both structured and free-text reports. This \napproach ensures comprehensive information capture \nduring training. The framework implements a two-stage \nprocess: first, both report types undergo embedding \nencoding and then encoded by the transformer model for \nfeature extraction. Subsequently, the extracted features \nundergo concatenation and pooling, followed by trans -\nformation through a fully connected layer and a softmax \nfunction, ultimately producing a prediction correspond -\ning to the sample category.\nA transformer model contains a sequence of layers, \neach containing a multi-head attention mechanism and \na feed-forward neural network (FFN) [37] with residual \nconnections and layer normalization. In the multihead \nattention mechanism, the attention function maps a \nquery and a set of key-value pairs to an output. The input \nto the attention function consists of query Q, key K, and \nvalue V, and is computed as follows:\n(2)h= W 0x+ /Delta1Wx= W 0x+ BAx\nFig. 2 Overview of knowledge-driven prompts. a A knowledge-driven prompt consists of three components: system description, instruction, \nand input, collectively forming a complete prompt. The “Expert instruction” and “Input data” on the right side of the figure are inserted into {Expert \nInstruction} and {Input Data} on the left side, respectively. The generated result appears in {Output Impression}; b Illustrates the differences \nbetween knowledge-driven and default prompts for structuring breast MRI reports, where the knowledge-driven prompts provide explicit \ndefinitions for each structured category. The report examples shown in this figure are English translations of the original Chinese reports. The \nprompts are displayed in truncated form\n(See figure on next page.)\nPage 5 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nFig. 2 (See legend on previous page.)\nPage 6 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nHere, Q, K, and V represent the query, key, and value, \nrespectively, and dk represents the key dimensions. The \nsoftmax function calculates the weighted sum of the val -\nues using the weights determined by the compatibility \nfunction between the query and its corresponding key. \n(3)Attention(Q , K , V ) = softmax\n(\nQK ⊤\n√\ndk\n)\nV\nThe multi-head attention mechanism projects the query, \nkey, and value into multiple subspaces using learned lin -\near projections as follows:\nHere,\nX j ∈ Rn∗d represents the input representation of \nsequence j, and W Q\ni  , W K\ni  , W V\ni  , W O\ni  are the projection \nparameter matrices with dimensions Rd∗dk , Rd∗dk , Rd∗dv \nand Rh∗dv∗d , respectively. In addition to the multihead \nattention layer, each layer of the model includes an FFN, \ndefined as follows:\nwhere W1 and W2 are linear transformation matrices, and \n b1 and  b2 are the corresponding bias vectors.\nResults\nDatasets\nThis retrospective study analyzed 11,884 breast MRI \nreports, which were used as the internal dataset, in Chi -\nnese from the Sun Yat-sen Memorial Hospital (SYS -\nMHReports). Additionally, 5043 Chinese reports from the \nShantou Central Hospital (SCHReports) were included \n(4)\nMultiHead X j = Concat(head1 ,... ,headh)W O\n(5)\nhead i(Q,K ,V ) = Attention\n(\nXjW Q\ni ,XjW K\ni ,XjW V\ni\n)\n(6)FFN\n(\nXj\n)\n= max (0,XjW 1 + b1)W 2 + b2\nFig. 3 Schematic of LoRA fine-tuning\nFig. 4 This figure illustrates the process of adding “not mentioned” for missing categories in structured reports. The radiology report content shown \nis a simplified version created based on real breast MRI reports. The yellow sections indicate the “not mentioned” additions, while the red boxes \nhighlight category information absent from the free-text report. The report examples shown in this figure are English translations of the original \nChinese reports\nPage 7 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nas the external test dataset. The dataset included MRI \nreports from multiple anatomical regions, including \nthe brain, breast, thorax, lungs, heart, liver, gallbladder, \nabdominal cavity, mediastinum, lumbar spine, sacrum, \nand bladder. For this study, only the reports pertaining \nto breast and metastatic lesions were considered. Each \nreport comprised two sections: a detailed radiological \ndescription and summary of the main findings. This study \nfocused on the detailed radiological description. Expert \nradiologists with more than five years of clinical experi -\nence were invited to annotate the data. Reports were \nclassified into two categories: “Suggestion for Follow-up” , \nwhich included lesions classified as BI-RADS 1–3 (benign \nlesions not typically requiring biopsy), and “Suggestion \nfor Biopsy” , which included lesions classified as BI-RADS \n4A-6 (malignant-leaning lesions typically recommended \nfor biopsy). Details of the dataset are listed in Table 1. The \ninternal dataset was randomly split into a 70% training \nset, 20% testing set, and 10% validation set.\nAfter referencing the fifth edition of the MRI imag -\ning lexicon [31], radiologists structured the reports into \nten categories: amount of fibroglandular tissue, level of \nbackground parenchymal enhancement, mass/non-mass, \naxillary lymph nodes, skin lesions, non-enhancing find -\nings, associated findings, fat-containing lesions, dynamic \ncontrast-enhanced curve assessment, and prosthesis. The \ndetails of each category are presented in Table  8 in the \nAppendix. Approval was obtained from the local Medi -\ncal Ethics Committee to ensure ethical compliance. The \nrequirement for informed consent was waived due to the \nuse of de-identified data in this study.\nNetwork training and implementation details\nThe Qwen-14B-Chat model was initially used to auto -\nmatically extract information from free-text reports using \nknowledge-driven prompts, thereby generating struc -\ntured breast MRI reports. These outputs underwent com-\nprehensive preprocessing, including denoising and review \nby physicians. The denoising phase employs automated \nregular expression methods to remove irrelevant symbols \nand characters followed by physician reviews and cor -\nrections. The analysis results identified two main chal -\nlenges: (1) Insufficient information extraction, which was \nmost prominent in categories such as “associated find -\nings” and “dynamic contrast-enhanced curve evaluation. ” \nThis challenge stems primarily from the diverse and het -\nerogeneous content types within these categories, which \nhinder the accurate extraction of information. (2) Inac -\ncurate information extraction, particularly evident in the \n“amount of fibroglandular tissue” category. This issue \narises from the discrepancy between the clinical descrip -\ntions used in real reports and the standardized terminol -\nogy incorporated into knowledge-driven prompts. To \naddress these challenges, 10,000 screened and organized \nstructured reports were used as a dataset to fine-tune \nthe Qwen-7B-Chat model using the LoRA method. The \nselection of Qwen-7B-Chat model over Qwen-14B-Chat \nbalanced resource efficiency with performance require -\nments. This fine-tuned model subsequently processes a \nsecond round of inference, targeting previously underper-\nforming data.\nThis study utilized the Hugging Face Transformer \nlibrary and PyTorch framework [38, 39] for experimenta-\ntion. A transformer-based model pre-trained by Google \non a large-scale Chinese corpus was utilized for text \nembedding and fine-tuning to extract textual features \nfrom breast MRI reports. The model’s hidden layer had \na dimension of H = 768, with A = 12 attention heads and \nL = 12 transformer layers.\nFor LoRA fine-tuning of the Qwen-7B-Chat model and \nall classification experiments, the hardware used con -\nsisted of an NVIDIA GPU 3090 (24GB) and an Intel(R) \nXeon(R) Gold 6133 CPU @ 2.50GHz. Fine-tuning was \nconducted with initial learning rates of 3 × 10−4 and \n1 × 10−6 over 5 and 10 epochs, respectively. Prompt \ninference using the Qwen-Chat model was performed on \na system featuring an NVIDIA GPU A40 (48GB) and a \n15-vCPU AMD EPYC 7543 32-Core Processor.\nFor structuring breast MRI reports, the research strat -\negy proposed by Jeblick et al. [40] was adopted, in which \nradiologists created 50 virtual breast MRI reports and \ncorresponding structured reference standards. This test -\ning set was used to evaluate the performance of the fine-\ntuned Qwen-7B-Chat (LoRA) model against other LLMs, \nincluding GPT-3.5 [41], GPT-4o [42], and unfine-tuned \nQwen-7B-Chat, with virtual reports employed to ensure \ndata privacy. Traditional metrics primarily assess surface-\nform similarity, which limits their ability to accurately \ncapture the quality of the generated text, particularly \nin terms of lexical semantics and component diversity. \nTable 1 Details of the datasets\nClass Training set Validation set Testing set External test set Label\nTotal 8320 1188 2376 5043 -\nSuggestion for follow-up 2119 302 604 1408 0\nSuggestion for biopsy 6201 886 1772 3635 1\nPage 8 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nTherefore, this study employed the BERTScore metric \n[43], which aligns more closely with human judgment, to \nevaluate the model’s performance in extracting informa -\ntion across the ten categories. BERTScore is computed as \nfollows: for a reference sequence x = �x1 , ...,xk � and a gen-\nerated sequence ˆx =\n⟨ˆx 1 , ...,ˆx l\n⟩\n , the BERT model encodes \nboth sequences to obtain their hidden-layer represen -\ntations. In this study, a BERT-based Chinese model was \nused. The F1 score was then calculated as the harmonic \nmean of precision and recall. For a reference x and candi-\ndate ˆx , the recall, precision, and F1 scores are as follows:\nTo evaluate the effectiveness of the method, ablation \nand comparative experiments were conducted using \ndifferent classification models. Several text classifica -\ntion models were tested through comparative experi -\nments to verify the superiority of the proposed method. \nRepresentative models from traditional deep-learning \nmethods, including TextCNN [44], TextRCNN [45], and \nDPCNN [46], were selected. For the transformer mod -\nels pre-trained on large corpora, MacBERT [47], BERT-\nwwm [48], BERT-wwm-ext [48], and RoBERTa-wwm-ext \n[48], were chosen. Additionally, the performance of the \nQwen-14B-Chat model in few-shot settings (K = 9) [49], \nwas assessed. The evaluation metrics included precision, \nrecall, F1 score, and area under the curve (AUC).\nExperimental results\nResult of breast MRI report structuring\nTable 2 presents the performance evaluation of the struc-\ntured reports for extracting information from original \n(7)PBERT = 1⏐⏐ˆx\n⏐⏐\n∑\nˆxj∈ˆx\nmax\nxi∈x\nx⊤\ni ˆxj\n(8)RBERT = 1\n|x|\n∑\nxi∈x\nmax\nˆxj∈ˆx\nx⊤\ni ˆxj\n(9)FBERT = 2 PBERT · RBERT\nPBERT + RBERT\nreports. Among baseline models, GPT-4o achieved \nsuperior performance with the highest FBERT  of 0.8963. \nNotably, the LoRA-fine-tuned Qwen-7B-Chat model \ndemonstrated enhanced performance, achieving an \nFBERT  of 0.9298, representing a 3.35% improvement. \nTable 3 details FBERT  across ten categories in the struc -\ntured breast MRI reports. The fine-tuned Qwen-7B-Chat \nmodel exhibited substantial improvements in multiple \ncategories. However, for certain categories, such as “level \nof background parenchymal enhancement” , “dynamic \ncontrast-enhanced curve assessment” , and “fat-contain -\ning lesions” , the model underperformed compared to the \nGPT-4o.\nFigure 5 illustrates the inference results of each model \nfor a virtual report, with the results denoised and trans -\nlated into English. Red crosses and wavy red lines high -\nlight errors in the extraction, whereas green checks \nindicate accurate semantic information extraction. \nCompared with the online GPT models, the results \nfrom direct inference using Qwen-7B-Chat and Qwen-\n14B-Chat showed more errors. However, the fine-tuned \nQwen-7B-Chat model significantly improved the accu -\nracy of information extraction.\nResult of breast MRI report classification\nThe proposed method was evaluated using both an \ninternal test set (SYSMHReports) and an external test \nset (SCHReports). Table  4 lists the four evaluation met -\nrics for the various comparison methods. The pro -\nposed method achieved the highest precision, recall, \nF1 score, and AUC values for both datasets. Among the \ncompared methods, transformer-based models exhib -\nited the second-best overall performance. Specifically, \nthe BERT-wwm model demonstrated the second-best \nrecall, F1 score, and AUC on the SYSMHReports data -\nset and the second-best precision, recall, and F1 score \non the SCHReports dataset. The BERT-wwm-ext model \nachieved the second-best precision on the SYSMHRe -\nports dataset. As a representative of traditional deep \nlearning methods, TextCNN performed well on SYS -\nMHReports, whereas TextRCNN excelled on SCHRe -\nports. The TextCNN model achieved the second best \nAUC for the SCHReports dataset. In contrast, the few-\nshot learning performance of Qwen-14B-Chat was \napproximately 10% lower compared to the other models.\nAblation study\nSeveral ablation studies were conducted and the corre -\nsponding analyses were provided.\n#1: Effects of knowledge-driven prompt. The MRI lexi -\ncon was removed from the knowledge-driven prompts, \nand the performance of the Qwen-14B-Chat model was \nTable 2 Evaluation results of structured breast MRI for various \nmodels\nThe best results are highlighted in bold\nModel PBERT RBERT FBERT\nQwen-7B-Chat 0.8033 0.8127 0.8080\nQwen-14B-Chat 0.8395 0.8356 0.8376\nGPT-3.5 0.8690 0.8914 0.8801\nGPT-4o 0.8868 0.9059 0.8963\nQwen-7B-Chat (Fine-tuned) 0.9381 0.9217 0.9298\nPage 9 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nevaluated using the default prompts. Figure  6 presents \nthe performance results for the different categories. The \nexperiments demonstrated that the knowledge-driven \nprompt significantly improved the information extrac -\ntion performance for most categories, effectively miti -\ngating the risk of extracting irrelevant information owing \nto literal interpretations of category names, as illustrated \nin Fig.  7. However, the performance of the model exhib -\nited a notable degradation in certain categories. Com -\nplete examples are provided in Table 9 in the Appendix.\n#2: Effect of in-context example quantity. The impact \nof varying the number of in-context examples on the \nperformance of the Qwen-14B-Chat in structured \ninformation extraction from breast MRI reports was \nextracted. As shown in Table  5, the model’s perfor -\nmance consistently improved as we increased the \nnumber of examples from 0 to 5, with the accuracy \nincreasing from 0.7178 to 0.8376. However, a slight \ndecline in performance was observed when the number \nof examples was further increased to 7.\n#3: Effects of MCI. The MCI was removed from the \nstructured reports, and the model was trained using \nstructured reports to assess its performance on the \nSYSMHReports dataset. The first section of Table  6 \nsummarizes the performance of the model in terms of \nprecision, recall, F1 score, and AUC. The results indi -\ncate that when MCI is included, the model’s F1 score \nimproves to 0.8865 (+ 2.46%) and AUC increases to \n0.9405 (+ 2.83%). Figure  8a shows a visualization of \nthe model’s weight assignment to a structured report, \nwhere the “not mentioned” areas are highlighted in \ndarker colors, indicating a higher weight assignment.\n#4: Effects of PH. During the conversion of free-text \nreports to structured reports, a subtle yet important phe -\nnomenon was observed. Owing to the absence of the “per-\nsonal history” category in the template (as shown in Fig. 9), \nLLMs were employed to automatically extract the PH. After \nremoving PH  from the free-text reports, the model was \ntrained using free-text reports, and its performance was \nevaluated on the SYSMHReports dataset. The second sec-\ntion of Table 6 presents the performance of the model in \nterms of precision, recall, F1 score, and AUC. The results \nindicate that including PH improves the AUC to 0.9311 \n(+1.15%). Figure 8b visualizes the model’s weight distribu-\ntion for a free-text report with sections related to PH (e.g., \n“post-surgery” and “follow-up”) highlighted in darker colors, \nsignifying higher weight. From a clinical perspective, PH \nplays a vital role in breast cancer MRI screening [50–52].\n#5: Effects of fusion strategy. Table  7 presents the \nmodel performance for various data fusion strategies. \nThe experimental results show that the fusion strategies \nsignificantly improved the model performance compared \nto training with structured reports or free-text reports \nindividually. Notably, the proposed concatenation fusion \nstrategy achieved the best performance in terms of preci -\nsion, recall, F1 score, and AUC.\nDiscussion\nThis study proposed a novel BI-RADS classification \nmethod for breast MRI reports that thoroughly explored \nthe information contained within the reports. Comprehen-\nsive experimental results demonstrated that the proposed \napproach outperformed the baseline methods in terms \nof reporting classification performance. Ablation studies \nhighlighted the critical significance of the MCI.\nDuring report structuring, the introduced knowledge-\ndriven prompts effectively enhanced the extraction of \ncategory information across most classes. However, cer -\ntain categories posed challenges, as the model struggled \nto fully leverage prior knowledge. This limitation was due \nto the disparity between intuitive clinical descriptions \nand strict medical terminology, leading to mismatches \nTable 3 FBERT for 10 categories obtained via different methods in structured breast MRI reports\nThe best results are highlighted in bold\nCategory Qwen-7B-Chat Qwen-14B-Chat GPT-3.5 GPT-4o Qwen-7B-\nChat (Fine-\ntuned)\nAmount of fibroglandular tissue 0.7145 0.6166 0.7901 0.8259 0.9490\nLevel of background parenchymal enhancement 0.8711 0.9976 0.9700 0.9801 0.9623\nMass/non-mass 0.7849 0.8423 0.8877 0.9133 0.9443\nAxillary lymph nodes 0.8883 0.9530 0.9734 0.9728 0.9787\nSkin lesions 0.6720 0.7608 0.7923 0.7866 0.9149\nNon-enhancing findings 0.9069 0.9357 0.9131 0.9391 0.9437\nAssociated findings 0.7177 0.6669 0.7461 0.7770 0.8900\nFat-containing lesions 0.9157 0.9707 0.9821 0.9831 0.9749\nDynamic contrast-enhanced curve assessment 0.6821 0.7863 0.8039 0.8455 0.7523\nProsthesis 0.9065 0.9219 0.9192 0.9205 0.9731\nPage 10 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nFig. 5 Comparison of different model outputs. Red wavy lines in the figure indicate the occurrence of information extraction errors. The “red \ncross mark” denotes an error in information extraction, while the “green check mark” denotes correct information extraction. Each structured output \nshown is translated from the original Chinese reports\nPage 11 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nbetween real-world reports and predefined terms. Model \nfine-tuning successfully addressed these limitations. The \nrobust performance of knowledge-driven prompts across \nmost categories provides a solid foundation for further \noptimization of the prior knowledge system and contin -\nued enhancement of model learning performance.\nAlthough the proposed fusion strategy demonstrates \npromising performance, it required accommodating a \ndegree of information redundancy when merging struc -\ntured reports with free-text reports to ensure the capture \nof comprehensive clinical information. Future work will \naim to refine this approach by developing more efficient \nfusion mechanisms that minimize redundancy while \nmaintaining information completeness, thereby enhanc -\ning model efficiency and performance.\nTable 4 Classification performance of various models on the test set\nThe best results are highlighted in bold\nModel SYSMHReport SCHReport\nPrecision Recall F1 score AUC Precision Recall F1 score AUC \nTraditional deep learning model\n TextRCNN [45] 0.8599 0.8628 0.8537 0.8944 0.8425 0.8435 0.8430 0.9041\n TextCNN [44] 0.8721 0.8742 0.8662 0.9085 0.8588 0.8620 0.8593 0.9208\n DPCNN [46] 0.8653 0.8683 0.8606 0.9086 0.8529 0.8562 0.8499 0.9088\nTransformer model\n MacBERT [47] 0.8563 0.8603 0.8518 0.9007 0.8473 0.8431 0.8448 0.9073\n RoBERTa-wwm-ext [48] 0.8626 0.8653 0.8567 0.9177 0.8496 0.8517 0.8504 0.9149\n BERT-wwm-ext [48] 0.8744 0.8746 0.8658 0.9320 0.8603 0.8612 0.8607 0.9152\n BERT-wwm [48] 0.8733 0.8758 0.8693 0.9324 0.8653 0.8626 0.8637 0.9165\nLLM (few-shot learning)\n Qwen-14B-Chat [32] 0.7461 0.7377 0.7419 - 0.7209 0.7061 0.7134 -\nOurs 0.9003 0.9024 0.9000 0.9542 0.8759 0.8665 0.8694 0.9295\nFig. 6 Information extraction performances of Qwen-14B-Chat model with different prompts\nPage 12 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nThe optimization of example quantities in prompts was \ninvestigated. The results show that the performance sig -\nnificantly improved as the number of examples increased \nfrom 0 to 5, demonstrating substantial gains in accu -\nracy. However, when the number of examples was fur -\nther increased to 7, a slight decline in performance was \nobserved. This finding reveals that simply increasing the \nnumber of examples is not an optimal strategy. Experimen-\ntal results indicate that, under the constraints of limited \ncontext windows, an excessive number of examples can \ndilute the model’s attention and affect its focus on tasks. \nIn particular, for domain-specific tasks, it was found that \na moderate set of examples was sufficient to establish the \nnecessary task patterns and achieve optimal performance.\nDespite the limited sample size of the real-world \ndataset, the model exhibited exceptional performance, \nhighlighting its significant potential for large-scale \ntraining with datasets from additional centers in the \nfuture. Although this study focused on single-modal \ntext data, existing research has demonstrated that mul -\ntimodal learning can integrate information from differ -\nent sources to enhance model understanding [53–55]. \nFuture research could explore the combination of tex -\ntual data with medical images to develop more efficient \nFig. 7 Effect of using knowledge-driven prompts on free-text reports. The “red cross mark” denotes incorrect information extraction, \nwhile the “green check mark” denotes correct information extraction. The reports shown are the English translations of the original Chinese reports. \nFree-text and structured reports are shown in truncated form\nTable 5 Evaluation results of Qwen-14B-Chat on structured \nbreast MRI reports with different numbers of in-context examples\nThe best performance is highlighted in bold\nNumber of example PBERT RBERT FBERT\n0 0.7352 0.7012 0.7178\n1 0.8065 0.7993 0.8029\n3 0.8277 0.8228 0.8253\n5 0.8395 0.8356 0.8376\n7 0.8234 0.8110 0.8172\nTable 6 Performance analysis of report formats for BI-RADS \nclassification on the SYSMHReports dataset\nThe best results are highlighted in bold\nPH Personal history\nIndex Precision Recall F1 score AUC \nStructured report\n Without MCI 0.8687 0.8704 0.8619 0.9122\n With MCI 0.8862 0.8889 0.8865 0.9405\nFree-text report\n Without PH 0.8687 0.8708 0.8628 0.9196\n With PH 0.8710 0.8729 0.8652 0.9311\nPage 13 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nFig. 8 Visualization of attention weights assigned to a sample structured and free-text report by the model. Words with higher weights are shown \nin darker red, indicating greater importance to the model\nFig. 9 PH in a free-text report. When a free-text report is converted to a structured report, the PH is lost (the PH highlighted in yellow). The reports \nshown are the English translations of the original Chinese reports\nPage 14 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \nmultimodal methods for improving medical classification \ndecisions.\nIn recent years, artificial intelligence has demonstrated \nextensive applicability in clinical decision support, dis -\nease diagnosis, and health monitoring [56]. As a cut -\nting-edge artificial intelligence technology, LLMs offer \npromising opportunities to address challenges in the \nmedical field. Although LLMs have provided significant \nadvances and convenience, the substantial memory and \ncomputational resources required for fine-tuning remain \nmajor obstacles to their widespread application. Addi -\ntionally, the effectiveness of LLM fine-tuning depends \nheavily on data quality, which can significantly impact \nmodel performance and robustness. Similar to the image \nand video quality assessments [57–60], text data quality \nevaluation is crucial. While current data screening and \nevaluation still rely on manual operations, future work \nwill focus on developing automated quality assessment \nmethods to optimize the text data screening process, \nthereby better addressing the clinical needs in practice.\nConclusions\nThis study presented a BI-RADS classification method \nleveraging LLMs and transformer models to thor -\noughly explore information from breast MRI reports. \nThis method incorporated the MCI by converting free-\ntext reports into structured reports, thereby effectively \nenriching the learning content of the model. To ensure \ndata privacy and enhance the adaptability of LLMs in \nspecialized domains, LLMs were deployed locally, and \na knowledge-driven prompt was designed. To improve \nthe capability of the model in structuring breast MRI \nreports, targeted fine-tuning was conducted. Further -\nmore, to ensure the comprehensiveness and diversity \nof the training data, a fusion strategy was proposed \nto synergistically utilize information from both struc -\ntured and free-text reports. Compared with other \nbaseline methods, the proposed approach achieved \nsignificant advantages in reporting classification tasks. \nThe ablation studies verified the influence of each com -\nponent. Additionally, the proposed method was evalu -\nated using datasets from two independent centers, and \nthe experimental results demonstrated its robustness \nand reliability.\nAbbreviations\nMRI  Magnetic resonance imaging\nMCI  Missing category information\nNLP  Natural language processing\nSVM  Support vector machine\nKNN  K-nearest neighbor\nNB  Naive Bayes\nCNN  Convolutional neural network\nRNN  Recurrent neural network\nBERT  Bidirectional encoder representations from transformers\nLLM  Large language model\nGLM  Generative language model\nLoRA  Low-rank adaptation\nFFN  Feed-forward neural network\nAUC   Area under the curve\nPH  Personal history\nSYSMHReports  Sun Yat-sen Memorial Hospital Breast MRI Reports\nSCHReports  Shantou Central Hospital Breast MRI Reports\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s42492- 025- 00189-8.\nSupplementary Material 1.\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nYL performed the conceptualization, methodology, formal analysis, investiga-\ntion, writing original draft, validation, and visualization; XZ performed con-\nceptualization, funding acquisition, project administration, and supervision; \nWWC and WJC performed the investigation, methodology, and writing review \nand editing; YP and JH performed the data curation; ZL and TT performed the \nwriting review and editing; JS performed the supervision; JZ performed the \nfunding acquisition, resources, project administration, supervision, and writing \nreview and editing. All the authors have inputs in manuscript revision.\nFunding\nThis work was supported in part by the National Natural Science Foundation \nof China, Nos. 62371499, U23A20483, 82102130; in part by the Depart-\nment of Science and Technology of Shandong Province, No. SYS202208; \nTable 7 Comparison of model performance with and without fusion, as well as under alternative fusion strategies\nThe best results are highlighted in bold\nFusion strategy Input Index\nFree-text report Structured report Precision Recall F1 score AUC \nWithout fusion ✓ 0.8710 0.8729 0.8652 0.9311\nWithout fusion ✓ 0.8862 0.8889 0.8865 0.9405\nCross-attention fusion ✓ ✓ 0.8874 0.8902 0.8870 0.9453\nAverage-pooling fusion ✓ ✓ 0.8964 0.8986 0.8956 0.9502\nAddition fusion ✓ ✓ 0.8947 0.8969 0.8938 0.9511\nMax-pooling fusion ✓ ✓ 0.8961 0.8981 0.8948 0.9513\nConcatenation fusion (ours) ✓ ✓ 0.9003 0.9024 0.9000 0.9542\nPage 15 of 16\nLiu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n \nin part by the Suzhou Science and Technology Bureau, No. SJC2021023; \nin part by the Guangdong Basic and Applied Basic Research Foundation, \nNo. 2023A1515011305; and in part by the Guangzhou Basic and Applied Basic \nResearch Foundation, No. 2023A04J2112.\nAvailability of data and materials\nThe clinical data used in this research, SYSMHReports, were provided by Sun \nYat-sen Memorial Hospital, and SCHReports were provided by Shantou Central \nHospital. Clinical data are not publicly available as they contain private patient \nhealth information. To ensure ethical compliance, approval was obtained from \nthe local medical ethics committee. The requirement for informed consent \nwas waived due to the use of de-identified data in this study.\nDeclarations\nCompeting interests\nThe authors declare that they have no known competing financial interests \nor personal relationships that could have appeared to influence the work \nreported in this paper.\nReceived: 7 December 2024   Accepted: 26 February 2025\nReferences\n 1. Zhao XM, Liao YH, Xie JH, He XX, Zhang SQ, Wang GY et al (2023) \nBreastDM: a DCE-MRI dataset for breast tumor image segmentation and \nclassification. Comput Biol Med 164:107255. https:// doi. org/ 10. 1016/j. \ncompb iomed. 2023. 107255\n 2. Bellhouse S, Hawkes RE, Howell SJ, Gorman L, French DP (2021) Breast \ncancer risk assessment and primary prevention advice in primary care: a \nsystematic review of provider attitudes and routine behaviours. Cancers \n(Basel) 13(16):4150. https:// doi. org/ 10. 3390/ cance rs131 64150\n 3. Loving VA, Johnston BS, Reddy DH, Welk LA, Lawther HA, Klein SC et al \n(2023) Antithrombotic therapy and hematoma risk during image-guided \ncore-needle breast biopsy. Radiology 306(1):79–86. https:// doi. org/ 10. \n1148/ radiol. 220548\n 4. Kowal M, Filipczuk P , Obuchowicz A, Korbicz J, Monczak R (2013) \nComputer-aided diagnosis of breast cancer based on fine needle biopsy \nmicroscopic images. Comput Biol Med 43(10):1563–1572. https:// doi. org/ \n10. 1016/j. compb iomed. 2013. 08. 003\n 5. Sandbank J, Bataillon G, Nudelman A, Krasnitsky I, Mikulinsky R, Bien L \net al (2022) Validation and real-world clinical application of an artificial \nintelligence algorithm for breast cancer detection in biopsies. NPJ Breast \nCancer 8(1):129. https:// doi. org/ 10. 1038/ s41523- 022- 00496-w\n 6. Wei Q, Yan YJ, Wu GG, Ye XR, Jiang F, Liu J et al (2022) The diagnostic \nperformance of ultrasound computer-aided diagnosis system for \ndistinguishing breast masses: a prospective multicenter study. Eur Radiol \n32(6):4046–4055. https:// doi. org/ 10. 1007/ s00330- 021- 08452-1\n 7. Kim SY, Choi Y, Kim EK, Han BK, Yoon JH, Choi JS et al (2021) Deep \nlearning-based computer-aided diagnosis in screening breast ultrasound \nto reduce false-positive diagnoses. Sci Rep 11(1):395\n 8. Diamond CJ, Laurentiev J, Yang J, Wint A, Harris KA, Dang TH et al (2022) \nNatural language processing to identify abnormal breast, lung, and cervi-\ncal cancer screening test results from unstructured reports to support \ntimely follow-up. Stud Health Technol Inform 290:433–437. https:// doi. \norg/ 10. 3233/ SHTI2 20112\n 9. Wang GS, Lou XX, Guo F, Kwok D, Cao C (2024) EHR-HGCN: an enhanced \nhybrid approach for text classification using heterogeneous graph \nconvolutional networks in electronic health records. IEEE J Biomed Health \nInform 28(3):1668–1679. https:// doi. org/ 10. 1109/ JBHI. 2023. 33462 10\n 10. Kłos M, Żyłkowski J, Spinczyk D (2019) Automatic classification of text \ndocuments presenting radiology examinations. In: Pietka E, Badura P , \nKawa J, Wieclawek W (eds) Information technology in biomedicine: Pro-\nceedings 6th international conference, ITIB’2018, Kamień Śląski, Poland, \n18–20 June 2018. Springer, Cham, pp 495–505. https:// doi. org/ 10. 1007/ \n978-3- 319- 91211-0_ 43\n 11. Banerjee I, Ling Y, Chen MC, Hasan SA, Langlotz CP , Moradzadeh N et al \n(2019) Comparative effectiveness of convolutional neural network (CNN) \nand recurrent neural network (RNN) architectures for radiology text \nreport classification. Artif Intell Med 97:79–88. https:// doi. org/ 10. 1016/j. \nartmed. 2018. 11. 004\n 12. Dahl FA, Rama T, Hurlen P , Brekke PH, Husby H, Gundersen T et al (2021) \nNeural classification of Norwegian radiology reports: using NLP to detect \nfindings in CT-scans of children. BMC Med Inform Decis Mak 21(1):84. \nhttps:// doi. org/ 10. 1186/ s12911- 021- 01451-8\n 13. Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: pre-\ntraining of deep bidirectional transformers for language \nunderstanding. arXiv preprint arXiv: 1810.04805\n 14. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T et al \n(2019) Publicly available clinical BERT embeddings. arXiv preprint arXiv: \n1904.03323. https:// doi. org/ 10. 48550/ arXiv. 1904. 03323\n 15. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH et al (2020) BioBERT: a pre-\ntrained biomedical language representation model for biomedical text \nmining. Bioinformatics 36(4):1234–1240. https:// doi. org/ 10. 1093/ bioin \nforma tics/ btz682\n 16. Yan A, McAuley J, Lu X, Du J, Chang EY, Gentili A et al (2022) RadBERT: \nadapting transformer-based language models to radiology. Radiol Artif \nIntell 4(4):e210258. https:// doi. org/ 10. 1148/ ryai. 210258\n 17. Zhai GT, Min XK (2020) Perceptual image quality assessment: a survey. Sci \nChina Inf Sci 63(11):211301. https:// doi. org/ 10. 1007/ s11432- 019- 2757-1\n 18. Min XK, Duan HY, Sun W, Zhu YC, Zhai GT (2024) Perceptual video quality \nassessment: a survey. Sci China Inf Sci 67(11):211301. https:// doi. org/ 10. \n1007/ s11432- 024- 4133-3\n 19. Min XK, Gu K, Zhai GT, Yang XK, Zhang WJ, Le Callet P et al (2021) Screen \ncontent quality assessment: overview, benchmark, and beyond. ACM \nComput Surv 54(9):187. https:// doi. org/ 10. 1145/ 34709 70\n 20. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW \n(2023) Large language models in medicine. Nat Med 29(8):1930–1940\n 21. Hegselmann S, Buendia A, Lang H, Agrawal M, Jiang XY, Sontag D (2023) \nTaBLLM: few-shot classification of tabular data with large language \nmodels. In: Proceedings of the 26th international conference on artificial \nintelligence and statistics, AISTATS, Valencia, 25–27 April 2023\n 22. Sushil M, Zack T, Mandair D, Zheng ZW, Wali A, Yu YN et al (2024) A com-\nparative study of large language model-based zero-shot inference and \ntask-specific supervised classification of breast cancer pathology reports. \nJ Am Med Inform Assoc 31(10):2315–2327. https:// doi. org/ 10. 1093/ jamia/ \nocae1 46\n 23. Chen S, Li YY, Lu S, Van H, Aerts HJWL, Savova GK et al (2024) Evaluating \nthe ChatGPT family of models for biomedical reasoning and classification. \nJ Am Med Inform Assoc 31(4):940–948. https:// doi. org/ 10. 1093/ jamia/ \nocad2 56\n 24. Wei X, Cui XY, Cheng N, Wang XB, Zhang X, Huang S et al (2024) ChatIE: \nzero-shot information extraction via chatting with ChatGPT. arXiv pre-\nprint arXiv: 2302.10205\n 25. Zhong TY, Zhao W, Zhang YT, Pan Y, Dong PX, Jiang ZW et al (2023) \nChatRadio-Valuer: a chat large language model for generalizable radiol-\nogy report generation based on multi-institution and multi-system data. \narXiv preprint arXiv: 2310.05242. https:// doi. org/ 10. 48550/ arXiv. 2310. \n05242\n 26. Adamson B, Waskom M, Blarre A, Kelly J, Krismer K, Nemeth S et al (2023) \nApproach to machine learning for extraction of real-world data variables \nfrom electronic health records. Front Pharmacol 14:1180962\n 27. Nobel JM, van Geel K, Robben SGF (2022) Structured reporting in \nradiology: a systematic review to explore its potential. Eur Radiol \n32(4):2837–2854\n 28. Fanni SC, Gabelloni M, Alberich-Bayarri A, Neri E (2022) Structured report-\ning and artificial intelligence. In: Fatehi M, dos Santos DP (eds) Structured \nreporting in radiology. Springer, Cham, pp 169–183\n 29. Adams LC, Truhn D, Busch F, Kader A, Niehues SM, Makowski MR et al \n(2023) Leveraging GPT-4 for post hoc transformation of free-text radiol-\nogy reports into structured reporting: a multilingual feasibility study. \nRadiology 307(4):e230725. https:// doi. org/ 10. 1148/ radiol. 230725\n 30. Bhayana R, Nanda B, Dehkharghanian T, Deng YQ, Bhambra N, Elias G \net al (2024) Large language models for automated synoptic reports \nand resectability categorization in pancreatic cancer. Radiology \n311(3):e233117. https:// doi. org/ 10. 1148/ radiol. 233117\nPage 16 of 16Liu et al. Visual Computing for Industry, Biomedicine, and Art             (2025) 8:8 \n 31. Rao AA, Feneis J, Lalonde C, Ojeda-Fournier H (2016) A pictorial review \nof changes in the BI-RADS fifth edition. RadioGraphics 36(3):623–639. \nhttps:// doi. org/ 10. 1148/ rg. 20161 50178\n 32. Bai JZ, Bai S, Chu YF, Cui ZY, Dang K, Deng XD et al (2023) Qwen technical \nreport. arXiv preprint arXiv: 2309.16609. https:// doi. org/ 10. 48550/ arXiv. \n2309. 16609\n 33. Heston TF, Khun C (2023) Prompt engineering in medical education. Int \nMed Educ 2(3):198–205. https:// doi. org/ 10. 3390/ ime20 30019\n 34. Hu EJ, Shen YL, Wallis P , Allen-Zhu Z, Li YZ, Wang SA et al (2021) Lora: \nlow-rank adaptation of large language models. arXiv preprint arXiv: \n2106.09685. https:// doi. org/ 10. 48550/ arXiv. 2106. 09685\n 35. Ding N, Qin YJ, Yang G, Wei FC, Yang ZH, Su YS et al (2023) Parameter-effi-\ncient fine-tuning of large-scale pre-trained language models. Nat Mach \nIntell 5(3):220–235. https:// doi. org/ 10. 1038/ s42256- 023- 00626-4\n 36. Lanfredi RB, Mukherjee P , Summers RM (2025) Enhancing chest X-ray \ndatasets with privacy-preserving large language models and multi-type \nannotations: a data-driven approach for improved classification. Med \nImage Anal 99:103383. https:// doi. org/ 10. 1016/j. media. 2024. 103383\n 37. Vaswani A, Shazeer A, Parmar N, Uszkoreit J, Jones L, Gomez AN et al \n(2017) Attention is all you need. In: Proceedings of the 31st international \nconference on neural information processing systems, Curran Associates \nInc., Long Beach, 4–9 December 2017\n 38. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A et al (2020) Hug-\ngingFace’s transformers: state-of-the-art natural language processing. \narXiv preprint arXiv: 1910.03771\n 39. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G et al (2019) \nPyTorch: an imperative style, high-performance deep learning library. In: \nProceedings of the 33rd international conference on neural information \nprocessing systems, Curran Associates Inc., Vancouver, 8–14 December \n2019\n 40. Jeblick K, Schachtner B, Dexl J, Mittermeier A, Stüber AT, Topalis J et al \n(2024) Chatgpt makes medicine easy to swallow: an exploratory case \nstudy on simplified radiology reports. Eur Radiol 34(5):2817–2825. https:// \ndoi. org/ 10. 1007/ s00330- 023- 10213-1\n 41. OpenAI (2022) Introducing ChatGPT. https:// openai. com/ blog/ chatg pt/. \nAccessed 1 June 2024\n 42. OpenAI (2024) Hello GPT-4o. https:// openai. com/ index/ hello- gpt- 4o/. \nAccessed 1 June 2024\n 43. Zhang TY, Kishore V, Wu F, Weinberger KQ, Artzi Y (2020) BERTscore: evalu-\nating text generation with BERT. arXiv preprint arXiv: 1904.09675. https:// \ndoi. org/ 10. 48550/ arXiv. 1904. 09675\n 44. Chen YH (2015) Convolutional neural network for sentence classification. \nDissertation, University of Waterloo\n 45. Lai SW, Xu LH, Liu K, Zhao J (2015) Recurrent convolutional neural net-\nworks for text classification. In: Proceedings of the 29th AAAI conference \non artificial intelligence, AAAI Press, Austin, 25–30 January 2015\n 46. Johnson R, Zhang T (2017) Deep pyramid convolutional neural networks \nfor text categorization. In: Proceedings of the 55th annual meeting of the \nassociation for computational linguistics (volume 1: long papers), ACL, \nVancouver, 30 July–4 August 2017. https:// doi. org/ 10. 18653/ v1/ P17- 1052\n 47. Cui YM, Che WX, Liu T, Qin B, Wang SJ, Hu GP (2020) Revisiting pre-\ntrained models for Chinese natural language processing. arXiv pre-\nprint arXiv: 2004.13922. https:// doi. org/ 10. 48550/ arXiv. 2004. 13922\n 48. Cui YM, Che WX, Liu T, Qin B, Yang ZQ (2021) Pre-training with whole \nword masking for Chinese BERT. IEEE/ACM Trans Audio Speech Lang \nProcess 29:3504–3514. https:// doi. org/ 10. 1109/ TASLP . 2021. 31243 65\n 49. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P et al (2020) \nLanguage models are few-shot learners. In: Proceedings of the 34th inter-\nnational conference on neural information processing systems, Curran \nAssociates Inc., Vancouver, 6–12 December 2020\n 50. Lee JM, Ichikawa LE, Wernli KJ, Bowles E, Specht JM, Kerlikowske K et al \n(2021) Digital mammography and breast tomosynthesis performance in \nwomen with a personal history of breast cancer, 2007-2016. Radiology \n300(2):290–300. https:// doi. org/ 10. 1148/ radiol. 20212 04581\n 51. Schacht DV, Yamaguchi K, Lai J, Kulkarni K, Sennett CA, Abe H (2014) \nImportance of a personal history of breast cancer as a risk factor for the \ndevelopment of subsequent breast cancer: results from screening breast \nMRI. Am J Roentgenol 202(2):289–292. https:// doi. org/ 10. 2214/ AJR. 13. \n11553\n 52. Lehman CD, Lee JM, DeMartini WB, Hippe DS, Rendi MH, Kalish G et al \n(2016) Screening MRI in women with a personal history of breast cancer. \nJ Natl Cancer Inst 108(3):djv349. https:// doi. org/ 10. 1093/ jnci/ djv349\n 53. Wang JR, Duan HY, Zhai GT, Min XK (2025) Quality assessment for AI gen-\nerated images with instruction tuning. arXiv preprint arXiv: 2405.07346. \nhttps:// doi. org/ 10. 48550/ arXiv. 2405. 07346\n 54. Jia ZH, Zhang ZC, Qian JY, Wu HN, Sun W, Li CY et al (2024) VQA2 : visual \nquestion answering for video quality assessment. arXiv preprint arXiv: \n2411.03795. https:// doi. org/ 10. 48550/ arXiv. 2411. 03795\n 55. Wang JR, Duan HY, Zhai GT, Wang JT, Min XK (2024) AIGV-assessor: \nbenchmarking and evaluating the perceptual quality of text-to-video \ngeneration with LMM. arXiv preprint arXiv: 2411.17221. https:// doi. org/ \n10. 48550/ arXiv. 2411. 17221\n 56. Huang T, Xu HY, Wang HT, Huang HF, Xu YJ, Li BH et al (2023) Artificial \nintelligence for medicine: progress, challenges, and perspectives. Innov \nMed 1(2):100030\n 57. Min XK, Gu K, Zhai GT, Liu J, Yang XK, Chen CW (2018) Blind quality \nassessment based on pseudo-reference image. IEEE Trans Multimedia \n20(8):2049–2062. https:// doi. org/ 10. 1109/ TMM. 2017. 27882 06\n 58. Min XK, Zhai GT, Gu K, Liu YT, Yang XK (2018) Blind image quality estima-\ntion via distortion aggravation. IEEE Trans Broadcast 64(2):508–517. \nhttps:// doi. org/ 10. 1109/ TBC. 2018. 28167 83\n 59. Min XK, Zhai GT, Zhou JT, Farias MCQ, Bovik AC (2020) Study of subjective \nand objective quality assessment of audio-visual signals. IEEE Trans Image \nProcess 29:6054–6068. https:// doi. org/ 10. 1109/ TIP . 2020. 29881 48\n 60. Min XK, Gao YX, Cao YQ, Zhai GT, Zhang WJ, Sun HF et al (2024) Exploring \nrich subjective quality information for image quality assessment in the \nwild. arXiv preprint arXiv: 2409.05540. https:// doi. org/ 10. 48550/ arXiv. \n2409. 05540\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7687757015228271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5552684664726257
    },
    {
      "name": "BI-RADS",
      "score": 0.5535715222358704
    },
    {
      "name": "Machine learning",
      "score": 0.5156525373458862
    },
    {
      "name": "Breast MRI",
      "score": 0.5091087222099304
    },
    {
      "name": "Breast imaging",
      "score": 0.49825525283813477
    },
    {
      "name": "Data mining",
      "score": 0.47737976908683777
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4665657579898834
    },
    {
      "name": "Magnetic resonance imaging",
      "score": 0.4289296269416809
    },
    {
      "name": "Bootstrapping (finance)",
      "score": 0.42754003405570984
    },
    {
      "name": "Breast cancer",
      "score": 0.3008647561073303
    },
    {
      "name": "Mammography",
      "score": 0.19830602407455444
    },
    {
      "name": "Medicine",
      "score": 0.18094909191131592
    },
    {
      "name": "Radiology",
      "score": 0.1583871841430664
    },
    {
      "name": "Cancer",
      "score": 0.11835023760795593
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Financial economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210092817",
      "name": "Suzhou Institute of Biomedical Engineering and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I308837",
      "name": "Suzhou University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097354",
      "name": "Sun Yat-sen Memorial Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I154099455",
      "name": "Shandong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I49835588",
      "name": "Macao Polytechnic University",
      "country": "MO"
    },
    {
      "id": "https://openalex.org/I4210094879",
      "name": "Shandong Institute of Automation",
      "country": "CN"
    }
  ],
  "cited_by": 4
}