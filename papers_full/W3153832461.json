{
  "title": "Action-Conditioned 3D Human Motion Synthesis with Transformer VAE",
  "url": "https://openalex.org/W3153832461",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3010215994",
      "name": "Mathis Petrovich",
      "affiliations": [
        "Université Gustave Eiffel",
        "Laboratoire d'Informatique Gaspard-Monge",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2159537190",
      "name": "Michael J. Black",
      "affiliations": [
        "Max Planck Institute for Intelligent Systems"
      ]
    },
    {
      "id": "https://openalex.org/A1989602520",
      "name": "Gül Varol",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique Gaspard-Monge",
        "Université Gustave Eiffel"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3108307345",
    "https://openalex.org/W2949924544",
    "https://openalex.org/W3034514130",
    "https://openalex.org/W3167478287",
    "https://openalex.org/W3204221554",
    "https://openalex.org/W6781874425",
    "https://openalex.org/W2990365862",
    "https://openalex.org/W2975420824",
    "https://openalex.org/W3015625764",
    "https://openalex.org/W2897682634",
    "https://openalex.org/W2902757548",
    "https://openalex.org/W6770208262",
    "https://openalex.org/W3035551320",
    "https://openalex.org/W6640963894",
    "https://openalex.org/W1803024989",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2991727786",
    "https://openalex.org/W2969416295",
    "https://openalex.org/W2118931255",
    "https://openalex.org/W3035581100",
    "https://openalex.org/W2469134594",
    "https://openalex.org/W2983796203",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W6774842091",
    "https://openalex.org/W6775031022",
    "https://openalex.org/W6781523853",
    "https://openalex.org/W2982625143",
    "https://openalex.org/W2964076328",
    "https://openalex.org/W4288079574",
    "https://openalex.org/W3048474702",
    "https://openalex.org/W6751605722",
    "https://openalex.org/W6744627333",
    "https://openalex.org/W2945629925",
    "https://openalex.org/W2611706523",
    "https://openalex.org/W3048665613",
    "https://openalex.org/W6786980705",
    "https://openalex.org/W2978956737",
    "https://openalex.org/W2963092440",
    "https://openalex.org/W2986708244",
    "https://openalex.org/W6687045409",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W2768683308",
    "https://openalex.org/W6752114883",
    "https://openalex.org/W3034423770",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6750468714",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6790948428",
    "https://openalex.org/W6788231789",
    "https://openalex.org/W1735317348",
    "https://openalex.org/W1991366868",
    "https://openalex.org/W2121899951",
    "https://openalex.org/W6635103780",
    "https://openalex.org/W2993797559",
    "https://openalex.org/W2963324990",
    "https://openalex.org/W1536003180",
    "https://openalex.org/W2769102608",
    "https://openalex.org/W6604360534",
    "https://openalex.org/W2026720449",
    "https://openalex.org/W2987787977",
    "https://openalex.org/W2964203186",
    "https://openalex.org/W2971856312",
    "https://openalex.org/W2120852816",
    "https://openalex.org/W6774631009",
    "https://openalex.org/W6762557054",
    "https://openalex.org/W6774882322",
    "https://openalex.org/W6781447135",
    "https://openalex.org/W1967554269",
    "https://openalex.org/W3108664802",
    "https://openalex.org/W3013310839",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2753738274",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2962916650",
    "https://openalex.org/W3111487461",
    "https://openalex.org/W3134060426",
    "https://openalex.org/W3068510429",
    "https://openalex.org/W2798670545",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1582128237",
    "https://openalex.org/W3109585842",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W3107914916",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W3119469378",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2802441648",
    "https://openalex.org/W3107471220",
    "https://openalex.org/W3102937540",
    "https://openalex.org/W3110022498",
    "https://openalex.org/W3046205681",
    "https://openalex.org/W2803747201",
    "https://openalex.org/W3042719542",
    "https://openalex.org/W107615784",
    "https://openalex.org/W3095559139",
    "https://openalex.org/W2989607414",
    "https://openalex.org/W3162938128",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2944006115",
    "https://openalex.org/W3108262631",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "We tackle the problem of action-conditioned generation of realistic and\\ndiverse human motion sequences. In contrast to methods that complete, or\\nextend, motion sequences, this task does not require an initial pose or\\nsequence. Here we learn an action-aware latent representation for human motions\\nby training a generative variational autoencoder (VAE). By sampling from this\\nlatent space and querying a certain duration through a series of positional\\nencodings, we synthesize variable-length motion sequences conditioned on a\\ncategorical action. Specifically, we design a Transformer-based architecture,\\nACTOR, for encoding and decoding a sequence of parametric SMPL human body\\nmodels estimated from action recognition datasets. We evaluate our approach on\\nthe NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the\\nstate of the art. Furthermore, we present two use cases: improving action\\nrecognition through adding our synthesized data to training, and motion\\ndenoising. Code and models are available on our project page.\\n",
  "full_text": "Action-Conditioned 3D Human Motion Synthesis with Transformer V AE\nMathis Petrovich1 Michael J. Black2 G¨ul Varol1\n1 LIGM, ´Ecole des Ponts, Univ Gustave Eiffel, CNRS, France\n2 Max Planck Institute for Intelligent Systems, T¨ubingen, Germany\n{mathis.petrovich,gul.varol}@enpc.fr, black@tue.mpg.de\nhttps://imagine.enpc.fr/˜petrovim/actor\nAbstract\nWe tackle the problem of action-conditioned generation\nof realistic and diverse human motion sequences. In con-\ntrast to methods that complete, or extend, motion sequences,\nthis task does not require an initial pose or sequence. Here\nwe learn an action-aware latent representation for human\nmotions by training a generative variational autoencoder\n(VAE). By sampling from this latent space and querying\na certain duration through a series of positional encod-\nings, we synthesize variable-length motion sequences con-\nditioned on a categorical action. Specifically, we design\na Transformer-based architecture, ACTOR, for encoding\nand decoding a sequence of parametric SMPL human body\nmodels estimated from action recognition datasets. We eval-\nuate our approach on the NTU RGB+D, HumanAct12 and\nUESTC datasets and show improvements over the state of\nthe art. Furthermore, we present two use cases: improv-\ning action recognition through adding our synthesized data\nto training, and motion denoising. Code and models are\navailable on our project page [57].\n1. Introduction\nDespite decades of research on modeling human motions\n[4, 5], synthesizing realistic and controllable sequences re-\nmains extremely challenging. In this work, our goal is to\ntake a semantic action label like “Throw” and generate an\ninfinite number of realistic 3D human motion sequences, of\nvarying length, that look like realistic throwing (Figure 1).\nA significant amount of prior work has focused on taking\none pose, or a sequence of poses, and then predicting future\nmotions [3, 6, 22, 71, 74]. This is an overly constrained\nscenario because it assumes that one already has a motion\nsequence and just needs more of it. On the other hand,\nmany applications such as virtual reality and character con-\ntrol [28, 61] require generating motions of a given type (se-\nmantic action label) with a specified duration.\nWe address this problem by training an action-cond-\nitioned generative model with 3D human motion data that\nFig. 1: Goal: Action-Conditioned TransfORmer V AE(ACTOR)\nlearns to synthesize human motion sequences conditioned on a\ncategorical action and a duration, T. Sequences are generated by\nsampling from a single motion representation latent vector, z, as\nopposed to the frame-level embedding space in prior work.\nhas corresponding action labels. In particular, we construct\na Transformer-based encoder-decoder architecture and train\nit with the V AE objective. We parameterize the human body\nusing SMPL [46] as it can output joint locations or the body\nsurface. This paves the way for better modeling of inter-\naction with the environment, as the surface is necessary to\nmodel contact. Moreover, such a representation allows the\nuse of several reconstruction losses: constraining part rota-\ntions in the kinematic tree, joint locations, or surface points.\nThe literature [40] and our results suggest that a combina-\ntion of losses gives the most realistic generated motions.\nThe key challenge of motion synthesis is to generate se-\nquences that are perceptually realistic while being diverse.\nMany approaches for motion generation have taken an au-\ntoregressive approach such as LSTMs [16] and GRUs [49].\nHowever, these methods typically regress to the mean pose\n1\nafter some time [49] and are subject to drift. The key nov-\nelty in our Transformer model is to provide positional en-\ncodings to the decoder and to output the full sequence at\nonce. Positional encoding has been popularized by recent\nwork on neural radiance fields [50]; we have not seen it\nused for motion generation as we do. This allows the gen-\neration of variable length sequences without the problem of\nthe motions regressing to the mean pose. Moreover, our\napproach is, to our knowledge, the first to create an action-\nconditioned sequence-level embedding. The closest work is\nAction2Motion [21], which, in contrast, presents an autore-\ngressive approach where the latent representation is at the\nframe-level. Getting a sequence-level embedding requires\npooling the time dimension: we introduce a new way of\ncombining Transformers and V AEs for this purpose, which\nalso significantly improves performance over baselines.\nA challenge specific to our action-condition generation\nproblem is that there exists limited motion capture (MoCap)\ndata paired with distinct action labels, typically on the or-\nder of 10 categories [31, 63]. We instead rely on monocu-\nlar motion estimation methods [38] to obtain 3D sequences\nfor actions and present promising results on 40 fine-grained\ncategories of the UESTC action recognition dataset [32]. In\ncontrast to [21], we do not require multi-view cameras to\nprocess monocular trajectory estimates, which makes our\nmodel potentially applicable to larger scales. Despite be-\ning noisy, monocular estimates prove sufficient for training\nand, as a side benefit of our model, we are able to denoise\nthe estimated sequences by encoding-decoding through our\nlearned motion representation.\nAn action-conditioned generative model can augment\nexisting MoCap datasets, which are expensive and limited\nin size [48, 63]. Recent work, which renders synthetic hu-\nman action videos for training action recognition models\n[65], shows the importance of motion diversity and large\namounts of data per action. Such approaches can benefit\nfrom an infinite source of action-conditioned motion syn-\nthesis. We explore this through our experiments on action\nrecognition. We observe that, despite a domain gap, the\ngenerated motions can serve as additional training data, spe-\ncially in low-data regimes. Finally, a compact action-aware\nlatent space for human motions can be used as a prior in\nother tasks such as human motion estimation from videos.\nOur contributions are fourfold: (i) We introduce\nACTOR, a novel Transformer-based conditional V AE, and\ntrain it to generate action-conditioned human motions by\nsampling from a sequence-level latent vector. (ii) We\ndemonstrate that it is possible to learn to generate realis-\ntic 3D human motions using noisy 3D body poses estimated\nfrom monocular video; (iii) We present a comprehensive ab-\nlation study of the architecture and loss components, obtain-\ning state-of-the-art performance on multiple datasets; (iv)\nWe illustrate two use cases for our model on action recog-\nnition and MoCap denoising. The code is available on our\nproject page [57].\n2. Related Work\nWe briefly review relevant literature on motion predic-\ntion, motion synthesis, monocular motion estimation, as\nwell as Transformers in the context of V AEs.\nFuture human motion prediction. Research on human\nmotion analysis has a long history dating back to 1980s [5,\n17, 19, 52]. Given past motion or an initial pose, predicting\nfuture frames has been referred as motion prediction. Statis-\ntical models have been employed in earlier studies [7, 18].\nRecently, several works show promising results following\nprogress in generative models with neural networks, such as\nGANs [20] or V AEs [37]. Examples include HP-GAN [6]\nand recurrent V AE [22] for future motion prediction. Most\nwork treats the body as a skeleton, though recent work ex-\nploits full 3D body shape models [3, 74]. Similar to [74],\nwe also go beyond sparse joints and incorporate vertices\non the body surface. DLow [71] focuses on diversifying\nthe sampling of future motions from a pretrained model.\n[11] performs conditional future prediction using contex-\ntual cues about the object interaction. Very recently, [42]\npresents a Transformer-based method for dance generation\nconditioned on music and past motion. Duan et al. [14]\nuse Transformers for motion completion. There is a related\nline of work on motion “in-betweening” that takes both past\nand future poses and “inpaints” plausible motions between\nthem; see [23] for more. In contrast to this prior work, our\ngoal is to synthesize motions without any past observations.\nHuman motion synthesis. While there is a vast literature\non future prediction, synthesis from scratch has received\nrelatively less attention. Very early work used PCA [51] and\nGPLVMs [64] to learn statistical models of cyclic motions\nlike walking and running. Conditioning synthesis on mul-\ntiple, varied, actions is much harder. DVGANs [43] train\na generative model conditioned on a short text representing\nactions in MoCap datasets such as Human3.6M [30, 31] and\nCMU [63]. Text2Action [1] and Language2Pose [2] simi-\nlarly explore conditioning the motion generation on textual\ndescriptions. Music-to-Dance [39] and [41] study music-\nconditioned generation. QuaterNet [56] focuses on gen-\nerating locomotion actions such as walking and running\ngiven a ground trajectory and average speed. [69] presents\na convolution-based generative model for realistic, but un-\nconstrained motions without specifying an action. Simi-\nlarly, [73] synthesizes arbitrary sequences, focusing on un-\nbounded motions in time.\nMany methods for unconstrained motion synthesis are\noften dominated by actions such as walking and running.\nIn contrast, our model is able to sample from more general,\nacyclic, pre-defined action categories, compatible with ac-\ntion recognition datasets. In this direction, [75] introduces\na Bayesian approach, where Hidden semi-Markov Models\nare used for jointly training generative and discriminative\nmodels. Similar to us, [75] shows that their generated mo-\ntions can serve as additional training data for action recog-\nnition. However, their generated sequences are pseudo-\n2\nFigure 2: Method overview: We illustrate the encoder (left) and the decoder (right) of our Transformer-based V AE model that generates\naction-conditioned motions. Given a sequence of body posesP1, . . . , PT and an action label a, the encoder outputs distribution parameters\non which we define a KL loss (LKL). We use extra learnable tokens per action (µtoken\na and Σtoken\na ) as a way to obtain µ and Σ from the\nTransformer encoder. Using µ and Σ, we sample the motion latent representation z ∈ M. The decoder takes the latent vector z, an action\nlabel a, and a duration T as input. The action determines the learnable btoken\na additive token, and the duration determines the number of\npositional encodings (PE) to input to the decoder. The decoder outputs the whole sequence bP1, . . . ,bPT against which the reconstruction\nloss LP is computed. In addition, we compute vertices with a differentiable SMPL layer to define a vertex loss ( LV ). For training z is\nobtained as the output of the encoder; for generation it is randomly sampled from a Gaussian distribution.\nlabelled with actions according to the discriminator classifi-\ncation results. On the other hand, our conditional model can\nsynthesize motions in a controlled way, e.g. balanced train-\ning set. Most similar to our work is Action2Motion [21],\na per-frame V AE on actions, using a GRU-based architec-\nture. Our sequence-level V AE latent space, in conjunction\nwith the Transformer-based design provides significant ad-\nvantages, as shown in our experiments.\nOther recent works [25, 72] use normalizing flows to ad-\ndress human motion estimation and generation problems.\nSeveral works [29, 36, 67] learn a motion manifold, and use\nit for motion denoising, which is one of our use cases.\nThere is also a significant graphics literature on the topic,\nwhich tends to focus on animator control. See, for exam-\nple, [27] on learning motion matching and [40] on charac-\nter animation. Most relevant here are the phase-functioned\nneural networks [28] and neural state machines [61]. Both\nexploit the notion of actions being driven by the phase of\na sinusoidal function. This is related to the idea of posi-\ntional encoding, but unlike our approach, their methods re-\nquire manual labor to segment actions and build these phase\nfunctions.\nMonocular human motion estimation. Motion estima-\ntion from videos [35, 38, 47] has recently made significant\nprogress but is beyond our scope. In this work, we adopt\nVIBE [38] to obtain training motion sequences from action-\nlabelled video datasets.\nTransformer V AEs.Recent successes of Transformers in\nlanguage tasks has increased interest in attention-based neu-\nral network models. Several works use Transformers in\nconjunction with generative V AE training. Particular exam-\nples include story generation [15], sentiment analysis [10],\nresponse generation [44], and music generation [33]. The\nwork of [33] learns latent embeddings per timeframe, while\n[10] averages the hidden states to obtain a single latent code.\nOn the other hand, [15] performs attention averaging to pool\nover time. In contrast to these works, we adopt learnable to-\nkens as in [12, 13] to summarize the input into a sequence-\nlevel embedding.\n3. Action-Conditioned Motion Generation\nProblem definition. Actions defined by body-motions can\nbe characterized by the rotations of body parts, independent\nof identity-specific body shape. To be able to generate mo-\ntions with actors of different morphology, it is desirable to\ndisentangle the pose and the shape. Consequently, without\nloss of generality, we employ the SMPL body model [46],\nwhich is a disentangled body representation (similar to re-\ncent models [53, 55, 58, 68]). Ignoring shape, our goal,\nis then to generate a sequence of pose parameters. More\nformally, given an action label a (from a set of predefined\naction categories a ∈ A) and a duration T, we generate\na sequence of body poses R1, . . . , RT and a sequence of\ntranslations of the root joint represented as displacements,\nD1, . . . , DT (with Dt ∈ R3, ∀t ∈ {1, . . . , T}).\nMotion representation. SMPL pose parameters per-frame\nrepresent 23 joint rotations in the kinematic tree and one\nglobal rotation. We adopt the continuous 6D rotation repre-\nsentation for training [76], making Rt ∈ R24×6. Let Pt be\n3\nthe combination of Rt and Dt, representing the pose and lo-\ncation of the body in a single frame,t. The full motion is the\nsequence P1, . . . , PT . Given a generator output posePt and\nany shape parameter, we can obtain body mesh vertices (Vt)\nand body joint coordinates (Jt) differentiably using [46].\n3.1. Conditional Transformer V AE for Motions\nWe employ a conditional variational autoencoder\n(CV AE) model [60] and input the action category infor-\nmation to both the encoder and the decoder. More specifi-\ncally, our model is an action-conditioned Transformer V AE\n(ACTOR), whose encoder and decoder consist of Trans-\nformer layers (see Figure 2 for an overview).\nEncoder. The encoder takes an arbitrary-length sequence\nof poses, and an action label a as input, and outputs distri-\nbution parameters µ and Σ of the motion latent space. Us-\ning the reparameterization trick [37], we sample from this\ndistribution a latent vector z ∈ M with M ⊂ Rd. All\nthe input pose parameters (R) and translations (D) are first\nlinearly embedded into a Rd space. As we embed arbitrary-\nlength sequences into one latent space (sequence-level em-\nbedding), we need to pool the temporal dimension. In other\ndomains, a [class] token has been introduced for pool-\ning purposes, e.g., in NLP with BERT [12] and more re-\ncently in computer vision with ViT [13]. Inspired by this\napproach, we similarly prepend the inputs with learnable to-\nkens, and only use the corresponding encoder outputs as a\nway to pool the time dimension. To this end, we include two\nextra learnable parameters per action, µtoken\na and Σtoken\na ,\nwhich we called “distribution parameter tokens”. We ap-\npend the embedded pose sequences to these tokens. The\nresulting Transformer encoder input is the summation with\nthe positional encodings in the form of sinusoidal functions.\nWe obtain the distribution parametersµ and Σ by taking the\nfirst two outputs of the encoder corresponding to the distri-\nbution parameter tokens (i.e., discarding the rest).\nDecoder. Given a single latent vector z and an action la-\nbel a, the decoder generates a realistic human motion for a\ngiven duration in one shot (i.e., not autoregressive).\nWe use a Transformer decoder model where we feed\ntime information as a query (in the form of T sinusoidal\npositional encodings), and the latent vector combined with\naction information, as key and value. To incorporate the ac-\ntion information, we simply add a learnable bias btoken\na to\nshift the latent representation to an action-dependent space.\nThe Transformer decoder outputs a sequence of T vectors\nin Rd from which we obtain the final poses bP1, . . . ,bPT fol-\nlowing a linear projection. A differentiable SMPL layer is\nused to obtain vertices and joints given the pose parameters\nas output by the decoder.\n3.2. Training\nWe define several loss terms to train our model and\npresent an ablation study in Section 4.2.\nReconstruction loss on pose parameters (LP ). We use an\nL2 loss between the ground-truth posesP1, . . . , PT , and our\npredictions bP1, . . . ,bPT as LP = PT\nt=1∥Pt − bPt∥2\n2. Note\nthat this loss contains both the SMPL rotations and the root\ntranslations. When we experiment by discarding the trans-\nlations, we break this term into two: LR and LD, for rota-\ntions and translations, respectively.\nReconstruction loss on vertex coordinates (LV ). We feed\nthe SMPL poses Pt and bPt to a differentiable SMPL layer\n(without learnable parameters) with a mean shape (i.e.,\nβ = ⃗0) to obtain the root-centered vertices of the mesh Vt\nand bVt. We define an L2 loss by comparing to the ground-\ntruth vertices Vt as LV = PT\nt=1∥Vt − bVt∥2\n2 . We further\nexperiment with a loss LJ on a more sparse set of points\nsuch as joint locations bJt obtained through the SMPL joint\nregressor. However, as will be shown in Section 4.2, we do\nnot include this term in the final model.\nKL loss ( LKL). As in a standard V AE, we regularize the\nlatent space by encouraging it to be similar to a Gaussian\ndistribution with µ the null vector and Σ the identity ma-\ntrix. We minimize the Kullback–Leibler (KL) divergence\nbetween the encoder distribution and this target distribution.\nThe resulting total loss is defined as the summation of\ndifferent terms: L = LP + LV + λKLLKL. We empiri-\ncally show the importance of weighting with λKL (equiv-\nalent to the β term in β-V AE [26]) in our experiments to\nobtain a good trade-off between diversity and realism (see\nSection A.1 of the appendix). The remaining loss terms are\nsimply equally weighed, further improvements are poten-\ntially possible with tuning. We use the AdamW optimizer\nwith a fixed learning rate of 0.0001. The minibatch size\nis set to 20 and we found that the performance is sensitive\nto this hyperparameter (see Section A.2 of the appendix).\nWe train our model for 2000, 5000 and 1000 epochs on\nNTU-13, HumanAct12 and UESTC datasets, respectively.\nOverall, more epochs produce improved performance, but\nwe stop training to retain a low computational cost. Note\nthat to allow faster iterations, for ablations on loss and ar-\nchitecture, we train our models for 1000 epochs on NTU-13\nand 500 epochs on UESTC. The remaining implementation\ndetails can be found in Section C of the appendix.\n4. Experiments\nWe first introduce the datasets and performance mea-\nsures used in our experiments (Section 4.1). Next, we\npresent an ablation study (Section 4.2) and compare to pre-\nvious work (Section 4.3). Then, we illustrate use cases in\naction recognition (Sections 4.4). Finally, we provide qual-\nitative results and discuss limitations (Section 4.5).\n4.1. Datasets and evaluation metrics\nWe use three datasets originally proposed for action\nrecognition, mainly for skeleton-based inputs. Each dataset\nis temporally trimmed around one action per sequence.\nNext, we briefly describe them.\n4\nUESTC NTU-13\nLoss FID tr↓ FIDtest↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal 2.93±0.26 2.79±0.29 98.8±0.1 33.34±0.32 14.16±0.06 0.02±0.00 99.8±0.0 7.07±0.02 2.27±0.01\nLJ 3M∗ 3M∗ 3.3±0.2 267.68±346.06 153.62±50.62 0.49±0.00 93.6±0.2 7.04±0.04 2.12±0.01\nLR 292.54±113.35 316.29±26.05 42.4±1.7 23.16±0.47 14.37±0.08 0.23±0.00 95.4±0.2 7.08±0 .04 2.18±0.02\nLV 4M∗ 4M∗ 2.7±0.2 314.66±476.18 169.49±27.90 0.25±0.00 95.8±0.3 7.08±0.04 2.07±0.01\nLR +LV 20.49±2.31 23.43±2.20 91.1±0.3 31.96±0.36 14.66±0.03 0.19±0.00 96.2±0.2 7.09±0.04 2.08±0.01\nTable 1: Reconstruction loss: We define the loss on the SMPL pose parameters which represent the rotations in the kinematic tree ( LR),\ntheir joint coordinates (LJ), as well as vertex coordinates (LV ). We show that constraining both rotations and vertex coordinates is critical\nto obtain smooth motions. In particular, coordinate-based losses alone do not converge to a meaningful solution on UESTC (*). → means\nmotions are better when the metric is closer to real.\nNTU RGB+D dataset [45, 59]. To be able to compare to\nthe work of [21], we use their subset of 13 action categories.\n[21] provides SMPL parameters obtained through VIBE\nestimations. Their 3D root translations, obtained through\nmulti-view constraints, are not publicly available, therefore\nwe use their approximately origin-centered version. We re-\nfer to this data as NTU-13 and use it for training.\nHumanAct12 dataset [21]. Similarly, we use this data for\nstate-of-the-art comparison. HumanAct12 is adapted from\nthe PHSPD dataset [77] that releases SMPL pose param-\neters and root translations in camera coordinates for 1191\nvideos. HumanAct12 temporally trims the videos, anno-\ntates them into 12 action categories, and only provides their\njoint coordinates in a canonical frame. We also process the\nSMPL poses to align them to the frontal view.\nUESTC dataset [32]. This recent dataset consists of 25K\nsequences across 40 action categories (mostly exercises,\nand some represent cyclic movements). To obtain SMPL\nsequences, we apply VIBE on each video and select the per-\nson track that corresponds best to the Kinect skeleton pro-\nvided in case there are multiple people. We use all 8 static\nviewpoints (we discard the rotating camera) and canoni-\ncalize all bodies to the frontal view. We use the official\ncross-subject protocol to separate train and test splits, in-\nstead of the cross-view protocols since generating different\nviewpoints is trivial for our model. This results in 10650\ntraining sequences that we use for learning the generative\nmodel, as well as the recognition model: the effective di-\nversity of this set can be seen as 33 sequences per action\non average (10K divided by 8 views, 40 actions). The re-\nmaining 13350 sequences are used for testing. Since the\nprotocols on NTU-13 and HumanAct12 do not provide test\nsplits, we rely on UESTC for recognition experiments.\nEvaluation metrics. We follow the performance measures\nemployed in [21] for quantitative evaluations. We mea-\nsure FID, action recognition accuracy, overall diversity, and\nper-action diversity (referred to as multimodality in [21]).\nFor all these metrics, a pretrained action recognition model\nis used, either for extracting motion features to compute\nFID, diversity, and multimodality; or directly the accuracy\nof recognition. For experiments on NTU-13 and Human-\nAct12, we directly use the provided recognition models of\n[21] that operate on joint coordinates. For UESTC, we train\nour own recognition model based on pose parameters ex-\npressed as 6D rotations (we observed that the joint-based\nmodels of [21] are sensitive to global viewpoint changes).\nWe generate sets of sequences 20 times with different ran-\ndom seeds and report the average together with the confi-\ndence interval at 95%. We refer to [21] for further details.\nOne difference in our evaluation is the use of average shape\nparameter (β = ⃗0) when obtaining joint coordinates from\nthe mesh for both real and generated sequences. Note also\nthat [21] only reports FID score comparing to the training\nsplit (FIDtr), since NTU-13 and HumanAct12 datasets do\nnot provide test splits. On UESTC, we additionally provide\nan FID score on the test split as FIDtest, which we rely most\non to make conclusions.\n4.2. Ablation study\nWe first ablate several components of our approach in a\ncontrolled setup, studying the loss and the architecture.\nLoss study. Here, we investigate the influence of the recon-\nstruction loss formulation when using the parametric SMPL\nbody model in our V AE. We first experiment with using (i)\nonly the rotation parameters LR, (ii) only the joint coor-\ndinates LJ, (iii) only the vertex coordinates LV , and (iv)\nthe combination LR + LV . Here, we initially discard the\nroot translation to only assess the pose representation. Note\nthat for representing the rotation parameters, we use the 6D\nrepresentation from [76] (further studies on losses with dif-\nferent rotation representations can be found in Section A.4\nof the appendix). In Table 1, we observe that a single loss\nis not sufficient to constrain the problem, especially losses\non the coordinates do not converge to a meaningful solu-\ntion on UESTC. On NTU-13, qualitatively, we also observe\ninvalid body shapes since joint locations alone do not fully\nconstrain the rotations along limb axes. We provide exam-\nples in our qualitative analysis. We conclude that using a\ncombined loss significantly improves the results, constrain-\ning the pose space more effectively. We further provide an\nexperiment on the influence of the weight parameter λKL\ncontrolling the KL divergence loss termLKL in Section A.1\nof the appendix and note its importance to obtain high di-\nversity performance.\nRoot translation. Since we estimate the 3D human body\nmotion from a monocular camera, obtaining the 3D trajec-\ntory of the root joint is not trivial for real training sequences,\n5\nUESTC NTU-13\nArchitecture FID tr↓ FIDtest↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal 2.93±0.26 2.79±0.29 98.8±0.1 33.34±0.32 14.16±0.06 0.02±0.00 99.8±0.0 7.07±0.02 2.27±0.01\nFully connected 562.09±48.12 548.13±38.34 10.5±0.5 12.96±0.11 10.87±0.05 0.47±0.00 88.7±0.6 6.93±0.03 3.05±0.01\nGRU 25.96±3.02 27.08±2.98 87.3±0.4 30.66±0.33 15.24±0.08 0.28±0.00 94.8±0.2 7.08±0.04 2.20±0.01\nTransformer 20.49±2.31 23.43±2.20 91.1±0.3 31.96±0.36 14.66±0.03 0.19±0.00 96.2±0.2 7.09±0.04 2.08±0.01\na) w/ autoreg. decoder55.75±2.62 60.10±4.87 88.4±0.6 33.46±0.69 10.62±0.10 2.62±0.01 88.0±0.5 6.80±0.03 1.76±0.01\nb) w/outµtokena ,Σtokena 27.46±3.43 31.37±3.04 86.2±0.4 31.82±0.38 15.71±0.12 0.26±0.00 94.7±0.2 7.09±0.03 2.15±0.01\nc) w/outbtokena 24.38±2.37 28.52±2.55 89.4±0.7 32.11±0.33 14.52±0.09 0.16±0.00 96.2±0.2 7.08±0.04 2.19±0.02\nTable 2: Architecture: We compare various architectural designs, such as the encoder and the decoder of the V AE, and different compo-\nnents of the Transformer model, on both NTU-13 and UESTC datasets.\nand is subject to depth ambiguity. We assume a fixed focal\nlength and approximate the distance from the camera based\non the ratio between the 3D body height and the 2D pro-\njected height. Similar to [65], we observe reliable transla-\ntion in xy image plane, but considerable noise in z depth.\nNevertheless, we still train with this type of data and vi-\nsualize generated examples in Figure 3 with and without\nthe loss on translation LD. Certain actions are defined by\ntheir trajectory (e.g., ‘Left Stretching’) and we are able to\ngenerate the semantically relevant translations despite noisy\ndata. Compared to the real sequences, we observe much\nless noise in our generated sequences (see the supplemental\nvideo at [57]).\nArchitecture design. Next, we ablate several architectural\nchoices. The first question is whether an attention-based\ndesign (i.e., Transformer) has advantages over the more\nwidely used alternatives such as a simple fully-connected\nautoencoder or a GRU-based recurrent neural network. In\nTable 2, we see that our Transformer model outperforms\nboth fully-connected and GRU encoder-decoder architec-\ntures on two datasets by a large margin. In contrast to\nthe fully-connected architecture, we are also able to han-\ndle variable-length sequences. We further note that our\nsequence-level decoding strategy is key to obtain an im-\nprovement with Transformers, as opposed to an autoregres-\nsive Transformer decoder as in [66] (Table 2, a). At training\ntime, the autoregressive model uses teacher forcing, i.e., us-\ning the ground-truth pose for the previous frame. This cre-\nates a gap with test time, where we observed poor autoen-\ncoding reconstructions such as decoding a left-hand waving\nencoding into a right-hand waving.\nWe also provide a controlled experiment by changing\ncertain blocks of our Transformer V AE. Specifically, we\nremove the µtoken\na and Σtoken\na distribution parameter to-\nkens and instead obtain µ and Σ by averaging the outputs\nof the encoder, followed by two linear layers (Table 2, b).\nThis results in considerable drop in performance. More-\nover, we investigate the additivebtoken\na token and replace it\nwith a one-hot encoding of the action label concatenated to\nthe latent vector, followed by a linear projection (Table 2,\nc). Although this improves a bit the results on the NTU-13\ndataset, we observe a large decrease in performance on the\nUESTC dataset which has a larger number of action classes.\nBased on an architectural ablation of the number of\nLeft Stretching Rope Skipping\nwith withwithoutwithout\nForward Lunging\nwithwithout\nLeft Stretching Rope Skipping\nwith withwithoutwithout\nForward Lunging\nwithwithout\nFigure 3: Generating the 3D root translation:Despite our model\nlearning from noisy 3D trajectories, we show that our generations\nare smooth and they capture the semantics of the action. Examples\nare provided from the UESTC dataset for translations in x (‘Left\nStretching’), y (Rope Skipping), and z (‘Forward Lugging’) with\nand without the loss on the root displacement LD.\nFigure 4: Generating variable-length sequences: We evaluate\nthe capability of the models trained on UESTC with (left) fixed-\nsize 60 frames and (right) variable-size between [60, 100] frames\non generating various durations. We report accuracy and FID met-\nrics. For the fixed model, we observe that the best performance\nis when tested at the seen duration of 60, but over 85% accuracy\nis retained even at ranges between [40, 120] frames. The perfor-\nmance is overall improved when the model has previously seen\nduration variations in training; there is a smaller drop in perfor-\nmance beyond the seen range (denoted with dashed lines).\nTransformer layers (see Section A.3 of the appendix), we\nset this parameter to 8.\nTraining with sequences of variable durations. A key\nadvantage of sequence-modeling with architectures such as\nTransformers is to be able to handle variable-length mo-\ntions. At generation time, we control how long the model\nshould synthesize, by specifying a sequence of positional\nencodings to the decoder. We can trivially generate more\ndiversity by synthesizing sequences of different durations.\nHowever, so far we have trained our models with fixed-size\ninputs, i.e., 60 frames. Here, we first analyze whether a\nfixed-size trained model can directly generate variable sizes.\nThis is presented in Figure 4 (left). We plot the performance\n6\nNTU-13 HumanAct12\nMethod FID tr↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal [21] 0.03±0.00 99.9±0.1 7.11±0.05 2.19±0.03 0.09±0.01 99.7±0.1 6.85±0.05 2.45±0.04\nReal* 0.02±0.00 99.8±0.0 7.07±0.02 2.25±0.01 0.02±0.00 99.4±0.0 6.86±0.03 2.60±0.01\nCondGRU ([21]†) 28.31±0.14 7.8±0.1 3.66±0.02 3.58±0.03 40.61±0.14 8.0±0.2 2.38±0.02 2.34±0.04\nTwo-stage GAN [9] ([21]†) 13.86±0.09 20.2±0.3 5.33±0.04 3.49±0.03 10.48±0.09 42.1±0.6 5.96±0.05 2.81±0.04\nAct-MoCoGAN [62] ([21]†) 2.72±0.02 99.7±0.1 6.92±0.06 0.91±0.01 5.61±0.11 79.3±0.4 6.75±0.07 1.06±0.02\nAction2Motion [21] 0.33±0.01 94.9±0.1 7.07±0.04 2.05±0.03 2.46±0.08 92.3±0.2 7.03±0.04 2.87±0.04\nACTOR (ours) 0.11±0.00 97.1±0.2 7.08±0.04 2.08±0.01 0.12±0.00 95.5±0.8 6.84±0.03 2.53±0.02\nTable 3: State-of-the-art comparison: We compare to the recent work of [21] on the NTU-13 and HumanAct12 datasets. Note that due\nto differences in implementation (e.g., random sampling, using zero shape parameter), our metrics for the ground truth real data (Real*)\nare slightly different than the ones reported in their paper. The performance improvement with our Transformer-based model shows a clear\ngap from Action2Motion. † Baselines implemented by [21].\nover several sets of generations of different lengths between\n40 and 120 frames (with a step size of 5). Since our recog-\nnition model used for evaluation metrics is trained on fixed-\nsize 60-frame inputs, we naturally observe performance de-\ncrease outside of this length. However, the accuracy still\nremains high which indicates that our model is already ca-\npable of generating diverse durations.\nNext, we train our generative model with variable-length\ninputs by randomly sampling a sequence between 60 and\n100 frames. However, simply training this way from ran-\ndom weight initialization converges to a poor solution, lead-\ning all generated motions to be frozen in time. We address\nthis by pretraining at 60-frame fixed size and finetuning at\nvariable sizes. We see in Figure 4 (right) that the perfor-\nmance is greatly improved with this model.\nFurthermore, we investigate how the generations longer\nor shorter than their average durations behave. We observe\nqualitatively that shorter generations produce partial actions\ne.g., picking up without reaching the floor, and longer gen-\nerations slow down somewhat non-uniformly in time. We\nrefer to the supplemental video [57] for qualitative results.\n4.3. Comparison to the state of the art\nAction2Motion [21] is the only prior work that gener-\nates action-conditioned motions. We compare to them in\nTable 3 on their NTU-13 and HumanAct12 datasets. On\nboth datasets, we obtain significant improvements over this\nprior work that uses autoregressive GRU blocks, as well\nas other baselines implemented by [21] by adapting other\nworks [9, 62]. The improvements over [21] can be ex-\nplained mainly by removing autoregression and adding the\nproposed learnable tokens (Table 2). Note that our GRU\nimplementation obtains similar performance as [21], while\nusing the same hyperparameters as the Transformer. In ad-\ndition to the quantitative performance improvement, mea-\nsured with recognition models based on joint coordinates,\nour model can directly output human meshes, which can\nfurther be diversified with varying the shape parameters.\n[21] instead applies an optimization step to fit SMPL mod-\nels on their generated joint coordinates, which is typically\nsubstantially slower than a neural network forward pass.\nTest accuracy (%)\nRealorig Realdenoised\nRealorig 91.8 93.2\nRealdenoised 83.8 97.0\nRealinterpolated 77.6 93.9\nGenerated 80.7 97.0\nRealorig + Generated 91.9 98.3\nTable 4: Action recognition: We employ a standard architecture\n(ST-GCN [70]) and perform action recognition experiments using\nseveral sets of training data on the UESTC cross-subject proto-\ncol [32]. Training only with generated samples obtains 80% ac-\ncuracy on the real test set which is another indication our action-\nconditioning performs well. Nevertheless, we observe a domain\ngap between generated and real samples, mainly due to the noise\npresent in the real data. We show that simply by encoding-\ndecoding the test sequences, we observe a denoising effect, which\nin turn shows better performance. However, one should note\nthat the last-column experiments are not meant to improve perfor-\nmance in the benchmark since it uses the action label information.\n4.4. Use cases in action recognition\nIn this section, we test the limits of our approach by illus-\ntrating the benefits of our generative model and our learned\nlatent representation for the skeleton-based action recogni-\ntion task. We adopt a standard architecture, ST-GCN [70],\nthat employs spatio-temporal graph convolutions to classify\nactions. We show that we can use our latent encoding for\ndenoising motion estimates and our generated sequences as\ndata augmentation to action recognition models.\nUse case I: Human motion denoising. In the case when\nour motion data source relies on monocular motion esti-\nmation such as [38], the training motions remain noisy.\nWe observe that by simply encoding-decoding the real mo-\ntions through our learned embedding space, we obtain much\ncleaner motions. Since it is difficult to show motion quality\nresults on static figures, we refer to our supplemental video\nat [57] to see this effect. We measure the denoising capabil-\nity of our model through an action recognition experiment\nin Table 4. We change both the training and test set motions\nwith the encoded-decoded versions. We show improved\nperformance when trained and tested on Real denoised mo-\n7\nFigure 5: Qualitative results: We illustrate the diversity of our generations on the ‘Throw’ action from NTU-13 by showing 3 sequences.\nThe horizontal axis represent the time axis and 20 equally spaced frames are visualized out of 60-frame generations. We demonstrate that\nour model is capable of generating different ways of performing a given action. More results can be found in Section B of the appendix\nand the supplemental video at [57].\nFigure 6: Data augmentation: We show the benefit of augment-\ning the real data with our generative model (real+gen), especially\nat low-data regime. We have limited gains when the real data is\nsufficiently large.\ntions (97.0%) compared to Real orig (91.8). Note that this\nresult on its own is not sufficient for this claim, but is only\nan indication since our decoder might produce less diversity\nthan real motions. Moreover, the action label is given at de-\nnoising time. We believe that such denoising can be ben-\neficial in certain scenarios where the action is known, e.g.,\nocclusion or missing markers during MoCap collection.\nUse case II: Augmentation for action recognition. Next,\nwe augment the real training data (Realorig), by adding gen-\nerated motions to the training. We first measure the action\nrecognition performance without using real sequences. We\nconsider interpolating existing Real orig motions that fall\nwithin the same action category in our embedding space\nto create intra-class variations (Real interpolated). We then\nsynthesize motions by sampling noise vectors conditioned\non each action category (Generated). Table 4 summarizes\nthe results. Training only on synthetic data performs 80.7%\non the real test set, which is promising. However, there\nis a domain gap between the noisy real motions and our\nsmooth generations. Consequently, adding generated mo-\ntions to real training only marginally improves the perfor-\nmance. In Figure 6, we investigate whether the augmented\ntraining helps for low-data regimes by training at several\nfractions of the data. In each minibatch we equally sample\nreal and generated motions. However, in theory we have\naccess to infinitely many generations. We see that the im-\nprovement is more visible at low-data regime.\n4.5. Qualitative results\nIn Figure 5, we visualize several examples from our gen-\nerations. We observe a great diversity in the way a given\naction is performed. For example, the ‘Throw’ action is\nperformed with left or right hand. We notice that the model\nkeeps the essence of the action semantics while changing\nnuances (angles, speed, phase) or action-irrelevant body\nparts. We refer to the supplemental video at [57] and Sec-\ntion B of the appendix for further qualitative analyses.\nOne limitation of our model is that the maximum du-\nration it can generate depends on computational resources\nsince we output all the sequence at once. Moreover, the\nactions are from a predefined set. Future work will ex-\nplore open-vocabulary actions, which might become pos-\nsible with further progress in 3D motion estimation from\nunconstrained videos.\n5. Conclusions\nWe presented a new Transformer-based V AE model to\nsynthesize action-conditioned human motions. We pro-\nvided a detailed analysis to assess different components\nof our proposed approach. We obtained state-of-the-art\nperformance on action-conditioned motion generation, sig-\nnificantly improving over prior work. Furthermore, we\nexplored various use cases in motion denoising and ac-\ntion recognition. One especially attractive property of our\nmethod is that it operates on a sequence-level latent space.\nFuture work can therefore exploit our model to impose pri-\nors on motion estimation or action recognition problems.\nAcknowledgements. This work was granted access to the HPC resources\nof IDRIS under the allocation 2021-101535 made by GENCI. The authors\nwould like to thank Mathieu Aubry and David Picard for helpful feedback,\nChuan Guo and Shihao Zou for their help with Action2Motion details.\nDisclosure: MJB has received research funds from Adobe, Intel, Nvidia,\nFacebook, and Amazon. While MJB is a part-time employee of Amazon,\nhis research was performed solely at, and funded solely by, Max Planck.\nMJB has financial interests in Amazon, Datagen Technologies, and Mesh-\ncapade GmbH.\n8\nReferences\n[1] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and\nSonghwai Oh. Text2Action: Generative adversarial synthe-\nsis from language to action. In International Conference on\nRobotics and Automation (ICRA), pages 5915–5920, 2018. 2\n[2] Chaitanya Ahuja and Louis-Philippe Morency. Lan-\nguage2Pose: Natural language grounded pose forecasting.\nIn 2019 International Conference on 3D Vision (3DV), pages\n719–728, 2019. 2\n[3] Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Struc-\ntured prediction helps 3D human motion modelling. In In-\nternational Conference on Computer Vision (ICCV) , pages\n7143–7152, 2019. 1, 2\n[4] Norman Badler. Temporal Scene Analysis: Conceptual De-\nscriptions of Object Movements . PhD thesis, University of\nToronto, 1975. 1\n[5] Norman I. Badler, Cary B. Phillips, and Bonnie Lynn Web-\nber. Simulating Humans: Computer Graphics Animation and\nControl. Oxford University Press, Inc., New York, NY , USA,\n1993. 1, 2\n[6] Emad Barsoum, John Kender, and Zicheng Liu. HP-GAN:\nProbabilistic 3D human motion prediction via GAN. InCon-\nference on Computer Vision and Pattern Recognition Work-\nshops (CVPRW), pages 1499–149909, 2018. 1, 2\n[7] Richard Bowden. Learning statistical models of human mo-\ntion. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), Workshop on Human Modeling, Analysis and\nSynthesis, 2000. 2\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2020. 12\n[9] Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung\nTang. Deep video generation, prediction and completion of\nhuman action sequences. In European Conference on Com-\nputer Vision (ECCV), pages 374–390, 2018. 7\n[10] Xingyi Cheng, Weidi Xu, Taifeng Wang, Wei Chu, Weipeng\nHuang, Kunlong Chen, and Junfeng Hu. Variational semi-\nsupervised aspect-term sentiment analysis via transformer.\nIn Computational Natural Language Learning (CoNLL) ,\npages 961–969, 2019. 3\n[11] Enric Corona, Albert Pumarola, Guillem Aleny `a, and\nFrancesc Moreno-Noguer. Context-aware human motion\nprediction. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 6990–6999, 2020. 2\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In North American\nChapter of the Association for Computational Linguistics\n(NAACL), pages 4171–4186, 2019. 3, 4, 12\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. InInternational Con-\nference on Learning Representations (ICLR), 2021. 3, 4\n[14] Yinglin Duan, Tianyang Shi, Zhengxia Zou, Yenan Lin, Zhe-\nhui Qian, Bohan Zhang, and Yi Yuan. Single-shot motion\ncompletion with transformer. arXiv:2103.00776, 2021. 2\n[15] Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen\nDong, and Changyou Chen. Transformer-based conditional\nvariational autoencoder for controllable story generation.\narXiv:2101.00828, 2021. 3\n[16] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-\ntendra Malik. Recurrent network models for human dynam-\nics. In International Conference on Computer Vision (ICCV),\npages 4346–4354, 2015. 1\n[17] Robert P. Futrelle and Glen C. Speckert. Extraction of mo-\ntion data by interactive processing. In Conference Pattern\nRecognition and Image Processing (CP) , pages 405–408,\n1978. 2\n[18] Aphrodite Galata, Neil Johnson, and David Hogg. Learning\nvariable length markov models of behaviour. Computer Vi-\nsion and Image Understanding (CVIU) , 81:398–413, 2001.\n2\n[19] Dariu M. Gavrila. The visual analysis of human move-\nment: A survey. Computer Vision and Image Understanding\n(CVIU), 73:82–98, 1999. 2\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin Neural Information Processing Systems (NeurIPS) , vol-\nume 27, 2014. 2\n[21] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao\nSun, Annan Deng, Minglun Gong, and Li Cheng. Ac-\ntion2motion: Conditioned generation of 3d human mo-\ntions. In ACM International Conference on Multimedia\n(ACMMM), pages 2021–2029, 2020. 2, 3, 5, 7, 12\n[22] Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe\nYearsley, and Taku Komura. A recurrent variational autoen-\ncoder for human motion synthesis. InBritish Machine Vision\nConference (BMVC), pages 119.1–119.12, 2017. 1, 2\n[23] F ´elix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and\nC. Pal. Robust motion in-betweening. ACM Transactions on\nGraphics (TOG), 39:60:1 – 60:12, 2020. 2\n[24] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (GELUs). arXiv:1606.08415, 2016. 12\n[25] Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow.\nMoGlow: Probabilistic and controllable motion synthesis\nusing normalising flows. ACM Transactions on Graphics\n(TOG), 39(6), 2020. 3\n[26] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,\nXavier Glorot, Matt Botvinick, Shakir Mohamed, and\nAlexander Lerchner. beta-V AE: Learning basic visual con-\ncepts with a constrained variational framework. In Inter-\nnational Conference on Learning Representations (ICLR) ,\n2017. 4\n[27] Daniel Holden, Oussama Kanoun, Maksym Perepichka, and\nTiberiu Popa. Learned motion matching. ACM Transactions\non Graphics (TOG), 39:53:1 – 53:12, 2020. 3\n[28] Daniel Holden, Taku Komura, and Jun Saito. Phase-\nfunctioned neural networks for character control. ACM\nTransactions on Graphics (TOG), 36(4), 2017. 1, 3\n[29] Daniel Holden, Jun Saito, and Taku Komura. A deep learning\nframework for character motion synthesis and editing. ACM\nTransactions on Graphics (TOG), 35(4), 2016. 3\n9\n[30] Catalin Ionescu, Fuxin Li, and Cristian Sminchisescu. La-\ntent structured models for human pose estimation. In In-\nternational Conference on Computer Vision (ICCV) , pages\n2220–2227, 2011. 2\n[31] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and pre-\ndictive methods for 3D human sensing in natural environ-\nments. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI), 36(7):1325–1339, 2014. 2\n[32] Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao\nShen, and Wei-Shi Zheng. A large-scale RGB-D database\nfor arbitrary-view human action recognition. In ACM\nInternational Conference on Multimedia (ACMMM) , page\n1510–1518, 2018. 2, 5, 7\n[33] Junyan Jiang, Gus G. Xia, Dave B. Carlton, Chris N. An-\nderson, and Ryan H. Miyakawa. Transformer V AE: A hi-\nerarchical model for structure-aware and interpretable music\nrepresentation learning. In IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages\n516–520, 2020. 3\n[34] Justin Johnson, Nikhila Ravi, Jeremy Reizenstein, David\nNovotny, Shubham Tulsiani, Christoph Lassner, and Steve\nBranson. Accelerating 3D deep learning with PyTorch3D.\nIn SIGGRAPH Asia 2020 Courses, 2020. 12\n[35] Angjoo Kanazawa, Jason Y . Zhang, Panna Felsen, and Ji-\ntendra Malik. Learning 3D human dynamics from video.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 5607–5616, 2019. 3\n[36] Seong Uk Kim, Hanyoung Jang, and Jongmin Kim. Human\nmotion denoising using attention-based bidirectional recur-\nrent neural network. In SIGGRAPH Asia, 2019. 3\n[37] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In International Conference on Learning Rep-\nresentations (ICLR), 2014. 2, 4\n[38] Muhammed Kocabas, Nikos Athanasiou, and Michael J.\nBlack. Vibe: Video inference for human body pose and\nshape estimation. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 5252–5262, 2020. 2, 3,\n7\n[39] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun\nWang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.\nDancing to music. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2019. 2\n[40] Kyungho Lee, Seyoung Lee, and Jehee Lee. Interactive char-\nacter animation by learning multi-objective control. ACM\nTransactions on Graphics (TOG), 2018. 1, 3\n[41] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang,\nSanja Fidler, and Hao Li. Learning to generate diverse dance\nmotions with transformer. arXiv:2008.08171, 2020. 2\n[42] Ruilong Li, Shan Yang, David A. Ross, and Angjoo\nKanazawa. Ai choreographer: Music conditioned 3D dance\ngeneration with AIST++, 2021. 2\n[43] X. Lin and M. Amer. Human motion modeling using DV-\nGANs. arXiv:1804.10652, 2018. 2\n[44] Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, and\nPascale Fung. Variational transformers for diverse response\ngeneration. arXiv:2003.12738, 2020. 3\n[45] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,\nLing-Yu Duan, and Alex C. Kot. NTU RGB+D 120: A large-\nscale benchmark for 3D human activity understanding.IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI), pages 1–18, 2019. 5\n[46] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. ACM Transactions on Graphics (TOG),\n34(6), Oct. 2015. 1, 3, 4\n[47] Zhengyi Luo, S. Alireza Golestaneh, and Kris M. Kitani. 3D\nhuman motion estimation via motion compression and re-\nfinement. In Asian Conference on Computer Vision (ACCV),\n2020. 3\n[48] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael Black. AMASS: Archive of mo-\ntion capture as surface shapes. In International Conference\non Computer Vision (ICCV), pages 5441–5450, 2019. 2\n[49] Julieta Martinez, Michael J. Black, and Javier Romero. On\nhuman motion prediction using recurrent neural networks.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4674–4683, 2017. 1, 2\n[50] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view\nsynthesis. In European Conference on Computer Vision\n(ECCV), pages 405–421, 2020. 2\n[51] Dirk Ormoneit, Michael J. Black, Trevor Hastie, and Hedvig\nKjellstr¨om. Representing cyclic human motion using func-\ntional analysis. Image and Vision Computing, 23(14):1264–\n1276, 2005. 2\n[52] Joseph O’Rourke and Norman I. Badler. Model-based im-\nage analysis of human motion using constraint propagation.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence (TPAMI), PAMI-2(6):522–536, 1980. 2\n[53] Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black.\nSTAR: Sparse trained articulated human body regressor. In\nEuropean Conference on Computer Vision (ECCV) , pages\n598–613, 2020. 3\n[54] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-\ntin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py-\nTorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2019. 12\n[55] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3D hands, face,\nand body from a single image. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10967–\n10977, 2019. 3, 12\n[56] Dario Pavllo, David Grangier, and Michael Auli. QuaterNet:\nA quaternion-based recurrent model for human motion. In\nBritish Machine Vision Conference (BMVC), 2018. 2\n[57] Mathis Petrovich, Michael J. Black, and G ¨ul Varol. ACTOR\nproject page: Action-conditioned 3D human motion synthe-\nsis with Transformer V AE. https://imagine.enpc.\nfr/˜petrovim/actor. 1, 2, 6, 7, 8, 12\n[58] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\nEmbodied hands: Modeling and capturing hands and bod-\nies together. ACM Transactions on Graphics (TOG), 36(6),\n2017. 3\n[59] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNTU RGB+D: A large scale dataset for 3d human activity\nanalysis. In Conference on Computer Vision and Pattern\n10\nRecognition (CVPR), pages 1010–1019, 2016. 5\n[60] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning\nstructured output representation using deep conditional gen-\nerative models. In Advances in Neural Information Process-\ning Systems (NeurIPS), volume 28. Curran Associates, Inc.,\n2015. 4\n[61] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito.\nNeural state machine for character-scene interactions. ACM\nTransactions on Graphics (TOG), 38(6), 2019. 1, 3\n[62] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz. MoCoGAN: Decomposing motion and content for\nvideo generation. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1526–1535, 2018. 7\n[63] Carnegie Mellon University. CMU graphics lab motion cap-\nture database. http://mocap.cs.cmu.edu/. 2\n[64] Raquel Urtasun, David J. Fleet, and Neil D. Lawrence. Mod-\neling human locomotion with topologically constrained la-\ntent variable models. In Ahmed Elgammal, Bodo Rosen-\nhahn, and Reinhard Klette, editors, Human Motion – Under-\nstanding, Modeling, Capture and Animation, pages 104–118,\n2007. 2\n[65] G ¨ul Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zis-\nserman. Synthetic humans for action recognition from un-\nseen viewpoints. International Journal of Computer Vision\n(IJCV), page 2264–2287, 2021. 2, 6\n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neu-\nral Information Processing Systems (NeurIPS) , volume 30,\n2017. 6\n[67] He Wang, Edmond S. L. Ho, Hubert P. H. Shum, and Zhanx-\ning Zhu. Spatio-temporal manifold learning for human mo-\ntions via long-horizon modeling. IEEE Transactions on Vi-\nsualization and Computer Graphics (TVCG) , 27:216–227,\n2021. 3\n[68] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,\nWilliam T. Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. GHUM and GHUML: Generative 3D human shape\nand articulated pose models. In Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 6183–6192,\n2020. 3\n[69] Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, and\nDahua Lin. Convolutional sequence generation for skeleton-\nbased action synthesis. In International Conference on Com-\nputer Vision (ICCV), pages 4393–4401, 2019. 2\n[70] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\nral graph convolutional networks for skeleton-based action\nrecognition. In AAAI Conference on Artificial Intelligence ,\n2018. 7\n[71] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for\ndiverse human motion prediction. In Andrea Vedaldi, Horst\nBischof, Thomas Brox, and Jan-Michael Frahm, editors, Eu-\nropean Conference on Computer Vision (ECCV), pages 346–\n364, 2020. 1, 2\n[72] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu,\nWilliam T. Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. Weakly supervised 3d human pose and shape re-\nconstruction with normalizing flows. In European Confer-\nence on Computer Vision (ECCV), pages 465–481, 2020. 3\n[73] Y . Zhang, Michael J. Black, and Siyu Tang. Per-\npetual motion: Generating unbounded human motion.\narXiv:2007.13886, 2020. 2\n[74] Yan Zhang, Michael J. Black, and Siyu Tang. We are more\nthan our joints: Predicting how 3D bodies move. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 3372–3382, 2021. 1, 2\n[75] Rui Zhao, Hui Su, and Qiang Ji. Bayesian adversarial human\nmotion synthesis. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 6224–6233, 2020. 2\n[76] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao\nLi. On the continuity of rotation representations in neural\nnetworks. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 3, 5, 12, 13\n[77] Shihao Zou, Xinxin Zuo, Yiming Qian, Sen Wang, Chi Xu,\nMinglun Gong, and Li Cheng. 3D human shape reconstruc-\ntion from a polarization image. In European Conference on\nComputer Vision (ECCV), pages 351–368, 2020. 5\nAPPENDIX\nWe provides additional experiments (Section A), addi-\ntional qualitative results (Section B), and implementation\ndetails (Section C).\nA. Additional experiments\nWe ablate the model and vary key parameters to evaluate\nthe influence of design choices on the quality of the results.\nIn particular, we present results on the effect of λKL (Sec-\ntion A.1), batch size (Section A.2), number of layers (Sec-\ntion A.3), and the rotation representation for SMPL pose\nparameters (Section A.4).\nA.1. Weight of the KL loss\nAs explained in Section 3.2 of the main paper, we em-\npirically show the importance of the weighting between the\nreconstruction loss and the KL loss, controlled byλKL. Ta-\nble A.1 presents results for several values of λKL and we\nfind that there is a trade-off between diversity and realism\nthat is best balanced at λKL = 1e−5. We use this value in\nall our experiments.\nA.2. Influence of the batch size\nAs pointed out in Section 3.2 of the main paper, we find\nthat the batch size significantly influences the performance.\nIn Table A.2, we report results with batch sizes of 10, 20,\n30, 40 for a fixed learning rate. The best performance is\nobtained at 20, which is used in all our experiments.\nA.3. Number of layers\nWe experiment with the number of Transformer layers\nin both of our encoder and decoder architectures. Table A.3\nsummarizes the results. While 2 and 4 layers are sub-\noptimal, the performance difference between 6 and 8 layers\nis minimal. We use 8 layers in all our experiments.\nA.4. SMPL pose parameter representation\nIn Table A.4, we explore different rotation representa-\ntions for SMPL pose parameters. Note that we also pre-\n11\nserve the loss on the vertices LV in all rows. We find that\nan axis-angle representation is difficult to train due to dis-\ncontinuities, while others, such as quaternions, rotation ma-\ntrices and 6D continuous representations [76] are similar in\nperformance on NTU-13. On UESTC, we obtain the best\nperformance with the 6D representation and use this in all\nour experiments.\nB. Additional qualitative results\nFigure A.1 demonstrates the diversity of our generated\nmotions for additional actions on NTU-13 and UESTC.\nVideo. We provide a supplemental video at [57] to illus-\ntrate qualitatively the diversity in our generations and com-\npare with Action2Motion [21]. Moreover, we visualize the\neffect of using a combined reconstruction loss defined both\non rotations and vertex coordinates, as opposed to a single\nloss. We further present results of changing the duration of\nthe generations. We also inspect the latent space by inter-\npolating the noise vector. Finally, we present the denoising\ncapability of our model by encoding-decoding through our\nlatent space. This takes jerky motions and produces smooth\nbut natural looking motion.\nJitter removal for Action2Motion [21]. Besides the quan-\ntitative improvement of ACTOR over Action2Motion, we\nobserve qualitatively that Action2Motion generations have\nsignificant temporal jitter. To investigate whether our im-\nprovement stems from this difference, we removed jitter\n(using 1e filter) from Action2Motion generations (that we\nobtained with their code). The result becomes worse (FID:\n0.41 → 0.63, Acc: 94.3% → 93.0%)1, perhaps because the\nreal data also has considerable jitter. This suggests that\nour significant quantitative improvement can be attributed\nto other factors such as more distinguishable actions.\nC. Implementation details\nArchitectural details. For all our experiments, we set the\nembedding dimensionality to 256. In the Transformer, we\nset the number of layers to 8, the number of heads in multi-\nhead attention to 4, the dropout rate to 0.1 and the dimen-\nsion of the intermediate feedforward network to 1024. As\nin GPT-3 [8] and BERT [12], we use Gaussian Linear Error\nUnits (GELU) [24] in our Transformer architecture.\nLibrary credits. Our models are implemented with Py-\nTorch [54], and we use PyTorch3D [34] to perform differ-\nentiable conversion between rotation representations. We\nintegrate the differentiable SMPL layer using the PyTorch\nimplementation of SMPL-X [55].\nMetrics. For the evaluation metrics, we use the implemen-\ntations provided by Action2Motion [21].\nRuntime. Training takes 24 hours for 2K epochs on NTU,\n19h hours for 5K epochs on HumanAct12, and 33 hours for\n1These two values for Action2Motion does not match Table 3 of the\npaper because we use our own evaluation script and normalized the ground\ntruth shape by taking the average shape of SMPL.\n1K epochs on UESTC on a single Tesla V100 GPU, using\n4GB GPU memory with batch size 20.\nTraining with sequences of variable durations. As ex-\nplained in Section 4.2 of the main paper, we finetune our\nmodel with variable-durations after pretraining on fixed-\ndurations. For this, we restore the model weights from the\nfixed-duration pretraining and finetune for 100 additional\nepochs, with the same training hyperparameters.\n12\nUESTC NTU-13\nFIDtr↓ FIDtest↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal 2.93±0.26 2.79±0.29 98.8±0.1 33.34±0.32 14.16±0.06 0.02±0.00 99.8±0.0 7.07±0.02 2.27±0.01\nλKL= 1e−3 460 .72±90.36 490.12±36.10 34.4±1.4 20.69±0.60 1.25±0.00 13.79±0.03 46.6±0.7 5.79±0.04 1.53±0.01\nλKL= 1e−4 367 .95±94.07 390.68±41.02 38.1±0.9 20.91±0.38 9.19±0.08 9.90±0.02 50.3±1.0 6.15±0.04 2.86±0.02\nλKL= 1e−5 20.02±1.79 23.64±3.59 90.5±0.4 32.77±0.48 14.64±0.07 0.17±0.00 96.4±0.2 7.08±0.03 2.12±0.01\nλKL= 1e−6 34 .13±5.52 39.74±3.57 77.4±0.8 29.60±0.35 18.08±0.08 13.83±0.03 46.6±0.7 5.78±0.04 1.54±0.01\nλKL= 1e−7 80 .05±7.71 83.68±12.55 47.1±2.1 25.06±0.15 19.96±0.08 7.04±0.03 43.0±2.1 6.17±0.03 4.18±0.01\nTable A.1: Weighting the KL loss term: To obtain a good trade-off between diversity and realism, it is important to find the balance\nbetween the reconstruction loss term and the KL loss term in training. We set the weight λKL to 1e−5 in our training.\nUESTC NTU-13\nFIDtr↓ FIDtest↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal 2.93±0.26 2.79±0.29 98.8±0.1 33.34±0.32 14.16±0.06 0.02±0.00 99.8±0.0 7.07±0.02 2.27±0.01\nBatch size = 10283.28±94.40 309.15±33.90 39.7±1.5 23.24±0.43 15.73±0.11 13.95±0.03 46.2±0.6 5.77±0.05 1.56±0.01\nBatch size = 20 20.02±1.79 23.64±3.59 90.5±0.4 32.77±0.48 14.64±0.07 0.17±0.00 96.4±0.2 7.08±0.03 2.12±0.01\nBatch size = 30 23.37±2.95 26.06±1.28 89.7±0.5 32.07±0.58 14.59±0.05 0.18±0.00 96.2±0.2 7.07±0.04 2.13±0.01\nBatch size = 40 25.36±1.82 28.22±2.16 89.2±0.7 32.22±0.44 14.52±0.10 0.26±0.00 95.4±0.1 7.06±0.05 2.10±0.01\nTable A.2: Batch size: We observe sensitivity of the Transformer V AE training to different batch sizes and report performances at several\nbatch size values. We set this hyperparameter to 20 in our training.\nUESTC NTU-13\nFIDtr↓ FIDtest↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal 2.93±0.26 2.79±0.29 98.8±0.1 33.34±0.32 14.16±0.06 0.02±0.00 99.8±0.0 7.07±0.02 2.27±0.01\n2-layers 34.66±2.58 37.17±3.53 84.9±0.6 30.87±0.36 15.83±0.08 0.24±0.00 94.6±0.2 7.07±0.03 2.22±0.01\n4-layers 23.93±1.50 26.75±1.99 88.9±0.5 32.24±0.76 15.06±0.06 0.19±0.00 96.1±0.2 7.09±0.04 2.10±0.01\n6-layers 21.68±1.78 24.92±2.09 89.0±0.6 32.61±0.41 15.31±0.05 0.16±0.00 96.6±0.1 7.09±0.04 2.11±0.01\n8-layers 20.02±1.79 23.64±3.59 90.5±0.4 32.77±0.48 14.64±0.07 0.17±0.00 96.4±0.2 7.08±0.03 2.12±0.01\nTable A.3: Number of layers: We use 8 layers in both the encoder and the decoder of the Transformer V AE. While the performance\ndegrades at 2 or 4 layers, we see marginal gains after 6 layers.\nUESTC NTU-13\nFIDtr↓ FIDtest↓ Acc.↑ Div.→ Multimod.→ FIDtr↓ Acc.↑ Div.→ Multimod.→\nReal 2.93±0.26 2.79±0.29 98.8±0.1 33.34±0.32 14.16±0.06 0.02±0.00 99.8±0.0 7.07±0.02 2.27±0.01\nAxis-angle 513.39±98.35 531.88±43.41 16.4±0.4 19.75±0.44 1.81±0.00 14.98±0.03 41.7±0.7 5.29±0.02 1.96±0.01\nQuaternion 281.9±87.5 305.02±21.97 41.2±1.0 23.48±0.39 14.57±0.06 0.20±0.00 95.6±0.3 7.08±0.04 2.23±0.01\nRotation matrix277.14±76.59 300.29±29.53 41.6±1.9 22.25±0.30 14.56±0.10 0.17±0.00 95.9±0.2 7.08±0.04 2.19±0.01\n6D continuous 20.02±1.79 23.64±3.59 90.5±0.4 32.77±0.48 14.64±0.07 0.17±0.00 96.4±0.2 7.08±0.03 2.12±0.01\nTable A.4: SMPL pose parameter representation:We investigate different rotation representations for the SMPL pose parameters. While\non NTU-13, all except axis-angle representations perform similarly, the best performing representation on UESTC is the 6D continuous\nrepresentation [76]. Note that the action recognition model which is used for evaluation is based on 6D rotations on UESTC and joint\ncoordinates on NTU-13. Therefore, we convert each generation to these representations before evaluation.\n13\nFigure A.1: Additional qualitative results: We provide more action categories from NTU-13 (top two actions: ‘Side kick’ and ‘Standing\nup’) and UESTC (bottom two actions: ‘Punching and knee lifting’ and ‘Spinal stretching’). As in Section 5 of the main paper, we show 3\ngenerations per action. Our model generates different ways to perform the same action.\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7099881768226624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6737619042396545
    },
    {
      "name": "Autoencoder",
      "score": 0.5401671528816223
    },
    {
      "name": "Action recognition",
      "score": 0.5148229002952576
    },
    {
      "name": "Transformer",
      "score": 0.48418381810188293
    },
    {
      "name": "Motion capture",
      "score": 0.4794415533542633
    },
    {
      "name": "Decoding methods",
      "score": 0.4672515392303467
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4404067397117615
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4361226558685303
    },
    {
      "name": "Generative model",
      "score": 0.4325892925262451
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4071994423866272
    },
    {
      "name": "Motion (physics)",
      "score": 0.394853413105011
    },
    {
      "name": "Machine learning",
      "score": 0.3803812861442566
    },
    {
      "name": "Generative grammar",
      "score": 0.27758198976516724
    },
    {
      "name": "Algorithm",
      "score": 0.16769278049468994
    },
    {
      "name": "Artificial neural network",
      "score": 0.14129525423049927
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210152518",
      "name": "Laboratoire d'Informatique Gaspard-Monge",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210154111",
      "name": "Université Gustave Eiffel",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I142631665",
      "name": "École nationale des ponts et chaussées",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210135521",
      "name": "Max Planck Institute for Intelligent Systems",
      "country": "DE"
    }
  ]
}