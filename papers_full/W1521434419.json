{
    "title": "Supervised and unsupervised web-based language model domain adaptation",
    "url": "https://openalex.org/W1521434419",
    "year": 2012,
    "authors": [
        {
            "id": "https://openalex.org/A2365347095",
            "name": "Gwénolé Lecorvé",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146088085",
            "name": "John Dines",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2140737305",
            "name": "Thomas Hain",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A188602560",
            "name": "Petr Motlíček",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2007469869",
        "https://openalex.org/W2116045774",
        "https://openalex.org/W3090633",
        "https://openalex.org/W2119203697",
        "https://openalex.org/W2122560783",
        "https://openalex.org/W2014516359",
        "https://openalex.org/W2136780519",
        "https://openalex.org/W161909597",
        "https://openalex.org/W2095848329",
        "https://openalex.org/W1733633583"
    ],
    "abstract": "Domain language model adaptation consists in re-estimating probabilities of a baseline LM in order to better match the specifics of a given broad topic of interest. To do so, a common strategy is to retrieve adaptation texts from the Web based on a given domain-representative seed text. In this paper, we study how the selection of this seed text influences the adaptation process and the performances of resulting adapted language models in automatic speech recognition. More precisely, the goal of this original study is to analyze the differences of our Web-based adaptation approach between the supervised case, in which the seed text is manually generated, and the unsupervised case, where the seed text is given by an automatic transcript. Experiments were carried out on data sourced from a real-world use case, more specifically, videos produced for a university YouTube channel. Results show that our approach is quite robust since the unsupervised adaptation provides similar performance to the supervised case in terms of the overall perplexity and word error rate.",
    "full_text": null
}