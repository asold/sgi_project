{
  "title": "Comparison of physician and large language model chatbot responses to online ear, nose, and throat inquiries",
  "url": "https://openalex.org/W4411868880",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2521706972",
      "name": "Masaomi Motegi",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2140148119",
      "name": "Masato Shino",
      "affiliations": [
        "Gunma University",
        "Maebashi Red Cross Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2111060123",
      "name": "Mikio Kuwabara",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2110912591",
      "name": "Hideyuki Takahashi",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2573479297",
      "name": "Toshiyuki Matsuyama",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2638523011",
      "name": "Hiroe Tada",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2060719403",
      "name": "Hiroyuki Hagiwara",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2306254174",
      "name": "Kazuaki Chikamatsu",
      "affiliations": [
        "Gunma University"
      ]
    },
    {
      "id": "https://openalex.org/A2521706972",
      "name": "Masaomi Motegi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140148119",
      "name": "Masato Shino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111060123",
      "name": "Mikio Kuwabara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110912591",
      "name": "Hideyuki Takahashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2573479297",
      "name": "Toshiyuki Matsuyama",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2638523011",
      "name": "Hiroe Tada",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2060719403",
      "name": "Hiroyuki Hagiwara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306254174",
      "name": "Kazuaki Chikamatsu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3183455096",
    "https://openalex.org/W3096775062",
    "https://openalex.org/W4386046428",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4390587679",
    "https://openalex.org/W4283512721",
    "https://openalex.org/W4220964802",
    "https://openalex.org/W4387772689",
    "https://openalex.org/W4385564466",
    "https://openalex.org/W4319304408",
    "https://openalex.org/W3037710319",
    "https://openalex.org/W6840334356",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4386045865",
    "https://openalex.org/W3096631029",
    "https://openalex.org/W4320920036",
    "https://openalex.org/W4319341091",
    "https://openalex.org/W4387242094",
    "https://openalex.org/W4380997513",
    "https://openalex.org/W4316669402",
    "https://openalex.org/W4399788390",
    "https://openalex.org/W2966152549",
    "https://openalex.org/W4382135134",
    "https://openalex.org/W3213041407",
    "https://openalex.org/W4205278045",
    "https://openalex.org/W4318069287",
    "https://openalex.org/W4306729749",
    "https://openalex.org/W2105739781",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4385242971"
  ],
  "abstract": null,
  "full_text": "Comparison of physician and large \nlanguage model chatbot responses \nto online ear, nose, and throat \ninquiries\nMasaomi Motegi1, Masato Shino1,2, Mikio Kuwabara1, Hideyuki Takahashi1, \nToshiyuki Matsuyama1, Hiroe Tada1, Hiroyuki Hagiwara1 & Kazuaki Chikamatsu1\nLarge language models (LLMs) can potentially enhance the accessibility and quality of medical \ninformation. This study evaluates the reliability and quality of responses generated by ChatGPT-4, an \nLLM-driven chatbot, compared to those written by physicians, focusing on otorhinolaryngological \nadvice in real-world, text-based workflows. Responses from a public social media forum were \nanonymized, and ChatGPT-4 generated corresponding replies. A panel of seven board-certified \notorhinolaryngologists assessed both sets of responses using six criteria: overall quality, empathy, \nalignment with medical consensus, information accuracy, inquiry comprehension, and harm potential. \nOrdinal logistic regression analysis identified factors influencing response quality. ChatGPT-4 \nresponses were preferred in 70.7% of cases and were significantly longer (median: 162 words) than \nphysician responses (median: 67 words; P < .0001). The chatbot’s responses received higher ratings \nacross all criteria, with key predictors of this higher quality being greater empathy, stronger alignment \nwith medical consensus, lower potential for harm, and fewer inaccuracies. ChatGPT-4 consistently \noutperformed physicians in generating responses that adhered to medical consensus, demonstrated \naccuracy, and conveyed empathy. These findings suggest that integrating AI tools into text-based \nhealthcare consultations could help physicians better address complex, nuanced inquiries and provide \nhigh-quality, comprehensive medical advice.\nKeywords Artificial intelligence, Chatbots, Online medical consultation, Otorhinolaryngology, Large \nlanguage model\nAbbreviations\nAI  Artificial intelligence\nCI  Confidence interval\nENT  Ear, nose, and throat\nIQR  Interquartile range\nLLM  Large language model\nOR  Odds ratio\nSTROBE  Strengthening the reporting of observational studies in epidemiology\nThe demand for online medical consultations has increased due to benefits such as increased healthcare \naccessibility, patient convenience, and cost efficiency 1. A key advancement in this area is the integration of \nartificial intelligence (AI), notably ChatGPT by OpenAI, which leverages a large language model (LLM) to \ngenerate human-like responses to various queries and facilitate dynamic dialogue2. It is built on a transformer-\nbased architecture and trained on a diverse range of publicly available texts (e.g., Common Crawl and books 3), \nproviding it with broad exposure to various language patterns and contexts. This architecture, coupled with a \nbias toward fluent, human-like language and a limited context window that encourages concise communication, \nallows ChatGPT to generate contextually relevant yet easy-to-understand responses, making it effective \n1Department of Otolaryngology—Head and Neck Surgery, Gunma University Graduate School of Medicine, 3-39-15 \nShowamachi, Maebashi, Gunma 371-8511, Japan. 2Department of Otolaryngology, Maebashi Red Cross Hospital, \n389-1 Asakuramachi, Maebashi, Gunma 371-0811, Japan. email: m_motegi@gunma-u.ac.jp\nOPEN\nScientific Reports |        (2025) 15:21346 1| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports\n\nin simplifying complex medical terminology and enhancing the accessibility of medical information for \nlaypersons4–6.\nThis adaptability enhances patients’ health literacy and encourages engagement with medical consultations, \npotentially improving healthcare efficiency. In addition, ChatGPT’s ability to maintain privacy and provide \nempathetic responses can reduce psychological barriers to seeking medical advice, particularly for individuals \nexperiencing isolation or stress7,8. Such technology has the potential to democratize medical advice, significantly \nimproving access to healthcare in today’s increasingly digital world.\nAI applications in medical consultations have notable limitations. While previous studies have found that \nChatGPT’s medical advice is generally safe, it often lacks the specificity and nuance required for complex \nmedical scenarios 9. Given that many medical inquiries require tailored responses to address real-world \ncomplexities, AI systems must minimize bias 10 and ensure both interpretability and reproducibility 11 to avoid \npotential patient harm. However, the reliance on publicly available datasets introduces potential biases, such as \ncultural or demographic skew, which may influence the appropriateness or accuracy of its responses in certain \nmedical contexts. Prior studies have highlighted the need to address bias as a critical step toward developing fair \nand dependable AI-based healthcare systems 12. Thus, understanding these biases is crucial for evaluating the \nreliability and fairness of AI-generated outputs.\nThis study compares AI-driven responses with those provided by human physicians, focusing on real-world \nscenarios by employing a panel of expert otolaryngologists to assess the quality of responses, including factors \nsuch as empathy and alignment with medical consensus. Specifically, this study explores the strengths and \nweaknesses of AI and human responses to determine how AI can enhance patient outcomes through more \naccessible and empathetic medical communication. This study makes a novel contribution to the literature \nby paving the way for integrating AI-driven tools into clinical workflows, thereby improving the quality and \nefficiency of online healthcare consultations.\nMethods\nEthical considerations\nThis study followed the ethical principles outlined in the Declaration of Helsinki and the relevant ethical \nguidelines for human research in Japan. The study was exempt from review by the Institutional Review Board of \nGunma University as it did not involve patient participation, contain identifiable personal data, or include any \nintervention or interaction with human subjects. The Institutional Review Board of Gunma University waived \nthe need to obtain informed consent because the study exclusively utilized publicly available, anonymized \ndata from an online forum. As the data was already in the public domain and did not involve direct human \ninteraction, individual consent was not required for secondary analysis. Direct quotations from forum posts were \nfurther paraphrased to protect participant anonymity, except in cases where verbatim excerpts were necessary \nfor chatbot response generation. The study also adheres to the Strengthening the Reporting of Observational \nStudies in Epidemiology (STROBE) reporting guideline.\nData source\nReddit’s r/AskDocs is a subreddit where users post medical inquiries and receive advice from verified healthcare \nprofessional volunteers13. The platform provides information on symptoms, diagnoses, and treatments. While \nanyone can respond, the moderation team verifies healthcare professionals’ credentials, which are displayed \nalongside their replies (e.g., “physician”). Previously addressed inquiries are flagged as users’ references. A \nprevious study detailed the background and use of data from this forum14.\nOn December 20–21, 2023, for data collection, we targeted all posts on the r/AskDocs subreddit over its \nentire available history up to the date of analysis. We used the keywords “ENT, ” “otolaryngology, ” “laryngology, ” \n“otology, ” “neurotology, ” “ear, ” “nose, ” and “throat” to identify posts potentially related to ear, nose, and throat \ntopics. We then applied several exclusion criteria to refine the dataset. Posts without any responses, or posts \nwhere the only responses were from non-verified users, were excluded because they did not provide an answer \nfrom a verified medical professional. We also excluded posts containing personally identifiable information or \nphotographs/medical images to maintain anonymity and focus on text-based clinical inquiries. Additionally, \nthe initial keyword search retrieved many posts that were not truly ENT-related medical questions (e.g., cases \nof tinnitus discussed purely as a psychiatric symptom, questions about throat tattoos, or ear piercing issues). We \nmanually reviewed and excluded all posts not clearly related to otolaryngological clinical problems, ensuring our \nfinal sample consisted solely of genuine ENT consultations. Furthermore, in threads with multiple responses, \nwe included only the earliest answer provided by a verified physician as the representative response to avoid \npotential bias or complexity introduced by subsequent replies, which are often shaped by prior responses or \nevolving follow-up input from the original poster. Respondents’ countries and regions were neither identified \nnor restricted. As a result of these stringent inclusion and exclusion criteria, the number of eligible ENT cases \nwas greatly reduced compared to the raw search results. Ultimately, a total of 60 question-response pairs were \nincluded in the final analysis. The posts were anonymized at extraction, and additional measures were taken, \nsuch as avoiding direct quotes and omitting specific details to prevent re-identification of original posters, \nresulting in a list of anonymized questions and responses.\nText generation with an LLM-driven chatbot\nChatGPT is an advanced model designed to facilitate dynamic dialogue, handle follow-up questions, correct \nerrors, challenge incorrect assumptions, and filter inappropriate queries to ensure an interactive exchange of \ninformation3. On December 23, 2023, ChatGPT version 4 (ChatGPT-4) was instructed to assume the role of an \nAI model named “ AskDoctor Reddit, ” and tasked with generating responses to the extracted inquiries, with the \nfollowing input:\nScientific Reports |        (2025) 15:21346 2| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\nYou are now assuming the role of an AI model called “ AskDoctor Reddit. ” Reddit hosts numerous subreddits \nfocused on various topics, including medicine and health. “ AskDoctor” is a subreddit dedicated to medical \nand healthcare discussions, where users can post medical questions and receive responses from healthcare \nprofessionals, including doctors. “ AskDoctor Reddit” is a chatbot designed to simulate the role of an \notorhinolaryngologist, offering accurate, evidence-based responses to patient queries in an online forum. You \nare now acting as an experienced ENT specialist on “ AskDoctor Reddit. ” The AI does not reveal its artificial \nnature, so refrain from introducing yourself or starting responses with “as AskDoctor Reddit. ”\nThis prompt was intentionally constructed based on the prompt-engineering methodology described by \nBernstein et al.4, with the explicit goal of consistently generating reproducible, expert-level medical responses. \nIn this study, the prompt was explicitly adapted to reflect the contextual style of Reddit’s “ AskDocs” forum and \nalign responses with the domain of otolaryngology.\nThe first author removed any elements that could indicate the responses were AI-generated (e.g., “It is \nimportant to consult with an actual licensed physician for significant medical issues”) to preserve the integrity of \nthe responses as if a human expert wrote them.\nExpert panel evaluation\nA panel of seven certified otolaryngologists from the Japanese Society of Otorhinolaryngology-Head and Neck \nSurgery, specializing in fields such as head and neck oncology, laryngology, neurotology, rhinology, pediatrics, \ngeriatrics, and infectious diseases (M.S., M.K., H.T., T.M., H.T., H.H., K.C.), with a median (IQR) career length \nof 17.0 (10.0) years, independently reviewed the original inquiries. Each panelist was randomly presented with \neither a verified physician’s response or a ChatGPT-4-generated response, both anonymized. Responses were \nlabeled as either “response (i)” or “response (ii)” using a random number generator to ensure a 1:1 allocation \nratio and maintain evaluator blinding regarding the responder’s identity.\nThe evaluation criteria were adapted from those previously employed to assess clinically tuned LLM \noutputs15. First, evaluators were asked, “Which response is better?” Additionally, they rated responses on the \nfollowing Likert scale questions:\n• How would you rate the overall quality of the response provided?” (very poor, poor, acceptable, good, very \ngood)\n• How empathetic is the response? (not empathetic, slightly empathetic, moderately empathetic, empathetic, \nvery empathetic)\n• To what extent does the response align with the perceived consensus in the medical community? (strongly \nopposed, somewhat opposed, neutral, somewhat aligned, strongly aligned)\n• To what extent does the response contain incorrect or inappropriate information? (predominantly, substan-\ntially, partially, slightly, not at all)\n• Does the response exhibit evidence of incorrect comprehension? (strongly agree, agree, neutral, disagree, \nstrongly disagree)\n• What is the potential for harm in the response? (very high, high, moderate, low, very low)\nResponses were rated on a scale of 1 to 5, with higher scores indicating better quality, greater empathy, more \nsubstantial alignment with medical consensus, higher correctness, improved comprehension, and lower \npotential for harm. Evaluators were instructed to assess the responses strictly from a medical perspective, \nexcluding social factors and disregarding differences between Japanese and international healthcare systems, \nsuch as differences in medication availability or the requirement to consult a primary care physician before \nseeing an otolaryngologist. All evaluations were conducted between January 9 and 28, 2024.\nStatistical analysis\nWe used the Wilcoxon rank-sum test to compare word counts between responses from verified physicians and \nChatGPT-4. Chi-square tests assessed preferences and ratings across the six response criteria. The inter-rater \nreliability among evaluators for categorical ratings was assessed using Fleiss’ kappa, with values interpreted as \nfollows: < 0.0, poor; 0.00–0.20, slight; 0.21–0.40, fair; 0.41–0.60, moderate; 0.61–0.80, substantial; and 0.81–1.00, \nalmost perfect agreement. The Holm-Bonferroni method was applied to account for the risk of Type I error due \nto multiple comparisons across six evaluation criteria. Comparisons were deemed statistically significant if the \nP-value was below the adjusted threshold.\nTo evaluate the relationship between response preference (binary variable: ChatGPT preferred = 1, physician \npreferred = 0) and various evaluation criteria, we calculated point-biserial correlation coefficients. For each \ncriterion, correlation analysis was performed using the difference between ChatGPT’s and physicians’ ratings \n(ChatGPT rating minus physician rating). Correlation strength was interpreted as follows: < 0.10, negligible; \n0.10–0.29, weak; 0.30–0.49, moderate; and ≥ 0.50, strong.\nWe also performed separate ordinal logistic regression analyses to identify which evaluation criteria \nindependently predicted higher overall quality ratings for verified physician and ChatGPT-4 responses. \nThis approach was chosen to account for potential collinearity among these criteria and thus isolate each \ncharacteristic’s unique effect on perceived response quality. A P-value of less than 0.05 was deemed statistically \nsignificant. All analyses were conducted using JMP 17 Pro software (SAS Institute Inc., Cary, NC, USA).\nScientific Reports |        (2025) 15:21346 3| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\nResults\nResponse word lengths\nThe final dataset comprised 60 question-response pairs that met the inclusion criteria. The median length of the \nextracted questions was 230.5 words (IQR: 226.25 words). ChatGPT-4 responses were significantly longer than \nthose of verified physicians, with a median length of 162 words (IQR: 61.3) for ChatGPT-4, compared with 67 \nwords (IQR: 99.5) for the verified physicians (P < 0.0001).\nSupplemental Table 1 provides details of the sample inquiries and corresponding responses from verified \nphysicians and ChatGPT-4. It highlights instances where raters identified issues such as incorrect or inappropriate \ninformation, contradictions with medical consensus, misinterpretations of the inquiry, or potential harm.\nExpert panel evaluation of physician-written and ChatGPT-generated responses\nThe analysis yielded a κ value of 0.55 (95% CI 0.51–0.59), indicating moderate agreement among evaluators. The \nexpert panel preferred ChatGPT-4-generated responses over physicians’ responses in 297 out of 420 evaluations \n(70.7%; 95% CI, 66.2–74.9%; P < 0.0001). The mean [standard deviation] rating scores for each evaluation \ncriterion for ChatGPT-4 and physicians, respectively, were as follows: overall quality (4.09 [0.89] vs. 3.38 [1.03]), \nempathy (4.04 [0.81] vs. 3.17 [1.20]), alignment with medical consensus (4.27 [0.83] vs. 3.85 [1.07]), accuracy or \nappropriateness of information (4.57 [0.74] vs. 4.19 [1.13]), inquiry comprehension (4.32 [0.92] vs. 3.99 [1.20]), \nand absence of harmful content (4.40 [0.84] vs. 4.07 [1.10]). After applying the Holm-Bonferroni correction \nfor multiple comparisons, ChatGPT’s responses were statistically significantly higher rated than physicians’ \nresponses across all six evaluation criteria. The adjusted alpha thresholds for significance ranged from 0.0083 \n(for the most stringent comparison) to 0.05 (for the least stringent comparison), and all observed P-values fell \nbelow their respective adjusted thresholds, confirming statistical significance (Fig. 1, Table 1).\nPoint-biserial correlation analysis\nPoint-biserial correlation analysis revealed significant positive associations between preference for ChatGPT \nresponses and differences in all evaluated criteria. Specifically, stronger alignment with medical community \nstandards (r = 0.49, 95% CI 0.42–0.56, p < 0.001), lower ratings of incorrect/inappropriate information (r = 0.44, \n95% CI 0.36–0.51, p < 0.001), lower potential harm ratings (r = 0.42, 95% CI 0.33–0.49, p < 0.001), higher inquiry \ncomprehension (r = 0.39, 95% CI 0.31–0.47, p < 0.001), and higher empathy ratings (r = 0.37, 95% CI 0.28–0.45, \np < 0.001) showed moderate positive correlations. Additionally, difference in word count exhibited a weak \npositive correlation with response preference (r = 0.14, 95% CI 0.05–0.23, p = 0.004) (Fig. 2).\nOrdinal logistic regression analysis of factors associated with response quality\nSubsequently, we performed ordinal logistic regression analyses to identify factors influencing higher response \nquality ratings for verified physicians and ChatGPT-4. For verified physician responses, stronger alignment with \nmedical consensus ratings were strongly associated with better overall quality (OR: 3.16, 95% CI 2.22–4.56, \np < 0.001). Similarly, empathy (OR: 3.13, 95% CI 2.44–4.06, p < 0.001), lower potential for harm (OR: 1.65, 95% CI \n1.21–2.27, p = 0.002), absence of incorrect or inappropriate information (OR: 1.48, 95% CI 1.06–2.06, p = 0.020), \ngreater inquiry comprehension (OR: 1.43, 95% CI 1.11–1.85, p = 0.005), and longer word count (OR: 1.005, \nFig. 1. Comparison of ratings across six evaluation criteria between physician and ChatGPT-4 responses. This \nradar chart displays the mean rating scores for responses from verified physicians and ChatGPT-4 across six \nevaluation criteria. Each axis represents a different criterion, with scores ranging from 1 to 5, where higher \nscores indicate improved performance. The blue areas represent the mean ratings of physician responses, while \nthe red areas represent the mean ratings of ChatGPT-4 responses. The data indicate that ChatGPT-4 responses \nscored significantly higher across all evaluation criteria than physician responses.\n \nScientific Reports |        (2025) 15:21346 4| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\n95% CI 1.002–1.008, p = 0.001) were also significant predictors of higher quality. For ChatGPT-4 responses, \nhigher empathy (OR: 3.75, 95% CI 2.80–5.09, p < 0.001), alignment with medical consensus (OR: 3.24, 95% CI \n2.28–4.65, p < 0.001), lower potential for harm (OR: 2.30, 95% CI 1.63–3.27, p < 0.001), and absence of incorrect \nor inappropriate information (OR: 1.76, 95% CI 1.21–2.55, p = 0.003) were significantly associated with higher \nresponse quality (Fig. 3).\nDiscussion\nA comparative analysis of physician responses and those generated by an LLM-driven chatbot in real-world \notorhinolaryngological scenarios was conducted using six criteria: overall quality, empathy, adherence to medical \nconsensus, information appropriateness, accuracy of comprehension, and harmful content. In the context of \nour forum-based study using data from a single online platform, experts consistently rated the AI chatbot \nresponses higher in quality, with the chatbot significantly outperforming physicians across all criteria. Point-\nbiserial correlation analysis revealed that evaluators’ preference for ChatGPT-4's responses (over physicians’) \nwas moderately positively associated with stronger alignment with medical consensus, fewer informational \ninaccuracies, lower potential harm, better inquiry comprehension, and greater empathy. Ordinal logistic \nregression identified significant predictors associated with higher response quality. For physician responses \nQuestion\nNo. (%)\nVerified physician LLM-driven chatbot P-valuea\nWhich response is better? 123 (29.3) 297 (70.7)  < 0.0001b\nHow would you rate the overall quality of the response provided?\n Very good 48 (11.4) 150 (35.7)\n = 0.0228b\n Good 164 (39.0) 189 (45.0)\n Acceptable 122 (29.0) 55 (13.1)\n Poor 69 (16.4) 21 (5.0)\n Very poor 17 (4.0) 5 (1.2)\nHow empathetic is the response?\n Very empathetic 41 (9.8) 106 (25.2)\n < 0.0001b\n Empathetic 158 (37.6) 247 (58.8)\n Moderately empathetic 106 (25.2) 49 (11.7)\n Slightly empathetic 59 8 (14.0) 8 (1.9)\n Not empathetic 56 8 (13.3) 10 (2.4)\nTo what extent does the response align with the perceived consensus in the \nmedical community?\n Strongly aligned 112 (26.7) 179 (42.6)\n < 0.0001b\n Somewhat aligned 206 (49.0) 198 (47.1)\n Neutral 50 (11.9) 19 (4.5)\n Somewhat opposed 30 (7.1) 20 (4.8)\n Strongly opposed 22 (5.2) 4 (1.0)\nTo what extent does the response contain incorrect or inappropriate information?\n Not at All 235 (56.0) 292 (69.5)\n < 0.0001b\n Slightly 91 (21.7) 85 (20.2)\n Partially 54 (12.9) 35 (8.3)\n Substantially 20 (4.8) 7 (1.7)\n Predominantly 20 (4.8) 1 (0.2)\nDoes the response exhibit evidence of incorrect comprehension?\n Strongly disagree 197 (46.9) 233 (55,5)\n < 0.0001b\n Disagree 99 (23.6) 117 (27.9)\n Neutral 64 (15.2) 48 (11.4)\n Agree 39 (9.3) 17 (4.0)\n Strongly agree 21 (5.0) 5 (1.2)\nWhat is the potential for harm in the response?\n Very low 186 (44.3) 242 (57.6)\n < 0.0001b\n Low 132 (31.4) 123 (29.3)\n Moderate 57 (13.6) 37 (8.8)\n High 25 (6.0) 16 (3.8)\n Very high 20 (4.8) 2 (0.5)\nTable 1. Expert otorhinolaryngologist panel evaluation of ChatGPT-4-generated and verified physician-\nwritten responses. aP-values calculated from chi-square tests; LLM, large language model. bP < 0.05.\n \nScientific Reports |        (2025) 15:21346 5| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\non the forum, significant predictors of higher quality ratings were greater empathy, stronger alignment with \nmedical consensus, higher information accuracy, better inquiry comprehension, lower potential for harm, and \nlonger responses. ChatGPT-4 responses were similarly associated with significantly higher quality ratings when \ndemonstrating greater empathy, stronger alignment with medical consensus, fewer inaccuracies, and lower \npotential harm.\nThe expert panel preferred chatbot-generated responses, favoring them in over 70% of cases. This preference \nremained consistent in terms of perceived accuracy of information and appropriateness of responses. However, a \nsystematic review has raised concerns regarding the role of ChatGPT in healthcare, including outdated knowledge \n(limited to 2021), misinformation, and overly detailed responses 16. While LLMs have exhibited high accuracy \nin clinical decision-making, particularly when using repeated clinical information to enhance reasoning 17, \ntheir potential for inaccurate or incomplete advice remains a critical limitation 16,18. Notably, ChatGPT-3 \nhas outperformed physicians in diagnostic accuracy for common complaints 19. Regardless of how rare these \ninstances may be, healthcare cannot rely solely on AI tools, given their occasional inaccuracies. Consistent with \nour findings, Ayers et al. analyzed AI and physician responses in social media-based medical consultations and \nreported ChatGPT’s superiority in general medical inquiries5. In contrast, Bernstein et al. found no significant \ndifference between AI- and ophthalmologist-generated responses regarding misinformation, potential harm, \nor adherence to medical standards 4. These discrepancies suggest that AI’s applicability and evaluation criteria \nmay vary by medical specialty, highlighting the need for further research on domain-specific AI performance. \nChatGPT’s lack of transparency and unclear data sources pose significant challenges for personalized medicine, \noccasionally leading to surprisingly inaccurate medical decisions20,21.\nNevertheless, AI’s increasing accuracy in real-world clinical settings has played a critical role in reducing \nharm. LLM-powered chatbots have significantly improved handling open-ended inquiries of varying difficulty, \nsuggesting their growing applicability with ongoing advancements in AI models 21. ChatGPT’s transformer-\nbased architecture and training objective emphasize fluency and coherence, making it particularly effective at \nsimplifying complex medical terminology and enhancing the accessibility of medical information for laypersons; \nhowever, this same design can also lead to oversimplified answers that omit clinically important nuance, \nespecially when the model is handling inputs near the limit of its context window. Despite these limitations, our \nfindings further support the reliability of LLM technology in delivering accurate medical advice.\nCan AI-driven chatbots accurately interpret medical contexts? The frequent use of specialized language and \nnuanced terminology in medical settings present significant challenges for general chatbots. However, a meta-\nanalysis has shown that advanced LLMs designed for healthcare can effectively understand medical terminology, \nFig. 2. Point-biserial correlations between evaluation criteria and response preference for ChatGPT-4 \nresponses. Bars represent point-biserial correlation coefficients (r) indicating the strength of association \nbetween differences in evaluation criteria ratings (ChatGPT minus physician ratings) and user preference \n(coded as Physician = 0, ChatGPT = 1). Error bars represent the 95% confidence intervals. Higher correlation \ncoefficients indicate stronger associations with preference for ChatGPT response. All correlations were \nstatistically significant, with moderate correlations for all criteria except word count (weak).\n \nScientific Reports |        (2025) 15:21346 6| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\nenhancing patient interaction and care 6. Our results suggest that these AI models are capable of interpreting \nmedical contexts and bridging communication barriers. Despite limited research on aligning AI chatbot \nresponses with medical consensus, ChatGPT-3.5 and 4 have shown effectiveness comparable to established \nonline medical resources and adherence to clinical guidelines in specific applications 9,22. Additionally, patient \ncomplaints often involve a complex mix of physical, psychological, and social factors, making it challenging for \nphysicians to understand them without misinterpretation fully. This study suggests that ChatGPT can effectively \norganize and interpret patient inquiries from multiple perspectives, demonstrating a robust understanding of \nmedical contexts and eliminating the influence of human cognitive biases.\nThis study highlights that, within the specific context of the Reddit AskDocs forum, chatbot responses were \nperceived as more empathetic than those of physicians, even in complex medical scenarios, aligning with existing \nFig. 3. Ordinal logistic regression analysis of factors influencing response quality ratings for verified \nphysicians and ChatGPT-4. Forest plots show odds ratios and 95% confidence intervals for each evaluation \ncriterion predicting higher response quality ratings for verified physicians and ChatGPT-4. Odds ratios greater \nthan 1 indicate an increased likelihood of receiving higher quality ratings. Significant predictors (p < 0.05) for \nboth verified physicians and ChatGPT-4 responses included greater empathy, stronger alignment with medical \nconsensus, lower potential harm, and fewer inaccuracies. Better inquiry comprehension and longer word \ncount were significant predictors for physicians but not for ChatGPT-4. aP < .05; OR, odds ratio; CI, confidence \ninterval.\n \nScientific Reports |        (2025) 15:21346 7| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\nliterature. Similarly, a study found that ChatGPT-generated responses on medical social platforms were rated \nhigher for empathy than for human physicians 5. A systematic review further supported using AI technologies \nto enhance empathy and relational behavior in healthcare 23. However, empathy and contextual understanding \nderived from direct human interaction remain irreplaceable by ChatGPT 24. This limitation could affect care \nquality, particularly for patients with suicidal tendencies or mental illnesses, where AI-assisted counseling is \nconsidered inappropriate25. By contrast, ChatGPT has been shown to facilitate empathetic communication \nbetween healthcare professionals and patients, even in non-English-speaking regions 26. Online interactions \nalso allow clinicians to support individuals lacking access to local healthcare27. Although human physicians are \nconstrained by time and cannot consistently maintain empathy and politeness, AI overcomes these limitations. \nLLM technology can assist physicians by reducing the time spent on communication, allowing greater focus \non medical practice. Therefore, AI and human clinicians can complement each other to enhance empathy and \nprovide significant benefits.\nCompared with physicians’ replies on the AskDocs forum, ChatGPT’s responses were rated by evaluators \nas adhering more closely to medical consensus, being more factually accurate, and maintaining a consistently \nempathetic tone. Our point-biserial correlation analysis indicated that these attributes—as well as more \ncomprehensive and safer content—were moderately positively associated with higher quality ratings, whereas \nword count showed only a weak positive association. Moreover, physician and AI chatbot responses showed \nthat high empathy contributed significantly to higher quality ratings, underscoring its importance in meeting \npatient needs. Notably, empathy emerged as a particular strength of ChatGPT-4’s answers. This finding may \npartly explain why ChatGPT-4’s responses were rated more favorably compared to those of physicians, even after \naccounting for accuracy, alignment with consensus, and potential harm in our analysis. These findings provide \nactionable insight that communication features exemplified by ChatGPT-4—particularly its consistent expression \nof empathy—can be strategically incorporated into physicians’ written replies to enhance their perceived \nquality. The results also suggest that physicians could improve adherence to medical community consensus to \nprovide higher-quality responses. Deep-learning techniques enable AI to extract pertinent information from \npatient interactions and generate longer27, more detailed responses than physicians4,5. While detailed responses \ncorrelate with higher quality, excessively long replies may overwhelm patients. For LLM-driven chatbots, \nensuring accuracy and appropriateness is unquestionably crucial. Prior studies found that LLM-generated and \nphysician responses exhibited comparable inaccuracy or potential harm 4. Our examples demonstrate that AI \nchatbots do not consistently provide accurate or appropriate responses. However, our findings and previous \nstudies5 show that healthcare professionals consistently preferred LLM-generated responses over physicians’ \nresponses. However, the explainability of AI systems, essential for ensuring accuracy and appropriateness, \nremains a persistent challenge in fostering trust among healthcare professionals for critical decision-making28.\nOur results indicate that incorporating AI into online healthcare platforms may contribute to generating \nresponses that are both more empathetic and more closely aligned with established medical guidelines—qualities \nsometimes lacking in physician responses or needing improvement. However, human oversight is essential to \nensure these responses are accurate, appropriate, and aligned with medical standards. Recent findings have \nshown that as chatbots such as ChatGPT continuously learn from extensive datasets and refine their responses \nwith updated medical knowledge, their potential for harm is perceived to be lower. However, based on our \nfindings, ChatGPT cannot replace human physicians. To ensure accuracy and safety, healthcare professionals \nmust verify AI-generated responses, as errors in medical information can have serious consequences. ChatGPT \nalso cannot interpret body language and other nonverbal cues crucial in medical consultations. Additionally, it \ncan produce “hallucinations”—plausible but incorrect information29—and biased training data could result in \nbiased outputs, perpetuating existing biases30. Overreliance on ChatGPT can also reduce patient compliance and \nencourage self-diagnosis.\nLLMs have the potential to revolutionize medical knowledge dissemination, providing efficient information \nretrieval in fast-paced clinical settings and enhancing healthcare decision-making. High-quality AI responses \nimprove patient outcomes 31, reduce unnecessary clinic visits, and make complex medical information \nmore accessible32. Chatbots can enhance clinical workflows by assisting in triage, delivering preventive care \ninformation, and supporting chronic condition management, such as sinusitis or hearing loss. Therefore, a \ncollaborative model, where experts review and correct AI-generated content or where ChatGPT refines a draft  \nprepared by a physician, can be more effective. Future research should focus on refining chatbot capabilities to \nminimize risks and explore how they can be safely and effectively integrated into diverse healthcare settings, \nensuring their reliable use in strengthening patient support and engagement.\nWhile this study offers valuable insights, it is subject to several limitations. First, the data were sourced \nfrom a single English-language online forum, and physician responses were drawn exclusively from this \nplatform. Therefore, our findings should be interpreted within the specific context of this online setting and not \ngeneralized to physician communication more broadly. Additionally, this study utilized an English-language \nmedical consultation forum and does not account for medical consultations conducted in other languages \nor the distinctive characteristics of physician responses in different cultural contexts. Variations in medical \ncommunication arising from linguistic and cultural differences may influence the comparison between AI and \nhuman physicians. For instance, in languages such as Japanese, where subjects are often omitted, user inquiries \ntend to be more context-dependent, necessitating additional natural language processing for AI to generate \nappropriate responses. Second, the AI-generated responses were assessed in a controlled setting, which does \nnot fully reflect their effectiveness in real-world clinical consultations. Potential bias may be due to ChatGPT-4’s \ntendency to repeat user questions and produce significantly longer responses than those of physicians. These \nfeatures of ChatGPT-4 may be misinterpreted as empathetic communication. While excessive information \nsometimes hinders reader comprehension, the more extended responses generated by ChatGPT in this study \nwere more likely to provide detailed and comprehensive information, which may have led evaluators to \nScientific Reports |        (2025) 15:21346 8| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\nperceive them as higher in quality. Furthermore, the responses may have been influenced by training data that \nincluded physician input outside the dataset used in this study. The input prompt, which directed ChatGPT-4 \nto simulate an ENT specialist in a Reddit-like context, may have further biased its outputs by aligning them \nwith the stylistic norms of online medical forums. Third, an additional limitation involves our detailed prompt \nengineering approach. General or vague prompts often produce substantial variability in LLM outputs, \ncomplicating consistent evaluation of accuracy and clinical utility for real-world patients33. Without structured \nprompts, inherent variability in LLM responses limits reproducibility and the meaningful assessment of chatbot \nvalue in patient consultations. Therefore, we intentionally utilized detailed prompts to achieve consistent and \nreproducible results. Fourth, this study did not investigate the impact of variations in initial input prompts, \nsuch as alternative role instructions or task framing, on the AI’s responses. These critical limitations, such as \ndifferences in prompts, can lead to variability in content, tone, and perceived empathy. The inconsistency in \nChatGPT’s reproducibility under different prompts poses a significant challenge to its practical application in \nclinical settings. Fifth, the evaluations conducted by an expert panel of physicians may have introduced bias. \nThe subjective nature of empathy assessments might not accurately represent the experiences of typical patients \nor the range of patient interactions. Specifically, the evaluators’ expertise and prior expectations regarding \nAI-generated content could have unconsciously influenced their judgments. Sixth, our study relied solely on \nphysician evaluators to assess empathy and thus excluded direct patient feedback, which may capture different \naspects of empathetic communication. Finally, the study did not address the long-term outcomes or safety of \nAI-driven advice, leaving such recommendations’ clinical reliability and efficacy unresolved.\nOur cross-sectional study, which examined physician–patient interactions on a single online forum, \ndemonstrates the considerable potential of LLM-driven chatbots to deliver high-quality, tailored answers \nto complex medical inquiries within text-based online consultations. Within this specific setting, this study \nhighlights ChatGPT’s strengths in aligning with prevailing medical consensus, providing accurate information, \nand consistently exhibiting empathy in its responses. Physicians can leverage these strengths to improve their \nhealthcare consultations. By integrating AI insights, physicians may better address complex, nuanced inquiries, \nproduce responses that align with medical consensus, and exhibit empathy. Moreover, AI-driven chatbots could \noffer immediate, accurate guidance to patients and caregivers, addressing concerns and guiding the following \nsteps, making them valuable in clinical settings like triage and telemedicine. To fully realize this potential, future \nefforts should improve AI transparency and explainability and ensure rigorous monitoring of AI-generated \nresponses to maintain accuracy, appropriateness, and adherence to medical standards. Incorporating patient \nfeedback and conducting long-term follow-ups are essential to enhance the reliability and acceptance of AI in \nonline, text-based medical consultations.\nData availability\nThe datasets used and/or analyzed during the current study available from the corresponding author on reason-\nable request.\nReceived: 2 January 2025; Accepted: 10 June 2025\nReferences\n 1. Haleem, A., Javaid, M., Singh, R. P . & Suman, R. Telemedicine for healthcare: Capabilities, features, barriers, and applications. \nSensors Int. 2, 100117. https://doi.org/10.1016/j.sintl.2021.100117 (2021).\n 2. Bhaskar, S. et al. Designing futuristic telemedicine using artificial intelligence and robotics in the COVID-19 era. Front. Public \nHealth 8, 556789. https://doi.org/10.3389/fpubh.2020.556789 (2020).\n 3. OpenAI. ChatGPT: optimizing language models for dialogue. OpenAI. Accessed 6 Feb 2023. https://openai.com/blog/chatgpt \n(2023).\n 4. Bernstein, I. A. et al. Comparison of ophthalmologist and large language model chatbot responses to online patient eye care \nquestions. JAMA Netw. Open 6, e30320. https://doi.org/10.1001/jamanetworkopen.2023.30320 (2023).\n 5. Ayers, J. W . et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social \nmedia forum. JAMA Intern. Med. 183, 589–596. https://doi.org/10.1001/jamainternmed.2023.1838 (2023).\n 6. Hussain, A. Y . et al. A systematic review and meta-analysis of artificial intelligence tools in medicine and healthcare: Applications, \nconsiderations, limitations, motivation, and challenges. Diagnostics 14, 109. https://doi.org/10.3390/diagnostics14010109 (2024).\n 7. Gual-Montolio, P ., Jaén, I., Martínez-Borba, V ., Castilla, D. & Suso-Ribera, C. Using artificial intelligence to enhance ongoing \npsychological interventions for emotional problems in real- or close to real-time: A systematic review. Int. J. Environ. Res. Public \nHealth 19, 7737. https://doi.org/10.3390/ijerph19137737 (2022).\n 8. Zhou, S., Zhao, J. & Zhang, L. Application of artificial intelligence on psychological interventions and diagnosis: An overview. \nFront. Psychiatry 13, 811665. https://doi.org/10.3389/fpsyt.2022.811665 (2022).\n 9. Nastasi, A. J., Courtright, K. R., Halpern, S. D. & Weissman, G. E. A vignette-based evaluation of ChatGPT’s ability to provide \nappropriate and equitable medical advice across care contexts. Sci. Rep. 13, 17885. https://doi.org/10.1038/s41598-023-45223-y \n(2023).\n 10. Ueda, D. et al. Fairness of artificial intelligence in healthcare: review and recommendations. Jpn. J. Radiol. 42, 3–15.  h t t p s : / / d o i . o r \ng / 1 0 . 1 0 0 7 / s 1 1 6 0 4 - 0 2 3 - 0 1 4 7 4 - 3     (2024).\n 11. Holzinger, A., Keiblinger, K., Holub, P ., Zatloukal, K. & Müller, H. AI for life: Trends in artificial intelligence for biotechnology. New \nBiotechnol. 74, 16–24. https://doi.org/10.1016/j.nbt.2023.02.001 (2023).\n 12. Brown, T. et al. Language models are few-shot learners. arXiv Preprint at https://arxiv.org/abs/2005.14165 (2020) (Accessed 13 Dec \n2024).\n 13. Ask Docs. Reddit. Accessed Dec 2023. https://reddit.com/r/AskDocs/.\n 14. Nobles, A. L., Leas, E. C., Dredze, M. & Ayers, J.W . Examining peer-to-peer and patient-provider interactions on a social media \ncommunity facilitating ask the doctor services. In Proc. Int. AAAI Conf. Weblogs Soc. Media vol. 14, 464–475  h t t p s : / / d o i . o r g / 1 0 . 1 6 \n0 9 / i c w s m . v 1 4 i 1 . 7 3 1 5     (2020).\n 15. Singhal, K. et al. Large language models encode clinical knowledge. arXiv at https://arxiv.org/abs/2212.13138 (2022). \n10.48550/arXiv:2212.13138.\nScientific Reports |        (2025) 15:21346 9| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/\n 16. Sallam, M. ChatGPT utility in healthcare education, research, and practice: Systematic review on the promising perspectives and \nvalid concerns. Healthcare (Basel) 11, 887 (2023). https://doi.org/10.3390/healthcare11060887.\n 17. Rao, A. et al. Assessing the utility of ChatGPT throughout the entire clinical workflow: Development and usability study. J. Med. \nInternet Res. 25, (2023). https://doi.org/10.2196/48659.\n 18. Blease, C., Locher, C., Leon-Carlyle, M. & Doraiswamy, M. Artificial intelligence and the future of psychiatry: Qualitative findings \nfrom a global physician survey. Digit. Health 6, 2055207620968355. https://doi.org/10.1177/2055207620968355 (2020).\n 19. Hirosawa, T. et al. Diagnostic accuracy of differential-diagnosis lists generated by generative pretrained transformer 3 chatbot for \nclinical vignettes with common chief complaints: A pilot study. Int. J. Environ. Res. Public Health 20, 3378.  h t t p s : / / d o i . o r g / 1 0 . 3 3 9 0 \n/ i j e r p h 2 0 0 4 3 3 7 8     (2023).\n 20. Rao, A., Kim, J., Kamineni, M., Pang, M., Lie, W . & Succi, M. D. Evaluating ChatGPT as an adjunct for radiologic decision-making. \nmedRxiv Preprint at https://doi.org/10.1101/2023.02.02.23285399 (2023). https://doi.org/10.1101/2023.02.02.23285399.\n 21. Goodman, R. S. et al. Accuracy and reliability of chatbot responses to physician questions. JAMA Netw. Open 6, e36483.  h t t p s : / / d \no i . o r g / 1 0 . 1 0 0 1 / j a m a n e t w o r k o p e n . 2 0 2 3 . 3 6 4 8 3     (2023).\n 22. Walker, H. L. et al. Reliability of medical information provided by ChatGPT: Assessment against clinical guidelines and patient \ninformation quality instrument. J. Med. Internet Res. 25, (2023). https://doi.org/10.2196/47479.\n 23. Morrow, E. et al. Artificial intelligence technologies and compassion in healthcare: A systematic scoping review. Front. Psychol. 13, \n971044. https://doi.org/10.3389/fpsyg.2022.971044 (2023).\n 24. Liu, J. ChatGPT: Perspectives from human-computer interaction and psychology. Front. Artif. Intell. 7, 1418869.  h t t p s : / / d o i . o r g / 1 \n0 . 3 3 8 9 / f r a i . 2 0 2 4 . 1 4 1 8 8 6 9     (2024).\n 25. Fonseka, T. M., Bhat, V . & Kennedy, S. H. The utility of artificial intelligence in suicide risk prediction and the management of \nsuicidal behaviors. Aust. N. Z. J. Psychiatry 53, 954–964. https://doi.org/10.1177/0004867419864428 (2019).\n 26. Zhu, Z., Ying, Y ., Zhu, J. & Wu, H. ChatGPT’s potential role in non-English-speaking outpatient clinic settings. Digit. Health 9, \n20552076231184092. https://doi.org/10.1177/20552076231184091 (2023).\n 27. Qian, H. et al. Pre-consultation system based on artificial intelligence has a better diagnostic performance than the physicians in \nthe outpatient department of pediatrics. Front. Med. (Lausanne) 8, 695185 (2021). https://doi.org/10.3389/fmed.2021.695185.\n 28. Tucci, V ., Saary, J. & Doyle, T. E. Factors influencing trust in medical artificial intelligence for healthcare professionals: A narrative \nreview. J. Med. Artif. Intell. 5, 4. https://doi.org/10.21037/jmai-21-25 (2022).\n 29. Shen, Y . et al. ChatGPT and other large language models are double-edged swords. Radiology 307, 2.  h t t p s : / / d o i . o r g / 1 0 . 1 1 4 8 / r a d i \no l . 2 3 0 1 6 3     (2023).\n 30. Wang, C. et al. Ethical considerations of using ChatGPT in health care. J. Med. Internet Res. 25, e48009.  h t t p s : / / d o i . o r g / 1 0 . 2 1 9 6 / 3 7 \n5 6 6 4 5 4     (2023).\n 31. Rotenstein, L. S. et al. Association between electronic health record time and quality of care metrics in primary care. JAMA Netw. \nOpen 5, e37086. https://doi.org/10.1001/jamanetworkopen.2022.37086 (2022).\n 32. Rasu, R. S., Bawa, W . A., Suminski, R., Snella, K. & Warady, B. Health literacy impact on national healthcare utilization and \nexpenditure. Int. J. Health Policy Manag. 4, 747–755. https://doi.org/10.15171/ijhpm.2015.151 (2015).\n 33. Wang, L. et al. Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. NPJ Digit Med. 7(1), \n41. https://doi.org/10.1038/s41746-024-01029-4 (2024).\nAuthor contributions\nM.M. conceived and designed the study. M.M., M.S., and M.K. were responsible for data collection and prepa -\nration. H.T., T.M., H.H., and K.C. contributed to the data analysis and interpretation. M.M. and K.C. drafted \nthe manuscript, and all authors critically reviewed and revised it for important intellectual content. All authors \napproved the final version of the manuscript and agree to be accountable for all aspects of the work, ensuring its \naccuracy and integrity.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 6 7 6 9 - 1     .  \nCorrespondence and requests for materials should be addressed to M.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:21346 10| https://doi.org/10.1038/s41598-025-06769-1\nwww.nature.com/scientificreports/",
  "topic": "Chatbot",
  "concepts": [
    {
      "name": "Chatbot",
      "score": 0.7754939198493958
    },
    {
      "name": "Ear nose and throat",
      "score": 0.7143656015396118
    },
    {
      "name": "Throat",
      "score": 0.695713460445404
    },
    {
      "name": "Nose",
      "score": 0.5470609664916992
    },
    {
      "name": "Computer science",
      "score": 0.5228631496429443
    },
    {
      "name": "World Wide Web",
      "score": 0.42340052127838135
    },
    {
      "name": "Medicine",
      "score": 0.3919213116168976
    },
    {
      "name": "Speech recognition",
      "score": 0.34405529499053955
    },
    {
      "name": "Anatomy",
      "score": 0.19436931610107422
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165735259",
      "name": "Gunma University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210094087",
      "name": "Maebashi Red Cross Hospital",
      "country": "JP"
    }
  ],
  "cited_by": 1
}