{
  "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
  "url": "https://openalex.org/W4385573236",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108963801",
      "name": "Wenhu Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2488937960",
      "name": "Hexiang Hu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1983188002",
      "name": "Xi Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4209920322",
      "name": "Pat Verga",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1989912291",
      "name": "William Cohen",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3209532394",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4386566764",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4287214436",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4224442590",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3170560353",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3024786184",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2998536339",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W4226278471",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W4286905627",
    "https://openalex.org/W3207095490",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3036878841",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3091588028"
  ],
  "abstract": "While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images – much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5558–5570\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nMuRAG: Multimodal Retrieval-Augmented Generator\nfor Open Question Answering over Images and Text\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\nGoogle Research\n{wenhuchen,hexiang,patverga,wcohen}@google.com\nAbstract\nWhile language Models store a massive\namount of world knowledge implicitly in their\nparameters, even very large models often fail\nto encode information about rare entities and\nevents, while incurring huge computational\ncosts. Recently, retrieval-augmented models,\nsuch as REALM, RAG, and RETRO, have\nincorporated world knowledge into language\ngeneration by leveraging an external non-\nparametric index and have demonstrated im-\npressive performance with constrained model\nsizes. However, these methods are restricted\nto retrieving only textual knowledge, neglect-\ning the ubiquitous amount of knowledge in\nother modalities like images – much of which\ncontains information not covered by any text.\nTo address this limitation, we propose the\nﬁrst Multimodal Retrieval-Augmented Trans-\nformer (MuRAG), which accesses an external\nnon-parametric multimodal memory to aug-\nment language generation. MuRAG is pre-\ntrained with a mixture of large-scale image-\ntext and text-only corpora using a joint con-\ntrastive and generative loss. We perform ex-\nperiments on two different datasets that re-\nquire retrieving and reasoning over both im-\nages and text to answer a given query: We-\nbQA, and MultimodalQA. Our results show\nthat MuRAG achieves state-of-the-art accu-\nracy, outperforming existing models by 10-\n20% absolute on both datasets and under both\ndistractor and full-wiki settings.\n1 Introduction\nPre-trained language models like GPT-3 (Brown\net al., 2020), PaLM (Chowdhery et al., 2022), etc\nhave been shown to capture a massive amount\nof world knowledge implicitly in their parame-\nters. However, using such large models incurs an\nextremely high computation cost. As an alterna-\ntive to a singular monolithic transformer, retrieval-\naugmented architectures like KNN-LM (Khandel-\nwal et al., 2019), REALM (Guu et al., 2020),\nFigure 1: Visual information-seeking queries: These\nqueries are unanswerable with text-only retrieval and\nrequire retrieving and reasoning over images.\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\n2021), and RETRO (Borgeaud et al., 2021) have\nbeen proposed to decouple world knowledge from\nthe model’s parameters. More speciﬁcally, these\nmodels are trained to access an external mem-\nory to enhance the model’s predictions. Such\nretrieval-augmented architectures have multiple\nbeneﬁcial properties including: decreased model\nsize (Borgeaud et al., 2021), better attribution/-\nexplanation for model predictions (Lewis et al.,\n2020), and adaptability to new information with-\nout retraining (Verga et al., 2021). However, pre-\nvious retrieval-augmented models are limited to\nmemories that contain only text or structured data\nand hence cannot make use of the massive amount\nof multimodal knowledge available on the web—\nmuch of which contains information only available\nin non-text modalities.\nFigure 1, shows several information-seeking\nqueries that require retrieving and reasoning over\nvisual knowledge. Here, a user ﬁrst poses a ques-\ntion such as “What can be found on the White\nHouse balconies at Christmas”. The system then\nretrieves relevant items from its memory, for exam-\n5558\nple, the ﬁrst image of Figure 1 with the caption\n“White House during Christmas”, which it uses to\nproduce the answer “wreaths and garlands”. Ex-\nisting text retrieval-augmented models would strug-\ngle with such queries because, in many cases, they\nwould simply not have access to the answer as some\nknowledge does not exist in text form. That, cou-\npled with the abundance of multimodal knowledge\nthat exists, leads to the conclusion that retrieval-\naugmented models should ultimately be developed\nto retrieve and reason over multiple modalities.\nFigure 2: Model Overview: retrieval-and-predict pro-\ncess of MuRAG on downstream datasets.\nIn this paper, we are speciﬁcally interested in\nendowing pre-trained language models with a non-\nparametric multimodal memory containing images,\ntext, or image-text pairs. To accomplish this, we\nﬁrst combine pre-trained T5 (Raffel et al., 2020)\nand ViT (Dosovitskiy et al., 2020) models to build\na backbone encoder (Figure 3), which encodes\nimage-text pairs, image-only, and text-only inputs\ninto a multimodal representation. MuRAG uses the\nbackbone encoder to embed items into an external\nmemory as well as queries to retrieve multimodal\nknowledge from that memory. These retrievals\nthen augment a language model to generate more\nvisually-grounded outputs.\nWe pre-train MuRAG with a mixture of\nimage-text and text-only datasets including\nLAION (Schuhmann et al., 2021), Conceptual-\nCaption (Sharma et al., 2018), VQA (An-\ntol et al., 2015) and Probably-Asked-Questions\n(PAQ) (Lewis et al., 2021). More speciﬁcally, we\nreformulate these datasets in a retrieve-and-predict\nformat. Here, the model’s input is an image along\nwith a text prompt. The model then retrieves from\na memory containing captions and passages, which\nit uses to generate a target token sequence. The\nmodel is trained with both a contrastive and a gen-\nerative loss; this teaches the model to discriminate\nrelevant from irrelevant memory entries, and guides\nthe model to leverage the multimodal knowledge\ninto generation.\nUnlike the pre-training stage, during ﬁne-\ntuning Figure 2 the model’s input is a question,\nand the memory contains a collection of captioned\nimages and text snippets. We ﬁne-tune MuRAG\non the downstream datasets with a contrastive and\ngenerative loss similar to pre-training. To avoid ex-\ncessive computation cost, we develop a two-stage\ntraining pipeline to ﬁrst train with small in-batch\nmemory, and then with a statically encoded and\nindexed large global memory.\nOur experiments show that MuRAG achieves\nstate-of-the-art performance on two different open-\nmultimodal-QA datasets, both of which require\nretrieving images and text from a large corpus to\nanswer factoid questions: WebQA (Chang et al.,\n2022) and MultimodalQA (Talmor et al., 2021). On\nboth datasets, we outperform sophisticated base-\nlines (Li et al., 2020; Radford et al., 2021; Zhang\net al., 2021) by 10-20% accuracy under both dis-\ntractor (from 40+ candidates) and full-wiki settings\n(from 1M candidates). We also perform a compre-\nhensive study to ablate different components of the\npre-training to see their contributions. These em-\npirical results demonstrate the effectiveness of our\nproposed models to integrate multimodal knowl-\nedge into pre-trained generation models and pave\nthe way to uniﬁed retrieval-augmented frameworks.\n2 Related Work\nRetrieval Augmented Models Retrieval aug-\nmented models are hybrid models containing\nboth parameterized sequence models and a non-\nparametric memory, infusing world knowledge into\nexisting language models. Among them, KNN-\nLM (Khandelwal et al., 2019) was ﬁrst proposed\nto retrieve instances from a text training corpus to\nhelp language modeling. Later, RETRO (Borgeaud\net al., 2021) was proposed to scale up the text cor-\npus to trillions of tokens, enabling the model to\nachieve similar perplexity to GPT-3 (Brown et al.,\n2020) with 25x fewer model parameters. Another\nfamily of models, such as REALM (Guu et al.,\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\nand Grave, 2021), integrate Wikipedia passages as\na datastore to beneﬁt downstream knowledge in-\ntensive tasks (e.g. Question Answering). REALM\nis an encoder-only model trained with masked lan-\n5559\nguage modeling, while RAG and FiD adopt an\nencoder-decoder model with a generative language\nmodeling objective. Compared to them, MuRAG\nis the ﬁrst retrieval-augmented model that is ca-\npable of using knowledge presented in multiple\nmodalities (i.e. visual and textual knowledge data),\nwhereas all prior methods are restricted to using\ntext-only knowledge.\nMultimodal Transformers Multimodal trans-\nformers have demonstrated strong performances\nin learning cross-modal representation that are gen-\nerally beneﬁcial on downstream vision and lan-\nguage tasks, such as image-text retrieval (Karpa-\nthy and Fei-Fei, 2015), image captioning (Chen\net al., 2015), and VQA (Antol et al., 2015). These\nmethods typically learn a joint transformer model\non top of unimodal visual and textual backbones,\nvia fusing deep features from each modality. The\nearly version of multimodal transformers (Lu et al.,\n2019; Chen et al., 2020; Li et al., 2020) usually\nlearns a Transformer on pre-extracted unimodal\nfeatures for contextualization, which makes it im-\npossible to adjust those unimodal features to the\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\nand COCA (Yu et al., 2022) proposed end-to-end\ntraining for both deep multimodal transformers and\nunimodal featurization networks and demonstrated\nstrong performance in both multimodal and uni-\nmodal downstream tasks. The multimodal memory\nencoder of MuRAG is broadly similar to SimVLM\nand CoCa, but has a different focus to encode and\nretrieve multimodal knowledge ( i.e. images and\ntexts) to augment language generation models.\nMultimodal Question Answering The problem\nof multimodal question answering has been ex-\ntensively studied. VQA was the ﬁrst proposed to\nanswer questions from visual-only inputs. Later,\nOK-VQA (Marino et al., 2019) enlarged VQA’s\nscope to annotate questions requiring both image\nand implicit textual/common-sense knowledge to\nanswer. More recently, MuMuQA (Reddy et al.,\n2021), ManyModelQA (Hannan et al., 2020) and\nMIMOQA (Singh et al., 2021) provide questions\nwhich require reasoning over images and explicitly\nprovided text snippets. However, these datasets\nare restricted to dealing with given text and images\nwithout requiring any retrieval from the web: they\nare analogous to machine-reading approaches to\nQA from text like SQuAD, rather than open-book\nQA. To study the more realistic open multimodal\nQA task, WebQA (Chang et al., 2022) and Multi-\nmodalQA (Talmor et al., 2021) have been proposed\nto evaluate answers to open queries which require\nretrieving and reasoning over a large-scale web\nmultimodal corpus. Our model uses these datasets\nto study open-world multimodal question answer-\ning, obtaining state-of-the-art results.\n3 Model\n3.1 Backbone Encoder\nFigure 3: Backbone encoder: ViT encodes image\npatches into a sequence of vectors eI, while word em-\nbedding converts text tokens into another sequence of\nvectors eT. These vectors are concatenated to form\nfθ(e) and fed to a decoder for text generation.\nMuRAG is built on top of a simpler model we\ncall a “backbone” model, which is pre-trained to\nencode image-text pairs such that they are suitable\nfor both answer generation and retrieval. The back-\nbone model’s encoder is used as a component of\nthe MuRAG model. The backbone model is built\nwith a pre-trained visual Transformer (Dosovitskiy\net al., 2020) and a T5 text Transformer (Raffel et al.,\n2020), and consists of a multimodal encoderfθ and\ndecoder gθ. The encoder takes as input a sequence\nof image-text pairs, where either the image or the\ntext component can be empty to accommodate text-\nonly and image-only cases.\nAs depicted in Figure 3, the encoder can take a\nsequence of images and text. For image input, we\nﬁrst split each into 16x16 patches and feed them\nto a ViT (Dosovitskiy et al., 2020) transformer to\ngenerate a sequence of visual embedding denoted\nas eI ∈RLi×D, where Li is the length of the im-\nage tokens. For text input, we use word embedding\nto produce another sequence of textual embedding\neT ∈RLt×D. For k images and n text inputs, we\nconcatenate all their embeddings in the input or-\nder as e = [e1\nI; e1\nT; ··· ; ek\nI; en\nT] ∈R(kLt+nLi)×D,\nwhich is fed to another bi-directional transformer\nfθ initialized from T5. We enable cross-attention\n5560\nbetween the two modalities to produce a fused rep-\nresentation, denoted as fθ(e) ∈ R(kLt+nLi)×D.\nWe add a [CLS] token to obtain a pooled repre-\nsentation fθ(e)[CLS] ∈RD for dense retrieval.\n3.2 MuRAG\nWe build MuRAG (shown in Figure 4) on top of\nthe backbone model. During the retriever stage,\nMuRAG takes a query q of any modality as in-\nput and retrieves from a memory Mof image-text\npairs. Speciﬁcally, we apply the backbone encoder\nfθ to encode a query q, and use maximum inner\nproduct search (MIPS (Guo et al., 2020)) over all of\nthe memory candidates m ∈M to ﬁnd the Top-K\nnearest neighbors TopK(M|q) = [ m1, ··· , mk].\nFormally, we deﬁne TopK(M|q) as follows:\nTopK(M|q) = TopK\nm∈M\nfθ(q)[CLS] ·fθ(m)[CLS]\nDuring the reader stage, the retrievals (the raw im-\nage patches) are combined with the query q as\nan augmented input [m1, ··· , mk, q], which is fed\nto the backbone encoder fθ to produce retrieval-\naugmented encoding. The decoder model gθ uses\nattention over this representation to generate tex-\ntual outputs y = y1, ··· , yn token by token.\np(yi|yi−1) = gθ(yi|fθ(TopK(M|q); q); y1:i−1)\nwhere y is decoded from a given vocabulary V.\n3.3 Pre-training\nThe pre-training implementation is depicted in the\nupper portion of Figure 4, where the input query\nis an image xI plus a text prompt xp. The exter-\nnal memory Mcontains textual-only entries mT.\nThe Top-K retrievalsmT\n1 , ··· , mT\nk are leveraged to\ngenerate the textual output. To avoid the excessive\ncomputation cost of backpropagation over the mas-\nsive external memory, we adopt an in-batch mem-\nory MB, dynamically constructed from the input\nexamples in a batch. The small in-batch memory\nenables MuRAG to continuously update the mem-\nory encoder efﬁciently similar to TOME (de Jong\net al., 2022) and QAMAT (Chen et al., 2022).\nDataset The pre-training corpus consists of\nLAION (Schuhmann et al., 2021), Conceptual-\nCaption-12M+3M (CC) (Sharma et al., 2018;\nChangpinyo et al., 2021), VQA (Antol et al., 2015)\nand PAQ (Lewis et al., 2021) Table 1. LAION is\na publicly-released image-text dataset containing\ncrawled image-text pairs ﬁltered by CLIP (Rad-\nford et al., 2021). We apply rules to ﬁlter LAION\nfrom 400M to 200M by removing text with HTTP-\nURLs or image width/height beyond 1000 pixels.\nCC contains 15M (image, anonymized alt-text)\npairs crawled from the web but ﬁltered more ex-\ntensively to maintain high alignment quality. VQA\ncontains annotated QA pairs aligned to MSCOCO\nimages. We further add captions to each image\nfrom MSCOCO-Captioning (Lin et al., 2014) to\ncreate (Image, Caption, QA) triples. PAQ is a text-\nonly dataset containing 65M machine-generated\nQA pairs along with their source Wikipedia pas-\nsage.\nDataset #Size Format Source\nCC 15M (Image, Caption) Crawled\nLAION 200M (Image, Alt-Text) Crawled\nPAQ 65M (Passage, QA) Generated\nVQA 400K (Image, Caption, QA) Annotated\nTable 1: Pre-training Dataset Statistics\nFor LAION and CC, we use the input image as\nxI, and ‘generate caption:’ as the text promptxp.\nFor VQA, we use the input image as xI and the\nquestion as the prompt xp. For PAQ, we use an\nempty array as the input image and the question\nas the prompt. The in-batch memory MB is con-\nstructed by stacking the captions associated with\nthe input images in LAION/CC/VQA and the pas-\nsages associated with the questions in PAQ. Each\ntextual memory entry is denoted as mT. The de-\ncoder is optimized to generate either a caption or\nan answer, depending on the source dataset. Since\nthe four dataset sizes are highly unbalanced, we\nuse ﬁxed mixture sampling ratios to balance their\npresence during pre-training.\nWe train the model with a joint loss L = Lgen +\nLcon as follows:\nLcon = −log exp(fθ(xI, xp) ·fθ(mT))∑\nm∈MB\nexp(fθ(xI, xp) ·fθ(mT))\nLgen = −log gθ(y|fθ(Mp; xI; xp))\nMp =\n{\nTopK(MB|xI, xp) If (xI, xp) ∈PAQ/VQA\nØ If (xI, xp) ∈LAION/CC\nwhere Mp is the retrieved augmentation: if the\ninput query is from PAQ/VQA, we use the retrieved\nmemory entries, otherwise, we use null. The reason\nfor setting it to null for LAION/CC is to avoid a\ntrivial solution when the generation target (caption)\nalso exactly appears in the memory.\nThe contrastive loss Lcon is minimized to dis-\ncriminate between the positive query-memory pairs\n5561\nFigure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\nin images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\nimplementation, while the lower part deﬁnes ﬁne-tuning implementation.\nand all other query-memory pairs from the mem-\nory. The pairwise matching score is computed as\nthe dot product between query fθ(xI; xp)[CLS] and\ncandidates fθ(mT)[CLS]. This objective enables\nthe model to retrieve the most relevant knowledge\nfrom the memory. The generative loss Lgen is min-\nimized to generate target tokens y conditioned on\nthe retrieval-augmented representation. This ob-\njective enables the model to combine information\nacross different modalities for text generation.\n3.4 Fine-tuning\nWe ﬁnetune MuRAG to align with the expected\ninputs of the downstream datasets which require an-\nswering text questions by retrieving image-caption\npairs or text snippets from the external knowledge\ndatastore. As depicted in the lower part of Figure 4,\nthe input query for the downstream task is a text\nquestion xq, and the memory Mcontaining (im-\nage, text) pairs (mI, mT).1 The Top-K retrievals\n{(mI\n1, mT\n1 ), ··· , (mI\nk, mT\nk)}are leveraged to gen-\nerate the answer a. To minimize the computation\ncost, we develop a two-stage pipeline to optimize\nwith an in-batch memory and then resume with\nﬁxed retrieval from global memory.\nIn-Batch Training In this stage, we aim to mini-\nmize the joint loss functionL = Lcon+Lgen based\n1We set the image to a zero array if the memory entry is a\ntext snippet.\non the in-batch memory MB as follows:\nLcon = −log exp(fθ(xq) ·fθ(mI; mT))∑\nm∈MB\nexp(fθ(xq) ·fθ(mI; mT))\nLgen = −log gθ(y|fθ(TopK(MB|xq); xq))\nThe in-batch memory MB is constructed in the\nfollowing way: the k-th example in the dataset is\nrepresented as (xq,k, yk, {mI\ni, mI\ni}k, {¯mI\nj, ¯mT\nj }k),\nwhere m represents the positive (image, text)\nsource, and ¯m represents the hard negative\n(image, text) source provided by the dataset 2.\nFor a batch with B examples, we assemble\nall the associated positive and negative knowl-\nedge source as our in-batch memory MB =\n{{mI\ni, mI\ni}1, {¯mI\nj, ¯mT\nj }1, ··· , {¯mI\nj, ¯mT\nj }B}.\nFixed-Retrieval Training After in-batch train-\ning, we encode all available cross-modal pairs, and\nindex these encodings for fast MIPS retrieval. We\nthen apply the trained retriever to search over the\nfull multimodal corpus Mto obtain the global top-\nK retrievals TopK(M|xq) and continue to opti-\nmize Lgen. During this training phase, the stored\nencodings are not updated. During inference time,\nwe use ﬁxed encodings to generate the answers.\n2These hard negatives are mined through Bing Search API\nand Wikipedia page, refer to (Chang et al., 2022) for details.\n5562\n4 Experiments\n4.1 Implementation Details\nThe backbone model uses T5-base (Raffel et al.,\n2020) and a ViT-large model (Dosovitskiy et al.,\n2020) as described in Table 2. We adopt the\nsentence-piece model from T5 with a vocabulary\nsize of 32128. The ViT model was pre-trained\non the JFT dataset. We resize every image into\n224x224 pixels and split them into a sequence of\n16x16 patches. The output of ViT is a sequence\nof 1024-dimension vectors, which are projected\nto 768-dimension for consistency with T5 model.\nMuRAG reuses the model as retriever and reader,\nthus the full model size is 527M parameters.\nModel #Enc #Dec Hidden Heads Params\nViT-large 24 0 1024 16 307M\nT5-base 12 12 768 12 220M\nTable 2: The model size and conﬁgurations, with\n#Enc/#Dec denoting encoder/decoder layers.\nOur model is implemented in JAX (Bradbury\net al., 2018), based on the T5X codebase (Roberts\net al., 2022). During pre-training, we ﬁrst train the\nmodel on LAION for 1M steps, and then continue\ntraining on CC/PAQ/VQA with 1:1:1 sample ratio\nfor another 200K steps. We optimize the model\nwith Adafactor (Shazeer and Stern, 2018). For both\nstages, we adopt a constant learning rate of 5e-4\nand a batch size of 4096. The models are trained\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\nWe then ﬁne-tune MuRAG on WebQA and Mul-\ntimodalQA with a constant learning rate of 3e-4\nfor 20K steps. The checkpoint with the highest\nvalidation score is run on the test set. We use a\nbatch size of 64 and set TopK=4 for both in-batch\ntraining and ﬁxed-retrieval training. We noticed\nthat increasing Top-K further does not yield further\nimprovement. We use a beam size of 2 to search\nfor the best hypothesis for both datasets (increasing\nit further doesn’t yield better performance).\n4.2 Datasets\nFor evaluation, we choose two multimodal QA\ndatasets: WebQA (Chang et al., 2022) and Mul-\ntimodalQA (Talmor et al., 2021) and demonstrate\ntheir statistics in Table 3.\nWebQA This dataset contains multi-hop, multi-\nmodal question-answer pairs where all questions\nare knowledge-seeking queries. The queries re-\nquire 1-2 images or 1-2 text snippets to answer.\nDataset Train Dev Test\nImage/Text Image/Text Image/Text\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\nMultimodalQA 2.1K/7.4K 230/721 -\nTable 3: Overall Statistics of downstream dataset.\nEach query in WebQA is associated with a set of\nvisual/text distractors (hard negatives). The an-\nswers in WebQA are normally complete sentences\nto better assess the model’s generation capabil-\nity. Two evaluation setups are used, namely dis-\ntractor and full-wiki. Under the distractor setup,\nthe model needs to retrieve from these hard neg-\natives + positives to answer the question. Under\nthe full-wiki setup, the model needs to search over\n1.1M text and visual sources from Wikipedia to an-\nswer the question. For evaluation, WebQA uses\nBARTScore (Yuan et al., 2021) to measure the\nﬂuency between the generation and the reference,\nand keyword accuracy score to measure the cor-\nrectness/truthfulness of the generation. These two\nscores are multiplied to calculate the overall score.\nMultimodalQA-Subset This dataset contains\nhuman-annotated multimodal questions over differ-\nent modalities including tables, text, and images.\nWikipedia tables are used as anchors to connect dif-\nferent modalities. The authors ﬁrst use the template\nto generate questions and then ask crowd-workers\nto ﬁlter and paraphrase the generated questions.\nSince tables are outside the scope of our paper, we\nfocus on the subset of queries requiring only text\nand image information. Speciﬁcally, we choose the\nquestions with types of ‘TextQ’ and ‘ImageQ’ to\nconstruct the subset. The query requires 1 image\nor 1 text snippet to answer. Each query in Multi-\nmodalQA is also associated with visual and text dis-\ntractors (hard negatives). Similarly, two evaluation\nsetups are used as before. Under a full-wiki setup,\nMultimodalQA uses a database containing 500K\ntext and visual sources. The evaluation scores are\nbased on Exact Match and F1.\n4.3 Baselines\nFor WebQA and MultimodalQA, we mainly\ncompare different variants of pre-trained vision-\nlanguage models.\nVLP In WebQA, VLP-like models (Zhou et al.,\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\net al., 2021) are used as the standard baselines.\nThese models were pre-trained on Conceptual\n5563\n3M (Sharma et al., 2018) with a masked language\nobjective. During ﬁne-tuning, the VLP model takes\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\nﬁrst to select the most plausible source si, and then\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\nto autoregressively decode answer A with masked\nlanguage model prediction.\nAutoRouting In MultimodalQA, this method\nﬁrst applies a question type classiﬁer to detect the\nmodality of the question (either a passage or an\nimage), and then routes the question to its sub-\nmodel. The method uses RoBERTa-large (Roberts\net al., 2022) for text-questions and VilBERT (Lu\net al., 2019) with features extracted from Faster-\nRCNN (Ren et al., 2015) for image questions.\nCLIP (K) CLIP (Radford et al., 2021) is used for\nfull-wiki retrieval. Speciﬁcally, the baselines sys-\ntems adopt CLIP to encode queries and all the im-\nage/text candidates separately into vectors and then\nrun approximated nearest neighbor searches to ﬁnd\na set of K potential candidates. After the coarse-\nlevel retrieval without cross-attention, it adopts a\nreranker to further narrow down to the 1-2 candi-\ndates to feed as input S to the QA model.\n4.4 Experimental Results\nWe demonstrate WebQA’s results in Table 4. All\nresults reported are the medium score from three\nruns with different random seeds, and the variance\nof the Overall score is within 0.2%. We can observe\nthat MuRAG can signiﬁcantly outperform VLP\nwith different backends including Oscar, ResNet,\nand VinVL. In retrieval performance, our model\noutperforms VLP by 15% in the full-wiki setting.\nFor Fluency, our model outperforms VLP by 12%\nunder the distractor setting and 14% under the full-\nwiki setting. For Accuracy, our model manages\nto achieve 16% under the distractor setting and\neven 20% the under the full-wiki setting. These\nimprovements reﬂect the high ﬂuency and accuracy\nof MuRAG’s generation, and the improvement is\nmore pronounced for full wiki.\nWe show the MultimodalQA results in Table 5.\nWe can see that MuRAG is also able to vastly\noutperform the routing-based multimodality QA\nmodel. For text questions, our model improves\nover AutoRouting by 10+% EM under both set-\ntings. For image questions, the gap becomes more\nsigniﬁcant, with 20+% improvement under both\nsettings. Similarly, we ﬁnd that our model is more\ncapable of handling full-wiki corpus.\nEvaluation Distractor\nMetrics Retr FL Accuracy Overall\nQuestion-Only - 34.9 22.2 13.4\nVLP (Oscar) 68.9 42.6 36.7 22.6\nVLP + ResNeXt 69.0 43.0 37.0 23.0\nVLP + VinVL 70.9 44.2 38.9 24.1\nMuRAG 74.6 55.7 54.6 36.1\nEvaluation Full-Wiki\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\nMuRAG 39.7 50.7 47.8 31.5\nTable 4: WebQA ofﬁcial test-set results indicated\non leaderboard 3 as of May 2022. Retr denotes\nthe retrieval-F1 score. FL refers to ﬂuency metric\nBARTSCcore, and Accuracy refers to keyword match-\ning F1 score, they are combined as Overall.\nEvaluation Distractor\nMetrics Text Image All\nEM F1 EM F1 EM\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\nAutoRouting 49.5 56.9 37.8 37.8 46.6\nMuRAG 60.8 67.5 58.2 58.2 60.2\nEvaluation Full-Wiki\nMetrics Text Image All\nEM F1 EM F1 EM\nCLIP (10) +\nAutoRouting 35.6 40.2 32.5 32.5 34.7\nMuRAG 49.7 56.1 56.5 56.5 51.4\nTable 5: Multimodal dev-set results on the subset.\n4.5 Ablation Study\nHere we ablate the properties of MuRAG to better\nunderstand our experimental results.\nPre-training Corpus In order to study the contri-\nbutions of different pre-training corpora, we investi-\ngated several pre-training corpus combinations. We\nreport their ﬁne-tuned results on WebQA test set\nin Table 6. As can be seen, without any pre-training,\nour model only achieves an overall score of 23.5,\nwhich lags behind the baseline models. After pre-\ntraining on different singular datasets, MuRAG is\nable to achieve better performance than the base-\nlines. Among the individual datasets, LAION is\nshown to yield the highest score, and adding CC,\nPAQ, and VQA to the pre-training corpus set one\nby one produces steady improvements.\nTwo-Stage Fine-tuning In order to study the ne-\ncessity of the two-stage ﬁne-tuning, we perform an\nablation study to see the impact of the two stages.\nWe display our results in Table 7. (Only In-Batch)\n5564\nPre-train Dataset FL Accuracy Overall\nNone 42.5 36.1 23.5\nCC 46.4 41.3 25.6\nLAION 47.8 44.8 28.3\nVQA 47.0 44.4 27.4\nPAQ 46.8 42.8 27.0\nLAION+CC 49.5 47.4 30.7\nLAION+CC+PAQ 53.7 51.8 34.4\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\nTable 6: Ablation Study for different pre-training cor-\npus, score under distractor setting.\nModel WebQA Multimodal\nMuRAG (Only In-Batch) 29.4 49.6\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\nMuRAG (Two Stage) 31.5 51.4\nTable 7: Ablation Study for different ﬁne-tuning stages\nto see their contributions. WebQA uses the overall\nscore, and MultimodalQA refers to EM-all score.\nEvaluation Model Correct Wrong\nDistractor MuRAG (Text) 80% 20%\nMuRAG (Image) 64% 36%\nFull-Wiki MuRAG (Text) 72% 28%\nMuRAG (Image) 54% 46%\nTable 8: The human evaluation results on WebQA\ndataset separately for image/text queries.\nrefers to the model trained only with in-batch mem-\nory are directly used to generate outputs by access-\ning the global memory. Without further tuning,\nthe performance will drop by roughly 2% on both\ndatasets. (Only Fixed-Retrieval) refers to using the\npre-trained retriever directly to obtain Top-K and\nthen optimize the generative loss. As can be seen,\nthe performance drop is more severe in this case\nfor both datasets. This is understandable due the\nmisalignment between pre-training retrieval is (im-\nage + text->text) while the ﬁne-tuning retrieval is\n(text -> image+text). Thus, it is necessary to adapt\nthe MuRAG’s pre-trained retriever to different use\ncases depending on the downstream datasets.\n4.6 Human Analysis\nIn order to better understand the model’s perfor-\nmance, we manually study 200 model outputs and\nclassify them into three categories and show our\nmanual analysis results in Table 8. As can be seen,\nimage queries are much harder than text queries.\nMuRAG only achieves 64% accuracy for the dis-\ntractor setting and 54% accuracy for the full-wiki\nsetting, falling signiﬁcantly behind text accuracy.\nWe further categorize the image-query errors\nFigure 5: Upper left: correct prediction, Upper Right:\nerror due to miscounting, Lower: error due to misrecog-\nnition (multiple image reasoning). Q refers to the ques-\ntion, P refers to prediction and R refers to the reference.\nmanually into the categories of Table 9. Counting\nis the most difﬁcult question type, and constitutes\n52% of the total errors, while object recognition\nerrors rank second, constituting 29% of errors. In\ncontrast, identifying color, shape, and gender is\ncomparatively easier, with fairly low error rates.\nWe demonstrate some correct and typical error\ncases in Figure 5 including miscounting and mis-\nrecognizing objects. We observe that these errors\nare mostly due to several reasons: 1) the question\nis related to infrequent objects, thus making recog-\nnition errors, 2) the image scene is highly complex\nwith a large number of objects, thus grounding to a\nspeciﬁc region is difﬁcult, 3) the questions require\noptical character recognition ability from images.\nHence, the bottleneck of MuRAG is still in the\nvisual understanding module.\nCategory Count Object Color Shape Gender\nRatio 52% 29.4% 5.8% 5.8% 5.8%\nTable 9: Error categorization and their ratios on sam-\npled WebQA-dev image queries.\n5 Examples\nWe list more examples in Figure 6 and Figure 7.\nAs can be seen, in the ﬁrst example, the model is\n5565\ngrounded on the oracle image-text pair to make the\ncorrect prediction. However, in the second exam-\nple, though the model retrieves the wrong image-\ntext pair, it is able to make the correct prediction of\n‘the angel is holding a dead body’. We conjecture\nthat the model utilizes textual clues to make the pre-\ndiction rather than grounding on the image itself.\nSuch shortcut learning is concerning and needs to\nbe addressed through better learning algorithms.\nFigure 6: Examples: we demonstrate model retrieval\nvs. groundtruth and model answer vs. reference.\n6 Conclusion\nIn this paper, we build the ﬁrst visually-grounded\nlanguage generator capable of retrieving multi-\nmodal knowledge from a large-scale corpus. Our\nexperiments show the promise of this approach, as\nit outperforms existing baselines by a large margin.\nAt the same time, the performance on knowledge-\nseeking queries that require reasoning over images\nis still signiﬁcantly lower than the performance on\nqueries requiring only text. This indicates that there\nis still ample room for further improvements and\nwe hope our study can motivate more research on\nbetter multimodal retrieval-augmented models.\nLimitations\nThe current approach has several limitations: 1)\nsince we do not mine hard negatives during pre-\ntraining, negatives come from other examples\nwithin the same batch. This requires that we set the\nbatch size sufﬁciently large enough to collect hard-\nenough negatives. This results in the pre-training\nFigure 7: Examples: we demonstrate model retrieval\nvs. groundtruth, and model answer vs. reference.\nrequiring a large number of computation resources\nto reach competitive retrieval abilities. 2) our pre-\ntraining corpus’s format (image -> text) is differ-\nent from ﬁne-tuning (text -> image+text). This\nmisalignment limits the model’s performance. Fu-\nture work should consider how to design a better-\naligned pre-training objective to achieve better per-\nformance. 3) Current visual representation in the\nreader stage is relatively expensive, i.e. 16x16=196\ntokens per image, which poses great challenges for\nthe transformer encoder to scale up to large Top-K\nvalues due to the quadratic attention complexity.\nEthical Statement\nOur work uses the LAION dataset, a widely-used\nand publicly available large-scale visual-language\ncorpus crawled from the web. The authors have\nconducted automatic ﬁltering to greatly reduce\nharmful content. However, it is not possible to\nfully remove all of the potential risks from the data\ngiven its tremendous size. Being trained on this\ndataset, we anticipate our model to contain some\nbiases (racial, gender, etc.). During our manual\ninspection, we saw some such biases, for example,\n5% of errors are caused by misrecognition of gen-\nder. However, there are other many other forms of\nbiases that we cannot fully enumerate or observe\n5566\nexplicitly.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\nWebqa: Multihop and multimodal qa. The Confer-\nence on Computer Vision and Pattern Recognition.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 3558–3568.\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\neting, and William Cohen. 2022. Augmenting\npre-trained language models with qa-memory for\nopen-domain question answering. arXiv preprint\narXiv:2204.04581.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server.arXiv preprint\narXiv:1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European conference on\ncomputer vision, pages 104–120. Springer.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Fei Sha, and William Cohen. 2022. Mention\nmemory: incorporating textual knowledge into trans-\nformers through entity mention attention. ICLR.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. In International\nConference on Learning Representations.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\nAccelerating large-scale inference with anisotropic\nvector quantization. In International Conference on\nMachine Learning, pages 3887–3896. PMLR.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 3929–3938. PMLR.\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\nManymodalqa: Modality disambiguation and qa\nover diverse inputs. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 34, pages\n7879–7886.\nGautier Izacard and Édouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 874–880.\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\nand David Patterson. 2020. A domain-speciﬁc\nsupercomputer for training deep neural networks.\nCommunications of the ACM, 63(7):67–78.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n3128–3137.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\n5567\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\nmillion probably-asked questions and what you can\ndo with them. Transactions of the Association for\nComputational Linguistics, 9:1098–1115.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. Advances in neural information processing\nsystems, 32.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\nquestion answering benchmark requiring external\nknowledge. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recogni-\ntion, pages 3195–3204.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\nhit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\nMumuqa: Multimedia multi-hop news question an-\nswering via cross-media knowledge extraction and\ngrounding. arXiv preprint arXiv:2112.10728.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. Advances\nin neural information processing systems, 28.\nAdam Roberts, Hyung Won Chung, Anselm Lev-\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\nels and data with t5x and seqio. arXiv preprint\narXiv:2203.17189.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. 2021. Laion-400m: Open dataset of clip-\nﬁltered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2556–2565.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning ,\npages 4596–4604. PMLR.\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\nvasan. 2021. Mimoqa: Multimodal input multi-\nmodal output question answering. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5317–5332.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\nmodalqa: complex question answering over text, ta-\nbles and images. In ICLR.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam Weston Cohen. 2021. Adaptable and inter-\npretable neural memory over symbolic knowledge.\nIn Proceedings of NAACL-HLT, pages 3678–3691.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. ICLR.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5579–5588.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\nﬁed vision-language pre-training for image caption-\ning and vqa. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 13041–\n13049.\n5568\nA Pre-training\nDuring Pre-trainnig, we found that directly train-\ning with a mixture of all four datasets will lead to\ninstability. We experimented with different vari-\nants and found that a scheduled pre-training can\nlead to a stable solution. We propose to ﬁrst pre-\ntrain the model on the largest LAION dataset for\n1M steps, and then continue training on the other\nthree datasets with a ﬁxed sample ratio. We plot\nthe ﬁrst stage of LAION training in Figure 8. We\nmonitor the generation quality (LAION image ->\ntext captioning), and the retrieval quality (image ->\n4096 in-batch caption retrieval). As can be seen,\nthe LAION pre-training converges after 1M steps,\nwhere we ﬁrst warm up and then decrease the learn-\ning rate using a scheduler.\nFigure 8: LAION Pre-training, validation accuracy,\ngeneration Cider score and retrieval recall score from\nthe in-batch memory.\nWe further the pre-training on a mixture of the\nother three datasets. We plot their inference eval-\nuation scores in Figure 9. We can see that the\nmodel is able to achieve very strong performance\non these datasets, i.e. higher than 1.2 CiDEr\non CC12M+3M validation set. The model also\nachieves strong performance on text-only reading\ncomprehension on PAQ (similar to NQ), i.e. higher\nthan 55% EM score. On the VQA dataset, the\nmodel is able to achieve higher than 72% VQA ac-\ncuracy on the validation set. These results demon-\nstrate the efﬁciency and multi-tasking capabilities\nof the pre-trained model. The overall retrieval\naccuracy from the multimodal memory consist-\ning of captions, and passages are plotted in Fig-\nure 10, where the model is able to achieve 85%\nRECALL@1 from a 4K memory.\nB Model Conﬁguration\nWe demonstrate the ViT conﬁguration as follows:\n\" v i t _ c o n f i g \" : {\n\" model \" : \" ViT \" ,\n\" p a t c h e s \" : {\n\" s i z e \" : [ 1 6 , 16]\n} ,\n\" h i d d e n _ s i z e \" : 1024 ,\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\n\" num_heads \" : 16 ,\n\" n u m _ l a y e r s \" : 24 ,\n\" mlp_dim \" : 4096 ,\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\n\" d r o p o u t _ r a t e \" : 0 . 1\n} ,\nWe demonstrate the T5-EncDec conﬁguration as\nfollows:\n\" m o d e l _ c o n f i g \" : {\n\" v o c a b _ s i z e \" : 32128 ,\n\" h i d d e n _ s i z e \" : 768 ,\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\n\" memory_key_dim \" : 768 ,\n\" e n c o d e r _ l a y e r s \" : 12 ,\n\" d e c o d e r _ l a y e r s \" : 12 ,\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\n\" m a x _ d i s t a n c e \" : 128 ,\n\" num_buckets \" : 32 ,\n\" s c a l e \" : 1 . 0 ,\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\n}\n5569\nFigure 9: Mixture Pre-training, CiDEr, EM, and VQA\naccuracy for CC, PAQ, and VQA datasets.\nFigure 10: Mixture Pre-training retrieval accuracy over\nCC, PAQ, and VQA datasets.\n5570",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8886414766311646
    },
    {
      "name": "Transformer",
      "score": 0.6663053035736084
    },
    {
      "name": "Language model",
      "score": 0.5952524542808533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5791003108024597
    },
    {
      "name": "ENCODE",
      "score": 0.5568493604660034
    },
    {
      "name": "Question answering",
      "score": 0.5418006777763367
    },
    {
      "name": "Natural language processing",
      "score": 0.5078142285346985
    },
    {
      "name": "Generative grammar",
      "score": 0.4773521423339844
    },
    {
      "name": "Parametric statistics",
      "score": 0.46440672874450684
    },
    {
      "name": "Modalities",
      "score": 0.4238080084323883
    },
    {
      "name": "Information retrieval",
      "score": 0.40259304642677307
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ]
}