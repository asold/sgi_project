{
  "title": "Large Language Models as a Substitute for Human Experts in Annotating Political Text",
  "url": "https://openalex.org/W4387220962",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1851028635",
      "name": "Michael Heseltine",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2619433690",
      "name": "Bernhard Clemm von Hohenberg",
      "affiliations": [
        "GESIS - Leibniz-Institute for the Social Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4211098108",
    "https://openalex.org/W4361193900",
    "https://openalex.org/W2993698134",
    "https://openalex.org/W4321175700",
    "https://openalex.org/W4323697401",
    "https://openalex.org/W6831098966",
    "https://openalex.org/W4323239061",
    "https://openalex.org/W3049565363",
    "https://openalex.org/W4313679888",
    "https://openalex.org/W3027440908",
    "https://openalex.org/W4406376809",
    "https://openalex.org/W1995863888",
    "https://openalex.org/W4306732251",
    "https://openalex.org/W4360818870",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W4223578676",
    "https://openalex.org/W4234173896",
    "https://openalex.org/W1989827237",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W4365601444"
  ],
  "abstract": "Large-scale text analysis has grown rapidly as a method in political science and beyond. To date, text-as-data methods rely on large volumes of human-annotated training examples, which places a premium on researcher resources. However, advances in large language models (LLMs) may make automated annotation increasingly viable. This paper tests the performance of GPT-4 across a range of scenarios relevant for analysis of political text. We compare GPT-4 coding with human expert coding of tweets and news articles across four variables (whether text is political, negativity, sentiment, and ideology) and across four countries (the United States, Chile, Germany, and Italy). GPT-4 coding is highly accurate, especially for shorter texts such as tweets, correctly classifying texts up to 95\\% of the time. Performance drops for longer news articles, and very slightly for non-English text. We introduce a ``hybrid'' coding approach, in which disagreements of multiple GPT-4 runs are adjudicated by a human expert, which boosts accuracy. Finally, we explore downstream effects, finding that transformer models trained on hand-coded or GPT-4-coded data yield almost identical outcomes. Our results suggests that LLM-assisted coding is a viable and cost-efficient approach, although consideration should be given to task complexity.",
  "full_text": "Large Language Models as a Substitute for\nHuman Experts in Annotating Political Text\nMichael Heseltine1 and Bernhard Clemm von Hohenberg 2\n1Amsterdam School of Communication Research, University of Amsterdam, m.j.heseltine@uva.nl\n2GESIS Leibniz Institute for the Social Sciences, Cologne, bernhard.clemm@gesis.org\nSeptember 29, 2023\nAbstract\nLarge-scale text analysis has grown rapidly as a method in political science and\nbeyond. To date, text-as-data methods rely on large volumes of human-annotated\ntraining examples, which places a premium on researcher resources. However, ad-\nvances in large language models (LLMs) may make automated annotation increasingly\nviable. This paper tests the performance of GPT-4 across a range of scenarios relevant\nfor analysis of political text. We compare GPT-4 coding with human expert coding\nof tweets and news articles across four variables (whether text is political, negativity,\nsentiment, and ideology) and across four countries (the United States, Chile, Germany,\nand Italy). GPT-4 coding is highly accurate, especially for shorter texts such as tweets,\ncorrectly classifying texts up to 95% of the time. Performance drops for longer news\narticles, and very slightly for non-English text. We introduce a “hybrid” coding ap-\nproach, in which disagreements of multiple GPT-4 runs are adjudicated by a human\nexpert, which boosts accuracy. Finally, we explore downstream effects, finding that\ntransformer models trained on hand-coded or GPT-4-coded data yield almost identical\noutcomes. Our results suggests that LLM-assisted coding is a viable and cost-efficient\napproach, although consideration should be given to task complexity.\nKeywords— large language models; GPT; machine learning; text analysis; text-as-data\n1\nPolitical science has increasingly embraced supervised machine learning as an accurate and\ncutting-edge tool in the large-scale analysis of political text, greatly supported by ready-to-use\nmethods such as the transformer-based text classifier BERT. Existing applications range from\nanalyses of elite rhetoric (Ballard et al., 2023), to classifications of news sentiment (Rozado et al.,\n2022), and the detection of hate speech (Mozafari et al., 2020). However, one central limitation\nof these methods is that each classification requires large amounts of human-annotated training\ndata. Depending on the complexity of the task, reliable modelling requires training data ranging\nin the 1,000s to 10,000s of annotated text examples, often from multiple coders. This places severe\nfinancial and time constraints on researchers.\nRecent works have, however, shown the potential for large language models (LLMs) such as the\nGPT family to perform a range of tasks in the social sciences, including ideological scaling (Wu\net al., 2023), the classification of legislation (Nay, 2023), and the detection of hate speech (Huang\net al., 2023). LLM classification may therefore be a viable means of reducing manual annotation\nlabour and cutting costs, while providing high levels of classification accuracy or even outperforming\nhuman coders (Gilardi et al., 2023; Ornstein et al., 2023; Tornberg, 2023). Additionally, LLMs have\nalso shown the potential for the accurate classification of texts across languages (Kuzman et al.,\n2023), opening up avenues for research in languages not spoken by researchers.\nWith these developments in mind, this paper evaluates the potential for large language models\nto act as a substitute for manual text coding in political science (and potentially beyond). Since\nprevious studies have focused primarily on single tasks (Huang et al., 2023; Kuzman et al., 2023;\nTornberg, 2023) or single contexts (Gilardi et al., 2023; Nay, 2023; Ornstein et al., 2023) and do not\ntest downstream effects, further exploration is warranted. We assess the accuracy of coding with\nGPT-4—the most up-to-date version of the GPT client 1—across a range of text annotation tasks\nubiquitous in political science, namely determining whether text is political, whether it transports\nnegativity (both as a binary and a multi-category variable) and scaling its ideological leaning.\nWe offer several contributions to the fast-evolving literature on LLM-assisted methodology:\nFirst, as the bulk of extant evidence tests GPT coding performance on short text such as tweets\n1While ChatGPT has been the recent focus of debate, the LLM space is fast-evolving, with ChatGPT\n(based on GPT v3.5) already superseded by GPT-4.\n2\n(but Gilardi et al., 2023), we assess performance also for longer texts, i.e., news articles. Second,\nfew studies test GPT coding performance beyond the U.S., despite its known cultural bias (Johnson\net al., 2022). We advance the literature by testing GPT-4 coding accuracy in three other languages\nand contexts, namely Chile, Germany, and Italy. Third, we complement previous work by testing\na “hybrid” coding approach of humans supporting the machine, which is still much cheaper than\nhuman coding, but potentially more accurate. Fourth, we explore downstream impacts of differ-\nences between expert and GPT-4 coding using a case study of political rhetoric in the U.S. 2022\nCongressional primary elections.\nThe results show, first, that GPT-4 coding can be highly accurate, achieving as much as 91%\nagreement with expert coding on the classification of political rhetoric, 95% agreement on the\nclassification of negative rhetoric (binary), 82% on sentiment (three categories) and 85% on ideology.\nThrough our hybrid approach, which includes minimal levels of hand-validation (typically less than\n10% of the full training set), accuracy of all measures can be further improved. Second, GPT-4’s\nperformance drops slightly for full news articles compared to tweets, suggesting potential limitations\nto automated classification depending on the specific text format. Third, promisingly, GPT-4\ngenerally shows similar (though slightly lower) levels of accuracy in non-English classification of\ntweets, suggesting that GPT-4 coding is a viable option for researchers working with data across\nlanguages. Last, downstream, the modeling based on manual and GPT-4 coding produced almost\nidentical results, suggesting that the level of disagreement between human and GPT-4 coding may\nhave minimal implications for substantive research. We consider the limitations of our approach in\nmore depth in the Discussion.\nData and Method\nOur baseline test of accuracy is based on a sample of 635 tweets from Members of Congress in\nthe United States, randomly chosen out of all their tweets from between 2009 and 2022. To test\nthe effect of text length on classification accuracy, we also collected a random sample of 200 news\narticles from 2016 and 2017 across a range of U.S. news outlets (NYT, WaPo, Bloomberg, Breitbart,\n3\nVox, The Atlantic). To test accuracy across languages and contexts, we further selected a random\nsample of tweets from all tweets posted by members of parliament between 2009 and 2022 in Chile\n(330 tweets), Germany (700 tweets) and Italy (330 tweets).2 Although this selection of countries is\nby no means exhaustive and was influenced by our own expertise and access to expert coders, this\nmulti-country approach still goes beyond existing U.S.-focused evidence.\nManual expert coding. Experts coded the sets of tweets (U.S., Chile, Germany, Italy) across\nfour dimensions, according to detailed instructions shown in SI C: (1) whether the text was political\nor not (binary); (2) whether the text contained negative messaging or not (binary); (3) whether\nthe text contained negative, positive, or neutral messaging (three categories); (4) whether the text\nwas ideologically left-wing, centrist, or right-wing (three categories). The U.S. data were coded\nby two coders, with any discrepancies then mutually resolved, with the final coding serving as\n“ground truth” for our accuracy assessments. Non-U.S. tweets were single-coded by an expert of\nthe respective country, with a “ground truth” review then conducted based on translation and\nconfirmation with a second reviewer. For the test of varying text length, U.S. news articles were\ncoded for only the binary political and negativity criteria. 3\nGPT-4 coding and performance assessment. For each of the four coding tasks, we\nprompted GPT-4 twice with coding instructions aligning with those given to the human experts\n(see SI C). For each concept, therefore, the data have scores from two GPT-4 “coders”, which we\nrefer to as “GPT-4 first run” and “GPT-4 second run”. In an alternative approach, we also try a\nsimple prompt that just mentions the concept and gives no further explanations. Full comparisons\nwith this “zero-shot” approach are shown in SI E. Due to message length restrictions, tweets were\nclassified in batches of twenty, with each batch run in a fresh instance of the GPT-4 chat client\nto avoid any biasing from previous prompts. News articles were classified in batches of four. To\nquantify the accuracy of GPT-4 coding, we present the percentage of classifications in each run\nwhich agree with the final expert coding (alternatively, results using F1 scores are also presented in\n2Some non-U.S. tweets were actually written in English, but these were left in as a test of how GPT-4\nhandled the annotation of multiple languages within a single batch. There appeared to be no issues.\n3News articles were deemed to be too rarely “positive” for a multi-category sentiment classification and\nthe ideological classification of primarily fact-based reporting was deemed to be unfeasible.\n4\nSI D). For all four concepts, we start with a “baseline accuracy” for U.S. tweets, before we moving\non to news articles, and then non-U.S. tweets.\nHybrid Human-GPT-4 coding. We further exploit the fact that the two rounds of GPT-4\ncoding based on the exactly the same instructions and data will, due to randomness, yield slightly\ndifferent results. This disagreement likely occurs on edge cases, which represent important nuance in\nany given concept. We therefore test a “hybrid” model, where disagreements between the “GPT-\n4 first run” and the “GPT-4 second run” are adjudicated by a human expert. Of course, this\nadjudication process pushes the classification more towards the expert coding (although only for\ncontested cases) and is therefore likely to improve the accuracy. However, the results illustrate that\nLLM-assisted coding can be optimized through minimal additional human effort. In the results,\nwe also report the frequency of disagreement between the two GPT-4 runs as an indicator of\nadditionally required human input.\nDownstream effects. Finally, we expert-coded, GPT-4-coded (twice), and hybrid-coded a set\nof 3,000 additional tweets from candidates in the 2022 U.S. congressional elections for both negative\nmessaging (binary concept) and political ideology. Based on these four codings, we fine-tuned four\nmodels of negative messaging and four models of ideology using BERTweet (Nguyen et al., 2020),\na transformer package designed specifically for handling social media data. We use the trained\nclassifiers to predict negativity and ideology in all tweets (excluding retweets) sent by congressional\ncandidates before their state primary in 2022. The resulting classifications are then compared side-\nby-side in both descriptive and predictive models to test for meaningful differences in the resulting\nanalyses.\n5\nClassification Performance\nBaseline accuracy\nBeginning with the U.S. tweets, Figure 1 below shows the percentage agreement between the expert\ncoding and the two GPT-4 coding runs. We report F1 scores as an alternative measure in SI D, with\nsubstantively identical results. Bars are color-coded by classification approach, indicating whether\nthe result is based purely on GPT-4 coding or also includes an expert reconciliation of disagreements\nbetween GPT-4 runs (i.e., the hybrid approach). Note that all results are based on GPT-4 prompts\nusing full coding instructions. In SI E, we also report results when prompting GPT-4 just with the\nconcept of interest without defining the concept further. In most cases, including details improves\naccuracy.\nThe results, overall, show a relatively high degree of accuracy, but also highlight some important\nvariance across classification tasks. Beginning with political classification, the two rounds of GPT-4\ncoding agreed with expert coding 88.3% and 91.1% of the time. When reconciling disagreements be-\ntween GPT-4 rounds using human validation (7.6% of instances), this accuracy increases to 93.4%.\nFor the binary negative classification task, accuracy is even higher, with the two coding rounds\nagreeing with the expert coding 94.5% and 94.3% of the time. The hybrid validation approach\nimproves these results further to 96.9% agreement, based on 4.9% disagreement between GPT-4\nrounds. In general, for these two binary classification tasks, GPT-4 results closely approximate\nhuman annotation.\nBeyond the binary tasks, however, accuracy does drop. In the case of three-category sentiment\ncoding, this decrease is particularly notable. The GPT-4 coding rounds were accurate 81.7% and\n80.6% of time, with the hybrid approach then increasing this accuracy to a respectable 86.6%\n(based on 13.4% GPT-4 disagreement). For context, the rate of agreement between human coders\non this concept was 87.2%, suggesting that GPT-4 does underperform human coding accuracy, but\nnot to an extreme degree. For ideology, the level of accuracy (84.7% and 85%) is lower than in the\nbinary tasks. With the hybrid approach, based on 10.6% disagreement between rounds, accuracy\nimproves to over 90%. For reference, the baseline rate of agreement between human coders was\n6\nalso 85%. Collectively, given the complexity of the task at hand, the results actually highlight the\nlikely strength of GPT-4 in this particular coding task.\nFigure 1: Classification accuracy rates for U.S. tweets, by task and coding method.\nAccuracy for longer texts\nClassification accuracy may change when applied to differing text types, especially in terms of\nthe overall length of the text. Indeed, when classifying full news articles, some notable changes\nin accuracy occur. As illustrated by Figure 2, for the classification of political, accuracy increases\nslightly to 96.5% and 95%, while the accuracy of negativity classification drops dramatically to 80%\nand 76%. Evaluating the divergences qualitatively, longer texts appear to be providing differing\ncues for the two types of classification task. For political classification, longer text provides greater\ncontext and more opportunities for political keywords. For negativity classifications, however,\nlonger text provides more conflicting signals, with single articles often containing positive, neutral,\nand negative components. However, given the black-boxiness of LLMs, we ultimately do not know\nwhat the reason for the decrease in performance is. In any case, researchers should consider the\ncombination of text type and classification task when deciding about the viability of GPT-4 for\n7\ntheir coding requirements.\nFigure 2: Classification accuracy for U.S. tweets and news articles, by task and coding\nmethod\nAccuracy across languages\nHaving established performance levels on English-language data, the question is whether GPT-4\nwill perform consistently in other languages. Figure 3 shows the percentage agreement of GPT-4\nwith expert coding of tweets by Italian, German and Chilean politicians. Performance is very\nsimilar across languages. When classifying political tweets, accuracy is above or just below 90%\nacross countries and runs, closely tracking accuracy rates in the U.S. context. In terms of negativity\nclassification, accuracy is still high, but somewhat lower in Germany and Italy, sitting just above\n85% as opposed to above 90% in the U.S. Accuracy for the three-way sentiment classification\nagain drops to below 80% in all countries, a potentially unsatisfactory result. For the ideology\nclassification, results are strong (between 81% and 84% across countries), but again, just below the\nlevel of accuracy seen in the U.S. Again, the hybrid coding approach increases overall accuracy,\nbringing accuracy in many contexts above or approaching 90%.\n8\nBased on these results, the potential for simultaneous translation and classification of text is a\nparticularly appealing opportunity for automated coding approaches. In some tasks, performance is\nmarginally lower than in the U.S., although still strong, while in others (especially the classification\nof political content) results are largely indistinguishable from the classifications in the U.S. context.\nFigure 3: Classification accuracy for tweets from Chile, Germany and Italy, by task and\ncoding method.\n9\nDownstream Effects: Congressional Primary Case Study\nAlthough the differences in annotation results may be minimal between GPT-4 and expert coders,\nthey may still have significant downstream impacts on modelling and results, especially if GPT-4\ncoding is systemically biased. Therefore, to assess whether the differences are meaningful, we offer\ntwo insights from a U.S. case study, based on expert-coded and GPT-4-coded versions of the binary\nnegativity classification, as well as the three-way ideology classification discussed above.\nOur case study connects to long-running threads of research about negativity and ideological\nmessaging during political campaigns. Studies have found that negativity fluctuates across the\ncourse of a campaign (Lau & Rovner, 2009), with negative messaging often increasing as the\ngeneral election approaches, (Hassell, 2021), while, conversely, decreasing prior to primary elections\n(Peterson & Djupe, 2005). Similarly, research has shown that candidates may be incentivized to\nvary their publicly presented ideology across different stages of a campaign (Brady et al., 2007).\nFor our case study, we trained a total of eight NLP models (four for each of the two variables\nof interest) based on the different classification approaches presented above. We use these models\nto test whether levels of negativity and the ideology in Congressional candidate messaging changes\nin the run-up to the U.S. 2022 Congressional primary date, using tweets sent within the final 90\ndays of each campaign. To do this, we split a set of 3,000 tweets (distinct from the set discussed\nabove) into 2,500 training and 500 test examples. We coded these for negativity and ideology, first,\nby hand, and second, with two GPT-4 runs. Where disagreement occurred between GPT-4 coding\nruns, an expert coder adjudicated the disagreement to create the fourth hybrid coding. We used\nthe 600 tweets discussed above as a validation set. These data were then used to train four separate\nBERTweet (Nguyen et al., 2020) models for each classification task (GPT-4 Run 1; GPT-4 Run 2;\nmanual coding; hybrid coding), each of which then predicted negativity and ideology of all tweets\nsent by congressional candidates in the U.S. House and Senate primaries in 2022. The descriptives,\ntrends, and modelling we present below are based on 391,973 tweets in total.\nFigure 4 below shows the daily percentages of tweets classified as negative in all four classifi-\ncations side-by-side. Importantly, average negativity across the period are similar across all four\n10\nmethods: The manual coding classifies 31.1% of tweets as negative, the two pure GPT-4 runs clas-\nsify 29.5% and 29.3% as negative, respectively, and the hybrid approach classifies 30.2% of tweets\nas negative. Additionally, the over-time trends are almost identical across all four models, with\nnegativity being relatively steady in the run-up to the election with a perceptible slight decrease\nin the week before the election.\nFigure 4: Daily Trends in Tweet Negativity Based on the Four Different Classifiers.\nFigure 5 shows the daily trends in tweet ideology, aggregated by political party, with results\ncloser to -1 indicating more ideologically liberal content. Again, the results are almost identical\nacross the four methods. The overall Democratic and Republican party averages, respectively, are\n0.79 and 1.24 in the manual coding model, 0.75 and 1.19 in the first GPT-4 model, 0.82 and 1.26\nin the second GPT-4 model, and 0.81 and 1.25 in the hybrid model. Over time, all models show\nidentical trends, with no signs of any moderation or increased extremity directly before the primary\ndate.\n11\nFigure 5: Daily Trends in Tweet Ideology, by Party, Based on the Four Different Classifiers.\nThus, the two over-time plots provide consistent evidence that across both measures of interest,\ncentral descriptive findings are identical independent of whether the training data were manually\ncoded, GPT-4-coded, or coded using our hybrid approach.\nMoving beyond descriptive trends, we test the consistency of the methods when modelling the\ntwo concepts of interest as a dependent variable. Figures 6 and 7 below show, at the candidate level,\nlinear regressions predicting the percentage of negativity and the average ideology of messages sent\nby a candidate in the pre-primary period, based on a set of key covariates (see SI F for details). As\ncan be seen, results across models based on the four different codings are almost identical, with no\nchanges in significance of any predictors across models. As such, using either hand-coded, GPT-\n4-coded, or hybrid-coded training data to explore candidate messaging in the 2022 congressional\nprimaries produces generally indistinguishable results.\n12\nFigure 6: Coefficient Estimates for Predictors of Percentage of Negative Twitter Messaging\nfrom a Given candidate.\nFigure 7: Coefficient Estimates for Predictors of the Average Ideology of Twitter Messaging\nfrom a Given candidate.\n13\nDiscussion and Conclusion\nIn this study, we assessed the potential of GPT-4 (or similar LLMs) to accurately substitute for\nmanual human text annotation, with a particular focus on applications in political science. The\nresults show that GPT-4 coding, benchmarked against the “ground truth” of expert coding, is\nhighly accurate and demonstrates clear potential for use across a range of research scenarios. The\nedge of our hybrid coding approach over pure GPT-4 coding suggests the following three-stage\nprocess for researchers interested in LLM-supported coding: First, classify all training text at least\ntwice using a LLM. Second, manually reconcile discrepancies between the two rounds of coding.\nThird, use this single reconciled training set for downstream tasks such as training transformer\nmodels. While this approach will not perfectly replicate an expert manual coding approach on all\ntasks, our results indicate that differences in downstream applications may be negligible.\nOne might also wonder why it is necessary to limit ourselves to using LLMs to merely classify\ntraining data instead of all data, given the accuracy. Indeed, side-by-side comparisons suggest\nthat ChatGPT and BERT models can produce similar classification results (Zhong et al., 2023).\nHowever, at present, the rate of classification through GPT-4 is considerably slower than, for\nexample, a transformer model classifying data on a high-end GPU. Non-English GPT-4 coding in\nour project was even slower, given the integrated language detection and translation inherent in\nthe classification process.\nDespite the promising results, our approach has important limitations. First, researchers should\nconsider the complexity of the classification objects, as GPT-4 performed worse on longer, more\ncomplex texts. Second, it is also unclear how GPT-4 would perform for even more complex concepts.\nWe noticed a performance drop for the more complex three-category classifications, compared to the\nbinary concepts. However, we note that simple classifications such as whether content is “political”\nor “negative” are very common in the field, and even human experts may not always disagree on\nhow to code text on more complex dimensions. Third, we cannot say how LLM-assisted coding\nwould do in other, non-Western contexts. We already noticed a small drop in performance for\nnon-English texts, although results looked still satisfactory.\n14\nFinally, standard limitations in the everyday use of LLMs also apply to their usage for classifica-\ntion tasks. Biases inherent in the training of these models (Bisbee et al., 2023; Motoki et al., 2023)\nmay seep into text annotation, especially ones more specific or contentious than classifications done\nhere. Researchers should be mindful of these potential biases and carefully consider their impact\non potential outcomes.\nThe implications of our findings are potentially substantial. All GPT-4 coding for this project\nwas completed using a single $20 monthly subscription. Hence, financial disparities between re-\nsearchers effectively evaporate for tasks where LLMs can substitute human labour. By achieving\ncomparable results at a fraction of the time and cost, GPT-4 coding opens up machine learning\napplications to an incredibly diverse pool of researchers, benefiting the discipline through new per-\nspectives, datasets, and areas of focus. High levels of cross-language accuracy provide significant\nopportunity and incentive for researchers to increase the levels of comparative studies. The field of\npolitical science may benefit from more generalizable, global work, with less of a targeted focus on\nsingle regions and countries (especially the United States).\n15\nReferences\nBallard, A. O., DeTamble, R., Dorsey, S., Heseltine, M., & Johnson, M. (2023). Dynamics\nof polarizing rhetoric in congressional tweets. Legislative Studies Quarterly, 48 (1),\n105–144.\nBisbee, J., Clinton, J., Dorff, C., Kenkel, B., & Larson, J. (2023). Artificially precise extrem-\nism: How internet-trained llms exaggerate our differences.SocArXiv. osf.io/preprints/\nsocarxiv/5ecfa\nBrady, D. W., Han, H., & Pope, J. C. (2007). Primary elections and candidate ideology: Out\nof step with the primary electorate? Legislative Studies Quarterly, 32 (1), 79–105.\nGilardi, F., Alizadeh, M., & Kubli, M. (2023). Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv. https://doi.org/10.48550/arXiv.2303.15056\nHassell, J., Hans. (2021). Desperate times call for desperate measures: Electoral competi-\ntiveness, poll position, and campaign negativity. Political Behavior, 43, 1137–1159.\nHuang, F., Kwak, H., & An, J. (2023). Is chatgpt better than human annotators? potential\nand limitations of chatgpt in explaining implicit hate speech. arXiv. https://doi.org/\n10.48550/arXiv.2302.07736\nJohnson, R. L., Pistilli, G., Men´ edez-Gonz´ alez, N., Duran, L. D. D., Panai, E., Kalpokiene,\nJ., & Bertulfo, D. J. (2022). The ghost in the machine has an american accent: Value\nconflict in gpt-3. arXiv preprint arXiv:2203.07785.\nKuzman, T., Mozetic, I., & Ljubeˇ sic, N. (2023). Chatgpt: Beginning of an end of man-\nual linguistic data annotation? use case of automatic genre identification. ArXiv,\nabs/2303.03953.\nLau, R. R., & Rovner, I. B. (2009). Negative campaigning. Annual Review of Political Sci-\nence, 1, 285–306.\nMotoki, F., Neto, V. P., & Rodrigues, V. (2023). More human than human: Measuring\nchatgpt political bias. Public Choice, 1–21.\n16\nMozafari, M., Farahbakhsh, R., & Crespi, N. (2020). Hate speech detection and racial bias\nmitigation in social media based on bert model. PLoS ONE, 8.\nNay, J., John. (2023). Large language models as corporate lobbyists. arXiv. https://doi.org/\n10.48550/arXiv.2301.01181\nNguyen, D. Q., Vu, T., & Nguyen, A. T. (2020). BERTweet: A pre-trained language model for\nEnglish Tweets. Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, 9–14.\nOrnstein, J. T., Blasingame, E. N., & Truscott, J. S. (2023). How to train your stochastic\nparrot: Large language models for political texts. Working Paper. https://joeornstein.\ngithub.io/publications/ornstein-blasingame-truscott.pdf\nPeterson, D. A. M., & Djupe, P. A. (2005). When primary campaigns go negative: The\ndeterminants of campaign negativity. Political Research Quarterly, 58 (1), 45–54.\nRozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and\nemotion in news media headlines using automated labelling with transformer language\nmodels. PLoS ONE, 10.\nTornberg, P. (2023). Chatgpt-4 outperforms experts and crowd workers in annotating polit-\nical twitter messages with zero-shot learning. arXiv. https://doi.org/10.48550/arXiv.\n2304.06588\nWu, P. Y., Tucker, J. A., Nagler, J., & Messing, S. (2023). Large language models can be\nused to estimate the ideologies of politicians in a zero-shot learning setting. arXiv.\nhttps://doi.org/10.48550/arXiv.2303.12057\nZhong, Q., Ding, L., Liu, J., Du, B., & Tao, D. (2023). Can chatgpt understand too? a\ncomparative study on chatgpt and fine-tuned bert. arXiv. https://doi.org/10.48550/\narXiv.2302.10198\n17\nSupplementary Information\nA Coding Breakdown\nThree individual coders were collectively responsible for all hand-coding used in this anal-\nysis. The first coder, the corresponding author, coded all English language text across all\nclassification tasks. The second coder, an assistant fluent in English, Spanish and Italian\nwith expertise in the respective political contexts of the U.S., Chile and Italy, coded all\nSpanish and Italian text as well as acted as a second coder on some English text. A third\ncoder, the second author, expert in U.S. and German politics, coded all German text and\nadditional English language text. A complete breakdown of coding combinations on each\ntext type and classification type is shown below.\nText Type Classification\nTask\nCoder1 Coder2 Coder3 Coder Reconcili-\nation\nGPT-4 Reconcili-\nation\nU.S. Tweets Political 100% 100% NA Disagreements mu-\ntually reconciled by\nboth coders\nReviewed by coder 2\nU.S. Tweets Negative 100% 100% NA Disagreements mu-\ntually reconciled by\nboth coders\nReviewed by coder 2\nU.S. Tweets Sentiment\n(Multi)\n100% NA 100% Disagreements mu-\ntually reconciled by\nboth coders\nReviewed by coder 2\nU.S. Tweets Ideology 100% NA 100% Disagreements mu-\ntually reconciled by\nboth coders\nReviewed by coder 3\nMedia Articles Political 100% 100% NA Disagreements mu-\ntually reconciled by\nboth coders\nReviewed by coder 1\nMedia Articles Negative 100% 100% NA Disagreements mu-\ntually reconciled by\nboth coders\nReviewed by coder 1\nSpanish Tweets All NA 100% NA NA Tweets translated by\ncoder 2 for review by\ncoder 1\nItalian Tweets All NA 100% NA NA Tweets translated by\ncoder 2 for review by\ncoder 1\nGerman Tweets All NA NA 100% NA Tweets translated by\ncoder 2 for review by\ncoder 1\nU.S. Tweets (Full\nTraining Data)\nNegative 100% NA NA NA Reviewed by coder 2\nU.S. Tweets (Full\nTraining Data)\nIdeology 100% NA NA NA Reviewed by coder 2\n18\nB Coding Instructions\nB.1 Political:\nIn general, references to political developments, political actors, or political topics (abortion,\nthe economy, gun control, environmental regulation etc.) are all included, be it at the\ninternational, national, or local level. References to federal organisations are political, as\nare references to branches of government. Importantly, references to financial institutions\nand national economic developments are political (as in the fed raises interest rates, or GDP\nestimated to increase by 1% this year), but references to individual stock prices or company\nperformances are not. We very much want to avoid including content like ”Tesla stock\nfalls 10% in June” or ”xyz becomes CEO at Blackrock” from being classed as political.\nAdditionally, crime is not always political. ”man arrested for drunk driving” is not political.\nNor is ”Sherriff arrests 3 in shoot out”. Crimes with a racial element or that have some\nconnection to politics, like anti-immigrant crimes, are included.\nB.2 Negative:\nThis is specifically ”negative messaging”. This definition is designed to be distinct from\nnaive definitions which do not separate negative sentiment from merely negative events or\noutcomes. This therefore includes direct attacks, criticism, disparaging comments, or gen-\nerally comments which expressly say or at least suggest that some person/event/outcome\nis bad/negative. For example, while ”Hurricane kills 50” is certainly bad, it is not nega-\ntive messaging. Alternatively, ”Failure to address climate change means 50 have died in\nhurricane” is negative. As such, negative is defined as a message which expresses negative\nsentiment directly towards an individual, group, or outcome and is not merely any message\nwhich contains negative valence.\nB.3 Sentiment (3 Category):\nAs with the binary classification of negativity, positive is defined as a message which expresses\npositive sentiment directly towards an individual, group, or outcome and is not merely a\nmessage which mentions something positive. Therefore, ”man wins lottery” is not positive\nbut ”really happy to see that my neighbour won the lottery” is. In a political context, this\nmeans that matter of fact statements about events such as bills passing or being introduced\nare taken as neutral. Sometimes messages contain multiple elements, but intent is again\nimportant. For example, ”I introduced a bill to end childhood hunger because the government\nhas failed our children” is negative.\n19\nB.4 Ideology:\nThis may vary depending on country context, but there are three basic codes: 0 = ideologi-\ncally left, 1 = ideologically neutral or non-ideological, 2 = ideologically right. First, tweets\nwith no ideological content are classified as neutral. Centrally, two types of content are\ngenerally included as ideological. The first is content that would be typically associated with\none side or the other. In the US, this means that things like climate change awareness mes-\nsages are classed as 0 and messages rejecting climate change are a 2. Additionally, messages\nwhich attack the other side are also associated with the respective party ideologies. ”The\nRepublican party doesn’t care about childhood hunger” would be a 0, therefore. Similarly,\nclaiming ownership for one side counts, so ”As Democrats, we are the only party working\nto end childhood hunger” is a 0, even if a general mention of childhood hunger by itself\nwould probably not be classified as either a 0 or a 2. For our purposes, content that is not\npolitical like a happy birthday message is classified as a 1, along with all content that does\nnot necessarily have an ideological slant to it or isn’t associated with one side or the other.\nVague general messaging, even of topic areas is a 1. When there is a clear slant to a policy\ndiscussion, this is assigned to the relevant ideology, so ”we should improve healthcare” is\nneutral, while ”we need better healthcare by making it free for everyone” is a 0 and ”we can\nimprove healthcare by increasing private market options” is a 2.\nC Classification Prompts\nBelow are the various prompts fed into the GPT-4 interface to produce the coding results\nfor each type of classification approach. In this case, results were printed through the widely\naccessible chat interface. If using the API to create classifications of data, wording would\nof course need to be modified slightly, asking for an appended classification output, for\nexample, rather than asking for the creation of a new table. For more complex definitions,\nor to hone precision of concept even further, researchers could of course provide even more\ndetailed definition and coding instructions, similar to the types of instructions given to a\nhuman coder.\nC.1 Political (no instructions):\ncreate a table with a classification of the following messages as political (1) or non-political\n(0).\nC.2 Negative (no instructions):\ncreate a table classifying the following messages as negative (1) or not negative (0):\n20\nC.3 Political (with instructions):\ncreate a table with a classification of the following messages as political (1) or non-political\n(0). Political is defined as any message which is directly about a political topic, references\npolitical developments, or makes reference to a political figure, group, or agency. References\nto federal organisations are political, as are references to branches of government. Broad\nmentions of national economic developments are political, but discussions of individual stock\nprices are not:\nC.4 Negative (with instructions):\ncreate a table with a classification of the following messages as negative (1) or not negative\n(0). Negative is defined as a message which expresses negative sentiment directly towards\nan individual, group, or outcome and is not merely any message which contains negative\nvalence:\nC.5 Negative/Neutral/Positive (with instructions):\ncreate a table with a classification of the following messages as negative (0), neutral (1), or\npositive (2). Negative is defined as a message which expresses negative sentiment directly\ntowards an individual, group, or outcome and is not merely any message which contains\nnegative valence. Positive is defined as a message which expresses positive sentiment directly\ntowards an individual, group, or outcome and is not merely a message which mentions\nsomething positive. Matter of fact statements about events such as bills passing or being\nintroduced are taken as neutral:\nC.6 Ideology, Validation Set:\nCreate a table classifying the following tweets as ideologically liberal (0), ideologically neutral\n(1), or ideologically conservative (2). Ideology here is defined in the context of the [United\nStates/German/Italian] political system. Tweets with no ideological content are classified\nas neutral:\nC.7 Ideology, Full Models Training Data:\nCreate a table classifying the following tweets as ideologically liberal (0), ideologically neutral\n(1), or ideologically conservative (2). Ideology here is defined in the context of the United\nStates political system during the Biden presidency. Tweets with no ideological content are\nclassified as neutral:\n21\nD Alternate Accuracy Measure: F1 scores\nWhile the raw percentage accuracy rates of each coding round shown in the main paper are\nrevealing, it is also possible that the high frequency of political messages and low frequency\nof negative messages produces somewhat misleading accuracy results (classifying everything\nas 0 for negativity, for example, would still produce as a high accuracy score). Addressing\nthis, Figures A1 through A3 below show the classification results using macro F1 scores, a\nmeasure which accounts for the frequency of each category in producing an accuracy score.\nUsing this alternative measure, the central trends found in the main paper hold. F1 scores,\nare again, very high and show expected changes between classification approaches. The\nsame is true of F1 scores for the international classifications, which are consistent with U.S.\nmessage scores.\nFigure A.8: Classification F1 scores for U.S. tweets, by task and coding method.\n22\nFigure A.9: Classification F1 scores for U.S. tweets and news articles, by task and coding\nmethod\n23\nFigure A.10: Classification F1 scores for tweets from Chile, Germany and Italy, by task and\ncoding method.\nE Comparison of Coding Performance With and With-\nout Definition Instructions\nWhen no coding instructions were provided, the accuracy (the rate of agreement between the\nexpert coding and the GPT-4 coding) of the political classification was 89% and 88% in the\ntwo coding rounds, matching the performance of the classification rounds when instructions\n24\nwere included. The accuracy of the negative classifications was 90.2% in both rounds, around\nfour percentage points lower than when coding instructions were included. For the three-way\nsentiment classification, the difference is more significant, with both rounds of coding with\nno instructions accurately classifying tweets around 70% of the time, while the addition of\ncoding instructions increased this level of accuracy by just over 10 percentage points in each\nround to over 80%.\nThese results suggests that, at a basic level, GPT-4 understands the concepts of “politi-\ncal” and ”negative” in a very similar way to how social science researchers would define the\nconcept, meaning that GPT-4 can likely be used to identify these concepts with a very high\ndegree of accuracy in an “off the shelf” or zero-shot fashion. For the more complex sentiment\ntask however, over-classification of positive messages was common and was only remedied\n(in-part) by the addition of instructions. This does suggest that, in the same way that\nhuman coders are often trained and provided with definitions and coding instructions, addi-\ntional detail in prompts can improve GPT-4 classification accuracy of certain concepts. It\nis therefore recommended that researchers apply at least basic instructions to their prompts\nto ensure higher levels of coding accuracy.\nFigure A.11: Classification accuracy with and without coding instructions included in GPT-\n4 prompts\nF 2022 Case Study Data, Covariates, and Results\nAs this research note focuses primarily on the methodological aspect of GPT-4 classifica-\ntion comparisons, information on the data collection and construction of covariates for the\n25\nregression modelling in the 2022 primary case study were not outlined in full in the main\npaper. Instead, information on these is outlined here below:\nF.1 Data\nInformation on all major party candidates running in a House or Senate primary was hand-\ncollected by the author, including personal, electoral, and social media information. In\ntotal, 1,814 candidates had one or more Twitter handles and sent at least one tweet between\nJanuary 2021 and their 2022 primary election date. 1,719 of these candidates sent tweets\nwithin the chosen 90 day pre-primary window. Within this 90 day window, the median\ncandidate sent 154 tweets and 35 negative tweets (as classified by the hand-coded model).\nF.2 Variables\nChamber: Binary for whether the candidate is running for the House or Senate\nIncumbent: Binary variable for whether the candidate is the incumbent in the district/state.\nExperience: Binary variable reflecting whether the candidate has ever previously held elected\noffice at any level (all incumbents are 1 by definition).\nPartisan Electoral Advantage: Continuous variable taken as the Cook PVI (Partisan Voter\nIndex) score for a district/state relative to the candidate’s party.\nGender: Binary variable for whether the candidate identifies as male or female (one non-\nbinary candidate was excluded for simplicity of modelling).\nF.3 Regression Results\n26\nTable A.1\nDependent variable: Pct. Negative Messages\nNegative Hand Negative Code1 Negative Code2 Negative Hybrid\n(1) (2) (3) (4)\nSenate 0.435 0.335 0.097 0.198\n(1.287) (1.265) (1.263) (1.286)\nIncumbent 7.180 ∗∗∗ 6.761∗∗∗ 6.100∗∗∗ 6.215∗∗∗\n(1.439) (1.414) (1.412) (1.437)\nExperience −4.179∗∗∗ −4.229∗∗∗ −4.611∗∗∗ −4.587∗∗∗\n(1.302) (1.280) (1.278) (1.301)\nGender −4.166∗∗∗ −3.641∗∗∗ −3.722∗∗∗ −3.965∗∗∗\n(0.907) (0.891) (0.890) (0.906)\nRepublican 11.893 ∗∗∗ 11.503∗∗∗ 11.617∗∗∗ 11.578∗∗∗\n(0.849) (0.834) (0.832) (0.847)\nElectoral Safety −0.517 −0.680 −0.720 −0.687\n(0.458) (0.450) (0.449) (0.457)\nTotal Tweets 2.429 ∗∗∗ 2.373∗∗∗ 2.375∗∗∗ 2.471∗∗∗\n(0.419) (0.412) (0.411) (0.419)\nConstant 22.814 ∗∗∗ 21.397∗∗∗ 21.547∗∗∗ 22.314∗∗∗\n(0.820) (0.806) (0.805) (0.819)\nObservations 1,573 1,573 1,573 1,573\nR2 0.163 0.156 0.159 0.156\nAdjusted R2 0.159 0.152 0.155 0.152\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\n27\nTable A.2\nDependent variable: Average Message Ideology\nIdeology Hand Ideology Code1 Ideology Code2 Ideology Hybrid\n(1) (2) (3) (4)\nSenate −0.002 −0.005 0.007 −0.004\n(0.016) (0.016) (0.016) (0.016)\nIncumbent 0.004 0.008 0.022 0.013\n(0.018) (0.018) (0.018) (0.018)\nExperience −0.062∗∗∗ −0.057∗∗∗ −0.073∗∗∗ −0.064∗∗∗\n(0.016) (0.016) (0.016) (0.016)\nGender −0.071∗∗∗ −0.065∗∗∗ −0.090∗∗∗ −0.088∗∗∗\n(0.011) (0.011) (0.011) (0.011)\nRepublican 0.448 ∗∗∗ 0.412∗∗∗ 0.423∗∗∗ 0.430∗∗∗\n(0.010) (0.011) (0.011) (0.011)\nElectoral Safety −0.011∗ −0.010∗ −0.014∗∗ −0.017∗∗∗\n(0.006) (0.006) (0.006) (0.006)\nTotal Tweets 0.015 ∗∗∗ 0.017∗∗∗ 0.017∗∗∗ 0.016∗∗∗\n(0.005) (0.005) (0.005) (0.005)\nConstant 0.813 ∗∗∗ 0.780∗∗∗ 0.853∗∗∗ 0.836∗∗∗\n(0.010) (0.010) (0.010) (0.010)\nObservations 1,573 1,573 1,573 1,573\nR2 0.583 0.534 0.552 0.561\nAdjusted R2 0.581 0.532 0.550 0.559\nNote: ∗p<0.1; ∗∗p<0.05; ∗∗∗p<0.01\n28",
  "topic": "Coding (social sciences)",
  "concepts": [
    {
      "name": "Coding (social sciences)",
      "score": 0.6744803190231323
    },
    {
      "name": "Computer science",
      "score": 0.6485322117805481
    },
    {
      "name": "Annotation",
      "score": 0.6000874042510986
    },
    {
      "name": "Politics",
      "score": 0.5905382633209229
    },
    {
      "name": "Natural language processing",
      "score": 0.5077860951423645
    },
    {
      "name": "Language model",
      "score": 0.4856640100479126
    },
    {
      "name": "Transformer",
      "score": 0.45621150732040405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41554537415504456
    },
    {
      "name": "Data science",
      "score": 0.37567025423049927
    },
    {
      "name": "Political science",
      "score": 0.19743722677230835
    },
    {
      "name": "Social science",
      "score": 0.11443603038787842
    },
    {
      "name": "Sociology",
      "score": 0.0941133201122284
    },
    {
      "name": "Engineering",
      "score": 0.08086833357810974
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}