{
  "title": "AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition",
  "url": "https://openalex.org/W4285601618",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2996760600",
      "name": "Saed Rezayi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103923875",
      "name": "Zhengliang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158761448",
      "name": "Zihao Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2566173206",
      "name": "Chandra Dhakal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115822559",
      "name": "Bao Ge",
      "affiliations": [
        "Shaanxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1532195434",
      "name": "Chen Zhen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306912518",
      "name": "Tianming Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978672188",
      "name": "Sheng Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W2903298154",
    "https://openalex.org/W3169141681",
    "https://openalex.org/W2900338099",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2118091490",
    "https://openalex.org/W3171723960",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2951476960",
    "https://openalex.org/W3104748221",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2948335087",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W3029927342",
    "https://openalex.org/W3105959138",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2028742638",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2950985889",
    "https://openalex.org/W3162734203",
    "https://openalex.org/W3173780338",
    "https://openalex.org/W3100742171",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W4386506836"
  ],
  "abstract": "Pretraining domain-specific language models remains an important challenge which limits their applicability in various areas such as agriculture. This paper investigates the effectiveness of leveraging food related text corpora (e.g., food and agricultural literature) in pretraining transformer-based language models. We evaluate our trained language model, called AgriBERT, on the task of semantic matching, i.e., establishing mapping between food descriptions and nutrition data, which is a long-standing challenge in the agricultural domain. In particular, we formulate the task as an answer selection problem, fine-tune the trained language model with the help of an external source of knowledge (e.g., FoodOn ontology), and establish a baseline for this task. The experimental results reveal that our language model substantially outperforms other language models and baselines in the task of matching food description and nutrition.",
  "full_text": "AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food\nand Nutrition\nSaed Rezayi1 , Zhengliang Liu1 , Zihao Wu1 , Chandra Dhakal1 , Bao Ge2 ,\nChen Zhen1∗ , Tianming Liu1∗ and Sheng Li1∗\n1University of Georgia\n2Shanxi Normal University\n{saedr,zl18864,zihao.wu1,chandra.dhakal25,czhen,tliu,sheng.li}@uga.edu, bob ge@snnu.edu.cn\nAbstract\nPretraining domain-specific language models re-\nmains an important challenge which limits their\napplicability in various areas such as agriculture.\nThis paper investigates the effectiveness of lever-\naging food related text corpora (e.g., food and agri-\ncultural literature) in pretraining transformer-based\nlanguage models. We evaluate our trained language\nmodel, called AgriBERT, on the task of seman-\ntic matching, i.e., establishing mapping between\nfood descriptions and nutrition data, which is a\nlong-standing challenge in the agricultural domain.\nIn particular, we formulate the task as an answer\nselection problem, fine-tune the trained language\nmodel with the help of an external source of knowl-\nedge (e.g., FoodOn ontology), and establish a base-\nline for this task. The experimental results reveal\nthat our language model substantially outperforms\nother language models and baselines in the task of\nmatching food description and nutrition.\n1 Introduction\nUnited States Department of Agriculture (USDA) maintains a\ndatabase called Food and Nutrient Database for Dietary Stud-\nies (FNDDS) which provides the nutrient values for foods and\nbeverages reported in what is eaten in the US1. Additionally,\nhousehold and retail scanner data on grocery purchases, such\nas the Nielsen data available through the Kilts Center for Mar-\nketing, have been extensively used in food policy research 2.\nMapping these two databases, i.e., food description found in\nretail scanner data, to nutritional information database is of\n∗Corresponding authors\n1https://www.ars.usda.gov/northeast-area/\nbeltsville-md-bhnrc/beltsville-human-nutrition-research-center/\nfood-surveys-research-group/docs/fndds/\n2Researcher(s)’ own analyses calculated (or derived) based in\npart on data from Nielsen Consumer LLC and marketing databases\nprovided through the NielsenIQ Datasets at the Kilts Center for\nMarketing Data Center at The University of Chicago Booth School\nof Business. The conclusions drawn from the NielsenIQ data are\nthose of the researcher(s) and do not reflect the views of NielsenIQ.\nNielsenIQ is not responsible for, had no role in, and was not involved\nin analyzing and preparing the results reported herein.\nutmost importance. This linkage can capture the relationship\nbetween the retail food purchase and community health and\nalso the difference between poor and non-poor diets across\nthe whole diet spectrum, and thus it can impact future funding\npolicies that can provide healthy food for low-income house-\nholds.\nIn this work, we aim to develop and employ Natural Lan-\nguage Processing (NLP) techniques to find the best linkage\nbetween the two databases. A common approach to tackle\nthis kind of problems is semantic matching, which is the task\nof determining whether two or more elements have similar\nmeaning. Bi-encoders are the most common techniques for\nsemantic matching. A bi-encoder inputs two strings and en-\ncodes them in the embedding space and in the final layer cal-\nculates the similarity in a supervised fashion. That is why\nword embedding techniques are a good candidate for this pur-\npose. Word embeddings have been utilized extensively for\nthe task of semantic matching [Kenter and De Rijke, 2015 ],\nbut recent advancements in contextual word embeddings, in\nwhich each word is assigned a vector representation based\non the context, have resulted in significant improvements in\nmany NLP tasks, including semantic matching.\nTransformer-based language models, e.g., BERT [Devlin\net al., 2019], have been widely used in research and practice\nto study computational linguistics and they have shown supe-\nrior performance in variety of applications including text clas-\nsification [Jin et al., 2020], question answering [Yang et al.,\n2019], and many more. However, these models are not gener-\nalizable to every domain when used with their default objec-\ntives, i.e., pretrained on generic corpora such as Wikipedia.\nTo address this issue, previous work has attempted to incor-\nporate domain-specific knowledge into the language model\nby different strategies. One of the prominent approaches is\nin biomedical domain where a BERT-based language model\nis pretrained on a large corpus of biomedical literature called\nBioBERT [Lee et al. , 2020 ]. Motivated by the impressive\nperformance of BioBERT, we use a large corpus of agricul-\ntural literature to train a language model for agricultural ap-\nplications from scratch. The trained model will be further\nfine-tuned by the downstream tasks.\nAnother method to incorporate domain knowledge into the\nlanguage model is to use an external source of knowledge\nsuch as a knowledge graph (KG). Knowledge graphs are rich\nsources of information that are carefully curated around ob-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5150\njects called entities and their relations. A basic building block\nof a KG is called a triplet which consists of two entities and\na relation between the two. Previous work has attempted to\ninject triples into the sentences [Liu et al., 2020 ], however,\ninjecting triples can introduce noise to the sentences which\nwill mislead the underlying text encoder. To address this is-\nsue, we propose to add n entities from an external knowl-\nedge source (i.e., a knowledge graph) based on similarity that\ncan be obtained by various methods such as entity linking.\nThis augments the semantic space but keeps the vocabular-\nies within the domain. We show how changing n can affect\nthe performance of the downstream task, quantitatively and\nqualitatively.\nMoreover, we propose to formulate the task of mapping\nretail scanner data (also known as Nielsen) to USDA descrip-\ntion as an answer selection problem. Given a question and\na set of candidate answers, answer selection is the task of\nidentifying which of the candidates answers the question cor-\nrectly. An answer selection model inputs a pair of question\nand answer and outputs a binary label (true or false), so it is\na binary classification problem. Similarly, we can consider\nNielsen product descriptions as the set of all questions and\nUSDA descriptions as the set of all answers, and the goal\nis to find the best answer for each question. The difference\nis that in the original answer selection task, the number of\nanswers is limited and usually unique to each question, how-\never, in this setting there is a shared set of answers and its\nsize is much larger. We use our pre-trained language model\nas the backbone for the answer selection component and we\naugment both questions and answers during fine-tuning using\nexternal knowledge to boost the performance. In summary,\nwe make the following contributions in this paper.\n• We collect a large-scale corpus of agricultural literature\nwith more than 300 million tokens. This domain corpus\nhas been instrumental to fine-tune generic BERT into\nAgriBERT.\n• We propose a knowledge graph guided approach to aug-\nment the dataset for the answer selection component.\nWe inject related entities to the sentences before the fine-\ntuning step.\n• AgriBERT substantially outperforms existing language\nmodels on USDA datasets in the task answer selection.\nWe plan to release our datasets and language models to\nthe community upon publication.\nThe rest of the paper is organized as follows: in the next\nsection we discuss related works in language modeling in\nspecific domains, next in the Section 3.3 we describe our\nproposed approach to train a language model in agricul-\ntural domain and discuss how we inject external knowledge\nin the fine-tuning step. In Section 4.5 we introduce dif-\nferent datasets including our corpus for training a language\nmodel, the external sources of knowledge, and finally the\nfood dataset to evaluate our language model. We conclude\nour paper in Section 5.\n2 Related Works\n2.1 Pre-trained Language Models\nIn NLP, Pre-trained language models learn from large text\ncorpora and build representations beneficial for downstream\ntasks. In recent years, there are two successive generations\nof languages models. Earlier models, such as Skip-Gram\n[Mikolov et al., 2013] and GloVe [Pennington et al., 2014],\nprimarily focus on learning word embeddings from statisti-\ncal patterns, semantic similarities and syntactic relationships\nat the word level. With this first group of language embed-\nding methods, polysemous words are mapped to the same\nrepresentation, irregardless of word contexts. For example,\nthe word ”bear” in ”I see a bear” and ”Rising car sales bear\nwitness to population increase in this area” will not be distin-\nguishable in the vector space.\nA later group of models, however, recognizes the impor-\ntance of textual contexts and aims to learn context-dependent\nrepresentations at the sentence level or higher. For example,\nCoVe [McCann et al., 2017] utilizes a LSTM model trained\nfor machine translation to encode contextualized word vec-\ntors. Another popular model, Bidirectional Encoder Repre-\nsentations from Transformers (BERT) [Devlin et al., 2019 ]\nis based on bidirectional transformers and pre-trained with\nMasked Language Modeling (MLM) and Next Sentence Pre-\ndiction (NSP) tasks, both ideal training tasks for learning ef-\nfective contextual representations from unlabelled data. It de-\nlivers exceptional performance and can be easily fine-tuned\nfor downstream tasks.\nBERT has enjoyed wide acceptance from the NLP com-\nmunity and practitioners from other domains. In particular,\ndomain experts can build domain-specific BERT models that\ncater to specific environments and task scenarios.\n2.2 Domain Specific Language Models\nBERT has become a fundamental building block for training\ntask specific models. It can be further extended with domain\nspecific pre-training to achieve additional gains over general\ndomain models.\nPrior work has shown that language models perform better\nwhen the source and target domains are highly relevant [Lee\net al., 2020; Gu et al., 2021 ]. In other words, pre-training\nBERT models with in-domain corpora can significantly im-\nprove overall performance on a wide variety of downstream\ntasks [Gu et al., 2021].\nThere is also a correlation between a model’s performance\nand the extent of domain specific training [Gu et al., 2021].\nIn particular, Gu et al. [Gu et al., 2021 ] note that training\nmodels from scratch (i.e., not importing pre-trained weights\nfrom the original BERT model [Devlin et al., 2019 ] or any\nother existing BERT-based models) is more effective than\nsimply fine-tuning an existing BERT model with domain spe-\ncific data.\nIn this paper, agricultural text such as food-related research\npapers are considered in-domain while other sources such\nas Wikipedia and news corpus are regarded as out-domain\nor general domain. Our primary approach is in line with\ntraining-from-scratch with in-domain data.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5151\nProduct Description USD\nA Description Label\ndomino white\nsugar granulated 1lb salsa, red, commercially-prepared False\ndomino white sugar granulated 1lb cookie-crisp False\ndomino white sugar granulated 1lb sugar, white, granulated or lump True\nTable 1: An example of how we propose to extend the dataset.\n2.3 Augmenting Pre-trained Language Models\nData augmentation refers to the practice of increasing training\ndata size and diversity without collecting new data [Feng et\nal., 2021]. Data augmentation aims to address practical data\nchallenges related to model training. It is applicable to sce-\nnarios such training with low-resource languages [Xia et al.,\n2019], rectifying class imbalance [Chawla et al., 2002], miti-\ngating gender bias [Zhao et al., 2018], and few-shot learning\n[Wei et al., 2021].\nSome data augmentation methods incorporate knowledge\ninfusion. For example, Feng et al. [Feng et al., 2020] used\nWordNet [Miller, 1995 ] as the knowledge base to replace\nwords with synonyms, hyponyms and hypernyms. Another\nstudy [Grundkiewicz et al., 2019 ] extracts confusion sets\nfrom the Aspell spellchecker to perform synthetic data gener-\nation in an effort to enhance the training data, which consists\nof erroneous sentences used for training a neural grammar\ncorrection model.\nHowever, there is limited research on the efficacy of apply-\ning data augmentation to large pre-trained language models\n[Feng et al., 2021 ]. In fact, some data augmentation meth-\nods have been found to have limited benefit for large lan-\nguage models [Feng et al., 2021; Longpre et al., 2020]. For\nexample, EDA [Wei and Zou, 2019 ], which consists of 4\noperations (synonym replacement, random insertion, random\nswap and random deletion), provides minimal performance\nenhancement for BERT [Devlin et al., 2019] and RoBERTa.\nNonetheless, researchers [Feng et al., 2021] advocate for\nmore work to explore scenarios in which data augmentation is\neffective for large pre-trained language models, because some\nstudies [Shi et al., 2021] demonstrate results contrary to the\nclaims of [Longpre et al., 2020].\nIn this study, we investigate the effectiveness of data aug-\nmentation with knowledge infusion and apply our method to\nthe Answer Selection task scenario. We find that our method\nsignificantly improves semantic matching performance.\n2.4 Answer Selection\nAnswer Selection refers to the task of finding the correct an-\nswer among a set of candidate answers for a specific ques-\ntion. For example, given the question ”What is the capital\nof France?”, a solution to this task is required to select the\ncorrect answer among the following choices:\n• A) Paris is the capital of France.\n• B) Paris is the most populous city in France.\n• C) London and Paris are financial hubs in Europe.\nIn this case, the first answer should be selected. It is clear\nthat matching words or phrases is not sufficient for this task.\nA common approach is to formulate this problem as a rank-\ning problem such that candidate answers are assigned ranking\nscores based on their relevance to the question. Earlier work\nprimarily relies on feature engineering and linguistic infor-\nmation [Yih et al., 2013 ]. However, the advance of deep\nlearning introduces powerful models [R¨uckl´e et al., 2019;\nLaskar et al., 2020] that outperform traditional methods with-\nout the need of manual efforts or feature engineering.\nIn this study, our goal is to establish valid mapping between\nfood descriptions and nutrition data. We formulate this task\nas an Answer Selection problem and demonstrates the supe-\nriority of our method over baselines.\n3 Methodology\n3.1 Domain Specific Language Model\nTraining language models is a powerful tool for a variety of\nNLP applications, and when it comes to a particular task in\na specific domain, it becomes more effective if the language\nmodel is trained on a corpora that contain large amount of\ntext in that specific domain. Such practices exist in the litera-\nture in various domains, for instance BioBERT and FinBERT\nare successful examples of training a domain-specific lan-\nguage models in biomedical and financial domains, respec-\ntively. Building upon previous research and motivated by the\nlack of existing corpora or a pre-trained model in agricultural\ndomain and because we are interested in a model that pro-\nduces vocabulary and word embeddings better suited for this\ndomain than the original BERT, we collect 46,446 articles\nrelated to food and agricultural that contain more than 300\nmillion tokens and use it to train a BERT model from scratch\n(more details about the dataset are provided in Section 4.1).\nThis trained model can be used for various NLP applications\nin agricultural field. We adopt the standard procedure in train-\ning a language model which is masked language modeling. In\nMasked Language Modelling, a certain fraction of words in\na given sentence are masked, and the model is expected to\npredict those masked words based on other words in that sen-\ntence. In this process it learns meaningful representation for\neach sentence.\n3.2 Answer Selection Problem Definition\nTo evaluate the trained language model we require a down-\nstream task in this specific domain. For instance most\nbiomedical language models are evaluated on named entity\nrecognition tasks on medical datasets. Due to the lack of\nbenchmark NLP dataset in this domain, and since the se-\nmantic matching problem has practical values, we evaluate\nour model on this task. Semantic matching is a technique to\ndetermine whether two sentences have similar meaning. We\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5152\nQ = Thomas unsliced English muffin\nEmbedding …\nq q q q a\nA = muffin\nEncoding \nSoftmaxClassification \nP(C=1|Q, A)\n…\n……\na……\nFoodOn Knowledge Graph\nQ' = Thomas unsliced English muffin\n         + bread wheat multigrain\nA' = muffin\nmuffin\nwheat\nmultigrainbread\nAgriBERT\nPre-training \nCorpora\nFood & \nAgricultural-related \nJournal Papers\n(46,446 papers\n2.4M words)\nFigure 1: The overall framework of AgriBERT which is trained on\nagriculture literature from scratch. AgriBERT is evaluated on an-\nswer selection task. The answer selection component has two in-\nputs: a question and an answer, and before we input them to the\nframework, we add new entities to them from an external source of\nknowledge such as Wikidata or FoodOn. The output of the frame-\nwork is a score (a probability) which is used for ranking the answers.\nexpress the task at hand as an answer selection problem, we\ncould assign all USDA descriptions (answers) to each Nielsen\nproduct description (question) and select S random incorrect\nUSDA description per product description as negative sam-\nples where S << D. In this case the size of extended dataset\nis S × D. Table 1 provides an example of how we extend the\ndataset when S = 2.\n3.3 Knowledge Infused Finetuning\nAs discussed in 2.4, there have been studies that success-\nfully inject related information from an external source\nof knowledge to enhance the performance of the down-\nstream task. For instance incorporating facts (i.e., a\ncurated triple extracted from a knowledge graph in the\nform of (entity,relation,entity)) from knowl-\nedge graphs [Liu et al., 2020], or injecting refined entities ex-\ntracted from text to a knowledge graph [Rezayi et al., 2021].\nIn our setting, since we are dealing with answer selection and\nthe size of training set is small, we propose to append exter-\nnal knowledge to both questions and answers to enhance the\nperformance of the answer selection module.\nFinding relevant external knowledge can be fulfilled via\ndifferent mechanisms such as entity linking, querying, cal-\nculating similarity, etc. In this paper we suggest to use en-\ntity linking and querying. In entity linking, all the named\nentities in a text are recognized and then linked to the enti-\nties of a knowledge graph which is an ideal solution for our\ncase. However the downside of this approach is that both en-\nDataset Articles\nTokens Words Size\nPenn T\nreebank - 887,521 10,000 10MB\nWikiText-103 28,475 103,227,021 267,735 0.5GB\nAgriculture corpus 46,446 311,101,592 2,394,343 4.0GB\nTable 2: Basic statistics of our dataset compared with two bench-\nmark datasets in the standard language modeling field.\ntity recognition and entity linking algorithms are commonly\ntrained on general text corpora such as Wikipedia and in our\ncase it is not of practical value if we use these tools without\nreconfiguring them for our purposes. Hence, we consider a\ndomain-specific knowledge graph, e..g, FoodOn [Dooley and\nGriffiths, 2018] and we obtain new knowledge by querying\nit using the keywords in the text of question answer pairs.\nWe simply append new entities to the end of questions or an-\nswers. More details about this knowledge graph will be pro-\nvided in Section 4.1. Figure 1 illustrates our proposed frame-\nwork.\n4 Experiment\n4.1 Datasets\nWe employ several datasets for training the language models,\nevaluating the trained language model, and augmenting the\ndownstream dataset. In this section we briefly explain these\ndatasets.\nLanguage Training Datasets\nOur main dataset is a collection of 46,446 food- and\nagricultural-related journal papers. We downloaded all pub-\nlished articles from 26 journals and converted the pdf files\nto text format for use in the masked language modeling task.\nWe also cleaned the dataset by removing URLs, emails, ref-\nerences, and non-ASCII characters. In order to compare the\ncontributions of different components of our model, we con-\nsider a secondary dataset for training (WikiText-103) that\ncontains all articles extracted from Wikipedia. This datasets\nalso retains numbers, case, and punctuation, which is simi-\nlar to our dataset described above. The statistics of the two\ndatasets is provided in Table 2. We also include Penn Tree-\nbank dataset [Marcus et al., 1994], which is another common\ndataset for the task of language modeling, as a reference.\nAnswer Selection Dataset\nWe use two different data sources for this part. First, we use\nthe consumer panel product from the Nielsen Homescan data.\nNielsen provides very granular data on the food purchases\nfrom the stores at the product barcode or Universal Product\nCode (UPC) with detailed attributes for each UPC, includ-\ning UPC description. While scanner data come with some\nnutrition-related product attribute variables, this information\nis not sufficient to examine the nutritional quality. To address\nthis issue, we link product level data from the Nielsen with\nthe USDA Food Acquisition and Purchase Survey (FoodAPS)\nthat supplements scanner data with the detailed nutritional in-\nformation. The survey contains detailed information about\nthe food purchased or otherwise acquired for consumption\nduring a seven-day period by a nationally representative sam-\nple of 4826 US households. The FoodAPS matched 32000+\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5153\nbarcodes with the Food and Nutrient Database for Dietary\nStudies (FNDDS) food codes of high quality. The linked data\nset has UPC description for each product, and the correspond-\ning FNDDS food code. In addition, the final data set has full\ninformation needed to construct diet quality indexes to evalu-\nate the healthfulness of overall purchases.\nExternal Source of Knowledge\nWe use FoodOn knowledge graph for the augmentation pur-\nposes. FoodOn is formatted in the form of OWL ontol-\nogy. The OWL ontology provides a globally unique identifier\n(URI) for each term which is used for lookup services and\nfacilitates the query processing system. Much of FoodOn’s\ncore vocabulary comes from transforming LanguaL, a mature\nand popular food indexing thesaurus [Dooley and Griffiths,\n2018]. That is why FoodOn is a unique and valuable resource\nfor enhancing our language model.\n4.2 Metrics\nSince the output of the evaluation task on the answer selection\ndataset is a ranked list of answers per question, we require\nmetrics that take into account the order of results. That is why\nwe propose to use precision at 1 (P@1) and Mean Average\nPrecision (MAP).\nPrecision@1 or P@1. We sort the selected answers based\non final similarity score and we count how many times the\ntop answer is correctly selected.\nP@1 =\n|N|X\ni=1\n1 if rankai == 1\nwhere N is the set of all questions.\nMean Average Precision (MAP). MAP measures the per-\ncentage of relevant selected answers. Given a ranked list of\nselected answers per question we mark them as relevant if\nthey are correctly selected and calculate AP as follows:\nAP = 1\nn\nnX\ni=1\n(P(i) × rel(k)),\nwhere n is the set of all selected answers, rel(k) ∈ {0,1} indi-\ncates if the answer is relevant or not, andP(i) is the precision\nat i in the ranked list. Once we obtain AP for each question\nwe can average across all questions to find MAP:\nMAP = 1\n|N|\n|N|X\nq=1\nAP(q),\nwhere N is the set of all questions.\n4.3 Baselines\nTo perform ablation study and make sure that our dataset im-\nproves the performance of the downstream task and not using\na pretrained model nor training from scratch, we consider fol-\nlowing scenarios:\n• kNN: we compute the embeddings3 of the Nielsen prod-\nuct descriptions and USDA descriptions and for each\n3We use sentence-transformer library for this task: https://github.\ncom/UKPLab/sentence-transformers\nvector belonging to the product description embedding\nspace we find the most similar vector from the USDA\ndescription embedding space. This naive approach is ef-\nfective if the number of unique USDA descriptions is\nsmall. However, this does not hold in our case.\n• We use BERT without any modification as the underly-\ning language model and use an existing answer selection\ntechnique to further fine-tune the model on the answer\nselection dataset.\n• We consider BERT as the base language model and fur-\nther train it with our own corpus. In this case the vo-\ncabularies of the final language model is the union of\nWikipedia and our corpus. We employ the fine-tuned\nlanguage model as the backbone of the answer selec-\ntion tool and apply it on the unmodified answer selection\ndataset.\n• We train a BERT model from scratch using masked lan-\nguage modeling technique on WikiText-103. We em-\nploy the fine-tuned language model as the backbone of\nthe answer selection tool and apply it on the unmodified\nanswer selection dataset.\n• We train a BERT model from scratch to train a new\nlanguage model using masked language modeling tech-\nnique on our own dataset. We employ the fine-tuned\nlanguage model as the backbone of the answer selec-\ntion tool and apply it on the unmodified answer selection\ndataset.\nFor the trained language model we also consider a scenario\nwhere we use entity linking algorithm to find related entities\nfrom Wikidata and append them to question and answers. As\ndiscussed this approach introduces noise to the text and may\nharm the performance but we include it as a baseline for the\nsake of comparison with the case where we query the FoodOn\nknowledge graph to augment the text of questions and an-\nswers.\n4.4 Experimental Settings\nOnce the extended dataset is generated we can apply any\nanswer selection method on the dataset. There are a num-\nber of studies in the literature on this topic, including\nCOALA [R¨uckl´e et al., 2019 ], CETE [Laskar et al., 2020 ],\nMTQA [Deng et al., 2019 ], and many more, among which\nCETE is considered state-of-the-art in the answer selec-\ntion task by the ACL community 4. CETE implements a\ntransformer-based encoder (e.g., BERT) to encode the ques-\ntion and answer pair into a single vector and calculates the\nprobability that a pair of question/answer should match or\nnot5.\nFor the entity linking process we use the implementation\nproposed by [Wu et al., 2020], called BLINK. BLINK is an\nentity linking python library that uses Wikipedia as the tar-\nget knowledge base. Moreover to send in efficient SPARQL\nqueries to FoodOn knowledge graph which is in the format of\n4Reported here: https://aclweb.org/aclwiki/Question\nAnswering (State of the art)\n5The code for this study is open source and available for public\nuse: https://github.com/tahmedge/CETE-LREC\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5154\nSentence W\nikidata entity FoodOn entity\nnestle nido\npowder infant formula nestle rice powder\naunt jemima frozen french toast breakfast entree aunt jemima frozen dairy dessert\nwoodys hickory barbecue cooking sauce woody’s chicago style hickory nut\nsour punch sour watermelon fruit chew straw sour punch sour milk beverage\nphilly steak frozen beef sandwich steak philly steaks wagyu steak\nyoplait original rfg harvest peach yogurt low fat yoplait creamy salad dressing\nTable 3: Examples to demonstrate the quality of added entities to the text of product descriptions. For Wikidata we use entity linking and\nwe present here the top linked entity (highest confidence score). For FoodOn we use SPARQL to query the ontology and the first outcome is\nlisted here.\n# Training\nDataset Model MAP P@1\n1 - kNN 26.70\n14.49\n2 - BER\nTp 27,77 10.88\n3 WikiT\next-103 BERT p 28.03 11.12\n4 WikiT\next-103 BERT s+EL (Wikidata) 27.36 10.09\n5 WikiT\next-103 BERT s+FoodOn (n=1) 28.78 24.83\n6 Agricultural Corpus\nBERTp 29.72 12.71\n7 Agricultural Corpus\nBERTs 44.21 22.72\n8 Agricultural Corpus\nBERTs+EL (Wikidata) 42.33 21.52\n9 Agricultural Corpus\nBERTs+FoodOn (n=1) 31.54 47.89\n10 Agricultural Corpus\nBERTs+FoodOn (n=3) 30.65 49.80\n11 Agricultural Corpus\nBERTs+FoodOn (n=5) 29.91 49.98\nTable 4: Test performances of all models trained on all datasets\nfor the task of answer selection. for kNN model we use sentence-\ntransformers to compute embeddings. EL stands for Entity Link-\ning and bold numbers indicate the best performance. BERT p is a\npre-trained BERT and BERTs means training a BERT model from\nscratch.\nOWL ontology we use ROBOT which is a tool for working\nwith Open Biomedical Ontologies 6. Additionally, we try to\nsimulate a setting where the number of labeled training sam-\nples is small, thus we use 20% of the dataset for training and\nthe remaining 80% for the test. We believe this is a more\nrealistic scenarios in real world applications.\n4.5 Results\nAs Table 4 presents, not surprisingly, the best performance\nis obtained when the language model is trained on the agri-\ncultural corpus. We summarize the main observations as fol-\nlows: First the kNN performs surprisingly well compared to\nother complicated methods, in fact in terms of P@1 it sur-\npasses methods that are trained with BERT and the answers\nare selected using state-of-the-art answer selection frame-\nwork. Next, the best MAP score (MAP= 44.21%) is obtained\nwhen the language model is trained with agricultural corpus\nfrom scratch, and any additional augmentation hurts the per-\nformance in terms of MAP. On the other hand, the model per-\nformance improves in terms of P@1 when incorporating ex-\nternal knowledge. More specifically, P@1= 49.98% when 5\nnew entities are added. This implies that by adding related ex-\nternal knowledge we can find the correct match roughly 50%\nof the times but if the correct match is not at the top position\nthey ranked very low, and hence the low MAP score.\nMoreover, by comparing lines 5 and 9 we can see that the\n6Find it here: https://github.com/ontodev/robot\ninclusion of external knowledge alone cannot improve the\nquality of language model in a domain specific task. We also\ninvestigate the number of external entities that we include in\nthe text of questions and answers and as lines 9-10 of Table 4\ndemonstrates, increasing n decreases the MAP score and in-\ncreases the P@1. This suggests that incorporating related en-\ntities from a relevant knowledge source helps to find the cor-\nrect match in 50% of the times but it mislead the answer selec-\ntion module and rank the correct match lower that it drops the\nMAP score. Table 3 provides some examples of augmented\nsentences by Wikidata and FoodOn knowledge sources. As\nthis table presents, linking the food description to Wikidata\nentities can easily go wrong, first three rows for instance,\nwhere the food descriptions are linked to brand names7. How-\never this does not happens in querying method, as the entities\nare purely food related. Additionally, FoodOn entities con-\ntain food-related adjectives such as frozen, creamy, etc. that\nhelp in matching the food descriptions to nutrition data.\n5 Conclusion\nIn this paper we trained a language model called AgriBERT\nthat will facilitate the NLP tasks in the food and agricultural\ndomain. AgriBERT is a BERT model trained from scratch\nwith a large corpus of academic journal in the field. To eval-\nuate our language model we propose to solve the problem of\nsemantic matching which aims at matching two databases of\nfood description (Nielsen database and USDA database). We\nreformulate the problem as an answer selection task and used\nour language model as a backbone of a generic answer selec-\ntion module to find the best match. Before feeding the pairs of\nquestions and answers to the model we augmented them with\nexternal entities obtained from FoodOn knowledge graph, a\ndomain-specific ontology in the field of food. We showed\nthat inclusion of external knowledge can help boost the per-\nformance in terms of the more strict P@1 measure but it low-\ners the performance in terms of mean average precision. As\na future direction, we plan to investigate more sophisticated\napproaches for incorporating external knowledge such as re-\nfining the knowledge before including it in the text.\nReferences\n[Chawla et al., 2002] Nitesh V Chawla, Kevin W Bowyer,\nLawrence O Hall, and W Philip Kegelmeyer. Smote: syn-\nthetic minority over-sampling technique. Journal of artifi-\ncial intelligence research, 16, 2002.\n7https://en.wikipedia.org/wiki/Aunt Jemima\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5155\n[Deng et al., 2019] Yang Deng, Yuexiang Xie, Yaliang Li,\nMin Yang, Nan Du, Wei Fan, Kai Lei, and Ying Shen.\nMulti-task learning with multi-view attention for answer\nselection and knowledge base question answering. In\nAAAI, volume 33, 2019.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In NAACL, 2019.\n[Dooley and Griffiths, 2018] Damion M Dooley and Emma\nJ et al. Griffiths. Foodon: a harmonized food ontology to\nincrease global food traceability, quality control and data\nintegration. Science of Food, 2(1), 2018.\n[Feng et al., 2020] Steven Y Feng, Varun Gangal, Dongyeop\nKang, Teruko Mitamura, and Eduard Hovy. Genaug: Data\naugmentation for finetuning text generators. In DeepIO,\n2020.\n[Feng et al., 2021] Steven Y Feng, Varun Gangal, Jason Wei,\nSarath Chandar, Soroush V osoughi, Teruko Mitamura, and\nEduard Hovy. A survey of data augmentation approaches\nfor nlp. In ACL-IJCNLP, 2021.\n[Grundkiewicz et al., 2019] Roman Grundkiewicz, Marcin\nJunczys-Dowmunt, and Kenneth Heafield. Neural gram-\nmatical error correction systems with unsupervised pre-\ntraining on synthetic data. In ACL BEA, 2019.\n[Gu et al., 2021] Yu Gu, Robert Tinn, Hao Cheng, Michael\nLucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. Domain-specific lan-\nguage model pretraining for biomedical natural language\nprocessing. ACM Transactions on Computing for Health-\ncare, 3(1), 2021.\n[Jin et al., 2020] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and\nPeter Szolovits. Is bert really robust? a strong baseline\nfor natural language attack on text classification and en-\ntailment. In AAAI, volume 34, 2020.\n[Kenter and De Rijke, 2015] Tom Kenter and Maarten\nDe Rijke. Short text similarity with word embeddings. In\nCIKM, 2015.\n[Laskar et al., 2020] Md Tahmid Rahman Laskar, Xiangji\nHuang, and Enamul Hoque. Contextualized embeddings\nbased transformer encoder for sentence similarity model-\ning in answer selection task. In LREC, pages 5505–5514,\n2020.\n[Lee et al., 2020] Jinhyuk Lee, Wonjin Yoon, Sungdong\nKim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioin-\nformatics, 36(4), 2020.\n[Liu et al., 2020] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo\nWang, Qi Ju, Haotang Deng, and Ping Wang. K-bert: En-\nabling language representation with knowledge graph. In\nAAAI, volume 34, 2020.\n[Longpre et al., 2020] Shayne Longpre, Yu Wang, and Chris\nDuBois. How effective is task-agnostic data augmentation\nfor pretrained transformers? In EMNLP, 2020.\n[Marcus et al., 1994] Mitch Marcus, Grace Kim, Mary Ann\nMarcinkiewicz, Robert MacIntyre, Ann Bies, Mark Fergu-\nson, Karen Katz, and Britta Schasberger. The penn tree-\nbank: Annotating predicate argument structure. In Human\nLanguage Technology, 1994.\n[McCann et al., 2017] Bryan McCann, James Bradbury,\nCaiming Xiong, and Richard Socher. Learned in trans-\nlation: Contextualized word vectors. In NIPS, 2017.\n[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai\nChen, Greg S Corrado, and Jeff Dean. Distributed rep-\nresentations of words and phrases and their composition-\nality. NIPS, 26, 2013.\n[Miller, 1995] George A Miller. Wordnet: a lexical database\nfor english. Communications of the ACM, 38(11), 1995.\n[Pennington et al., 2014] Jeffrey Pennington, Richard\nSocher, and Christopher D Manning. Glove: Global\nvectors for word representation. In EMNLP, 2014.\n[Rezayi et al., 2021] Saed Rezayi, Handong Zhao, Sungchul\nKim, Ryan Rossi, Nedim Lipka, and Sheng Li. Edge: En-\nriching knowledge graph embeddings with external text.\nIn NAACL, 2021.\n[R¨uckl´e et al., 2019] Andreas R ¨uckl´e, Nafise Sadat\nMoosavi, and Iryna Gurevych. Coala: A neural\ncoverage-based approach for long answer selection with\nsmall data. In AAAI, volume 33, 2019.\n[Shi et al., 2021] Haoyue Shi, Karen Livescu, and Kevin\nGimpel. Substructure substitution: Structured data aug-\nmentation for nlp. In ACL-IJCNLP, 2021.\n[Wei and Zou, 2019] Jason Wei and Kai Zou. Eda: Easy data\naugmentation techniques for boosting performance on text\nclassification tasks. In EMNLP-IJCNLP, 2019.\n[Wei et al., 2021] Jason Wei, Chengyu Huang, Soroush\nV osoughi, Yu Cheng, and Shiqi Xu. Few-shot text classi-\nfication with triplet networks, data augmentation, and cur-\nriculum learning. In NAACL, 2021.\n[Wu et al., 2020] Ledell Wu, Fabio Petroni, Martin Josi-\nfoski, Sebastian Riedel, and Luke Zettlemoyer. Scalable\nzero-shot entity linking with dense entity retrieval. In\nEMNLP, pages 6397–6407, 2020.\n[Xia et al., 2019] Mengzhou Xia, Xiang Kong, Antonios\nAnastasopoulos, and Graham Neubig. Generalized data\naugmentation for low-resource translation. In ACL, 2019.\n[Yang et al., 2019] Wei Yang, Yuqing Xie, Aileen Lin,\nXingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy\nLin. End-to-end open-domain question answering with\nbertserini. In NAACL, 2019.\n[Yih et al., 2013] Scott Wen-tau Yih, Ming-Wei Chang,\nChris Meek, and Andrzej Pastusiak. Question answering\nusing enhanced lexical semantic models. In ACL, 2013.\n[Zhao et al., 2018] Jieyu Zhao, Tianlu Wang, Mark Yatskar,\nVicente Ordonez, and Kai-Wei Chang. Gender bias in\ncoreference resolution: Evaluation and debiasing methods.\nIn NAACL, 2018.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSpecial Track on AI for Good\n5156",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7984435558319092
    },
    {
      "name": "Task (project management)",
      "score": 0.6607480645179749
    },
    {
      "name": "Matching (statistics)",
      "score": 0.6176319718360901
    },
    {
      "name": "Ontology",
      "score": 0.6123866438865662
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5633397102355957
    },
    {
      "name": "Agriculture",
      "score": 0.5632432699203491
    },
    {
      "name": "Natural language processing",
      "score": 0.5531324148178101
    },
    {
      "name": "Language model",
      "score": 0.547239363193512
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5313755869865417
    },
    {
      "name": "Transformer",
      "score": 0.519980251789093
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4942915439605713
    },
    {
      "name": "Ecology",
      "score": 0.07743382453918457
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I94310126",
      "name": "Shanxi Normal University",
      "country": "CN"
    }
  ]
}