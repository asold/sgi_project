{
  "title": "On Semantic Cognition, Inductive Generalization, and Language Models",
  "url": "https://openalex.org/W3208046308",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5064041005",
      "name": "Kanishka Misra",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2929581986",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W6635443538",
    "https://openalex.org/W1994114516",
    "https://openalex.org/W6684161233",
    "https://openalex.org/W1488309867",
    "https://openalex.org/W2968398601",
    "https://openalex.org/W3104178968",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W4287758766",
    "https://openalex.org/W4256106986",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3183524006",
    "https://openalex.org/W1594369375",
    "https://openalex.org/W2164797238"
  ],
  "abstract": "My doctoral research focuses on understanding semantic knowledge in neural network models trained solely to predict natural language (referred to as language models, or LMs), by drawing on insights from the study of concepts and categories grounded in cognitive science. I propose a framework inspired by 'inductive reasoning,' a phenomenon that sheds light on how humans utilize background knowledge to make inductive leaps and generalize from new pieces of information about concepts and their properties. Drawing from experiments that study inductive reasoning, I propose to analyze semantic inductive generalization in LMs using phenomena observed in human-induction literature, investigate inductive behavior on tasks such as implicit reasoning and emergent feature recognition, and analyze and relate induction dynamics to the learned conceptual representation space.",
  "full_text": "On Semantic Cognition, Inductive Generalization, and Language Models\nKanishka Misra*\nPurdue University, West Lafayette, IN, USA\nkmisra@purdue.edu\nAbstract\nMy doctoral research focuses on understanding semantic\nknowledge in neural network models trained solely to predict\nnatural language (referred to as language models, or LMs),\nby drawing on insights from the study of concepts and cate-\ngories grounded in cognitive science. I propose a framework\ninspired by ‘inductive reasoning,’ a phenomenon that sheds\nlight on how humans utilize background knowledge to make\ninductive leaps and generalize from new pieces of informa-\ntion about concepts and their properties. Drawing from exper-\niments that study inductive reasoning, I propose to analyze\nsemantic inductive generalization in LMs using phenomena\nobserved in human-induction literature, investigate inductive\nbehavior on tasks such as implicit reasoning and emergent\nfeature recognition, and analyze and relate induction dynam-\nics to the learned conceptual representation space.\nIntroduction\nHumans often engage in ‘Inductive Reasoning,’ the use\nof existing semantic knowledge to make inferences about\nnovel cases. For example, on encountering a new prop-\nerty - ‘Robins have T9 hormones, ’one might generalize (or\nproject) it to all birds (Osherson et al. 1990). Inductive in-\nferences are often domain dependent—e.g. biological in-\nformation may be projected across a taxonomy (robin and\nswan), whereas behavioral information may project across a\nshared property (hawk and tiger)—and may change during\na human’s development. Inductive reasoning sheds light on\nthe organization of human concept knowledge, and therefore\nplays an important role in theories of semantic cognition.\nMeanwhile, computational advances in the ﬁeld of natu-\nral language processing (NLP) have led to the development\nof complex neural network models of language (LMs). LMs\nprimarily represent language by encoding the distribution of\nwords in their contexts from large corpora, using a process\nknown as ‘pre-training.’ Their success on a range of higher\nlevel semantic tasks, combined with their black-box nature\nhas given rise to a research program, the goals of which\nare to develop an understanding of the knowledge LMs gain\nfrom pre-training (Alishahi, Chrupała, and Linzen 2019). In\n*The author gratefully acknowledges feedback from Julia Rayz\nand Allyson Ettinger.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nthis thesis, I attempt to contribute to this goal by developing\nan inductive reasoning-based analysis framework to better\nunderstand the synthetic semantic cognition of LMs.\nRelated Work\nAs LMs grow larger, so does their capacity to retrieve mem-\norized facts about the world (birds can ﬂy), leading to the\ndevelopment of the the paradigm known as ‘LMs as Knowl-\nedge Bases’. This paradigm uses LMs to perform ‘common-\nsense reasoning’ by simply querying pre-trained LMs with\nprompts that elicit word predictions corresponding to the\nretrieved fact (Petroni et al. 2019), or by ﬁne-tuning and\nevaluating LMs on knowledge bases (Bosselut et al. 2019).\nThis paradigm exclusively focuses onwhat aspects of world\nknowledge are accessible through pre-training. Such an in-\nquiry sheds important light on the access to long-term se-\nmantic memory as it emerges from predicting words in con-\ntext. I hope to extend this line of research by focusing on\nhow pre-trained LMs use semantic knowledge to process\nand generalize novel information, and to what extent their\nbehavior aligns to that in humans.\nInspired by research in cognitive science, my thesis ties in\nthe inﬂuential work of Rogers and McClelland (2004, R&M,\nhenceforth). R&M present a connectionist account of induc-\ntive reasoning, where they describe a feed-forward network\nthat performs inductive projections of novel properties, and\ndisplays patterns comparable to inductions in children across\nmultiple ages (Carey 1985). Despite this connection, my the-\nsis pursues a line of research independent to that of R&M\nas it exclusively relies on representations learned by models\nfrom the statistics contained in language corpora. It therefore\ntargets the role played by language—more speciﬁcally, the\npre-training of LMs to predict words in context—in facili-\ntating the learning of semantic knowledge as opposed to the\nlocalist representations of R&M, who make no such com-\nmitment. More recently, Sinha et al. (2019) introduced the\nCLUTRR benchmark to study a different kind of inductive\nreasoning—one that is rooted in formal logic—in LMs on\nsynthetic kinship information expressed as language. Unlike\nCLUTRR , the inductive reasoning capacities considered in\nmy thesis make graded distinctions between generalizations\nacross two different concepts – i.e., generalization of a prop-\nerty to robin may differ from that to penguins. Such distinc-\ntions are not considered in CLUTRR , which instead focuses\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n12894\non discrete and logical generalization.\nInductive Reasoning with Language Models\nI argue that a test of induction in LMs must satisfy two\ndesiderata: (1) The LM must perform a semantically mean-\ningful task that facilitates inductive reasoning – i.e., during\ninduction, the LM must accept novel information about con-\ncepts (robins can queem), and then produce an output that\ncan be used to conclude about how it applies the informa-\ntion to other concepts (canaries can queem vs. giraffes can\nqueem); and (2) It must cast induction as a probabilistic in-\nference – induction experiments involve supplying humans\nwith novel ‘premises’, followed by an investigation of how\nlikely they think a ‘conclusion’ is, usually interpreted as a\nconditional probability. Based on the aforementioned con-\nstraints, I propose to ﬁne-tune the LMs under investigation\nto classify generic beliefs as true (cats have fur ) or false\n(dogs can ﬂy). The beliefs will be sourced from a belief-\nbank, a data structure consisting of facts about the world,\nretrieved from existing commonsense knowledge-bases that\nrepresent concepts and their properties. Novel information\nwill be formed by linking existing concepts to properties\nconsisting of nonce words (e.g., dax, fep, queem, etc.). Dur-\ning induction, the novel information will be processed by\nthe LM using a standard backpropagation step, repeated un-\ntil correct classiﬁcation is achieved. Next, the LM’s weights\nwill be frozen and the LM will be evaluated over a range of\ndifferent conclusion statements by investigating its probabil-\nity for the true label during a forward pass.\nUsing this general framework for simulating induction in\nLMs, I aim to answer the following research questions:\nRQ1: What kinds of inductive generalizations about con-\ncepts are made by LMs?To answer this question, I pro-\npose to devise induction experiments targeting ﬁndings from\nthe human-induction literature (Osherson et al. 1990; Kemp\nand Tenenbaum 2009). The goal of these experiments is to\nzero in on the kinds of semantic inductive biases represented\nin the LMs’ generalization capacities, and compare them to\nphenomena that drive induction in humans.\nRQ2: To what extent do LMs recognize and use emergent\nfeatures during induction? Capturing implicit knowl-\nedge is a long-standing goal for commonsense reasoning\n(Bosselut et al. 2019; Talmor et al. 2020). This question tar-\ngets whether LMs implicitly use features that emerge in the\ninduction environment. For instance, the property ofﬂight is\nimplicitly encoded in the set of concepts: frobins, bats, air-\nplanesg, which may share the same novel property during\ninduction. The objective here is to quantify the tendency of\nLMs to generalize the novel property to other concepts that\npossess the emergent feature (e.g., butterﬂies).\nRQ3: How do the inductive generalization capacities of\nLMs relate to their representational space?In order to\nextend the interpretability of my methods and yield a mech-\nanistic insight on how LMs characterize novel information,\nI propose to measure the correspondence between the in-\nduction dynamics (loss during induction backpropagation)\nof the LMs and their representational geometry.\nObjective Timeline\nRQ1 October 2021 - February 2022\nProposal Defense January/February 2022\nRQ2 March 2022 - August 2022\nRQ3 September 2022 - January 2022\nThesis Writing January 2023 - February 2023\nTable 1: Research Timeline\nTaken together, the above questions target a comprehen-\nsive exploration of induction in LMs, with experiments rang-\ning from tests of hallmark phenomena in human-induction\nliterature, to implicit reasoning and learning dynamics.\nPreliminary Work and Research Timeline\nIn my previous research (Misra, Ettinger, and Rayz 2021), I\ninvestigated induction in LMs using experiments that con-\nform with the ‘LMs as knowledge bases’ paradigm, with\na focus on whether typicality of concepts (e.g., robins are\nmore typical birds than are penguins) manifests in the word\nprediction capacities of LMs. My current research involves\ntraining LMs on the true/false task using data from exist-\ning knowledge-bases, as well as running preliminary experi-\nments that target the presence of speciﬁc inductive biases in\nLMs (part of RQ1). Table 1 shows my research timeline.\nReferences\nAlishahi, A.; Chrupała, G.; and Linzen, T. 2019. Analyzing\nand interpreting neural networks for NLP: A report on the\nﬁrst BlackboxNLP workshop. Natural Language Engineer-\ning, 25(4): 543–557.\nBosselut, A.; Rashkin, H.; Sap, M.; Malaviya, C.; Celikyil-\nmaz, A.; and Choi, Y . 2019. Comet: Commonsense trans-\nformers for automatic knowledge graph construction. ACL.\nCarey, S. 1985. Conceptual Change in Childhood. MIT\npress.\nKemp, C.; and Tenenbaum, J. B. 2009. Structured statis-\ntical models of inductive reasoning. Psychological review,\n116(1): 20.\nMisra, K.; Ettinger, A.; and Rayz, J. T. 2021. Do language\nmodels learn typicality judgments from text? CogSci.\nOsherson, D. N.; Smith, E. E.; Wilkie, O.; Lopez, A.; and\nShaﬁr, E. 1990. Category-based induction. Psychological\nreview, 97(2): 185.\nPetroni, F.; Rockt¨aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;\nMiller, A. H.; and Riedel, S. 2019. Language models as\nknowledge bases? In EMNLP.\nRogers, T. T.; and McClelland, J. L. 2004. Semantic cogni-\ntion: A parallel distributed processing approach. MIT press.\nSinha, K.; Sodhani, S.; Dong, J.; Pineau, J.; and Hamilton,\nW. L. 2019. CLUTRR: A diagnostic benchmark for induc-\ntive reasoning from text. In EMNLP.\nTalmor, A.; Tafjord, O.; Clark, P.; Goldberg, Y .; and Berant,\nJ. 2020. Leap-of-thought: Teaching pre-trained models to\nsystematically reason over implicit knowledge. In NeurIPS.\n12895",
  "topic": "Inductive reasoning",
  "concepts": [
    {
      "name": "Inductive reasoning",
      "score": 0.7821561098098755
    },
    {
      "name": "Computer science",
      "score": 0.685291051864624
    },
    {
      "name": "LEAPS",
      "score": 0.6811280250549316
    },
    {
      "name": "Generalization",
      "score": 0.6695737242698669
    },
    {
      "name": "Inductive bias",
      "score": 0.5406081676483154
    },
    {
      "name": "Cognition",
      "score": 0.5378583669662476
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5357407927513123
    },
    {
      "name": "Cognitive science",
      "score": 0.5168641209602356
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.5110045671463013
    },
    {
      "name": "Natural language",
      "score": 0.47867709398269653
    },
    {
      "name": "Representation (politics)",
      "score": 0.47509896755218506
    },
    {
      "name": "Natural language processing",
      "score": 0.4677484333515167
    },
    {
      "name": "Mental representation",
      "score": 0.4106840491294861
    },
    {
      "name": "Psychology",
      "score": 0.1755245327949524
    },
    {
      "name": "Multi-task learning",
      "score": 0.14493009448051453
    },
    {
      "name": "Task (project management)",
      "score": 0.13555192947387695
    },
    {
      "name": "Mathematics",
      "score": 0.11715421080589294
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Financial economics",
      "score": 0.0
    }
  ]
}