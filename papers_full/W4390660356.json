{
  "title": "Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering",
  "url": "https://openalex.org/W4390660356",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093115559",
      "name": "Rahman S M Wahidur",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5093115560",
      "name": "Ishmam Tashdeed",
      "affiliations": [
        "Islamic University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5020133324",
      "name": "Manjit Kaur",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5034347136",
      "name": "Heung-No Lee",
      "affiliations": [
        "Gwangju Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2816065698",
    "https://openalex.org/W4205499116",
    "https://openalex.org/W4210404112",
    "https://openalex.org/W3021511960",
    "https://openalex.org/W3206339540",
    "https://openalex.org/W6772793610",
    "https://openalex.org/W4283784468",
    "https://openalex.org/W3140987722",
    "https://openalex.org/W4311306009",
    "https://openalex.org/W4223591627",
    "https://openalex.org/W2900961456",
    "https://openalex.org/W4300555876",
    "https://openalex.org/W2781487490",
    "https://openalex.org/W3007466464",
    "https://openalex.org/W2981840673",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6810296985",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W2810665353",
    "https://openalex.org/W4316012225",
    "https://openalex.org/W4319977973",
    "https://openalex.org/W4313307542",
    "https://openalex.org/W4224129762",
    "https://openalex.org/W4213159554",
    "https://openalex.org/W4312686575",
    "https://openalex.org/W4378418765",
    "https://openalex.org/W4226185762",
    "https://openalex.org/W4378364795",
    "https://openalex.org/W4315631870",
    "https://openalex.org/W6791312270",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6810220367",
    "https://openalex.org/W6852584927",
    "https://openalex.org/W6849107809",
    "https://openalex.org/W4367595583",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W6850191221",
    "https://openalex.org/W4389092034",
    "https://openalex.org/W6850769804",
    "https://openalex.org/W6802669662",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W6845943790",
    "https://openalex.org/W6847118041",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W2094848746",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W4385571886",
    "https://openalex.org/W4364320763",
    "https://openalex.org/W6850202480",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W6790003725",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4301393026",
    "https://openalex.org/W4382498938",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4207056801",
    "https://openalex.org/W4315588830",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3003547652",
    "https://openalex.org/W4391453730",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3164567032",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4330337916"
  ],
  "abstract": "Blockchain technology has revolutionized the financial landscape, witnessing widespread adoption of cryptocurrencies due to their decentralized and transparent nature. As sentiments expressed on social media platforms wield substantial influence over cryptocurrency market dynamics, sentiment analysis has emerged as a crucial tool for gauging public opinion and predicting market trends. This paper explores fine-tuning techniques for large language models to enhance sentiment analysis performance. Experimental results demonstrate a significant average zero-shot performance gain of 40&#x0025; on unseen tasks after fine-tuning, highlighting its potential. Additionally, the impact of instruction-based fine-tuning on models of varying scales is examined, revealing that larger models benefit from instruction tuning, achieving the highest average accuracy score of 75.16&#x0025;. In contrast, smaller-scale models may experience reduced generalization due to complete model capacity utilization. To gain deeper insight into instruction effectiveness, the paper presents experimental investigations under different instruction tuning setups. Results show the model achieves an average accuracy score of 72.38&#x0025; for short and simple instructions, outperforming long and complex instructions by over 12&#x0025;. Finally, the paper explores the relationship between fine-tuning corpus size and model performance, identifying an optimal corpus size of 6,000 data points for the highest performance across different models. Microsoft&#x2019;s MiniLM, a distilled version of BERT, excels in efficient data use and performance optimization, while Google&#x2019;s FLAN-T5 demonstrates consistent and reliable performance across diverse datasets.",
  "full_text": "Date of submission xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nEnhancing Zero-Shot Crypto Sentiment with\nFine-tuned Language Model and Prompt\nEngineering\nRAHMAN S M WAHIDUR1, ISHMAM TASHDEED2, MANJIT KAUR3, (Senior Member, IEEE), and\nHEUNG-NO LEE1, (Senior Member, IEEE)\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea\n2Department of Computer Science and Engineering, Islamic University of Technology, Dhaka, Bangladesh.\n3School of Computer Science and Artificial Intelligence, SR University, Warangal, Telangana, India-506371.\nCorresponding author: Heung-No Lee (heungno@gist.ac.kr).\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the\nKorea government (MSIT) (No.2023-2021-0-00118, Development of decentralized consensus composition technology for large-scale\nnodes) and This research was supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC (Information Technology\nResearch Center) support program (IITP-2023-2021-0-01835) supervised by the IITP (Institute of Information & Communications\nTechnology Planning & Evaluation)\nABSTRACT Blockchain technology has revolutionized the financial landscape, witnessing widespread\nadoption of cryptocurrencies due to their decentralized and transparent nature. As sentiments expressed on\nsocial media platforms wield substantial influence over cryptocurrency market dynamics, sentiment analysis\nhas emerged as a crucial tool for gauging public opinion and predicting market trends. This paper explores\nfine-tuning techniques for large language models to enhance sentiment analysis performance. Experimental\nresults demonstrate a significant average zero-shot performance gain of 40% on unseen tasks after fine-\ntuning, highlighting its potential. Additionally, the impact of instruction-based fine-tuning on models of\nvarying scales is examined, revealing that larger models benefit from instruction tuning, achieving the highest\naverage accuracy score of 75.16%. In contrast, smaller-scale models may experience reduced generalization\ndue to complete model capacity utilization. To gain deeper insight into instruction effectiveness, the paper\npresents experimental investigations under different instruction tuning setups. Results show the model\nachieves an average accuracy score of 72.38% for short and simple instructions, outperforming long and\ncomplex instructions by over 12%. Finally, the paper explores the relationship between fine-tuning corpus\nsize and model performance, identifying an optimal corpus size of 6,000 data points for the highest\nperformance across different models. Microsoft’s MiniLM, a distilled version of BERT, excels in efficient\ndata use and performance optimization, while Google’s FLAN-T5 demonstrates consistent and reliable\nperformance across diverse datasets.\nINDEX TERMS Zero-Shot Learning, In-Context Learning, Supervised Fine-tuning, Instruction Tuned,\nPrompt Engineering.\nI. INTRODUCTION\nI\nN the recent decade, cryptocurrency has gained much trac-\ntion in finance and business for its decentralization, per-\nmissionless, and open nature built on the public blockchain\n[1]. This architectural framework possesses the capability\nto create a financial system known for its immutability and\nextensive interoperability, providing unprecedented trans-\nparency, equal access rights, and a substantial reduction in\nreliance on central authorities, governed by smart contracts\n[2]. Cryptocurrencies employ cryptographic ciphers to facil-\nitate financial transactions, distinguishing them from tradi-\ntional forms of currency [3]. Cryptocurrencies represent the\npioneering class of pure digital assets to be embraced by\nasset managers [4]. They mitigate double-spending attacks by\nusing several confirmations from adjacent nodes within the\nblockchain network. With each additional confirmation, the\ntransaction becomes more trustworthy and less susceptible\nto reversal. Because of these favorable attributes and the\nwidespread accessibility of cryptocurrencies, they serve as\nboth a medium of exchange and a store of wealth. [4], [5].\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\nIn cryptocurrency discussions, social media networks have\ngained significant importance as information-sharing plat-\nforms. Communicating crypto events and market conditions\nthrough social media is widely recognized. Moreover, there is\na prevalent belief in the correlation between altcoin prices and\nthe sentiment expressed within the Twitter community [6].\nThe demand for cryptocurrency is intricately tied to people’s\ntrust in Bitcoin and its underlying technology. As people’s\ntrust plays a significant role in the growth of the cryptocur-\nrency market, the sentiment of the general population has\na substantial impact on the future market capitalization of\ncryptocurrencies [7]. The effect of social media on crypto dis-\ncourse is growing [8]. Social media platforms play a pivotal\nrole in shaping cryptocurrency discourse, influencing public\nperceptions, fueling market fluctuations, facilitating informa-\ntion dissemination, and generating excitement. In 2019, over\n300 million active monthly users shared their emotions in\nmultiple languages on various social media platforms, includ-\ning Twitter, Facebook, and YouTube. Among these platforms,\nTwitter has ascended to a position of eminence as a highly\ninfluential social media network [9], [10]. Twitter stands out\nas a unique platform due to its straightforward way of gauging\nindividuals’ sentiments toward a text. Performing real-time\ndata analysis is also possible on the Twitter platform [11].\nIn the realm of computational Natural Language Process-\ning (NLP) research, sentiment analysis stands as a distinct\nsub-area. It serves as a prevalent method for context-based\nmining, aimed at extracting valuable and subjective informa-\ntion from textual data [6], [12]. There are several approaches\nto sentiment analysis, which is the process of determining the\nsentiment or emotion expressed in the text based on the type\nand size of the corpus. The rule-based approaches use a set of\npredefined rules, such as regular expressions and dictionaries,\nto classify text as positive, negative, or neutral [12]. Machine\nlearning approaches try to find patterns from the provided\ndata. This methodology entails training a machine learning\nmodel on a labeled dataset to categorize new text. These\nmachine-learning techniques are further expanded over super-\nvised and unsupervised. Wherein supervised approaches are\ntrained on an annotated dataset, unsupervised does not require\nwork on experience to improve accuracy [13]. Despite the\nneed for substantial labeled data and computational resources\nduring the initial training of pre-trained models, fine-tuning\nthem on specific datasets can optimize their performance\n[14].\nIn text mining, sentence-level sentiment analysis has\nemerged as a burgeoning area of research. The analytical\nprocess comprises six core stages: data acquisition, data pre-\nprocessing, feature extraction, model training, model evalu-\nation, and model deployment. Feature extraction, in partic-\nular, holds significant importance in optimizing the model’s\nefficacy. Conventionally, Bag-of-Words (BoW) [15] and N-\ngram [16] techniques are widely employed for this purpose.\nHowever, their one-hot word representation approach creates\nhigh-dimensional feature spaces and scalability challenges,\nfailing to capture word sequences and their syntactic and\nsemantic nuances. Word embedding models like word2vec\n[17] and Glove [18] have gained popularity for their ability to\ncapture word semantics in high-dimensional spaces, although\nthey require substantial training data and are susceptible to\ndata sparsity issues for rare or out-of-vocabulary (OVV) [12]\nwords, which can impact performance.\nTo address the abovementioned limitations, a transformer-\nbased NLP model was proposed in the paper [19] \"Attention\nis All You Need.\" The transformer architecture relies on\nself-attention, enabling the model to assess the significance\nof various words within a sentence during prediction. In\nshort, it creates contextual awareness between the words of\na sentence. BERT [20], T5 [21], GPT-3 [22], and LLaMA\n[23] are a few examples among all pre-trained transformer-\nbased models for NLP tasks. All these models have shown\nremarkable performance on various NLP tasks, especially in\nfew-shot learning. However, they are less successful in zero-\nshort learning [24]. This paper explores a simple but powerful\nmethod called instruction tuned to improve the performance\nof zero-shot learning of Large Language Models (LLMs) for\ncryptocurrency sentiment classification tasks. Moreover, this\npaper utilizes an In-context learning (ICL) and prompt engi-\nneering method to generate effective instructions using LLMs\nthat allow the creation of effective instructional datasets for\nfine-tuning the pre-trained language models to extract the\ncryptocurrency sentiment from social media data.\nThe following provides a concise overview of the main\ncontributions of this paper:\n• Supervised fine-tuning and instruction-based ap-\nproaches are used to fine-tune pre-trained DistilBERT,\nMiniLM, and FLAN-T5 models.\n• Three models from FLAN-T5 are intended to evaluate\nthe effectiveness of instruction-based fine-tuning at dif-\nferent scales.\n• Two classes of instructions such as short and simple, and\nlong and complex are employed to examine the impact\nof instruction tuning setups on model performance.\n• Various subsets of the corpus are used to investigate the\neffect of fine-tuning corpus size on model efficacy.\nThe subsequent sections of this paper are structured as\nfollows: Section II delves into the background information\nand provides an extensive review of pertinent literature. Sec-\ntion III unveils the intricate architectural framework of the\nproposed work. Section IV meticulously presents the eval-\nuation process, including the analysis of relevant metrics\nand experimental outcomes. Finally, in Section V , the paper\nconcludes and summarizes the findings while also discussing\nthe potential areas for future research.\nII. RELATED WORK\nRecently, researchers have perceived a growing interest in\nleveraging sentiment analysis techniques for cryptocurrency.\nSeveral models are available for sentiment acquisition, each\nwith varying precision and applicability [25]. Hasan et al.\n[26] investigated public sentiment analysis by employing\na ML (machine learning) algorithm called SVM (Support\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVector Machine). The researchers also utilized Chi-square\nfor feature selection to mitigate sentence noise. Satrya et al.\n[27] employed a similar approach to ascertain the sentiment\npolarity. To facilitate numerical analysis, they employed TF-\nIDF weighting to convert the textual data into a numerical rep-\nresentation. The methodology employed by Padmalatha et al.\n[28] is based on Naive Bayes model to analyze social media\nopinions. Prasad et al. [29] designed an ensemble classifier to\nclassify YouTube comments based on cryptocurrency. They\nused Decision Tree, K Nearest Neighbors, Random Forest\nClassifier, XGBoost, and a Logistic Regression base classifier\nto create a stacked ensemble model. Sasmaz et al. [6] studied\nthe feasibility of automated sentiment analysis for cryptocur-\nrencies using the Random Forest Classifier.\nThe Valence Aware Dictionary for Sentiment Reasoning\n(V ADER), a widely utilized and straightforward sentiment\ncalculation model, is commonly employed to predict cryp-\ntocurrency prices by analyzing cryptocurrency-related news\nand tweets. Suardi et al. [30] employed V ADER for an inquiry\ninto the predictive capabilities of information embedded in\nsocial media tweets concerning cryptocurrency market fluc-\ntuations. Initiating the study with an exploration of poten-\ntial price fluctuations, Oikonomopoulos et al. [31] embarked\non an examination of twitter sentiment analysis within the\ncryptocurrency domain, specifically focusing on its predictive\ncapabilities in forecasting cryptocurrency price fluctuations.\nJagini et al. [32] intend to analyze the effect of tweets on the\nstock price of Bitcoin. To calculate the associated sentiment,\nthey also used V ADER. Parekh et al. [33] introduced a re-\nsilient hybrid DL-Gues framework designed for forecasting\ncryptocurrency prices. They also utilized the similar V ADER\ntechnique in their framework to extract the polarity from the\ntwitter sentiment.\nRegarding the recent superior performance transformers-\nbased model, Dwivedi et al. [34] utilized the BERT (Bidi-\nrectional Encoder Representation) to predict the sentiments\nof cryptocurrency news articles. Kim et al. [35] introduced\nCBITS, a fine-tuned version of BERT designed explicitly for\ncryptocurrency sentiment analysis, mainly focusing on the\nKorean crypto market. Widianto et al. [36] have created a\nBERT model for the purpose of conducting sentiment anal-\nysis on cryptocurrency and NFTs. This was accomplished\nthrough the utilization of data crawling and pre-processing\nusing Rapiminer. The findings of Ortu et al. [37] indicate\nthat incorporating features derived from BERT-based emotion\nclassification of comments on GitHub and Reddit results in\na notable improvement in the predictability of Bitcoin and\nEthereum’s hourly and daily return direction. Despite ex-\ntensive exploration of sentiment extraction methods, includ-\ning rule-based and machine learning-based approaches, they\noften prove inadequate in cryptocurrencies due to domain-\nspecific jargon, slang, and sentiment ambiguity not ade-\nquately covered by general-purpose texts [35]. Rule-based\nsystems, although less accurate, are limited by the need for\nmore powerful linguistic resources [12]. On the other hand,\nmachine learning-based methods tend to achieve higher ac-\ncuracy but require large amounts of labeled training data\nand significant computational resources. Existing literature\nreveals a notable gap in utilizing large language models for\nsentiment extraction. Additionally, there is a lack of research\nusing fine-tuning techniques, despite their potential to en-\nhance performance and adaptability.\nThis paper proposes integrating large language models and\nutilizing fine-tuning techniques in sentiment extraction to\naddress these research gaps. By leveraging the capabilities\nof large language models and optimizing their performance\nthrough fine-tuning, this research aims to overcome the lim-\nitations of current approaches and advance the accuracy and\napplicability of sentiment extraction, especially in cryptocur-\nrency. This contribution expects to facilitate the develop-\nment of more effective and comprehensive sentiment analysis\nmethodologies, ultimately enhancing decision-making pro-\ncesses in related industries and domains.\nIII. PROPOSED SYSTEM\nThis section serves as a comprehensive exposition of the pro-\ncedural methodology employed in this paper, divided into six\nsub-sections, denoted as sections III-A through III-F. Section\nIII-A addresses the issue of problem formulation and includes\nthe mathematical representation of the proposed problem.\nSection III-B elucidates the proposed system architecture, in-\ncorporating its visualization. Section III-C delves into the ex-\nploration of pre-trained language models’ utilization. Section\nIII-D provides an extensive analysis of the fine-tuning process\nfor large language models. In section III-E, an exploration of\nin-context learning is undertaken, while section III-F offers\nan in-depth examination of prompt engineering.\nA. PROBLEM FORMULATION\nLet X be the set of tweets related to cryptocurrency where\neach tweet x ∈ X is represented as a feature matrix x ∈ R1×n\nthus, X can be represented as\nX ∈ Rm×n (1)\nhere, m represents the number of tweets, and n signifies the\nnumber of features utilized to represent each tweet.\nLet Y be a set of sentiments represented as a target matrix\nY ∈ Rm×1 (2)\nFor this experiment, only positive, negative sentiment tweets\nare considered. Thus dataset D can be formulated as\nD = {(X(i), Y (i))|X(i) ∈ R(1×n), Y (i) ∈ R(1×1)} (3)\nwhich consisting of pairs {X(i), Y (i)}. We can then define the\nmodel\nf : R(m×n) → R(m×1) (4)\nthat maps each tweet matrix X ∈ R(m×n) to its sentiment\nmatrix Y ∈ R(m×1). The objective is to fine-tune the model f\non the data set D such that\n∀x ∈ X : f (x, Θ) ∈ Y (5)\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\nwhere Θ is the model parameters. Various mathematical tech-\nniques, such as gradient descent, backpropagation, and cross-\nentropy loss, are used to fine-tune the model f on D. Addi-\ntionally, a combination of optimization algorithms, including\nstochastic gradient descent and Adam, is utilized to identify\nthe most favorable parameter set Θ∗ that reduces the cross-\nentropy loss. The loss function can be defined as\nL(Θ) = −\nmX\ni=1\ny(i)log(f (x(i), Θ))+\n(1 − y(i))log(1 − f (x(i), Θ))\n(6)\nwhere x(i) represents the ith tweet vector in the training set,\ny(i) is the corresponding sentiment label (either 0 or 1 for\nnegative or positive sentiment, respectively), and f (x(i), Θ)\nis the predicted sentiment score for x(i) given the current set\nof parameters Θ. The objective is to curtail the cross-entropy\nloss L(Θ) by finding the optimal set of parameters Θ∗ that\nmaximizes the likelihood of the training data D given the\nmodel.\nUpon fine-tuning the model, one can utilize it to predict the\nsentiment of new tweets. This would be achieved by inputting\ntheir feature matrix representations into the model, which\nyields the predicted sentiment matrix as output. Mathemat-\nically, the final model can be represented as\nMeval = (ftuned, Dtest) (7)\nwhere ftuned is the fine-tuned model and Dtest is the evaluation\ndataset.\nB. PROPOSED SYSTEM ARCHITECTURE\nThe depiction in Figure 1 illustrates the overall system ar-\nchitecture of our proposed methodology. The proposed ar-\nchitecture begins with the initial user interaction step and\nprompt generation. The user engages with the \"text-davinci-\n003\" model made by OpenAI, initiating the prompt genera-\ntion process. The users provide contextual information using\nprompt templates, which input the subsequent stages. The\nprovided text is transmitted to the language model’s backend\nthrough an API call, facilitating communication between the\nusers and the large language model. Next, a content moderator\ncomponent is employed to evaluate the content. If the content\nis determined to be unsafe, the model responds with a default\nmessage. In the case of safe content, the system proceeds\nthrough a safety gateway to engage with the core parts of\nthe large language model. This model encompasses several\nkey components, like a response generator, context processor,\nknowledge domain, and system response generator. The sys-\ntem leverages its knowledge domain through interaction with\nthese components to generate an effective response based on\nthe provided input. The resulting response, an AI-generated\nprompt, is then delivered to the user. Upon receiving the\nAI-generated prompt, a filtering process takes place. Human\nfeedback is vital in determining whether the prompt should\nprogress to the subsequent stages.\nThe primary objective of this filtering is to generate ade-\nquate instructions by harnessing the advantages of in-context\nlearning inherent in large language models and to prune\nlow-quality and repeated instructions before adding them to\nthe task pool. Finally, the prompt generation is refined by\nincorporating user feedback and capitalizing on the model’s\ncontextual understanding to enhance its efficacy. The process\ninvolving user interaction, prompt generation, filtering, and\nhuman feedback collectively constitutes the in-context learn-\ning and prompt engineering phase. The overall process of\ngenerating instructions using in-context learning can be seen\nin Algorithm 1. This phase aims to iteratively improve the\nprompt generation procedure by leveraging user feedback and\nmaximizing the model’s contextual comprehension capabili-\nties.\nFollowing the generation of effective instructions, the sub-\nsequent objective is to create an augmented dataset. This\ndataset is constructed by concatenating the introductions with\nthe original dataset, ensuring a comprehensive collection\nof relevant information for training purposes. Algorithm 2\nshows the process of generating an augmented crypto sen-\ntiment dataset. The instructional dataset is then divided into\nseparate training and validation sets. Three additional datasets\nare withheld to measure zero-shot performance, remaining\nuntouched for evaluation. The large language models undergo\nfine-tuning at this stage, utilizing the instructional dataset.\nThe initial model weights are modified, resulting in a newly\ninstructed, fine-tuned model version. The overall fine-tuning\nprocess of a large language model can be observed in Al-\ngorithm 3. Upon completion of the fine-tuning stage, the\nefficacy of diverse models is meticulously assessed. This\nevaluation gives important intuition into the efficacy and ef-\nfectiveness of the in-context learning and prompt engineering\ntechniques employed in instructing fine-tuning for Zero-shot\nlearning.\nAlgorithm 1Generating instructions using in-context learn-\ning.\nConstant model, model, temp, max_len, top_p, penalty\nInput A human-generated prompt p ∈ P.\nOutput An effective instructions pool Y .\n1: Define the Language Model (LLM) as a function G :\nP → R that maps a given prompt p ∈ P to a model-\ngenerated response r ∈ R.\n2: r ← G(p; mode, model, temp, max_len, top_p)\n3: Pass the response through a filter F : R → (True, False)\n4: for response r ∈ R do\n5: if F(R) = True then\n6: Add r to the effective instructions pool Y .\n7: else\n8: Discard the response r.\n9: end if\n10: end for\n11: Return Effective instructions pool Y\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 1. The Proposed System Architecture.\nC. PRE-TRAINED LANGUAGE MODELS (PLMS)\nRecently, a substantial emphasis has been placed on pre-\ntrained language models (PLMs), which leverage self-\nsupervised learning with extensive raw text data [38]. Notable\nexamples of such models include GPT-3 [22], PaLM [39],\nChinchilla [40], LLaMA [23], and Falcon 40B [41]. Through\nextensive training on vast text corpora, the model is imbued\nwith the ability to perform self-learning tasks such as masked\nword prediction, sentence sequence recognition, text comple-\ntion, and text generation [19], [42]. PLMs acquire a compre-\nhensive understanding of language. In addition, these models\nenhance the semantic representation of words by considering\ncontextual dynamics and provide a unified framework for\nvarious NLP tasks. Currently, there are three standard models\n[43] structures in PLMs: autoregressive language models,\nautoencoding language models, and hybrid language mod-\nels. Representative models for each design are GPT [22],\nBERT [20], and T5 [21], respectively. Autoregressive lan-\nguage models follow a standard approach in which language\nmodeling is performed decoder-only. They predict words\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\nAlgorithm 2Generating augmented crypto sentiment dataset.\nInput Effective instructions pool Y , and Crypto sentiment\ndataset X.\nOutput Augmented crypto sentiment dataset Xaugmented .\n1: Let function C : x → x which cleans a dataset entry.\n2: Xfiltered ⊆ Xwhich contains only non-neutral sentiments.\n3: for i ← (1 . . .len(Xfiltered)) do\n4: X(i)\ncleaned ← C(X(i)\nfiltered).\n5: end for\n6: Select an instruction y ∈ Y .\n7: Let function A : (x, y) → x which augments each entry\nx ∈ X with an instruction y.\n8: for i ← (1 . . .len(Xcleaned )) do\n9: X(i)\naugmented ← A(X(i)\ncleaned , y).\n10: end for\n11: Return Augmented dataset Xaugmented\nAlgorithm 3Fine-tuning of a pre-trained language model.\nConstant random_seed, input sequence size, number of\nlayers, number of hidden layer nodes, number of classifier\noutputs.\nInput Training set (X(1)\ntrain, Y (1)\ntrain), ...,(X(M)\ntrain, Y (M)\ntrain ).\nOutput Trained language model network parameters..\n1: for epoch ∈ epochs do\n2: for batch ∈ batches do\n3: for i ← (1 . . .len(batch)) do\n4: M(i)\ntrain ← tokenizer(X(i)\ntrain)\n5: ˆM(i)\ntrain ← model(M(i)\ntrain)\n6: end for\n7: loss ← E(Y (batch)\ntrain , Y (batch)\ntrain )\n8: Calculate ∇Θ for backpropagation.\n9: Adjust parameters using an optimizer to minimize\nthe loss.\n10: end for\n11: end for\n12: Return Task-specific fine-tuned model.\none by one through a one-way language encoding-decoding\nprocess, with word predictions made on a token-by-token\nbasis. Autoencoding language models randomly mask words\nin a sentence, use bidirectional encoding to capture context,\nand then predict the masked words based on the encoded\ninformation. Finally, hybrid language models combine the\napproaches of the previous two models. The model employs a\nstrategy of random word masking within a sentence, followed\nby bidirectional encoding and a step-by-step prediction of\nsubsequent words using the encoded representation of the\npreceding text [43].\nThe progress in computational language modeling has re-\nvealed that LLMs can attain a more profound understanding\nof language and demonstrate enhanced abilities in under-\nstanding and generating data. These models learn abstract\nknowledge from raw data, resulting in better generality and\ngeneralization. The autoregressive language model employed\nin GPT-3 and its succeeding versions, such as GPT3.5 and\nGPT4, has proven advantageous in utilizing natural language\nfor various tasks in different fields [43]. Under the initial\nassumption, increasing the number of parameters in models\nwas purported to enhance performance. However, recent re-\nsearch by Hoffmann et al. [23] has shown that smaller models\ntrained on more data can achieve the best performances given\na specific computing budget.\nAccording to Figure 2, the data analysis shows a clear\ntrend of increasing pre-trained token usage over the years.\nIn the initial years (2020-2021), models exhibited relatively\nlow token counts during the pre-training phase. However, in\nrecent years (2022-2023), there has been a notable surge in\nthe number of pre-trained tokens utilized by language models.\nSuch expansion signifies model developers’ recognition of\nthe benefits of leveraging more extensive and diverse pre-\ntraining corpora, enabling improved contextual understand-\ning and enhanced performance in downstream tasks.\nFIGURE 2. Yearly trend analysis of model parameter size and token usage.\nContrary to the trend observed in pre-trained token usage,\nmodel parameter sizes exhibit a different pattern. In 2021\nand 2022, models with considerably large parameter sizes\nemerged. However, in 2023, a noticeable decrease in model\nparameter sizes is observed. This shift suggests a growing\nfocus on optimizing computational efficiency and addressing\nthe resource-intensive nature of large models. As a result,\nmodel developers are actively exploring methods to achieve\ncomparable performance with fewer parameters, which may\nreduce computational costs and carbon footprint.\nAccording to Figure 3, language models can be categorized\ninto three distinct clusters: models with low token usage and\nsmall parameter sizes, representing the majority, are suitable\nfor resource-constrained environments. Models with substan-\ntial token counts but relatively small parameter sizes exhibit\nan exciting trade-off, leveraging massive amounts of pre-\ntraining data while keeping the parameter sizes reasonably\nsmall. Models characterized by minimal token consumption\nand expansive parameterization prioritize performance opti-\nmization, striking a delicate balance between computational\nresource utilization and model capacity. The observed trends\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nin language model development hold significant implications\nfor researchers and practitioners. The increasing usage of\npre-trained tokens emphasizes the importance of diverse and\nextensive training data for capturing the degree of language\nunderstanding. Conversely, the fluctuation in model parame-\nter sizes highlights ongoing efforts to balance model capacity\nand computational efficiency. Further research is required to\nexplore novel techniques and architectures that optimize the\ntrade-offs in token usage and parameter sizes.\nD. FINE-TUNING OF LARGE LANGUAGE MODELS (LLMS)\nLLMs have exhibited exceptional abilities in various NLP\ntasks [39], [44], [45]. Nevertheless, these models can some-\ntimes display unintended behaviors, such as generation of\nfalse information, the pursuit of inaccurate objectives, and the\nproduction of harmful, misleading, and biased expressions\n[46], [47]. In the pre-training stage, pre-trained language\nmodels acquire non-task-specific language knowledge. The\nsubsequent fine-tuning stage facilitates task-specific adjust-\nments of the model, enabling it to be utilized for various\ndownstream tasks [48]. Two primary strategies have been\nproposed [49] to tailor pre-trained language models for spe-\ncific tasks: the feature-based approach and fine-tuning. The\nfeature-based approach involves loading a pre-trained LLM\nand utilizing it on a target dataset. The primary focus is\ncreating output embeddings for the training set, which can\nbe used as input features for a classification model. While\nthis approach is often used for embedding-centric models\nlike BERT, embeddings can also be extracted from generative\nGPT-style models like \"text-embedding-ada-002\". The classi-\nfication model can be any desired model, such as a logistic re-\ngression model, random forest, or XGBoost. However, linear\nclassifiers, specifically logistic regression, have demonstrated\nsuperior performance [49].\nFine-tuning is essential for adapting pre-trained language\nmodels to perform specific tasks using labeled training data.\nInitially, a pre-trained language model is used as a starting\npoint and then fine-tuned on a task-specific dataset with la-\nbeled examples. This process is called supervised fine-tuning\n(SFT). The SFT process is an essential step in effectively uti-\nlizing PLMs for tasks such as sentence sentiment classifica-\ntion, named entity recognition, summarization, and question-\nanswering. Unlike pre-training, this fine-tuning requires less\ndata, typically around 100k words [50]. During SFT, a task-\nspecific layer is added to the PLMs, and the model param-\neters, including those of the task-specific layer(usually fully\nconnected layers), are updated through gradient descent using\nan appropriate loss function [49]. One advantage of the PLMs\nis the ability to freeze specific layers while fine-tuning the\nremaining ones, potentially improving performance. How-\never, it has been observed that freezing too many layers may\nlead to poor performance [50]. The widespread adoption and\nremarkable capabilities of PLMs stem from the fact that their\npre-training process is not task specific and requires execution\nonly once. Subsequently, a simple and more cost-effective\nfine-tuning process is sufficient for each specific task. This\nFIGURE 3. Cluster distribution analysis considering model parameters\nand token utilization.\nis possible because the dataset size required for fine-tuning\nis considerably smaller, reducing time and resource require-\nments [51].\nAnother form of fine-tuning is instruction tuning (IT) –\nfine-tuning language models on a set of data described via\ninstructions. Fundamentally, instruction tuning involves fine-\ntuning pre-trained LLMs using a set of formatted instances in\nnatural language form [24]. This approach is closely related\nto SFT [46]. The initial step involves gathering or construct-\ning instances formatted as instructions to initiate instruction\ntuning. Subsequently, these formatted instances are employed\nto finetune LLMs using supervised learning techniques, such\nas training with the sequence-to-sequence loss. Following\ninstruction tuning, LLMs exhibit enhanced generalization ca-\npabilities towards unseen tasks [24], [52], [53], even within\na multilingual context [54]. IT is a more efficient alternative\nto pre-training, as it relies on a moderate number of instances\nfor training. This supervised training process introduces dis-\ntinct optimization considerations compared to pre-training,\nincluding utilizing a sequence-to-sequence loss as the training\nobjective and special attention to factors like smaller batch\nsize and learning rate in the optimization configuration [53].\nIT significantly impacts LLMs by improving performance\nacross various models and enhancing task generalization by\nenabling LLMs to understand and follow natural language\ninstructions [47], [55].\nE. IN-CONTEXT LEARNING (ICL)\nIn-Context Learning (ICL) pertains to comprehending the\ncontext of the input information and leveraging it accurately\nto produce the intended output. It exhibits similarities to the\nhuman decision-making process, wherein individuals learn\nfrom analogy [56]. In LLM utilization, ICL was initially\nintroduced as a distinctive prompting method, specifically\nalongside GPT-3 [44], and has since emerged as a promi-\nnent approach [43]. In contrast to supervised learning, which\ninvolves a training phase that utilizes backward gradients\nto modify model parameters, ICL does not engage in pa-\nrameter updates but instead performs direct predictions on\npre-trained language models. The effectiveness of ICL lies\nin its dependence on the existing knowledge of the model\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\nand its ability to decipher the concealed pattern present in\nthe demonstrations, thus facilitating precise predictions. The\napproach holds significant potential in situations that demand\nswift adaptation to new tasks, as it prevents the need for ex-\ntensive training periods [57]. The vanilla GPT-3 model shows\nconsiderable potential for ICL, as pre-training adaptation has\nbeen shown to enhance its capabilities [58]. Moreover, the\nefficacy of ICL is contingent upon various parameters such\nas the choice of prompting template, in-context examples, and\ntheir sequence [59]. Despite being plausible, the underlying\nworking principle of ICL remains obscure, and only a few\nstudies have provided initial insights into its workings [45],\n[60].\nThe context window size in LLMs, such as GPT-4, de-\ntermines the maximum amount of input text the model can\nconsider when generating its output. With the release of GPT-\n4, the context window’s size increased, surpassing the 2048\ntoken limit imposed on GPT-3. The context window for the\nGPT-4 API is 8195 tokens, and a 32K context window exists\nin the most significant model [43]. However, its usual perfor-\nmance tends to lag behind when it comes to fine-tuning, since\nit necessitates parameter adjustments to align with particular\ntasks, potentially limiting its ability to address task-specific\nintricacies [49]. Moreover, ICL is associated with potential\nrisks. The risk of prejudice and misinformation is a significant\nconcern, as the LLMs cannot fact-check the input provided as\npart of the prompt, resulting in the possibility of incorporating\nerroneous and biased information into any generated output,\nincluding fabricated news or blog posts [61].\nF. PROMPT ENGINEERING\nAs technological progress continues to unfold, the signifi-\ncance of prompt engineering grows in parallel. The ascen-\ndancy of large language models, such as ChatGPT [43], ne-\ncessitates a skillset that can proficiently engage with them.\nBy furnishing appropriate prompts, adherence to prescribed\nnorms and the automation of processes can be ensured, re-\nsulting in outputs that align with desired quality and quantity\nbenchmarks. Like programming, prompts facilitate the cus-\ntomization of interactions with large language models, opti-\nmizing their utilization to meet specific requirements [62].\nA prompt comprises a specific set of directives furnished to\na LLM, enabling customization and refinement of the model’s\ncapabilities through programming [63]. In short, it is a text\nthat provides context and instructions for LLMs to gener-\nate a response. Prompts are pivotal in facilitating LLMs to\nundertake extensive linguistic tasks, encompassing language\ntranslation, sentiment classification, article summarization,\nQ&A, and even producing human-like and coherent text [61].\nThe use of prompting techniques in NLP tasks has been\nextensively studied, covering both zero-short and few-shot\nsettings, as indicated by various research papers [64]–[67].\nCurrent prompt-based models predominantly rely on Trans-\nformers [19]. Prompting is particularly prevalent in online\ndemonstrations, where generative models are interacted with\nusing Transformers as assistive agents. By leveraging prompt\nengineering, the challenging task of language understanding\ncan be addressed with improved efficiency [68]. The \"show-\nand-tell\" technique [69], where examples and instructions are\nprovided within the prompt, represents the most effective\nmethodology for eliciting the intended output from large\nlanguage models (LLMs). We provide an example prompt\nin our LLM to show how prompts for LLMs can be crafted\nto output certain desired results. The structure of the prompt\nis presented in Figure 4. Prompt engineering possesses sev-\neral notable advantages: Firstly, it enables the pre-training\nof the language model on extensive volumes of raw text.\nFurthermore, the introduction of a novel prompting function\nempowers the model to exhibit proficiency in few-shot or\neven zero-shot learning, enabling it to effectively acclimate\nto new scenarios with minimal or no labeled data. [63].\nFIGURE 4. Prompt engineering template.\nIV. RESULT ANALYSIS\nThis result analysis section is organized into three sub-\nsections. Collectively, they encompass the dataset descrip-\ntion, discussion of evaluation metrics, presentation of de-\ntailed experimental findings, and a comparative assessment\nof model performance. The experiments were conducted iter-\natively to identify anomalies and mitigate any potential bias.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. DATASET DESCRIPTION AND PRE-PROCESSING\nFour datasets, namely the Neo, Reddit, Bitcoin sentiment,\nand Cryptocurrency sentiment datasets are utilized in the\nexperimental analysis. These datasets are the foundation for\nconducting comprehensive investigations and analyses re-\nlated to sentiment analysis in cryptocurrency. The statistics\nare presented in Table 1.\nThe Neo dataset consists of tweets related to cryptocurren-\ncies, specifically focused on the sentiment towards the Neo\ncryptocurrency. The dataset contains 12,000 tweets, evenly\ndistributed between positive and negative emotions, with\n6,000 tweets in each category. The Reddit dataset contains\nposts and comments extracted from the popular social media\nplatform Reddit. The dataset focuses on the sentiment ex-\npressed towards various cryptocurrencies. It includes a total\nof 562 posts. Among these posts, 302 are classified as positive\nsentiment, while 260 are classified as negative sentiment.\nThe Bitcoin sentiment dataset consists of tweets specifically\nrelated to the Bitcoin cryptocurrency. The dataset comprises\n1,029 tweets, with 779 tweets classified as positive sentiment\nand 250 tweets classified as negative sentiment. It is important\nto note that this dataset exhibits an imbalance in sentiment\ndistribution, with more positive than negative tweets taken\nintentionally for the experiments. The Cryptocurrency sen-\ntiment dataset is a collection of tweets that cover a wide\nrange of cryptocurrencies. This dataset comprises 500 tweets,\nmeticulously balanced between positive and negative senti-\nment categories, with 250 tweets representing each sentiment.\nTABLE 1. Volume of all tweets and volume of tweets for each sentiment\nlabel and dataset.\nDescription V olume Percentage\nAll tweets 14,091 100.00%\nPositive label 7,331 52.03%\nNegative label 6,760 47.97%\nNeo dataset 12,000 85.16%\nBitcoin sentiment dataset 1,029 7.30%\nReddit dataset 562 3.99%\nCryptocurrency sentiment dataset 500 3.55%\nThis paper uses the Neo dataset to finetune the pre-trained\nlanguage models. The other datasets, Reddit, Bitcoin senti-\nment, and Cryptocurrency sentiment datasets, are employed\nto judge the performance of the fine-tuned models on unseen\ntasks. The concept of unseen tasks is defined based on prior\nwork, which disallows the same dataset to appear during\ntraining. By separating the training and evaluation datasets\nin this manner, the study aims to provide a robust assessment\nof the model’s ability to generalize and perform effectively on\nnew, unseen data.\nB. EVALUATION METRICS\nTo assess the efficacy of the fine-tuned models, binary classi-\nfication accuracy and F1-score were employed as the evalua-\ntion metrics. The calculation of binary classification accuracy\ncan be defined as follows:\nT = θ + ∅\nθ + Λ +∅ + Ψ (8)\nwhere, T denotes accuracy, defined as the proportion of\ncorrectly predicted instances by the model out of the total\nsample size. θ represents true positives, instances where the\nactual output label and the model’s predicted result are both\npositive. ∅ denotes true negatives, instances where both the\npredicted and actual result are negative. Λ corresponds to false\npositives, instances where the predicted result is positive, but\nthe actual result is negative. Ψ represents false negatives,\ninstances where the predicted result is negative, but the actual\nresult is positive.\nThe F1 score aims for a balance between precision and\nrecall, achieved by taking the harmonic mean of the two. This\ncan be represented as\nF1 = 2\nPrecision −1 + Recall −1 (9)\nF1 = θ\nθ + 0.5(∅ + Ψ) (10)\nPrecision is defined as the ratio of correctly identified positive\ninstances to the total number of positive instances, achieved\nby dividing the number of true positives by the sum of all\ntrue positives and false positives. Similarly, recall is defined\nas the proportion of actual positive instances that are correctly\nidentified as positive, calculated by dividing the number of\ntrue positives by the sum of true positives and false negatives.\n[12].\nC. COMPARISON EXPERIMENT AND DISCUSSION\nThis section will describe the research questions, the experi-\nment details, the result, and the discussion.\n1) How do supervised fine-tuning and instruction tuning\nimpact the efficiency of pre-trained large language models in\nterms of performance on unseen tasks?\nThe research question of whether fine-tuning improves the\nmodel efficiency was investigated through a series of experi-\nments. Three pre-trained language models, namely DistilBert,\nMiniLM, and FLAN-T5-Base, were employed in conjunction\nwith four datasets to facilitate the training and performance\nevaluation of the models. This work was mainly focused on\nthe zero-shot setup under the default parameter setup except\nfor the learning rate, batch size, and no of epochs. Training\nthe models, a maximum of 3 epochs was conducted, with the\nutilization of the Adam optimizer featuring a learning rate of\n0.00002 and a batch size of 8.\nBased on the experimental setup the pre-trained language\nmodels were divided into three groups: untuned model, SFT\nmodel, and IT model. Untuned model is the vanilla LM hav-\ning the original checkpoint (only pre-training, no additional\nfine-tuning). The SFT model is considered the fine-tuning se-\ntups in a standard supervised way without instruction. It does\nnot follow any predefined templates. The model receives only\ninputs and generates the outputs (e.g., for text classification,\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\nthe input would be \"Earn bitcoin on a daily basis!\" and the\noutput would be \"Positive\"). On the other hand, the IT model\nis considered fine-tuning setups in a standard supervised way\nwith natural instruction. For IT model, first the instruction and\ninstance input were concatenated to make a single prompt\n(e.g., for text classification. the input would be \"Detect the\nsentiment of the given text, Text: Earn bitcoin on a daily\nbasis!\" and the output would be \"Positive\") and then trained\nthe model to generate the instance output.\nTables 2, 3, and 4 present the results of experiments con-\nducted for the classification of cryptocurrency-related tweets,\ndistinguishing between positive class and negative class. The\nperformances were compared between three language mod-\nels with their vanilla, supervised fine-tuned, and instruction-\nbased fine-tuned versions in terms of accuracy, F1 score,\nprecision, and recall.\nThe overall analysis of the results based on the accu-\nracy score is shown in Figure 5, which indicates that fine-\ntuning improves the model performance dramatically. The\nbaseline performance for this study was established with the\nvanilla LM models. The DistiBERT-vanilla model achieved\nan accuracy score of 35.66%, 58.95%, and 60.33% across\nthree datasets Bitcoin sentiment dataset, Reddit dataset, and\nCryptocurrency sentiment dataset respectively. The MiniLM-\nvanilla model achieved an accuracy score of 41.67%, 46.67%,\nand 51.25%. And the Flan-T5-Base-vanilla model achieved\nan accuracy score of 25.10%, 46.07%, and 45.89%. The\naverage accuracy score of all three vanilla LM models across\nall three different datasets is 45.73%. However, after fine-\ntuning, the average performance improved significantly to\n59.52% for SFT model and 68.80% for IT model. The exper-\nimental results provide compelling evidence that fine-tuning\nenhances model efficiency. The substantial improvements in\naccuracy scores demonstrate the effectiveness of fine-tuning\nacross different datasets and models. The average perfor-\nmance gain of 40% highlights the significant impact of fine-\ntuning on model performance, indicating its potential for\npractical applications.\nAdditionally, a comparison was made between the SFT\nand IT models to identify the role of instructions. The results\nrevealed that the fine-tuned models with instructions outper-\nformed their counterparts without instructions. On average,\nthe performance of the IT model was 16% higher than the\nSFT model. The consistent improvements observed across\nmultiple models and datasets reinforce the generalizability\nof the findings. The results indicate that fine-tuning can be a\nvaluable technique for optimizing model efficiency in various\ndomains and tasks, providing researchers and practitioners\nwith a powerful tool to enhance model performance.\n2) How the benefits of instruction tuning are affected by\nmodel scale?\nBased on the study conducted by Brown et al. [44], which\nrevealed that zero and few-shot capabilities of larger language\nmodels substantially improve for larger models, the present\nresearch delves into investigating how the scale of the model\nFIGURE 5. Zero-shot performance analysis between untuned, supervised,\nand instruction-base fine-tuned models on unseen tasks.\nTABLE 2. Classification results on Bitcoin sentiment dataset.\nModel Accuracy F1 score Precision Recall\nDistilBERT-vanilla 35.66% 24.42% 62.60% 15.97%\nMiniLM-vanilla 41.67% 27.99% 25.00% 33.33%\nFlan-T5-Base-vanilla 25.10% 0.26% 0.78% 0.16%\nAvg_vanilla 34.14% 17.56% 29.46% 16.49%\nDistilBERT-SFT 42.83% 41.28% 70.68% 31.68%\nMiniLM-SFT 66.07% 63.03% 70.91% 61.21%\nFlan-T5-Base-SFT 65.89% 62.13% 72.57% 60.13%\nAvg_SFT 58.26% 55.48% 71.39% 51.00%\nDistilBERT-IT 68.12% 70.93% 91.87% 61.53%\nMiniLM-IT 70.38% 78.01% 78.59% 81.40%\nFlan-T5-Base-IT 75.00% 83.98% 75.00% 100.00%\nAvg_IT 71.17% 77.64% 81.82% 80.98%\nBest_Score 75.00% 83.98% 91.87% 100.00%\nTABLE 3. Classification results on Reddit dataset.\nModel Accuracy F1 score Precision Recall\nDistilBERT-vanilla 58.95% 30.79% 49.60% 23.45%\nMiniLM-vanilla 46.67% 0.96% 1.90% 0.77%\nFlan-T5-Base-vanilla 46.07% 1.05% 2.86% 0.64%\nAvg_vanilla 50.56% 10.93% 18.12% 8.29%\nDistilBERT-SFT 66.07% 63.03% 70.91% 61.21%\nMiniLM-SFT 60.71% 57.00% 63.48% 57.08%\nFlan-T5-Base-SFT 53.75% 67.42% 53.70% 99.71%\nAvg_SFT 60.18% 62.48% 62.70% 72.67%\nDistilBERT-IT 64.82% 55.60% 76.86% 48.04%\nMiniLM-IT 63.75% 62.03% 65.06% 65.94%\nFlan-T5-Base-IT 74.29% 75.00% 70.19% 86.60%\nAvg_IT 67.62% 64.21% 70.70% 66.86%\nBest_Score 74.29% 75.00% 76.86% 99.71%\nTABLE 4. Classification results on Cryptocurrency sentiment dataset.\nModel Accuracy F1 score Precision Recall\nDistilBERT-vanilla 60.33% 30.76% 49.80% 23.15%\nMiniLM-vanilla 51.25% 45.64% 35.83% 66.67%\nFlan-T5-Base-vanilla 45.89% 0.95% 2.86% 0.57%\nAvg_vanilla 52.49% 25.78% 29.50% 30.13%\nDistilBERT-SFT 65.89% 62.13% 72.57% 60.13%\nMiniLM-SFT 60.54% 58.19% 67.19% 57.16%\nFlan-T5-Base-SFT 53.93% 68.49% 53.88% 99.71%\nAvg_SFT 60.12% 62.94% 64.54% 72.34%\nDistilBERT-IT 64.82% 55.60% 76.86% 48.04%\nMiniLM-IT 63.75% 62.03% 65.06% 65.94%\nFlan-T5-Base-IT 74.29% 75.00% 70.19% 86.60%\nAvg_IT 67.62% 64.21% 70.70% 66.86%\nBest_Score 74.29% 75.00% 76.86% 99.71%\ninfluences the benefits of instruction tuning. The impact of\ninstruction tuning was evaluated across FLAN-T5 models of\ndifferent sizes: small (80M), base (250M), and large (780M),\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nbased on their parameters. The model’s architecture and their\ncomparative analysis have been summarized in Table 5.\nTable 6 shows the experimental results of classifying the\ncrypto-related tweets for three models on three datasets. The\nmodels are divided into two groups. One is the vanilla LM,\nand the other one is the instruction-based fine-tuned model.\nAs depicted in Figure 6, the results shed light on the ef-\nfectiveness of instruction tuning with a larger model size\nenhancing the performance on unseen tasks. The untuned\nmodels achieved an average accuracy of 54.28% for the small\nmodel, 39.02% for the base model, and 39.28% for the large\nmodel. The achieved accuracy result was inconsistent com-\nFIGURE 6. Evaluating instruction tuning efficacy for sentiment detection\nin FLAN-T5 models of varying sizes.\npared to the model’s size. However, after applying instruction\ntuning, the accuracy improved to 57.98% for the small model,\n73.10% for the base model, and 75.17% for the large model.\nThis potential result could be explained based on the study\nconducted by Wei et al. [19] that instruction tuning serves two\npurposes for larger-scale models. Firstly, it occupies some of\nthe model’s capacity. Secondly, it instructs these models on\nfollowing instructions, enabling them to apply this skill to\nnew tasks using the remaining capacity. This phenomenon\naids in the capacity of larger models to adapt to novel tasks.\nHowever, it has the opposite effect on smaller models, hin-\ndering their ability to generalize to unfamiliar tasks, possibly\ndue to the total model capability being allocated to learn the\namalgamation of various instruction tuning tasks.\n3) How does the instruction-based model respond over\ndifferent instruction tuning setups?\nThe conducted experimentation aimed to measure and com-\npare the quality of models under different instruction tuning\nsetups, focusing on the response of the instruction-based\nmodel. By introducing diversity in the styles and formats of\ntasks through instructions of varying lengths and complexi-\nties, the study provided insights into how the model handles\ndiverse instructions and facilitates prompt tuning. Generally,\nthey can be formulated as the following equations:\nargmaxMtuned\nX\ni∈I\nQ(Mtuned, Li, Ci) (11)\nwhere I = {i1, i2, i3, . . . ,in}. ij represents the instruction.\nQ(Mtuned, Li, Ci) represents the quality of the instruction-\ntuned model. Mtuned be the instruction-tuned model, Li de-\nnotes the length of instruction i, and Ci denotes the com-\nplexity of instruction i. Six instructions were created for this\nexperiment, representing different lengths and complexities\nas shown in Table 7.\nThe results of prompt tuning experiments were analyzed\nfor both short and simple instructions, as well as long and\ncomplex instructions. The experimental findings are depicted\nin Figure 7. Preliminary experiments revealed that prompt\nFIGURE 7. Performance analysis of untuned and instruction-based\nfine-tuned FLAN-T5 model across various prompts.\ntuning had no significant impact on the performance of the\nvanilla LM model, which achieved an average accuracy score\nof 46.43% for both setups. However, prompt tuning demon-\nstrated a significant improvement in the performance of the\ninstruction-based fine-tuned model. Under short and simple\ninstructions, the model achieved an average accuracy score\nof 72.38%, showcasing its effectiveness in understanding and\nexecuting such instructions. Conversely, the model demon-\nstrated a slightly inferior performance for long and complex\ninstructions, with an average accuracy score of 63.39%.\nThe findings suggest that the instruction-based model ex-\ncels in responding to and executing short and simple instruc-\ntions, outperforming its performance under long and complex\ninstructions by over 12%.\n4) How does the size of the fine-tuning dataset impact the\nperformance of different language models, and what is the\noptimal sample size for achieving the highest performance\nof any model?\nThe experimental evaluation aimed to explore the relationship\nbetween the size of the fine-tuning dataset and the SFT mod-\nels’ performance. Three models, namely DistilBert, MiniLM,\nand FLAN-T5-Base, were examined, and the sample size was\nvaried from 2,000 to 12,000 data points to assess the influence\nof data availability. Table 8 presents the average zero-shot\naccuracy achieved by each model at different sample sizes\non unseen tasks. The experimental findings provide valuable\ninsights into the impact of fine-tuning dataset size on model\nperformance, highlighting data efficiency and consistency\nconsiderations across the examined models. Among the three\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\nTABLE 5. Architectural details of FLAN-T5 models.\nModel Architecture #Heads #Layers Estimated model params size #Parameters #Trainable parameters Model Checkpoint\nFLAN-T5-SMALL encoder-decoder 6 8 308MB 80M 77M google/flan-t5-small\nFLAN-T5-BASE encoder-decoder 12 12 990MB 250M 247M google/flan-t5-base\nFLAN-T5-LARGE encoder-decoder 16 24 3133MB 780M 783M google/flan-t5-large\nTABLE 6. Performance analysis of untuned and instruction-based fine-tuned FLAN-T5 model across different sizes and datasets.\nModel Untuned Instruction-based tuned\nBitcoin sentiment Reddit Cryptocurrency sentiment Bitcoin sentiment Reddit Cryptocurrency sentiment\nFLAN-T5-SMALL 59.98% 51.43% 51.43% 75.00% 49.46% 49.46%\nFLAN-T5-BASE 25.10% 46.07% 45.89% 75.00% 72.14% 72.14%\nFLAN-T5-LARGE 25.87% 46.07% 45.89% 76.94% 74.29% 74.29%\nTABLE 7. Prompt type variations for sentiment detection and\ninstruction-based fine-tuned model evaluation across diverse text types.\nType Prompt\nShot and simple\nPlease detect the sentiment.\nDetect the sentiment of the text.\nPlease detect the sentiment of the given text.\nLong and complex\nClassify the sentiment of the provided cryptocurrency\nrelated social media posts or messages.\nDetermine the emotional tone of the given text, which\nprimarily revolves around cryptocurrencies and their\nassociated concepts.\nCategorize the sentiment expressed in the provided\ndataset consisting of the text snippets related to\ncryptocurrency and computer science, focusing on\ncapturing positive or negative sentiments.\nTABLE 8. Average zero-shot accuracy at different sample sizes on\nsupervised fine-tuned models.\nSample size DistilBert MiniLM FLAN-T5 Average Best Score\n2K 54.58% 63.03% 58.51% 58.71% 63.03%\n4K 56.54% 69.08% 60.27% 61.93% 69.08%\n6K 66.64% 69.67% 61.16% 65.82% 69.67%\n8K 57.83% 66.82% 60.89% 61.85% 66.82%\n10K 55.93% 54.58% 61.04% 57.19% 61.04%\n12K 58.26% 59.72% 60.93% 59.64% 60.93%\nmodels, the best accuracy result was consistently achieved\nwith a sample size of 6,000 data points. At this sample size,\nDistilBert reached an accuracy of 66.64%, MiniLM achieved\n69.67%, and FLAN-T5-Base demonstrated an accuracy of\n61.16%. This indicates that 6,000 data points serve as an\noptimal balance for achieving the highest performance for\neach model. MiniLM exhibited significant data efficiency by\nachieving an average zero-shot accuracy of 63.03% with a\nsample size of 2,000 data points. In comparison, FLAN-T5-\nBase and DistilBert required larger sample sizes, approxi-\nmately double and triple the data, respectively, to attain com-\nparable accuracy levels. This showcases MiniLM’s ability to\nleverage limited data and deliver competitive performance\neffectively.\nFurthermore, FLAN-T5-Base displayed consistent perfor-\nmance across varying sample sizes, maintaining relatively\nstable average zero-shot accuracy values. This suggests the\nmodel’s robustness and resilience to variations in the dataset\nsize. Conversely, DistilBert exhibited performance fluctua-\ntions and a declining trend as the sample size increased be-\nyond 6,000 data points. This implies that DistilBert may reach\na saturation point where additional data does not contribute\nsignificantly to its performance improvement.\nThe findings have practical implications for model se-\nlection in the fine-tuning process. The data efficiency of\nMiniLM, combined with the consistent performance of\nFLAN-T5-Base, offers advantages in scenarios with limited\ndata availability or where performance stability is crucial.\nMeanwhile, considering the appropriate sample size, such as\n6,000 data points, is essential to optimize the performance of\nDistilBert.\nV. CONCLUSION\nThis paper underscores the significance of employing fine-\ntuning techniques on large language models within the cryp-\ntocurrency domain. The experimental results reveal a note-\nworthy 40% average gain in zero-shot performance after fine-\ntuning, emphasizing the potential of this strategy in optimiz-\ning the effectiveness of pre-trained language models. Two\ndistinct fine-tuning techniques were implemented, revealing\nthat the instruction-based fine-tuned model surpassed the SFT\nmodel by an average of 16%. Furthermore, the outcomes\ndemonstrate that larger models achieved an impressive av-\nerage accuracy score of 75.17% through instruction adjust-\nments, whereas smaller parameter size models struggled to\nmeet the target baseline. The study also elucidates the model’s\nproficiency when presented with short, clear instructions,\noutperforming its performance under longer, more complex\ninstructions by almost 12%. Additionally, the experimental\nresults pinpoint an optimal corpus size of 6,000 data points,\ncontributing to the performance of the models in this experi-\nment. Among these models, MiniLM distinguishes itself for\nits optimized data utilization. Impressively, FLAN-T5 consis-\ntently upholds reliable performance across diverse corpora.\nThe future work will focus on conducting empirical experi-\nments to validate the effectiveness of the proposed approach\nin real-world scenarios. This comprehensive approach aims to\ntackle two fundamental aspects: enhancing decision-making\nprocesses in cryptocurrency investments and establishing en-\nduring correlations within long-term trading activities.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nREFERENCES\n[1] Muhammad Saad and Aziz Mohaisen. Towards characterizing blockchain-\nbased cryptocurrencies for highly-accurate predictions. INFOCOM 2018\n- IEEE Conference on Computer Communications Workshops , pages 704–\n709, 7 2018.\n[2] Vitalik Buterin et al. A next-generation smart contract and decentralized\napplication platform. white paper, 3(37):2–1, 2014.\n[3] Zeinab Shahbazi and Yung Cheol Byun. Improving the cryptocurrency\nprice prediction performance based on reinforcement learning. IEEE\nAccess, 9:162651–162659, 2021.\n[4] Fan Fang, Carmine Ventre, Michail Basios, Leslie Kanthan, David\nMartinez-Rego, Fan Wu, and Lingbo Li. Cryptocurrency trading: a com-\nprehensive survey. Financial Innovation, 8:1–59, 12 2022.\n[5] Patel Jay, Vasu Kalariya, Pushpendra Parmar, Sudeep Tanwar, Neeraj Ku-\nmar, and Mamoun Alazab. Stochastic neural networks for cryptocurrency\nprice prediction. IEEE Access, 8:82804–82818, 2020.\n[6] Emre Şaşmaz and F. Boray Tek. Tweet sentiment analysis for cryptocur-\nrencies. Proceedings - 6th International Conference on Computer Science\nand Engineering, UBMK 2021 , pages 613–618, 2021.\n[7] Pratikkumar Prajapati. Predictive analysis of bitcoin price considering\nsocial sentiments. arXiv preprint arXiv:2001.10343 , 2020.\n[8] Nikolaos Passalis, Loukia Avramelou, Solon Seficha, Avraam Tsantekidis,\nStavros Doropoulos, Giorgos Makris, and Anastasios Tefas. Multisource\nfinancial sentiment analysis for detecting bitcoin price change indications\nusing deep learning. Neural Computing and Applications , 34:19441–\n19452, 11 2022.\n[9] Yogev Matalon, Ofir Magdaci, Adam Almozlino, and Dan Yamin. Using\nsentiment analysis to predict opinion inversion in tweets of political com-\nmunication. Scientific Reports 2021 11:1 , 11:1–9, 3 2021.\n[10] Kogilavani Shanmugavadivel, V . E. Sathishkumar, Sandhiya Raja,\nT. Bheema Lingaiah, S. Neelakandan, and Malliga Subramanian. Deep\nlearning based sentiment analysis and offensive language identification on\nmultilingual code-mixed data. Scientific Reports 2022 12:1 , 12:1–12, 12\n2022.\n[11] Binder Tweet. Twitter real time monitoring and twitter live analytics. https:\n//www.tweetbinder.com/blog/real-time-twitter/, 2019.\n[12] Sayyida Tabinda Kokab, Sohail Asghar, and Shehneela Naz. Transformer-\nbased deep learning models for the sentiment analysis of social media data.\nArray, 14:100157, 7 2022.\n[13] María Hernández-Rubio, Iván Cantador, and Alejandro Bellogín. A com-\nparative analysis of recommender systems based on item aspect opinions\nextracted from user reviews. User Modeling and User-Adapted Interaction ,\n29:381–441, 4 2019.\n[14] Sonali Sharma, Manoj Diwakar, Kapil Joshi, Prabhishek Singh,\nShaik Vaseem Akram, and Anita Gehlot. A critical review on sentiment\nanalysis techniques. Proceedings of 3rd International Conference on\nIntelligent Engineering and Management, ICIEM 2022 , pages 741–746,\n2022.\n[15] Zhao Jianqiang, Gui Xiaolin, and Zhang Xuejun. Deep convolution neural\nnetworks for twitter sentiment analysis. IEEE Access , 6:23253–23260, 1\n2018.\n[16] Ummara Ahmed Chauhan, Muhammad Tanvir Afzal, Abdul Shahid,\nMoloud Abdar, Mohammad Ehsan Basiri, and Xujuan Zhou. A compre-\nhensive analysis of adverb types for mining user sentiments on amazon\nproduct reviews. World Wide Web, 23:1811–1829, 5 2020.\n[17] Sani Kamiş and Dionysis Goularas. Evaluation of deep learning techniques\nin sentiment analysis from twitter data. Proceedings - 2019 International\nConference on Deep Learning and Machine Learning in Emerging Appli-\ncations, Deep-ML 2019 , pages 12–17, 8 2019.\n[18] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing (EMNLP) ,\npages 1532–1543, 2014.\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is\nall you need. Advances in Neural Information Processing Systems , 2017-\nDecember:5999–6009, 6 2017.\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\nPre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805 , 2018.\n[21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Explor-\ning the limits of transfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research , 21:1–67, 10 2019.\n[22] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,\nSamyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye,\nGeorge Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to\ntrain megatron-turing nlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990 , 2022.\n[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971 , 2023.\n[24] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu,\nBrian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language\nmodels are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.\n[25] Lin Yue, Weitong Chen, Xue Li, Wanli Zuo, and Minghao Yin. A survey of\nsentiment analysis in social media. Knowledge and Information Systems ,\n60:617–663, 8 2019.\n[26] Isabella Donita Hasan, Raymond Sunardi Oetama, and Aldo Lionel\nSaonard. Sentiment analysis on cryptocurrency based on tweets and\nretweets using support vector machines and chi-square. 2022 7th Inter-\nnational Conference on Informatics and Computing, ICIC 2022 , 2022.\n[27] Raja Nanda Satrya, Oktariani Nurul Pratiwi, Riska Yanu Farifah, and Jemal\nAbawajy. Cryptocurrency sentiment analysis on the twitter platform using\nsupport vector machine (svm) algorithm. Proceedings - International\nConference Advancement in Data Science, E-Learning and Information\nSystems, ICADEIS 2022 , 2022.\n[28] E Padmalatha, Sailekya Sheral, K Dhanush, S Samveeth, B Ruchi Datta,\nand K Tarun Krishna. Sentiment analysis of bitcoin data by tweets through\nnaive bayes. In 2022 International Conference on Electrical, Computer,\nCommunications and Mechatronics Engineering (ICECCME) , pages 1–4.\nIEEE, 2022.\n[29] Gaurav Prasad, Gaurav Sharma, Dinesh Kumar Vishwakarma, et al. Sen-\ntiment analysis on cryptocurrency using youtube comments. In 2022 6th\nInternational Conference on Computing Methodologies and Communica-\ntion (ICCMC), pages 730–733. IEEE, 2022.\n[30] Sandy Suardi, Atiqur Rahman Rasel, and Bin Liu. On the predictive power\nof tweet sentiments and attention on bitcoin. International Review of\nEconomics & Finance , 79:289–301, 5 2022.\n[31] Sotirios Oikonomopoulos, Katerina Tzafilkou, Dimitrios Karapiperis, and\nVassilios Verykios. Cryptocurrency price prediction using social media\nsentiment analysis. 13th International Conference on Information, Intelli-\ngence, Systems and Applications, IISA 2022 , 2022.\n[32] Achyut Jagini, Kaushal Mahajan, Namita Aluvathingal, Vedanth Mohan,\nand Prajwala TR. Twitter sentiment analysis for bitcoin price prediction.\n2023 3rd International Conference on Smart Data Intelligence (ICSMDI) ,\npages 32–37, 3 2023.\n[33] Raj Parekh, Nisarg P Patel, Nihar Thakkar, Rajesh Gupta, Sudeep Tanwar,\nGulshan Sharma, Innocent E Davidson, and Ravi Sharma. Dl-guess: Deep\nlearning and sentiment analysis-based cryptocurrency price prediction.\nIEEE Access, 10:35398–35409, 2022.\n[34] Himanshu Dwivedi. Cryptocurrency sentiment analysis using bidirectional\ntransformation. In 2023 3rd International Conference on Smart Data\nIntelligence (ICSMDI), pages 140–142. IEEE, 2023.\n[35] Gyeongmin Kim, Minsuk Kim, Byungchul Kim, and Heuiseok Lim. Cbits:\nCrypto bert incorporated trading system. IEEE Access , 11:6912–6921,\n2023.\n[36] Mochammad Haldi Widianto and Yhudi Cornelius. Sentiment analysis\ntowards cryptocurrency and nft in bahasa indonesia for twitter large amount\ndata using bert. International Journal of Intelligent Systems and Applica-\ntions in Engineering , 11:303 – 309, 2 2023.\n[37] M Ortu, N Uras, C Conversano, G Destefanis, and S Bartolucci. On techni-\ncal trading and social media indicators in cryptocurrencies’ price classifica-\ntion through deep learning. arxiv 2021. arXiv preprint arXiv:2102.08189 ,\n2021.\n[38] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuan-\njing Huang. Pre-trained models for natural language processing: A survey.\nScience China Technological Sciences , 63(10):1872–1897, 2020.\n[39] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles\nSutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311 , 2022.\n[40] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne\nHendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRahman. Wahiduret al.: Enhancing Zero-Shot Crypto Sentiment with Fine-tuned Language Model and Prompt Engineering\n[41] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojo-\ncaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam\nAlmazrouei, and Julien Launay. The refinedweb dataset for falcon llm:\noutperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\n[42] Hongyang Du, Zonghang Li, Dusit Niyato, Jiawen Kang, Zehui Xiong,\nDong In Kim, et al. Enabling ai-generated content (aigc) services in\nwireless edge networks. arXiv preprint arXiv:2301.03220 , 2023.\n[43] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing Long Han,\nand Yang Tang. A brief overview of chatgpt: The history, status quo and\npotential future development. IEEE/CAA Journal of Automatica Sinica ,\n10:1122–1136, 5 2023.\n[44] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam Mccandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners. Advances in Neural Information\nProcessing Systems, 33:1877–1901, 2020.\n[45] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\nShuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\net al. Opt: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022.\n[46] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, et al. Training language models to follow instructions with human\nfeedback. Advances in Neural Information Processing Systems , 35:27730–\n27744, 2022.\n[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng\nHou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A\nsurvey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\n[48] Chengwei Wei, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. An\noverview on language models: Recent developments and outlook. arXiv\npreprint arXiv:2303.05759, 2023.\n[49] Raschka Sebastian. Finetuning large language models. https:\n//magazine.sebastianraschka.com/p/finetuning-large-language-models, 4\n2023.\n[50] Shah Kushal. Pre-training, fine-tuning and in-context learning in large\nlanguage models (llms). https://bekushal .medium.com/pre-training-\nfine-tuning-and-in-context-learning-in-large-language-models-llms-\ndd483707b122, 8 2022.\n[51] Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin\nLeong, Dennis DeCoste, Sean Lie, and Shreyas Saxena. Spdf: Sparse pre-\ntraining and dense fine-tuning for large language models. arXiv preprint\narXiv:2303.10464, 2023.\n[52] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang\nSutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,\nArun Raja, et al. Multitask prompted training enables zero-shot task\ngeneralization. arXiv preprint arXiv:2110.08207 , 2021.\n[53] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,\nWilliam Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha\nBrahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\n[54] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts,\nStella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-\nXin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through\nmultitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.\n[55] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki,\nEllie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexan-\ndra Sasha Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-\naccess multilingual language model. arXiv preprint arXiv:2211.05100 ,\n2022.\n[56] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang,\nXu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.\narXiv preprint arXiv:2301.00234 , 2022.\n[57] Layton Dennis. Prompt engineering, leveraging in context learn-\ning. https://www .linkedin.com/pulse/prompt-engineering-leveraging-\ncontext-learning-dennis-layton/, 4 2023.\n[58] C. Montgomery and Patrick H. Winston. Learning and reasoning by\nanalogy. Communications of the ACM , 23:689–703, 12 1980.\n[59] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.\nMetaicl: Learning to learn in context. NAACL 2022 - 2022 Conference\nof the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Proceedings of the Conference ,\npages 2791–2809, 10 2021.\n[60] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui,\nand Furu Wei. Why can gpt learn in-context? language models implicitly\nperform gradient descent as meta-optimizers. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foundation Models , 2023.\n[61] Alejandro Lopez-Lira and Yuehua Tang. Can chatgpt forecast stock\nprice movements? return predictability and large language models. arXiv\npreprint arXiv:2304.07619, 2023.\n[62] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry\nGilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A\nprompt pattern catalog to enhance prompt engineering with chatgpt. arXiv\npreprint arXiv:2302.11382, 2023.\n[63] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi,\nand Graham Neubig. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. ACM Computing\nSurveys, 55(9):1–35, 2023.\n[64] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language\nmodels better few-shot learners. arXiv preprint arXiv:2012.15723 , 2020.\n[65] Teven Le Scao and Alexander M. Rush. How many data points is a prompt\nworth? NAACL-HLT 2021 - 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language\nTechnologies, Proceedings of the Conference , pages 2627–2636, 3 2021.\n[66] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small\nlanguage models are also few-shot learners. NAACL-HLT 2021 - 2021\nConference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Proceedings of the\nConference, pages 2339–2352, 9 2020.\n[67] Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna\nBeyer, Hanspeter Pfister, and Alexander M. Rush. Interactive and visual\nprompt engineering for ad-hoc task adaptation with large language models.\nIEEE Transactions on Visualization and Computer Graphics , 29:1146–\n1156, 1 2023.\n[68] Paula Maddigan and Teo Susnjak. Chat2vis: Generating data visualisations\nvia natural language using chatgpt, codex and gpt-3 large language models.\nIEEE Access, 2023.\n[69] OpenAI Platform. Gpt best practices - openai api. https://\nplatform.openai.com/docs/guides/gpt-best-practices, 2023.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3350638\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6914159655570984
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.615249514579773
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5350621342658997
    },
    {
      "name": "Natural language processing",
      "score": 0.42990145087242126
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4166775047779083
    },
    {
      "name": "Linguistics",
      "score": 0.13013657927513123
    },
    {
      "name": "Materials science",
      "score": 0.09543341398239136
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39534123",
      "name": "Gwangju Institute of Science and Technology",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I59805279",
      "name": "Islamic University of Technology",
      "country": "BD"
    }
  ]
}