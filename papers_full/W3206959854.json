{
  "title": "Language Models As or For Knowledge Bases",
  "url": "https://openalex.org/W3206959854",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3173397216",
      "name": "Razniewski, Simon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160855242",
      "name": "Yates, Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282188277",
      "name": "Kassner, Nora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746179788",
      "name": "Weikum, Gerhard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3180536457",
    "https://openalex.org/W2972182717",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W3153457449",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3079786700",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2759136286",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3016309009",
    "https://openalex.org/W3174169804",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2964221236",
    "https://openalex.org/W3094145761",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W3033176962",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W3169432481",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3154575616",
    "https://openalex.org/W3035214886"
  ],
  "abstract": "Pre-trained language models (LMs) have recently gained attention for their potential as an alternative to (or proxy for) explicit knowledge bases (KBs). In this position paper, we examine this hypothesis, identify strengths and limitations of both LMs and KBs, and discuss the complementary nature of the two paradigms. In particular, we offer qualitative arguments that latent LMs are not suitable as a substitute for explicit KBs, but could play a major role for augmenting and curating KBs.",
  "full_text": "Language Models As or For Knowledge Bases\nSimon Razniewskia, Andrew Yates a,b, Nora Kassner c and Gerhard Weikum a\naMax Planck Institute for Informatics\nbUniversity of Amsterdam\nbLMU Munich\nAbstract\nPre-trained language models (LMs) have recently gained attention for their potential as an alternative\nto (or proxy for) explicit knowledge bases (KBs). In this position paper, we examine this hypothesis,\nidentify strengths and limitations of both LMs and KBs, and discuss the complementary nature of the two\nparadigms. In particular, we offer qualitative arguments that latent LMs are not suitable as a substitute\nfor explicit KBs, but could play a major role for augmenting and curating KBs.\n1. Introduction\nThe ability of pre-trained contextual language models (LMs) to capture and retrieve factual\nknowledge has recently stirred discussion as to what extent LMs could be an alternative to, or\nat least a proxy for, explicit knowledge bases (KBs). LMs, such as BERT [1], GPT [2] or T5 [3]\nare huge transformer-based neural networks trained in a self-supervised manner on huge text\ncorpora, in order to predict sentence completions or masked-out text parts. In a setting called\n(masked) prompting or probing [4], these LMs complete a text sequence intended to elicit a\nrelational assertion for a given subject. For example, GPT-3 correctly completes the phrase“Alan\nTuring was born in” with “London”, which can be seen as yielding a subject-predicate-object\ntriple ⟨ Alan Turing, born in, London ⟩.\nStarting from the LAMA probe [ 5], many works have explored whether this LM-as-KB\nparadigm could provide an alternative to structured knowledge bases such as Wikidata. Ex-\nemplary analyses investigated the inclusion of entity information [6], how to turn LMs into\nstructured KBs [7], and how to incrementally add knowledge without side effects [8]. Other\nwork studied how accuracy relates to the neural network’s storage capacity [9] and whether\nQA performance scales with model size [10]. Another focus area is how LMs-as-KBs can be\nfurther augmented with a text retrieval component, to include informative passages (e.g., from\nWikipedia) [11, 12, 13].\nAlthough most works make their speculative nature clear (e.g., the title of [5] ends with a\nquestion mark), there is an implicit suggestion that LMs could replace structured KBs. On the\nother hand, NLP-centric works have identified various kinds of inconsistencies in LM outputs\n[14] or questioned their quantitative performance [15].\nThis paper discusses the potential of LMs as KBs and its “softer” variation of LMs for KBs.\nDeep Learning for Knowledge Graphs (DL4KG)\n/envelope-opensrazniew@mpi-inf.mpg.de (S. Razniewski); ayates@mpi-inf.mpg.de (A. Yates); kassner@cis.lmu.de (N. Kassner);\nweikum@mpi-inf.mpg.de (G. Weikum)\n© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\narXiv:2110.04888v1  [cs.CL]  10 Oct 2021\nLM-as-KB Structured KB\nConstruction Self/Unsupervised\n Manual or semi-automatic\nSchema Open-ended\n Typically fixed\nMaintenance- adding facts Difficult, unpredictable side effects\n Easy\n- correcting/deleting Difficult\n Easy\nKnows what it knowsNo, assigns probability to everything\nYes, content enumerable\nEntity disambiguationNo/limited\n Common\nProvenance No\n Common\nTable 1\nDifferences of LMs-as-KBs and structured KBs .\n2. Background\nLM-as-KB refers to efforts to use an LM as a source of world knowledge, as proposed by\n[5]. The knowledge representation is inherently latent, given by the entirety of the neural\nnetwork’s parameter values (in the billions). LMs in general have greatly advanced tasks like\ntext classification, machine translation, information retrieval, and question answering (see, e.g.,\nsurvey [16]).\nKBs, on the other hand, have been steadily advanced since the mid 2000s (with early works\nlike DBpedia, Freebase and Yago) [ 17]. They represent knowledge in the form of subject-\npredicate-object (SPO) triples along with qualifiers for non-binary statements. KBs have become\nkey assets in major industry applications [18, 19], including search engines. A major issue for\nongoing KB research is quality assurance as the KB is grown and maintained. This includes\nhuman-in-the-loop approaches throughout the KB life-cycle [20, 21, 22].\nAll LM-as-KB examples that follow are based on the GPT-3 daVinci model [ 2], one of the\nlargest pre-trained LMs as of October 2021.\n3. LM-as-KB\n3.1. Intrinsic Considerations\nThe following are principal differences between LMs-as-KBs and structured KBs.\nPredictions vs. lookups: While content of structured KBs can be explicitly looked up, LMs\nhave a latent representation and output probabilities at probing time. This has the advantage of\nnot requiring any schema design upfront. However, it implies that it is not possible to enumerate\nthe knowledge stored in an LM, nor can we look up whether a certain fact is contained or not.\nFor predictions with very high confidence scores, this is still viable. However, even top-ranked\npredictions often have low scores and near-ties. Properly calibrating scores and thresholding is\na black art.\nExample: GPT-3 does not have tangible knowledge that Alan Turing was born in London; it\nmerely assigns this a high confidence of 83%. Yann LeCun, on the other hand, is given medium\nconfidence in being a citizen of France and Canada (67% and 26%), but he actually has French and\nUSA citizenship, not Canadian. The LM assigns USA a very low score. The Wikidata KB, on the\nother hand, only states his French citizenship, not USA. Wikidata is incomplete, but it does not\ncontain any errors.\nStatistical correlations vs. explicit knowledge: Errors made by LMs-as-KBs are not random,\nbut exhibit systematic biases [ 23, 15] due to frequent values and co-occurrences (including\nindirect co-occurrences captured latently).\nExample: When prompting GPT-3 for awards won by Alan Turing, its top-confidence prediction\nis the Turing Award, and lower-ranked outputs include “Nobel Prize” and “the war” (none of them\ncorrect).\nAwareness of limits: In KBs, absence of facts is explicit and easy to assert. Wikidata even\nsupports a way of stating non-existence (no-value statements) to impose a local-closed-world\nview while following a general open-world assumption [ 24]. LM’s latent representations\ninherently lack awareness of cases where no object exist, and so they easily produce non-zero\nor even high scores for incorrect assertions.\nExample: Alan Turing was homosexual and never married. When prompting GPT-3 with the\nphrase “Alan Turing married”, the top prediction is “Sara Lavington” with score 21%, and for the\nprompt “Alan Turing and his wife” it is “Sara Turing” (his mother’s name). This is a case of LM\nhallucination [25, 26]. In contrast, Wikidata has an explicit statement ⟨ Alan Turing, spouse,\nno value ⟩ denoting that he was unmarried.\nCoverage: The scope of KBs is usually limited by the fixed set of predicates specified in the KB\nschema. These can be hundreds (or even a few thousands) of interesting relations, but will hardly\never be complete. In particular, “non-standard relations”, such as worked with colleague ,\nsong is about person (or event), movie based on person’s biography, are missing in all\nof the major KBs. LMs, on the other hand, latently tap into the full text of Wikipedia, books,\nnews, and more, and are thus able to capture some of these predicates.\nExample: Creatively prompting GPT-3 can yield impressive nuggets of knowledge: the input\nphrases “Turing’s colleague” and “Turing worked with” result in outputs like John Womersley, Hugh\nAlexander, Gordon Welchman (all correct). Likewise, the prompt “The Imitation Game film is about\nthe life of” is completed with the high-confidence output Alan Turing. These anecdotes indicate the\ngreat power of LMs to go beyond the current scope and coverage of explicit KBs.\nCuratability: In structured KBs, a knowledge curator can correct, add or remove assertions. For\nLMs, this is an open challenge, as these operations require major (non-monotonic) re-training,\nor the addition of explicit exceptions, which means reverting to a KB [27, 28].\nExample: For the prompt “Alan Turing died in the town of”, GPT-3 returns the top prediction\n“Warrington”, which is wrong (he died in Wilmslow). The LM does not provide any hint on how to\nfix this (e.g., by changing the training corpus or parameters), and a knowledge curator has no way\nto tackle such errors.\nProvenance: LMs have no ability to trace their outputs back to specific source documents\n(and passages) in the training data. KBs, on the other hand, consider reference sources as an\nindispensable pillar of scrutable veracity. Provenance is crucial for giving explanations to users,\nincluding knowledge engineers who maintain the KB and end-users in downstream applications.\nAlso, without provenance, LMs have no way of pinpointing an incorrect prediction’s root cause\nand correcting the underlying corpus (e.g., removing misleading documents).\nExample: Reconsider the previous example of predicting “Warrington” as Turing’s death place.\nThe LM itself does not give any cue where this comes from. A diligent and smart Google user could\ndetect a possible origin, namely, news and other reports about a memorial plaque at 2 Warrington\nCrescent in Maida Vale, London, which is near Turing’s birth place. However, the knowledge\nengineer cannot be certain that this is indeed the culprit.\nCorrectly predicted facts need explanations, too. For example, the assertion that Turing was engaged\nwith Joan Clarke may appear puzzling given his homosexuality. Pointing to explicit provenance is\ncrucial evidence.\n3.2. Pragmatic Considerations\nEntity disambiguation: Although LMs are lauded for their ability to disambiguate words\nbased on context, this happens latently, and there is no easy way to explicitly build this into\nprobing procedures [9, 6]. Consequently, LMs mix up facts from distinct entities that share\nsurface forms. Although structured KBs cannot perform disambiguation on their own either,\nthey can correctly separate assertions.\nExample: GPT-3 completes “Turing was a famous” with “mathematician”, “computer”, “code”\netc., stemming from very different entities (including the Turing Machine).\nNumbers and singletons: LMs are good at latently capturing knowledge about predicates\nwith few possible object values, such as nationality or language-spoken. However, when the\nobject values are rarely occurring values or even singletons (i.e., occurring only with a single\nsubject), the latent representation is bound to produce errors, and explicit KB storage is superior.\nThe same applies to many cases of numeric values, where the value distribution exhibits high\nentropy.\nExample: For the input “The Turing Institute’s address in London is”, GPT-3 returns “Dilly’s Den”\nor “the street called Dilly’s Den” (possibly derived from the famous Piccadilly Circus; the correct\nvalue is British Library, 96 Euston Road, London NW1 2DB). Rephrasing the prompt does not lead\nto success either.\nSubjects with zero or many objects: An important case where the brittleness of LM predic-\ntions becomes a significant problem is when a subject entity has no object value for a given\npredicate or has many distinct true values. The zero-value case often leads to the pitfall that the\nLM must predict some value. In the many-values case, we could go deep in the ranking of the\nLM output, but this would usually result in a wild mix of valid and spurious objects, and there\nis no guideline for how deep we should go into the ranking.\nExample: To obtain a list of Turing Award winners, we could prompt GPT-3 with the phrase “the\nTuring Award was won by” and receive various predictions like “Stuart Shieber”, “John Hopcroft”\nand “Andrew Yao” (1 false, 2 correct). There are currently 73 winners, all captured in Wikidata. By\nprobing LMs, we would have to go very deep in the prediction ranking to see all of them, but only\nin a confusing mix of true and false positives.\nAs for zero-objects, the prompt for “the first woman on the moon was” returns Sally Ride, Eileen\nCollins and others. These are astronauts, but unfortunately, none of them ever landed on the moon.\nThe ground-truth for this example is empty.\nWe summarize the main differences in Table 2.\n4. LM-for-KB\nOur view of how to harness the great potential of LMs is to leverage them for KB curation:\nmaintaining high quality as the KB grows throughout its life-cycle. This is a major pain point in\nKB practice [20, 21, 22]. For example, when adding new entities, one needs to ensure that they\nare not duplicates (with slightly different alias names) of existing entities. Likewise, keeping\nthe type system (aka ontology) clean while gradually extending it and ensuring the correctness\nof new facts are never-ending challenges.\nThe envisioned role of LMs is to scrutinize SPO assertions considered for augmenting the KB.\nFor example, a new fact such as ⟨ Leonardo da Vinci, has won, Turing Award ⟩ could\nbe “double-checked” by prompting the LM as to whether it yields high-confidence predictions\nfor this candidate assertion. This is akin to the way knowledge graph embeddings [29] have\nbeen considered for KB completion. However, the key difference is that KG embeddings draw\nfrom the KG itself, and thus do not provide complementary evidence. LMs, on the other hand,\nbring in a new and largely independent perspective, by tapping into text corpora (including\nWikipedia, but also news, books etc.). If the LM does not yield sufficiently confident support for\nthe candidate fact, it should be refuted.\nThe converse direction, using LMs to predict assertions and thus generate candidates for new\nfacts, is conceivable too. However, this needs major research to advance prediction accuracy.\n5. Conclusion\nIn this paper we discussed the strengths and limitations of LMs as KBs in comparison to\nstructured KBs. We believe that LMs cannot broadly replace KBs as explicit repositories of\nstructured knowledge. While the probabilistic nature of LM-based predictions is suitable\nfor task-specific end-to-end learning, the inherent uncertainty of outputs does not meet the\nquality standards of KBs. LMs cannot separate facts from correlations, and this entails major\nimpediments for KB maintenance. We advocate, on the other hand, that LMs can be valuable\nassets for KB curation, by providing a “second opinion” on new fact candidates or, in the absence\nof corroborated evidence, signal that the candidate should be refuted. Other ways of combining\nthe strengths of latent knowledge (LMs) and structured knowledge (KBs) could be promising as\nwell, such as “KB-for-LM” approaches that allow a LM to look up facts from an external memory\n(e.g., [12, 30, 31, 32]) and thus have the potential to combine the strengths of both approaches.\nReferences\n[1] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional\ntransformers for language understanding, in: NAACL, 2019.\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are\nunsupervised multitask learners, 2019. OpenAI technical report.\n[3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu,\nExploring the limits of transfer learning with a unified text-to-text transformer, arXiv\n(2019).\n[4] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, Pre-train, prompt, and predict: A\nsystematic survey of prompting methods in natural language processing, arXiv (2021).\n[5] F. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y. Wu, A. Miller, Language\nmodels as knowledge bases?, in: EMNLP, 2019.\n[6] N. Poerner, U. Waltinger, H. Schütze, E-BERT: Efficient-yet-effective entity embeddings\nfor BERT, in: Findings of EMNLP, 2020.\n[7] C. Wang, X. Liu, D. Song, Language models are open knowledge graphs, arXiv (2021).\n[8] R. Wang, et al., K-adapter: Infusing knowledge into pre-trained models with adapters,\narXiv (2021).\n[9] B. Heinzerling, K. Inui, Language models as knowledge bases: On entity representations,\nstorage capacity, and paraphrased queries, in: EACL, 2021.\n[10] A. Roberts, C. Raffel, N. Shazeer, How much knowledge can you pack into the parameters\nof a language model?, in: EMNLP, 2020.\n[11] F. Petroni, P. Lewis, A. Piktus, T. Rocktäschel, Y. Wu, A. H. Miller, S. Riedel, How context\naffects language models’ factual predictions, in: AKBC, 2020.\n[12] K. Guu, K. Lee, Z. Tung, P. Pasupat, M.-W. Chang, Realm: Retrieval-augmented language\nmodel pre-training, in: ICML, 2020.\n[13] P. Lewis, et al., Retrieval-augmented generation for knowledge-intensive NLP tasks, in:\nNeurIPS, 2021.\n[14] Y. Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, H. Schütze, Y. Goldberg,\nMeasuring and improving consistency in pretrained language models, arXiv (2021).\n[15] B. Cao, H. Lin, X. Han, L. Sun, L. Yan, M. Liao, T. Xue, J. Xu, Knowledgeable or educated\nguess? revisiting language models as knowledge bases, in: ACL, 2021.\n[16] T. Young, D. Hazarika, S. Poria, E. Cambria, Recent trends in deep learning based natural\nlanguage processing, Computational intelligence magazine (2018).\n[17] S. Razniewski, P. Das, Structured knowledge: Have we made progress? an extrinsic study\nof KB coverage over 19 years, in: CIKM, 2020.\n[18] N. Noy, et al., Industry-scale knowledge graphs: lessons and challenges, CACM (2019).\n[19] G. Weikum, L. Dong, S. Razniewski, F. Suchanek, Machine knowledge: Creation and\ncuration of comprehensive knowledge bases, in: Foundations and Trends in Databases,\n2021.\n[20] J. Taylor, Automated knowledge base construction, AKBC invited talk, 2020. https://\nyoutu.be/JsB4T35We0w?t=12032.\n[21] A. Piscopo, E. Simperl, What we talk about when we talk about Wikidata quality: a\nliterature survey, in: Symposium on Open Collaboration, 2019.\n[22] K. Shenoy, F. Ilievski, D. Garijo, D. Schwabe, P. Szekely, A study of the quality of Wikidata,\narXiv (2021).\n[23] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dangers of stochastic\nparrots: Can language models be too big?, in: FAccT, 2021.\n[24] H. Arnaout, S. Razniewski, G. Weikum, J. Z. Pan, Negative knowledge for open-world\nWikidata, in: Companion Proceedings of the Web Conference, 2021.\n[25] A. Rohrbach, et al., Object hallucination in image captioning, in: EMNLP, 2018.\n[26] C. Wang, R. Sennrich, On exposure bias, hallucination and domain shift in neural machine\ntranslation, in: ACL, 2020.\n[27] C. Zhu, A. S. Rawat, M. Zaheer, S. Bhojanapalli, D. Li, F. Yu, S. Kumar, Modifying memories\nin transformer models, in: arXiv, 2020.\n[28] N. D. Cao, W. Aziz, I. Titov, Editing factual knowledge in language models, in: EMNLP,\n2021.\n[29] Q. Wang, Z. Mao, B. Wang, L. Guo, Knowledge graph embedding: A survey of approaches\nand applications, TKDE (2017).\n[30] T. Févry, L. B. Soares, N. FitzGerald, E. Choi, T. Kwiatkowski, Entities as experts: Sparse\nmemory access with entity supervision, in: EMNLP, 2020.\n[31] H. Sun, L. B. Soares, P. Verga, W. W. Cohen, Adaptable and interpretable neural memory\nover symbolic knowledge, in: NAACL, 2021.\n[32] N. Kassner, O. Tafjord, H. Schutze, P. Clark, Enriching a model’s notion of belief using a\npersistent memory, in: arXiv, 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4785374104976654
    },
    {
      "name": "Cognitive science",
      "score": 0.383979856967926
    },
    {
      "name": "Knowledge management",
      "score": 0.3488726019859314
    },
    {
      "name": "Natural language processing",
      "score": 0.3473334312438965
    },
    {
      "name": "Linguistics",
      "score": 0.3393363356590271
    },
    {
      "name": "Data science",
      "score": 0.33269035816192627
    },
    {
      "name": "Psychology",
      "score": 0.28360408544540405
    },
    {
      "name": "Philosophy",
      "score": 0.11283707618713379
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210109712",
      "name": "Max Planck Institute for Informatics",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I149899117",
      "name": "Max Planck Society",
      "country": "DE"
    }
  ]
}