{
  "title": "A survey on LoRA of large language models",
  "url": "https://openalex.org/W4403753254",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3034547314",
      "name": "Yuren Mao",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    },
    {
      "id": "https://openalex.org/A2491453817",
      "name": "Yuhang Ge",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    },
    {
      "id": "https://openalex.org/A3128580400",
      "name": "Yijiang Fan",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2146325448",
      "name": "Wenyi Xu",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    },
    {
      "id": "https://openalex.org/A2112837044",
      "name": "Yu Mi",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2149202877",
      "name": "Zhonghao Hu",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2139831545",
      "name": "Yun-jun Gao",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    },
    {
      "id": "https://openalex.org/A3034547314",
      "name": "Yuren Mao",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    },
    {
      "id": "https://openalex.org/A2491453817",
      "name": "Yuhang Ge",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3128580400",
      "name": "Yijiang Fan",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2146325448",
      "name": "Wenyi Xu",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    },
    {
      "id": "https://openalex.org/A2112837044",
      "name": "Yu Mi",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2149202877",
      "name": "Zhonghao Hu",
      "affiliations": [
        "Ningbo University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2139831545",
      "name": "Yun-jun Gao",
      "affiliations": [
        "Zhejiang University",
        "Ningbo University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2602856279",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2759609474",
    "https://openalex.org/W4402716159",
    "https://openalex.org/W4387702340",
    "https://openalex.org/W4389524340",
    "https://openalex.org/W4386566659",
    "https://openalex.org/W4403277434",
    "https://openalex.org/W4402684143",
    "https://openalex.org/W4385572804",
    "https://openalex.org/W4366516900",
    "https://openalex.org/W4402667063",
    "https://openalex.org/W4398777692",
    "https://openalex.org/W4400525231",
    "https://openalex.org/W4402704566",
    "https://openalex.org/W4404782602",
    "https://openalex.org/W2911293880",
    "https://openalex.org/W4392343910",
    "https://openalex.org/W4389919338",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4399391277",
    "https://openalex.org/W4391032878",
    "https://openalex.org/W4392113247",
    "https://openalex.org/W3143835353",
    "https://openalex.org/W4389523890",
    "https://openalex.org/W89279510",
    "https://openalex.org/W2604738573",
    "https://openalex.org/W2911495555",
    "https://openalex.org/W2136885855",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W2530938137",
    "https://openalex.org/W4387105517"
  ],
  "abstract": "Abstract Low-Rank Adaptation (LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA’s performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field.",
  "full_text": "A survey on LoRA of large language models\nYur\nen MAO1,2, Yuhang GE1, Yijiang FAN1, Wenyi XU1, Yu MI1, Zhonghao HU1,\nYunjun GAO (✉)1,2\n1   School of Software Technology, Zhejiang University, Ningbo 315000, China\n2   College of Computer Science and Technology, Zhejiang University, Hangzhou 310007, China\n The Author(s) 2024. This article is published with open access at link.springer.com and journal.hep.com.cn\n \nAbstr\nact    Low-Rank  Adaptation  (LoRA),  which  updates  the\ndense neural network layers with pluggable low-rank matrices,\nis  one  of  the  best  performed  parameter  efficient  fine-tuning\nparadigms. Furthermore, it has significant advantages in cross-\ntask  generalization  and  privacy-preserving.  Hence,  LoRA  has\ngained  much  attention  recently,  and  the  number  of  related\nliterature  demonstrates  exponential  growth.  It  is  necessary  to\nconduct  a  comprehensive  overview  of  the  current  progress  on\nLoRA. This survey categorizes and reviews the progress from\nthe  perspectives  of  (1)  downstream  adaptation  improving\nvariants  that  improve  LoRA’s  performance  on  downstream\ntasks;  (2)  cross-task  generalization  methods  that  mix  multiple\nLoRA  plugins  to  achieve  cross-task  generalization;\n(3)  efficiency-improving  methods  that  boost  the  computation-\nefficiency  of  LoRA;  (4)  data  privacy-preserving  methods  that\nuse  LoRA  in  federated  learning;  (5)  application.  Besides,  this\nsurvey also discusses the future directions in this field.\nKeywords    low-rank  adaptation,  LoRA,  large  language\nmodels, LLMs\n \n1    Introduction\nR\napidly  increasing  parameter  scales  of  pre-training  language\nmodels  improves  their  generalization  ability  and  brings\nemergent abilities. In the last few years, the parameter scales\nof pre-training languages models have increased by thousands\nof  times  (e.g.,  from  330  M  parameter  BERT  [1]  to  540  B\npa\nrameter  PaLM  [2]).  These  pre-training  language  models\nhaving  large  parameter  scales  are  termed  Large  language\nmodels  (LLMs).  Nevertheless,  due  to  the  knowledge\nboundaries  of  the  LLMs,  their  abilities  on  some  downstream\ntasks are still limited. To expand the knowledge boundaries, it\nremains  necessary  to  fine-tune  LLMs  on  the  downstream\ntasks.\nHowever,  fine-tuning  the  full  parameters  of  an  LLM,\nnamely  full  fine-tuning,  is  extremely  computationally\nexpensive, for example, full fine-tuning of a LLaMA2-7B [ 3\n]\nmodel  requires  approximately  60  GB  of  memory,  which\nexceeds  the  capacity  of  common  consumer  GPUs  [4\n].  To\nreduce  the  computational  cost,  various  parameter-efficient\nfine-tuning  (PEFT)  methods  have  been  proposed  [5].  They\nadapt LLMs to downstream tasks by only fine-tuning a small\nnumber of (extra) model parameters. From the perspective of\nwhether extra parameters are involved, PEFT methods can be\ndivided  into  two  categories:  extra-parameter  methods  and\nintra-parameter methods. The extra-parameter methods freeze\nall  of  the  original  parameters  of  an  LLM  and  insert  a  set  of\nlearnable  parameters  to  optimize  the  model  input  or  model\nlayers  such  as  adapter  tuning  [6\n]  and  prompt  tuning  [7].  By\nc\nontrast,  intra-parameter  methods  freeze  most  of  the  original\nparameters  of  an  LLM  and  only  tune  a  small  number  of\nparameters  of  the  LLM  such  as  BitFit  [8],  LISA  [4]  and\nL\noRA [9].\nW\nhen  we  do  not  have  access  to  modify  the  model\narchitecture,  intra-parameter  methods  are  desirable.  Among\nthe  intra-parameter  methods,  LoRA  is  the  most  widely  used\none,  because  it  can  achieve  a  comparable  or  better\ndownstream adaptation performance to the full fine-tuning on\na  range  of  downstream  tasks  [9\n]  and  is  easy  to  implement.\nBesides, there are many variants have been proposed to further\nimprove the downstream adaptation ability of LoRA on more\nchallenging downstream tasks.\nLoRA achieves parameter efficiency by updating the dense\nneural  network  layers  of  an  LLM  with  pluggable  low-rank\nmatrices.  These  matrices  (a.k.a,  LoRA  plugins)  are\nindependent  of  the  LLM,  which  can  be  stored  and  reused  in\nother  related  downstream  tasks.  Furthermore,  these  LoRA\nplugins can be combined to achieve cross-task generalization,\nwhich  can  facilitate  multi-task  learning,  domain  adaptation,\nand continual learning.\nAs the LoRA modules accumulate, the computation cost of\nmanaging  LoRA  modules  is  increasing.  Although  LoRA  is\ncomputation-efficient,  the  computational  cost  of  managing  a\nlarger  number  of  LoRA  modules  is  unignorable.  It  is\nnecessary  to  further  improve  the  computation  efficiency  of\nLoRA.  The  improvement  can  come  from  reducing  the\ncomputation  cost  of  single  LoRA  modules  and  accelerating\nthe  scalable  serving  of  multiple  modules.  It  can  boost  the\napplication  of  LoRA  in  real-world  use  cases,  such  as\nGenerative-as-a-Service (GaaS) cloud products.\nIn  some  cases,  the  training  data  are  privately  owned  by\n \nReceived July 2, 2024; accepted August 12, 2024\nE-mail: gaoyj@zju.edu.cn\nFront. Comput. Sci., 2025, 19(7): 197605\nhttps://doi.org/10.1007/s11704-024-40663-9\nREVIEW ARTICLE\nmultiple  clients  and  cannot  be  centralized.  To  adapt  LLMs\nwi\nth  the  distributed  training  data,  we  can  adopt  federated\nlearning  to  protect  the  data  privacy  of  each  client.  However,\nfederated  learning  suffers  expensive  communication  and\ncomputation costs. To reduce costs, LoRA is a natural choice.\nIts parameter-efficient nature helps to reduce the computation\ncost  of  each  client  and  the  communication  cost  of  sharing\nparameters  across  clients.  Furthermore,  the  pluggable  feature\nof  LoRA,  which  supports  the  localization  or  encryption  of\npersonalized  parameters,  enhances  privacy  protection  within\nfederated learning. Therefore, LoRA has a great potential for\nprivacy-preserving.\nWhile  some  previous  surveys  have  mentioned  LoRA\n[5\n,10,11],  they  mainly  focus  on  PEFT  and  only  introduce  a\nsm\nall  number  of  LoRA-related  works,  lacking  systematic\ntreatment  and  comprehensive  overview  on  LoRA  and  its\nvariants. In this survey, we give a comprehensive overview of\nthe  current  progress  on  LoRA  for  methods  (1)  improving\ndownstream  adaption  performance  of  LoRA;  (2)  mixing\nLoRA  modules  to  achieve  cross-task  generalization;\n(3)  boosting  the  computation-efficiency  of  LoRA;\n(4)  adopting  LoRA  in  federated  learning.  Besides,  the\napplication of LoRA is briefly introduced. This taxonomy of\nLoRA-related  methods  is  illustrated  in Fig. 1.  This  survey  is\ne\nxpected  to  give  comprehensive  background  knowledge,\nresearch trends and technical insights for LoRA.\n \n \nF\nig. 1    The taxonomy of this paper\n2 Front. Comput. Sci., 2025, 19(7): 197605\nThe  rest  of  this  survey  is  organized  as  follows.  Section  2\ni\nntroduces the background knowledge of LoRA, and Section 3\nintroduces  the  LoRA’s  variants  that  aim  to  improve  the\ndownstream adaptation performance. In Section 4, we review\nthe  LoRA  mixture  methods  that  mix  LoRA  modules  to\nachieve  cross-task  generalization.  Section  5  discusses  the\nmethods  that  are  propsoed  to  improve  the  computational\nefficiency  of  LoRA.  The  LoRA-driven  federated  learning\nmethods  are  introduced  in  Section  6.  Section  7  reports  the\napplications  of  LoRA.  We  conclude  this  survey  and  discuss\nthe future directions in Section 8. \n2    Low-rank adaptation (LoRA)\nT\nhe  Low-dimensional  intrinsic  dimensionality  hypothesis\n[189] presents that over-parameterized models reside on a low\ni\nntrinsic  dimension,  which  demonstrates  that  we  can  achieve\nproper  learning  performance  by  only  updating  parameters\nrelated to the intrinsic rank. Based on this hypothesis, LoRA\n[9] proposes to update dense layers in a model with low-rank\nm\natrices.  It  can  achieve  both  parameter-  and  computational-\nefficiency.  In  this  section,  we  first  introduce  the  details  of\nLoRA  and  then  introduce  existing  works  that  focus  on  the\ntheoretical  analysis  of  LoRA.  Furthermore,  we  demonstrate\nLoRA’s  efficiency  in  practice.  At  last,  this  section  presents\nthat LoRA can be used in other use cases except fine-tuning. \n2.1    LoRA\nW0 ∈Rd×k\n∆W∈Rd×k\nW =W0 + ∆ W ∆W\nd ×k\n∆W\nB ∈Rd×r A ∈Rr×k\nGiven  a  dense  neural  network  layer  parameterized  by\n,\n to adapt it to a downstream task, we update it with\n  and  obtain  an  updated  layer  parameterized  by\n. For full fine-tuning,   is computed based on\ngradients  of  all  the    parameters  for  the  layer,  which  is\ncomputationally  expensive  and  requires  a  large  amount  of\nGPU  memory  for  LLMs.  To  improve  the  computational\nefficiency,  LoRA  decomposes    into  two  small  matrices\n and  , i.e.,\n \nW =W0 +αB A, (1)\nr ≪min{d,k} B A\nα\nr ×(d +k)\nd ×k\nwhere  ,    and    are  initialized  with  a  random\nGa\nussian  distribution  and  zero  respectively,    represents  the\nscaling  factor  that  controls  the  strength  of  updates.  The\nparameter  number  of  LoRA  is  ,  which  is\nsignificantly  less  than  .  Figure 2(a)  and  (b)  compare  the\nst\nructures of full fine-tuning and LoRA.\nLoRA  is  highly parameter  efficient  for  it  updates  only  a\nsmall subset of model parameters, which reduces the memory\nand  computational  requirements  for  fine-tuning  without\nincreasing  inference  latency  [ 190].  Furthermore,  The\npa\nrameter  efficiency  can  be  further  improved  by  extending\nfrom  the  low-rank  matrix  to  low-rank  tensor  [191]  or\nc\nombining  with  the  Kronecker  decomposition  [192,193].\nE\nxcept for parameter efficiency, LoRA is also  pluggable for\nthe  LoRA  parameters  that  can  be  separated  from  the  model\nafter training. The pluggable character of LoRA enables it to\nbe shared and reused by multiple users [194]. When we have\nL\noRA  modules  for  multiple  tasks,  we  can  combine  these\nmodules  and  expect  a  proper  cross-task  generalization\nperformance [60]. Besides, the low-rank mechanism of LoRA\ni\ns compatible with other parameter-efficient methods, such as\nadapter  [195,196].  Besides,  LoRA  can  achieve  pr oper\ndownstream  adaptation  performance   on  various\ndownstream tasks. For example, on MMLU [197] benchmark,\nc\nomparing  with  full  fine-tuning,  fine-tuning  with  LoRA  can\nachieve  comparable  or  even  better  performance  across  57\ntasks [4].\nIn\n practice, for a Transformer-based LLM, the dense layers\ntypically  consist  of  two  types  of  weight  matrices:  the\nprojection  matrices  in  attention  modules  and  feed-forward\nneural (FFN) modules. The experiments mentioned above are\nconducted based on the original LoRA settings, applying it to\nthe query and value weight matrices in the attention modules.\nIt  is  worth  mentioning  that  subsequent  work  shows  that\napplying  it  to  the  FFN  layers  can  further  improve  model\nperformance [198]. \n2.2    Theoretical analysis\nf\n¯f ⩾ f × de pth o f ¯f\nde pth o f f\nTo understand why LoRA is effective and how LoRA can be\nm\nore  effective,  several  works  have  provided  theoretical\nanalyses  from  various  aspects.  To  answer  the  question  that\nwhy  LoRA  is  effective,  Malladi  et  al.  [12]  analyze  the  fine-\ntuning  dynamics  of  LoRA  from  the  kernel  view  and\ndemonstrate  that  in  the  lazy  regime,  LoRA  fine-tuning  is\nnearly equivalent to full fine-tuning. Besides, Zeng et al. [ 16]\nprovi\ndes  a  theoretical  analysis  of  the  LoRA’s  expressive\npower  for  both  fully  connected  neural  networks  (FNNs)  and\nTransformer  networks  (TFNs).  They  proved  that  LoRA  can\nadapt any model   to accurately represent any smaller target\nmodel    if  LoRA-rank    (width  of  )      under  a\nmild assumption, where the depth and width are the number of\n \n \nF\nig. 2    An illustration of full fine-tuning (a), LoRA (b) and its variants for improving downstream adaptation, which includes breaking the low-\nrank bottleneck (c) and dynamic rank allocation (d)\nYuren MAO et al.    A survey on LoRA of large language models 3\n(embedding size\n2\n)\nlayers  and  the  number  of  neurons  of  the  layer  having  the\nl\nargest  number  of  neurons,  respectively.  Moreover,  they\nquantify  the  approximation  error  when  the  LoRA-rank  falls\nbelow this threshold. Regarding TFNs, they showed that any\nmodel  can  be  adapted  to  a  target  model  of  equivalent  size\nusing  a  rank-   for  LoRA.  Additionally,  Koubbi\net  al.  [ 13]  utilize  the  mathematical  framework  for\nT\nransformers established by [ 199–201] to investigate the how\nl\now-rank perturbations in attention parameters affect.\nN\nr ≳\n√\nN\nA\nB\nA B\n2×\nAs  to  the  question  that  how  LoRA  can  be  more  effective,\nJa\nng  et  al.  [14]  analyze  the  fine-tuning  of  LoRA  within  the\nneural  tangent  kernel  (NTK)  [202]  framework  when    data\npoi\nnts are available. They demonstrate that employing a rank\n  in  LoRA  helps  to  avoid  spurious  local  minima  and\nfacilitates  the  discovery  of  low-rank  solutions  that  exhibit\ngood generalization. Besides, Zhu et al. [ 15] observe that the\nproj\nect-down matrix   is utilized for extracting features from\nthe  input,  while  the  project-up  matrix    employs  these\nfeatures  to  create  the  desired  output.  Based  on  this\nobservation,  they  demonstrate  that  freezing  the  project-down\nmatrix    while  tuning  only  the  project-up  matrix    leads  to\nbetter  generalization  compared  to  tuning  both  matrices,  in\naddition to achieving a   reduction in parameters. \n2.3    Efficiency in practice\n11,008 ×4,096 =\n45,088,768\n(11,008 ×4) +(4 ×4,096) =60,416 r =4\nThe computational efficiency of LoRA is significantly higher\nt\nhan  that  for  full  fine-tuning.  Taking  fine-tuning  the  dense\nweight  matrix  of  the  first  FFN  layer  in  LLaMA2-7B  as  an\nexample, full fine-tuning needs to fine-tune \n  parameters  while  LoRA  only  needs  to  tune\n  parameters  when  .\nFor this layer, LoRA only adjusts nearly one-thousandth of the\nparameters compared to full fine-tuning.\nLoRA can significantly decrease the memory usage of fine-\ntuning  an  LLM,  which  can  be  divided  into  four  parts:\n(1)  Model  Memory:  the  memory  required  to  store  the  model\nweights;  (2)  Activation  Memory:  the  memory  occupied  by\nintermediate activations during forward propagation. It mainly\ndepends  on  factors  such  as  batch  size  and  sequence  length;\n(3) Gradient Memory: the memory required to store gradients\nduring backpropagation. The gradients are only calculated for\ntrainable  parameters;  (4)  Optimization  Memory:  the  memory\nused  to  store  optimizer  states.  For  example,  the  Adam\noptimizer stores the “first moment” and “second moment” of\ntrainable parameters.\nPan  et  al.  [ 4]  provides  a  comprehensive  empirical\nc\nomparison between full fine-tuning and LoRA fine-tuning on\nan  LLaMA2-7B  model  with  batch  size  1,  utilizing  a  single\nNVIDIA  RTX4090  (24  GB)  GPU.  According  to  this  study,\nfull  fine-tuning  requires  approximately  60  GB  of  memory,\nwhich exceeds the capacity of an RTX4090 GPU; by contrast,\nLoRA fine-tuning only needs about 23 GB of memory. LoRA\nsignificantly  reduces  memory  usage  and  makes  fine-tuning\nLLaMA2-7B feasible on a single NVIDIA RTX4090 (24 GB)\nGPU.  Specifically,  due  to  fewer  trainable  parameters,  both\noptimization  memory  and  gradient  memory  decrease\nsignificantly by approximately 25 GB and 14 GB respectively.\nOn  the  other  hand,  while  LoRA  introduces  additional\n1.9×\n“incremental  parameters”  resulting  in  slight  increases  in\na\nctivation memory and weight memory (totaling about 2 GB),\nthis  increase  is  negligible  when  considering  the  overall\nreduction in memory. Moreover, reducing memory brings an\nacceleration  of  forward  propagation.  LoRA  is    times\nfaster compared to full fine-tuning. \n2.4    Beyond fine-tuning\nB\nesides  fine-tuning,  LoRA  can  be  applied  to  other  learning\nparadigms, such as pre-training [ 17,19] and continual training\n[20].  For  pre-training, Re LoRA  [17]  and M oRA  [18]  are\npropose\nd to use low-rank updates to train high-rank networks;\nmoreover, LTE  [19]  is  proposed  to  perform  parallel  training\nof\n multiple  low-rank  heads  across  computing  nodes  to\nminimize  the  need  for  frequent  synchronization,  which\nfacilitates  the  utilization  of  LoRA  in  pre-training.  As  for\ncontinual  training,  there  are  several  methods  have  been\nproposed  to  address  the  catastrophic  forgetting  problem.\nInfLoRA  [ 20\n]  addresses  catastrophic  forgetting  by\nreparameterizing  pre-trained  weights  with  a  minimal  set  of\nparameters  in  a  subspace. GS-LoRA  [21]  uses  group  sparse\nre\ngularization  to  automatically  select  specific  LoRA  groups\nwhile  zeroing  out  others  to  mitigate  catastrophic  forgetting\neffects.  I-LoRA  [22\n]  leverages  dual-memory  experience\nreplay  combined  with  LoRA  parameter  interpolation  to\ncombat catastrophic forgetting.\nFurthermore,  LoRA  can  be  used  to  overcome  the  limited\ncontext  size  for  LLMs  [3,23].  For  instance, L ongLoRA  [3]\nsuccessfully  computaitional  efficiently  extends  the  context\nwindow  of  LLaMA2-7B  [203]  from  4k  to  100k  tokens  by\nc\nombining  LoRA  with  shifted  sparse  attention.  However,\nLongLoRA does not match the efficiency of vanilla attention\ndue  to  chaotic  attention  head  structures  and  unnecessary\ninformation exchange between token groups. To address these\nissues, SinkLoRA [23] introduces Sink Fixed Attention (SF-\nAt\ntn)  to  proportionally  returns  cyclically  shifted  groups  of\nattention  heads  to  their  un-shifted  state  and  achieves  proper\nperformance. \n3    Downstream adaptation improving\nAl\nthough  LoRA  can  achieve  proper  adaptation  performance\non  some  downstream  tasks,  there  is  still  a  performance  gap\nbetween  LoRA  and  full  fine-tuning  on  many  downstream\ntasks, such as mathematical reasoning [ 204–206]. To fill this\nga\np,  many  methods  are  proposed  to  further  improve  the\ndownstream  adaptation  performance  of  LoRA.  Typically,\nexisting  methods  improve  the  downstream  adaptation\nperformance from the following perspectives: (1) breaking the\nlow-rank  bottleneck,  refer  to  Fig. 2(c);  (2)  adaptively\na\nllocating  the  ranks  of  different  LoRA  modules,  refer  to\nFig. 2(d);  (3)  optimizing  the  learning  procedure  of  LoRA;\n(4)\n combining with other learning paradigms. In this section,\nwe introduce these four types of methods respectively. \n3.1    Breaking the low-rank bottleneck\nT\nhe low-rank updates enable LoRA to be parameter efficient;\nhowever,  it  restricts  LLMs’  ability  to  memorize  downstream\nknowledge  and  generalization  on  downstream  tasks\n[18,205–208].  This  low-rank  limitation  causes  inferior\n4\nFront. Comput. Sci., 2025, 19(7): 197605\n×\nperformance  of  LoRA  in  knowledge-  and  skill-intensive\ndom\nains comparing to full-fine tuning, such as code and math.\nExperimental  study  [206]  demonstrates  that  the  rank  for  full\nfi\nne-tuning  is  significant  (10-100 )  higher  than  that  for\nLoRA, and increasing the rank of LoRA updation can narrow\nthe  performance  gap  between  LoRA  and  full  fine-tuning.  To\nincrease  the  rank  of  LoRA  and  improve  its  performance,\nseveral  methods  have  been  proposed  [17,24,27,209],  which\nt\nypically increase the rank through (1) stacking LoRAs along\nlearning  iterations;  (2)  updating  as  gradient  compressors;\n(3) co-updating LLM and LoRA modules during fine-tuning. \n3.1.1    Stacking LoRAs along fine-tuning\nrank(M1 +M2 ) ⩽\nrank(M1 ) +r ank(M2 ) M1 M2\nMatrix  rank  is  subadditive,  i.e., \n \nfor  matrices    and    that  have  the\nsame  size.  Based  on  the  subadditivity,  we  can  aggregate\nmultiple  LoRA  modules  together  to  increase  the  rank  and\nbreak the low-rank bottleneck. Following this idea,  ReLoRA\n[17] proposes a merge-and-reinit procedure for LoRA, which\npe\nriodically merges the LoRA modules to the LLM and then\nreinitializes  the  LoRA  modules  during  fine-tuning.  It  equals\nstacking  multiple  LoRA  modules  along  with  fine-tuning  and\ncan increase the rank of the overall updates. Similarly,  COLA\n[24]  proposes  another  merge-and-reinit  method  based  on\nFra\nnk-Wolfe  algorithm  [210].  However,  M ELoRA  [25]\npoi\nnts  out  that  the  merge-and-reinit  procedure  does  not\nnecessarily guarantee an increase in rank, because there can be\noverlap  between  the  series  of  LoRA  modules  along  fine-\ntuning.  To  solve  this  problem,  MELoRA  proposes  to\ndecompose the LoRA modules into smaller mini LoRAs and\nthen parallelly stack these mini LoRAs, whose effectiveness in\nincreasing the rank is theoretically verified. \n3.1.2    Updating as gradient compressor\nT\nhe  above  methods  break  the  low-rank  bottleneck  in  the\nparameter  space.  As  a  supplement, FLoRA  [26]  finds  that\nL\noRA  performs  a  fixed  random  projection  to  compress\ngradients and restricts the total weight matrix change to low-\nrank. To overcome this low-rank bottleneck in gradient space,\nFLoRA proposes to resample the random projection, which is\ndemonstrated to largely recover the performance of full-matrix\nSGD. \n3.1.3    Co-updating LLM and LoRA\nT\nhe  above  two  kinds  of  methods  focus  on  improving  the\nrepresentation  ability  of  LoRA  itself.  Different  from  them,\nDelta-LoRA  [27]  proposes  to  jointly  update  the  LLM  and\nL\noRA  modules,  which  directly  updates  the  high-rank  LLM\nand  can  gain  better  representation  capable  than  updating\nLoRA  independently.  It  updates  the  LLM  based  on  the\ndifference  between  two  LoRA  modules  of  two  consecutive\niterations,  which  enables  it  to  update  the  LLM  without  any\nextra memory. \n3.2    Dynamic rank allocation\nFor\n the  rank  of  LoRA,  higher  is  not  always  better.  The\nabundant  LoRA  ranks  may  cause  degeneration  in  both\nperformance  and  efficiency.  Furthermore,  the  importance  of\nweights  can  vary  across  different  layers  of  a  Transformer\nmodel  during  fine-tuning,  requiring  different  ranks  for  each\nlayer  [28,31,33,211].  Therefore,  assigning  the  same  rank  to\nL\noRA modules of different layers is not the optimal choice. It\nis  better  to  adaptively  allocate  ranks  to  LoRA  modules  of\ndifferent layers. Existing methods adaptively allocate ranks for\nLoRA  modules  from  the  perspectives  of  (1)  singular  value\ndecomposition  (SVD);  (2)  single-rank  decomposition  (SRD);\n(3) rank sampling. \n3.2.1    SVD-based methods\nBA\nPΛQ P Q Λ\nΛ BA\nBA\nP Q\nP Q\nL0\nΛ\nDecomposing  a  matrix  with  singular  value  decomposition\n(SVD)\n and  selectively  truncating  its  singular  values  is  an\neffective  way  to  control  the  rank  of  the  matrix.  Inspire  by\nSVD, we can decompose the LoRA parameter matrix   into\nan SVD form, i.e.,   where   and   are orthogonal and \nis a non-negative diagonal matrix. By controlling the elements\nin  ,  we  can  control  the  rank  of    and  allocate  ranks  for\nLoRA  modules.  Following  this  idea,  several  rank  allocation\nmethods  approximate  the  SVD  decomposition  for    and\nallocate  the  ranks  by  filtering  the  diagonal  matrix.  For\ninstance,  AdaLoRA  [ 28\n]  approximates  the  SVD\ndecomposition by regularizing the orthogonality of   and  .\nThen,  it  drops  unimportant  singular  values  based  on  novel\nimportance  scoring  methods.  Similarly, SaLoRA  [29]  also\ni\nntroduces  an  orthogonality  regularization  for    and  ;  by\ncontrast, it drops unimportant singular values based on the \nnorm.  However,  the  above  methods  are  not  efficient  enough\nfor  they  start  with  a  high  rank  and  then  reduce  the  rank\niteratively,  which  brings  a  pre-defined  budget  [30].  To  solve\nt\nhis problem, IncreLoRA [30] proposes to start from a single\nra\nnk  and  then  automatically  increase  the  rank  based  on  a\nheuristic  importance  score,  where  the  orthogonality\nregularization is also involved while the elements in   is not\nrequired to be non-negative. \n3.2.2    SRD-based methods\nBA\nBA\nBA\nP Q\nHowever, the orthogonality regularization brings unignorable\nc\nomputational costs for LoRA and degenerates its efficiency.\nTo  address  this  problem,  several  methods  omit  the\northogonality requirement of SVD and directly decompose \ninto single-rank components. Then, they allocate the ranks by\nselecting  the  proper  components. DoRA  (Dynamic  Low-\nRank  Adaptation)  [31]  proposes  to  decompose  the  LoRA\npa\nrameter matrix   into single-rank components and prunes\nthe  components  based  on  a  heuristic  importance  score.\nSimilarly,  AutoLoRA  [32\n]  also  decomposes  the  LoRA\nparameter  matrix    into  single-rank  components,  but  it\nprunes  the  components  based  on  meta-learning. SoRA  [33]\ne\nliminates the orthogonality regularization and filters columns\nand  rows  of    and    (their  combination  can  be  regarded  as\nsingle-rank  components)  by  directly  controlling  the  diagonal\nmatrix. It controls the diagonal matrix by formulating them as\na set of learnable gating units which are updated in the fine-\ntuning procedure. ALoRA [34] also filters the components by\nusi\nng gating units; by contrast, it learns the gating units based\non neural architecture search [212]. \n3.2.3    Rank sampling-based methods\nIn\n the  SVD  parameterization-  and  component-wise\nYuren MAO et al.    A survey on LoRA of large language models 5\nb\nb A B\nb\nb A b B\ndecomposition-based  methods,  we  need  to  spend  the  extra\nc\nomputational costs to search proper ranks. To avoid the extra\ncost,  DyLoRA  [35]  points  out  that  we  can  allocate  ranks\ndi\nrectly by random sampling. In each training step, it samples\na value   from a pre-defined discrete distribution and allocates\n  as  the  rank.  Then,  the  matrices    and    are  truncated  to\nrank- .  In  the  fine-tuning  procedure,  only  the  parameters  on\nthe  th row of   and  th column of   are tunable while other\nparameters are frozen. Besides, the distribution can be defined\nbased on users’ preferences. \n3.3    Optimizing the learning procedure\nIn\n practice,  LoRA  converges  more  slowly  than  full  fine-\ntuning.  Moreover,  it  is  also  sensitive  to  hyperparameters  and\nsuffers from overfitting. These issues affect LoRA’s efficiency\nand hinder its downstream adaptation performance. To address\nthese issues, researchers have developed several approaches to\noptimize  the  learning  procedure  of  LoRA,  which  can  be\ncategorized  into  the  following  three  types:  (1)  Initialization\nImprovement;  (2)  Gradient  Update  Optimization;\n(3) Overfitting Mitigation. \n3.3.1    Initialization improvement\nL\noRA usually initializes its parameter matrices  A and B using\nGaussian  noise  and  zeros  respectively.  There  are  two  simple\nschemes:  Init[A],  which  sets  matrix B  to  zero  and  randomly\ninitializes  matrix  A,  and  Init[B],  which  does  the  reverse.\nLiterature  [36]  compares  these  two  schemes  and  concludes\nt\nhat  Init[A]  is  better  through  theoretical  analysis.  It  reveals\nthat Init[A] allows using a larger learning rate without causing\ninstability,  making  the  learning  process  more  efficient.\nHowever, even with Init[A], this random initialization method\nstill  results  in  small  initial  gradients,  leading  to  slower\nconvergence. To solve this,  PiSSA [37] initializes LoRA with\nt\nhe  principal  singular  components  of  the  pre-trained  matrix.\nSince  principal  singular  components  represent  the  most\nsignificant directions in the matrix, aligning the initial weights\nwith  these  components  can  accelerate  convergence  and\nimprove  performance.  In  contrast, MiLoRA  [38]  initializes\nL\noRA  with  the  minor  singular  components.  Given  that\nrandom  initialization  of  low-rank  matrices  can  interfere  with\nthe  important  features  learned  in  the  pre-trained  matrix,  it\nreduces this interference to improve overall performance while\nadapting to new tasks. \n3.3.2    Gradient update optimization\nr ×r\nTo further enhance the convergence and reliability of LoRA,\nse\nveral  studies  have  proposed  improvements  from  the\nperspective  of  gradient  updates.  [39]  introduces  a  scaled\ngradient  method  based  on  Riemannian  optimization,  which\nincorporates  an    preconditioner  item  in  the  gradient\nupdate  step  to  improve  the  convergence  and  hyperparameter\nrobustness  of  LoRA.  Through  theoretical  analysis, LoRA+\n[40] discovered the necessity of setting a proportional learning\nra\nte for matrices A and B to achieve stable feature learning and\naccelerate  convergence. ResLoRA  [41]  introduced  residual\nconnections  into  LoRA  to  optimize  the  gradient  propagation\npath, speeding up training convergence and enhancing model\nperformance.  Similarly, SIBO  [42]  mitigate  over-smoothing\nby\n injecting  residual  connections  of  initial  token\nrepresentations  into  LoRA’s  input.  Additionally,  to  further\nreduce  computational  resources,  literature  [ 43]  employs\ngra\ndient-free  optimization  methods  such  as  CMA-ES  and\nFWA  to  optimize  LoRA,  demonstrating  competitive\nperformance in few-shot NLU tasks. Besides, DoRA (Weight-\nDecomposed  Low-Rank  Adaptation)  [44]  constrains  the\ngra\ndient  update,  focusing  on  the  directional  change  of  the\nparameter.  It  decomposes  pre-trained  weight  into  two\ncomponents, direction and magnitude, and applies LoRA only\nto the direction component to enhance training stability. \n3.3.3    Overfitting mitigation\nAl\nthough  LoRA  effectively  reduces  the  number  of  trainable\nparameters  compared  to  full  fine-tuning,  some  studies  have\nshown  that  LoRA  is  also  prone  to  overfitting  [47\n],  which\ncontradicts  previous  views.  To  address  this  issue, BiLoRA\n[45]  adopts  a  bi-level  optimization  strategy.  It  alternately\nt\nrains the singular vectors and singular values of the low-rank\nincrement matrix on different subsets of the training data. This\napproach avoids the simultaneous optimization of parameters\nat  different  levels  on  a  single  dataset,  thus  mitigating\noverfitting.  In  addition,  literature  [46]  applies  dropout  to\nL\noRA  parameters  to  reduce  overfitting,  while HiddenKey\n[47]  employs  column-wise  dropout  for  attention  layers  and\ne\nlement-wise dropout for feedforward layers. \n3.4    Combining with other learning paradigms\nL\noRA  is  compatible  with  other  learning  paradigms,  such  as\nBayesian Learning, In-context Learning and Active Learning.\nCombining LoRA with these learning paradigms can address\nseveral  problems  that  hurt  the  downstream  adaptation\nperformance.  For  example,  combining  with  Bayesian\nLearning, Laplace-LoRA [48] can relieve the overconfidence\nphe\nnomenon  that  happened  in  downstream  adaptation.\nCombining with In-context Learning,  PILLOW [49] aims to\nsol\nve the low-resource dilemmas existing in some downstream\ntasks.  Combining  with  Active  Learning, STAR  [50]  can\ne\nffectively improve the data efficiency.\nAt  last,  to  illustrate  the  performance  difference  between\nLoRA  and  some  of  its  variants,  we  report  their  performance\nfor  RoBERTa-base  [213]  model  on  the  GLUE  benchmark\n[214]  in Table 1.  These  results  are  derived  from  previous\nst\nudies [9,16,32,45,90]. \n4    Cross-task generalization\nL\noRA’s  pluggable  nature  enables  users  to  accumulate  LoRA\nplugins  for  different  tasks.  For  example,  on  Hugging  Face\nplatform,  there  are  more  than  20,000  LoRA  plugins\ncompatible  with  various  LLMs  for  different  tasks.  These\naccumulated  LoRA  plugins  can  not  only  be  utilized\nindependently  but  also  be  mixed  to  achieve  cross-task\ngeneralization  [60].  Mixing  multiple  LoRA  plugins  together,\nna\nmely  LoRA  mixture,  has  been  widely  applied  in  areas\nrequiring  cross-task  generalization,  such  as  multi-task\nlearning,  domain  adaptation,  and  continual  learning.  Existing\nLoRA  mixture  methods  can  be  categorized  into  (1)  mixture\nwith  manually  designed  weights;  (2)  mixture  with  learnt\nweights; (3) mixture of LoRA experts. This section introduces\n6 Front. Comput. Sci., 2025, 19(7): 197605\neach category of methods respectively, as shown in Fig. 3. \n4.1    Mixture with manually designed weights\nE\narly  LoRA  mixture  methods  attempt  to  linearly  combine\ndifferent  LoRA  modules  with  manually  designed  weights.\nSome research demonstrates that we can achieve proper cross-\ntask generalization ability by simply averaging LoRA modules\nor their related outputs [51–53]. Furthermore, several methods\nha\nve been proposed to further improve the performance of the\nLoRA  mixture  via  adopting  manually  designed  weights.  For\nexample, ControlPE [54], [55] and [56] set the weight factors\nas  hyperparameters,  and  ControlPE  uses  hyperparameter\nsearch  to  determine  the  optimal  combination  of  two  LoRA\nmodules. Additionally, Token-Level Adaptation [57] utilizes\nc\nosine  similarity  between  the  input  feature  and  the  adapter\ndataset  center  as  weight  factors,  while BYOM  [58]  applies\nba\nsic model fusion methods such as Task Arithmetic, Fisher-\nMerging, and RegMean.\nMixture  with  manually  designed  weights  can  quickly  mix\nmultiple  LoRAs  without  extra  training,  which  demonstrates\nsimplicity  and  computational  efficiency.  However,  it  often\nfails  to  find  the  optimal  weights,  leading  to  unstable\nperformance  and  limited  generalization.  Subsequently,\nresearchers  have  explored  using  learning-based  methods  to\nachieve more precise and adaptive mixtures. \n4.2    Mixture with learnt weights\nT\no  learn  the  optimal  mixture  weights,  several  methods  have\nbeen proposed at task level, instance level and token level to\nmeet different needs. Task-level methods focus on enhancing\ntask  transferability,  which  can  be  either  gradient-based,  such\nas  [59],  or  gradient-free,  as  seen  in  L oRAHub  [60].\nL\noRAHub  employs  a  black-box  algorithm  named  CMA-ES\n[216]  to  optimize  weight  factors  for  LoRA  modules,\nsimplifying  the  training  process.  Later, ComPEFT  [61]  and\nL\n-LoRA  [62]  use  LoRAHub  to  mix  quantized  LoRA\nm\nodules, further improving computational efficiency.\nCompared  to  task-level  methods,  instance-level  and  token-\nlevel  methods  can  provide  flexibility  and  precision  for\ncomplex inputs. For multimodal instruction tuning, MixLoRA\n[63\n] dynamically chooses appropriate low-rank decomposition\nvectors based on the input instance, which are then integrated\ninto  LoRA  matrices  for  training.  To  conduct  protein\nmechanics analysis and design tasks, X-LoRA [64] develops a\ndyna\nmic  gating  mechanism  to  assign  weights  for  LoRA\nmodules  at  the  token  level  and  layer  granularity.  These\napproaches demonstrate better performance in specific tasks or\napplication scenarios. \n4.3    Mixture of LoRA experts\nW\nhen  the  LoRA  modules  are  trainable,  we  can  jointly  learn\nthe mixture weights and the LoRA modules, which can further\nimprove  the  performance  of  the  LoRA  mixture.  To  jointly\nlearn  the  mixture  weights  and  LoRA  modules,  Mixture  of\nLoRA  Experts  (LoRA  MoE)  is  a  natural  choice,  where  each\nLoRA  module  acts  as  an  expert,  while  a  router  network\ntypically  assigns  the  mixture  weights.  LoRA  MoE  has  been\nproven  to  be  effective  in  many  tasks,  such  as  continual\nlearning  [65,66],  vision-language  tasks  [67]  and  multi-task\nm\nedical applications [68].\n  \nTable 1    Performance of LoRA and its variants for RoBERTa-base model on the GLUE benchmark. We report Matthew’s correlation for CoLA, Pearson\ncorrelation for STS-B, and accuracy for the other datasets. The results are reported according to the results reported in literature [9,32,45,89,90]\nMethod # Params SST-2 MPRC CoLA QNLI RTE STS-B\nT\nied-LoRA [215] 0.043 M 94.4 88.5 61.9 92.0 76.2 89.8\nAutoLoRA [32] 0.3 M 94.9 89.4 61.3 92.9 77.0 90.8\nDyLoRA [35] 0.3 M 94.3 89.5 61.1 92.2 78.7 91.1\nAdaLoRA [28] 0.3 M 94.5 88.7 62.0 93.1 81.0 90.5\nFourierFT [90] 0.024 M 94.2 90.0 63.8 92.2 79.1 90.8\nVeRA [88] 0.043 M 94.6 89.5 65.6 91.8 78.7 90.7\nFull Fine-tuning [9] 125 M 94.8 90.2 63.6 92.8 78.7 91.2\nLoRA [9] 0.3 M 95.1 89.7 63.4 93.3 78.4 91.5\nVB-LoRA [89] 0.023 M 94.4 89.5 63.3 92.2 82.3 90.8\nBiLoRA [45] 0.3 M 95.1 91.7 64.8 93.3 87.2 91.7\n \n \nF\nig. 3    An illustration of LoRA mixture methods\nYuren MAO et al.    A survey on LoRA of large language models 7\nExisting  methods  improve  the  performance  of  LoRA  MoE\nfrom\n the  perspectives  of  initialization,  task  relationship\nmanagement  and  efficiency.  For  initialization, Mixture-of-\nLoRAs  [69]  first  trains  multiple  LoRAs  separately  as\ninitialization and then optimizes the router and LoRAs jointly.\nMultiLoRA [70] proposes refining the initialization to reduce\npa\nrameter dependency, which can yield more balanced unitary\nsubspaces. As for task balance,  MLoRE [71] adds a low-rank\nc\nonvolution  path  in  the  MoE  structure  to  capture  global  task\nrelationships.  MTLoRA  [72]  adopts  both  task-agnostic  and\nt\nask-specific  LoRA  modules  to  address  task  conflicts.  For\nefficiency, MoLA [73] adaptively allocates different numbers\nof\n LoRA experts to different layers of the Transformer model\nto  save  the  number  of  LoRA  modules. LLaVA-MoLE  [74]\na\nnd  SiRA  [75]  leverage  sparse  computation  to  reduce\nc\nomputational  cost.  Additionally,  Octavius  [76]  sparsely\nactivates  independent  LoRA  experts  with  instance-level\ninstructions  to  mitigate  task  interference  and  improve\nefficiency. Fast LoRA [77] allows each sample in a minibatch\nt\no  have  its  unique  low-rank  adapters,  enabling  efficient\nbatching.\nBesides, some methods are not explicitly based on MoE but\nfollow MoE ideas. For example,  MoSLoRA [78] decomposes\nL\noRA  into  subspaces  and  employs  a  learnable  mixer  to  fuse\nthese subspaces. \n5    Efficiency improving\nW\nith the popularization of LLMs, the demand for training and\nrunning  LoRA  modules  increases  rapidly.  This  increasing\ndemand brings an unignorable computational burden; thus, for\nLoRA, the smaller, the faster, the better. To meet this demand,\nexisting  methods  improve  the  computational  efficiency  of\nLoRA  from  the  perspectives  of  (1)  parameter  reduction;\n(2)  parameter  quantization;  (3)  parallel  LoRA  computing\nframeworks.  This  section  introduces  each  category  of\nmethods, as illustrated in Fig. 4. \n5.1    Parameter reduction\nL\noRA significantly reduces the number of tunable parameters\nfor  fine-tuning  LLMs.  However,  it  still  requires  expensive\nactivation  memory  to  update  low-rank  matrices.  To  further\nreduce the memory cost, existing methods reduce the number\nof  tunable  parameters  of  LoRA  via  parameter  freezing,\nparameter pruning, and parameter sharing. \n5.1.1    Parameter freezing\nPa\nrameter  freezing  methods  reduce  the  number  of  tunable\nparameters  for  LoRA  via  freezing  some  of  its  parameters.\nThey  can  be  divided  into  two  categories:  intra-parameter\nmethods and extra-parameter methods.\nThe intra-parameter methods tune a subset of parameters of\nLoRA  while  freezing  the  others. LoRA-SP  [79\n]  randomly\nselects  half  of  the  LoRA  parameters  to  freeze  during  fine-\ntuning.  LoRA-FA  [80]freezes  the  down-projection  weights\na\nnd updates the up-projection weights in each layer of LoRA.\nAFLoRA  [81]  constructs  a  low-rank  trainable  path  and\ngra\ndually  freezes  parameters  during  training  LoRA.\nAdditionally, DropBP [82] accelerates the training process by\nra\nndomly  dropping  some  LoRA  gradient  calculations  during\nbackpropagation.\nr ×r\nr ×r\nBy contrast, the extra-parameter methods introduce and tune\na\n set  of  extra  parameters  while  freezing  the  original\nparameters  of  LoRA.  Most  of  them  are  proposed  based  on\nSingular Value Decomposition (SVD).  LoRA-XS [83] adds a\nsm\nall    weight  matrix  between  frozen  LoRA  matrices,\nwhich  are  constructed  using  the  SVD  of  the  original  weight\nmatrix;  then  it  tunes  only  the    weight  matrices  in  fine-\ntuning.  Similarly,  BYOM-LoRA  [ 58\n]  adopts  SVD  to\ncompress LoRA matrices for multi-task models. \n5.1.2    Parameter pruning\nPa\nrameter pruning methods aim to remove unimportant LoRA\nparameters  during  training  and  inference.  They  prune\nparameters  by  either  pruning  LoRA  independently  or  jointly\npruning LoRA and the LLM. LoRA-drop [84] uses the output\nof  LoRA  at  each  layer  to  evaluate  the  importance  of\nparameters and prune the unimportant parameters. By contrast,\nLoRAPrune  [85]  jointly  pruning  LoRA  matrices  and  the\nL\nLM parameters based on LoRA’s gradients. Besides, we can\nalso  use  LoRA  to  support  parameters  pruning  for  LLMs\n[86,87\n]. \n5.1.3    Parameter sharing\nPa\nrameter-sharing  methods  reduce  the  number  of  parameters\nby  sharing  parameters  across  different  layers  or  modules  of\nLLMs. VeRA [88] and VB -LoRA [89] are two representative\npa\nrameter-sharing  methods  for  LoRA.  Specifically,  VeRA\nproposes to share a pair of frozen random matrices across all\nlayers  and  conduct  layer-wise  adaptation  with  “scaling\nvectors”.  By  contrast,  VB-LoRA  proposes  a  “divide-and-\n \n \nF\nig. 4    An illustration of efficiency improving methods\n8 Front. Comput. Sci., 2025, 19(7): 197605\n∆W\nshare”  paradigm,  which  divides  LoRA’s  low-rank\nde\ncomposition  by  a  rank-one  decomposition  and  achieves\nglobal  sharing  based  on  an  admixture  model.  Instead  of\nsharing  parameters  in  the  original  parameter  space,\nFourierFT [90] converts the incremental matrix   into the\nspa\ntial  domain  using  Fourier  transform.  It  shares  spectral\nentries  across  all  layers  and  only  learns  its  sparse  spectral\ncoefficients  for  each  layer,  thus  reducing  the  number  of\ntrainable parameters. \n5.2    Parameter quantization\nQua\nntization, which reduces the bit width of parameters (e.g.,\nfrom 32-bit floats to 4-bit integers), can be used to reduce the\nmemory  and  computational  cost  of  LoRA.  Existing\nquantization-aware  LoRA  methods  consist  of  post-training\nquantization  (PTQ)-based  methods  and  quantization-aware\ntraining (QAT)-based methods [95]. \n5.2.1    PTQ-based methods\nIn\n PTQ-based  methods,  we  first  quantize  an  LLM  and  then\nfine-tune the quantized model, namely quantization and fine-\ntuning  are  sequentially  conducted. QLoRA  [91]  is  the  first\nPT\nQ-based  quantization-aware  LoRA  method.  In  the  fine-\ntuning stage, it first quantizes an LLM to 4 bits and then fine-\ntunes  a  LoRA  module  on  it  with  a  higher  precision,  such  as\nBFloat16 or Float16. In the inference stage, it dequantizes the\nLLM to the same precision as LoRA and then adds the LoRA\nupdates to the LLM.\nAlthough QLoRA can significantly reduce memory cost for\nfine-tuning, it does not bring benefits for inference, because it\nrequires  dequantizing  the  LLM  to  high  precision  again.  To\nsolve  this  problem, QA-LoRA  [92]  is  proposed  to  reduce\nm\nemory  cost  for  both  the  fine-tuning  and  inference  stages.\nQA-LoRA uses group-wise operators to balance the degrees of\nfreedom  of  the  LLM  quantization  and  fine-tuning,  which\nenables it to obtain a LoRA module having identical precision\nwith  the  quantized  LLM.  Thus,  it  can  perform  inference\nwithout dequantization. \n5.2.2    QAT-based methods\nIn\n QAT-based methods, we jointly quantize and fine-tune an\nLLM, namely quantization and fine-tuning are simultaneously\nconducted.  These  methods  can  alleviate  the  quantization\ndiscrepancies observed in PTQ-based methods. To address the\nquantization discrepancy of QLoRA,  LoftQ [93] alternatively\na\npplies quantization and low-rank approximation during fine-\ntuning  to  minimize  the  quantization  error.  However, ApiQ\n[94] points out that LoftQ ignores the error propagation across\nl\nayers and proposes activation-preserved initialization to avoid\nerror  propagation.  Besides, L4Q  [95]  is  another  QAT-based\nm\nethod that has an advanced layer design. \n5.3    Parallel LoRA computing frameworks\nL\noRA’s  parameter-efficient  nature  enables  us  to  fine-tune  or\ninfer  multiple  modules  on  a  single  GPU  or  a  GPU  cluster,\nwhich  can  save  computational  resources  and  improve  the\nefficiency of LoRA. This section introduces the parallel fine-\ntuning and parallel inference frameworks, respectively. \n5.3.1    Parallel fine-tuning\nPa\nrallelly  fine-tuning  multiple  LoRA  modules  on  a  single\nGPU  can  reduce  GPU  memory  usage  and  improve\ncomputation  efficiency.  ASPEN  [96]  proposes  a  high-\nt\nhroughput  parallel  finetuning  framework  for  LoRA,  which\nconsists  of  a  BatchFusion  approach  and  an  adaptive  job\nscheduling algorithm. Specifically, the BatchFusion approach\nsupports  parallelly  fine-tuning  multiple  LoRA  modules  on  a\nshared  LLM  by  fusing  multiple  input  batches  into  a  single\nbatch,  while  the  adaptive  job  scheduling  algorithm  allocates\ncomputation resources to the fine-tuning jobs. \n5.3.2    Parallel inference\nPa\nrallel inference framework for LoRA can not only improve\nthe  computational  efficiency  but  also  support  the  needs  of\nmulti-tenant  service. Punica  [97]  uses  a  new  CUDA  kernel\ndesign to batch GPU operations for different LoRA modules.\nBased on Punica,  S-LoRA [98] further optimizes the parallel\ni\nnference  framework  by  introducing  a  unified  paging\nmechanism  and  a  new  tensor  parallelism  strategy,  which\nenables the service of thousands of concurrent LoRA modules.\nThen, based on Punica and S-LoRA,  CaraServe [99] reduces\nt\nhe  cold-start  overhead  and  further  improves  the  service\nefficiency  and  SLO  (service-level  objective)  attainment  rates\nby CPU-GPU cooperation and rank-aware scheduling. \n6    LoRA for federated learning\nW\nhen  adapting  LLMs  to  vertical  domains  such  as  medicine\nand finance, the available training data can be privately owned\nby  multiple  clients.  In  this  scenario,  the  training  data  is  not\ncentralized, and we have to fine-tune LLMs while keeping the\ndata  localized,  namely  federated  learning.  In  federated\nlearning, the clients typically compute weight updates locally\nand then share these updates with others to globally update the\n \n \nF\nig. 5    An illustration of LoRA for federated learning\nYuren MAO et al.    A survey on LoRA of large language models 9\nLLM. It brings both communication and computation costs for\nt\nhe  clients.  Fortunately,  LoRA  is  parameter  efficient  and\npluggable, which can reduce communication costs and lower\ncomputational  resource  requirements.  LoRA  can  enhance  the\noverall efficiency and scalability of federated learning.\nHowever, adopting LoRA in federated learning is not trivial\nfor  federated  learning  faces  challenges  such  as  data\nheterogeneity, device heterogeneity, and model heterogeneity.\nTo address these issues, recent studies have designed various\nmethods  for  LoRA  to  meet  the  diverse  needs  of  federated\nlearning,  as  shown  in Fig. 5.  Additionally,  as  a  localized\npa\nrameter  component,  LoRA’s  pluggable  nature  allows  it  to\nsupport parameter privacy protection in federated learning. \n6.1    Data heterogeneity\nDa\nta  heterogeneity  refers  to  differences  in  data  distribution\nacross  clients.  In  federated  learning,  different  clients  usually\nhave  different  data  distributions.  The  inconsistency  in  data\ndistribution  affects  the  overall  performance  of  the  model.\nResearch  reveals  that  in  federated  learning,  as  user  data\nbecomes  more  diverse,  the  performance  gap  between  LoRA\nand  full  fine-tuning  widens  [100].  To  address  this  issue,\nre\nsearchers have proposed several improvement methods.\nSLoRA [100] introduces a data-driven initialization method\nfor\n LoRA. It first performs sparse federated fine-tuning before\napplying  LoRA  and  then  performs  SVD  to  decompose  the\naccumulated  gradient  updates  into  low-rank  matrices  for\nLoRA initialization. The goal is to enable the LoRA modules\nto better adapt to the data distribution of each client, thereby\nintegrating  these  heterogeneous  data  characteristics  into  the\nglobal model more effectively.  FeDeRA [101] uses a simpler\ni\nnitialization  method.  It  directly  applies  SVD  to  pre-trained\nweights  to  initialize  LoRA.  Retaining  the  principal\ncomponents of the pre-trained weights aligns the direction and\nmagnitude of weight updates across different clients to handle\ndata  heterogeneity.  Additionally, FFA-LoRA  [102]  freezes\none\n low-rank  matrix  and  fine-tunes  only  the  other.  This\nreduces  inconsistency  during  server  aggregation  of  LoRA\ngradients,  alleviating  the  optimization  instability  caused  by\nnon-IID data. \n6.2    Device heterogeneity\nDe\nvice  heterogeneity  refers  to  the  differences  in  hardware\ncapabilities,  and  network  connectivity  among  clients\nparticipating  in  federated  learning.  Traditional  federated\nlearning  methods  often  encounter  the  “buckets  effect”,\nimplying  that  the  system’s  overall  performance  is  limited  by\nthe  capability  of  the  least  powerful  client.  Specifically,  these\nmethods  use  the  smallest  LoRA  rank  to  accommodate  all\nclients, which prevents many resource-rich clients from fully\nutilizing their potential.\nTo  address  this  issue,  a  dynamic  parameter  allocation\nstrategy can be adopted. FedMS [103] dynamically adjusts the\nnum\nber  of  activated  LoRA  matrices  based  on  the  real-time\ncomputational  resources  of  clients. FlexLoRA  [104]  uses  a\ndyna\nmic  parameter  allocation  strategy.  It  adjusts  the  LoRA\nrank  and  redistributes  the  SVD  components  of  the  global\nLoRA  weights  based  on  resource  constraints.  Similarly,\nHETLORA [105] assigns different ranks for different clients.\nHowe\nver,  it  performs  weighted  aggregation  according  to  the\nsparsity of the updates from different clients, balancing update\ninformation better than simple aggregation. \n6.3    Model heterogeneity\nMode\nl heterogeneity indicates differences in model structures\namong  clients.  In  traditional  federated  learning,  clients  use\nlocal  models  with  the  same  architecture,  allowing  their\nparameters to be aggregated into a global model on the server.\nHowever,  in  practice,  clients  may  prefer  unique  local  model\narchitectures  due to  personal needs  and  often  do  not want to\ndisclose  model  details.  Thus,  it  is  necessary  to  transfer\nknowledge  between  heterogeneous  models  without  sharing\nprivate data or revealing local model structures [217].\nPre\nvious  work  has  used  knowledge  distillation,  model\nensembling,  and  mutual  learning  to  address  model\nheterogeneity. However, these methods have limitations, such\nas reliance on public datasets, additional communication costs\nand poor local model performance. To avoid these limitations,\npFedLoRA [106] uses LoRA as a carrier of both global and\nl\nocal  knowledge.  It  adopts  an  iterative  training  strategy  to\nfacilitate  knowledge  transfer  and  integration,  enabling\nknowledge  sharing  among  heterogeneous  models  across\ndifferent clients. \n6.4    Parameter privacy\nIn\n federated  learning,  protecting  client-specific  parameters  is\ncrucial because ensuring the privacy of these parameters also\nindirectly  safeguards  client  data  privacy.  As  a  modular\napproach to adjusting personalized parameters, LoRA can be\neffectively  integrated  into  federated  learning  systems  to\nachieve parameter privacy protection.\nM A B\nM\nLiterature  [107]  proposes  a  secure  distributed  language\nm\nodel  training  framework  based  on  model  slicing.  They\ndeploy LoRA in a Trusted Execution Environment (TEE) and\nuse OTP encryption to transmit features between the GPU and\nTEE,  protecting  model  parameter  privacy.  PrivateLoRA\n[108] introduces a distributed system based on LoRA. It adds\na\n square matrix   between low-rank matrices   and  . The\nnon-trainable  matrices A  and B,  along  with  most  of  the  pre-\ntrained weights, are deployed on the global server to enhance\ncomputation. Meanwhile, the trainable matrix   is stored on\nthe client as personalized parameters, thus ensuring parameter\nprivacy protection.\nA\nFurthermore,  recent  works  have  integrated  differential\npri\nvacy  (DP)  techniques  with  LoRA  in  federated  learning  to\nenhance  data  privacy. DP-LoRA  [218]  ensures  differential\npri\nvacy by adding Gaussian noise to LoRA’s weight updates\nduring  the  update  process.  This  approach  maintains  privacy\nand  improves  communication  efficiency.  To  solve  the  noise\namplification  when  applying  differential  privacy  in  LoRA,\nFFA-LoRA [102] fixes the matrix  , avoiding the local semi-\nqua\ndratic  structure  and  enhancing  robustness  and\nperformance. \n7    Applications of LoRA\nIn\n the  rapidly  evolving  field  of  deep  learning,  LoRA  has\nbecome widely used due to its unique advantages. Researchers\n10 Front. Comput. Sci., 2025, 19(7): 197605\nutilize  LoRA  to  fine-tune  pre-trained  models  for  various\ndownst\nream  tasks,  reducing  computational  resource\nrequirements  while  enhancing  performance.  LoRA’s  strong\nadaptability  and  efficiency  have  significantly  improved\nvarious applications. In this section, we will introduce LoRA’s\napplications  in  the  following  scenarios:  (1)  language  tasks;\n(2) vision tasks; (3) multimodal tasks. \n7.1    Language tasks\nR\necently,  the  rapid  development  of  pre-trained  language\nmodels,  especially  LLMs,  is  revolutionizing  the  approach  to\nlanguage  tasks  due  to  their  outstanding  performance.\nHowever,  these  pre-trained  models  are  trained  on  a  large\namount of general data and still require further fine-tuning on\ntask-specific data to adapt to downstream tasks. Therefore, it\nis natural to use LoRA to fine-tune these pre-trained language\nmodels,  as  it  reduces  computational  resource  requirements.\nWe  mainly  focus  on  some  representative  downstream  tasks,\nwhich  include  traditional  NLP  tasks,  code  tasks,  model\nalignment and vertical domain tasks. \n7.1.1    Traditional NLP tasks\nGi\nven  the  strong  instruction-following  and  contextual\nunderstanding abilities of LLMs, some researches apply LoRA\nto  fine-tune  these  models  for  traditional  NLP  tasks.  For\nexample,  LoRA  is  widely  adopted  in  LLaMA  for  various\ntasks,  such  as  emotion  recognition  [109],  text  classification\n[110]  and  role  recognition  [111]. AutoRE \n [112]  applies\nQL\noRA  to  three  document-level  relation  extraction  tasks,\nachieving great performance on different LLMs. Some studies\n[113–115]  leverage  LoRA  from  different  perspectives  to\ne\nnhance  the  model’s  capability  in  machine  translation  tasks.\nAdditionally,  LoRA  can  also  improve  the  performance  of\nmodels  like  BERT  and  T5  for  text  understanding  tasks\n[116,117]. \n7.1.2    Code tasks\nSom\ne researchs apply LoRA to improve model performance in\nvarious  code-related  tasks.  For  example,  BERT-style  models\nfine-tuned  with  LoRA  are  suitable  for  code-change-related\ntasks,  specifically  in  Just-In-Time  defect  prediction  (JIT-DP)\n[118,119].  Similarly,  training  CodeT5  and  PLBART  with\nL\noRA can enhance their adaptability for code summarization\nand  code  clone  detection  [120].  As  for  the  decoder-only\nm\nodel,  RepairLLaMA  [121]  uses  LoRA  to  fine-tune  Llama\nfor\n automated program repair (APR), while WizardCoder-15B\nis  fine-tuned  with  LoRA  for  Text-to-SQL  task  [122 ].\nAdditionally,  SteloCoder  [123],  a  fine-tuned  version  of\nSt\narCoder,  is  designed  for  multi-language  to  Python  code\ntranslation. \n7.1.3    Model alignment tasks\nMode\nl alignment tasks focus on adjusting a machine learning\nmodel to align with human values and intentions, often using\ntechniques  like  Reinforcement  Learning  from  Human\nFeedback (RLHF). To reduce memory requirements of RLHF,\nsome  studies  use  LoRA  to  fine-tune  the  reward  model  and\npolicy  model  [124–126].  Furthermore,  other  works  improve\nre\nward  models  by  integrating  multiple  LoRA  adapters.  For\nexample,  DMoERM  [127]  combines  MoE  with  LoRA,\nrout\ning model inputs to multiple LoRA experts while another\nwork [128] proposes a LoRA-based ensemble method as well.\nT\nhe  integration  can  also  benefit  the  quantification  of\nuncertainty  in  reward  models  [129].  Besides,  literature  [130]\na\npplies Laplace-LoRA with a Gaussian prior assumption [131]\nt\no  train  Bayesian  reward  models,  which  mitigates  reward\noveroptimization in best-of-n sampling. \n7.1.4    Vertical domain tasks\nL\nLMs  often  perform  suboptimally  in  vertical  domains,\nrequiring  fine-tuning  with  domain-specific  expertise.  Some\nworks apply LoRA to improve the performance of LLMs on\ndomain-specific  tasks.  For  example,  some  studies  fine-tune\nLLMs  on  medical  datasets  with  LoRA  to  adapt  them  to  the\nmedical  domain  [ 132–134].  Additionally,  other  studies\ni\nmprove  medical  tasks  like  clinical  dialogue  summarization\n[135],  assertion  detection  [136]  and  medical  QA  tasks\n[137,138].  Similarly,  several  studies  fine-tune  LLMs  with\nL\noRA on financial data to solve tasks such as financial news\nanalytics  and  sentiment  classification  [139–142].  Besides,\nL\noRA  can  also  be  used  to  enhance  the  performance  in\ndatabase tasks like query rewrite and index tuning [143]. \n7.2    Vision tasks\nIn\n vision tasks, LoRA is primarily applied to image generation\nand  image  segmentation,  significantly  improving  training\nefficiency and optimizing model performance. \n7.2.1    Image generation\nIm\nage generation tasks hold significant importance in the field\nof  computer  vision.  In  recent  years,  diffusion  model  have\ndemonstrated  exceptional  performance  in  image  generation\ntasks.  LoRA  is  widely  used  in  diffusion  models  to  address\nvarious image generation tasks while reducing computational\nresources.  Some  works  use  LoRA  to  fine-tune  diffusion\nmodels for image style transfer [ 144–148], while others apply\ni\nt to text-to-image generation [149–153].\nFurt\nhermore,  researchers  have  designed  several  LoRA-\nbased  methods  to  improve  image  generation  quality.  For\ninstance,  Smooth  Diffusion  [154]  uses  LoRA  to  achieve\nsm\noothness in the latent space, leading to better performance\nin  various  image  generation  and  editing  tasks. ResAdapter\n[155] employs LoRA to learn resolution priors, adjusting the\nre\nceptive  fields  of  convolutional  layers  to  dynamical\nresolution. Additionally, to specifically enhance text-to-image\nquality,  STAMINA  [156]  uses  LoRA  to  fine-tune  diffusion\nm\nodels for longer concept sequences.  DreamSync [157] and\nStyl\neAdapter  [158]  use  LoRA  to  improve  text  fidelity  and\ni\nmage  quality.  Mix-of-Show  [159]  captures  out-of-domain\ni\nnformation  with  LoRA  weights  to  combine  multiple\ncustomized  concepts  with  high  fidelity,  reducing  concept\nconflicts. Other studies combine LoRA with model distillation\nto  accelerate  image  generation  [160,161].  Moreover,  LoRA\nc\nan  also  be  applied  to  video  generation  [162–167]  and  3D\nge\nneration tasks [168–172]. \n7.2.2    Image segmentation\nIm\nage  segmentation  is  a  significant  challenge  in  computer\nYuren MAO et al.    A survey on LoRA of large language models 11\nvision,  aiming  to  divide  an  image  into  multiple  meaningful\nre\ngions or objects. To address this, SAM has been proposed as\na  foundational  model  for  image  segmentation  and\ndemonstrated  superior  generalization  ability.  To  further\nenhance  its  performance  in  specific  vertical  domains,  many\nstudies  utilize  LoRA  to  fine-tune  it.  For  instance,  in  license\nplate detection, SamLP [173] utilizes LoRA to adapt SAM for\ne\nfficient  segmentation  of  license  plates.  In  structural  damage\ndetection,  literature  [174]  fine-tunes  SAM’s  encoder  using\nL\noRA for instance segmentation task. In the medical domain,\nmany studies also apply LoRA to fine-tune SAM for a variety\nof  tasks,  including  nuclei  segmentation  [175],  OCTA  image\nse\ngmentation  [176],  brain  tumor  segmentation  [177],  organ\nse\ngmentation  [178],  and  surgical  instrument  segmentation\n[179].  Additionally,  some  studies  use  LoRA  to  fine-tune\nVi\nsion  Transformer  (ViT)  for  visual  tracking  [180]  and  face\nforge\nry detection [181]. \n7.3    Multimodal tasks\nMul\ntimodal  Large  Language  Models  (MLLMs)  aim  to\nintegrate text with various modalities such as audio, image and\nvideo, which enable cross-modal understanding and reasoning\nthrough a unified embedding space. The success of LoRA in\nboth NLP and vision tasks has sparked considerable interest in\napplying them to MLLMs.\nIn MLLMs, LoRA can not only improve training efficiency\nbut  also  facilitate  effective  modality  alignment.  In  audio-text\ntasks,  SALM  [182]  comprises  LoRA  layers,  a  frozen  text-\nba\nsed  LLM,  an  audio  encoder  and  a  modality  adapter  to\nhandle speech inputs and corresponding task instructions. For\nimage-text  tasks,  InternLM-XComposer2  [183]  achieves\nm\nodality  alignment  by  applying  LoRA  to  image  tokens,\nmPLUG-Owl  [184]  freezes  the  visual  module  while  jointly\nfi\nne-tuning  LoRA  and  abstractor  of  the  text  module,  and\nCoLLaVO  [185]  employs  QLoRA  to  preserve  object-level\ni\nmage  understanding.  In  the  realm  of  video-text  tasks, VSP-\nLLM [186] fine-tunes the text module with QLoRA for visual\nspe\nech  processing, MolCA  [187]  uses  LoRA  to  understand\n2D\n molecular graphs and text, while  TPLLM [188] employs\nL\noRA  for  efficient  traffic  prediction  by  integrating  sequence\nand  spatial  features.  These  applications  demonstrate  the\nversatility and power of LoRA in MLLMs tasks. \n8    Conclusion and future direction\nIn\n this  survey,  the  recent  progress  of  LoRA  have  been\nsystematically  reviewed  from  the  perspective  of  downstream\nadaptation  improving,  cross-task  generalization,  efficiency\nimproving,  federated  learning  and  applications.  From  this\nreview,  we  can  find  that  LoRA  is  parameter  efficient,\npluggable,  compatible  and  easy  to  achieve  cross-task\ngeneralization,  which  enables  it  to  be  one  of  the  most\nimportant technology for LLMs applications. Recent progress\nfurther boosts the generalization and efficiency of LoRA, and\nstimulate its potential to be used in more scenarios. Here, we\nlist three future directions where LoRA will be indispensable. \n8.1    LoRA for GaaS\nIn\n Generative-as-a-Service  (GaaS),  cloud-based  platforms\nprovide  users  with  generative  artificial  intelligence  (AGI)\nservices.  GaaS  enables  users  enjoy  AGI  without  deploying\nlocal  computational  resources.  For  the  users’  needs  are\ndiverse, it is necessary to provides various functions for GaaS.\nTo implement the various functions, we can construct a LoRA\nmodule  for  each  function.  The  pramameter  efficiency  and\nplugability  of  LoRA  can  facilitate  efficient  functions’\nconstruction  and  execution.  Besides,  the  services  on  GaaS\nplatforms  can  change  rapidly  alonging  time.  To  follow  the\nchanges, we can train new LoRA modules that initialized by\ncombination  of  previous  LoRA  modules.  The  cross-task\ngeneralization  ability  of  LoRA  can  facilitate  fast  adaption  to\nservice updations. \n8.2    LoRA for continued pre-training\nIn\n continued  pre-training,  a  foundation  model  is  continuely\ntrained with unlabeled user data to adapt the model to specific\ndomains.  Typically,  the  self-supervised  training  objective  is\nsame with that for pre-training, and the learning rate is much\nsmaller than than for pre-training. Continued pre-training is a\nimportant  stage  for  constructing  vertical  domain  LLMs.\nHowever,  it  is  highly  computational  expensive,  which\nimpedes the development of vertical domain LLMs, especailly\nfor  the  organizations  with  limited  computational  resources.\nEnhancing  LoRA  for  continued  pre-training  and  reducing  its\ncomputational cost is worth to explored. \n8.3    LoRA for autonomous agents\nIn\n LLM-based  autonomous  agents,  the  agents  are  assigned\nwith  specific  roles.  Based  the  roles  and  environment,  agents\nmake actions to response users’ or other agents’ request. The\nactions  can  be  made  based  on  self-knowledge  or  tools  that\ndesigned  for  domain-specific  tasks.  The  request  and  the\nactions are stored in memory to support the future requests.\nIn  the  current  agents,  the  roles  are  typically  assigned  by\nprompts; however, prompt may cannot give a comprehensive\ndiscription  of  the  role  when  the  role  is  complex  and  the\nnumber  of  related  data  is  large.  Assiging  roles  with  LoRA\nmodules training from data related to the roles can be a better\nchoice.  Furthermore,  the  tools  for  agent  can  be  LoRA\nmodules.  Besides,  the  memory  usually  augments  the  agents\nwith retrieval augmented generation (RAG); however, due to\nthe input token limitation and the short-comings of in-context\nlearning,  the  RAG-based  support  may  be  less  effective.  By\ncontrast,  we  can  use  LoRA-based  continual  learning  to\nconstruct  memory  modules,  which  can  solve  the  problem  of\nRAG. Therefore, LoRA-driven agents are worth to explore. \nAcknowledgements    T\n his  work  was  supported  in  part  by  the  National\nNatural  Science  Foundation  of  Chian  (Grant  Nos.  62025206,  62302436,\nU23A20296),  the  Zhejiang  Province’s  \"Lingyan\"  R&D  Project  (No.\n2024C01259), and the Ningbo Science and Technology Special Projects (No.\n2023Z212). \nCompeting  interests    T he  authors  declare  that  they  have  no  competing\ninterests or financial conflicts to disclose. \nOpen Access    T his article is licensed under a Creative Commons Attribution\n4.0 International License, which permits use, sharing, adaptation, distribution\nand  reproduction  in  any  medium  or  format,  as  long  as  you  give  appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made.\n12 Front. Comput. Sci., 2025, 19(7): 197605\nThe images or other third party material in this article are included in the\na\nrticle’s Creative Commons licence, unless indicated otherwise in a credit line\nto the material. If material is not included in the article’s Creative Commons\nlicence  and  your  intended  use  is  not  permitted  by  statutory  regulation  or\nexceeds  the  permitted  use,  you  will  need  to  obtain  permission  directly  from\nthe copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/\n4.0/.\nRe\nferences \n  Devlin  J,  Chang  M  W,  Lee  K,  Toutanova  K.  BERT:  pre-training  of\ndeep  bidirectional  transformers  for  language  understanding.  In:\nProceedings  of  the  North  American  Chapter  of  the  Association  for\nComputational Linguistics: Human Language Technologies, NAACL-\nHLT. 2019, 4171−4186\n1.\n  Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A,\nBarham  P,  Chung  H  W,  Sutton  C,  Gehrmann  S,  Schuh  P,  Shi  K,\nTsvyashchenko  S,  Maynez  J,  Rao  A,  Barnes  P,  Tay  Y,  Shazeer  N,\nPrabhakaran  V,  Reif  E,  Du  N,  Hutchinson  B,  Pope  R,  Bradbury  J,\nAustin J, Isard M, Gur-Ari G, Yin P, Duke T, Levskaya A, Ghemawat\nS, Dev S, Michalewski H, Garcia X, Misra V, Robinson K, Fedus L,\nZhou D, Ippolito D, Luan D, Lim H, Zoph B, Spiridonov A, Sepassi R,\nDohan  D,  Agrawal  S,  Omernick  M,  Dai  A  M,  Pillai  T  S,  Pellat  M,\nLewkowycz A, Moreira E, Child R, Polozov O, Lee K, Zhou Z, Wang\nX, Saeta B, Diaz M, Firat O, Catasta M, Wei J, Meier-Hellstern K, Eck\nD, Dean J, Petrov S, Fiedel N. PaLM: scaling language modeling with\npathways.  The  Journal  of  Machine  Learning  Research,  2023,  24(1):\n240\n2.\n  Chen  Y,  Qian  S,  Tang  H,  Lai  X,  Liu  Z,  Han  S,  Jia  J.  LongLoRA:\nefficient  fine-tuning  of  long-context  large  language  models.  In:\nProceedings  of  the  12th  International  Conference  on  Learning\nRepresentations. 2024\n3.\n  Pan R, Liu X, Diao S, Pi R, Zhang J, Han C, Zhang T. LISA: layerwise\nimportance sampling for memory-efficient large language model fine-\ntuning. 2024, arXiv preprint arXiv: 2403.17919\n4.\n  Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, Hu S, Chen Y, Chan C\nM, Chen W, Yi J, Zhao W, Wang X, Liu Z, Zheng H T, Chen J, Liu Y,\nTang J, Li J, Sun M. Parameter-efficient fine-tuning of large-scale pre-\ntrained  language  models.  Nature  Machine  Intelligence,  2023,  5(3):\n220−235\n5.\n  Houlsby  N,  Giurgiu  A,  Jastrzebski  S,  Morrone  B,  de  Laroussilhe  Q,\nGesmundo  A,  Attariyan  M,  Gelly  S.  Parameter-efficient  transfer\nlearning for NLP. In: Proceedings of the 36th International Conference\non Machine Learning. 2019, 2790−2799\n6.\n  Lester B, Al-Rfou R, Constant N. The power of scale for parameter-\nefficient  prompt  tuning.  In:  Proceedings  of  2021  Conference  on\nEmpirical Methods in Natural Language Processing. 2021, 3045−3059\n7.\n  Zaken E B, Goldberg Y, Ravfogel S. BitFit: simple parameter-efficient\nfine-tuning  for  transformer-based  masked  language-models.  In:\nProceedings  of  the  60th  Annual  Meeting  of  the  Association  for\nComputational Linguistics (Volume 2: Short Papers). 2022, 1−9\n8.\n  Hu E J, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen\nW.  LoRA:  low-rank  adaptation  of  large  language  models.  In:\nProceedings  of  the  10th  International  Conference  on  Learning\nRepresentations. 2022\n9.\n  Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B,\nZhang J, Dong Z, Du Y, Yang C, Chen Y, Chen Z, Jiang J, Ren R, Li\nY, Tang X, Liu Z, Liu P, Nie J Y, Wen J R. A survey of large language\nmodels. 2023, arXiv preprint arXiv: 2303.18223\n10.\n  Han  Z,  Gao  C,  Liu  J,  Zhang  J,  Zhang  S  Q.  Parameter-efficient  fine-\ntuning for large models: a comprehensive survey. 2024, arXiv preprint\n11.\narXiv: 2403.14608\n Malladi S, Wettig A, Yu D, Chen D, Arora S. A kernel-based view of\nlanguage model fine-tuning. In: Proceedings of the 40th International\nConference on Machine Learning. 2023, 23610−23641\n12.\n Koubbi  H,  Boussard  M,  Hernandez  L.  The  impact  of  LoRA  on  the\nemergence  of  clusters  in  transformers.  2024,  arXiv  preprint  arXiv:\n2402.15415\n13.\n Jang U, Lee J D, Ryu E K. LoRA training in the NTK regime has no\nspurious local minima. 2024, arXiv preprint arXiv: 2402.11867\n14.\n Zhu J, Greenewald K, Nadjahi K, de Ocáriz Borde H S, Gabrielsson R\nB, Choshen L, Ghassemi M, Yurochkin M, Solomon J. Asymmetry in\nlow-rank  adapters  of  foundation  models.  2024,  arXiv  preprint  arXiv:\n2402.16842\n15.\n Zeng  Y,  Lee  K.  The  expressive  power  of  low-rank  adaptation.  In:\nProceedings  of  the  12th  International  Conference  on  Learning\nRepresentations. 2024\n16.\n Lialin V, Muckatira S, Shivagunde N, Rumshisky A. ReLoRA: high-\nrank  training  through  low-rank  updates.  In:  Proceedings  of  the  12th\nInternational Conference on Learning Representations. 2024\n17.\n Jiang T, Huang S, Luo S, Zhang Z, Huang H, Wei F, Deng W, Sun F,\nZhang  Q,  Wang  D,  Zhuang  F.  MoRA:  high-rank  updating  for\nparameter-efficient  fine-tuning.  2024,  arXiv  preprint  arXiv:\n2405.12130\n18.\n Huh  M,  Cheung  B,  Bernstein  J,  Isola  P,  Agrawal  P.  Training  neural\nnetworks  from  scratch  with  parallel  low-rank  adapters.  2024,  arXiv\npreprint arXiv: 2402.16828\n19.\n Liang Y S, Li W J. InfLoRA: interference-free low-rank adaptation for\ncontinual learning. 2024, arXiv preprint arXiv: 2404.00228\n20.\n Zhao  H,  Ni  B,  Wang  H,  Fan  J,  Zhu  F,  Wang  Y,  Chen  Y,  Meng  G,\nZhang  Z.  Continual  forgetting  for  pre-trained  vision  models.  2024,\narXiv preprint arXiv: 2403.11530\n21.\n Ren  W,  Li  X,  Wang  L,  Zhao  T,  Qin  W.  Analyzing  and  reducing\ncatastrophic  forgetting  in  parameter  efficient  tuning.  2024,  arXiv\npreprint arXiv: 2402.18865\n22.\n Zhang  H.  SinkLoRA:  enhanced  efficiency  and  chat  capabilities  for\nlong-context  large  language  models.  2024,  arXiv  preprint  arXiv:\n2406.05678\n23.\n Xia  W,  Qin  C,  Hazan  E.  Chain  of  LoRA:  efficient  fine-tuning  of\nlanguage  models  via  residual  learning.  2024,  arXiv  preprint  arXiv:\n2401.04151\n24.\n Ren  P,  Shi  C,  Wu  S,  Zhang  M,  Ren  Z,  de  Rijke  M,  Chen  Z,  Pei  J.\nMELoRA:  mini-ensemble  low-rank  adapters  for  parameter-efficient\nfine-tuning. 2024, arXiv preprint arXiv: 2402.17263\n25.\n Hao Y, Cao Y, Mou L. Flora: low-rank adapters are secretly gradient\ncompressors. 2024, arXiv preprint arXiv: 2402.03293\n26.\n Zi B, Qi X, Wang L, Wang J, Wong K F, Zhang L. Delta-LoRA: fine-\ntuning high-rank parameters with the delta of low-rank matrices. 2023,\narXiv preprint arXiv: 2309.02411\n27.\n Zhang  Q,  Chen  M,  Bukharin  A,  He  P,  Cheng  Y,  Chen  W,  Zhao  T.\nAdaptive  budget  allocation  for  parameter-efficient  fine-tuning.  In:\nProceedings  of  the  11th  International  Conference  on  Learning\nRepresentations. 2023\n28.\n Hu  Y,  Xie  Y,  Wang  T,  Chen  M,  Pan  Z.  Structure-aware  low-rank\nadaptation  for  parameter-efficient  fine-tuning.  Mathematics,  2023,\n11(20): 4317\n29.\n Zhang  F,  Li  L,  Chen  J,  Jiang  Z,  Wang  B,  Qian  Y.  IncreLoRA:\nincremental parameter  allocation  method  for  parameter-efficient fine-\ntuning. 2023, arXiv preprint arXiv: 2308.12043\n30.\n Mao  Y,  Huang  K,  Guan  C,  Bao  G,  Mo  F,  Xu  J.  DoRA:  enhancing\nparameter-efficient  fine-tuning  with  dynamic  rank  distribution.  2024,\narXiv preprint arXiv: 2405.17357\n31.\nYuren MAO et al.    A survey on LoRA of large language models 13\n  Zhang R, Qiang R, Somayajula S A, Xie P. AutoLoRA: automatically\ntuning  matrix  ranks  in  low-rank  adaptation  based  on  meta  learning.\n2024, arXiv preprint arXiv: 2403.09113\n32.\n  Ding N, Lv X, Wang Q, Chen Y, Zhou B, Liu Z, Sun M. Sparse low-\nrank  adaptation  of  pre-trained  language  models.  In:  Proceedings  of\n2023  Conference  on  Empirical  Methods  in  Natural  Language\nProcessing. 2023, 4133−4145\n33.\n  Liu Z, Lyn J, Zhu W, Tian X, Graham Y. ALoRA: allocating low-rank\nadaptation for fine-tuning large language models. 2024, arXiv preprint\narXiv: 2403.16187\n34.\n  Valipour  M,  Rezagholizadeh  M,  Kobyzev  I,  Ghodsi  A.  DyLoRA:\nparameter-efficient tuning of pre-trained models using dynamic search-\nfree low-rank adaptation. In: Proceedings of the 17th Conference of the\nEuropean  Chapter  of  the  Association  for  Computational  Linguistics.\n2023, 3274−3287\n35.\n  Hayou  S,  Ghosh  N,  Yu  B.  The  impact  of  initialization  on  LoRA\nfinetuning dynamics. 2024, arXiv preprint arXiv: 2406.08447\n36.\n  Meng  F,  Wang  Z,  Zhang  M.  PiSSA:  principal  singular  values  and\nsingular  vectors  adaptation  of  large  language  models.  2024,  arXiv\npreprint arXiv: 2404.02948\n37.\n  Wang  H,  Xiao  Z,  Li  Y,  Wang  S,  Chen  G,  Chen  Y.  MiLoRA:\nharnessing  minor  singular  components  for  parameter-efficient  LLM\nfinetuning. 2024, arXiv preprint arXiv: 2406.09044\n38.\n  Zhang F, Pilanci M. Riemannian preconditioned LoRA for fine-tuning\nfoundation models. 2024, arXiv preprint arXiv: 2402.02347\n39.\n  Hayou  S,  Ghosh  N,  Yu  B.  LoRA+:  efficient  low  rank  adaptation  of\nlarge models. 2024, arXiv preprint arXiv: 2402.12354\n40.\n  Shi S, Huang S, Song M, Li Z, Zhang Z, Huang H, Wei F, Deng W,\nSun  F,  Zhang  Q.  ResLoRA:  identity  residual  mapping  in  low-rank\nadaption. 2024, arXiv preprint arXiv: 2402.18039\n41.\n  Wen  Z,  Zhang  J,  Fang  Y.  SIBO:  a  simple  booster  for  parameter-\nefficient fine-tuning. 2024, arXiv preprint arXiv: 2402.11896\n42.\n  Jin  F,  Liu  Y,  Tan  Y.  Derivative-free  optimization  for  low-rank\nadaptation  in  large  language  models.  2024,  arXiv  preprint  arXiv:\n2403.01754\n43.\n  Liu S Y, Wang C Y, Yin H, Molchanov P, Wang Y C F, Cheng K T,\nChen  M  H.  DoRA:  weight-decomposed  low-rank  adaptation.  2024,\narXiv preprint arXiv: 2402.09353\n44.\n  Qiang R, Zhang R, Xie P. BiLoRA: a bi-level optimization framework\nfor  overfitting-resilient  low-rank  adaptation  of  large  pre-trained\nmodels. 2024, arXiv preprint arXiv: 2403.13037\n45.\n  Lin Y, Ma X, Chu X, Jin Y, Yang Z, Wang Y, Mei H. LoRA dropout\nas  a  sparsity  regularizer  for  overfitting  control.  2024,  arXiv  preprint\narXiv: 2404.09610\n46.\n  Wang S, Chen L, Jiang J, Xue B, Kong L, Wu C. LoRA meets dropout\nunder a unified framework. 2024, arXiv preprint arXiv: 2403.00812\n47.\n  Yang  A  X,  Robeyns  M,  Wang  X,  Aitchison  L.  Bayesian  low-rank\nadaptation  for  large  language  models.  In:  Proceedings  of  the  12th\nInternational Conference on Learning Representations. 2024\n48.\n  Qi Z, Tan X, Shi S, Qu C, Xu Y, Qi Y. PILLOW: enhancing efficient\ninstruction fine-tuning via prompt matching. In: Proceedings of 2023\nConference  on  Empirical  Methods  in  Natural  Language  Processing:\nIndustry Track. 2023, 471−482\n49.\n  Zhang L, Wu J, Zhou D, Xu G. STAR: constraint LoRA with dynamic\nactive learning for data-efficient fine-tuning of large language models.\n2024, arXiv preprint arXiv: 2403.01165\n50.\n  Wang  X,  Aitchison  L,  Rudolph  M.  LoRA  ensembles  for  large\nlanguage model fine-tuning. 2023, arXiv preprint arXiv: 2310.00035\n51.\n  Zhao  Z,  Gan  L,  Wang  G,  Zhou  W,  Yang  H,  Kuang  K,  Wu  F.\nLoraRetriever: input-aware LoRA retrieval and composition for mixed\ntasks in the wild. 2024, arXiv preprint arXiv: 2402.09997\n52.\n Smith  J  S,  Cascante-Bonilla  P,  Arbelle  A,  Kim  D,  Panda  R,  Cox  D,\nYang  D,  Kira  Z,  Feris  R,  Karlinsky  L.  ConStruct-VL:  data-free\ncontinual  structured  VL  concepts  learning.  In:  Proceedings  of  the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition.\n2023, 14994−15004\n53.\n Sun Y, Li M, Cao Y, Wang K, Wang W, Zeng X, Zhao R. To be or not\nto be? An exploration of continuously controllable prompt engineering.\n2023, arXiv preprint arXiv: 2311.09773\n54.\n Zhang J, Chen S, Liu J, He J. Composing parameter-efficient modules\nwith arithmetic operations. 2023, arXiv preprint arXiv: 2306.14870\n55.\n Chitale R, Vaidya A, Kane A, Ghotkar A. Task arithmetic with LoRA\nfor continual learning. 2023, arXiv preprint arXiv: 2311.02428\n56.\n Belofsky J. Token-Level Adaptation of LoRA adapters for downstream\ntask  generalization.  In:  Proceedings  of  the  6th  Artificial  Intelligence\nand Cloud Computing Conference. 2023, 168−172\n57.\n Jiang  W,  Lin  B,  Shi  H,  Zhang  Y,  Li  Z,  Kwok  J  T.  Effective  and\nparameter-efficient  reusing  fine-tuned  models.  2023,  arXiv  preprint\narXiv: 2310.01886\n58.\n Asadi  N,  Beitollahi  M,  Khalil  Y,  Li  Y,  Zhang  G,  Chen  X.  Does\ncombining  parameter-efficient  modules  improve  few-shot  transfer\naccuracy? 2024, arXiv preprint arXiv: 2402.15414\n59.\n Huang C, Liu Q, Lin B Y, Pang T, Du C, Lin M. LoraHub: efficient\ncross-task generalization via dynamic LoRA composition. 2023, arXiv\npreprint arXiv: 2307.13269\n60.\n Yadav P, Choshen L, Raffel C, Bansal M. ComPEFT: compression for\ncommunicating  parameter  efficient  updates  via  sparsification  and\nquantization. 2023, arXiv preprint arXiv: 2311.13171\n61.\n Tang  A,  Shen  L,  Luo  Y,  Zhan  Y,  Hu  H,  Du  B,  Chen  Y,  Tao  D.\nParameter-efficient  multi-task  model  fusion  with  partial  linearization.\nIn:  Proceedings  of  the  12th  International  Conference  on  Learning\nRepresentations. 2024\n62.\n Shen  Y,  Xu  Z,  Wang  Q,  Cheng  Y,  Yin  W,  Huang  L.  Multimodal\ninstruction tuning with conditional mixture of LoRA. In: Proceedings\nof  the  62nd  Annual  Meeting  of  the  Association  for  Computational\nLinguistics (Volume 1: Long Papers). 2024, 637−648\n63.\n Buehler  E  L,  Buehler  M  J.  X-LoRA:  mixture  of  low-rank  adapter\nexperts,  a  flexible  framework  for  large  language  models  with\napplications in protein mechanics and molecular design. APL Machine\nLearning, 2024, 2(2): 026119\n64.\n Yang  S,  Ali  M  A,  Wang  C  L,  Hu  L,  Wang  D.  MoRAL:  MoE\naugmented  LoRA  for  LLMs’  lifelong  learning.  2024,  arXiv  preprint\narXiv: 2402.11260\n65.\n Dou S, Zhou E, Liu Y, Gao S, Zhao J, Shen W, Zhou Y, Xi Z, Wang\nX,  Fan  X,  Pu  S,  Zhu  J,  Zheng  R,  Gui  T,  Zhang  Q,  Huang  X.\nLoRAMoE:  alleviate  world  knowledge  forgetting  in  large  language\nmodels via MoE-style plugin. 2023, arXiv preprint arXiv: 2312.09979\n66.\n Gou Y, Liu Z, Chen K, Hong L, Xu H, Li A, Yeung D Y, Kwok J T,\nZhang  Y.  Mixture  of  cluster-conditional  LoRA  experts  for  vision-\nlanguage instruction tuning. 2023, arXiv preprint arXiv: 2312.12379\n67.\n Liu Q, Wu X, Zhao X, Zhu Y, Xu D, Tian F, Zheng Y. When MOE\nmeets  LLMs:  parameter  efficient  fine-tuning  for  multi-task  medical\napplications.  In:  Proceedings  of  the  47th  International  ACM  SIGIR\nConference  on  Research  and  Development  in  Information  Retrieval.\n2024, 1104−1114\n68.\n Feng  W,  Hao  C,  Zhang  Y,  Han  Y,  Wang  H.  Mixture-of-LoRAs:  an\nefficient  multitask  tuning  method  for  large  language  models.  In:\nProceedings of 2024 Joint International Conference on Computational\nLinguistics, Language Resources and Evaluation. 2024, 11371−11380\n69.\n Wang Y, Lin Y, Zeng X, Zhang G. MultiLoRA: democratizing LoRA\nfor better multi-task learning. 2023, arXiv preprint arXiv: 2311.11501\n70.\n Yang Y, Jiang P T, Hou Q, Zhang H, Chen J, Li B. Multi-task dense\nprediction via mixture of low-rank experts. 2024, arXiv preprint arXiv:\n71.\n14 Front. Comput. Sci., 2025, 19(7): 197605\n2403.17749\n  Agiza A, Neseem M, Reda S. MTLoRA: low-rank adaptation approach\nfor  efficient  multi-task  learning.  In:  Proceedings  of  the  IEEE/CVF\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR).\n2024, 16196−16205\n72.\n  Gao C, Chen K, Rao J, Sun B, Liu R, Peng D, Zhang Y, Guo X, Yang\nJ, Subrahmanian V S. Higher layers need more LoRA experts. 2024,\narXiv preprint arXiv: 2402.08562\n73.\n  Chen S, Jie Z, Ma L. LLaVA-MoLE: sparse mixture of LoRA experts\nfor  mitigating  data  conflicts  in  instruction  finetuning  MLLMs.  2024,\narXiv preprint arXiv: 2401.16160\n74.\n  Zhu Y, Wichers N, Lin C C, Wang X, Chen T, Shu L, Lu H, Liu C,\nLuo L, Chen J, Meng L. SiRA: sparse mixture of low rank adaptation.\n2023, arXiv preprint arXiv: 2311.09179\n75.\n  Chen Z, Wang Z, Wang Z, Liu H, Yin Z, Liu S, Sheng L, Ouyang W,\nQiao Y, Shao J. Octavius: mitigating task interference in MLLMs via\nMoE. 2023, arXiv preprint arXiv: 2311.02684\n76.\n  Wen  Y,  Chaudhuri  S.  Batched  low-rank  adaptation  of  foundation\nmodels.  In:  Proceedings  of  the  Twelfth  International  Conference  on\nLearning Representations. 2024\n77.\n  Wu T, Wang J, Zhao Z, Wong N. Mixture-of-Subspaces in Low-Rank\nAdaptation. 2024, arXiv preprint arXiv:2406.11909\n78.\n  Wu  Y,  Xiang  Y,  Huo  S,  Gong  Y,  Liang  P.  LoRA-SP:  streamlined\npartial parameter adaptation for resource efficient fine-tuning of large\nlanguage models. In: Proceedings of the 3rd International Conference\non Algorithms, Microchips, and Network Applications. 2024, 131711Z\n79.\n  Zhang L, Zhang L, Shi S, Chu X, Li B. LoRA-FA: memory-efficient\nlow-rank  adaptation  for  large  language  models  fine-tuning.  2023,\narXiv preprint arXiv: 2308.03303\n80.\n  Liu Z, Kundu S, Li A, Wan J, Jiang L, Beerel P A. AFLoRA: adaptive\nfreezing  of  low  rank  adaptation  in  parameter  efficient  fine-tuning  of\nlarge models. 2024, arXiv preprint arXiv: 2403.13269\n81.\n  Woo  S,  Park  B,  Kim  B,  Jo  M,  Kwon  S,  Jeon  D,  Lee  D.  DropBP:\naccelerating  fine-tuning  of  large  language  models  by  dropping\nbackward propagation. 2024, arXiv preprint arXiv: 2402.17812\n82.\n  Bałazy  K,  Banaei  M,  Aberer  K,  Tabor  J.  LoRA-XS:  low-rank\nadaptation  with  extremely  small  number  of  parameters.  2024,  arXiv\npreprint arXiv: 2405.17604\n83.\n  Zhou H, Lu X, Xu W, Zhu C, Zhao T, Yang M. LoRA-drop: efficient\nLoRA  parameter  pruning  based  on  output  evaluation.  2024,  arXiv\npreprint arXiv: 2402.07721\n84.\n  Zhang  M,  Chen  H,  Shen  C,  Yang  Z,  Ou  L,  Yu  X,  Zhuang  B.\nLoRAPrune:  structured  pruning  meets  low-rank  parameter-efficient\nfine-tuning.  In:  Proceedings  of  the  Findings  of  the  Association  for\nComputational Linguistics. 2024, 3013−3026\n85.\n  Chen T, Ding T, Yadav B, Zharkov I, Liang L. LoRAShear: efficient\nlarge  language  model  structured  pruning  and  knowledge  recovery.\n2023, arXiv preprint arXiv: 2310.18356\n86.\n  Zhu Y, Yang X, Wu Y, Zhang W. Parameter-efficient fine-tuning with\nlayer pruning on free-text sequence-to-sequence modeling. 2023, arXiv\npreprint arXiv: 2305.08285\n87.\n  Kopiczko  D  J,  Blankevoort  T,  Asano  Y  M.  VeRA:  vector-based\nrandom  matrix  adaptation.  In:  Proceedings  of  the  12th  International\nConference on Learning Representations. 2024\n88.\n  Li Y, Han S, Ji S. VB-LoRA: extreme parameter efficient fine-tuning\nwith vector banks. 2024, arXiv preprint arXiv: 2405.15179\n89.\n  Gao  Z,  Wang  Q,  Chen  A,  Liu  Z,  Wu  B,  Chen  L,  Li  J.  Parameter-\nefficient  fine-tuning  with  discrete  Fourier  transform.  2024,  arXiv\npreprint arXiv: 2405.03003\n90.\n  Dettmers  T,  Pagnoni  A,  Holtzman  A,  Zettlemoyer  L.  QLORA:\nefficient  finetuning  of  quantized  LLMs.  In:  Proceedings  of  the  37th\nInternational  Conference  on  Neural  Information  Processing  Systems.\n91.\n2023\n Xu Y, Xie L, Gu X, Chen X, Chang H, Zhang H, Chen Z, Zhang X,\nTian  Q.  QA-LoRA:  quantization-aware  low-rank  adaptation  of  large\nlanguage models. In: Proceedings of the 12th International Conference\non Learning Representations. 2024\n92.\n Li  Y,  Yu  Y,  Liang  C,  He  P,  Karampatziakis  N,  Chen  W,  Zhao  T.\nLoftQ:  LoRA-fine-tuning-aware  quantization  for  large  language\nmodels.  In:  Proceedings  of  the  12th  International  Conference  on\nLearning Representations. 2024\n93.\n Liao  B,  Herold  C,  Khadivi  S,  Monz  C.  ApiQ:  finetuning  of  2-bit\nquantized  large  language  model.  2024,  arXiv  preprint  arXiv:\n2402.05147\n94.\n Jeon H, Kim Y, Kim J J. L4Q: parameter efficient quantization-aware\ntraining  on  large  language  models  via  LoRA-wise  LSQ.  2024,  arXiv\npreprint arXiv: 2402.04902\n95.\n Ye Z, Li D, Tian J, Lan T, Zuo J, Duan L, Lu H, Jiang Y, Sha J, Zhang\nK,  Tang  M.  ASPEN:  high-throughput  LoRA  fine-tuning  of  large\nlanguage  models  with  a  single  GPU.  2023,  arXiv  preprint  arXiv:\n2312.02515\n96.\n Chen  L,  Ye  Z,  Wu  Y,  Zhuo  D,  Ceze  L,  Krishnamurthy  A.  Punica:\nmulti-tenant  LoRA  serving.  In:  Proceedings  of  the  Seventh  Annual\nConference on Machine Learning and Systems. 2024, 1−13\n97.\n Sheng  Y,  Cao  S,  Li  D,  Hooper  C,  Lee  N,  Yang  S,  Chou  C,  Zhu  B,\nZheng  L,  Keutzer  K,  Gonzalez  J  E,  Stoica  I.  S-LoRA:  serving\nthousands  of  concurrent  LoRA  adapters.  2023,  arXiv  preprint  arXiv:\n2311.03285\n98.\n Li S, Lu H, Wu T, Yu M, Weng Q, Chen X, Shan Y, Yuan B, Wang\nW.  CaraServe:  CPU-assisted  and  rank-aware  LoRA  serving  for\ngenerative LLM inference. 2024, arXiv preprint arXiv: 2401.11240\n99.\n Babakniya  S,  Elkordy  A  R,  Ezzeldin  Y  H,  Liu  Q,  Song  K  B,  El-\nKhamy M, Avestimehr S. SLoRA: federated parameter efficient fine-\ntuning of language models. 2023, arXiv preprint arXiv: 2308.06522\n100.\n Yan  Y,  Tang  S,  Shi  Z,  Yang  Q.  FeDeRA:  efficient  fine-tuning  of\nlanguage  models  in  federated  learning  leveraging  weight\ndecomposition. 2024, arXiv preprint arXiv: 2404.18848\n101.\n Sun  Y,  Li  Z,  Li  Y,  Ding  B.  Improving  LoRA  in  privacy-preserving\nfederated  learning.  In:  Proceedings  of  the  12th  International\nConference on Learning Representations. 2024\n102.\n Wu  P,  Li  K,  Wang  T,  Wang  F.  FedMS:  federated  learning  with\nmixture of sparsely activated foundations models. 2023, arXiv preprint\narXiv: 2312.15926\n103.\n Bai  J,  Chen  D,  Qian  B,  Yao  L,  Li  Y.  Federated  fine-tuning  of  large\nlanguage  models  under  heterogeneous  language  tasks  and  client\nresources. 2024, arXiv preprint arXiv: 2402.11505\n104.\n Cho Y J, Liu L, Xu Z, Fahrezi A, Barnes M, Joshi G. Heterogeneous\nLoRA  for  federated  fine-tuning  of  on-device  foundation  models.  In:\nProceedings  of  the  International  Workshop  on  Federated  Learning  in\nthe Age of Foundation Models in Conjunction with NeurIPS. 2023\n105.\n Yi L, Yu H, Wang G, Liu X, Li X. pFedLoRA: model-heterogeneous\npersonalized  federated  learning  with  LoRA  tuning.  2023,  arXiv\npreprint arXiv: 2310.13283\n106.\n Huang  W,  Wang  Y,  Cheng  A,  Zhou  A,  Yu  C,  Wang  L.  A  fast,\nperformant,  secure  distributed  training  framework  for  large  language\nmodel. 2024, arXiv preprint arXiv: 2401.09796\n107.\n Wang Y, Lin Y, Zeng X, Zhang G. PrivateLoRA for efficient privacy\npreserving LLM. 2023, arXiv preprint arXiv: 2311.14030\n108.\n Zhang  Y,  Wang  M,  Wu  Y,  Tiwari  P,  Li  Q,  Wang  B,  Qin  J.\nDialogueLLM:  context  and  emotion  knowledge-tuned  large  language\nmodels for emotion recognition in conversations. 2024, arXiv preprint\narXiv: 2310.11374\n109.\n Li  Z,  Li  X,  Liu  Y,  Xie  H,  Li  J,  Wang  F  L,  Li  Q,  Zhong  X.  Label\nsupervised  LLaMA  finetuning.  2023,  arXiv  preprint  arXiv:\n110.\nYuren MAO et al.    A survey on LoRA of large language models 15\n2310.01208\n  Bornheim T, Grieger N, Blaneck P G, Bialonski S. Speaker attribution\nin German parliamentary debates with QLoRA-adapted large language\nmodels. 2024, arXiv preprint arXiv: 2309.09902\n111.\n  Xue  L,  Zhang  D,  Dong  Y,  Tang  J.  AutoRE:  document-level  relation\nextraction  with  large  language  models.  2024,  arXiv  preprint  arXiv:\n2403.14888\n112.\n  Alves D M, Guerreiro N M, Alves J, Pombal J, Rei R, de Souza J G C,\nColombo  P,  Martins  A  F  T.  Steering  large  language  models  for\nmachine  translation  with  finetuning  and  in-context  learning.  In:\nProceedings  of  the  Findings  of  the  Association  for  Computational\nLinguistics. 2023, 11127−11148\n113.\n  Zheng  J,  Hong  H,  Wang  X,  Su  J,  Liang  Y,  Wu  S.  Fine-tuning  large\nlanguage models for domain-specific machine translation. 2024, arXiv\npreprint arXiv: 2402.15061\n114.\n  Mujadia  V,  Urlana  A,  Bhaskar  Y,  Pavani  P  A,  Shravya  K,\nKrishnamurthy  P,  Sharma  D  M.  Assessing  translation  capabilities  of\nlarge language models involving English and Indian languages. 2023,\narXiv preprint arXiv: 2311.09216\n115.\n  Zhang Y, Wang J, Yu L C, Xu D, Zhang X. Personalized LoRA for\nhuman-centered  text  understanding.  In:  Proceedings  of  the  Thirty-\nEighth  AAAI  Conference  on  Artificial  Intelligence.  2024,\n19588−19596\n116.\n  Liu Y, An C, Qiu X.  Y-tuning: an efficient tuning paradigm for large-\nscale pre-trained models via label representation learning. Frontiers of\nComputer Science, 2024, 18(4): 184320\n117.\n  Liu  S,  Keung  J,  Yang  Z,  Liu  F,  Zhou  Q,  Liao  Y.  Delving  into\nparameter-efficient  fine-tuning  in  code  change  learning:  an  empirical\nstudy.  In:  Proceedings  of  the  IEEE  International  Conference  on\nSoftware  Analysis,  Evolution  and  Reengineering  (SANER).  2024,\n465−476\n118.\n  Guo Y, Gao X, Jiang B. An empirical study on JIT defect prediction\nbased on BERT-style model. 2024, arXiv preprint arXiv: 2403.11158\n119.\n  Ayupov S, Chirkova N. Parameter-efficient finetuning of transformers\nfor source code. 2022, arXiv preprint arXiv: 2212.05901\n120.\n  Silva  A,  Fang  S,  Monperrus  M.  RepairLLaMA:  efficient\nrepresentations  and  fine-tuned  adapters  for  program  repair.  2023,\narXiv preprint arXiv: 2312.15698\n121.\n  Roberson R, Kaki G, Trivedi A. Analyzing the effectiveness of large\nlanguage models on text-to-SQL synthesis. 2024, arXiv preprint arXiv:\n2401.12379\n122.\n  Pan  J,  Sadé  A,  Kim  J,  Soriano  E,  Sole  G,  Flamant  S.  SteloCoder:  a\ndecoder-only  LLM  for  multi-language  to  python  code  translation.\n2023, arXiv preprint arXiv: 2310.15539\n123.\n  Sidahmed  H,  Phatale  S,  Hutcheson  A,  Lin  Z,  Chen  Z,  Yu  Z,  Jin  J,\nKomarytsia  R,  Ahlheim  C,  Zhu  Y,  Chaudhary  S,  Li  B,  Ganesh  S,\nByrne B, Hoffmann J, Mansoor H, Li W, Rastogi A, Dixon L. PERL:\nparameter  efficient  reinforcement  learning  from  human  feedback.\n2024, arXiv preprint arXiv: 2403.10704\n124.\n  Santacroce M, Lu Y, Yu H, Li Y, Shen Y. Efficient RLHF: reducing\nthe memory usage of PPO. 2023, arXiv preprint arXiv: 2309.00754\n125.\n  Sun S, Gupta D, Iyyer M. Exploring the impact of low-rank adaptation\non  the  performance,  efficiency,  and  regularization  of  RLHF.  2023,\narXiv preprint arXiv: 2309.09055\n126.\n  Quan S. DMoERM: recipes of mixture-of-experts for effective reward\nmodeling. 2024, arXiv preprint arXiv: 2403.01197\n127.\n  Zhang  S,  Chen  Z,  Chen  S,  Shen  Y,  Sun  Z,  Gan  C.  Improving\nreinforcement  learning  from  human  feedback  with  efficient  reward\nmodel ensemble. 2024, arXiv preprint arXiv: 2401.16635\n128.\n  Zhai  Y,  Zhang  H,  Lei  Y,  Yu  Y,  Xu  K,  Feng  D,  Ding  B,  Wang  H.\nUncertainty-penalized  reinforcement  learning  from  human  feedback\nwith  diverse  reward  LoRA  ensembles.  2023,  arXiv  preprint  arXiv:\n129.\n2401.00243\n Yang  A  X,  Robeyns  M,  Coste  T,  Shi  Z,  Wang  J,  Bou-Ammar  H,\nAitchison L. Bayesian reward models for LLM alignment. 2024, arXiv\npreprint arXiv: 2402.13210\n130.\n Daxberger E, Kristiadi A, Immer A, Eschenhagen R, Bauer M, Hennig\nP.  Laplace  redux-effortless  bayesian  deep  learning.  Advances  in\nNeural Information Processing Systems. 2021\n131.\n Tran H, Yang Z, Yao Z, Yu H. BioInstruct: instruction tuning of large\nlanguage  models  for  biomedical  natural  language  processing.  2023,\narXiv preprint arXiv: 2310.19975\n132.\n Gema  A  P,  Minervini  P,  Daines  L,  Hope  T,  Alex  B.  Parameter-\nefficient  fine-tuning  of  LLaMA  for  the  clinical  domain.  2023,  arXiv\npreprint arXiv: 2307.03042\n133.\n Toma  A,  Lawler  P  R,  Ba  J,  Krishnan  R  G,  Rubin  B  B,  Wang  B.\nClinical  camel:  an  open-source  expert-level  medical  language  model\nwith dialogue-based knowledge encoding. 2023, arXiv preprint arXiv:\n2305.12031\n134.\n Suri  K,  Mishra  P,  Saha  S,  Singh  A.  Suryakiran  at  MEDIQA-Sum\n2023:  leveraging  LoRA  for  clinical  dialogue  summarization.  In:\nProceedings of the Working Notes of the Conference and Labs of the\nEvaluation Forum. 2023, 1720−1735\n135.\n Ji  Y,  Yu  Z,  Wang  Y.  Assertion  detection  large  language  model  in-\ncontext  learning  LoRA  fine-tuning.  2024,  arXiv  preprint  arXiv:\n2401.17602\n136.\n Wang R, Duan Y, Lam C, Chen J, Xu J, Chen H, Liu X, Pang P C I,\nTan  T.  IvyGPT:  InteractiVe  Chinese  pathway  language  model  in\nmedical  domain.  In:  Proceedings  of  the  3rd  CAAI  International\nConference on Artificial Intelligence. 2024, 378−382\n137.\n Bhatti A, Parmar S, Lee S. SM70: a large language model for medical\ndevices. 2023, arXiv preprint arXiv: 2312.06974\n138.\n Konstantinidis T, Iacovides G, Xu M, Constantinides T G, Mandic D.\nFinLlama:  financial  sentiment  classification  for  algorithmic  trading\napplications. 2024, arXiv preprint arXiv: 2403.12285\n139.\n Pavlyshenko  B  M.  Financial  news  analytics  using  fine-tuned  llama  2\nGPT model. 2023, arXiv preprint arXiv: 2308.13032\n140.\n Liu X Y, Wang G, Yang H, Zha D. FinGPT: democratizing internet-\nscale  data  for  financial  large  language  models.  2023,  arXiv  preprint\narXiv: 2307.10485\n141.\n Li J, Lei Y, Bian Y, Cheng D, Ding Z, Jiang C. RA-CFGPT: Chinese\nfinancial  assistant  with  retrieval-augmented  large  language  model.\nFrontiers of Computer Science, 2024, 18(5): 185350\n142.\n Zhou X, Sun Z, Li G. DB-GPT: large language model meets database.\nData Science and Engineering, 2024, 9(1): 102−111\n143.\n Li S. DiffStyler: diffusion-based localized image style transfer. 2024,\narXiv preprint arXiv: 2403.18461\n144.\n Frenkel  Y,  Vinker  Y,  Shamir  A,  Cohen-Or  D.  Implicit  style-content\nseparation using B-LoRA. 2024, arXiv preprint arXiv: 2403.14572\n145.\n Liu Y, Yu C, Shang L, He Y, Wu Z, Wang X, Xu C, Xie H, Wang W,\nZhao Y, Zhu L, Cheng C, Chen W, Yao Y, Zhou W, Xu J, Wang Q,\nChen  Y,  Xie  X,  Sun  B.  FaceChain:  a  playground  for  human-centric\nartificial  intelligence  generated  content.  2023,  arXiv  preprint  arXiv:\n2308.14256\n146.\n Liao Q, Xia G, Wang Z. Calliffusion: Chinese calligraphy generation\nand style transfer with diffusion modeling. 2023, arXiv preprint arXiv:\n2305.19124\n147.\n Shrestha S, Sripada V S S, Venkataramanan A. Style transfer to Calvin\nand Hobbes comics using stable diffusion. 2023, arXiv preprint arXiv:\n2312.03993\n148.\n Li L, Zeng H, Yang C, Jia H, Xu D. Block-wise LoRA: revisiting fine-\ngrained  LoRA  for  effective  personalization  and  stylization  in  text-to-\nimage generation. 2024, arXiv preprint arXiv: 2403.07500\n149.\n Kong Z, Zhang Y, Yang T, Wang T, Zhang K, Wu B, Chen G, Liu W,150.\n16 Front. Comput. Sci., 2025, 19(7): 197605\nLuo  W.  OMG:  occlusion-friendly  personalized  multi-concept\nge\nneration  in  diffusion  models.  2024,  arXiv  preprint  arXiv:\n2403.10983\n  Shi  J,  Hua  H.  Space  narrative:  generating  images  and  3D  scenes  of\nChinese garden from text using deep learning. In: Proceedings of the\nxArch-Creativity  in  the  Age  of  Digital  Reproduction  Symposium.\n2024, 236−243\n151.\n  Jin  Z,  Song  Z.  Generating  coherent  comic  with  rich  story  using\nChatGPT and stable diffusion. 2023, arXiv preprint arXiv: 2305.11067\n152.\n  Wang  H,  Xiang  X,  Fan  Y,  Xue  J  H.  Customizing  360-degree\npanoramas through text-to-image diffusion models. In: Proceedings of\nthe  IEEE/CVF  Winter  Conference  on  Applications  of  Computer\nVision. 2024, 4921−4931\n153.\n  Guo J, Xu X, Pu Y, Ni Z, Wang C, Vasu M, Song S, Huang G, Shi H.\nSmooth  diffusion:  crafting  smooth  latent  spaces  in  diffusion  models.\n2023, arXiv preprint arXiv: 2312.04410\n154.\n  Cheng J, Xie P, Xia X, Li J, Wu J, Ren Y, Li H, Xiao X, Zheng M, Fu\nL.  ResAdapter:  domain  consistent  resolution  adapter  for  diffusion\nmodels. 2024, arXiv preprint arXiv: 2403.02084\n155.\n  Smith J S, Hsu Y C, Kira Z, Shen Y, Jin H. Continual diffusion with\nSTAMINA: STack-and-mask INcremental adapters. In: Proceedings of\nthe  IEEE/CVF  Conference  on  Computer  Vision  and  Pattern\nRecognition. 2024, 1744−1754\n156.\n  Sun J, Fu D, Hu Y, Wang S, Rassin R, Juan D C, Alon D, Herrmann\nC,  van  Steenkiste  S,  Krishna  R,  Rashtchian  C.  Dreamsync:  aligning\ntext-to-image  generation  with  image  understanding  feedback.  In:\nProceedings  of  the  Synthetic  Data  for  Computer  Vision  Workshop@\nCVPR 2024. 2023\n157.\n  Wang  Z,  Wang  X,  Xie  L,  Qi  Z,  Shan  Y,  Wang  W,  Luo  P.\nStyleAdapter:  a  single-pass  LoRA-free  model  for  stylized  image\ngeneration. 2023, arXiv preprint arXiv: 2309.01770\n158.\n  Gu  Y,  Wang  X,  Wu  J  Z,  Shi  Y,  Chen  Y,  Fan  Z,  Xiao  W,  Zhao  R,\nChang  S,  Wu  W,  Ge  Y,  Shan  Y,  Shou  M  Z.  Mix-of-show:\ndecentralized  low-rank  adaptation  for  multi-concept  customization  of\ndiffusion models. In: Proceedings of the 37th International Conference\non Neural Information Processing Systems. 2023\n159.\n  Luo S, Tan Y, Patil S, Gu D, von Platen P, Passos A, Huang L, Li J,\nZhao H. LCM-LoRA: a universal stable-diffusion acceleration module.\n2023, arXiv preprint arXiv: 2311.05556\n160.\n  Golnari P A. LoRA-enhanced distillation on guided diffusion models.\n2023, arXiv preprint arXiv: 2312.06899\n161.\n  Ren Y, Zhou Y, Yang J, Shi J, Liu D, Liu F, Kwon M, Shrivastava A.\nCustomize-A-video:  one-shot  motion  customization  of  text-to-video\ndiffusion models. 2024, arXiv preprint arXiv: 2402.14780\n162.\n  Deng  Y,  Wang  R,  Zhang  Y,  Tai  Y  W,  Tang  C  K.  DragVideo:\ninteractive  drag-style  video  editing.  2023,  arXiv  preprint  arXiv:\n2312.02216\n163.\n  Yang S, Zhou Y, Liu Z, Loy C C. Rerender A video: zero-shot text-\nguided video-to-video translation. In: Proceedings of the SIGGRAPH\nAsia 2023 Conference Papers. 2023, 95\n164.\n  Khandelwal A. InFusion: inject and attention fusion for multi concept\nzero-shot  text-based  video  editing.  In:  Proceedings  of  the  IEEE/CVF\nInternational  Conference  on  Computer  Vision  Workshops.  2023,\n3009−3018\n165.\n  Blattmann A, Dockhorn T, Kulal S, Mendelevitch D, Kilian M, Lorenz\nD,  Levi  Y,  English  Z,  Voleti  V,  Letts  A,  Jampani  V,  Rombach  R.\nStable  video  diffusion:  scaling  latent  video  diffusion  models  to  large\ndatasets. 2023, arXiv preprint arXiv: 2311.15127\n166.\n  Guo Y, Yang C, Rao A, Liang Z, Wang Y, Qiao Y, Agrawala M, Lin\nD,  Dai  B.  AnimateDiff:  animate  your  personalized  text-to-image\ndiffusion models without specific tuning. In: Proceedings of the 12th\nInternational Conference on Learning Representations. 2024\n167.\n Huang T, Zeng Y, Zhang Z, Xu W, Xu H, Xu S, Lau R W H, Zuo W.\nDreamControl: control-based text-to-3D generation with 3D self-prior.\n2023, arXiv preprint arXiv: 2312.06439\n168.\n Ma Y, Fan Y, Ji J, Wang H, Sun X, Jiang G, Shu A, Ji R. X-dreamer:\ncreating high-quality 3D content by bridging the domain gap between\ntext-to-2D  and  text-to-3D  generation.  2023,  arXiv  preprint  arXiv:\n2312.00085\n169.\n Yu K, Liu J, Feng M, Cui M, Xie X. Boosting3D: high-fidelity image-\nto-3D  by  boosting  2D  diffusion  prior  to  3D  prior  with  progressive\nlearning. 2023, arXiv preprint arXiv: 2311.13617\n170.\n Yoo  S,  Kim  K,  Kim  V  G,  Sung  M.  As-plausible-as-possible:\nplausibility-aware  mesh  deformation  using  2D  diffusion  priors.  In:\nProceedings  of  the  IEEE/CVF  Conference  on  Computer  Vision  and\nPattern Recognition. 2024, 4315−4324\n171.\n Zhang  Y,  Xu  Q,  Zhang  L.  DragTex:  generative  point-based  texture\nediting on 3D mesh. 2024, arXiv preprint arXiv: 2403.02217\n172.\n Ding  H,  Gao  J,  Yuan  Y,  Wang  Q.  SamLP:  a  customized  segment\nanything model for license plate detection. 2024, arXiv preprint arXiv:\n2401.06374\n173.\n Ye  Z,  Lovell  L,  Faramarzi  A,  Ninic  J.  SAM-based  instance\nsegmentation  models  for  the  automation  of  structural  damage\ndetection. 2024, arXiv preprint arXiv: 2401.15266\n174.\n Na  S,  Guo  Y,  Jiang  F,  Ma  H,  Huang  J.  Segment  any  cell:  a  SAM-\nbased auto-prompting fine-tuning framework for nuclei segmentation.\n2024, arXiv preprint arXiv: 2401.13220\n175.\n Chen  X,  Wang  C,  Ning  H,  Li  S,  Shen  M.  SAM-OCTA:  prompting\nsegment-anything for OCTA image segmentation. 2023, arXiv preprint\narXiv: 2310.07183\n176.\n Feng W, Zhu L, Yu L. Cheap lunch for medical image segmentation\nby  fine-tuning  SAM  on  few  exemplars.  2023,  arXiv  preprint  arXiv:\n2308.14133\n177.\n Zhang  K,  Liu  D.  Customized  segment  anything  model  for  medical\nimage segmentation. 2023, arXiv preprint arXiv: 2304.13785\n178.\n Wang  A,  Islam  M,  Xu  M,  Zhang  Y,  Ren  H.  SAM  meets  robotic\nsurgery:  an  empirical  study  on  generalization,  robustness  and\nadaptation. In: Proceedings of the International Conference on Medical\nImage  Computing  and  Computer-Assisted  Intervention.  2023,\n234−244\n179.\n Lin  L,  Fan  H,  Zhang  Z,  Wang  Y,  Xu  Y,  Ling  H.  Tracking  meets\nLoRA: faster training, larger model, stronger performance. 2024, arXiv\npreprint arXiv: 2403.05231\n180.\n Kong C, Li H, Wang S. Enhancing general face forgery detection via\nvision transformer with low-rank adaptation. In: Proceedings of the 6th\nInternational  Conference  on  Multimedia  Information  Processing  and\nRetrieval. 2023, 102−107\n181.\n Chen  Z,  Huang  H,  Andrusenko  A,  Hrinchuk  O,  Puvvada  K  C,  Li  J,\nGhosh  S,  Balam  J,  Ginsburg  B.  SALM:  speech-augmented  language\nmodel with in-context learning for speech recognition and translation.\nIn:  Proceedings  of  the  IEEE  International  Conference  on  Acoustics,\nSpeech and Signal Processing (ICASSP). 2024, 13521−13525\n182.\n Dong X, Zhang P, Zang Y, Cao Y, Wang B, Ouyang L, Wei X, Zhang\nS, Duan H, Cao M, Zhang W, Li Y, Yan H, Gao Y, Zhang X, Li W, Li\nJ,  Chen  K,  He  C,  Zhang  X,  Qiao  Y,  Lin  D,  Wang  J.  InternLM-\nXComposer2:  mastering  free-form  text-image  composition  and\ncomprehension  in  vision-language  large  model.  2024,  arXiv  preprint\narXiv: 2401.16420\n183.\n Ye Q, Xu H, Xu G, Ye J, Yan M, Zhou Y, Wang J, Hu A, Shi P, Shi\nY, Li C, Xu Y, Chen H, Tian J, Qian Q, Zhang J, Huang F, Zhou J.\nmPLUG-Owl:  modularization  empowers  large  language  models  with\nmultimodality. 2023, arXiv preprint arXiv: 2304.14178\n184.\n Lee  B  K,  Park  B,  Kim  C  W,  Ro  Y  M.  CoLLaVO:  crayon  large\nlanguage and vision mOdel. 2024, arXiv preprint arXiv: 2402.11248\n185.\nYuren MAO et al.    A survey on LoRA of large language models 17\n  Yeo  J  H,  Han  S,  Kim  M,  Ro  Y  M.  Where  visual  speech  meets\nlanguage: VSP-LLM framework for efficient and context-aware visual\nspeech processing. 2024, arXiv preprint arXiv: 2402.15151\n186.\n  Liu Z, Li S, Luo Y, Fei H, Cao Y, Kawaguchi K, Wang X, Chua T S.\nMolCA:  molecular  graph-language  modeling  with  cross-modal\nprojector and uni-modal adapter. In: Proceedings of 2023 Conference\non  Empirical  Methods  in  Natural  Language  Processing. 2023,\n15623−15638\n187.\n  Ren  Y,  Chen  Y,  Liu  S,  Wang  B,  Yu  H,  Cui  Z.  TPLLM:  a  traffic\nprediction  framework  based  on  pretrained  large  language  models.\n2024, arXiv preprint arXiv: 2403.02221\n188.\n  Aghajanyan  A,  Gupta  S,  Zettlemoyer  L.  Intrinsic  dimensionality\nexplains  the  effectiveness  of  language  model  fine-tuning.  In:\nProceedings  of  the  59th  Annual  Meeting  of  the  Association  for\nComputational Linguistics and the 11th International Joint Conference\non Natural Language Processing. 2021, 7319−7328\n189.\n  Fomenko V, Yu H, Lee J, Hsieh S, Chen W. A note on LoRA. 2024,\narXiv preprint arXiv: 2404.05086\n190.\n  Bershatsky  D,  Cherniuk  D,  Daulbaev  T,  Mikhalev  A,  Oseledets  I.\nLoTR: low tensor rank weight adaptation. 2024, arXiv preprint arXiv:\n2402.01376\n191.\n  Edalati A, Tahaei M, Kobyzev I, Nia V P, Clark J J, Rezagholizadeh\nM.  KronA:  parameter  efficient  tuning  with  kronecker  adapter.  2022,\narXiv preprint arXiv: 2212.10650\n192.\n  He X, Li C, Zhang P, Yang J, Wang X E. Parameter-efficient model\nadaptation for vision transformers. In: Proceedings of the 37th AAAI\nConference on Artificial Intelligence. 2023, 817−825\n193.\n  Zhao  Z,  Gan  L,  Wang  G,  Hu  Y,  Shen  T,  Yang  H,  Kuang  K,  Wu  F.\nRetrieval-augmented  mixture  of  lora  experts  for  uploadable  machine\nlearning. 2024 , arXiv preprint arXiv:2406.16989.\n194.\n  Mahabadi R K, Henderson J, Ruder S. COMPACTER: efficient low-\nrank  hypercomplex  adapter  layers.  In:  Proceedings  of  the  35th\nInternational  Conference  on  Neural  Information  Processing  Systems.\n2021, 79\n195.\n  Liao  B,  Meng  Y,  Monz  C.  Parameter-efficient  fine-tuning  without\nintroducing new latency. In: Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics. 2023, 4242−4260\n196.\n  Hendrycks  D,  Burns  C,  Basart  S,  Zou  A,  Mazeika  M,  Song  D,\nSteinhardt J. Measuring massive multitask language understanding. In:\nProceedings  of  the  9th  International  Conference  on  Learning\nRepresentations. 2021\n197.\n  He  J,  Zhou  C,  Ma  X,  Berg-Kirkpatrick  T,  Neubig  G.  Towards  a\nunified  view  of  parameter-efficient  transfer  learning.  In:  Proceedings\nof  the  10th  International  Conference  on  Learning  Representations.\n2022\n198.\n  Geshkovski B, Letrouit C, Polyanskiy Y, Rigollet P. A mathematical\nperspective on transformers. 2023, arXiv preprint arXiv: 2312.10794\n199.\n  Geshkovski B, Letrouit C, Polyanskiy Y, Rigollet P. The emergence of\nclusters  in  self-attention  dynamics.  In:  Proceedings  of  the  37th\nInternational  Conference  on  Neural  Information  Processing  Systems.\n2023\n200.\n  Sander M E, Ablin P, Blondel M, Peyré G. Sinkformers: transformers\nwith  doubly  stochastic  attention.  In:  Proceedings  of  the  25th\nInternational Conference on Artificial Intelligence and Statistics. 2022,\n3515−3530\n201.\n  Jacot A, Gabriel F, Hongler C. Neural tangent kernel: convergence and\ngeneralization  in  neural  networks.  In:  Proceedings  of  the  32nd\nInternational  Conference  on  Neural  Information  Processing  Systems.\n2018, 8580−8589\n202.\n  Touvron  H,  Martin  L,  Stone  K,  Albert  P,  Almahairi  A,  Babaei  Y,\nBashlykov  N,  Batra  S,  Bhargava  P,  Bhosale  S,  Bikel  D,  Blecher  L,\nCanton Ferrer C, Chen M, Cucurull G, Esiobu D, Fernandes J, Fu J, Fu\n203.\nW, Fuller B, Gao C, Goswami V, Goyal N, Hartshorn A, Hosseini S,\nHou\n R,  Inan  H,  Kardas  M,  Kerkez  V,  Khabsa  M,  Kloumann  I,\nKorenev A, Koura P S, Lachaux M A, Lavril T, Lee J, Liskovich D,\nLu Y, Mao Y, Martinet X, Mihaylov T, Mishra P, Molybog I, Nie Y,\nPoulton  A,  Reizenstein  J,  Rungta  R,  Saladi  K,  Schelten  A,  Silva  R,\nSmith E M, Subramanian R, Tan X E, Tang B, Taylor R, Williams A,\nKuan  J  X,  Xu  P,  Yan  Z,  Zarov  I,  Zhang  Y,  Fan  A,  Kambadur  M,\nNarang  S,  Rodriguez  A,  Stojnic  R,  Edunov  S,  Scialom  T.  Llama  2:\nopen  foundation  and  fine-tuned  chat  models.  2023,  arXiv  preprint\narXiv: 2307.09288\n Chang  Y,  Chang  Y,  Wu  Y.  Bias-Aware  Low-Rank  Adaptation:\nMitigating Catastrophic Inheritance of Large Language Models. 2024 ,\narXiv preprint arXiv:2408.04556\n204.\n Zhao J, Zhang Z, Chen B, Wang Z, Anandkumar A, Tian Y. Galore:\nmemory-efficient LLM training by gradient low-rank projection. 2024,\narXiv preprint arXiv: 2403.03507\n205.\n Biderman  D,  Ortiz  J  G,  Portes  J,  Paul  M,  Greengard  P,  Jennings  C,\nKing D, Havens S, Chiley V, Frankle J, Blakeney C, Cunningham J P.\nLoRA  learns  less  and  forgets  less.  2024,  arXiv  preprint  arXiv:\n2405.09673\n206.\n Han A, Li J, Huang W, Hong M, Takeda A, Jawanpuria P, Mishra B.\nSLTrain: a sparse plus low-rank approach for parameter and memory\nefficient pretraining. 2024, arXiv preprint arXiv: 2406.02214\n207.\n Sui Y, Yin M, Gong Y, Xiao J, Phan H, Yuan B. ELRT: efficient low-\nrank training for compact convolutional neural networks. 2024, arXiv\npreprint arXiv: 2401.10341\n208.\n Meng X, Dai D, Luo W, Yang Z, Wu S, Wang X, Wang P, Dong Q,\nChen  L,  Sui  Z.  PeriodicLoRA:  breaking  the  low-rank  bottleneck  in\nLoRA optimization. 2024, arXiv preprint arXiv: 2402.16141\n209.\n Frank  M,  Wolfe  P.  An  algorithm  for  quadratic  programming.  Naval\nResearch Logistics Quarterly, 1956, 3(1-2): 95−110\n210.\n Rajabzadeh  H,  Valipour  M,  Zhu  T,  Tahaei  M,  Kwon  HJ,  Ghodsi  A,\nChen  B,  Rezagholizadeh  M.  Qdylora:  Quantized  dynamic  low-rank\nadaptation  for  efficient  large  language  model  tuning.  2024  ,  arXiv\npreprint arXiv:2402.10462\n211.\n Elsken T, Metzen J H, Hutter F. Neural architecture search: a survey.\nThe Journal of Machine Learning Research, 2019, 20(1): 1997−2017\n212.\n Liu  Y,  Ott  M,  Goyal  N,  Du  J,  Joshi  M,  Chen  D,  Levy  O,  Lewis  M,\nZettlemoyer  L,  Stoyanov  V.  RoBERTa:  a  robustly  optimized  BERT\npretraining approach. 2019, arXiv preprint arXiv: 1907.11692\n213.\n Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S R. GLUE: a\nmulti-task  benchmark  and  analysis  platform  for  natural  language\nunderstanding.  In:  Proceedings  of  2018  EMNLP  Workshop\nBlackboxNLP:  Analyzing  and  Interpreting  Neural  Networks  for\nNLP. 2018, 353−355\n214.\n Renduchintala  A,  Konuk  T,  Kuchaiev  O.  Tied-LoRA:  enhancing\nparameter  efficiency  of  LoRA  with  weight  tying.  In:  Proceedings  of\n2024 Conference of the North American Chapter of the Association for\nComputational  Linguistics:  Human  Language  Technologies. 2024,\n8694−8705\n215.\n Hansen  N,  Ostermeier  A.  Adapting  arbitrary  normal  mutation\ndistributions in evolution strategies: the covariance matrix adaptation.\nIn: Proceedings of the IEEE International Conference on Evolutionary\nComputation. 1996, 312−317\n216.\n Ye  M,  Fang  X,  Du  B,  Yuen  P  C,  Tao  D.  Heterogeneous  federated\nlearning:  state-of-the-art  and  research  challenges.  ACM  Computing\nSurveys, 2024, 56(3): 79\n217.\n Liu  X  Y,  Zhu  R,  Zha  D,  Gao  J,  Zhong  S,  White  M,  Qiu  M.\nDifferentially  private  low-rank  adaptation  of  large  language  model\nusing federated learning. 2023, arXiv preprint arXiv: 2312.17493\n218.\n18 Front. Comput. Sci., 2025, 19(7): 197605\nYuren  Mao  received  his  PhD  degree  under  the\nsupe\nrvision  of  Prof.  Xuemin  Lin  in  computer\nscience  from  University  of  New  South  Wales,\nAustralia  in  2022.  He  is  currently  an  assistant\nprofessor  with  the  School  of  Software\nTechnology,  Zhejiang  University,  China.  His\ncurrent research interests include Large Language\nModels and its applications in Data Intelligence.\nYuhang  Ge  is  currently  working  toward  his  PhD\nde\ngree  in  the  School  of  Software  Technology  at\nZhejiang University, China. His research interests\ninclude  Large  Language  Models  and  Data\nManagement.\nYijiang  Fan  is  currently  studying  as  a  master’s\nst\nudent  in  the  School  of  Software  Technology  at\nZhejiang University, China. His research interests\ninclude Large Language Models and collaborative\ninference.\nWenyi  Xu  is  currently  studying  as  a  master’s\nst\nudent  in  the  School  of  Software  Technology  at\nZhejiang University, China. His research interests\ninclude Multimodal Large Models and RAG.\nYu Mi is currently studying as a master’s student\ni\nn the School of Software Technology at Zhejiang\nUniversity,  China.  Her  research  interests  include\nLarge Language Models and AI for science.\nZhonghao  Hu  is  currently  studying  as  a  master’s\nst\nudent  in  the  School  of  Software  Technology  at\nZhejiang University, China. His research interests\ninclude  Large  Language  Models  and  data\ndiscovery.\nYunjun Gao received the PhD degree in computer\nsc\nience from Zhejiang University, China, in 2008.\nHe  is  currently  a  professor  in  the  College  of\nComputer  Science  and  Technology,  Zhejiang\nUniversity,  China.  His  research  interests  include\nDatabase,  Big  Data  Management  and  Analytics,\nand AI interaction with DB technology.\n \n \n \n \n \n \n \nYuren MAO et al.    A survey on LoRA of large language models 19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.37281399965286255
    }
  ],
  "institutions": [],
  "cited_by": 38
}