{
    "title": "Searching for Efficient Multi-Stage Vision Transformers",
    "url": "https://openalex.org/W3198221608",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5058593712",
            "name": "Yi-Lun Liao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5081073767",
            "name": "Sertaç Karaman",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5066351389",
            "name": "Vivienne Sze",
            "affiliations": [
                "Massachusetts Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W2810075754",
        "https://openalex.org/W2594529350",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W2982083293",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W2944779197",
        "https://openalex.org/W3138796575",
        "https://openalex.org/W3035452548",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3173704789",
        "https://openalex.org/W2994749257",
        "https://openalex.org/W3139587317",
        "https://openalex.org/W2964259004",
        "https://openalex.org/W2962861284",
        "https://openalex.org/W2937843571",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2965658867",
        "https://openalex.org/W2952355681",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2947681860",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W3030728803",
        "https://openalex.org/W3035400692",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2963374479",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2997347790",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W3035251378",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3159885121",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3109946440",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W2736941579",
        "https://openalex.org/W3034609471",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3109154950",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3139445856",
        "https://openalex.org/W3167695527",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3153842237",
        "https://openalex.org/W2962746461",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W3170227631",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2967733054",
        "https://openalex.org/W2963918968",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2954769836",
        "https://openalex.org/W3168649818",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W3096533519",
        "https://openalex.org/W2912521296"
    ],
    "abstract": "Vision Transformer (ViT) demonstrates that Transformer for natural language processing can be applied to computer vision tasks and result in comparable performance to convolutional neural networks (CNN), which have been studied and adopted in computer vision for years. This naturally raises the question of how the performance of ViT can be advanced with design techniques of CNN. To this end, we propose to incorporate two techniques and present ViT-ResNAS, an efficient multi-stage ViT architecture designed with neural architecture search (NAS). First, we propose residual spatial reduction to decrease sequence lengths for deeper layers and utilize a multi-stage architecture. When reducing lengths, we add skip connections to improve performance and stabilize training deeper networks. Second, we propose weight-sharing NAS with multi-architectural sampling. We enlarge a network and utilize its sub-networks to define a search space. A super-network covering all sub-networks is then trained for fast evaluation of their performance. To efficiently train the super-network, we propose to sample and train multiple sub-networks with one forward-backward pass. After that, evolutionary search is performed to discover high-performance network architectures. Experiments on ImageNet demonstrate that ViT-ResNAS achieves better accuracy-MACs and accuracy-throughput trade-offs than the original DeiT and other strong baselines of ViT. Code is available at https://github.com/yilunliao/vit-search.",
    "full_text": "Searching for Efﬁcient Multi-Stage\nVision Transformers\nYi-Lun Liao, Sertac Karaman, Vivienne Sze\nMassachusetts Institute of Technology\n{ylliao, sertac, sze}@mit.edu\nAbstract\nVision Transformer (ViT) demonstrates that Transformer for natural language\nprocessing can be applied to computer vision tasks and result in comparable per-\nformance to convolutional neural networks (CNN), which have been studied and\nadopted in computer vision for years. This naturally raises the question of how the\nperformance of ViT can be advanced with design techniques of CNN. To this end,\nwe propose to incorporate two techniques and present ViT-ResNAS, an efﬁcient\nmulti-stage ViT architecture designed with neural architecture search (NAS). First,\nwe propose residual spatial reduction to decrease sequence lengths for deeper layers\nand utilize a multi-stage architecture. When reducing lengths, we add skip con-\nnections to improve performance and stabilize training deeper networks. Second,\nwe propose weight-sharing NAS with multi-architectural sampling. We enlarge\na network and utilize its sub-networks to deﬁne a search space. A super-network\ncovering all sub-networks is then trained for fast evaluation of their performance.\nTo efﬁciently train the super-network, we propose to sample and train multiple\nsub-networks with one forward-backward pass. After that, evolutionary search\nis performed to discover high-performance network architectures. Experiments\non ImageNet demonstrate that ViT-ResNAS achieves better accuracy-MACs and\naccuracy-throughput trade-offs than the original DeiT and other strong baselines of\nViT. Code is available athttps://github.com/yilunliao/vit-search.\n1 Introduction\nSelf-attention and Transformers [43], which originated from natural language processing (NLP), have\nbeen widely adopted in computer vision (CV) tasks, including image classiﬁcation [ 2, 20, 30, 35,\n49, 62], object detection [6, 35, 63], and semantic segmentation [46, 47]. Many works utilize hybrid\narchitectures and incorporate self-attention mechanisms into convolutional neural networks (CNN)\nto model long-range dependence and improve the performance of networks. On the other hand,\nVision Transformer (ViT) [10] demonstrates that a pure transformer without convolution can achieve\nimpressive performance on image classiﬁcation when trained on large datasets like JFT-300M [38].\nAdditionally, DeiT [41] shows that ViT can outperform CNN when trained on ImageNet [33] with\nstronger regularization. It is appealing to have powerful Transformers for CV tasks since it enables\nusing the same type of neural architecture for applications in both CV and NLP domains.\nA parallel line of research is to design efﬁcient neural networks with neural architecture search\n(NAS) [4, 5, 13, 19, 36, 37, 39, 45, 51, 54, 56, 57, 64, 65]. Pioneering works use reinforcement\nlearning to design efﬁcient CNN architectures. They sample many networks in a pre-deﬁned search\nspace and train them from scratch for a few epochs to approximate their performance, which requires\nexpensive computation. To accelerate the process, weight-sharing NAS has become popular. Instead\nof training individual networks in a search space, weight-sharing NAS trains a super-network whose\nweights are shared across all networks in the search space. Once the super-network is trained, we can\ndirectly use its weights to approximate the performance of different networks in the search space.\nThese methods successfully result in CNN architectures outperforming manually designed ones.\nPreprint. Under review.\narXiv:2109.00642v1  [cs.CV]  1 Sep 2021\n…\n…Reshaped to 2D feature map Zero pad\nAverage poolingPosition Embedding\nReshaped to a sequence \n…Position Embedding\nReshaped to a sequence \nLinear Projection of Flattened PatchesPatch EmbeddingClassification Token…Position Embedding\nTransfomer BlockStage 1×N1×N2…Transfomer BlockStage 2…Residual Spatial ReductionResidual Spatial Reduction…×N3Transfomer BlockStage 3\n…\n…MLP HeadClassSnowman\nEvaluate performance Mutate and crossoverUpdate Population…...…...…...Population of sub-networksTrained InitializeRepeatSearched network(Best sub-network)Population of sub-networks\nEvaluate performance Mutate and crossoverUpdate Population…...Super-networkSample multiple sub-networks with one forward-backward passPopulation of sub-networksTrained super-networkInitializeRepeat\n Sample for a batch of \nFigure 1: Architecture of ViT-Res. We propose residual spatial reduction (light orange) to reduce\nsequence length and increase embedding size for deeper blocks, which divides the network into\nseveral stages. Each stage has the same sequence length and embedding size and consists of several\ntransformer blocks. All stages except the ﬁrst one start with residual spatial reduction blocks.\nWhile CNN architectures have been studied and adopted in CV for years and optimized with NAS,\nrecently ViT demonstrates superior performance over CNN in some scenarios. Despite its promising\nperformance, ViT adopts the same architecture as Transformer for NLP [43]. This naturally leads\nto the question of how the performance of ViT can be further advanced with design techniques of\nCNN. Therefore, in this work, we propose to incorporate two design techniques of CNN, which are\nspatial reduction and neural architecture search, and present ViT-ResNAS, an efﬁcient multi-stage\nViT architecture with residual spatial reduction and designed with NAS.\nFirst, we propose residual spatial reduction to decrease sequence lengths and increase embedding\nsizes for deeper transformer blocks. As illustrated in Fig. 1, we transform the original single-stage\narchitecture into a multi-stage one, with each stage having the same sequence length and embedding\nsize. This architecture follows that of CNN, where the resolution of feature maps decreases and\nthe channel size increases for deeper layers. Additionally, we add skip connections when reducing\nsequence lengths, which can further improve performance and stabilize training deeper networks.\nViT with residual spatial reduction is named ViT-Res. Second, we propose weight-sharing neural\narchitecture search with multi-architectural sampling to improve the architecture of ViT-Res as\nshown in Fig. 2. We enlarge ViT-Res network by increasing its depth and width. Its sub-networks\nare utilized to deﬁne a search space. Then, a super-network covering all sub-networks is trained\nto directly evaluate their performance. For each training iteration and given a batch of examples,\nwe sample and train multiple sub-networks with one forward-backward pass to efﬁciently train\nthe super-network. Once the super-network is trained, evolutionary search [ 31, 32] is applied to\ndiscover high-performance ViT-ResNAS networks. Experiments on ImageNet [33] demonstrate the\neffectiveness of our proposed ViT-ResNAS. Compared to the original DeiT [41], ViT-ResNAS-Tiny\nachieves 8.6% higher accuracy than DeiT-Ti with slightly higher multiply-accumulate operations\n(MACs), and ViT-ResNAS-Small achieves similar accuracy to DeiT-B while having 6.3×less\nMACs and 3.7×higher throughput. Additionally, ViT-ResNAS achieves better accuracy-MACs and\naccuracy-throughput trade-offs than other strong baselines of ViT such as PVT [48] and PiT [17].\nOur main contributions are as follows: (1) We propose residual spatial reduction to improve the\nefﬁciency of ViT. (2) We propose weight-sharing NAS with multi-architectural sampling to improve\nViT with residual spatial reduction (ViT-Res). (3) Experiments on ImageNet demonstrate that our\nViT-ResNAS achieves comparable accuracy-MAC trade-offs to previous works.\n2 Method\nWe ﬁrst review the architecture of Vision Transformer (ViT) [10]. Then, we discuss residual spatial\nreduction and weight-sharing NAS with multi-architectural sampling to improve its architecture.\nOther extra techniques that help improve performance are presented as well.\n2.1 Background on Vision Transformer\nMain components are tokenization, position embedding, multi-head self-attention (MHSA), feed-\nforward network (FFN), and layer normalization (LN). MHSA and FFN form a transformer block.\nTokenization. The input to ViT is a ﬁxed-size image and is split into patches of pixels. Each patch\nis transformed into a vector of dimension dembed called patch embedding or patch token with a linear\nlayer. Thus, an image is viewed as a sequence of patch embeddings. To perform classiﬁcation, a\nlearnable vector called classiﬁcation token is appended to the sequence. After being processed with\ntransformer blocks, the feature captured by the classiﬁcation token is used to predict classes.\n2\n×N2\nock\nn Block\nn Block ×N3\nBlock Class\nSnowman\nEvaluate \nperformance \nMutate and \ncrossover\nUpdate \nPopulation\n…... …... …...\nSuper-network training\nSuper-network\nSample multiple sub-networks with \none forward-backward pass\nPopulation of sub-networks\nTrained \nsuper-network\nEvolutionary search Searched network training\nInitialize\nRepeat Searched network\n(Best sub-network)\nPopulation of sub-networks\nFigure 2: Algorithm ﬂow of NAS. First, we train a ViT-Res super-network with multi-architectural\nsampling. The performance of sub-networks can be directly evaluated using the super-network’s\ntrained weights without further training. Then, we perform evolutionary search to ﬁnd high-\nperformance sub-networks. Finally, the best sub-network becomes our searched network and is\ntrained from scratch to convergence.\nPosition Embedding. To preserve relative positions of image patches, position embeddings in\nthe form of pre-deﬁned or learnable vectors are added to patch embeddings. Then, joint patch and\nposition embeddings are processed with transformer blocks as shown on the left of Fig. 1.\nMHSA. The attention function in MHSA operates on a sequence of embeddings X ∈RN×dembed ,\nwith Nbeing sequence length and dembed being embedding dimension. It ﬁrst generates three vectors,\nkey, query and value, for each embedding with linear transformations, WK, WQ and WV , and packs\nthem into matrices K, Q, and V ∈RN×d. Then, for each query corresponding to an embedding,\nwe calculate its inner products with all N keys, divide each product by\n√\nd, and apply softmax\nfunction over all products to obtain N normalized weights. The output of the attention function for\nthe embedding is the weighted sum of N value vectors. The process is conducted for all embeddings\nand the attention function results in an output as follows:\nAttn(X,W K,WQ,WV ) =softmax(QKT\n√\nd\n)V, where K = XWK,Q = XWQ,V = XWV (1)\nOne MHSA contains hparallel attention functions or attention heads. The hdifferent outputs are\nconcatenated and projected with a linear transformation to produce the output of MHSA.\nFFN. It consists of two linear layers separated by GeLU [16]. The ﬁrst layer expands the dimension\nof each embedding to hidden size dhidden and the second projects the dimension back to dembed.\nLN. Layer normalization [1] normalizes each embedding xin the sequence Xseparately as follows:\nLN(x) =x−µ\nσ ◦γ+ β (2)\nµand σare the mean and standard deviation of xand the calculation is batch-independent.\n2.2 Residual Spatial Reduction\nViT maintains the same sequence length throughout the network, which is similar to maintaining\nthe same resolution of feature maps. In contrast, CNNs decrease the resolution of feature maps\nand increase the channel size for deeper layers. The former is based on the fact that there is\nspatial redundancy in feature maps, and the latter is to assign more channels to high-level features\nin deeper layers and to maintain similar computation when resolution is decreased. The design\ntechnique has been widely adopted in high-performance CNNs [15, 29], which motivates whether\nit can be introduced to improve the efﬁciency of ViT as well. To this end, we propose residual\nspatial reduction. As illustrated in Fig. 3, since there is spatial relationship in patch embeddings, we\nreshape the 1D sequence into a 2D feature map and then apply layer normalization [1] and strided\nconvolution (“Norm” and “Conv” in Fig. 3) to reduce the sequence length of patch embeddings\nand increase embedding dimension. Since the sequence length is changed, we update the relative\nposition information by adding new position embeddings. To maintain the same channel size of all\nembeddings, we apply layer normalization and a linear layer (“Norm” and “Linear” in Fig. 3) to the\nclassiﬁcation token. These constitute the residual branch.\n3\n×N1\n…\nReshaped to \na sequence\n Samp\nfor a b\nUpdate w\nfrom\n…\nZero pad\n…\nNorm\nLinear\nReshaped to \n2D feature map \nZero pad\nAverage \npooling\nNorm\nConv\nPosition \nEmbedding Reshaped to \na sequence \nPatch \nEmbedding\nClassification\nToken\nFigure 3: Structure of residual spatial reduc-\ntion. We use strided convolution to reduce se-\nquence length of patch embeddings. A skip con-\nnection as shown in the lower branch is added to\nstabilize training and improve performance.\nAlthough introducing only the residual branch\ncan signiﬁcantly improve the accuracy-MAC\ntrade-offs, training deeper networks can be un-\nstable. Speciﬁcally, under the training setting of\nDeiT-distill [41], using the residual branch signif-\nicantly improves the accuracy of DeiT-Tiny from\n74.5% to 79.6% with little increase in MACs.\nHowever, training loss can become “NaN” when\ntraining deeper networks like our ViT-Res super-\nnetworks. To remedy this, we introduce an extra\nskip connection [15] without any learnable op-\nerations as motivated by the residual structure\nof transformer blocks [43, 53]. We use 2D aver-\nage pooling to reduce sequence length (“Average\npooling” in Fig. 3) and concatenate embeddings\nwith zero tensors (“Zero pad” in Fig. 3) to in-\ncrease embedding dimension. The structure re-\nsults in the main branch and helps to stabilize\ntraining and improve performance.\nElastic Embedding DimensionFFN with Elastic Hidden SizeMHSA with Elastic Number of Attention HeadsViT-Res Super-NetworkTransformer Block\nElastic Number of Transformer Blocks\nFigure 4: Searchable dimensions in\neach stage of ViT-Res super-network.\nThe residual and main branches form residual spatial re-\nduction. We insert 2 residual spatial reduction blocks into\nViT with 12 blocks and divide the architecture evenly into\n3 stages. Following the design rule of ResNet [ 15], we\ndouble embedding dimension when halving spatial reso-\nlution or reducing sequence length of patch embeddings\nby 4×. The resulting ViT architecture is called ViT-Res\nas illustrated in Fig. 1. Since the spatial resolution is to\nbe reduced by 4×and input images are of size 224 ×224,\nthe patch size in tokenization before the ﬁrst stage is set to\n14, resulting in spatial resolution 16 (= 224\n14 ) and 16 ×16\npatch embeddings in the ﬁrst stage. After passing through\ntwo residual spatial reduction blocks, the sequence length\nof patch embeddings is reduced to 4 ×4. Please refer to\nAppendix A for further details.\n2.3 Weight-Sharing NAS with Multi-Architectural Sampling\nAnother design technique used in CNNs is neural architecture search (NAS). Due to its efﬁciency,\nwe adopt weight-sharing NAS to improve the archtiecture of ViT-Res in terms of designing better\narchitectural parameters like numbers of attention heads and transformer blocks as shown in Fig. 4.\nAlgorithm Overview. We enlarge ViT-Res network by increasing its depth and width. Sub-\nnetworks with smaller depths and widths deﬁne a search space. Then, a super-network covering all\nsub-networks is trained to directly evaluate their performance. For each training iteration and given a\nbatch of examples, we propose to sample multiple different sub-networks with one forward-backward\npass to efﬁciently train the super-network. After that, evolutionary search is applied to discover\narchitectures of high-performance sub-networks satisfying some resource constraints like MACs.\nFinally, the best sub-network evaluated becomes the searched network and is trained from scratch.\nSearch Space. We construct a large network by uniformly increasing depth and width of all stages\nof ViT-Res, and sub-networks contained in the large network deﬁne a search space. As shown in\nFig. 4, for each stage, we search embedding dimension and the number of transformer blocks. For\ndifferent blocks, we search different numbers of attention heads hin MHSA and different hidden\nsizes dhidden in FFN. The range of each searchable dimension is pre-deﬁned. During super-network\ntraining and evolutionary search, sub-networks of different conﬁgurations are sampled for training\nand evaluation. Details of search space are presented in Appendix B.\nMulti-Architectural Sampling for Super-Network Training. To evaluate the performance of\nsub-networks, their weights have to be optimized to reﬂect their relative quality. Therefore, we train a\nsuper-network whose architecture is the same as the largest network deﬁning the search space and\nwhose weights are shared across sub-networks. During super-network training, we sample and train\n4\n×N1×N2\n…Transfomer BlockStage 3…MLP Head Sample 1 sub-network for a batch of B images. Sample Na sub-networks for a batch of B images and train each sub-network with a sub-batch B/Na of images.Update weights with gradients from 1 sub-network.Update weights of the super-network with gradients from Na sub-networks with one forward-backward pass.\nFigure 5: Multi-architectural sampling for super-network training. Left: one sub-network is\nsampled and trained with one forward-backward pass given a batch of examples. Right: multi-\narchitectural sampling samples Na (= 3) different sub-networks with one forward-backward pass.\ndifferent sub-networks for different training iterations. Generally, the more sub-networks are sampled,\nthe more accurate the relative quality of sub-networks indicated by the trained super-network can\nbe. Previous works on NAS for CNNs [4, 13, 57] sample and train a single sub-network with one\nforward-backward pass for each training iteration. Given a ﬁxed amount of iterations, this, however,\nresults in room for improvement when training ViT-Res super-networks as we can samplemultiple\nsub-networks with one forward-backward pass for each training iteration as illustrated in Fig. 5.\nUnlike batch normalization [22] in CNNs, layer normalization (LN) [1] in ViT avoids normalizing\nalong batch dimension, which enables sampling different sub-networks with one forward-backward\npass without mixing their statistics. Speciﬁcally, for each training iteration, we sample Na sub-\nnetworks and divide a batch into Na sub-batches. Each sub-network is trained with its corresponding\nsub-batch. This can be achieved efﬁciently with one forward-backward pass and channel masking\n(ordered dropout) [44, 55, 57]. As shown in Fig. 6 (a), different masks are generated for different\nfeature maps corresponding to different examples in order to zero out different channels and simulate\nsampling different sub-networks. The shapes of feature maps remain the same, which maintains\nregular batch computation and therefore enables a single forward-backward pass.\nAdditionally, we re-calibrate the statistics in LN when sampling multiple sub-networks in order to\nprevent discrepancy between super-network training and standard training. As shown in Fig. 6 (b)\nand (c), LN can incorrectly normalize over a larger channel dimension when sampling networks with\nsmaller channel sizes. This is because channel masking only changes the values of feature maps not\nshapes. To avoid the problem, we propose masked layer normalization (MLN), which re-calibrates\nthe statistics of LN when its input is masked. Instead of only looking at the shape of input tensors,\nMLN calculates the ratio of the number of masked channels to the total number of channels and uses\nthat ratio to re-calibrate statistics as in Fig. 6 (d). With MLN, the statistics become the same as we\ntrain sub-networks separately as in Fig. 6 (b). Eventually, with channel masking and MLN, we can\nsample multiple sub-networks with one forward-backward pass, which improves sample efﬁciency\nwhen training ViT-Res super-networks and therefore the performance of searched networks. Other\ndetails of super-network training can be found in Appendix C.\nEvolutionary Search. Throughout the search process, we only sample sub-networks satisfying pre-\ndeﬁned resource constraints (e.g., MACs) and evaluate their performance (e.g., accuracy) with trained\nsuper-network weights. Evolutionary search maintains a population of networks and reﬁnes top-\nperforming ones for many iterations. We start with an initial population of P0 randomly sampled sub-\nnetworks. At every iteration, Nparent sub-networks with the highest performance in the population\nserve as parent networks that generate Nchild new sub-networks through mutation and crossover. For\nmutation, we randomly select one sub-network from parent networks and modify every architectural\nparameter with a pre-deﬁned probabilitypmutate. For crossover, we choose two random sub-networks\nfrom parent networks and randomly fuse every architectural parameter. Mutation and crossover\ngenerates the same amount of new sub-networks, and they are added to the population. The process is\nrepeated for Tsearch times, and the best sub-network in the population becomes the searched network\nand is trained from scratch to convergence.\n2.4 Extra Techniques\nWe present other techniques in training and architecture that help improve performance below.\n5\nSampling different sub-networks through masking (zeroing out) different channels for different examples in a batch.Super-networkSimulate SimulateChannel masking Channel zeroed outChannel unchanged Channel removed =Normalize over smaller channel dimension.Normalize over larger channel dimension.Layer Normalization (LN)=Masked Layer Normalization (MLN)(b)(c)(d)Compute the percentage of masked channels and re-scale statistics.(a)\nFigure 6: Channel masking and masked layer normalization for multi-architectural sampling.\n(a) We simulate sampling different sub-networks by masking different channels. (b) The channel\ndimension LN normalizes over when training a sub-network alone. (c) The channel dimension LN\nincorrectly normalizes over when sampling a sub-network during super-network training. (d) MLN\nre-calibrates statistics by considering the number of masked channels.\nToken Labeling with CutMix and Mixup. We incorporate token labeling [11, 23] and propose to\nimprove it with Mixup [60]. Token labeling provides labels for all patches in an input image. This\nenables training ViT to predict class labels of all patch tokens (patch embeddings) in addition to\npredicting class label of an input image with classiﬁcation token and can improve training Transform-\ners [8, 11, 23]. Token labeling generates an input image, its class label, and patch-wise class labels by\npatch-wise CutMix [59] described as follows. Given two images x1,x2 and their class labels y1,y2,\nwe divide each image into Kpatches, ﬂatten the patches, and associate each patch with its original\nimage-level class label. In this way, we have sequences of ﬂattened patches X1 = [x1\n1,...,x K\n1 ] and\nX2 = [x1\n2,...,x K\n2 ] and sequences of patch-wise labels Y1 = [y1,...,y 1] and Y2 = [y2,...,y 2]. Then,\nwe randomly generate a binary mask M to combine the images and associated patch-wise labels:\nX = X1 ⊙M+ X2 ⊙(1 −M) and Y = Y1 ⊙M+ Y2 ⊙(1 −M). We restructure the 1D sequence\nX into a 2D image x. The class label yassociated with xis obtained by combining the original class\nlabels: y= λy1 + (1−λ)y2 where λis the average of elements in the binary mask M. For ViT-Res,\nwe choose K = 4×4, which is equal to the sequence length of patch embeddings in the last stage.\nAdditionally, we ﬁnd that applying Mixup [60] along with token labeling can improve performance,\nwhich is contrary to previous results [ 11, 23]. We surmise that they ﬁrst apply Mixup and then\nperform patch-wise CutMix could lead to noisy training images and labels. In contrast, we improve\ntoken labeling with Mixup through switching between patch-wise CutMix and Mixup. Speciﬁcally,\nwe choose either patch-wise CutMix or Mixup to generate an image, its label and patch-wise labels.\nWhen the former is selected, we follow the practice as mentioned above. When the latter is chosen,\nthe image and its label are generated in the same way as Mixup, and this image-level label is assigned\nto all patches to produce the patch-wise labels.\nConvolution before Tokenization. Following previous works [10, 12, 23], we add some convolu-\ntional layers before tokenization. Speciﬁcally, we add three extra 3 ×3 convolutional layers, each\nwith C output channels. The stride of the ﬁrst layer is set to 2, and others are 1. A residual skip\nconnection is added between outputs of the ﬁrst and the third layers. The computation of these\nconvolutional layers is relatively low. Since the spatial resolution is reduced by 2×, the patch size in\ntokenization is set to 7 (= 14\n2 ). Further details can be found in Appendix A and B.\n3 Experiments\nIn this section, we ﬁrst describe the experiment setup. Then, we conduct experiments to study the\neffectiveness of our proposed methods. Finally, the comparison with related works is presented.\n3.1 Experiment Setup\nThe dataset used is ImageNet [ 33]. Our implementation is based on timm library [ 50] and that\nof DeiT [ 41]. Most of the training settings follow those in DeiT [ 41] except that we do not use\nrepeated augmentation [18]. We train models with 8 GPUs for 300 epochs, and batch size is 1024\nand input resolution is 224 ×224. As for token labeling, we adopt the same loss function as previous\nworks [11, 23], and the associated loss is directly added to the original classiﬁcation loss without any\nscaling. Unless otherwise stated, we incorporate token labeling with patch-wise CutMix and Mixup\n6\nMethods Top-1 Acc.\nSpatial reduction 78.5\nResidual spatial reduction 78.8 (+0.3)\n+ Token labeling with CutMix 79.6 (+0.8)\n+ Token labeling with CutMix & Mixup80.1 (+0.5)\nTable 1: Additive study of improving multi-stage network with residual connections and token\nlabeling. We start with DeiT-Tiny with spatial reduction and progressively introduce residual\nconnections and token labeling. Adding residual connections and combining token labeling with\nMixup in the proposed manner improve accuracy without increasing MACs.\nNumber of sampled sub-networks\nper forward-backward pass (Na)\nSingle-arch. Multi-arch.\n1 8 16 32\nTop-1 accuracy (%) 80.5 80.6\n(+0.1)\n80.8\n(+0.3)\n80.6\n(+0.1)\nTable 2: Effect of numbers of sampled sub-networks with one forward-backward pass (Na) on\nthe performance of searched networks. The type of resource constraint is MAC, and all searched\nnetworks have MAC around 1.8G. Empirically, Na = 16results in the best searched network.\nto train our networks and include several convolutional layers before tokenization. For ablation study\npresented in Section 3.2, when token labeling is used, drop path [21] rate is increased from 0.1 to 0.2.\nFor neural architecture search, we increase the depth and width of our ViT-Res network to build\nsearch spaces and search the architectures of ViT-ResNAS networks of three different sizes, which\nwe name Tiny, Small, and Medium. For ViT-ResNAS-Tiny, we enlarge ViT-Res-Tiny to build a\nsuper-network and train both the super-network and the searched network with drop path rate0.2. For\nViT-ResNAS-Small and Medium, we further enlarge ViT-Res-Tiny super-network and share the same\nsearch space. The super-network is trained with drop path rate 0.3. The drop path rates for Small\nand Medium are 0.3 and 0.4, respectively. The details of the search space are presented in Table B in\nappendix. A super-network is trained for 120 epochs, with other settings being the same as mentioned\nabove. We sample Na sub-networks with one forward-backward pass during super-network training,\nexperiment with different values of Na and empirically set Na to 16. For evolutionary search, the\nresource constraint is MAC. We set search iteration Tsearch = 20, the number of parent networks\nNparent = 75, the initial population size P0 = 500, the number of new sub-networks for each\niteration Nchild = 150and mutation probability pmutate = 0.3.\nIn addition, following DeiT [41], we ﬁne-tune networks at larger resolutions to obtain higher capacity.\nA network is ﬁne-tuned for 30 epochs, with batch size 512, learning rate 5 ×10−6, weight decay\n10−8 and drop path rate 0.75.\n3.2 Ablation Study\nMulti-Stage Network with Residual Connection and Token Labeling. We study how the per-\nformance of vanilla multi-stage networks can be enhanced with the proposed residual connections\nand improved token labeling training. We build such a network by starting with DeiT-Tiny network,\nadding three convolutional layers before tokenization and inserting two spatial reduction blocks\n(i.e., only the residual branch of residual spatial reduction). The results are shown in Table. 1.\nWithout token labeling, introducing only two residual connections (i.e., main branch of residual\nspatial reduction) can improve accuracy from 78.5% to 78.8% with negligible overhead. When token\nlabeling is used, incorporating Mixup in the proposed manner can further improve the accuracy by\n0.5%. With residual spatial reduction and token labeling, we can improve the accuracy by 1.6%\nwithout increasing MACs, and our ViT-Res-Tiny achieves80.1% top-1 accuracy with 1.8G MACs.\nWe also study the effectiveness of residual connections in the proposed residual spatial reduction\nunder another training setting. When using the training setting of DeiT-distill [41], which includes\nrepeated augmentation and distilling knowledge of CNN, residual connections can improve the\naccuracy from 79.6% to 80.1%. Moreover, when we train deeper networks such as our ViT-Res-Tiny\nsuper-network, without residual connections, the training can be unstable with “NaN” training loss.\nWeight-Sharing NAS with Multi-Architectural Sampling. With the proposed residual spatial\nreduction and token labeling, we study how weight-sharing NAS with multi-architectural sampling\n7\nMethod Model Size (M)MACs (G) Top-1\nAccuracy (%)\nThroughput\n(images/second)\nDeiT-Ti [41] 5 1.3 72.2 1968\nT2T-ViT-12 [58] 7 2.2 76.5 1192\nPiT-XS [17] 11 1.4 78.1 1647\nPVT-Tiny [48] 13 1.9 75.1 1133\nViL-Tiny [61] 7 1.3 76.3 754\nViT-Res-Tiny 43 1.8 80.1 1807\nViT-ResNAS-Tiny 41 1.8 80.8 1579\nDeiT-Small [41] 22 4.6 79.9 846\nT2T-ViT-14 [58] 22 5.2 81.5 682\nPiT-S [17] 24 2.9 80.9 986\nPVT-Small [48] 25 3.8 79.8 628\nPVT-Medium [48] 44 6.7 81.2 407\nTNT-S [14] 24 5.2 81.5 353\nSwin-T [26] 29 4.5 81.3 600\nTwins-PCPVT-S [7] 24 3.7 81.2 622\nViT-ResNAS-Small 65 2.8 81.7 1084\nDeiT-Base [41] 86 17.6 81.8 290\nT2T-ViT-19 [58] 39 8.9 81.9 428\nPiT-B [17] 74 12.5 82.0 316\nPVT-Large [48] 61 9.8 81.7 284\nViL-Small [61] 25 4.9 82.0 310\nCvT-13 [52] 20 4.5 81.6 587\nViT-ResNAS-Medium 97 4.5 82.4 751\nSwin-S [26] 50 8.7 83.0 351\nTwins-PCPVT-B [7] 44 6.4 82.7 403\nCvT-21 [52] 32 7.1 82.5 379\nViT-ResNAS-Medium↑280 97 7.1 83.1 467\nViL-Medium-Wide [61] 40 11.3 82.9 177\nTwins-PCPVT-L [7] 61 9.5 83.1 282\nCaiT-S36 [42] 68 13.9 83.3 191\nViT-ResNAS-Medium↑336 97 10.6 83.5 292\nSwin-B [26] 88 15.4 83.3 243\nCaiT-XS24↑384[42] 27 19.3 83.8 57\nViT-ResNAS-Medium↑392 97 15.2 83.8 194\nTable 3: Comparison with related works on ViT. “↑R” denotes that the model is ﬁrst trained\nat resolution 224 and then ﬁne-tuned at resolution R. Other models are trained at resolution 224.\nThroughput is measured on one Titan RTX GPU with batch size 128.\ncan further improve the performance of ViT-Res. During super-network training, we sample and train\nNa sub-networks with one forward-backward pass. Given the same amount of training iterations and\ntraining examples, the value of Na controls the trade-offs between sample efﬁciency (i.e., how many\nsub-networks are sampled) and the quality of training each sub-network (i.e., how many examples are\nused to train it). Therefore, instead of arbitrarily choosing a large value, we experiment with different\nNa. We use our ViT-Res-Tiny super-network to study the effect of differentNa on the performance\nof searched networks and search for networks with MACs around 1.8G. The results are presented\nin Table. 2. Compared to sampling one sub-network for each training iteration (“Single-arch.” in\nTable 2), sampling multiple sub-networks leads to better searched networks. Among different values,\nNa = 16empirically results in the best searched network. Please note that with NAS, the top-1\naccuracy is increased from 80.1% to 80.5% and that with the proposed multi-architectural sampling,\nthe accuracy is further increased from 80.5% to 80.8%. Based on the results, we choose Na = 16to\ndesign larger ViT-ResNAS networks.\n3.3 Comparison with Related Works\nWe design our ViT-ResNAS-Tiny, Small and Medium networks with NAS and MACs as our search\nmetric. Following DeiT [41], we ﬁne-tune our ViT-ResNAS-Medium at larger resolutions to obtain\nmodels with higher capacity. Since the patch size before the ﬁrst stage is 14 and there are 2 residual\nspatial reduction in the network, the spatial resolution is reduced by 56 (= 14×2 ×2) times in\nthe last stage, and therefore we can only increase the input resolution by multiples of 56. We also\nreport the performance of ﬁne-tuning ViT-ResNAS-Medium at resolutions280, 336 and 392. When\ncomparing different models, following the implementation of Swin Transformer [26], we measure\nthe inference throughput on a single Titan RTX GPU with batch size 128 as well.\n8\nTable 3 summarizes the comparison with previous works on ViT. Please note that why our models\nhave more parameters is that we further reduce the sequence length and increase the channel size\nof DeiT models. Even though we have more parameters, our ViT-ResNAS networks consistently\nachieves better accuracy-MACs trade-offs as well as accuracy-throughput trade-offs.\nCompared to the original single-stage DeiT [41], ViT-ResNAS-Tiny achieves8.6% higher accuracy\nthan DeiT-Ti while having only 0.5G higher MACs and slightly lower throughput and has 0.9%\nhigher accuracy than DeiT-Small with 2.5×less MACs and 1.9×higher throughput. ViT-ResNAS-\nSmall obtains the similar level of accuracy to DeiT-Base while having 6.3×lower MACs and\n3.7×higher throughput. Compared to other works on multi-stage architectures like PVT [ 48]\nand PiT [17], ViT-ResNAS consistently has better accuracy-MACs and accuracy-throughput trade-\noffs. The computation saving becomes more apparent as accuracy becomes higher. For example, in\ncomparison to PiT-S [17], ViT-ResNAS-Tiny achieves similar accuracy while having1.6×less MACs\nand 1.6×higher throughput. When compared with PiT-B [17], ViT-ResNAS-Medium achieves0.4%\nhigher accuracy with 2.8×less MACs and 2.4×higher throughput. This suggests the effectiveness\nof NAS to scale up models.\nAdditionally, for lower MACs around 2.0G, ViT-ResNAS-Tiny achieves better accuracy-MACs\ntrade-offs than ViL-Tiny. However, when MACs are around5.0G, ViT-ResNAS-Medium is on par\nwith ViL-Small in terms of accuracy-MACs trade-offs. This probably suggests that utilizing efﬁcient\nattention mechanisms [7, 26, 61] with convolution-like locality to process high-resolution features\nis necessary for models in higher MAC regimes to generalize better. Nevertheless, those methods\nare orthogonal to our approaches, and the proposed weight-sharing NAS with multi-architectural\nsampling could further improve their accuracy-MACs trade-offs as well.\n4 Related Works\nVision Transformers. Vision Transformer (ViT) [10] demonstrates that a pure transformer without\nconvolution can perform well on image classiﬁcation when trained on large datasets like JFT-\n300M [38]. To make it data-efﬁcient, DeiT [ 41] uses strong regularization and adds a distillation\ntoken to its architecture for knowledge distillation, and demonstrates comparable performance when\ntrained on ImageNet [33] only. Subsequent works improve the performance of ViT on ImageNet\nthrough either better training [ 11, 23] or architectures [ 9, 14, 17, 26, 27, 42, 48, 52, 58, 61]. For\nexample, they bring locality into network architectures by using convolutions [48, 52] or efﬁcient\nlocal attention [7, 14, 26, 61] or similarly adopt multi-stage architectures [17, 27, 48, 52, 61]. Our\nproposed methods are complementary to these works. Residual spatial reduction can derive a more\nefﬁcient multi-stage architectures, and weight-sharing NAS with multi-architectural sampling can be\napplied to further improve performance since they use layer normalization [1] as well.\nNeural Architecture Search. There have been increasingly interest in designing efﬁcient architec-\ntures with neural architecture search (NAS) [3, 4, 5, 13, 19, 24, 31, 32, 34, 36, 37, 39, 40, 45, 51, 54,\n55, 56, 57, 64, 65]. Among different methods, weight-sharing NAS [3, 4, 5, 13, 24, 25, 28, 36, 37, 45,\n51, 56, 57] has become popular due to efﬁciency. They train one over-parametrized super-network\nwhose weights are shared across all networks in a search space to conduct architecture search, which\nsaves computation cost signiﬁcantly. However, many of these works focus on CNN, which has been\nresearched for years, and have well-designed search space. In contrast, our proposed NAS with\nmulti-architectural sampling focuses on multi-stage ViT, which is much less studied, and we utilize\nits batch-independent property to further improve the performance of searched networks.\n5 Conclusion\nWe incorporate two design techniques of CNN, which are spatial reduction and NAS, into ViT and\npresent ViT-ResNAS, an efﬁcient multi-stage ViT designed with NAS. The proposed residual spatial\nreduction enhances accuracy-MACs trade-offs signiﬁcantly and the residual connections can improve\nperformance and stabilize training. Weight-sharing NAS with multi-architectural sampling trains\nsuper-networks more efﬁciently and results in better searched networks. Experiments on ImageNet\ndemonstrates the effectiveness of our methods and the efﬁciency of our ViT-ResNAS networks.\n9\nReferences\n[1] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V . Le. Attention augmented\nconvolutional networks. In International Conference on Computer Vision (ICCV), 2019.\n[3] Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, and Quoc V .\nLe. Can weight sharing outperform random architecture search? an investigation with tunas. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once for all: Train one network and\nspecialize it for efﬁcient deployment. In International Conference on Learning Representations (ICLR),\n2020.\n[5] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and\nhardware. In International Conference on Learning Representations (ICLR), 2019.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision\n(ECCV), 2020.\n[7] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint\narXiv:2104.13840, 2021.\n[8] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: Pre-training text\nencoders as discriminators rather than generators. InInternational Conference on Learning Representations\n(ICLR), 2020.\n[9] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations (ICLR), 2021.\n[11] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Improve vision transformers\ntraining by suppressing over-smoothing. arXiv preprint arXiv:2104.12753, 2021.\n[12] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and\nMatthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. arXiv preprint\narXiv:22104.01136, 2021.\n[13] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single\npath one-shot neural architecture search with uniform sampling. In European Conference on Computer\nVision (ECCV), 2020.\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n[16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[17] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\nRethinking spatial dimensions of vision transformers. arXiv preprint arXiv:2103.16302, 2021.\n[18] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your\nbatch: Improving generalization through instance repetition. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,\nYukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V . Le, and Hartwig Adam. Searching for mobilenetv3.\nIn International Conference on Computer Vision (ICCV), 2019.\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation networks. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 42(8):2011–2023, 2020.\n[21] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic\ndepth. In European Conference on Computer Vision (ECCV), 2016.\n10\n[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on Machine Learning (ICML), 2015.\n[23] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng. Token\nlabeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv\npreprint arXiv:2104.10858, 2021.\n[24] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun Chang.\nBossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search.\narXiv preprint arXiv:2103.12424, 2021.\n[25] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. InInternational\nConference on Learning Representations (ICLR), 2019.\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[27] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable visual transformers with\nhierarchical pooling. arXiv preprint arXiv:2103.10619, 2021.\n[28] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efﬁcient neural architecture search via\nparameters sharing. In International Conference on Machine Learning (ICML), 2018.\n[29] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Dollár. Designing\nnetwork design spaces. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[30] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019.\n[31] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V . Le. Regularized evolution for image classiﬁer\narchitecture search. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2019.\n[32] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V . Le,\nand Alexey Kurakin. Large-scale evolution of image classiﬁers. In International Conference on Machine\nLearning (ICML), 2017.\n[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252,\n2015.\n[34] David So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference on Machine\nLearning (ICML), 2019.\n[35] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021.\n[36] Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu,\nand Diana Marculescu. Single-path mobile automl: Efﬁcient convnet design and nas hyperparameter\noptimization. arXiv preprint arXiv:1907.00959, 2019.\n[37] Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu, and\nDiana Marculescu. Single-path nas: Designing hardware-efﬁcient convnets in less than 4 hours. arXiv\npreprint arXiv:1904.02877, 2019.\n[38] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness\nof data in deep learning era. In International Conference on Computer Vision (ICCV), 2017.\n[39] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V . Le. Mnasnet: Platform-aware\nneural architecture search for mobile. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\n[40] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching\nefﬁcient 3d architectures with sparse point-voxel convolution. InEuropean Conference on Computer Vision\n(ECCV), 2020.\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[42] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\nwith image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural Information Processing\n(NeurIPS), 2017.\n11\n[44] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, Matthew\nYu, Tao Xu, Kan Chen, Peter Vajda, and Joseph E. Gonzalez. Fbnetv2: Differentiable neural architecture\nsearch for spatial and channel dimensions. InIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\n[45] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, and Song Han. Hat:\nHardware-aware transformers for efﬁcient natural language processing. In Annual Conference of the\nAssociation for Computational Linguistics (ACL), 2020.\n[46] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end\npanoptic segmentation with mask transformers. arXiv preprint arXiv:2012.00759, 2020.\n[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on Computer\nVision (ECCV), 2020.\n[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\nIn International Conference on Computer Vision (ICCV), 2021.\n[49] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[50] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[51] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter\nVajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable\nneural architecture search. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2019.\n[52] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. InInternational\nConference on Machine Learning (ICML), 2020.\n[54] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and\nHartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In European\nConference on Computer Vision (ECCV), 2018.\n[55] Tien-Ju Yang, Yi-Lun Liao, and Vivienne Sze. Netadaptv2: Efﬁcient neural architecture search with fast\nsuper-network training and architecture optimization. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021.\n[56] Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot architecture search for channel numbers. arxiv\npreprint arxiv:1903.11728, 2019.\n[57] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas\nHuang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with\nbig single-stage models. In European Conference on Computer Vision (ECCV), 2020.\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:\nRegularization strategy to train strong classiﬁers with localizable features. In International Conference on\nComputer Vision (ICCV), 2019.\n[60] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. In International Conference on Learning Representations (ICLR), 2018.\n[61] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-\nscale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021.\n[62] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[63] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In International Conference on Learning Representations\n(ICLR), 2021.\n[64] Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In International\nConference on Learning Representations (ICLR), 2017.\n12\n[65] Barret Zoph, V . Vasudevan, Jonathon Shlens, and Quoc V . Le. Learning transferable architectures for\nscalable image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2018.\n13\nAppendix\nA Network Architecture of ViT-Res\nWe build our ViT-Res-Tiny network by introducing two modiﬁcations to DeiT-Tiny [ 41]. First,\nresidual spatial reduction (RSR) is applied to evenly divide the single-stage architecture into a multi-\nstage one as described in Sec. 2.2. For “Conv” in Fig. 3, we use3 ×3 convolutions to downsample\npatch embeddings. Second, we add three convolutional layers as mentioned in Sec. 2.4 and use\nC = 24. Table 4 summarizes the archtiecture of ViT-Res-Tiny.\nOutput Sequence\nLength ViT-Res-Tiny\nTokenization 16×16 + 1\nconv-3_24_2[conv-3_24_1\nconv-3_24_1\n]\nconv-7_192_7\nStage 1 16×16 + 1\n[MHSA-64_3\nFFN-768\n]\n×4\nStage 2\n8 ×8 + 1 RSR-384\n8 ×8 + 1\n[MHSA-64_6\nFFN-1536\n]\n×4\nStage 3\n4 ×4 + 1 RSR-768\n4 ×4 + 1\n[MHSA-64_12\nFFN-3072\n]\n×4\nTable 4: Architecture of ViT-Res-Tiny.“conv-k_C_s” stands for a k×kconvolutional layer with\noutput channel Cand stride s. “MHSA-d_h” is MHSA with head dimension dand hattention heads.\n“FFN-dhidden” is FFN with hidden size dhidden. “RSR-dembed” is residual spatial reduction with\noutput embedding dimension dembed. Note that the embedding dimension in Stage 1 is determined\nby the last convolutional layer in tokenization.\nB Search Space\nWe enlarge ViT-Res-Tiny to build ViT-Res-Tiny super-network. We increase numbers of attention\nheads hand decrease head dimensions dfor the ﬁrst two stage so that we have more choices of\nattention heads h. We do not search conﬁgurations of convolutional layers in tokenization except that\nwe search the number of output channels in the last layer, which determines the embedding dimension\nof the ﬁrst stage. For the search space for ViT-ResNAS-Tiny, each stage has three pairs of transformer\nblocks. The ﬁrst block in each pair always remains while the second one is skippable and can be\nremoved. Therefore, each stage can have three to six transformer blocks. Additionally, we further\nenlarge the ViT-Res-Tiny super-network to construct ViT-Res-Small and Medium super-network and\nsearch space by increasing width and adding one extra block for each stage. Table 5 summarizes the\nsearch spaces for ViT-ResNAS networks.\nC Additional Training Details\nSuper-Network Training. We divide the original training set into sub-train and sub-validation\nsets. The sub-validation set contains 25K images, with 25 images for each class. The rest of images\nform the sub-train set. We train super-networks on the sub-train set, and during evolutionary search,\nwe evaluate the accuracy of sub-networks on the sub-validation set.\nAdditionally, during super-network training, we warm up training different widths and depths (ﬁlter\nwarmup [3, 55] or progressive shrinking [4, 40]). Speciﬁcally, at the beginning, we only sample and\ntrain the largest sub-network in a search space. As the super-network training proceeds, we gradually\n14\nOutput\nLength\nViT-ResNAS-Tiny\nSearch Space\nToken.\nd1 ∈{256,224,192,176,160}\n16×16 + 1\nconv-3_24_2[conv-3_24_1\nconv-3_24_1\n]\nconv-7_d1_1_7\nStage 1\nh1 ∈{6,5,4,3},\nf1 ∈{768,704,640,576,512,448,384}\n16×16 + 1\n\n\n(MHSA-32_h1\nFFN-f1\n)\n((skippable)\nMHSA-32_h1\nFFN-f1\n)\n\n\n×3\nStage 2\nd2 ∈{512,448,384,352,320},\nh2 ∈{12,10,8,6},\nf2 ∈{1536,1408,1280,1152,1024,\n896,768}\n8×8 + 1 RSR-d2\n8×8 + 1\n\n\n(MHSA-48_h2\nFFN-f2\n)\n((skippable)\nMHSA-48_h2\nFFN-f2\n)\n\n\n×3\nStage 3\nd3 ∈{1024,896,768,704,640},\nh3 ∈{12,10,8,6},\nf3 ∈{3072,2816,2560,2304,2048,\n1792,1536}\n4×4 + 1 RSR-d3\n4×4 + 1\n\n\n(MHSA-64_h3\nFFN-f3\n)\n((skippable)\nMHSA-64_h3\nFFN-f3\n)\n\n\n×3\nOutput\nLength\nViT-ResNAS-Small and\nMedium Search Space\nToken.\nd1 ∈{320,280,240,220,200}\n16×16 + 1\nconv-3_24_2[conv-3_24_1\nconv-3_24_1\n]\nconv-7_d1_1_7\nStage 1\nh1 ∈{8,7,6,5},\nf1 ∈{960,880,800,720,640,560,480}\n16×16 + 1\n\n\n(MHSA-32_h1\nFFN-f1\n)\n((skippable)\nMHSA-32_h1\nFFN-f1\n)\n\n\n×3\n(MHSA-32_h1\nFFN-f1\n)\nStage 2\nd2 ∈{640,560,480,440,400},\nh2 ∈{16,14,12,10},\nf2 ∈{1920,1760,1600,1440,1280,\n1120,960}\n8×8 + 1 RSR-d2\n8×8 + 1\n\n\n(MHSA-48_h2\nFFN-f2\n)\n((skippable)\nMHSA-48_h2\nFFN-f2\n)\n\n\n×3\n(MHSA-48_h2\nFFN-f2\n)\nStage 3\nd3 ∈{1280,1120,960,880,800},\nh3 ∈{16,14,12,10},\nf3 ∈{3840,3520,3200,2880,2560,\n2240,1920}\n4×4 + 1 RSR-d3\n4×4 + 1\n\n\n(MHSA-64_h3\nFFN-f3\n)\n((skippable)\nMHSA-64_h3\nFFN-f3\n)\n\n\n×3\n(MHSA-64_h3\nFFN-f3\n)\nTable 5: ViT-ResNAS search space. Left: search space for ViT-ResNAS-Tiny.Right: search space\nfor ViT-ResNAS-Small and Medium. For each stagei, we search embedding dimension dembed and\nnumbers of transformer blocks. For each transformer block, we search the number of attention heads\nhin MHSA and hidden size dhidden in FFN. Different blocks can have different values of hand\ndhidden. The ﬁrst rows in tokenization and each stage deﬁne the range of each searchable dimension,\nwith di, hi, and fi corresponding to dembed, h, and dhidden in stage i, respectively. Transformer\nblocks with “skippable” can be removed during super-network training and evolutionary search,\nwhich supports different numbers of blocks in searched networks.\n15\nsample and train sub-networks with smaller widths and depths. After 25% of total training epochs,\nsub-networks with any width and depth can be sampled and trained.\nTraining Cost. We report the time for training networks and performing NAS when 8 V100 (16GB)\nGPUs are used. Training ViT-Res-Tiny takes about 32 hours. For ViT-ResNAS-Tiny, super-network\ntraining takes about 16.7 hours, evolutionary search takes 5.5 hours, and it takes about 34.5 hours to\ntrain the searched ViT-ResNAS-Tiny. For ViT-ResNAS-Small and Medium, the cost of training the\nshared super-network is 21 hours. For ViT-ResNAS-Small, the evolutionary search takes 6 hours and\ntraining the searched network takes 41.6 hours. For ViT-ResNAS-Medium, it takes 6 hours and 45\nhours to perform evolutionary search and training the searched network, respectively. Note that we\nreport the training time just for completeness and that the time can vary across different platforms.\nD Searched ViT-ResNAS Architecture\nWe report the searched architectures of ViT-ResNAS-Tiny, Small and Medium in Table 6, 7, and 8,\nrespectively. Compared to ViT-Res-Tiny, ViT-ResNAS-Tiny has smaller embedding dimensions\nbut more transformer blocks. From ViT-ResNAS-Tiny to Small, stage 1 has one extra block, and\nembedding dimensions are increased uniformly. As for ViT-ResNAS-Medium, the ﬁrst two stages\nhave more blocks than the last stage. These suggest that having more blocks in earlier stages is more\nimportant to scale up networks.\nE Limitation\nWe discuss some limitations of our approaches. First, the performance of searched networks designed\nwith NAS relies on manually designed search spaces. We build our search space by uniformly\nincreasing widths and depths of ViT-Res-Tiny, which is designed with some simple heuristics and\nwithout tuning. Compared to search spaces of CNN (e.g., MobileNet), our search space is less studied\nand less optimized. Optimizing search spaces such as setting a better range for each architectural\nparameter could potentially result in additional performance gain. Second, our searched networks use\nthe same type of attention mechanism as ViT, which could have large memory consumption when\nprocessing high-resolution feature maps (long sequences). Incorporating more efﬁcient attention\nmechanisms [7, 26, 61] into search spaces could solve the issue and result in networks with better\nperformance.\n16\nOutput\nLength ViT-ResNAS-Tiny\nTokenization 16×16 + 1\nconv-3_24_2[conv-3_24_1\nconv-3_24_1\n]\nconv-7_176_7\nStage 1 16×16 + 1\n(MHSA-32_3, FFN-704)\n(MHSA-32_3, FFN-576)\n(MHSA-32_3, FFN-640)\n(MHSA-32_4, FFN-576)\n(MHSA-32_4, FFN-704)\nStage 2\n8 ×8 + 1 RSR-352\n8 ×8 + 1\n(MHSA-48_10, FFN-1408)\n(MHSA-48_8, FFN-1408)\n(MHSA-48_8, FFN-1280)\n(MHSA-48_8, FFN-1408)\n(MHSA-48_10, FFN-1280)\n(MHSA-48_10, FFN-1024)\nStage 3\n4 ×4 + 1 RSR-704\n4 ×4 + 1\n(MHSA-64_10, FFN-2560)\n(MHSA-64_10, FFN-1792)\n(MHSA-64_10, FFN-2816)\n(MHSA-64_8, FFN-2816)\n(MHSA-64_8, FFN-2560)\nTable 6: Architecture of ViT-ResNAS-Tiny.\nOutput\nLength ViT-ResNAS-Small\nTokenization 16×16 + 1\nconv-3_24_2[conv-3_24_1\nconv-3_24_1\n]\nconv-7_220_7\nStage 1 16×16 + 1\n(MHSA-32_5, FFN-880)\n(MHSA-32_5, FFN-880)\n(MHSA-32_7, FFN-800)\n(MHSA-32_5, FFN-720)\n(MHSA-32_5, FFN-720)\n(MHSA-32_5, FFN-720)\nStage 2\n8 ×8 + 1 RSR-440\n8 ×8 + 1\n(MHSA-48_10, FFN-1760)\n(MHSA-48_10, FFN-1440)\n(MHSA-48_10, FFN-1920)\n(MHSA-48_10, FFN-1600)\n(MHSA-48_12, FFN-1600)\n(MHSA-48_12, FFN-1440)\nStage 3\n4 ×4 + 1 RSR-880\n4 ×4 + 1\n(MHSA-64_16, FFN-3200)\n(MHSA-64_12, FFN-3200)\n(MHSA-64_16, FFN-2880)\n(MHSA-64_12, FFN-2240)\n(MHSA-64_14, FFN-2560)\nTable 7: Architecture of ViT-ResNAS-Small.\n17\nOutput\nLength ViT-ResNAS-Medium\nTokenization 16×16 + 1\nconv-3_24_2[conv-3_24_1\nconv-3_24_1\n]\nconv-7_240_7\nStage 1 16×16 + 1\n(MHSA-32_7, FFN-960)\n(MHSA-32_6, FFN-960)\n(MHSA-32_7, FFN-800)\n(MHSA-32_8, FFN-960)\n(MHSA-32_7, FFN-880)\n(MHSA-32_8, FFN-880)\n(MHSA-32_6, FFN-800)\nStage 2\n8 ×8 + 1 RSR-640\n8 ×8 + 1\n(MHSA-48_10, FFN-1120)\n(MHSA-48_14, FFN-1760)\n(MHSA-48_14, FFN-1920)\n(MHSA-48_16, FFN-1760)\n(MHSA-48_14, FFN-1440)\n(MHSA-48_16, FFN-1760)\n(MHSA-48_16, FFN-1920)\nStage 3\n4 ×4 + 1 RSR-880\n4 ×4 + 1\n(MHSA-64_16, FFN-3200)\n(MHSA-64_10, FFN-3840)\n(MHSA-64_16, FFN-3840)\n(MHSA-64_12, FFN-3200)\n(MHSA-64_16, FFN-3520)\n(MHSA-64_14, FFN-3520)\nTable 8: Architecture of ViT-ResNAS-Medium.\n18"
}