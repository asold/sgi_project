{
  "title": "Transformer-Based Context Condensation for Boosting Feature Pyramids in Object Detection",
  "url": "https://openalex.org/W4381855289",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2073420235",
      "name": "Zhe Chen",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2107373440",
      "name": "Yufei Xu",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2073420235",
      "name": "Zhe Chen",
      "affiliations": [
        "UNSW Sydney",
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2087085944",
      "name": "Jing Zhang",
      "affiliations": [
        "UNSW Sydney",
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2107373440",
      "name": "Yufei Xu",
      "affiliations": [
        "UNSW Sydney",
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": [
        "UNSW Sydney",
        "University of Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2556833785",
    "https://openalex.org/W17540453",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2490270993",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W2606609115",
    "https://openalex.org/W2988396473",
    "https://openalex.org/W2895403383",
    "https://openalex.org/W3054755450",
    "https://openalex.org/W4313039766",
    "https://openalex.org/W2407521645",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W2964444661",
    "https://openalex.org/W1932624639",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2963549237",
    "https://openalex.org/W2962917547",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W3175630421",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2068868410",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2166761907",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3035296770",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4313136325",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2950703532",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2985228975",
    "https://openalex.org/W3175227919",
    "https://openalex.org/W2963323244",
    "https://openalex.org/W2530966705",
    "https://openalex.org/W56385144",
    "https://openalex.org/W4315705623",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2899607431",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W3098744844",
    "https://openalex.org/W3106250896"
  ],
  "abstract": "Abstract Current object detectors typically have a feature pyramid (FP) module for multi-level feature fusion (MFF) which aims to mitigate the gap between features from different levels and form a comprehensive object representation to achieve better detection performance. However, they usually require heavy cross-level connections or iterative refinement to obtain better MFF result, making them complicated in structure and inefficient in computation. To address these issues, we propose a novel and efficient context modeling mechanism that can help existing FPs deliver better MFF results while reducing the computational costs effectively. In particular, we introduce a novel insight that comprehensive contexts can be decomposed and condensed into two types of representations for higher efficiency. The two representations include a locally concentrated representation and a globally summarized representation, where the former focuses on extracting context cues from nearby areas while the latter extracts general contextual representations of the whole image scene as global context cues. By collecting the condensed contexts, we employ a Transformer decoder to investigate the relations between them and each local feature from the FP and then refine the MFF results accordingly. As a result, we obtain a simple and light-weight Transformer-based Context Condensation (TCC) module, which can boost various FPs and lower their computational costs simultaneously. Extensive experimental results on the challenging MS COCO dataset show that TCC is compatible to four representative FPs and consistently improves their detection accuracy by up to 7.8% in terms of average precision and reduce their complexities by up to around 20% in terms of GFLOPs, helping them achieve state-of-the-art performance more efficiently. Code will be released at https://github.com/zhechen/TCC .",
  "full_text": "International Journal of Computer Vision (2023) 131:2738–2756\nhttps://doi.org/10.1007/s11263-023-01830-w\nMANUSCRIPT\nTransformer-Based Context Condensation for Boosting Feature\nPyramids in Object Detection\nZhe Chen 1 · Jing Zhang 1 · Yufei Xu 1 · Dacheng Tao 1\nReceived: 14 July 2022 / Accepted: 22 May 2023 / Published online: 24 June 2023\n© Crown 2023\nAbstract\nCurrent object detectors typically have a feature pyramid (FP) module for multi-level feature fusion (MFF) which aims to\nmitigate the gap between features from different levels and form a comprehensive object representation to achieve better\ndetection performance. However, they usually require heavy cross-level connections or iterative reﬁnement to obtain better\nMFF result, making them complicated in structure and inefﬁcient in computation. To address these issues, we propose a\nnovel and efﬁcient context modeling mechanism that can help existing FPs deliver better MFF results while reducing the\ncomputational costs effectively. In particular, we introduce a novel insight that comprehensive contexts can be decomposed\nand condensed into two types of representations for higher efﬁciency. The two representations include a locally concentrated\nrepresentation and a globally summarized representation, where the former focuses on extracting context cues from nearby\nareas while the latter extracts general contextual representations of the whole image scene as global context cues. By collecting\nthe condensed contexts, we employ a Transformer decoder to investigate the relations between them and each local feature\nfrom the FP and then reﬁne the MFF results accordingly. As a result, we obtain a simple and light-weight Transformer-based\nContext Condensation (TCC) module, which can boost various FPs and lower their computational costs simultaneously.\nExtensive experimental results on the challenging MS COCO dataset show that TCC is compatible to four representative FPs\nand consistently improves their detection accuracy by up to 7.8% in terms of average precision and reduce their complexities\nby up to around 20% in terms of GFLOPs, helping them achieve state-of-the-art performance more efﬁciently. Code will be\nreleased at https://github.com/zhechen/TCC.\nKeywords Object detection · Feature pyramid · Context modeling\nMathematics Subject Classiﬁcation 35A01 · 65L10 · 65L12 · 65L20 · 65L70\nThis work was supported in part by Australian Research Council\nProjects IH-180100002 and FL-170100117.\nB Dacheng Tao\ndacheng.tao@gmail.com\nZhe Chen\nzhe.chen1@sydney.edu.au\nJing Zhang\njing.zhang1@sydney.edu.au\nYu f e i X u\nyuxu7116@uni.sydney.edu.au\n1 Faculty of Engineering, School of Computer Science, The\nUniversity of Sydney, Darlington, NSW 2008, Australia\n1 Introduction\nObject detection aims to detect and localize objects with var-\nious sizes on images. To detect objects of diversiﬁed sizes,\nresearchers have found that a feature pyramid (FP), which\ncollects features from different levels of the backbone net-\nwork and performs detection in a pyramidal hierarchy, are\nimportant for high-quality detection. Using a deep convolu-\ntional neural network (DCNN) like ResNet (He et al., 2016)\nwhich provides multiple levels of DCNN features as the\nbackbone network, the feature pyramid, such as the FPN\n(Lin et al., 2017a), detects smaller objects at shallower lev-\nels and larger objects at deeper levels. The shallower-level\nfeatures have higher resolutions and thus are better at describ-\ning objects with smaller sizes, but these features are worse\nat extracting abstract and robust representation. On the other\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2739\nhand, the deeper-level features have much smaller scales and\nare better for objects with larger sizes, while they lose too\nmany local details for detection. Although feature pyramids\ncan achieve more accurate detection by enabling the detec-\ntion of smaller objects on shallower-level features and larger\nobjects on deeper-level features, using either lower-level or\nhigher-level features alone still cannot satisfy high-quality\ndetection demand. In the related research, it shows that coop-\nerating the advantages of features from different levels by\nperforming multi-level feature fusion (MFF) is very helpful\nto obtain more descent representation of objects and achieve\nbetter detection performance.\nTo achieve effective MFF, gaps between features of dif-\nferent levels are supposed to be mitigated for better fusion.\nHowever, we found that current popular mechanisms usu-\nally require excessive computational complexity to ensure\nmore promising MFF results. For example, in the typical\nfeature pyramid as introduced in FPN (Lin et al., 2017a), the\nMFF is fulﬁlled by a normal 3 by 3 kernel based convolu-\ntion operation after adding each shallower-level feature with\nthe upsampled adjacent deeper-level feature. This design is\nsimple but very primitive. It can be quite difﬁcult for only\nu s i n ga3b y3c o n v o l u t i o nt oe f f e c t i v e l yb r i d g et h eg a p\nbetween different feature representations to reﬁne the ﬁnal\ndetection results promisingly. Nevertheless, the 3 by 3 con-\nvolution itself requires plenty of extra complexity, especially\nfor shallower levels with high resolutions. Derived from FPN,\nresearchers further developed more complicated and also\nmore costly architectures to help achieve better MFF. For\nexample, the authors of NasFPN (Ghiasi et al., 2019) applied\nan auto-network searching technology (Zoph & Le, 2017;\nZoph et al., 2018; Baker et al., 2016) to make the machine\nautomatically search for a better feature pyramid structure\nand better MFF strategy. The searched feature pyramid is\nthen iterated for multiple times in a successive order to further\nreﬁne the MFF results, which still brings considerable com-\nputational complexity. Later, the FPG (Chen et al., 2020) and\nRFP (Qiao et al., 2021) explored even more comprehensive\nand complicated MFF strategies in pyramid architectures.\nAlthough these methods achieved state-of-the-art perfor-\nmance, they continues to introduce multiple iterations of\nthe same processing structure to ensure more effective MFF\nin feature pyramids, making them still very computational\ncostly. Instead of repeated and exhaustive feature modeling,\nwe ﬁnd that contexts could be of great beneﬁt for reﬁning\nthe fused representation after MFF. More speciﬁcally, by\nconsidering extra visual cues from a larger surrounding area\nas contexts, it could become easier for shallower-level fea-\ntures to resist visual noises and for upsampled deeper-level\nfeatures to improve local detailed descriptions. As a result,\nthe gap between features from different levels could be alle-\nviated effectively with contexts. Some studies (Zhao et al.,\n2017; Chen et al., 2021) have demonstrated the effective-\nness of including contexts for improving detection and other\nvisual understanding tasks. However, a more comprehensive\ncontext modeling is usually still accompanied with a higher\ncomputational complexity, which would limit its advantages\nfor boosting feature pyramids efﬁciently.\nAlthough comprehensive context modeling may still be\ncostly, we further ﬁnd that the rich contexts can be decom-\nposed and simpliﬁed to avoid the need for heavy computation.\nWe are motivated by the studies (Uzkent & Ermon, 2020;\nZhang et al., 2014; Chen et al., 2019b) that successfully\nsummarized complicated visual appearances using a spe-\nciﬁc local representation and some sparse general contextual\nrepresentations. In those studies, the speciﬁc local represen-\ntation is used to represent local visual patterns in images,\nand the sparse general contextual representations summarize\noverall visual patterns into a few contextual features. The\nspeciﬁc local representation encodes more detailed visual\ninformation that is closely related to speciﬁc objects, describ-\ning visual patterns from surrounding areas of each object.\nFor example, the information from a round face, two arms,\nand two legs can form a speciﬁc representation of a human\nbody. Meanwhile, the sparse general contextual represen-\ntation describes the representative contextual feature of a\nlarge image area. It can provide general information and\nstrong hints about objects, but it may not directly relate to\na speciﬁc object. For example, only a few crosswalk lines\ncan provide strong hints about the presence of pedestrians\nor vehicles, thus these crosswalk lines can provide repre-\nsentative contextual cues for the entire image scene. Then,\nby making sparse general contextual representations tend to\ncomplement speciﬁc local representation based on the rela-\ntions between both types of representations, jointly using\nboth types of representations can avoid exhaustive feature\nprocessing without sacriﬁcing much visual understanding\nperformance. Accordingly, we propose that the rich contexts\ncan also be decomposed into a speciﬁc local representa-\ntion and some general contextual representations to achieve\nefﬁcient modeling. We tend to term such decomposition of\ncontexts as context condensation.\nTo condense and utilize contexts for boosting MFF and\nfeature pyramids efﬁciently, in this study, we propose a novel\nTransformer-based context condensation (TCC) module for\nobject detection. After adding features from different levels\nas fusion results, the TCC ﬁrst condenses rich contexts on\nthe fused representations and then use the condensed con-\ntexts to reﬁne the feature of each entry on the fused feature\nmap with the help of a Transformer decoder. More speciﬁ-\ncally, in a TCC module, we attempt to ﬁrst condense contexts\nby decomposing comprehensive context information into\na locally concentrated representation and a globally sum-\nmarized representation. We make the locally concentrated\nrepresentation provide speciﬁc local representation of local\ncontexts, and we attempt to summarize global contexts into a\n123\n2740 International Journal of Computer Vision (2023) 131:2738–2756\nFig. 1 The proposed Transformer-based Context Condenser (TCC)\n(dotted circles) can effectively improve the multi-level feature fusion in\ndifferent baseline feature pyramids (solid circles) and the ﬁnal detection\nperformance by utilising rich context information. In the meantime, the\nTCC also reduces the required computational complexity promisingly\nby condensing the contexts into two simpliﬁed representations, includ-\ning a locally concentrated representation and a globally summarized\nrepresentation\nfew general contextual features whose locations are predicted\nby an extra small neural network. Then, we collect the fea-\nture of locally concentrated representation and the features\nof globally summarized contexts as the condensed context\nrepresentation. With the condensed contexts, we apply a\nTransformer decoder to estimate the relations between the\ncondensed contexts and each feature to be reﬁned, so that\nmore related and useful context information can be identi-\nﬁed and highlighted from condensed contexts. Based on the\nestimated relations, the Transformer decoder translates the\nobtained condensed context features into a more appropriate\ncontextual representation to achieve better MFF reﬁnement.\nTherefore, using the condensed contexts, the exhaustive\nmodeling of comprehensive contexts on the feature map\ncan be reduced to the modeling of only a few context fea-\ntures. The TCC can thus achieve promising MFF reﬁnement\nperformance without introducing too large complexities. Fig-\nure 1 shows the performance improvement and complexity\nreduction of applying our proposed TCC on MFF of dif-\nferent typical feature pyramids. Despite being motivated by\nUzkent and Ermon ( 2020); Zhang et al. ( 2014); Chen et al.\n(2019b), we would like to clarify that our TCC is very differ-\nent from these studies. These studies mainly aim to augment\nDCNN features for image recognition tasks by extracting\ncomplementary information from the decomposed repre-\nsentations of images. In these studies, the complementary\ninformation is usually rough and primitive, thus requiring\ncarefully designed mechanisms to cooperate with the normal\nDCNN features. Besides, they lack explicit context model-\ning mechanisms for reasoning high-level relations between\neach local feature and its surrounding visual cues. This makes\nthem quite inappropriate for object detection which requires\ndescent local representations for dense predictions. Different\nfrom these studies, our method devises a novel simple context\ncollection mechanism that can learn to ﬁnd information that\nis beneﬁcial for improving each local feature accurately and\nefﬁciently. Using a high-level attention-based context fusion\nstrategy, our method delivers effective and explicit context\nmodeling for object detection.\nTo sum up, the major contributions of this work can be\nsummarized as follows:\n• A novel Transformer-based Context Condensation (TCC)\nmodule is presented to boost MFF in feature pyramids\nfor detecting object effectively and efﬁciently. It mainly\nhelps MFF achieve better detection performance without\nrequiring great computational complexities.\n• The proposed TCC module is easy-to-inject and can boost\nMFF in many different popular feature pyramid methods\ndeveloped for object detection. It is also compatible with\ndifferent backbone networks and is not sensitive to spe-\nciﬁc DCNN architectures. It can be trained with different\nfeature pyramids in an end-to-end manner.\n• Comprehensive empirical studies on MS COCO detec-\ntion dataset (Lin et al., 2014) show that our proposed\nTCC module can reduce the typical FPN method by more\nthan 30 GFlops while achieving around 7.8% relative\nimprovement. By applying the TCC module to state-of-\nthe-art feature pyramids like NasFPN (Ghiasi et al., 2019)\nand RFP (Qiao et al., 2021), our method consistently\nreduces complexity and improves detection performance,\naccessing compelling performance on the COCO dataset.\nCodes shall be released upon acceptance of this paper.\n2 Related Work\n2.1 Feature Pyramids for Object Detection\nIn computer vision, building pyramidal representations\nwhich describe visual appearances at different levels or reso-\nlutions have shown to be effective for tackling scale variation\nproblem (Chen et al., 2020). In modern deep learning-based\nobject detectors, feature pyramids have become a core com-\nponent. Early works like SSD (Liu et al., 2016) and MS-CNN\n(Cai et al., 2016) extract and utilize multi-level feature maps\nto perform detection, but they do not consider feature fusion\nor aggregation across different levels, which makes them\nsub-optimal for detection. Then, the most popular feature\npyramid for detection is introduced in the FPN (Lin et al.,\n2017a). It presents a simple multi-level feature extraction\nand fusion architecture, which consists of several top-down\npathways and lateral connections, to help distribute the detec-\ntion of objects with different sizes on features from different\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2741\nlevels of the backbone DCNN. Similar to the design logic\nof FPN, many methods (Kong et al., 2017; Huang et al.,\n2017; Liu et al., 2018; Sun et al., 2019; Y u et al., 2018; Zhao\net al., 2019) have been proposed to achieve more effective\npyramidal feature extraction by developing more comprehen-\nsive and more complicated multi-level feature extraction and\nfusion structures. In addition to these hand-crafted feature\npyramids, the work of NasFPN (Ghiasi et al., 2019) follows\nthe NAS algorithms (Zoph & Le, 2017; Zoph et al., 2018;\nBaker et al., 2016) which enables automatic searching for\nmore efﬁcient and more effective network architectures to\nfacilitate the automatic searching for a better feature pyra-\nmid for object detection. The NasFPN achieves promising\nimprovements over the FPN, but it requires several stacks of\nthe same feature pyramid structure to reﬁne the multi-level\nfeatures. Later, Auto-FPN (Xu et al., 2019) has introduced\na more comprehensive searching policy to improve detec-\ntion, but its searched architecture has different structures\nin different stacks, making it less scalable. More recently,\nresearchers have developed methods like FPG (Chen et al.,\n2020) and RFP (Qiao et al., 2021) to achieve more compre-\nhensive feature fusion in different levels of several stacks of\nfeature pyramids. Although state-of-the-art performance can\nbe accessed by these methods, they still introduce much extra\ncomputation complexity for object detection.\n2.2 Context Modeling for Object Detection\nVisual contexts have been demonstrated to be important\nfor visual understanding in both computer vision and cog-\nnitive science (Torralba, 2003; Biederman, 2017; Rumel-\nhart & McClelland, 1982). In particular, computer vision\nresearchers have introduced additional visual cues as useful\ncontextual information to facilitate the description of visual\nappearance at each particular location. In object detection,\ncontexts have been used for improving the representation\nof convolutional features extracted at different levels of the\nbackbone DCNN. For example, many methods (Gidaris &\nKomodakis, 2015; Komodakis & Gidaris, 2016) sampled\nextra windows with different scales around each region of\ninterest (RoI) to extract surrounding visual features that\ncan improve the later classiﬁcation and regression opera-\ntions in detectors. In addition to context windows, some\nother studies (Zeng et al., 2017; Chen & Gupta, 2017) suc-\ncessfully improved the features for describing objects of\ndifferent sizes by introducing recurrent neural networks to\nencode contextual information in larger areas. Authors of\nChen et al. ( 2021) further proposed a comprehensive and\ndynamic context modeling framework to improve multi-level\nfeature and boost object detection performance effectively.\nBesides, the non-local operation (Wang et al., 2018)s h o w s\ngreat beneﬁts for improving detection by modeling all the\nsub-sampled features extracted from input images. However,\ncurrent cutting-edge context modeling methods would usu-\nally introduce excessive computational complexity to model\ncontexts more comprehensively, making it difﬁcult to apply\ncurrent context modeling methods to augment multi-level\nfeature fusion in feature pyramids for object detection.\n2.3 Decomposition of Feature Representation\nResearchers have found that complicated visual patterns can\nbe decomposed into different types of simpliﬁed representa-\ntions. For example, Zhang et al. ( 2014) introduced part-based\nrepresentations to complement the holistic representation for\nﬁne-grained classiﬁcation. It relied on randomly sampled\nobject proposals and mainly extracts information from raw\nimage patches without explicitly considering relationships\nbetween patches. However, the random proposals may not be\nquite accurate, and the information extraction mechanism in\nthis study is complicated. Compared to this study, our method\nuses an attention-based structure to help the network learn\nby itself to ﬁnd precise locations of summarized global con-\ntexts with a more sensible and efﬁcient mechanism to explore\nthe relationships between different types of context features.\nThe authors of Uzkent and Ermon ( 2020) used part-global\nrepresentation to learn when and where to zoom images to\nfacilitate recognition. It investigated how to complement low-\nresolution representation with high-resolution image content\nfor better image recognition and remote sensing. To utilize\nhigh-resolution images with feasible complexities, this study\nattempted to search for better local areas to focus on. How-\never, since their method is based on global information from\nmulti-resolution which is difﬁcult to guide the focus, they\napplied a reinforcement learning mechanism to fulﬁll the\nsearch of focus areas, which is complicated and costly for\ntraining. Alternative to this study, our method devises an\neffective method that can also learn to search for mean-\ningful locations to summarize global contexts into a few\nfeatures but with a novel, simple and efﬁcient mechanism.\nAnother related study (Chen et al., 2019b) attempted to\nimprove the common convolution operations by using low-\nfrequency information to complement the high-frequency\nfeatures and thus avoid heavy computation costs. The authors\nfactorized the mixed feature maps by their frequencies and\nrepresented image features with information from low and\nhigh spatial frequency domains. This method is effective for\nimage and video recognition tasks, but it extracts and fuses\nlow-frequency features in a primitive way, without modeling\nlarger surrounding areas as contexts. This method also lacks\nthe modeling of high-level relationships between features\nat different locations. Instead, our proposed method explic-\nitly explores contexts from a large area and introduces an\nadvanced attention modeling mechanism to achieve effective\nfeature fusion. In general, we found in these studies that the\ncorrelation between the general and speciﬁc features lacks\n123\n2742 International Journal of Computer Vision (2023) 131:2738–2756\nsufﬁcient study. We argue that the correlation between dif-\nferent types of simpliﬁed features is important to achieve\npromising performance. In particular, the level of correla-\ntion between different features can indicate their relevance\nand correlation reveal whether involved features share similar\nand complementary information that is of interest. Although\nresearchers have developed cosine similarity-based (Wojke\n&B e w l e y ,2018), Euclidean distance-based (Qi et al., 2017),\nIntersect-over-Union (IoU) based (Chen et al., 2018), and\nalso learning-based (Hu et al., 2018; Chen & Gupta, 2017)\ncorrelation estimation methods for various vision tasks, these\nmethods either cannot be directly applied on the general and\nspeciﬁc features or could be difﬁcult to describe complicated\nrelations. Instead, inspired by the Transformers for vision\n(Dosovitskiy et al., 2020; Liu et al., 2021; Xu et al., 2021)\nthat show to be extremely powerful at modeling complicated\nrelations between features, we propose a Transformer-based\ncontext encoder to model speciﬁc features and learn to better\ncomplement the general features.\n2.4 Transformer for Object Detection\nRecently, the Transformer has achieved great success in\ncomputer vision (Dosovitskiy et al., 2021; Touvron et al.,\n2021; Wang et al., 2021a). Researchers also developed a\nTransformer-based object detector, named DETR (Carion\net al., 2020). It effectively takes advantage of the powerful\nrelation modeling in Transformer for high-quality detection.\nHowever, the pure Transformer-based detector generally suf-\nfers from excessively long training periods. To address this\nproblem, many researchers found that locality modeling is\nimportant for accelerating the training of DETR (Tay et al.,\n2020; Xu et al., 2021; Zhang et al., 2022). For example, the\nDeformable DETR (Zhu et al., 2020) applied deformable\noperations (Dai et al., 2017) to better focus on a few local\nareas in the Transformer. The method SMCA (Gao et al.,\n2021) introduced multi-scale co-attention to improve DETR\nwith reﬁned local representations. In addition, other stud-\nies like Conditional DETR (Meng et al., 2021) and Anchor\nDETR (Wang et al., 2021b) tend to improve the spatial\nembedding in Transformer to help accelerate training. These\ntwo methods enhance the locality modeling of Transformer\nby making attention focus on potentially valuable areas on\nthe image learned with positional embeddings. The authors\nof Chen et al. ( 2022) introduces recurrent reﬁnement to\nimprove the locality modeling of DETR. Despite progress,\nthe Transformer-based methods require a long training period\nto achieve promising performance, while our method aligns\nwith normal DCNN-based object detectors and requires\nshorter training periods to converge.\nDifferent from DETR methods that focus on the detection\ntask, there is another group of Transformer methods that tend\nto extract tend to summarize complicated features into a uni-\nﬁed latent space, including Perceiver (Jaegle et al., 2021b),\nPerceiver IO (Jaegle et al., 2021a), Flamingo (Alayrac et al.,\n2022), and so on. The Perceiver (Jaegle et al., 2021b)i sa\nTransformer-based method that is designed for scaling to\narbitrary inputs. Using an asymmetric attention mechanism,\nit can process inputs into a tight latent bottleneck representa-\ntion. By extracting information from arbitrary inputs, it can\nachieve effective image classiﬁcation. Rather than the Per-\nceiver that only focuses on classiﬁcation, Perceiver IO (Jaegle\net al., 2021a) introduces a uniﬁed visual language model that\ncan produce arbitrary outputs in addition to varied inputs.\nThe Perceiver IO can produce language, optical ﬂow, and\nmultimodal videos with audio. Using the same structure as\nthe Perceiver, the Perceiver IO is linear in the input and out-\nput size. Besides the Perceiver IO, the Flamingo (Alayrac et\nal., 2022) is another Perceiver-based method that focuses on\nvisual language modeling for visual understanding tasks like\nvisual question answering. It extracts information from arbi-\ntrarily interleaved text and image information and introduces\na Perceiver resampler from varying-size large feature maps\nto a few visual tokens, so that complicated visual inputs can\nbe encoded and decoded to a few desired outputs. It performs\nespecially well on few-shot learning tasks.\nThese great works have introduced impressive perfor-\nmance in visual-language understanding. Their idea of\nextracting only a few important information from an arbi-\ntrarily shaped input is similar to our globally summarized\ncontextual representations. However, there are a few major\ndifferences. For example, we have different information\nextraction methods - the Perceiver-based methods mainly use\na few queries and cross-attention to scale to any input, while\nour global summarized representation collects general con-\ntextual features based on max pooling and importance scores.\nOur method can be more efﬁcient at extracting informative\nglobal contexts by ignoring the massive areas with low impor-\ntance scores. In addition, the Perceiver-based methods rely\non multiple iterative attention layers to help encode compli-\ncated input information into a uniﬁed latent space, while our\nmethod only needs to extract general contextual features only\nonce, making our method more light-weighted for processing\nmassive input information.\n3 Method\nAs mentioned previously, we introduce Transformer contexts\ncondensation (TCC) modules to reﬁne MFF and boost differ-\nent feature pyramids for object detection without introducing\ntoo much computational overhead. In the TCC, condensed\ncontexts are ﬁrst collected and then a Transformer decoder is\nemployed to translate the collected contexts into a better rep-\nresentation for reﬁning features. Figure 2 shows an overview\nof the proposed method.\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2743\nFig. 2 Processing pipeline of the proposed Transformer-based Context\nCondensation (TCC) module for boosting multi-level feature fusion. In\nthe ﬁgure, the purple cross indicates a local entry on the fusion results\nat level i, the dots with different colors indicate predicted locations of\nglobally summarized context features, and dashed deep purple rectangu-\nlar represents locally concentrated context feature. Solid arrows, dotted\narrows, dashed arrows, and light grey arrows represent convolutions,\nresizing operations, and feature gathering operations, respectively\nIn the following sections, we will subsequently describe\nin details about our formulation of MFF operation in a fea-\nture pyramid, how we collect condensed contexts to improve\nMFF in the feature pyramids, and how we use Transformer\nto decode the collected contexts.\n3.1 MFF in a Feature Pyramid\nIn general, feature pyramids used in object detectors usu-\nally have the following processing steps, i.e. collecting\nmulti-level features from the backbone DCNN, re-sampling\nfeatures from another level to match the scale of features\nat the current level, and the MFF that fuses and reﬁnes the\ncollected multi-level features to deliver descent feature rep-\nresentation for detection at the current level.\nMathematically, we use the symbol f\ni to represent the i-\nth level feature extracted from the backbone DCNN and use\nthe symbol S j→i [ f j ] represents a re-sampling function that\nresizes the scale of the j-th level feature f j into the scale of\nfi . Then, we can summarize the MFF of the i-th level in the\nfeature pyramids as the following operation:\nˆfi = gr\n(\nfi +\n∑\nj∈N (i) S j→i [ f j ]\n)\n, (1)\nwhere N (i) refers to the other levels considered for fusion at\nlevel i, gr refers to the feature reﬁnement function used in dif-\nferent methods, and ˆfi is the obtained MFF result for level i\nin the feature pyramid. For example, in the typical FPN (Lin\net al., 2017a), N (i) is deﬁned as its adjacent deeper level,\ni.e. N (i) = i + 1. Meanwhile, the FPN employs a simple 3\nby 3 convolution to implement gr for reﬁning fused features.\nSome methods like FPG (Chen et al., 2020) and RFP (Qiao et\nal., 2021) have introduced more complicated mechanisms to\nimprove gr , while they usually requires several repeats of the\nsame feature processing procedure. In practice, these meth-\nods could introduce considerably large extra computational\ncomplexity for object detectors.\n3.2 TCC\nAlternative to existing MFF mechanisms in feature pyramids,\nwe introduce a Transformer-based context condensation\n(TCC) module to implement g\nr in Eq. 1 for improving the fea-\nture representation obtained after fusion through addition. As\nmentioned previously, the TCC ﬁrst collect condensed con-\ntexts and then decode them for better reﬁnement. As a result,\nthe general mathematical description about TCC re-writes\nthe Eq. 1 as follows:\n{\nˆf\ni = Trans (q = ˜fi ;k/v = gcc( ˜fi )),\n˜fi = fi + ∑\nj∈N (i) S j→i [ f j ], (2)\nwhere Trans refers to a Transformer decoder module, the\nq,k,v represent the query, key, and value of the Transformer,\nand gcc refers to the collection of condensed contexts.\n3.2.1 Condensed Context Collection\nTo take advantage of rich contexts without introducing exces-\nsive computational complexity, we propose to condense\ncontexts by decomposing them into a locally concentrated\nrepresentation and a globally summarized representation that\nconsists of a few general contextual features. As mentioned\npreviously, we make the locally concentrated representation\ndescribe a local speciﬁc description about nearby contexts\nand make the globally summarized representation delivers\nsimpliﬁed descriptions about informative cues in the whole\nscene. Accordingly, we can formulate the condensed context\ncollection as:\ng\ncc( ˜fi ) =[ ˜f lr\ni , ˜f gr\ni ], (3)\nwhere [·] represents a feature concatenation operation, ˜f lr\ni\nand ˜f gr\ni refer to the locally concentrated representation and\nthe globally summarized representation, respectively.\n123\n2744 International Journal of Computer Vision (2023) 131:2738–2756\nTo extract the locally concentrated representation as local\ncontexts, we can simply use a dilated convolution operation.\nHowever, different from common dilated convolutions, here\nwe only use a very small channel number and a small dilation\nrate to describe local appearances efﬁciently. Considering\nthat the ˜f\nlr\ni is the speciﬁc local representation for i-th level,\nwe have the following formulation:\n˜f lr\ni = DilatedCon v( ˜fi ), (4)\nwhere DilatedCon v means a dilated convolution operation.\nTo avoid excessive computational costs, the used channel\nnumber is calculated by 8 ×2i and the dilation rate is empir-\nically set as 2.\nTo extract the globally summarized representation, we\nattempt to use a few general contextual features to summa-\nrize and represent useful information from the whole scene.\nWe hypothesize that the objects in the same visual scene on\nan input image usually share information on similar global\ncontextual information. For example, when detecting a per-\nson and sheep in the same image, the sky or the meadow\ncan facilitate the recognition of both types of objects in this\nimage. That is, there can be strong statistical dependence\nbetween objects and global scene environments, making\nP(person |sky ,meado w) and P(sheep |sky ,meado w) rel-\natively high in a natural image. Our experiments can validate\nthat these shared general contextual features can improve the\noverall detection performance effectively.\nTo identify the general contextual features from the scene,\nwe tend to make networks automatically learn to discover\nwhich contextual features are more useful. In particular, we\nmainly perform a two-step extraction procedure: we ﬁrst\nemploy a small network to predict the locations of these\ngeneral contextual features, and then we gather these fea-\ntures according to the predicted locations as the summarized\nglobal contexts. Recall that ˜f\ngr\ni represents the desired glob-\nally summarized representation for the i-th level. Given a\nlocation set Pi ={ (x, y)i,1,...,( x, y)i,n } describing the\npredicted locations of general contextual features, we then\nhave the summarized global contexts according to:\n˜f gr\ni = /Phi1(˜fi , Pi ) (5)\nwhere /Phi1(˜fi , Pi ) is the function that gathers features from\ninput feature map ˜fi at the locations deﬁned in the set Pi .\nThe function /Phi1is further deﬁned as:\n/Phi1(˜fi , Pi ) =[ φ( ˜fi ,( x, y)i,1), . . . , φ(˜fi ,( x, y)i,k )}], (6)\nwhere φ( ˜fi ,( x, y)i,k ) is to collect detailed visual feature at\nthe location of (x, y)i,k .\nTo obtain an appropriate location set Pi for collecting gen-\neral contextual features, we make networks learn to predict\nthe location set automatically. To achieve this, we introduce\nimportance scores for locations on the feature map and build\nthe location set by collecting locations with the maximum\nimportance scores. Suppose we collect n general contex-\ntual features, then we make the importance scores have n\ndimensions (or n elements for each location on the feature\nmap). Further suppose importance scores are denoted by\nC\ni ={ ci,k | k = 1,2,..., n} where ci,k represents the k-\nth importance score map. In a TCC module, we compute Ci\nusing convolutions according to:\nCi = Split (Con v( ˜fi ),n), (7)\nwhere Con v represents a 1x1 convolution operation with a\nchannel number of n, and Split (·) means the operation that\nsplits the obtained feature map into n different feature maps\nalong channel dimension. Each split feature map ci,k has\n1 feature channel and can be considered as the importance\nscore map for indicating the location of the desired k-th gen-\neral contextual feature.\nWith the calculated and split importance score maps, we\ncollect Pi whose corresponding importance scores have the\nhighest values in its related score maps. For example, when\ncollecting the location of k-th general contextual feature, we\nperform global max-pooling operation on the feature map\nc\ni,k to help identify the location (x, y)i,k of interest and the\ncorresponding maximum importance score si,k :\nsi,k ,( x, y)i,k = MaxPool (ci,k ), (8)\nwhere MaxPool referes to the global max pooling operation\nthat returns the pooled maximum value si,k and its location\n(x, y)i,k .\nTo properly supervise the training of importance scores\nCi , we multiply the sigmoid-squashed maximum scores with\nthe corresponding features collected as general contextual\nfeatures. That is, in addition to original feature, we further\nformulate the collection operation φ( ˜f\ni ,( x, y)i,k ) as:\nφ( ˜fi ,( x, y)i,k ) = ˜fi |(x,y)i,k ·σ(si,k ), (9)\nwhere ˜fi |(x,y)i,k means the speciﬁc feature on the feature\nmap of ˜fi located at (x, y)i,k , and σ represents the sigmoid\nfunction.\n3.2.2 Transformer-Based Condensed Context Decoder\nWith the collected condensed contexts, it is then important to\nincorporate the contextual information into the MFF results\nfor reﬁnement. Since the Transformer is powerful at model-\ning complicated relationships between different features and\nfusing features according to the estimated relationships, we\napply a Transformer decoder to correlate each local feature\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2745\nto be reﬁned and its corresponding condensed contexts and\nthen translate the collected condensed contexts into a bet-\nter representation for reﬁning the fusion results of MFF. It\nis worth mentioning that we do not apply the encoder in a\ntypical Transformer structure because we found in the exper-\niments that this module delivers marginal improvement but\nextra costs in our proposed TCC module.\nIn the typical Transformer decoder architecture (V aswani\net al., 2017; Carion et al., 2020), a cross-attention structure is\nintroduced for modeling complicated relationships between\ndifferent visual features. The cross-attention estimate corre-\nlation between the input query and key and then aggregate\nvalue features according to the estimated correlations. In the\nproposed TCC modeling module, we formulate each local\nfeature as query and then describe the collected speciﬁc rep-\nresentations as both key and value.\nWe follow the typical Transformer decoder but make some\nminor modiﬁcations to save computation costs. As described\nin Eq. 2, we suppose that each local entry of the MFF result\n˜f\ni is the query q with E as its feature dimension. We unify\nthe key and value and consider condensed contexts gcc( ˜fi )\nas keys and values. Each feature of condensed contexts also\nhas E feature dimensions. We stack all the condensed con-\ntexts, include locally concentrated representation ˜f\nlc\ni and the\nglobally summarized representation ˜f gc\ni , into a uniﬁed ten-\nsor Fcc\ni : Fcc\ni = gcc( ˜fi ). With ˜fi and Fcc\ni :, we implement the\nTransformer decoder, abbreviated as “ D”, according to:\nD( ˜fi , Fcc\ni ) = WA( ˜fi + Ai Fcc\ni ), (10)\nwhere WA is a trainable linear projection matrix and Ai rep-\nresents the attentional weight:\nAi = Sof tmax\n( ˜fi (Fcc)T\n√\nE\n)\n. (11)\n3.2.3 Discussions\nAlthough our introduction of the condensed contexts is\nmotivated by the studies (Uzkent & Ermon, 2020; Chen\net al., 2019b) which show that rich visual details can be\nsigniﬁcantly simpliﬁed with the help of a speciﬁc local repre-\nsentation and a few sparse general contextual representations,\nwe would like to clarify that our approach is signiﬁcantly dif-\nferent from these methods. For example, existing methods\ndo not rely on importance score-based general contextual\nlocation prediction to summarize global contexts. Moreover,\nexisting methods generally require additional supervision\nfor training, while our method learns automatically and\nmaintains the end-to-end processing structure with differ-\nent feature pyramids and object detectors. Lastly, current\nmethods do not investigate the relations between condensed\ncontexts and the local representation for identifying poten-\ntially useful information with the help of Transformer.\n3.3 Other Implementation Details\nThe TCC is easy-to-plug-in for MFF in different feature pyra-\nmids. Besides, the TCC can also be added to the features\nboth after and before fusion. Adding TCC before fusion can\nbring 0.4 increase in AP point for MS COCO val dataset\ncomparing to only adding TCC after the fusion for MFF. In\nthis paper, we mainly adopt the implementation of adding\nTCC for both before and after fusion in the experiments.\nIn addition, we also apply the other implementation details\nas follows. Firstly, remind that we reduce the input channel\ndimension computed by 8 × 2\ni , which also means that we\nuse 64 times smaller channel numbers than the input fea-\ntures from backbone network when using the backbone like\nResNet50 (He et al., 2016), and the level i considered here\nranges from 0 to 3. Secondly, we apply a residual connection\nbetween the TCC result ˆf\ni and the MFF result ˜fi after Eq. 2 to\nperform the MFF reﬁnement. It is also worth mentioning that\nwe discard the multi-head structure for higher efﬁciency. We\nfound that this does not sacriﬁce much performance. Lastly,\nwhen improving MFF before or after fusion, we stack 2 TCC\nmodules together to achieve better reﬁnement performance\nThen, in each TCC module, we predict 4 general contextual\nlocations to extract globally summarized context features.\nThe ablation study of these hyper-parameters can be found\nat the experiment section.\nRegarding the Transformer implementation of our method,\nwe have the following implementation details. In general, we\nfollow the typical Transformer architectures (Carion et al.,\n2020; Dosovitskiy et al., 2021) to implement our decoder\nas described in Eq. 11. However, there are a few major dif-\nferences, including the following aspects: (1) In the typical\nTransformer, there are 6 encoding layers and 6 decoding lay-\ners. Instead, we only use 1 decoding layer. We adopt this\nimplementation because we found that it is a waste to apply\ntoo many layers to process only a few context features as gen-\neral contextual and value with only one local feature as query\nin the Transformer. Our ablation study validates that using\nmore layers in our TCC will introduce excessive computa-\ntion complexity and blow up GPU memory. (2) We discard\nthe multi-layer perception in the Transformer decoder, so that\nwe can avoid the introduction of massive parameters. We can\nshow in our ablation study that removing the multi-layer per-\nception does not compromise the performance too much. (3)\nWe use only 1 head in the decoding layer rather than 8 heads\nin the original Transformer. This is because we only have\nvery small channel numbers for the decoding layer, making\nit not very meaningful to apply many heads.\nAccording to the implementation as described above, the\ncomputational complexity of our TCC module would not\n123\n2746 International Journal of Computer Vision (2023) 131:2738–2756\nbe very large by performing feature dimension reduction.\nIn general, the complexity of the TCC is mainly from the\nextraction of condensed contexts and the correlation of two\ndifferent condensed contexts for reﬁnement. For condensed\ncontext extraction, the locally concentrated representation is\nlight-weighted because of the reduced feature dimensions,\nand the globally summarized representation would also not\nconsume many computational resources because we only use\na few (e.g. 4) general contextual points. We can show in the\nexperiments that adding more general contextual points can\nquickly reach saturation in improvement. Regarding corre-\nlating both types of condensed context for reﬁning MFF,\nthe Transformer structure we used would still not bring\ngreat computational burdens due to the following designs.\nFor example, we only employ decoders without involving\nencoders which we found in the experiments only have triv-\nial impacts on the performance. Besides, since the considered\nfeature dimension and the number of general contextual fea-\ntures are both small, it is quite efﬁcient for obtaining the\nattentional weights with Transformer.\n4 Experiments\n4.1 Setup\nFor evaluation, we mainly use the popular MS COCO (Lin\net al., 2014) dataset to help reveal the detection performance\nof different models. It has 118k training images, 5k valida-\ntion images, and around 21k test-dev images. We follow the\nMS COCO protocol and report the performance using the\nevaluation metrics of average precision (AP), AP at 0.5, AP\nat 0.75, and AP for small, medium, and large objects. The\nvalidation set is mainly used for ablation study, and the test-\ndev set is used for main comparison. To present the analysis\non computational complexities, we use the GFLOPs\n1 as the\nmain indicator.\nTo validate the effectiveness of the proposed TCC, we\napply the TCC on MFF for different popular and represen-\ntative feature pyramids, such as FPN (Lin et al., 2017a),\nNasFPN (Ghiasi et al., 2019), FPG (Chen et al., 2020), and\nRFP (Qiao et al., 2021). The details about these feature pyra-\nmids are listed as follows:\n• FPN (Lin et al., 2017a) is the initial and the most typi-\ncal feature pyramid used in object detection. Its MFF is\nimplemented after the fusion of higher-level feature and\nl o w e r - l e v e lf e a t u r ef o l l o w e db ya3b y3c o n v o l u t i o na s\nreﬁnement. To apply TCC in FPN, we simply replace the\n3 by 3 convolution with the proposed TCC module.\n1 GFLOPs: Giga ﬂoating point operations.\n• NasFPN (Ghiasi et al., 2019) is derived from the FPN\nand introduces neural architecture search technology to\nobtain a novel feature pyramid structure. The MFF in the\nsearched NasFPN structure is also based on operations\nlike concatenation, sum, and convolution. The detailed\nimplementation is discovered by the neural network by\nitself. The NasFPN is repeated several times to further\nreﬁne the MFF results. In NasFPN, we apply the TCC\nafter ﬁnal MFF of each level and reduce one repetition\nof processing to save costs.\n• FPG (Chen et al., 2020) refers to the feature pyramid grid\nwhich introduces a deep multi-pathway feature pyramid\nby formulating the features from different levels as a reg-\nular grid of parallel bottom-up pathways. The MFF is\nperformed by applying multi-directional lateral connec-\ntions. It is not based on searching, but its structure is\nalso repeated several times for reﬁnement. Similar with\nNasFPN, we apply the TCC after ﬁnal MFF of each level\nand also reduce one repetition.\n• RFP (Qiao et al., 2021) means recursive feature pyra-\nmid, which incorporates additional feedback connections\nfrom FPN into the bottom-up backbone layers. It coop-\nerates with a switchable atrous convolution for MFF. It is\nalso similar with NasFPN ans FPG and applies multiple\nRFP modules to achieve advanced performance. In RFP ,\nthe original FPN structure is also applied. We thus can\ndirectly apply the TCC in FPN. In addition, we also apply\nthe TCC after the switchable atrous convolution for MFF.\nIt is worth mentioning that these feature pyramids are\noriginally implemented in different detection pipelines such\nas Faster RCNN (Ren et al., 2015), RetinaNet (Lin et al.,\n2017b), and HTC (Chen et al., 2019a). For example, for FPN\nand FPG, we use the Faster RCNN (Ren et al., 2015)a st h e\ndetection pipeline for comparison. For NasFPN, we use the\nRetinaNet (Lin et al., 2017b) as the detection pipeline. For\nthe RFP , we use the HTC (Chen et al., 2019a) as the detection\npipeline. For a fair comparison, we do not alter the pipeline\nin the original implementation of each included FPN method\nand only modify the MFF part for experiments. For training\ndifferent feature pyramids, we follow the protocols reported\nin the related papers and reproduce all the baseline results\nusing the publicly released code.\n2\n4.2 Detection Performance\n4.2.1 Performance on Different Feature Pyramids\nFirstly, we study the improvement brought by TCC on dif-\nferent feature pyramids regarding efﬁciency and accuracy.\nWe also use different backbone networks to prove that our\n2 https://github.com/open-mmlab/mmdetection .\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2747\nFig. 3 Accuracy in terms of AP against computational complexity in\nterms of GFLOPs of whether using the proposed TCC on multi-level\nfusion of different feature pyramids. The experiments are performed on\nthe val s e to fM SC O C O( L i ne ta l . , 2014) and the compared feature\npyramids include FPN (Lin et al., 2017a), NasFPN (Ghiasi et al., 2019),\nFPG (Chen et al., 2020) ,a n dR F P( Q i a oe ta l . ,2021). The details about\nthese methods can be found in Sect. 4.1 and Sect. 4.2.1\nFig. 4 Accuracy in terms of AP against inference speed in terms of\ninference speed (ms) of whether using the proposed TCC on multi-level\nfusion of different feature pyramids. The experiments are performed on\nthe val s e to fM SC O C O( L i ne ta l . , 2014) and the compared feature\npyramids include FPN (Lin et al., 2017a), NasFPN (Ghiasi et al., 2019),\nFPG (Chen et al., 2020) ,a n dR F P( Q i a oe ta l . ,2021). The details about\nthese methods can be found in Sect. 4.1 and Sect. 4.2.1\nmethod is not sensitive to different implementation choices.\nIn particular, we use the aforementioned feature pyramid\nmethods, including FPN, NasFPN, FPG, and RFP , as our\nbaseline MFF methods. We mainly follow the released code\nto obtain their implementation as mentioned in the previous\nsection. In this section, we use the backbone networks, i.e.,\nResNet50 and ResNet101, for FPN, FPG, and RFP to per-\nform performance analysis. For the NasFPN, we mainly use\nthe ResNet50 as the backbone, but varying the input size\nfrom 640 by 640 (640 ×) to 1024 by 1024 (1024 ×).\nUsing the test-dev set of MS COCO for evaluation. Fig-\nure 3 shows the AP results against GFLOPs. From the\npresented results, we can observe that our TCC is able to\nconsistently improve the accuracy for different baseline fea-\nture pyramids and different backbone networks using less\ncomputational complexities. In particular, for a typical FPN\n(Lin et al., 2017a), the baseline method with ResNet50 (He et\nal., 2016) as backbone achieves an AP of 37.3 at a cost of 208\nGFLOPs, while using our proposed TCC can help the FPN\nwith ResNet50 achieve 40.6 at a cost of 163 GFLOPs, which\nmeans that the TCC improves AP with 3.3 points with 45\nless GFLOPs. With a deeper backbone, ResNet101, the TCC\nalso improves AP and reduces GFLOPs promisingly. For the\nNasFPN (Chen et al., 2020), the baseline uses different input\nimage sizes, e.g. 640 by 640 or 1024 by 1024. Using TCC, we\nimprove the AP of NasFPN from 39.9 and 44.1 to 41.6 and\n45.4 for 640 and 1024 input image sizes, respectively. Corre-\nspondingly, the GFLOPs are reduced from 139 and 355 to 128\nand 326, respectively. For the FPG (Chen et al., 2020), there\nare similar improvements of using TCC, i.e. increasing AP\nfrom 42.4 and 43.8 to 44.1 and 45.2, and decreasing GFLOPs\nfrom 602 and 681 to 539 and 518 for using ResNet50 and\nResNet101 as backbone networks, respectively. For the RFP\n(Qiao et al., 2021) that uses HTC (Chen et al., 2019a)a s\ndetection pipeline, we can observe that our method still helps\ndeliver improved performance with decreased GFLOPs and\ninference speed. For example, our method improves from\n50.7 in AP to 51.5 in AP with a GFLOPs from 449 to 365.\nIn addition to the GFLOPs, we also present the AP results\nagainst inference speeds of different methods in Fig. 4.A l l\nthe speeds are obtained using the same hardware environ-\nment. Similar to the performance of AP against GFLOPs, we\ncan observe that our method improves the AP and reduces\nthe processing time, demonstrating the effectiveness of the\nproposed TCC for MFF in different feature pyramids.\nComparison with DETR methods We also performed an\nexperiment to test our method against the DETR (Carion et\n123\n2748 International Journal of Computer Vision (2023) 131:2738–2756\nTable 1 Effects of our proposed\nTCC on the DETR and\nDeformable DETR (DDETR)\n(Zhu et al., 2020) comparing to\nthe original implementation\nMethod AP AP 50 AP75 GFLOPs Params (M)\nDETR Carion et al. ( 2020); Meng et al. ( 2021) 39.3 60.3 41.4 88 44\nTCC-DETR (ours) 39.6 57.9 42.8 77 42\nDDETR Zhu et al. ( 2020) 44.5 63.6 48.7 173 40\nTCC-DDETR (ours) 45.0 64.3 49.3 163 39\nThe DETR (Carion et al., 2020) here is a modiﬁed version following the (Meng et al., 2021)\nal., 2020; Meng et al., 2021) and Deformable DETR (Zhu et\nal., 2020). The results are shown in Table 1.\nIn general, the DETR (Carion et al., 2020)i saT r a n s f o r m e r -\nbased end-to-end object detector which directly transforms\nthe last DCNN feature map into desired detection outputs.\nThe Deformable DETR is a popular DETR-based method\nthat applies deformable operations to fuse features from\ndifferent levels. This fusion mechanism adaptively samples\nfeatures from different locations on adjacent levels to obtain\nthe fused feature, which is of great importance in reducing\nthe training difﬁculty of DETR methods (Carion et al., 2020)\nand accelerating their convergence speed.\nRather than the MFF in one-stage and two-stage detec-\ntors, the DETR-based methods ﬂatten DCNN features during\nencoding, making it difﬁcult to explicitly extract contextual\ninformation. To study the effect of our proposed TCC mod-\nule on the DETR-based methods, we attempt to restore the\nshapes of encoded features to match the shape of un-ﬂattened\nDCNN features for applying our method. In the decoding\nstage, we then continue to ﬂatten the DCNN features to main-\ntain the uniﬁed detection pipeline. It is worth mentioning that\nthe original implementation of DETR suffers from extremely\nlong training periods. We attempt to alleviate this by incor-\nporating the convergence acceleration tricks as introduced by\nthe study (Meng et al., 2021). In addition, the Deformable\nDETR has many tricks to further improve performance. Here,\nwe simply follow the basic setting of the Deformable DETR\nmainly to demonstrate the effectiveness of our method on\nboosting performance. Besides, the parameters of the TCC\nmodules we applied here are the same as the TCC on FPN in\nour paper. Similar to other feature pyramids, we attempt to\nreduce the complexity of the DETR-based methods to avoid\nexcessive computational complexity and mainly reduce 1\nencoding layer in the original method when applying our\nTCC.\nFrom the presented results, we can ﬁnd that our method\ncan reduce around 10 GFLOPs and can achieve around\n0.3 point and 0.5 point performance gain in AP for DETR\nand Deformable DETR (“DDETR” in the table) on the MS\nCOCO val dataset. Although the improvements on these\nDETR-based methods are not as signiﬁcant as on FPN meth-\nods, we analyze that this may be because the DETR-based\nmethods already model both short-range and long-range\nvisual patterns within its self-attention operations during the\nencoding stage. Such ability could lower the beneﬁts of con-\ntexts as modeled in our method. Especially, the DETR only\nhas one feature map level, which further limits the advantages\nof TCC in improving MFF. For instance, our method shows a\nslightly inferior performance compared to the DETR method\non the AP\n50 metric. This can be attributed to a reduced recall\nrate for larger objects. Although our method boasts a 0.12\npoint higher average recall rate for all objects, its average\nrecall rate for large objects lags by 0.4 point when compared\nto the DETR method. We deduce that this occurs because\nthe DETR method employs a single feature map level, caus-\ning the detection of both small and large objects to rely on\nthe same feature map. As a result, our method may tend\nto prioritize context learning for improving the detection of\nsmall objects over large ones. This results in a deeper impact\non the AP\n50 performance, as it emphasizes the detection of\nlarger objects. In contrast, our method outperforms across all\nmetrics when using the DDETR, which incorporates multi-\nlevel feature maps, thereby validating the effectiveness of our\napproach.\nNevertheless, our method still shows potential in reduc-\ning complexity and improving performance promisingly. We\nhypothesize that, since the DETR-based methods tend to dis-\ntribute almost the same attention weights on every location\nof the feature maps, it could be quite difﬁcult for these meth-\nods to identify useful information from trivial information\nefﬁciently, and they need multiple encoding and decod-\ning layers to gradually obtain better feature representation.\nAlternatively, our method by design has the potential of com-\nplementing these DETR-based methods by quickly focusing\non locally or globally useful information, ignoring other triv-\nial information to save efﬁciency and avoid the impacts of\nirrelevant information. The obtained results can validate the\neffectiveness of our TCC method. The study (Wang et al.,\n2022) can partially support our hypothesis by discovering\nthat sparse feature sampling from images is key to efﬁcient\nlearning for DETR methods.\n4.2.2 Results with High-Performance Detectors\nIn addition to the ResNet50 and ResNet101 based object\ndetectors, we further introduce high-performance implemen-\ntations to study the effects of TCC comparing to different\nstate-of-the-art object detection algorithms. Table 2 shows\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2749\nTable 2 Effects of TCC on different feature pyramids using more complicated backbone networks on the MS COCO text-dev split, comparing to other state-of-the-art object detectors\nFeature pyramid Detector Backbone AP AP 50 AP75 APS APM APL GFLOPs Params (M)\nFPN Lin et al. ( 2017a) FRCNN Ren et al. ( 2015) R101 He et al. ( 2016) 36.2 59.1 39.0 18.2 39.0 48.2 283 61\nFPN Lin et al. ( 2017a) RetinaNet Lin et al. ( 2017b) R101 He et al. ( 2016) 39.1 59.1 42.3 21.8 42.7 50.2 315 57\nFPN Lin et al. ( 2017a) FCOS Tian et al. ( 2019) X101 Xie et al. ( 2017) 44.7 64.1 48.4 27.6 47.5 55.6 439 90\nFPN Lin et al. ( 2017a) HTC Chen et al. ( 2019a) X101 Xie et al. ( 2017) 47.1 63.9 44.7 22.8 43.9 54.6 521 99\nHourglass Newell et al. ( 2016) CornerNet Law and Deng ( 2018) Hourglass-104 Newell et al. ( 2016) 40.5 56.5 43.1 19.4 42.7 53.9 – –\nPANet Liu et al. ( 2018) MaskRCNN He et al. ( 2017) X101 Xie et al. ( 2017) 46.6 65.1 50.6 29.3 50.5 60.1 627 135\nSMN Chen and Gupta ( 2017) FRCNN Ren et al. ( 2015) VGG Simonyan and Zisserman ( 2014) 31.6 52.2 33.2 14.4 35.7 45.8 – –\nOctaveConv Chen et al. ( 2019b) † R101 He et al. ( 2016) 41.0 61.9 44.5 22.9 43.8 52.5 202 62\nGC-NonLocal Wang et al. ( 2018); Cao et al. ( 2019) † R101 He et al. ( 2016) 41.9 63.4 46.5 24.5 45.1 52.5 283 79\nReCoR Chen et al. ( 2021) † R101 He et al. ( 2016) 42.3 64.2 45.6 25.0 45.8 54.4 303 63\nFPN Lin et al. ( 2017a) FRCNN Ren et al. ( 2015) R101 He et al. ( 2016) 40.5 61.2 44.1 22.3 43.7 51.6 287 60\nTCC-FPN (ours) 42.4 63.1 46.6 24.0 45.8 54.2 238 59\nFPN Lin et al. ( 2017a) FRCNN Ren et al. ( 2015) X101 Xie et al. ( 2017) 41.3 62.5 45.1 24.2 44.7 51.7 284 61\nTCC-FPN (ours) 43.0 64.0 47.0 24.2 45.9 55.3 237 60\nNasFPN Ghiasi et al. ( 2019) RetinaNet Lin et al. ( 2017b)R 5 0 ∗ He et al. ( 2016) 44.6 62.6 47.8 26.2 47.9 57.8 555 60\nTCC-NasFPN (ours) 46.2 65.0 49.7 29.0 49.7 57.6 510 56\nFPG Chen et al. ( 2020) FRCNN Ren et al. ( 2015) X101 Xie et al. ( 2017) 44.9 64.3 48.3 24.6 48.7 58.9 686 100\nTCC-FPG (ours) 46.3 66.9 50.8 28.7 49.4 56.5 626 100\nRFP Qiao et al. ( 2021) HTC Chen et al. ( 2019a) ViTAEv2-S Zhang et al. ( 2022) 53.1 72.2 57.7 32.8 56.2 68.3 548 115\nTCC-RFP (ours) 53.57 2 .65 8 .23 3 .35 6 .76 8 .6 455 114\nAll results of feature pyramids are reproduced using released codes. Best results are in bold\n*Input images are resized to 1280 by 1280. †Re-implemented methods\n123\n2750 International Journal of Computer Vision (2023) 131:2738–2756\nall the results. We compared the methods like Cascade Mask\nRCNN (He et al., 2017), HTC (Chen et al., 2019a), FCOS\n(Tian et al., 2019), and so on. More speciﬁcally, for FPN and\nFPG that use Faster RCNN detection pipeline, we use the\nResNeXt101 (Xie et al., 2017) as the backbone network to\nimprove detection accuracy. For the NasFPN that use Reti-\nnaNet, we use input image sizes of 1280 by 1280 to achieve\nhigh-performance detection. Then, for HTC-based RFP , we\napply the most recently introduced ViTAEv2 (Zhang et al.,\n2022) as backbone network for comparison.\nMoreover, to fairly compare with other cutting-edge con-\ntext modeling methods, we also report the performance of\nOctaveConv (Chen et al., 2019b), GCNet (Cao et al., 2019;\nWang et al., 2018), and ReCoR (Chen et al., 2021)o nt h e\nFRCNN (Ren et al., 2015) detection architecture using R101\n(He et al., 2016) as the backbone. The OctaveConv (Chen et\nal., 2019b) enhances image representation by incorporating\nboth high-frequency and low-frequency modeling processes\nin each convolutional block of a ResNet. GCNet (Cao et al.,\n2019; Wang et al., 2018) is an improved version of non-local\noperations that integrates global information into detection.\nReCoR (Chen et al., 2021) is a method that aims to achieve\ncomprehensive context modeling through a decomposed and\nrecursive context modeling procedure. In addition to these\nmethods, we also attempted to adapt other related methods\nlike (Uzkent & Ermon, 2020; Zhang et al., 2014) for generic\nobject detection but found it to be signiﬁcantly challeng-\ning. For example, the method (Uzkent & Ermon, 2020)w a s\ndesigned exclusively for image recognition and remote sens-\ning and did not provide a straightforward implementation for\nobject detection. The method (Zhang et al., 2014) improved\nﬁne-grained recognition but required part annotations that\nare not available in object detection datasets like MS COCO.\nImplementing the SMN (Chen & Gupta, 2017) that uses a\nmemory mechanism to reason object relations in an image is\nalso very difﬁcult. Here, we only include its performance as\na reference. For the compared methods (Chen et al., 2019b;\nCao et al., 2019; Chen et al., 2021), although they did not\nreport performance for FRCNN and R101 in object detec-\ntion, we were able to re-implement them based on their papers\nand released codes. We report the re-implemented results for\ncomparison with our method.\nAccording to the presented results, we can ﬁnd that\nour method continues improving AP and reducing required\nGFLOPs at the same time, even on the much better implemen-\ntations of detectors that may reach saturated performance.\nFor example, the TCC improves FPN by 1.7 AP points and\nreduces around 40 GFLOPs. Similar improvements can also\nbe observed for NasFPN and FPG. When using HTC-based\nRFP , we achieve 0.5 points’ increase in AP and around 90\nGFLOPs decrease in complexity compared to the HTC-based\nRFP using ViTAEv2-S (Zhang et al., 2022) as the backbone.\nWe would like to mention that the ViTAEv2 is a strong back-\nbone network that can make the detector easily saturate for\nMS COCO. Nevertheless, we still improve the AP with TCC\nand our TCC can outperform other compared state-of-the-art\ndetectors when using the HTC-based RFP . When compar-\ning our method with other context modeling methods under\nthe same setting, we observe that our method attains favor-\nable performance with signiﬁcantly lower complexity. For\ninstance, when applying our TCC to the FPN, our method\nachieves comparable performance with ReCoR (Chen et al.,\n2021) using only 238 GFLOPs which is 65 GFLOPs fewer\nthan the ReCoR. This highlights the effectiveness of our\nmethod, as ReCoR was speciﬁcally designed to capture com-\nprehensive context information as extensively as possible.\nThese results demonstrate that our method is effective in\nimproving detection performance while reducing complexi-\nties using contexts, emphasizing the efﬁciency and beneﬁts\nof our method.\n4.3 Ablation Study\nIn this section, we study the effects of different components\nin the TCC module. We also present ablation studies on the\nnumbers of predicted general contextual locations.\nTable 3 shows the performance of different MFF reﬁne-\nment methods and different components in the TCC. FPN\nwith ResNet50 is used as the baseline, and val set of MS\nCOCO is used for evaluation. According to the results, we\ncan ﬁnd that the feature pyramid achieves degraded per-\nformance without reﬁning MFF results. We then test the\nperformance of using only local context modeling imple-\nmented by the dilated convolution, the performance of using\nthe condensed contexts but without Transformer attention A\ni\nin Eq. 10, and the performance of using the complete TCC\nmodule. According to the results, we can observe progres-\nsive improvement by gradually introducing each component\nin the TCC. This illustrates that the contextual information\nis helpful for improving MFF in the feature pyramid and that\nthe proposed condensed contexts are effective for collecting\nrich contextual information. Then, by further including the\nTransformer, our proposed TCC module achieves the highest\naccuracy and introduces smaller computational complexity\nthan the baseline method.\nIn Table 4, we present the study of the effects of using\ndifferent numbers of summarized general contextual fea-\ntures. We have studied the numbers ranging from 1 to 8, and\nthe related detection accuracy and computational complex-\nities are listed for comparison. According to the presented\nresults, we can ﬁnd that using 4 general contextual features\nachieves the best speed/accuracy trade-off. Using 1 or 2 gen-\neral contextual features are beneﬁcial but can be improved\nwhile using 8 general contextual features makes the detector\nsaturates in accuracy. These results suggest that the global\ncontexts are helpful for improving MFF in feature pyra-\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2751\nTable 3 Ablation study of different multi-level feature fusion methods in FPN on MS COCO val set\nMFF reﬁnement AP AP 50 AP75 APS APM APL GFLOPs Params(M)\nNo reﬁnement 36.9 58.1 40.0 21.5 40.7 47.6 156 40\nReﬁne with original FPN (Convolution 3 × 3) 37.4 58.1 40.4 21.2 41.0 48.1 207 42\nReﬁne with local contexts only (Dilated convolution) 38.9 60.1 42.3 22.9 42.6 51.2 159 40\nReﬁne with condensed contexts only (without using Ai in Eq. 10) 39.3 60.2 42.9 23 24.7 51.1 161 41\nReﬁne with TCC 40.3 61.8 43.5 23.7 43.9 52.6 162 41\nTable 4 Effects of predicting\nand using different numbers of\ngeneral contextual features\nwhen condensing contexts in\nTCC\nNumber of general contextual features AP AP 50 AP75 APS APM APL\n1 39.8 60.9 43.2 22.9 43.4 51.7\n2 40.2 61.3 43.9 23.7 43.5 52.5\n4 40.3 61.8 43.5 23.7 43.9 52.6\n8 40.2 61.7 43.5 24.5 43.9 52.8\nTable 5 Comparison of different Transformer architecture implementation choices in TCC\nNE ND NC NH AP AP 50 AP75 APS APM APL GFLOPs Params (M)\nBaseline FPN – – – – 36.9 58.1 40.0 21.5 40.7 47.6 207 42\nTCC-FPN (ours) 0 1 – 1 40.3 61.8 43.5 23.7 43.9 52.6 162 41\n0 1 64 1 40.0 61.5 43.4 23.1 43.7 53.5 163 42\n0 1 512 1 40.1 61.6 42.8 22.8 43.8 54.2 164 44\nTCC-FPN with other 0 1 512 4 40.6 61.8 44.4 23.8 44.1 53.9 178 44\nTransformer architectures 0 1 512 8 40.6 61.4 44.7 25.4 45.2 53.4 195 49\n1 1 512 4 40.9 61.4 44.8 23.3 44.2 53.9 218 45\n2 2 512 4 40.8 63.0 44.1 24.0 44.4 54.2 265 47\nNE number of encoding layers; ND number of decoding layers; NC number of feature channels for multi-layer perception; NH number of heads\nmids and they can be condensed into features from only a\nfew general contextual locations. It is worth mentioning that\nvarying the number of considered general contextual features\ndoes not change the computational complexity too much, e.g.\nusing 8 general contextual features would introduce less than\n1 GFLOPs and less than 1 million (M) extra parameters.\nIn Table 5, we present the study of different choices for\nimplementing the Transformer architecture in our proposed\nTCC. As described previously, our proposed TCC mainly fol-\nlow the original implementation of Transformer (Carion et\nal., 2020; Dosovitskiy et al., 2021) to implement our decoder\nstructure. Different from original Transformer, we simpli-\nﬁed its architecture greatly to avoid excessive computational\ncosts. Please refer to Sect. 3.3 for more details. In the pre-\nsented results, we can ﬁnd that introducing more complicated\nTransformer architectures can consistently boost detection\nperformance, demonstrating that the condensed contexts in\nTCC are truly helpful for detection. However, more promis-\ning performance is usually accompanied with much greater\ncomputational complexity. For example, using 1 encoding\nlayer, 1 decoding layers, 512 feature channels for multi-layer\nperception, and 4 heads, we can achieve the highest average\nprecision scores, but we need around additional 54 GFLOPs\nand 6 millions of parameters than our proposed TCC module.\nSince this can hinder the efﬁciency of our proposed mod-\nule without introducing too much performance gain, we do\nnot intend to use highly complicated Transformer in our the\nTCC. In addition, although using 1 decoding layer, 512 fea-\nture channels, and 1 head can bring promising improvement\nwithout introducing too much complexity, we found in the\nexperiment that this architecture requires longer processing\ntime, increasing from around 18 ms (our TCC) to around\n27 ms. This could attribute to the reshaping operations and\nmulti-layer perception operations, making them slower than\nthe FPN baseline method. Furthermore, we would also like to\nmention that using more than 2 encoding layers and 2 decod-\ning layers at the same time can easily blow up our GPU\nmemory, making it infeasible for our experiment.\n4.4 Performance on Pascal VOC\nIn addition to MS COCO, we further validate the effective-\nness of our TCC on the PascalVOC (Everingham et al., 2010)\ndataset. The results are shown in Table 6. This dataset con-\n123\n2752 International Journal of Computer Vision (2023) 131:2738–2756\nTable 6 Performance of TCC (ours) for different feature pyramids on the Pascal VOC 07 (Everingham et al., 2010) test set\nMethod AP Plane Bicycle Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV GFLOPs Params (M)\nCC Zeng et al. ( 2017) 81.1 80.9 84.8 83.0 75.9 72.3 88.9 88.4 90.3 66.2 87.6 74.0 89.5 89.3 83.6 79.6 55.2 83.4 81.0 87.8 80.7 – –\nRFCN Dai et al. ( 2016) 80.5 79.9 87.2 81.5 72.0 69.8 86.8 88.5 89.8 67.0 88.1 74.5 89.8 90.6 79.9 81.2 53.7 81.8 81.5 85.9 79.9 – –\nCR Chen et al. ( 2018) 82.2 84.7 88.2 83.1 76.2 71.1 87.9 88.7 89.5 68.7 88.6 78.2 89.5 88.7 84.8 86.2 55.4 84.7 82.0 86.0 81.7 – –\nFPN 80.6 84.6 87.8 84.1 71.4 70.1 85.1 88.2 89.1 65.5 85.3 75.4 88.1 87.1 85.0 85.9 54.4 82.4 76.0 85.3 80.4 131 41\nTCC-FPN 81.4 85.7 87.6 84.2 72.6 71.0 86.3 87.9 89.6 68.0 87.4 73.5 88.5 88.0 83.6 86.4 58.5 84.0 79.1 86.4 79.4 103 40\nNasFPN 80.8 83.9 87.5 85.6 73.7 58.7 87.8 87.6 89.8 62.3 87.5 75.5 88.7 89.2 85.2 83.2 57.0 84.3 79.0 87.4 82.0 82 58\nTCC-NasFPN 81.2 85.5 86.5 86.2 75.3 60.8 87.9 88.1 89.5 63.8 88.7 73.9 88.4 88.9 85.5 83.0 59.4 84.4 79.8 87.3 81.7 73 52\nFPG 81.1 86.3 87.8 84.0 69.3 67.3 86.3 88.4 89.9 66.2 87.4 77.5 88.9 87.4 87.0 86.1 55.8 84.3 77.3 84.7 79.4 360 79\nTCC-FPG 81.7 88.2 87.4 83.9 72.7 70.8 86.5 88.6 89.2 68.5 84.3 77.8 87.9 87.2 85.7 85.8 59.2 85.1 78.3 86.5 81.2 296 75\nRFP 82.0 87.4 88.1 80.6 74.5 73.1 88.1 89.0 90.0 68.6 88.7 74.0 89.4 88.9 87.2 86.2 57.7 86.3 73.9 86.9 78.8 156 124\nTCC-RFP 82.48 8 .2 88.0 85.8 75.0 74.0 87.0 89.09 0 .0 68.5 88.4 75.1 89.5 89.2 85.3 86.4 57.8\n86.7 77.7 86.8 83.2 105 123\nWe use ResNet50 as the backbone for all the tested feature pyramid methods. Please refer to Sect. 4.4 for more details. Best results are in bold\ntains 39k images for training and 5k images for testing. It has\nlabels for 20 object categories. The appeared objects in this\ndataset usually have medium or large sizes. We follow the\npopular protocol and use its 07 and 12 dataset for training\nand use 07 dataset for evaluation. We tested the performance\nof TCC on FPN (Lin et al., 2017a), NasFPN (Ghiasi et al.,\n2019), FPG (Chen et al., 2020), and RFP (Qiao et al., 2021).\nWe mainly use ResNet50 as our backbone network. We also\nchange the image resolution for the evaluated methods. For\nFPN and FPG, we use the image size of 1000 by 600 (the\nshorter size is 600, but the longer size does not exceed 1000).\nFor NasFPN, we use 512 by 512 as the image size. We train\nnetworks with 12 epochs. We would also like to mention that\nwe are not able to directly apply RFP (Qiao et al., 2021)o n\nthe HTC (Chen et al., 2019a) method in the original imple-\nmentation because the HTC requires instance segmentation\nannotation that the Pascal VOC dataset does not provide. We\ninstead use the Cascade RCNN (Cai & V asconcelos, 2018)\nrather than HTC to evaluate the RFP on the Pascal VOC\ndataset.\nIn Table 6, we present the average precision (AP) per-\nformance at the IoU threshold of 0.5. From the results, we\ncan ﬁnd that our TCC module continues to improve different\nbaseline methods as well as reduce their complexities. For\nexample, the TCC improves the FPN baseline by 0.8 point\nin AP and reduce the complexity by around 30 GFLOPs. By\nboosting the FPG with TCC, we can achieve the highest AP\namong the compared methods while reducing the complex-\nity by nearly 60 GFLOPs at the same time. These results\ndemonstrate that the proposed TCC can be effective for dif-\nferent datasets. We would like to further mention that the\nPascalVOC contains a lot of large objects and thus is easier\nthan MS COCO, which enables all the compared methods to\nachieve promising AP scores. Nevertheless, even for easier\nobjects, our proposed TCC still shows advantages in further\nimproving the accuracy with much more efﬁcient process-\ning complexity for different feature pyramids. In particular,\ncompared with other context modeling methods like (Zeng\net al., 2017), our method still achieves the highest AP scores.\n4.5 Qualitative Analysis\nIn addition to quantitative analysis, we also attempt to visu-\nalize the forms of contextual information condensed by the\nproposed TCC and compare our method with the baseline\nmethod visually. The FPN with ResNet50 is selected as the\nbaseline in this section.\nFirstly, we present the spatial arrangements of learned\nlocally concentrated and globally summarised contexts w.r .t.\nthe feature from a speciﬁc local entry. Figure 5 shows the\nvisualization results. Note that the results of the last TCC on\nthe deepest-level feature is selected for visualization. In this\nﬁgure, we mainly present the locations of the speciﬁc local\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2753\nFig. 5 Visualization of the spatial arrangements of condensed contexts and their relations to an entity from a speciﬁc local entry\nfeature (pink cross), its locally concentrated contexts (purple\ndotted rectangular), and the summarized general contextual\nfeatures (coloured dots), respectively. In addition to the loca-\ntions, we also indicate the relations of different condensed\ncontexts w.r .t.the speciﬁc local feature. The relation orders\n(illustrated as coloured numbers: 1 for the most related; 5\nfor the least related) are also presented along side the corre-\nsponding contexts. From the ﬁgure, we can observe that the\nsummarized general contextual features are generally located\nat meaningful instances that can represent the theme of the\nscene presented in the image. For example, in the top middle\nﬁgure, the learned general contextual features are distributed\non vehicles, buildings, and poles, which are reasonable to\ndescribe the urban scene of the image. Besides, the loca-\ntions of summarized general contextual features, the relations\nbetween condensed contexts and the local feature learned by\nthe Transformer in TCC also look natural. As an example, in\nthe top-left ﬁgure, it shows that the TCC searches for human\nfeet, paddles, and local surrounding visual information as\nthe more useful cues for accurate detection of the surfboard,\nwhile the sea and the upper-body of the human are less impor-\ntant, which does not violate the common sense. These results\nhave clearly demonstrated that the TCC can effectively model\nthe relations between condensed contexts and local features\nand then help achieve effective detection based on the con-\ntexts and the modeled relations.\nBesides the contexts, we further present the comparison\nof detection results in Fig. 6. According to the presented\nexamples, we can ﬁnd that our proposed TCC can help the\nbaseline FPN improve the recognition quality promisingly.\nMore speciﬁcally, on the top-left ﬁgure, the FPN with TCC\ncan produce more accurate recognition of the train, avoiding\nmany false positive detection results from the baseline FPN.\nIn addition, we can observe the 2-nd row left ﬁgure and the\ntop right ﬁgure, which show that our method helps the FPN\nprovide more accurate detected bounding boxes to cover the\nobjects more compactly. In other presented examples, we can\nﬁnd that the TCC continues to improve the FPN by enhancing\nthe recognition quality and reducing the false alarms. These\nresults demonstrate that our method is effective to help the\ndetector understand the scene more robustly by introducing\ncontexts.\n5 Conclusions\nIn this study, we propose a Transformer-based context\ncondensation module to achieve improved MFF results\nin different feature pyramids with a reduced complexity.\nBy decomposing rich context information into a locally\nconcentrated representation and a globally summarised rep-\nresentation, we obtain condensed contexts that can avoid\nexhausted context modeling to achieve promising reﬁnement\nof fusion results. With the help of a Transformer decoder,\nour proposed TCC module can help boost detection accu-\nracy with a smaller computational cost. In practice, we inject\nthe TCC into 4 different feature pyramids, including FPN,\nNasFPN, FPG, and RFP , we can observe consistent improve-\n123\n2754 International Journal of Computer Vision (2023) 131:2738–2756\nFig. 6 Visual examples of the detection results of TCC compared to the baseline FPN method\nments, which validate the effectiveness of our method. We\nhope that our introduction of condensed contexts can open\nup a novel view of using contexts to improve visual under-\nstanding performance.\nFunding Open Access funding enabled and organized by CAUL and\nits Member Institutions.\nData Availability The datasets generated during and/or analysed during\nthe current study are available in the MS COCO repository, https://\ncocodataset.org/.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nAlayrac, J.B., Donahue. J., & Luc, P ., et al. (2022). Flamingo: A\nvisual language model for few-shot learning. arXiv preprint\narXiv:2204.14198.\nBaker, B., Gupta, O., & Naik, N., et al. (2016). Designing neural network\narchitectures using reinforcement learning. In ICLR.\nBiederman, I. (2017). On the semantics of a glance at a scene. Perceptual\norganization (pp. 213–253). Routledge.\n123\nInternational Journal of Computer Vision (2023) 131:2738–2756 2755\nCai, Z., & V asconcelos, N. (2018). Cascade r-cnn: Delving into high\nquality object detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (pp. 6154–6162).\nCai, Z., Fan, Q., & Feris, R.S., et al. (2016). A uniﬁed multi-scale deep\nconvolutional neural network for fast object detection. In European\nConference on Computer Vision (pp. 354–370). Springer.\nCao, Y ., Xu, J., & Lin, S., et al. (2019). Gcnet: Non-local networks meet\nsqueeze-excitation networks and beyond. In Proceedings of the\nIEEE International Conference on Computer Vision Workshops .\nCarion, N., Massa, F., & Synnaeve, G., et al. (2020). End-to-end object\ndetection with transformers. In European Conference on Computer\nVision (pp. 213–229). Springer.\nChen, K., Pang, J., & Wang, J., et al. (2019a). Hybrid task cas-\ncade for instance segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n4974–4983).\nChen, K., Cao, Y ., & Loy, C.C., et al. (2020). Feature pyramid grids.\narXiv preprint arXiv:2004.03580.\nChen, X., & Gupta, A. (2017). Spatial memory for context reasoning in\nobject detection. In Proceedings of the IEEE International Con-\nference on Computer Vision (pp. 4086–4096).\nChen, Y ., Fan, H., & Xu, B., et al. (2019b). Drop an octave: Reducing\nspatial redundancy in convolutional neural networks with octave\nconvolution. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (pp. 3435–3444).\nChen, Z., Huang, S., & Tao, D. (2018). Context reﬁnement for object\ndetection. In Proceedings of the European Conference on Com-\nputer Vision (ECCV) (pp. 71–86).\nChen, Z., Zhang, J., & Tao, D. (2021). Recursive context routing\nfor object detection. International Journal of Computer Vision,\n129(1), 142–160.\nChen, Z., Zhang, J., & Tao, D. (2022). Recurrent glimpse-based decoder\nfor detection with transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n5260–5269).\nDai, J., Li, Y ., & He, K., et al. (2016). R-fcn: Object detection via\nregion-based fully convolutional networks. In Advances in neural\ninformation processing systems , 29.\nDai, J., Qi, H., & Xiong, Y ., et al. (2017). Deformable convolutional\nnetworks. In Proceedings of the IEEE International Conference\non Computer Vision (pp. 764–773).\nDosovitskiy, A., Beyer, L., & Kolesnikov, A., et al. (2020). An image is\nworth 16x16 words: Transformers for image recognition at scale.\nIn International Conference on Learning Representations .\nDosovitskiy, A., Beyer, L., & Kolesnikov, A., et al. (2021). An image is\nworth 16x16 words: Transformers for image recognition at scale.\nIn ICLR.\nEveringham, M., V an Gool, L., Williams, C. K., et al. (2010). The\npascal visual object classes (voc) challenge. International Journal\nof Computer Vision, 88 (2), 303–338.\nGao, P ., Zheng, M., & Wang, X., et al. (2021). Fast convergence\nof detr with spatially modulated co-attention. arXiv preprint\narXiv:2101.07448.\nGhiasi, G., Lin, T.Y ., & Le, Q.V . (2019). Nas-fpn: Learning scalable\nfeature pyramid architecture for object detection. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\n(pp. 7036–7045).\nGidaris, S., & Komodakis, N. (2015). Object detection via a multi-\nregion and semantic segmentation-aware CNN model. In Proceed-\nings of the IEEE International Conference on Computer Vision (pp.\n1134–1142).\nHe, K., Zhang, X., & Ren, S., et al. (2016). Deep residual learning for\nimage recognition. In CVPR (pp 770–778).\nHe, K., Gkioxari, G., & Dollár, P ., et al. (2017). Mask r-cnn. In Proceed-\nings of the IEEE International Conference on Computer Vision (pp.\n2961–2969).\nHu, H., Gu, J., & Zhang, Z., et al. (2018). Relation networks for object\ndetection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (pp. 3588–3597).\nHuang, G., Chen, D., & Li, T., et al. (2017). Multi-scale dense\nconvolutional networks for efﬁcient prediction. arXiv preprint\narXiv:1703.09844.\nJaegle, A., Borgeaud, S., & Alayrac, J.B., et al. (2021a). Perceiver io: A\ngeneral architecture for structured inputs & outputs. arXiv preprint\narXiv:2107.14795.\nJaegle, A., Gimeno, F., & Brock, A., et al. (2021b). Perceiver: General\nperception with iterative attention. In International conference on\nmachine learning , PMLR, (pp. 4651–4664).\nKomodakis, N.,& Gidaris, S. (2016). Attend reﬁne repeat: Active box\nproposal generation via in-out localization. In BMVC.\nKong, T., Sun, F., & Yao, A., et al. (2017). Ron: Reverse connection with\nobjectness prior networks for object detection. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\n(pp. 5936–5944).\nLaw, H.,& Deng, J. (2018). Cornernet: Detecting objects as paired key-\npoints. In Proceedings of the European Conference on Computer\nVision (ECCV), (pp. 734–750).\nLin, T.Y ., Maire, M.,& Belongie, S., et al. (2014). Microsoft coco: Com-\nmon objects in context. In European Conference on Computer\nVision, (pp. 740–755). Springer.\nLin, T.Y ., Dollár, P .,& Girshick, R., et al. (2017a). Feature pyramid net-\nworks for object detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , (pp 2117–2125).\nLin, T.Y ., Goyal, P ., & Girshick, R., et al. (2017b). Focal loss for dense\nobject detection. In Proceedings of the IEEE International Con-\nference on Computer Vision , (pp. 2980–2988).\nLiu, S., Qi, L., & Qin, H., et al. (2018). Path aggregation network for\ninstance segmentation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , (pp. 8759–8768).\nLiu, W., Anguelov, D., & Erhan, D., et al. (2016). Ssd: Single shot\nmultibox detector. In European Conference on Computer Vision\n(pp. 21–37). Springer.\nLiu, Z., Lin, Y ., & Cao, Y ., et al. (2021). Swin transformer: Hierarchi-\ncal vision transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (pp.\n10012–10022).\nMeng, D., Chen, X., & Fan, Z., et al. (2021). Conditional detr for fast\ntraining convergence. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (pp. 3651–3660).\nNewell, A., Yang, K., & Deng, J. (2016). Stacked hourglass networks\nfor human pose estimation. In European Conference on Computer\nVision (pp. 483–499). Springer.\nQi, C.R., Su, H., & Mo, K., et al. (2017). Pointnet: Deep learning on\npoint sets for 3d classiﬁcation and segmentation. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\n(pp 652–660).\nQiao, S., Chen, L.C., & Y uille, A. (2021). Detectors: Detecting objects\nwith recursive feature pyramid and switchable atrous convolution.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (pp. 10213–10224).\nRen, S., He, K., & Girshick, R., et al. (2015). Faster r-cnn: Towards real-\ntime object detection with region proposal networks. In Advances\nin Neural Information Processing Systems 28.\nRumelhart, D. E., & McClelland, J. L. (1982). An interactive activation\nmodel of context effects in letter perception: Ii. the contextual\nenhancement effect and some tests and extensions of the model.\nPsychological Review, 89 (1), 60.\nSimonyan, K., & Zisserman, A. (2014). V ery deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556.\nSun, K., Xiao, B., & Liu, D., et al. (2019). Deep high-resolution repre-\nsentation learning for human pose estimation. In Proceedings of\n123\n2756 International Journal of Computer Vision (2023) 131:2738–2756\nthe IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (pp. 5693–5703).\nTay, Y ., Dehghani, M., & Bahri, D., et al. (2020). Efﬁcient transformers:\nA survey. arXiv preprint arXiv:2009.06732.\nTian, Z., Shen, C., & Chen, H., et al. (2019). Fcos: Fully convolutional\none-stage object detection. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (pp. 9627–9636).\nTorralba, A. (2003). Contextual priming for object detection. Interna-\ntional Journal of Computer Vision, 53 , 169–191.\nTouvron, H., Cord, M., & Douze, M., et al. (2021). Training data-\nefﬁcient image transformers & distillation through attention. In\nInternational Conference on Machine Learning , PMLR, (pp.\n10347–10357).\nUzkent, B., & Ermon, S. (2020). Learning when and where to zoom with\ndeep reinforcement learning. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp.\n12345–12354).\nV aswani, A., Shazeer, N., & Parmar, N., et al. (2017). Attention is all\nyou need. In NIPS.\nWang, W., Cao, Y ., & Zhang, J., et al. (2021a). Fp-detr: Detection\ntransformer advanced by fully pre-training. In: International Con-\nference on Learning Representations\nWang, W., Zhang, J., & Cao, Y ., et al. (2022). Towards data-efﬁcient\ndetection transformers. In European Conference on Computer\nVision (pp 88–105). Springer.\nWang, X., Girshick, R., & Gupta, A., et al. (2018). Non-local neural\nnetworks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (pp. 7794–7803).\nWang, Y ., Zhang, X., & Yang, T., et al. (2021b). Anchor detr:\nQuery design for transformer-based detector. arXiv preprint\narXiv:2109.07107.\nWojke, N., & Bewley, A. (2018). Deep cosine metric learning for person\nre-identiﬁcation. In 2018 IEEE Winter Conference on Applications\nof Computer Vision (WACV) (pp. 748–756). IEEE.\nXie, S., Girshick, R., & Dollár, P ., et al. (2017). Aggregated residual\ntransformations for deep neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition\n(pp. 1492–1500).\nXu, H., Yao, L., & Zhang, W., et al. (2019). Auto-fpn: Automatic net-\nwork architecture adaptation for object detection beyond classiﬁ-\ncation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (pp. 6649–6658).\nXu, Y ., Zhang, Q., & Zhang, J., et al. (2021). Vitae: Vision transformer\nadvanced by exploring intrinsic inductive bias. In Advances in\nNeural Information Processing Systems 34.\nY u, F., Wang, D., & Shelhamer, E., et al. (2018). Deep layer aggregation.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (pp. 2403–2412).\nZeng, X., Ouyang, W., Yan, J., et al. (2017). Crafting gbd-net for object\ndetection. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 40 (9), 2109–2123.\nZhang, N., Donahue, J., & Girshick, R., et al. (2014). Part-based r-cnns\nfor ﬁne-grained category detection. In: European conference on\ncomputer vision (pp. 834–849). Springer.\nZhang, Q., Xu, Y .,& Zhang, J., et al. (2022). Vitaev2: Vision transformer\nadvanced by exploring inductive bias for image recognition and\nbeyond. arXiv preprint arXiv:2202.10108.\nZhao, H., Shi, J., & Qi, X., et al. (2017). Pyramid scene parsing network.\nIn Proceedings of the IEEE conference on computer vision and\npattern recognition (pp. 2881–2890).\nZhao, Q., Sheng, T., & Wang, Y ., et al. (2019). M2det: A single-shot\nobject detector based on multi-level feature pyramid network. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence (pp\n9259–9266).\nZhu, X., Su, W.,& Lu, L., et al. (2020). Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159.\nZoph, B., & Le, Q.V . (2017). Neural architecture search with reinforce-\nment learning. In ICLR.\nZoph, B., V asudevan, V ., & Shlens, J., et al. (2018). Learning transfer-\nable architectures for scalable image recognition. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition (pp. 8697–8710).\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7818430662155151
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.6825148463249207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5864347219467163
    },
    {
      "name": "Object detection",
      "score": 0.5789587497711182
    },
    {
      "name": "Transformer",
      "score": 0.5721856951713562
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5518155097961426
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.48882243037223816
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.48321592807769775
    },
    {
      "name": "Computation",
      "score": 0.47075918316841125
    },
    {
      "name": "Representation (politics)",
      "score": 0.45661231875419617
    },
    {
      "name": "Computer vision",
      "score": 0.3240737020969391
    },
    {
      "name": "Algorithm",
      "score": 0.2533726692199707
    },
    {
      "name": "Mathematics",
      "score": 0.12576821446418762
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}