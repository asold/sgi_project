{
    "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining",
    "url": "https://openalex.org/W2994715919",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221541400",
            "name": "Huang, Wen-Chin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2340668191",
            "name": "Hayashi, Tomoki",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226243250",
            "name": "Wu, Yi-Chiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097367789",
            "name": "Kameoka Hirokazu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1979366287",
            "name": "Toda Tomoki",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2972970915",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2156142001",
        "https://openalex.org/W3006777338",
        "https://openalex.org/W2786868129",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2962780374",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2901607128",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2889329491",
        "https://openalex.org/W3034420534",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W2982055294",
        "https://openalex.org/W2749651610",
        "https://openalex.org/W3015338123",
        "https://openalex.org/W3099078140",
        "https://openalex.org/W2964243274",
        "https://openalex.org/W2963192573",
        "https://openalex.org/W2972999331",
        "https://openalex.org/W2978099976",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2767052532",
        "https://openalex.org/W3101689408",
        "https://openalex.org/W2963808252",
        "https://openalex.org/W2471520273",
        "https://openalex.org/W2949382160",
        "https://openalex.org/W95152782",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2899877258",
        "https://openalex.org/W2963609956",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2049686551",
        "https://openalex.org/W2963432880",
        "https://openalex.org/W2120605154",
        "https://openalex.org/W2963912924",
        "https://openalex.org/W2973142754",
        "https://openalex.org/W2959758584",
        "https://openalex.org/W2901254300"
    ],
    "abstract": "We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC) model based on the Transformer architecture with text-to-speech (TTS) pretraining. Seq2seq VC models are attractive owing to their ability to convert prosody. While seq2seq models based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been successfully applied to VC, the use of the Transformer network, which has shown promising results in various speech processing tasks, has not yet been investigated. Nonetheless, their data-hungry property and the mispronunciation of converted speech make seq2seq models far from practical. To this end, we propose a simple yet effective pretraining technique to transfer knowledge from learned TTS models, which benefit from large-scale, easily accessible TTS corpora. VC models initialized with such pretrained model parameters are able to generate effective hidden representations for high-fidelity, highly intelligible converted speech. Experimental results show that such a pretraining scheme can facilitate data-efficient training and outperform an RNN-based seq2seq VC model in terms of intelligibility, naturalness, and similarity.",
    "full_text": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using\nTransformer with Text-to-Speech Pretraining\nWen-Chin Huang1, Tomoki Hayashi1, Yi-Chiao Wu1, Hirokazu Kameoka2, Tomoki Toda1\n1Nagoya University, Japan 2NTT Communication Science Laboratories, Japan\nwen.chinhuang@g.sp.m.is.nagoya-u.ac.jp\nAbstract\nWe introduce a novel sequence-to-sequence (seq2seq) voice\nconversion (VC) model based on the Transformer architecture\nwith text-to-speech (TTS) pretraining. Seq2seq VC models\nare attractive owing to their ability to convert prosody. While\nseq2seq models based on recurrent neural networks (RNNs)\nand convolutional neural networks (CNNs) have been success-\nfully applied to VC, the use of the Transformer network, which\nhas shown promising results in various speech processing tasks,\nhas not yet been investigated. Nonetheless, their data-hungry\nproperty and the mispronunciation of converted speech make\nseq2seq models far from practical. To this end, we propose\na simple yet effective pretraining technique to transfer knowl-\nedge from learned TTS models, which beneﬁt from large-scale,\neasily accessible TTS corpora. VC models initialized with\nsuch pretrained model parameters are able to generate effective\nhidden representations for high-ﬁdelity, highly intelligible con-\nverted speech. Experimental results show that such a pretrain-\ning scheme can facilitate data-efﬁcient training and outperform\nan RNN-based seq2seq VC model in terms of intelligibility, nat-\nuralness, and similarity.\nIndex Terms: V oice Conversion, Sequence-to-Sequence\nLearning, Transformer, pretraining\n1. Introduction\nV oice conversion (VC) aims to convert the speech from a\nsource to that of a target without changing the linguistic content\n[1]. Conventional VC systems follow an analysis—conversion\n—synthesis paradigm [2]. First, a high quality vocoder such\nas WORLD [3] or STRAIGHT [4] is utilized to extract differ-\nent acoustic features, such as spectral features and fundamental\nfrequency (F0). These features are converted separately, and a\nwaveform synthesizer ﬁnally generates the converted waveform\nusing the converted features. Past VC studies have focused on\nthe conversion of spectral features while only applying a simple\nlinear transformation to F0. In addition, the conversion is usu-\nally performed frame-by-frame, i.e, the converted speech and\nthe source speech are always of the same length. To summa-\nrize, the conversion of prosody, including F0 and duration, is\noverly simpliﬁed in the current VC literature.\nThis is where sequence-to-sequence (seq2seq) models [5]\ncan play a role. Modern seq2seq models, often equipped with\nan attention mechanism [6, 7] to implicitly learn the alignment\nbetween the source and output sequences, can generate outputs\nof various lengths. This ability makes the seq2seq model a natu-\nral choice to convert duration in VC. In addition, the F0 contour\ncan also be converted by considering F0 explicitly (e.g, forming\nthe input feature sequence by concatenating the spectral and F0\nsequences) [8, 9, 10] or implicitly (e.g, using mel spectrograms\nPreprint. Work in progress.\nas the input feature) [11, 12, 13, 14, 15, 16]. Seq2seq VC can\nfurther be applied to accent conversion [14], where the conver-\nsion of prosody plays an important role.\nExisting seq2seq VC models are based on either recurrent\nneural networks (RNNs) [8, 9, 11, 12, 13, 14, 15, 16] or con-\nvolutional neural networks (CNNs) [10]. In recent years, the\nTransformer architecture [17] has been shown to perform ef-\nﬁciently [18] in various speech processing tasks such as auto-\nmatic speech recognition (ASR) [19], speech translation (ST)\n[20, 21], and text-to-speech (TTS) [22]. On the basis of atten-\ntion mechanism solely, the Transformer enables parallel train-\ning by avoiding the use of recurrent layers, and provides a re-\nceptive ﬁeld that spans the entire input by using multi-head\nself-attention rather than convolutional layers. Nonetheless, the\nabove-mentioned speech applications that have successfully uti-\nlized the Transformer architecture all attempted to ﬁnd a map-\nping between text and acoustic feature sequences. VC, in con-\ntrast, attempts to map between acoustic frames, whose high time\nresolution introduces challenges regarding computational mem-\nory cost and accurate attention learning.\nDespite the promising results, seq2seq VC models suffer\nfrom two major problems. First, seq2seq models usually re-\nquire a large amount of training data, although a large-scale\nparallel corpus, i.e, pairs of speech samples with identical lin-\nguistic contents uttered by both source and target speakers, is\nimpractical to collect. Second, as pointed out in [12], the con-\nverted speech often suffers from mispronunciations and other\ninstability problems such as phonemes and skipped phonemes.\nSeveral techniques have been proposed to address these issues.\nIn [11] a pretrained ASR module was used to extract phonetic\nposteriorgrams (PPGs) as an extra clue, whereas PPGs were\nsolely used as the input in [14]. The use of context preserva-\ntion loss and guided attention loss [23] to stabilize training has\nalso been proposed [9, 10]. Multitask learning and data aug-\nmentation were incorporated in [12] using additional text labels\nto improve data efﬁciency, and linguistic and speaker represen-\ntations were disentangled in [13] to enable nonparallel training,\nthus removing the need for a parallel corpus. In [16] a large\nhand-transcribed corpus was used to generate artiﬁcial training\ndata from a TTS model for a many-to-one (normalization) VC\nmodel, where multitask learning was also used.\nOne popular means of dealing with the problem of limited\ntraining data is transfer leaning, where knowledge from mas-\nsive, out-of-domain data is utilized to aid learning in the tar-\nget domain. Recently, TTS systems, especially neural seq2seq\nmodels, have enjoyed great success owing to the vast large-scale\ncorpus contributed by the community. We argue that lying at\nthe core of these TTS models is the ability to generate effec-\ntive intermediate representations, which facilitates correct atten-\ntion learning that bridges the encoder and the decoder. Transfer\nlearning from TTS has been successfully applied to tasks such\nas speaker adaptation [24, 25, 26, 27]. In [28] the ﬁrst attempt\narXiv:1912.06813v1  [eess.AS]  14 Dec 2019\nFigure 1: Model architecture of Transformer-TTS and VC.\nto apply this technique to VC was made by bootstrapping a\nnonparallel VC system from a pretrained speaker-adaptive TTS\nmodel.\nIn this work, we propose a novel yet simple pretraining\ntechnique to transfer knowledge from learned TTS models. To\ntransfer the core ability, i.e, the generation and utilization of ﬁne\nrepresentations, knowledge from both the encoder and the de-\ncoder is needed. Thus, we pretrain them in separate steps: ﬁrst,\nthe decoder is pretrained by using a large-scale TTS corpus to\ntrain a conventional TTS model. The TTS training ensures a\nwell-trained decoder that can generate high-quality speech with\nthe correct hidden representations. As the encoder must be pre-\ntrained to encode input speech into hidden representations that\ncan be recognized by the decoder, we train the encoder in an\nautoencoder style with the pretrained decoder ﬁxed. This is car-\nried out using a simple reconstruction loss. We demonstrate\nthat the VC model initialized with the above pretrained model\nparameters can generate high-quality, highly intelligible speech\neven with very limited training data.\nOur contributions in this work are as follows:\n• We apply the Transformer network to VC. To our knowl-\nedge, this is the ﬁrst work to investigate this combina-\ntion.\n• We propose a TTS pretraining technique for VC. The\npretraining process provides a prior for fast, sample-\nefﬁcient VC model learning, thus reducing the data\nsize requirement and training time. In this work, we\nverify the effectiveness of this scheme by transferring\nknowledge from Transformer-based TTS models to a\nTransformer-based VC model.\n2. Background\n2.1. Sequence-to-sequence speech systhesis\nSeq2seq models are used to ﬁnd a mapping between a source\nfeature sequence x1:n = (x1, ··· , xn) and a target feature se-\nquence y1:m = (y1, ··· , ym) which do not necessarily have\nto be of the same length, i.e, n ̸= m. Most seq2seq mod-\nels have an encoder—decoder structure [5], where advanced\nones are equipped with an attention mechanism [6, 7]. First,\nan encoder (Enc) maps x1:n into a sequence of hidden repre-\nsentations h1:n = (h1, ··· , hn). The decoding of the out-\nput sequence is autoregressive, which means that the previ-\nously generated symbols are considered an additional input at\neach decoding time step. To decode an output feature yt, a\nweighted sum of h1:n ﬁrst forms a context vector ct, where the\nweight vector is represented by a calculated attention probabil-\nity vector at = (a(1)\nt , ··· , a(n)\nt ). Each attention probability\na(k)\nt can be thought of as the importance of the hidden rep-\nresentation hk at the tth time step. Then the decoder (Dec)\nuses the context vector c and the previously generated features\ny1:t−1 = (y1, ··· , yt−1) to decode yt. Note that both the cal-\nculation of the attention vector and the decoding process take\nthe previous hidden state of the decoder qt−1 as the input. The\nabove-mentioned procedure can be formulated as follows:\nh1:n = Enc(x1:n), (1)\nat = attention(qt−1, h1:n), (2)\nct =\nn∑\nk=1\na(n)\nt hk, (3)\nyt, qt = Dec(y1:t−1, qt−1, ct). (4)\nAs pointed out in [28, 29], TTS and VC are similar since the\noutput in both tasks is a sequence of acoustic features. In such\nseq2seq speech synthesis tasks, it is a common practice to em-\nploy a linear layer to further project the decoder output to the\ndesired dimension. During training, the model is optimized via\nbackpropagation using an L1 or L2 loss.\n2.2. Transformer-based text-to-speech synthesis\nIn this subsection we describe the Transformer-based TTS sys-\ntem proposed in [22], which we will refer to as Transformer-\nTTS. Transformer-TTS is a combination of the Transformer\n[17] architecture and the Tacotron 2 [30] TTS system.\nWe ﬁrst brieﬂy introduce the Transformer model [17].\nThe Transformer relies solely on a so-called multi-head self-\nattention module that learns sequential dependences by jointly\nattending to information from different representation sub-\nspaces. The main body of Transformer-TTS resembles the orig-\ninal Transformer architecture, which, as in any conventional\nseq2seq model, consists of an encoder stack and a decoder\nstack that are composed of L encoder layers and L decoder\nFigure 2: Illustration of proposed TTS pretraining technique\nfor VC.\nlayers, respectively. An encoder layer contains a multi-head\nself-attention sublayer followed by a positionwise fully con-\nnected feedforward network. A decoder layer, in addition to\nthe two sub-layers in the encoder layer, contains a third sub-\nlayer, which performs multi-head attention over the output of\nthe encoder stack. Each layer is equipped with residual connec-\ntions and layer normalization. Finally, since no recurrent rela-\ntion is employed, sinusoidal positional encoding [31] is added\nto the inputs of the encoder and decoder so that the model can\nbe aware of information about the relative or absolute position\nof each element.\nThe model architecture of Transformer-TTS is depicted in\nFigure 1. Since the Transformer architecture was originally de-\nsigned for machine translation, several changes have been made\nto the architecture in [22] to make it compatible in the TTS task.\nFirst, as in Tacotron 2, prenets are added to the encoder and de-\ncoder sides. Since the text space and the acoustic feature space\nare different, the positional embeddings are employed with cor-\nresponding trainable weights to adapt to the scale of each space.\nIn addition to the linear projection to predict the output acoustic\nfeature, an extra linear layer is added to predict the stop token\n[30]. A weighted binary cross-entropy loss is used so that the\nmodel can learn when to stop decoding. As a common prac-\ntice in recent TTS models, a ﬁve-layer CNN postnet predicts a\nresidual to reﬁne the ﬁnal prediction.\nIn this work, our implementation is based on the open-\nsource ESPnet-TTS [32, 27], where the encoder prenet is dis-\ncarded and the guided attention loss is applied [23] to partial\nheads in partial decoder layers [18].\n3. Voice Transformer Network\nIn this section we describe the combination of Transformer and\nseq2seq VC. Our proposed model, called the V oice Transformer\nNetwork (VTN), is largely based on Transformer-TTS intro-\nduced in Section 2.2. Our model consumes the source log-mel\nspectrogram and outputs the converted log-mel spectrogram. As\npointed out in Section 2.1, TTS and VC respectively encode text\nand acoustic features to decode acoustic features. Therefore, we\nmake a very simple modiﬁcation to the TTS model, which is to\nreplace the embedding lookup layer in the encoder with a linear\nprojection layer, as shown in Figure 1. Although more com-\nplicated networks can be employed, we found that this simple\ndesign is sufﬁcient to generate satisfying results. The rest of the\nmodel architecture as well as the training process remains the\nsame as that for Transformer-TTS.\nAn important trick we found to be useful here is to use a re-\nduction factor in both the encoder and the decoder for accurate\nattention learning. In seq2seq TTS, since the time resolution\nof acoustic features is usually much larger than that of the text\ninput, a reduction factor rd is commonly used on the decoder\nside [33], where multiple stacked frames are decoded at each\ntime step. On the other hand, although the input and output of\nVC are both acoustic features, the high time resolution (about\n100 frames per second) not only makes attention learning dif-\nﬁcult but also increases the training memory footprint. While\npyramid RNNs were used to reduce the time resolution in [11],\nhere we simply introduce an encoder reduction factorre, where\nadjacent frames are stacked to reduce the time axis. We found\nthat this not only leads to better attention alignment but also re-\nduces the training memory footprint by half and subsequently\nthe number of required gradient accumulation steps [27].\n4. Proposed training strategy with\ntext-to-speech pretraining\nWe present a text-to-speech pretraining technique that enables\nfast, sample-efﬁcient training without introducing additional\nmodiﬁcation or loss to the original model structure or training\nloss. Assume that, in addition to a small, parallel VC dataset\nDVC = {Ssrc, Strg}, access to a large single-speaker TTS cor-\npus DTTS = {TTTS, STTS}is also available. Ssrc, Strg denote\nthe source, target speech respectively, and TTTS, STTS denote\nthe text and speech of the TTS speaker respectively. Our setup\nis highly ﬂexible in that we do not require any of the speakers\nto be the same, nor any of the sentences between the VC and\nTTS corpus to be parallel. We employ a two-stage training pro-\ncedure, where in the ﬁrst stage we use DTTS to learn the initial\nparameters as a prior, and then use DVC to adapt to the VC\nmodel in the second stage. As argued in Section 1, the ability\nto generate ﬁne-grained hidden representations H is the key to\na good VC model, so our goal is to ﬁnd a set of prior model\nparameters to train the ﬁnal encoder EncS\nVC and decoder DecS\nVC.\nThe overall procedure is depicted in Figure 2.\n4.1. Decoder pretraining\nThe decoder pretraining is as simple as training a conventional\nTTS model using DTTS. Since text itself contains pure linguis-\ntic information, the text encoder EncT\nTTS here is ensured to learn\nto encode an effective hidden representation that can be con-\nsumed by the decoder Dec S\nTTS. Furthermore, by leveraging the\nlarge-scale corpus, the decoder is expected to be more robust\nby capturing various speech features, such as articulation and\nprosody.\n4.2. Encoder pretraining\nA well pretrained encoder should be capable of encoding acous-\ntic features into hidden representations that are recognizable by\nthe pretrained decoder. With this goal in mind, we train an au-\ntoencoder whose decoder is the one pretrained in Section 4.1\nand kept ﬁxed during training. The desired pretrained encoder\nEncS\nTTS can be obtained by minimizing the reconstruction loss\nof STTS. As the decoder pretraining process described in Sec-\ntion 4.1 takes a hidden representation encoded from text as the\ninput, ﬁxing it in the encoder pretraining process guarantees the\nencoder to behave similarly to the text encoder Enc T\nTTS, which\nis to extract ﬁne-grained, linguistic-information-rich represen-\ntations.\n4.3. VC model training\nFinally, using DVC, we train the desired VC models, with the\nencoder and decoder initialized with Enc S\nTTS and Dec S\nTTS pre-\nTable 1: Validation-set objective evaluation results of adapted TTS, baseline (ATTS2S), and variants of the VTN trained on different\nsizes of data.\npretraining 932 training utterances 250 training utterances 80 training utterances\nSpeaker Description Encoder Decoder MCD CER WER MCD CER WER MCD CER WER\nclb-slt VTN\n- - 7.16 15.1 23.2 - - - - - -\n- ✓ 6.84 9.9 18.1 8.20 52.3 71.7 - - -\n✓ ✓ 6.44 2.4 6.3 6.82 2.9 7.0 7.15 10.6 16.1\nATTS2S - - 7.18 7.5 14.1 7.83 19.9 30.4 8.46 40.4 58.5\nbdl-slt VTN\n- - 7.32 20.9 32.6 - - - - - -\n- ✓ 7.07 20.3 31.7 8.52 58.9 80.7 - - -\n✓ ✓ 6.56 6.1 12.0 7.14 9.6 16.2 7.34 15.0 23.7\nATTS2S - - 7.36 13.0 22.7 8.01 28.4 43.9 8.76 51.2 72.7\nslt TTS adaptation - - - 3.3 6.2 - 4.5 7.2 - 4.8 7.8\ntrained in Section 4.2 and Section 4.1, respectively. The pre-\ntrained model parameters serve as a very good prior to adapt\nto the relatively scarce VC data, as we will show later. Also,\ncompared with training from scratch, the model takes less than\nhalf the training time to converge with the pretraining scheme,\nenabling extremely efﬁcient training.\n5. Experimental evaluation\n5.1. Experimental settings\nWe conducted our experiments on the CMU ARCTIC database\n[34], which contains parallel recordings of professional US En-\nglish speakers sampled at 16 kHz. One female ( slt) was cho-\nsen as the target speaker and one male ( bdl) and one female\n(clb) were chosen as sources. We selected 100 utterances each\nfor validation and evaluation, and the other 932 utterances were\nused as training data. For the TTS corpus, we chose a US fe-\nmale English speaker (judy bieber) from the M-AILABS speech\ndataset [35] to train a single-speaker Transformer-TTS model.\nWith the sampling rate also at 16 kHz, the training set contained\n15,200 utterances, which were roughly 32 hours long.\nThe entire implementation was carried out on the open-\nsource ESPnet toolkit [27, 32], including feature extraction,\ntraining and benchmarking. We extracted 80-dimensional mel\nspectrograms with 1024 FFT points and a 256 point frame shift.\nThe base settings for the TTS model and training follow the\nTransformer.v1 conﬁguration in [27], and we made minimal\nmodiﬁcations to it for VC. The reduction factors re, rd are\nboth 2 in all VC models. For the waveform synthesis mod-\nule, we used Parallel WaveGAN (PWG) [36], which is a non-\nautoregressive variant of the WaveNet vocoder [37, 38] and\nenables parallel, faster than real-time waveform generation 1.\nSince speaker-dependent neural vocoders outperform speaker-\nindependent ones [39], we trained a speaker-dependent PWG by\nconditioning on natural mel spectrograms using the full training\ndata of slt. Our goal here is to demonstrate the effectiveness of\nour proposed method, so we did not train separate PWGs for\ndifferent training sizes of the TTS/VC model used, although\ntarget speaker adaptation with limited data in VC can be used\n[40].\nWe carried out two types of objective evaluations between\nthe converted speech and the ground truth: the mel cepstrum\n1We followed the open-source implementation at https://\ngithub.com/kan-bayashi/ParallelWaveGAN\ndistortion (MCD), a commonly used measure of spectral distor-\ntion in VC, and the character error rate (CER) as well as the\nword error rate (WER), which estimate the intelligibility of the\nconverted speech. We used the WORLD vocoder [3] to extract\n24-dimensional mel cepstrum coefﬁcients with a 5 ms frame\nshift, and calculated the distortion of nonsilent, time-aligned\nframe pairs. The ASR engine is based on the Transformer archi-\ntecture [19] and is trained using the LibriSpeech dataset [41].\nThe CER and WER for the ground-truth evaluation set of slt\nwere 0.9% and 3.8%, respectively. We also reported the ASR\nresults of the TTS model adapted on different sizes of slt train-\ning data in Table 1, which can be regarded as upper bounds.\n5.2. Effectiveness of TTS pretraining\nTo evaluate the importance and the effectiveness of each pre-\ntraining scheme we proposed, we conducted a systematic com-\nparison between different training processes and different sizes\nof training data. The objective results are in Table 1. First, when\nthe network was trained from scratch without any pretraining,\nthe performance was not satisfactory even with the full train-\ning set. With decoder pretraining, a performance boost in MCD\nwas obtained, whereas the ASR results were similar. Nonethe-\nless, as we reduced the training size, the performance dropped\ndramatically, a similar trend to that reported in [13]. Finally, by\nincorporating encoder pretraining, the model exhibited a signif-\nicant improvement in all objective measures, where the effec-\ntiveness was robust against the reduction in the size of train-\ning data. Note that in the clb-slt conversion pair, our proposed\nmethod showed the potential to achieve extremely impressive\nASR results comparable to the TTS upper bound.\n5.3. Comparison with baseline method\nNext, we compared our VTN model with an RNN-based\nseq2seq VC model called ATTS2S [9]. This model is based on\nthe Tacotron model [33] with the help of context preservation\nloss and guided attention loss to stabilize training and maintain\nlinguistic consistency after conversion. We followed the conﬁg-\nurations in [9] but used mel spectrograms instead of WORLD\nfeatures.\nThe objective evaluation results of the baseline are reported\nin Table 1. For the different sizes of training data, our sys-\ntem not only consistently outperformed the baseline method but\nalso remained robust, whereas the performance of the baseline\nTable 2: Evaluation-set subjective evaluation results with 95% conﬁdence intervals for the ground truth, the TTS model, the baseline\nATTS2S, and the proposed model, VTN. The numbers in the parentheses indicate the numbers of training utterances.\nNaturalness Similarity\nDescription clb-slt bdl-slt Avg. clb-slt bdl-slt Avg.\nGround truth - 4.65 ±0.19 -\nTTS adaptation (932) - 3.80 ±0.17 -\nATTS2S (932) 2.83 ±0.23 2.51 ±0.27 2.67 ±0.23 52% ±20% 48% ±13% 50% ±16%\nVTN (932) 3.94 ±0.17 4.10 ±0.17 4.02 ±0.17 84% ±11% 82% ±10% 83% ±9%\nVTN (80) 3.77 ±0.17 3.72 ±0.24 3.75 ±0.17 65% ±17% 79% ±12% 72% ±12%\nmethod dropped dramatically as the size of training data was\nreduced. This proves that our proposed method can improve\ndata efﬁciency as well as pronunciation. We also observed that\nwhen trained from scratch, our VTN model had a similar MCD\nand inferior ASR performance compared with the baseline. As\nthe ATTS2S employed an extra mechanism to stabilize training,\nthis result may indicate the superiority of using the Transformer\narchitecture over RNNs. We leave rigorous investigation for fu-\nture work.\nSystemwise subjective tests on naturalness and conversion\nsimilarity were also conducted to evaluate the perceptual per-\nformance2. For naturalness, participants were asked to evaluate\nthe naturalness of the speech by the mean opinion score (MOS)\ntest on a ﬁve-point scale. For conversion similarity, each lis-\ntener was presented a natural speech of the target speaker and\na converted speech, and asked to judge whether they were pro-\nduced by the same speaker with the conﬁdence of the decision,\ni.e., sure or not sure. Ten non-native English speakers were re-\ncruited.\nTable 2 shows the subjective results on the evaluation set.\nFirst, with the full training set, our proposed VTN model sig-\nniﬁcantly outperformed the baseline ATTS2S by over one point\nfor naturalness and 30% for similarity. Moreover, when trained\nwith 80 utterances, our proposed method showed only a slight\ndrop in performance, and was still superior to the baseline\nmethod. This result justiﬁes the effectiveness of our method and\nalso showed that the pretraining technique can greatly increase\ndata efﬁciency without severe performance degradation.\nFinally, one interesting ﬁnding is that the VTN trained with\nthe full training set also outperformed the adapted TTS model,\nwhile the VTN with limited data exhibited comparable perfor-\nmance. Considering that the TTS models in fact obtained good\nASR results, we suspect that the VC-generated speech could\nbeneﬁt from encoding the prosody information from the source\nspeech. In contrast, the lack of prosodic clues in the linguistic\ninput in TTS reduced the naturalness of the generated speech.\n6. Conclusion\nIn this work, we successfully applied the Transformer structure\nto seq2seq VC. Also, to address the problems of data efﬁciency\nand mispronunciation in seq2seq VC, we proposed the trans-\nfer of knowledge from easily accessible, large-scale TTS cor-\npora by initializing the VC models with pretrained TTS mod-\nels. A two-stage training strategy that pretrains the decoder and\nthe encoder subsequently ensures that ﬁne-grained intermediate\nrepresentations are generated and fully utilized. Objective and\n2The audio samples can be found at https://unilight.\ngithub.io/Publication-Demos/publications/\ntransformer-vc/\nsubjective evaluations showed that our pretraining scheme can\ngreatly improve speech intelligibility, and it signiﬁcantly out-\nperformed an RNN-based seq2seq VC baseline. Even with lim-\nited training data, our system can be successfully trained with-\nout signiﬁcant performance degradation. In the future, we plan\nto more systematically examine the effectiveness of the Trans-\nformer architecture compared with RNN-based models. Exten-\nsion of our pretraining methods to more ﬂexible training condi-\ntions, such as nonparallel training [13, 28], is also an important\nfuture task.\n7. Acknowledgements\nThis work was supported in part by JST PRESTO Grant\nNumber JPMJPR1657 and JST CREST Grant Number JP-\nMJCR19A3, Japan.\n8. References\n[1] Y . Stylianou, O. Cappe, and E. Moulines, “Continuous probabilis-\ntic transform for voice conversion,”IEEE Transactions on Speech\nand Audio Processing, vol. 6, no. 2, pp. 131–142, 1998.\n[2] T. Toda, A. W. Black, and K. Tokuda, “V oice Conversion Based on\nMaximum-Likelihood Estimation of Spectral Parameter Trajec-\ntory,” IEEE Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 15, no. 8, pp. 2222–2235, 2007.\n[3] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A V ocoder-\nBased High-Quality Speech Synthesis System for Real-Time Ap-\nplications,” IEICE Transactions on Information and Systems ,\nvol. 99, pp. 1877–1884, 2016.\n[4] H. Kawahara, I. Masuda-Katsuse, and A. de Cheveign, “Re-\nstructuring speech representations using a pitch-adaptive time-\nfrequency smoothing and an instantaneous-frequency-based F0\nextraction: Possible role of a repetitive structure in sounds,”\nSpeech Communication, vol. 27, no. 3, pp. 187–207, 1999.\n[5] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to Sequence\nLearning with Neural Networks,” inAdvances in Neural Informa-\ntion Processing Systems, 2014, pp. 3104–3112.\n[6] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[7] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to\nattention-based neural machine translation,” inProceedings of the\nConference on Empirical Methods in Natural Language Process-\ning, Lisbon, Portugal, Sep. 2015, pp. 1412–1421.\n[8] H. Miyoshi, Y . Saito, S. Takamichi, and H. Saruwatari, “V oice\nConversion Using Sequence-to-Sequence Learning of Context\nPosterior Probabilities,” in Proc. Interspeech, 2017, pp. 1268–\n1272.\n[9] K. Tanaka, H. Kameoka, T. Kaneko, and N. Hojo, “ATTS2S-VC:\nSequence-to-sequence V oice Conversion with Attention and Con-\ntext Preservation Mechanisms,” inIEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), May 2019,\npp. 6805–6809.\n[10] H. Kameoka, K. Tanaka, T. Kaneko, and N. Hojo, “ConvS2S-\nVC: Fully convolutional sequence-to-sequence voice conversion,”\nCoRR, vol. abs/1811.01609, 2018.\n[11] J. Zhang, Z. Ling, L. Liu, Y . Jiang, and L. Dai, “Sequence-to-\nSequence Acoustic Modeling for V oice Conversion,”IEEE/ACM\nTransactions on Audio, Speech, and Language Processing ,\nvol. 27, no. 3, pp. 631–644, 2019.\n[12] J.-X. Zhang, Z.-H. Ling, Y . Jiang, L.-J. Liu, C. Liang, and\nL.-R. Dai, “Improving Sequence-to-sequence V oice Conversion\nby Adding Text-supervision,” IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , pp. 6785–\n6789, 2019.\n[13] J.-X. Zhang, Z.-H. Ling, and L.-R. Dai, “Non-Parallel Sequence-\nto-Sequence V oice Conversion with Disentangled Linguistic\nand Speaker Representations,” arXiv preprint arXiv:1906.10508,\n2019.\n[14] G. Zhao, S. Ding, and R. Gutierrez-Osuna, “Foreign Accent Con-\nversion by Synthesizing Speech from Phonetic Posteriorgrams,”\nin Proc. Interspeech, 2019, pp. 2843–2847.\n[15] P. Narayanan, P. Chakravarty, F. Charette, and G. Puskorius, “Hi-\nerarchical sequence to sequence voice conversion with limited\ndata,”arXiv preprint arXiv:1907.07769, 2019.\n[16] F. Biadsy, R. J. Weiss, P. J. Moreno, D. Kanvesky, and Y . Jia, “Par-\nrotron: An End-to-End Speech-to-Speech Conversion Model and\nits Applications to Hearing-Impaired Speech and Speech Separa-\ntion,” inProc. Interspeech, 2019, pp. 4115–4119.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is All you\nNeed,” in Advances in Neural Information Processing Systems ,\n2017, pp. 5998–6008.\n[18] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang et al., “A\ncomparative study on transformer vs RNN in speech applica-\ntions,”arXiv preprint arXiv:1909.06317, 2019.\n[19] L. Dong, S. Xu, and B. Xu, “Speech-Transformer: A No-\nRecurrence Sequence-to-Sequence Model for Speech Recogni-\ntion,” inIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2018, pp. 5884–5888.\n[20] L. Cross Vila, C. Escolano, J. A. R. Fonollosa, and M. R. Costa-\nJuss, “End-to-End Speech Translation with the Transformer,” in\nProc. IberSPEECH, 2018, pp. 60–63.\n[21] M. A. D. Gangi, M. Negri, R. Cattoni, R. Dessi, and M. Turchi,\n“Enhancing Transformer for End-to-end Speech-to-Text Transla-\ntion,” inProceedings of Machine Translation Summit XVII Volume\n1: Research Track, 2019, pp. 21–31.\n[22] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, “Neural Speech Synthe-\nsis with Transformer Network,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 6706–6713.\n[23] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently Train-\nable Text-to-Speech System Based on Deep Convolutional Net-\nworks with Guided Attention,” in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2018, pp.\n4784–4788.\n[24] S. Arik, J. Chen, K. Peng, W. Ping, and Y . Zhou, “Neural voice\ncloning with a few samples,” in Advances in Neural Information\nProcessing Systems, 2018, pp. 10 019–10 029.\n[25] Y . Jia, Y . Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen,\nP. Nguyen, R. Pang, I. Lopez Moreno, and Y . Wu, “Transfer learn-\ning from speaker veriﬁcation to multispeaker text-to-speech syn-\nthesis,” in Advances in Neural Information Processing Systems ,\n2018, pp. 4480–4490.\n[26] Y . Chen, Y . Assael, B. Shillingford, D. Budden, S. Reed,\nH. Zen, Q. Wang, L. C. Cobo, A. Trask, B. Laurie, C. Gulcehre,\nA. van den Oord, O. Vinyals, and N. de Freitas, “Sample efﬁcient\nadaptive text-to-speech,” inProc. ICLR, 2019.\n[27] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,\nT. Toda, K. Takeda, Y . Zhang, and X. Tan, “ESPnet-TTS: Uniﬁed,\nReproducible, and Integratable Open Source End-to-End Text-to-\nSpeech Toolkit,”arXiv preprint arXiv:1910.10909, 2019.\n[28] H.-T. Luong and J. Yamagishi, “Bootstrapping non-parallel voice\nconversion from speaker-adaptive text-to-speech,” in IEEE Auto-\nmatic Speech Recognition and Understanding Workshop (ASRU),\n2019.\n[29] M. Zhang, X. Wang, F. Fang, H. Li, and J. Yamagishi, “Joint\nTraining Framework for Text-to-Speech and V oice Conversion\nUsing Multi-Source Tacotron and WaveNet,” inProc. Interspeech,\n2019, pp. 1298–1302.\n[30] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, R. Skerry-Ryan, R. A. Saurous,\nY . Agiomyrgiannakis, and Y . Wu, “Natural TTS Synthesis by\nConditioning WaveNet on MEL Spectrogram Predictions,” in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2018, pp. 4779–4783.\n[31] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin,\n“Convolutional Sequence to Sequence Learning,” in Proceed-\nings of the 34th International Conference on Machine Learning ,\nvol. 70, 2017, pp. 1243–1252.\n[32] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y . Unno,\nN. E. Y . Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduch-\nintala, and T. Ochiai, “ESPnet: End-to-End Speech Processing\nToolkit,” inProc. Interspeech, 2018, pp. 2207–2211.\n[33] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio, Q. Le,\nY . Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron:\nTowards end-to-end speech synthesis,” inProc. Interspeech, 2017,\npp. 4006–4010.\n[34] J. Kominek and A. W. Black, “The CMU ARCTIC speech\ndatabases,” inFifth ISCA Workshop on Speech Synthesis, 2004.\n[35] Munich Artiﬁcial Intelligence Laboratories GmbH, “The\nM-AILABS speech dataset,” 2019, accessed 30 Novem-\nber 2019. [Online]. Available: https://www.caito.de/2019/01/\nthe-m-ailabs-speech-dataset/\n[36] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A\nfast waveform generation model based on generative adversar-\nial networks with multi-resolution spectrogram,” arXiv preprint\narXiv:1910.11480, 2019.\n[37] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,\n“Wavenet: A generative model for raw audio,” arXiv preprint\narXiv:1609.03499, 2016.\n[38] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda,\n“Speaker-dependent wavenet vocoder,” in Proc. Interspeech ,\n2017, pp. 1118–1122.\n[39] T. Hayashi, A. Tamamori, K. Kobayashi, K. Takeda, and\nT. Toda, “An investigation of multi-speaker training for WaveNet\nvocoder,” in IEEE Automatic Speech Recognition and Under-\nstanding Workshop (ASRU), 2017, pp. 712–718.\n[40] L.-J. Liu, Z.-H. Ling, Y . Jiang, M. Zhou, and L.-R. Dai, “WaveNet\nV ocoder with Limited Training Data for V oice Conversion,” in\nProc. Interspeech, 2018, pp. 1983–1987.\n[41] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nriSpeech: An ASR corpus based on public domain audio books,”\nin IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2015, pp. 5206–5210."
}