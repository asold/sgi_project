{
    "title": "A Systematic Approach for Assessing Large Language Models’ Test Case Generation Capability",
    "url": "https://openalex.org/W4408290845",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4211632595",
            "name": "Hung–Fu Chang",
            "affiliations": [
                "University of Indianapolis"
            ]
        },
        {
            "id": "https://openalex.org/A2113705862",
            "name": "Mohammad Shokrolah Shirazi",
            "affiliations": [
                "Marian University - Indiana"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2882984136",
        "https://openalex.org/W4392157268",
        "https://openalex.org/W4399668074",
        "https://openalex.org/W6651102376",
        "https://openalex.org/W1979345446",
        "https://openalex.org/W6695910771",
        "https://openalex.org/W3135005174",
        "https://openalex.org/W6852275782",
        "https://openalex.org/W4384345649",
        "https://openalex.org/W4362508616",
        "https://openalex.org/W4403536268",
        "https://openalex.org/W2041176136",
        "https://openalex.org/W2122205205",
        "https://openalex.org/W1971650562",
        "https://openalex.org/W2682664750",
        "https://openalex.org/W3082051432",
        "https://openalex.org/W4400582690",
        "https://openalex.org/W4403447349",
        "https://openalex.org/W4398239248",
        "https://openalex.org/W4367860052",
        "https://openalex.org/W4402671827",
        "https://openalex.org/W2288977256",
        "https://openalex.org/W1974571153",
        "https://openalex.org/W6683841033",
        "https://openalex.org/W3093870681",
        "https://openalex.org/W2287823335",
        "https://openalex.org/W4401996408",
        "https://openalex.org/W3124323960",
        "https://openalex.org/W2158373592"
    ],
    "abstract": "Software testing ensures the quality and reliability of software products, but manual test case creation is labor-intensive. With the rise of Large Language Models (LLMs), there is growing interest in unit test creation with LLMs. However, effective assessment of LLM-generated test cases is limited by the lack of standardized benchmarks that comprehensively cover diverse programming scenarios. To address the assessment of an LLM’s test case generation ability and lacking a dataset for evaluation, we propose the Generated Benchmark from Control-Flow Structure and Variable Usage Composition (GBCV) approach, which systematically generates programs used for evaluating LLMs’ test generation capabilities. By leveraging basic control-flow structures and variable usage, GBCV provides a flexible framework to create a spectrum of programs ranging from simple to complex. Because GPT-4o and GPT-3.5-Turbo are publicly accessible models, to present real-world regular users’ use cases, we use GBCV to assess LLM performance on them. Our findings indicate that GPT-4o performs better on composite program structures, while all models effectively detect boundary values in simple conditions but face challenges with arithmetic computations. This study highlights the strengths and limitations of LLMs in test generation, provides a benchmark framework, and suggests directions for future improvement.",
    "full_text": null
}