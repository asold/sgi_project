{
    "title": "A Multimodal Approach to Device-Directed Speech Detection with Large Language Models",
    "url": "https://openalex.org/W4392903087",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4225713119",
            "name": "Wagner, Dominik",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Churchill, Alexander",
            "affiliations": [
                "Apple (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A4226703028",
            "name": "Sigtia, Siddharth",
            "affiliations": [
                "Apple (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2750000345",
            "name": "Georgiou, Panayiotis",
            "affiliations": [
                "Apple (United Kingdom)"
            ]
        },
        {
            "id": null,
            "name": "Mirsamadi, Matt",
            "affiliations": [
                "Apple (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A4227102053",
            "name": "Mishra, Aarshee",
            "affiliations": [
                "Apple (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2743264742",
            "name": "Marchi, Erik",
            "affiliations": [
                "Apple (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2889511491",
        "https://openalex.org/W3003903817",
        "https://openalex.org/W3096662840",
        "https://openalex.org/W4284966478",
        "https://openalex.org/W2407023693",
        "https://openalex.org/W2783652918",
        "https://openalex.org/W4367860005",
        "https://openalex.org/W201747637",
        "https://openalex.org/W2887080793",
        "https://openalex.org/W4297841617",
        "https://openalex.org/W3011404287",
        "https://openalex.org/W4385822376",
        "https://openalex.org/W4313395525",
        "https://openalex.org/W4296068615",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6850503672",
        "https://openalex.org/W6849928207",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W4392903956",
        "https://openalex.org/W6850625674",
        "https://openalex.org/W6853249747",
        "https://openalex.org/W3196974791",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W6803567076",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W4372340819",
        "https://openalex.org/W6852818750",
        "https://openalex.org/W4372266552",
        "https://openalex.org/W4389520395",
        "https://openalex.org/W6859099255",
        "https://openalex.org/W6857054612",
        "https://openalex.org/W4372260078",
        "https://openalex.org/W4375869334",
        "https://openalex.org/W2963192057",
        "https://openalex.org/W2526425061",
        "https://openalex.org/W4385822970",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963211739",
        "https://openalex.org/W2132991150",
        "https://openalex.org/W6847363464",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W6859191622",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W1586405805",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4372349540",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4389471446",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4307106676",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4387891768",
        "https://openalex.org/W4388718054",
        "https://openalex.org/W4377130946",
        "https://openalex.org/W4320722432",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4377372369",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4311000453"
    ],
    "abstract": "Interactions with virtual assistants typically start with a predefined\\ntrigger phrase followed by the user command. To make interactions with the\\nassistant more intuitive, we explore whether it is feasible to drop the\\nrequirement that users must begin each command with a trigger phrase. We\\nexplore this task in three ways: First, we train classifiers using only\\nacoustic information obtained from the audio waveform. Second, we take the\\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\\nhypotheses, as input features to a large language model (LLM). Finally, we\\nexplore a multimodal system that combines acoustic and lexical features, as\\nwell as ASR decoder signals in an LLM. Using multimodal information yields\\nrelative equal-error-rate improvements over text-only and audio-only models of\\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\\nadaption leads to further relative EER reductions of up to 18% on our dataset.\\n",
    "full_text": "A MULTIMODAL APPROACH TO DEVICE-DIRECTED SPEECH DETECTION WITH\nLARGE LANGUAGE MODELS\nDominik Wagner1,∗, Alexander Churchill2, Siddharth Sigtia2, Panayiotis Georgiou2, Matt Mirsamadi2,\nAarshee Mishra2, Erik Marchi2\n1Technische Hochschule N¨urnberg, 2Apple\nABSTRACT\nInteractions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions\nwith the assistant more intuitive, we explore whether it is feasible\nto drop the requirement that users must begin each command with\na trigger phrase. We explore this task in three ways: First, we train\nclassifiers using only acoustic information obtained from the audio\nwaveform. Second, we take the decoder outputs of an automatic\nspeech recognition (ASR) system, such as 1-best hypotheses, as in-\nput features to a large language model (LLM). Finally, we explore\na multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal informa-\ntion yields relative equal-error-rate improvements over text-only and\naudio-only models of up to 39% and 61%. Increasing the size of the\nLLM and training with low-rank adaption leads to further relative\nEER reductions of up to 18% on our dataset.\nIndex Terms— device-directed speech detection, large lan-\nguage model, multimodal, conditional generation\n1. INTRODUCTION\nSpeech based virtual assistants allow users to interact with phones,\nwatches, mixed reality headsets, loudspeakers, and headphones via\nvoice commands. Usually, a trigger phrase or the press of a but-\nton precedes the first command, to distinguish audio that is directed\ntowards the device from background speech [1]. The problem of\ndetecting a trigger phrase is referred to as voice trigger detection\n[2, 3], wake-word detection [4, 5], or keyword spotting [6, 7, 8]. For\na more natural dialog flow, commands following the initial interac-\ntion should not need to include the trigger phrase. Device-directed\nspeech detection addresses the problem of whether a virtual assis-\ntant was addressed or not, without a trigger cue preceding the voice\ncommand at all times [9, 10, 11]. This problem is significantly more\nchallenging than voice trigger detection, since there might not be\na leading trigger phrase that marks the beginning of a voice com-\nmand. Many studies employ combinations of features such as acous-\ntic, lexical, and even accelerometer signals, using a range of methods\nfrom late fusion to transformer-based neural networks for device-\ndirectedness detection [9, 10, 12, 13, 14, 15].\nLarge language models (LLMs) have demonstrated state-of-\nthe-art text comprehension abilities over a wide range of tasks\n[16, 17, 18]. This makes them interesting for device-directed speech\ndecisions, where relevant information from all kinds of in-domain\n(e.g. voice commands) and out-of-domain signals (e.g. background\nspeech, ambient sounds, appliances etc.) needs to be processed.\nPrevious studies have extended LLMs with the ability to process\nnon-text input modalities, such as audio and video data. The multi-\nmodal PaLM-E [19] combines a large pretrained vision transformer\n[20] with the PaLM LLM [21] to perform robotic planning, as well\nas visual and language tasks. In [22], a LLaMA-7B [23] LLM is\n∗Work done during an internship at Apple.\nfinetuned to perform ASR by conditioning the model on a variable\nlength sequence of audio representations. In [24], a pretrained au-\ndio spectrogram transformer [25] provides audio representations\nto a pretrained LLaMA-7B LLM that is finetuned using low-rank\nadaptation (LoRA) [26] to perform a variety of tasks. ClipCap [27]\noperates as an image captioning system by using representations\nfrom a pretrained CLIP [28] model as prefixes in a pretrained frozen\nGPT2 [16]. In [29], audio captioning is performed using a con-\nvolutional audio encoder that generates prefixes for a pretrained\nand frozen GPT2 model. In [30], an audio encoder and a text en-\ncoder based on CLAP [31] provide representations that are used as\nprefixes to a pretrained frozen GPT2 model, which performs down-\nstream tasks such as audio captioning, speech emotion recognition\nand question answering. Other recent studies have extended LLMs\nwith the ability to process and understand audio inputs [32, 33, 34].\nIn this work, we explore a multimodal detection system to distin-\nguish device-directed utterances from non-directed audio in interac-\ntions with a virtual assistant. Our objective is to determine whether\na user addressed the assistant from the streaming audio captured by\nthe device’s microphone. The backbone is a pretrained LLM, which\npromises to offer advanced language understanding, multi-skill ca-\npabilities and ability to exploit turn taking as well as large context\nwindows [16].\nWe are motivated to enhance the LLM with non-text signals by\nthe observation that different modalities have disjoint weaknesses.\nAcoustic information alone is not reliable under background noise\nand overlapping speech, while lexical information suffers from er-\nrors due to ambiguous or poorly transcribed utterances. Further-\nmore, this study is inspired by the recent successful efforts to equip\nLLMs with signals from non-lexical modalities and their ability to\nperform a wide range of new tasks. The present work uses mul-\ntimodal information consisting of acoustic features obtained from a\npretrained audio encoder, as well as 1-best hypotheses and utterance-\nlevel decoder signals, such as acoustic cost and graph cost from an\nASR system. The acoustic features and decoder signals are repre-\nsented as prefix tokens and then concatenated with the token em-\nbeddings of the 1-best hypotheses, to form the input to a pretrained\nLLM. The system is finetuned to generate device-directedness deci-\nsions by jointly learning from all modalities (cf. Figure 1).\n2. DATA\n2.1. Training Data\nThe training data comprised a balanced randomized and anonymized\nset of ≈40k directed utterances (≈59 hours, mean duration 5.2±6.9\nsec) and ≈40k non-directed utterances ( ≈67 hours, mean duration\n6.0±5.3 sec), similar to the set used in [35] and [36]. Approximately\n21% of the device-directed examples in the training set start with a\ntrigger phrase. The remaining device-directed utterances represent\ntriggerless interactions with a virtual assistant.\n2.1.1. Text-only Training Data\nAn additional text corpus consisting of ≈3 million randomized and\nanonymized utterances of ASR-transcribed near-field speech signals\narXiv:2403.14438v2  [cs.CL]  26 Mar 2024\nrecorded with devices such as smartphones is used as additional text-\nonly information in our experiments. Similar data was used to fine-\ntune the text-based out-of-domain language detection component in\n[1]. The corpus is split into ≈1.96M device-directed and ≈1.98M\nnon-directed examples. Approximately 29% of the device-directed\nexamples start with a trigger phrase. A randomly sampled subset of\nthese 500k text-only utterances was combined with the multimodal\ntraining set in some of our experiments, to simulate dropout of the\naudio modality [37].\n2.2. Evaluation Data\nFor evaluation, we combine two randomized and anonymized in-\nhouse datasets with a total of ≈14k device-directed examples (mean\nduration 3.0±1.9 sec) and ≈23k non-directed examples (mean du-\nration 3.7±3.6 sec). The total duration of the evaluation data is ≈35\nhours. Approximately 12.3% of the device-directed utterances start\nwith a trigger phrase. The remaining device-directed utterances are\ntriggerless interactions with a virtual assistant.\n3. FEATURE EXTRACTION\n3.1. Text and ASR Features\nThe text portion of our data was transcribed with an on-device joint\nCTC-attention based end-to-end speech recognition system [38],\nrepresentative of what a device-directedness detector would see at\ndecision time. The ASR model is similar to the system used in [39].\nIt is trained on in-domain datasets and employs a Conformer [40]\nas the acoustic encoder with a subword unit segmentation based on\nbyte pair encodings (BPEs) [41].\nSimilar to [9, 10], we extract 4 additional utterance-level sig-\nnals that are generated by a decoder based on weighted finite-state\ntransducers [42]. For the most likely hypothesis in the N-best set\nof hypotheses, we extract the average of the graph cost associated\nwith each word in the hypothesis, the average of the acoustic cost,\nand the average of the word-level posterior confidence scores. The\ngraph cost is the sum of LM cost, transition probabilities, and pro-\nnunciation cost [43]. The acoustic cost is the negative log-likelihood\nof the BPE tokens from the decoder. Additionally, we include the av-\nerage number of alternative word options for each word in the 1-best\nhypothesis. Finally, we apply min-max scaling along each signal di-\nmension across the dataset to scale the feature values into the unit\ninterval [0, 1].\n3.2. Acoustic Features\nAlthough, we use an ASR system to obtain text transcriptions, we\ndecided against using acoustic representations from the same system\nin favor of more universal audio encoder models. The system de-\nscribed in Section 3.1 is trained on in-domain speech data, while we\nare processing all types of audio signals such as background speech\nand ambient noise in addition to the speech directed to an assistant.\nWe assume that including unbiased acoustic representations can pro-\nvide useful additional information that is less likely to contain the\nsame mistakes as the lexical information.\nWe compare two audio encoder backbones in our experiments.\nThe first model is the medium-sized (769M parameters) version of\nWhisper [44], an encoder-decoder transformer [45] for multilingual\nspeech recognition trained on 680k hours of audio paired with tran-\nscripts. Whisper is well-suited for our task, since it has seen a wide\nrange of acoustic conditions and is expected to generalize well across\ndomains and languages. Whisper operates on 80-dimensional log-\nmagnitude Mel spectrogram features with a window length of 25ms\nand a stride of 10ms.\nThe second model is CLAP [31] (Contrastive Language-Audio\nPretraining), which is trained to represent text descriptions and au-\ndio in the same latent space by using two encoders and contrastive\nlearning. The model is trained on audio captioning and sound event\nAudio encoder \nInput waveform\nMean pooling\nMapping\nnetwork\nASR decoder\nsignals\n1-best\nhypothesis\nMapping\nnetwork Token embeddings\nDecoder-only LLM\nyes/no \nTask prompt\nwhat time is it? [SEP] directed decision:\nASR system\nFig. 1. Architecture of the multimodal system. The weights of the\ngrey-shaded components (M1, M2 and LLM) are trained, all other\ncomponents remain frozen. The unimodal baselines differ from the\nmultimodal system as follows: In the text-only variant, the mapping\nnetworks M1 and M2 are removed and the only input features are\nthe 1-best hypotheses of the ASR system. In the audio-only variant,\nthe decoder signals including M2 and the 1-best hypotheses are re-\nmoved. The DS-only system relies only on the decoder signal input,\nwhich is transformed via M2, i.e., M1 and the 1-best hypotheses are\nremoved from the overall system.\nclassification datasets. CLAP has approximately 153 million train-\nable parameters and operates on 64-dimensional log-magnitude Mel\nspectrograms with a window length of 21ms and a stride of 10ms.\n4. METHOD\nThe architecture of our multimodal model is depicted in Figure 1. In\nthe interest of brevity, we only describe the full multimodal version\nof our system. The unimodal versions used as baselines in Section 5\nare implemented by keeping only the necessary components for one\nmodality (e.g. text) and by removing all components related to other\nmodalities. The full multimodal model has three main components:\n(1) an audio encoder that extracts a sequence of latent representa-\ntions in RN from the input audio, (2) two mapping networks M1\nand M2, that map the extracted audio features and the utterance-\nlevel ASR decoder signals into the token embedding space, and (3)\na decoder-only LLM that generates text conditioned on the prefixes,\nthe 1-best ASR hypotheses, and a text prompt.\nGiven a training dataset with L examples of audio, decoder sig-\nnals, and 1-best hypotheses {(x(i), d(i), t(i))}L\ni=1, the goal is the\ngeneration of a device-directedness decision (i.e., directed to a de-\nvice “yes” or “no”) for an unseen set of features. The 1-best hypoth-\nesis of the (i)-th example is represented by a sequence of tokens\nt(i) = (t(i)\n1 , . . . , t(i)\nl ) padded to a maximum length l.\nTo extract representations from the (i)-th audio input x(i), we\nuse the audio encoder W (either Whisper or CLAP), which yields a\nsequence of embeddings in RT×N . We aggregate these embeddings\nalong the time dimension T using mean pooling, which outputs a\nsingle vector in RN per utterance. Then, we employ a small feed-\nforward network M1 to map the aggregated embedding to a vector\nin the prefix token space of the LLM:\na(i) = M1\n \n1\nT\nTX\nt=1\nht\n!\n, W\n\u0010\nF\n\u0010\nx(i)\n\u0011\u0011\n= [h1 ··· hT ]⊤, (1)\nwhere F is the transformation of the input waveform x(i) into log-\nmagnitude Mel spectrogram features. Each representation a(i) has\nthe same dimensionality E as the token embeddings.\nWe use a second feedforward networkM2 to extract a latent rep-\nresentation for the decoder signals d(i), which yields a prefix b(i) in\nRE. We then concatenate the audio prefix a(i) and the decoder sig-\nnal prefix b(i) to the token embeddings of the corresponding 1-best\nhypothesis t(i). The language model is presented with the concate-\nnated input features. The training objective is the prediction of di-\nrectedness tokens conditioned on the audio prefix, the decoder signal\nprefix, and the 1-best hypothesis tokens in an autoregressive fashion.\nWe train the model using cross entropy loss:\nLθ = −\nLX\ni=1\nlX\nj=1\nlog pθ\n\u0010\nt(i)\nj | a(i), b(i), t(i)\n1 , . . . , t(i)\nj−1\n\u0011\n, (2)\nwhere θ are the trainable parameters of the model. During infer-\nence, we consider the score pθ (Y = yes | c) to make the device-\ndirectedness decision, where Y is a discrete random variable that\ncan realize one of m tokens y1, ..., ym from the vocabulary V and\npθ (Y = yes | c) +pθ (Y = no | c) ≃ 1. The context c is given by\nthe multimodal features, i.e., c = (a(i), b(i), t(i)\n1 , . . . , t(i)\nj−1).\n4.1. Large Language Models\nWe explore the 124.4M and 1.5B parameter versions of GPT2 in our\nexperiments. Several studies show that GPT2 can be adapted to new\ntext generation tasks by providing learnable prefixes [27, 29, 30].\nFurthermore, decoder-only LLMs have demonstrated stronger capa-\nbilities [17] than encoder-only and encoder-decoder systems such as\nBERT [46] and T5 [47] on a wide range of tasks. We choose the\nsmall version of GPT2 for the main part of our experiments, since\nit is relatively lightweight and can potentially run on devices such\nas smartphones. Since our primary concern is not to maintain the\nexpressivity of the LLM for text generation, but to receive accurate\ndevice-directedness decisions, we finetune the GPT2 weights during\ntraining along with the mapping networks. Nevertheless, we also\nanalyze the impact of increased model size and parameter-efficient\nfinetuning with LoRA [26], since this approach yielded promising\nresults in our preliminary study [48].\n4.2. Mapping Networks\nDuring training, we face the challenge of translating between the la-\ntent space of the audio encoder and the lexical embedding space of\nthe LLM. Although both models generate rich and diverse represen-\ntations, their latent spaces are independent. The mapping networks\nM1 and M2 are designed to bridge the gap between the two latent\nspaces and to meaningfully connect audio features and decoder sig-\nnals with GPT2. The outputs of the mapping networks are used as\nprefixes to the text tokens. Each audio feature is translated to RE\nsized prefixes, where E is the latent dimension of GPT2. Both map-\nping networks share the same architecture. They consist of 1 hidden\nlinear layer with 384 units, hyperbolic tangent activation and are\ntrained with a dropout probability of 10%.\n4.3. Training Details\nWe use the same training procedure for the unimodal and the mul-\ntimodal versions of our system. We extract 1024-dimensional\nrepresentations at the last encoder layer of the pretrained Whisper\nmodel and 512-dimensional representations at the audio projection\nlayer of the pretrained CLAP model. Our system is trained for 60\nepochs with an effective batch size of 256. For optimization, we\nuse AdamW [49] with an initial learning rate of 1 × 10−4, a linear\nschedule and a warm-up phase of 10% of total training steps. The\ntext token sequences are padded to l = 256.\nTable 1. Comparison of equal-error-rates (EERs) on the evalua-\ntion set. “CLF” is the frozen audio encoder with a linear classifica-\ntion head, “UM” refers to unimodal experiments, and “MM” refers\nto multimodal experiments. The column “Modality” indicates the\nmodalities used in the experiment (text = 1-best hypothesis ti, au-\ndio = audio representations ai, DS = decoder signals bi). “Add.\nText” shows the number of examples from the additional text corpus.\n“Trainable Params” is the total number of trainable parameters, i.e.,\nadapter and mapping networks in millions. When M1 was involved,\nwe used the trainable parameters of the larger mapping network (i.e.,\nthe one receiving Whisper features with ≈0.7M parameters, as op-\nposed to ≈0.5M for CLAP features) to compute the total number of\nparameters. Colors correspond to the DET curves in Figure 2.\nExp. LLM Modality Add.\nText\nEER\nWhisper\nEER\nCLAP\nTrainable\nParams\nUnimodal baselines\nCLF – audio-only – 16.70% 23.47% 0.3M\nUM1 GPT2 text-only – 12.70% 12.70% 124.4M\nUM2 GPT2 text-only 500k 12.15% 12.15% 124.4M\nUM3 GPT2 DS-only – 28.09% 28.09% 125.1M\nUM4 GPT2 audio-only – 10.98% 19.13% 125.1M\nMultimodal experiments – Full finetuning\nMM1 GPT2 text+DS – 10.54% 10.54% 125.1M\nMM2 GPT2 audio+DS – 8.81% 16.05% 125.8M\nMM3 GPT2 text+audio – 8.61% 9.34% 125.1M\nMM4 GPT2 text+audio 500k 8.59% 9.00% 125.1M\nMM5 GPT2 text+audio+DS – 8.07% 7.77% 125.8M\nMM6 GPT2 text+audio+DS 500k 7.95% 7.45% 125.8M\n4.4. Unimodal Baselines\nWe implement the following unimodal baseline systems: First, we\ntrain linear audio-only classifiers on top of Whisper and CLAP.\nTheir classification heads consist of a linear layer with 256 hidden\nunits, which receives representations obtained from the audio en-\ncoder model (i.e. Whisper or CLAP), followed by mean pooling\nacross the time dimension and a final linear layer. The classifiers\nare trained for 10 epochs with an effective batch size of 32 using\ncross entropy loss. We use the AdamW optimizer with an initial\nlearning rate of 2 × 10−5, and a warm-up phase of 10% of the\ntotal training steps with a linear schedule. During training, all pa-\nrameters of the respective audio encoder remain frozen. Second,\nwe train three unimodal versions of our framework (cf. Figure 1)\nby providing either text, decoder signals, or audio representations\nas the only input source to the LLM. We also explored zero-shot\nlearning on our task. However, zero-shot accuracy was close to\nrandom, which is expected given the GPT2 models have not been\ninstruction-finetuned.\n5. EXPERIMENTS\nTable 1 shows the equal-error-rates (EERs) on the evaluation set.\nEER is the location on a detection error trade-off (DET) curve where\nthe false acceptance rate (FAR) and the false rejection rate (FRR) are\nequal, i.e., both error types are considered equally important [50].\nThe unimodal baselines are the frozen Whisper and CLAP models\nwith a linear classification head (CLF), as well as text-only, audio-\nonly, and decoder signal (DS) only versions of our proposed system\n(UM1-4). Including additional in-domain text data improves EER\nTable 2. Comparison of equal-error-rates (EERs) for the best models\nin Table 1 (MM6) with LoRA and increased LLM size. The hyper-\nparameter r is the rank of the matrices used for adaptation, and α is\na scaling factor to adjust the magnitude of the adaptation.\nExp. LLM LoRA\nConfiguration\nEER\nWhisper\nEER\nCLAP\nTrainable\nParams\nMM6.1 GPT2 r = 64, α = 16 9.13% 9.32% 3.7M\nMM6.2 GPT2 1.5B r = 8, α = 32 6.75% 8.51% 5.2M\nMM6.3 GPT2 1.5B r = 64, α = 16 6.53% 8.41% 22.5M\nby 4.3% relative to the text-only model without additional text data\n(UM1 vs. UM2). The DS in experiment UM3 provide a weak sig-\nnal on their own (EER = 28.1%). Using our system with only the\naudio prefix and no text information (UM4) improves EER relative\nto CLF by 34.3% and 18.5%, highlighting the effectiveness of re-\nplacing the simple linear classification head with the more complex\nLLM.\nThe best system configuration (MM6) leverages 500k examples\nof additional text data (cf. Section 2.1.1) and combines text, audio,\nas well as decoder signals from the ASR system. Experiment MM6\nyields an EER of 7.95% with the Whisper audio encoder and an\nEER of 7.45% with the CLAP backbone, which translates to relative\nimprovements of 27.6% and 61.1% over the corresponding audio-\nonly models (UM4). The relative EER improvements over the best\nperforming text-only model (UM2) are 34.6% and 38.7%.\nUsing CLAP as the audio encoder performs consistently worse\nthan Whisper, as long as not all available modalities are used (MM1-\n4). However, when all three signals are combined (MM5 and MM6),\nthe features obtained with CLAP are more effective than those of\nWhisper.\nOne disadvantage of the method used in Table 1 is that by fine-\ntuning the LLM directly, we may lose its ability to perform a wide\nrange of tasks and full finetuning becomes less feasible with increas-\ning model size. In a more practical scenario, only a single LLM that\ncan not be directly finetuned might be available. Therefore, the ex-\nperiments in Table 2 show the EER with LoRA [26] adapters at-\ntached to the transformer layers of the LLM. LoRA adapters are\nsmall trainable matrices that are included into each layer of the trans-\nformer architecture and optimized instead of the underlying LLM\nweights, thereby reducing the number of trainable parameters for\ndownstream tasks. We find that applying LoRA to the small ver-\nsion of GPT2 (MM6.1) is less effective than full finetuning (MM6\nin Table 1). Using the GPT2 1.5B model yields EERs of 6.53%\nand 8.41% (MM6.3), which is a considerable improvement over ex-\nperiment MM6 in Table 1 with the Whisper backbone, but an EER\nincrease with the CLAP backbone.\nThe detection error trade-off (DET) curves for a selection of ex-\nperiments are depicted in Figure 2. We show an operating region\nfor false rejects and false accepts of ≤ 25%. The multimodal mod-\nels (solid lines) are more effective than the unimodal models (dotted\nlines), irrespective of the audio encoder backbone used. The best\nsystem (MM6.3) shows lower FARs and FRRs than the other exper-\niments across the entire operating region.\n5.1. Discussion\nWe find that CLAP representations on their own provide a weaker\nsignal than Whisper representations (cf. UM4 in Table 1). Mainly\ndue to its training on ASR, we believe Whisper is more capable of\ncapturing information that is also included in text (i.e., 1-best hy-\npotheses) and in ASR decoder signals. Consequently, the relative\nEER improvements from including additional modalities are smaller\nthan with the CLAP backbone. Once text and ASR decoder features\n0.0% 2.5% 5.0% 7.5% 10.0% 12.5% 15.0% 17.5% 20.0% 22.5% 25.0%\nFAR\n0.0%\n2.5%\n5.0%\n7.5%\n10.0%\n12.5%\n15.0%\n17.5%\n20.0%\n22.5%\n25.0%FRR\nEER\nUM2\nUM4 CLAP\nUM4 Whisper\nMM6 CLAP\nMM6 Whisper\nMM6.3 CLAP\nMM6.3 Whisper\nFig. 2. DET curves for a selection of experiments from Table 1 and\nTable 2. The false accept rate (FAR) represents non-directed utter-\nances that were falsely classified as directed utterances and the false\nreject rate (FRR) represents directed utterances that were falsely\nclassified as non-directed utterances. Dotted lines show unimodal\nbaselines (UM2 and UM4) and solid lines show multimodal exper-\niments (MM6 and MM6.3). The points on each curve indicate the\nEER of the respective experiment.\nare added, they seem to compensate for CLAP shortcomings and in\nfact using the CLAP backbone, which is trained on acoustic scene\ncharacteristics, outperforms Whisper features in this case (cf. MM6\nin Table 1). However, the EER also depends on the ability to per-\nform full finetuning, as well as the size of the LLM. Using the Whis-\nper backbone with LoRA and a larger LLM yields improvements\nover full finetuning, whereas the same experiment with CLAP does\nnot. Therefore, using acoustic information from the Whisper model\nseems to be a more robust choice in parameter-efficient finetuning\nscenarios.\n6. CONCLUSIONS\nWe described a multimodal model to distinguish device-directed ut-\nterances from background speech in the context of interactions with\na virtual assistant. We provided audio representations and ASR de-\ncoder signals as additional inputs to an LLM and showed that the\nsystem is able to effectively combine decoder signals with audio\nand lexical information. The lowest overall EER improved upon\nthe best unimodal system by 40%. This was achieved by using all\nthree modalities with the largest available version of GPT2 in combi-\nnation with parameter-efficient finetuning and Whisper as the audio\nencoder backbone.\nFuture work will focus on enhancing our approach with addi-\ntional tasks that can be useful during interactions with virtual as-\nsistants, such as audio captioning and acoustic scene classification.\nAdditionally, we will explore longer contexts, e.g., by including fea-\ntures from previous interactions.\n7. ACKNOWLEDGEMENTS\nWe would like to thank John Bridle, Pranay Dighe, Oggi Rudovic,\nAhmed Tewfik, Barry Theobald and Sachin Kajarekar for their sup-\nport and their comprehensive feedback on the paper. We also thank\nSeanie Lee for the moral support and numerous helpful discussions.\n8. REFERENCES\n[1] Siri Team, “V oice trigger system for Siri,” https:\n//machinelearning.apple.com/research/\nvoice-trigger, 2023.\n[2] Siddharth Sigtia, Rob Haynes, Hywel Richards, Erik Marchi, and John\nBridle, “Efficient V oice Trigger Detection for Low Resource Hard-\nware,” in Interspeech, 2018.\n[3] Siddharth Sigtia, Erik Marchi, Sachin Kajarekar, Devang Naik, and\nJohn Bridle, “Multi-task learning for speaker verification and voice\ntrigger detection,” in ICASSP, 2020.\n[4] Christin Jose, Yuriy Mishchenko, Thibaud S ´en´echal, Anish Shah, Alex\nEscott, and Shiv Naga Prasad Vitaladevuni, “Accurate Detection of\nWake Word Start and End Using a CNN,” inInterspeech, 2020.\n[5] Arindam Ghosh, Mark Fuhs, Deblin Bagchi, Bahman Farahani, and\nMonika Woszczyna, “Low-resource Low-footprint Wake-word Detec-\ntion using Knowledge Distillation,” in Interspeech, 2022.\n[6] Tara Sainath and Carolina Parada, “Convolutional neural networks for\nsmall-footprint keyword spotting,” in Interspeech, 2015.\n[7] Assaf Hurwitz Michaely, Xuedong Zhang, Gabor Simko, Carolina\nParada, and Petar Aleksic, “Keyword spotting for google assistant us-\ning contextual speech recognition,” in ASRU, 2017.\n[8] Dianwen Ng et al., “Contrastive speech mixup for low-resource key-\nword spotting,” in ICASSP, 2023.\n[9] Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-T ¨ur, and Larry\nHeck, “Learning when to listen: detecting system-addressed speech\nin human-human-computer dialog,” in Interspeech, 2012.\n[10] Sri Harish Mallidi, Roland Maas, Kyle Goehner, Ariya Rastrow, Spyros\nMatsoukas, and Bj¨orn Hoffmeister, “Device-directed Utterance Detec-\ntion,” in Interspeech, 2018.\n[11] Vineet Garg et al., “Device-Directed Speech Detection: Regularization\nvia Distillation for Weakly-Supervised Models,” in Interspeech, 2022.\n[12] Kellen Gillespie, Ioannis C. Konstantakopoulos, Xingzhi Guo,\nVishal Thanvantri Vasudevan, and Abhinav Sethy, “Improving device\ndirectedness classification of utterances with semantic lexical features,”\nin ICASSP, 2020.\n[13] Sai Srujana Buddi et al., “Efficient multimodal neural networks for\ntrigger-less voice assistants,” in Interspeech, 2023.\n[14] Hiroshi Sato, Yusuke Shinohara, and Atsunori Ogawa, “Multi-modal\nmodeling for device-directed speech detection using acoustic and lin-\nguistic cues,” Acoustical Science and Technology, 2023.\n[15] Dhanush Bekal, Sundararajan Srinivasan, Srikanth Ronanki, Sravan\nBodapati, and Katrin Kirchhoff, “Contextual Acoustic Barge-In Clas-\nsification for Spoken Dialog Systems,” in Interspeech, 2022.\n[16] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and\nIlya Sutskever, “Language models are unsupervised multitask learn-\ners,” 2019.\n[17] Tom B. Brown et al., “Language models are few-shot learners,” 2020,\narXiv:2005.14165.\n[18] OpenAI, “GPT-4 technical report,” 2023, arXiv:2303.08774.\n[19] Danny Driess et al., “PaLM-E: An embodied multimodal language\nmodel,” 2023, arXiv:2303.03378.\n[20] Mostafa Dehghani et al., “Scaling vision transformers to 22 billion\nparameters,” 2023, arXiv:2302.05442.\n[21] Aakanksha Chowdhery et al., “PaLM: Scaling language modeling with\npathways,” 2022, arXiv:2204.0231.\n[22] Yassir Fathullah et al., “Prompting large language models with speech\nrecognition abilities,” 2023, arXiv:2307.11795.\n[23] Hugo Touvron et al., “LLaMA: Open and efficient foundation language\nmodels,” 2023, arXiv:2302.13971.\n[24] Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and\nJames Glass, “Listen, think, and understand,” 2023, arXiv:2305.10790.\n[25] Yuan Gong, Yu-An Chung, and James Glass, “AST: Audio spectro-\ngram transformer,” in Interspeech, 2021.\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Shean Wang, Lu Wang, and Weizhu Chen, “LoRA: Low-rank adap-\ntation of large language models,” in ICLR, 2022.\n[27] Ron Mokady, Amir Hertz, and Amit H. Bermano, “ClipCap: CLIP\nprefix for image captioning,” 2021, arXiv:2111.09734.\n[28] Alec Radford et al., “Learning transferable visual models from natural\nlanguage supervision,” 2021, arXiv:2103.00020.\n[29] Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh, “Prefix tuning for\nautomated audio captioning,” in ICASSP, 2023.\n[30] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming\nWang, “Pengi: An audio language model for audio tasks,” in NeurIPS,\n2023.\n[31] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and\nHuaming Wang, “CLAP: Learning audio concepts from natural lan-\nguage supervision,” 2022, arXiv:2206.04769.\n[32] Srijith Radhakrishnan, Chao-Han Yang, Sumeer Khan, Rohit Kumar,\nNarsis Kiani, David Gomez-Cabrero, and Jesper Tegn ´er, “Whisper-\ning LLaMA: A cross-modal generative error correction framework for\nspeech recognition,” in EMNLP, 2023.\n[33] Yunfei Chu et al., “Qwen-audio: Advancing universal audio un-\nderstanding via unified large-scale audio-language models,” 2023,\narXiv:2311.07919.\n[34] Changli Tang et al., “Salmonn: Towards generic hearing abilities for\nlarge language models,” 2023, arXiv:2310.13289.\n[35] Oggi Rudovic et al., “Less is more: A unified architecture for device-\ndirected speech detection with multiple invocation types,” in ICASSP,\n2023.\n[36] Pranay Dighe, Prateeth Nayak, Oggi Rudovic, Erik Marchi, Xiaochuan\nNiu, and Ahmed Tewfik, “Audio-to-intent using acoustic-textual sub-\nword representations from end-to-end asr,” in ICASSP, 2023.\n[37] Natalia Neverova, Christian Wolf, Graham Taylor, and Florian Nebout,\n“Moddrop: Adaptive multi-modal gesture recognition,” IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 2016.\n[38] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTC-attention\nbased end-to-end speech recognition using multi-task learning,” in\nICASSP, 2017.\n[39] Maurits Bleeker, Pawel Swietojanski, Stefan Braun, and Xiaodan\nZhuang, “Approximate Nearest Neighbour Phrase Mining for Con-\ntextual Speech Recognition,” in Interspeech, 2023.\n[40] Anmol Gulati et al., “Conformer: Convolution-augmented Transformer\nfor Speech Recognition,” in Interspeech, 2020.\n[41] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neural machine\ntranslation of rare words with subword units,” in ACL, 2016.\n[42] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “EESEN:\nEnd-to-end speech recognition using deep RNN models and WFST-\nbased decoding,” in ASRU, 2015.\n[43] Daniel Povey et al., “Generating exact lattices in the WFST frame-\nwork,” in ICASSP, 2012.\n[44] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine\nMcLeavey, and Ilya Sutskever, “Robust speech recognition via large-\nscale weak supervision,” 2022, arXiv:2212.04356.\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention\nis all you need,” in NeurIPS, 2017.\n[46] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,\n“BERT: Pre-training of deep bidirectional transformers for language\nunderstanding,” in NAACL, 2019.\n[47] Colin Raffel et al., “Exploring the limits of transfer learning with a\nunified text-to-text transformer,” JMLR, 2020.\n[48] Dominik Wagner et al., “Multimodal Data and Resource Efficient\nDevice-Directed Speech Detection with Large Foundation Models,” in\nThird Workshop on Efficient Natural Language and Speech Processing\n(ENLSP-III) at NeurIPS 2023, 2023.\n[49] Ilya Loshchilov and Frank Hutter, “Decoupled weight decay regular-\nization,” in ICLR, 2019.\n[50] Alvin Martin, George Doddington, Terri Kamm, Mark Ordowski, and\nMark Przybocki, “The DET curve in assessment of detection task per-\nformance,” in Eurospeech, 1997."
}