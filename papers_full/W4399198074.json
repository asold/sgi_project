{
  "title": "Correctable Landmark Discovery via Large Models for Vision-Language Navigation",
  "url": "https://openalex.org/W4399198074",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2296134605",
      "name": "Bingqian Lin",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5111146476",
      "name": "Yunshuang Nie",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2267473942",
      "name": "Ziming Wei",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2111242427",
      "name": "Yi Zhu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2095922787",
      "name": "Hang Xu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099909956",
      "name": "Shikui Ma",
      "affiliations": [
        "Quanta Computer (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2127038590",
      "name": "Jianzhuang Liu",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2121312783",
      "name": "Xiaodan Liang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963800628",
    "https://openalex.org/W3034578524",
    "https://openalex.org/W2951973805",
    "https://openalex.org/W3100923070",
    "https://openalex.org/W2979727876",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W6751885507",
    "https://openalex.org/W3165915253",
    "https://openalex.org/W3035232877",
    "https://openalex.org/W2987914945",
    "https://openalex.org/W2981799368",
    "https://openalex.org/W6757724268",
    "https://openalex.org/W6784287907",
    "https://openalex.org/W6780451406",
    "https://openalex.org/W3172675210",
    "https://openalex.org/W6803039797",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6810640255",
    "https://openalex.org/W6839928859",
    "https://openalex.org/W4221167472",
    "https://openalex.org/W6847873901",
    "https://openalex.org/W4226052928",
    "https://openalex.org/W6770268497",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W6780419043",
    "https://openalex.org/W6759737764",
    "https://openalex.org/W6760216593",
    "https://openalex.org/W3107069568",
    "https://openalex.org/W3205276578",
    "https://openalex.org/W4304097971",
    "https://openalex.org/W6810557439",
    "https://openalex.org/W3195026654",
    "https://openalex.org/W6803123405",
    "https://openalex.org/W6809509765",
    "https://openalex.org/W6839459284",
    "https://openalex.org/W6843382641",
    "https://openalex.org/W4389524306",
    "https://openalex.org/W6838811116",
    "https://openalex.org/W6809692409",
    "https://openalex.org/W6849793301",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6692846177",
    "https://openalex.org/W6775865546",
    "https://openalex.org/W3034500398",
    "https://openalex.org/W4386065870",
    "https://openalex.org/W4386065508",
    "https://openalex.org/W4386065347",
    "https://openalex.org/W6855844794",
    "https://openalex.org/W6853064403",
    "https://openalex.org/W3176495323",
    "https://openalex.org/W6770775884",
    "https://openalex.org/W3170842411"
  ],
  "abstract": "Vision-Language Navigation (VLN) requires the agent to follow language instructions to reach a target position. A key factor for successful navigation is to align the landmarks implied in the instruction with diverse visual observations. However, previous VLN agents fail to perform accurate modality alignment especially in unexplored scenes, since they learn from limited navigation data and lack sufficient open-world alignment knowledge. In this work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential landmark discovery problem, by introducing a novel correctable landmark discovery scheme based on two large models ChatGPT and CLIP. Specifically, we use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and conduct CLIP-driven landmark discovery based on these commonsense priors. To mitigate the noise in the priors due to the lack of visual constraints, we introduce a learnable cooccurrence scoring module, which corrects the importance of each cooccurrence according to actual observations for accurate landmark discovery. We further design an observation enhancement strategy for an elegant combination of our framework with different VLN agents, where we utilize the corrected landmark features to obtain enhanced observation features for action decision. Extensive experimental results on multiple popular VLN benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE over strong baselines. Especially, our CONSOLE establishes the new state-of-the-art results on R2R and R4R in unseen scenarios.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1\nCorrectable Landmark Discovery via Large\nModels for Vision-Language Navigation\nBingqian Lin∗, Yunshuang Nie∗, Ziming Wei, Yi Zhu, Hang Xu,\nShikui Ma, Jianzhuang Liu, Xiaodan Liang†\nAbstract—Vision-Language Navigation (VLN) requires the agent to follow language instructions to reach a target position. A key factor\nfor successful navigation is to align the landmarks implied in the instruction with diverse visual observations. However, previous VLN\nagents fail to perform accurate modality alignment especially in unexplored scenes, since they learn from limited navigation data and\nlack sufficient open-world alignment knowledge. In this work, we propose a new VLN paradigm, called COrrectable LaNdmark\nDiScOvery via Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential landmark discovery problem, by\nintroducing a novel correctable landmark discovery scheme based on two large models ChatGPT and CLIP . Specifically, we use\nChatGPT to provide rich open-world landmark cooccurrence commonsense, and conduct CLIP-driven landmark discovery based on\nthese commonsense priors. To mitigate the noise in the priors due to the lack of visual constraints, we introduce a learnable\ncooccurrence scoring module, which corrects the importance of each cooccurrence according to actual observations for accurate\nlandmark discovery. We further design an observation enhancement strategy for an elegant combination of our framework with\ndifferent VLN agents, where we utilize the corrected landmark features to obtain enhanced observation features for action decision.\nExtensive experimental results on multiple popular VLN benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of\nCONSOLE over strong baselines. Especially, our CONSOLE establishes the new state-of-the-art results on R2R and R4R in unseen\nscenarios. Code is available at https://github.com/expectorlin/CONSOLE.\nIndex Terms—Vision-language navigation, open-world landmark discovery, large language models\n✦\nA I NTRODUCTION\nV\nISION -AND -LANGUAGE Navigation (VLN) [1], [2], [3],\n[4], [5], one of the most representative embodied AI\ntasks, requires an agent to navigate through complicated\nvisual environments to a goal position following a given\nnatural language instruction. It has led to a wide range\nof research recently since an instruction-following navi-\ngation agent is more flexible and practical in real-world\napplications. For successful navigation, the agent needs to\naccurately understand the instruction intention and align\nthe landmarks implied in the instruction to sequential visual\nobservations.\nEarly VLN methods utilize different data augmentation\nstrategies [6], [7], [8], efficient learning paradigms [9], [10],\n[11], and dedicated model architectures [12], [13], [14] to\nlearn useful modality alignment knowledge from limited\n• ∗These two authors contribute equally to this work.\n• †Xiaodan Liang is the corresponding author.\n• Bingqian Lin, Yunshuang Nie and Ziming Wei are with Shenzhen\nCampus of Sun Yat-sen University, Shenzhen, China.\nE-mail:{linbq6@mail2.sysu.edu.cn,nieysh@mail2.sysu.edu.cn,\nweizm3@mail2.sysu.edu.cn}\n• Xiaodan Liang is with Shenzhen Campus of Sun Yat-sen University,\nShenzhen, China, and also with PengCheng Laboratory.\nE-mail: liangxd9@mail.sysu.edu.cn\n• Yi Zhu and Hang Xu are with Huawei Noah’s Ark Lab.\nE-mail: zhu.yee@outlook.com, chromexbjxh@gmail.com.\n• Shikui Ma is with Dataa Robotics company.\nE-mail: maskey.bj@gmail.com.\n• Jianzhuang Liu is with Shenzhen Institute of Advanced Technology,\nShenzhen, China.\nE-mail: jz.liu@siat.ac.cn.\nhuman annotated navigation data. With the development\nof cross-modal pretraining models, increasing pretraining-\nbased VLN approaches are developed [15], [16] and have\nshown great progress in improving the modality alignment\nability of VLN agents. However, due to the limited scale\nand diversity of pretraining and navigational data, these\napproaches still cannot generalize well to unseen navigation\nscenarios, which usually require rich open-world alignment\nknowledge, e.g., recognizing the unseen landmark, to real-\nize accurate instruction following.\nRecent studies have shown that employing large models,\nsuch as Large Language Models (LLMs, e.g., GPT-3 [17])\nand large Vision-Language Models (VLMs, e.g., CLIP [18]),\nfor embodied AI tasks is a very promising way to improve\nthe task completion [19], [20], [21], since the large models\nare pretrained on ultra-large-scale web corpora and capture\nvast amount of open-world knowledge which is helpful\nfor embodied AI tasks. However, constrained by the large\ndomain gap between those training corpora and embodied\nAI task datasets, a direct introduction of large models for\nembodied AI tasks may bring unexpected noise [19], [22].\nTherefore, how to effectively activate and more importantly,\nutilize the task-related knowledge from large models to\nassist embodied agents, has been paid more and more\nattention in the embodied AI area.\nIn this paper, we propose a new paradigm called\nCOrrectable La Ndmark Di ScOvery via Large Mod Els\n(CONSOLE), where we activate and utilize rich open-\nworld alignment knowledge stored in two large models\nChatGPT [23] and CLIP [18] for assisting the VLN task\nin a correctable way. Through CONSOLE, we cast VLN\narXiv:2405.18721v2  [cs.CV]  5 Jun 2024\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\nTurn left to face bed. Walk past bed and out of door. Walk across hall to next bedroom. Stop in doorway.\nhall\nrug\nchandelier\nartwork\nInstruction: [Instruction] \nLandmarks:\nparse prompt \nexamples\nTell me 5 co-occurrences \nof [Landmark]:\ncooccurrence  \nprompt examples\nInstruction\nStep t     Observation\nLandmark Cooccurrence Priors\nbed door hall\ndoorknob\nhandle\nframe chandelier\nartwork rug\nsheets\nlamp pillow\nCorrectable Landmark Discovery\nLearnable Cooccurrence Scoring\nNavigational Sequential\nLandmark Prompt\nCooccurrence Prior\nPrompt\nartwork  hall  rug  chandelier\nCONSOLE prediction \nwith learnable scoring:\nCLIP prediction without \nlearnable scoring:\nAction Comparison\nChatGPT\n…\nCLIP\nartwork\nrugrug\nFig. 1: Correctable landmark discovery of CONSOLE. We provide customized prompts for ChatGPT to generate landmark\ncooccurrence priors. Then we introduce a learnable cooccurrence scoring module to conduct CLIP-driven correctable\nlandmark discovery based on the priors. The landmark/cooccurrence with bigger font size has a higher score.\nas an open-world sequential landmark discovery problem\nfor facilitating action decision. Our approach is inspired\nby the navigation commonsense, that is, the landmark\ncooccurrence is helpful for locating a target landmark. For\nexample, a fridge is likely to be placed near a stove. When a\nmentioned landmark in the instruction is hardly observable\nin the views, locating the landmark cooccurrence (which\nis not mentioned in the instruction) is an effective alterna-\ntive of locating the mentioned landmark to make correct\nnavigation decisions. Being trained using ultra-large-scale\ncorpora, ChatGPT captures sufficient open-world landmark\ncooccurrence commonsense. However, due to the lack of\nvisual constraints, these commonsense priors may be in-\nconsistent with actual navigation scenes and mislead the\nagent to making wrong action decision. To mitigate the\nimpact of the noisy priors, we introduce a CLIP-driven\ncorrectable landmark discovery scheme, where the priors\nare corrected in a learnable way according to actual ob-\nservations. A consistency loss is introduced to constrain\nthe prior correction using the navigation supervision. As\na result, at each navigation timestep, the importance of each\ncooccurrence is re-ranked based on the current observation\nto suppress the noisy cooccurrences for accurate navigation\ndecision. As shown in Fig. 1, through correctable landmark\ndiscovery, CONSOLE successfully highlights the important\nlandmark and cooccurrence (hall and artwork) for making\ncorrect action decision while getting rid of the impact of the\nnoisy ones (rug and chandelier).\nOur approach contains three core components: 1) Land-\nmark cooccurrence prior generation: We design customized\nprompts for ChatGPT to extract landmark cooccurrence pri-\nors, including navigational sequential landmarks and open-\nworld landmark cooccurrences. 2) Correctable landmark\ndiscovery: We introduce a learnable cooccurrence scoring\nmodule constrained by a consistency loss for accurate land-\nmark discovery based on the priors. The scoring module\nranks the importance of each cooccurrence according to\ndynamic navigation observations. 3) Observation enhance-\nment: We use corrected landmark features for enhancing\nobservation features to combine the landmark discovery\nframework with different VLN agents. The enhanced ob-\nservation features are adopted for final action decision.\nWe evaluate CONSOLE on multiple mainstream VLN\nbenchmarks, including R2R [1], REVERIE [2], R4R [3], and\nRxR [4]. The experimental results show that CONSOLE\noutperforms strong baselines [16], [24] on all benchmarks.\nMoreover, CONSOLE achieves the new state-of-the-art on\nR2R and R4R in unseen scenarios. We conduct extensive\ninsightful visualization to verify the effectiveness and neces-\nsity of different components in CONSOLE, such as the learn-\nable scoring module and the customized designs for the\nprompts. The visualization comprehensively reveals how\nCONSOLE successfully activates and utilizes the helpful\nnavigation knowledge in large models for assisting action\ndecision. We also present both quantitative and qualitative\ncomparison of different LLMs to deeply analyze their po-\ntential in assisting VLN tasks. We believe that our work will\nprovide a non-trivial reference in introducing large models\nfor assisting embodied AI tasks in a noise-suppressed way.\nTo summarize, our main contributions are:\n• We propose a novel VLN paradigm called CON-\nSOLE, where we activate and utilize the rich open-\nworld knowledge in large models in a correctable\nway, which effectively assists VLN while mitigating\nthe misleading brought by large models.\n• We cast VLN as an open-world sequential landmark\ndiscovery problem to effectively harvest the knowl-\nedge helpful for VLN from both LLM and VLM.\nBuilt upon an observation enhancement strategy, our\ncorrectable landmark discovery scheme can be com-\nbined with different VLN models elegantly.\n• CONSOLE outperforms strong baselines on multiple\npopular VLN benchmarks, including R2R, REVERIE,\nR4R and RxR. It also establishes new SOTA results\non R2R and R4R in unseen scenarios.\nB R ELATED WORK\nB.1 Vision-Language Navigation.\nIn Vision-Language Navigation (VLN), the agent needs to\nmove across complicated visual environments to reach a\ngoal location following human instructions [1], [2], [4].\nTherefore, accurate modality alignment is crucial for suc-\ncessful navigation. Early methods mainly design diverse\ndata augmentation techniques [6], [7], [8], [25], efficient\nlearning mechanisms [9], [10], [11], [26], [27], [28], [29], and\nuseful model architectures [12], [14], [26], [30] for learning\nuseful modality alignment knowledge. EnvDropout [6] cre-\nates the augmentation data by mimicking unseen scenes\nvia an environmental dropout scheme. AuxRN [9] designs\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\nmultiple self-supervised auxiliary training tasks to exploit\nrich semantic information in the environment. OAAM [30]\ndesigns a new model structure and loss for separately\nprocessing the mentioned objects and actions in the instruc-\ntions.\nRecent VLN works employ cross-modal pretraining\nparadigms and models for enhancing the modality align-\nment [15], [16], [24], [31], [32], [33], [34]. HAMT [16] in-\ntroduces a history-aware multimodal transformer to en-\ncode the long-horizon navigation history. DUET [24] builds\na dual-scale graph transformer to jointly perform long-\nterm action planning and fine-grained cross-modal under-\nstanding. HOP [33] introduces a new history-and-order\naware pretraining paradigm for encouraging the learning\nof spatio-temporal multimodal correspondence.\nIn this paper, we resort to existing powerful large models\nto cast VLN as an open-world sequential landmark dis-\ncovery problem, for facilitating action decision in unseen\nscenes. we introduce a novel correctable landmark discov-\nery scheme to correct the priors provided by large models\naccording to actual observations in a learnable way. By using\ncorrected landmark features for enhancing the observation\nfeatures, we fulfill an elegant combination of our framework\nwith different VLN agents.\nAlthough some VLN approaches have also explored to\ndecouple the landmark discovery for better cross-modality\nalignment [30], [31], [35], they tend to use the limited\ninformation in the instruction for aligning with the visual\nobservation. In contrast, our method uses large models to\nprovide rich commonsense prior accompanied by a power-\nful learnable prior correction paradigm, which enables the\nagent to learn much more open-world alignment knowledge\nfor successful navigation.\nB.2 LLMs & VLMs for Embodied Tasks.\nRecent studies have shown that LLMs and VLMs can con-\nnect human instructions to robot actions and facilitate the\nalignment between instructions and scenes [19], [20], [22],\n[36], [37], [38], [39], [40], [41], [42], thanks to rich real-world\nand open-world knowledge stored in them. Planner [36]\nemploys an LLM to decompose the abstract task instructions\nto concrete step-by-step executable actions for embodied\nagents. SayCan [19] further uses an LLM for assisting\nthe action decision by grounding the LLM through value\nfunctions of pretrained robotic skills. Inner Monologue [20]\nintroduces a VLM to provide environment feedbacks for an\nLLM to enable it to recover from failure in robotic control\nscenes. LM-Nav [37] combines an LLM, a VLM, and a visual\nnavigation model to enable long-horizon instruction fol-\nlowing without requiring user-annotated navigational data.\nHowever, directly using an LLM and/or a VLM for assisting\nVLN may introduce unexpected noise that confuses the\naction decision, due to the limited action space of VLN and\nthe large domain gap between the training corpora of large\nmodels and the VLN dataset.\nIn this work, we activate open-world alignment knowl-\nedge from both LLM and VLM to realize an open-world\nlandmark discovery paradigm for assisting VLN. To the\nbest of our knowledge, we are the first to harvest land-\nmark cooccurrence knowledge from LLM to assist the VLN\ntask. Through a correctable landmark discovery scheme, the\nnoise in the priors can be effectively suppressed for accurate\naction decision.\nA concurrent work ESC [43] also introduces the land-\nmark coocurrence knowledge from LLMs for assisting the\nzero-shot object navigation task. However, our CONSOLE\nis different from it in the following two key aspects: 1)\nwe introduce a learnable cooccurrence scoring module to\neffectively mitigate the noise in the knowledge, which does\nnot require complicated exploration procedure like Zhou\net al. [43] and hardly impacts the inference speed. This\nmodule can also be easily optimized through navigation\nsupervision. 2) Different from Zhou et al. [43] that focus on\nthe zero-shot setting, our work proposes a trainable scheme\nto elegantly combine LLM, VLM, and the VLN agent, which\nachieves significantly higher navigation performance than\nthe zero-shot setting.\nC P RELIMINARIES\nC.1 Problem Setup\nIn the discrete VLN task, an agent is initialized at a starting\nnode and needs to explore the navigation connectivity graph\nG = (V, E) to reach a target node, following a language\ninstruction I. V and E represent the nodes and edges in the\nnavigation connectivity graph. At each timestep t, the agent\nreceives a panoramic observation Ot of its current node. Ot\ncomposed of No single-view observations Ot,n, i.e., Ot =\n{Ot,n}No\nn=1. Each Ot,n contains an RGB image Bt,n and the\ndirectional information. With the instructions I and current\nvisual observations Ot, the agent infers the action at for\neach step t from the candidate navigable views N(Ot) ∈\nOt, which corresponds to the neighbor nodes of the current\nnode.\nIn this paper, we propose to cast VLN as an open-world\nlandmark discovery problem to introduce large models for\nassisting VLN tasks. We define the problem in the following.\nSpecifically, the goal of the open-world sequential landmark\ndiscovery is to discover the mentioned landmarks that the\nagent may not see before sequentially to complete the navi-\ngation. Denote the landmark list extracted from the instruc-\ntion I as Ula. At each timestep t, the agent first needs to\npredict the landmark Ula\nt which is needed to find currently\nfrom the landmark list Ula. Then, it discovers Ula\nt in the set\nof navigable viewpoints and makes the action prediction at\nby selecting the navigable viewpoint containing Ula\nt .\nC.2 Large Models\nBenefiting from ultra-large-scale web corpora, large pre-\ntrained models have shown excellent generalization abili-\nties to various tasks due to their vast knowledge storage.\nIn CONSOLE, we resort to two powerful large models\nChatGPT [23] and CLIP [18], to provide rich open-world\nalignment knowledge for facilitating action decision.\nChatGPT [23] is a powerful text generation model that\nshows great in-context reasoning ability, i.e., given with\ncustomized prompts with a few task examples, its internal-\nized knowledge about some specific task can be activated\nthrough text completion.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\nCLIP [18] is a vision-language model pretrained on\n400M image-text pairs collected from the web. It utilizes\na ViT [44] or a ResNet-50 [45] as the image encoder and a\nTransformer [46] as the text encoder. By conducting a sim-\nple similarity calculation between images and textual class\ndescriptions, it shows powerful zero-shot object recognition\nability in diverse datasets [18].\nC.3 Baseline VLN Agents\nWe choose two strong baselines HAMT [16] and DUET [24]\nto verify the effectiveness of CONSOLE. In this section,\nwe briefly describe one baseline HAMT. In HAMT, the\nagent receives the instruction I, the observation Ot, and\nthe navigation history Ht at each timestep t. For simplicity,\nwe omit the encoding process for the navigation history Ht.\nSpecifically, a text encoderEI(·) and a vision encoder EV (·)\nare employed to obtain the instruction feature fI and the\nobservation feature fOt, respectively:\nfI = EI(I), fOt = EV (Ot). (1)\nThen fI and fOt are updated through a cross-modal Trans-\nformer encoder Ec(·):\n˜ft\nI,˜fOt = Ec(fI, fOt). (2)\nThe action prediction probability at is generated through an\naction prediction module Ea(·) based on ˜ft\nI and ˜fOt:\nat = Ea(˜ft\nI,˜fOt). (3)\nThe action decision is optimized by the navigation loss\nLnav, which contains an imitation learning loss LIL and a\nreinforcement learning loss LRL [16] :\nLIL =\nX\nt\n−a∗\nt log(at), (4)\nLRL =\nX\nt\n−as\nt log(at)At, (5)\nLnav = LIL + λLRL, (6)\nwhere a∗\nt is the teacher action of the ground-truth path\nat timestep t, as\nt is the sampled action from the agent\naction prediction at, At is the advantage calculated by A2C\nalgorithm [47], and λ is a balance factor.\nIn CONSOLE, we use the corrected landmark features\nfUt and the observation features fOt to obtain the enhanced\nobservation features f′Ot at each timestep t. Then f′Ot are\nfed into Ec(·) and Ea(·) to get final action prediction ˜ at.\nD M ETHOD\nThe overview of CONSOLE is presented in Fig. 2. For a\ngiven instruction I, we first conduct the landmark cooccur-\nrence prior generation to obtain the landmark cooccurrence\npriors U (Sec. D.1). Then, at each timestep t, we perform the\ncorrectable landmark discovery through the landmark shift-\ning, the landmark discovery, and the learnable cooccurrence\nscoring, based on the priors U, the instruction I, and the\nobservations Ot (Sec. D.2). After the correctable landmark\ndiscovery, we obtain the corrected landmark feature fUtand\nuse it to enhance the observation feature fOt (Sec. D.3). We\nadopt the enhanced observation feature f′Ot for predicting\nTABLE 1: Notation summarization of variables.\nNotations Variables\nI instruction\nO t observation\nU landmark cooccurrence prior\nz shifting pointer\nU la\nt current important landmark\ns la landmark score\n{ s co\ni } N co\ni =1 cooccurrence score\nf I instruction feature\nf O t\nobservation feature\nf ′\nO t\nenhanced observation feature\nf S t\nstate feature\nf U t\ncorrected landmark feature\nL nav navigation loss\nL cs consistency loss\nL ct contrastive loss\nthe final action ˜ at. The total training objective of CONSOLE\ncontains the navigation loss Lnav and two additional intro-\nduced losses, i.e., a consistency loss Lcs and a contrastive\nloss Lct (Sec. D.4). For facilitating reading, we list the crucial\nvariables in CONSOLE in Table 1.\nD.1 Landmark Cooccurrence Prior Generation\nThe landmark cooccurrence prior contains the following\ntwo elements: 1) Navigational Sequential Landmarks: Due\nto the complexity of natural language, the order of land-\nmarks appearing in the instruction may be sometimes in-\nconsistent with the intended order to be searched dur-\ning navigation. Therefore, extracting navigational sequen-\ntial landmarks is crucial for the landmark discovery. 2)\nLandmark Cooccurrence: the open-world landmark cooc-\ncurrence knowledge, e.g., a counter may appear in a kitchen\nisland (Fig. 2), can greatly help the alignment. We design\ncustomized prompts with heuristic task examples to obtain\nthese two kinds of priors.\nNavigational Sequential Landmark Extraction. An ideal\nextraction of landmarks requires: (1) extraction of all men-\ntioned landmarks; (2) successful extraction of landmarks\nwith complex descriptions (e.g., landmarks with adjectives\nor phrases); (3) avoidance of abstract nouns (e.g., right); (4)\nnavigational order instead of textual order. To ensure the\ncoverage for diverse descriptions of landmarks for accurate\nextraction, we pre-define 5 different task examples for Chat-\nGPT. Taking REVERIE as an example, one task example is\nas follows:\nInstruction: Go to the lounge on the\nfirst level and bring the trinket that’s\nsitting on the fireplace.\nLandmarks:\n1.first level;\n2.lounge;\n3.fireplace;\n4.trinket.\nThrough such example, we can let ChatGPT know that\nit should extract navigational sequential landmarks rather\nthan textual sequential landmarks (1.lounge; 2.first level;\n3.trinket; 4.fireplace). Moreover, it should extract “first\nlevel” as the landmark rather than “level”. After in-context\nlearning with task examples, we can ask ChatGPT to extract\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\nText Encoder\n Cross-Modal Transformer Encoder\nAction Prediction\nObservations\nLandmark cooccurrence priors\n1. bathroom: bathtub, toilet,\nsink, ...\n2. kitchen: stove, oven,\nsink, ...\n3. kitchen island: kitchen,\ncounter, stove, ...\nLandmark \nShifting\nLearnable \nCooccurrence \nScoring\nkitchen island\nkitchen\ncounter\nstove\nsink\ndishwasher\ncurrent important \nlandmark/cooccurrence\nLandmark \nCooccurrence \nPrior Generation\nVision Encoder\nLandmark \nDiscovery\nkitchen island\nkitchen\ncounter\nstove\nsink\ndishwasher\nlandmark/cooccurrence \nprediction probability\nlandmark/cooccurrence \n score\nkitchen island\nkitchen\ncounter\nstove\nsink\ndishwasher\ncorrected landmark pre-\ndiction probability\nObservation \nEnhancement \nContrastive  Loss   \nTimestep \n Navigation Loss\nConsistency  Loss \nExit bathroom and turn right \ninto kitchen. Stop before \nkitchen island.\nInstruction\n1  2  3  4\n/\n/\nInstruction\n 4\n1 2 3 4\nCorrectable Landmark Discovery\nOriginal Input\nBaseline Module\nCONSOLE Module\nFrozen Module\nIntermediate Result\nFig. 2: Overview of CONSOLE. Before navigation, the landmark cooccurrence priors U are obtained through the landmark\ncooccurrence prior generation module. At navigation timestep t, the agent conducts the correctable landmark discovery\nbased on the landmark shifting, the landmark discovery, and the learnable cooccurrence scoring. An observation enhancing\nmodule is introduced to enhance the observation features fOt using the corrected landmark features fUt. And the enhanced\nobservations f′Ot are used for action decision. Besides the navigation loss Lnav, we introduce the consistency loss Lcs and\nthe contrastive loss Lct for optimization.\nR2R REVERIE R4R\nDatasets\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000Numbers\n5022 5786\n4273\n11334\n12948\n9843\nlandmarks landmarks and cooccurrences\nFig. 3: Numbers of Landmarks and Cooccurrences.\nthe landmarks of a specific instruction I with the following\nprompt:\nInstruction: I\nLandmarks:\nwhich is shown in Fig. 1 (upper left). We constrain Chat-\nGPT not to generate abstract landmarks and landmarks not\nmentioned in the instruction by stating it explicitly in the\nprompt.\nLandmark Cooccurrence Generation. Based on the ex-\ntracted navigational sequential landmarks, we can further\nobtain landmark cooccurrence priors from ChatGPT. Specifi-\ncally, we pre-define 2 task examples for ChatGPT to generate\ncooccurrences. One task example followed by the prompt is\nas follows:\nTell me 3 co-occurrences of bedroom:\n1.bed;\n2.mirror;\n3.nightstand;\nTell me 3 co-occurrences of Ula\nk :\nwhere Ula\nk is the k-th landmark in an instruction. We con-\nstrain ChatGPT to generate visible objects or scenes rather\nthan abstract objects (e.g., wind) as cooccurrences by stating\nit explicitly in the prompt. We only show 3 cooccurrences\ndue to the limited space. The whole prompts with all task\nexamples for landmark extraction and landmark cooccur-\nrence generation are given in the Supplementary Material.\nFig. 3 presents a visualization of number comparison\nbetween landmarks and cooccurrences. From Fig. 3 we\ncan find that by introducing cooccurrences for landmark\ndiscovery, the agent can learn to align more diverse ob-\njects with observations and therefore capture rich alignment\nknowledge.\nD.2 Correctable Landmark Discovery\nDue to the lack of visual constraints, the obtained priors\nmay be inconsistent with actual observations and cause mis-\nleading. To address this issue, we introduce a CLIP-driven\ncorrectable landmark discovery scheme to correct the priors\nin a learnable way according to actual observations, which\ncontains the landmark shifting, the landmark discovery, and\nthe learnable cooccurrence scoring. For the instruction I, let\nthe landmark cooccurrence priors beU = {Ula, Uco}, where\nUla = {Ula\nk }Nla\nk=1 and Uco = {{Uco\n1,i}Nco\ni=1 , ...,{Uco\nNla,i}Nco\ni=1 }\nrepresent the extracted landmark list and cooccurrences,\nrespectively. Nla and Nco are the numbers of landmarks\nand cooccurrences, respectively.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\nD.2.1 Landmark Shifting\nAt each timestep t, before the landmark discovery, we intro-\nduce a landmark shifting module based on CLIP to decide\nthe current important landmark Ula\nt from the landmark list\nUla. Since Ula already implies the intended order to be\nsearched, we keep a shifting pointer z for extracting two ad-\njacent landmarks {Ula\nz , Ula\nz+1} from Ula. The pointer z = 1\nwhen t = 0. The landmark shifting module predicts the\ncurrent important landmark Ula\nt from {Ula\nz , Ula\nz+1} based\non the observations Ot = {Ot,n}No\nn=1. For each image Bt,n\nin the single-view observation Ot,n, we first use the CLIP\nimage encoder EV\nCLIP(·) to obtain the feature fBt,n. And\nwe obtain the current observation feature fBt by an average\noperation:\nfBt = 1\nNo\nX\nn\nfBt,n = 1\nNo\nX\nn\nEV\nCLIP(Bt,n). (7)\nThen, we obtain the textual features {fUlaz\n, fUla\nz+1\n} via the\nCLIP text encoder ET\nCLIP(·):\nfUlaz\n= ET\nCLIP(Ula\nz ), fUla\nz+1\n= ET\nCLIP(Ula\nz+1). (8)\nWith fBt and fUlaz\n, we obtain the probability pUlaz\nthat the\ncurrent important landmark Ula\nt is Ula\nz by:\npUlaz\n= exp(sim(fBt, fUlaz\n)/τ)\nexp(sim(fBt, fUlaz\n)/τ) + exp(sim(fBt, fUla\nz+1\n)/τ),\n(9)\nwhere τ is the temperature parameter, exp(·) represents\nthe exponent operation, and sim(·, ·) denotes the similarity\ncomputation, which is the dot product between two fea-\ntures. The probability pUla\nz+1\nis calculated similarly. Then\nUla\nt = Ula\nz if pUlaz\n> pUla\nz+1\nand Ula\nt = Ula\nz+1 if pUlaz\n≤ pUla\nz+1\n.\nIf Ula\nt = Ula\nz+1, the pointer z will move to z + 1at timestep\nt + 1. Due to the scene complexity, the pointer z may not\nmove during many navigation steps. Therefore, we force\nthe movement of z when it continuously points to the same\nposition (see Fig. 4 for example). Specifically, we define\na counter to record the number of no-shifting steps. If\nthe number of no-shifting steps exceeds a pre-defined step\nthreshold, we force the pointer z to move to z + 1and reset\nthe counter to 0. The step threshold is set to 1 when an\ninstruction contains more than 3 landmarks. Otherwise, the\nstep threshold is set as 2.\nNote that Hong et al. also introduce a sub-instruction\nshifting module [48] which is learnable to facilitate the\nalignment between instructions and trajectories. Different\nfrom [48], we do not add learnable parameters in our\nheuristic landmark shifting module and instead construct\na learnable cooccurrence scoring module (Sec. D.2.3) that\neffectively facilitates cooccurrence-based correctable land-\nmark discovery. Moreover, we conduct fine-grained shifting\namong landmarks rather than sub-instructions to enable\nsequential landmark discovery.\nD.2.2 Landmark Discovery\nIn discrete VLN task, the actions are chosen from candidate\nsingle-view observations. Therefore, to adapt our landmark\ndiscovery framework for navigation action prediction, we\ntransform the landmark discovery as locating the landmark\nin one of the candidate observations, which is consistent\nwith the action prediction process. With the current im-\nportant landmark Ula\nt and its cooccurrences {Uco\nt,i}Nco\ni=1 , we\nconduct the landmark discovery through CLIP to highlight\nthe single-view observation where Ula\nt with {Uco\nt,i}Nco\ni=1 is\nmost likely to appear for assisting action decision. To this\nend, for each single-view observation Ot,n, we calculate the\nprobability pUla\nt,n\nthat the current important landmark Ula\nt\nappears in it by:\npUla\nt,n\n=\nexp(sim(fUla\nt\n, fBt,n)/τ)\nPNo\nm=1 exp(sim(fUla\nt\n, fBt,m)/τ)\n, (10)\nwhere fUla\nt\n= fUlaz\nor fUla\nz+1\n, fBt,n is the visual feature\nfor Ot,n (Eq. 7), exp(·) represents the exponent operation,\nand sim(·, ·) denotes the similarity computation, which is\nthe dot product between two features. We also calculate\nthe probability pUco\nt,i,n that Uco\nt,i appears in Ot,n for each\ncooccurrence Uco\nt,i like Eq. 10.\nD.2.3 Learnable Cooccurrence Scoring\nSince the cooccurrence priors are extracted without visual\nconstraints, the cooccurrences may not appear in the obser-\nvations or may appear in multiple single-view observations\nto cause misleading. Therefore, we introduce a learnable\ncooccurrence scoring module to correct the priors according\nto actual observations. Moreover, due to the complexity\nof the scene and the instruction, it is hard for the large\nmodel to determine the correct landmark totally correctly.\nThrough the introduction of the learnable cooccurrence\nscoring module trained together with the baseline agent,\nthe noise brought by the landmark shifting module and the\nlandmark discovery module can be effectively mitigated.\nSpecifically, we obtain the landmark score sla and the\ncooccurrence score {sco\ni }Nco\ni=1 through a dynamic updated\nstate feature fSt. Then we use sla and sco\ni to obtain the cor-\nrected landmark prediction probability ˜pUt,n based on the\nprobabilities pUla\nt,n\nand {{pUco\nt,1,n}No\nn=1, ...,{pUco\nt,Nco,n\n}No\nn=1}.\nThrough sla and sco\ni , the important cooccurrence in the\npriors can be highlighted while the unimportant ones are\nsuppressed by ˜pUt,n. At timestep t, we calculate fSt based on\nthe instruction feature fI (Eq. 1) and the current observation\nfeature fBt (Eq. 7):\nfSt = El([fcls\nI ; fBt]), (11)\nwhere fcls\nI is the instruction feature of the [CLS] token in the\nbaseline [16], [24], and El(·) contains a linear layer, a ReLU\nactivation function, a layer normalization function, and a\nDropout operation. Then, we obtain the cooccurrence score\nsco\ni for cooccurrence Uco\nt,i (1 ≤ i ≤ Nco) by:\nsco\ni = sim(fSt, fUco\nt,i), (12)\nwhere sim(·, ·) denotes the similarity computation which\nis the dot product between two features and fUco\nt,i =\nET\nCLIP(Uco\nt,i) is the textual feature. We also calculate the score\nsla for the current important landmark Ula\nt as Eq. 12. Based\non the original probability pUla\nt,n\nand {pUco\nt,i,n}Nco\ni=1 , we obtain\nthe corrected landmark prediction probability ˜pUt,n for the\nsingle-view observation Ot,n by:\n˜pUt,n = pUla\nt,n\n· sla +\nNco\nX\ni=1\npUco\nt,i,n · sco\ni , (13)\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\nConsistency Loss. To constrain the optimization of the\nlearnable cooccurrence scoring module, we introduce a\nconsistency loss Lcs, which is the cross-entropy loss and\ncalculated based on the corrected landmark prediction prob-\nability ˜pUt,n and the ground-truth action gt:\nLcs = −\nX\nt\ngtlog(˜ pUt), (14)\nwhere ˜ pUt = {˜pUt,n}No\nn=1. Since both action prediction and\nlandmark discovery aim at selecting the ground-truth candi-\ndate observation, by enforcing the consistency between ˜ pUt\nand gt, the scoring module can learn to correct the impor-\ntance of cooccurrences for accurate landmark discovery. As\nshown in Fig. 2, through the learnable cooccurrence scoring\nmodule under the constraint of the consistency loss Lcs,\nthe cooccurrence “counter” is significantly highlighted to\nindicate the ground-truth action.\nD.3 Observation Enhancement\nTo encourage VLN agents to learn sufficient open-world\nalignment knowledge under the correctable landmark dis-\ncovery framework, we design an observation enhancement\nstrategy, where we use the corrected landmark feature\nfUt = {fUt,n}No\nn=1 and the original observation feature\nfOt = {fOt,n}No\nn=1 to obtain the enhanced observation fea-\ntures f′Ot = {f′Ot,n}No\nn=1 for action decision. Specifically, at\ntimestep t, we calculate fUt,n by:\nfUt,n = pUla\nt,n\n· sla · fUla\nt\n+\nNco\nX\ni=1\npUco\nt,i,n · sco\ni · fUco\nt,i, (15)\nwhere fUla\nt\nand fUco\nt,i are the textual features of the landmark\nUla\nt and the cooccurrence Uco\nt,i. Through sla and sco\ni , the fea-\nture of the important cooccurrence can be highlighted. Then,\nwe use fUt,n and fOt,n to obtain the enhanced observation\nfeature f′Ot,n for Ot,n:\nf′\nOt,n = fOt,n + fUt,n. (16)\nThe enhanced observation features f′Ot = {f′Ot,n}No\nn=1 are\nfinally utilized for action decision.\nContrastive Loss. For improving the modality alignment\nand encouraging the enhanced observation feature f′Ot,n\nto be more discriminative for facilitating action decision,\nwe introduce a contrastive loss Lct. Through Lct, we en-\nforce the observation feature fOt,n to be close with the\npaired corrected landmark feature fUt,n while far away from\nthe non-paired corrected landmark feature fUt,n = {fUt,j }\n(1 ≤ j ≤ No and j ̸= n). Lct is composed of two losses\nLo→u\nct and Lu→o\nct . We calculate Lo→u\nct by:\nLo→u\nct = −\nX\nt,n\nlog( esim(fOt,n,fUt,n)/τ\nesim(fOt,n,fUt,n)/τ + P\nf Ut,n\nesim(fOt,n,f Ut,n)/τ ),\n(17)\nwhere sim(·, ·) denotes the similarity computation, which is\nthe dot product between two features. Lu→o\nct is calculated in\na symmetric way as Lo→u\nct . Then we obtain the contrastive\nloss Lct by Lct = 1\n2 (Lo→u\nct + Lu→o\nct ).\nD.4 Action Prediction\nThe action ˜ at is predicted based on the enhanced obser-\nvation feature f′Ot and the instruction feature fI through\nthe cross-modal Transformer encoder Ec(·) and the action\nprediction module Ea(·) (see Eq. 2 and Eq. 3). The total\ntraining objective L of CONSOLE is:\nL = Lnav + λ1Lcs + λ2Lct, (18)\nwhere λ1 and λ2 are the balance meta-parameters.\nE E XPERIMENTS\nE.1 Experimental Setup\nDatasets. We evaluate CONSOLE on four mainstream\nVLN benchmarks: R2R [1], REVERIE [2], R4R [3], and\nRxR [4]. R2R contains 90 indoor scenes with 7189 trajecto-\nries. REVERIE replaces the fine-grained instructions in R2R\nwith high-level instructions to locate remote objects. R4R\nconcatenates two adjacent tail-to-head trajectories in R2R,\nforming longer instructions and trajectories. RxR contains\nmuch more complex instructions and trajectories than R2R.\nSince CLIP [18] is pretrained on English language data,\nwe use the English subset of RxR (both en-IN and en-\nUS) for verification, which includes 26464, 2939, 4551 path-\ninstruction pairs for Training, Val Seen, and Val Unseen,\nrespectively.\nEvaluation Metrics.The evaluation metrics for R2R [1] are:\n1) Trajectory Length (TL): the average length of the agent’s\nnavigated path in meters, 2) Navigation Error (NE): the av-\nerage distance in meters between the agent’s destination and\nthe target viewpoint, 3) Success Rate (SR): the ratio of suc-\ncess, where the agent stops within three meters of the target\npoint, and 4) Success rate weighted by Path Length (SPL):\nsuccess rate normalized by the ratio between the length\nof the shortest path and the predicted path. For R4R [3]\nand RxR [4], we further employ 5) the Coverage weighted\nby Length Score (CLS), 6) the normalized Dynamic Time\nWarping (nDTW), and 7) the Success weighted by nDTW\n(SDTW) for measuring the path fidelity. For REVERIE [2],\nthree other metrics are used: 8) Remote Grounding Success\nRate (RGS): the ratio of grounding the correct object when\nstopping, 9) Remote Grounding Success rate weighted by\nPath Length (RGSPL): weight RGS by TL, and 10) Oracle\nSuccess Rate (OSR): the ratio of containing a viewpoint\nalong the path where the target object is visible.\nImplementation Details.We conduct all experiments on an\nNVIDIA 3090 GPU. The batch size is set to 4 and the model\nis trained for 300K iterations on all datasets. The navigation\nloss of CONSOLE is the same as that in each baseline. The\ntemperature parameter τ is set as 0.5 empirically. We use\nthe SGD optimizer with the learning rate 0.1 for training\nEl(·) in the learnable cooccurrence scoring module. The loss\nweights λ1 and λ2 are set as 0.1 on all datasets. The same\naugmented data in [16] is used for R2R for fair comparison.\nTo sufficiently utilize the navigation ability of existing VLN\nagents, we train CONSOLE using the pretrained checkpoint\nof the baseline.\nFor facilitating implementation, we use the CLIP visual\nfeatures (ViT-B/32) released by [16] instead of using the\nCLIP image encoder. And we pre-extract the landmark\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\nTABLE 2: Comparison with SOTA methods on R2R. * denotes using additional large-scale unannotated data. Bold and\nBlue denote the best and runner-up results.\nMethod Val Seen Val Unseen Test Unseen\nTL NE ↓ SR ↑ SPL ↑ TL NE ↓ SR ↑ SPL ↑ TL NE ↓ SR ↑ SPL ↑\nSeq2Seq [1] 11.33 6.01 39 - 8.39 7.81 22 - 8.13 7.85 20 18\nRCM+SIL(train) [26] 10.65 3.53 67 - 11.46 6.09 43 - 11.97 6.12 43 38\nEnvDropout [6] 11.00 3.99 62 59 10.70 5.22 52 48 11.66 5.23 51 47\nRelGraph [13] 10.13 3.47 67 65 9.99 4.73 57 53 10.29 4.75 55 52\nPREVALENT [49] 10.32 3.67 69 65 10.19 4.71 58 53 10.51 5.30 54 51\nORIST [31] - - - - 10.90 4.72 57 51 11.31 5.10 57 52\nVLN ⟳BERT [15] 11.13 2.90 72 68 12.01 3.93 63 57 12.35 4.09 63 57\nHOP [33] 11.51 2.46 76 70 12.52 3.79 64 57 13.29 3.87 64 58\nVLN-SIG [50] - - - - - - 72 62 - - 72 60\nKERM [51] 12.16 2.19 80 74 13.54 3.22 72 61 14.60 3.61 70 59\nMeta-Explore [52] 11.95 2.11 81 75 13.09 3.22 72 62 14.25 3.57 71 61\nScaleVLN* [53] 13.24 2.12 81 75 14.09 2.09 81 70 13.92 2.27 80 70\nNavGPT [54] - - - - 11.45 6.46 34 29 - - - -\nHAMT [16] (baseline) 11.15 2.51 76 72 11.46 3.62 66 61 12.27 3.93 65 60\nHAMT+CONSOLE (ours) 11.73 2.65 74 70 11.87 3.39 69 63 12.42 3.78 66 61\nDUET [24] (baseline) 12.32 2.28 79 73 13.94 3.31 72 60 14.73 3.65 69 59\nDUET+CONSOLE (ours) 12.74 2.17 79 73 13.59 3.00 73 63 14.31 3.30 72 61\ncooccurrence priors before training. To better activate the\nlandmark discovery capability of CLIP , we add the prompt\n“a photo of a” like [18] to obtain the CLIP textual feature\nfor a specific landmark during the landmark discovery. For\nthe landmark extraction, we use the same task examples\nfor R2R, R4R and RxR since the instructions for them\nare all fine-grained. For REVERIE which have high-level\ninstructions, we pre-define different task examples. Since\nthe number of landmarks in R2R augmented data is ∼75\ntimes larger than that of R2R Train split, using ChatGPT\ndirectly for cooccurrence extraction has low feasibility re-\ngarding both time and cost. Alternatively, we obtain the\ncooccurrences for the augmented data through a pre-built\nlandmark-cooccurrence vocabulary based on the landmark\ncooccurrence priors of R2R Train split. We compute the most\nsemantically similar landmark in the vocabulary for each\nlandmark in R2R augmentation data to ensure the quality\nof the extracted cooccurrences. For each path in R4R, we\ndirectly combine the landmarks and cooccurrences from 2\ncorresponding R2R instructions for improving efficiency.\nE.2 Quantitative Results\nComparison with SOTAs.Table 21, Table 3, Table 4, and\nTable 5 show the performance comparison between recent\napproaches and CONSOLE, where we can find that CON-\nSOLE outperforms the strong baselines HAMT [16] and\nDUET [24] consistently on R2R, REVERIE, R4R, and RxR\nespecially in unseen scenarios (e.g., CONSOLE outperforms\nDUET in SPL by 3% and 2% under Val Unseen and Test\nUnseen on R2R, respectively). The results demonstrate the\nstrong generalization ability of CONSOLE for diverse in-\nstructions and scenarios. Moreover, CONSOLE establishes\nnew SOTA results on R2R and R4R under unseen scenes.\nCONSOLE also shows good performance in seen scenar-\nios on different datasets, e.g., it significantly outperforms\nDUET in Val Seen on REVERIE. All these results show the\neffectiveness of CONSOLE, demonstrating that CONSOLE\n1. The original value 2.29 of NE under Val Unseen in HAMT is a\ntypo, which is actually 3.62 and confirmed by one of HAMT’s authors.\nsuccessfully activates and utilizes open-world knowledge\nfrom large models for assisting VLN.\nNote that although the concurrent work ScaleVLN [53]\noutperforms previous works as well as CONSOLE, it lever-\nages significantly larger data resources and therefore a direct\ncomparison would not be fair. Another concurrent work,\nNavGPT [54], introduces LLM to serve as the navigation\ndecision backbone directly. Different from NavGPT [54], our\nCONSOLE combines the advantages of both LLM and ex-\nisting vision-based VLN models to improve the navigation\nperformance.\nAblation Study. To further analyze the effects of different\ncomponents in CONSOLE, we conduct extensive ablation\nstudies, and the results are given in Table 6 and Table 7.\nSpecifically, to analyze the effects of two core modules in\nCONSOLE, i.e., the learnable cooccurrence scoring module\n(LS) and the observation enhancement module (OE), we\ngive the ablation study results of them for different base-\nlines and datasets in Table 6. Concretely, “LS & No OE”\noutperforms “No LS & No OE” by generally 0.8% improve-\nment on different baselines and datasets, which shows the\neffectiveness of learnable cooccurrence scoring module. For\nexample, “LS & No OE” outperforms “No LS & No OE”\nby 1.06% and 0.87% on SPL for REVERIE-DUET and R2R-\nDUET, respectively. Similarly, “LS & OE” outperforms “LS\n& No OE” on all metrics on different baselines and datasets,\nwhich also shows the effectiveness and the generalization\nability of the observation enhancement module. The results\nin Table 6 sufficiently show the effectiveness of two core\nmodules in CONSOLE.\nWe further conduct the ablation studies for the cooc-\ncurrence number, landmark shifting module, strategies for\nfusing the corrected landmark features and the original ob-\nservation features, loss weights, and temperature parameter,\nand the results are presented in Table 7. From Table 7 we\ncan draw into the following conclusions: 1) The results of\n“Cooccurrence number” show that the landmark cooccur-\nrence priors greatly facilitate the landmark discovery, e.g.,\n“num=5” outperforms “num=0” by 0.55% and 0.64% in SR\nand SPL, respectively. Moreover, the results of “num=5,10”\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\nTABLE 3: Navigation and object grounding performance on REVERIE. Bold and underline denote the best and runner-up\nresults.\nMethod Val Seen Val Unseen Test UnseenSR↑OSR↑SPL↑RGS↑RGSPL↑SR↑OSR↑SPL↑RGS↑RGSPL↑SR↑OSR↑SPL↑RGS↑RGSPL↑Seq2Seq [1]29.5935.7024.0118.9714.964.208.072.842.161.633.996.883.092.001.58RCM [26]23.3329.4421.8216.2315.369.2914.236.974.893.897.8411.686.673.673.14SMNA [12]41.2543.2939.6130.0728.988.1511.286.444.543.615.808.394.533.102.39FAST-MATTN [2]50.5355.1745.5031.9729.6614.4028.207.197.844.6719.8830.6311.6011.286.08SIA [55]61.9165.8557.0845.9642.6531.5344.6716.2822.4111.5630.8044.5614.8519.029.20VLN⟳BERT [15]51.7953.9047.9638.2335.6130.6735.0224.9018.7715.2729.6132.9123.9916.5013.51HOP [33]54.8156.0848.0540.5535.7930.3935.3025.1018.2315.3129.1232.2623.3717.1313.90TD-STP [32]- - - - - 34.8839.4827.3221.1616.5635.8940.2627.5119.8815.40KERM [51]71.8974.4964.0457.5551.2249.0253.6534.8333.9724.1452.2657.4437.4632.6923.15DUET [24]71.7573.8663.9457.4151.1446.9851.0733.7332.1523.0352.5156.9136.0631.8822.06CONSOLE (ours)74.1476.2565.1560.0852.6950.0754.2534.4034.0523.3355.1359.6037.1333.1822.25\nTABLE 4: Comparison on R4R val unseen split.\n* denotes the results of our re-implementation.\nMethods NE↓ SR↑ CLS↑ nDTW↑ SDTW↑\nSF [7] 8.47 24 30 - -\nRCM [26] - 29 35 30 13\nPTA [56] 8.25 24 37 32 10\nEGP [14] 8.0 30.2 44.4 37.4 17.5\nRelGraph [13] 7.43 36 41 47 34\nVLN⟳BERT [15] 6.67 43.6 51.4 45.1 29.9\nHAMT [16] 6.09 44.6 57.7 50.3 31.8\nHAMT* [16] 6.25 41.50 58.56 51.50 30.69\nCONSOLE (ours) 5.99 43.22 61.45 54.34 32.72\nTABLE 5: Comparison on RxR-En val unseen split.\n* denotes the results of our re-implementation.\nMethods SR↑ SPL↑ CLS↑ nDTW↑ SDTW↑\nEnvDrop [6] 38.5 34 54 51 32\nSyntax [57] 39.2 35 56 52 32\nHAMT* [16] 43.20 40.41 59.57 55.52 36.39\nCONSOLE (ours) 46.34 42.92 60.85 57.11 38.85\nshow that adding too many co-occurrences may not bring\nfurther performance improvement. It is reasonable since\ntoo many co-occurrences may introduce unexpected noise\nto mislead the agent in the meanwhile. 2) The results of\n“Landmark Shifting” show that landmark shifting mod-\nule is helpful for accurate landmark discovery. Especially,\n“Shift” outperforms “Global” by 0.91% in SPL. 3) The results\nof “strategies for fusing the corrected landmark features and\nthe original observation features” show that “summation”\noutperforms “concatenation” and “concatenation” is com-\nparable with the baseline. This result reveals that “sum-\nmation” is a more effective way for fusing the corrected\nlandmark features and the original observation features. 4)\nThe results of “Loss weights” show that all settings with\ndifferent scales of the loss weights λ1 and λ2 outperform\nthe baseline, demonstrating that our method is insensitive\nto the loss weight. Moreover, our current setting ( λ1 = 0.1\nand λ2 = 0.1) achieves the best performance among other\nloss weight settings, demonstrating that selecting the proper\nloss weight to balance the scale of different loss items is\nhelpful for improving the performance. 5) The results of the\ntemperature parameter τ show that our method is not sen-\nsitive to the selection of the temperature parameter and the\nperformance is best in most metrics when the temperature\nparameter is set to 0.5.\nComparison of different LLMs.We further conduct quan-\ntitative experiments to analyze how different LLMs impact\nthe navigation performance. Specifically, we use different\nLLMs for navigational sequential landmark extraction, in-\ncluding ChatGPT (used by our CONSOLE), GPT-3 [17] (the\npredecessor of ChatGPT), Vicuna [58] (a recent strong open\nsource alternative to ChatGPT which has two sizes——7B\nand 13B), and Spacy [59] (a NLP library), and feed the\nextracted landmarks into the consequent modules. We ac-\ncess the APIs of ChatGPT and Vicuna [58] in May, 2023.\nWe access the API of GPT-3 [17] in Dec, 2022. The results\nare given in Table 8. From Table 8 we can observe that: 1)\nChatGPT outperforms other competitors in most metrics; 2)\nGPT-3, Vicuna13B, and Vicuna7B are comparable to Chat-\nGPT in some metrics; 3) Vicuna13B outperforms Vicuna7B\nespecially in SPL; 4) Spacy performs the worst among all\ncompetitors. These results show that more powerful LLMs\ncan provide more accurate knowledge for navigation.\nE.3 Visualization\nIn this subsection, we present rich visualization results to\ndeeply analyze the effects of different modules in CON-\nSOLE, including the learnable cooccurrence scoring module,\nthe landmark shifting module, the customized designs of\nprompts, etc.\nAction Decision. Fig. 4 gives some visualization exam-\nples of action decision, where we can find that CONSOLE\neffectively corrects the landmark cooccurrence priors for\nfacilitating action prediction. For example, in Fig. 4(a), the\nimportance weight of “carpet” which appears in “candi-\ndate 1” but does not appear in “candidate 5 (GT)” is sig-\nnificantly low. And the importance weights of “landing”,\n“steps”, and “handrail’ which appear in “candidate 5 (GT)”\nare relatively high. As a result, the landmark prediction is\nsuccessfully corrected to indicate the GT action.\nLandark shifting. Fig. 5 shows that our landmark shifting\nmodule takes better advantage of the landmark navigation\norder for landmark discovery. It can effectively mitigate\nthe stuck and disordered landmark prediction caused by\nscene complexity. For example, in Fig. 5(a) since the mirror\nappears in the observations during multiple steps, “No-\nforcedshift” continuously predicts “mirror” as the current\nimportant landmark. CONSOLE, however, predicts correct\nlandmarks at different timesteps.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nTABLE 6: Ablation study of the learnable cooccurrence scoring module (LS) and the observation enhancement module\n(OE) for different baselines (HAMT [16] & DUET [24]) and datasets (R2R [1] & REVERIE [2]).\nMethod R2R-HAMT R2R-DUET REVERIE-DUET\nNE ↓ SR ↑ SPL ↑ NE ↓ SR ↑ SPL ↑ SR ↑ OSR ↑ SPL ↑ RGS ↑ RGSPL ↑\nBaseline 3.62 66 61 3.31 72 60 46.98 33.73 51.07 32.15 23.03\nNo LS & No OE 3.53 67.39 62.42 3.12 72.54 60.64 48.68 33.27 54.96 32.83 22.53\nLS & No OE 3.46 68.03 62.59 3.07 73.35 61.51 49.42 34.33 53.82 33.51 23.32\nLS & OE 3.39 68.62 63.06 3.00 73.44 63.02 50.07 34.40 54.25 34.05 23.33\n(b)\nWalk along the narrow rug past the statue on \nthe table, and go up two steps. Wait on the \nthird step.\nInstruction Importance weights(a)\nObservations\nhandrail  landing  carpet    riser   tread  steps \n0.13         0.41        0.07     0.18     0.07  0.13\n1   2   3  4   5 \ncarpet\nhandrail\nlanding\nsteps\nTurn around and walk across the kitchen. When you \nreach the stove, turn left and exit the room into the dining \nroom. Walk past the table and stop at the other end.\nInstruction Importance weights\nObservations\nkitchen  refrigerator  sink  cabinets   countertop  stove \n0.19          0.16         0.16     0.13             0.26          0.11\n1   2   3  4   5 \ncabinets countertop\nFig. 4: Visualization examples of action decision. The ground-truth (GT) action and the corrected landmark prediction are\ndenoted in the green boxes.\nTABLE 7: Ablation Studies of cooccurrence number, land-\nmark shifting module, feature fusion strategies, loss\nweights, and temperature parameter on R2R.\nMethod\nVal Unseen\nNE ↓ SR ↑ SPL ↑\nBaseline [16] 3.62 66 61\n▷ Cooccurrence number:\nnum=0 3.60 66.84 61.78\nnum=1 3.60 66.45 61.62\nnum=5 3.53 67.39 62.42\nnum=10 3.52 67.31 62.19\n▷ Landmark shifting:\nGlobal 3.59 66.88 61.51\nNoforcedshift 3.54 67.22 62.05\nShift 3.53 67.39 62.42\n▷ strategies for fusing landmark features and observation features:\nconcatenation 3.61 66.24 61.12\nsummation 3.46 68.03 62.59\n▷ Loss weights:\nλ 1 =0.1 & λ 2 =1 3.50 67.48 62.56\nλ 1 =1 & λ 2 =0.1 3.63 67.35 62.53\nλ 1 =1 & λ 2 =1 3.58 67.65 62.26\nλ 1 =0.1 & λ 2 =0.1 (ours) 3.39 68.62 63.06\n▷ Temperature parameter τ :\nτ =0.05 3.48 67.56 62.36\nτ =0.1 3.44 68.03 62.71\nτ =1 3.50 68.11 63.10\nτ =0.5 (ours) 3.39 68.62 63.06\nImpact of task examples and prompts.Fig. 6 gives the vi-\nsualization of landmark extraction and cooccurrence gener-\nation regarding different task examples and prompts. From\nTABLE 8: Comparison of LLMs on R2R. The baseline we\nchoose is DUET [24].\nMethod\nVal Unseen\nNE ↓ SR ↑ SPL ↑\nBaseline [16] 3.31 72 60\nSpacy 3.11 73.05 61.33\nGPT-3 3.00 73.82 61.56\nVicuna7B 3.06 73.31 61.65\nVicuna13B 3.07 73.39 62.14\nChatGPT (ours) 3.00 73.44 63.02\nFig. 6 we can find that by providing task examples regarding\ndiverse landmark descriptions, we can successfully improve\nthe landmark extraction for LLM. For example, in Fig. 6(a),\nwhen prompted with only one example that extracts land-\nmarks of a single noun, ChatGPT only extracts table. After\nadding the two examples that extract noun-phrase land-\nmarks, table with chinacan be successfully extracted.\nMoreover, we can also observe that adding the con-\nstraint in the prompt can effectively enable LLM to generate\ndesired occurrences. For example, in Fig. 6(b), ChatGPT\ngenerates abstract words such as support without the con-\nstraint. However, through adding the constraint, ChatGPT\ngenerates more reliable cooccurrences like statue.\nNavigational sequential landmark extraction. To verify\nthe capability of existing LLMs for accurate landmark ex-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\n(b)\nInstruction\nstep 1 step 3\nstep 2 step 4\nCurrent important landmark\nGlobal  steps  steps  narrow rug  narrow rug \nNoforcedshift  narrow rug  narrow rug  narrow rug  narrow rug\nCONSOLE  narrow rug  narrow rug  table  steps\n step 1  step 2  step 3  step 4 \nnarrow rug\nnarrow \nrug\nsteps\nnarrow \n rug\ntable\nsteps\nWalk along the narrow rug past the statue on the table , \nand go up two steps. Wait on the third step.\nnarrow \n rug\nInstruction\nstep 1 step 2 step 3\nmirror\nmirror\nopen\ndoor\nopen\ndoor\nstep 4 step 5 step 6\nlong \ntable\nmirror mirror\nCurrent important landmark\nGlobal  mirror  open door  open door  open door    open door  open door\nNoforcedshift     mirror  mirror  mirror  mirror          mirror  mirror\nCONSOLE  mirror  mirror  open door  open door    open door  long table\nstep 1  step 2             step 3        step 4  step 5  step 6 \nmirror\nWalk towards the mirror, and take a right. Walk straight \nthrough the open door in front of you. Make a hard left, \nthen another. Wait by the long table.\n(a)\nFig. 5: Visualization example of landmark shifting. The ground-truth action (GT) and the mentioned landmark are\nannotated in the green boxes and orange boxes, respectively.\nInput Instruction Task Examples Landmark Parsing\nExit the bedroom, go \npassed the table with \nchina on it, turn left into \nthe doorway, and then \nturn right, go straight \npassed the china case \nand then turn left into \nthe doorway. Step \nbetween the bed and \nthe window on the right \nand stop.\nInstruction: Exit the room. Turn left and go down \nthe hallway. Continue down the hallway until you \nget to a stairs. Turn left and go up the first step.\nLandmarks:\n1. room 2. hallway 3. hallway 4. stairs 5. step\n[bedroom, table, doorway, \nchina case, doorway, bed, \nwindow]\n+Instruction: Walk into the hallway and through \nthe entrance to the kitchen area. Walk Passed the \nsink and stove area and stop between the \nrefrigerator and dining table.\nLandmarks:\n1. hallway 2. entrance 3. kitchen area 4. sink 5.\nstove area 6. refrigerator 7. dining table\n+Instruction: Walk past the TV and continue \ntoward the bathroom. Stop before walking \nthrough the bathroom door.\nLandmarks:\n1. TV 2. bathroom 3. bathroom door\n[bedroom, table with china, \ndoorway, china case, \ndoorway, bed, window] \nTask Description Cooccurrence Generation\nNone\n+\nbedroom example\n+sink example\nTell me 10 cooccurrences of pillar:\n1. column     2. support\n3. structure   4. foundation\n5. architecture\nGiven a target landmark in navigation, you need to \ngive 10 possible\nco-occurrences of this landmark based on real-world \ncommon sense.\nRequirement: These co-occurrences need to be \nobjects or scenes in an indoor or outdoor \nenvironment that can be observed by the robot.\nBelow you will find several examples of \ncooccurrences extraction and the target landmark, \noutput by format:\n+\nbedroom example\n+sink example\nTell me 10 cooccurrences of pillar:\n1. column     2. statue\n3. arch 4. staircase\n5. balcony\n(a) Landmark Parsing (b) Cooccurrence Generation\nFig. 6: Visualization of comparison of different task examples and prompts.\ntraction, we give the visualization examples of landmark\nextraction comparison among different methods in Fig. 7.\nFrom Fig. 7, we can find that: (1) ChatGPT performs the best\ngenerally among all compared models; (2) Vicuna13B [58]\nis comparable to ChatGPT while sometimes extracting\nwrong navigational order; (3) Vicuna7B [58] also suffers\nfrom wrong navigational order (e.g., in Fig. 7(a), Vicuna7B\nextracts the “stone tiles” before extracting the “wodden\nfloor”); (4) GPT-3 [17] is slightly inferior to ChatGPT for\nfine-grained landmark extraction (e.g., in Fig. 7(a), GPT-3\nextracts the “wall” and the “fireplace” rather than extracting\nthe “wall containing fireplace”); (5) Spacy [59] can only\nextract the textual order rather than the navigational order\nand extracts a lot of noise (e.g., in Fig. 7(b), Spacy extracts\nthe “right”, the “rest”, and the “base”). These landmark\nextraction visualization results reveal the great potential of\nutilizing LLMs for assisting VLN. Moreover, it is also im-\nportant to resort to more low-cost LLMs such as Vicuna [58]\nfor helping embodied agents in the future.\nLandmark cooccurrence extraction.To verify the superior-\nity of LLM in landmark cooccurrence extraction, we present\nthe comparison of obtaining landmark cooccurrences be-\ntween ChatGPT and ConceptNet in Fig. 8. To get the cooc-\ncurrences of a target landmark through ConceptNet, we\nrespectively request its related terms and nodes which are\nconnected with relations of ’AtLocation’ and ’LocatedNear’.\nFrom Fig. 8 we can observe that LLM can generate visible\ncooccurrences as constrained in the prompt (Sec. D.1) which\nare informative and high-quality. ConceptNet, however,\ntends to generate synonyms and concepts rather than visible\ncooccurrences. For example, in Fig. 8 (a), for the target\nhall, ChatGPT generates multiple cooccurrences of hall like\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\n[bed, door, hallway, stairs, animals]SpacyGPT-3ChatGPT[wall, fireplace, long table, your left, double doors, your right, wooden floor, stone tiles][wall with fireplace, long table, double doors, wooden floor, stone tiles]InstructionLandmarks[wall, fireplace, long table, double doors, wooden floor, stone tiles](a)(b)Instruction(c)Instruction(d)Instruction\n[wall containing fireplace, long table, double doors, (woorden floor),  stone tiles] [wall, fireplace, long table, double doors, stone tiles, wooden floor]Vicuna7BVicuna13BWith the wall containing the fireplace behind you, walk forward with the long table to your left, exiting through the double doors on your right after passing the long table.Stop after moving from the wooden floor to the stone tiles.Walk up the stairs, turn right and walk up the rest of the stairs, turn right at the top of the stairs, stop at the base of the next staircase.SpacyGPT-3ChatGPT[stairs, right, rest, top, base, next staircase][stairs, top of the stairs, base of the next staircase]Landmarks[stairs, stairs, top of stairs, staircase][stairs, (top of the stairs), (base of the next staircase)]Vicuna7BVicuna13B[stairs, top of stairs, base of next staircase]Go to bathroom on the second floor that has a linen closet a mosaic floor tiled shower stall and a window and open the window to air out the room.SpacyGPT-3ChatGPT[bathroom, second floor, that, linen closet, mosaic floor, shower stall, window, window, room][second floor, bathroom, linen closet, mosaic floor, shower stall, window]Landmarks[second floor, bathroom, linen closet, mosaic floor, tiled shower stall, window, window][second floor, bathroom, mosaic floor, tiled shower stall, window, linen closet]Vicuna7BVicuna13B[second floor, linen closet, bathroom, shower stall, mosaic floor, window]Go to the utilityroom on the second floor and clean the picture hanging between the door and window.SpacyGPT-3ChatGPT[utilityroom, second floor, picture, door, window][second floor, utility room, door, window, picture]Landmarks[second floor, utilityroom, door, window, picture][second floor, utilityroom, picture, wall, window, door]Vicuna7BVicuna13B[second floor, utilityroom, picture, door, window]\nFig. 7: Comparison of navigational sequential landmark extraction among different LLMs. The correct landmarks are\ndenoted in orange. For extraction results, noise is marked in red, landmarks in the wrong order are marked in blue, and\nmissing landmarks are marked in purple with brackets.\nTarget Landmark Hall Base of stairs Column\nConceptNet\n1. hallway\n2. foyer\n3. banquet hall\n4. building\n5. conference\n1. baserunning\n2. bases\n3. base\n4. baseman\n5. sac bunt\n1. excel spreadsheet\n2. columns\n3. entasis\n4. columnist\n5. memorial\nChatGPT\n1. stairs\n2. door\n3. rug\n4. chandelier\n5. artwork\n1. handrail\n2. steps\n3. landing\n4. carpet\n5. wall\n1. arch\n2. beam\n3. statue\n4. pedestal\n5. staircase\nLandmark ConceptNet ChatGPT\nhall\n1. hallway\n2. foyer\n3. banquet hall\n4. building\n5. conference\n1. stairs\n2. door\n3. rug\n4. chandelier\n5. artwork\n(a) Landmark ConceptNet ChatGPT\nbase of \nstairs\n1. baserunning\n2. bases\n3. base\n4. baseman\n5. sac bunt\n1. handrail\n2. steps\n3. landing\n4. carpet\n5. wall\n(b) Landmark ConceptNet ChatGPT\ncolumn\n1. excel spreadsheet\n2. columns\n3. entasis\n4. columnist\n5. memorial\n1. arch\n2. beam\n3. statue\n4. pedestal\n5. staircase\n(c)\nFig. 8: Comparison results of obtaining cooccurrences between ChatGPT and ConceptNet.\nartwork and chandelier to help recognize the location of the\nhall. However, ConceptNet retrieves hallway and building\nwhich are uninformative.\nMoreover, LLM can adapt to flexible inputs and process\nthem robustly, whereas ConceptNet may generate poor re-\nsults without preprocessing. In Fig. 8 (b), for the target base\nof stairs(landmark formed as a phrase), ChatGPT generates\nvalid cooccurrences of stairs such as handrail. However,\nConceptNet is unable to focus on the important part ( stairs)\nand instead searches invalid related terms of base.\nF C ONCLUSION\nIn this paper, we propose CONSOLE for VLN, which\ncasts VLN as an open-world sequential landmark discovery\nproblem by introducing a correctable landmark discovery\nframework based on two powerful large models. We harvest\nrich landmark cooccurrence commonsense from ChatGPT\nand employ CLIP for conducting landmark discovery. A\nlearnable cooccurrence scoring module is constructed to\ncorrect the priors provided by ChatGPT according to ac-\ntual observations. Experimental results show that CON-\nSOLE outperforms strong baselines consistently on R2R,\nREVERIE, R4R, and RxR. CONSOLE establishes new SOTA\nresults on R2R and R4R under unseen scenarios.\nWe believe that our work can provide a meaningful\nreference to the researchers in both the VLN and the em-\nbodied AI areas in how to effectively harvest and utilize\nthe helpful knowledge inside large models for assisting\nrobotic tasks, which is a very promising way to improve\nthe performance of robots. In future work, we would like to\nresort to more low-cost large models for assisting embodied\nAI tasks, which is more practical in real-world applications.\nIt is also worth studying to develop efficient in-domain\npretraining paradigms to adapt large models to embodied\nAI tasks.\nAPPENDIX\nNAVIGATIONAL SEQUENTIAL LANDMARK EXTRAC -\nTION PROMPTS\nIn this subsection, we present the prompt with task ex-\namples of landmark extraction. Specifically, we state in the\nprompt to 1) ask ChatGPT to extract navigational sequential\nlandmarks rather than textual sequential landmarks, and\n2) constrain ChatGPT not to generate abstract landmarks\nand landmarks not mentioned in the instruction. We use\nthe same task examples for R2R, R4R and RxR since the in-\nstructions for them are all fine-grained. For REVERIE which\nhave high-level instructions, we pre-define 5 different task\nexamples. Through being provided the customized prompt\nwith diverse task examples, ChatGPT is able to extract\nnavigational sequential landmarks with different kinds of\ndescriptions, such as landmarks composed of several nouns\nand landmarks with adjectives.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13\nFor a given instruction I, the whole prompt with task\nexamples for the landmark extraction on R2R, R4R, and RxR\nare as follows:\nGiven an instruction, you need to extract\nthe landmarks in the instruction and sort\nthem in the order in which they appear\nin the real navigation (not in the order\nthey appear in the instruction). Landmarks\nmust be the actual objects and scenes you\nsee in the navigation, and do not include\nother abstract nouns such as ‘‘left’’ and\n‘‘right’’.\nRequirement 1: Extract all landmarks in\nthe instruction.\nRequirement 2: do not generate landmarks\nthat are not in the instruction.\nBelow you will find several examples of\nlandmark extraction and the instruction you\nneed to complete the extraction, output by\nformat:\nInstruction: Exit the room. Turn left\nand go down the hallway. Continue down the\nhallway until you get to the stairs. Turn\nleft and go up the first step.\nLandmarks:\n1. room;\n2. hallway;\n3. hallway;\n4. stairs;\n5. step.\nInstruction: Walk into the hallway and\nthrough the entrance to the kitchen area.\nWalk Passed the sink and stove area and stop\nbetween the refrigerator and dining table.\nLandmarks:\n1. hallway;\n2. entrance;\n3. kitchen area;\n4. sink;\n5. stove area;\n6. refrigerator;\n7. dining table.\nInstruction: Walk past the TV and\ncontinue toward the bathroom. Stop before\nwalking through the bathroom door.\nLandmarks:\n1. TV;\n2. bathroom;\n3. bathroom door.\nInstruction: Walk between the columns and\nmake a sharp turn right. Walk down the steps\nand stop on the landing.\nLandmarks:\n1. columns;\n2. steps;\n3. landing.\nInstruction: With the windows on your\nleft, walk through the large room past the\nsitting areas. Go through the door left of\nthe tapestry and enter a wood-paneled room\nwith a circular table in the middle. Go up\nthe stairs and stop on the sixth step from\nthe bottom.\nLandmarks:\n1. windows;\n2. large room;\n3. sitting areas;\n4. door;\n5. tapestry;\n6. wood-paneled room;\n7. circular table;\n8. stairs;\n9. step.\nInstruction: I\nLandmarks:\nThe task examples on REVERIE are shown below:\nInstruction: Go to the lounge on the\nfirst level and bring the trinket with the\nclock that’s sitting on the fireplace.\nLandmarks:\n1. first level;\n2. lounge;\n3. fireplace;\n4. clock;\n5. trinket.\nInstruction: Go to the staircase by\nentryway and touch the front of the banister\nof the staircase.\nLandmarks:\n1. entryway;\n2. staircase;\n3. staircase;\n4. banister.\nInstruction: Go to the bedroom with the\nfireplace and bring me the lowest hanging\nsmall picture on the right wall across from\nthe bedside table with the lamp on it.\nLandmarks:\n1. bedroom;\n2. fireplace;\n3. bedside table;\n4. lamp;\n5. wall;\n6. small picture.\nInstruction: Go to the bedroom on level\n2 to the right of the green bathroom and\nremove the white pillow closest to the\nbedroom door from the bed.\nLandmarks:\n1. level 2;\n2. green bathroom;\n3. bedroom;\n4. bedroom door;\n5. bed;\n6. white pillow.\nInstruction: Go to the first level\nbedroom adjacent to the hallway leading to\nthe lounge and dust the sofa chair and place\n2 more pillows on it.\nLandmarks:\n1. first level;\n2. hallway;\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14\n3. lounge;\n4. bedroom;\n5. sofa chair;\n6. pillows.\nLANDMARK COOCCURRENCE GENERATION\nPROMPTS\nIn this subsection, we give the prompt with task examples of\ncooccurrence extraction. We state in the prompt to constrain\nChatGPT to generate visible objects or scenes rather than\nabstract objects (e.g., wind) as cooccurrences. For the k-th\nlandmark Ula\nk in the extracted landmark lists of instruction\nI, the whole prompt with task examples are presented\nbelow:\nGiven a target landmark in navigation,\nyou need to give 10 possible co-occurrences\nof this landmark based on real-world common\nsense.\nRequirement: These co-occurrences need\nto be objects or scenes in an indoor or\noutdoor environment that can be observed\nby the robot.\nBelow you will find several examples of\nco-occurrences extraction and the target\nlandmark, output by format:\nTell me 10 co-occurrences of bedroom:\n1. bed;\n2. door;\n3. window;\n4. mirror;\n5. closet;\n6. rug;\n7. curtains;\n8. walls;\n9. ceiling;\n10. floor.\nTell me 10 co-occurrences of sink:\n1. water;\n2. faucet;\n3. basin;\n4. counter;\n5. tile;\n6. porcelain;\n7. chrome;\n8. soap;\n9. towel;\n10. mirror.\nTell me 10 co-occurrences of Ula\nk :\nWe empirically find that 2 task examples for the cooccur-\nrence generation are enough in CONSOLE.\nACKNOWLEDGMENTS\nThis work was supported in part by National Science and\nTechnology Major Project (2020AAA0109704), Guangdong\nOutstanding Youth Fund (Grant No. 2021B1515020061),\nMobility Grant Award under Grant No. M-0461, Shen-\nzhen Science and Technology Program (Grant No.\nGJHZ20220913142600001), Nansha Key RD Program un-\nder Grant No.2022ZD014, CAAI-Huawei MindSpore Open\nFund. We thank MindSpore for the partial support of this\nwork, which is a new deep learning computing framwork 2.\nREFERENCES\n[1] P . Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunder-\nhauf, I. Reid, S. Gould, and A. van den Hengel, “Vision-and-\nlanguage navigation: Interpreting visually-grounded navigation\ninstructions in real environments,” in CVPR, 2018.\n[2] Y. Qi, Q. Wu, P . Anderson, X. Wang, W. Y. Wang, C. Shen, and\nA. van den Hengel, “Reverie: Remote embodied visual referring\nexpression in real indoor environments,” in CVPR, 2020.\n[3] V . Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge,\n“Stay on the path: Instruction fidelity in vision-and-language\nnavigation,” in ACL, 2019.\n[4] A. Ku, P . Anderson, R. Patel, E. Ie, and J. Baldridge, “Room-across-\nroom: Multilingual vision-and-language navigation with dense\nspatiotemporal grounding,” in EMNLP, 2020.\n[5] H. Chen, A. Suhr, D. K. Misra, N. Snavely, and Y. Artzi, “Touch-\ndown: Natural language navigation and spatial reasoning in vi-\nsual street environments,” in CVPR, 2019.\n[6] H. Tan, L. Yu, and M. Bansal, “Learning to navigate unseen\nenvironments: Back translation with environmental dropout,” in\nNAACL-HLT, 2019.\n[7] D. Fried, R. Hu, V . Cirik, A. Rohrbach, J. Andreas, L.-P . Morency,\nT. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell, “Speaker-\nfollower models for vision-and-language navigation,” in NeurIPS,\n2018.\n[8] C. Liu, F. Zhu, X. Chang, X. Liang, Z. Ge, and Y.-D. Shen,\n“Vision-language navigation with random environmental mixup,”\nin ICCV, 2021.\n[9] F. Zhu, Y. Zhu, X. Chang, and X. Liang, “Vision-language navi-\ngation with self-supervised auxiliary reasoning tasks,” in CVPR,\n2020.\n[10] X. Li, C. Li, Q. Xia, Y. Bisk, A. C ¸ elikyilmaz, J. Gao, N. A. Smith,\nand Y. Choi, “Robust navigation with language pretraining and\nstochastic sampling.” in EMNLP, 2019.\n[11] H. Huang, V . Jain, H. Mehta, A. Ku, G. Magalhaes, J. Baldridge,\nand E. Ie, “Transferable representation learning in vision-and-\nlanguage navigation,” in ICCV, 2019.\n[12] C.-Y. Ma, jiasen lu, Z. Wu, G. AlRegib, Z. Kira, richard socher, and\nC. Xiong, “Self-monitoring navigation agent via auxiliary progress\nestimation,” in ICLR, 2019.\n[13] Y. Hong, C. Rodriguez, Y. Qi, Q. Wu, and S. Gould, “Language and\nvisual entity relationship graph for agent navigation,” in NeurIPS,\n2020.\n[14] Z. Deng, K. Narasimhan, and O. Russakovsky, “Evolving graph-\nical planner: Contextual global planning for vision-and-language\nnavigation,” in NeurIPS, 2020.\n[15] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, “Vln\nbert: A recurrent vision-and-language bert for navigation,” in\nCVPR, 2021.\n[16] S. Chen, P .-L. Guhur, C. Schmid, and I. Laptev, “History aware\nmultimodal transformer for vision-and-language navigation,” in\nNeurIPS, 2021.\n[17] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-\nwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell, and et al.,\n“Language models are few-shot learners,” in NeurIPS, 2020.\n[18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P . Mishkin, J. Clark, G. Krueger, and\nI. Sutskever, “Learning transferable visual models from natural\nlanguage supervision,” in ICML, 2021.\n[19] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho,\nJ. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey,\nS. Jesmonth, nd N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-\nH. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P . Pastor, J. Quiambao,\nK. Rao, J. Rettinghouse, D. Reyes, P . Sermanet, N. Sievers, C. Tan,\nA. Toshev, V . Vanhoucke, F. Xia, T. Xiao, P . Xu, S. Xu, and M. Yan.,\n“Do as i can, not as i say: Grounding language in robotic affor-\ndances,” ArXiv, vol. abs/2204.01691, 2022.\n[20] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P . R. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y. Chebotar, P . Sermanet, N. Brown,\nT. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, “Inner\nmonologue: Embodied reasoning through planning with language\nmodels,” ArXiv, vol. abs/2207.05608, 2022.\n2. https://www.mindspore.cn/\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15\n[21] X. Liang, F. Zhu, L. Li, H. Xu, and X. Liang, “Visual-language\nnavigation pretraining via prompt-based environmental self-\nexploration,” in ACL, 2022.\n[22] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and\nY. Su, “Llm-planner: Few-shot grounded planning for embodied\nagents with large language models,” ArXiv, vol. abs/2212.04088,\n2022.\n[23] OpenAI, “Introducing chatgpt,” https://openai.com/blog/\nchatgpt, 2022.\n[24] S. Chen, P .-L. Guhur, M. Tapaswi, C. Schmid, and I. Laptev, “Think\nglobal, act local: Dual-scale graph transformer for vision-and-\nlanguage navigation,” in CVPR, 2022.\n[25] T.-J. Fu, X. E. Wang, M. F. Peterson, S. T. Grafton, M. P . Eckstein,\nand W. Y. Wang, “Counterfactual vision-and-language navigation\nvia adversarial path sampling,” ArXiv, vol. abs/1911.07308, 2019.\n[Online]. Available: https://api.semanticscholar.org/CorpusID:\n208138631\n[26] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang,\nW. Y. Wang, and L. Zhang, “Reinforced cross-modal matching\nand self-supervised imitation learning for vision-language navi-\ngation,” in CVPR, 2019.\n[27] H. Wang, W. Wang, T. Shu, W. Liang, and J. Shen, “Active\nvisual information gathering for vision-language navigation,” in\nECCV, 2020. [Online]. Available: https://api.semanticscholar.org/\nCorpusID:220546187\n[28] C.-Y. Ma, Z. Wu, G. Al-Regib, C. Xiong, and Z. Kira, “The regretful\nagent: Heuristic-aided navigation through progress estimation,”\n2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pp. 6725–6733, 2019. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:67877155\n[29] L. Ke, X. Li, Y. Bisk, A. Holtzman, Z. Gan, J. Liu,\nJ. Gao, Y. Choi, and S. S. Srinivasa, “Tactical rewind: Self-\ncorrection via backtracking in vision-and-language navigation,”\n2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) , pp. 6734–6742, 2019. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:70350040\n[30] Y. Qi, Z. Pan, S. Zhang, A. van den Hengel, and Q. Wu, “Object-\nand-action aware model for visual language navigation,” in ECCV\n(10), 2020, pp. 303–317.\n[31] Y. Qi, Z. Pan, Y. Hong, M.-H. Yang, A. van den Hengel, and\nQ. Wu, “The road to know-where: An object-and-room informed\nsequential bert for indoor vision-language navigation,” in ICCV,\n2021.\n[32] Y. Zhao, J. Chen, C. Gao, W. Wang, L. Yang, H. Ren, H. Xia, and\nS. Liu, “Target-driven structured transformer planner for vision-\nlanguage navigation,” in ACM MM, 2022.\n[33] Y. Qiao, Y. Qi, Y. Hong, Z. Yu, P . Wang, and Q. Wu, “Hop: History-\nand-order aware pre-training for vision-and-language naviga-\ntion,” in CVPR, 2022.\n[34] P .-L. Guhur, M. Tapaswi, S. Chen, I. Laptev, and C. Schmid, “Air-\nbert: In-domain pretraining for vision-and-language navigation,”\nin ICCV, 2021.\n[35] A. Moudgil, A. Majumdar, H. Agrawal, S. Lee, and D. Batra, “Soat:\nA scene- and object-aware transformer for vision-and-language\nnavigation,” in NeurIPS, 2021.\n[36] W. Huang, P . Abbeel, D. Pathak, and I. Mordatch, “Language\nmodels as zero-shot planners: Extracting actionable knowledge for\nembodied agents,” ArXiv, vol. abs/2201.07207, 2022.\n[37] D. Shah, B. Osinski, B. Ichter, and S. Levine, “Lm-nav: Robotic\nnavigation with large pre-trained models of language, vision, and\naction,” in CoRL, 2022.\n[38] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo,\nA. Stone, and D. Kappler, “Open-vocabulary queryable scene rep-\nresentations for real world planning,” ArXiv, vol. abs/2209.09874,\n2022.\n[39] A. Z. Liu, L. Logeswaran, S. Sohn, and H. Lee, “A picture is worth\na thousand words: Language models plan from pixels,” ArXiv,\nvol. abs/2303.09031, 2023.\n[40] V . S. Dorbala, J. F. Mullen, and D. Manocha, “Can an embodied\nagent find your ”cat-shaped mug”? llm-based zero-shot object\nnavigation,” ArXiv, vol. abs/2303.03480, 2023.\n[41] Y. Lu, W. Feng, W. Zhu, W. Xu, X. E. Wang, M. P . Eckstein, and\nW. Y. Wang, “Neuro-symbolic procedural planning with common-\nsense prompting,” in ICLR, 2023.\n[42] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song,\n“Cows on pasture: Baselines and benchmarks for language-driven\nzero-shot object navigation,” in CVPR, 2022. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:254636632\n[43] K.-Q. Zhou, K. Zheng, C. Pryor, Y. Shen, H. Jin, L. Getoor, and X. E.\nWang, “Esc: Exploration with soft commonsense constraints for\nzero-shot object navigation,” in ICML, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:256390546\n[44] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in ICLR, 2021.\n[45] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NIPS, 2017.\n[47] V . Mnih, A. P . Badia, M. Mirza, A. Graves, T. P . Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in ICML, 2016.\n[48] Y. Hong, C. Rodriguez-Opazo, Q. Wu, and S. Gould,\n“Sub-instruction aware vision-and-language navigation,” in\nEMNLP, 2021. [Online]. Available: https://api.semanticscholar.\norg/CorpusID:214803082\n[49] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learn-\ning a generic agent for vision-and-language navigation via pre-\ntraining,” in CVPR, 2020.\n[50] J. Li and M. Bansal, “Improving vision-and-language navigation\nby generating future-view image semantics,” in CVPR, 2023.\n[51] X. Li, Z. Wang, J. Yang, Y. Wang, and S. Jiang, “Kerm: Knowl-\nedge enhanced reasoning for vision-and-language navigation,” in\nCVPR, 2023.\n[52] M. Hwang, J. Jeong, M. Kim, Y. Oh, and S. H. Oh, “Meta-\nexplore: Exploratory hierarchical vision-and-language navigation\nusing scene object spectrum grounding,” in CVPR, 2023.\n[53] Z. Wang, J. Li, Y. Hong, Y. Wang, Q. Wu, M. Bansal, S. Gould,\nH. Tan, and Y. Qiao, “Scaling data generation in vision-\nand-language navigation,” in ICCV, 2023. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:260315945\n[54] G. Zhou, Y. Hong, and Q. Wu, “Navgpt: Explicit reasoning\nin vision-and-language navigation with large language models,”\nArXiv, vol. abs/2305.16986, 2023. [Online]. Available: https:\n//api.semanticscholar.org/CorpusID:258947250\n[55] X. Lin, G. Li, and Y. Yu, “Scene-intuitive agent for remote embod-\nied visual grounding,” in CVPR, 2021.\n[56] F. Landi, L. Baraldi, M. Cornia, M. Corsini, and R. Cucchiara,\n“Perceive, transform, and act: Multi-modal attention networks\nfor vision-and-language navigation,” ArXiv, vol. abs/1911.12377,\n2019.\n[57] J. Li, H. Tan, and M. Bansal, “Improving cross-modal alignment in\nvision language navigation via syntactic information,” in NAACL-\nHLT, 2021, pp. 1041–1050.\n[58] “Vicuna,” https://github.com/lm-sys/FastChat, 2023.\n[59] M. Honnibal, I. Montani, S. V . Landeghem, and A. Boyd, “spacy:\nIndustrial-strength natural language processing in python,” 2020.\nBingqian Lin received the B.E. and the M.E.\ndegree in Computer Science from University of\nElectronic Science and Technology of China and\nXiamen University, in 2016 and 2019, respec-\ntively. She is currently working toward the D.Eng\nin the school of intelligent systems engineering\nof Sun Y at-sen University. Her research interests\ninclude multi-view clustering, image processing\nand vision-and-language understanding.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 16\nYunshuang Niereceived the B.E. degree in Sun\nY at-sen University, Shenzhen, China, in 2023.\nShe is currently working toward the M.E. in the\nschool of intelligent systems engineering of Sun\nY at-sen University. Her current research interest\nis vision-and-language understanding.\nZiming Wei is currently an undergraduate in\nthe school of intelligent systems engineering of\nSun Y at-sen University. His current research in-\nterests include vision-and-language understand-\ning, multimodality and embodied agent.\nYi Zhu received the B.S. degree in soft-\nware engineering from Sun Y at-sen University,\nGuangzhou, China, in 2013. Since 2015, she\nhas been a Ph.D student in computer science\nat the School of Electronic, Electrical, and Com-\nmunication Engineering, University of Chinese\nAcademy of Sciences, Beijing, China. Her cur-\nrent research interests include object recogni-\ntion, scene understanding, weakly supervised\nlearning, and visual reasoning.\nHang Xu is currently a senior researcher in\nHuawei Noah Ark Lab. He received his BSc in\nFudan University and Ph.D in Hong Kong Univer-\nsity in Statistics. His research Interest includes\nmulti-modality learning, machine Learning, ob-\nject detection, and AutoML. He has published\n70+ papers in Top AI conferences: NeurIPS,\nCVPR, ICCV, AAAI and some statistics journals,\ne.g. CSDA, Statistical Computing.\nShikui Ma received the M.S. degree in\nNorthern Jiaotong University in 2003. He\nhas been serving as an assistant vice\npresident of Dataa Robotics company\n(https://www.dataarobotics.com/en) since 2015,\nwhere he is leading HARIX and AI R&D team\nto consistently enhance their HARIX intelligent\nsystem, especially significantly improve the\nsmart vision and motion capabilities of their\nrobots via real-time digital twin, multi-modal\nfusion perception and advanced cognition, and\ndeep reinforcement learning technologies. He has approximately 19\nyears of experience in communications technology industry, expertized\nin large-scale carrier-grade applications, especially competitive in\nRobotics, AI, Cloud, OSS and IPTV fields.\nJianzhuang Liu (Senior Member, IEEE) re-\nceived the PhD degree in computer vision from\nThe Chinese University of Hong Kong, in 1997.\nFrom 1998 to 2000, he was a research fellow\nwith Nanyang Technological University, Singa-\npore. From 2000 to 2012, he was a post-doctoral\nfellow, an assistant professor, and an adjunct\nassociate professor with The Chinese University\nof Hong Kong, Hong Kong. In 2011, he joined\nthe Shenzhen Institute of Advanced Technol-\nogy, University of Chinese Academy of Sciences,\nShenzhen, China, as a professor. He was a principal researcher in\nHuawei Company from 2012 to 2023. He has authored more than\n200 papers in the areas of computer vision, image processing, deep\nlearning, and AIGC.\nXiaodan Liangis currently an Associate Profes-\nsor at Sun Y at-sen University. She was a postdoc\nresearcher in the machine learning department\nat Carnegie Mellon University, working with Prof.\nEric Xing, from 2016 to 2018. She received\nher PhD degree from Sun Y at-sen University in\n2016, advised by Liang Lin. She has published\nseveral cutting-edge projects on human-related\nanalysis, including human parsing, pedestrian\ndetection and instance segmentation, 2D/3D hu-\nman pose estimation, and activity recognition.",
  "topic": "Landmark",
  "concepts": [
    {
      "name": "Landmark",
      "score": 0.9522619247436523
    },
    {
      "name": "Computer science",
      "score": 0.7678979635238647
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5426573753356934
    },
    {
      "name": "Code (set theory)",
      "score": 0.47728338837623596
    },
    {
      "name": "Prior probability",
      "score": 0.4626380503177643
    },
    {
      "name": "Computer vision",
      "score": 0.36578959226608276
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33882877230644226
    },
    {
      "name": "Programming language",
      "score": 0.09302827715873718
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.08090916275978088
    },
    {
      "name": "Bayesian probability",
      "score": 0.0
    }
  ]
}