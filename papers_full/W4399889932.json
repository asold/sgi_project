{
  "title": "LEAP: LLM instruction-example adaptive prompting framework for biomedical relation extraction",
  "url": "https://openalex.org/W4399889932",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5026404757",
      "name": "Huixue Zhou",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A5101907703",
      "name": "Mingchen Li",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A5034370661",
      "name": "Yongkang Xiao",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A5101605012",
      "name": "Han Yang",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A5100421963",
      "name": "Rui Zhang",
      "affiliations": [
        "University of Minnesota"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2768488789",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4388605937",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W6857563048",
    "https://openalex.org/W4319663047",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W2170189740",
    "https://openalex.org/W4283318325",
    "https://openalex.org/W4393187333",
    "https://openalex.org/W4385571451",
    "https://openalex.org/W4394869910",
    "https://openalex.org/W4387617694"
  ],
  "abstract": "Abstract Objective To investigate the demonstration in large language models (LLMs) for biomedical relation extraction. This study introduces a framework comprising three types of adaptive tuning methods to assess their impacts and effectiveness. Materials and Methods Our study was conducted in two phases. Initially, we analyzed a range of demonstration components vital for LLMs’ biomedical data capabilities, including task descriptions and examples, experimenting with various combinations. Subsequently, we introduced the LLM instruction-example adaptive prompting (LEAP) framework, including instruction adaptive tuning, example adaptive tuning, and instruction-example adaptive tuning methods. This framework aims to systematically investigate both adaptive task descriptions and adaptive examples within the demonstration. We assessed the performance of the LEAP framework on the DDI, ChemProt, and BioRED datasets, employing LLMs such as Llama2-7b, Llama2-13b, and MedLLaMA_13B. Results Our findings indicated that Instruction + Options + Example and its expanded form substantially improved F1 scores over the standard Instruction + Options mode for zero-shot LLMs. The LEAP framework, particularly through its example adaptive prompting, demonstrated superior performance over conventional instruction tuning across all models. Notably, the MedLLAMA_13B model achieved an exceptional F1 score of 95.13 on the ChemProt dataset using this method. Significant improvements were also observed in the DDI 2013 and BioRED datasets, confirming the method’s robustness in sophisticated data extraction scenarios. Conclusion The LEAP framework offers a compelling strategy for enhancing LLM training strategies, steering away from extensive fine-tuning towards more dynamic and contextually enriched prompting methodologies, showcasing in biomedical relation extraction.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7832518815994263
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5714784264564514
    },
    {
      "name": "Task (project management)",
      "score": 0.5451090931892395
    },
    {
      "name": "Relation (database)",
      "score": 0.49172475934028625
    },
    {
      "name": "Relationship extraction",
      "score": 0.44812506437301636
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4056650996208191
    },
    {
      "name": "Machine learning",
      "score": 0.35776910185813904
    },
    {
      "name": "Data mining",
      "score": 0.24851247668266296
    },
    {
      "name": "Engineering",
      "score": 0.07364937663078308
    },
    {
      "name": "Systems engineering",
      "score": 0.06187474727630615
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}