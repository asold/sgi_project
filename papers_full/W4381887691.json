{
  "title": "Automating intended target identification for paraphasias in discourse using a large language model",
  "url": "https://openalex.org/W4381887691",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3153746423",
      "name": "Alexandra C Salem",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": "https://openalex.org/A4320899958",
      "name": "Robert C. Gale",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": "https://openalex.org/A4311019582",
      "name": "Mikala Fleegle",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A2026954565",
      "name": "Gerasimos Fergadiotis",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A1975412413",
      "name": "Steven Bedrick",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": "https://openalex.org/A3153746423",
      "name": "Alexandra C Salem",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": "https://openalex.org/A4320899958",
      "name": "Robert C. Gale",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": "https://openalex.org/A4311019582",
      "name": "Mikala Fleegle",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A2026954565",
      "name": "Gerasimos Fergadiotis",
      "affiliations": [
        "Portland State University"
      ]
    },
    {
      "id": "https://openalex.org/A1975412413",
      "name": "Steven Bedrick",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2739697855",
    "https://openalex.org/W3047016605",
    "https://openalex.org/W2963518130",
    "https://openalex.org/W4213241316",
    "https://openalex.org/W2158243193",
    "https://openalex.org/W2085906926",
    "https://openalex.org/W4230446462",
    "https://openalex.org/W4320896618",
    "https://openalex.org/W4226136526",
    "https://openalex.org/W2052168556",
    "https://openalex.org/W2020155291",
    "https://openalex.org/W4200064546",
    "https://openalex.org/W2094790441",
    "https://openalex.org/W2097188373",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3162734203",
    "https://openalex.org/W2566552364",
    "https://openalex.org/W2808129867",
    "https://openalex.org/W2102689857",
    "https://openalex.org/W2160743210",
    "https://openalex.org/W2122940614",
    "https://openalex.org/W2251658484",
    "https://openalex.org/W3186084730",
    "https://openalex.org/W6875570683",
    "https://openalex.org/W2076118331",
    "https://openalex.org/W641067470",
    "https://openalex.org/W1975833823",
    "https://openalex.org/W4253871295",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2796439139",
    "https://openalex.org/W2748199382",
    "https://openalex.org/W2508777706",
    "https://openalex.org/W2136816081",
    "https://openalex.org/W2019862519",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2028041302",
    "https://openalex.org/W1999318234",
    "https://openalex.org/W1564373635",
    "https://openalex.org/W2071045182",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W2087350083",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3080569196",
    "https://openalex.org/W2123330196",
    "https://openalex.org/W2793515187",
    "https://openalex.org/W1568041512",
    "https://openalex.org/W2104836160",
    "https://openalex.org/W4311018845",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1994667876",
    "https://openalex.org/W2142127073",
    "https://openalex.org/W2160946777",
    "https://openalex.org/W2060770722",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2086052626",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W4200170981"
  ],
  "abstract": "Abstract Purpose To date there are no automated tools for the identification and fine-grained classification of paraphasias within discourse, the production of which is the hallmark characteristic of most people with aphasia (PWA). In this work we fine-tune a large language model (LLM) to automatically predict paraphasia targets in Cinderella story retellings. Method Data consisted of 353 Cinderella story retellings containing 2,489 paraphasias from PWA, for which research assistants identified their intended targets. We supplemented this training data with 256 sessions from control participants, to which we added 2,427 synthetic paraphasias. We conducted four experiments using different training data configurations to fine-tune the LLM to automatically “fill in the blank” of the paraphasia with a predicted target, given the context of the rest of the story retelling. We tested the experiments’ predictions against our human-identified targets and stratified our results by ambiguity of the targets and clinical factors. Results The model trained on controls and PWA achieved 46.8% accuracy at exactly matching the human-identified target. Fine-tuning on PWA data, with or without controls, led to comparable performance. The model performed better on targets with less human ambiguity, and on paraphasias from participants with less severe or fluent aphasia. Conclusion We were able to automatically identify the intended target of paraphasias in discourse using just the surrounding language about half of the time. These findings take us a step closer to automatic aphasic discourse analysis. In future work, we will incorporate phonological information from the paraphasia to further improve predictive utility.",
  "full_text": " \n   \n \nAutomating intended target identification for paraphasias in discourse using a large 1 \nlanguage model 2 \nAlexandra C. Salem1, Robert C. Gale1, Mikala Fleegle2, Gerasimos Fergadiotis2, Steven Bedrick1 3 \n1Department of Medical Informatics and Clinical Epidemiology, Oregon Health & Science 4 \nUniversity 5 \n2Department of Speech and Hearing Sciences, Portland State University 6 \n 7 \nCorresponding Author: Alexandra C. Salem, salem@ohsu.edu 8 \n 9 \nConflict of Interest Statement 10 \nWe have no known conflict of interest to disclose. 11 \n 12 \nFunding Statement 13 \nThis work was supported by National Institute on Deafness and Other Communication Disorders 14 \nGrant R01DC015999 (Principal Investigators: Steven Bedrick and Gerasimos Fergadiotis).   15 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n   \n \nAbstract 16 \nPurpose: To date there are no automated tools for the identification and fine-grained 17 \nclassification of paraphasias within discourse, the production of which is the hallmark 18 \ncharacteristic of most people with aphasia (PWA). In this work we fine-tune a large language 19 \nmodel (LLM) to automatically predict paraphasia targets in Cinderella story retellings. 20 \nMethod: Data consisted of 353 Cinderella story retellings containing 2,489 paraphasias from 21 \nPWA, for which research assistants identified their intended targets. We supplemented this 22 \ntraining data with 256 sessions from control participants, to which we added 2,427 synthetic 23 \nparaphasias. We conducted four experiments using different training data configurations to fine-24 \ntune the LLM to automatically “fill in the blank” of the paraphasia with a predicted target, given 25 \nthe context of the rest of the story retelling. We tested the experiments’ predictions against our 26 \nhuman-identified targets and stratified our results by ambiguity of the targets and clinical factors. 27 \nResults: The model trained on controls and PWA achieved 46.8% accuracy at exactly matching 28 \nthe human-identified target. Fine-tuning on PWA data, with or without controls, led to 29 \ncomparable performance. The model performed better on targets with less human ambiguity, and 30 \non paraphasias from participants with less severe or fluent aphasia. 31 \nConclusion: We were able to automatically identify the intended target of paraphasias in 32 \ndiscourse using just the surrounding language about half of the time. These findings take us a 33 \nstep closer to automatic aphasic discourse analysis. In future work, we will incorporate 34 \nphonological information from the paraphasia to further improve predictive utility.  35 \n  36 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nAnomia or word-finding difficulty is a prominent and persistent feature of aphasia 37 \n(Goodglass and Wingfield, 1997) and manifests in all communicative contexts, from single word 38 \nresponses to complex conversations. Given the ubiquitous nature of anomia, anomia assessments 39 \nare given in most clinical settings and are of high practical value for quantifying performance 40 \nand monitoring outcomes. Typically, anomia assessments include confrontation picture naming 41 \ntests (Rabin et al., 2005; Simmons-Mackie, Threats, & Kagan, 2005), in which a person with 42 \naphasia is asked to name a series of pictured objects and/or actions. The popularity of 43 \nconfrontation picture naming tests can be attributed to their well-documented validity and 44 \nreliability (e.g., Roach et al., 1996; Strauss, Sherman, & Spreen, 2006; Walker & Schwartz, 45 \n2012), and also to their relatively low testing burden, particularly in the context of short forms 46 \nand simple accuracy scoring schemes. Other sources of diagnostic information such as discourse-47 \nlevel analyses may provide additional clinically useful information for completing a patient’s 48 \nclinical profile (Fergadiotis et al., 2019; Richardson et al., 2018) but such analyses are not 49 \nperformed routinely in clinical settings. Viewed through an implementation science lens 50 \n(Damschroder et al., 2009; Breimaier et al., 2015), several barriers hinder the utilization of 51 \ndiscourse-based analyses including their complexity, reliability, and time burden. The latter 52 \nfactor especially can be an insurmountable barrier for implementation in most real-world clinical 53 \nsettings. Therefore, there is a need to develop new approaches that will enable professionals to 54 \nassess people with aphasia (PWA) in a more objective, precise, efficient, and ecologically valid 55 \nmanner. 56 \nComputational methods, especially those from the field of Natural Language Processing 57 \n(NLP), have the potential to be essential tools in designing such approaches. Recent work has 58 \ndemonstrated these methods’ efficacy in automating certain aspects of confrontation naming test 59 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nscoring (Casilio et al., 2023; Salem et al., 2022; Fergadiotis et al., 2016; McKinney-Bock & 60 \nBedrick, 2019; described later in more detail). In this work, we report on a crucial first step in 61 \napplying such methods to discourse samples. Specifically, we describe the results of a 62 \ncomputational model that analyzes the context in which a paraphasia occurs in a discourse 63 \nsample and predicts the speaker's intended word (or a set of possible intended words). Below, we 64 \ndescribe the key role that this specific task of target word prediction plays in the clinical 65 \nassessment of discourse samples from PWA, motivate our overall computational approach, and 66 \ndescribe our model and its behavior. In addition, we evaluate the impact of clinical features of 67 \nthe speaker on our model's ability to correctly predict target words. This part of the work 68 \nhighlights specific areas where current technology falls short and points to missing pieces that 69 \nthe field must address. 70 \nAssessing Anomia at Discourse Level 71 \nIt is well documented in the literature that the ability to produce discourse is what matters 72 \nmost to PWA and their families (Cruice et al., 2003; Mayer & Murray, 2003). Yet, despite their 73 \npopularity, there is evidence that confrontation naming tests cannot fully account for the severity 74 \nand patterns of anomia exhibited during connected speech. First, connectionist accounts of word 75 \nretrieval at the discourse level highlight how lexical characteristics of target words interact with 76 \nactivated representations within and across different linguistic levels (e.g., phonological, 77 \nsemantic) (Bock, 1995; Dell, 1986; Dell et al., 1999; Schwartz et al., 2006; Levelt, 1999; Levelt 78 \net al., 1999). In addition, several models (e.g. MacDonald, 1994; Tabor et al., 1997) emphasize 79 \nthe influence and relative strength of naturally occurring probabilistic constraints in language use 80 \non the activation of linguistic representations. In fact, there seems to be a general consensus in 81 \nrecent empirical investigations that while performance in confrontation naming tests is related to 82 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \ndiscourse-level performance, analyzing discourse directly may provide unique and useful clinical 83 \ninsights not gained via confrontation naming tests (Fergadiotis et al., 2019; Hickin et al., 2001; 84 \nMayer & Murray, 2003; Pashek & Tompkins, 2002). Therefore, relevant assessment tools for 85 \naphasia should a) operate at the discourse level, b) be able to capture changes in language skills 86 \nover time, and c) be routinely included as therapy outcome measures.  87 \nAt the level of single words, anomia severity is commonly assessed using picture naming 88 \ntests and reported in terms of overall accuracy scores or ability estimates. Further, a more in-89 \ndepth analysis of the types and frequencies of word production errors can reveal which linguistic 90 \nprocesses that support word access and retrieval are more or less disrupted (Dell et al., 1997). 91 \nTheoretical accounts of word production  allow professionals and/or algorithms to classify an 92 \nindividual’s collection of paraphasias in order to create a detailed profile of that individual’s 93 \nanomia. This paraphasia classification process requires a series of binary judgments with regards 94 \nto the paraphasia and its relationship to the intended target word. Specifically, those judgments 95 \nare: 1) lexicality, i.e., whether or not the paraphasia is a real word; 2) semantic similarity, i.e., 96 \nwhether or not the paraphasia is semantically related to the target; and 3) phonological similarity, 97 \ni.e., whether or not the paraphasia is phonologically related to the target. To highlight a couple of 98 \nclassification examples, a Semantic paraphasia is a real word that is semantically related to its 99 \nintended target but phonologically unrelated (e.g., \"beard\" for \"mustache\"); whereas a neologism 100 \nis a nonword, not semantically related by definition, that is phonologically related to the target 101 \n(e.g., \"mustaff\" for \"mustache\"). Lexical or real word paraphasias are understood to represent 102 \nmostly impairments in lexical-semantic access while nonword paraphasias are thought to reflect 103 \ndeficits in phonological encoding. To help make this time- and labor- intensive assessment 104 \nprocess more efficient and therefore more feasible for clinical settings, our research team has 105 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \ndeveloped a paraphasia classification algorithm called ParAlg (Paraphasia Algorithms) that 106 \nautomatically classifies word production errors in the context of object picture naming tests 107 \n(Casilio et al., 2023; Salem et al., 2022; Fergadiotis et al., 2016; McKinney-Bock & Bedrick, 108 \n2019). ParAlg's paraphasia classifiers algorithmically mirror the main paraphasia classification 109 \ncriteria of the Philadelphia Naming Test (Roach et al., 1996), which includes one of the most 110 \nwell-established and thorough frameworks for error classification during object picture naming. 111 \nThe accuracy of this multistep paraphasia classification process, however, is entirely 112 \npredicated on successfully identifying a given paraphasia's intended target. Target identification 113 \nis relatively straightforward in the context of confrontation picture naming tests, where the target 114 \nis presumed to be the word depicted in the picture, but in the context of discourse, determining 115 \nthe target is not as straightforward. Researchers and clinicians undertake this task by applying 116 \nbackground knowledge of word production disorders and common anomic patterns (Martin, 117 \n2017), as well as general knowledge of the discourse task itself, such as the expected lexicon and 118 \nthe expected temporal arrangement of that lexicon given the overall narrative structure. 119 \nFurthermore, target prediction can incorporate a multitude of localized contextual factors such as 120 \ntimely gestures, re-tracings from the paraphasia to or toward the intended target, phonological 121 \nfragments or false starts leading up to the paraphasia, syntactic/semantic information 122 \nimmediately surrounding the paraphasia, and/or semantic and phonological similarities between 123 \nthe paraphasia and its working hypothesis target.  124 \nIn light of this highly variable and complex process, the preliminary focus of this 125 \nautomation work and of the current paper is to leverage and model the semantic information 126 \nsurrounding word production breakdowns. Elegantly enough, this approach mirrors widely 127 \naccepted models of spoken word production, such as Dell’s model described earlier where step 128 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \none involves identification and activation of semantic representations surrounding the target 129 \nword. One additional and imminent aim of this work, though outside of the scope of this paper, is 130 \nthe exploration of a more fully-automated and naturalistic application of ParAlg - classification 131 \nof paraphasias in discourse using machine-generated targets. While the present paper explores 132 \nautomatic target prediction for a full range of content words (nouns, verbs, adverbs, adjectives), 133 \nwe do not anticipate being able to classify paraphasias with non-noun targets until equally robust 134 \npsycholinguistic models are developed for additional parts of speech. 135 \nNovel Approaches for Assessing Paraphasias at Discourse Level 136 \nGiven the resource-intensive nature of discourse analysis, several computational 137 \napproaches have been developed to assist researchers and clinicians in analyzing discourse such 138 \nas automated speech and language measures (e.g., Fergadiotis & Wright, 2011; Bryant et al., 139 \n2013; Miller & Iglesias, 2012; Forbes et al., 2014; Day et al., 2021; Chatzoudis et al., 2022). An 140 \nactive area of research is establishing automatic speech recognition (ASR) systems that are 141 \neffective on aphasic speech (e.g., Le & Provost, 2016; Perez et al., 2020; Gale et al., 2022), some 142 \nof which are developed and used for diagnosing aphasia or aphasia subtypes (e.g., Fraser et al., 143 \n2013; Le et al., 2018). Some preliminary attempts have been made at automated classification of 144 \nparaphasias in connected speech, but these studies have focused solely on the task of detecting 145 \nparaphasias and determining if they are real words or neologisms (Le et al., 2017; Pai et al., 146 \n2020), as opposed to complete classification. Despite the recent advances in automated 147 \napproaches, to this date there are no computer assisted discourse analyses for the identification 148 \nand fine-grained classification of paraphasias, the production of which is the hallmark 149 \ncharacteristic of most PWA. 150 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nOur first attempts at predicting targets of paraphasias in discourse were made using more 151 \ntraditional n-gram and early neural net based language models (Adams et al., 2017), but since 152 \nthen, there have been significant developments in the field of language modeling. In this work, to 153 \nautomatically predict the intended targets of paraphasias in discourse using the surrounding 154 \nlanguage, we use a machine learning-based transformer language model. Transformer models 155 \nwere first introduced in 2017 (Vaswani et al., 2017) and have since become ubiquitous in NLP 156 \nresearch due to their high performance; their structure allows them to be trained on large scale 157 \ndatasets with graphical processing units (GPUs). The introduction of transformer models led to 158 \nthe development of BERT (Bidirectional Encoder Representations from Transformers; Devlin et 159 \nal., 2019), a large language model (LLM) which has been successful on a variety of NLP tasks 160 \nsuch as Google search, text summarization, and question answering (Devlin et al., 2019; Liu & 161 \nLapata, 2019; B. Schwartz, 2020). BERT is designed to be pre-trained on a very large scale 162 \ngeneral purpose dataset and can then be used in its out-of-the-box pre-trained format, or one can 163 \nuse transfer learning to adapt them for a specific domain and task with a process called fine-164 \ntuning. During fine-tuning, the model is trained further on a downstream task with domain-165 \nspecific data. This process allows the models to work well even on tasks with fewer data 166 \nresources (Zaheer et al, 2021).  167 \nLLMs have been successfully applied to a variety of biomedical language tasks. For 168 \nexample, by fine-tuning BERT with PubMed abstracts and clinical notes, Peng et al. (2019) 169 \noutperformed previous state-of-the-art on five biomedical tasks (e.g., similarity of two sentences 170 \nfrom Mayo Clinic clinical data). Researchers have also found success applying these models to 171 \nclinical language research. For instance, Balagopalan et al. (2020) fine-tuned BERT to detect 172 \nAlzheimer’s disease from transcribed spontaneous speech. They found that BERT performed 173 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nbetter than a standard model based on hand-crafted features. Gale et al. (2021) fine-tuned a 174 \nvariation of BERT called DistilBERT (Sanh et al., 2019) to automatically score commonly used 175 \nexpressive language tasks on a diverse group of children (Autism Spectrum Disorder, Attention-176 \nDeficit Hyperactivity Disorder, Developmental Language Disorder, and typical development; 177 \nage 5-9 years) with high accuracy (83-99%). In previous work developing ParAlg, our group 178 \nfine-tuned DistilBERT to automatically determine the semantic similarity of lexical paraphasias 179 \nto the target word with 95.3% accuracy (Salem et al., 2022).  180 \nWhile models like BERT have been very successful, one drawback is that they are 181 \ndesigned for relatively short sequences of words; in fact, BERT has a hard limit of taking 182 \nsequences of text of maximum length 512 tokens. Our data, which consists of retellings of the 183 \nCinderella story, includes many sessions longer than that limit. In this work, we instead use a 184 \nrecent LLM called BigBird (Zaheer et al., 2021) which was specifically designed to address this 185 \nlimitation of BERT. Importantly, BigBird, like its predecessor BERT, was trained using “masked 186 \nlanguage modeling”, a type of sentence cloze task. In this task, randomly selected words from 187 \nthe corpus are masked (i.e., removed and replaced with a special blank token [MASK]), and the 188 \nmodel learns to fill in the blank and predict those masked words using the surrounding context, 189 \nallowing it to learn what words occur in what contexts. This task is in fact similar to our task at 190 \nhand: we want to predict what target word a person with aphasia was intending to say, given the 191 \ncontext of their discourse. Thus, considering the wide success of LLMs, the adaptation of this 192 \nmodel to long sequences, and the similarity of its training process to our task, we hypothesized 193 \nthat BigBird would be a good fit for automatically predicting paraphasia targets in discourse. 194 \nGiven that the current study represents a novel application of a LLM to data from a 195 \nclinical population, it is worthwhile to explore factors that might influence the accuracy of that 196 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \napproach. It is generally accepted that PWA represent a heterogeneous group in terms of the 197 \nnature and severity of deficits exhibited during discourse production. For example, some 198 \nindividuals on the mild end of the ability continuum may present with well-constructed 199 \nutterances during connected speech with only occasional hesitations and single word 200 \nparaphasias. On the other hand, people on the more severe end of the distribution may exhibit 201 \nmorphosyntactic disturbances as well as significant manifestations of word retrieval deficits 202 \nincluding abandoned phrases, revisions, retracings, reformulations, as well as multiple 203 \nparaphasias. Therefore, given that the LLM relies on the surrounding context of a masked word 204 \nfor prediction, it is conceivable that the success of the model may depend on overall aphasia 205 \nseverity of the speaker. In addition to overall aphasia severity, the predictive utility of the LLM 206 \nmay also depend on the nature of the syntactic deficits exhibited by people with aphasia. 207 \nSpecifically, connected speech from PWA can be characterized as agrammatic or paragrammatic 208 \n(Butterworth & Howard, 1987; Goodglass, 1993; Saffran et al., 1989; Thompson et al., 1997). 209 \nAgrammatic speech is typically characterized by an overall reduction of grammatical 210 \nmorphology, simplification of syntactic structure, and overreliance on content words, primarily 211 \nnouns. On the other hand, paragrammatism is associated with misuse of grammatical aspects 212 \nincluding inflectional morphology, significant word substitutions that cross word class, as well as 213 \npronounced errors in word ordering. Finally, during discourse production, there are instances 214 \nwhere a speaker's intended target is clear, but that is not always the case, and different raters can 215 \ndisagree. In this study, in addition to clinical factors, we investigated the performance of our 216 \nLLM as a function of the certainty with which raters can perform the same task. 217 \nPurpose of Study 218 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nThe purpose of the current study was to create a baseline model for automated target 219 \nword prediction of paraphasias within spoken discourse using the surrounding language alone. 220 \nWe fine-tuned the LLM BigBird to predict the intended target word of paraphasias within 221 \ntranscripts of the Cinderella story retell task using data from controls, PWA, and a combination. 222 \nWe compared the various models’ accuracy at predicting the correct target word that the human 223 \nraters identified. We hypothesized that fine-tuning the LLM using task data from control 224 \nparticipants as well as PWA would lead to the highest accuracy. Additionally, we evaluated the 225 \nimpact of clinical characteristics and human certainty of target prediction on the model 226 \nperformance. These aims can be summarized in two research objectives: 1) assess the feasibility 227 \nof applying a modern LLM to this task and establish a performance baseline; 2) explore the 228 \nimpact of clinical factors (specifically fluency and aphasia severity) and intended target 229 \nambiguity (according to human raters) on model performance. 230 \nMethod 231 \nData 232 \nData consisted of 353 Cinderella story retelling transcripts from 254 PWA from the 233 \nEnglish AphasiaBank database (MacWhinney et al., 2011). In this task, participants are first 234 \ngiven a wordless picture book of the Cinderella fairytale to briefly review, and then are given a 235 \nfew minutes to recite the story from memory. Demographic and clinical information on these 236 \n254 participants at their first session is shown in Table 1. We also supplemented this data with 237 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \n256 transcripts from control participants without aphasia in AphasiaBank. Our data preparation 238 \npipeline is illustrated in Figure 1. More details are provided in the sections below. 239 \nParaphasia Identification 240 \nArchival audiovisual recordings and CHAT transcript files (Codes for the Human 241 \nAnalysis of Transcripts; MacWhinney, 2000) of the Cinderella story retell task were retrieved 242 \nfrom the English AphasiaBank database on May 4, 2022 for any and all PWA whose sample 243 \ncontained at least one word-level error as annotated by AphasiaBank.1 We defined paraphasias as 244 \nword-level errors made to the lemma of content words (i.e., nouns, verbs, adjectives, adverbs) 245 \nand excluded from target prediction all other kinds of word-level errors, including those related 246 \nto disfluency, morphological markings (e.g., plurality, tense), and non-content words (e.g., 247 \narticles, pronouns). Referencing the CHAT manual (MacWhinney, 2000) accessed on April 13, 248 \n2022, we developed a list of word-level error codes for preliminary inclusion and exclusion.  249 \nTarget Identification 250 \nTarget words were identified and annotated in ELAN transcription software (version 6.2), 251 \nusing custom generated templates that also allowed for review of the retellings’ transcripts as 252 \nwell as playback of audiovisual recordings. To maximize transcript readability and efficacy for 253 \nthis task, AphasiaBank transcripts were preprocessed to remove from view additional 254 \nannotations irrelevant to the task (e.g., utterance-level error coding) as well as the original 255 \nannotator’s target prediction, if provided.  256 \nTarget word identifications were completed by five trained student research assistants in 257 \npseudorandom order under the supervision of a research SLP, resulting in a total of three 258 \n \n1 Although the content of the transcripts is based on the AphasiaBank database on May 4, 2022, we applied \nupdates to the clinical scores that were unavailable on AphasiaBank until December, 2022. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nindependent target identifications for each paraphasia. Research assistants were instructed to 259 \nwatch the audiovisual recordings of the Cinderella story retell task and make their paraphasia 260 \ntarget predictions based on a number of contextual factors, including background knowledge 261 \nrelated to word production disorders and the Cinderella story. For each identified target, a 262 \nconfidence rating ranging from 1 to 4 was assigned with 1 signifying very unconfident, 2 263 \nunconfident, 3 confident, and 4 very confident. In the process, research assistants flagged for 264 \npotential exclusion any word errors believed to be outside the scope of this project (e.g., the 265 \npredicted target is not a noun, verb, adjective, or adverb) or produced in the context of personal 266 \ncommentary (e.g., a comment about the difficulty of the task, performance on the task, etc.). 267 \nIdentified targets from our research assistants as well as AphasiaBank annotators were 268 \nautomatically extracted and compiled for side-by-side comparison and resolution in a 269 \nspreadsheet. Discrepancies in target words and word errors flagged for exclusion were resolved 270 \nby a research SLP to arrive at a single, best target identification and in some cases multiple 271 \nviable target words were provided (e.g., shoe vs. slipper, coach vs. carriage).  If there was 272 \nuniversal agreement among all three raters and AphasiaBank, then that target was not subject to 273 \nresolution. If there was disagreement among raters, rater confidence was low, and the resolver 274 \ncould not arrive at a suitable prediction upon review, then the target was listed as “unknown”. 275 \nAll paraphasia-target pairs were reviewed by the research SLP for phonological similarity and 276 \nwhether or not an intermediary target was readily apparent (e.g., the paraphasia “bot”, where 277 \n“bot” could be interpreted as phonemic paraphasia of “boot”, the intermediary target,  and “boot” 278 \ncould be interpreted as a semantic paraphasia of “slipper”, the ultimate target). We calculated 279 \naverage confidence scores (between the three research assistants) and percent agreement 280 \n(between the three research assistants and the original AphasiaBank target, where available) for 281 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \neach identified target. After filtering to content word paraphasias and excluding paraphasias with 282 \nunknown targets, we were left with 353 Cinderella story sessions from 254 participants, with a 283 \ntotal of 2489 paraphasias.  284 \nSession Text Cleaning 285 \nWe compiled our target identifications as well as human rater confidence and percent 286 \nagreement in the CHAT file format. We added our annotations within the “comment on main 287 \nline” markers specified in the CHAT manual, formatted in a structured notation (YAML) which 288 \ncan be parsed in common programming languages such as Python. The following example shows 289 \none such transcript, with our additional annotations highlighted in boldface type: 290 \n*PAR: and she rode off with the pɪnts@u [: prince] [% {target: a, agreement: 291 \n1.0, confidence: 3.33}] [* p:n] . •680333_684666•  292 \nTo prepare the transcripts for use with our LLM, we automated a process to convert the 293 \ntranscripts to a more natural-looking written English. Motivated by the long-term goal of a fully 294 \nautomated anomia system, we generally aimed to prepare the transcripts to look like those an 295 \nautomatic speech recognition system would produce. Markings indicating prosodic (e.g. pauses) 296 \nand paralinguistic details (e.g. gestures) were removed. The CHAT format also uses special 297 \nmarkers to indicate phenomena peculiar to the spoken modality, such as retracing and repeats. 298 \nFor situations like these, we omitted the special markers, but retained most of the spoken content, 299 \nthough we discarded extraneous words that could be identified by simple rules (e.g. a list of filler 300 \nwords like “um”).  301 \nIn the AphasiaBank files, the transcripts are segmented into units called “utterances” or 302 \n“conversational units.” These units look similar to sentences—they are delimited by periods—303 \nbut tend to be shorter and more fragmentary, owing to the inherent differences between spoken 304 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nand written language. Especially as compared to the written text used to pre-train LLMs, the 305 \nutterance segmentation guidelines laid out by the CHAT manual would not reliably contain a 306 \nsubstantial amount of semantic context for our masked word prediction task. So, while popular 307 \nLLMs (e.g. BERT) typically process a sentence or two at a time, our transcripts do not divide 308 \ncleanly into sentences. Rather than attempt to redraw the AphasiaBank-provided utterance 309 \nboundaries to suit our task, we chose to prepare our data with a full context. In other words, for 310 \neach paraphasia shown to the LLM, the model was working with a participant’s complete 311 \nretelling of the Cinderella story. 312 \nEach paraphasia was prepared for training or testing by replacing it with a “blank” token 313 \n(also known as a “mask”) and filling in the other paraphasias in the session with the human 314 \nidentified target word. The following example from above illustrates the cleaned sentence in 315 \ncontext, where the paraphasia has been replaced with a mask token: 316 \n... and then and and she put her foot in the. and she rode off with the [MASK]. 317 \nCinderella was pretty girl. ... 318 \nDuring fine-tuning and testing, the model learned to fill in the blank of the mask token with the 319 \nmost likely word given the context of the rest of the Cinderella story retelling. 320 \nData Splitting 321 \nWe used ten-fold cross validation of the PWA data in order to reduce model overfitting. 322 \nThat is, we divided the 2,489 instances into ten groups and trained ten separate models for each 323 \nexperiment, in each of which one group was held out as testing data. This was done in such a 324 \nway that for each of the ten iterations, a participant’s responses were only in either the training 325 \ndata or the testing data to prevent the models from learning participant-specific information, and 326 \nthe distribution of Western Aphasia Battery-Revised (WAB-R; Kertesz, 2007) Aphasia Quotient 327 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \n(AQ) scores in training and testing was as close as possible. When evaluating overall 328 \nperformance, the results from the ten test set splits were concatenated, and performance on the 329 \nentire set of 2489 paraphasias was examined. The same ten-fold splits were used for all 330 \nexperiments.  331 \nControl Data Augmentation 332 \nTo add additional training data for our experiments and reduce overfitting, we conducted 333 \ndata augmentation (a method of adding synthetic data; see Feng et al., 2021 for more 334 \nbackground) on sessions of the Cinderella retelling task from control participants without 335 \naphasia. We retrieved all files in AphasiaBank from control participants with a Cinderella story 336 \ntask on April 12, 2022 and added synthetic paraphasias to these sessions. For each session, for 337 \neach utterance spoken by the participant, with a 20% chance we randomly assigned a content 338 \nword (one of: noun, verb, adjective, adverb) to be a “paraphasia” to be predicted. This left a 339 \ncontrol dataset with 256 sessions from 248 participants, with a total of 2427 synthetic 340 \nparaphasias, which was very close to the number of paraphasias from the PWA data (2489). We 341 \ncleaned and prepared these sessions using the same process as for PWA data, described in the 342 \nsubsection Session Text Cleaning. 343 \nModel Training and Experiments 344 \nIn all experiments we used a pre-trained version of the LLM BigBird (Zaheer et al., 345 \n2021). This model is a machine learning-based transformer model. Specifically, it is a sparse-346 \nattention version of BERT designed for longer sequences of text. As previously mentioned, it 347 \nwas pre-trained on masked language modeling. During masked language model training, the 348 \nmodel is given sentences from the corpus where 15% of the tokens are masked (i.e., removed 349 \nand replaced with a special non-word token, “[MASK]”), and the model attempts to predict what 350 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nthose masked words were given the context of the surrounding sentence. By doing this on the 351 \nwhole corpus of sentences, the model learns what words occur in what contexts. We accessed 352 \nthis pre-trained BigBird from the HuggingFace transformer library (Wolf et al., 2020).  353 \nFor each experiment (excluding the baseline experiment), we fine-tuned the LLM using 354 \nanother masked language modeling task. Specifically, given the context of the whole Cinderella 355 \nstory transcript, the model tried to fill in the blank of the mask token with the intended target.2 356 \nThe model then compared that prediction with the human-determined ground truth intended 357 \ntarget (or the original word for control participants), and learned from its correct and incorrect 358 \npredictions. The fine-tuning process was repeated on the whole training data set until early-359 \nstopping occurred, meaning performance stopped improving on a small portion of the testing 360 \ndata that was held out. Once the model was fine-tuned, we tested it on the PWA paraphasias, 361 \nwhich were prepared in the same way as the training data, with each paraphasia sequentially 362 \nreplaced with a mask, and all others filled in with their target. At test time, we pulled out the 363 \nmodel’s top prediction, as well as its nineteen next most likely predictions, giving us its top 364 \ntwenty predictions for the target, sorted from most likely to least likely. We considered more 365 \nthan just the top prediction because there is inherent ambiguity in target identification, and in 366 \nfuture work we may consider multiple possible targets when classifying paraphasias in discourse. 367 \nWe conducted four experiments using different preparations of training data, which are 368 \nsummarized in Table 2. In Experiment 1, we used the pre-trained BigBird model without any 369 \nfine-tuning using Cinderella story data. We considered this our “baseline” model to beat. In 370 \nExperiment 2, we fine-tuned the LLM using just the Cinderella story sessions from control 371 \n \n2 There exist certain subtleties to how this is done at a technical level, which we describe in detail in \nAppendix A. The precise manner in which we performed our masking, and ensuing prediction experiments, would \nbe slightly different had we chosen a different neural model, but the overall methodology would be the same. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nparticipants with synthetic paraphasias. In Experiment 3, the pre-trained model was fine-tuned 372 \nusing Cinderella story sessions from PWA. Finally, in Experiment 4, the model was fine-tuned 373 \nusing a combined data set of control participant data and PWA data.  374 \nEvaluation 375 \nWe evaluated performance of the four experiments using accuracy. We calculated the 376 \naccuracy of “exact match” between the model’s top predicted intended word and the human 377 \ndetermined target word by counting up the number of matches and dividing by the total number 378 \nof test instances. Additionally, we calculated the accuracy within the top one-20 model 379 \npredictions. That is, we counted up how many times out of all test instances the human 380 \ndetermined target word was: the top model prediction (i.e., top one or exact match); the first or 381 \nsecond model prediction (top two); the first, second or third model prediction (top three); and so 382 \non for up to 20 chances to predict the right target. We primarily compared accuracy within one 383 \nchance (exact match) and accuracy within five chances for the four experiments. We determined 384 \nwhether disagreements between exact match accuracy of the models were significant using 385 \nMcNemar’s test with continuity correction (McNemar, 1947). 386 \nFirst, we calculated accuracy on all 2489 paraphasias. To determine what factors 387 \ninfluenced model performance, we also calculated exact match and within five accuracy on 388 \nseveral different test set stratifications for each model. We calculated performance separately on 389 \nsessions from participants with WAB-R AQ above or below the median, participants with fluent 390 \naphasia (Wernicke, Anomic, Conduction, or Transcortical Sensory aphasia, or those considered 391 \n“non aphasic” by the WAB-R) and non-fluent aphasia (Broca, Global, or Transcortical Motor 392 \naphasia), test instances where the human raters had high confidence (above median) or low 393 \nconfidence (below median) in intended target determination, and test instances where human 394 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nraters had perfect agreement in determining the intended target, or imperfect agreement. We 395 \ntested whether differences in performance between these stratifications were significant using 396 \ntwo-sided z-tests for independent proportions. Throughout, a p-value of <0.05 was retained as a 397 \nlevel of statistical significance. 398 \nResults 399 \nAccuracy results from Experiments 1-4 are shown in Tables 3, 4, 5, and 6, respectively. 400 \nExperiment 1, our baseline model, achieved 25.5% for exact match accuracy on all paraphasias. 401 \nExperiment 2, the model fine-tuned on control data, achieved 34.6% exact match accuracy. 402 \nExperiments 3 and 4 (fine-tuned on PWA data and controls plus PWA data respectively) both 403 \nachieved exact match accuracy of 46.8%, 21.3 points above the baseline model. According to 404 \nMcNemar’s test, Experiment 3 and Experiment 4’s exact match accuracy levels were 405 \nsignificantly different than both Experiment 1 (the baseline model) and Experiment 2, all with p 406 \n< 0.001. Experiment 3’s exact match accuracy was not significantly different from Experiment 407 \n4’s exact match accuracy (p = 0.963). 408 \nFigure 3 shows accuracy within the top 20 model predictions for all four experiments. 409 \nAccuracy of all experiments saw the sharpest increase within the top one (exact match) and top 410 \nfive model predictions, and then slower increase when allowing the remaining 15 chances to find 411 \nthe correct target. As stated previously, Experiments 3 and 4 achieved the highest performance of 412 \n46.8% exact match accuracy on all paraphasias. Considering within five accuracy, experiment 4 413 \nobtained 66.8% accuracy within its top five predictions, which was just one point higher than 414 \nExperiment 3, which obtained 65.7% accuracy within top five predictions. Regardless of the 415 \nnumber of top predicted targets we considered, the baseline performed the lowest, followed by 416 \nExperiment 2 (trained on controls), and then the two experiments fine-tuned with PWA data 417 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nwere our highest performing models. When looking across accuracy within top one through 20 418 \npredictions, the difference in performance between Experiment 4 (fine-tuned on PWA and 419 \ncontrols data) and Experiment 3 (fine-tuned on PWA data) was an increase of just one point or 420 \nless. These findings indicate that performance between these two models was not significantly 421 \ndifferent. So, without loss of generality, we discuss Experiment 4 in more detail below. 422 \nWe explored the impact of clinical factors and intended target ambiguity on model 423 \nperformance by sequentially calculating accuracy of the test set stratified by these factors. 424 \nConsidering exact match accuracy, performance in Experiment 4 was higher (59.5%) on the 425 \nparaphasias with targets humans all agreed upon and lower (34.2%) on the paraphasias with less 426 \nthan perfect agreement. A similar pattern emerged for human confidence, with higher accuracy 427 \n(60.5%) on paraphasias with targets humans were more confident at identifying and lower 428 \naccuracy (36.2%) on targets with lower human confidence. We also saw higher performance on 429 \nsessions where the participant had a WAB-R AQ higher than the median (52.7% accuracy) 430 \nversus those where the participant had a WAB-R AQ below the median (41.6% accuracy). 431 \nSimilarly, we saw higher performance on the participants with fluent aphasia (48.7% accuracy) 432 \nthan the participants with non-fluent aphasia (41.2% accuracy). Overall, the highest accuracy out 433 \nof all test sets was on the paraphasias with high human confidence in target determination. For 434 \neach of these four comparisons, the two test set stratifications (e.g., perfect human agreement vs 435 \nimperfect human agreement) obtained significantly different performance levels according to the 436 \ntwo-sided z-test for independent proportions (see Supplemental Table 1 in the Supplemental 437 \nMaterial). P-values were all <= 0.001 except for the fluent versus non-fluent stratification, which 438 \nhad p = 0.016. The same directions of performance difference were seen for the accuracy within 439 \nthe top five predictions of these comparisons. The highest within-five accuracy out of all test set 440 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nstratifications was also seen for the above median human confidence paraphasias, which 441 \nExperiment 4 got correct 76.8% of the time within the top five model predictions. 442 \nDiscussion 443 \nIn this study, we trained a LLM to automatically predict the intended targets for 444 \nparaphasias in discourse during the Cinderella story retelling task. We tried various training data 445 \nconfigurations and our two best performing experiments were fine-tuned using PWA data, with 446 \nor without controls data, and achieved exact match accuracy 47%, and accuracy within top five 447 \npredictions between 66-67%. Considering just one of these (Experiment 4, fine-tuned on PWA 448 \nand controls data), the model performed better on paraphasias which had targets that were easier 449 \nfor humans to identify. It also performed better on paraphasias from participants with less severe 450 \naphasia and fluent aphasia. Overall, this work produced a relatively high performing model for 451 \nautomatically determining paraphasia targets in connected speech, while just using the 452 \nsurrounding context.  453 \nOur baseline model achieved an overall exact match accuracy of 25.5%. This model, 454 \nwhich was not fine-tuned to our data at all, was able to use its general-purpose recognition of 455 \nlanguage patterns to make some correct predictions, without having been exposed to the specific 456 \nvocabulary and structure of the Cinderella story retellings. It is likely that the original corpus of 457 \ntext used in pre-training the LLM would have included examples of various forms of the 458 \nCinderella story, but to a much lesser degree had it been fine-tuned to it. The model used in 459 \nExperiment 2, fine-tuned using data from control-group participants with the addition of 460 \nsynthesized paraphasias, improved by almost ten points beyond the baseline model with exact 461 \nmatch accuracy 34.6%. In this experiment, the pre-trained LLM was specifically exposed to the 462 \nvocabulary and structure of the Cinderella story, as well as the general task of filling in words in 463 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nit, but it was not exposed to any real-world examples of paraphasias. In contrast, Experiment 3, 464 \nfine-tuned on just PWA data, saw a 21 point increase in exact match accuracy over the baseline 465 \nmodel. Thus, training the model for this task required not just exposing the pre-trained model to 466 \nthe vocabulary of the Cinderella story, but also specifically examples of real-world paraphasias 467 \nthat occur in that task. Somewhat surprisingly, the model using both PWA data and controls data 468 \n(Experiment 4) did not improve beyond the model fine-tuned with just PWA data (Experiment 469 \n3). This likely indicates that the PWA data gave enough of that vocabulary knowledge to the 470 \nLLM, and the controls data did not provide any further information. However, more work could 471 \nbe done to synthesize paraphasias in the controls data to make them more similar to real-world 472 \nparaphasias. As described in the Control Data Augmentation subsection, we attempted to make 473 \nthem more “realistic” by only making content words paraphasias, but there are other possibilities 474 \nthat could be explored in future work: adding synthetic re-tracings, for example, as well as 475 \nutilizing psycholinguistic variables (e.g. length in phonemes, frequency of occurrence, 476 \nimageability, etc.) to produce more realistic synthetic training data.  477 \nWe found that human certainty about paraphasia targets was associated with model 478 \nperformance. Specifically, our best performing model (Experiment 4) performed significantly 479 \nbetter on paraphasias with targets that humans were more confident on or had perfect agreement 480 \non. This association is reassuring and acts as a simple validity check, since it indicates that our 481 \ntrained models had an easier time with the more obvious targets. There is inherent ambiguity in 482 \ndetermining targets for paraphasias in discourse. Half of the paraphasias had percent agreement 483 \nbelow 100%, and in fact, average percent agreement on target identification was 76.8%. 484 \nMoreover, this percentage agreement is only on the paraphasias for which we were able to 485 \nresolve a target and excludes targets where ground truth could not be determined. Considering 486 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \n76.8% agreement as a stand-in for the obtainable human accuracy on this task, obtaining 46.8% 487 \naccuracy on paraphasias with known targets appears high. Relatedly, while the LLM was 488 \ndesigned to rely exclusively on the surrounding language for its predictions, human raters had 489 \naccess to audiovisual recordings and transcripts and thus were able predict targets utilizing 490 \nadditional sources of information such as phonological similarity and gestures. 491 \nWe also found that, as expected, Experiment 4 saw significantly different performance 492 \nbetween participants with above median severity and below median severity, according to the 493 \nWAB-R AQ, with exact match accuracy 8.4% higher on participants with less severe aphasia. 494 \nThe exact reason for this difference in performance, whether it be factors such as increased 495 \noccurrence of abandoned phrasings or multiple paraphasias from more severe participants, could 496 \nbe examined further. Relatedly, Experiment 4 performed significantly better on fluent 497 \nparticipants than non-fluent participants. Our fluent (Wernicke, Anomic, Conduction, 498 \nTranscortical Sensory, or non-aphasic by WAB-R) and non-fluent (Broca, Global, or 499 \nTranscortical Motor) stratifications acted as a proxy for capturing paragrammatic and 500 \nagrammatic aphasia types respectively. The non-fluent (and perhaps agrammatic) participants 501 \nmay have harder to identify targets because of a lack of content words and context for the LLM 502 \nto rely on. However, we recognize limitations with this approach. We had substantially fewer 503 \ntraining examples from non-fluent participants (449 paraphasias) than fluent participants (1666 504 \nparaphasias), which may have impacted that performance difference.  Additionally, classification 505 \nbased on the WAB-R is not perfect as there is both classification error and considerable 506 \nheterogeneity within groups. Finally, the mapping between fluency types and type of 507 \ngrammatical deficits is not perfect. Nonetheless, these stratifications of the test set provided 508 \nsome clues on what features impact performance and where the models can improve. It is also 509 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \npossible that, particularly with more training data, separate models trained for use on specific 510 \ntypes of aphasia could see higher performance and better clinical utility.  511 \nAfter our quantitative analyses, we conducted an informal review of  Experiment 4’s 512 \noutput, observing some of the more apparent patterns. Some errors were rather unsurprising, like 513 \nswapping similar verbs (e.g. “sweeping” for “cleaning”). Others were random and garbled (e.g. 514 \n“Cinderellaipper” for “slipper”) and obviously a consequence of the text encoding constraints 515 \n(see Appendix A). Where larger patterns stood out, though, they tended to point to a few 516 \npeculiarities of the dataset. 517 \nFor example, about 26% of the samples in our dataset involved paraphasias which 518 \nAphasiaBank had annotated as part of a “retracing” event. Retracing is when a speaker abandons 519 \na segment of speech and then retries that segment again (e.g. “Cinderella <put on> [//] tried on 520 \nthe slipper”). When a target word was involved in a retracing event, our LLM’s top-five 521 \naccuracy for target prediction increased to 80% (vs. 62% when it was not).  Since we fill in all 522 \nthe paraphasia targets except the current target (see Model Training and Experiments) any other 523 \nparaphasias in the immediate context would have been filled in with the correct target word, 524 \nwhich provides an advantage for the task at hand. However, this can also work against the model 525 \nwhen a target was not actually a part of a retracing event. Informally, we observed that the model 526 \nsometimes incorrectly chose a word from the immediate context, predicting a retracing where 527 \nthere was none. 528 \nAnother peculiarity of our dataset was the storytelling task itself, marked by a Cinderella-529 \ncentric distribution of target words. Out of the 523 unique target words, about 30% of targets 530 \nwere one of five salient words from the fairy tale ( “Cinderella,” “prince,” “slipper,” “ball,” or 531 \n“godmother”). For the most common word, “Cinderella” (265 examples, 11% of total), the LLM 532 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nwas correct 170 times (64%) within the first guess and 227 times (86%) within five guesses. 533 \nHowever, this advantage was largely canceled out when the correct target was not the 534 \nprotagonist’s name: the model incorrectly predicted “Cinderella” 157 times as a first guess, and 535 \n443 times as a top-five guess. Looking at a subset of the data unaffected by the above factors, we 536 \nfind 233 samples which had a unique target word (occurring only once) and also were not part of 537 \na retracing event. The first-guess accuracy for these samples dropped from 39% to 15% between 538 \nthe baseline and fine-tuned models, respectively.  539 \nThese three patterns—predicting targets that were repeats from the surrounding context, 540 \nfrequently predicting common words from the task, and having difficulty with more rare 541 \nwords—are all consequences of fine-tuning a model. There is a tradeoff between the desirable 542 \noutcome of improving performance by following common patterns in the training data and the 543 \nloss in performance when new data points break that pattern; this is known as the bias-variance 544 \ntradeoff and is well documented in machine learning literature (Geman et al., 1992; Belkin et al., 545 \n2019). We employed techniques to reduce overfitting to the training data (data augmentation, 546 \ncross validation, early stopping), but more strategies could be explored.  547 \nGiven the architecture of our LLM, we suspect various utterance-related measures would 548 \nalso influence target prediction accuracy for a given speaker and/or utterance. For example, we 549 \nwould predict that speakers with longer utterances, i.e., mean length of utterance in words, would 550 \nbe supplying the model with more linguistic information and therefore increase the likelihood of 551 \ntarget prediction success. Another set of hypotheses relates to the quality of the speaker's 552 \nutterances in terms of completeness, percentage of utterances that are complete sentences; 553 \ncorrectness, percentage of syntactically and/or semantically correct sentences; complexity, 554 \nnumber of embedded clauses per sentence, sentence complexity ratio (Thompson et al., 1995), 555 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nand verbs per utterance; as well as lexical diversity measures like type-token ratio and vocd 556 \n(Malvern, Richards, Chipere, & Purán, 2004). As mentioned previously, these factors may 557 \nfurther explain why performance was affected by fluency and aphasia severity. All of the 558 \naforementioned speaker outcome measures can be automatically calculated using CLAN 559 \nsoftware (MacWhinney, 2000), and we posit all of them would be positive predictors of target 560 \nprediction accuracy. To deepen our understanding and interpretation of our results, therefore, a 561 \nfuture direction of this work is to employ a generalized linear mixed effects model to test these 562 \nhypothesizes and quantify the magnitude of any significant predictors.  563 \nThere are many other future directions for this work. Currently, we achieve 46.8% 564 \naccuracy at predicting paraphasia targets by just using the text of the story, excluding the 565 \nparaphasia. However, in many cases the details of the paraphasia itself would provide useful 566 \ninformation for determining the target. In future work, we plan to develop a model that uses both 567 \nthe semantic context surrounding the paraphasia as well as the phonemes of the paraphasia itself 568 \nto further improve predictive utility. Considering the difficulty of the task at hand, our 569 \nperformance using just the surrounding language is surprisingly high. However, as mentioned, 570 \nthe Cinderella retelling task is a highly constrained activity, with a much smaller expected target 571 \nvocabulary than in standard speech. In the context of test and scale development for clinical 572 \nassessment, when batteries typically include one or two specific stories, gains due to the 573 \nconstrained nature of the stimuli are advantageous. However, in the future, it could be beneficial 574 \nto train models for less constrained tasks or more naturalistic speech. Additionally, these findings 575 \nopen up possibilities for novel applications that extend beyond assessment, such as augmentative 576 \nand alternative communication systems. Finally, as previously mentioned, we intend to 577 \neventually extend ParAlg, our automated system for classifying paraphasias, to use it on 578 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \ndiscourse. This work generates a preliminary model for the first step in that process: 579 \nautomatically identifying the most likely targets for paraphasias in discourse.  580 \nAcknowledgments 581 \nThis work was supported by National Institute on Deafness and Other Communication 582 \nDisorders Grant R01DC015999 (Principal Investigators: Steven Bedrick and Gerasimos 583 \nFergadiotis). We would also like to thank Mia Cywinski, Samuel Hedine, Lidiya Khoroshenkikh, 584 \nJonathan Madrigal, and Anya Russell for their crucial work identifying paraphasia targets. 585 \nData Availability Statement 586 \nData from PWA and controls is available from AphasiaBank to all members of the AphasiaBank 587 \nconsortium group (https://aphasia.talkbank.org/).  588 \nReferences 589 \nAdams, J., Bedrick, S., Fergadiotis, G., Gorman, K., & van Santen, J. (2017). Target word 590 \nprediction and paraphasia classification in spoken discourse. BioNLP 2017, 1–8. 591 \nhttps://doi.org/10.18653/v1/W17-2301 592 \nBalagopalan, A., Eyre, B., Rudzicz, F., & Novikova, J. (2020). To BERT or not to BERT: 593 \nComparing speech and language-based approaches for Alzheimer’s disease detection. 594 \nInterspeech 2020, 2167–2171. https://doi.org/10.21437/Interspeech.2020-2557  595 \nBelkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning 596 \npractice and the classical bias-variance trade-off. Proceedings of the National Academy 597 \nof Sciences, 116(32), 15849–15854. https://doi.org/10.1073/pnas.1903070116 598 \n 599 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nBock, K. (1995). Sentence production: From mind to mouth. In Speech, Language, and 600 \nCommunication (pp. 181–216). Elsevier. https://doi.org/10.1016/B978-012497770-601 \n9/50008-X 602 \nBreimaier, H. E., Heckemann, B., Halfens, R. J. G., & Lohrmann, C. (2015). The Consolidated 603 \nFramework for Implementation Research (CFIR): A useful theoretical framework for 604 \nguiding and evaluating a guideline implementation process in a hospital-based nursing 605 \npractice. BMC Nursing, 14(1), 43. https://doi.org/10.1186/s12912-015-0088-4 606 \nBryant, L., Spencer, E., Ferguson, A., Craig, H., Colyvas, K., & Worrall, L. (2013). 607 \nPropositional Idea Density in aphasic discourse. Aphasiology, 27(8), 992–1009. 608 \nhttps://doi.org/10.1080/02687038.2013.803514 609 \nButterworth, B., & Howard, D. (1987). Paragrammatisms. Cognition, 26(1), 1–37. 610 \nhttps://doi.org/10.1016/0010-0277(87)90012-6 611 \n Casilio, M., Fergadiotis, G., Salem, A. C., Gale, R., McKinney-Bock, K., & Bedrick, S. (2023). 612 \nParAlg: A paraphasia algorithm for multinomial classification of picture naming errors. 613 \nJournal of Speech, Language, and Hearing Research. 614 \nhttps://doi.org/10.1044/2022_JSLHR-22-00255 615 \nChatzoudis, G., Plitsis, M., Stamouli, S., Dimou, A., Katsamanis, N., & Katsouros, V. (2022).  616 \nZero-shot cross-lingual aphasia detection using automatic speech recognition. 617 \nInterspeech 2022, 2178–2182. https://doi.org/10.21437/Interspeech.2022-10681 618 \nCruice, M., Worrall, L., Hickson, L., & Murison, R. (2003). Finding a focus for quality of life 619 \nwith aphasia: Social and emotional health, and psychological well-being. Aphasiology, 620 \n17(4), 333–353. https://doi.org/10.1080/02687030244000707 621 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nDamschroder, L. J., Aron, D. C., Keith, R. E., Kirsh, S. R., Alexander, J. A., & Lowery, J. C. 622 \n(2009). Fostering implementation of health services research findings into practice: A 623 \nconsolidated framework for advancing implementation science. Implementation Science, 624 \n4(1), 50. https://doi.org/10.1186/1748-5908-4-50 625 \nDay, M., Dey, R. K., Baucum, M., Paek, E. J., Park, H., & Khojandi, A. (2021).  Predicting 626 \nseverity in people with aphasia: A natural language processing and machine learning 627 \napproach. 2021 43rd Annual International Conference of the IEEE Engineering in 628 \nMedicine & Biology Society (EMBC), 2299–2302. 629 \nhttps://doi.org/10.1109/EMBC46164.2021.9630694 630 \nDell, G. S. (1986). A spreading-activation theory of retrieval in sentence production. 631 \nPsychological Review, 93(3), 283–321. https://doi.org/10.1037/0033-295X.93.3.283 632 \nDell, G. S., Chang, F., & Griffin, Z. M. (1999).  Connectionist models of language production: 633 \nLexical access and grammatical encoding. Cognitive Science, 23(4), 517–542. 634 \nhttps://doi.org/10.1207/s15516709cog2304_6 635 \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019).  BERT: Pre-training of deep 636 \nbidirectional transformers for language understanding. Proceedings of the 2019 637 \nConference of the North American Chapter of the Association for Computational 638 \nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–639 \n4186. https://doi.org/10.18653/v1/N19-1423 640 \nFeng, S. Y., Gangal, V., Wei, J., Chandar, S., Vosoughi, S., Mitamura, T., & Hovy, E. (2021). A 641 \nsurvey of data augmentation approaches for NLP. Findings of the Association for 642 \nComputational Linguistics: ACL-IJCNLP 2021, 968–988. 643 \nhttps://doi.org/10.18653/v1/2021.findings-acl.84 644 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nFergadiotis, G., Gorman, K., & Bedrick, S. (2016).  Algorithmic classification of five 645 \ncharacteristic types of paraphasias. American Journal of Speech-Language Pathology, 646 \n25(4S). https://doi.org/10.1044/2016_AJSLP-15-0147 647 \nFergadiotis, G., Kapantzoglou, M., Kintz, S., & Wright, H. H. (2019). Modeling confrontation 648 \nnaming and discourse informativeness using structural equation modeling. Aphasiology, 649 \n33(5), 544–560. https://doi.org/10.1080/02687038.2018.1482404 650 \nFergadiotis, G., & Wright, H. H. (2011). Lexical diversity for adults with and without aphasia 651 \nacross discourse elicitation tasks. Aphasiology, 25(11), 1414–1430. 652 \nhttps://doi.org/10.1080/02687038.2011.603898 653 \nFergadiotis, G., Wright, H. H., & Capilouto, G. J. (2011). Productive vocabulary across 654 \ndiscourse types. Aphasiology, 25(10), 1261–1278. 655 \nhttps://doi.org/10.1080/02687038.2011.606974 656 \nFergadiotis, G., Wright, H. H., & West, T. M. (2013). Measuring lexical diversity in narrative 657 \ndiscourse of people with aphasia. American Journal of Speech-Language Pathology, 658 \n22(2). https://doi.org/10.1044/1058-0360(2013/12-0083 659 \nForbes, M., Fromm, D., Holland, A., & MacWhinney, B. (2014). EVAL: A tool for clinicians 660 \nfrom AphasiaBank. Clinical Aphasiology Conference, St. Simons Island, GA. 661 \nFraser, K., Rudzicz, F., Graham, N., & Rochon, E. (2013). Automatic speech recognition in the 662 \ndiagnosis of primary progressive aphasia. Proceedings of the Fourth Workshop on 663 \nSpeech and Language Processing for Assistive Technologies, 47–54. 664 \nhttps://www.aclweb.org/anthology/W13-3909 665 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nGale, R., Bird, J., Wang, Y., van Santen, J., Prud’hommeaux, E., Dolata, J., & Asgari, M. (2021).  666 \nAutomated scoring of tablet-administered expressive language tests. Frontiers in 667 \nPsychology, 12, 668401. https://doi.org/10.3389/fpsyg.2021.668401 668 \nGale, R. C., Fleegle, M., Fergadiotis, G., & Bedrick, S. (2022). The Post-Stroke Speech 669 \nTranscription (PSST) Challenge. Proceedings of the RaPID Workshop - Resources and 670 \nProcessIng of Linguistic, Para-Linguistic and Extra-Linguistic Data from People with 671 \nVarious Forms of Cognitive/Psychiatric/Developmental Impairments - within the 13th 672 \nLanguage Resources and Evaluation Conference, 41–55. 673 \nhttps://aclanthology.org/2022.rapid-1.6 674 \nGeman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the Bias/Variance 675 \ndilemma. Neural Computation, 4(1), 1–58. https://doi.org/10.1162/neco.1992.4.1.1 676 \nGoodglass, H. (1993). Understanding aphasia. Academic Press. 677 \nGoodglass, H., & Wingfield, A. (Eds.). (1997). Anomia: Neuroanatomical and cognitive 678 \ncorrelates. Academic Press. 679 \nHickin, J., Best, W., Herbert, R., Howard, D., & Osborne, F. (2001).  Treatment of word retrieval 680 \nin aphasia: Generalisation to conversational speech. International Journal of Language & 681 \nCommunication Disorders, 36(s1), 13–18. https://doi.org/10.3109/13682820109177851 682 \nKertesz, A. (2012). Western Aphasia Battery—Revised [Data set]. American Psychological 683 \nAssociation. https://doi.org/10.1037/t15168-000 684 \nKudo, T., & Richardson, J. (2018). SentencePiece: A simple and language independent subword 685 \ntokenizer and detokenizer for Neural Text Processing. Proceedings of the 2018 686 \nConference on Empirical Methods in Natural Language Processing: System 687 \nDemonstrations, 66–71. https://doi.org/10.18653/v1/D18-2012 688 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nLe, D., Licata, K., & Mower Provost, E. (2018). Automatic quantitative analysis of spontaneous 689 \naphasic speech. Speech Communication, 100, 1–12. 690 \nhttps://doi.org/10.1016/j.specom.2018.04.001 691 \nLe, D., Licata, K., & Provost, E. M. (2017). Automatic paraphasia detection from aphasic 692 \nspeech: A preliminary study. Proc. Interspeech 2017, 294–298. 693 \nhttps://doi.org/10.21437/Interspeech.2017-626 694 \nLe, D., & Provost, E. M. (2016). Improving automatic recognition of aphasic speech with 695 \nAphasiaBank. Interspeech 2016, 2681–2685. https://doi.org/10.21437/Interspeech.2016-696 \n213 697 \nLevelt, W. J. M. (1999). Models of word production. Trends in Cognitive Sciences, 3(6), 223–698 \n232. https://doi.org/10.1016/S1364-6613(99)01319-4 699 \nLevelt, W. J. M., Roelofs, A., & Meyer, A. S. (1999). A theory of lexical access in speech 700 \nproduction. Behavioral and Brain Sciences, 22(01). 701 \nhttps://doi.org/10.1017/S0140525X99001776 702 \nLiu, Y., & Lapata, M. (2019). Text summarization with pretrained encoders. Proceedings of the 703 \n2019 Conference on Empirical Methods in Natural Language Processing and the 9th 704 \nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 705 \n3728–3738. https://doi.org/10.18653/v1/D19-1387 706 \nLowerre, T. B. (1976). The Harpy speech recognition system [Ph.D. Thesis]. Carnegie Mellon 707 \nUniversity. 708 \nMacDonald, M. C., Pearlmutter, N. J., & Seidenberg, M. S. (1994). The lexical nature of 709 \nsyntactic ambiguity resolution. Psychological Review, 101(4), 676–703. 710 \nhttps://doi.org/10.1037/0033-295X.101.4.676 711 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nMacWhinney, B. (2000).  The CHILDES project: Tools for analyzing talk (3rd ed.). Lawrence 712 \nErlbaum Associates. 713 \nMacWhinney, B., Fromm, D., Forbes, M., & Holland, A. (2011). AphasiaBank: Methods for 714 \nstudying discourse. Aphasiology, 25(11), 1286–1307. 715 \nhttps://doi.org/10.1080/02687038.2011.589893 716 \nMalvern, D. (Ed.). (2008). Lexical diversity and language development: Quantification and 717 \nassessment. Palgrave Macmillan. 718 \nMayer, J., & Murray, L. (2003). Functional measures of naming in aphasia: Word retrieval in 719 \nconfrontation naming versus connected speech. Aphasiology, 17(5), 481–497. 720 \nhttps://doi.org/10.1080/02687030344000148 721 \nMcNemar, Q. (1947). Note on the sampling error of the difference between correlated 722 \nproportions or percentages. Psychometrika, 12(2), 153–157. 723 \nhttps://doi.org/10.1007/BF02295996 724 \nMiller, J., & Iglesias, A. (2012).  Systematic Analysis of Language Transcripts (SALT), research 725 \nversion 2012 [computer software]. SALT Software, LLC. 726 \nPapathanasiou, I., & Coppens, P. (2017). Disorders of word production. In Aphasia And Related 727 \nNeurogenic Communication Disorders (1st ed., pp. 169–195). Jones & Bartlett Learning. 728 \nPashek, G. V., & Tompkins, C. A. (2002). Context and word class influences on lexical retrieval 729 \nin aphasia. Aphasiology, 16(3), 261–286. https://doi.org/10.1080/02687040143000573 730 \nPeng, Y., Yan, S., & Lu, Z. (2019).  Transfer learning in biomedical natural language processing: 731 \nAn evaluation of BERT and ELMo on ten benchmarking datasets. Proceedings of the 732 \n18th BioNLP Workshop and Shared Task, 58–65. https://doi.org/10.18653/v1/W19-5006 733 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nPerez, M., Aldeneh, Z., & Provost, E. M. (2020). Aphasic speech recognition using a mixture of 734 \nspeech intelligibility experts. Interspeech 2020, 4986–4990. 735 \nhttps://doi.org/10.21437/Interspeech.2020-2049 736 \nRabin, L., Barr, W., & Burton, L. (2005). Assessment practices of clinical neuropsychologists in 737 \nthe United States and Canada: A survey of INS, NAN, and APA Division 40 members. 738 \nArchives of Clinical Neuropsychology, 20(1), 33–65. 739 \nhttps://doi.org/10.1016/j.acn.2004.02.005 740 \nRichardson, J. D., Hudspeth Dalton, S. G., Fromm, D., Forbes, M., Holland, A., & MacWhinney, 741 \nB. (2018). The relationship between confrontation naming and story gist production in 742 \naphasia. American Journal of Speech-Language Pathology, 27(1S), 406–422. 743 \nhttps://doi.org/10.1044/2017_AJSLP-16-0211 744 \nRoach, A., Schwartz, M. F., Martin, N., Grewal, R. S., & Brecher, A. (1996). The Philadelphia 745 \nNaming Test: Scoring and rationale. Clinical Aphasiology, 24, 121–133. 746 \nSaffran, E. M., Berndt, R. S., & Schwartz, M. F. (1989). The quantitative analysis of agrammatic 747 \nproduction: Procedure and data. Brain and Language, 37(3), 440–479. 748 \nhttps://doi.org/10.1016/0093-934X(89)90030-8 749 \nSalem, A. C., Gale, R., Casilio, M., Fleegle, M., Fergadiotis, G., & Bedrick, S. (2022). Refining 750 \nsemantic similarity of paraphasias using a contextual language model. Journal of Speech, 751 \nLanguage, and Hearing Research, 1–15. https://doi.org/10.1044/2022_JSLHR-22-00277 752 \nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: 753 \nSmaller, faster, cheaper and lighter. https://doi.org/10.48550/ARXIV.1910.01108 754 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nSchwartz, B. (2020, October 15). Google: BERT now used on almost every English query. 755 \nSearch Engine Land. https://searchengineland.com/google-bert-used-on-almost-every-756 \nenglish-query-342193 757 \nSchwartz, M., Dell, G., Martin, N., Gahl, S., & Sobel, P. (2006). A case-series test of the 758 \ninteractive two-step model of lexical access: Evidence from picture naming. Journal of 759 \nMemory and Language, 54(2), 228–264. https://doi.org/10.1016/j.jml.2005.10.001 760 \nSimmons-Mackie, N., Threats, T. T., & Kagan, A. (2005). Outcome assessment in aphasia: A 761 \nsurvey. Journal of Communication Disorders, 38(1), 1–27. 762 \nhttps://doi.org/10.1016/j.jcomdis.2004.03.007 763 \nStrauss, E., Sherman, E. M. S., & Spreen, O. (2006). A compendium of neuropsychological tests: 764 \nAdministration, norms, and commentary (E. M. S. Sherman, E. Strauss, & O. Spreen, 765 \nEds.; 3rd ed). Oxford University Press. 766 \nTabor, W., Juliano, C., & Tanenhaus, M. K. (1997).  Parsing in a dynamical system: An 767 \nattractor-based account of the interaction of lexical and structural constraints in sentence 768 \nprocessing. Language and Cognitive Processes, 12(2–3), 211–271. 769 \nhttps://doi.org/10.1080/016909697386853 770 \nThompson, C. K., Shapiro, L. P., Tait, M. E., Jacobs, B., Schneider, S. L., & Ballard, K. (1995). 771 \nA system for the linguistic analysis of agrammatic language production. Brain and 772 \nLanguage, 51(1), 124–129. 773 \nThompson, C. K., Lange, K. L., Schneider, S. L., & Shapiro, L. P. (1997). Agrammatic and non-774 \nbrain-damaged subjects’ verb and verb argument structure production. Aphasiology, 775 \n11(4–5), 473–490. https://doi.org/10.1080/02687039708248485 776 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \n Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & 777 \nPolosukhin, I. (2017). Attention is all you need. Proceedings of the 31st International 778 \nConference on Neural Information Processing Systems, 6000–6010. 779 \nWalker, G. M., & Schwartz, M. F. (2012).  Short-form Philadelphia Naming Test: Rationale and 780 \nempirical evaluation. American Journal of Speech-Language Pathology, 21(2). 781 \nhttps://doi.org/10.1044/1058-0360(2012/11-0089) 782 \nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, 783 \nR., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., 784 \nXu, C., Le Scao, T., Gugger, S., … Rush, A. (2020).  Transformers: State-of-the-art 785 \nnatural language processing. Proceedings of the 2020 Conference on Empirical Methods 786 \nin Natural Language Processing: System Demonstrations, 38–45. 787 \nhttps://doi.org/10.18653/v1/2020.emnlp-demos.6 788 \nZaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, 789 \nA., Wang, Q., Yang, L., & Ahmed, A. (2020). Big Bird: Transformers for longer 790 \nsequences. Proceedings of the 34th International Conference on Neural Information 791 \nProcessing Systems.  792 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nFigures 793 \nFigure 1 794 \nData preparation pipeline 795 \n 796 \nNote. CHAT stands for Codes for the Human Analysis of Transcripts, and is a format for 797 \ntranscription. PWA stands for people with aphasia.  798 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nFigure 2 799 \nAccuracy within top 1-20 predicted targets for experiments 1-4 800 \n 801 \nNote. PWA stands for people with aphasia.  802 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nTables 803 \nTable 1 804 \nClinical and demographic information for the 254 participants at their first session. 805 \nCharacteristic Value \nAge (years)  \nM (SD) 61.916 (12.408) \nMin - Max 25.600 - 91.718 \nMissing (N) 24 \nGender  \nM (N) 133 \nF (N) 100 \nMissing (N) 21 \nRace  \nWhite (N) 201 \nAfrican American (N) \nAsian (N) \nHispanic/Latino (N) \nNative Hawaiian/ Pacific Islander (N) \nMixed (N) \nUnavailable (N) \n23 \n2 \n5 \n1 \n1 \n21 \nEducation (years)  \nM (SD) 15.498 (2.828) \nMin - Max 8.000 - 25.000  \nMissing (N) 31 \nAphasia duration  \nM (SD) 5.429 (4.829) \nMin - Max 0.080 - 30.000 \nMissing (N) 24 \nWAB-R AQ  \nM (SD) 72.271 (17.992) \nMin - Max 10.800 - 99.600 \nMissing (N) 11 \nBNT-SF  \nM (SD) 7.369 (4.512) \nMin - Max 0.000 - 15.000 \nMissing (N) 32 \nVNT  \nM (SD) 15.000 (6.275) \nMin - Max 0.000 - 22.000 \nMissing (N) 32 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nNote. WAB-R AQ is the Western Aphasia Battery-Revised Aphasia Quotient (Kertesz, 2012). 806 \nBNT-SF is the raw score from the Boston Naming Test-Short Form (Kaplan et al., 2001). VNT 807 \nis the raw score from the Verb Naming Test (Cho-Reyes et al., 2012).  808 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nTable 2 809 \nDescriptions of experiments 1-4 810 \nExperiment \nNumber \nExperiment \nName Description Training data Testing data \n1 Baseline Pre-trained LLM, \nwithout any fine-tuning \nto our data \nN/A PWA testing \ndata \n2 Controls Pre-trained LLM, fine-\ntuned using all data from \nthe control participants \nof the Cinderella story \ntask \nControls training \ndata \nPWA testing \ndata \n3 PWA Pre-trained LLM, fine-\ntuned using all PWA \ndata from the Cinderella \nstory task \nPWA training \ndata \nPWA testing \ndata \n4 Controls + \nPWA \nPre-trained LLM, fine-\ntuned using all data from \nthe control participants \nand PWA, from the \nCinderella story task \nControls training \ndata + PWA \ntraining data \nPWA testing \ndata \n \n \n \nNote. PWA stands for people with aphasia. LLM stands for large language model. Note that all 811 \nmodels are tested on PWA testing data.  812 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nTable 3 813 \nExperiment 1: Baseline 814 \nTest set Number of \nparaphasias \nAccuracy \nexact match \nAccuracy \nwithin 5 \nAll paraphasias 2489 0.255 0.379 \nHuman agreement = 100% 1244 0.309 0.405 \nHuman agreement < 100% 1245 0.201 0.353 \nHuman confidence > median (3.3) 1089 0.319 0.419 \nHumans confidence <= median (3.3) 1400 0.206 0.348 \nWAB-R AQ > median (74.6) 1039 0.294 0.410 \nWAB-R AQ <= median (74.6) 1076 0.204 0.325 \nFluent participants 1666 0.261 0.385 \nNon-fluent participants 449 0.198 0.301 \nNote. WAB-R AQ is the Western Aphasia Battery-Revised Aphasia Quotient (Kertesz, 2012). 815 \nFluent participants are those with Wernicke, Anomic, Conduction, or Transcortical Sensory 816 \naphasia, or those considered “non aphasic” by the WAB-R. Non-fluent participants are those 817 \nwith the Broca, Global, or Transcortical Motor aphasia. 48 out of 353 total sessions had 818 \nunavailable WAB-R results and were excluded just from analyses involving WAB-R scores. 819 \nAccuracy exact match refers to the top model prediction of target word matching the human-820 \nidentified target word. Accuracy within 5 refers to the human-identified target word being one of 821 \nthe top five model predictions.  822 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nTable 4 823 \nExperiment 2: Fine-tuned on controls data 824 \nTest set Number of \nparaphasias \nAccuracy \nexact match \nAccuracy \nwithin 5 \nAll paraphasias 2489 0.346 0.517 \nHuman agreement = 100% 1244 0.436 0.600 \nHuman agreement < 100% 1245 0.255 0.434 \nHuman confidence > median (3.3) 1089 0.453 0.614 \nHumans confidence <= median (3.3) 1400 0.263 0.441 \nWAB-R AQ > median (74.6) 1039 0.398 0.580 \nWAB-R AQ <= median (74.6) 1076 0.290 0.453 \nFluent participants 1666 0.362 0.543 \nNon-fluent participants 449 0.274 0.414 \n 825 \nNote. WAB-R AQ is the Western Aphasia Battery-Revised Aphasia Quotient (Kertesz, 2012). 826 \nFluent participants are those with Wernicke, Anomic, Conduction, or Transcortical Sensory 827 \naphasia, or those considered “non aphasic” by the WAB-R. Non-fluent participants are those 828 \nwith the Broca, Global, or Transcortical Motor aphasia. 48 out of 353 total sessions had 829 \nunavailable WAB-R results and were excluded just from analyses involving WAB-R scores. 830 \nAccuracy exact match refers to the top model prediction of target word matching the human-831 \nidentified target word. Accuracy within 5 refers to the human-identified target word being one of 832 \nthe top five model predictions.  833 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nTable 5 834 \nExperiment 3: Fine-tuned on PWA data 835 \nTest set Number of \nparaphasias \nAccuracy \nexact match \nAccuracy \nwithin 5 \nAll paraphasias 2489 0.468 0.657 \nHuman agreement = 100% 1244 0.595 0.767 \nHuman agreement < 100% 1245 0.342 0.548 \nHuman confidence > median (3.3) 1089 0.605 0.768 \nHumans confidence <= median (3.3) 1400 0.362 0.571 \nWAB-R AQ > median (74.6) 1039 0.527 0.703 \nWAB-R AQ <= median (74.6) 1076 0.416 0.621 \nFluent participants 1666 0.487 0.670 \nNon-fluent participants 449 0.412 0.626 \nNote. PWA stands for people with aphasia. WAB-R AQ is the Western Aphasia Battery-Revised 836 \nAphasia Quotient (Kertesz, 2012). Fluent participants are those with Wernicke, Anomic, 837 \nConduction, or Transcortical Sensory aphasia, or those considered “non aphasic” by the WAB-R. 838 \nNon-fluent participants are those with the Broca, Global, or Transcortical Motor aphasia. 48 out 839 \nof 353 total sessions had unavailable WAB-R results and were excluded just from analyses 840 \ninvolving WAB-R scores. Accuracy exact match refers to the top model prediction of target 841 \nword matching the human-identified target word. Accuracy within 5 refers to the human-842 \nidentified target word being one of the top five model predictions.  843 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nTable 6 844 \nExperiment 4: Fine-tuned on controls and PWA data 845 \nTest set Number of \nparaphasias \nAccuracy \nexact match \nAccuracy \nwithin 5 \nAll paraphasias 2489 0.468 0.668 \nHuman agreement = 100% 1244 0.572 0.767 \nHuman agreement < 100% 1245 0.363 0.569 \nHuman confidence > median (3.3) 1089 0.600 0.792 \nHumans confidence <= median (3.3) 1400 0.365 0.572 \nWAB-R AQ > median (74.6) 1039 0.510 0.700 \nWAB-R AQ <= median (74.6) 1076 0.426 0.638 \nFluent participants 1666 0.478 0.681 \nNon-fluent participants 449 0.425 0.624 \nNote. PWA stands for people with aphasia. WAB-R AQ is the Western Aphasia Battery-Revised 846 \nAphasia Quotient (Kertesz, 2012). Fluent participants are those with Wernicke, Anomic, 847 \nConduction, or Transcortical Sensory aphasia, or those considered “non aphasic” by the WAB-R. 848 \nNon-fluent participants are those with the Broca, Global, or Transcortical Motor aphasia. 48 out 849 \nof 353 total sessions had unavailable WAB-R results and were excluded just from analyses 850 \ninvolving WAB-R scores. Accuracy exact match refers to the top model prediction of target 851 \nword matching the human-identified target word. Accuracy within 5 refers to the human-852 \nidentified target word being one of the top five model predictions.  853 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nAppendix 854 \nAppendix A: Details of Masking and Decoding 855 \nTo encode our inputs and outputs into a discrete numerical form recognizable to our 856 \nspecific choice of LLM, the text is encoded as sub-word units called SentencePieces (Kudo & 857 \nRichardson, 2018). For example, the word “slipper” is represented by two tokens: “sl” and 858 \n“ipper”. The SentencePieces algorithm identifies token boundaries using an unsupervised 859 \nstatistical algorithm, and its outputs reflect patterns of corpus frequency rather than morphology 860 \nor any other linguistic principle (though, in practice, on English text there is often some 861 \nincidental overlap with morphology). For most purposes, these SentencePieces and their contents 862 \nare an implementation detail, encoded and decoded automatically by tools included with the 863 \nlanguage modeling software. However, the detail is relevant to two of our methodological 864 \nchoices. First, due to input and output constraints imposed by the architecture of the baseline 865 \nmodel, each target word was masked with as many [MASK] tokens as corresponded to its 866 \nSentencePiece-encoded length. Relatedly, upon decoding our model’s target word predictions, 867 \nthe model produced as many SentencePieces as there were [MASK] tokens in the input 868 \nsequence. In other words, for our present experimental setup, the model could not produce a 869 \nprediction with too many or too few SentencePieces. Second, for outputs requiring more than 870 \none SentencePiece, we decoded the output using a standard technique known as “beam search” 871 \n(Lowerre, 1976). Given that the number of possible SentencePiece permutations grows 872 \nexponentially with each additional [MASK] token, a beam search allows us to efficiently identify 873 \npossible combinations of SentencePieces by estimating conditional probabilities for only the n 874 \nmost likely tokens at each step in the sequence. We used a limit (“beam width”) of n=20 while 875 \ndecoding our model’s output. 876 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \nSupplemental Material 877 \nSupplemental Table 1 878 \nTwo-sided z-tests for independent proportions for test set stratifications of exact match accuracy 879 \nfor all experiments 880 \nExp Comparison z p \n1. Baseline \nHuman agreement = 100% vs Human agreement < 100% 4.891 <0.001 \nHuman confidence > median vs Human confidence <= \nmedian \n5.692 <0.001 \nWAB-R AQ > median vs WAB-R AQ <= median 4.170 <0.001 \nFluent participants vs Non-fluent participants 2.879 0.004 \n2. Controls \nHuman agreement = 100% vs Human agreement < 100% 8.471 <0.001 \nHuman confidence > median vs Human confidence <= \nmedian \n9.532 <0.001 \nWAB-R AQ > median vs WAB-R AQ <= median 5.795 <0.001 \nFluent participants vs Non-fluent participants 4.746 <0.001 \n3.  \nPWA \nHuman agreement = 100% vs Human agreement < 100% 11.353 <0.001 \nHuman confidence > median vs Human confidence <= \nmedian \n11.121 <0.001 \nWAB-R AQ > median vs WAB-R AQ <= median 4.793 <0.001 \nFluent participants vs Non-fluent participants 2.581 0.010 \n4. Controls \n+ PWA \nHuman agreement = 100% vs Human agreement < 100% 10.336 <0.001 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint \n \n   \n \n Human confidence > median vs Human confidence <= \nmedian \n11.783 <0.001 \nWAB-R AQ > median vs WAB-R AQ <= median 3.335 0.001 \nFluent participants vs Non-fluent participants 2.419 0.016 \nNote. Exp stands for experiment. PWA stands for people with aphasia. WAB-R AQ is the 881 \nWestern Aphasia Battery-Revised Aphasia Quotient. Fluent participants are those with 882 \nWernicke, Anomic, Conduction, or Transcortical Sensory aphasia, or those considered “non 883 \naphasic” by the WAB-R. Non-fluent participants are those with the Broca, Global, or 884 \nTranscortical Motor aphasia. 48 out of 353 total sessions had unavailable WAB-R results and 885 \nwere excluded just from analyses involving WAB-R scores. 886 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 22, 2023. ; https://doi.org/10.1101/2023.06.18.23291555doi: medRxiv preprint ",
  "topic": "Ambiguity",
  "concepts": [
    {
      "name": "Ambiguity",
      "score": 0.7165359854698181
    },
    {
      "name": "Aphasia",
      "score": 0.7057220339775085
    },
    {
      "name": "Computer science",
      "score": 0.595732569694519
    },
    {
      "name": "Identification (biology)",
      "score": 0.5880634784698486
    },
    {
      "name": "Matching (statistics)",
      "score": 0.5610405206680298
    },
    {
      "name": "Context (archaeology)",
      "score": 0.49819183349609375
    },
    {
      "name": "Language disorder",
      "score": 0.4370579719543457
    },
    {
      "name": "Psychology",
      "score": 0.4366626441478729
    },
    {
      "name": "Natural language processing",
      "score": 0.434823602437973
    },
    {
      "name": "Linguistics",
      "score": 0.4249078333377838
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34363529086112976
    },
    {
      "name": "Cognitive psychology",
      "score": 0.25723138451576233
    },
    {
      "name": "History",
      "score": 0.11501771211624146
    },
    {
      "name": "Mathematics",
      "score": 0.10217595100402832
    },
    {
      "name": "Programming language",
      "score": 0.09269589185714722
    },
    {
      "name": "Cognition",
      "score": 0.08929553627967834
    },
    {
      "name": "Neuroscience",
      "score": 0.08881595730781555
    },
    {
      "name": "Biology",
      "score": 0.08726352453231812
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165690674",
      "name": "Oregon Health & Science University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I126345244",
      "name": "Portland State University",
      "country": "US"
    }
  ],
  "cited_by": 2
}