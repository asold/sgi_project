{
    "title": "Improved prediction of MHC-peptide binding using protein language models",
    "url": "https://openalex.org/W4385982832",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2624746825",
            "name": "Nasser Hashemi",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A2965841095",
            "name": "Boran Hao",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A2465817663",
            "name": "Mikhail Ignatov",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A1549817177",
            "name": "Ioannis Ch. Paschalidis",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A664201829",
            "name": "Pirooz Vakili",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A2031281642",
            "name": "Sandor Vajda",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A1966578480",
            "name": "Dima Kozakov",
            "affiliations": [
                "Boston University",
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2624746825",
            "name": "Nasser Hashemi",
            "affiliations": [
                "Boston University",
                "University of Massachusetts Boston"
            ]
        },
        {
            "id": "https://openalex.org/A2965841095",
            "name": "Boran Hao",
            "affiliations": [
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A2465817663",
            "name": "Mikhail Ignatov",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A1549817177",
            "name": "Ioannis Ch. Paschalidis",
            "affiliations": [
                "University of Massachusetts Boston",
                "Boston University"
            ]
        },
        {
            "id": "https://openalex.org/A664201829",
            "name": "Pirooz Vakili",
            "affiliations": [
                "Boston University",
                "University of Massachusetts Boston"
            ]
        },
        {
            "id": "https://openalex.org/A2031281642",
            "name": "Sandor Vajda",
            "affiliations": [
                "Boston University",
                "University of Massachusetts Boston"
            ]
        },
        {
            "id": "https://openalex.org/A1966578480",
            "name": "Dima Kozakov",
            "affiliations": [
                "Stony Brook University",
                "Boston University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2589139221",
        "https://openalex.org/W6761260114",
        "https://openalex.org/W2977541300",
        "https://openalex.org/W3082589298",
        "https://openalex.org/W2748343486",
        "https://openalex.org/W2518214177",
        "https://openalex.org/W2908403765",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W1991461541",
        "https://openalex.org/W2187048772",
        "https://openalex.org/W6786765785",
        "https://openalex.org/W1984541135",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2051294170",
        "https://openalex.org/W3188448537",
        "https://openalex.org/W6780145172",
        "https://openalex.org/W4284885089",
        "https://openalex.org/W6804297011",
        "https://openalex.org/W6801150430",
        "https://openalex.org/W3113995795",
        "https://openalex.org/W6840624247",
        "https://openalex.org/W3205598784",
        "https://openalex.org/W3164264961",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2793062918",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3195593976",
        "https://openalex.org/W6840065848",
        "https://openalex.org/W2901513648",
        "https://openalex.org/W6683033130",
        "https://openalex.org/W3190997979",
        "https://openalex.org/W2125732073",
        "https://openalex.org/W3042910002",
        "https://openalex.org/W3115778640",
        "https://openalex.org/W2166831581",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W3133458480",
        "https://openalex.org/W3111174583",
        "https://openalex.org/W3024570138",
        "https://openalex.org/W6762363781",
        "https://openalex.org/W2741742659",
        "https://openalex.org/W3043768357",
        "https://openalex.org/W4230239510",
        "https://openalex.org/W1996567630",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2898389621",
        "https://openalex.org/W2794284562",
        "https://openalex.org/W2959124424",
        "https://openalex.org/W2057771742",
        "https://openalex.org/W3188579603",
        "https://openalex.org/W4287724045",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3108185661",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W2943495267",
        "https://openalex.org/W4306903922",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2154318594",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W3197480237",
        "https://openalex.org/W3166730506",
        "https://openalex.org/W2892181857",
        "https://openalex.org/W2971227267"
    ],
    "abstract": "Major histocompatibility complex Class I (MHC-I) molecules bind to peptides derived from intracellular antigens and present them on the surface of cells, allowing the immune system (T cells) to detect them. Elucidating the process of this presentation is essential for regulation and potential manipulation of the cellular immune system. Predicting whether a given peptide binds to an MHC molecule is an important step in the above process and has motivated the introduction of many computational approaches to address this problem. NetMHCPan, a pan-specific model for predicting binding of peptides to any MHC molecule, is one of the most widely used methods which focuses on solving this binary classification problem using shallow neural networks. The recent successful results of Deep Learning (DL) methods, especially Natural Language Processing (NLP-based) pretrained models in various applications, including protein structure determination, motivated us to explore their use in this problem. Specifically, we consider the application of deep learning models pretrained on large datasets of protein sequences to predict MHC Class I-peptide binding. Using the standard performance metrics in this area, and the same training and test sets, we show that our models outperform NetMHCpan4.1, currently considered as the-state-of-the-art.",
    "full_text": "Improved prediction of\nMHC-peptide binding using\nprotein language models\nNasser Hashemi1*† , Boran Hao2† , Mikhail Ignatov3,4,\nIoannis Ch. Paschalidis1,2,5, Pirooz Vakili1, Sandor Vajda1,5,6 and\nDima Kozakov3,4,5*\n1Division of Systems Engineering, Boston University, Boston, MA, United States,2Department of Electrical\nand Computer Engineering, Boston University, Boston, MA, United States,3Department of Applied\nMathematics and Statistics, Stony Brook University, Stony Brook, NY, United States,4Laufer Center for\nPhysical and Quantitative Biology, Stony Brook University, Stony Brook, NY, United States,5Department of\nBiomedical Engineering, Boston University, Boston, MA, United States,6Department of Chemistry, Boston\nUniversity, Boston, MA, United States\nMajor histocompatibility complex Class I (MHC-I) molecules bind to peptides\nderived from intracellular antigens and present them on the surface of cells,\nallowing the immune system (T cells) to detect them. Elucidating the process of\nthis presentation is essential for regulation and potential manipulation of the\ncellular immune system. Predicting whether a given peptide binds to an MHC\nmolecule is an important step in the above process and has motivated the\nintroduction of many computational approaches to address this problem.\nNetMHCPan, a pan-speciﬁc model for predicting binding of peptides to any\nMHC molecule, is one of the most widely used methods which focuses on\nsolving this binary classiﬁcation problem using shallow neural networks. The\nrecent successful results of Deep Learning (DL) methods, especially Natural\nLanguage Processing (NLP-based) pretrained models in various applications,\nincluding protein structure determination, motivated us to explore their use in\nthis problem. Speciﬁcally, we consider the application of deep learning models\npretrained on large datasets of protein sequences to predict MHC Class I-peptide\nbinding. Using the standard performance metrics in this area, and the same\ntraining and test sets, we show that our models outperform NetMHCpan4.1,\ncurrently considered as the-state-of-the-art.\nKEYWORDS\nMHC class I, deep learning, transformers, natural language processing, cellular immune\nsystem\n1 Introduction\nMajor Histocompatibility Complex molecules (MHC) are large cell surface proteins that\nplay a key role in immune response by detecting and responding to foreign proteins and\nantigens. An MHC molecule detects and binds to a peptide (a small fragment of a protein\nderived from an antigen), creating a peptide-MHC complex, and presents it to the surface of\nthe cell; then, based on the interactions between this complex and the T cell receptor at the\ncell surface, an immune response is triggered to control the compromised cell (Maimela\net al., 2019; Janeway et al., 2001; Teraguchi et al., 2020; Ong et al., 2021). MHC molecules are\nclassiﬁed into two classes: (i) MHC Class I which controls non-self intracellular antigens by\npresenting antigenic peptides (of 8– 14 sequence length) to cytotoxic T cell lymphocytes\nOPEN ACCESS\nEDITED BY\nIgor N. Berezovsky,\nBioinformatics Institute (ApSTAR),\nSingapore\nREVIEWED BY\nJian Zhang,\nXinyang Normal University, China\nGiuseppe Maccari,\nThe Pirbright Institute, United Kingdom\n*CORRESPONDENCE\nNasser Hashemi,\nnhashemi@bu.edu\nDima Kozakov,\nmidas@laufercenter.org\n†These authors have contributed equally\nto this work\nRECEIVED 17 April 2023\nACCEPTED 04 August 2023\nPUBLISHED 17 August 2023\nCITATION\nHashemi N, Hao B, Ignatov M,\nPaschalidis IC, Vakili P, Vajda S and\nKozakov D (2023), Improved prediction of\nMHC-peptide binding using protein\nlanguage models.\nFront. Bioinform. 3:1207380.\ndoi: 10.3389/fbinf.2023.1207380\nCOPYRIGHT\n© 2023 Hashemi, Hao, Ignatov,\nPaschalidis, Vakili, Vajda and Kozakov.\nThis is an open-access article distributed\nunder the terms of theCreative\nCommons Attribution License (CC BY).\nThe use, distribution or reproduction in\nother forums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nFrontiers in Bioinformatics frontiersin.org01\nTYPE Original Research\nPUBLISHED 17 August 2023\nDOI 10.3389/fbinf.2023.1207380\n(CD8+ TCR) and (ii) MHC Class II, which controls extracellular\nantigens by presenting antigenic peptides (of 13 – 25 sequence\nlength) to helper T cell lymphocytes (CD4+ TCR). One of the\nmain steps in studying the role of the MHC molecules in the\nimmune system is developing insights into the interactions of the\nMHC molecules and non-self pathogen peptides, referred to as\nMHC-peptide binding ( Reynisson et al., 2020 ). MHC-peptide\nbinding prediction plays an important role in vaccine design and\nstudies of infectious diseases, autoimmunity, and cancer therapy\n(O’Donnell et al., 2020; Grebenkin et al., 2020).\nThere are two basic experimental methods to study MHC-\npeptide binding: (i) Peptide-MHC binding afﬁnity (BA) assays in\nwhich, given a peptide, binding preferences of different MHC\nmolecules to the peptide are measured (Townsend et al., 1990);\n(ii) MHC associated eluted ligands (EL) generated by Liquid\nChromatography Mass Spectrometry (LC-MS) in which, based\non a single experiment, a large number of eluted ligands\ncorresponding to an MHC are identi ﬁed (Caron et al., 2015 ).\nCompared to the BA method, the EL method is highly accurate\nand thorough and it is a reliable way to determine the peptides\nincluded in the immunopeptidome (namely, the entire set of\npeptides forming MHC-peptides complexes ( Alvarez et al.,\n2019)). Both methods, however, are labor-intensive and time-\nconsuming. As a result, a number of computational methods\nhave been developed to predict MHC-peptide binding ( Boehm\net al., 2019). These methods include heuristic approaches using\nMHC allele– speciﬁc motifs to identify potential ligands in a protein\nsequence (Bui et al., 2005), supervised machine learning approaches,\nincluding artiﬁcial neural networks (ANN) (Nielsen et al., 2003),\nhidden Markov models (HMM) (Zhang et al., 2006), and regression\nmodels (Parker et al., 1994; Doytchinova and Flower, 2001). The\nperformance of these machine learning methods increases with the\namount of data available in epitope databases such as SysteMHC\n(Shao et al., 2018) and Immune Epitope Database (IEDB) (Vita et al.,\n2019). While some of these methods are trained for only one speciﬁc\nMHC allele (known as allele-speciﬁc methods), there are more\ngeneralized models (pan-speciﬁc methods) where a single model\ncovers all of alleles of interest. The methods are also categorized by\nthe type of predicted variables. Among these methods, some have\nbeen shown to be more promising, such as NetMHCpan (Reynisson\net al., 2020), DeepLigand (Zeng and Gifford, 2019), and MHCﬂurry\n(O’Donnell et al., 2020; Aranha et al., 2020). The most recent version\nof NetMHCpan (NetMHCpan 4.1) is currently considered as the\nstate-of-the-art in the MHC Class I-peptide binding prediction\nproblem (Reynisson et al., 2020).\nNetMHCpan is a pan-speciﬁc model which predicts binding of\npeptides to any MHC molecule of known sequence using artiﬁcial\nneural networks. Since 2003, this model has gradually been\nimproved and its last version for MHC Class I (NetMHCpan 4.1)\nhas been introduced in 2020. This model is trained on a combination\nof the BA and EL peptide datasets where the inputs are sequences\nassociated with MHC-peptide complexes (Tong, 2013). There are\nsome speciﬁc features associated with this method that helps it to\noutperform other approaches: (i) instead of using the complete\nsequence of MHC molecules as input, NetMHCpan uses pseudo-\nsequences of MHC molecules with aﬁxed length (34 amino acids);\nthese pseudo-sequences include those amino acids associated with\nthe binding sites of the MHC molecule inferred from a priori\nknowledge; (ii) to accommodate peptides of different lengths\n(8– 15 in MHC Class I), the length isﬁxed to a uniform length of\n9 via insertion and deletion of amino acids; (iii) additional features\nwith speciﬁcity information of the peptides are used during the\ninsertion and deletion steps; for example, the original length of the\npeptide is encoded as a categorical variable and the length of the\nsequence that was inserted/deleted is added as a different feature;\n(iv) NetMHCpan consists of several shallow neural networks and it\nimplements the ensemble technique: using cross-validation, the\ntraining dataset is split into 5 parts and the model is trainedﬁve\ntimes, one for each split. Also, NetMHCpan uses shallow neural\nnetworks with one hidden layer containing 56 or 66 neurons that are\ntrained using 10 different random initial weight conﬁgurations; thus,\nthe ensemble NetMHCpan contains 100 different models.\nAs indicated above, the most recent NetMHCpan approach\n[version 4.1 (Reynisson et al., 2020)] is based on shallow neural\nnetworks. In recent years, a number of more complex yet efﬁcient\nmethods such as deep neural networks have shown promising\nresults in a number of ﬁelds (Deng et al., 2013; LeCun et al.,\n2015; Khan and Yairi, 2018; Voulodimos et al., 2018; Iuchi et al.,\n2021; Mohammadzadeh and Lejeune, 2021 ). For example,\ntransformer models (Vaswani et al., 2017), a recent breakthrough\nin natural language processing, have shown that large models\ntrained on unlabeled data are able to learn powerful\nrepresentations of natural languages and can lead to signiﬁcant\nimprovements in many language modeling tasks (Devlin et al., 2018;\nHu et al., 2022). Furthermore, it has been shown that collections of\nprotein sequences can be treated as sentences so that similar\ntechniques can be used to extract useful biological information\nfrom protein sequence databases (Rao et al., 2019; Rives et al.,\n2019). A highly successful example of this approach has been\nDeepMind’s recent protein-folding method, using attention-based\nmodels (Jumper et al., 2020; Lensink et al., 2021; Egbert et al., 2021;\nGhani et al., 2021). Currently, there are a number of publicly\navailable pre-trained models which have been shown to be\nhelpful in a variety of downstream protein related tasks ( Rao\net al., 2019; Rives et al., 2019; Elnaggar et al., 2020; Rao et al.,\n2020; Rao et al., 2021).\nIn the work reported in this paper, we consider using a number\nof such pre-trained models and Deep Learning (DL) methods to\naddress the MHC Class I peptide binding prediction problem. One\ncomponent of the approach in this work is based on transfer\nlearning. In Deep Learning (DL), transfer learning is a method in\nwhich a DL model is ﬁrst trained on a problem similar to the\nproblem of interest; then, a portion or the whole of this pre-trained\nmodel is used for training the model of the desired problem. This\napproach is particularly advantageous when the amount of data for\nthe problem of interest is limited, however, large databases\nassociated with other problems with some similarity to the\nproblem of interest exist (Fu and Bates, 2022\n). Fine-tuning a pre-\ntrained model using the dataset associated with the problem of\ninterest is one of the approaches in transfer learning and one that is\nused in this work. In this case, a portion, or all of the weights\nassociated with the pre-trained model are used as the initial weights\nof a new DL model for the desired task. For example, in NLP, BERT\n(Bidirectional Encoder Representations from Transformers) is a\npre-trained transformer model which is trained on a large corpus\nof unlabelled text including the entire Wikipedia (about\nFrontiers in Bioinformatics frontiersin.org02\nHashemi et al. 10.3389/fbinf.2023.1207380\n2,500 million words) and the Book Corpus (800 million words)\n(Devlin et al., 2018). Thereafter, the pre-trained model has been used\nfor a number of NLP tasks such as text classiﬁcation, text annotation,\nquestion answering, and language inference, to name a few.\nRecently, following the successful results of pre-trained\ntransformer models such as BERT and their transfer learning\nderivatives in NLP applications, similar approaches have been\nattempted in the proteinﬁeld thanks to the substantial growth in\nthe number of protein sequences. As a result, there are a number of\npre-trained self-supervised BERT-like models applied to protein\ndata in the form of unlabeled amino acid sequences which can be\nvery useful for many protein task-speciﬁc problems using transfer\nlearning (Elnaggar et al., 2020; Rao et al., 2020). Two recent works\nhave considered using protein language models in the MHC-peptide\nbinding problem. BERTMHC (Cheng et al., 2020) explores whether\npre-trained protein sequence models can be helpful for MHC Class\nII-peptide binding prediction by focusing on algorithms that predict\nthe likelihood of presentation of a peptide given a set of MHC Class\nII molecules. They show that models generated from transfer\nlearning can achieve better performance on both binding and\npresentation prediction tasks compared to NetMHCIIpan4.0 (last\nversion of NetMHCpan in MHC Class II (Reynisson et al., 2020)).\nAnother BERT-based model known as ImmunoBERT (Gasser et al.,\n2021) applies pre-trained transformer models in the MHC Class\nI-peptide binding problem. As reported, in this work they were not\nable to compare their model fairly with NetMHCPan (Reynisson\net al., 2020) and MHCﬂurry (O’Donnell et al., 2020) performance\ndue to a lack of access to the same training set. BERTMHC and\nImmunoBERT both use the TAPE pre-trained models (Rao et al.,\n2019) which were trained on 31 million protein sequences, whereas\nnow there are larger and more informative pre-trained models\navailable such as ESM (Rao et al., 2020; Lin et al., 2022) and\nProtTrans (Elnaggar et al., 2020) which are trained on more than\n250 million protein sequences.\nIn the work reported in this paper, we focus on MHC Class\nI-peptide binding prediction and develop different approaches using\nthe larger pre-trained protein language models. Two of these\napproaches are based on ﬁne-tuning using a soft-max layer in\none and a Graph Attention Network (GAT) in the other. Our\nthird approach is based on a domain adaptation method to\nfurther pre-train the protein language models and enhance the\nﬁne-tuning performance. We evaluate the performance of our\nmodels using the standard metrics of the ﬁeld and the same\ntraining and test sets as those of NetMHCpan 4.1. We show that\nour methods outperform NetMHCpan 4.1 over these test sets.\n2 Materials and methods\n2.1 Methods\nIn this work, we considered two large protein language pre-\ntrained models, ESM1b (Rao et al., 2020) and ESM2 (Lin et al.,\n2022), two BERT-based models which are trained on hundreds of\nmillions of protein sequences. ESM1b is a pre-trained Transformer\nprotein language model from Facebook AI Research which has been\nshown to outperform all tested single-sequence protein language\nmodels across a range of protein structure prediction tasks (Rao\net al., 2020 ); its successor, ESM2, has achieved even better\nperformance on protein folding related tasks. ESM1b and ESM2-\n650M have 33 layers with 650 million parameters and an embedding\ndimension of 1280, and the largest model we used, ESM2-3B, has\n36 layers, embedding dimension of 2560 and 3 billion parameters. In\nour ﬁne-tuning approaches, after including an additional layer at the\nend of the ESM models, we re-trained the entire set of parameters of\nESM1b and ESM2 and trained the parameters of the added layer\nusing the MHC-peptide dataset. Thus, the entire parameters,\nincluding the pre-trained weights of the model, were updated\nbased on our dataset (Figure 1).\n2.1.1 ESMﬁne-tuning\nSince ESM models can be regarded as transformer-based\nbidirectional language models (bi-LM), we borrowed an idea\nfrom a basic NLP task called Natural Language Inference (NLI)\n(Bowman et al., 2015) to perform MHC-peptide binding prediction.\nOne of the NLI tasks is the sequence-pair classiﬁcation problem,\nnamely, predicting whether a text A (e.g.,“rabbits are herbivorous”)\ncan imply the semantics in a text B (e.g.,“rabbits do not eat rats”).\nSimilarly, in the MHC-peptide case, we would like to know whether\na given peptide sequence (same as text A) binds to a given MHC\nsequence (same as text B), suggesting that applying an NLI-based\nmodel could be effective in MHC-peptide binding prediction. A\ncommon transformer-based NLI model combines text A and B into\none sequence “[BOS] seq-A [SEP] seq-B [EOS]” as input, where\n[BOS], [SEP] and [EOS] are special tokens\n1 in bi-LM vocabulary.\nSuppose the amino acids in the MHC and the peptide sequences\nare M1, ... , Mp and P1, ... , Pq, respectively. We generate the\nsequence “[BOS], M1, ... , Mp, [SEP], P1, ... , Pq, [EOS]” with\nlength p + q + 3 as the ESM model input, and obtain the same\nsize embedding vectors vBOS, vM1, ... , vMp, vSEP, vP1, ... , vPq, vEOS\nfrom the last layer of ESM models, corresponding to the special\ntokens and the amino acids in the MHC and the peptide. As a\ncommon strategy in NLP sequence classiﬁcation tasks, we use the\nembedding of [BOS] to be the MHC-peptide sequence-pair\nembedding vector /C22v (Ibtehaz and Kihara, 2023). Finally, passing\n/C22v through a softmax classiﬁer layer, we output the probability of\nbinding and use it to compute the loss and apply back-propagation.\nCompared to embedding the MHC and the peptide separately, this\ncompound input allows the transformer to use the attention\nmechanism to further extract the interactive information between\nthe amino acids in the MHC and the peptide, thus, helping the\nbinding prediction.\nAlthough ESM models are well pre-trained in an unsupervised\nmanner, using a large number of universal sequences, we know that\nMHCs are highly speci ﬁc types of protein sequences, so the\nembedding from the pre-trained ESM models may not be\noptimal for the speciﬁc MHC task and input format. Therefore,\nwe not only need to train theﬁnal softmax classiﬁer but need to train\nthe ESM model parameters as well to improve the sequence-pair\nembedding. We applied aﬁne-tuning which is commonly used in\nNLP. Initialized from the pre-trained ESM model parameters, we\n1 A token is a string of contiguous characters between two spaces, or\nbetween a space and punctuation marks.\nFrontiers in Bioinformatics frontiersin.org03\nHashemi et al. 10.3389/fbinf.2023.1207380\nupdated the parameters in the whole network using a small learning\nrate during the back-propagation, so that valuable information in\nthe pre-trained ESM models is maintained while theﬁne-tuned ESM\nmodels provided a more informative embedding speciﬁc to the\nMHC tasks.\n2.1.2 ESM domain adaptation\nIn NLP, domain adaptation pre-training is an important tool\nto introduce domain-speciﬁc information into a bi-LM. A BERT\nmodel pre-trained on general corpora (Devlin et al., 2018)( e . g . ,\nWikipedia) can be further pre-trained by the same masked\nFIGURE 1\nOur ﬁne-tuning architecture using NLP-based pre-trained models.\nTABLE 1 Distribution of training set used in NetMHCpan 4.1 (Reynisson et al., 2020); Columns correspond to each type of training data, for which the number of\npositive and negative samples, and the total amount of unique MHCs are shown. A threshold of 500 nM is used to deﬁne positive BA data points.\nBinding afﬁnity EL (single allele) EL (multi allele)\nPositives Negatives MHCs Positives Negatives MHCs Positives Negatives MHCs\n52,402 155,691 170 218,962 3,813,877 142 446,530 8,395,021 112\nFIGURE 2\nPPV Comparison of ESM2 model vs NetMHCpan 4.1 over different hit-decoy ratio.\nFrontiers in Bioinformatics frontiersin.org04\nHashemi et al. 10.3389/fbinf.2023.1207380\nlanguage modeling (MLM) met hods but using corpora from\nspeciﬁc domains such as clinical medicine ( Alsentzer et al.,\n2019) in order to gain better down- stream task performance\nin different knowledge domains. This idea ﬁts our protein\nlanguage models as well because ESM models were pre-trained\non general full protein sequences whereas our MHC-peptide\nbinding prediction focuses on MHC pseudo-sequences and\nshort peptides, which were not available in the ESM pre-\ntraining data. Therefore, we applied domain adaptation to the\nE S Mm o d e l si no r d e rt oo f f e rt h eE S Mm o d e l sm o r ek n o w l e d g e\nabout the MHC pseudo-sequences and the peptides.\nWe still use the NetMHCpan V4.1 training set as our domain\nadaptation pre-training set. For an MHC-peptide pair“[BOS], M\n1,\n... , Mp, [SEP],P1, ... , Pq, [EOS]”,w eﬁrst randomly mask 7 amino\nacids (around 15%), and then feed this masked sequence pair to the\npre-trained ESM models. Note that with a probability of 0.8, an\namino acid to be masked will be masked by a special token [MASK],\notherwise it will be “masked” by the original amino acid, which\nresembles the MLM setting in BERT. The ESM models will then\nexploit the information from the visible context of amino acids, and\nﬁnally use a classiﬁcation head to predict the masked amino acids.\nAs a result, the special structural characteristics of the MHC pseudo-\nsequences and the peptides will be further learned, and our domain-\nadapted ESM models can better ﬁt the MHC-related tasks,\ncompared with the vanilla ESM models. For one MHC-peptide\npair, the loss to be minimized is the mean cross-entropy loss between\nthe predicted and the ground truth masked amino acids. During the\nESM domain adaptation pre-training, we still update all parameters\nof the ESM models, and our domain-adapted ESM models will be\nused as the initialization of the MHC-peptide binding prediction\nﬁne-tuning task described in the previous section.\n2.1.3 ESM-GATﬁne-tuning\nHere, we consider our second approach to ﬁne-tuning.\nMolecular structure-based biological data such as proteins, can be\nmodeled with graph structures in which amino-acids or atoms are\nconsidered as nodes, and contacts or bonds are considered as edges.\nIt has been shown that Graph Neural Networks (GNNs), as a branch\nof deep learning in non-Euclidean spaces, perform well in various\napplications in bioinformatics (Zhang et al., 2021). In our context,\nthe interaction between an MHC and a peptide can be described by a\ngraph in which the amino-acids are considered as the nodes and the\ninteraction between them as edges. To model such a graph\ninformation, we added a variant model of GNN known as Graph\nAttention Network (GAT) as the last layer of the ESM network. GAT\nis a novel neural network architecture that operates on graph-\nstructured data by leveraging attention layers to address the\nshortcomings of prior methods based on graph convolutions or\ntheir approximations (Veličković et al., 2017). For each MHC-\npeptide pair, we used a directed graph G, where the nodes N\n1,\n... , Np+q+3 represent thep + q + 3 amino acids and the special tokens\nas described above, and an edge (Ni, Nj) indicates that amino acidsi\nand j are in contact with each other. Denote the neighbor set of\namino acid i as A(i)/equals{ j: (Ni,N j) ∈ G}; then, each embedding\nvector vi is updated as a weighted average of its transformed\nneighbor embedding vectors:\nvi′ /equals ∑\nj∈Ai()\nαijWvj,\nwhere W is a weight matrix for vector transformation, and the\nweight αij is computed using an attention mechanism. Supposezij is\nthe concatenation of vectors Wvi and Wvj and c is a parameter\nvector, then the weightαij is given by:\nαij /equals\nexp σ 〈c, zij〉()()\n∑k∈Ai() exp σ 〈c, zik〉()() ,\nwhere σ is an activation function. Note that the attention mechanism\nhere is known asadditive attention, which is different from the dot-\nproduct attention mechanism used in ESM and other transformer-\nbased models.\nAfter each GAT layer, we update the embedding vector for\nthe amino acids and the special tokens as v\nBOS′ , vM1′ , ... , vMp′ ,\nTABLE 2 Summary table of comparison of the mean of our models and NetMHCpan (V4.1) AUC-ROC and PPV over all test sets.\nModels PPV (hit-decoy ratio: 1:19) PPV (hit-decoy ratio: 1:99) AUC-ROC\nNetMHCpan4_ 1 0.791 0.671 0.950\nESM1b 0.834 0.737 0.977\nESM2_650M 0.837 0.742 0.976\nESM2_3B 0.844 0.753 0.976\nESM1b_domain 0.851 0.756 0.979\nESM2_650M_domain 0.850 0.756 0.980\nESM2_3B_domain 0.857 0.769 0.981\nTABLE 3 Summary table of comparison of the mean of our models and\nNetMHCpan (V4.1) F1, AUC-PR, and MMC over all test sets.\nModels Fl AUC -PR MMC\nNetMHCpan4_l 0.711 0.833 0.726\nESM1b 0.771 0.885 0.779\nESM2 650M 0.785 0.888 0.791\nESM2 3B 0.788 0.893 0.794\nESM1b_domain 0.794 0.902 0.799\nESM2_650M_domain 0.796 0.900 0.801\nESM2 3B domain 0.801 0.908 0.806\nFrontiers in Bioinformatics frontiersin.org05\nHashemi et al. 10.3389/fbinf.2023.1207380\nvSEP′ , vP1′ , ... , vPq′ , vEOS′ , and more GAT layers follow. Here, in our\nimplementation, we use two fully connected GAT layers. Same as\nvanilla transformer model (Vaswani et al., 2017), we apply multi-\nhead attention mechanism in which for each GAT layer, we split the\nparameters and pass each split independently through a separate\nhead. Particularly, in theﬁrst GAT layer we use 8 attention heads\nwhich are then concatenated together and passed to the next layer\nwhile in theﬁnal GAT layer we average the heads of a certain token.\nWe ﬁnally use the embedding vector of [BOS] in theﬁnal GAT layer\nas the MHC-peptide sequence pair embedding vector to determine\nthe binding prediction. Theﬁnal GAT layer was meant to use the\nattention mechanism to aggregate all the node information into\n[BOS] position by letting [BOS] token contact with all the amino\nacids in the graphs which makes the [BOS] embedding potentially a\nmore powerful sequence embedding than simply using the average\nof the embedding vectors output by theﬁrst GAT layer. Compared\nto using only ESM dot-product attention layers and a linear\nclassiﬁcation head, now we are adding more GAT additive\nattention layers to dynamically reﬁne the ESM embedding and\nenhance the ﬁnal binding classiﬁcation.\nNote that the contact information can be deﬁned differently\nthrough graphs. If in the absence of speciﬁc information about\nthe contacts, fully-connected graphs are used as we did, the\ndependency among any amino acids can be further exploited\nby those additive attention lay ers, similar to the ESM layers.\nHowever, if prior information on contacts is available and is\nrepresented in the graphs, such information can also be\nintroduced to the GAT laye rs by allowing the additive\nattention mechanism to happe n only between the desired\namino acids.\nFIGURE 3\nPPV Comparison (hit-decoy ratio: 1:19) of ourﬁne-tuning method with the latest NetMHCpan server (Version 4.1) over the same training and test\nsets (Reynisson et al., 2020). (A) Bar plots associated with each test set.(B) Scatter plot: each point is the PPV of each group of test set.\nFrontiers in Bioinformatics frontiersin.org06\nHashemi et al. 10.3389/fbinf.2023.1207380\n2.2 Dataset\n2.2.1 Training set\nWe used the training set used by the last version of NetMHCpan\n(Reynisson et al., 2020), including 13 millions binary labeled MHC-\npeptide binding samples, generated from two main data sources: (i)\nthe BA peptides derived from in-vitro Peptide-MHC binding assays,\nand (ii) the EL peptides derived from mass spectrometry\nexperiments. However, it has been shown that the results from\nthe mass spectrometry EL experiments are mostly poly-speciﬁc,\ni.e., they contain ligands matching multiple binding motifs (Alvarez\net al., 2019). That being said, for most of the samples in the EL\ndataset, each peptide is associated with multiple alleles (from 2 to\n6 alleles for each peptide). Thus, in this training set, the EL dataset is\ncomposed of two subsets: (i): Single-Allele (SA) peptides assigned to\nsingle MHCs and (ii) Multi-Allele (MA) peptides with multiple\nMHC options to be assigned.Table 1shows the distribution of the\naforementioned dataset which indicates that more than 67% of the\ndataset is associated with EL-MA. According to (Alvarez et al.,\n2019), the existence of the MA dataset introduces some challenges in\nterms of data analysis and interpretation; therefore, to train a binary\nMHC-peptide predictor, a process, known as deconvoluting the MA\nbinding motifs, is needed to convert these EL-MA data to a single\npeptide-MHC pair (Reynisson et al., 2020).\n2.2.2 Deconvolution of multi allelic (MA) data\nTo deconvolute the EL-MA dataset, several computational\napproaches have been used based on unsupervised sequence\nclustering (Bassani-Sternberg and Gfeller, 2016; Bassani-Sternberg\net al., 2017). Although these methods show some progress in dealing\nwith the MA dataset, they have some shortcomings; for example,\nthey do not work in cell lines including MHC alleles with similar\nFIGURE 4\nPPV Comparison (hit-decoy ratio: 1:99) of ourﬁne-tuning method with the latest NetMHCpan server (Version 4.1) over the same training and test\nsets (Reynisson et al., 2020). (A) Bar plots associated with each test set.(B) Scatter plot: each point is the PPV of each group of test set.\nFrontiers in Bioinformatics frontiersin.org07\nHashemi et al. 10.3389/fbinf.2023.1207380\nbinding motifs. Therefore, in the new version of NetMHCPan\n(Version 4.1), they present a new framework, NNAlign-MA\n(Alvarez et al., 2019 ), which works better than the previous\napproaches. NNAlign-MA is a neural network framework, which\nis able to deconvolute the MA dataset during the training of the\nMHC-peptide binding predictor. Recently (Cheng et al., 2020),\nattempted to solve this problem in MHC Class II by using a\nmultiple instance learning (MIL) framework. MIL is a supervised\nmachine learning approach, where the task is to learn from data\nincluding positive and negative bags of instances. Each bag may\ncontain many instances and a bag is labeled positive if at least one\ninstance in it is positive (Maron and Lozano-Pérez, 1998). Assume\nthe i-th bag includes m alleles asA\ni ={ ai1, ai2, ... , aim} which is\nassociated with peptide sequencesi. At each training epoch, for each\ninstance in thei-th bag,xij =( aij, si), the probability of whether that\ninstance is positive,p(yij =1 |xij)i sd eﬁned as^yij /equals fθ(aij,s i) where\nfθ is the neural network model; in ( Cheng et al., 2020 ) max\npooling is used as a symmetric pooling operator to calculate the\nprediction of the bag from the predictions of instances within it.\nHere, in our work, we follow this MIL idea to deal with the EL-MA\ndataset.\n2.2.3 Test set\nIn order to have a fair comparison of our model and\nNetMHCPan 4.1, we used the same test set as provided in their\nwork (available in the Supplementary Section). This dataset is\nassociated with a collection of 36 EL-SA datasets, downloaded\nfrom (Abelin et al., 2017). Each dataset is well enriched, length-\nwise, with a number of negative decoy peptides equal to 5 times the\nnumber of ligands of the most abundant peptide length.\nFIGURE 5\nPPV Comparison (hit-decoy ratio: 1:19) of our domain-adaptation method with the latest NetMHCpan server (Version 4.1) over the same training and\ntest sets (Reynisson et al., 2020). (A) Bar plots associated with each test set.(B) Scatter plot: each point is the PPV of each group of test set.\nFrontiers in Bioinformatics frontiersin.org08\nHashemi et al. 10.3389/fbinf.2023.1207380\n2.3 Metric\nPredicting the binding af ﬁnity of MHC with a peptide is a\nbinary classi ﬁcation problem. Typical metrics for assessing the\nquality of binary classi ﬁcation models for a given task include\nprecision, accuracy, recall, receiver operating characteristic curve\n(ROC) and the corresponding Area Under the Curve (AUC). In\nthis work, we use AUC-ROC and a speci ﬁc precision metric\nknown as positive predictive value (PPV); AUC and PPV have\nbeen used as the main metrics in previous works in MHC-peptide\nbinding prediction ( Reynisson et al., 2020 ; O’Donnell et al.,\n2020). AUC is an evaluation metric for binary classi ﬁcation\nproblems which measures the area under the ROC curve.\nA U Cr a n g e si nv a l u ef r o m0t o1a n dm o d e l sw i t hh i g h e r\nAUC perform better at distinguishing between the positive\nand negative classes. PPV is another metric which speci ﬁcally\nis deﬁned in this area and is interpretable as a model’s ability to\nrank positive samples far abov e the negative samples. PPV is\ndeﬁned as fraction of true positive samples (hits) among the top-\nscoring\n1\nN+1 fraction of samples, assuming that the ratio of the\nnumber of positive samples to negatives (decoys) is 1: N (known\nas hit-decoy ratio). Since NetMHCpan (Reynisson et al., 2020)\nuses hit-ratio 1:19 and MHCﬂurry (O’Donnell et al., 2020)u s e s\nhit-ratio 1:99, here in this work, we use both.\nBeyond AUC-ROC and PPV, we also consider three more\nmetrics: F1 score, Precision-Recall Area Under Curve (AUC-PR),\nand Matthews Correlation Coef ﬁcient (MMC). These metrics\nprovide a comprehensive evaluation of the model’s performance\nby measuring the balance between precision and recall, and\nsummarizing performance on imbalanced datasets.\nFIGURE 6\nPPV Comparison (hit-decoy ratio: 1:99) of our domain-adaptation method with the latest NetMHCpan server (Version 4.1) over the same training\nand test sets (Reynisson et al., 2020). (A) Bar plots associated with each test set.(B) Scatter plot: each point is the PPV of each group of test set.\nFrontiers in Bioinformatics frontiersin.org09\nHashemi et al. 10.3389/fbinf.2023.1207380\n3 Results\nI no r d e rt oe v a l u a t ea n dc o m p a r et h ep e r f o r m a n c eo fo u r\napproaches with the state-of-the-art method, we used the latest\nversion of NetMHCpan server (Version 4.1); as mentioned above,\nthe same training and test sets from (Reynisson et al., 2020)w e r eu s e di n\nthis study. The list of independent EL SA test set including the MHC\nmolecules, the number of peptides and the distribution of positives and\nnegatives for each case is provided in theSupplementary Material.\nTo arrive at the hit-decoy ratios of 1:19 and 1:99 for each case, we\nfollowed a random sampling approach that was repeated 1000 times.\nAs a result, for each MHC molecule, the PPV values are sample\naverages of 1000 values. Additionally, in Figure 2 we provide a\ncomparison over a range of hit-decoy ratios.\nTo present the results of the comparison of ourﬁne-tuning as\nwell as our domain adaptation approaches with NetMHCpan, we\nprovide twoﬁgures for each hit-decoy ratio in what follows: (a) a bar\nplot that provides a comparison of PPVs of our approach and\nNetMHCpan for each MHC molecule in the test set, and (b) a scatter\nplot of the same PPV values that provides a better visual summary of\nperformance comparison.\nSince there was not a signiﬁcant difference in performance when\nusing the ESM1b, ESM2-650M, or ESM2-3B, we report the ESM2-\n3B values in this section which were slightly better in mean\nperformance than others. Tables 2, 3 below show the summary\nof the results forﬁne-tuning and domain adaptation which provides\nthe mean of using PPV, AUC-ROC, F1, AUC-PR, and MMC\naverages over all MHC molecules in the test set.\n3.1 ESM ﬁne-tuning\nAs seen in Figure 3 our ﬁne-tuning method outperforms\nNetMHCpan over all hit-decoy ratios in the 35 different test sets;\nonly for HL-B18:01, at ratio 1:19, NetMHCpan performs slightly\nbetter. Also, as seen inFigure 4, at ratio 1:99 the model outperforms\nNetMHCpan for all 36 test set including the HL-B18:01.\n3.2 ESM domain adaptation\nFigures 5 , 6 show that our domain adaptation model\noutperforms NetMHCpan over all hit-decoy ratios in the\n35 different test sets; only for HL-B18:01, at ratio 1:19,\nNetMHCpan slightly performs better. In addition, the\nperformance of the domain adaptation approach is slightly better\nthan the ﬁne-tuning approach.\n3.3 ESM-GAT ﬁne-tuning\nGiven the superior performance of ESMﬁne-tuning in comparison\nwith NetMHCpan, to assess the performance of ESM-GATﬁne-tuning,\nwe compared its performance with that of ESMﬁne-tuning. In this case,\na hit-decoy ratios of 1:19 was considered. We found that in the case\nwhere we subdivided the trainingand test sets between peptides of\nl e n g t h8a n d9o nt h eo n eh a n da n dp e p t i d e so fs i z e1 0– 15 on the other,\nESM-GATﬁne-tuning outperformed ESMﬁne-tuning. Speciﬁcally, we\nused subsets of the training set that included samples associated with\npeptides of length 8 and 9 and compared both methods over two test\nsets. As can be seen inFigure 7, ESM-GAT outperformed ESMﬁne-\ntuning when the test set with peptide length 10– 15 was considered (red\ndots), while the results were almost the same when using the test set\nwith peptides of length 8 and 9 (blue dots). Bar plots associated with\nthese ﬁgures are available in the Supplementary Section.T h i s\nobservation suggests that GAT has the potential to improve the\nability of the model to predict binding of peptides with lengths\ndifferent from those considered in the training set. The testing of\nthis conjecture is a subject of future research.\n4 Conclusion\nPredicting peptides that bind to the major histocompatibility\ncomplex (MHC) Class I is an important problem in studying the\nimmune system response and a plethora of approaches have been\ndeveloped to tackle this problem. NetMHCpan 4.1 is developed\nbased on training shallow neural networks (Reynisson et al., 2020)\nand is currently considered the state-of-the-art for MHC Class\nI-peptide binding prediction. A number of recent works have\nfocused on using protein language models in MHC-peptide\nbinding problems. Protein language models developed based on\ndeep learning approaches, such as attention-based transformer\nmodels, have shown signi ﬁcant progress towards solving a\nnumber of challenging problems in biology, most importantly,\nthe protein structure prediction problem (Jumper et al., 2021).\nBERTMHC ( Cheng et al., 2020 ) and ImmunoBERT ( Gasser\net al., 2021) for the ﬁrst time applied the pre-trained protein\nlanguage models in MHC-peptide binding problems. Both\nmethods used a relatively small pre-trained model ((Rao et al.,\n2019) was trained with 31 million protein sequences); currently,\nthere are substantially larger and more informative models such as\nFIGURE 7\nESM-GAT ﬁne-tuning outperforms the ESMﬁne-tuning method\nwhen the test set with peptide length 10– 15 is considered (red points)\nwhile the results are almost the same when using the test set with\npeptides of length 8 and 9 (blue points).\nFrontiers in Bioinformatics frontiersin.org10\nHashemi et al. 10.3389/fbinf.2023.1207380\nESM1b (Rao et al., 2020) and ProtTrans (Elnaggar et al., 2020) which\nare trained on more than 250 million protein sequences. In the work\nreported in this paper we focus on MHC Class I peptide binding\nprediction by developing approaches based on large pre-trained\nprotein language models, ESM1b (Rao et al., 2020) and ESM2 (Lin\net al., 2022). We follow twoﬁne-tuning approaches using a soft-max\nlayer and Graph Attention Network (GAT) as well as implement a\ndomain adaptation pre-training for ESM models. In order to have a\nfair comparison, we train our model using the same training set used\nby NetMHCpan 4.1 (Reynisson et al., 2020) and evaluate our model\nusing the same test set. We show, using the standard performance\nmetrics in this area, that our model outperforms NetMHCpan. As\nreported in the paper, adding Graph Attention Network (GAT) to\nthe ESM networks, improved the ability of the model to predict\npeptides with lengths different from those considered in the training\nset; this feature is expected to be beneﬁcial for training models\nbeyond MHC Type I.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here:https://services.healthtech.dtu.dk/suppl/immunology/\nNAR_NetMHCpan_NetMHCIIpan/.\nAuthor contributions\nDK, NH, PV, and IP designed research; NH, BH, and MI\nperformed research; NH, BH, IP, PV, SV, and DK analyzed data;\nPV, NH, and BH wrote the paper, NH, BH, IP, PV, SV, and DK\nreviewed and edited the paper. All authors contributed to the article\nand approved the submitted version.\nFunding\nThis work was supported in part by the National Institutes of\nHealth grants R01 GM135930, RM1135136, R35GM118078, and\nR01GM140098, by the Boston University Clinical and Translational\nScience Award (CTSA) under NIH/NCATS grant UL54 TR004130;\nby the National Science Foundation grants IIS-1914792, DMS-\n1664644, DMS-2054251, and CNS-1645681; and by the Ofﬁce of\nNaval Research grant N00014-19-1-2571.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found online\nat: https://www.frontiersin.org/articles/10.3389/fbinf.2023.1207380/\nfull#supplementary-material\nReferences\nAbelin, J. G., Keskin, D. B., Sarkizova, S., Hartigan, C. R., Zhang, W., Sidney, J., et al.\n(2017). Mass spectrometry proﬁling of hla-associated peptidomes in mono-allelic cells\nenables more accurate epitope prediction. Immunity 46, 315– 326. doi:10.1016/j.\nimmuni.2017.02.007\nAlsentzer, E., Murphy, J. R., Boag, W., Weng, W.-H., Jin, D., Naumann, T., et al.\n(2019). Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323.\nAlvarez, B., Reynisson, B., Barra, C., Buus, S., Ternette, N., Connelley, T., et al. (2019).\nNnalign_ma; mhc peptidome deconvolution for accurate mhc binding motif\ncharacterization and improved t-cell epitope predictions. Mol. Cell. Proteomics 18,\n2459– 2477. doi:10.1074/mcp.tir119.001658\nAranha, M. P., Jewel, Y. S., Beckman, R. A., Weiner, L. M., Mitchell, J. C., Parks, J. M.,\net al. (2020). Combining three-dimensional modeling with artiﬁcial intelligence to\nincrease speciﬁcity and precision in peptide– mhc binding predictions.J. Immunol.205,\n1962– 1977. doi:10.4049/jimmunol.1900918\nBassani-Sternberg, M., Chong, C., Guillaume, P., Solleder, M., Pak, H., Gannon, P. O.,\net al. (2017). Deciphering hla-i motifs across hla peptidomes improves neo-antigen\npredictions and identiﬁes allostery regulating hla speciﬁcity. PLoS Comput. Biol. 13,\ne1005725. doi:10.1371/journal.pcbi.1005725\nBassani-Sternberg, M., and Gfeller, D. (2016). Unsupervised hla peptidome\ndeconvolution improves ligand prediction accuracy and predicts cooperative effects\nin peptide – hla interactions. J. Immunol. 197, 2492 – 2499. doi:10.4049/jimmunol.\n1600808\nBoehm, K. M., Bhinder, B., Raja, V. J., Dephoure, N., and Elemento, O. (2019).\nPredicting peptide presentation by major histocompatibility complex class i: an\nimproved machine learning approach to the immunopeptidome. BMC Bioinforma.\n20, 7– 11. doi:10.1186/s12859-018-2561-z\nBowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large annotated\ncorpus for learning natural language inference. arXiv preprint arXiv:1508.05326.\nBui, H.-H., Sidney, J., Peters, B., Sathiamurthy, M., Sinichi, A., Purton, K.-A., et al.\n(2005). Automated generation and evaluation of speciﬁc mhc binding predictive tools:\narb matrix applications.Immunogenetics 57, 304– 314. doi:10.1007/s00251-005-0798-y\nCaron, E., Kowalewski, D., Koh, C. C., Sturm, T., Schuster, H., and Aebersold, R.\n(2015). Analysis of major histocompatibility complex (mhc) immunopeptidomes using\nmass spectrometry.Mol. Cell. Proteomics14, 3105– 3117. doi:10.1074/mcp.o115.052431\nCheng, J., Bendjama, K., Rittner, K., and Malone, B. (2020). Bertmhc: improves mhc-\npeptide class ii interaction prediction with transformer and multiple instance learning.\nbioRxiv.\nDeng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., et al. (2013).“Recent advances\nin deep learning for speech research at microsoft,” in 2013 IEEE international conference\non acoustics, speech and signal processing(IEEE), 8604– 8608.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:\n1810.04805.\nDoytchinova, I. A., and Flower, D. R. (2001). Toward the quantitative prediction of\nt-cell epitopes: comfa and comsia studies of peptides with afﬁnity for the class i mhc\nmolecule hla-a* 0201.J. Med. Chem.44, 3572–\n3581. doi:10.1021/jm010021j\nEgbert, M., Ghani, U., Ashizawa, R., Kotelnikov, S., Nguyen, T., Desta, I., et al. (2021).\nAssessing the binding properties of casp14 targets and models.Proteins Struct. Funct.\nBioinforma. 89, 1922– 1939. doi:10.1002/prot.26209\nElnaggar, A., Heinzinger, M., Dallago, C., Rihawi, G., Wang, Y., Jones, L., et al. (2020).\nProttrans: towards cracking the language of life’s code through self-supervised deep\nlearning and high performance computing. arXiv preprint arXiv:2007.06225.\nFu, X., and Bates, P. A. (2022). Application of deep learning methods: from molecular\nmodelling to patient classiﬁcation. Exp. Cell. Res.418, 113278. doi:10.1016/j.yexcr.2022.\n113278\nFrontiers in Bioinformatics frontiersin.org11\nHashemi et al. 10.3389/fbinf.2023.1207380\nGasser, H.-C., Bedran, G., Ren, B., Goodlett, D., Alfaro, J., and Rajan, A. (2021).\nInterpreting bert architecture predictions for peptide presentation by mhc class i\nproteins. arXiv preprint arXiv:2111.07137.\nGhani, U., Desta, I., Jindal, A., Khan, O., Jones, G., Kotelnikov, S., et al. (2021).\nImproved docking of protein models by a combination of alphafold2 and cluspro.\nbioRxiv.\nGrebenkin, A., Gaivoronsky, I., Kazyonnov, K., and Kulagin, a. (2020). Application of\nan ensemble of neural networks and methods of statistical mechanics to predict binding\nof a peptide to a major histocompatibility complex.Comput. Res. Model.\nHu, Y., Hosseini, M., Parolin, E. S., Osorio, J., Khan, L., Brandt, P., et al. (2022).\n“Conﬂibert: a pre-trained language model for political con ﬂict and violence,” in\nProceedings of the 2022 conference of the north American chapter of the association\nfor computational linguistics: human language technologies, 5469– 5482.\nIbtehaz, N., and Kihara, D. (2023).“Application of sequence embedding in protein\nsequence-based predictions,” in Machine learning in bioinformatics of protein sequences:\nalgorithms, databases and resources for modern protein bioinformatics (World\nScientiﬁc), 31– 55.\nIuchi, H., Matsutani, T., Yamada, K., Iwano, N., Sumi, S., Hosoda, S., et al. (2021).\nRepresentation learning applications in biological sequence analysis.Comput. Struct.\nBiotechnol. J. 19, 3198– 3208. doi:10.1016/j.csbj.2021.05.039\nJaneway, C. A., Travers, P., Walport, M., and Capra, D. J. (2001).Immunobiology.\nTaylor & Francis Group UK: Garland Science.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., et al.\n(2021). Highly accurate protein structure prediction with alphafold. Nature 596,\n583– 589. doi:10.1038/s41586-021-03819-2\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Tunyasuvunakool, K., et al.\n(2020). High accuracy protein structure prediction using deep learning.Fourteenth Crit.\nAssess. Tech. Protein Struct. Predict.22, 24. Abstract Book.\nKhan, S., and Yairi, T. (2018). A review on the application of deep learning in system\nhealth management.Mech. Syst. Signal Process.107, 241– 265. doi:10.1016/j.ymssp.2017.\n11.024\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.nature 521, 436– 444.\ndoi:10.1038/nature14539\nLensink, M. F., Brysbaert, G., Mauri, T., Nadzirin, N., Velankar, S., Chaleil, R. A., et al.\n(2021). Prediction of protein assemblies, the next frontier: the casp14-capri experiment.\nProteins Struct. Funct. Bioinforma.89, 1800– 1823. doi:10.1002/prot.26222\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., et al. (2022). Evolutionary-scale\nprediction of atomic level protein structure with a language model.bioRxiv.\nM a i m e l a ,N .R . ,L i u ,S . ,a n dZ h a n g ,Y .( 2 0 1 9 ) .F a t e so fc d 8 +tc e l l si nt u m o r\nmicroenvironment.Comput. Struct. Biotechnol. J.17, 1– 13. doi:10.1016/j.csbj.2018.11.004\nMaron, O., and Lozano-Pérez, T. (1998). A framework for multiple-instance learning.\nAdv. neural Inf. Process. Syst., 570– 576.\nMohammadzadeh, S., and Lejeune, E. (2021). Predicting mechanically driven full-\nﬁeld quantities of interest with deep learning-based metamodels.Extreme Mech. Lett.\n50, 101566. doi:10.1016/j.eml.2021.101566\nNielsen, M., Lundegaard, C., Worning, P., Lauemøller, S. L., Lamberth, K., Buus, S.,\net al. (2003). Reliable prediction of t-cell epitopes using neural networks with novel\nsequence representations. Protein Sci. 12, 1007– 1017. doi:10.1110/ps.0239403\nO’Donnell, T. J., Rubinsteyn, A., and Laserson, U. (2020). Mhcﬂurry 2.0: improved\npan-allele prediction of mhc class i-presented peptides by incorporating antigen\nprocessing. Cell. Syst. 11, 42– 48.e7. doi:10.1016/j.cels.2020.06.010\nOng, E., Huang, X., Pearce, R., Zhang, Y., and He, Y. (2021). Computational design of\nsars-cov-2 spike glycoproteins to increase immunogenicity by t cell epitope engineering.\nComput. Struct. Biotechnol. J.19, 518– 529. doi:10.1016/j.csbj.2020.12.039\nParker, K. C., Bednarek, M. A., and Coligan, J. E. (1994). Scheme for ranking potential\nhla-a2 binding peptides based on independent binding of individual peptide side-\nchains. J. Immunol. 152, 163– 175. doi:10.4049/jimmunol.152.1.163\nRao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, X., Canny, J., et al. (2019).\nEvaluating protein transfer learning with tape. Adv. Neural Inf. Process. Syst. 32,\n9689– 9701. doi:10.1101/676825\nRao, R., Liu, J., Verkuil, R., Meier, J., Canny, J. F., Abbeel, P., et al. (2021). Msa\ntransformer. bioRxiv.\nRao, R. M., Meier, J., Sercu, T., Ovchinnikov, S., and Rives, A. (2020). Transformer\nprotein language models are unsupervised structure learners.bioRxiv. doi:10.1101/2020.\n12.15.422761\nReynisson, B., Alvarez, B., Paul, S., Peters, B., and Nielsen, M. (2020). Netmhcpan-\n4.1 and netmhciipan-4.0: improved predictions of mhc antigen presentation by\nconcurrent motif deconvolution and integration of ms mhc eluted ligand data.\nNucleic acids Res.48, W449– W454. doi:10.1093/nar/gkaa379\nRives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C. L., et al. (2019). Biological\nstructure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. bioRxiv, 622803.\nShao, W., Pedrioli, P. G., Wolski, W., Scurtescu, C., Schmid, E., Vizcaíno, J. A., et al.\n(2018). The systemhc atlas project.Nucleic acids Res.46, D1237– D1247. doi:10.1093/\nnar/gkx664\nTeraguchi, S., Saputri, D. S., Llamas-Covarrubias, M. A., Davila, A., Diez, D., Nazlica,\nS. A., et al. (2020). Methods for sequence and structural analysis of b and t cell receptor\nrepertoires. Comput. Struct. Biotechnol. J.18, 2000– 2011. doi:10.1016/j.csbj.2020.07.008\nTong, J. (2013). “Blocks substitution matrix (blosum),” in Encyclopedia of systems\nbiology (Springer).\nTownsend, A., Elliott, T., Cerundolo, V., Foster, L., Barber, B., and Tse, A. (1990).\nAssembly of mhc class i molecules analyzedin vitro. Cell. 62, 285– 295. doi:10.1016/\n0092-8674(90)90366-m\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\nVeličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. (2017).\nGraph attention networks. arXiv preprint arXiv:1710.10903.\nVita, R., Mahajan, S., Overton, J. A., Dhanda, S. K., Martini, S., Cantrell, J. R., et al.\n(2019). The immune epitope database (iedb): 2018 update. Nucleic acids Res. 47,\nD339– D343. doi:10.1093/nar/gky1006\nVoulodimos, A., Doulamis, N., Doulamis, A., and Protopapadakis, E. (2018). Deep\nlearning for computer vision: a brief review.Comput. Intell. Neurosci.2018, 1, 13. doi:10.\n1155/2018/7068349\nZeng, H., and Gifford, D. K. (2019). Deepligand: accurate prediction of mhc class i\nligands using peptide embedding. Bioinformatics 35, i278 – i283. doi:10.1093/\nbioinformatics/btz330\nZhang, C., Bickis, M. G., Wu, F.-X., and Kusalik, A. J. (2006). Optimally-connected\nhidden markov models for predicting mhc-binding peptides.J. Bioinforma. Comput.\nBiol. 4, 959– 980. doi:10.1142/s0219720006002314\nZhang, X.-M., Liang, L., Liu, L., and Tang, M.-J. (2021). Graph neural networks and\ntheir current applications in bioinformatics. Front. Genet. 12, 690049. doi:10.3389/\nfgene.2021.690049\nFrontiers in Bioinformatics frontiersin.org12\nHashemi et al. 10.3389/fbinf.2023.1207380"
}