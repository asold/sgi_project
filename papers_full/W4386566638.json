{
  "title": "Should You Mask 15% in Masked Language Modeling?",
  "url": "https://openalex.org/W4386566638",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3200543743",
      "name": "Alexander Wettig",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2120113606",
      "name": "Tianyu Gao",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2886587608",
      "name": "Zexuan Zhong",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2095803999",
      "name": "Danqi Chen",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W4229686017",
    "https://openalex.org/W4280490805",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4285251104",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3199241049",
    "https://openalex.org/W4221167396",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4206136559",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3034878914",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W3197120431",
    "https://openalex.org/W3201490875",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W4280652569",
    "https://openalex.org/W4385573911",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4313156423"
  ],
  "abstract": "Masked language models (MLMs) conventionally mask 15% of tokens due to the belief that more masking would leave insufficient context to learn good representations; this masking rate has been widely used, regardless of model sizes or masking strategies. In this work, we revisit this important choice of MLM pre-training. We first establish that 15% is not universally optimal, and larger models should adopt a higher masking rate. Specifically, we find that masking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD. Interestingly, an extremely high masking rate of 80% can still preserve 95% fine-tuning performance and most of the accuracy in linguistic probing, challenging the conventional wisdom about the role of the masking rate. We then examine the interplay between masking rates and masking strategies and find that uniform masking requires a higher masking rate compared to sophisticated masking strategies such as span or PMI masking. Finally, we argue that increasing the masking rate has two distinct effects: it leads to more corruption, which makes the prediction task more difficult; it also enables more predictions, which benefits optimization. Using this framework, we revisit BERT’s 80-10-10 corruption strategy. Together, our results contribute to a better understanding of MLM pre-training.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2985–3000\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nShould You Mask 15% in Masked Language Modeling?\nAlexander Wettig∗ Tianyu Gao∗ Zexuan Zhong Danqi Chen\nDepartment of Computer Science, Princeton University\n{awettig,tianyug,zzhong,danqic}@cs.princeton.edu\nAbstract\nMasked language models (MLMs) convention-\nally mask 15% of tokens due to the belief that\nmore masking would leave insufﬁcient con-\ntext to learn good representations; this mask-\ning rate has been widely used, regardless of\nmodel sizes or masking strategies. In this\nwork, we revisit this important choice of MLM\npre-training. We ﬁrst establish that 15% is\nnot universally optimal, and larger models\nshould adopt a higher masking rate. Speciﬁ-\ncally, we ﬁnd that masking 40% outperforms\n15% for BERT-large size models on GLUE\nand SQuAD. Interestingly, an extremely high\nmasking rate of 80% can still preserve 95%\nﬁne-tuning performance and most of the ac-\ncuracy in linguistic probing, challenging the\nconventional wisdom about the role of the\nmasking rate. We then examine the interplay\nbetween masking rates and masking strate-\ngies and ﬁnd that uniform masking requires a\nhigher masking rate compared to sophisticated\nmasking strategies such as span or PMI mask-\ning. Finally, we argue that increasing the mask-\ning rate has two distinct effects: it leads to\nmore corruption, which makes the prediction\ntask harder; it also enables more predictions,\nwhich beneﬁts optimization. Using this frame-\nwork, we revisit BERT’s 80-10-10 corruption\nstrategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.1\n1 Introduction\nPre-trained language models have transformed the\nlandscape of natural language processing (Devlin\net al., 2019; Liu et al., 2019; Raffel et al., 2020;\nBrown et al., 2020, inter alia). They are trained\non vast quantities of text data and acquire rich and\nversatile language representations. Compared to\nautoregressive models, which always predict the\nnext token in a sequence, masked language models\n*The ﬁrst two authors contributed equally.\n1Our code and pre-trained models are publicly available at\nhttps://github.com/princeton-nlp/DinkyTrain.\n(MLMs) like BERT (Devlin et al., 2019) predict a\nmasked subset of input tokens based on the remain-\ning context and are more effective on downstream\ntasks due to their bidirectional nature.\nBERT chooses a 15% masking rate, based on\nthe reasoning that models cannot learn good rep-\nresentations when too much text is masked, and\nthe training is inefﬁcient when too little is masked.\nSurprisingly, this important choice has been under-\nexplored since 15% masking is used ubiquitously\nby BERT’s successors (Liu et al., 2019; Joshi et al.,\n2020; Lan et al., 2020; He et al., 2021; Levine et al.,\n2021; Izsak et al., 2021), regardless of model sizes,\nmasking strategies and optimization recipes.2\nIn this work, we aim to understand the impact\nof masking rates. We hypothesize that the optimal\nmasking rate is not universally 15%, but should\ndepend on other factors. First, we consider the\nimpact of model sizes and establish that indeed\nlarger models should adopt higher masking rates\n(§3). Speciﬁcally, we ﬁnd that under an efﬁcient\npre-training recipe (Izsak et al., 2021), 40% out-\nperforms 15% for BERT-large size models when\nﬁne-tuning on GLUE and SQuAD.\nInterestingly, we observe that large models can\nstill learn good representations even for very high\nmasking rates: if we mask as much as 80% of input\ntokens and pre-trained models have a perplexity\nof more than 1000, the learned representations can\nstill preserve more than 95% of ﬁne-tuning per-\nformance on downstream tasks, compared to the\ndefault 15% masking (Table 1), and show consider-\nable performance in linguistic probing (§4). This\nchallenges common intuitions about masking rates\nand what models learn in MLM pre-training.\nWe then focus on the strategy of which tokens to\nmask as an additional factor to the optimal masking\nrate of MLMs (§5). We ﬁnd that different mask-\ning rates should be used with different masking\nstrategies, and the default uniform masking bene-\n2Some exceptions are discussed in §8.\n2985\nPre-training Fine-tuning\nm Example PPL MNLI QNLI SQuAD 3\n15% We study highmask ing ratesfor pre-training language models . 17.7 84.2 90.9 88.0\n40% We study highmasking ratesfor pre-traininglanguage models . 69.4 84.5↑0.3 91.6↑0.7 89.8↑1.8\n80% We study highmaskingratesforpre-traininglanguage models. 1141.4 80.8 ↓3.4 87.9↓3.0 86.2↓1.8\nRandom initialization 61.5↓22.7 60.9↓30.0 10.8↓77.2\nTable 1: Masked examples, validation perplexity (calculated in the same way as Devlin et al., 2019) of different\nmasking rates on the one billion word benchmark (Chelba et al., 2013), and downstream task development per-\nformance (SQuAD: F1; accuracy for others). All the pre-trained models have a BERT-large architecture and are\ntrained with the efﬁcient pre-training recipe (§2.2). Full results are provided in Table 7.\nﬁts more from higher masking rates than more so-\nphisticated masking strategies such as span (Joshi\net al., 2020; Raffel et al., 2020) and PMI mask-\ning (Levine et al., 2021); when all methods are\nconsidered at their optimal masking rate, uniform\nmasking achieves competitive performance.\nFinally, we propose to dissect the masking rate\ninto two factors (§6): the corruption rate—how\nmuch of the context is corrupted (masked)—and\nthe prediction rate—how much of the tokens the\nmodel predicts on. In MLMs, both are set to the\nmasking rate. However, these two factors have\nopposing effects: higher prediction rates generate\nmore training signals and beneﬁt the optimization,\nwhile higher corruption rates make the prediction\ntask more challenging by providing less context.\nTo study the two factors independently, we design\nablation experiments to disentangle corruption and\nprediction rates. Thus, we can verify that mod-\nels beneﬁt from higher prediction rates and suffer\nfrom more corruption. Using this framework, we\nalso discuss BERT’s practice of predicting on orig-\ninal or random tokens (the 80-10-10 rule), and we\nﬁnd that models usually perform worse under this\ncorruption strategy (§7).\nTogether, our results demonstrate the overlooked\nimpact of the masking rate in MLM pre-training\nand our analysis disentangles its opposing effects\nof corruption and prediction. We conclude by dis-\ncussing the relation to work in other models and\nmodalities (§8) and by highlighting several new\navenues for efﬁcient MLM in the future (§9).\n2 Background\n2.1 Masked Language Modeling\nWe focus on the widely popular masked language\nmodeling (Devlin et al., 2019), a form of denoising-\n3For our SQuAD v1.1 experiments, we continue training\nthe models with 512-token sequences for 2,300 steps and\nreport F1. See Appendix A for more details.\nautoencoding, where a model is trained to restore\na corrupted input sequence. Speciﬁcally, masked\nlanguage models make independent predictions on\nthe subset of masked tokens:\nL(C) = E\nx∈C\nE\nM⊂x\n|M|=m|x|\n[\n∑\nxi∈M\nlog p(xi|˜x)\n]\n, (1)\nwhere one masks m (masking rate, typically 15%)\npercentage of tokens from the original sentence x\nand predicts on the masked token set Mgiven the\ncorrupted context ˜x (the masked version of x).\nDifferent masking strategies have been proposed\nto sample M: Devlin et al. (2019) randomly\nchoose from the input tokens with a uniform dis-\ntribution; Joshi et al. (2020) sample contiguous\nspans of text; Levine et al. (2021) sample words\nand spans with high pointwise mutual informa-\ntion (PMI). These advanced sampling strategies are\nadopted to prevent models from exploiting shallow\nlocal cues from uniform masking.\nMLMs can encode bidirectional context while\nautoregressive language models can only “look at\nthe past”, and thus MLMs are shown to be more\neffective at learning contextualized representations\nfor downstream use (Devlin et al., 2019). On the\nother hand, MLMs suffer a signiﬁcant computa-\ntional cost because it only learns from 15% of the\ntokens per sequence, whereas autoregressive LMs\npredict every token in a sequence. In this work, we\nfocus on MLMs and study the effects of different\nmasking rates on downstream performance.\n2.2 Experiment Setup\nWe build most of our experiments on a recent ef-\nﬁcient pre-training recipe—the 24hBERT recipe\nfrom Izsak et al. (2021)—by using which mod-\nels can match BERT-base performance 6×faster\n(tested on 8×Titan-V). This efﬁcient pre-training\nrecipe allows us to run a large amount of exper-\niments in an academic setup. Izsak et al. (2021)\n2986\n5k 10k 15k 20k\nTraining Step\n81.0\n82.5\n84.0Performance (%)\nMNLI\n5k 10k 15k 20k\nTraining Step\n88.5\n90.0\n91.5\nQNLI\n5k 10k 15k 20k\nTraining Step\n86.8\n87.2\n87.6\n88.0\nQQP\n5k 10k 15k 20k\nTraining Step\n84.0\n86.0\n88.0\nSTS-B\n5k 10k 15k 20k\nTraining Step\n91.0\n92.0\n93.0\nSST-2\n5k 10k 15k 20k\nTraining Step\n80.0\n82.0\n84.0\nSQuAD\n15%\n40%\nFigure 1: Downstream task development performance of large models trained with the efﬁcient pre-training\nrecipe, under masking rates of 15% and 40%. We highlight by the blue dotted line how long the 40% model takes\nto achieve the same performance as the 15% baseline; On QNLI and QQP, the 40% model achieved the same\nperformance with almost half the training time.\nmake the pre-training faster by using a BERT-large\narchitecture, a larger learning rate (2e-3), a larger\nbatch size (4,096), a shorter sequence length (128)4,\nand fewer training steps. We deviate from the\n24hBERT with a few simple changes:\n1. We adopt RoBERTa’s BPE tokenizer (Sennrich\net al., 2016; Liu et al., 2019) rather than BERT’s\ntokenizer for it performs better in our prelimi-\nnary experiments (see Appendix C).\n2. Instead of adopting BERT’s 80-10-10 token\ncorruption strategy, we simply replace all the\nmasked tokens with [MASK] by default. We ﬁnd\nthat the 80-10-10 corruption strategy does not\nperform better for most downstream tasks, as\ndiscussed in §7.\nFollowing 24hBERT, we also do not perform\nnext sentence prediction during pre-training, which\nwas shown to hurt performance (Liu et al.,\n2019). We show hyperparameters for the efﬁ-\ncient pre-training recipe and a comparison to other\nrecipes (Devlin et al., 2019; Liu et al., 2019) in Ap-\npendix A. For models of different sizes, masking\nrates, and masking strategies, we follow the same\nrecipe as our preliminary experiments show that it\nstill performs the best.\nWe use ﬁne-tuning downstream task perfor-\nmance as the measurement of how good the MLMs\nare, since ﬁne-tuning is the predominant way to use\npre-trained MLMs in downstream use. As evident\nfrom Table 1, pre-training metrics like perplexity\ndo not correlate well with the downstream perfor-\nmance. We describe our downstream ﬁne-tuning\nsetting and hyperparameters in Appendix A.\n4Izsak et al. (2021) only evaluate on GLUE tasks instead\nof SQuAD because of the short sequence length. We further\ntrain the model with 512 tokens for SQuAD in Table 1.\n5For each task and each model size, normalized perfor-\nmance is calculated by x−x15%\nσ where x15% is the perfor-\nmance of 15% masking rate and σis the standard deviation\nacross all masking rates. Relative F1 is the F1 score subtracted\nby the 15% model F1.\n15 20 30 40 50\nMasking Rate (%)\n-2\n-1\n0\n1\nNormalized Performance\nAverage GLUE\nlarge\nbase\nmedium\n15 20 30 40 50\nMasking Rate (%)\n−0.5\n0.0\n0.5\n1.0\n1.5\nRelative F1 (%)\nSQuAD\nlarge\nbase\nmedium\nFigure 2: Impact of masking rates on different model\nsizes (large>base>medium).5 We see that larger mod-\nels favor larger optimal masking rates.\n3 Larger Models Can Beneﬁt From\nHigher Masking Rates\nDevlin et al. (2019) choose the mysterious mask-\ning rate of 15%, for the belief that masking more\nleads to insufﬁcient context to decode the tokens,\nand masking fewer makes the training inefﬁcient,\nand this masking rate has been viewed as a con-\nstant across different model sizes. In this section,\nwe train models of size large (354M parameters),\nbase (124M parameters), and medium (51M param-\neters) for masking rates varying from 15% to 50%.\nThe model conﬁgurations are listed in Appendix E.\nOptimal masking depends on model sizes. The\nimpact of masking rate across the model sizes is\nsummarized by Figure 2, with detailed results given\nin Appendix E. We see that larger models possess\nhigher optimal masking rates: on average, under\nthe efﬁcient pre-training recipe, large models take\n40% as the optimal masking rate;base models take\n20% and medium models take 15%. This shows that\nlarger MLM models favor higher masking rates.\nWe hypothesize that the additional capacity allows\nthe large MLM to “handle” the more challenging\ntask of predicting many tokens given less context.\nLarge models learn faster with 40% masking.\nWe now compare the best performing masking rate\n40% to the conventional 15% in more detail for our\n2987\n20 30 40 50 60 70 80\nMasking Rate (%)\n81.0\n82.5\n84.0Performance (%)\nMNLI\n20 30 40 50 60 70 80\nMasking Rate (%)\n88.5\n90.0\n91.5\nQNLI\n20 30 40 50 60 70 80\nMasking Rate (%)\n87.2\n87.6\n88.0\nQQP\n20 30 40 50 60 70 80\nMasking Rate (%)\n86.4\n87.2\n88.0\nSTS-B\n20 30 40 50 60 70 80\nMasking Rate (%)\n90.0\n91.0\n92.0\n93.0\nSST-2\n20 30 40 50 60 70 80\nMasking Rate (%)\n80.0\n82.0\n84.0\nSQuAD\nMask 15%\nFigure 3: Impact of masking rates on large models with the efﬁcient pre-training recipe. We see that on most\ntasks, higher masking rates outperform 15%. 40% is the optimal masking rate overall.\nlarge model. First, we plot how the downstream\ntask performance changes with different training\nsteps in Figure 1. For most tasks, we see that 40%\nmasking outperforms 15% consistently during the\ncourse of training, such that on QNLI and QQP,\nthe 40% model can achieve the same performance\nas the 15% baseline with only half the training\ntime. We also report the test results in Table 2,\nwhere again masking 40% outperforms 15% with\nour efﬁcient pre-training recipe. However, the opti-\nmal masking rate can be task-dependent, as SST-2\nperforms better with 15% masking at the end of\ntraining. We acknowledge that the optimal mask-\ning rate may also depend on the training recipe.\nSince the efﬁcient pre-training recipe uses a rela-\ntively small number of training steps, we explore\ntraining for over 4×more steps, as well as training\nwith a more expensive recipe from RoBERTa (Liu\net al., 2019), and we ﬁnd in Appendix D that using\na 40% masking rate still performs well, achieving\nsimilar performance to the 15% masking rate. The\nexperiments in the remaining sections of this paper\nare all based on large models.\n4 MLMs in High-Masking Regimes\nThe success of masking 40% over 15% motivates\nus to explore what happens at even larger masking\nrates. Therefore, we pre-train additional large\nmodels with masking rates of up to 80%. We\nconsider the question of what representations an\nMLM can learn with such limited input as the last\nmasked sentence in Table 1, which is hard to de-\ncipher even for a human. While He et al. (2022)\nrecently pioneered such high masking rates in the\nvision domain, and they reason that images are nat-\nural signals with heavy redundancy, while language\nis highly semantic and information-dense. To our\nknowledge, nobody has examined such high mask-\ning rates in masked language modeling before.\nMLMs learn with extreme masking. We ﬁrst\nconﬁrm in Table 1 that the validation perplexity\nwhen pre-training with an 80% masking rate is ex-\n20 40 60 80\nMasking Rate (%)\n60\n75\n90Accuracy (%)\nBLiMP by Linguistic Category\nanaphor agreement\nirregular forms\ndeterminer noun agreement\nellipsis\nsubject verb agreement\nargument structure\nbinding\ncontrol raising\nnpi licensing\nfiller gap dependency\nquantifiers\nisland effects\nFigure 4: Evaluating our models on the BLiMP\nbenchmark (Warstadt et al., 2020) using pseudo log-\nlikelihood scoring (Salazar et al., 2020).\ntremely high ( >1,000), which suggests that the\nMLM is unable to reconstruct corrupted inputs\nwith independent token predictions. Therefore our\nsetting differs from vision, where good reproduc-\ntions are possible with high masking rates (He\net al., 2022). Nevertheless, we ﬁnd that MLMs\ncan surprisingly still learn good representations:\nFigure 3 shows the performance of the models ﬁne-\ntuned on a range of tasks, and we observe that pre-\ntraining with an 80% masking rate can retain 95%\nof ﬁne-tuning performance, which is substantially\nbetter than ﬁne-tuning from a random initialization,\nwhich is reported in Appendix B.\nWe hypothesize that MLMs at such high mask-\ning rates may be understood as a powerful skip-\ngram model (Mikolov et al., 2013), e.g., masking\n80% of a 128 token sequence still learns skip-grams\nof length up to 26. Furthermore, when compared\nto the simple word2vec model, our Transformer\nmodels have access to positional information for\neach context token and prediction.\nAnalysis of linguistic probing. Besides down-\nstream performance, we study the models’ lin-\nguistic abilities by evaluating them on the BLiMP\nbenchmark (Warstadt et al., 2020). We employ\nzero-shot pseudo log-likelihood scoring (Salazar\net al., 2020), where a score is computed by masking\neach token individually, which is a greater distri-\n2988\nMNLI-m/mm QNLI QQP RTE SST-2 MRPC CoLA STS-B SQuAD\nMasking 15% 84.2/83.4 90.9 70.8 73.5 92.8 88.8 51.8 87.3 88.0\n⋆Masking 40% 84.7/84.0 91.3 70.9 75.5 92.6 89.8 50.7 87.6 89.8\nTable 2: The test results on the GLUE benchmark with large models, the efﬁcient pre-training recipe (Izsak et al.,\n2021), and with 15% or 40% masking rates. For RTE, MRPC, and STS-B we ﬁne-tune from the MNLI model\nfollowing convention set by Phang et al. (2018). For SQuAD v1.1, we take the same setting as Table 1.\nbutional shift from higher masking rates. We show\nour results in Figure 4. We ﬁnd that most linguis-\ntic phenomena are acquired evenly across mask-\ning rates from 15% to 60%, but they are still cap-\ntured well by an MLM trained with 80% masking—\nwhich on average preserves 90% of the probing\naccuracy of the 15% model baseline. However,\nsome categories such as ﬁller gap dependencies\nand island effects show clear trends that perfor-\nmance deteriorates with higher masking rates—\nalthough it remains unclear to what extent such\nlinguistic knowledge is required by downstream\ntasks in GLUE (Sinha et al., 2021). Overall, our\nresults suggest that useful linguistic knowledge can\nbe learned from a “patchy” training signal.\n5 Masking Rates vs. Masking Strategies\nDevlin et al. (2019); Liu et al. (2019) use uniform\nsampling for selecting which tokens to mask. Sub-\nsequent work showed that adopting more sophisti-\ncated masking strategies—such as span masking or\nPMI masking—can outperform uniform masking\non a range of downstream tasks (Joshi et al., 2020;\nLevine et al., 2021). The argument for adopting\nadvanced masking is that uniform masking enables\nmodels to exploit shallow local cues (Levine et al.,\n2021). An example is given by “ [MASK] Kong”:\nthe model can easily predict “Hong” without using\nmore context. However, all the previous studies\nused a constant 15% masking rate regardless of\nmasking strategies, which raises the question of\nwhether the conclusions still hold with a higher\nmasking rate.\nWe experiment with multiple masking strategies\nas an additional factor for the optimal masking rate\nin large models. Figure 5 shows the results of uni-\nform masking, T5-style span masking (Raffel et al.,\n2020)6, and PMI masking (Levine et al., 2021) un-\nder masking rates from 15% to 40%. We see that\n(1) for all masking strategies, the optimal masking\n6Span maskings in Raffel et al. (2020) and Joshi et al.\n(2020) differ in sampling procedures and we follow Raffel\net al. (2020) for implementation simplicity.\n15 20 30 40\nMasking Rate (%)\n0\n1Normalized Performance\nAverage GLUE\n15 20 30 40\nMasking Rate (%)\n83.0\n83.5\n84.0\n84.5\n85.0F1 (%)\nSQuAD\nPMI\nSpan\nUniform\nFigure 5: Performance of different masking strate-\ngies trained with different masking rates (efﬁcient pre-\ntraining recipe, large models).\n0 20 40 60 80 100\nMasking Rate (%)\n0.0\n0.2\n0.5\n0.8\n1.0P ( PMI Unit Masked )\nPMI Spans\nPMI@15%\nPMI\nSpan\nUniform\n0 20 40 60 80 100\nMasking Rate (%)\n0\n3\n6\n9\n12Avg. Masked Span Length\nSpan Lengths\nPMI@15%\nPMI\nSpan\nUniform\nFigure 6: Higher masking rates increase the probability\nthat an entire PMI span is masked (left) under differ-\nent masking strategies. Uniform masking with a 40%\nrate masks as many PMI spans as regular PMI masking\nat 15%. Masks form longer spans for higher masking\nrates in uniform sampling, while the average length is\nﬁxed at 3 for T5-style span masking (which cannot be\nenforced for very high masking rates).\nrates are higher than 15%; (2) the optimal masking\nrates for span masking and PMI masking are lower\nthan that of uniform masking; (3) when all strate-\ngies adopt the optimal masking rates, the uniform\nmasking achieves similar and even better results\ncompared to the advanced strategies. We also re-\nmark that, when masking with 15%, simply increas-\ning the masking rate can be a more effective way\nto increase performance on SQuAD than switching\nfrom uniform masking to another more advanced\nstrategy. More ﬁne-grained results with these mask-\ning strategies are included in Appendix E.\nInterestingly, higher masking rates naturally in-\ncrease the chance of masking neighbouring co-\n2989\nmcorr mpred MNLI QNLI QQP STS-B SST-2\n40% 40% 84.50.1 91.60.1 88.10.0 88.20.1 92.80.1\n40% 20% 83.7↓ 90.6↓ 87.8↓ 87.5↓ 92.9↑\n20% 20% 84.1↓ 91.3↓ 87.9↓ 87.4↓ 92.7↓\n20% 40% 85.7↑ 92.0↑ 87.9↓ 88.6↑ 93.4↑\n10% 40% 86.3↑ 92.3↑ 88.3↑ 88.9↑ 93.2↑\n05% 40% 86.9↑ 92.2↑ 88.5↑ 88.6↑ 93.9↑\nTable 3: Corruption vs. prediction. We take 40% mask-\ning as the baseline model (standard deviation reported),\ndisentangle mcorr and mpred, and manipulate each inde-\npendently. The trend is clear: more prediction helps\nand more corruption hurts.\noccuring tokens, similar to the effect of the ad-\nvanced masking strategies. We consider the masked\ntokens over one epoch of training, and count the\nnumber of PMI n-grams (e.g., “Hong Kong”) that\nwere completely covered by different masking\nstrategies. Figure 6 shows that raising the masking\nrate from 15% to 40% results in an 8-fold increase\nin the chance of masking a PMI n-gram under uni-\nform masking and gives a value comparable to PMI\nmasking at 15% masking rate. Similarly, higher\nmasking rates also make the masked tokens form\nlonger spans. However, at a given masking rate,\nuniform masking remains an easier task than span\nmasking or PMI masking—it appears reasonable\nfor uniform masking to admit a higher optimal\nmasking rate for a given model capacity.\n6 Understanding Masking As\nCorruption and Prediction\nIn this section, we analyze how masking rates af-\nfect the pre-training process of MLMs, through\ntwo distinct perspectives: task difﬁculty and op-\ntimization. We identify that the masking rate m\ndetermines two import aspects of the pre-training\nproblem: the corruption rate mcorr and the predic-\ntion rate mpred. mcorr is the proportion of tokens\nthat are erased from the input sequence—typically\nby substituting [MASK]. mpred is the proportion of\ntokens that the models predict, and each of those\ntokens contributes to the cross-entropy loss.\nIn Eq. (1), mcorr controls how much content is\ncorrupted in ˜x compared to the original sentence\nx, and mpred controls the number of predictions\nin the set M. Usually, both the corruption and\nthe prediction rates are tied to the masking rate,\ni.e., mcorr = mpred = m, but they may impact\nrepresentation quality differently.\nmcorr controls task difﬁculty. Masked language\nmodeling attempts to learn a conditional proba-\nbility distribution over the vocabulary given the\ncorrupted context p(·| ˜x) during pre-training. If a\nlarger proportion of the input is corrupted, a token\nprediction is conditioned on fewer context tokens,\nmaking predictions harder and more uncertain.\nmpred affects optimization. Predicting more\nmeans the model learns from more training signals,\nso higher prediction rates boost the model perfor-\nmance. From another perspective, each prediction\nat each masked token leads to a loss gradient, which\nis averaged to optimize the weights of the model.\nAveraging across more predictions has a similar\neffect to increasing the batch size, which is proved\nto be beneﬁcial for pre-training (Liu et al., 2019).\nExperiments. In masked language modeling, both\nmcorr and mpred are determined by the overall mask-\ning rate. To study how mcorr and mpred affect the\ndownstream performance independently, we design\na simple ablation experiment to disentangle them:\n1. If mpred < mcorr, we mask mcorr of tokens and\nonly make predictions on mpred of the tokens. This\ncan be implemented without additional cost. For\nexample, with mcorr = 40%and mpred = 20%, we\nmask 40% and only predict on 20% tokens.\n2. If mpred > mcorr, we duplicate each sequence\n⌈mpred\nmcorr ⌉times and mask disjoint sets of mcorr of\nthe tokens in different sequences. For example,\nwith mcorr = 20% and mpred = 40%, for each\nsentence, we do twice 20% masking on different\ntokens and predict on all the masked tokens—this\nleads to a 20% corruption but a 40% prediction\non each sequence. Note that this ablation takes\n⌈mpred\nmcorr ⌉times longer because we do multiple passes\non every sequence, and is not efﬁcient in practice.\nTable 3 shows the ablation results with disen-\ntangled mcorr and mpred. We see that (1) ﬁxing\nthe mcorr as 40%, lowering the mpred from 40% to\n20% results in a consistent drop on downstream\ntasks, showing that more predictions lead to bet-\nter performance; (2) ﬁxing the mpred as 40%, low-\nering the mcorr leads to consistently better perfor-\nmance, suggesting that lower corruption rates make\nthe pre-training task easier to learn and are better\nfor pre-training. Though we see that the perfor-\nmance gain by lowering mcorr from 10% to 5%\nis much smaller than that by lowering mcorr from\n40% to 20%, suggesting a diminishing marginal\n2990\nMNLI QNLI QQP STS-B SST-2\n40% mask 84.50.1 91.60.1 88.10.0 88.20.1 92.80.1\n+5% same 84.2↓ 91.0↓ 87.8↓ 88.0↓ 93.3↑\nw/ 5% rand 84.5↓ 91.3↓ 87.9↓ 87.7↓ 92.6↓\nw/ 80-10-10 84.3↓ 91.2↓ 87.9↓ 87.8↓ 93.0↑\nTable 4: Impact of substituting masks with ran-\ndom/same tokens. “+5% same”: do extra 5% same to-\nken predictions. “w/ 5% rand”: use mask for 35% mask\ntokens and random tokens for 5% . “w/ 80-10-10”: for\nthe 40% masked tokens, 10% are same token predic-\ntions and 10% are random token corruptions.\nreturn of reducing the corruption rate. (3) com-\nparing mcorr = 20%, mpred = 20% and mcorr =\n40%, mpred = 40%, we see that the gain brought\nby more predictions transcends the drawback of\nmore corruption, leading to better performance.\nThe ablation shows that when we tune the mask-\ning rate, we are tuning the corruption rate and the\nprediction rate together, which have antagonistic\neffects. The ﬁnal outcome is decided by which\nrate weighs more—the model beneﬁts from higher\nmasking rates if the hindrance brought by high\ncorruption is surpassed by the advantage from pre-\ndicting more. Many factors may affect the balance\nbetween the two—for example, model sizes and\nmasking strategies as we discussed in §3 and §5.\n7 Revisiting BERT’s Corruption\nStrategy\nDevlin et al. (2019) suggest that it is beneﬁcial to\nreplace 10% of [MASK] tokens with the original to-\nken (same token predictions) and 10% with random\ntokens (random token corruptions). Since then, this\n80-10-10 rule has been widely adopted in almost\nall the MLM pre-training work (Liu et al., 2019;\nJoshi et al., 2020; He et al., 2021). The motivation\nis that masking tokens create a mismatch between\npre-training and downstream ﬁne-tuning, and us-\ning original or random tokens as an alternative to\n[MASK] may mitigate the gap. With our corruption\nand prediction framework, we revisit the two kinds\nof mask replacements in the 80-10-10 rules and\nempirically verify whether they are beneﬁcial to\ndownstream performance.\nSame token predictions. The loss from same to-\nken predictions is very small and should be re-\ngarded as an auxiliary regularization. Thus, same\ntoken predictions should neither count towards the\ncorruption nor to the prediction—they do not cor-\nrupt the input and contribute little to learning.\nRandom token corruptions. Replacing with ran-\ndom tokens contribute to corruption and prediction\nrate, as the input is corrupted and the prediction\ntask is non-trivial. In fact, we ﬁnd that the loss\nis slightly higher on random tokens compared to\n[MASK], as (1) the model needs to decide for all\ntokens whether the information at the input is from\na corruption or not, and (2) predictions need to be\ninvariant to large changes in the input embeddings.\nAblation experiments. We adopt the m = 40%\nmodel using only [MASK] replacements as the base-\nline, on top of which we add three models:\n1. “+5% same”: we mask 40% of tokens but pre-\ndict on 45% of tokens. Adding same token predic-\ntions does not change mcorr or mpred.\n2. “w/ 5% random”: we mask 35% of tokens and\nrandomly replace another 5% of tokens, predicting\non 40% in total.\n3. “80-10-10”: the original BERT recipe. Due to\nsame token predictions, mcorr = mpred = 36%.\nAs shown in Table 4, we observe that same to-\nken predictions and random token corruptions de-\nteriorate performance on most downstream tasks.\nThe 80-10-10 rule performs worse than simply\nusing all [MASK]—with the exception of SST-2,\nwhere same token predictions are beneﬁcial. Over-\nall, our results suggest that in the ﬁne-tuning\nparadigm, the model can adapt to full, uncorrupted\nsentences, regardless of the use of alternative cor-\nruption strategies in pre-training. Therefore, we\nsuggest to use only [MASK] for MLM pre-training.\nWe also present an analysis based on information\nﬂow (V oita et al., 2019) in Appendix G.\n8 Related Work\nMasking rates and masking strategies.There ex-\nist a few works on studying the impact of masking\nrates, among which Liao et al. (2020) show that\ndynamically sampling the masking rate from 0% to\n100% for each sequence can improve MLM’s down-\nstream performance as well as the ability as a gener-\nation model. On the other hand, masking strategies\nare heavily explored for both pre-training (Joshi\net al., 2020; Raffel et al., 2020; Levine et al., 2021)\nand intermediate pre-training (Ye et al., 2021) with-\nout considering the effect of masking rates.\n“Unrealistic” MLM training. A recent line of\nwork shows that linguistically implausible MLM\n2991\nobjectives can achieve competitive or non-trivial\ndownstream performance, e.g., training with shuf-\nﬂed word order (Sinha et al., 2021), with randomly\ngenerated sequences (Krishna et al., 2021), or pre-\ndicting only the ﬁrst character of masked tokens\n(Yamaguchi et al., 2021; Alajrami and Aletras,\n2022). These studies echo our ﬁndings that even\nan “unrealistical” high masking rate can still lead\nto good downstream results.\nMasking in other language models. Besides\nMLMs, there are other pre-training schemes,\nnamely autoregressive language models (Radford\net al., 2018; Brown et al., 2020) and sequence-\nto-sequence (seq2seq) language models (Raffel\net al., 2020; Lewis et al., 2020). Similar to MLMs,\nseq2seq models corrupt text with a masking rate,\nbut they predict with an autoregressive decoder and\nare ﬁne-tuned in different ways; Song et al. (2019)\nalso point out that masking rates control whether\nseq2seq models are closer to encoder-only MLMs\n(masking less) or decoder-only autoregressive LMs\n(masking more). Thus, we expect the masking rate\nstudies in seq2seq models to draw a different con-\nclusion from ours (Raffel et al., 2020; Tay et al.,\n2022b). Besides, Tay et al. (2022a) show that pre-\ntraining metrics are not correlated with downstream\nperformance, echoing our ﬁndings that perplexity\ndoes not correlate with ﬁne-tuning results.\nELECTRA (Clark et al., 2020) uses a smaller\nMLM to ﬁll in 15% of the blanks and trains a model\nto distinguish whether a token was generated by\nthe MLM or not. Despite the complicated training\nprocedure, the main motivation of ELECTRA is\nto improve the training efﬁciency by predicting\non 100% of tokens. Interestingly, we ﬁnd that the\ncorruption rate in ELECTRA becomes very low\ntowards the end of training—the average corruption\nrate is roughly only 7%, but the replacements are\n“hard” negatives generated by the smaller MLM.\nWe leave the study of its connection to corruption\nand prediction rates as future work.\nMasking in other modalities. Recently, a num-\nber of works extend MLM training to images\nand videos and demonstrate strong pre-training re-\nsults (He et al., 2022; Zhou et al., 2022; Feichten-\nhofer et al., 2022; Tong et al., 2022) . They adopt\nextremely high masking rates (e.g., 75% on images\nand 90% on videos) compared to their language\ncounterparts, with the argument that images and\nvideos are highly information redundant. Baevski\net al. (2020) propose a similar style masked model\nin speech and adopt a masking rate of around 50%.\n9 Conclusion & Discussion\nIn this work, we conduct a comprehensive study\non the masking rates of MLMs. We discover that\n15% is not universally optimal, and larger models\nshould adopt a higher masking rate. We also ﬁnd\nthat masking strategies should be considered to-\ngether with masking rates, and uniform masking\nneeds a higher masking rate than more sophisti-\ncated masking strategies. We gain a better under-\nstanding of masking rates by disentangling them as\ncorruption rates and prediction rates and analyze\nthe 80-10-10 corruption strategy that are widely\nused in BERT models. Based on our ﬁndings, we\ndiscuss the implications of high masking rates and\nfuture directions of efﬁcient MLM pre-training:\nImplications on higher masking rates. A direct\ntakeaway from our ﬁndings is that larger models\nmay adopt higher masking rates for better sample\nefﬁciency. Figure 1 shows that a large model with\n40% masking can achieve comparable results to a\n15% baseline on several tasks with half the training\ntime. Larger models also exhibit faster convergence\nfor a given computational budget: Li et al. (2020)\nsuggest it is more efﬁcient to train larger models\nfor fewer steps, as opposed to training smaller mod-\nels for longer. This can be combined with higher\nmasking rates for better sample efﬁciency.\nSeparating masked and unmasked tokens. The\ntraining efﬁciency can potentially beneﬁt from en-\ncoding masked and unmasked tokens separately,\nwhere masked tokens use a much lighter-weight\nmodule. If a high masking rate is taken, this can sig-\nniﬁcantly reduce the training cost due to the shorter\ninput to the encoder. A similar approach has been\nexplored by masked autoencoders in vision (He\net al., 2022), where 75% of the input patches are\nmasked and removed from the input of the heavy\nencoder to achieve a 4.1×speedup. Recently, Liao\net al. (2022) have applied these architectural im-\nprovements to natural language pre-training, and\ntogether with a high masking rate can accelerate\nMLM by a third of the pre-training budget.\nDisentangling corruption and prediction. Mod-\nels perform better when trained with lower corrup-\ntion rates and higher prediction rates. However, in\nstandard MLMs, those two factors are always tied\n2992\nto the masking rate. Methods which can encode\na sequence once and then efﬁciently predict many\nsmall sets of masks, for example by manipulating\nthe attention, could substantially accelerate masked\nlanguage modeling pre-training.\nLimitations\n(1) Our analysis of masking rates applies to a spe-\nciﬁc type of pre-training method, masked language\nmodeling. We are also interested in studying mask-\ning rates in other pre-trained methods, e.g., seq2seq\nmodels and ELECTRA, and leave it for future work.\n(2) While we have shown how the optimal masking\nrate depends on model size and masking strategy,\nthere may be additional factors, such as the vocab-\nulary size, pre-training corpus or language family.\nIn particular, our experiments focus on English, but\nlanguages with different structural and morphologi-\ncal features may have lower or even higher optimal\nmasking rates, or rely more on advanced masking\nstrategies. (3) We consider a well-established yet\nrelatively small set of downstream tasks, which\ndo not benchmark domain-speciﬁc knowledge or\nmore advanced reasoning skills. (4) Due to the ex-\npensive nature of our pre-training experiments, we\nwere not able to train multiple pre-trained models\nover multiple seeds. (5) Finally, our ﬁndings point\nout several promising directions but the paper pri-\nmarily aims to study and understandthe impact of\nmasking rates with respect to different factors. We\nleave exploring better architectures and methods\nfor efﬁcient pre-training to future work.\nEthical Considerations\nLarge language models can exhibit various kinds\nof stereotypes, as they capture societal biases en-\ncoded in the training data. These associations are\nnot detected by standard GLUE or SQuAD evalua-\ntion. We do not expect that simple modiﬁcations of\nmasking rates can make progress towards solving\nthese problems. Language model pre-training is\nalso computationally expensive, which comes at\na signiﬁcant environmental cost. Furthermore, it\nmakes re-production and follow-up research difﬁ-\ncult within an academic context. We reduce the\ncomputational requirements by following and pro-\nmoting an efﬁcient pre-training recipe and our ﬁnd-\nings point to future research for efﬁcient MLM.\nAcknowledgements\nWe thank Sadhika Malladi and the members of the\nPrinceton NLP group for helpful discussion and\nvaluable feedback. Alexander Wettig is supported\nby a Graduate Fellowship at Princeton University.\nThis work is also supported by a Google Research\nScholar Award.\nReferences\nAhmed Alajrami and Nikolaos Aletras. 2022. How\ndoes the pre-training objective affect what large lan-\nguage models learn about linguistic properties? In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 131–147, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A frame-\nwork for self-supervised learning of speech represen-\ntations. In Advances in Neural Information Process-\ning Systems (NeurIPS) , volume 33, pages 12449–\n12460.\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth PASCAL recognizing\ntextual entailment challenge. In TAC.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In the 11th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2017).\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations (ICLR).\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\n2993\nchallenge. In the First International Conference on\nMachine Learning Challenges: Evaluating Predic-\ntive Uncertainty Visual Object Classiﬁcation, and\nRecognizing Textual Entailment.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In North American Chapter of the Associ-\nation for Computational Linguistics (NAACL).\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn the Third International Workshop on Paraphras-\ning (IWP2005).\nChristoph Feichtenhofer, Haoqi Fan, Yanghao\nLi, and Kaiming He. 2022. Masked autoen-\ncoders as spatiotemporal learners. arXiv preprint\narXiv:2205.09113.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In the ACL-\nPASCAL Workshop on Textual Entailment and Para-\nphrasing.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\nPiotr Dollár, and Ross Girshick. 2022. Masked au-\ntoencoders are scalable vision learners. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 16000–\n16009.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-\nenhanced bert with disentangled attention. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nPeter Izsak, Moshe Berchansky, and Omer Levy. 2021.\nHow to train BERT with an academic budget. In\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 10644–10652.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion of Computational Linguistics (TACL), 8:64–77.\nKundan Krishna, Jeffrey Bigham, and Zachary C. Lip-\nton. 2021. Does pretraining for summarization re-\nquire knowledge transfer? In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021 ,\npages 3178–3189, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite bert for self-supervised learn-\ning of language representations. In International\nConference on Learning Representations (ICLR).\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and\nYoav Shoham. 2021. PMI-Masking: Principled\nmasking of correlated spans. In International Con-\nference on Learning Representations (ICLR).\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Association for Computa-\ntional Linguistics (ACL), pages 7871–7880.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joey Gonzalez. 2020.\nTrain big, then compress: Rethinking model size\nfor efﬁcient training and inference of transformers.\nIn International Conference on Machine Learning\n(ICML), pages 5958–5968.\nBaohao Liao, David Thulke, Sanjika Hewavitha-\nrana, Hermann Ney, and Christof Monz. 2022.\nMask more and mask later: Efﬁcient pre-training\nof masked language models by disentangling the\n[MASK] token. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , pages\n1478–1492, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nYi Liao, Xin Jiang, and Qun Liu. 2020. Probabilisti-\ncally masked language model capable of autoregres-\nsive generation in arbitrary word order. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 263–274, On-\nline. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013. Efﬁcient estimation of word rep-\nresentations in vector space. In ICLR.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53.\nJason Phang, Thibault Févry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport, OpenAI.\n2994\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text Trans-\nformer. The Journal of Machine Learning Research\n(JMLR), 21(140).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Empirical Meth-\nods in Natural Language Processing (EMNLP).\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of\nthe 26th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , page\n3505–3506.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Association for Computational\nLinguistics (ACL), pages 1715–1725.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2888–2913, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2019. MASS: Masked sequence\nto sequence pre-training for language generation.\nIn International Conference on Machine Learning\n(ICML), pages 5926–5936.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fe-\ndus, Samira Abnar, Hyung Won Chung, Sharan\nNarang, Dani Yogatama, Ashish Vaswani, and Don-\nald Metzler. 2022a. Scale efﬁciently: Insights from\npretraining and ﬁnetuning transformers. In Interna-\ntional Conference on Learning Representations.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022b. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\nZhan Tong, Yibing Song, Jue Wang, and Limin\nWang. 2022. VideoMAE: Masked autoencoders are\ndata-efﬁcient learners for self-supervised video pre-\ntraining. In Advances in Neural Information Pro-\ncessing Systems.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Empirical Methods\nin Natural Language Processing and International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. Blimp: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the As-\nsociation for Computational Linguistics, 8:377–392.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association of Computational\nLinguistics (TACL), 7.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT).\nAtsuki Yamaguchi, George Chrysostomou, Katerina\nMargatina, and Nikolaos Aletras. 2021. Frustrat-\ningly simple pretraining alternatives to masked lan-\nguage modeling. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3116–3125, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nQinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin\nBolte, Hao Ma, Wen-tau Yih, Xiang Ren, and Ma-\ndian Khabsa. 2021. On the inﬂuence of masking\npolicies in intermediate pre-training. In Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7190–7202.\n2995\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Ci-\nhang Xie, Alan Yuille, and Tao Kong. 2022. Image\nBERT pre-training with online tokenizer. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19–\n27.\n2996\nA Experiment Setup\nA.1 Pre-training\nWe implement our pre-training work based on\nfairseq (Ott et al., 2019). To further speed up\npre-training, we integrate the DeepSpeed (Rasley\net al., 2020) Transformer kernel for speedup.\nWe keep the other setting the same as the\n24hBERT (Izsak et al., 2021), except that we use\nthe RoBERTa tokenizer (Liu et al., 2019) and we\ndo not adopt the 80-10-10 rule. We train our model\non the English Wikipedia and BookCorpus (Zhu\net al., 2015). We want to emphasize that using pre-\nlayernorm (Shoeybi et al., 2019) is essential for the\nhigh learning rate in Izsak et al. (2021) to work.\nThe hyperparameters for the efﬁcient pre-training\nrecipe are shown in Table 5. We train with 8 Nvidia\nGTX 2080 GPUs and use gradient accumulation to\nachieve the large batch sizes.\nHyperparameter Efﬁcient pre-training recipe\nPeak learning rate 2e-3\nWarmup proportion 6%\nBatch size 4,096\nTraining steps 23,000\nSequence length 128\nArchitecture large\nTable 5: Our pre-training hyperparameter settings.\nA.2 Downstream Task Evaluation\nWe ﬁne-tune our model on the GLUE bench-\nmark (Wang et al., 2019), including SST-2 (Socher\net al., 2013), CoLA (Warstadt et al., 2019),\nMNLI (Williams et al., 2018), QNLI (Rajpurkar\net al., 2016), RTE (Dagan et al., 2005; Bar Haim\net al., 2006; Giampiccolo et al., 2007; Bentivogli\net al., 2009), MRPC (Dolan and Brockett, 2005),\nQQP7 and STS-B (Cer et al., 2017), and the\nSQuAD v1.1 (Rajpurkar et al., 2016) dataset. For\neach dataset we run three random seeds and aver-\nage the results. We apply grid search for the GLUE\ndatasets, as shown in Table 6. For SQuAD, we use\na learning rate of 1e-4, a batch size of 16, and train\nfor 2 epochs. For both GLUE and SQuAD we use\na linear scheduling for learning rates.\nFor all the results in the paper, we report ac-\ncuracy for MNLI, QNLI, RTE, SST-2; we report\nF1 score for QQP, MRPC, and SQuAD; we report\nMatthew’s correlation for CoLA and Spearman’s\ncorrelation for STS-B.\n7https://www.quora.com/q/quoradata/\nHyperparameter MNLI, QNLI, QQP\nPeak learning rate {5e-5, 8e-5}\nBatch size 32\nMax epochs {3, 5}\nRTE, SST-2, MRPC, CoLA, STS-B\nPeak learning rate {1e-5, 3e-5, 5e-5, 8e-5}\nBatch size {16, 32}\nMax epochs {3, 5, 10}\nTable 6: Grid search hyperparameters for GLUE tasks.\nFor the SQuAD results in Table 1 and Table 2,\nwe further train the models for 2300 steps (10% of\nthe training) with a sequence length of 512, a learn-\ning rate of 5e-4, and a warmup rate of 10%. For\nother tables and ﬁgures, we present the SQuAD re-\nsults without further pre-training, and the absolute\nnumbers are lower because of the short pre-training\nsequence length. For some of the ﬁgures in the\npaper, we only show the results of MNLI, QNLI,\nQQP, STS-B, SST-2, and SQuAD due to limited\nspace. Those tasks are selected because they have\nlarger training set and the results are more reliable.\nWe always show the development results in all our\nﬁgures and tables except Table 2, where we report\nthe test numbers for GLUE tasks.\nB Different Masking Rates: Full Results\nTable 7 shows the performance of 15%, 40%\nand 80% masked models on all GLUE tasks and\nSQuAD. We can see that 80% masking largely\npreserves the downstream performance and 40%\noutperforms 15% on most tasks.\nC Tokenizer Comparison\nTable 9 shows the performance of different tokeniz-\ners on downstream tasks. We see that on most tasks\nRoBERTa tokenizer is better than BERT tokenizer.\nMNLI-m/mm QNLI QQP RTE\nWordPieces 84.3/84.9 90.8 88.2 64.8\nBPE 84.5/84.8 91.6 88.1 67.0\nSST-2 MRPC CoLA STS-B\nWordPieces 92.5 75.5 56.6 88.7\nBPE 92.8 76.9 61.0 88.2\nTable 9: Comparison between BERT’s uncased Word-\nPieces tokenizer and RoBERTa’s BPE tokenizer. Both\nmodels are large and trained with the efﬁcient pre-\ntraining recipe with a 40% masking rate.\n2997\nMNLI-m/mm QNLI QQP RTE SST-2 MRPC CoLA STS-B SQuAD\nMasking 15% 84.2/84.6 90.9 87.8 67.3 93.3 77.0 59.2 87.7 88.0\nMasking 40% 84.5/84.8 91.6 88.1 67.0 92.8 76.9 61.0 88.2 89.8\nMasking 80% 80.8/81.0 87.9 87.1 58.6 90.5 72.1 38.7 86.3 86.2\nRandom initialization† 61.5/61.2 60.9 70.7 49.6 80.0 45.4 11.9 17.5 10.8\nTable 7: The development results on the GLUE benchmark with large models, the efﬁcient pre-training recipe,\nand with 15%, 40%, or 80% masking rates. The SQuAD development results are attained with the same contin-\nuous training as in Table 1. Compared to the random initialization model, 80% masking rates clearly learn good\nrepresentations for downstream tasks, despite having a very high perplexity. †: The random initialization models\nare trained with the same ﬁne-tuning hyperparameters as pre-trained models, thus they could be undertrained.\nMNLI-m/mm QNLI QQP RTE SST-2 MRPC CoLA STS-B SQuAD\nTrain longer with the efﬁcient pre-training recipe\nMasking 15% 87.47/87.02 92.95 88.40 69.93 94.07 82.50 61.00 88.89 87.29\nMasking 40% 86.63/86.83 93.13 88.40 68.87 94.67 79.50 61.23 89.60 87.16\nRecipe from RoBERTa\nMasking 15% 87.40/87.23 93.04 88.43 67.53 94.13 80.80 59.80 90.05 90.72\nMasking 40% 87.30/87.03 92.90 88.83 67.63 94.10 63.90 56.07 87.94 91.23\nTable 8: Development results of 15% vs 40% masking with larger pre-training budget. We use the recipe from\nTable 3 in Liu et al. (2019), and the efﬁcient pre-training recipe with more training steps. See Table 10 for hyper-\nparameters.\nD Longer Training\nHyperparameter Train longer RoBERTa\nPeak learning rate 2e-3 7e-4\nWarmup proportion 6% 6%\nBatch size 4,096 2,048\nTraining steps 125,000 125,000\nSequence length 128 512\nTable 10: Comparison between our longer pre-training\nrecipes and a recipe from RoBERTa (Liu et al., 2019).\nTo see that how the different masking rates per-\nform with longer training, we modify the efﬁcient\npre-training recipe for longer steps. We also experi-\nment with a recipe used in the RoBERTa paper (Liu\net al., 2019). Since the ﬁnal RoBERTa models use\nmore training data, we refer to the recipe used in\nRoBERTa’s ablation in its Table 3. Table 10 shows\nthe hyperparameters for the longer training, as well\nas a comparison to the RoBERTa’s recipe. The\nmajor difference is that we train with much larger\nlearning rate and only a sequence length of 128.\nWe train the models with 15% and 40% mask-\ning rates longer and evaluate them on downstream\ntasks. Figure 7 shows the results. We see that\non most of the tasks, the trend that 40% is better\nthan 15% still holds, though the 40% has a larger\nadvantage when the training steps are limited.\nWe also train the model using a recipe from\nRoBERTa and present the results in Table 8. We\nsee that (1) on most tasks 40% achieves compara-\nble results compared to 15%; (2) our “train longer”\nresults, which uses shorter sequences and larger\nlearning rates, are comparable to the RoBERTa\nrecipe results though with much shorter time.\nE Results of Different Model Sizes and\nMasking Strategies\nWe show the conﬁgurations of different model sizes\nin Table 11. Figure 8 and Figure 9 show the results\nof the base model and the medium model, which\nserve as complementary materials for Figure 2.\nFigure 10 shows the performance of uniform\nmasking, T5-style span masking, and PMI masking\non downstream tasks. This serves as a complemen-\ntary material for Figure 5.\nmedium base large\n#Layers 8 12 24\n#Attention heads 8 12 16\nHidden size 512 768 1024\nTable 11: Conﬁgurations of different model sizes.\n2998\n25k 50k 75k 100k 125k\nTraining Step\n85.0\n86.0\n87.0Performance (%)\nMNLI\n25k 50k 75k 100k 125k\nTraining Step\n90.0\n91.0\n92.0\n93.0\nQNLI\n25k 50k 75k 100k 125k\nTraining Step\n87.8\n88.0\n88.2\n88.5\nQQP\n25k 50k 75k 100k 125k\nTraining Step\n88.0\n88.8\n89.6\nSTS-B\n25k 50k 75k 100k 125k\nTraining Step\n92.8\n93.6\n94.4\nSST-2\n25k 50k 75k 100k 125k\nTraining Step\n82.0\n84.0\n86.0\nSQuAD\n15%\n40%\nFigure 7: 15% vs 40% masking rates with large models and the efﬁcient pre-training recipe but trained longer.\n20 30 40 50\nMasking Rate (%)\n81.6\n81.8\n82.0\n82.2Performance (%)\nMNLI\n20 30 40 50\nMasking Rate (%)\n88.8\n89.2\n89.6\nQNLI\n20 30 40 50\nMasking Rate (%)\n86.8\n87.0\n87.2\nQQP\n20 30 40 50\nMasking Rate (%)\n84.0\n84.8\n85.6\nSTS-B\n20 30 40 50\nMasking Rate (%)\n91.5\n92.0\n92.5\nSST-2\n20 30 40 50\nMasking Rate (%)\n79.2\n79.8\n80.4\n81.0\nSQuAD\nMask 15%\nFigure 8: Results on selected downstream tasks with the base (124M parameter) model.\n20 30 40 50\nMasking Rate (%)\n79.2\n79.4\n79.6\n79.8Performance (%)\nMNLI\n20 30 40 50\nMasking Rate (%)\n86.0\n86.4\n86.8\n87.2\nQNLI\n20 30 40 50\nMasking Rate (%)\n85.8\n85.9\n86.0\n86.1\nQQP\n20 30 40 50\nMasking Rate (%)\n82.4\n83.2\n84.0\n84.8\nSTS-B\n20 30 40 50\nMasking Rate (%)\n90.0\n90.6\n91.2\nSST-2\n20 30 40 50\nMasking Rate (%)\n74.5\n75.0\n75.5\nSQuAD\nMask 15%\nFigure 9: Results on selected downstream tasks with the medium (51M parameter) model.\n15 20 30 40\nMasking Rate (%)\n83.6\n84.0\n84.4\n84.8Performance (%)\nMNLI\n15 20 30 40\nMasking Rate (%)\n91.0\n91.5\n92.0\nQNLI\n15 20 30 40\nMasking Rate (%)\n87.8\n88.0\n88.2\n88.5\nQQP\n15 20 30 40\nMasking Rate (%)\n87.5\n88.0\n88.5\n89.0\nSTS-B\n15 20 30 40\nMasking Rate (%)\n91.0\n92.0\n93.0\nSST-2\n15 20 30 40\nMasking Rate (%)\n83.4\n84.0\n84.6\n85.2\nSQuAD\nUniform\nSpan\nPMI\nFigure 10: Comparison of different masking strategies on selected tasks. Models are trained with the efﬁcient\npre-training recipe, the large conﬁguration, and several masking rates.\nF Results on French MLM\nTo validate our conclusions in a new setting, we\nconduct experiments on MLM on a corpus in\nFrench. Similar to Izsak et al. (2021), we pre-\ntrain on 2020 French Wikipedia and ﬁne-tuned on\nFrench XNLI. We report accuracy averaged over 4\nseeds, and make the observation that 40% is better\nthan 15%.\nXNLI-fr\nvalid test\nMasking 15% 78.3 77.3\nMasking 40% 78.9 77.5\nTable 12: We pre-train on 2020 French Wikipedia and\nﬁne-tuned on French XNLI. We report accuracy aver-\naged over 4 seeds.\n0 10 20\nLayer\n3.6\n3.9\n4.2\n4.5\n4.8Mutual Information\nMI(Layer; Src. Token)\n40% mask\n  +5% same\n  w/ 5% rand\n  w/ 80-10-10\nFigure 11: Mutual information between an input token\nand its intermediate representations for four different\ncorruption strategies. See Table 4 for details on models.\nG Information Flow Analysis\nTo visualize the effect of these corruption strategies\n(the 80-10-10 rule), we follow V oita et al. (2019)’s\n2999\nanalysis of measuring mutual information between\nan input token and its intermediate representations.\nFigure 11 shows that each model initially loses\nsome information about the source token while\nacquiring information from the surrounding con-\ntext. Using same token predictions during pre-\ntraining leads to a “reconstruction” stage in the\nlast few layers, as observed by V oita et al. (2019),\nwhereby information about the source token is re-\nstored from the context. However, this second stage\nis not present when same token predictions tokens\nare ablated: the [MASK]-only baseline propagates\ncontextual features only—and no reconstruction\noccurs. This is more pronounced with random to-\nken corruption, where source information (that was\nless reliable during pre-training) is lost at a greater\nrate. One consequence is that information about\nthe input tokens can be more easily extracted when\npre-training with same token predictions. However,\nthe reconstruction of the source tokens does not\nappear to be as important in the ﬁne-tuning setting,\nas shown in our experiments in Table 4.\n3000",
  "topic": "Masking (illustration)",
  "concepts": [
    {
      "name": "Masking (illustration)",
      "score": 0.923551082611084
    },
    {
      "name": "Computer science",
      "score": 0.7383036613464355
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5766932964324951
    },
    {
      "name": "Speech recognition",
      "score": 0.48562827706336975
    },
    {
      "name": "Backward masking",
      "score": 0.4738790690898895
    },
    {
      "name": "Auditory masking",
      "score": 0.4225679337978363
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4036492705345154
    },
    {
      "name": "Psychology",
      "score": 0.10244283080101013
    },
    {
      "name": "Biology",
      "score": 0.05415588617324829
    },
    {
      "name": "Perception",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}