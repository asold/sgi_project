{
  "title": "Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs.",
  "url": "https://openalex.org/W3191962800",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2782952679",
      "name": "Md. Shamim Hussain",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2165917828",
      "name": "Mohammed J. Zaki",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2158793934",
      "name": "Dharmashankar Subramanian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2156718197",
    "https://openalex.org/W2951418500",
    "https://openalex.org/W2989226908",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2977957659",
    "https://openalex.org/W3096487860",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1908728294",
    "https://openalex.org/W2963460103",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3035649237",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W3098382540",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2118246710",
    "https://openalex.org/W3159727442",
    "https://openalex.org/W3136399186",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W3007332492",
    "https://openalex.org/W2768242641",
    "https://openalex.org/W3004136411",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W1501856433",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2952575904",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3110111880",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W2162630660",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3132041002",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3016124664",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2962900880",
    "https://openalex.org/W2176412452",
    "https://openalex.org/W2559839022",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W3042313988",
    "https://openalex.org/W3113177135"
  ],
  "abstract": "Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative.",
  "full_text": "Global Self-Attention as a Replacement for Graph Convolution\nMd Shamim Hussain\nhussam4@rpi.edu\nRensselaer Polytechnic Institute\nTroy, New York, USA\nMohammed J. Zaki\nzaki@cs.rpi.edu\nRensselaer Polytechnic Institute\nTroy, New York, USA\nDharmashankar Subramanian\ndharmash@us.ibm.com\nIBM T. J. Watson Research Center\nYorktown Heights, New York, USA\nABSTRACT\nWe propose an extension to the transformer neural network archi-\ntecture for general-purpose graph learning by adding a dedicated\npathway for pairwise structural information, called edge chan-\nnels. The resultant framework â€“ which we call Edge-augmented\nGraph Transformer (EGT) â€“ can directly accept, process and out-\nput structural information of arbitrary form, which is important\nfor effective learning on graph-structured data. Our model exclu-\nsively uses global self-attention as an aggregation mechanism rather\nthan static localized convolutional aggregation. This allows for\nunconstrained long-range dynamic interactions between nodes.\nMoreover, the edge channels allow the structural information to\nevolve from layer to layer, and prediction tasks on edges/links\ncan be performed directly from the output embeddings of these\nchannels. We verify the performance of EGT in a wide range of\ngraph-learning experiments on benchmark datasets, in which it\noutperforms Convolutional/Message-Passing Graph Neural Net-\nworks. EGT sets a new state-of-the-art for the quantum-chemical\nregression task on the OGB-LSC PCQM4Mv2 dataset containing\n3.8 million molecular graphs. Our findings indicate that global self-\nattention based aggregation can serve as a flexible, adaptive and\neffective replacement of graph convolution for general-purpose\ngraph learning. Therefore, convolutional local neighborhood ag-\ngregation is not an essential inductive bias.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Neural networks; Artificial in-\ntelligence.\nKEYWORDS\ngraph neural networks, graph representation learning, self-attention\nACM Reference Format:\nMd Shamim Hussain, Mohammed J. Zaki, and Dharmashankar Subramanian.\n2022. Global Self-Attention as a Replacement for Graph Convolution. In\nProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining (KDD â€™22), August 14â€“18, 2022, Washington, DC, USA. ACM,\nNew York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539296\n1 INTRODUCTION\nGraph-structured data are ubiquitous in different areas such as\ncommunication networks, molecular structures, citation networks,\nknowledge bases and social networks. Due to the flexibility of the\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nÂ© 2022 Association for Computing Machinery.\nThis is the authorâ€™s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of\nthe 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™22),\nAugust 14â€“18, 2022, Washington, DC, USA , https://doi.org/10.1145/3534678.3539296.\nstructural information in graphs, they are powerful tools for com-\npact and intuitive representation of data originating from a very\nwide range of sources. However, this flexibility comes at the cost of\nadded complexity in processing and learning from graph-structured\ndata, due to the arbitrary nature of the interconnectivity of the\nnodes. Recently the go-to solution for deep representation learn-\ning on graphs has been Graph Neural Networks (GNNs) [17, 34].\nThe most commonly used GNNs follow a convolutional pattern\nwhereby each node in the graph updates its state based on that of\nits neighbors [24, 42] in each layer. On the other hand, the pure\nself-attention based transformer architecture [ 38] has displaced\nconvolutional neural networks for more regularly arranged data,\nsuch as sequential (e.g., text) and grid-like (images) data, to become\nthe new state-of-the-art, especially in large-scale learning. Trans-\nformers have become the de-facto standard in the field of natural\nlanguage processing, where they have achieved great success in\na wide range of tasks such as language understanding, machine\ntranslation and question answering. The success of transformers\nhas translated to other forms of unstructured data in different do-\nmains such as audio [8, 28] and images [7, 13] and also on different\n(classification/generation, supervised/unsupervised) tasks.\nTransformers differ from convolutional neural networks in some\nimportant ways. A convolutional layer aggregates a localized win-\ndow around each position to produce an output for that position.\nThe weights that are applied to the window are independent of\nthe input, and can therefore be termed as static. Also, the slid-\ning/moving window directly follows the structure of the input data,\ni.e., the sequential or grid-like pattern of positions. This is an apri-\nori assumption based on the nature of the data and how it should\nbe processed, directly inspired by the filtering process in signal\nprocessing. We call this assumption theconvolutional inductive bias .\nOn the other hand, in the case of a transformer encoder layer, the\ninternal arrangement of the data does not directly dictate how it is\nprocessed. Attention weights are formed based on the queries and\nthe keys formed at each position, which in turn dictate how each\nposition aggregates other positions. The aggregation pattern is thus\nglobal and input dependent, i.e., it is dynamic. The positional infor-\nmation is treated as an input to the network in the form of positional\nencodings. In their absence, the transformer encoder is permutation\nequivariant and treats the input as a multiset. Information is propa-\ngated among different positions only via the global self-attention\nmechanism, which is agnostic to the internal arrangement of the\ndata. Due to this property of global self-attention, distant points\nin the data can interact with each other as efficiently as nearby\npoints. Also, the network learns to form appropriate aggregation\npatterns during the training process, rather than being constrained\nto a predetermined pattern.\nAlthough it is often straightforward to adopt the transformer\narchitecture for regularly structured data such as text and images by\narXiv:2108.03348v3  [cs.LG]  3 Jun 2022\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Hussain, Zaki and Subramanian\nFigure 1: A conceptual demonstration of Graph Convolu-\ntion (left) and Global Self-Attention (right). It takes three\nstages of convolution for node 0 to aggregate node 6. With\nglobal self-attention, the model can learn to do so in a sin-\ngle step. The attention heads are formed dynamically for a\ngiven graph.\nemploying an appropriate positional encoding scheme, the highly\narbitrary nature of structure in graphs makes it difficult to represent\nthe position of each node only in terms of positional encodings.\nAlso, it is not clear how edge features can be incorporated in terms\nof node embeddings. For graph-structured data, the edge/structural\ninformation can be just as important as the node information, and\nthus we should expect the network to process this information\nhierarchically, just like the node embeddings. To facilitate this, we\nintroduce a new addition to the transformer, namely residual edge\nchannels â€“ a pathway that can leverage structural information. This\nis a simple yet powerful extension to the transformer framework in\nthat it allows the network to directly process graph-structured data.\nThis addition is also very general in the sense that it facilitates the\ninput of structural information of arbitrary form, including edge fea-\ntures, and can handle different variants of graphs such as directed\nand weighted graphs in a systematic manner. Our framework can\nexceed the results of widely used Graph Convolutional Networks\non datasets of moderate to large sizes, in supervised benchmarking\ntasks while maintaining a similar number of parameters. But our\narchitecture deviates significantly from convolutional networks in\nthat it does not impose any strong inductive bias such as the convo-\nlutional bias, on the feature aggregation process. We rely solely on\nthe global self-attention mechanism to learn how best to use the\nstructural information, rather than constraining it to a fixed pattern.\nAdditionally, the structural information can evolve over layers and\nthe network can potentially form new structures. Any prediction\non the structure of the graph, such as link prediction or edge clas-\nsification, can be done directly from the outputs of edge channels.\nHowever, these channels do add to the quadratic computational\nand memory complexity of global self-attention, with respect to\nthe number of nodes, which restricts us to moderately large graphs.\nIn addition to the edge channels, we generalize GNN concepts like\ngated aggregation [4], degree scalers [12] and positional encodings\n[15] for our framework.\nOur experimental results indicate that given enough data and\nwith the proposed edge channels, the model can utilize global self-\nattention to learn the best aggregation pattern for the task at hand.\nThus, our results indicate that following a fixed convolutional ag-\ngregation pattern whereby each node is limited to aggregating its\nclosest neighbors (based on adjacency, distance, intimacy, etc.) is\nnot an essential inductive bias. With the flexibility of global self-\nattention, the network can learn to aggregate distant parts of the\ninput graph in just one step as illustrated in Fig. 1. Since this pattern\nis learned rather than being imposed by design, it increases the\nexpressivity of the model. Also, this aggregation pattern is dynamic\nand can adapt to each specific input graph. Similar findings have\nbeen reported for unstructured data such as images [ 11, 13, 32].\nSome recent works have reported global self-attention as a means\nfor better generalization or performance by improving the expres-\nsivity of graph convolutions [31, 40]. Very recently, Graphormer\n[43] performed well on graph level prediction tasks on molecular\ngraphs by incorporating edges with specialized encodings. How-\never, it does not directly process the edge information and therefore\ndoes not generalize well to edge-related prediction tasks. By in-\ncorporating the edge-channels, we are the first to propose global\nself-attention as a direct and general replacement for graph convo-\nlution for node-level, link(edge)-level and graph-level prediction,\non all types of graphs.\n2 RELATED WORK\nIn relation to our work, we discuss self-attention based GNN mod-\nels, where the attention mechanism is either constrained to a local\nneighborhood (local self-attention) of each node or unconstrained\nover the whole input graph (global self-attention). Methods like\nGraph Attention Network (GAT) [39] and Graph Transformer (GT)\n[14] constrain the self-attention mechanism to local neighborhoods\nof each node only, which is reminiscent of the graph convolu-\ntion/local message-passing process. Several works have attempted\nto adopt the global self-attention mechanism for graphs as well.\nGraph-BERT [45] uses a modified transformer framework on a\nsampled linkless subgraph (i.e., only node representations are pro-\ncessed) around a target node. Since the nodes do not inherently\nbear information about their interconnectivity, Graph-BERT uses\nseveral types of relative positional encodings to embed the infor-\nmation about the edges within a subgraph. Graph-BERT focuses\non unsupervised representation learning by training the model to\npredict a single masked node in a sampled subgraph. GROVER [33]\nused a modified transformer architecture with queries, keys and\nvalues produced by Message-Passing Networks, which indirectly\nincorporate the input structural information. This framework was\nused to perform unsupervised learning on molecular graphs only.\nGraph Transformer [6] and Graphormer [ 43] directly adopt the\ntransformer framework for specific tasks. Graph Transformer sepa-\nrately encodes the nodes and the relations between nodes to form a\nfully connected view of the graph which is incorporated into a trans-\nformer encoder-decoder framework for graph-to-sequence learning.\nGraphormer incorporates the existing structure/edges in the graph\nas an attention bias, formed from the shortest paths between pairs\nof nodes. It focuses on graph-level prediction tasks on molecular\ngraphs (e.g., classification/regression on molecular graphs). Unlike\nthese models which handle graph structure in an ad-hoc manner\nand only for a specific problem, we directly incorporate graph struc-\nture into the transformer model via the edge channels and propose\na general-purpose learning framework for graphs based only on the\nglobal self-attention mechanism, free of the strong inductive bias of\nconvolution. Apart from being used for node feature aggregation,\nattention has also been used to form metapaths in heterogeneous\ngraphs, such as the Heterogeneous Graph Transformer (HGT) [22]\nGlobal Self-Attention as a Replacement for Graph Convolution KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nand the Graph Transformer network (GTN) [44]. However, these\nworks are orthogonal to ours since metapaths are only relevant in\nthe case of heterogeneous graphs and these methods use attention\nspecifically to combine heterogeneous edges, over multiple hops.\nWe focus only on homogeneous graphs, but more importantly, we\nuse attention as a global aggregation mechanism.\n3 NETWORK ARCHITECTURE\n3.1 Preliminaries\nThe transformer architecture was proposed by Vaswani et al. [38] as\na purely attention-based model. The transformer encoder uses self-\nattention to communicate information between different positions,\nand thus produces the output embeddings for each position. In\nthe absence of positional encodings, this process is permutation\nequivariant and treats the input embeddings as a multiset.\nEach layer in the transformer encoder consists of two sublay-\ners. The key component of the transformer is the multihead self-\nattention mechanism which takes place in the first sublayer, which\ncan be expressed as:\nAttn(Q,K,V)= ËœAV (1)\nWhere, ËœA = softmax\n \nQKğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n!\n(2)\nwhere Q,K,V are the keys, queries and values formed by learned\nlinear transformations of the embeddings and ğ‘‘ğ‘˜ is the dimen-\nsionality of the queries and the keys. ËœA is known as the (softmax)\nattention matrix, formed from the scaled dot product of queries and\nkeys. This process is done for multiple sets of queries, keys and val-\nues, hence the name multihead self-attention. The second sublayer\nis the feedforward layer which serves as a pointwise non-linear\ntransformation of the embeddings.\n3.2 Edge-augmented Graph Transformer (EGT)\nThe EGT architecture (Fig. 2) extends the original transformer archi-\ntecture. The permutation equivariance of the transformer is ideal\nfor processing the node embeddings in a graph because a graph is\ninvariant under the permutation of the nodes, given that the edges\nare preserved. We call the residual channels present in the original\ntransformer architecture node channels . These channels transform\na set of input node embeddings {â„0\n1,â„0\n2,...,â„ 0\nğ‘}into a set of output\nnode embeddings (â„ğ¿\nğ‘– )ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ (for 1 â‰¤ğ‘– â‰¤ğ‘), where â„â„“\nğ‘– âˆˆRğ‘‘â„, ğ‘‘â„\nis the node embeddings dimensionality, ğ‘ is the number of nodes,\nand ğ¿is the number of layers. Our contribution to the transformer\narchitecture is the introduction of edge channels , which start with\nan embedding for each pair of nodes . Thus, there are ğ‘ Ã—ğ‘ input\nedge embeddings ğ‘’0\n11,ğ‘’0\n12,...,ğ‘’ 0\n1ğ‘,ğ‘’0\n21,...,ğ‘’ 0\nğ‘ğ‘ where, ğ‘’ğ‘™\nğ‘–ğ‘— âˆˆRğ‘‘ğ‘’, ğ‘‘ğ‘’\nis the edge embeddings dimensionality. The input edge embeddings\nare formed from graph structural matrices and edge features. We\ndefine a graph structural matrix as any matrix with dimensionality\nğ‘ Ã—ğ‘, which can completely or partially define the structure of a\ngraph (e.g., adjacency matrix, distance matrix). The edge embed-\ndings are updated by EGT in each layer and finally, it produces a set\nof output edge embeddings (ğ‘’ğ¿\nğ‘–ğ‘—)ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ (for 1 â‰¤ğ‘–,ğ‘— â‰¤ğ‘) from which\nstructural predictions such as edge labeling and link prediction can\nbe performed.\nFigure 2: Edge-augmented Graph Transformer (EGT)\nFrom equations (1) and (2) we see that the attention matrix is\ncomparable with a row-normalized adjacency matrix of a directed\nweighted complete graph. It dictates how the node features in a\ngraph are aggregated, similarly to GCN [24]. Unlike the input graph,\nthis graph is dynamically formed by the attention mechanism. How-\never, the basic transformer does not have a direct way to incorporate\nthe input structure (existing edges) while forming these weighted\ngraphs, i.e., the attention matrices. Also, these dynamic graphs are\ncollapsed immediately after the aggregation process is done. To\nremedy the first problem we let the edge channels participate in\nthe aggregation process as follows (as shown in Fig. 2) â€“ in the â„“â€™th\nlayer and for the ğ‘˜â€™th attention head,\nAttn(Qğ‘˜,â„“\nâ„ ,Kğ‘˜,â„“\nâ„ ,Vğ‘˜,â„“\nâ„ )= ËœAğ‘˜,â„“Vğ‘˜,â„“\nâ„ (3)\nWhere, ËœAğ‘˜,â„“ = softmax(Ë†Hğ‘˜,â„“)âŠ™ ğœ(Gğ‘˜,â„“\nğ‘’ ) (4)\nWhere, Ë†Hğ‘˜,â„“ = clip Â©Â­\nÂ«\nQğ‘˜,â„“\nâ„ (Kğ‘˜,â„“\nâ„ )ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\nÂªÂ®\nÂ¬\n+Eğ‘˜,â„“\nğ‘’ (5)\nwhere âŠ™denotes elementwise product. Eğ‘˜,â„“\nğ‘’ ,Gğ‘˜,â„“\nğ‘’ âˆˆRğ‘Ã—ğ‘ are con-\ncatenations of the learned linear transformed edge embeddings.\nEğ‘˜,â„“\nğ‘’ is a bias term added to the scaled dot product between the\nqueries and the keys. It lets the edge channels influence the atten-\ntion process. Gğ‘˜,â„“\nğ‘’ drives the sigmoidğœ(Â·)function and lets the edge\nchannels also gate the values before aggregation, thus controlling\nthe flow of information between nodes. The scaled dot product is\nclipped to a limited range which leads to better numerical stability\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Hussain, Zaki and Subramanian\n(we used [âˆ’5,+5]). To ensure that the network takes advantage of\nthe full-connectivity the attention process is randomly masked by\nadding âˆ’âˆto the inputs to the softmax with a small probability\nduring training (i.e., random attention masking). Another approach\nis to apply dropout [37] to the attention matrix.\nTo let the structural information evolve from layer to layer, the\nedge embeddings are updated by a learnable linear transformation\nof the inputs to the softmax function. The outputs of the atten-\ntion heads are also mixed by a linear transformation. To facilitate\ntraining deep networks, Layer Normalization (LN) [1] and residual\nconnections [19] are used. We adopted the Pre-Norm architecture\nwhereby normalization is done immediately before the weighted\nsublayers [41] rather than after, because of its better optimization\ncharacteristics. So, Ë†â„â„“\nğ‘– = LN(â„â„“âˆ’1\nğ‘– ), Ë†ğ‘’â„“\nğ‘–ğ‘— = LN(ğ‘’â„“âˆ’1\nğ‘–ğ‘— ). The residual\nupdates can be expressed in an elementwise manner as:\nË†Ë†â„â„“\nğ‘– = â„â„“âˆ’1\nğ‘– +Oâ„“\nâ„\n\r\rğ»\nğ‘˜=1\nğ‘âˆ‘ï¸\nğ‘—=1\nËœAğ‘˜,â„“\nğ‘–ğ‘— (Vğ‘˜,â„“ Ë†â„â„“\nğ‘–) (6)\nË†Ë†ğ‘’â„“\nğ‘–ğ‘— = ğ‘’â„“âˆ’1\nğ‘–ğ‘— +Oâ„“\nğ‘’\n\r\rğ»\nğ‘˜=1 Ë†Hğ‘˜,â„“\nğ‘–ğ‘— (7)\nHere, âˆ¥denotes concatenation. Oâ„“\nâ„ âˆˆRğ‘‘â„Ã—ğ‘‘â„ and Oâ„“ğ‘’ âˆˆRğ‘‘ğ‘’Ã—ğ»\nare the learned output projection matrices, with edge embeddings\ndimensionality ğ‘‘ğ‘’ and ğ» attention heads.\nThe feed-forward sublayer following the attention sublayer con-\nsists of two consecutive pointwise fully connected linear layers\nwith a non-linearity such as ELU [10] in between. The updated em-\nbeddings are â„â„“\nğ‘– = Ë†Ë†â„â„“\nğ‘– +FFNâ„“\nh(LN(Ë†Ë†â„â„“\nğ‘–)), ğ‘’â„“\nğ‘– = Ë†Ë†ğ‘’â„“\nğ‘– +FFNâ„“e(LN(Ë†Ë†ğ‘’â„“\nğ‘–)). The\nPre-Norm architecture also ends with a layer normalization over\nthe final embeddings as (â„ğ¿\nğ‘– )ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ = LN(â„ğ¿\nğ‘– ), (ğ‘’ğ¿\nğ‘–ğ‘—)ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ = LN(ğ‘’ğ¿\nğ‘–ğ‘—).\n3.3 Dynamic Centrality Scalers\nThe attention mechanism in equation (3) is a weighted average\nof the gated node values, which is agnostic to the degree of the\nnodes. However, we may want to make the network sensitive to the\ndegree/centrality of the nodes, in order to make it more expressive\nwhen distinguishing between non-isomorphic (sub-)graphs, similar\nto GIN [42]. While this can be achieved by directly encoding the\ndegrees of the nodes as an additional input like [43], we aimed for\nan approach that is adaptive to the dynamic nature of self-attention.\nCorso et al. [12] propose scaling the aggregated values by a function\nof the degree of the node, more specifically a logarithmic degree\nscaler. But it is tricky to form a notion of degree/centrality for\nthe dynamically formed graph represented by the attention matrix\nbecause this row-normalized matrix bears no notion of degree. In\nour network, the sigmoid gates control the flow of information to a\nparticular node which are derived from the edge embeddings. So\nwe use the sum of the sigmoid gates as a measure of centrality for a\nnode and scale the aggregated values by the logarithm of this sum.\nWith centrality scalers, equation (6) becomes:\nË†Ë†â„â„“\nğ‘– = â„â„“âˆ’1\nğ‘– +Oâ„“\nâ„\n\r\rğ»\nğ‘˜=1 ğ‘ ğ‘˜,ğ‘™\nğ‘–\nğ‘âˆ‘ï¸\nğ‘—=1\nËœAğ‘˜,â„“\nğ‘–ğ‘— (Vğ‘˜,â„“ Ë†â„â„“\nğ‘–) (8)\nWhere, ğ‘ ğ‘˜,ğ‘™\nğ‘– = ln Â©Â­\nÂ«\n1 +\nğ‘âˆ‘ï¸\nğ‘—=1\nğœ(Gğ‘˜,â„“eğ‘–,ğ‘—)ÂªÂ®\nÂ¬\n(9)\nHere, ğ‘ ğ‘˜,ğ‘™\nğ‘– is the centrality scaler for node ğ‘–, for attention head ğ‘˜ at\nlayer â„“. As pointed out by Ying et al . [43], with the addition of a\ncentrality measure the global self-attention mechanism becomes at\nleast as powerful as the 1-Weisfeiler-Lehman (1-WL) isomorphism\ntest and potentially even more so, due to aggregation over multiple\nhops. Note that commonly used convolutional GNNs like GIN are\nat most as powerful as the 1-WL isomorphism test [42].\n3.4 SVD-based Positional Encodings\nWhile applying the transformer on regularly arranged data such as\nsequential (e.g., text) and grid-like (e.g., images) data it is customary\nto use sinusoidal positional encodings introduced by Vaswani et al.\n[38]. However, the arbitrary nature of structure in graphs makes it\ndifficult to devise a consistent positional encoding scheme. Nonethe-\nless, positional encodings have been used for GNNs to embed global\npositional information within individual nodes and to distinguish\nisomorphic nodes and edges [29, 36]. Inspired by matrix factoriza-\ntion based node embedding methods for graphs [3], Dwivedi et al.\n[15] proposed to use the ğ‘˜ smallest non-trivial eigenvectors of the\nLaplacian matrix of the graph as positional encodings. However,\nsince the Laplacian eigenvectors can be complex-valued for directed\ngraphs, this method is more relevant for undirected graphs which\nhave symmetric Laplacian matrices. To remedy this we propose a\nmethod, that is more general and applies to all variants of graphs\n(e.g., directed, weighted). We propose a form of positional encoding\nbased on precalculated SVD of the graph structural matrices. We\nuse the largest ğ‘Ÿ singular values and corresponding left and right\nsingular vectors to form our positional encodings. We use the adja-\ncency matrix A (with self-loops) as the graph structural matrix, but\nit can be generalized to other structural matrices since the SVD of\nany real matrix produces real singular values and vectors.\nA\nSVD\nâ‰ˆ UÎ£Vğ‘‡ = (U\nâˆš\nÎ£)Â·( V\nâˆš\nÎ£)ğ‘‡ = Ë†U Ë†Vğ‘‡ (10)\nË†Î“ = Ë†U âˆ¥Ë†V (11)\nWhere U,V âˆˆRğ‘Ã—ğ‘Ÿ matrices contain the ğ‘Ÿ left and right singu-\nlar vectors as columns, respectively, corresponding to the top ğ‘Ÿ\nsingular values in the diagonal matrix Î£ âˆˆRğ‘ŸÃ—ğ‘Ÿ. Here, âˆ¥denotes\nconcatenation along the columns. From (10) we see that the dot\nproduct between ğ‘–â€™th row of Ë†U and ğ‘—â€™th row of Ë†V can approximate\nAğ‘–ğ‘— which denotes whether there is an edge between nodes ğ‘–and ğ‘—.\nThus, the rows of Ë†Î“, namely Ë†ğ›¾1,Ë†ğ›¾2,..., Ë†ğ›¾ğ‘, each with dimensionality\nË†ğ›¾ğ‘– âˆˆR2ğ‘Ÿ, bear denoised information about the edges and can be\nused as positional encodings. Note that this form of representation\nbased on the dot product is consistent with the scaled dot product\nattention used in the transformer framework. Since the signs of\ncorresponding pairs of left and right singular vectors can be ar-\nbitrarily flipped, we randomly flip the signs of Ë†ğ›¾ğ‘– during training\nfor better generalization. Instead of directly adding Ë†ğ›¾ğ‘– to the input\nembeddings of the node ğ‘–, we add a projection ğ›¾ğ‘– = Wğ‘’ğ‘›ğ‘Ë†ğ›¾ğ‘–, where\nWğ‘’ğ‘›ğ‘ âˆˆRğ‘‘â„Ã—2ğ‘Ÿ is a learned projection matrix. This heuristically\nleads to better results. Since our architecture directly takes structure\nas input via the edge channels, the inclusion of positional encodings\nis optional for most tasks. However, positional encodings can help\ndistinguish isomorphic nodes [46] by serving as an absolute global\ncoordinate system. Thus, they make the model more expressive.\nGlobal Self-Attention as a Replacement for Graph Convolution KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nHowever, the absolute coordinates may, in theory, hamper gener-\nalization, because they are specific to a particular reference frame\nthat depends on the input graph. But in practice, we did not find\nany detrimental effect on the performance for any task.\n3.5 Embedding and Prediction\nGiven an input graph, both node and edge feature embeddings are\nformed by performing learnable linear transformations for contin-\nuous vector values, or vector embeddings for categorical/discrete\nvalues. In the case of multiple sets of features, their correspond-\ning embeddings are added together. When positional encodings ğ›¾ğ‘–\nare used, they are added to the input node embeddings. The edge\nembeddings are formed by adding together the embeddings from\nthe graph structural matrix and the input edge feature embeddings\n(when present). For non-existing edges, a masking value/vector is\nused in the place of an edge feature. As input structural matrix,\nwe use the distance matrix clipped up to ğ‘˜-hop distance, i.e., D(ğ‘˜)\nwhere D(ğ‘˜)\nğ‘–ğ‘— âˆˆ{0,1,...,ğ‘˜ }are the shortest distances between nodesğ‘–\nand ğ‘—, clipped to a maximum value of ğ‘˜. We use vector embeddings\nof the discrete values contained in these matrices.\nFor node and edge classification/regression tasks, we apply a\nfew final MLP layers on the final node and edge embeddings, re-\nspectively, to produce the output. For graph-level classification/reg-\nression we adopt one of two different methods. In global average\npooling method, all the output node embeddings are averaged to\nform a graph-level embedding, on which final linear layers are ap-\nplied. In virtual nodes method, ğ‘new virtual nodes with learnable\ninput embeddings â„0\nğ‘+1,â„0\nğ‘+2,...,â„ 0\nğ‘+ğ‘ are passed through EGT\nalong with existing node embeddings. There are also ğ‘ different\nlearnable edge embeddings Ëœğ‘’ğ‘– which are used as follows â€“ the edge\nembedding between a virtual node ğ‘– and existing graph node ğ‘— is\nassigned ğ‘’0\nğ‘–ğ‘— = ğ‘’0\nğ‘—ğ‘– = Ëœğ‘’ğ‘–, and the edge embeddings between two\nvirtual nodes ğ‘–,ğ‘— , are assigned ğ‘’0\nğ‘–ğ‘— = ğ‘’0\nğ‘—ğ‘– = 1\n2 (Ëœğ‘’ğ‘– +Ëœğ‘’ğ‘—). Finally, the\ngraph embedding is formed by concatenating the output node em-\nbeddings of the virtual nodes. This method is more flexible and\nbetter suited for larger models. The centrality scalers mentioned\nabove are not applied to the virtual nodes, because by nature these\nnodes have high levels of centrality which are very different from\nthe graph nodes. So a fixed scaler value of ğ‘ ğ‘˜,ğ‘™\nğ‘– = 1 is used instead\nfor these virtual nodes.\nFor smaller datasets, we found that adding a secondary distance\nprediction objective alongside the graph-level prediction task in a\nmulti-task learning setting serves as an effective means of regular-\nization and thus improves the generalization of the trained model.\nThis self-supervised objective is reminiscent of the unsupervised\nlink prediction objective often used to pre-train GNNs to form node\nembeddings. In our case, we take advantage of the fact that we\nhave output edge embeddings from the edge channels (alongside\nthe node embeddings, which are used for graph-level prediction).\nWe thus pass the output edge embeddings through a few (we used\nthree) MLP layers and set the distance matrix up to ğœˆ-hop, D(ğœˆ), as\na categorical target. Hops greater than ğœˆ are ignored while calculat-\ning the loss. The loss from this secondary objective is multiplied by\na small factorğœ…and added to the total loss. Note that in this case we\nalways use the adjacency matrix, rather than the distance matrix as\nthe input graph structural matrix so that the edge channels do not\nsimply learn an identity transformation. We emphasize that this\nobjective is only potentially beneficial as a regularization method\nfor smaller datasets by guiding the aggregation process towards a\nBreadth-First Search pattern, which is a soft form of the convolu-\ntional bias. In the presence of enough data, the network is able to\nlearn the best aggregation pattern for the given primary objective,\nwhich also generalizes to unseen data.\n4 EXPERIMENTS AND RESULTS\nWe evaluate the performance of our proposed EGT architecture\nin a supervised and inductive setting. We focus on a diverse set\nof supervised learning tasks, namely, node and edge classification,\nand graph classification and regression. We also experiment on the\ntransfer learning performance of EGT.\nDatasets: In the medium-scale supervised learning setting, we ex-\nperimented with the benchmarking datasets proposed by Dwivedi\net al . [15] , namely PATTERN (14K synthetic graphs, 44-188\nnodes/graph) and CLUSTER (12K synthetic graphs, 41-190\nnodes/graph) for node classification; TSP (12K synthetic graphs, 50-\n500 nodes/graph) for edge classification; and MNIST (70K superpixel\ngraphs, 40-75 nodes/graph), CIFAR10 (60K superpixel graphs, 85-\n150 nodes/graph) and ZINC (12K molecular graphs, 9-37\nnodes/graph) for graph classification/regression. To evaluate the\nperformance of EGT at large-scale we consider the graph regres-\nsion task on the PCQM4M and its updated version PCQM4Mv2\ndatasets [20] which contain 3.8 million molecular graphs with 1-\n51 nodes/graph. We also experimented on tranfer learning from\nPCQM4Mv2 dataset to the graph classification tasks on OGB [21]\ndatasets MolPCBA (438K molecular graphs, 1-332 nodes/graph) and\nMolHIV (41K molecular graphs, 2-222 nodes/graph).\nEvaluation Setup: We use the PyTorch [30] numerical library to\nimplement our model. Training was done in a distributed manner on\na single node with 8 NVIDIA Tesla V100 GPUs (32GB RAM/GPU),\nand 2 20-core 2.5GHz Intel Xeon CPUs (768GB RAM). Masked\nattention was used to process mini-batches containing graphs of\ndifferent numbers of nodes. This allowed us to use highly parallel\ntensor operations on the GPU. The results are evaluated in terms of\naccuracy, F1 score, Mean Absolute Error (MAE), Average Precision\n(AP), or Area Under the ROC Curve (AUC), as recommended for\neach dataset. Hyperparameters were tuned on the validation set.\nFull details of hyperparameters are included in the appendix and\nthe code is available at https://github.com/shamim-hussain/egt.\n4.1 Medium-scale Performance\nFor the benchmarking datasets, we follow the training setting sug-\ngested by Dwivedi et al. [15] and evaluate the performance of EGT\nfor a given parameter budget. Comparative results are presented\nin Table 1. All datasets except PATTERN and CLUSTER include\nedge features. From the results, we see that EGT outperforms other\nGNNs (including GAT and GT which use local self-attention, and\nGraphormer which uses global self-attention but without edge\nchannels) on all datasets except CIFAR10. We see a high level of\noverfitting for all models on CIFAR10, including our model which\noverfits the training dataset due to its higher capacity. The edge\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Hussain, Zaki and Subramanian\nTable 1: Experimental results on 6 benchmarking datasets from Dwivedi et al. [15]. Results on PATTERN and CLUSTER\ndatasets are given in terms of weighted accuracy. Red: best model, Violet: good model; arrow next to a metric indicates whether\nhigher or lower is better. Results not shown are not available for that method.\nPATTERN CLUSTER MNIST CIFAR10 TSP ZINC\n% Accuracyâ†‘ % Accuracyâ†‘ % Accuracyâ†‘ % Accuracyâ†‘ F1â†‘ MAEâ†“\n#Param #Param #Param #Param #Param #Param #Param #Param #Param\nModel â‰ˆ100K â‰ˆ500K â‰ˆ500K â‰ˆ100K â‰ˆ100K â‰ˆ100K â‰ˆ500K â‰ˆ100K â‰ˆ500K\nGCN [24] 63.880Â±0.074 71.892Â±0.334 68.498Â±0.976 90.705Â±0.218 55.710Â±0.381 0.630Â±0.001 0.459Â±0.006 0.367 Â±0.011\nGraphSage [18]50.516Â±0.001 50.492Â±0.001 63.844Â±0.110 97.312Â±0.097 65.767Â±0.308 0.665Â±0.003 0.468Â±0.003 0.398 Â±0.002\nGIN [42] 85.590Â±0.011 85.387Â±0.136 64.716Â±1.553 96.485Â±0.097 55.255Â±1.527 0.656Â±0.003 0.387Â±0.015 0.526 Â±0.051\nGAT [39] 75.824Â±1.823 78.271Â±0.186 70.587Â±0.447 95.535Â±0.205 64.223Â±0.455 0.671Â±0.002 0.475Â±0.007 0.384 Â±0.007\nGT [14] 84.808Â±0.068 73.169Â±0.622 0.226Â±0.014\nGatedGCN [4] 84.480Â±0.122 86.508Â±0.085 76.082Â±0.196 97.340Â±0.143 67.312Â±0.311 0.808Â±0.003 0.838 Â±0.002 0.375Â±0.003 0.214 Â±0.013\nPNA [12] 86.567Â±0.075 97.690Â±0.022 70.350Â±0.630 0.188Â±0.004 0.142 Â±0.010\nDGN [2] 86.680Â±0.034 72.700Â±0.540 0.168Â±0.003\nGraphormer [43] 86.650Â±0.033 74.660Â±0.236 97.905Â±0.176 65.978Â±0.579 0.698Â±0.007 0.122Â±0.006\nEGT 86.816Â±0.027 86.821Â±0.020 79.232Â±0.348 98.173Â±0.087 68.702Â±0.409 0.822Â±0.000 0.853Â±0.001 0.143Â±0.011 0.108Â±0.009\nTable 2: Results on OGB-LSC PCQM4M and PCQM4Mv2\ndatasets in terms of Mean Absolute Error (lower is better).\nResults not shown are not available.\nPCQM4M PCQM4Mv2\nModel #ParamValidate Test Validate Test-dev\nGCN [24] 2.0M 0.1684 0.1838 0.1379 0.1398\nGIN [42] 3.8M 0.1536 0.1678 0.1195 0.1218\nGCN-VN [16, 24] 4.9M 0.1510 0.1579 0.1153 0.1152\nGIN-VN [16, 42] 6.7M 0.1396 0.1487 0.1083 0.1084\nGINE-VN [5, 16] 13.2M 0.1430\nDeeperGCN-VN [16, 27]25.5M 0.1398\nGT [14] 0.6M 0.1400\nGT (bigger model) [14]83.2M 0.1408\nGraphormerSMALL[43] 12.5M 0.1264\nGraphormer [43] 47.1M 0.1234 0.1328 0.0906\nEGTSmall(6 layers) 11.5M 0.1260 0.0899\nEGTMedium(18 layers) 47.4M 0.1224 0.0881\nEGTLarge(24 layers) 89.3M 0.0869 0.0872\nchannels allow us to use the distance prediction objective in a multi-\ntask learning setting, which helps lessen the overfitting problem\non CIFAR10, ZINC and MNIST. Also, the output embeddings of the\nedge channels are directly used for edge classification on the TSP\ndataset which leads to very good results. Note that, Graphormer,\nwhich also uses global self-attention but does not have such edge\nchannels, performs satisfactorily for other tasks but not so much\non edge classification on the TSP dataset. Since we do not take\nadvantage of the convolutional inductive bias our model shows\nvarious levels of overfitting on these medium-sized datasets. While\nEGT still outperforms other GNNs, we posit that it would further\nexceed the performance level of convolutional GNNs if more train-\ning data were present (we confirm this in the next section). Also, the\nresults indicate that convolutional aggregation is not an essential\ninductive bias, and global attention can learn to make the best use\nof the structural information.\n4.2 Large-scale Performance\nThe results for the graph regression task on the OGB-LSC PCQM4M\nand PCQM4Mv2 datasets [20] are presented in Table 2. We show\nTable 3: Results on OGB Mol datasets. EGT uses trans-\nfer learning from PCQM4Mv2, whereas GIN-VN and\nGraphormer use transfer learning from PCQM4M. AP\nstands for Average Precision and AUC for Area Under the\nROC Curve, higher is better for both. Results not shown are\nnot available.\nMolPCBA MolHIV\nModel #Param Test AP(%)#Param Test AUC(%)\nDeeperGCN-FLAG [25, 27]6.55M 28.42 Â±0.43 532K 79.42 Â±1.20\nDeeperGCN-VN-FLAG 6.55M 28.42 Â±0.43\n[16, 25, 27]\nPNA [12] 6.55M 28.38 Â±0.35 326K 79.05 Â±1.32\nDGN [2] 6.73M 28.85 Â±0.30 110K 79.70 Â±0.97\nGINE-VN [5, 16] 6.15M 29.17 Â±0.15\nPHC-GNN [26] 1.69M 29.47 Â±0.26 114K 79.34 Â±1.16\nGIN-VN [16, 42] 3.4M 29.02 Â±0.17 3.3M 77.80 Â±1.82\n(pre-trained)\nGraphormer-FLAG [43]119.5M 31.40Â±0.34 47.2M 80.51Â±0.53\n(pre-trained)\nEGTLarger(30 layers) 110.8M 29.61Â±0.24 110.8M 80.60Â±0.65\n(pre-trained)\nresults for EGT models of small, medium and large network sizes\nbased on number of parameters (details are included in the ap-\npendix). Note that the PCQM4M dataset was later deprecated in\nfavor of PCQM4Mv2. So its test labels are no longer available and\nresults are given over the validation set. We include these results\nfor a thorough comparison with established models that report\ntheir results on the older dataset. We see that EGT achieves a much\nlower MAE thanall the convolutional and local self-attention based\n(i.e., GT [14]) GNNs. Its performance even exceeds Graphormer\n[43], which is also a global self-attention based model and can be\nthought of as an ablated variant of EGT with specialized encodings,\nsuch as centrality, spatial and edge encodings and requires simi-\nlar training time and resources. We hypothesize that EGT gets a\nbetter result than Graphormer because of a combination of several\nfactors, including its edge channels, unique gating mechanism and\ndynamic centrality scalers. Our model is currently the best perform-\ning model on the PCQM4Mv2 leaderboard. These results show the\nGlobal Self-Attention as a Replacement for Graph Convolution KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nTable 4: Comparison of results for two ablated variants of EGT (EGT-Constrained and EGT-Simple), along with the complete\narchitecture with (EGT) and without (EGT w/o PE) SVD based positional encodings\nPATTERN CLUSTER MNIST CIFAR10 TSP ZINC PCQM4Mv2\n% Accuracyâ†‘ % Accuracyâ†‘ % Accuracyâ†‘ % Accuracyâ†‘ F1â†‘ MAEâ†“ MAEâ†“\nModel #Paramâ‰ˆ500K #Paramâ‰ˆ500K #Paramâ‰ˆ100K #Paramâ‰ˆ100K #Paramâ‰ˆ500K #Paramâ‰ˆ500K #Paramâ‰ˆ11.5M\nEGT-Constrained86.629Â±0.041 76.701Â±0.257 96.823Â±0.204 65.192Â±0.475 0.846Â±0.001 0.174Â±0.004 0.0934\nEGT-Simple 86.813Â±0.013 79.182Â±0.213 98.148Â±0.139 64.967Â±1.263 0.831Â±0.002 0.228Â±0.020 0.0900\nEGT w/o PE 86.812Â±0.031 77.665Â±0.343 99.218Â±0.219 68.555Â±0.624 0.853Â±0.001 0.187Â±0.005 0.0901\nEGT 86.821Â±0.020 79.232Â±0.348 98.173Â±0.087 68.702Â±0.409 0.853Â±0.001 0.108Â±0.009 0.0899\nscalability of our framework and further confirm that given enough\ndata, global self-attention based aggregation can outperform local\nconvolutional aggregation.\n4.3 Transfer Learning Performance\nIn order to experiment on the transferability of the representations\nlearned by EGT, we take an EGT model pre-trained on the large-\nscale PCQM4Mv2 molecular dataset and fine-tune the weights on\nthe OGB molecular datasets MolPCBA and MolHIV. Although the\nvalidation performance improvement seems to plateau for larger\nmodels on the PCQM4Mv2 dataset at a certain point, we found\nthat larger pre-trained models perform better when fine-tuned on\nsmaller datasets, so we select the largest model (EGTLarger) with 30\nlayers for transfer learning experiments (it achieves a validation\nMAE of 0.0869 on PCQM4Mv2, same as EGTLarge). The results are\npresented in Table 3. We see that both EGT and Graphormer achieve\ncomparable results which exceed convolutional GNNs. Graphormer\nuses pre-trained models from PCQM4M and they found it essential\nto use the FLAG training method [25] to achieve good fine-tuning\nresults. FLAG uses an inner optimization loop to augment the node\nembeddings by adding adversarial perturbations to them. However,\nwe do not use any form of specialized training during the fine-\ntuning process. This is due to two reasons - firstly, we wanted to\nevaluate our model in the conventional transfer learning setting\nwhere the weights of a pre-trained model are simply fine-tuned\non a new dataset for a very few epochs which saves training time\nand resources â€“ whereas, FLAG training takes several times longer\ntraining time with additional FLAG hyperparameter tuning. An-\nother reason is that FLAG is an adversarial perturbation method for\nnode embeddings and since we have both node and edge embed-\ndings (including non-existing edges) it is not clear how this method\nshould be adopted for our model â€“ which requires further research.\n4.4 Ablation Study\nOur architecture is based upon two important ideas â€“ global self-\nattention based aggregation and residual edge channels. To analyze\nthe importance of these two features, we experiment with two\nablated variants of EGT: i) EGT-Simple: incorporates global self-\nattention, but instead of having dedicated residual channels for\nedges, it directly uses a linear transformation of the input edge\nembeddings ğ‘’0\nğ‘–ğ‘— (formed from adjacency matrix and edge features)\nto guide the self-attention mechanism. The absence of edge chan-\nnels means that the edge embeddings ğ‘’ğ‘–ğ‘— are not updated from\nlayer to layer. So, edge classification is performed by applying MLP\nlayers on pairwise node-embeddings. It is architecturally similar\nTable 5: Ablation study on the PCQM4Mv2 dataset for\nEGTSmall (from Table 2).\nGated Attention Virtual Centrality Positional Validate\nAggregation Dropout Nodes Scalers Encodings MAEâ†“\nâ€“ â€“ â€“ â€“ â€“ 0.0965\nâ€“ â€“ â€“ â€“ 0.0943\nâ€“ â€“ â€“ 0.0926\nâ€“ â€“ 0.0919\nâ€“ 0.0900\n0.0899\nto Graphormer [43]. While it is slightly less expensive in terms\nof computation and memory, it still scales quadratically with the\nnumber of nodes. ii) EGT-Constrained limits the self-attention\nprocess to the 1-hop neighborhood of each node, which allows us to\ncompare global self-attention to convolutional local self-attention\nbased aggregation. Also, it only keeps track of the edge embeddings\nğ‘’ğ‘–ğ‘— in the edge channels if there is an edge from node ğ‘– to node ğ‘—\nor ğ‘– = ğ‘— (self-loop). Architecturally, this variant is similar to GT\n[14] and can take advantage of the sparsity of the graph to reduce\ncomputational and memory costs. More details about these variants\ncan be found in the appendix.\nThe results for the ablated variants are presented in Table 4.\nWe see that, EGT-Simple can come close to EGT, but is especially\nsubpar when the targeted task is related to edges (e.g., edge clas-\nsification on the TSP dataset) or when the distance objective can-\nnot be applied (ZINC, CIFAR10) due to the lack of dedicated edge\nchannels. Both EGT-Simple and EGT enjoy an advantage over EGT-\nConstrained on the large PCQM4Mv2 dataset due to their global\naggregation mechanism. This indicates that given enough data,\nglobal self-attention based aggregation can outperform local self-\nattention based aggregation. Additionally to demonstrate the effect\nof the SVD based positional encodings we include results without\npositional encodings. Note that the positional encodings lead to a\nsignificant improvement for the ZINC and the CLUSTER datasets,\nbut slight/no improvement in other cases. This is consistent with\nour statement that the positional encodings are optional for our\nmodel on some tasks, but their inclusion can often lead to a perfor-\nmance improvement.\nTo further examine the contribution of different features of our\nmodel we carried out a series of experiments on the PCQM4Mv2\ndataset for the smallest EGT network. The results are presented in\nTable 5. We see that the use of gates during aggregation leads to a\nsignificant improvement. Another important contributing factor is\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Hussain, Zaki and Subramanian\nFigure 3: Analysis of aggregation patterns on three datasets â€“ (a) ZINC, (b) PCQM4Mv2, (c) TSP. Left to right â€“ adjacency (i) and\ndistance matrices (ii), an example attention head (iii), average of attention heads in a middle layer (iv) and in a deeper layer\n(v) â€“ for a particular input graph in the validation set (matrices have been cropped for the TSP dataset). On the right â€“ weights\nassigned for different hops in different layers, averaged over all heads and all nodes in all the graphs in the validation set.\ndropout on the attention matrix which encourages the network to\ntake advantage of long-distance interactions. The dynamic central-\nity scalers also help by making the network more expressive. Virtual\nnodes and positional encodings lead to a more modest performance\nimprovement.\n4.5 Analysis of Aggregation Patterns\nTo understand how global self-attention based aggregation trans-\nlates to performance gains we examined the attention matrices\ndynamically formed by the network. These matrices dictate the\nweighted aggregation of the nodes and thus show how each node is\nlooking at other nodes. This is demonstrated in Fig. 3. We show the\nadjacency matrix and the distance matrix to demonstrate how far\neach node is looking. First, we look at an example attention matrix\nformed by an attention head. Next, for the sake of visualization,\nwe merge the attention matrices for different heads together by\naveraging and normalizing them to values between [0 1]. We do\nthis for two different layers at different depths of the model. Note\nthat these patterns are specific to a particular input graph â€“ since\nthe aggregation process is dynamic they would be different for dif-\nferent inputs. To make a complete analysis of each layerâ€™s attention\nwe also plot the weights assigned at different distances averaged\nover all the attention heads for all the nodes and all the graphs\nin a dataset. Note that a convolutional aggregation of immediate\nneighbors would correspond to non-zero weights being assigned\nto only 0/1 hop.\nWe see that the attention matrices for individual attention heads\nare quite sparse. So, the nodes are selective about where to look.\nFor the ZINC dataset, from Fig. 3 (a), at layerâ„“ = 1 we see that EGT\napproximately follows a convolutional pattern. But as we go deeper,\nthe nodes start to take advantage of global self-attention to look\nfurther. Finally, at â„“ = 10 we see highly non-local behavior. This\nshows why EGT has an advantage over local aggregation based\nconvolutional networks because of its ability to aggregate global\nfeatures. For PCQM4Mv2, in Fig. 3 (b), we notice such non-local\naggregation patterns starting from the lowest layers. This shows\nwhy a global aggregation based model such as EGT has a clear\nadvantage over convolutional networks (as seen in Table 2), because\nit would take a large number of consecutive convolutions to mimic\nsuch patterns. This non-local behavior is more subtle in TSP, where,\nexcept for the last layer, attention is mostly constrained to 1-3\nhops, as seen from Fig. 3(c). This also shows why EGT-Constrained\nachieves good results on this dataset (Table 4). However, even the\nslight advantage of global aggregation gives pure EGT an edge\nover EGT-constrained. To conclude, the aggregation performed by\nour model is sparse and selective, like convolution, yet capable of\nbeing non-local and dynamic, which leads to a clear advantage over\nconvolutional networks.\n5 CONCLUSION AND FUTURE WORK\nWe proposed a simple extension â€“ edge channels â€“ to the trans-\nformer framework. We preserve the key idea, namely, global atten-\ntion, while making it powerful enough to take structural informa-\ntion of the graph as input and also to process it and output new\nstructural information such as new links and edge labels. One of our\nkey findings is that the incorporation of the convolutional aggrega-\ntion pattern is not an essential inductive bias for GNNs and instead\nthe model can directly learn to make the best use of structural\nGlobal Self-Attention as a Replacement for Graph Convolution KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\ninformation. We established this claim by presenting experimental\nresults on both medium-scale, large-scale and transfer learning\nsettings where our model achieves superior performance, beating\nconvolutional GNNs. We also achieve a new state-of-the-art result\non the large-scale PCQM4Mv2 molecular dataset. We demonstrated\nthat the performance improvement is directly linked to the non-\nlocal nature of aggregation of the model. In future work, we aim to\nevaluate the performance of EGT in transductive, semi-supervised\nand unsupervised settings. Also, we plan to explore the prospect\nof reducing the computation and memory cost of our model to a\nsub-quadratic scale by incorporating linear attention [9, 23, 35] and\nsparse edge channels.\nACKNOWLEDGMENTS\nThis work was supported by the Rensselaer-IBM AI Research Col-\nlaboration, part of the IBM AI Horizons Network.\nREFERENCES\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).\n[2] Dominique Beani, Saro Passaro, Vincent LÃ©tourneau, Will Hamilton, Gabriele\nCorso, and Pietro LiÃ². 2021. Directional graph networks. In International Confer-\nence on Machine Learning . PMLR, 748â€“758.\n[3] Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral\ntechniques for embedding and clustering.. In Nips, Vol. 14. 585â€“591.\n[4] Xavier Bresson and Thomas Laurent. 2017. Residual gated graph convnets. arXiv\npreprint arXiv:1711.07553 (2017).\n[5] RÃ©my Brossard, Oriel Frigo, and David Dehaene. 2020. Graph convolutions that\ncan finally model local structure. arXiv preprint arXiv:2011.15069 (2020).\n[6] Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning.\nIn Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 7464â€“7471.\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,\nand Ilya Sutskever. 2020. Generative pretraining from pixels. In International\nConference on Machine Learning . PMLR, 1691â€“1703.\n[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\n[9] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, and Others. 2020. Rethinking attention with performers. arXiv\npreprint arXiv:2009.14794 (2020).\n[10] Djork-ArnÃ© Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2015. Fast and\naccurate deep network learning by exponential linear units (elus). arXiv preprint\narXiv:1511.07289 (2015).\n[11] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. 2019. On the\nrelationship between self-attention and convolutional layers. arXiv preprint\narXiv:1911.03584 (2019).\n[12] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro LiÃ², and Petar VeliÄkoviÄ‡.\n2020. Principal neighbourhood aggregation for graph nets. arXiv preprint\narXiv:2004.05718 (2020).\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[14] Vijay Prakash Dwivedi and Xavier Bresson. 2020. A generalization of transformer\nnetworks to graphs. arXiv preprint arXiv:2012.09699 (2020).\n[15] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and\nXavier Bresson. 2020. Benchmarking graph neural networks. arXiv preprint\narXiv:2003.00982 (2020).\n[16] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E\nDahl. 2017. Neural message passing for quantum chemistry. In International\nconference on machine learning . PMLR, 1263â€“1272.\n[17] Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for\nlearning in graph domains. InProceedings. 2005 IEEE International Joint Conference\non Neural Networks, 2005. , Vol. 2. IEEE, 729â€“734.\n[18] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning\non graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[20] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure\nLeskovec. 2021. Ogb-lsc: A large-scale challenge for machine learning on graphs.\narXiv preprint arXiv:2103.09430 (2021).\n[21] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen\nLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets\nfor machine learning on graphs. arXiv preprint arXiv:2005.00687 (2020).\n[22] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous\ngraph transformer. In Proceedings of The Web Conference 2020 . 2704â€“2710.\n[23] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret.\n2020. Transformers are rnns: Fast autoregressive transformers with linear atten-\ntion. In International Conference on Machine Learning . PMLR, 5156â€“5165.\n[24] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[25] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem,\nGavin Taylor, and Tom Goldstein. 2020. Flag: Adversarial data augmentation for\ngraph neural networks. arXiv preprint arXiv:2010.09891 (2020).\n[26] Tuan Le, Marco Bertolini, Frank NoÃ©, and Djork-ArnÃ© Clevert. 2021. Parameter-\nized hypercomplex graph neural networks for graph classification. In Interna-\ntional Conference on Artificial Neural Networks . Springer, 204â€“216.\n[27] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. 2020. Deepergcn:\nAll you need to train deeper gcns. arXiv preprint arXiv:2006.07739 (2020).\n[28] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019. Neural\nspeech synthesis with transformer network. InProceedings of the AAAI Conference\non Artificial Intelligence , Vol. 33. 6706â€“6713.\n[29] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro.\n2019. Relational pooling for graph representations. In International Conference\non Machine Learning . PMLR, 4663â€“4673.\n[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library. Advances\nin neural information processing systems 32 (2019).\n[31] Omri Puny, Heli Ben-Hamu, and Yaron Lipman. 2020. Global Attention Improves\nGraph Networks Generalization. arXiv preprint arXiv:2006.07846 (2020).\n[32] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Lev-\nskaya, and Jonathon Shlens. 2019. Stand-alone self-attention in vision models.\narXiv preprint arXiv:1906.05909 (2019).\n[33] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang,\nand Junzhou Huang. 2020. Self-supervised graph transformer on large-scale\nmolecular data. arXiv preprint arXiv:2007.02835 (2020).\n[34] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele\nMonfardini. 2008. The graph neural network model. IEEE transactions on neural\nnetworks 20, 1 (2008), 61â€“80.\n[35] Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. 2021. Linear transformers\nare secretly fast weight memory systems. arXiv preprint arXiv:2102.11174 (2021).\n[36] Balasubramaniam Srinivasan and Bruno Ribeiro. 2019. On the equivalence\nbetween positional node embeddings and structural graph representations. arXiv\npreprint arXiv:1910.00452 (2019).\n[37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: a simple way to prevent neural networks from\noverfitting. The journal of machine learning research 15, 1 (2014), 1929â€“1958.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998â€“6008.\n[39] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[40] Chen Wang and Chengyuan Deng. 2021. On the Global Self-attention Mechanism\nfor Graph Convolutional Networks. In 2020 25th International Conference on\nPattern Recognition (ICPR) . IEEE, 8531â€“8538.\n[41] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,\nHuishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. On layer\nnormalization in the transformer architecture. In International Conference on\nMachine Learning . PMLR, 10524â€“10533.\n[42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful\nare graph neural networks? arXiv preprint arXiv:1810.00826 (2018).\n[43] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,\nYanming Shen, and Tie-Yan Liu. 2021. Do Transformers Really Perform Bad for\nGraph Representation? arXiv preprint arXiv:2106.05234 (2021).\n[44] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim.\n2019. Graph transformer networks. Advances in Neural Information Processing\nSystems 32 (2019), 11983â€“11993.\n[45] Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. 2020. Graph-bert:\nOnly attention is needed for learning graph representations. arXiv preprint\narXiv:2001.05140 (2020).\n[46] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. 2020. Revisiting\ngraph neural networks for link prediction.arXiv preprint arXiv:2010.16103 (2020).\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Hussain, Zaki and Subramanian\nA DATA AND CODE AVALABILITY\nData: All datasets used in this work are publicly available. The\nmedium-scale GNN benchmarking datasets by Dwivedi et al. [15]\nare available at https://github.com/graphdeeplearning/benchmarking-\ngnns. The OGB-LSC [21] PCQM4M and PCQM4Mv2 large-scale\ndatasets, and the OGB datasets [ 20] MolPCBA and MolHIV are\navailable at https://ogb.stanford.edu.\nCode: The code to reproduce the results presented in this work is\navailable at https://github.com/shamim-hussain/egt.\nB TRAINING METHOD AND\nHYPERPARAMETERS\nB.1 Medium-scale Experiments\nFor medium-scale experiments on the PATTERN, CLUSTER, MNIST,\nCIFAR10, TSP, and ZINC datasets we follow the benchmarking\nsetting suggested by Dwivedi et al. [15] and maintain a specified\nparameter budget of either 100K or 500K. The number of layers,\nthe width of the node and the edge channels (ğ¿, ğ‘‘â„ and ğ‘‘ğ‘’, corre-\nspondingly) were varied to get the best results on the validation\nset. We used the Adam optimizer and reduce the learning rate by a\nfactor of 0.5 if the validation loss does not improve for a given num-\nber of epochs (Reduce LR when validation loss plateaus). We keep\ntrack of the validation loss at the end of each epoch and pick the\nset of weights that produces the least validation loss. No dropout\nor weight decay is used for a fair comparison with other GNNs.\nEach experiment (training and evaluation) was run 4 times with 4\ndifferent random seeds and the results were used to calculate the\nmean and standard deviations of the metric. The common hyperpa-\nrameters and methods for all datasets are given in Table 6, whereas\nthe hyperparameters which are specific for each dataset are given\nin Table 7.\nB.2 Large-scale Experiments\nWhile training large models on the PCQM4M and PCQM4Mv2\ndatasets, we found it essential to use learning rate warmup. Fol-\nlowing the warmup, we applied cosine decay to the learning rate.\nTable 6: Common hyperparameters used in medium-scale experi-\nments on all datasets.\nHyperparameter Value\nNumber of attention heads,ğ» 8\nNode channels FFN multiplier 2\nEdge channels FFN multiplier 2\nFinal (two) MLP layers dimensionğ‘‘â„/2,ğ‘‘â„/4\nVirtual nodes Not used\nSVD encoding rank,ğ‘Ÿ 8\nRandom attention masking rate0.1\nDynamic Centrality Scalers Not used\nDropout Not used\nAdam: initial LR 5Ã—10âˆ’4\nAdam:ğ›½1 0.9\nAdam:ğ›½2 0.999\nAdam:ğœ– 10âˆ’7\nReduce LR by factor 0.5\nMinimum LR 5Ã—10âˆ’6\nLR warmup Not used\nCosine decay Not used\nWe used virtual nodes which is a more scalable method than global\naverage pooling because the use of multiple virtual nodes allows the\nmodel to collect more graph-level information. Instead of random\nmasking of the attention matrices, we applied dropout to the at-\ntention matrices, which showed better regularization performance.\nAttention dropout is the only regularization method used for all\nmodels. We trained all models for a fixed number (1 million) of\ngradient update steps. The hyperparameters are shown in Table 8.\nB.3 Transfer Learning Experiments\nWe took the EGTLarger model pre-trained on the PCQM4Mv2 dataset\n(Table 8) and fine-tuned it on the OGB datasets MolPCBA and Mol-\nHIV. We used the same learning rate and warmup and cosine decay\nmethod mentioned above, although for a smaller number of total\ngradient update steps. Hyperparameters specific to the fine-tuning\nstage are shown in Table 9. Other hyper hyperparameters were the\nsame as in Table 8. Each experiment (training and evaluation) was\nrun 4 times with 4 different random seeds and the results were used\nto calculate the mean and standard deviations of the metric.\nC DETAILS OF ABLATED VARIANTS\nFor the ablation study presented in section 4.4 of the paper, we\ndiscuss here different ablation methods in more detail.\nEGT-Simple: EGT-simple uses global self-attention, but does not\nhave dedicated residual channels for updating pairwise information\n(edges). The input edge embeddings (formed from graph structural\nmatrix and edge features) directly participate in the aggregation\nprocess as follows:\nËœAğ‘˜,â„“ = softmax(Ë†Hğ‘˜,â„“)âŠ™ ğœ(Gğ‘˜,â„“\n0 ) (12)\nWhere, Ë†Hğ‘˜,â„“ = clip Â©Â­\nÂ«\nQğ‘˜,â„“\nâ„ (Kğ‘˜,â„“\nâ„ )ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\nÂªÂ®\nÂ¬\n+Eğ‘˜,â„“\n0 (13)\nEğ‘˜,â„“\n0 ,Gğ‘˜,â„“\n0 âˆˆRğ‘Ã—ğ‘ are directly formed by concatenations of the\nlearned linear transformed input edge embeddings, i.e., Eğ‘˜,â„“ğ‘’0\nğ‘–ğ‘—,\nGğ‘˜,â„“ğ‘’0\nğ‘–ğ‘— respectively. Also, dynamic centrality scalers are derived\nfrom ğ‘’0\nğ‘–ğ‘—. The absence of edge channels means that the edge em-\nbeddings ğ‘’ğ‘–ğ‘— are not updated from layer to layer. So, edge classi-\nfication is performed from pairwise node embeddings and input\nedge features. We denote this variant as EGT-Simple since it is\narchitecturally simpler than EGT.\nWe use the same hyperparameters for this variant as the ones\nused for original EGT (Table 7, Table 8; ğ‘‘ğ‘’ denotes only the dimen-\nsionality of the input edge embeddings) except, ğ‘‘â„ = 64, ğ‘‘ğ‘’ = 8\nfor CIFAR10, and ğ‘‘â„ = 80, ğ‘‘ğ‘’ = 8 for ZINC are used to make the\nnumber of parameters comparable.\nEGT-Constrained: EGT-Constrained is a convolutional variant of\nEGT which limits the self-attention process to the 1-hop neighbor-\nhood of each node. It only keeps track of the edge embeddings ğ‘’ğ‘–ğ‘—\nin the edge channels if there is an edge from node ğ‘– to node ğ‘— or\nğ‘– = ğ‘—(self-loop). So, pairwise information corresponding to only the\nexisting edges is updated by the edge channels. This architecture\ncan be derived by taking the softmax over ğ‘— âˆˆN( ğ‘–)âˆª{ ğ‘–}while\ncalculating the attention weights ËœAğ‘˜,â„“\nğ‘–ğ‘— and limiting the aggregation\nGlobal Self-Attention as a Replacement for Graph Convolution KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nTable 7: Specific hyperparameters used for each dataset in medium-scale experiments. D(16) is the distance matrix clipped to 16 hops. A is\nthe adjacency matrix with self-loops. Distance prediction objective is only used for MNIST, CIFAR10 and ZINC datasets.\nPATTERN CLUSTERMNIST CIFAR10 TSP ZINC\n#Param #Param #Param #Param #Param #Param #Param#Param #Param\nHyperparameter â‰ˆ100K â‰ˆ500K â‰ˆ500K â‰ˆ100K â‰ˆ100K â‰ˆ100K â‰ˆ500K â‰ˆ100K â‰ˆ500K\nInput structural matrix D(16) D(16) D(16) A A D(16) D(16) A A\nBatch size 128 128 128 128 128 8 8 128 128\nMaximum no. of epochs 200 200 200 200 200 200 200 600 600\nReduce LR patience (epochs) 10 10 10 10 10 5 5 20 20\nDistance prediction objective:ğœˆ(when used) 3 hops 3 hops 3 hops 3 hops\nDistance prediction objective:ğœ…(when used) 5Ã—10âˆ’4 5Ã—10âˆ’4 5Ã—10âˆ’2 5Ã—10âˆ’2\nNumber of layers,ğ¿ 4 16 16 4 4 4 16 4 10\nNode channels width,ğ‘‘â„ 64 64 64 64 48 64 64 48 64\nEdge channels width,ğ‘‘ğ‘’ 8 8 8 8 48 8 8 48 64\nTable 8: Hyperparameters used in large-scale experiments.\nHyperparameter Value\nInput structural matrix Distance matrix\n(clipped up to 16 hops)\nNumber of attention heads,ğ» 32\nEdge channels width,ğ‘‘ğ‘’ 64\nNode channels FFN multiplier 1\nEdge channels FFN multiplier 1\nFinal (two) MLP layers dimension ğ‘‘â„,ğ‘‘â„\nVirtual nodes 4\nDynamic Centrality Scalers Used\nSVD encoding rank,ğ‘Ÿ 8\nDistance prediction objective Not used\nRandom attention masking Not used\nAttention matrix dropout rate 0.3\nAdam:ğ›½1 0.9\nAdam:ğ›½2 0.999\nAdam:ğœ– 10âˆ’7\nReduce LR on loss plateau Not used\nMinimum LR 1Ã—10âˆ’6\nBatch size 512\nLR warmup 200,000 steps\nCosine decay 800,000 steps\nSpecific to EGTSmall\nMaximum LR 2Ã—10âˆ’4\nNumber of layers,ğ¿ 6\nNode channels width,ğ‘‘â„ 512\nSpecific to EGTMedium\nMaximum LR 1Ã—10âˆ’4\nNumber of layers,ğ¿ 18\nNode channels width,ğ‘‘â„ 640\nSpecific to EGTLarge\nMaximum LR 1Ã—10âˆ’4\nNumber of layers,ğ¿ 24\nNode channels width,ğ‘‘â„ 768\nSpecific to EGTLarger\nMaximum LR 8Ã—10âˆ’5\nNumber of layers,ğ¿ 30\nNode channels width,ğ‘‘â„ 768\nprocess to neighbors as:\nË†Ë†â„â„“\nğ‘– = â„â„“âˆ’1\nğ‘– +Oâ„“\nâ„\n\r\rğ»\nğ‘˜=1\nâˆ‘ï¸\nğ‘—âˆˆN(ğ‘–)âˆª{ğ‘–}\nËœAğ‘˜,â„“\nğ‘–ğ‘— (Vğ‘˜,â„“ Ë†â„â„“\nğ‘–) (14)\nSince this architecture is constrained to the existing edges we denote\nthis as EGT-Constrained. It has the advantage that depending on\nthe sparsity of the graph, it can have sub-quadratic computational\nTable 9: Hyperparameters used in transfer learning experiments.\nHyperparameterMolPCBA MolHIV\nMaximum LR 1Ã—10âˆ’4 1Ã—10âˆ’4\nMinimum LR 1Ã—10âˆ’6 1Ã—10âˆ’6\nBatch size 16 12\nLR warmup 20,000 steps1,000 steps\nCosine decay 180,000 steps2,000 steps\nand memory costs. However, it can be difficult to perform sparse\naggregation in parallel on the GPU. Instead of sparse operations,\nwe used masked attention to implement this architecture for faster\ntraining on datasets containing smaller graphs because we can take\nadvantage of highly parallel tensor operations.\nFor this variant, ğ‘‘â„,ğ‘‘ğ‘’ bear their usual meanings in the hyperpa-\nrameters tables (Table 7, Table 8). We use the same hyperparameters\nfor this variant as the ones used for original EGT.\nUngated Variant: In EGT, the edge channels participate in the\naggregation process in two ways - by an attention bias and also\nby gating the values before they are aggregated by the attention\nmechanism. To verify the utility of the gating mechanism used\nin EGT, an ungated variant can be formulated by simplifying the\naggregation process as follows:\nËœAğ‘˜,â„“ = softmax(Ë†Hğ‘˜,â„“) (15)\nWhere, Ë†Hğ‘˜,â„“ = clip Â©Â­\nÂ«\nQğ‘˜,â„“\nâ„ (Kğ‘˜,â„“\nâ„ )ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\nÂªÂ®\nÂ¬\n+Eğ‘˜,â„“\nğ‘’ (16)\nEğ‘˜,â„“\nğ‘’ âˆˆRğ‘Ã—ğ‘ is a concatenation of the learned linear transformed\nedge embeddings, i.e., Eğ‘˜,â„“Ë†ğ‘’â„“\nğ‘–ğ‘—. Note that we omitted the sigmoid\ngates. The edge channels influence the aggregation process only\nvia an attention bias.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6897943615913391
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5624114274978638
    },
    {
      "name": "Transformer",
      "score": 0.5506089925765991
    },
    {
      "name": "Graph",
      "score": 0.47844743728637695
    },
    {
      "name": "Inductive bias",
      "score": 0.4753037393093109
    },
    {
      "name": "Theoretical computer science",
      "score": 0.46748560667037964
    },
    {
      "name": "Feature learning",
      "score": 0.4366905391216278
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36014634370803833
    },
    {
      "name": "Multi-task learning",
      "score": 0.10351133346557617
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}