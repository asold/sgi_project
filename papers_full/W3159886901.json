{
    "title": "CoSformer: Detecting Co-Salient Object with Transformers",
    "url": "https://openalex.org/W3159886901",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3131867301",
            "name": "Tang Lv",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967255241",
            "name": "Li Bo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2793668851",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2990984982",
        "https://openalex.org/W2981680738",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W3035422681",
        "https://openalex.org/W2990844506",
        "https://openalex.org/W3132607382",
        "https://openalex.org/W2054241314",
        "https://openalex.org/W2342491128",
        "https://openalex.org/W2403333215",
        "https://openalex.org/W2069241582",
        "https://openalex.org/W2104915577",
        "https://openalex.org/W3101633331",
        "https://openalex.org/W3034499925",
        "https://openalex.org/W2904945062",
        "https://openalex.org/W2943125866",
        "https://openalex.org/W2141303268",
        "https://openalex.org/W2963868681",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2982331121",
        "https://openalex.org/W2789646185",
        "https://openalex.org/W3035635522",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W3039991645",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3104979525",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2344340558",
        "https://openalex.org/W2358876993",
        "https://openalex.org/W2946824329",
        "https://openalex.org/W3096289386",
        "https://openalex.org/W2972640707",
        "https://openalex.org/W2989907986",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2954275231",
        "https://openalex.org/W2153760331",
        "https://openalex.org/W1992992668",
        "https://openalex.org/W2792965491",
        "https://openalex.org/W3035666869",
        "https://openalex.org/W2740667773",
        "https://openalex.org/W2605793178",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2963834057",
        "https://openalex.org/W3034978746",
        "https://openalex.org/W2518666399",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2112553126",
        "https://openalex.org/W2964429685",
        "https://openalex.org/W2963529609",
        "https://openalex.org/W2961348656",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W1964884769"
    ],
    "abstract": "Co-Salient Object Detection (CoSOD) aims at simulating the human visual system to discover the common and salient objects from a group of relevant images. Recent methods typically develop sophisticated deep learning based models have greatly improved the performance of CoSOD task. But there are still two major drawbacks that need to be further addressed, 1) sub-optimal inter-image relationship modeling; 2) lacking consideration of inter-image separability. In this paper, we propose the Co-Salient Object Detection Transformer (CoSformer) network to capture both salient and common visual patterns from multiple images. By leveraging Transformer architecture, the proposed method address the influence of the input orders and greatly improve the stability of the CoSOD task. We also introduce a novel concept of inter-image separability. We construct a contrast learning scheme to modeling the inter-image separability and learn more discriminative embedding space to distinguish true common objects from noisy objects. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate that our CoSformer outperforms cutting-edge models and achieves the new state-of-the-art. We hope that CoSformer can motivate future research for more visual co-analysis tasks.",
    "full_text": "CoSformer: Detecting Co-Salient Object with\nTransformers\nLv Tang\nNanjing University\nluckybird1994@gmail.com\nBo Li\nIndependent Researcher\nnjumagiclibo@gmail.com\nAbstract\nCo-Salient Object Detection (CoSOD) aims at simulating the human visual system\nto discover the common and salient objects from a group of relevant images. Recent\nmethods typically develop sophisticated deep learning based models have greatly\nimproved the performance of CoSOD task. But there are still two major drawbacks\nthat need to be further addressed, 1) sub-optimal inter-image relationship modeling;\n2) lacking consideration of inter-image separability. In this paper, we propose the\nCo-Salient Object Detection Transformer (CoSformer) network to capture both\nsalient and common visual patterns from multiple images. By leveraging Trans-\nformer architecture, the proposed method address the inﬂuence of the input orders\nand greatly improve the stability of the CoSOD task. We also introduce a novel\nconcept of inter-image separability. We construct a contrast learning scheme to\nmodeling the inter-image separability and learn more discriminative embedding\nspace to distinguish true common objects from noisy objects. Extensive experi-\nments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015,\ndemonstrate that our CoSformer outperforms cutting-edge models and achieves\nthe new state-of-the-art. We hope that CoSformer can motivate future research for\nmore visual co-analysis tasks.\n1 Introduction\nAiming at simulates the human visual system to discover the common and salient objects from a group\nof relevant images, co-salient object detection (CoSOD) often serves as a preliminary step for various\ndown-streaming computer vision tasks, e.g., image co-segmentation [24], co-localization [44, 26]\nand person re-identiﬁcation [ 37]. Unlike the standard salient object detection (SOD) which only\nfocuses on the attractive regions from a single image, CoSOD also needs to leverage the similar\nattributes shared by objects in image group to distinguish the real common objects under the presence\nof noise objects. Although the co-salient objects share the same semantic category, their explicit\ncategory attributes are unknown in CoSOD task. That is to say, CoSOD methods are not supposed\nto model the consistency relations of common objects by using the supervision of speciﬁc category\nlabels or other information like temporal relations, which is quite different from video sequences\ntasks [11, 27]. These unique features make CoSOD an emerging and challenging task which has\nbeen rapidly growing in recent few years [17, 10, 58].\nConventional approaches explore the inter-image correlation between image-pairs [34] or a group\nof relevant images [ 3] by using constraints or heuristic characteristics like manifold ranking [ 4]\nand clustering [20]. However, the discrimination of hand-crafted descriptors is too limited to face\ncomplex scenes, leading to unsatisfactory performance. Recently deep learning based models have\ngreatly improved the performance of CoSOD task. By leveraging the Convolutional Neural Networks\n(CNNs) [54, 62, 47, 64, 29] and Recurrent Neural Networks (RNNs) [33, 32], they learn both single\nimage representation (intra-image saliency) and group-wise semantic representation (inter-image\nconsistency) in an end-to-end supervised manner to detect co-salient objects in image group. Despite\n1\narXiv:2104.14729v2  [cs.CV]  22 Sep 2022\nImgGTOursICNetGICD\nFigure 1: Comparison with state-of-the-art methods in complex real-world scenarios.\ntheir promising results, we ﬁnd there are still two major drawbacks that prevent the CoSOD from\nprogressing to the next high level: First, current inter-image relationship modeling is sub-optimal.\nSecond, current methods lack consideration of inter-image separability.\nFor the ﬁrst issue, previous works directly concatenate CNNs features [ 54] or use RNNs [33, 32]\nto model the inter-image relationships. However, when assigning different orders of the input\nimages there can output different group representations, which makes both training and inferring\nprocedure unstable. Recent studies [ 18, 64, 29] try to alleviate this limitation by applying some\nsophisticated modiﬁcation on CNNs architectures. Unfortunately, their efforts do not address the\ninherent deﬁciencies of CNNs and RNNs in sequential order modeling. To better model the inter-\nimage relationships, we propose to employ the Transformers [46], which is a widely used sequence\nto sequence model in Natural Language Processing (NLP) [23]. The self-attention mechanism is\ndesigned to learn all pairwise similarities between the input sequence, which empowers Transformers\ngreat ability to capture long-range dependencies. Besides, a Transformer model itself is invariant\nwith respect to re-orderings of the input [46, 14]. These characteristics of Transformers make them\nnaturally suitable for modeling the inter-image relationships across multiple images. Essentially,\nintra-image saliency and inter-image consistency are both concerned with relationship modeling:\nintra-image saliency is to learn the pixel-level relationship within a single image and inter-image\nconsistency is to learn the relationship between images. Thus, we construct Transformer based\nstructures for both intra-image saliency and inter-image consistency modeling.\nFor the second issue, current methods believe that they can well handle the CoSOD task by only\nusing intra-image saliency and inter-image consistency. However, the inter-image consistency only\nprovides positive relations while lacking negative relations between different objects. Training the\nmodel only using positive pairs cannot provide enough information for learning a discriminative\nrepresentation. When facing complex real-world scenarios, the model cannot distinguish true common\nobjects from noisy objects. As can be seen in Fig.1, two representative existing methods ICNet [30]\nand GICD [65] fail to distinguish co-salient objects because they lack consideration of inter-image\nseparability. Inspired by contrastive learning [23], we propose a novel contrastive loss for CoSOD\nto model the inter-image separability. We not only regard the co-salient regions in an image group\nas positive relations but also utilize the non-co-salient regions to build negative relations. Through\ncontrastive loss, the true common objects should be similar to each other and dissimilar to other noisy\nobjects in the embedding space. Finally, we can learn a discriminative representation to get better\nperformance.\nIn this paper, we propose the Co-Salient Object Detection Transformer (CoSformer) network, which\nviews the CoSOD task as an end-to-end sequence prediction problem. The framework is signiﬁcantly\ndifferent from existing approaches. The main contributions can be summarized as follows.\n• CoSformer solves the CoSOD from a new perspective of relationship modeling. By lever-\naging Transformer architecture, we address the inﬂuence of the input orders and greatly\nimprove the stability of deep-based CoSOD methods. Both intra-image saliency and inter-\nimage consistency are naturally modeled by the similar Transformer framework.\n2\n• We provide some insights on the drawbacks of previous methods and proposed a novel\nconcept of inter-image separability. We construct a contrast learning scheme to modeling\nthe inter-image separability and learn more discriminative representations to distinguish true\ncommon objects from noisy objects.\n• We validate the performance of our CoSformer on three widely used CoSOD datasets\n(CoCA, CoSOD3k and Cosal2015), and the performance can outperform other state-of-\nthe-art methods by a large margin. This shows the proposed Transformer framework and\ncontrastive loss can help the network detect a more accurate co-salient result.\n2 Related Work\nCo-saliency Detection.The traditional CoSOD methods explore the inter-image correspondence\nbetween image-pairs [34, 7] or a group of relevant images [ 3, 26] based on shallow handcrafted\ndescriptors [6]. Several studies attempt to capture the inter-image constraints by employing an\nefﬁcient manifold ranking scheme [35] to obtain guided saliency maps, or using a global association\nconstraint with clustering [ 20]. However, the discrimination of hand-crafted descriptors are too\nlimited to face the complex scenes, leading to unsatisfactory performance.\nRecently deep-based models simultaneously explore the intra-saliency and inter-image consistency in\na supervised manner with different approaches, such as graph convolution networks (GCN) [28, 63],\nself-learning methods [ 61, 59], correlation techniques [ 30], or co-clustering [ 57]. Other works\nexplore group-wise semantic representation which is used to detect co-salient regions for each image.\nThere are different methods to capture the discriminative semantic representation, such as group\nattention semantic aggregation [64], gradient feedback [65], recurrent co-attention [33, 32] and even\nexplicit supervision of speciﬁc category labels [ 47, 29]. However, most of the previous methods\nare unstable during both training and inferring procedure when assigning different orders of the\ninput images. And they all lack consideration of inter-image separability and cannot distinguish true\ncommon objects from noisy objects, resulting in ambiguous results when facing complex real-world\nscenarios. For more about CoSOD tasks, please refer to [ 17, 10, 58]. Another task related to CoSOD\nis SOD [49, 45, 21, 53, 25]. For more information about the SOD methods, please refer to survey [2].\nTransformer.Transformer were ﬁrst proposed in [46] for the sequence-to-sequence machine trans-\nlation task, which has revolutionized machine translation and natural language processing. The\nTransformer models are then extended to some popular computer-vision tasks including image pro-\ncessing [8], object detection [5], semantic segmentation [68], object tracking [43], video instance\nsegmentation [50], etc. DETR [5] builds an object detection system based on Transformers, which\nlargely simpliﬁes the traditional detection pipeline, and achieves on par performances compared\nwith highly-optimized CNN based detectors [ 40]. ViT [ 13] introduces the Transformer to image\nrecognition and models an image as a sequence of patches, which attains excellent results compared\nto state-of-the-art convolutional networks. The above works show the effectiveness of Transformers\nin image understanding tasks. More detailed information of the application of the Transformer in the\nﬁeld of computer vision can be found in survey [22, 31].\nAs presented in DETR [5], transformer architecture is permutation-invariant, which cannot leverage\nthe order of the tokens in an input sequence. To mitigate this gap, previous works [13, 46] add an\nabsolute positional encoding to each token in the input sequence, which enables order-awareness.\nHowever, in co-saliency detection task, we want the model should be insensitive to input order when\ncapturing group-wise relationships. Hence, it is natural to use transformer to model group-wise\nrelationships without positional encoding. To our knowledge, thus far there are no prior applications\nof Transformers to co-saliency detection.\n3 Proposed Method\nCo-saliency detection aims at discovering the common and salient objects in a group of N relevant\nimages I= {I(n)}N\nn=1. It is worth raising that directly constructs a pure transformer-based network\nfor co-saliency detection will produce unsatisfactory performance. In SETR [68], which is a pure\ntransformer-based network, Transformers treat the input as 1D sequences and exclusively focus on\nmodeling the global context at all Transformer layers, therefore result in low-resolution features\nwhich lack detailed low-level information. And this information cannot be effectively recovered by\n3\nCNN\nLP\nCNN\nCo-SaliencyMapsCNN EncoderTSIR CNN Decoder\n!\"\n…cc\nSOD Samples\n# #ℒ\nFlattened FeaturePosition Encoding\nCNNCNN\nLPLP\nTSIRTSIRTSIR\nTGLLP\nTGFTGFTGF\nCNNCNN\nCNN\nLP\nTSIR\nLPLinear Projection\nContrastive Loss\nSkip Connection\n!(&)\n!(\n)(&)\n)\"\n)( )(\n)\"\n)(&)\n)*(\n)*\"\n)*(&)\nTGL TGF\n+(\n+\"\n+(&)\nShared Weights(a)\n(b)\n Single imageImage group\n…\nSaliency Maps D\nD ,-(.) ,/(.),0. 12(3)\n4×4and 3×6Conv \n12(7) 1/(7)\n1/(8)\n10(&)\n1-(&) 1/(&)\nmaximizeminimize \n… … … …\nFigure 2: The proposed CoSformer framework.\ndirect upsampling to the full resolution. In co-saliency detection task, Transformers can well model\nthe relation between different pixels, but can not recover ﬁne detailed information. On the other hand,\nCNN architectures (e.g.,U-Net [41]) provide an avenue for extracting low-level visual cues which\ncan well remedy such ﬁne details. To this end, CoSformer employs a CNN-Transformer architecture\nto leverage both detailed low-level detailed information from CNN features and the relation encoded\nby Transformers. We hope that the simplicity of our method will attract new researchers to the\nco-saliency detection community. Our proposed CoSformer architecture is simple and illustrated in\nFig.2.\n3.1 CNN Backbone\nStarting from the initial image I(n) ∈RH0×W0×3, a conventional CNN backbone (VGG-16 [42])\ngenerates different levels feature maps F(n)\nl ∈RHl×Wl×Cl . Following [66], we connect another side\npath to the last pooling layer in VGG-16, and only use the last four levels features for the following\nprocess. For simplicity, these four features can be denoted as a feature set F(n):\nF(n) = {F(n)\n3 ,F(n)\n4 ,F(n)\n5 ,F(n)\n6 }. (1)\n3.2 Transformer Encoder\nA good co-saliency detection framework should not only express the intra-saliency of an image,\nbut also reﬂect the interaction among group images for co-saliency referring. To address these two\nproblems, the proposed Transformer encoder contains three modules: (1) Transformer-based single\nimage representation learning (TSIR) module, which is used to processes each image individually\nto suppress background noise and capture the saliency of the potential co-salient objects. (2)\nTransformer-based group representation learning (TGL) module, which can explore all images in the\ngroup to learn the inter-image consistency. (3) Transformer-based group fusion (TGF) module, which\nfuses the learned inter-image consistency and unique intra-image saliency, so the group representation\nand single saliency representation are sufﬁciently exploited to facilitate the co-saliency reasoning.\n3.2.1 Preliminary Knowledge\nTransformer [46] is composed of multi-head attention (MHA) and fully connected feed forward\nnetwork (FFN). The FFN consists of a 1 ×1 convolution with ReLU activation. Layer Normalization\n(Norm) is usually added in each MHA and FFN. The structure of a Transformer layer is illustrated in\nFig.3.\n3.2.2 TSIR Module\nAs a basic rule in co-saliency, in most cases, the co-salient regions should be salient with respect\nto the background in each image. So the network should be able to suppress the background noise\n4\nCNN\nCNN\nTransformer Layer4×\nU-Net\nTransformer Layer6×\nTransformer Layer6×\nSkip\nSkip\nMHA\nAdd&Norm\nFFN\nAdd&Norm\nFigure 3: The Brief ﬂow chart of the proposed CoSformer.\nand learn the intra-saliency of the potential co-salient objects. To achieve this purpose, the network\nshould consider the relationships between different pixels, then highlights the salient pixels and\nsuppress noisy background pixels. While these pixels will locate in different positions, capturing\ntheir long-range dependencies is important. Thus we use Transformer to address this problem.\nTransformer.The TSIR module consists of four Transformer layers. First, a 1 ×1 convolution is\napplied to the F(n)\n6 , reducing the dimension from C6 to d, resulting in new feature maps F(n)\n6 ∈\nRH6×W6×d. To form a feature sequence that can be fed into the TSIR, we ﬂatten the spatial\ndimensions of F(n)\n6 , resulting in a 2D feature map of size Q×d, where Q= H6 ×W6. While the\nco-saliency detection task requires position information to locate co-salient objects. To compensate\nfor this, we supplement the features with ﬁxed positional encodings information suggested in [ 5],\nthat contains the two dimensional (horizontal and vertical) positional information. This encoding is\nadded to the input of each MHA. As can be seen in Fig.3, we use a skip connection to fuse the output\nfeatures of TSIR with the previous features. Finally, the fused features can be written as:\nS= {S(n)}N\nn=1, (2)\nwhere S∈ RN×Q×d.\nInspired by CoAD [64], in the training phase, in addition to the group inputs Iloaded from a CoSOD\ndataset, we simultaneously feed K auxiliary samples loaded from a SOD dataset into the shared\nCNN backbone and TSIR, generating single-image saliency maps H= {Hk)}K\nk=1. The saliency and\nco-saliency prediction is jointly optimized as a multi-task learning framework with better ﬂexibility\nand expansibility in terms of providing reliable saliency priors.\n3.2.3 TGL Module\nAfter TSIR module, the cleaner feature with less background noise will be obtained. Then, the\nnetwork should capture group-wise relationships to locate co-salient regions. Usually, co-salient\nobjects may be located at different positions across images, so well modeling the relationships\nbetween different pixels is difﬁcult for convolution operation. While the self-attention used in\nTransformer can calculate the relationships between all pixels in group images, which can help model\na robust global relationship. it is natural to use Transformer to capture group-wise relationships.\nMoreover, the learned group-wise relationship is insensitive to the input order of group images.\nBecause the Transformer models the pixel-level relationship among the group features.\nTransformer.To form a group level feature sequence that can be fed into the TGL module, we ﬂatten\nthe ﬁrst and second dimensions of Sinto one dimension, resulting in a group feature sequence Gof\nsize ∈RL×d. L= N ×Qis the length of the sequence. The GL module has 6 Transformer layers\nand each layer consists of an MHA and an FFN. The output of TGL is GL∈RL×d, which means the\ngroup representation of the image group. We do not add positional encodings on G.\n5\n3.2.4 TGF Module\nAs described previously, the group feature is then broadcasted to each individual image, which\nallows the network to leverage the synergetic information and unique properties between the images.\nWith group representation GL, the network can suppress non-co-salient pixels in S(n) and highlight\nco-salient pixels.\nTransformer.We ﬁrst use linear projection on GLto project it to the same size as S(n). The group\nfeature GLis then broadcasted to each individual image. Taking the concatenation of S(n) and GL\nas input, the Transformer decoder outputs feature S(n)\nG for each image. The TGF module has 6\nTransformer layers and each layer consists of an MHA and an FFN. Like TSIR, We also add ﬁxed\npositional encodings in each MHA.\n3.3 CNN Decoder\nAs the goal of the decoder is to generate the co-saliency results in the original 2D image space\n(H0×W0), we need to reshape the features fromL×dto a standard 3D feature mapV∈ RN×H×W×d.\nThe CNN decoder together with CNN backbone forms a U-shaped architecture that enables feature\naggregation at different resolution levels via skip-connections, as shown in Fig.3. It is worth to be\nraised that we do not design any extra modules in CNN decoder, so the performance improvement is\nmainly coming from the proposed Transformer encoder.\n4 Loss Function\nInspired by CoAD [64], we jointly optimize the co-saliency and single image saliency predictions\nin a multi-task learning framework. Similar to BASNet [ 39], we use pixel-level, region-level and\nobject-level supervision strategy to better keep the uniformity and wholeness of the co-salient objects.\nSpeciﬁcally, binary cross-entropy (BCE) [12], SSIM [51] and F-measure (Fm) loss [67] are denoted\nas pixel-level, region-level and object-level loss. we supervise the predicted co-saliency maps\nM= {M(n)}N\nn=1 by the corresponding groundtruth T = {T(n)}N\nn=1 under these three loss:\nLc = BCE(M,T) +SSIM(M,T) +Fm(M,T). (3)\nFor Kauxiliary saliency predictions H= {H(k)}K\nk=1 , we also supervise them with their groundtruth\nTs = {Ts\n(k)}K\nk=1 under BCE and Fm loss:\nLs = BCE(H,Ts) +Fm(H,Ts). (4)\nBecause we do not care about the boundary details of H, so we do not need use SSIM loss here.\nFor the limited space, more details about BCE, SSIM andFm losses can be found in Supplemental\nMaterials.\n4.1 Contrastive Loss\nAs described, the purpose of TGF module is to suppress noisy pixels and highlight the remaining\nco-salient pixels. So we add a novel contrastive loss that can promote the differences between noisy\nand co-salient pixels, which can help model inter-image separability. The existing contrastive learning\nmethods (e.g. [23, 9]) are a simple instance discrimination task. It treats each image as an individual\ninstance, and the purpose is that each image can be well distinguished by the contrastive learning\nframework. Next, we will describe the way to construct positive and negative samples for CoSOD\ntask.\nWe ﬁrst use a 3 ×3 and 1 ×1 convolution on {S(n)}N\nn=1 to obtain co-saliency maps {M(n)\nS }N\nn=1,\nwhich are supervised by BCE and Fm loss. We denote this loss function as Lct.\nMasks M(n)\nS and M(n) are binarized with a threshold of 0.5, then we can get a mask by:\nM(n)\nC = M(n)\nS ⊕M(n), (5)\nwhere ⊕means variance operation. Mask M(n) means ours current detected co-saliency map, while\nM(n)\nS means a saliency map that may contain many noisy regions. M(n)\nC means the difference\n6\n!(#) singleD %&(#) %'(#)\n%(#) ((#)\nmaximizeminimize \n)*(+) ),(+))-+ .-(#)\n.*(#) .,(#)\n./(0) ./(1) .,(1)\n.,(2)\ngroup\nFigure 4: Illustration of the proposed contrastive loss.\nbetween M(n)\nS and M(n).\nContrastive in single image.We ﬁrst compare M(n)\nC with groundtruth T(n), and get three masks:\nM(n)\nA = M(n)\nC ∩T(n),M(n)\nP = T(n) −M(n)\nC ,M(n)\nN = M(n)\nC −T(n), (6)\nwhere ∩means the intersection operation, and −means the subtraction operation. As can be seen in\nFig.4, M(n)\nN only contains noisy regions, and M(n)\nP and M(n)\nA form co-salient regions.\nThen we can get corresponding semantic features of {M(n)\nA ,M(n)\nP ,M(n)\nN }by multiplying them with\nfeature S(n)\nG , denoted as {Z(n)\nA ,Z(n)\nP ,Z(n)\nN }. We ensure the integrity of the co-salient objects by\nminimizing the distance between Z(n)\nA and Z(n)\nP , such as the pineapple and its leaves in Fig.4. On the\nother hand, we suppress noisy objects by maximizing the distance between Z(n)\nA and Z(n)\nN like the\npineapple and cat in Fig.4. Thus we can construct positive sample pairs {Z(n)\nA ,Z(n)\nP }and negative\nsample pairs {Z(n)\nA ,Z(n)\nN }in single image. So the contrastive loss in every single image can be\ndescribed as:\nLsingle = −\nN∑\ni=1\nlog exp(g(Z(i)\nA) ·g(Z(i)\nP )/τ)\n∑exp(g(Z(i)\nA) ·g(Z(i)\nN)/τ)\n. (7)\nContrastive in image group.For CoSOD, the co-salient regions in a image group can be considered\nas objects which share same semantic. And the indistinguishable regions in different images can help\nto establish negative relations with co-salient regions. Having more negative samples is crucial for\nlearning good representations. By maximizing the distance between them and the co-salient regions,\nmore constraints can be provided for the co-salient regions, so as to obtain a good embedding space.\nSo we can construct positive sample pairs {Z(i)\nT ,Z(j)\nT }and negative sample pairs {Z(i)\nT ,Z(m)\nN }in\nimage group, where i ̸= j , i,j,m ∈[1,N]. Z(i)\nT can be obtained by multiplying S(i)\nG with T(i).\nFinally, we get the following optimization criterion for image groups:\nLgroup = −\n∑\ni\n∑\nj\nlog exp(g(Z(i)\nT ) ·g(Z(j)\nT )/τ)\n∑\nmexp(g(Z(i)\nT ) ·g(Z(m)\nN )/τ)\n. (8)\nThe above g(·) is another non-linear projection head followed [9] and the temperature τ relaxes the\ndot product. The total contrastive loss can be written as:\nLcont = Lsingle + Lgroup. (9)\nNote that all parts of CoSformer are trained jointly, so the over all loss function is given as:\nL= Ls + Lc + Lct + Lcont. (10)\n5 Experiments\n5.1 Implementation Details\nFollowing [64, 65, 30, 19], We use VGG-16 as our backbone. The training set is a subset of the\nCOCO dataset [36] (9213 images) and saliency dataset DUTS [48], as suggested by [64]. In training\n7\nTable 1: Quantitative comparison with SOTA on three CoSOD datasets. The best two results are in\nred , green. Larger Emax\nφ , Sα, Fmax\nβ , smaller MAE mean better results.\nCoCA CoSOD3k Cosal2015Methods Type Emaxφ Sα Fmaxβ MAE Emaxφ Sα Fmaxβ MAE Emaxφ Sα Fmaxβ MAE\nCBCS(TIP2013)Co 0.641 0.523 0.313 0.180 0.637 0.528 0.466 0.228 0.656 0.544 0.532 0.233\nGWD(IJCAI2017)Co 0.701 0.602 0.408 0.166 0.777 0.716 0.649 0.147 0.802 0.744 0.706 0.148\nRCAN(IJCAI2019)Co 0.702 0.616 0.422 0.160 0.808 0.744 0.688 0.130 0.842 0.779 0.764 0.126\nCSMG(CVPR2019)Co 0.735 0.632 0.508 0.124 0.804 0.711 0.709 0.157 0.842 0.774 0.784 0.130\nCoEG(TPAMI2020)Co 0.717 0.616 0.499 0.104 0.825 0.762 0.736 0.092 0.882 0.836 0.832 0.077\nGICD(ECCV2020)Co 0.712 0.658 0.510 0.125 0.831 0.778 0.744 0.089 0.885 0.842 0.840 0.071\nICNet(NIPS2020)Co 0.698 0.651 0.506 0.148 0.832 0.780 0.743 0.097 0.900 0.856 0.855 0.058\nCoAD(NIPS2020)Co - - - - 0.874 0.822 0.786 0.078 0.915 0.861 0.857 0.063\nOurs Co 0.770 0.724 0.603 0.103 0.879 0.835 0.807 0.066 0.929 0.894 0.891 0.047\nEGNet(ICCV2019)Sin 0.631 0.595 0.388 0.179 0.793 0.762 0.702 0.119 0.843 0.818 0.786 0.099\nF3Net(AAAI2020)Sin 0.678 0.614 0.437 0.178 0.802 0.772 0.717 0.114 0.866 0.841 0.815 0.084\nMINet(CVPR2020)Sin 0.634 0.550 0.387 0.221 0.782 0.754 0.707 0.122 0.847 0.831 0.805 0.181\n“Axe” Group“Bow tie” Group“Croquetball” Group\nInputGTOursCoADICNetGICDCoEGCSMGCBCSF3N\n“Beaker”Group\nFigure 5: Visual comparison between our method and other SOTA methods. It can be clearly observed\nthat our method achieves impressive performance in all these cases.\niteration, 8 (N = 8) images from a sub-group of COCO dataset and 8 (K = 8) images from DUTS\nare simultaneously fed into the network for jointly optimizing. The images are all resized to256×256\nfor training and testing, and the output co-saliency maps are resized to the original size for evaluation.\nThe network is trained over 100 epochs in total with the Adam optimizer. The initial learning rate is\nset to 1e−4, β1 = 0.9 and β2 = 0.99.\n8\nTable 2: Ablation studies on the CoSOD3k and Cosal2015 datasets.\nConﬁgurations CoSOD3k Cosal2015\nBaseline TSIR TGL TGF Cont Emaxφ Sα Fmaxβ MAE Emaxφ Sα Fmaxβ MAE√ 0.785 0.720 0.655 0.144 0.807 0.748 0.710 0.145√ √ 0.803 0.735 0.695 0.128 0.840 0.783 0.767 0.122√ √ √ 0.839 0.784 0.738 0.089 0.880 0.851 0.840 0.078√ √ √ √ 0.860 0.822 0.785 0.071 0.910 0.871 0.875 0.061√ √ √ √ √ 0.879 0.835 0.807 0.066 0.929 0.894 0.891 0.047\n5.2 Evaluation Datasets and Metrics\nWe employ three challenging datasets for evaluation: CoCA [65], CoSOD3k [19], and Cosal2015 [60].\nThe last is a large dataset widely used in the evaluation of CoSOD methods. The ﬁrst two were recently\nproposed for challenging real-world co-saliency evaluation, with the images usually containing\nmultiple common and non-common objects against a complex background. We use maximum\nE-measure Emax\nφ [16], S-measure Sα [15], maximum F-measure Fmax\nβ [2], and mean absolute\nerror (MAE) to evaluate methods in our experiments. Evaluation toolbox: https://dpfan.net/\nCoSOD3K/.\n5.3 Comparisons with the State-of-the-Arts\nSince not all CoSOD models have publicly released codes or results, we only compare our CoSformer\nwith one representative traditional algorithm (CBCS [ 20]) and seven deepbased CoSOD models,\nincluding GWD [55], RCAN [33], CSMG [62], CoEG [19], GICD [65], ICNet [30], CoAD [64].\nWe also compare our method with 3 famous single-SOD methods EGNet [ 66], F3Net [ 52] and\nMINet [38].\nQuantitative Results.From Table.1, We can see that compared to other state-of-the-art methods, our\nmodel outperforms all of them in all metrics. For example, for dataset CoCA, our method improves\nthe performance by a large margin. Compared to the second ranked performance, the percentage\ngain reaches 4.7% for Emax\nφ , 10% for Sα, and 18.2% for Fmax\nβ . On the challenging CoSOD3k and\nCosal2015 datasets, our model capitalizes on our better consensus and signiﬁcantly outperforms\nother methods. These results demonstrate the efﬁciency of the proposed CoSformer framework and\ncontrastive loss. The second best method CoAD is established upon the VGG-16 backbone network,\ncontaining 121 MB parameters totally. The proposed CoSformer shares a very close number of\nparameters (115 MB). For the limited space, P-R curves can be found in Supplemental Materials.\nQualitative Results.Fig.5 shows the co-saliency maps generated by different methods for qualitative\ncomparison. As can be seen, the SOD method F3N can only detect salient objects and fail to\ndistinguish co-salient objects. The CoSOD methods perform better than the SOD methods because\nof considering group-wise relationships in designing the model. As can be seen in \"Beaker Group\",\nthese CoSOD can suppress some non-co-salient regions. However, these CoSOD methods only model\nfeature-level group relationships and lack consideration of inter-image separability. When facing\ncomplex real-world scenarios, they are unable to handle these challenging cases, like \"Axe Group\"\nand \"Bow tie Group\", where non-co-salient objects are very close to co-salient objects. While our\nproposed CoSformer models the pixel-level group relationships, and use contrastive loss to model\ninter-image separability, therefore performs much better on detecting co-salient objects.\n5.4 Ablation Studies\nTo verify our contributions, we design different variants of our CoSformer with the VGG-16 backbone\nby replacing the three key modules (i.e., TSIR, TGL and TGF). To construct our baseline model, we\nsimpliﬁed the CoSformer as follows: 1) replacing the TSIR module with standard 3 ×3 convolutions;\n2) replacing the TGL module with direct concatenation followed by a 1 ×1 convolution; 3) replacing\nTGF with standard 3 ×3 convolutions layers. We train the baseline model in BCE, SSIM, and Fm\nlosses. The baseline model is carefully designed to share a very similar parameter number with the\nfull model.\nIn Fig.6, it is observed that the baseline model can roughly locate the salient object, but fails to\nsuppress the non-common salient object and background (red boxes). As can be seen in Table.2,\n9\nInputBaseline+TSIR+TGL+TGF+ContrastiveGT\n“Chook” Group“Baseball” Group\nFigure 6: Visualization of different ablative results. From left to right: Input image, Co-saliency\nmaps produced by the Baseline, Baseline+TSIR, Baseline+TSIR+TGL, Baseline+TSIR+TGL+TGF\nand Baseline+TSIR+TGL+TGF+Contrastive Loss\nImgTSIRTGFCo-saliencyMaps\n“Monkey” Group“Lemon” Group\nFigure 7: Visualization of TSIR and TGF.\nBy introducing the TSIR, background regions are effectively suppressed, thereby promoting the\npercentage gain of Fmax\nβ reaches 6.1% on CoSOD3k and 4.6% on Cosal2015. Then, introducing the\nTGL module that learns more discriminative group semantic representations further suppresses the\nnon-common salient objects, and boosts the performance with large margins. Subsequently, the TGF\nmodule can further suppress non-co-salient pixels. As can be seen in Fig.6, even though TGF can\nsuppress the noise as much as possible, some hard pixels which are close to real co-salient regions\nare still preserved. Hence, the proposed contrastive loss (Cont) is designed to solve this problem,\nwhich can help model inter-image separability. With the contrastive loss, the co-salient objects are\nhighlighted, which further boosts the whole framework to the state-of-the-art on all datasets.\nTo get a deeper understanding of the proposed methods, we visualize the learned features from TSIR\nand TGF in Fig.7. The features from TSIR may contain much noise, such as \"car\", \"strawberries\".\nAfter our proposed pixel-level group relationships modeling and contrastive loss, the features in TGF\ncan be cleaner and focus on co-salient regions.\n6 Conclusion\nRecent methods typically develop sophisticated deep learning based models have greatly improved\nthe performance of CoSOD task. But there are still two major drawbacks that need to be further\naddressed, 1) sub-optimal inter-image relationship modeling; 2) lacking consideration of inter-image\nseparability. In this paper, we propose the Co-Salient Object Detection Transformer (CoSformer)\nnetwork to capture both salient and common visual patterns from multiple images. By leveraging\nTransformer architecture, the proposed method address the inﬂuence of the input orders and greatly\nimprove the stability of the CoSOD task. We also introduce a novel concept of inter-image separability.\nWe construct a contrast learning scheme to modeling the inter-image separability and learn more\ndiscriminative embedding space to distinguish true common objects from noisy objects. Extensive\n10\nexperiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate\nthat our CoSformer outperforms cutting-edge models and achieves the new state-of-the-art.\n11\nSupplementary Materials for CoSformer: Detecting\nCo-Salient Object with Transformers\nLv Tang\nNanjing University\nluckybird1994@gmail.com\nBo Li\nIndependent Researcher\nnjumagiclibo@gmail.com\n7 Introduction\nThis supplemental material contains three parts:\n• Section 8 gives more quantitative and qualitative experimental results to demonstrate the\nsuperiority of our CoSformer.\n• Section 9 gives more details about the BCE, SSIM and Fm losses, and analyzes the role\nthey play in co-saliency detection task.\n• Section 10 gives more analyses of the proposed TGL module, which further veriﬁes the\nTGL can make the CoSformer insensitive to the input order of group images.\nWe hope this supplemental material can help you get a better understanding of our work.\n8 More Quantitative and Qualitative Results\n8.1 Quantitative Comparison on more datasets\nWe compare our method with other methods on another two conventional CoSOD datasets iCoseg [1]\nand MSRC [56]. The results are shown in Table.3. We can see that compared to other state-of-the-art\nmethods, our model outperforms all of them in all metrics.\n8.2 Qualitative Comparison\nAs shown in Fig.8, we provide a comprehensive qualitative comparison of our method with other\nstate-of-the-art (SOTA) methods on challenging cases. When facing complex real-world scenarios,\nother methods are unable to handle these challenging cases, like \"snail Group\", \"Hat Group\" and\n\"Basketball Group\", where non-co-salient objects are very close to co-salient objects. While our\nproposed CoSformer models the pixel-level group relationships, and uses contrastive loss to model\ninter-image separability, therefore performs much better on detecting co-salient objects. As shown\nin Fig.9, we can see that our method (the red line) achieves the highest precision on all datasets.\nOur CoSformer runs averagely at 40 FPS on an Nvidia 2080Ti GPU. In conclusion, both qualitative\nand quantitative results in the main text and supplementary material demonstrate the superiority and\neffectiveness of our proposed CoSformer.\n9 Details of Losses\nAs described in main text, Transformer can well model intra-image saliency and inter-image con-\nsistency. Contrast learning scheme can help model the inter-image separability and learn more\ndiscriminative representations to distinguish true common objects from noisy objects. Hence, as can\nbe seen in Table.4, if we only supervise the predicted co-saliency maps M= {M(n)}N\nn=1 by the\n12\nTable 3: Quantitative comparison with SOTA methods on another two conventional datasets.\niCoSeg MSRCModel Type Emaxφ Sα Fmaxβ MAE Emaxφ Sα Fmaxβ MAE\nCBCS(TIP2013) Co 0.797 0.658 0.705 0.172 0.676 0.480 0.630 0.314\nGWD(IJCAI2017) Co 0.841 0.801 0.829 0.132 0.789 0.719 0.727 0.210\nRCAN(IJCAI2019) Co 0.878 0.820 0.841 0.122 0.789 0.719 0.727 0.210\nCSMG(CVPR2019) Co 0.889 0.821 0.850 0.106 0.859 0.722 0.847 0.190\nCoEG(TPAMI2020)Co 0.912 0.875 0.876 0.060 0.793 0.696 0.751 0.188\nGICD(ECCV2020) Co 0.891 0.832 0.845 0.068 0.726 0.665 0.692 0.196\nICNet(NIPS2020) Co 0.929 0.869 0.886 0.047 0.822 0.731 0.805 0.160\nCoAD(NIPS2020) Co 0.930 0.878 0.889 0.045 0.850 0.782 0.842 0.132\nOurs Co 0.943 0.904 0.907 0.038 0.869 0.795 0.852 0.122\nEGNet(ICCV2019) Sin 0.911 0.875 0.875 0.060 0.794 0.702 0.752 0.186\nF3Net(AAAI2020) Sin 0.918 0.879 0.874 0.048 0.811 0.733 0.763 0.161\nMINet(CVPR2020) Sin 0.846 0.789 0.784 0.099 0.769 0.688 0.729 0.194\n“Snali”Group“Frog”Group“Mushroom”Group “Basketball”Group“Hat with a wide brim”Group\nInputGTOursCoADICNetGICDCoEGCSMGCBCSF3N\nFigure 8: Visual comparison between our method and other state-of-the-art methods.\nPrecision\nRecall\nPrecision\nRecall\nPrecision\nRecall\nPrecision\nRecall\nPrecision\nRecall\nCoCACoSOD3KCosal2015iCoSegMSRC\nFigure 9: Comparison of PR curves across ﬁve CoSOD datasets.\ncorresponding groundtruth T = {T(n)}N\nn=1 under contrastive loss and BCE loss (Cont+ BCE),\nthe performance can already outperform the second ranked performance (COAD) by a large margin,\nwhich demonstrates the efﬁciency of the proposed CoSformer framework and contrast learning\nscheme.\n13\nTable 4: Ablation Studies of Losses\nCoSOD3k Cosal2015Conﬁgurations Emax\nφ Sα Fmax\nβ MAE Emax\nφ Sα Fmax\nβ MAE\nCoAD(NIPS2020) 0.874 0.822 0.786 0.078 0.915 0.861 0.857 0.063\nCont+BCE 0.878 0.828 0.800 0.071 0.921 0.888 0.884 0.052\nCont+BCE+SSIM 0.878 0.831 0.803 0.068 0.925 0.890 0.889 0.050\nCont+BCE+SSIM+Fm 0.879 0.835 0.807 0.066 0.929 0.894 0.891 0.047\nTable 5: Analyses of TGL module.\nCoSOD3k Cosal2015Conﬁgurations Emax\nφ Sα Fmax\nβ MAE Emax\nφ Sα Fmax\nβ MAE\nOurs(order1) 0.879 0.835 0.807 0.066 0.929 0.894 0.891 0.047\nOurs(order2) 0.879 0.835 0.807 0.066 0.929 0.894 0.891 0.047\nOurs(order3) 0.879 0.835 0.807 0.066 0.929 0.894 0.891 0.047\nOurs(Std) 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\nOurs-P(order1) 0.870 0.830 0.800 0.070 0.919 0.887 0.885 0.051\nOurs-P(order2) 0.878 0.835 0.807 0.066 0.928 0.893 0.891 0.047\nOurs-P(order3) 0.874 0.829 0.802 0.068 0.924 0.889 0.886 0.050\nOurs-P(Std) 0.005 0.003 0.004 0.002 0.004 0.004 0.003 0.002\nRCAN(order1) 0.808 0.744 0.688 0.130 0.842 0.779 0.764 0.126\nRCAN(order2) 0.800 0.732 0.680 0.138 0.831 0.764 0.754 0.136\nRCAN(order3) 0.804 0.739 0.685 0.132 0.838 0.772 0.758 0.130\nRCAN(Std) 0.006 0.007 0.004 0.004 0.006 0.005 0.006 0.002\nICNet(order1) 0.832 0.780 0.743 0.097 0.900 0.856 0.855 0.058\nICNet(order2) 0.827 0.771 0.736 0.102 0.893 0.850 0.845 0.062\nICNet(order3) 0.825 0.775 0.739 0.101 0.896 0.852 0.850 0.060\nICNet(Std) 0.007 0.005 0.003 0.004 0.005 0.004 0.008 0.003\nHowever, because of limited GPU memory , we only use the highest level feature F(n)\n6 (8 ×8\nresolution), which contains less detailed information, to model group-wise relationships. To make\nﬁnal predicted co-saliency maps contain more detailed information, we ﬁrst use a simple U-shaped\narchitecture that enables feature aggregation at low-level via skip-connections. Moreover, inspired by\nBASNet [39], which uses pixel-level, region-level and object-level supervision strategy to predict the\nsalient objects with ﬁne structures and clear boundaries, we also useSSIM and Fmlosses in addition\nto BCE loss. The results can be seen in Table.4. A better performance has been achieved through\nthe combination of BCE, SSIM and Fm. The work [19] addresses that predicted co-saliency maps\nwith ﬁne boundaries is one of future directions, and we try to address this problem by simply using\nthe SSIM and Fm losses in this paper. While the main contributions of this paper are the proposed\nCoSformer framework and contrast learning scheme.\nThe equations of BCE,SSIM and Fm are shown below. It should be noted that, if the model is\nonly trained with Cont+ BCE or Cont+ BCE + SSIM, the Ls (line.480 in main text) and Lct\n(line.516 in main text) only contain BCE loss.\nThe BCE loss is deﬁned as:\nBCE =\nN∑\nn=1\n−(T(n)log(M(n)) + (1−T(n))log(1 −M(n))). (11)\nFollowing the setting of [ 51, 15], we use the sliding window fashion to model region similarity\nbetween groundtruth and saliency map. The corresponding regions are denoted as M(n)\ni = {M(n)\ni :\ni= 1,...D}and T(n)\ni = {T(n)\ni : i= 1,...D}, where Dis the total number of region. Then we use\nSSIM to evaluate the similarity between M(n)\ni and G(n)\ni , which is deﬁned as:\nSSD(n)\ni = (2µmµt + C1)(2σmt + C2)\n(µ2m + µ2\nt + C1)(σ2m + σ2\nt + C2) (12)\n14\nwhere local statistics µm, σm is mean and std vector of S(n)\ni , µt, σt is mean and std vector of T(n)\ni .\nThe overall loss function is deﬁned as:\nSSIM =\nN∑\nn=1\n(1 − 1\nD\nD∑\ni=1\nSSD(n)\ni ). (13)\nFinally, inspired by [67], we directly optimize the F-measure to learn the global information from\ngroundtruth. For easy remembering, we denote F-measure as Fβ in the following. F(n)\nβ is deﬁned as:\nprecision(n) =\n∑M(n) ·T(n)\n∑M(n) + ϵ , recall(n) =\n∑M(n) ·T(n)\n∑T(n) + ϵ , (14)\nF(n)\nβ = (1 +β2) ·precision(n) ·recall(n)\nβ2 ·precision(n) + recall(n) , (15)\nwhere ·means pixel-wise multiplication, ϵ= 1e−7 is a regularization constant to avoid division of\nzero. LObject loss function is deﬁned as:\nFm =\nN∑\nn=1\n(1 −F(n)\nβ ). (16)\n10 Analyses of TGL module\nIn main text, we claim that our proposed TGL can let the CoSformer insensitive to the input order of\ngroup images, which can greatly improve the stability of CoSOD network. We further verify this\nthrough experiments, and the results are shown in Table.5. Speciﬁcally, during testing, for each image\ngroup, we randomize 10 different orders, and only show three results (Ours) in Table.5 because of\nlimited space. It can be seen that performance has no change. Ours(Std) means the standard deviation\nof these 10 orders. This result veriﬁes that our proposed TGL can let the CoSformer insensitive to the\ninput order of group images. Moreover, we do additional experiments to see what the impact would\nbe when the positional encoding is added to the TGL module during training and testing. During\ntesting, for each image group, we randomize 10 different orders, and show three results (Ours-P) in\nTable.5 as representations. It can be seen that extra positional encoding information will destabilize\nthe CoSformer framework. Positional encoding assigning input order to related images, if the order of\nan image changed, the output group representation from TGL will be different. So adding positional\nencoding in TGL would make inter-image relationship modeling sub-optimal.\nIn the Introduction of the main text, we argue that existing sequential order modeling approaches make\nCoSOD networks unstable. So we do experiments on two typical methods, including RCAN [33] and\nICNet [30], to verify the inferring procedure of these two methods are unstable. Because CoAD [64]\ndoes not release their code, so we can not do experiments on CoAD. During testing, for each image\ngroup, we randomize 10 different orders, and the three results are shown in Table.5. As can be seen\nin Table.5, whether using RNN (RCAN), or applying some sophisticated modiﬁcation on CNNs\narchitectures (ICNet), can not let the CoSOD network insensitive to the input order of group images,\nleading to an unstable inferring procedure. Because in sequential order modeling, both CNNs and\nRNNs have inherent deﬁciencies. Through these experiments, we further verify that the proposed\nCoSformer greatly improves the stability CoSOD network.\nReferences\n[1] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and Tsuhan Chen. icoseg: Interactive co-\nsegmentation with intelligent scribble guidance. In CVPR, pages 3169–3176, 2010.\n[2] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark. IEEE TIP,\n24(12):5706–5722, 2015.\n[3] Xiaochun Cao, Yupeng Cheng, Zhiqiang Tao, and Huazhu Fu. Co-saliency detection via base reconstruc-\ntion. In Proceedings of the ACM International Conference on Multimedia, MM ’14, Orlando, FL, USA,\nNovember 03 - 07, 2014, pages 997–1000, 2014.\n[4] Xiaochun Cao, Zhiqiang Tao, Bao Zhang, Huazhu Fu, and Wei Feng. Self-adaptively weighted co-saliency\ndetection via rank constraint. IEEE Trans. Image Process., 23(9):4175–4186, 2014.\n15\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV (1), volume 12346 of Lecture Notes\nin Computer Science, pages 213–229. Springer, 2020.\n[6] Kai-Yueh Chang, Tyng-Luh Liu, and Shang-Hong Lai. From co-saliency to co-segmentation: An efﬁcient\nand fully unsupervised energy minimization model. In cvpr, pages 2129–2136, 2011.\n[7] Hwann-Tzong Chen. Preattentive co-saliency detection. In Proceedings of the International Conference\non Image Processing, ICIP, pages 1117–1120. IEEE, 2010.\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing transformer. CoRR, abs/2012.00364, 2020.\n[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. In Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119, pages 1597–1607, 2020.\n[10] Runmin Cong, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng, Weisi Lin, and Qingming Huang. Review\nof visual saliency detection with comprehensive information. IEEE Trans. Circuits Syst. Video Technol.,\n29(10):2941–2959, 2019.\n[11] Runmin Cong, Jianjun Lei, Huazhu Fu, Fatih Porikli, Qingming Huang, and Chunping Hou. Video saliency\ndetection via sparsity-based reconstruction and propagation. IEEE Trans. Image Process., 28(10):4819–\n4831, 2019.\n[12] Pieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y . Rubinstein. A tutorial on the cross-\nentropy method. Ann. Oper. Res., 134(1):19–67, 2005.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR,\nabs/2010.11929, 2020.\n[14] Philipp Dufter, Martin Schmitt, and Hinrich Schütze. Position information in transformers: An overview.\nCoRR, abs/2102.11090, 2021.\n[15] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji. Structure-measure: A new way to\nevaluate foreground maps. In ICCV, pages 4558–4567(2017). IEEE, 2017.\n[16] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, and Ali Borji. Enhanced-alignment\nmeasure for binary foreground map evaluation. In IJCAI, pages 698–704. ijcai.org, 2018.\n[17] Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng, Huazhu Fu,\nand Jianbing Shen. Re-thinking co-salient object detection. CoRR, abs/2007.03380, 2020.\n[18] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, and Ming-Ming Cheng. Taking a\ndeeper look at co-salient object detection. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 2916–2926, 2020.\n[19] Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng, Huazhu Fu,\nand Jianbing Shen. Re-thinking co-salient object detection. arXiv preprint arXiv:2007.03380, 2020.\n[20] Huazhu Fu, Xiaochun Cao, and Zhuowen Tu. Cluster-based co-saliency detection. IEEE Trans. Image\nProcess., 22(10):3766–3778, 2013.\n[21] Junwei Han, Dingwen Zhang, Gong Cheng, Nian Liu, and Dong Xu. Advanced deep-learning techniques\nfor salient and category-speciﬁc object detection: A survey. IEEE Signal Process. Mag., 35(1):84–100,\n2018.\n[22] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,\nChunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao. A survey on visual transformer.\nCoRR, abs/2012.12556, 2020.\n[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsu-\npervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR, 2020, pages 9726–9735, 2020.\n[24] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Deepco3: Deep instance co-segmentation by co-peak\nsearch and co-saliency detection. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, pages 8846–8855, 2019.\n[25] Jiagao Hu, Zhengxing Sun, Bo Li, Kewei Yang, and Dongyang Li. Online user modeling for interactive\nstreaming image classiﬁcation. In International Conference on Multimedia Modeling, pages 293–305.\nSpringer, 2017.\n[26] Koteswar Rao Jerripothula, Jianfei Cai, and Junsong Yuan. CATS: co-saliency activated tracklet selection\nfor video co-localization. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part VII, pages 187–202, 2016.\n[27] Koteswar Rao Jerripothula, Jianfei Cai, and Junsong Yuan. Efﬁcient video object co-localization with\nco-saliency activated tracklets. IEEE Trans. Circuits Syst. Video Technol., 29(3):744–755, 2019.\n[28] Bo Jiang, Xingyue Jiang, Ajian Zhou, Jin Tang, and Bin Luo. A uniﬁed multiple graph learning and\nconvolutional network model for co-saliency estimation. In ACM Multimedia, pages 1375–1382. ACM,\n2019.\n16\n[29] Wenda Jin, Jun Xu, Ming-Ming Cheng, Yi Zhang, and Wei Guo. Icnet: Intra-saliency correlation network\nfor co-saliency detection. In Advances in Neural Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n[30] Wen-Da Jin, Jun Xu, Ming-Ming Cheng, Yi Zhang, and Wei Guo. Icnet: Intra-saliency correlation network\nfor co-saliency detection. Advances in Neural Information Processing Systems, 33, 2020.\n[31] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. CoRR, abs/2101.01169, 2021.\n[32] Bo Li, Zhengxing Sun, Qian Li, Yunjie Wu, and Anqi Hu. Group-wise deep object co-segmentation with\nco-attention recurrent neural network. In 2019 IEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 8518–8527, 2019.\n[33] Bo Li, Zhengxing Sun, Lv Tang, Yunhan Sun, and Jinlong Shi. Detecting robust co-saliency with recurrent\nco-attention neural network. In Proceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 818–825, 2019.\n[34] Hongliang Li and King Ngi Ngan. A co-saliency model of image pairs. IEEE Trans. Image Process.,\n20(12):3365–3375, 2011.\n[35] YiJun Li, Keren Fu, Zhi Liu, and Jie Yang. Efﬁcient saliency-model-guided visual co-saliency detection.\nIEEE Signal Process. Lett., 22(5):588–592, 2015.\n[36] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. Microsoft COCO: common objects in context. In Computer Vision - ECCV\n2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages\n740–755, 2014.\n[37] Jiawei Liu, Zheng-Jun Zha, Xierong Zhu, and Na Jiang. Co-saliency spatio-temporal interaction network\nfor person re-identiﬁcation in videos. In IJCAI, pages 1012–1018. ijcai.org, 2020.\n[38] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient\nobject detection. In CVPR, pages 9410–9419. IEEE, 2020.\n[39] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jägersand. Basnet:\nBoundary-aware salient object detection. In CVPR, pages 7479–7489. Computer Vision Foundation /\nIEEE, 2019.\n[40] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object\ndetection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 39(6):1137–1149, 2017.\n[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI (3), pages 234–241. Springer, 2015.\n[42] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. In ICLR, 2015.\n[43] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu\nWang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. CoRR, abs/2012.15460, 2020.\n[44] Kevin D. Tang, Armand Joulin, Li-Jia Li, and Fei-Fei Li. Co-localization in real-world images. In CVPR,\npages 1464–1471. IEEE Computer Society, 2014.\n[45] Lv Tang and Bo Li. Class: Cross-level attention and supervision for salient objects detection. InProceedings\nof the Asian Conference on Computer Vision (ACCV), November 2020.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pages 5998–6008, 2017.\n[47] Chong Wang, Zheng-Jun Zha, Dong Liu, and Hongtao Xie. Robust deep co-saliency detection with group\nsemantic. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First\nInnovative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -\nFebruary 1, 2019, pages 8917–8924, 2019.\n[48] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan.\nLearning to detect salient objects with image-level supervision. In CVPR, pages 3796–3805(2017). IEEE,\n2017.\n[49] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, and Haibin Ling. Salient object detection in the\ndeep learning era: An in-depth survey. CoRR, abs/1904.09146, 2019.\n[50] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia.\nEnd-to-end video instance segmentation with transformers. CoRR, abs/2011.14503, 2020.\n[51] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from\nerror visibility to structural similarity. IEEE TIP, 13(4):600–612, 2004.\n[52] Jun Wei, Shuhui Wang, and Qingming Huang. F3net: Fusion, feedback and focus for salient object\ndetection. CoRR, abs/1911.11445, 2019.\n[53] Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang, and Qi Tian. Label decoupling framework for\nsalient object detection. In CVPR, pages 13022–13031. IEEE, 2020.\n17\n[54] Lina Wei, Shanshan Zhao, Omar El Farouk Bourahla, Xi Li, and Fei Wu. Group-wise deep co-saliency\ndetection. In Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI 2017, Melbourne, Australia, August 19-25, 2017, pages 3041–3047, 2017.\n[55] Lina Wei, Shanshan Zhao, Omar El Farouk Bourahla, Xi Li, Fei Wu, and Yueting Zhuang. Deep group-wise\nfully convolutional network for co-saliency detection with graph propagation. IEEE Trans. Image Process.,\n28(10):5052–5063, 2019.\n[56] John M. Winn, Antonio Criminisi, and Thomas P. Minka. Object categorization by learned universal visual\ndictionary. In ICCV, pages 1800–1807, 2005.\n[57] Xiwen Yao, Junwei Han, Dingwen Zhang, and Feiping Nie. Revisiting co-saliency detection: A novel\napproach based on two-stage multi-view spectral rotation co-clustering. IEEE Trans. Image Process.,\n26(7):3196–3209, 2017.\n[58] Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, and Xuelong Li. A review of co-saliency detection\nalgorithms: Fundamentals, applications, and challenges. ACM Trans. Intell. Syst. Technol., 9(4):38:1–38:31,\n2018.\n[59] Dingwen Zhang, Junwei Han, Jungong Han, and Ling Shao. Cosaliency detection based on intrasaliency\nprior transfer and deep intersaliency mining. IEEE Trans. Neural Networks Learn. Syst., 27(6):1163–1176,\n2016.\n[60] Dingwen Zhang, Junwei Han, Chao Li, Jingdong Wang, and Xuelong Li. Detection of co-salient objects\nby looking deep and wide. International Journal of Computer Vision, 120(2):215–232, 2016.\n[61] Dingwen Zhang, Deyu Meng, and Junwei Han. Co-saliency detection via a self-paced multiple-instance\nlearning framework. IEEE Trans. Pattern Anal. Mach. Intell., 39(5):865–878, 2017.\n[62] Kaihua Zhang, Tengpeng Li, Bo Liu, and Qingshan Liu. Co-saliency detection via mask-guided fully\nconvolutional networks with multi-scale label smoothing. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3095–3104, 2019.\n[63] Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen, and Qingshan Liu. Adaptive graph\nconvolutional network with attention graph clustering for co-saliency detection. In CVPR, pages 9047–\n9056. IEEE, 2020.\n[64] Qijian Zhang, Runmin Cong, Junhui Hou, Chongyi Li, and Yao Zhao. Coadnet: Collaborative aggregation-\nand-distribution networks for co-salient object detection. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020.\n[65] Zhao Zhang, Wenda Jin, Jun Xu, and Ming-Ming Cheng. Gradient-induced co-saliency detection. In\nECCV (12), volume 12357 of Lecture Notes in Computer Science, pages 455–472. Springer, 2020.\n[66] Jiaxing Zhao, Jiangjiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet:\nEdge guidance network for salient object detection. In ICCV, pages 8778–8787. IEEE, 2019.\n[67] Kai Zhao, Shanghua Gao, Wenguan Wang, and Ming-Ming Cheng. Optimizing the f-measure for threshold-\nfree salient object detection. In ICCV, pages 8848–8856. IEEE, 2019.\n[68] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. CoRR, abs/2012.15840, 2020.\n18"
}