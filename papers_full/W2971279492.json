{
  "title": "Short Text Understanding Combining Text Conceptualization and Transformer Embedding",
  "url": "https://openalex.org/W2971279492",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2012836890",
      "name": "Jun Li",
      "affiliations": [
        "Guilin University of Electronic Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102878200",
      "name": "Guimin Huang",
      "affiliations": [
        "Guilin University of Electronic Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2253955300",
      "name": "Jianheng Chen",
      "affiliations": [
        "Guilin University of Electronic Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2340942746",
      "name": "Yabing Wang",
      "affiliations": [
        "Guilin University of Electronic Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6732025295",
    "https://openalex.org/W2105424043",
    "https://openalex.org/W2099868020",
    "https://openalex.org/W1529533208",
    "https://openalex.org/W2892280852",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963595025",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W6750384732",
    "https://openalex.org/W2739722817",
    "https://openalex.org/W2165612380",
    "https://openalex.org/W2803241296",
    "https://openalex.org/W2913932916",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W2404527150",
    "https://openalex.org/W6691016536",
    "https://openalex.org/W2249777566",
    "https://openalex.org/W2519290916",
    "https://openalex.org/W6785755946",
    "https://openalex.org/W2434898965",
    "https://openalex.org/W6730529904",
    "https://openalex.org/W2293428384",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W6628877408",
    "https://openalex.org/W2901078833",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2000411838",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6696905269",
    "https://openalex.org/W2067506377",
    "https://openalex.org/W2164385956",
    "https://openalex.org/W6629486361",
    "https://openalex.org/W2574626468",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3104486441",
    "https://openalex.org/W2252137719",
    "https://openalex.org/W2293188561",
    "https://openalex.org/W2962808855",
    "https://openalex.org/W1491611863",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Short text understanding is a key task and popular issue in current natural language processing. Because the content of short texts is characterized by sparsity and semantic limitation, the traditional search methods that analyze only the semantics of literal text for short text understanding and similarity matching have certain restrictions. In this paper, we propose a combined method based on knowledge-based conceptualization and a transformer encoder. Specifically, for each term in a short text, we obtain its concepts and enrich the short text information from a knowledge base based on cooccurrence terms and concepts, construct a convolutional neural network (CNN) to capture local context information, and introduce the subnetwork structure based on a transformer embedding encoder. Then, we embed these concepts into a low-dimensional vector space to obtain more attention from these concepts based on a transformer. Finally, the concept space and transformer encoder space construct the understanding models. An experiment shows that the method in this paper can effectively capture more semantics of short texts and can be applied to a variety of applications, such as short text information retrieval and short text classification.",
  "full_text": "Received June 26, 2019, accepted August 24, 2019, date of publication August 29, 2019, date of current version September 11, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2938303\nShort Text Understanding Combining\nText Conceptualization and\nTransformer Embedding\nJUN LI\n 1, GUIMIN HUANG2,3, JIANHENG CHEN1, AND YABING WANG2\n1School of Information and Communication, Guilin University of Electronic Technology, Guilin 541004, China\n2School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin 541004, China\n3Guangxi Key Laboratory of Trusted Software, Guilin University of Electronic Technology, Guilin 541004, China\nCorresponding author: Guimin Huang (send_huang@126.com)\nThis work was supported in part by the National Natural Science Foundation of China under Grant 61662012, in part by the Foundation of\nKey Laboratory of Cognitive Radio and Information Processing, and in part by the Ministry of Education, Guilin University of Electronic\nTechnology, under Grant CRKL150105.\nABSTRACT Short text understanding is a key task and popular issue in current natural language processing.\nBecause the content of short texts is characterized by sparsity and semantic limitation, the traditional\nsearch methods that analyze only the semantics of literal text for short text understanding and similarity\nmatching have certain restrictions. In this paper, we propose a combined method based on knowledge-based\nconceptualization and a transformer encoder. Speciﬁcally, for each term in a short text, we obtain its concepts\nand enrich the short text information from a knowledge base based on cooccurrence terms and concepts,\nconstruct a convolutional neural network (CNN) to capture local context information, and introduce the\nsubnetwork structure based on a transformer embedding encoder. Then, we embed these concepts into a\nlow-dimensional vector space to obtain more attention from these concepts based on a transformer. Finally,\nthe concept space and transformer encoder space construct the understanding models. An experiment shows\nthat the method in this paper can effectively capture more semantics of short texts and can be applied to a\nvariety of applications, such as short text information retrieval and short text classiﬁcation.\nINDEX TERMS Short text understanding, text conceptualization, transformer encoder.\nI. INTRODUCTION\nCurrently, with the rapid development of the Internet, short\ntext can be seen everywhere, and relevant studies based on\nshort text, such as information extraction and text classiﬁ-\ncation [1], especially question-answering (QA) systems and\nshort text understanding [2], are receiving increasing atten-\ntion. Unlike long texts, short texts lack sufﬁcient semantic\ninformation for further processing. For example, the word\n‘‘Apple’’ in the short text ‘‘Steve Jobs is the founder of\nApple’’ and the word ‘‘apple’’ in ‘‘this kind of apple is sweet’’\nhave completely different meaning, but it is difﬁcult for the\nmachine to distinguish this word between the two sentences.\nThis lack of sufﬁcient semantic information will eventually\nlead to errors in our understanding of the text, and the existing\ntext analysis methods and algorithms are not well suited to\nshort texts [3], [4]. In particular, short texts that lack rich\nThe associate editor coordinating the review of this article and approving\nit for publication was Mohamed Elhoseny.\nbackground information are more likely to be ambiguous and\ndifﬁcult to understand. As in the previous example, if the\nmachine can identify that the word ‘‘apple’’ belongs to the\nconcepts ‘‘fruit’’ or ‘‘company’’, it can help the computer\nunderstand the entire text. Conceptualization [5], [6] con-\ncentrates objects and contains rich semantic information,\nespecially for short texts that lack more text information.\nIn this paper, we enrich the semantic information of short text\nthrough conceptualization, and we propose a novel approach\nfor short text understanding. There are two components in our\napproach: i) introduce textual conceptualization and enrich\nshort texts with cooccurrence terms and concepts; ii) con-\nstruct a convolutional neural network (CNN) to automatically\nlearn high-level features and redesign the subnetwork struc-\nture based on a transformer encoder.\nTo obtain the appropriate hidden semantics for a noun term\nunder different contexts, we need conceptual knowledge [7].\nFor example, given the word ‘‘apple’’, we conceptualize it\nto concepts such as ‘‘fruit’’ and ‘‘company’’; in the context\nVOLUME 7, 2019 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ 122183\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\n‘‘Steve Jobs is the founder of Apple’’, we conceptualize\n‘‘apple’’ to the more suitable concept ‘‘company’’. Actually,\nmany terms can belong to several types, and each type has\ndifferent semantics in different contexts. We aim to ﬁnd the\nappropriate type based on the contextual information. Tradi-\ntional methods for determining lexical types rely on linguistic\nrules or learn semantics from a tagged corpus. However, it is\ndifﬁcult to apply these methods to short texts because the\nlinguistic rules of short texts are not always normative or the\ncontext information is not rich enough. Therefore, we intro-\nduce text conceptualization, and the concept information is\nmore powerful in capturing the semantic information of the\nshort text because it can explicitly express the conceptual\nsemantic information of a term. In other words, we ﬁrst\nmap the short text to a high-dimensional concept space. The\nconcept space we use is provided by ConceptNet [8], which is\na largescale, graphical, commonsense database that contains\na large amount of information about the world that a computer\nshould know. This information helps the computer to per-\nform better searches, answer questions and understand human\nintentions. It consists of nodes representing concepts that are\nexpressed in natural language words or phrases and indicates\nthe relationships of these concepts. Through the effective\nanalysis of query and text concept space, we can judge the\nconcepts of entities corresponding to multiple concepts in\na speciﬁc context to achieve concept-level reasoning and\nimprove the ability to generalize model knowledge.\nHowever, text conceptualization sometimes does not com-\npletely distinguish the true semantics of two short texts. For\nexample, in the short texts ‘‘read Harry Potter’’ and ‘‘watch\nHarry Potter’’ [3], after conceptualization, we can obtain the\nsame concept ‘‘Harry Potter’’, but the machine still does not\nknow that the concept in the ﬁrst short text represents a book\nwhile the second represents a movie, although it is easy for\nhumans to distinguish. To distinguish and expose these sim-\nilar conceptual semantics, we further introduce transformer\nencoders based on knowledge graph nodes, effectively utilize\nthe attention mechanism from the transformer encoder, learn\nto capture the concept semantic information between vocab-\nulary, and project the lexical nodes into the low-dimensional\nsemantic space to solve the problem of semantic similarity\ncalculation caused by sparse lexical features. Most current\nnetwork architectures for text understanding are based on\nrecurrent neural networks (RNNs), particularly long short-\nterm memory (LSTM) networks [9]. Although RNNs and\nLSTMs can learn long-term dependency and obtain strong\nresults on multiple benchmarks, difﬁculties arise due to gra-\ndient vanishing and explosion [10].\nAttention [11], [12] mechanisms have become the new\nstandard in many sequence tasks. One advantage of the atten-\ntion mechanism is that it allows for variable-sized inputs to\nfocus more on the relevant parts to make decisions. Compared\nto RNNs, self-attention has been proven to perform well on\nmultiple tasks, such as machine reading [13], [14] and learn-\ning sentence representations [15]. Speciﬁcally, compared\nwith excessive dependence on long sequences over long texts,\nself-attention tends to rely on local context information and\nbackground in short texts. In our work, we construct a CNN\nmodel to capture the local features based on text conceptu-\nalization. To better adapt the CNN to the task, we propose a\ntransformer encoder-decoder architecture, and we introduce\nthe self-attention mechanism into our CNN model.\nThe remainder of this paper is organized as follows:\nSection II introduces related work, Section III describes our\napproach, including text conceptualization and the trans-\nformer encoder-decoder, Section IV introduces the datasets\nand experimental results, and conclusions are presented in\nSection V .\nII. RELATED WORK\nAlthough text understanding plays an important role in nat-\nural language processing, most of the current text compre-\nhension methods, such as relationship extraction, information\nretrieval, and QA systems, are task oriented. The purpose\nof text understanding is to enable machines to understand\nhuman language symbols. Therefore, it is necessary to con-\nvert text into symbols that can be recognized and calculated\nby machines. Most traditional methods are based on the bag-\nof-words model, such as the vector space model (VMS) [16]\nand BM25 [17], to represent words in the text. These meth-\nods cannot analyze words at the semantic and conceptual\nlevels. Moreover, because the text is usually short and the\ngenerated feature space is sparse, it is difﬁcult to calculate\nthe semantic similarity between texts, and the generalization\nability is limited. Salakhutdinov and Hinton [18] proposed a\nmethod of semantic hashing to ﬁlter the documents given to\nTF-IDF, which improved the accuracy of traditional TF-IDF\nto a certain extent. However, the TF-IDF is inherently lacking\nin semantic similarity information, which makes it impossible\nto truly understand the semantic information of the text. With\nthe development of deep learning and the strengthening of\ncomputer power, researchers began to use distributed repre-\nsentation methods, such as word2vec, global vectors (GloVe),\nand bidirectional transformers (BERT) [19]–[21], to repre-\nsent text. Mapping words into a low-dimensional vector space\ncan obtain the semantic information between words to some\nextent. For example, the distance between two vectors in\nspace indicates the degree of semantic similarity between\ntwo words. Despite the great success of distributed semantic\nrepresentation, the support for text understanding is insufﬁ-\ncient. In recent years, the emergence of various knowledge\ngraphs and semantic concept grids has attracted the interest\nof researchers [22], [23]. This form of storing knowledge\nin a graph model seems to be more suitable for the way\nhumans think and understand the world. Kim et al. [24]\ndevelop a corpus-based framework for text conceptualization\nwhich combining latent Dirichlet allocation (LDA) model\nand Probase. This model can capture the context-instance\nrelationships in the knowledge base through a probabilis-\ntic topic model, but the model cannot understand the true\nsemantic information of the whole text. Song et al. [25]\nproposed a conceptualization method which can map terms\n122184 VOLUME 7, 2019\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\nin text to instances in the knowledgebase, then, they used a\nBayesian inference mechanism to derive the most likely con-\ncepts. Afterwards, Wang et al.[26], [27] proposed building\nthe same candidate relationship diagram for text by means\nof a knowledge graph and using a random walk method\nto derive the concepts of optimal word segmentation, part-\nof-speech (POS) and word and to improve the accuracy of\nknowledge generalization. In our method, we ﬁrst conceptu-\nalize the text and then use ConceptNet to parse and reason\nthe text, obtain the conceptualization of the text and similar\nconcepts in ConceptNet, and ﬁnally establish a new edge\nconnection through similarity to obtain a richer conceptual\nknowledge representation. At the same time, we introduce\nthe attention mechanism to assign the weights of the relevant\nconcepts in the knowledge base of the current text, and obtain\nthe most similar concepts.\nThe attention mechanism was ﬁrst applied to the task\nof machine translation and has made great achievements,\nso it has received considerable attention in the recent deep\nlearning model. This encoder-decoder architecture with arbi-\ntrary size input not only achieves fast computation but\nalso better focuses on the most important parts of the text.\nHermann et al. [28] introduced a class of attention-based\ndeep neural networks that learn to read and comprehend\nreal documents. However, this method needs to provide\nlarge scale supervised reading comprehension data, thus\nseriously affecting the generalization ability of the model.\nCui et al. [29] proposed an attention-over-attention neural\nnetwork approach for reading comprehension. The model\nfocuses on the comparison of sentence levels in the text,\nmaking it different to obtain conceptualized semantic infor-\nmation. Radford et al.[30] proposed a method that uses task-\naware input transformations during ﬁne-tuning to achieve\neffective transfer while requiring minimal changes to the\nmodel architecture. Bauer et al.[31] proposed a novel mul-\ntihop QA system for selecting grounded, relational, com-\nmonsense information from ConceptNet. Nickel et al. [32]\nreviewed how to predict new facts about the world based on\nlarge knowledge graphs. Most of the current methods of intro-\nducing attention mechanism in text understanding are task-\noriented. Although these methods have good expressiveness\non speciﬁc tasks, they still lack understanding and reasoning\nof text semantics.\nIn this paper, we propose a text understanding model\nbased on text conceptualization and the transformer attention\nmechanism. Speciﬁcally, text is mapped to a concept set by\nConceptNet, concept nodes in ConceptNet are represented\nas low-dimensional vectors by representation learning, and\na text vector relevance score is calculated by the transformer\nattention mechanism. Finally, text conceptualization and rel-\nevance scores are used as features to construct a text under-\nstanding model. At the same time, our model gains attention\nby explicitly calculating the attention weights at different\nlevels and ultimately by calculating the weighted sum of\nthem. Experiment results show that our model is versatile and\nsimple, and can improve the performance of the system.\nIII. PROPOSED METHOD\nIn this section, we introduce the method of text conceptualiza-\ntion based on ConceptNet and the structure of the transformer\nencoder. Finally, we build the understanding model by com-\nbining these two parts.\nA. TEXT CONCEPTUALIZATION\nThe purpose of text conceptualization is to infer the con-\nceptual distribution of each entity or term in the text using\nthe conceptual knowledge graph. For example, in the short\ntext ‘‘‘Roma’ won the 91st Oscar for Best Foreign Language\nFilm Award’’, the machine can learn about terms such as\n‘‘Roma’’, ‘‘Oscar’’ and ‘‘Film’’. Then, these entities or terms\nare mapped to the ConceptNet semantic space to construct\na conceptual semantic space model. In particular, we adopt\nthe explicit semantic analysis (ESA) algorithm [33] for the\nconceptual semantic analysis of short texts. Each concept\nis represented as an attribute vector for the corresponding\nentity or term; in other words, the entity or term is arranged\ninto a vector of the concept space for extracting and esti-\nmating concepts in this space. Entities or terms in short\ntexts are mapped to weighted sequences of related concepts\nby an inverted indexing technique; thus, the original text is\nrepresented as the weight vector C in the conceptualized\nspace. The probability of the term t in the short text under the\ncorresponding concept c is P(c|t), which can be calculated by\nthe number cooccurrences of terms t and concept c.\nP(c|t) = count(t,c)∑\nc∈ci\ncount(t,c), (1)\nwhere count(•) denotes the number of cooccurrences.\nAfter conceptualization, the short text is transformed from\nthe word vector space to the concept space; that is, C =\n(wc1 ,wc2 ,··· ,wck ). Each item wci , which represents the cor-\nresponding weight of the short text STi under the concept ci,\nindicating the strength of the association between the concept\nand the short text, can be calculated using formula (2).\nwci =log\n∑\nti∈STi,Ti∈Tci\nTi ×idfc(ti) ×icf (ci), (2)\nwhere Ti denotes the probability of the concept mapped to\nterm ti, which corresponds to a set of concepts when matched,\nand the term is represented by a set of the probability of\nconcept relevance. To make the term t and the concept c\nhave a better distinguishing ability, we introduce the inverse\ndocument frequency idfc and the inverse concept frequency\nicf to indicate the particularity of the term in the concept and\nthe representativeness of the concept in the whole concept set,\nrespectively, and calculate it using the following formula.\nidfc(ti) =log M\nN(ti) +1, (3)\nicf (ci) =log M\nN(ci) +1, (4)\nwhere M denotes the total number of concepts in the concept\nset, N(ti) denotes the number of times the term ti cooccurs\nVOLUME 7, 2019 122185\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\nwith the concept ci, and N(ci) denotes the number of times\nconcept ci appears in all term mappings.\nB. ENRICHING SEMANTIC KNOWLEDGE\nTo further enrich the semantic knowledge of the terms in the\ntext, we assume that ‘‘terms appearing in the same context\nhave similar semantics’’, which is similar to distant super-\nvision in relation extraction. In our work, we build a cooc-\ncurrence network to measure the probability of terms that\ncooccur in a short text. To obtain similar semantics of terms,\nwe obtain POS tags by the Stanford POS tagger from the\nknowledge base corpus. For verbs or adjectives, we directly\nobtain a set of similar semantic words as an aid from the\ncorpus, but for nouns or noun phrases, which usually repre-\nsent an instance or belong to a concept, we aim to determine\ntheir types (instance or concept). We intuitively think that\nonly one topic is represented in a short text. For example,\nin the short text ‘‘‘Roma’ won the 91st Oscar for Best Foreign\nLanguage Film Award’’, usually, ‘‘Roma’’ is recognized as\nthe capital of Italy and a city, but in this text, it denotes a ﬁlm.\nWe deﬁne a score function to measure the probability of term\nx that cooccurs with a target term y in a short text s, and the\ntarget term y belongs to a certain concept c in ConceptNet.\nWe believe that the score function is not only related to the\nprobability of the cooccurrence of two terms x and y but also\nhas semantic coherence in text s. Therefore, the score function\nbetween x and y under a short text s can be calculated as\nfollows:\nScore(x|y,s)=αPco−occur (x|y)+(1−α)Psemantic(x|y,s),\n(5)\nwhere α is a variable parameter that is used to adjust the\nprobability of the two components. Pco−occur (x|y) denotes\nthe cooccurrence probability between terms x and y and is\ndeﬁned in ConceptNet. Psemantic(x|y,s) denotes the semantic\ncoherence between x and y in the text s. We aim to calculate\nthe probability that the term x belongs to the concept c in the\ncase that y belongs to concept c under the text s. Formally,\nPsemantic(x|y,s) is deﬁned as:\nPsemantic(x|y,s) ∝P(ci|c,x,y,s), (6)\nwhere ci is a concept of term x, and concept c denotes the con-\ncept of target term y. Since we already know the probability\nof term y belonging to concept c in ConceptNet, the above\nformula can be converted as follows:\nPsemantic(x|y,s) =\n∑\nci\np(ci,c)p(ci|x), (7)\nwhere p(ci,c) is the cooccurrence probability between con-\ncepts ci and c and can be obtained from the cooccurrence\ninstance network in the knowledge base because each con-\ncept is the distribution of a set of instances. More details\nof the construction of the concept network can be found in\nreference [8].\nFIGURE 1. Illustration of the transformer structure. (a) shows the\nconventional transformer attention structure of the encoder-decoder, and\n(b) shows the structure after the dropout mechanism.\nC. TRANSFORMER EMBEDDING\nA transformer is a currently popular encoder-decoder archi-\ntecture that is widely used in most neural sequence-to-\nsequence models [34], [35]. The transformer’s encoder and\ndecoder both operate the representation of each location of\nthe input and output sequence by applying an RNN. In our\nwork, we ﬁrst exchange information across all positions in a\nsequence based on a self-attention mechanism. Although this\nallows for more contextual information, the computational\ncomplexity increases dramatically. In fact, we do not care\nabout some contextual information; we need to focus on only\nthe entities and terms that contain rich semantics. Therefore,\nwe introduce a dropout-like mechanism [36] in encoding-\ndecoding to discard some of the attention. Of course, we make\nchoices based on the conceptualization semantics of the text.\nThe structure is shown in Fig. 1.\nWe now describe the encoder and decoder of the structure\nin more detail.\nEncoder: Given an input text of length n, after conceptual-\nization, we ﬁrst introduce an embedding matrix M, in which\neach row denotes the embedding representation of the term\nat each position of the text T ∈Rn×d , and the dimension\nis d. Then, we apply the multihead dot-product self-attention\nmechanism from [11]. After the dropout layer, it is combined\nwith conceptualization embedding representation, which we\nbelieve not only preserves conceptualization information but\nalso introduces attention information. Finally, we use layer\nnormalization [37] after a transition function, and the model\nis shown in Fig. 2.\nSpeciﬁcally, we use the dot product to calculate the weight\ncoefﬁcients for queries Q and keys K and determine the\nweighting summation of the values V according to the weight\ncoefﬁcients, and the formula is as follows:\nSimilarity(query,key) =Q ·K, (8)\nwhich calculates the correlation between the queries and keys\nusing the dot product and then uses the softmax function to\ndetermine the results.\na =softmax(Similarity) = eSim\n∑\ni\neSimi\n, (9)\n122186 VOLUME 7, 2019\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\nFIGURE 2. The model of the encoder-decoder based on the transformer.\nwhere a denotes the weight coefﬁcients of the value, and we\ncan calculate the attention as follows:\nAttention(Q,K,V ) =\n∑\na ·V , (10)\nWe use the multihead attention mechanism with heads\n(please refer to [11] for more details)\nMultiHeadAttention(Ti) =Concat(head1,..., headk )Wi,\n(11)\nand we map the ith text to the queries, keys and values using\nlearned matrices M. The head can be calculated as follows:\nheadi =Attention(TiMQ\ni ,TiMK\ni ,TiMV\ni ), (12)\nThen, we can obtain the representation of the text using the\nfollowing formula:\nTi =Norm(Ai +Transition(Ai)), (13)\nwhere Ai =Norm((Ti−1 +Pi) +MultiHeadAttention(Ti−1 +\nPi)), Norm() is deﬁned in [36], and Transition() is a separable\nconvolution. Pi denotes two-dimensional coordinate embed-\ndings, which can be obtained by the position embedding\nvectors [11]. The ﬁnal output is a vector representation matrix\nof n rows and d columns T ∈Rn×d for the input text.\nDecoder: The decoder is similar to the encoder structure.\nBefore, we injected the target concept recognized in Concept-\nNet into the decoder. Then, we used the same multihead atten-\ntion dot-product function as in formula (11). In the decoding\nstep i, the decoder receives the embedded representation of\nFIGURE 3. The encoder-decoder CNN structure with text\nconceptualization.\nthe last step’s output Wi, the hidden state hi−1 and previous\ncontext vector Ti−1. Then, the current hidden state hi can be\ncomputed as:\nhi =CNN([Wi;hi−1],Ti−1), (14)\nWe can calculate the probability distribution using this\nhidden state:\np(yi|yi−1,Ti) =soft max(Whi +b), (15)\nwhere W and b denote the weight and bias vector,\nrespectively.\nD. MODEL DESIGN\nAssume that there are m training words in short text T , which\nis denoted as T = {w1,w2,··· ,wm} ∈Rd×m, where d\ndenotes the dimension of the word embedding representation.\nIn pretraining, we design a three-layer autoencoder CNN\nstructure, as shown in Fig. 3.\nwhere C denotes the conceptual representation of the term\nin the text T , which is the output of the encoder of transformer\nand provides additional information during encoding and\ndecoding. The hidden layer feature can be calculated using\nthe following formula:\nh =W (vw +vc)+b, (16)\nwhere W is a weight matrix, b is a bias vector in decoding,\nvw denotes the vector representation of the word w, vc denotes\nthe conceptual representation related to the word w, after\npooling, the hidden feature h is converted to feature h’, and\nthen fully connected, the output y can be obtained as follows:\ny =W ′h′+b′, (17)\nwhere W ′and b′denote the weight and bias vector, respec-\ntively, of the decoder layer. We also use the squared loss\nJ =1\n2 ∥y −x∥2 as the loss function, and using dropout helps\nto speed up the iteration convergence rate.\nVOLUME 7, 2019 122187\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\nE. OPINION RETRIEVAL BASED ON A RANKING MODEL\nThe ranking learning model is a basic method in opinion\nretrieval tasks. Luo et al. [38] proposed a training ranking\nmodel based on tweet features, author features and opin-\nion features, and we call it WWW2015 for convenience.\nKim et al. [39] further exploited the subjective information\nof blogger features and tag features to describe the per-\nspective of the document, called COLING 2016. However,\nour approach adds additional text conceptualization features,\nsemantic rich features, and transformer attentional features.\nThese new features are combined and analyzed in subsequent\nexperiments.\nSpeciﬁcally, based on the common features of opinion\nretrieval, we add our features to the experiment and select the\noptimal feature combination. The evaluation index uses the\nmean average precision (MAP), NDCG@10 and binary pref-\nerence (bpref), which are commonly used in opinion retrieval.\nFormulas (18), (19), and (20) show their calculations.\nMAP =\n∑Nq\ni=1 APi\nNq , APi = 1\n∑N\ni=1 ri\n∑N\ni=1\nri\n∑i\nj=1 rj\ni ,\n(18)\nwhere Nq denotes the number of requirements and N denotes\nthe total number of texts. If the i −th text is a topic-related\ndocument with an opinion, ri =1; otherwise, ri =0.\nNDCG(n) =Zn\n∑n\nj=1 (2r(j) −1)/log(1 +j), (19)\nwhere Zn is a normalization factor, normalized by using\nNDCG(n) of the returned ideal list as a factor. r(j) refers to\nthe score of the returned document: if relevant, set it to 2;\notherwise, set it to 1.\nbpref =1\nR\n∑\nr 1 −|n ranked higher than r|\nR , (20)\nwhere R is the number of documents related to the query and\nr is a speciﬁc related document. |n ranked higher than r|is\nthe number of unrelated documents ranked higher than r.\nIV. EXPERIMENT AND ANALYSIS\nA. DESCRIPTION OF DATASETS\nTo evaluate our model more comprehensively and effec-\ntively, we used a variety of datasets, including the New York\nTimes corpus (NYT). 1 The dataset is divided into two parts,\nthe training and testing sets and the number of sentences\nand relations, as shown in Tab. 1. We also added the Twitter\ndataset,2 which is much smaller than NYT, to the test; it con-\ntains 3,308 documents, of which the number of documents\nwith views is 590.\nB. INFORMATION RETRIEVAL TASK\nThe ﬁrst experiment we conducted was information retrieval\nin the NYT and Twitter datasets. Similar to previous work,\n1https://catalog.ldc.upenn.edu/LDC2008T19.\n2http://www.sananalytics.com/lab/twitter-sentiment/.\nTABLE 1. Information on the NYT dataset.\nFIGURE 4. The loss curves of the NYT training data; we trained the\nnetworks with dropout and without dropout.\nwe evaluate the performances of our model by using\nthe aggregate precision/recall (P@R) curves. Before that,\nwe compared the effect of using the dropout mechanism\non the convergence of iterations in training. Fig. 4 shows\nthat the addition of dropout greatly decreases the number\nof iterations. For comparison, we compare our model to the\nrelated modes mentioned above. Let’s brieﬂy review these\nmodels.\nTF-IDF (Semantic hashing) [18]: It uses semantic hash-\ning to ﬁlter the documents given to TF-IDF.\nLDA [24]: It obtains text conceptualization by combining\nLDA and Probase.\nIJCAI-11 [25]: It introduces a Bayesian mechanism to\nconceptualize words by using a probabilistic knowledgebase.\nIJCAI-15 [27]: It applies random walk algorithm to dis-\ncover ﬁne-grained semantic information from the input.\nFig. 5 shows the P@R curves of the performances of these\nmethods on the NYT corpus dataset. Furthermore, we apply\nthese methods to the Twitter dataset to obtain the expressive-\nness of these methods in the short text, and the result is shown\nin Fig. 6.\nFig. 5 and Fig. 6 show the performance of several meth-\nods on long text and short text. From these ﬁgures, we can\nsee that our method achieves better performance on both\nlong and short texts. Speciﬁcally, the traditional methods\nTF-IDF and LDA perform worse on short text than on long\ntext. In contrast, our method performs better on short text.\nWe believe that traditional methods have difﬁculty obtaining\nricher semantic features in dealing with short text tasks, thus\naffecting the model. The performance of our method using\ntext conceptualization to map the semantics of short text\nto a richer semantic space obtains richer semantic features.\n122188 VOLUME 7, 2019\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\nFIGURE 5. P@R curves obtained by different methods based on the NYT\ncorpus.\nFIGURE 6. P@R curves obtained by different methods based on the\nTwitter dataset (short text).\nThe current IJCAI method performs relatively smoothly.\nTo verify that our joint model performs better than a single\nmodel, our model is compared to a single text conceptualiza-\ntion and transformer embedding model; the results are shown\nin Fig. 7.\nWe also analyze some query examples on the LDA [24]\nand IJCAI-15 [27] methods and our joint model; Tab. 2 shows\nthe details of the retrieval results of three query news titles.\nIn Table 2, we select three query titles from different ﬁelds\nto predict the relevant query results and calculate the exact\nvalues of the top 10 query results. We can see that in this task,\nthe title of the query is short sentences. It is difﬁcult to obtain\nmore semantic information through only explicit relations.\nTherefore, we construct cooccurrence terms or concepts with\nconceptual knowledge in the knowledge base through text\nconceptualization to enrich the semantic information of the\ntext. For example, in the ﬁrst sentence, when the words\n‘‘Steve Jobs’’ and ‘‘Apple’’ appear together in the same\nsentence, we think of the word ‘‘Apple’’ as an electronics\nFIGURE 7. Comparison of the joint model and single model.\nTABLE 2. The retrieval results of three queries.\ncompany, not a fruit. Similarly, in the second sentence,\n‘‘Roma’’ coexists with ‘‘Oscar’’ and ‘‘ﬁlm’’; thus, ‘‘Roma’’\nis the name of a movie. In the third sentence, ‘‘Djokovic’’\nis a tennis player in the knowledge base. In other words,\nby enriching the embedding of semantic knowledge, we can\nobtain more relevant concepts and semantic knowledge in\nshort texts. Compared with the traditional LDA, the current\nIJCAI-15 and our joint method, our method achieves the best\nresults.\nC. OPINION RETRIEVAL EXPERIMENT\nTab. 3 and 4 show the comparison of our method with the pre-\nvious benchmark methods on the NYT and Twitter datasets,\nrespectively.\nFrom the tables above, we can see that our approach\nimproved all three metrics by an average of more than 2 per-\ncentage points on the short Twitter dataset. We consider that\nVOLUME 7, 2019 122189\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\nTABLE 3. Comparison of our method and the benchmark method on the\nNYT dataset.\nTABLE 4. Comparison of our method and the benchmark method on the\ntwitter dataset.\nTABLE 5. Comparison of the different feature combinations on the NYT\nand twitter datasets.\nthe previous method is not sufﬁcient in feature expression,\nso the feature space generated is relatively sparse. After\nadding the text conceptualization representation, our method\nenriches the text semantic features by using the association\nin the conceptual network, and the conceptual representation\ncan alleviate the problem of an excessively high dimension to\nsome extent.\nIn opinion retrieval based on the sorting learning method,\nthe retrieval performance may be different due to different\nfeature combinations. Therefore, we tested the performance\nof different combinations on two datasets. Tab. 5 shows the\ncomparison results of these different combinations on the two\ndatasets.\nTab. 5 shows the experimental results of the different\ncombinations of the two types of features proposed in this\npaper on the two datasets. We can see that the single-use\ntext conceptualization has little effect on the performance\nof the model because the single conceptualization, although\nacquiring the lower-dimensional representation of the text,\nhas not been extended by conceptual semantics after con-\nceptualization. In contrast, using only transformer embed-\nding to achieve a certain performance improvement is due to\nthe transformer attention mechanism obtaining the semantic\ninformation of related concepts, thus enriching the semantic\nrepresentation of the text. Therefore, we combine these two\nparts to build a joint model to achieve better results.\nV. CONCLUSION\nIn this paper, we propose a novel framework for effective and\nefﬁcient short text understanding. Speciﬁcally, we design a\ncombined understanding model of text conceptualization and\ntransformer embedding. We design a text conceptualization\nmethod and make full use of the ConceptNet semantic struc-\ntured information-rich text conceptual semantics. Finally,\nwe use the transformer attention mechanism for encoding\nand decoding. The experimental results show that compared\nwith the existing work, the method proposed in this paper has\nobvious improvement in MAP and other indicators. In future\nwork, we will attempt to combine the common sense knowl-\nedge graph to improve the generalization ability of the model.\nREFERENCES\n[1] D. Zhang, L. Tian, M. Hong, F. Han, Y . Ren, and Y . Chen, ‘‘Combin-\ning convolution neural network and bidirectional gated recurrent unit for\nsentence semantic classiﬁcation,’’ IEEE Access, vol. 6, pp. 73750–73759,\n2018.\n[2] R. Collobert and J. Weston, ‘‘A uniﬁed architecture for natural language\nprocessing: Deep neural networks with multitask learning,’’ in Proc. 25th\nInt. Conf. Mach. Learn., Jul. 2008, pp. 160–167.\n[3] Z. Yu, H. Wang, X. Lin, and M. Wang, ‘‘Understanding short texts through\nsemantic enrichment and hashing,’’ IEEE Trans. Knowl. Data Eng., vol. 28,\nno. 2, pp. 566–579, Feb. 2016.\n[4] W. Hua, Z. Wang, H. Wang, K. Zheng, and X. Zhou, ‘‘Understand short\ntexts by harvesting and analyzing semantic knowledge,’’ IEEE Trans.\nKnowl. Data Eng., vol. 29, no. 3, pp. 499–512, Mar. 2017.\n[5] A. O. Constantinescu, J. X. O’Reilly, and T. Behrens, ‘‘Organizing con-\nceptual knowledge in humans with a gridlike code,’’ Science, vol. 352,\nno. 6292, pp. 1464–1468, Jun. 2016.\n[6] E. W. T. Ngai, S. S. C. Tao, and K. L. Moon, ‘‘Social media research:\nTheories, constructs, and conceptual frameworks,’’ Int. J. Inf. Manage,\nvol. 35, no. 1, pp. 33–44, Feb. 2015.\n[7] G. N. Cervetti, T. S. Wright, and H. Hwang, ‘‘Conceptual coherence,\ncomprehension, and vocabulary acquisition: A knowledge effect,’’Reading\nWriting, vol. 29, no. 4, pp. 761–779, Apr. 2016.\n[8] R. Speer, J. Chin, and C. Havasi, ‘‘Conceptnet 5.5: An open multilingual\ngraph of general knowledge,’’ in Proc. 31st AAAI Conf. Artif. Intell.,\nFeb. 2017, pp. 4444–4451.\n[9] X. Shi, Z. Chen, H. Wang, D.-Y . Yeung, W.-K. Wong, and W.-C. Woo,\n‘‘Convolutional LSTM network: A machine learning approach for pre-\ncipitation nowcasting,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015,\npp. 802–810.\n[10] K. Greff, R. K. Srivastava, J. Koutnìk, B. R. Steunebrink, and J. Schmidhu-\nber, ‘‘LSTM: A search space odyssey,’’ IEEE Trans. Neural Netw. Learn.\nSyst., vol. 28, no. 10, pp. 2222–2232, Oct. 2017.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[12] R. Kadlec, M. Schmid, O. Bajgar, and J. Kleindienst, ‘‘Text understanding\nwith the attention sum reader network,’’ 2016, arXiv:1603.01547. [Online].\nAvailable: https://arxiv.org/abs/1603.01547\n[13] D. Chen, A. Fisch, J. Weston, and A. Bordes, ‘‘Reading wikipedia\nto answer open-domain questions,’’ 2017, arXiv:1704.00051. [Online].\nAvailable: https://arxiv.org/abs/1704.00051\n[14] M. Hu, Y . Peng, Z. Huang, X. Qiu, F. Wei, and M. Zhou, ‘‘Rein-\nforced mnemonic reader for machine reading comprehension,’’ 2017,\narXiv:1705.02798. [Online]. Available: https://arxiv.org/abs/1705.02798\n[15] Y . Lin, Z. Liu, and M. Sun, ‘‘Neural relation extraction with multi-lingual\nattention,’’ inProc. 55th Annu. Meeting Assoc. Comput. Linguistics, vol. 1,\nJul. 2017, pp. 34–43.\n[16] G. Salton, A. Wong, and C. S. Yang, ‘‘A vector space model for automatic\nindexing,’’Commun. ACM, vol. 18, no. 11, pp. 613–620, 1975.\n[17] S. Jimenez, S.-P. Cucerzan, F. A. Gonzalez, A. Gelbukh, and G. Dueñas,\n‘‘BM25-CTF: Improving TF and IDF factors in BM25 by using collection\nterm frequencies,’’ J. Intell. Fuzzy Syst., vol. 34, no. 5, pp. 2887–2899,\nJan. 2018.\n[18] R. Salakhutdinov and G. Hinton, ‘‘Semantic hashing,’’ Int. J. Approx.\nReasoning, vol. 50, no. 7, pp. 969–978, Jul. 2009.\n[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ in Proc.\nAdv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.\n[20] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. 2014 Conf. Empirical Methods Natural\nLang. Process. (EMNLP), Oct. 2014, pp. 1532–1543.\n122190 VOLUME 7, 2019\nJ. Liet al.: Short Text Understanding Combining Text Conceptualization and Transformer Embedding\n[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: https://arxiv.org/abs/1810.04805\n[22] J. Dalton, L. Dietz, and J. Allan, ‘‘Entity query feature expansion using\nknowledge base links,’’ in Proc. 37th Int. ACM SIGIR Conf. Res. Develop.\nInf. Retr., Jul. 2014, pp. 365–374.\n[23] C. Xiong and J. Callan, ‘‘Query expansion with Freebase,’’ in Proc. Int.\nConf. Theory Inf. Retr., Sep. 2015, pp. 111–120.\n[24] D. Kim, H. Wang, and A. Oh, ‘‘Context-dependent conceptualization,’’ in\nProc. 23rd Int. Joint Conf. Artif. Intell., Jun. 2013, pp. 2654–2661.\n[25] Y . Song, H. Wang, Z. Wang, H. Li, and W. Chen, ‘‘Short text conceptual-\nization using a probabilistic knowledgebase,’’ in Proc. 22nd Int. Joint Conf.\nArtif. Intell., Jun. 2011, pp. 2330–2336.\n[26] F. Wang, Z. Wang, Z. Li, and J. Wen, ‘‘Concept-based short text classiﬁ-\ncation and ranking,’’ in Proc. 23rd ACM Int. Conf. Inf. Knowl. Manage.,\nNov. 2014, pp. 1069–1078.\n[27] Z. Wang, K. Zhao, H. Wang, X. Meng, and J.-R. Wen, ‘‘Query understand-\ning through knowledge-based conceptualization,’’ in Proc. 24th Int. Joint\nConf. Artif. Intell., Jun. 2015, pp. 3264–3270.\n[28] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay,\nM. Suleyman, and P. Blunsom, ‘‘Teaching machines to read and compre-\nhend,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 1693–1701.\n[29] Y . Cui, Z. Chen, S. Wei, S. Wang, T. Liu, and G. Hu, ‘‘Attention-over-\nattention neural networks for reading comprehension,’’ in Proc. 55th Annu.\nMeeting Assoc. Comput. Linguistics, Jul. 2016, pp. 593–602.\n[30] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, (2018). Improv-\ning Language Understanding by Generative Pre-Training. [Online]. Avail-\nable: https://blog.openai.com/language-unsupervised/\n[31] L. Bauer, Y . Wang, and M. Bansal, ‘‘Commonsense for generative multi-\nhop question answering tasks,’’ 2018, arXiv:1809.06309. [Online]. Avail-\nable: https://arxiv.org/abs/1809.06309\n[32] M. Nickel, K. Murphy, V . Tresp, and E. Gabrilovich, ‘‘A review of rela-\ntional machine learning for knowledge graphs,’’ Proc. IEEE, vol. 104,\nno. 1, pp. 11–33, Jan. 2016.\n[33] O. Egozi, S. Markovitch, and E. Gabrilovich, ‘‘Concept-based information\nretrieval using explicit semantic analysis,’’ ACM Trans. Inf. Syst., vol. 29,\nno. 2, p. 8, Apr. 2011.\n[34] K. Cho, Bart van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using RNN\nencoder-decoder for statistical machine translation,’’ in Proc. Conf. Empir-\nical Methods Natural Lang. Process. (EMNLP), 2014, pp. 1724–1734.\n[35] I. Sutskever, O. Vinyals, and Q. V . Le, ‘‘Sequence to sequence learning\nwith neural networks,’’ in Proc. Adv. Neural Inf. Process. Syst., 2014,\npp. 3104–3112.\n[36] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, ‘‘Dropout: A simple way to prevent neural networks\nfrom overﬁtting,’’ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958,\n2014.\n[37] J. L. Ba, J. R. Kiros, and G. Hinton, ‘‘Layer normalization,’’ 2016,\narXiv:1607.06450.[Online]. Available: https://arxiv.org/abs/1607.06450\n[38] Z. Luo, M. Osborne, and T. Wang, ‘‘An effective approach to tweets\nopinion retrieval,’’World Wide Web, vol. 18, no. 3, pp. 545–566, May 2015.\n[39] Y .-S. Kim, Y .-I. Song, and H.-C. Rim, ‘‘Opinion Retrieval Systems using\nTweet-external Factors,’’ in Proc. 26th Int. Conf. Comput. Linguistics,\nDec. 2016, pp. 126–130.\nJUN LI was born in Xinyang, China. He is cur-\nrently pursuing the Ph.D. degree with the School of\nInformation and Communication, Guilin Univer-\nsity of Electronic Technology, China. His current\nresearch interests include natural language pro-\ncessing and text understanding.\nGUIMIN HUANGreceived the Ph.D. degree from\nthe School of Computer Science, East China Uni-\nversity of Science and Technology, in 2005. He is\ncurrently a Full Professor with the Guilin Univer-\nsity of Electronic Technology, China. His research\ninterests include natural language processing and\ntext mining.\nJIANHENG CHEN was born in Jinhua, China.\nHe is currently pursuing the Ph.D. degree with the\nSchool of Information and Communication, Guilin\nUniversity of Electronic Technology, China. His\ncurrent research interests include natural language\nprocessing and text summarization.\nYABING WANG was born in Henan, China.\nShe is currently pursuing the Ph.D. degree with\nthe School of Computer Science and Informa-\ntion Security, Guilin University of Electronic\nTechnology, China. Her current research interests\ninclude natural language processing and sentiment\nanalysis.\nVOLUME 7, 2019 122191",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8112953901290894
    },
    {
      "name": "Transformer",
      "score": 0.6470935344696045
    },
    {
      "name": "Encoder",
      "score": 0.5816609859466553
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5787606835365295
    },
    {
      "name": "Embedding",
      "score": 0.5619701743125916
    },
    {
      "name": "Natural language processing",
      "score": 0.5496493577957153
    },
    {
      "name": "Conceptualization",
      "score": 0.5402059555053711
    },
    {
      "name": "Knowledge base",
      "score": 0.41899800300598145
    },
    {
      "name": "Information retrieval",
      "score": 0.3896365761756897
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}