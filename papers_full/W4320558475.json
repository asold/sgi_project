{
  "title": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",
  "url": "https://openalex.org/W4320558475",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4221483793",
      "name": "Polak, Maciej P.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320703551",
      "name": "Modi, Shrey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320703552",
      "name": "Latosinska, Anna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2202033333",
      "name": "Zhang Jin-ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320703554",
      "name": "Wang, Ching-Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2506245703",
      "name": "Wang, Shaonan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320703556",
      "name": "Hazra, Ayan Deep",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2421200280",
      "name": "Morgan, Dane",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953641512",
    "https://openalex.org/W3026048580",
    "https://openalex.org/W3201869313",
    "https://openalex.org/W4283074682",
    "https://openalex.org/W2936166854",
    "https://openalex.org/W3200122731",
    "https://openalex.org/W2766362701",
    "https://openalex.org/W4307139584",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4280555259",
    "https://openalex.org/W3008287297",
    "https://openalex.org/W4226293470",
    "https://openalex.org/W3127365350",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4289544215",
    "https://openalex.org/W4311409687",
    "https://openalex.org/W4394535785",
    "https://openalex.org/W2971874326",
    "https://openalex.org/W4220813361",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4390041668",
    "https://openalex.org/W4391709329",
    "https://openalex.org/W2956007643",
    "https://openalex.org/W4386168831",
    "https://openalex.org/W4292289324",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W3094582681",
    "https://openalex.org/W2165671627",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W2999645992",
    "https://openalex.org/W2964864162",
    "https://openalex.org/W4206078246",
    "https://openalex.org/W4224442790",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4389518977",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W3115677442",
    "https://openalex.org/W4214535912",
    "https://openalex.org/W3100221827",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W4281476575",
    "https://openalex.org/W2336874854",
    "https://openalex.org/W4225378608",
    "https://openalex.org/W4392002118",
    "https://openalex.org/W4225409008",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W3015467311",
    "https://openalex.org/W4281617541",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W2610394652",
    "https://openalex.org/W4389761608",
    "https://openalex.org/W4224909481"
  ],
  "abstract": "Accurate and comprehensive material databases extracted from research papers are crucial for materials science and engineering, but their development requires significant human effort. With large language models (LLMs) transforming the way humans interact with text, LLMs provide an opportunity to revolutionize data extraction. In this study, we demonstrate a simple and efficient method for extracting materials data from full-text research papers leveraging the capabilities of LLMs combined with human supervision. This approach is particularly suitable for mid-sized databases and requires minimal to no coding or prior knowledge about the extracted property. It offers high recall and nearly perfect precision in the resulting database. The method is easily adaptable to new and superior language models, ensuring continued utility. We show this by evaluating and comparing its performance on GPT-3 and GPT-3.5/4 (which underlie ChatGPT), as well as free alternatives such as BART and DeBERTaV3. We provide a detailed analysis of the method's performance in extracting sentences containing bulk modulus data, achieving up to 90% precision at 96% recall, depending on the amount of human effort involved. We further demonstrate the method's broader effectiveness by developing a database of critical cooling rates for metallic glasses over twice the size of previous human curated databases.",
  "full_text": "Digital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nFlexible, Model-Agnostic Method for Materials Data Extraction from Text Using\nGeneral Purpose Language Models\nMaciej P. Polak, ∗ Shrey Modi, Anna Latosinska, Jinming Zhang,\nChing-Wen Wang, Shaonan Wang, Ayan Deep Hazra, and Dane Morgan †\nDepartment of Materials Science and Engineering,\nUniversity of Wisconsin-Madison, Madison, Wisconsin 53706-1595, USA\nAccurate and comprehensive material databases extracted from research papers are crucial for ma-\nterials science and engineering, but their development requires significant human effort. With large\nlanguage models (LLMs) transforming the way humans interact with text, LLMs provide an oppor-\ntunity to revolutionize data extraction. In this study, we demonstrate a simple and efficient method\nfor extracting materials data from full-text research papers leveraging the capabilities of LLMs com-\nbined with human supervision. This approach is particularly suitable for mid-sized databases and\nrequires minimal to no coding or prior knowledge about the extracted property. It offers high recall\nand nearly perfect precision in the resulting database. The method is easily adaptable to new and\nsuperior language models, ensuring continued utility. We show this by evaluating and comparing its\nperformance on GPT-3 and GPT-3.5/4 (which underlie ChatGPT), as well as free alternatives such as\nBART and DeBERTaV3. We provide a detailed analysis of the method’s performance in extracting\nsentences containing bulk modulus data, achieving up to 90% precision at 96% recall, depending on\nthe amount of human effort involved. We further demonstrate the method’s broader effectiveness\nby developing a database of critical cooling rates for metallic glasses over twice the size of previous\nhuman curated databases.\nI. INTRODUCTION\nObtaining reliable and comprehensive materials data is\ncrucial for many research and industrial applications. If\nnecessary information is not accessible through curated\ndatabases researchers typically must manually extract\nthe data from research papers, a process that can be\ntime-consuming and labor-intensive. Natural language\nprocessing (NLP) with general Language Models (LMs),\nand in particular, powerful large LMs (LLMs) trained\non massive bodies of data, offer a new and potentially\ntransformative technology for increasing the efficiency of\nextracting data from papers. These methods are partic-\nularly valuable when the data is embedded in the text of\nthe documents, rather than being presented in a struc-\ntured or tabular format, making it harder to find and\nextract. While currently LLMs often fall short in practi-\ncal applications, struggling with comprehending and rea-\nsoning over complex, interconnected knowledge domains,\nthey offer significant potential for innovation in materials\nscience and are likely to play a crucial role in materials\ndata extraction [1].\nThe rapid pace of development in NLP and frequent\nrelease of improved LLMs suggests they can be best uti-\nlized by methods which are easily adapted to new LLMs.\nIn this paper we present such a flexible method for mate-\nrials data extraction and demonstrate that it can achieve\nexcellent precision and recall.\nSo far, the majority of materials data extraction ap-\nproaches focus on fully automatic data extraction [2–7].\n∗ mppolak@wisc.edu\n† ddmorgan@wisc.edu\nAutomation is clearly desirable, particularly when ex-\ntracting very large databases. However, more automa-\ntion tends to require more complexity in the software,\nsophistication in training schemes, and knowledge about\nthe extracted property. In addition, if a high level of\ncompleteness is required from a database, the recall of\nthese approaches may not be sufficient. In such fully au-\ntomated approaches a large amount of focus has been\nplaced on the complex task of named entity recogni-\ntion (NER) [8–12], so that the property, material, values\nand units can be extracted accurately. However, auto-\nmatic identification of an improper recognition is still\nvery challenging, which can reduce the precision of such\napproaches. Tools for automatic materials and chemistry\ndata extraction, like OSCAR4 [13] or ChemDataExtrac-\ntor [14, 15], have been developed and used to success-\nfully extract large databases. A recent example includes\na database of over 22 thousand entries for relatively com-\nplex thermoelectric properties [16], at an average preci-\nsion of 82.5% and a recall of 39.23%, or over 100 thousand\nband gap values [17], with an average precision of 84%\nand a recall of 64%. More complex information such as\nsynthesis recipes [18–23] have also been extracted with\nautomated NLP-based methods. Although not complete\ndue to the relatively low recall, databases of that size are\nuseful for training machine learning models [24–30], and\nwould be very time consuming or impossible to extract\nwith virtually any other method than full automation.\nOther recent examples of databases created in a simi-\nlar way include photovoltaic properties and device mate-\nrial data for dye-sensitized solar cells [31], yield strength\nand grain size [32], and refractive index [33, 34]. Other\nnotable databases gathered with NLP-based approaches\ninclude more complex information than just data values,\nsuch as synthesis procedures [19, 27]. Recently, another\narXiv:2302.04914v3  [cond-mat.mtrl-sci]  12 Jun 2024\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nFIG. 1. Qualitative behavior of different types of approaches\nto data extraction, presented as human time required as a\nfunction of the size of the dataset. The broad range of the\ngreen (fully automatic), and orange (this work) represents the\npotential variation in the initial fixed time requirement, which\nmay slightly influence the quality of the result. The dashed\nline suggests which method is the best choice for a given size\nof dataset.\nmethod for structured information extraction, making\nuse of the GPT-3 capabilities was presented [35]. In that\nwork, the focus is placed on the complicated NER tasks\nand relation extraction, at which GPT-3 excels. In that\napproach, more complex sentences can be successfully\nparsed into structured information. A ”human in the\nloop” approach was used to fine-tune the model, a tech-\nnique that seems to be emerging as a method of choice\nto obtain higher performing models. Impressive perfor-\nmance was achieved in this work for structured informa-\ntion extraction, although at a price of a relatively large\nset of relatively complex training examples.\nIn addition, the emergence of highly specialized LLMs\nunderscores the rapid advancement in the field. In Ref.\n[36] an instruction-based process specifically designed for\nmaterials science enhanced the accuracy and relevance of\ndata extraction. Such specialized fine-tuning shows sig-\nnificant advantages in dealing with niche materials sci-\nence tasks.\nRecently, fully automated agent-based LLM ap-\nproaches to analyze scientific text have been proposed\nas well, which are capable of answering science questions\nwith information from research papers [37], and gener-\nating customizable datasets [38]. Other fully automated\nLLM-based methods, including those that leverage com-\nplex prompt engineering workflows within LLMs have\nbeen proposed to curate large materials datasets of a\nhigher quality than conventional automated NLP meth-\nods when used with state-of-the-art LLMs [39].\nDepending on the nature of the data and the end goal\nfor which the database is needed, there are different re-\nquirements for the resultant database and different op-\ntimal approaches for the data extraction. It is useful to\norganize methods along two broad axes. One axis is hu-\nman time, which generally has the form\nt = A + B · N. (1)\nHere A is up-front fixed time to develop the method\nfor a specific case and B is a marginal time cost whose\ncontribution scales with some measure of data quantity\nN (N represents some function of the number of pa-\npers, sentences, and data to extract). The other axis\nis database quality, generally represented by some com-\nbination of precision (what fraction of the found data\nare right) and recall (what fraction of the available data\nwas found) of the database. A schematic plot represent-\ning required human time as a function of the size of the\ndatabase for different methods, and the quality of their\nresults, is presented in Fig. 1, where a logarithmic scale\nwas used to emphasize the behavior for small-moderate\nsized databases, where the method is the most advanta-\ngeous. While the classification of a dataset’s size depends\nsignificantly on the context and scientific field, in this pa-\nper we base this classification, as detailed in the following\nparagraph, on an evaluation of the typical quantities of\nrelevant materials data found in existing literature.\nOne limiting case, which we will call ”small data”, is\none where only a small amount of data, up to around 100\npoints, is available in the literature (for example, proper-\nties that are very new, very hard to measure, or studied\nby only a small community), and where completeness\nand accuracy are highly valued. For example, as of this\nwriting ”small data” might refer to superconductors with\nTc > 200K [40]. It is typical to gather data for proper-\nties in the very small data limit fully manually, usually\nby experts in the field. Full manual curation is practical\ndue to the limited number of papers and data and as-\nsures that the data is comprehensive and accurate. This\nfully manual approach is represented in Fig. 1 as the blue\nline, which is preferred (dashed) for very small amounts\nof available data. Even though it is technically slower\nthan other methods, even in the very small data range,\nit is still the method of choice due to the highest possible\nquality of the results.\nThe opposite limiting case, which we will call ”large\ndata”, is when there is a lot of data in the literature,\nmore than a couple thousand datapoints, the database\nis expected to be large, and modest precision and recall\nare acceptable. For example, such a database might be\npursued for use for building machine learning regression\nand classification models on widely studied properties.\nFor this large data case fully automated NLP-based ap-\nproaches may be the most appropriate solution (see green\ncurve in Fig. 1 for large number of entries). However,\nsuch an automated approach can result in an incomplete\ndatabase that may not be sufficient for certain research\nor industrial applications, e.g., where extremes of per-\nformance of just a few materials might be the primary\ninterest. In addition, conventional, fully automated NLP\napproaches often require extensive retraining and build-\ning of parsers specific for different properties, as well as a\nsignificant amount of coding. These methods thus often\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nrequire a significant initial investment of human time.\nDatasets in the middle between ”small” and ”large”\nare considered in this paper as mid-size, i.e. containing\nbetween around a hundred and a couple thousand data-\npoints.\nThe logic of the best approaches for these extremes\nis simple. Large data (e.g., > 104 data points) can be\nmost efficiently extracted by spending human time on au-\ntomating the extraction (leading to large A and small B\nin Eq. 1), and reduced precision and recall is often of lim-\nited consequence since so much data is available. Small\ndata sets (e.g, < 10 data points) can be most efficiently\nextracted by spending human time on directly extract-\ning the data (leading to small A and large B in equation\nEq. 1), and high precision and recall is typically more im-\nportant for smaller databases. However, the optimal ap-\nproach for the middle ground between these scales, which\nrepresents many databases in materials, is not obvious.\nWe propose the use of a method that is most suited\nto creating these mid-size databases. With the recent\nsignificant advances in performance and availability of\nLLMs, there is opportunity for significant improvements\nby employing them as a part of a language processing\nworkflow for the purpose of materials data extraction.\nThis method uses a combination of LLMs methods, with\nsome degree of human supervision and input, which al-\nlows one to relatively quickly extract data of high quality\nwhile at the same time requiring minimal coding expe-\nrience and upfront fixed human time cost. The method\nleads to modest A and B in Eq. 1, making it better than\nhuman extraction or full automation in the medium-data\nscale range. Two variants of the method are represented\nin Fig. 1 by the red and orange curves. They provide data\nof relatively high quality, approaching that of a fully man-\nually created database, and scale well for medium sized\ndatabases. The proposed methods allow a database of\nup to the order of 1000 data points to be gathered in a\nfew hours.\nThe general idea of breaking up the papers into sen-\ntences and classifying those sentences as relevant or not,\nperhaps with a model fine-tuned with human supervision\nis a commonly utilized language processing practice, in-\ncluding in materials science [41] . This general idea is also\nthe core of the method presented here. However, we use\na LLM to classify each sentence as relevant or not, parse\neach relevant category sentence with a LLM into a struc-\ntured set of target data, and then perform human review\nof the extracted structured data for validation and fixing.\nThe LLM classification is done either fully automatically\n(in a zero-shot fashion) or with some small human ef-\nfort to fine-tune the LLM with example sentences. The\nLLM classification step typically removes about 99% of\nthe irrelevant data and leaves only about 1% to be fur-\nther analyzed, dramatically reducing human labor. The\nfinal human review is very efficient as only highly struc-\ntured data is presented, and most are already correct or\nnearly correct. This method results in an almost perfect\nprecision and recall for the resultant database, compara-\nble to a fully human curated database, but at 100 times\nor less human effort.\nThere are three major advantages of this method com-\npared to other possible approaches of data extraction\nwith NLP. First, the method is very easy to apply, requir-\ning almost no coding, NLP or LLM expertise and very\nlimited time from the user. For example, the case where\nthe LLM is provided by transformers zero-shot classifi-\ncation pipeline [42] requires just 3 lines of code that are\nprovided on the huggingface website. As another exam-\nple, in the case where the LLM GPT-3/3.5/4 is used, the\nAPI request is also just a few lines and provided to the\nuser explicitly by the developers. Second, the method in-\nterfaces with the LLM through a standard classification\ntask available in any modern LLM, making it possible to\neasily use the method with many present and likely any\nfuture LLMs. Thus the method can easily take advantage\nof the rapid improvements occuring in LLMs. Third, the\nmethod requires almost no knowledge about the prop-\nerty for which the data is to be extracted, with just the\nproperty name required for the basic application of the\nmethod.\nIn this paper we demonstrate the method by develop-\ning databases with multiple LLMs. The simplicity and\nflexibility of the method is illustrated by repeating the\ndevelopment of a benchmark bulk modulus sentence clas-\nsification database with multiple OpenAI GPT models, in-\ncluding the recently released GPT-3.5 davinci, GPT-3.5\n(turbo) and GPT-4 [43, 44], as well as the bart- and\nDeBERTaV3-based language models [45–48] hosted on hug-\ngingface, currently the most downloaded models for text\nclassification. It is important to demonstrate the ap-\nplicability and efficiency of the method on both simple,\nfree, and accessible LLMs that can be easily used on a\npersonal computer, and on LLMs which require signifi-\ncantly more computation and may be out of reach of some\nmost people’s resources for now. Even though there exist\nfully free and open LLMs, such as OPT[49], BLOOM[50] or\nLLaMA[51], their use is computationally expensive and not\nconvenient, which contradicts the spirit of ease and ac-\ncessibility of the presented method. Therefore, we opted\nfor The OpenAI’s models whose API allows one to effi-\nciently use the LLM on outside servers, although is not\nfree. GPT-3/3.5/4 are also currently the most popular\nLLMs, so its a choice that will likely be relevant for many\nusers.\nWe demonstrate and benchmark the method on raw\ntexts of actual research papers, simulating how the\nmethod will likely be used by science and engineering\ncommunities. We first assess the precision and recall of\nthe method on a small set of papers and the property\nbulk modulus in order to demonstrate the excellent ac-\ncuracy of the classification that can be obtained with this\nmethod. We then use the method to extract a modest\nsized but high quality database of critical cooling rates\nfor metallic glasses.\nThe paper is organized as follows: Section II describes\nthe approach in detail; Section III shows the results of\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nbenchmarks and statistical analysis of the obtained clas-\nsification results for a bulk modulus sentences database;\nSection IV discusses the developed database of critical\ncooling rates for metallic glasses, the possible utility of\nthe method for purposes other than simple data extrac-\ntion, and future possibilities in light of the rapid evolu-\ntion of NLP methods and new LLMs. Section VI de-\nscribes in detail the benchmark bulk modulus sentence\nclassification database used for assessment as well as the\ncritical cooling rate of metallic glasses database.\nII. DESCRIPTION OF THE APPROACH\nThis section describes the steps involved in the method\nfor data extraction from research papers. These steps are\nschematically presented in Fig. 2. Each step is first sum-\nmarized (in bold), and then expanded to include details\nand observations we had during the work. Note that the\nfirst step is focused on gathering and basic processing of\nthe papers and does not use any LLMs. Furthermore,\nthis step (or one very similar) is present in any data ex-\ntraction method and is not specific to our method. Since\nthis step is largely a universal preprocessing that does\nnot have any direct connection to the method we label it\nthe zeroth step, thereby allowing the first step to denote\na step directly connected to data extraction.\nSTEP 0:\nObtaining and postprocessing the raw html/xml\npaper texts into human-readable format.\nInput: This step starts with gathering the papers\nfor analysis (e.g. from ScienceDirect API [52]) in an\nxml/html format. This usually involves searching for pa-\npers through a query to the publisher’s search engine,\nand simply downloading every paper that comes up as a\nresult. If at this stage any relevant papers are missed, the\ndata will not be extracted, so it is safer to use a broad\nsearch query or combine results of multiple queries. In-\ncluding additional data does not increase the amount of\nhuman time, only involves more processing for the NLP\nmodel. This step takes very little human time, does not\ndepend on the size of the database and is already mostly\nautomated through the publisher, since only the query is\nrequired to obtain a list of matching documents.\nSimple text processing: After the papers are\ndownloaded the metadata and html/xml syntax is re-\nmoved. We keep all the paragraphs and the title and\nremove the rest of the content. Then, we remove the\nhtml/xml markup syntax and tags. At this point all\nthat is left is pure text. This cleaned up text is then\nsplit into separate sentences, according to regular rules\nfor how sentences are terminated. At the end of this\nstep we are left with the raw data that may be fed to\nthe LLM and analyzed. There is no need for any hu-\nman evaluation of the data at this point. Whether an\nentire paper is unrelated, or some of the paragraphs con-\ntents are, it will simply be analyzed by the LLM and\ndeemed irrelevant. This step takes very little human\ntime, and the amount required does not depend on the\nsize of the database or the extracted property. The ex-\nact method for removal of html/xml syntax and splitting\ninto sentences can vary. It can be done by text pro-\ncessing through regular expressions (an example can be\nfound in the codes, Sec. VIII B), or ready-made special-\nized python libraries and their functions (such as lxml,\nnltk.tokenize.sent tokenize [53, 54]), depending on\nthe user preference.\nIt is important to note here that further simple text\nprocessing of the cleaned up text to keep only plausi-\nble sentences, e.g. using regular expressions to keep sen-\ntences with easily identified essential information, can,\nand probably should be performed at this point. Al-\nthough such an additional processing step does not influ-\nence the method or the final outcome and quality of the\nproduced database, such processing can significantly re-\nduce the amount of data to be categorized by the LLMs.\nThis simple processing will certainly reduce the compute\ntime needed for the LLM and can reduce costs if the\nLLM is not free. How this text processing is performed\ndepends on the task and the amount of knowledge about\nthe data or property to be extracted. For example, if\nwe know the data is numeric we can keep just sentences\ncontaining a number. In the case of bulk modulus (see\nDataset 1 in Sec. VI)), keeping only sentences contain-\ning a number cuts the amount of data to be processed in\nhalf and does not lower recall (i.e., keeps all relevant sen-\ntences). If some amount of knowledge about the quantity\nto be extracted is available it can be used to further select\nthe most promising sentences. For example, if we know\nthe expected units of the data we can further process\nthe remaining sentences to keep only those that contain\nsuch units. In the case of bulk modulus (see Dataset\n1 in Sec. VI)) keeping only sentences containing units\nof pressure (pascals and bars with possible metric pre-\nfixes, N/m2), lowers the amount of possible candidates\nto less than 20% of the initial set, still without any loss\nof recall. Such refinements can be continued to further\nnarrow down the search, but each subsequent step relies\non a deeper knowledge of the property in question and\nincreases the risk of reducing recall. In the work pre-\nsented here we assume the most demanding situation for\nthe method, in which no prior knowledge of the property\nis assumed. Therefore we only narrow down the search\nto sentences containing numerical values.\nSTEP 1:\nZero-Shot binary classification of sentences to\nproduce unstructured data, i.e. set of sentences\ncontaining values for a given property. The classi-\nfication puts sentences in two categories: positive,\nwhich are sentences containing the data (numeri-\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nFIG. 2. A diagram of the steps necessary for NLP/LLM data extraction in the proposed method. The process starts with\ngathering and preparing the documents to be analyzed, a process not involving any NLP (Step 0), then a LLM is used to\nclassify sentences by whether a sentence does or does not contain data for a given property (value and units) in a zero-shot\nfashion (Step 1). The pre-classified sentences are then (optionally) validated and used for fine-tuning the LLM and reclassifying\nthe sentences with higher quality (Step 2). Finally the data is structured by a LLM/human assisted process, where the name\nof the material/system, the numerical value of the given property, its unit, and in some cases an additional detail, such as the\ntemperature at which the value are obtained (Step 3). A detailed description of all steps can be found in Sec. II\n.\ncal value for a given property with its correspond-\ning unit), and negative, which are sentences not\ncontaining the data.\nDepending on the LLM used, the zero-shot may require\nas little as just the name of the desired property as the\nlabel of the class (name of the property), or require a\nfull prompt phrase (e.g., GPT-3/3.5/4). Since the most\nrecent and powerful LLMs make use of a prompt (e.g.,\nGPT-3/3.5/4), we focus on that case here. The prompt\n(a single set of words, typically a phrase that makes gram-\nmatical sense) given to the model plays an important\nrole. The impact of prompts has already been widely\nobserved in NLP-based text to image generation tools\n(e.g., DALL-E2 [55], MidJourney [56], Stable Diffusion\n[57]) and a similar situation occurs in the present appli-\ncation. Depending on the completeness and phrasing of\nthe prompt, the results for classification may be dramati-\ncally different. In our experience, however, prompts that\ndo not contain false and misleading information almost\nalways result high recall, and it is mainly the precision\nthat is affected. In addition, more complex prompts do\nnot necessarily guarantee a better result and may not\nbe necessary. It is worth mentioning that with modern\nLLMs, other approaches such as one/few-shot (providing\na prompt together with one or a few example outputs)\nor even more complex ways of extracting data, involv-\ning multiple subsequent prompts, have been shown to be\nvery effective [43]. In this work, however, not only is\nthe zero-shot efficient enough for the classification task,\nbut it is also the simplest, most straightforward to apply\nand assess its performance, and ensures higher flexibility\nand transferability to other properties, so the other more\ncomplex methods have not been explored.\nFig. 3 shows the zero-shot result statistics for the\ndifferent models, including GPT-3.5 (whose technical\nnames are text-davinci-002 and text-davinci-003)\nand other GPT models including 3.5 (turbo) and 4, which\nunderlie ChatGPT. The Chat models do not output prob-\nabilities so full precision recall curves cannot be plotted,\nonly a single point, which for all Chat models has 100%\nrecall. The p1 and p2 stand for two different prompts.\np1: Does the following sentence contain the value of bulk\nmodulus?.\np2: A sentence containing bulk modulus data must have\nits numerical value and the units of pressure. Does the\nfollowing sentence contain bulk modulus data?\nOnly the first token of the model’s response was evaluated\nand in all cases it was either a ”yes” or a ”no” (case-\ninsensitive), as expected, allowing for an unambiguous\nclassification. As an example consider the two following\nsentences:\n1. After full lithiation, the phase transformed to Li13Sn5,\nwhich has the bulk modulus of 33.32 GPa and the Pois-\nson’s ratio of 0.205.\n2. The structure of polycrystalline copper is cubic with\nlattice parameters a = b = c = 3.6128 (1) ˚A at 0.0 GPa.\nWe would get a ”yes” response for the first, and a ”no”\nresponse for the second.\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nEven though p2 contains more seemingly valuable infor-\nmation, it did not necessarily perform better. We exper-\nimented with various different prompts, and straightfor-\nward prompts similar to p1 performed most consistently\nand predictably for most models. Therefore a simple\nprompt: Does the following sentence contain the value of\n[name of property]? is our strong recommendation. The\none exception is GPT-4, where a more detailed prompt\nresulted in a significantly better result. This is due to an\nimproved accuracy of prompt interpretation and follow-\ning the prompt instructions in GPT-4.\nIt is worth noting that some models, such as\nGPT-3/3.5 davinci and GPT-3.5/4 (chat) at the time\nof writing this article are not free to use. Therefore,\nthe flexibility to use different LLMs within the method\nis very valuable, as some free models, while not neces-\nsarily capable of accurately performing the more com-\nplex tasks such as automated data structurization, and\nalthough overall generally less capable than GPT-based\nmodels, perform well enough in the simple task of clas-\nsification to produce satisfying results. However, in the\ncase of OpenAI GPT-3, both model usage and fine-tuning\nis done on outside servers, so in a situation where com-\nputational resources are not available to run locally, it\nmay enable one to use the best models at a low cost.\nSTEP 2 (optional):\nHuman assisted verification of the zero-shot clas-\nsification of sentences. The highest scoring un-\nstructured data (most likely to be a true positive),\npre-classified in Step 1, is manually classified into\npositive and negative sentences to provide a new\ntraining set of sentence for fine-tuning the clas-\nsification process. The new training set is then\nused to fine-tune the model and classify the sen-\ntences again to obtain higher precision and recall.\nThis optional step is just a chance for the human user to\nprovide confirmation or correction to particularly impor-\ntant zero-shot classification data from Step 1 and then\nuse those checks to fine-tune the LLM. Similar steps are\noften taken in other data extraction approaches, and ma-\nchine learning in general [35, 41, 58]. Specifically, as\nthe highest scoring sentences are being manually veri-\nfied, a new training sets consisting of true positive and\ntrue negative examples is built. Since the precision of\nresults of Step 1 is typically around 50% at 90% recall\n(see Fig. 3 (d)), the created sets are typically close to\nequal in size. The human labeled sets consist of positive\ncases, which represent true positives from Step 1, and\nnegative cases, consisting of false positives from Step 1.\nThe latter are the most valuable counter-examples for\nthe negative training set, as these are the sentences the\neasiest for the model to confuse for positives. If after\nreaching the desired amount of verified positive sentences\nthe corresponding set of negative sentences is smaller,\nit may be complemented with random sentences from\nthe analyzed papers (the exceeding majority of which\nare negative). Fig. 4 shows how the classification model\nimproves when fine-tuned on datasets of increasing size.\nA detailed analysis of that figure is present in Sec. III,\nwhere a conclusion is made that after around 100 positive\nsentences for the quicker learning models such as GPT-3\ndavinci/GPT-3.5 or bart, we start to observe diminish-\ning returns with this human labeled dataset size increase\nand it may not be worth spending more human time on\nobtaining more examples. Therefore, we recommend to\nperform the manual verification of the zero-shot classifi-\ncation until 100 positive sentences (and a corresponding\n100 negative - made easy due to the 50% precision) are\nobtained, a number easy to remember and satisfactory\nfor an efficient fine-tuning dataset.\nThis steps usually takes no more than 30 minutes for\napproximately 100 sentences - each sentence has to be\nclassified only in a binary fashion which is a very simple\ntask and takes just a few seconds per sentence. The clas-\nsification is as simple and straightforward as assigning\n1 for positive and 0 to each sentence in a spreadsheet.\nThe fine-tuning itself, for the small locally hosted mod-\nels (bart and DeBERTaV3), takes around 30 minutes on\nan older workstation CPU (Intel(R) Xeon(R) CPU E5-\n2670), 20 minutes on a modern laptop CPU (Intel(R)\nCore(TM) i9-9880H), and can be reduced to just a few\nminutes if GPUs are used. The OpenAI models are fine-\ntuned on external OpenAI servers in less than 30 minutes\nand do not require any local resources. After this step is\nperformed and the sentences are once again reclassified\nusing the now fine-tuned model the precision and recall\nare greatly improved, as can be seen in Fig. 4.\nThis Step 2 is optional and is generally done to im-\nprove the quality data collected from Step 1. Improving\nprecision of data at this stage will reduce the human time\nneeded in data structurization in Step 3 (see Sec. II) to\nreview the data. However, for small datasets the human\ntime in Step 3 is very modest, and this Step 2 may not be\nworth the extra effort. Thus whether it is performed or\nnot typically depends on the size of the dataset. For small\ndatasets, and if a recall of around 90% is satisfactory this\nstep can be entirely omitted. As seen in Fig. 3 d), the\nprecision at 90% recall after Step 1 is over 50% for the\nbest models, which means that for every true positive\nsentence, there is only one false positive - a reasonable\nnumber to be removed by hand during data structuriza-\ntion (Step 3). For small datasets, up to a few hundred\nvalues, verifying around 100 positive sentences to perform\nadditional fine-tuning to improve the precision may turn\nout to be more labor intensive that proceeding straight to\ndata structurization, and improving the precision man-\nually by simply ignoring false positives. It is crucial to\nunderstand that the recall obtained at this step (or that\nhas been obtained after Step 1, if this optional step is\nskipped) will be the recall of the final database, while\nthe precision will be improved to near perfect in the next\nstep (Step 3).\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nSTEP 3:\nData structurization (template filling). In this\nstep, extraction of the structured data is per-\nformed. Here, by structured data we mean the\nfull information necessary to provide a datapoint:\nthe name of the material/system, the numerical\nvalue of the given property, its unit, and in some\ncases an additional detail, such as the temper-\nature at which the value was obtained. At the\nsame time as the data is extracted the precision\nof the result is improved to perfect (or near per-\nfect, depending on the expertise and accuracy of\nthe human supervising this step), by simply ig-\nnoring the false positive sentences left over from\nprevious steps. The result of this step is a final,\ncurated structured database.\nThe user will typically perform this step by first rank-\ning the sentences by their probability of being relevant\n(classification scores in the case of small LMs, bart and\nDeBERTaV3, or log probabilites in the case of GPT-3),\nwhich is the output from Step 1 (or Step 2 if performed),\nand start reviewing the list at the top, working down un-\ntil they decide to stop. As the user works through the\nresults in that fashion, they traverse down the precision\nrecall curve (PRC) (see. Figs. 3 (a) and 4 (a)). While\nthe recall is impossible to assess without knowing the\nground truth, the user is fully aware of the precision of\nthe data they have already analyzed, therefore using the\nPRC they can estimate the recall and decide to stop when\na desired recall is reached (with the assumption that the\nPRC are similar to those shown in Figs. 3 (a) and 4 (a)).\nFor best models, reaching recall of around 90% (close to\nthat of a fully manual data curation) without performing\nthe optional Step 2 happens for a precision close to 60%,\nwhile for a fine-tuned model (with Step 2), for a preci-\nsion over 80%. It is entirely up to the user to decide the\nquality they require from their database, and the quality\nof the results will be proportional to the amount of time\nspent in this step. Recall of 90% seems to be a reason-\nable value to stop the process, as the precision sharply\ndrops for higher values, which diminishes returns for the\nhuman time involved. However, this behavior may vary\ndepending on the case, which will be discussed further in\nthe Sec. III.\nIn general, human assisted data structurization, even\nwhen only the sentences containing the relevant data are\ngiven, may be a tedious and time consuming task. How-\never, at this point it is the only method that can guar-\nantee an almost perfect precision. For an inexperienced\nuser, extracting one datapoint from a given sentence and\nits surrounding context fully by hand may take as long\nas 30 seconds, depending on the complexity of the prop-\nerty being analyzed and how it is typically expressed in\nresearch papers. Considering this, only relatively mod-\nest sized databases are reasonable to create. However,\nwith experience, this time quickly reduces as the user\ngets used to the process. In addition, more experienced\nusers may employ simple computer codes, e.g. based on\nregular expressions, which would preselect possible can-\ndidates for values and units, reducing the time signifi-\ncantly. In the longer term, it is likely the NLP tools will\nhelp automate this data structurization step. Some mod-\nels, like GPT4, offer structured format output, such as\njson, which may be used to assist the final data extrac-\ntion step. However, they do not do this very effectively\nat present without either human supervision or a ma-\njor effort to tune them. For example, GPT-3/3.5/4 is\ncapable of parsing unstructured data in a zero-shot fash-\nion, with no need for retraining. In the case of our bulk\nmodulus sentences dataset we found that in over 60% of\ncases GPT-3/3.5/4 is capable of correctly providing the\nentire data entry for a given property (name of the ma-\nterial/system, value, unit), and an incomplete datapoint\n(wrong material/system, but correct value and unit) in\nover 95% of cases. The only drawback that prohibits a\nfull automation of this step with a LLM is the inability\nto automatically and unambiguously distinguish between\ncorrect and incorrect extracted datapoints. Even though\nthe model does not tend to make up (hallucinate) data, it\nsometimes provides an incomplete or inaccurate extrac-\ntion (e.g. ”alloy” instead of ”AlCu alloy” for the mate-\nrial, or ”100” instead of ”greater than / >100” for the\nvalue, etc.). However, human assistance in determining\nwhether the data has been structurized properly, and in\ncase it was not, fixing it by hand, can easily remedy that\nproblem. Since almost all values and units are extracted\ncorrectly, and only less than half of the material names\nrequire fixing, using a LLM approach greatly reduces the\nhuman time and effort required for data structurization.\nUsing an LLM we found that the average human time\nrequired to extract each good datapoint was reduced to\nunder 10 seconds, keeping the same, almost perfect preci-\nsion. Thus, and NLP-assisted data structurization, while\nstill a tedious process, enables one create databases of up\nto around 1000 entries (more or less, depending on the\nusers predisposition to and efficiency at repetitive tasks),\nin one workday. This timing includes the whole process,\nbeginning (Step 0) to end (structurized database after\nStep 3), although almost all the human time is spent in\nStep 3.\nWhile the value, units and the optional additional de-\ntails most often occur within the positive sentence, the\nname of the material is often missing from that sentence\n(sentences are often similar to e.g. We determined the\nbulk modulus to be 123 GPa. ). In those cases the system\nis described most often in the preceding sentence, and if\nnot, then in the title of the paper. In a vast majority\nof cases (96% in our bulk modulus dataset) the full data\ninformation is available to be extracted from a sentence,\nthat preceding it, and the title, so we do not search for it\nin other places. In the rare case when the full datapoint\ncannot be extracted, we record an incomplete datapoint.\nWe also note that even in NLP models finely tuned for\nstructurized data extractions, the further apart the rel-\nevant data are from each other, the more difficult it is\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nfor the model to accurately extract the relevant data, so\nthose datapoints would very likely be incomplete with\nother NLP-based approaches as well.\nIII. RESULTS\nThis section provides a comprehensive analy-\nsis of various language models’ performance in\nclassifying relevant sentences. The analysis high-\nlights the superior performance of the GPT fam-\nily of models in a zero-shot approach and demon-\nstrates the effectiveness of fine-tuning, while also\ndiscussing the results in the context of the acces-\nsibility of different models. It also addresses the\nchallenges posed by highly imbalanced datasets\nand discusses strategies for reducing human ef-\nfort in data processing.\nFig. 3 summarizes the result from Step 1 in Sec. II for\na bulk modulus analysis. The papers, sentences, ground\ntruth category statistics, and other information is pro-\nvided in Sec. VI. The ground truth for Step 1 was de-\ntermined by human labeling. The following precision\nrecall curves (PRCs) and receiver operating character-\nistic curves (ROCs) are constructed in the usual way,\nwhich is by plotting the relevant metrics while varying\nthe cutoff used for the lowest value of probability ac-\ncepted as a positive classification for sentence relevance.\nPanel (a) shows a PRC for the models tested in this pa-\nper. The two different curves for each of the GPT-3.5\ndavinci and GPT-3.5/4 (chat) models correspond to\ntwo different prompts used in classification (see Sec. II).\nAll of the tested models perform similarly, with bart\nstruggling slightly more than others in achieving higher\nrecall. The ChatGPT models result in only a single point,\nas the probability is not output from these models. All\nChat models result in 100% recall, with GPT-3.5 (chat)\nperforming similarly to base GPT-3 models, which was ex-\npected since they are a part of the same family of models\nand based on similar architecture. The next generation\nGPT-4 performs better, in particular with a more infor-\nmative prompt ( p2, see Sec. II). This is a result of an\nimproved instruction-following capabilities of the GPT-4\nand a higher capability to apply knowledge provided in\nthe prompt when producing results, which suggests that\nfurther prompt engineering may provide an even more\nimproved performance in zero-shot classification in this,\nand likely in future, LLMs. However, this better per-\nformance of GPT-4, although impressive, ultimately may\nstill be eclipsed by the even better performance of the\nfine-tuned GPT-3 davinci model (discussed later), and\nits significantly lower cost. A more quantitative mea-\nsure of models’ performance is presented in panel (b),\nwhere the area under the precision recall curve (AUC-\nPRC) alongside a maximum F1 score are presented. The\nGPT-3.5 models, in particular using the first prompt (p1)\nshow the highest scores, while bart and DeBERTaV3 rank\nlowest in PRC-AUC. It is important to notice, however,\nthat the datasets analyzed here are heavily imbalanced,\nwith negative results outnumbering positives by more\nthan 2 orders of magnitude. This places the naive no\nskill in Fig. 3 (a) line, representing an entirely random\nmodel, close to zero (as opposed to at 0.5 for a fully\nbalanced set), lowering the entire PRC compared to a\nbalance set. Fig. 3 (c) shows the ROC, which is in-\nsensitive to dataset imbalance, and shows much higher\nAUCs (panel (b)) than those of PRCs. The conclusions\nfrom ROCs are similar to those from PRCs; GPT-3.5/4\nperforms best, with bart scoring lowest, while still per-\nforming reasonably well. A non-LM approach based on\nregular expressions was also evaluated for comparison. In\nthe case of bulk modulus sentences, a simple regular ex-\npression (regex) capturing sentences containing any num-\nber ([0-9]), the case-insensitive phrase ”bulk modulus,”\nand units of pressure ([MG]*Pa|kbar) resulted in an 82%\nprecision and 72% recall (F1=0.76). While this result is\ncomparable to the maximum F1 of zero-shot results of\nsmaller LMs, LLMs such as GPT-4, as well as fine-tuned\nmodels, perform noticeably better. In addition, regex-\nbased approaches do not directly offer a precision-recall\ncurve, which would allow adjusting the balance to maxi-\nmize recall without significantly sacrificing precision (see\nSec. II). Even though chat models such as GPT-4 do not\noffer the precision-recall curve either, in our test they\nperformed at 100% recall, so this fact was irrelevant.\nIt is informative to consider the implication of the\nROCs and PRCs for the efficiency of the human effort\nin our method. The step that requires most of the hu-\nman time for a modest size database or larger (e.g., a\nfew hundred entries or more) is Step 3, where the user\nmust read and structure output from each sentence cate-\ngorized as positive in Step 1 (or Step 2 if used). In Sec. II\nwe suggested that the user limit their review of sentences\nin Step 3 unless a desired recall (implied by precision\nthrough the PRC) is achieved.\nIn some applications one might wish to target a high\nrecall irrelevant of the human time required in step 3.\nTo give a sense of how that might impact the method,\nFig. 3 (d) and 4 (b) show the precision for 90% recall\nafter Step 1 and after the optional Step 2, respectively.\nConsistent with our above discussions, the best models\ncan achieve this recall with more than 50% precision us-\ning even just the zero-shot approach (Step 1). For less\nrobust models, a 50% precision requires tuning (Step 2).\nFor the worst models and using just zero-shot learning,\nthe precision is about 17%, meaning the user would be\nextracting useful data from only about 1 in every 6 sen-\ntences reviewed. This would likely still be practical, but\ncould become very tedious for a database of even a few\nhundred final entries. However, the important implica-\ntion is that if one uses the best models ( GPT-3.5/4),\neven a quite high recall requirement, e.g., 90%, can be\nachieved using very efficient sentence review, with almost\nevery (more than 90%) sentence presented to the user\ncontaining relevant data.\nFig 4 demonstrates how the performance of each of\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nFIG. 3. Performance of different models after Step 1 (zero-shot binary classification of relevant sentences based on whether they\ncontain bulk modulus data). (a) precision recall curves, (b) area under precision recall curve (PRC-AUC) (bars), maximum F1\nscore (circles), and area under receiver operating characteristic curves (ROC-AUC) (squares, right y-axis), (c) receiver operating\ncharacteristic curves with an inset the upper left corner, (d) precision at 90% recall and recall at 50% precision (right y-axis).\nThe no skill line represent a baseline model where the classification is random. Chat models do not output probabilities,\ntherefore only one point of the curves in (a) and (c) is available for each GPT-3.5 (chat) and GPT-4 models and is labeled with\ndark blue and dark red × respectively. Note that all Chat models have 100% recall. Labels in panels (b) and (d) have been\nshortened, but represent the same models as those in the legend of (a) and (c). p1 and p2 in the davinci models represent two\ndifferent prompts (see Sec. II).\nthe models is improved if the optional fine-tuning in\nStep 2 is performed, as a function of the size of the\ntraining set. Panel (a) shows PRCs before fine-tuning\n(zero-shot) and compares them to PRCs after fine tun-\ning on 100 and 200 positive sentences. While all models\neventually show improvement, fine-tuning is clearly the\nmost beneficial for the GPT-3 davinci (note that cur-\nrently only the older generation GPT-3 davinci is avail-\nable for fine-tuning). Similarly, various metrics describ-\ning the quality of the model are presented in Fig. 4 (b),\nwhere learning curves as a function of the size of the\nfine-tuning training set are shown. The the x-axis rep-\nresents the number of positive sentences included in the\ntraining set (with an assumed equal number of negative\nsentences). The shape of the learning curves differs for\ndifferent models, with GPT-3 davinci model perform-\ning best (i.e. achieves higher performance metric values\nfor smaller training sets) and learning the quickest (i.e.\nconverges closer to best observed performance metric val-\nues for smaller training sets), bart following second, and\nDeBERTaV3 third, across all metrics. One may notice that\nperformance of the fine-tuned models trained with very\nsmall training sets perform worse than zero-shot (Fig. 3).\nWhen the model is fine tuned on a very specific and not\nvery diverse set of information, the model’s weights are\nupdated with information inadequate to constrain it re-\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nFIG. 4. Comparison of performance of different methods after fine-tuning (Step 2, Sec. II, binary classification of relevant\nsentences based on whether they contain bulk modulus data). Panel (a) shows precision recall curves, dotted lines correspond\nto the zero-shot (0-shot) result ( davinci are averaged into one as described in the text), dashed and solid line correspond to\nfine-tuning on 100 and 200 positive sentence examples, respectively, (b) learning curves, i.e. performance metrics as a function\nof training set size, top to bottom: area under precision recall curve (PRC-AUC), maximum F1 score, area under receiver\noperating characteristic curves (ROC-AUC), recall at 50% precision, and precision at 90% recall. The horizontal thin dashed\nlines in corresponding colors represent zero-shot results. (c) receiver operating characteristic curves for the same data as in (a).\nsulting in less accurate performance. For davinci, slope\nstarts to decrease rapidly (curve starts to saturate) for\nas few as 60-80 positive sentences in the training set,\nfor bart that occurs at around 100 positive sentences,\nand for DeBERTaV3 closer to 160. Even though not all\nof the curves are fully saturated for the above mentioned\ndataset sizes, constructing larger fine-tuning training sets\nis likely to waste more human time than it is going to gain\nin Step 3. Our recommendation, if the optional step 2\nis performed, is to initially use a training set of around\n100 positive sentences and the GPT-3 davinci model or\nthe smaller and free bart. This size of 100 positive sen-\ntences is very manageable to obtain with human-assisted\nverification of classification after Step 1, and typically\ndoes not take more than 30 minutes. It is worth noting\nthat although we expect this number to be transferable\nto other properties it has not been verified thoroughly\non other properties. Whether to perform the optional\nStep 2 (fine-tuning) will ultimately depend on the size of\nthe database. As mentioned before, for larger databases,\nthis improvement will be beneficial and save overall hu-\nman time needed to curate the database by making Step\n3 more efficient, while for small databases, up to a cou-\nple hundred datapoints, the time spent on the fine-tuning\nin Step 2 might be more than is saved during the data\nstructurization in Step 3.\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\nIV. DISCUSSION\nThe paragraph discusses the practical applica-\ntion of the presented approach to curate an ex-\ntensive and accurate database of critical cooling\nrates for metallic glasses by analyzing a large vol-\nume of scientific literature. Comparison to ex-\nisting, manually curated database and other au-\ntomated methods is provided. Utility for com-\nplex data-oriented tasks like machine learning\nand the method’s potential to handle unrestricted\nsearches effectively is then discussed.\nTo provide an example use-case for the method, we\napplied it to curate a high quality and highly accu-\nrate database of critical cooling rates for metallic glasses\n(Sec. VI B). 668 papers responded to the query ”bulk\nmetallic glass”+”critical cooling rate”, which is more\nthan what a human researcher would be analyze man-\nually in a reasonable timeframe. The proposed method\nresulted in 443 datapoints consisting of the value of mate-\nrials, their critical cooling rate, and the unit in which they\nwere expressed in the paper. These results, include all\nmentions of critical cooling rates, with different degrees\nof specificity, e.g., accurate values for specific composi-\ntions (the ideal result), value ranges for specific materi-\nals, and value ranges for broad families of materials. The\nobtained database covers the range of expected values\nvery well, with values ranging from 10 −3 K/s for known\nbulk metallic glass formers, to 10 11 K/s for particularly\nbad glass formers. The well known Pd-based bulk metal-\nlic glasses ( Pd43Cu2Ni10P20 and Pd43.2Ni8.8Cu28P20)\nare identified as those with the lowest values of critical\ncooling rates, while simpler alloys such as AgCu, PdNi\nor NiBe and pure metals such as Co are identified as\nthose with the highest critical cooling rates, which fur-\nther validates the results. The obtained data, cleaned\nup for direct use in data oriented tasks (such as machine\nlearning) i.e postprocessed to only include unique val-\nues for uniquely specified systems yielded 211 entries.\nWithin these, 129 are unique systems (multiple values\nare reported for some systems and we kept these to allow\nthe user to manage them as they wish). The database\nis larger than the size of a recently published manually\ncurated database of critical cooling rates [59], which is\nthe most state-of-the-art and complete such database of\nwhich we are aware, and consists of only 77 unique com-\npound datapoints. To provide comparison to other ex-\nisting methods, we used ChemDataExtractor2 (CDE2)\n[15], a state-of-the-art named entity recognition (NER)\nbased data extraction tool. With CDE we obtain a re-\ncall of 37% and precision of 52%, which are comparable\nto those reported for thermoelectric properties (31% and\n78%, respectively) obtained in Ref. [16].\nSearching for a given property does not typically add\nany restrictions on the search other than the property\nitself, i.e., the search is unrestricted. In the case of the\nmethod proposed here, unrestricted search will identify\nand help extract all datapoints for the target property\nfrom the input set of documents. Therefore, if the user\ndesires a database limited to, for example, a given family\nof systems, the limitation would have to be enforced in\nsome additional way. This constraint could be done by\nlimiting the input set of documents through a more strict\nsearch query, but even that does not guarantee that only\nthe desired values will be extracted, as many papers men-\ntion a wide range of results, even if technically focused\non a particular topic. Limiting the final database can be\neasily done manually in Step 3 (Sec. II), but depending\non the property and the size of the desired subset, limit-\ning the data at that stage may take a lot of human time\nand be inefficient. In principle, more restrictions than\njust the property can be imposed on the NLP level, but\nsuch abstract concepts as families of materials are very\nchallenging even for the best LLMs and greatly reduce\nthe quality of the zero-shot results (Sec. II) and would\nrequire significantly more training in (Sec. II). This prob-\nlem is highly dependent on the property in question. For\nexample, an unrestricted search for critical cooling rates\nwhile limiting the search in Step 0 to papers responding\nto a query ”bulk metallic glasses”+”critical cooling rate”\nwas quite effective for our goals of obtaining all ranges\nof critical cooling rates for metallic glasses. But if one\nwanted, say, an overpotential for water splitting, restric-\ntions on many factors, e.g., temperature or pH, might be\nessential to obtaining useful result and difficult to screen\non in the initial Step 0.\nA particular example of where unrestricted searches\ncan be problematic occurs when searching for properties\nwhich are relevant in many fields when one is interested\nin only a particular field, and/or which have many pos-\nsible associated restrictions which are needed to make\nthe data useful. A specific example of this problem oc-\ncurred for us when we explored constructing a database\nfor ”area specific resistance” (ASR) for anode materials\nof proton conducting cells. In step zero we searched for\n”area specific resistance”+”proton conducting fuel cells\n(and similar terms)” The method proved very successful\nat identifying sentences containing ASR and structuring\nthe data, as it was asked to do. However, the method\ncaptured ASR in a wide variety of contexts, including\nsingle phase and composite materials, porous and non-\nporous materials, electrodes and electrolytes, steels, in-\nterconnects, coatings, varying temperatures, and ASR in\nboth fuel cell and electrolysis operation modes. To ob-\ntain a simple and immediately useful dataset we were\ninterested in single phase dense anodes operating in fuel\ncell mode with temperature information. Imposing such\nlimitations was dramatically harder than the basic data\nextraction. Although one might have different goals than\nthe ones just mentioned, it is very unlikely that one is\ninterested in gathering information for all of the above\ndata in a single database. Restricting the set of input\ndocuments was able to help to a certain degree to move\nthe balance of the obtained results in the desired direc-\ntion, but did not solve the issue entirely. From such a\nwide variety of contexts, identifying only those we were\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\ninterested in required a relatively deep knowledge from\nthe person performing the data extraction and required\nsignificantly more human time to extract than in case of\ndatasets where the property is more uniquely identified.\nIn fact, we stopped developing this database due to these\nmany challenges, although for someone willing to commit\n4-5 days of human time in step 3 the desired database is\ncertainly practical to develop.\nOn the other hand, the lack of restrictions in the model\nmay have other benefits, as it expands the possibili-\nties of the kinds of information that can be extracted.\nFor example, the method can be used to extract many\nkinds of information, not just property values. Step 1\nwith models like GPT-3 davinci/GPT-3.5 or GPT-3.5/4\n(chat) broadly describe the type of text we are looking\nfor, and Step 2 fine-tunes to better classify the relevant\nsentences. While we utilized this classification search to\nfind sentences containing numerical data for a given prop-\nerty within text paragraphs of research papers, data may\nbe present in other places such as tables or figures. The\nclassification approach can be easily used to search for\nnon-textual data such as tables or figures containing the\nrelevant information, by classifying their captions. In\ncase of a positive table classification, it would be fol-\nlowed by manual or algorithmical extraction from the al-\nready structurized table. Furthermore, classification can\nbe used for more abstract concepts, such as suitability\nof a given material for a certain application, personal\nopinions of authors about promising directions of future\nresearch, or any other concept that can be characterized\nas a group of example texts for the model to train on,\nand classify in a binary fashion.\nIt is also important to remember that the method we\npresent here is not restricted to the LLMs explored in\nthis paper, and is in fact designed to be quickly adapted\nto new and improved LMs.\nV. SUMMARY\nWe have shown a simple and efficient method for ma-\nterials data extraction from research papers. The simple\nconcept of binary text classification using a LLM is in-\nvolved as a key step in the method, which allows for a\nhigh flexibility in the language model used as virtually all\nmodern language models are very capable at text classi-\nfication. We determined GPT-3/3.5/4 models to be the\nbest performers, but evaluated other, less expensive and\nmore accessible alternatives such as bart or DeBERTaV3.\nBy including a highly-optimized human-assisted step in\nthe process, we minimized the amount of coding and\nprior knowledge about the extracted property necessary\nto achieve a high recall and nearly perfect precision. A\nmodest sized database of up to around 1000 entries can be\nextracted in around one workday with this method. The\nmethod is assessed vs. ground truth on a bulk modulus\ndatabase and then applied to construct curated database\nof critical cooling rate of metallic glasses.\nVI. DATASETS\nBelow, the details about the datasets are provided. As\na result of this paper a high quality database of critical\ncooling rates for bulk metallic glasses has been curated,\nas well as a benchmark-only dataset - the bulk modulus\ndataset, which was used to assess the model. Information\non accessing the datasets can be found in Sec. VIII B. It\nis important to note that we only used papers for which\na full text is available in a text (xml) format. The cut-\noff date for availability of full texts of papers varies from\njournal to journal, but is usually around the mid 2000s.\nFortunately, however, despite not having access to older\npapers, a significant amount of valuable or relevant older\ndata is likely gathered as well, as that data is often re-\npeated and referred to in more recent papers, which is\nthen subsequently extracted with our method.\nA. Bulk modulus sentences\nThe bulk modulus is a benchmark dataset of sentences.\nFrom over 10000 paper results of a search query ”bulk\nmodulus”+”crystalline”, a subset of 100 papers from the\nfirst 6000 full-text papers available through ScienceDi-\nrect API was randomly selected. In the written text of\nthese 100 papers, there are 18408 sentences in total, out\nof which 237 sentences mention the value of bulk mod-\nulus. This sentence dataset is used as a benchmark for\nthe classification so a human ground truth is extracted.\nTo avoid excessive time spent establishing the human la-\nbeled ground truth this database uses only a very small\nfraction of the total available papers. Thus the bulk mod-\nulus sentences database is not nearly complete, and the\nhuman-assisted extraction of a final database is not per-\nformed.\nFor the zero-shot case (only step 1 and not step 2) the\napproach effectively has no training data and can just\nbe assessed on the test data described above. However,\nwhen step 2 included the fine-tuning requires additional\ndata (effectively a training data set). For this fine-tuning\nprocess an additional 339 positive and 484 negative sen-\ntences have been extracted from papers not included in\nthe 100 papers in the test set. These additional sentences\nare use to investigate how fine-tuning improves the model\nand plot learning curves (see Fig. 4).\nB. Critical cooling rates for bulk metallic glasses\nThis dataset consists of data gathered from 668 pa-\npers based on a result of a search query ”bulk metallic\nglass”+”critical cooling rate” from Elsevier’s ScienceDi-\nrect API. These papers consisted of 107386 sentences, out\nof which 347 were identified as positive (containing values\nof critical cooling rates), after applying the workflow de-\nscribed in Sec. II, including the optional Step 2 in order to\nprovide best quality data. From these 347 sentences, 443\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\ncritical cooling rate data points (consisting of the ma-\nterial name, critical cooling rate value and units) were\nextracted and are collected as a final database presented.\nAdditionally, that data was manually postprocessed to\ninclude only unique datapoints (removing duplicate re-\nsults, i.e. the same values reported in multiple papers),\nremove those which included ranges or limits or values,\nor where the material’s composition was not explicitly\ngiven, and unify the formatting of the materials compo-\nsitions, which resulted in 211 unique datapoints. The\ntotal human time required for gathering this dataset did\nnot exceed 5 hours.\nACKNOWLEDGMENTS\nThe research in this work was primarily supported\nby the National Science Foundation Cyberinfrastructure\nfor Sustained Scientific Innovation (CSSI) Award No.\n1931298. Additional support for engaging undergradu-\nates in research was provided by the National Science\nFoundation Training-based Workforce Development for\nAdvanced Cyberinfrastructure (CyberTraining) Award\nNo. 2017072. We thank Shishmitha Adusumilli, Har-\nmon Bhasin, and Shanchao Liang for their participation\nin this project.\nVII. CONTRIBUTIONS\nM. P. P. conceived the study, performed the modeling,\ntests and prepared/analyzed the results, S. M., J. Z., J.\nW., S. W. and A. D. H. performed tests and assessed\nmethods as a part of the Informatics Skunkworks pro-\ngram for undergraduate researchers, A. L. post-processed\nand extracted data, D. M. guided and supervised the re-\nsearch. Writing of the manuscript was done by M. P. P.\nand D. M.. All authors read, revised and approved the\nfinal manuscript.\nVIII. METHODS\nAs with any machine learning model, there are hy-\nperparameters that may be optimized. Our experience\nshowed that there is very little to be gained by per-\nforming the costly optimization, and throughout the pa-\nper we used default recommended values for all mod-\nels. In OpenAI GPT-3 davinci we used the default val-\nues for fine-tuning, and when using both the pretrained\ntext-davinci-002/003 and fine-tuned davinci we set\nthe frequency and presence penalties as well as temper-\nature to 0. The fine-tuning of bart and DeBERTaV3 was\nperformed with default recommended values too, which\nis a learning rate of 2 e − 5, batch size of 16, 5 epochs,\nand 0 .01 weight decay. Full and detailed input files\ncan be found in [60]. Python codes were executed with\nPython ver. 3.10.6. For zero-shot classification with Ope-\nnAI models, the model’s response was limited to a sin-\ngle token to facilitate a yes/no answer and preserve re-\nsources by cutting off further completion. A 0613 (June\n13th 2023) snapshots of OpenAI chat models, GPT-3.5\n(turbo) and GPT-4, were used, with an empty system\nmessage. For each sentence classification a separate chat\nwas initiated.\nA. Definition of statistical quantities\nTrue positive (TP) - a sentence containing numerical\ndata for a given property.\nTrue negative (TN) - a sentence not containing numeri-\ncal data for a given property.\nFalse positive (FP) - a sentence not containing the\nnumerical data for a given property but is identified as\none that does.\nFalse negative (FN) - a sentence containing numerical\ndata for a given property but is identified as one that\ndoes not.\nPrecision:\nPrecision = TP\nTP + FP (2)\nRecall (True Positive Rate):\nRecall = TP\nTP + FN (3)\nFalse Positive Rate (FPR):\nFPR = FP\nFP + TN (4)\nF1 score:\nFPR = 2TP\n2TP + FP + FN (5)\nB. Data Availability\nThe databases curated as a result of this pa-\nper, all datasets used in the assessment of the\nmethod, as well as the codes and software used\nin this paper are available on figshare [60]:\nhttps://doi.org/10.6084/m9.figshare.21861948. The\ncodes are included for full transparency, but were\ndeveloped for internal use only, so they contain very\nlimited error handling, and the authors do not guarantee\nthat they will work universally on every system. All\nparameters used for the model fine-tuning and zero-shot\nclassification can be found in the codes.\nC. Competing Interest\nThe authors declare no competing interest.\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\n[1] S. Miret and N. M. A. Krishnan, Are llms ready for real-\nworld materials discovery?, (2024), arXiv:2402.05200.\n[2] E. A. Olivetti, J. M. Cole, E. Kim, O. Kononova,\nG. Ceder, T. Y.-J. Han, and A. M. Hiszpanski, Data-\ndriven materials research enabled by natural language\nprocessing and information extraction, Applied Physics\nReviews 7, 041317 (2020).\n[3] O. Kononova, T. He, H. Huo, A. Trewartha, E. A.\nOlivetti, and G. Ceder, Opportunities and challenges of\ntext mining in materials research, iScience 24, 102155\n(2021).\n[4] M. Krallinger, O. Rabal, A. Louren¸ co, J. Oyarzabal,\nand A. Valencia, Information retrieval and text mining\ntechnologies for chemistry, Chemical Reviews 117, 7673\n(2017).\n[5] K. Choudhary and M. L. Kelley, Chemnlp: A natural\nlanguage processing based library for materials chemistry\ntext data, The Journal of Physical Chemistry C 127,\n17545 (2023), https://doi.org/10.1021/acs.jpcc.3c03106.\n[6] V. Tshitoyan, J. Dagdelen, L. Weston, A. Dunn, Z. Rong,\nO. Kononova, K. A. Persson, G. Ceder, and A. Jain,\nUnsupervised word embeddings capture latent knowledge\nfrom materials science literature, Nature 571, 95 (2019).\n[7] O. Isayev, Text mining facilitates materials discovery, Na-\nture 571, 42 (2019).\n[8] A. Trewartha, N. Walker, H. Huo, S. Lee, K. Cruse,\nJ. Dagdelen, A. Dunn, K. A. Persson, G. Ceder, and\nA. Jain, Quantifying the advantage of domain-specific\npre-training on named entity recognition tasks in mate-\nrials science, Patterns 3, 100488 (2022).\n[9] L. Weston, V. Tshitoyan, J. Dagdelen, O. Kononova,\nA. Trewartha, K. A. Persson, G. Ceder, and A. Jain,\nNamed entity recognition and normalization applied to\nlarge-scale information extraction from the materials sci-\nence literature, Journal of Chemical Information and\nModeling 59, 3692 (2019).\n[10] X. Zhao, J. Greenberg, Y. An, and X. T. Hu, Fine-tuning\nbert model for materials named entity recognition, 2021\nIEEE International Conference on Big Data (Big Data),\n, 3717 (2021).\n[11] T. Isazawa and J. M. Cole, Single model for organic\nand inorganic chemical named entity recognition in\nchemdataextractor, Journal of Chemical Information and\nModeling 62, 1207 (2022).\n[12] T. Gupta, M. Zaki, N. M. A. Krishnan, and Mausam,\nMatscibert: A materials domain language model for text\nmining and information extraction, npj Computational\nMaterials 8, 102 (2022).\n[13] D. M. Jessop, S. E. Adams, E. L. Willighagen, L. Hawizy,\nand P. Murray-Rust, OSCAR4: a flexible architecture for\nchemical text-mining, Journal of Cheminformatics 3, 41\n(2011).\n[14] M. C. Swain and J. M. Cole, Chemdataextractor: A\ntoolkit for automated extraction of chemical information\nfrom the scientific literature, Journal of Chemical Infor-\nmation and Modeling 56, 1894 (2016).\n[15] J. Mavraˇ ci´ c, C. J. Court, T. Isazawa, S. R. Elliott, and\nJ. M. Cole, Chemdataextractor 2.0: Autopopulated on-\ntologies for materials science, Journal of Chemical Infor-\nmation and Modeling 61, 4280 (2021).\n[16] O. Sierepeklis and J. Cole, A thermoelectric materials\ndatabase auto-generated from the scientific literature us-\ning chemdataextractor, Sci Data 9, 648 (2022).\n[17] Q. Dong and J. Cole, Auto-generated database of semi-\nconductor band gaps using chemdataextractor, Sci Data\n9, 193 (2022).\n[18] O. Kononova, H. Huo, T. He, Z. Rong, T. Botari, W. Sun,\nV. Tshitoyan, and G. Ceder, Text-mined dataset of inor-\nganic materials synthesis recipes, Scientific Data 6, 203\n(2019).\n[19] Z. Wang, O. Kononova, K. Cruse, et al., Dataset of\nsolution-based inorganic materials synthesis procedures\nextracted from the scientific literature, Sci Data 9, 231\n(2022).\n[20] Z. Wang, O. Kononova, K. Cruse, T. He, H. Huo, Y. Fei,\nY. Zeng, Y. Sun, Z. Cai, W. Sun, and G. Ceder, Dataset\nof solution-based inorganic materials synthesis proce-\ndures extracted from the scientific literature, Scientific\nData 9, 231 (2022).\n[21] E. Kim, K. Huang, A. Saunders, A. McCallum, G. Ceder,\nand E. Olivetti, Materials synthesis insights from scien-\ntific literature via text extraction and machine learning,\nChemistry of Materials 29, 9436 (2017).\n[22] Z. Jensen, E. Kim, S. Kwon, T. Z. H. Gani, Y. Rom´ an-\nLeshkov, M. Moliner, A. Corma, and E. Olivetti, A ma-\nchine learning approach to zeolite synthesis enabled by\nautomatic literature data extraction, ACS Central Sci-\nence 5, 892 (2019).\n[23] E. Kim, Z. Jensen, A. van Grootel, K. Huang, M. Staib,\nS. Mysore, H.-S. Chang, E. Strubell, A. McCallum,\nS. Jegelka, and E. Olivetti, Inorganic materials synthesis\nplanning with literature-trained neural networks, Journal\nof Chemical Information and Modeling 60, 1194 (2020).\n[24] D. Morgan and R. Jacobs, Opportunities and challenges\nfor machine learning in materials science, Annual Review\nof Materials Research 50, 71 (2020).\n[25] J. E. Saal, A. O. Oliynyk, and B. Meredig, Machine learn-\ning in materials discovery: Confirmed predictions and\ntheir underlying approaches, Annual Review of Materi-\nals Research 50, 49 (2020).\n[26] C. Court and J. Cole, Magnetic and superconducting\nphase diagrams and transition temperatures predicted\nusing text mining and machine learning, npj Comput\nMater 6, 18 (2020).\n[27] H. Huo, C. J. Bartel, T. He, A. Trewartha, A. Dunn,\nB. Ouyang, A. Jain, and G. Ceder, Machine-learning ra-\ntionalization and prediction of solid-state synthesis con-\nditions, Chemistry of Materials 34, 7323 (2022).\n[28] J. Zhao and J. M. Cole, Reconstructing chromatic-\ndispersion relations and predicting refractive indices us-\ning text mining and machine learning, Journal of Chem-\nical Information and Modeling 62, 2670 (2022).\n[29] C. Karpovich, Z. Jensen, V. Venugopal, and\nE. Olivetti, Inorganic synthesis reaction condi-\ntion prediction with generative machine learning\n10.48550/ARXIV.2112.09612 (2021).\n[30] A. B. Georgescu, P. Ren, A. R. Toland, S. Zhang, K. D.\nMiller, D. W. Apley, E. A. Olivetti, N. Wagner, and\nJ. M. Rondinelli, Database, features, and machine learn-\ning model to identify thermally driven metal–insulator\ntransition compounds, Chemistry of Materials 33, 5591\nDigital Discovery, 2024, 3, 1221-1235 https://doi.org/10.1039/D4DD00016A\n(2021).\n[31] E. Beard and J. Cole, Perovskite- and dye-sensitized\nsolar-cell device databases auto-generated using chem-\ndataextractor, Sci Data 9, 329 (2022).\n[32] P. Kumar, S. Kabra, and J. Cole, Auto-generating\ndatabases of yield strength and grain size using chem-\ndataextractor, Sci Data 9, 292 (2022).\n[33] J. Zhao and J. M. Cole, Reconstructing chromatic-\ndispersion relations and predicting refractive indices us-\ning text mining and machine learning, Journal of Chem-\nical Information and Modeling 62, 2670 (2022).\n[34] J. Zhao and J. Cole, A database of refractive indices and\ndielectric constants auto-generated using chemdataex-\ntractor, Sci Data 9, 192 (2022).\n[35] A. Dunn, J. Dagdelen, N. Walker, S. Lee, A. S.\nRosen, G. Ceder, K. Persson, and A. Jain, Struc-\ntured information extraction from complex scien-\ntific text with fine-tuned large language models\n10.48550/ARXIV.2212.05238 (2022).\n[36] Y. Song, S. Miret, H. Zhang, and B. Liu, HoneyBee:\nProgressive instruction finetuning of large language mod-\nels for materials science, in Findings of the Association\nfor Computational Linguistics: EMNLP 2023, edited by\nH. Bouamor, J. Pino, and K. Bali (Association for Com-\nputational Linguistics, Singapore, 2023) pp. 5724–5739.\n[37] J. L´ ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Ro-\ndriques, and A. D. White, Paperqa: Retrieval-augmented\ngenerative agent for scientific research (2023).\n[38] M. Ansari and S. M. Moosavi, Agent-based learning\nof materials datasets from scientific literature (2023),\narXiv:2312.11690.\n[39] M. P. Polak and D. Morgan, Extracting accurate mate-\nrials data from research papers with conversational lan-\nguage models and prompt engineering, Nature Commu-\nnications 15, 1569 (2024).\n[40] P. J. Ray, Master’s thesis: Structural investigation of\nLa(2-x)Sr(x)CuO(4+y) - Following staging as a function\nof temperature. Fig. 2.4., (2016).\n[41] A. M. Hiszpanski, B. Gallagher, K. Chellappan, P. Li,\nS. Liu, H. Kim, J. Han, B. Kailkhura, D. J. Buttler, and\nT. Y.-J. Han, Nanomaterial synthesis insights from ma-\nchine learning of scientific articles by extracting, struc-\nturing, and visualizing knowledge, Journal of Chemical\nInformation and Modeling 60, 2876 (2020).\n[42] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jer-\nnite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush, Transformers: State-of-the-\nart natural language processing, in Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations (Association\nfor Computational Linguistics, Online, 2020) pp. 38–45.\n[43] T. Brown, et. al, Language models are few-shot learners,\nAdvances in Neural Information Processing Systems, 33,\n1877 (2020).\n[44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L.\nWainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kel-\nton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe, Training lan-\nguage models to follow instructions with human feedback\n10.48550/ARXIV.2203.02155 (2022).\n[45] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, Bart:\nDenoising sequence-to-sequence pre-training for natu-\nral language generation, translation, and comprehension\n10.48550/ARXIV.1910.13461 (2019).\n[46] W. Yin, J. Hay, and D. Roth, Benchmarking zero-shot\ntext classification: Datasets, evaluation and entailment\napproach 10.48550/ARXIV.1909.00161 (2019).\n[47] P. He, J. Gao, and W. Chen, Debertav3: Im-\nproving deberta using electra-style pre-training\nwith gradient-disentangled embedding sharing\n10.48550/ARXIV.2111.09543 (2021).\n[48] A. Conneau, G. Lample, R. Rinott, A. Williams,\nS. R. Bowman, H. Schwenk, and V. Stoyanov,\nXnli: Evaluating cross-lingual sentence representations\n10.48550/ARXIV.1809.05053 (2018).\n[49] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen,\nS. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mi-\nhaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,\nP. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,\nOpt: Open pre-trained transformer language models\n10.48550/ARXIV.2205.01068 (2022).\n[50] B. Workshop, :, T. L. Scao, A. Fan, C. Akiki,\nE. Pavlick, S. Ili´ c, D. Hesslow, R. Castagn´ e, A. S.\nLuccioni, F. Yvon, and M. Gall´ e et al., Bloom: A\n176b-parameter open-access multilingual language model\n10.48550/ARXIV.2211.05100 (2022).\n[51] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.\nLachaux, T. Lacroix, B. Rozi` ere, N. Goyal, E. Ham-\nbro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and\nG. Lample, Llama: Open and efficient foundation lan-\nguage models (2023), arXiv:2302.13971.\n[52] Elsevier developer portal, https://dev.elsevier.com,\n[Online; accessed 08-Feb-2023].\n[53] S. Bird, E. Klein, and E. Loper, Natural Language Pro-\ncessing with Python: Analyzing Text with the Natural\nLanguage Toolkit(O’Reilly, 2009).\n[54] S. Behnel, M. Faassen, and I. Bicking, lxml: Xml and\nhtml with python (2005).\n[55] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and\nM. Chen, Hierarchical text-conditional image generation\nwith clip latents 10.48550/ARXIV.2204.06125 (2022).\n[56] Midjourney, https://www.midjourney.com, [Online; ac-\ncessed 08-Feb-2023].\n[57] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, High-resolution image synthesis with la-\ntent diffusion models, in 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR)\n(2022) pp. 10674–10685.\n[58] E. Mosqueira-Rey, E. Hernandez-Pereira, D. Alonso-\nRios, J. Bobes-Bascaran, and A. Fernandez-Leal,\nHuman-in-the-loop machine learning: a state of the art,\nArtificial Intelligence Review 56, 3005 (2023).\n[59] B. T. Afflerbach, C. Francis, L. E. Schultz, J. Speth-\nson, V. Meschke, E. Strand, L. Ward, J. H. Perepezko,\nD. Thoma, P. M. Voyles, I. Szlufarska, and D. Morgan,\nMachine learning prediction of the critical cooling rate\nfor metallic glasses from expanded datasets and elemen-\ntal features, Chemistry of Materials 34, 2945 (2022).\n[60] M. P. Polak, S. Modi, A. Latosinska, J. Zhang, J. Wang,\nS. Wang, A. D. Hazra, and D. Morgan, Data for ’Flex-\nible, Model-agnostic Method for Data Extraction from\nText Using General Purpose Natural Language Process-\ning’ 10.6084/m9.figshare.21861948 (2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7681708335876465
    },
    {
      "name": "Data extraction",
      "score": 0.5045186281204224
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.49960827827453613
    },
    {
      "name": "Natural language processing",
      "score": 0.4763094186782837
    },
    {
      "name": "Language model",
      "score": 0.4721275269985199
    },
    {
      "name": "Recall",
      "score": 0.4696243107318878
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.44390809535980225
    },
    {
      "name": "Property (philosophy)",
      "score": 0.4390963912010193
    },
    {
      "name": "Data mining",
      "score": 0.4066567122936249
    },
    {
      "name": "Information retrieval",
      "score": 0.3885917663574219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38818037509918213
    },
    {
      "name": "MEDLINE",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    }
  ]
}