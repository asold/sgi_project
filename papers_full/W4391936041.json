{
  "title": "CNX-B2: A Novel CNN-Transformer Approach For Chest X-Ray Medical Report Generation",
  "url": "https://openalex.org/W4391936041",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2891718678",
      "name": "Fawaz F. Alqahtani",
      "affiliations": [
        "Najran University"
      ]
    },
    {
      "id": "https://openalex.org/A5061725368",
      "name": "Mashood Mohammad Mohsan",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2804016433",
      "name": "Khalaf Alshamrani",
      "affiliations": [
        "Najran University"
      ]
    },
    {
      "id": "https://openalex.org/A2179969884",
      "name": "Jahan Zeb",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": null,
      "name": "Salihah Alhamami",
      "affiliations": [
        "Najran University"
      ]
    },
    {
      "id": "https://openalex.org/A5093955922",
      "name": "Dareen Alqarni",
      "affiliations": [
        "Najran University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4210315652",
    "https://openalex.org/W3080971459",
    "https://openalex.org/W6751711410",
    "https://openalex.org/W2942396292",
    "https://openalex.org/W6838434570",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W3128439646",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6793467064",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2890888035",
    "https://openalex.org/W2980483218",
    "https://openalex.org/W2997704374",
    "https://openalex.org/W4389778294",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W6859773563",
    "https://openalex.org/W4386076127",
    "https://openalex.org/W4382202677",
    "https://openalex.org/W4312443924",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W6773815586",
    "https://openalex.org/W6766193724",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1924770834"
  ],
  "abstract": "Medical imaging techniques are the most popular non-invasive methods to diagnose chest diseases. Chest X-ray scans are employed commonly to detect chronic obstructive pulmonary diseases and other respiratory diseases. Despite the significance of these diagnostic methods, the process of disease detection and the subsequent task of CXR report writing is tedious for radiologists. Therefore, Automated radiological report generation is a highly desirable task for radiologists. Previous studies were focused on the automated generation of medical reports to achieve greater quantitative scores rather than focusing on the quality of reports. Such approaches suffer from the problem of generating normal reports for CXR with diseases. Additionally, the absence of clear segregation between normal and abnormal samples in publicly available datasets its impossible to evaluate the performance of models in generating rare abnormal reports. To address these issues, we propose CNX-B2 which is a Convolutional Neural Network (CNN) combined with a Transformer approach to generate medical reports. The proposed encoder is designed to be both hybrid and efficient, capturing meaningful spatial features through inherent convolution biases. This enables the transformer-based decoder to robustly convert these features into coherent medical reports. Secondly, we also introduce a new radiological report dataset to evaluate model performances on abnormal reports separately. Our proposed model is further evaluated on the IU-Xray dataset, achieving competitive scores of 0.479 BLEU-1, 0.188 METEOR, and 0.586 CIDER.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nCNX-B2: A Novel CNN-Transformer\nApproach For Chest X-ray Medical\nReport Generation\nFAWAZ F. ALQAHTANI1,3, MASHOOD M. MOHSAN2, KHALAF ALSHAMRANI1, JAHAN ZEB2,\nSALIHAH ALHAMAMI1, AND DAREEN ALQARNI1\n1Department of Radiological Sciences, College of Applied Medical Sciences, Najran University, Najran, Saudi Arabia\n2Department of Computer and Software Engineering, National University of Sciences and Technology, Islamabad, 44000, Pakistan\n3Health Research Centre, Najran University, Najran, Saudi Arabia\nCorresponding author: Fawaz F. Alqahtani (e-mail: ffalqahtani@nu.edu.sa).\nABSTRACT\nMedical imaging techniques are the most popular non-invasive methods to diagnose chest diseases. Chest\nX-ray scans are employed commonly to detect chronic obstructive pulmonary diseases and other respiratory\ndiseases. Despite the significance of these diagnostic methods, the process of disease detection and the\nsubsequent task of CXR report writing is tedious for radiologists. Therefore, Automated radiological report\ngeneration is a highly desirable task for radiologists. Previous studies were focused on the automated\ngeneration of medical reports to achieve greater quantitative scores rather than focusing on the quality\nof reports. Such approaches suffer from the problem of generating normal reports for CXR with diseases.\nAdditionally, the absence of clear segregation between normal and abnormal samples in publicly available\ndatasets its impossible to evaluate the performance of models in generating rare abnormal reports. To\naddress these issues, we propose CNX-B2 which is a Convolutional Neural Network (CNN) combined\nwith a Transformer approach to generate medical reports. The proposed encoder is designed to be both\nhybrid and efficient, capturing meaningful spatial features through inherent convolution biases. This enables\nthe transformer-based decoder to robustly convert these features into coherent medical reports. Secondly,\nwe also introduce a new radiological report dataset to evaluate model performances on abnormal reports\nseparately. Our proposed model is further evaluated on the IU-Xray dataset, achieving competitive scores of\n0.479 BLEU-1, 0.188 METEOR, and 0.586 CIDER.\nINDEX TERMS Convolution Neural Networks, Transformers, Medical Report Generation, Natural\nLanguage Processing\nI. INTRODUCTION\nDiseases that target the chest region are specifically danger-\nous as they can lead to fatality. Lungs play a vital role in\nthe respiratory system and any damage to them can cause\ninefficiency of the human body or could have life-threatening\nimplications too. Chronic respiratory diseases are among\nthe most common non-communicable diseases worldwide,\nlargely due to the ubiquity of noxious environmental, occu-\npational, and behavioural inhalation exposures. In addition to\nchronic obstructive pulmonary disease (COPD) and asthma,\nchronic respiratory diseases include interstitial lung disease,\npulmonary sarcoidosis, and pneumoconiosis, such as silicosis\nand asbestosis. On average, 58000 deaths per year occur only\ndue to pneumonia [1]. The primary practice to detect such\ndiseases is the use of various medical imaging techniques\nsuch as Chest X-ray (CXR), Ultrasound and Magnetic Res-\nonance imaging (MRI) [2]. CXR is the most popular non-\ninvasive imaging technique to detect chest diseases. Under-\nstanding and interpreting CXR and then reporting findings\nis a time-consuming task, labouring and error-prone task for\nradiologists. Automated radiological report generators can\nlighten the workload of radiologists and can also assist them.\nRecent attempts to generate reports have been motivated\nby image captioners. Both tasks share the same generic\nproblem, an image to sequence one. A radiological report\nconsists of patient data, history, radiologist’s findings, and\nimpressions along with multiple CXR scans. It takes approx-\nimately 15-20 minutes to complete this labour procedure.\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThis lengthy and hectic procedure can lead to error if done\nby inexperienced radiologists also it is a time-consuming\nprocedure for experienced radiologists. The problem of it\nbeing a tedious task was greatly highlighted during the 2019\npandemic when a large number of CXR images needed to be\nanalyzed [3]. The objective of automated report generation is\nto take CXR image pixels as input and generate findings and\nimpressions as part of a radiology report.\nDue to the clinical importance of the problem, researchers\nhave proposed many architectures [4], [5] and [6]. Almost\nall research has utilized the Indiana University dataset [7]\nit contains abnormal and normal cases. The frequency of\nnormal sentences in this dataset is very high and was high-\nlighted in [8] which makes the previous proposed architec-\ntures report generation capabilities biased. Another major\nissue of IU dataset is the lack of distribution between normal\nand abnormal reports which makes it difficult to measure\narchitecture performances on rare frequent sentences and\ndiseases. In this work, we propose a new dataset to overcome\nthese limitations.\nA comprehensive review of the literature concluded that\nthe initial attempts to generate radiological reports employed\na CNN image encoder to infer latent features and a Re-\ncurrent Neural Network (RNN) to decode those features to\ngenerate medical findings. The recent advancements in lan-\nguage modelling have brought out a novel architecture named\nTransformers. It is the only architecture that is recurrence\nand convolution free. From 2017, Transformers [9] has not\nonly dominated language models such as RNN and RNN\nwith attention but also has dominated CNN in the Computer\nVision field. To improve the current accuracy of the report\ngenerator this article has proposed a novel configuration in\nthe Encoder-Decoder arrangement.\nThe contribution of this article is summarized as follows:\n1) A novel CNX-B2 model is proposed which is based on\nCNN-Transformer approach. ConvNeXT is employed\nas image encoder which has the the ability to capture\nvisual features due to spatial biasness but its design is\nmodified according to Vision Transformers making it\nmore efficent. BioBERT is a medical language based\nTransformer which is employed to generate radiologi-\ncal reports as the decoder.\n2) A new dataset was proposed to overcome the limitation\nof IU dataset along with a distribution of normal and\nabnormal reports. This dataset is public and can be\nused for evaluating model performance on normal and\nabnormal reports.\n3) The proposed architecture is evaluated on IU dataset\nand as well as on our proposed dataset. CNX-B2 has\noutperformed CDGPT2 [10], a previous attempt of\ncombining CNN and Transformer and achieved re-\nmarkable BLEU scores against earlier attempts.\nThe remaining part of the article is structured as follows.\nSection 2 examines related literature. The methodology is\nexplained in Section 3. The experimental details and results\nare discussed in Section 4.\nII. REVIEW LITERATURE\nAutomated Radiological Report Generation is a derivative\ntechnique to describe clinical details of Chest X-ray images.\nIt is a combination of computer vision and Natural Language\nProcessing which have a strong societal impact. Description\nretrieval, template filling, and hand-crafted NLP techniques\nwere some of the earlier methods of report writing.\nThere were many advancements in automated medical\nreport generation later, but the base arrangement of each\nmethod was to utilize an image encoder for converting CXR\nimages into a latent space and then bring a decoder into play\nto generate medical reports. The problem was generically\nidentified as an image-to-sequence problem. We have divided\nthe review literature based on the encoder-decoder architec-\ntures used in Automated radiological report generation.\nCNN-RNN is the most employed encoder-decoder config-\nuration. RNN [11] are employed in time series forecasting,\nclassification, signal processing, and NLP due to their ability\nto learn over multiple time steps. Long Short-Term Memory\n(LSTM) and Gate Recurrent Unit (GRU) [12] are popular ar-\nchitectures maintaining context over extended periods. Later\nAttention mechanisms enabled context learning without time\nstep constraints, allowing the RNN’s to encode or decode rel-\nevant information without fixed-length context vectors [13].\nThe CNN encoder captures visual dominant features from\nCXR using its inherent spatial biases and Recurrent Neural\nNetwork (RNN) decodes these spatial features to text. To fur-\nther increase the accuracy many techniques were introduced.\nLi et al. [4] utilized a unique hierarchical decision-making\nprocedure known as Hybrid Retrieval-Generation Reinforced\nAgent (HRGR-Agent). A high-level retrieval policy module\ndecided whether to use a low-level generation module to cre-\nate a new sentence or to obtain a template sentence from an\nexisting template database for each sentence. This technique\nassisted the generator to detect abnormal findings at a higher\nrate. Moreover, Xue et al. [14] achieved multimodality and\nemployed the attention mechanism,\nThe idea of having a separate generator or decoder for nor-\nmal and abnormal cases was further explored using LSTM\nnetworks. The encoded vector was passed to a topic genera-\ntor, consisting of a single layer LSTM, to generate a sequence\nof high-level topics and was assisted by a gate module to\nproduce distribution over normal and abnormal cases. The\nAttention-based Abnormal-Aware Fusion Network (A3FN)\ngenerated structured, topic-coherent and abnormality-aware\nradiological reports [5]. To furthermore increase the ability\nof an RNN to generate rare diseases, techniques such as\nfew shot, Graph CNN, relational memory networks, and\nGenerative Adversarial Networks were also explored [15] [8]\n[16] [17].\nTransformers [22] was a revolutionary approach for\nsequence-to-sequence tasks such as Machine Translation. It\nwas recurrence and convolution free network. Li et al. [23]\nfirst employed Transformers for text recognition. It leveraged\nthe Transformer architecture for both image understanding\nand wordpiece-level text generation. Memory-Driven Trans-\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1. Literature review summary table\nSr# Method Year Approach\n1 HRGR-Agent [4] 2018 CNN, RNN and Retrieval Policy Module\n2 CNN + 3 RNN [14] 2018 CNN, Sentence and Recurrent Paragraph Generative model\n3 A3FN [5] 2019 CNN and separate LSTM abnormal report generator\n4 ResBlock + Multi-attention [15] 2019 Multi-attention, and Fusion module, Sentence and Word RNN\n5 Raregen [8] 2020 Few shot GAN, Garph Network and hierarchical LSTM\n6 SentSAT + KG [16] 2020 Densenet, Graph Convolution and LSTM\n7 R2Gen [18] 2020 Relational Memory driven Transformers\n8 CDGPT2 [10] 2021 Chexnet combined with GPT2\n9 VFE + GKE + SKE + RG [6] 2022 A unique combination of CNN, Knowledge graph and Transformer\n10 JPG [19] 2022 Traditional CNN combined with a complete Transformer\n11 AERMNET [17] 2023 CNN combined with LSTM and Relational Memory Module\n12 AdaMatch-based LLM [20] 2023 Two Vision Transformers, BioClinicalBERT and a Large Language\nModel (LLM)\n13 DCL [21] 2023 Two unimodal encoders, a multimodal encoder, a report decoder and\nthree dynamic graph modules\nformer was proposed for automated medical report gener-\nation. The decoder layer incorporates Memory-Conditioned\nLayer Normalization, enhancing report generation capability\n[18].\nRecently, Large Langauge Model (LLM) was employed\nusing Cyclic techniques for report generation [20]. Con-\ntrastive Learning is a technique to improve representation\nlearning. Li et al. [21] proposed a Dynamic graph combined\nwith Contrastive Learning in Transformers. This improved\nvisual and text representation in medical report generation\ntask. Furthermore, 3D shared subspace was also explored for\nrepresentation improvement [19].\nCNN-Transformers approach employs a CNN to capture\nspatial features from CXR and a Transformer, dominant\nin language generation, decodes these features to generate\nreports. Alfarghaly et al. [10] was the first to employ this\napproach. It used visual and semantic features to condi-\ntion the decoder for faster training. Furthermore, various\nknowledge graphs and word embeddings were also combined\nwith spatial features to assist Transformers-based decoders\n[6]. Although these approaches show improvements but a\npure CNN-Transformer approach for report generation is still\nunexplored. Table 1 summarizes the review literature.\nIII. PROPOSED METHOD\nFigure 2 illustrates the proposed methodology. ConvNext\n[24], a CNN based encoder, is employed to encode image\nfeatures and BioBERT [25], a transformer based, language\nmodel processes the image features (Hidden values) to gen-\nerate radiological medical reports.The output of ConvNext is\nused to calculateQ query, andK key vectors in cross attention\nlayers of decoder. The output of encoder is fed to BioBERT\nbeing a variant of BERT models lack the capability of text\ngeneration so additional layers of cross attention were added\nin it to be used as decoder.\nA. ENCODER\nConvNext models are constructed from Residual Networks\n(ResNets) but having similar architectural design improve-\nments of transformers excluding attention layers. In 2021,\nHierarchical transformers such as SWIN transformer [26]\nproposed a novel \"sliding window\" mechanism employing at-\ntention within them indicating the use of convolution was still\nrelevant. ConvNext being a pure convolution based model\nadapts the advanced architectural feature of transformer net-\nwork visualizing the importance of CNN as a generic back\nbone or as encoders. It contains a stack of stages each\nconsisting of different number of layers with distinct kernel\nsizes of convolution in it. The design changes in ResNet to\nproduce ConvNeXt model are stated below.\nFIGURE 1. (a)A simple ResNet block. (b) A ConvNeXt block after macro\ndesigning, ResNext-ify and other alteration of a ResNet block.\n(1) Macro Designing: CNN networks has multi-stage\nblock like designs and each of them has different feature\nmap resolution. SWIN models adopted them in transformers\nwith a stage ratio of 1:1:3:1 and for larger variants of it had\n1:1:9:1 and whereas standard ResNet has 3:4:6:3 stages so in\nConvNext it was converted to 1:1:3:1 stage ratio. Secondly,\n\"stem cell\" structure also known as the input layer of Resnet\nemploys a 7x7 convolution layer with a stride of 2, as natural\nimages are inherent with redundancy, the stem cell gener-\nally down sample the input images. SWIN transformer uses\n\"Patchify\" strategy with a more aggressive approach to down-\nscale input image with non-overlapping convolution in order\nto accommodate the multi-stage design. ConvNext \"Stem\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. Figure represents the proposed architecture. The left side illustrates the Image Encoder consisting of ConvNeXt block and right side is the BioBERT\nlanguage decoder along with cross attention for radiologist report generation.\ncell\" layer was replaced by SWIN transformer \"Patchify\"\nwith a convolution layer of 4x4 with a stride of 4.\n(2) ResNext-ify:The ResNeXt [27] principle is to separate\nconvolution kernels into groups in bottleneck layer. This in\nshort expands the width of model and significantly reduce\nFLOPs. For ConvNext we have employed depth-wise con-\nvolution as it is similar to the weighted sum operation in\nattention mechanism. The use of 1x1 convolution and depth\nwise convolution combines the spatial and channel features\nseparately. This reduced FLOPS of the model but also re-\nduced the accuracy so following the strategy of RexNeXt we\nalso increased the width of ConvNext from 64 to 96.\n(3) Inverted bottle neck and Kernel size:A major design\nin Transformers is the inverted bottle neck and it layer is 4x\ntimes wider than the input feature map. A similar scheme was\napplied in ConvNeXt model. This step although increases the\nFLOPs of the block but for overall model, it was decreased\ndue to lack operation for down sampling in the architecture.\nAnother design parameter of SWIN transformer is the use\n7x7 kernel in the inverted block where as ConvNext used 3x3.\nTo increase the size of kernel we first had to move the depth\nwise layer up and then increase the kernel size to 7x7. Figure\n1 illustrates the design changes.\n(4) Micro Design: Some micro scale design features of\nResnet was also replaced by SWIN transformer features. The\nGaussian Error Linear Unit (GELU) a smoother version of\nReLU is employed as the activation function of the model.\nAnother major difference between a transformer block and\nResnet block is the use of less activation function. Therefore\nin Resnet block all activation function were removed expect\nfor the middle layer, replicating it with SWIN model. Simi-\nlarly, the count of normalization layer was also reduced and\nBatch Normalization was replaced by Layer Normalization.\nB. DECODER\nThe architecture of BioBERT was to serve as an Encoder\n[25]. It was a stack of identical layers and each layer con-\nsisted of two sub-layers: The multi-headed Self Attention\nsub-layer and Position wise feed-forward sub-layer. In our\napproach, we employed BioBERT as a decoder. We added\ncross-attention between the two sublayers so it can process\nthe hidden states or context vector from the image-based\nencoder, ConvNeXt. To add a third sub-layer to the two sub-\nlayers that are already present in each Decoder, the decoder\nexecutes multi-head attention over the hidden states from\nthe encoder, which are then turned into queries and keys.\nEach sub-layer employs residual connections in a way that is\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ncomparable to the encoder’s behaviour before layer normal-\nization. The \"Masked Multi-head attention\" layer is used by\nthe decoder to stop focusing on succeeding places. Due to the\nmasking technique and the moving of the output embedding\noffset by one place as a result of Casual Language Modelling\n(CLM), the prediction of location i can only rely on the\nknown outputs at positions lower thani. The hyperparameters\nof BioBERT we used are listed in table 2.\n.\n(1) Text input representation:The decoder receives a set\nof tokens as input. The feature vectors of each token’s d\ndimension are retrieved using the Embedding look-up table\nof V vocabulary length, and positional encoding of the se-\nquence is then appended. The entire report, token by token\nin a causality manner, is provided as decoder input during\ntraining, but only the initial start token is provided during\ninference in order to construct subsequent words using CLM.\n(2) Self attention: The key element of a Transformer\nmodel is the self attention mechanism which extracts most\nimportant or attends relevant features from input text. After\nthe combination of embedding vector along with positional\nencoding, resultant passed though various linear layers to\ngenerate Q Query, K Key and V Value vectors for calcula-\ntion of Self attention. The following equation represents the\ncomputation of attention mechanism:\nSelf Attention (Q, K, V) =softmax((Q · KT)/\np\ndk) · V\n(1)\nDot product multiplication of Q and K vectors makes\ncomputation efficient in Transformers and makes it possible\nto compile multi headed self attention (MSA) layers and large\nlanguage models. Gradient explosion is avoided by applying\nthe scaling factor of √dk dimension of key vector. Later,\nsoftmax function is implemented to assign probabilities to\nmost attended values fromV vector. The multi headed self at-\ntention mechanism helps gather information from several sub\nspaces, h times, so any relevant features are never left behind.\nThe projected results from MSA are concatenated altogether\nto maintain the same dimension as the input feature.\nMultihead(Q, K, V) =concat(h1...hn) · Wo (2)\nwhere\nheadi = Self Attention(Wq\ni qj, Wk\ni K, Wv\ni V )\nEquation 2 depicts calculation of a MSA layer in which\nn represents total number of attentions in one MSA layer\nwhereas W iq, W ik and W iv represents weight matrices of\nQuery, Key and Value respectively of each single attention\nhead.\n(3) Cross attention:In sequence to sequence tasks such\nas machine translation, question answering and text summa-\nrizing, the transformer architecture has a special sub layer\nknown cross attention layer in decoder which attends dif-\nferent parts of encoded hidden vector and decoder vector\nto generate text. The hidden feature vector generated by the\nencoder is also known as latent vector, which has concealed\nall information of one domain or language.\nCross Attention (Qdec, Kenc, Venc) = softmax(Qdec · KT\nenc)p\ndkenc · Venc\n(3)\nIn our case, BioBERT is an encoder based architecture and\nwe are translating images to text in form of medical reports\nso cross attention, also known as encoder-decoder attention,\nis introduced with random initialization of weights. Equation\n3 demonstrates the computation of a single cross attention\nhead where Qdec, Kenc and Venc are Query from previous\nDecoder layer, Key and Value are vectors generated from\nEncoder’s hidden states respectively. Based on the context\nof the output sequence created so far, cross attention enables\nthe decoder to choose to pay attention to different sections of\nthe input sequence. Due to its significance in many sequence-\nto-sequence tasks, Transformer models have excelled at these\ntasks, achieving state-of-the-art outcomes.\nIV. EXPERIMENTATION\nThe proposed novel methodology for medical report genera-\ntion was experimented on two datasets and all implementa-\ntion details are mentioned below.\nA. IMPLEMENTATION DETAILS\nThe proposed architecture for radiological report generation\nwas implemented on PyTorch utilizing Hugging Face library.\nThe model was trained on dual Nvidia RTX 2070 8GB\nsystem for 20 epochs with a batch size of 4 on each GPU. The\nother hyper parameters are mention in table 2. The training\nand testing phase for each dataset was done separately and no\nweights were shared between them.\nTABLE 2. List of parameter and hyper parameters of proposed architecture.\nSr# Parameters Encoder Decoder\n1 Model ConvNeXt BioBERT\n2 Hidden layers - 12\n3 Number of stages 4 -\n4 Depth 3, 3, 27, 3 -\n5 Attention heads - 12\n6 Intermediate size - 3072\n7 Hidden size 128, 256, 512, 1024 768\n8 Patch size 4 -\n9 Image size 224 -\n10 V ocabulary size - 28996\n11 Max length - 50\n12 Beams - 4\n13 Layer normalization 1 e−12 1e−12\nB. EVALUATION METRICS\nTo asses the radiological report generation capability of our\nproposed method. We employed BLEU [28], METEOR [29],\nROUGE L [30] and CIDER [31]. BLEU even being used\nfor language translation tasks is also employed in report\ngeneration tasks in literature. CIDER metric was proposed\nto capture human judgment of consensus in image captioning\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 3. (a)A sample IU dataset [7] containing X-ray image, Indication,\nImpression and Findings. (b) A sample of our dataset containing Xray image,\nAge, Gender, and Findings.\ntasks, but is not employed much in medical report generation.\nOriginal implementation of COCO image captioning 1. was\nutilized to measure scores to maintain standardization.\nC. DATASETS\nThe proposed method was evaluated on two datsets: IU\ndataset and Our dataset. Training and Testing of both datasets\nwere conducted separately.\nIU dataset: The Indiana University (IU) chest X-ray\ndataset [7] is one the most widely used dataset which contains\n3995 reports along with 7470 multiple CXR images. It is\na public dataset created for training and testing of X-ray\nimages to test algorithm for classification, summary gener-\nation or report generation tasks. Each report contains patient\ndetails, impressions, reports and tags for Indiana database.\nPersonal information of patients revealing identity is hidden\nby XXXX. Unfortunately, train and test split were never re-\nleased hence making it difficult for bench marking purposes\nalong with that, many information is missing and pairs are\nalso incomplete.\nOur dataset:We present a novel dataset that is consistent\nwith distribution of normal and abnormal reports. The Pic-\nture Archiving and Communication System of King Khalid\nHospital in Najran, Saudi Arabia was searched for all chest x-\nray performed between November 2019 and November 2022.\nA total of 1250 image and report pairs were collected. The\nanalysed data included 528 abnormal cases and 722 normal:\n471 female and 779 male cases. The mean age of these\nsubjects was approximately 53 years. Our dataset is a small\nand consistent dataset elaborating variety in medical reports\nalong with clear distribution of abnormal and normal pairs\nfor quicker harmonious benchmarking.\nFigure 3 provides a sample CXR images along with the\nImpression, Findings, Indication, Age, and Gender of pa-\ntients from both datasets. Table 3 contains the details of the\nTrain, Validation, and Test split of both datasets.\n1https://github.com/yikang-li/coco-caption\nTABLE 3. Train, Validation, and Test split details of Our dataset and IU\ndataset [7]\nSr# Split IU dataset [7] Ours dataset\n1 Train 2403 873\n2 Validation 500 253\n3 Test 300 124\nV. RESULTS AND ANALYSIS\nIn this section, we present a comprehensive analysis of the\nresults obtained from our experimentation, shedding light on\nkey findings and their implications. We performed quantita-\ntive and qualitative analyses separately. Finally, we compared\nthe results of our proposed architecture with existing meth-\nods.\nA. QUANTITATIVE RESULTS\nThe proposed model employs a pre-trained CNN image\nmodel, trained on natural images, and a language model with-\nout pre-trained weights. In our experimental investigation,\ntests with three language models were performed on IU and\nOur dataset. The effectiveness of the models was evaluated\nusing automatic assessment metrics like the n-gram similarity\nbetween the produced sentences and the ground truth reports.\nGPT2 [32], MiniLM [33] and BioBERT [25] transformer-\nbased language models are employed for experimental study.\nTable 4 displays the experimental results, displaying the\neffectiveness of each model.\nTABLE 4. Results of experimental investigation on IU [7] and our dataset\nSr# Encoder Decoder BLEU 1 BLEU 2 BLEU 3 BLEU 4\nIndiana University Dataset [7]\n1 GPT2 0.356 0.221 0.152 0.098\n2 ConvNeXt BioBERT 0.479 0.363 0.261 0.173\n3 MiniLM 0.371 0.257 0.183 0.109\nOur Dataset\n5 GPT2 0.341 0.318 0.302 0.284\n6 ConvNeXt BioBERT 0.445 0.409 0.389 0.375\n7 MiniLM 0.364 0.307 0.281 0.263\nThe IU dataset does not have a distribution between nor-\nmal and abnormal cases. This makes it difficult to evaluate\nmodels performance on abnormal cases. Our dataset on the\nother hand evaluates model performance on both cases and\nspecially highlighting the false positive reports.\nB. QUALITATIVE RESULTS\nRelying merely on numeric numbers, as shown in table 4,\nmay not give an exhaustive review due to the imbalance and\nbias towards normal reports in the IU dataset. To evaluate the\noverall quality of the reports that were generated, we thus car-\nried out a comparison between various test samples on both\ndatasets. The results provided by our proposed model closely\nmatch the actual data in IU dataset, which corresponds to\nthe most typical occurrences. Figure 4 displays multiple test\nsamples of IU. These reports are clear and offer information\nin-depth enough to compare to the reference report.\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4. A comparison of generated reports by various models on multiple samples of IU dataset [7]. The colour green means similar and red indicates wrong\nfindings.\nWe compared CNX-BR2 report generation with recent\nSOTA’s for qualitative comparison. We used AERMNet and\nCDGPT2 models which are public. The generated reports\nfrom unseen CXR of our and SOTA’s model can viewed at\nfigure 4. In the first example (topmost) CNX-B2 generated\nthe most similar report to ground truth. The AERMNet model\nindicated diseases such as \"Cardiomegaly\" and indicated \"left\nretrocardiac opacities\" whereas the ground truth suggests\nthat CXR is normal. Another important finding is that most of\nthe reports generated by our CNX-B2 and CDGPT2 are sim-\nilar except for the first case. It is to be noted that AERMNet\nand CDGPT2 are trained on a different IU dataset split and\nif they were trained on similar splits then reports may vary.\nUnfortunately, due to a lack of abnormal reports, a proper\nevaluation of all models cannot be performed.\nOn the other hand, Figure 5 displays some abnormal\nand normal samples from test split of our dataset. The first\nsample is abnormal and the proposed model predicted exactly\nsimilarly to the reference report indicating a complete True\nnegative. An interesting finding can be highlighted in the\nsecond sample where the proposed model utilized a synonym\nmaintaining the context of the report. \"no signs\" was re-\nplaced by \"no evidence\" and this type of accurate prediction\nis missed in quantitative analysis. Lastly, a normal report\nsample is generated as it is.\nC. COMPARISON WITH LITERATURE\nAfter the experimental research had completed the model,\nwe moved forward with a comparison to current state-of-the-\nart methods. The findings of the automated metrics for the\nFIGURE 5. A comparison of generated reports by proposed model on multiple\nsamples of our dataset.\ngenerated reports in comparison to the literature are shown in\nTable 5. While using a Traditional CNN pre-trained on CXR\npictures for the Encoder, CDGPT2 used a Transformer with\npre-trained weights for the Decoder. In spite of this strategy,\nCDGPT2 was able to obtain a BLEU-1 score of 0.38.\nOur proposed model has the greatest BLEU-2 score when\ncomparing all of these methods on the IU dataset, demon-\nstrating its overall competence in producing reports that\nare very similar to those produced by radiologists. It has\noutperformed CDGPT2. Other studies such as HRGR-Agent,\nResBlock Multi-attention and SentSAT + KG have employed\nRNN for language decoder and a traditional CNN for image\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5. Comparison with review literature\nSr# Model Dataset BLEU 1 BLEU 2 BLEU 3 BLEU 4 METEOR ROUGE CIDER\n1 HRGR-Agent [4]\nIU [7]\n0.438 0.298 0.208 0.151 - 0.322 0.343\n2 CNN + 3 RNN [14] 0.464 0.358 0.270 0.195 0.274 0.366 -\n3 A3FN [5] 0.443 0.337 0.236 0.181 - 0.347 0.374\n4 ResBlock + Multi-attention [15] 0.476 0.340 0.238 0.169 - 0.347 0.297\n5 Raregen [8] 0.448 0.340 0.255 0.178 - 0.371 0.378\n6 SentSAT + KG [16] 0.441 0.291 0.203 0.147 - 0.367 0.304\n7 R2Gen [18] 0.470 0.304 0.219 0.165 0.187 0.371 -\n8 CDGPT2 [10] 0.387 0.245 0.166 0.111 0.164 0.289 0.257\n9 VFE + GKE + SKE + RG [6] 0.496 0.327 0.238 0.178 - 0.381 0.382\n10 JPG [19] 0.479 0.319 0.222 0.174 0.193 0.377 -\n11 AERMNET [17] 0.486 0.321 0.236 0.183 0.219 0.398 0.560\n12 AdaMatch-based LLM [20] 0.416 0.300 0.207 0.144 0.162 0.365 -\n13 DCL [21] - - - 0.163 0.193 0.383 0.586\n14 Ours 0.479 0.363 0.261 0.173 0.188 0.354 0.408\n15 Ours Ours 0.445 0.409 0.389 0.375 0.279 0.60 0.603\nencoder. Comparing all these techniques on the IU dataset,\nour proposed model demonstrates remarkable scores, indicat-\ning its overall effectiveness in generating reports that closely\nresemble those written by radiologists.\nIn comparison to previous CNN-Transformer approaches\n[10] [18], the CNX-B2 models achieve the best quantitative\nscores in BLEU-2, BLEU-3, METEOR, and CIDER metrics.\nThis demonstrates the effectiveness of having a hybrid CNN\nencoder for a transformer decoder in medical report genera-\ntion.\nD. DISCUSSION\nEven though Transformers are dominating CNN in vision-\nbased tasks, this does not mean that CNN is cornered. The po-\ntential of Convolutional layers still plays a critical role in cap-\nturing spatial features. Our CNX-B2 highlights that having an\nimage encoder of CNN generates useful reports. The Con-\nvNeXt adopts efficient methods of a vision transformer with-\nout using attention layers. The proposed approach, CNN-\nTransformer, competes with previous CNN-RNN or com-\nplete Transformers-based research as highlighted in table 5.\nIn IU dataset, many reports exhibit repetitive and iden-\ntical descriptive sentences. The doctor’s report frequency\nplot reveals a skewed distribution, with abnormal sentences\nfrequently occurring (frequency = 1) across the entire dataset.\nSpecifically, sentences with a frequency of less than 3 ac-\ncount for 6,290 out of 8,022 unique sentences [34]. To\nevaluate CNX-B2 on abnormal we evaluate it on our dataset.\nThe robustness and understanding of CNX-B2 are shown in\nfigure 5 where it uses a synonym with the correct context.\nThe training of CNX-B2 involved the utilization of pre-\ntrained weights, facilitating its easy integration into real-time\napplications by various institutions. This adaptability allows\nit to quickly grasp language understanding from a small\ncorpus, rather than undergoing training from scratch.\nThe computational complexity of the CNX-B2 is 222\nGFLOPS with 224.31 M parameter. It is one of the lim-\nitations of our research and we hope future research can\nexplore developing a lightweight approach for medical report\ngeneration.\nVI. CONCLUSION\nIn this research, we studied the effect of hybrid CNN as\nan image encoder for radiology medical report generation.\nWe aim to address two issues: 1) The lack of distribution\nof abnormal and normal reports in datasets, and 2) How\nto improve model report generation capability. To address\nthem, We first introduced a novel dataset collected from King\nKhalid Hospital in Najran, Saudi Arabia. This dataset con-\ntains a segregation of reports which assisted in the evaluation\nof our approach. Secondly, We propose a CNX-B2, a CNN-\nTransformer, approach which employs ConvNeXt (hybrid\nCNN) capable of capturing spatial features and BioBERT as\na decoder for report generation. Our experimentation demon-\nstrates that CNX-B2 competes with previous approaches in\nNatural Language Generation (NLG) metrics. The qualitative\nanalysis demonstrates CNX-B2 has a good understanding\nof language which makes it justified to be employed as an\nautomated radiological medical report generator.\nVII. FUTURE WORK AND LIMITATIONS\nOur CNX-B2 is not the perfect approach. One of the ma-\njor limitations is that it has a computational complexity\nof 222 GFLOPS and contains 224.31 million parameters.\nThis makes it impossible to be inferred on mobile devices.\nWe intend to propose lightweight approaches in future for\ninference purposes. Secondly, some of the generated reports\nby CNX-B2 are not similar to ground truths even by attaining\ngood language understanding. However, this problem also\nhappens in previous works. Therefore, our future plan is\nto develop automated medical report generators using vi-\nsion and language-based mobile networks and achieve better\nquantitative and qualitative results. Furthermore, We also aim\nto evaluate automated generated reports from radiologists.\nACKNOWLEDGEMENT\nAuthors would like to acknowledge the support of the\nDeputy for Research and InnovationMinistry of Education,\nKingdom of Saudi Arabia for this research through a grant\n(NU/IFC/2/MRC/-/6) under the Institutional Funding Com-\nmittee at Najran University, Kingdom of Saudi Arabia.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nREFERENCES\n[1] T. Gupte, A. Knack, and J. D. Cramer, “Mortality from aspiration pneu-\nmonia: incidence, trends, and risk factors,” Dysphagia, vol. 37, no. 6, pp.\n1493–1500, 2022.\n[2] D. Sutton, Textbook of Radiology and Imaging - 2 V ol\nSet IND Reprint. Elsevier India, 2014. [Online]. Available:\nhttps://books.google.ae/books?id=8NrboAEACAAJ\n[3] N. L. Demirjian, “Impacts of the coronavirus disease 2019 (covid-19)\npandemic on healthcare workers: A nationwide survey of united states\nradiologists,” Clinical Imaging, 2020.\n[4] Y . Li, X. Liang, Z. Hu, and E. P. Xing, “Hybrid retrieval-generation\nreinforced agent for medical image report generation,” Advances in neural\ninformation processing systems, vol. 31, 2018.\n[5] X. Xie, Y . Xiong, P. S. Yu, K. Li, S. Zhang, and Y . Zhu, “Attention-\nbased abnormal-aware fusion network for radiology report generation,”\nin Database Systems for Advanced Applications: DASFAA 2019 Inter-\nnational Workshops: BDMS, BDQM, and GDMA, Chiang Mai, Thailand,\nApril 22–25, 2019, Proceedings 24. Springer, 2019, pp. 448–452.\n[6] S. Yang, X. Wu, S. Ge, S. K. Zhou, and L. Xiao, “Knowledge matters:\nChest radiology report generation with general and specific knowledge,”\nMedical Image Analysis, vol. 80, p. 102510, 2022. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1361841522001578\n[7] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan,\nL. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, “Preparing a\ncollection of radiology examinations for distribution and retrieval,” Journal\nof the American Medical Informatics Association, vol. 23, no. 2, pp. 304–\n310, 2016.\n[8] X. Jia, Y . Xiong, J. Zhang, Y . Zhang, and Y . Zhu, “Few-shot radiology re-\nport generation for rare diseases,” in 2020 IEEE International Conference\non Bioinformatics and Biomedicine (BIBM), 2020, pp. 601–608.\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,\nvol. 30. Curran Associates, Inc., 2017.\n[10] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal, and\nA. Fahmy, “Automated radiology report generation us-\ning conditioned transformers,” Informatics in Medicine Un-\nlocked, vol. 24, p. 100557, 2021. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S2352914821000472\n[11] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14, no. 2,\npp. 179–211, 1990.\n[12] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical evaluation of\ngated recurrent neural networks on sequence modeling,” arXiv preprint\narXiv:1412.3555, 2014.\n[13] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\n2014.\n[14] Y . Xue, T. Xu, L. Rodney Long, Z. Xue, S. Antani, G. R. Thoma, and\nX. Huang, “Multimodal recurrent model with attention for automated\nradiology report generation,” in Medical Image Computing and Com-\nputer Assisted Intervention–MICCAI 2018: 21st International Conference,\nGranada, Spain, September 16-20, 2018, Proceedings, Part I. Springer,\n2018, pp. 457–466.\n[15] X. Huang, F. Yan, W. Xu, and M. Li, “Multi-attention and incorporating\nbackground information model for chest x-ray image report generation,”\nIEEE Access, vol. 7, pp. 154 808–154 817, 2019.\n[16] Y . Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, and D. Xu, “When radiology\nreport generation meets knowledge graph,” in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 12 910–\n12 917.\n[17] X. Zeng, T. Liao, L. Xu, and Z. Wang, “Aermnet: Attention-enhanced re-\nlational memory network for medical image report generation,” Computer\nMethods and Programs in Biomedicine, p. 107979, 2023.\n[18] Z. Chen, Y . Song, T.-H. Chang, and X. Wan, “Generating radiology reports\nvia memory-driven transformer,” in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), 2020,\npp. 1439–1449.\n[19] J. You, D. Li, M. Okumura, and K. Suzuki, “Jpg-jointly learn to align: Au-\ntomated disease prediction and radiology report generation,” in Proceed-\nings of the 29th International Conference on Computational Linguistics,\n2022, pp. 5989–6001.\n[20] W. Chen, X. Li, L. Shen, and Y . Yuan, “Fine-grained image-text alignment\nin medical imaging enables cyclic image-report generation,” arXiv preprint\narXiv:2312.08078, 2023.\n[21] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang, “Dynamic\ngraph enhanced contrastive learning for chest x-ray report generation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 3334–3343.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[23] M. Li, T. Lv, J. Chen, L. Cui, Y . Lu, D. Florencio, C. Zhang, Z. Li,\nand F. Wei, “Trocr: Transformer-based optical character recognition\nwith pre-trained models,” in AAAI 2023, February 2023. [Online].\nAvailable: https://www.microsoft.com/en-us/research/publication/trocr-\ntransformer-based-optical-character-recognition-with-pre-trained-models/\n[24] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A\nconvnet for the 2020s,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 11 976–11 986.\n[25] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert:\na pre-trained biomedical language representation model for biomedical\ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[26] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin\ntransformer: Hierarchical vision transformer using shifted windows,” in\nProceedings of the IEEE/CVF international conference on computer vi-\nsion, 2021, pp. 10 012–10 022.\n[27] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual\ntransformations for deep neural networks,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 1492–\n1500.\n[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics, 2002,\npp. 311–318.\n[29] A. Agarwal and A. Lavie, “Meteor: An automatic metric for mt evaluation\nwith high levels of correlation with human judgments,” Proceedings of\nWMT-08, 2007.\n[30] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,” in\nText summarization branches out, 2004, pp. 74–81.\n[31] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-\nbased image description evaluation,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2015, pp. 4566–4575.\n[32] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[33] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, “Minilm:\nDeep self-attention distillation for task-agnostic compression of pre-\ntrained transformers,” Advances in Neural Information Processing Sys-\ntems, vol. 33, pp. 5776–5788, 2020.\n[34] P. Harzig, Y .-Y . Chen, F. Chen, and R. Lienhart, “Addressing data\nbias problems for chest x-ray image report generation,” arXiv preprint\narXiv:1908.02123, 2019.\nDR FAWAZ F . ALQAHTANIreceived his Ph.D.\ndegree in Sheffield Children’s Hospital, The Uni-\nversity of Sheffield, UK. He has more than 13\nyears of academic experience (since 2010) in\nteaching and research. Currently, he is an Asso-\nciate Professor with the Radiological Sciences De-\npartment, Najran University, Saudi Arabia. He has\na strong scientific background in spine imaging,\nArtificial Intelligence methods in medical imaging\nfield. Furthermore, He is actively exploring and\nconducting various experimental and computational research work including\nthe potential of using glass technologies in medical imaging and radiological\nsciences applications.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nMR. MASHOOD MOHAMMAD MOHSAN did\nhis BS degrees in Computer Science from the\nUniversity of Agriculture (UAF), Faisalabad, in\n2020. He has recently completed his MS in Com-\nputer Software Engineering from the College of\nElectrical and Mechanical Engineering, National\nUniversity of Sciences and Technology (NUST),\nRawalpindi and is currently doing collaborative\nresearch in medical imaging.\nDR. KHALAF ALSHAMRANI received the MSc\ndegree in diagnostic radiography from Cardiff\nUniversity in 2014 and his Ph.D. Degree in pae-\ndiatric radiology from the Medical School at\nSheffield University in 2019. His research interests\ninclude, skeletal imaging in paediatric, artificial\nintelligence in radiology and bone density research\nin children.\nMR. JAHAN ZEB received his Masters degree\nin Computer Software Engineering from National\nUniversity of Science and Technology, Pakistan.\nHe is currently serving as assistant professor at\nCEME, NUST.\nMISS SALIHAH ALHAMAMI received the BSc.\ndegree in Diagnostic Radiology from the Applied\nMedical Sciences, Najran University , Saudi Ara-\nbia, in 2022 . She currently works in Najran Uni-\nversity hospital as radiographer and quality Officer\nof radiology Department in the hospital. Her cur-\nrent research interest include artificial intelligence,\nVascular Ultrasound, and radiation protection.\nMISS DAREEN ALQARNIobtained a bachelor’s\ndegree in diagnostic radiology from Najran Uni-\nversity, Najran, KSA, in 2022 She is currently a\ndiagnostic radiologist at Najran University Hospi-\ntal and an assistant in the Quality Department of\nDiagnostic Radiology She is interested in research\nand her current research interests include the fields\nof diagnostic radiology, especially the field of\nultrasound and the field of artificial intelligence.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367360\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.779159426689148
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6659356355667114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5253738164901733
    },
    {
      "name": "Deep learning",
      "score": 0.5203249454498291
    },
    {
      "name": "Medical imaging",
      "score": 0.5092719793319702
    },
    {
      "name": "Transformer",
      "score": 0.4660424292087555
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3999471366405487
    },
    {
      "name": "Machine learning",
      "score": 0.36403340101242065
    },
    {
      "name": "Voltage",
      "score": 0.10329580307006836
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}