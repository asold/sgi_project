{
  "title": "One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers",
  "url": "https://openalex.org/W3174510164",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2108092137",
      "name": "Chuhan Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142281011",
      "name": "Fangzhao Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2117581150",
      "name": "Yongfeng Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2470673105",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3110846353",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W3092302658",
    "https://openalex.org/W2743289088",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2747909401",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3035164270",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W3094444847",
    "https://openalex.org/W3045672834",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4230636291",
    "https://openalex.org/W3164886736",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034503922",
    "https://openalex.org/W3084269450"
  ],
  "abstract": "Pre-trained language models (PLMs) achieve great success in NLP.However, their huge model sizes hinder their applications in many practical systems.Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM.However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model.In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs.In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching.In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model.Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4408‚Äì4413\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n4408\nOne Teacher is Enough?\nPre-trained Language Model Distillation from Multiple Teachers\nChuhan Wu‚Ä† Fangzhao Wu‚Ä° Yongfeng Huang‚Ä†\n‚Ä†Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084, China\n‚Ä°Microsoft Research Asia, Beijing 100080, China\n{wuchuhan15, wufangzhao}@gmail.com, yfhuang@tsinghua.edu.cn\nAbstract\nPre-trained language models (PLMs) achieve\ngreat success in NLP. However, their huge\nmodel sizes hinder their applications in many\npractical systems. Knowledge distillation is a\npopular technique to compress PLMs, which\nlearns a small student model from a large\nteacher PLM. However, the knowledge learned\nfrom a single teacher may be limited and even\nbiased, resulting in low-quality student model.\nIn this paper, we propose a multi-teacher\nknowledge distillation framework named MT-\nBERT for pre-trained language model com-\npression, which can train high-quality student\nmodel from multiple teacher PLMs. In MT-\nBERT we design a multi-teacher co-Ô¨Ånetuning\nmethod to jointly Ô¨Ånetune multiple teacher\nPLMs in downstream tasks with shared pool-\ning and prediction layers to align their output\nspace for better collaborative teaching. In ad-\ndition, we propose a multi-teacher hidden loss\nand a multi-teacher distillation loss to transfer\nthe useful knowledge in both hidden states and\nsoft labels from multiple teacher PLMs to the\nstudent model. Experiments on three bench-\nmark datasets validate the effectiveness of MT-\nBERT in compressing PLMs.\n1 Introduction\nPre-trained language models (PLMs) such as BERT\nand RoBERTa have achieved notable success in\nvarious NLP tasks (Devlin et al., 2019; Yang et al.,\n2019; Liu et al., 2019). However, many PLMs have\na huge model size and computational complexity,\nmaking it difÔ¨Åcult to deploy them to low-latency\nand high-concurrence online systems or devices\nwith limited computational resources (Jiao et al.,\n2020; Wu et al., 2021).\nKnowledge distillation is a widely used tech-\nnique for compressing large-scale pre-trained lan-\nguage models (Sun et al., 2019; Wang et al., 2020).\nFor example, Sanh et al. (2019) proposed Distil-\nBERT to compress BERT by transferring knowl-\nedge from the soft labels predicted by the teacher\nmodel to student model with a distillation loss. Jiao\net al. (2020) proposed TinyBERT, which aligns the\nhidden states and the attention heatmaps between\nstudent and teacher models. These methods usu-\nally learn the student model from a single teacher\nmodel (Gou et al., 2020). However, the knowledge\nand supervision provided by a single teacher model\nmay be insufÔ¨Åcient to learn an accurate student\nmodel, and the student model may also inherit the\nbias in the teacher model (Bhardwaj et al., 2020).\nFortunately, many different large PLMs such as\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019) and UniLM (Dong et al., 2019) are off-the-\nshelf. These PLMs may encode complementary\nknowledge because they usually have different con-\nÔ¨Ågurations and are trained on different corpus with\ndifferent self-supervision tasks (Qiu et al., 2020).\nThus, incorporating multiple pre-trained language\nmodels into knowledge distillation has the potential\nto learn better student models.\nIn this paper, we present a multi-teacher knowl-\nedge distillation method named MT-BERT for pre-\ntrained language model compression. 1 In MT-\nBERT, we propose a multi-teacher co-Ô¨Ånetuning\nframework to jointly Ô¨Ånetune multiple teacher mod-\nels with a shared pooling and prediction module to\nalign their output hidden states for better collabo-\nrative student teaching. In addition, we propose a\nmulti-teacher hidden loss and a multi-teacher dis-\ntillation loss to transfer the useful knowledge in\nboth hidden states and soft labels from multiple\nteacher models to student model. Experiments on\nthree benchmark datasets show MT-BERT can ef-\nfectively improve the quality of student models for\nPLM compression and outperform many single-\nteacher knowledge distillation methods.\n1We focus on task-speciÔ¨Åc knowledge distillation.\n4409\n2 MT-BERT\nNext, we introduce the details of our multi-teacher\nknowledge distillation method MT-BERT for pre-\ntrained language model compression.2 We Ô¨Årst in-\ntroduce the multi-teacher co-Ô¨Ånetuning framework\nto jointly Ô¨Ånetune multiple teacher models in down-\nstream tasks, and then introduce the multi-teacher\ndistillation framework to collaboratively teach the\nstudent with multiple teachers.\n2.1 Multi-Teacher Co-Finetuning\nResearchers have found that distilling the knowl-\nedge in the hidden states of a teacher model is\nimportant for effective student teaching (Sun et al.,\n2019; Jiao et al., 2020). However, since different\nteacher PLMs are separately pre-trained with dif-\nferent settings, Ô¨Ånetuning them independently may\nlead to some inconsistency in their feature space,\nwhich is not optimal for transferring knowledge in\nthe hidden states of multiple teachers. Thus, we\ndesign a multi-teacher co-Ô¨Ånetuning framework to\nobtain some uniformity among the hidden states\noutput by the last layer of different teacher models\nfor better collaborative student teaching, as shown\nin Fig. 1. Assume there are N teacher models, and\ndenote the hidden states output by the top layer of\nthe i-th teacher as Hi. We use a shared pooling 3\nlayer to summarize each hidden matrix Hi into\na uniÔ¨Åed text embedding, and then use a shared\ndense layer to convert it into a soft probability vec-\ntor yi. Finally, we jointly optimize the summation\nof the task-speciÔ¨Åc losses of all teacher models,\ni.e., ‚àëN\ni=1 CE(y, yi), where CE(¬∑, ¬∑) stands for the\ncross-entropy loss and y is the ground-truth label.\nSince the pooling and prediction layers are shared\namong different teachers, the feature space of the\noutput hidden states from different teacher PLMs\ncan be aligned, which can help them collaborate\nbetter for student teaching.\n2.2 Multi-Teacher Knowledge Distillation\nNext, we introduce our proposed multi-teacher\nknowledge distillation framework, which is shown\nin Fig. 2. Two loss functions are used for knowl-\nedge distillation, i.e., a multi-teacher hidden loss\nand a multi-teacher distillation loss.\nThe multi-teacher hidden loss aims to transfer\nknowledge in the hidden states of multiple teachers.\n2Codes available at https://github.com/wuch15/MT-BERT\n3In MT-BERT we use attentive pooling because it performs\nbetter than average pooling and ‚Äú[CLS]‚Äù token embedding.\n‚Ä¶\nShared Pooling & Dense\nTask Loss\nùíöùíö3ùíöùíö2ùíöùíö1\nTeacher 1\n Teacher 2\n Teacher 3\n‚Ä¶ ‚Ä¶\nInput Text\nFigure 1: The multi-teacher co-Ô¨Ånetuning framework.\nAssume there are N teacher PLMs, and each of\nthem has T √óK Transformer layers. They collab-\noratively teach a student model with K layers, and\neach layer in the student model corresponds to T\nlayers in teacher PLMs.4 Denote the hidden states\noutput by the j-th layer of the student model asHs\nj,\nand the corresponding hidden states output by the\n(T √ój)-th layer of the i-th teacher model as Hi\nTj .\nFollowing (Sun et al., 2019), we apply the mean\nsquared error (MSE) to the hidden states of corre-\nsponding layers in the student and teacher models\nto encourage the student model to have similar\nfunctions with teacher models. The multi-teacher\nhidden loss LMT ‚àíHid is formulated as follows:\nLMT ‚àíHid =\nN‚àë\ni=1\nT‚àë\nj=1\nMSE(Hs\nj , WijHi\nTj), (1)\nwhere Wij is a learnable transformation matrix.\nThe multi-teacher distillation loss aims to trans-\nfer the knowledge in the soft labels output by multi-\nple teachers to student. The predictions of different\nteachers on the same sample may have different\ncorrectness and conÔ¨Ådence. Thus, it may be sub-\noptimal to simply ensemble (Fukuda et al., 2017;\nLiu et al., 2020) or choose (Yuan et al., 2020) soft\nlabels without the help of task labels. Since in task-\nspeciÔ¨Åc knowledge distillation the labels of train-\ning samples are available, we propose a distillation\nloss weighting method to assign different weights\nto different samples. The weights are based on the\nloss inferred from the predictions of corresponding\nteacher against the gold labels. More speciÔ¨Åcally,\nthe multi-teacher distillation loss LMT ‚àíDis is for-\nmulated as follows:\nLMT ‚àíDis =\nN‚àë\ni=1\nCE(yi/t, ys/t)\n1 + CE(y, yi) , (2)\n4Here we assume that all teacher models have the same\nnumber of layers. We will explore to generalize MT-BERT to\nscenarios where teacher models have different architectures in\nour future work.\n4410\nTransformer\nTransformer\n‚Ä¶\n‚Ä¶\n‚Ä¶\nTransformer\nTransformer\n‚Ä¶\n‚Ä¶\n‚Ä¶\nShared Pooling & Dense\nTransformer\nTransformer\n‚Ä¶\n‚Ä¶\n‚Ä¶‚Ä¶\nTransformer\nTransformer\n‚Ä¶\n‚Ä¶\n‚Ä¶\nTransformer\nTransformer\n‚Ä¶\n‚Ä¶\n‚Ä¶‚Ä¶\nTransformer\nTransformer\n‚Ä¶\n‚Ä¶\n‚Ä¶\nTransformer\n‚Ä¶\n‚Ä¶‚Ä¶\nTransformer\n‚Ä¶\n‚Ä¶\nMulti-teacher distillation loss\nMulti-teacher hidden loss\nTeacher 1 Teacher 2 Teacher 3\nStudent\nùëØùëØ1\nùë†ùë†\nùëØùëØùêæùêæ\nùë†ùë†\nùëØùëØùëáùëá\n3\nùëØùëØùëáùëáùêæùêæ\n3\nùëØùëØùëáùëá\n2\nùëØùëØùëáùëáùêæùêæ\n2\nùëØùëØùëáùëá\n1\nùëØùëØùëáùëáùêæùêæ\n1\nùíöùíö3ùíöùíö2ùíöùíö1 ùíöùíöùë†ùë†\nTask loss\n‚Ä¶\nFigure 2: The multi-teacher knowledge distillation framework in MT-BERT.\nwhere t is the temperature coefÔ¨Åcient. In this way,\nif a teacher‚Äôs prediction on a certain sample is more\nclose to the ground-truth label, its corresponding\ndistillation loss will gain higher weight.\nFollowing (Tang et al., 2019; Lu et al., 2020),\nwe also incorporate gold labels to compute the task-\nspeciÔ¨Åc loss LTask based on the predictions of the\nstudent model, i.e., LTask = CE(y, ys). The Ô¨Ånal\nloss function Lfor learning the student model is a\nsummation of the multi-teacher hidden loss, multi-\nteacher distillation loss and the task-speciÔ¨Åc loss,\nwhich is formulated as follows:\nL= LMT ‚àíHid + LMT ‚àíDis + LTask . (3)\n3 Experiments\n3.1 Datasets and Experimental Settings\nWe conduct experiments on three benchmark\ndatasets with different sizes. The Ô¨Årst one is SST-\n2 (Socher et al., 2013), which is a benchmark for\ntext sentiment classiÔ¨Åcation. The second one is\nRTE (Bentivogli et al., 2009), which is a widely\nused dataset for natural language inference. The\nthird one is the MIND dataset (Wu et al., 2020c),\nwhich is a large-scale public English news dataset.5\nWe perform the news topic classiÔ¨Åcation task on\nthis dataset. The detailed statistics of the three\ndatasets are shown in Table 1.\nIn our experiments, we use the pre-trained 12-\nlayer BERT, RoBERTa and UniLM (Bao et al.,\n2020)6 models as the teachers to distill a 6-layer\n5https://msnews.github.io/\n6We used the UniLMv2 version.\nDataset #Train #Dev #Test #Class\nSST-2 67k 872 1.8k 2\nRTE 2.5k 276 3.0k 2\nMIND 102k 2.6k 26k 18\nTable 1: The statistics of the three datasets.\nand a 4-layer student models respectively. We use\nthe token embeddings and the Ô¨Årst 4 or 6 Trans-\nformer layers of UniLM to initialize the parameters\nof the student model. The pooling layer is imple-\nmented by an attention network (Yang et al., 2016;\nWu et al., 2020a). The temperature coefÔ¨Åcient t is\nset to 1. The attention query dimension in the at-\ntentive pooling layer is 200. The optimizer we use\nis Adam (Bengio and LeCun, 2015). The teacher\nmodel learning rate is 2e-6 while the student model\nlearning rate is 5e-6. The batch size is 64. Follow-\ning (Jiao et al., 2020), we report the accuracy score\non the SST-2 and RTE datasets. In addition, since\nthe news topics in the MIND dataset are highly\nimbalanced, following (Wu et al., 2020b) we report\nboth accuracy and macro-F1 scores. Each exper-\niment is independently repeated 5 times and the\naverage scores are reported.\n3.2 Performance Evaluation\nWe compare the performance of MT-BERT with\ntwo groups of baselines. The Ô¨Årst group includes\nthe 12-layer version of the teacher models, i.e.,\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019) and UniLM (Bao et al., 2020). The sec-\nond group includes the 6-layer and 4-layer student\n4411\nMethods SST-2\n(Acc.)\nRTE\n(Acc.)\nMIND\n(Acc./Macro-F)#Param\nBERT12 92.8 68.6 73.6 51.3 109M\nRoBERTa12 94.8 78.7 73.9 51.5 109M\nUniLM12 95.1 81.3 74.6 51.9 109M\nDistilBERT6 92.5 58.4 72.5 50.4 67.0M\nDistilBERT4 91.4 54.1 72.1 50.2 52.2M\nBERT-PKD6 92.0 65.5 72.7 50.6 67.0M\nBERT-PKD4 89.4 62.3 72.4 50.3 52.2M\nTinyBERT6 93.1 70.0 73.4 50.8 67.0M\nTinyBERT4 92.6 66.6 73.0 50.4 14.5M\nMT-BERT6 94.6 75.7 74.0 51.5 67.0M\nMT-BERT4 93.9 73.8 73.8 51.2 52.2M\nTable 2: Results and parameters of different methods.\nSST RTE91.0\n92.0\n93.0\n94.0\n95.0\n96.0SST Accuracy\n94.60\n94.2094.00\n70.0\n71.5\n73.0\n74.5\n76.0\n77.5\nRTE Accuracy\n75.70\n75.1074.80\nAverage Ensemble\nWeighted Ensemble\nMT-BERT\n(a) SST-2 and RTE datasets.\nAccuracy Macro-F72.0\n72.5\n73.0\n73.5\n74.0\n74.5Accuracy\n74.00\n73.70\n73.50\n49.5\n50.0\n50.5\n51.0\n51.5\n52.0\nMacro-F\n51.50\n51.20\n50.90\nAverage Ensemble\nWeighted Ensemble\nMT-BERT\n(b) MIND dataset.\nFigure 3: Comparison of MT-BERT and ensemble-\nbased multi-teacher distillation methods.\nmodels distilled by DistilBERT (Sanh et al., 2019),\nBERT-PKD (Sun et al., 2019) and TinyBERT (Jiao\net al., 2020), respectively. The results of different\nmethods are summarized in Table 2. 7 Referring\nto this table, we Ô¨Ånd MT-BERT can consistently\noutperform all the single-teacher knowledge distil-\nlation methods compared here. This is because the\nknowledge provided by a single teacher model may\nbe insufÔ¨Åcient, and incorporating the complemen-\ntary knowledge encoded in multiple teacher models\ncan help learn better student model. In addition,\n7We take the original reported results of baseline methods\non the SST-2 and RTE datasets, and we run their codes to\nobtain their results on the MIND dataset.\nTeachers SST-2\n(Acc.)\nRTE\n(Acc.)\nMIND\n(Acc./Macro-F)\nBERT 92.1 65.8 72.8 50.6\nRoBERTa 92.9 68.9 73.0 50.7\nUniLM 93.3 70.6 73.4 50.9\nBERT+RoBERTa 93.6 71.2 73.3 50.9\nBERT+UniLM 93.9 73.7 73.6 51.1\nRoBERTa+UniLM 94.3 74.9 73.7 51.3\nAll 94.6 75.7 74.0 51.5\nTable 3: Different combinations of teacher models.\ncompared with the teacher models, MT-BERT has\nmuch fewer parameters and its performance is com-\nparable or even better than these teacher models.\nIt shows that MT-BERT can effectively inherit the\nknowledge of multiple teacher models even if the\nmodel size is signiÔ¨Åcantly compressed.\nWe also compare MT-BERT with several multi-\nteacher knowledge distillation methods proposed in\nthe computer vision Ô¨Åeld that ensemble the outputs\nof different teachers for student teaching (You et al.,\n2017; Liu et al., 2020). The results are shown in\nFig. 3. We Ô¨Ånd our MT-BERT performs better than\nthese ensemble-based multi-teacher knowledge dis-\ntillation methods. This is because these methods do\nnot consider the correctness of the teacher model\npredictions on a speciÔ¨Åc sample and cannot trans-\nfer useful knowledge encoded in the intermediate\nlayers, which may not be optimal for collaborative\nknowledge distillation from multiple teachers.\n3.3 Effectiveness of Multiple Teachers\nNext, we study the effectiveness of using multiple\nteacher PLMs for knowledge distillation. We com-\npare the performance of the 6-layer student model\ndistilled from different combinations of teacher\nmodels. The results are summarized in Table 3.\nIt shows that using multiple teacher PLMs can\nachieve better performance than using a single one.\nThis is because different teacher models can en-\ncode complementary knowledge and combining\nthem together can provide better supervision for\nstudent model. In addition, combining all three\nteacher PLMs can further improve the performance\nof student model, which validates the effectiveness\nof MT-BERT in distilling knowledge from multiple\nteacher models.\n3.4 Ablation Study\nWe study the effectiveness of the two important\ntechniques in MT-BERT, i.e., the multi-teacher\nco-Ô¨Ånetuning framework and the distillation loss\n4412\nSST RTE91.0\n92.0\n93.0\n94.0\n95.0\n96.0SST Accuracy\n93.20\n94.10\n94.60\n70.0\n71.5\n73.0\n74.5\n76.0\n77.5\nRTE Accuracy\n73.40\n74.90\n75.70\nMT-BERT\nw/o Distillation Loss Weighting\nw/o Multi-Teacher Co-finetuning\n(a) SST-2 and RTE datasets.\nAccuracy Macro-F72.0\n72.5\n73.0\n73.5\n74.0\n74.5Accuracy\n73.20\n73.60\n74.00\n49.5\n50.0\n50.5\n51.0\n51.5\n52.0\nMacro-F\n50.70\n51.10\n51.50\nMT-BERT\nw/o Distillation Loss Weighting\nw/o Multi-Teacher Co-finetuning\n(b) MIND dataset.\nFigure 4: Effectiveness of multi-teacher co-Ô¨Ånetuning\nand distillation loss weighting.\nweighting method. We compare MT-BERT and\nits variants with one of these modules removed, as\nshown in Fig. 4. The student model has 6 layers.\nWe Ô¨Ånd the multi-teacher co-Ô¨Ånetuning framework\nis very important. This is because the hidden states\nof different teacher models can be in very differ-\nent spaces, and jointly Ô¨Ånetuning multiple teachers\nwith shared pooling and prediction layers can align\ntheir output hidden spaces for better collaborative\nstudent teaching. In addition, the distillation loss\nweighting method is also useful. This is because\nthe predictions of different teachers on the same\nsample may have different correctness, and focus-\ning on the more reliable predictions is helpful for\ndistilling accurate student models.\nWe also verify the effectiveness of different loss\nfunctions in MT-BERT, which is shown in Fig. 5.\nWe Ô¨Ånd the task loss is very important. It is because\nin our experiments the corpus for task-speciÔ¨Åc dis-\ntillation are not large and the direct supervision\nfrom task labels is useful. In addition, the dis-\ntillation loss is also important. It indicates that\ntransferring the knowledge in soft labels plays a\ncritical role in knowledge distillation. Moreover,\nthe hidden loss is also helpful. It shows that hid-\nden states of different teacher models can provide\nuseful knowledge for student model learning.\nSST RTE86.0\n88.0\n90.0\n92.0\n94.0\n96.0SST Accuracy\n90.80\n93.40\n94.3094.60\n55.0\n60.0\n65.0\n70.0\n75.0\n80.0\nRTE Accuracy\n67.60\n73.90\n75.4075.70\nMT-BERT\n- Hidden Loss\n- Distillation Loss\n- Task Loss\n(a) SST-2 and RTE datasets.\nAccuracy Macro-F65.0\n67.0\n69.0\n71.0\n73.0\n75.0Accuracy\n70.60\n72.90\n73.7074.00\n42.0\n44.0\n46.0\n48.0\n50.0\n52.0\n54.0\nMacro-F\n47.10\n49.80\n51.2051.50\nMT-BERT\n- Hidden Loss\n- Distillation Loss\n- Task Loss\n(b) MIND dataset.\nFigure 5: Effectiveness of different loss functions.\n4 Conclusion\nIn this paper, we propose a multi-teacher knowl-\nedge distillation method named MT-BERT for pre-\ntrained language model compression, which can\nlearn small but strong student model from multiple\nteacher PLMs in a collaborative way. We propose\na multi-teacher co-Ô¨Ånetuning framework to align\nthe output hidden states of multiple teacher models\nfor better collaborative student teaching. In addi-\ntion, we design a multi-teacher hidden loss and a\nmulti-teacher distillation loss to transfer the useful\nknowledge in both hidden states and prediction of\nmultiple teacher models to student model. The ex-\ntensive experiments on three benchmark datasets\nshow that MT-BERT can effectively improve the\nperformance of pre-trained language model com-\npression, and can outperform many single-teacher\nknowledge distillation methods.\nAcknowledgments\nThis work was supported by the National Natural\nScience Foundation of China under Grant numbers\nU1936208, U1936216, U1836204, and U1705261.\nWe thank Xing Xie, Tao Qi, Ruixuan Liu and Tao\nDi for their great comments and suggestions which\nare important for improving this work.\n4413\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for uniÔ¨Åed language model\npre-training. In ICML, pages 642‚Äì652. PMLR.\nYoshua Bengio and Yann LeCun. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The Ô¨Åfth pascal recognizing tex-\ntual entailment challenge. In TAC.\nRishabh Bhardwaj, Navonil Majumder, and Soujanya\nPoria. 2020. Investigating gender bias in BERT.\narXiv preprint arXiv:2009.05021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171‚Äì4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In NeurIPS, pages 13042‚Äì\n13054.\nTakashi Fukuda, Masayuki Suzuki, Gakuto Kurata,\nSamuel Thomas, Jia Cui, and Bhuvana Ramabhad-\nran. 2017. EfÔ¨Åcient knowledge distillation from an\nensemble of teachers. In Interspeech, pages 3697‚Äì\n3701.\nJianping Gou, Baosheng Yu, Stephen John Maybank,\nand Dacheng Tao. 2020. Knowledge distillation: A\nsurvey. arXiv preprint arXiv:2006.05525.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. Tinybert: Distilling BERT for natural lan-\nguage understanding. In EMNLP Findings, pages\n4163‚Äì4174.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYuang Liu, Wei Zhang, and Jun Wang. 2020. Adap-\ntive multi-teacher multi-level knowledge distillation.\nNeurocomputing, 415:106‚Äì113.\nWenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twin-\nbert: Distilling knowledge to twin-structured com-\npressed bert models for large-scale retrieval. In\nCIKM, pages 2645‚Äì2652.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, pages 1‚Äì26.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP, pages 1631‚Äì1642.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In EMNLP-IJCNLP, pages 4314‚Äì4323.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciÔ¨Åc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In NeurIPS.\nChuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, and\nYongfeng Huang. 2020a. Attentive pooling with\nlearnable norms for text representation. In ACL,\npages 2961‚Äì2970.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2020b. Improving attention mechanism\nwith query-value interaction. arXiv preprint\narXiv:2010.03766.\nChuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng\nHuang, and Qi Liu. 2021. Newsbert: Distilling pre-\ntrained language model for intelligent news applica-\ntion. arXiv preprint arXiv:2102.04887.\nFangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan\nWu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,\nJianfeng Gao, Winnie Wu, et al. 2020c. Mind: A\nlarge-scale dataset for news recommendation. In\nACL, pages 3597‚Äì3606.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS, pages 5753‚Äì\n5763.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiÔ¨Åcation. In\nNAACL-HLT, pages 1480‚Äì1489.\nShan You, Chang Xu, Chao Xu, and Dacheng Tao.\n2017. Learning from multiple teacher networks. In\nKDD, pages 1285‚Äì1294.\nFei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming\nGong, Yan Fu, and Daxin Jiang. 2020. Reinforced\nmulti-teacher selection for knowledge distillation.\narXiv preprint arXiv:2012.06048.",
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.8195368051528931
    },
    {
      "name": "Computer science",
      "score": 0.788020133972168
    },
    {
      "name": "Pooling",
      "score": 0.7485578656196594
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7288389801979065
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5547088384628296
    },
    {
      "name": "Language model",
      "score": 0.5495211482048035
    },
    {
      "name": "Machine learning",
      "score": 0.5216872692108154
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5135017037391663
    },
    {
      "name": "Natural language processing",
      "score": 0.4517211616039276
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}