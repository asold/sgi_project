{
  "title": "Revisiting Robust Neural Machine Translation: A Transformer Case Study",
  "url": "https://openalex.org/W3118509208",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5076656061",
      "name": "Peyman Passban",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5003270434",
      "name": "Puneeth S. M. Saladi",
      "affiliations": [
        "University of Waterloo",
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5100426170",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2971134989",
    "https://openalex.org/W3005724337",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2970926924",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970558573",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2970169061",
    "https://openalex.org/W2964048171",
    "https://openalex.org/W3035812575",
    "https://openalex.org/W2984060780",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2984051011",
    "https://openalex.org/W2963919854",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2964132420",
    "https://openalex.org/W2964247056",
    "https://openalex.org/W2552839021",
    "https://openalex.org/W2963823140",
    "https://openalex.org/W2970798168"
  ],
  "abstract": "Transformers have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behavior of conventional models for the problem of noise but Transformers are relatively understudied in this context. Motivated by this, we introduce a novel data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two other novel extensions to the original Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that modify the neural architecture as well as the training process to handle noise. One important characteristic of our techniques is that they only impact the training phase and do not impose any overhead at inference time. We evaluated our techniques to translate the English–German pair in both directions and observed that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10% of entire test words are infected by noise.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3831–3840\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n3831\nRevisiting Robust Neural Machine Translation:\nA Transformer Case Study\nPeyman Passban1,∗,† Puneeth S.M. Saladi1,2,∗ Qun Liu1\n1Huawei Noah’s Ark Lab\n2Faculty of Mathematics, University of Waterloo\npassban.peyman@gmail.com\n{puneeth.saladi,qun.liu}@huawei.com\nAbstract\nTransformers (Vaswani et al., 2017) have\nbrought a remarkable improvement in the per-\nformance of neural machine translation (NMT)\nsystems but they could be surprisingly vul-\nnerable to noise. In this work, we try to in-\nvestigate how noise breaks Transformers and\nif there exist solutions to deal with such is-\nsues. There is a large body of work in the\nNMT literature on analyzing the behavior of\nconventional models for the problem of noise\nbut Transformers are relatively understudied\nin this context. Motivated by this, we intro-\nduce a novel data-driven technique called Tar-\nget Augmented Fine-tuning (TAFT) to incor-\nporate noise during training. This idea is com-\nparable to the well-known ﬁne-tuning strategy.\nMoreover, we propose two other novel exten-\nsions to the original Transformer: Controlled\nDenoising (CD) and Dual-Channel Decoding\n(DCD), that modify the neural architecture as\nwell as the training process to handle noise.\nOne important characteristic of our techniques\nis that they only impact the training phase and\ndo not impose any overhead at inference time.\nWe evaluated our techniques to translate the\nEnglish–German pair in both directions and\nobserved that our models have a higher toler-\nance to noise. More speciﬁcally, they perform\nwith no deterioration where up to 10% of en-\ntire test words are infected by noise.\n1 Introduction\nNMT is the task of transforming a source sequence\ninto a new form in a particular target language\nusing deep neural networks. Such networks com-\nmonly have an encoder-decoder architecture (Cho\net al., 2014a,b; Sutskever et al., 2014), in which an\nencoder maps a given input sequence to an inter-\nmediate representation and a decoder then uses this\nrepresentation to generate candidate translations.\nBoth encoder and decoder are neural networks that\n∗Equal contribution.\n†Work done while Peyman Passban was at Huawei.\nare trained jointly. Due to the sequential nature of\nthe NMT task, early models usually relied on recur-\nrent architectures (Yang et al., 2020), or beneﬁted\nfrom the sliding feature of convolutional kernels to\nencode/decode variable-length sequences (Kalch-\nbrenner et al., 2014; Gehring et al., 2017).\nRecently, Transformers (Vaswani et al., 2017)\nhave shown promising results for NMT and be-\ncome the new standard in the ﬁeld. They follow\nthe same concept of encoding and decoding but in\na relatively different fashion. A Transformer is fun-\ndamentally a feed-forward model with its unique\nneural components (self-attention, layer norm, etc)\nthat altered the traditional translation pipeline ac-\ncordingly. It is expected if such a new architec-\nture would behave differently than its recurrent or\nconvolutional counterparts, and our goal in this re-\nsearch is to study this aspect in the presence of\nnoise.\nNMT engines trained on clean samples provide\nhigh-quality results when tested on similarly clean\ntexts, but they break easily if noise appears in the\ninput (Michel and Neubig, 2018). They are not\ndesigned to handle noise by default and Transform-\ners are no exception. Many previous works have\nfocused on this issue and studied different archi-\ntectures (Li et al., 2019). However, in this work,\nwe particularly focus on Transformers1 as they are\nrelatively new and to some extent understudied.\nA common approach to make NMT models im-\nmune to noise is ﬁne-tuning (FT), where a noisy\nversion of input tokens is intentionally introduced\nduring training and the decoder is forced to gen-\nerate correct translations despite deformed inputs.\nFT is quite useful for almost all situations but it\nneeds to be run with an optimal setting to be ef-\nfective. In our experiments, we propose a slightly\ndifferent learning-rate scheduler to improveFT. We\nalso deﬁne a new extension that not only modiﬁes\n1We assume that the reader is already familiar with the\nTransformer architecture.\n3832\ninput words but also adds complementary tokens to\nthe target side. We refer to this extension as Target\nAugmented Fine-Tuning (TAFT), which is the ﬁrst\ncontribution of this paper.\nIn our study, we realized that data augmenta-\ntion techniques (FT and TAFT) might not be suf-\nﬁcient enough and we need a compatible training\nprocess and neural architecture to deal with noise.\nTherefore, we propose Controlled Denoising (CD)\nwhereby noise is added to source sequences during\ntraining and the encoder is supposed to ﬁx noisy\nwords before feeding the decoder. This approach\nis implemented via an auxiliary loss function and\nis similar to adversarial training. CD is our second\ncontribution.\nCD only takes care of noise on the encoder side.\nWe also propose a Dual-Channel Decoding (DCD)\nstrategy to study what happens if the decoder is\nalso informed about the input noise. DCD sup-\nports multi-tasking through a 2-channel decoder\nthat samples target tokens and corrects noisy input\nwords simultaneously. This form of fusing transla-\ntion knowledge with noise-related information has\nled to interesting results in our experiments. DCD\nis the third and last contribution of this work.\nThe remainder of the paper is organised as fol-\nlows: We ﬁrst review previously reported solutions\nfor the problem of noise in NMT in Section 2. Then,\nwe present details of our methods and the intuition\nbehind them in Section 3. To validate our methods,\nwe report experimental results in Section 4. Finally,\nwe conclude the paper and discuss possible future\ndirections in Section 5.\n2 Related Work\nFine-tuning (FT) is one of the most straightforward\nand reliable techniques to protect NMT systems\nfrom noise. Berard et al. (2019), Dabre and Sumita\n(2019), and Helcl et al. (2019) studied its impact\nand showed how it needs to be utilized to boost\nNMT quality.\nAdversarial training is another common solution\nto build noise-robust models. Cheng et al. (2019)\nproposed a gradient-based method to construct ad-\nversarial examples for both source and target sam-\nples. Source-side inputs are supposed to attack the\nmodel while adversarial target inputs help defend\nthe translation model. In their model, a candidate\nword is replaced with its semantically-close peer to\nintroduce noise. This way, the neural engine visits\ndifferent forms of the same sample, which extends\nits generalization. In other words, the network is\ntrained to deliver the same, consistent functional-\nity even though it is fed with different forms of\na sample. Although this strategy showed promis-\ning results, in our setting we replace input words\nwith real noisy candidates instead of synonyms or\nsemantically-related peers. We ﬁnd this way of\nadding noise more realistic and closer to real-world\nscenarios.\nKarpukhin et al. (2019) experimented another\nidea by generating adversarial examples using syn-\nthetic noise. Their proposed architecture relies on\nTransformers but the encoder is equipped with a\ncharacter-based convolutional model (Kim et al.,\n2016). This work is one of the few attempts that\nstudied Transformers’ behaviour in the presence\nof noise. However, their results are based on rela-\ntively small datasets. We know that NMT models’\nperformance could change proportionally with a\nchange in the size of training sets. Therefore, we\nused larger datasets in our experiments.\nThe application of adversarial training is not\nlimited to the aforementioned examples. Cheng\net al. (2018) deﬁned additional loss functions which\nforce the encoder and decoder to ignore pertur-\nbations and generate clean outputs. This idea is\nsimilar to our CD approach, but the underlying ar-\nchitecture is different. Cheng et al. (2018) only\nreported results on recurrent NMT models.\nProviding better representations is as important\nas designing tailored training strategies for noise-\nrobust models. A group of researchers focused\non how different segmentation schemes and en-\ncoding techniques can play a role. Sennrich et al.\n(2016) and Michel and Neubig (2018) showed\nthat subwords are better alternatives than surface\nforms (words) to handle perturbations and out-\nof-vocabulary words. Belinkov and Bisk (2018)\ncomprehensively studied this by using different\ncharacter- and subword-based representations in\ndifferent architectures. Sakaguchi et al. (2017) also\ncarried out a similar investigation where they pro-\nposed a new encoding that is invariant to the order\nof characters.\nBesides these approaches, translating noisy in-\nputs can be viewed as a two-pass process performed\nvia two connected neural networks. The ﬁrst one\nacts as a monolingual translator to correct noisy in-\nputs and the second one is an engine that consumes\ndenoised sequences to generate clean translations\n(Sun and Jiang, 2019; Zhou et al., 2019). This\n3833\nidea can be implemented as an end-to-end, differ-\nentiable solution or as a pipeline, but it should be\nnoted that such a mechanism could be hard to de-\nploy or slow(er) to run in practice.\n3 Methodology\nThis section covers details of our proposed meth-\nods. FT is a well-known technique so we skip its\ndetails and only focus on TAFT, our own extension\nof it (Section 3.1). Besides FT and TAFT that lever-\nage data, we introduce CD (Section 3.2) and DCD\n(Section 3.3), which modify the training procedure\nas well as the neural architecture of Transformers.\n3.1 Fine-Tuning Transformers\nFT simply exposes an already-trained translation\nengine to noise during training in the hope of ex-\ntending its coverage at test time. This simple idea is\nquite effective, but it requires to be run with an op-\ntimal setting, e.g. the type/amount of noise added\nto the training set directly impacts performance. It\nis also crucial to ﬁnd an optimal number of itera-\ntions. Overrunning FT could hurt quality and be as\ncostly as training a new model, and running it for\nan insigniﬁcant number of iterations might not be\nenough to reveal its power. Clearly, a better choice\nof these hyper-parameters leads to better results,\nbut in addition to this empirical side of FT, we real-\nized that it can be boosted even more via a simple\nmodiﬁcation.\nFT only alters source sequences. In our exten-\nsion ( TAFT), we change the target side as well\nby appending clean versions of perturbed source\nwords to the target sequence. Table 1 provides an\nexample for this form of data augmentation. An or-\ndinary model works with clean forms of source and\ntarget tokens, as shown in the ﬁrst block. The sec-\nond source word ‘anderen’ is randomly selected\nto be substituted with its noisy version ‘andare. In\nFT, a source sequence including this noisy form (or\nits preprocessed version) is sent to the translation\nengine but the target sequence remains untouched.\nTAFT works with a slightly different data format\nwhere the source sequence includes the noisy in-\nput and at the same time its clean version (namely\n‘anderen’) is appended to the original target se-\nquence. With this simple technique, the NMT\nmodel is forced to generate translations, spot noisy\nsource tokens, and correct them all together. This\ncould be considered as an implicit form of multi-\ntasking without changing the neural architecture.\nOriginal\nalle anderen waren anderer meinung .\nall of the others were of a different opinion .\nFT\nalle and@@ are waren anderer meinung .\nall of the others were of a different opinion .\nTAFT\nalle and@@ are waren anderer meinung .\nall of the others were of a different opinion . anderen\nTable 1: How FT and TAFT process training examples.\nNoise is added to the boldfaced word. This example\nis selected from our German→English corpus. In each\nblock, the ﬁrst sequence is from the source and the sec-\nond sequence is from the target side. Sequences are\npre-processed and tokenized.\nThe fusion of translation and correction knowledge\non the decoder side seems to be useful (see our\nexperimental results in Tables 3 and 4).\nIt should be noted that at test time in TAFT, the\nengine only generates tokens of the target sequence,\ni.e. it stops decoding as soon as it visits the end\nof the target sequence. Generating target tokens\ntogether with clean source words is a training-time\ntechnique to improve the robustness of the model.\nTherefore, this extension does not slow down the\nmodel or change anything about it at inference.\nMoreover, if any segmentation scheme such as\nbyte-pair encoding ( bpe) (Sennrich et al., 2016)\nis applied to input words during preprocessing, the\nnoisy form also needs to be preprocessed accord-\ningly. The same rule applies to the clean form ap-\npended to the target sequence too, namely it needs\nto follow the segmentation scheme of the target\nside. In Table 1, the noisy form ‘ andare’ is seg-\nmented into ‘and@@’ and ‘are’ via the source-side\nbpe model and the correct form ‘anderen’ is ap-\npended as is because the target bpe model was able\nto recognize it as an existing entry.\n3.2 Controlled Denoising\nFT and TAFT have no control over the encoder’s\noutput, and it is assumed that the decoder alone\nis powerful enough to handle representations of\nnoisy inputs and deliver correct translations. This\nassumption might fail in practice, so we place a\nﬁlter after the encoder to purify source representa-\ntions before sending them to the decoder. We refer\nto this process as Controlled Denoising (CD).\nThe idea behind CD is to force the encoder to\ncorrect its noisy representations. To implement this\nmechanism, we accompany the main encoder (the\none that is connected to the decoder) with an auxil-\n3834\niary, pre-trained encoder. These two encoders have\nan identical architecture and work with the same\nvocabulary. The main encoder consumes sequences\nthat may include noisy tokens but the auxiliary one\nis always fed with clean sequences. These two en-\ncoders meet before the ﬁrst layer of the decoder to\nensure they both generate the same representations\nregardless of any discrepancies that may occur in\ntheir input. If there appears a noisy token in the\ninput of the main encoder its output would differ\nfrom that of the auxiliary one. Therefore, we match\nthe output of these two encoders via a loss function\nto ensure the main encoder is able to handle/ignore\nthe input noise.\nIn the absence of noise, the main encoder mim-\nics the auxiliary encoder, but when noise is added\nthe main encoder’s outputs may deviate from ex-\npected ones. The loss function in between the two\nencoders helps the main encoder correct itself and\npush its outputs closer to clean representations (aux-\niliary encoder’s outputs). This architecture is illus-\ntrated in Figure 1.\nFigure 1: The CD architecture. Si\nj is the j-th token of\nthe i-th sequence whose perturbed form is Ni\nj . REPmn\nand REPax show the representations of the input se-\nquences generated by the main and auxiliary encoders,\nrespectively. These two representations are compared\nto each other via a loss function (denoted with /search) to\nensure that the main encoder is able to handle noisy in-\nputs.\nConventional recurrent and convolutional en-\ncoders usually squeeze the representation of the\ninput sequence into a single vector, but Transform-\ners due to their non-recurrent architecture perform\ndifferently and instead generate s vectors if the\ninput sequence consists of stokens. This makes\nthe comparison between outputs of the main and\nauxiliary encoders challenging. Because, when an\ninput sequence is perturbed with noise the length of\nthe noisy sequence could vary from the clean one,\ne.g. one token can be added/dropped or the noisy\ntoken can be decomposed into multiple units via\nbpe (as shown in Table 1). In such cases, the shape\nof encoders’ outputs does not even match and a\nvector-to-vector comparison is impossible. To han-\ndle these issues, we learn a dedicated representa-\ntion for the entire input sequence , so comparing\noutputs would be straightforward. We do this form\nof sequence modelling by following the same idea\nproposed for the CLS token in Devlin et al. (2019).\nWe refer to this sequence-level representation as\nREP in our setting.\nIn Figure 1, the inputs to the main and auxiliary\nencoders are [Si\n1,Ni\n2,Si\n3,Si\n4] and [Si\n1,Si\n2,Si\n3,Si\n4],\nrespectively, and their sequence-level representa-\ntions are REPmn and REPax. If our Transformer\nencoder is fed with stokens it returns s+ 1vectors\nwith the last one being REP. This token is only\nused for comparison purposes between encoders\nand is not sent to the decoder.\nTo train our models with CD, we extend the\noriginal translation loss ( Ltr) with an additional\nterm, LCD , as deﬁned in Equation 1:\nL= αLtr + βLCD\nLCD =\n∑\ni\nMSE(REPi\nmn,REPi\nax) (1)\nwhere MSE() is the mean-square error and REPi\nmn\nand REPi\nax are the sequence-level representations\nof the i-th training sequence generated by our two\nencoders. αand βare weights to adjust the contri-\nbution of each loss function in the training process.\nCheck Section 4 for their values.\nAs previously mentioned, the auxiliary encoder\nis a pre-trained model (trained on the clean/original\nsentences of the same dataset with an identical vo-\ncabulary set and architecture as the main encoder)\nand is only used to generate reference representa-\ntions for the main encoder, so its parameters are\nnot updated during training and only main encoder\nand decoder’s parameters are impacted in the back-\npropagation phase. It should also be noted that the\nauxiliary encoder is used during training and is de-\ncoupled later for inference. Therefore, the size of\nthe ﬁnal model and inference time remain the same\nas in the original Transformer.\n3.3 Dual-Channel Decoding\nThis approach relies on the idea of data augmen-\ntation and multi-tasking, and tries to break down\n3835\nnoise-robust NMT into two tasks of denoising and\ntranslation. Our DCD architecture has two decoder\nchannels: One for generating the rectiﬁed noisy\ntokens (Ddn) detected in the input sequence and\nthe other one (Dtr) for actual translations. The ex-\ntra task deﬁned in addition to translation is meant\nto guide the decoder by providing richer informa-\ntion and make it robust against input noise. This\narchitecture is illustrated in Figure 2.\nFigure 2: The DCD architecture. Ddn generates\n[Si\n1,Si\n3] whcih are the clean version of noisy tokens on\nthe input side and Dtr samples ttokens from the target\nvocabulary to generate the ﬁnal translation [Ti\n1,...,T i\nt ].\nThe decoder has l layers where (l −1) of them are\nshared.\nThe original Transformer decoder has llayers.\nIn our DCD extension, the ﬁrst l−1 layers are\nshared in between two tasks, but the last layer has\ndedicated components for each. Ddn is a unique\ndecoder layer that is responsible to generate clean\nforms of any noisy token that can appear in the\ninput, e.g. in Figure 2 the ﬁrst (Ni\n1) and third (Ni\n3)\ntokens are perturbed so Ddn generates Si\n1 and Si\n3\n(their clean versions) as its output. Ddn is trained\nvia a dedicated loss function ( Ldn) designed for\nthis task. On the other side, Dtr is another decoder\nlayer that shares no parameter withDdn. This layer\nis placed over the decoder’s lexicon and samples\ntarget words to generate the ﬁnal translation. This\nlayer is connected toLtr to penalize incorrect trans-\nlations. In this setting, the ﬁnal loss function is a\ncomposition of two losses, as deﬁned in Equation\n2:\nL= αLtr + βLdn (2)\nThe main purpose of having such a semi-shared\narchitecture for each task is to beneﬁt from the\npower of multi-tasking. Both Dtr and Ddn are trig-\ngered with a mixture of information about transla-\ntion and denoising provided by the ﬁrst l−1 layers\nof the decoder; then they use their dedicated mod-\nules/parameters to generate different outputs for\ntheir particular task. Similar to CD, this technique\nis also employed during training and at inference\nwe do not require Ddn. This should ensure similar\nmemory consumption and inference speed as the\nvanilla Transformer model.\n4 Experimental Study\nDatasets To evaluate our models, we trained\nengines to translate the English–German (En–De)\npair in both directions. In the interest of fair com-\nparisons, we used the same datasets as the orig-\ninal Transformer (Vaswani et al., 2017), so our\ntraining set is the WMT-14 dataset2 with 4.5M par-\nallel sentences and for development and test sets\nwe used newstest-13 and newstest-14, re-\nspectively.\nNowadays, almost all state-of-the-art NMT mod-\nels rely on subwords. We also followed the same\ntradition and preprocessed the target side of our\ndatasets with bpe (Sennrich et al., 2016). For\nthe source side, we used different segmentation\nschemes with different granularities as we are\nstudying the impact of noisy inputs. Our source\ntokens can appear in surface forms (words) or can\nbe segmented into bpe tokens or oven characters.\nWe refer to these settings as word2bpe, bpe2bpe,\nand char2bpe, respectively.\nThe size of the lexicon generated by our bpe\nmodel also matches the setting proposed for the\noriginal Transformer model (Vaswani et al., 2017).\nFor the word2bpe setting, we keep the top 48K\nfrequent words for each English and German sides\nand ignore the rest by substituting with a special\nUNK token. This conﬁguration is learned through\nan empirical study to maximize translation quality.\nHyper-parameters We carried out multi-\nple experiments to study how each of word2bpe,\nbpe2bpe, and char2bpe settings react in conjunc-\ntion with our models and what values should be\nused for hyper-parameters.\nIn these experiments, we did not change the\nneural architecture for the FT and TAFT mod-\nels and only trained translation engines with aug-\nmented datasets. We realized that ﬁne-tuning can\nbe improved if we slightly change the learning-\nrate scheduling. The original Transformer uses\nthe Noam scheduling (Vaswani et al., 2017) that\nemploys a linear warm-up strategy followed by a\ndecaying function. We changed it to a simple expo-\nnential staircase decay with an initial learning rate\nof 0.001 and a decay rate of 0.5 after every 5,000\n2http://statmt.org/wmt14/\ntranslation-task.html\n3836\nsteps.\nAs our translation engines are already trained\nand we only need to ﬁne-tune them, we can ignore\nthe warm-up strategy. We can start from a non-\nzero value and carefully decrease the learning rate\nuntil models converge. We ﬁne-tune all our mod-\nels for 50,000 steps with this scheduler. Figure 3\nillustrates the difference between Noam and our\nscheduler.\nFigure 3: Comparing Noam to our custom learning rate\nscheduler. The y axis represents the learning rate and\nthe x axis shows the number of steps.\nApart from char2bpe that uses the batch size of\n12,288, all other settings process 4,096 tokens in\neach batch. char2bpe is a character-based model\nand the more characters it processes, the better\nperformance it gains. We setαand β(loss weights)\nto [0.75, 0.25] and [ 0.9, 0.1] for CD and DCD,\nrespectively. Our models are trained using Adam\n(Kingma and Ba, 2014). If the value of any other\nhyper-parameter (such as the embedding dimension\netc) is not clearly mentioned in the paper that means\nwe use the original value of it proposed by Vaswani\net al. (2017).\n4.1 Introducing Noise\nTo train/test our engines, we need to perturb source\nsentences by injecting noise. A noisy word can be\ncreated by adding, dropping, or replacing a char-\nacter in a word or by imposing any other defor-\nmations (Cheng et al., 2018; Michel and Neubig,\n2018). However, all these techniques artiﬁcially\nproduce new forms that might not necessarily re-\nﬂect real-world noise. We thus use a particular type\nof noise which is known as natural noise in the lit-\nerature (Belinkov and Bisk, 2018). This form is\nan error that can naturally appear in any text. Re-\nsearchers collected lists3 of frequently-occurring\n3https://github.com/ybisk/\ncharNMT-noise/tree/master/noise\nmistakes/typos in different languages from existing\ncorpora and made them available. In our exper-\niments, we randomly pick a candidate word and\nretrieve its noisy version from the aforementioned\nlists. This way we could ensure that our noisy\ndataset is representative of what we may encounter\nin real life.\nTo create our training sets, we randomly select\n50% of sentences to perturb with noise. We only de-\nstroy one word in each sentence. Noise is added to\nsurface forms, so if the neural encoder is designed\nto work with a different granularity, all necessary\npreprocessing steps are applied accordingly, e.g.\nin Table 1, ﬁrst the candidate word ( anderen) is\nperturbed (andare), then bpe is applied to the noisy\nform to have a consistent input (and@@ are) with\nthe encoder’s vocabulary.\nTo add noise to our test sets we have a slightly\ndifferent and relatively aggressive approach. We\nare interested in challenging our models to see if\nthey can tolerate high volumes of noise, so we\ncreated 4 noisy test sets in which 5%, 10%, 20%,\nand 30% of entire words (not sentences) are de-\nstroyed. Adding noise based on the percentage of\nwords instead of sentences makes translation quite\nchallenging because perturbing for example 10%\nof sentences (with one noisy word) in our 3003-\nsentence test set only generates 300 noisy words\nwhereas this number would be around 7000 if we\nperturb 10% of the entire words. Unlike the train-\ning setting where we only perturb one word in a\nsentence, in the test setting, multiple words can be\nimpacted.\nSince, this is the ﬁrst work (to the best of our\nknowledge) that particularly studies Transformers\nfor their ability to tackled noise we only selected\nto work with natural noise, which seems to be\nthe most realistic form. However, our work can\nbe extended by investigating the impact of other\nfamouse noise classes such as Swap, Mid, Rand\netc. For detailed information on noise classes see\nBelinkov and Bisk (2018), Khayrallah and Koehn\n(2018), and Michel and Neubig (2018).\n4.2 Baseline Models\nAs our baseline, we trained a Transformer with\na slight modiﬁcation in its architecture. Kasai\net al. (2020) conducted research and showed that\nthe number of encoder and decoder layers do not\nnecessarily need to match and we can have imbal-\nanced Transformers with deep(er) encoders and\n3837\n0% 5% 10% 20% 30%\nEn→De\nword2bpe 28.48 22 .21 (-22%) 17.05 (-40%) 10.28 (-64%) 5.99 (-79%)\nbpe2bpe 28.46 24 .82 (-13%) 21.58 (-24%) 15.98 (-44%) 11.89 (-58%)\nchar2bpe 26.07 24 .23 (-7%) 21.84 (-16%) 18.37 (-30%) 15.01 (-42%)\nConvT 25.46 22 .55 (-11%) 20.13 (-21%) 14.9 (-41%) 11.29 (-56%)\nDe→En\nword2bpe 25.94 23 .28 (-10%) 20.32 (-22%) 15.79 (-39%) 12.00 (-54%)\nbpe2bpe 28.04 24 .87 (-11%) 21.61 (-23%) 16.11 (-43%) 11.48 (-59%)\nchar2bpe 26.59 25 .01 (-6%) 22.73 (-15%) 19.42 (-27%) 15.93 (-40%)\nConvT 27.08 24 .01 (-11%) 21.39 (-21%) 16.44 (-39%) 11.59 (-57%)\nTable 2: BLEU scores of baseline models. Numbers inside parentheses show how much noise impacts models’\nperformance. The ﬁrst row shows the percentage of perturbed test words. All our encoders and decoders have 8\nand 4 layers, respectively. The char2bpe models consumes one character at a time. The ConvT model consumes\none word at each step but applies a convolutional operation over all characters of the word before feeding it to the\nﬁrst encoder layer.\nshallow(er) decoders. Inspired by that work, we\nincreased the number of encoder layers 4 from 6\nto 8 and decreased the number of decoder layers\nfrom 6 to 4. Our Transformer still has 12 layers\nin total, but the encoder is more powerful which\nis favourable in our scenario. Noise appears on\nthe source side and we require better encoders to\ntackle this. Based on our experiments, the 8–4 con-\nﬁguration (T8\n4 ) is able to handle noise better than\nthe 6–6 version and all other variants. T8\n4 is our\nbaseline for all experiments and our other novel\narchitectures are also implemented based on the\n8–4 setting.\nBelinkov and Bisk (2018) and Karpukhin et al.\n(2019) used a convolutional, character-based en-\ncoder and showed that this improves the robustness\nof NMT models. They tested this conﬁguration\nwith relatively small datasets or recurrent architec-\ntures. We adapted the same idea and equipped the\nTransformer model with the same convolutional\nmodule. This model, which is referred to as ConvT\nin this paper, is another baseline for our experi-\nments. Similar to Karpukhin et al. (2019), character\nembeddings have 256 dimensions and the convo-\nlutional module follows the speciﬁcations of Kim\net al. (2016). Table 2 summarizes our baseline re-\nsults. We use the BLEU metric (Papineni et al.,\n2002) to compare our models. All scores are cal-\nculated on detokenized outputs using SacreBLEU5\n(Post, 2018).\nAs the table shows, no matter how powerful the\nengine is, adding even 5% noise is enough to break\n4The original Transformer architecture (Vaswani et al.,\n2017) proposes 6 encoder and 6 decoder layers.\n5https://github.com/mjpost/sacrebleu\nthe model. Each segmentation scheme shows a\nunique behaviour. We were expecting a signiﬁcant\ndeterioration in the word2bpe case but for both di-\nrections it provides relatively competitive results.\nbpe2bpe seems to be the best as it gives the highest\nbaseline where no noise is involved and shows less\ndrop for noisy test sets. char2bpe has the least de-\ncline when noise is added but it should be noted that\nit is not able to compete with others in the absence\nof noise. Although its degradation is minimal, it de-\ngrades from a non-optimal baseline.ConvT, despite\nits sophisticated architecture, could not outperform\nbpe2bpe and this was expected as tuning such a net-\nwork over our (relatively) large dataset (wmt-14\nwith 4.5M samples) could be challenging.\n4.3 Results for Proposed Models\nIn this section we report results for word2bpe,\nbpe2bpe, and char2bpe settings when used with\nour solutions (TAFT, CD, and DCD).\nTable 3 summarizes results related to word2bpe.\nWhen translating into German, DCD outperforms\nall other models where up to 10% noise is added\nto the test set. For extreme cases with 20% and\n30% noise, FT is more effective. In the opposite\ndirection, FT and our TAFT extension provide the\nbest performance. TAFT also shows a very promis-\ning result for the 30% test set and even defeats\nthe noise-free setting. The huge gap between the\nvanilla T8\n4 and engines equipped with our tech-\nniques shows the necessity of building noise-robust\nNMT models, specially if they are supposed to be\ndeployed in real-world applications.\nResults for bpe2bpe models are reported in Ta-\nble 4. For the En →De direction, CD is superior\n3838\nModel 0% 5% 10% 20% 30%\nEn→De\nT8\n4 28.48 22.21 17.05 10.28 5.99\nFT 29.21 27.15 25.32 21.93 17.79\nTAFT 29.47 27.33 25.35 21.33 17.29\nCD 29.03 27.02 25.00 20.70 16.75\nDCD 29.48 27.52 25.65 21.68 17.76\nDe→En\nT8\n4 25.94 23.28 20.32 15.79 12.00\nFT 27.16 26.96 26.69 26.16 25.83\nTAFT 27.00 26.87 26.84 26.27 26.08\nCD 27.1 26.83 26.61 26.04 25.57\nDCD 27.06 26.86 26.83 26.13 26.03\nTable 3: word2bpe results. T8\n4 is a Transformer with 8\nencoder and 4 decoder layers. Boldfaced numbers are\nthe best scores of each column.\nfor all test sets, and this indicates that a loss func-\ntion over a bpe-based encoder could remarkably\nincrease robustness. We observe a similar trend\nin the previous experiment for the De→En direc-\ntion. DCD is also quite successful when test sets\nare fairly noisy. It seems bpe-based Transformers\nbeneﬁt a lot from multi-tasking since both TAFT\nand DCD force the decoder to perform a second\ntask in addition to translation.\nModel 0% 5% 10% 20% 30%\nEn→De\nT8\n4 28.46 24.82 21.58 15.98 11.89\nFT 28.8 27.95 27.01 24.6 21.84\nTAFT 28.96 28.03 26.65 24.02 21.16\nCD 29.49 28.51 27.68 25.27 22.63\nDCD 28.91 28.02 26.89 24.37 21.47\nDe→En\nT8\n4 28.04 24.87 21.61 16.11 11.48\nFT 28.46 28.4 28.22 27.83 27.51\nTAFT 28.73 28.53 28.51 27.93 27.63\nCD 28.52 28.42 28.25 27.84 27.50\nDCD 28.65 28.49 28.4 27.98 27.65\nTable 4: bpe2bpe results.\nFinally, we summarize results of char2bpe mod-\nels in Table 5. Trends for this set of experiments\nare relatively consistent with previous ones. For\nthe En→De direction, the best result on average is\ndelivered by CD but DCD also shows comparable\nperformance. For the opposite direction, TAFT and\nModel 0% 5% 10% 20% 30%\nEn→De\nT8\n4 26.07 24.23 21.84 18.37 15.01\nConvT 25.46 22.55 20.13 14.9 11.29\nFT 27.24 26.50 25.92 24.51 23.36\nTAFT 27.11 26.41 25.56 23.90 22.14\nCD 27.29 26.5 26.05 24.71 23.37\nDCD 27.2 26.73 25.88 24.58 23.12\nDe→En\nT8\n4 26.59 25.01 22.73 19.42 15.93\nConvT 27.08 24.01 21.39 16.44 11.59\nFT 27.31 27.14 27.07 26.83 26.49\nTAFT 27.64 27.52 27.32 26.95 26.53\nCD 27.26 27.15 26.89 26.78 26.4\nDCD 27.71 27.52 27.45 27.06 26.78\nTable 5: char2bpe results. ConvT is added as addi-\ntional baseline as encoders rely on characters.\nDCD are better choices and multi-tasking again\nshows its impact. Because char2bpe is a character-\nbased model we also added results from ConvT as\nanother baseline to study if the convolutional op-\neration can mitigate the problem and handle noise\nbetter. Experimental results demonstrate that there\nis no need for such a complex conﬁguration and\nour techniques can train high-quality engines.\n5 Conclusion and Future work\nIn this paper, we studied the problem of noise in\nthe context of NMT and particularly focused on\nTransformers. We proposed three novel techniques\nto augment data and change the training procedure\nas well as the neural architecture. Experimental\nresults show that our techniques can protect NMT\nengines from noise. Our models only affect the\ntraining phase and do not add any overhead in terms\nof space and/or time complexities at inference time.\nThe ﬁndings of our research can be summarized as\nfollows:\n• There is no clear winner among our proposed\nmodels. Each approach has its own strength\nand should be adapted with respect to the prob-\nlem.\n• FT and TAFT are data-driven techniques and\ncan be applied to existing translation models\nwith minimal effort.\n• CD and DCD require some modiﬁcations in\n3839\nthe neural architecture but they are able to\nprovide promising results.\n• Multi-tasking was quite useful in this scenario\nand it seems Transformers beneﬁt a lot when\ntheir decoder is informed about source-side\nnoise.\nIn this research, we ran an extensive number\nof experiments in order to ﬁnd the best conﬁgura-\ntion of each model and optimize hyper-parameters,\nbut there still exist some unexplored topics/areas.\nIn our future work, we are planning to experiment\nwith other language pairs with different morpholog-\nical and grammatical structures.We are also inter-\nested in studying other noise classes. We could only\nafford to work with one class and we selected natu-\nral noise as we ﬁnd it more realistic among others,\nbut this work can be extended to other noise classes.\nFinally, our models are not unique to Transformer\nand NMT. We aim to evaluate them in other lan-\nguage processing/understanding tasks with other\narchitectures.\nAcknowledgements\nWe would like to thank our anonymous reviewers\nas well as Mehdi Rezagholizadeh and Yimeng Wu\nfor their valuable comments and feedback.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In International Conference on Learning Rep-\nresentations.\nAlexandre Berard, Ioan Calapodescu, and Claude\nRoux. 2019. Naver labs Europe’s systems for the\nWMT19 machine translation robustness task. In\nProceedings of the Fourth Conference on Machine\nTranslation. Association for Computational Linguis-\ntics.\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333, Florence, Italy. Associa-\ntion for Computational Linguistics.\nYong Cheng, Zhaopeng Tu, Fandong Meng, Junjie\nZhai, and Yang Liu. 2018. Towards robust neural\nmachine translation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1756–\n1766, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014a. On the proper-\nties of neural machine translation: Encoder–decoder\napproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014b. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nRaj Dabre and Eiichiro Sumita. 2019. NICT’s su-\npervised neural machine translation systems for the\nWMT19 translation robustness task. In Proceedings\nof the Fourth Conference on Machine Translation\n(Volume 2: Shared Task Papers, Day 1), pages 533–\n536, Florence, Italy. Association for Computational\nLinguistics.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL-HLT.\nJonas Gehring, Michael Auli, David Grangier, and\nYann Dauphin. 2017. A convolutional encoder\nmodel for neural machine translation. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 123–135, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nJindˇrich Helcl, Jind ˇrich Libovický, and Martin Popel.\n2019. CUNI system for the WMT19 robustness task.\nIn Proceedings of the Fourth Conference on Ma-\nchine Translation . Association for Computational\nLinguistics.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blun-\nsom. 2014. A convolutional neural network for mod-\nelling sentences. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics. Association for Computational Linguistics.\nVladimir Karpukhin, Omer Levy, Jacob Eisenstein, and\nMarjan Ghazvininejad. 2019. Training on synthetic\nnoise improves robustness to natural noise in ma-\nchine translation. In Proceedings of the 5th Work-\nshop on Noisy User-generated Text (W-NUT 2019) ,\npages 42–47, Hong Kong, China. Association for\nComputational Linguistics.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James\nCross, and Noah A Smith. 2020. Deep encoder,\nshallow decoder: Reevaluating the speed-quality\ntradeoff in machine translation. arXiv preprint\narXiv:2006.10369.\n3840\nHuda Khayrallah and Philipp Koehn. 2018. On the im-\npact of various types of noise on neural machine\ntranslation. In Proceedings of the Second Work-\nshop on Neural Machine Translation and Genera-\ntion, Melbourne. Association for Computational Lin-\nguistics.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In Proceedings of the Thirtieth AAAI\nConference on Artiﬁcial Intelligence, AAAI’16.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nXian Li, Paul Michel, Antonios Anastasopoulos,\nYonatan Belinkov, Nadir Durrani, Orhan Firat,\nPhilipp Koehn, Graham Neubig, Juan Pino, and Has-\nsan Sajjad. 2019. Findings of the ﬁrst shared task\non machine translation robustness. arXiv preprint\narXiv:1906.11943.\nPaul Michel and Graham Neubig. 2018. MTNT: A\ntestbed for machine translation of noisy text. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 543–\n553, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nKeisuke Sakaguchi, Kevin Duh, Matt Post, and Ben\nVan Durme. 2017. Robsut wrod reocginiton via\nsemi-character recurrent neural network. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence (AAAI 2017).\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics.\nYifu Sun and Haoming Jiang. 2019. Contextual text\ndenoising with masked language model. In Proceed-\nings of the 5th Workshop on Noisy User-generated\nText (W-NUT 2019) , pages 286–290, Hong Kong,\nChina. Association for Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nShuoheng Yang, Yuxin Wang, and Xiaowen Chu. 2020.\nA survey of deep learning techniques for neural ma-\nchine translation. arXiv preprint arXiv:2002.07526.\nShuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios\nAnastasopoulos, and Graham Neubig. 2019. Im-\nproving robustness of neural machine translation\nwith multi-task learning. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 565–571, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7918473482131958
    },
    {
      "name": "Computer science",
      "score": 0.7538074851036072
    },
    {
      "name": "Machine translation",
      "score": 0.7498139142990112
    },
    {
      "name": "Inference",
      "score": 0.5589122772216797
    },
    {
      "name": "Decoding methods",
      "score": 0.5120790600776672
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5114492177963257
    },
    {
      "name": "Noise reduction",
      "score": 0.43926432728767395
    },
    {
      "name": "Noise (video)",
      "score": 0.4281435012817383
    },
    {
      "name": "Artificial neural network",
      "score": 0.4158726930618286
    },
    {
      "name": "Machine learning",
      "score": 0.4123387932777405
    },
    {
      "name": "Computer engineering",
      "score": 0.34318652749061584
    },
    {
      "name": "Algorithm",
      "score": 0.21389657258987427
    },
    {
      "name": "Voltage",
      "score": 0.19764479994773865
    },
    {
      "name": "Engineering",
      "score": 0.15859156847000122
    },
    {
      "name": "Electrical engineering",
      "score": 0.08450460433959961
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ]
}