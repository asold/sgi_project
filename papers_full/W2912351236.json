{
    "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    "url": "https://openalex.org/W2912351236",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2259244113",
            "name": "Alessandro Raganato",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1899304389",
            "name": "Jörg Tiedemann",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964204621",
        "https://openalex.org/W2962776659",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2563351168",
        "https://openalex.org/W2140030083",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2885588803",
        "https://openalex.org/W2554915555",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2752194699",
        "https://openalex.org/W2515741950",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2962717763",
        "https://openalex.org/W1951216520",
        "https://openalex.org/W2962982474",
        "https://openalex.org/W2962911926",
        "https://openalex.org/W2952800841",
        "https://openalex.org/W2963088995",
        "https://openalex.org/W22168010",
        "https://openalex.org/W2963899396",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2773956126",
        "https://openalex.org/W2118434577",
        "https://openalex.org/W2098921539",
        "https://openalex.org/W2963123635",
        "https://openalex.org/W2625014264",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W2552839021",
        "https://openalex.org/W2951299559",
        "https://openalex.org/W2963641561",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W2167615167",
        "https://openalex.org/W2760656271",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2292919134",
        "https://openalex.org/W4248358431",
        "https://openalex.org/W2087946919",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W630532510",
        "https://openalex.org/W4299668319",
        "https://openalex.org/W2250606284",
        "https://openalex.org/W2741040846",
        "https://openalex.org/W2740840489",
        "https://openalex.org/W2757154661",
        "https://openalex.org/W2560864221",
        "https://openalex.org/W2563574619",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2962777840",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2739827909",
        "https://openalex.org/W2594470997",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2791751435",
        "https://openalex.org/W2962708992"
    ],
    "abstract": "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.",
    "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n287\nAn Analysis of Encoder Representations in\nTransformer-Based Machine Translation\nAlessandro Raganato and J¨org Tiedemann\nDepartment of Digital Humanities\nUniversity of Helsinki\n{alessandro.raganato,jorg.tiedemann}@helsinki.fi\nAbstract\nThe attention mechanism is a successful tech-\nnique in modern NLP, especially in tasks like\nmachine translation. The recently proposed\nnetwork architecture of the Transformer is\nbased entirely on attention mechanisms and\nachieves new state of the art results in neu-\nral machine translation, outperforming other\nsequence-to-sequence models. However, so\nfar not much is known about the internal prop-\nerties of the model and the representations it\nlearns to achieve that performance. To study\nthis question, we investigate the information\nthat is learned by the attention mechanism\nin Transformer models with different transla-\ntion quality. We assess the representations\nof the encoder by extracting dependency rela-\ntions based on self-attention weights, we per-\nform four probing tasks to study the amount of\nsyntactic and semantic captured information\nand we also test attention in a transfer learn-\ning scenario. Our analysis sheds light on the\nrelative strengths and weaknesses of the vari-\nous encoder representations. We observe that\nspeciﬁc attention heads mark syntactic depen-\ndency relations and we can also conﬁrm that\nlower layers tend to learn more about syntax\nwhile higher layers tend to encode more se-\nmantics.\n1 Introduction\nMachine translation (MT) is one of the promi-\nnent tasks in Natural Language Processing, tack-\nled in several ways (Bojar et al., 2017). Neural\nMT (NMT) has become the de-facto standard with\na performance that clearly outperforms the alter-\nnative approach of Statistical Machine Transla-\ntion (Luong et al., 2015b; Bojar et al., 2016; Ben-\ntivogli et al., 2016). NMT also improves training\nprocedures due to the end-to-end fashion without\ntedious feature engineering and complex setups.\nDuring recent years, a lot of research has been\ndone on NMT, designing new architectures, start-\ning from the plain sequence-to-sequence model\n(Sutskever et al., 2014; Cho et al., 2014), to an im-\nproved version featuring an attention mechanism\n(Bahdanau et al., 2015; Luong et al., 2015a), to\nmodels that only use attention instead of recurrent\nlayers (Vaswani et al., 2017) and models that ap-\nply convolution networks (Gehring et al., 2017a,b).\nAmong the different architectures, the Transformer\n(Vaswani et al., 2017) has emerged as the dominant\nNMT paradigm.1 Relying only on attention mech-\nanisms, the model is fast, highly accurate and has\nbeen proven to outperform the widely used recur-\nrent networks with attention and ensembling (Wu\net al., 2016) by more than 2 BLEU points. Im-\nproved translation quality is typically related to bet-\nter representation of structural information. While\nother approaches make use of external information\nto improve the internal representation of NMT mod-\nels (Arthur et al., 2016; Niehues and Cho, 2017;\nAlkhouli and Ney, 2017), the Transformer seems\nto be able to encode a lot of structural informa-\ntion without explicitly incorporating any structural\nconstraints. However, being a rather new architec-\nture, little is known about what the model exactly\nlearns internally. A better understanding of the in-\nternal representations of neural models has become\na major challenge in NMT (Koehn and Knowles,\n2017).\nIn this work we investigate the kind of linguistic\ninformation that is learned by the encoder. We start\nby training the Transformer system from English\nto seven languages, with different training set sizes,\nresulting in models that are not only trained for\ndifferent target languages but also with expected\ndifferences in translation quality. First, we visu-\nally inspect the attention weights of the encoders,\n1Most submissions for the WMT18 shared task on news\n(http://matrix.statmt.org/) employ the Trans-\nformer architecture.\n288\nin order to ﬁnd linguistic patterns. As the next\nstep, we exploit the attention weights of the net-\nwork to build a graph and induce tree structures for\neach sentence, showing whether syntactic depen-\ndencies between words have been learned or not\nin the spirit of Williams et al. (2018) and Liu and\nLapata (2018). Additionally, following previous\nstudies on how to analyze the internal representa-\ntion of neural systems (Adi et al., 2016; Shi et al.,\n2016; Belinkov et al., 2017a), we probe the encoder\nweights of the trained models to address different\nsequence labeling tasks: Part-of-Speech tagging,\nChunking, Named Entity Recognition and Seman-\ntic tagging. We evaluate the quality of the decoder\non a given task to assess how discriminative the\nencoder representation is for that task. Lastly, in\norder to check whether the learned information can\nbe transferred across models, we use the encoder\nweights of a high-resource language pair to initial-\nize a low-resource language pair, inspired by the\nwork of Zoph et al. (2016). We show that, also\nfor the Transformer, the knowledge of an encoder\nrepresentation can be shared with other models,\nhelping them to achieve better translation quality.\nOverall, our analysis leads to interesting insights\nabout strengths and weaknesses of the attention\nweights of the Transformer, giving more empirical\nevidence about the kind of information the model\nis learning at each layer:\n•We ﬁnd that each layer has at least one atten-\ntion head that encodes a signiﬁcant amount of\nsyntactic dependencies.\n•Consistent with previous ﬁndings on the\nsequence-to-sequence paradigm, probing the\nencoder to four different sequence labeling\ntasks reveals that lower layers tend to encode\nmore syntactic information, whereas upper\nlayers move towards semantic tasks.\n•The information about the length of the input\nsentence starts to vanish after the third layer.\n•The study corroborates that attention can be\nused to transfer knowledge between high- and\nlow-resource languages.\n2 Architecture\nThe architecture of the Transformer system follows\nthe so called encoder-decoder paradigm, trained in\nan end-to-end fashion. Without using any recurrent\nlayer, the model takes advantage of the positional\nFigure 1: The Transformer architecture (illustration\nfrom Vaswani et al. (2017)).\nembedding as a mechanism to encode order within\na sentence. The encoder, typically stacks 6 iden-\ntical layers, in which each of them makes use of\nthe so called multi-head attention and of a 2 sub-\nlayers feed-forward network, coupled with layer\nnormalization and residual connection (see Figure\n1). The multi-head attention mechanism computes\nattention weights, i.e., a softmax distribution, for\neach word within a sentence, including the word\nitself. Speciﬁcally:\nAttention(Q, K, V) =softmax(QKT\n√dk\n)V (1)\nwhere the input consists of queries Q and keys\nK of dimension dk, and values V of dimension\ndv. The queries, keys and values are linearly pro-\njected h times, to allow the model to jointly attend\nto information from different representation, con-\ncatenating the result,\nMultiHead (Q, K, V) =Concat(head1, ..., headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nwith parameter matrices WQ\ni ∈ Rdmodel×dk ,\nWK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv and WO ∈\nRhdv×dmodel. 2\n2As hyper-parameters we used the base version from\nVaswani et al. (2017).\n289\nOn top of the multi-head attention there is a\nfeed-forward network that consists of two layers\nwith a ReLU activation in between. Each encoder\nlayer takes as input the output of the previous layer,\nallowing it to attend to all positions of the previous\nlayer.\nThe decoder has the same architecture as the\nencoder, stacking 6 identical layers of multi-head\nattention with feed-forward networks. However,\nhere there are two multi-head attention sub-layers:\ni) a decoder self-attention and ii) a encoder-decoder\nattention. The decoder self-attention attends on the\nprevious predictions made step by step, masked\nby one position. The second multi-head attention\nperforms an attention between the ﬁnal encoder\nrepresentation and the decoder representation.\nTo summarize, the Transformer model consists\nof three different attentions: i) the encoder self-\nattention, in which each position attends to all po-\nsitions in the previous layer, including the position\nitself, ii) the encoder-decoder attention, in which\neach position of the decoder attends to all posi-\ntions in the last encoder layer, and iii) the decoder\nself-attention, in which each position attends to all\nprevious positions including the current position.\nIn this work, we focus on analyzing the structure\nthat is learned by the ﬁrst type of attention weights\nof the model, i.e., the encoder self-attention, across\ndifferent models with different target language and\ntranslation quality.\n3 Methodology\nWe aim at analyzing the encoder representation of\ndifferent models by assessing their quality through\nseveral experiments: i) by visualizing the attention\nweights (Section 5), ii) by inducing tree structure\nfrom the encoder weights (Section 6), iii) by prob-\ning the encoder as input representation for various\nprediction tasks (Section 7), and iv) by transfer-\nring the knowledge of one encoder to another (Sec-\ntion 8). We start by looking for linguistic patterns\nthrough the visualization of the heat-maps of the en-\ncoder weights. Next, we use the softmax weights\nextracted from the multi-head attention to build\nmaximum spanning trees from the input sentences,\nassessing the quality of the induced tree through\ndependency parsing. Additionally, we evaluate the\nability of the decoder, using a ﬁxed encoder rep-\nresentation as input, on several sequence labeling\ntasks, measuring how important the input features\nare for various tasks. As test bed we use four dif-\n#Training sentences\nEnglish→Czech 51.391.404\nEnglish→German 25.746.259\nEnglish→Estonian 1.064.658\nEnglish→Finnish 2.986.131\nEnglish→Russian 9.140.469\nEnglish→Turkish 205.579\nEnglish→Chinese 23.861.542\nTable 1: Number of training instances used to train\neach system.\nnewstest 2017newstest 2018\nEnglish→Czech 18.11 17.36\nEnglish→German 23.37 34.46\nEnglish→Estonian – 13.05\nEnglish→Finnish 15.06 10.32\nEnglish→Russian 21.30 18.96\nEnglish→Turkish 6.93 6.22\nEnglish→Chinese 23.10 23.75\nTable 2: BLEU score for the newstest2017 and new-\nstest2018 test data.\nferent tasks, ranging from syntax to semantics, i.e,\nPoS tagging, Chunking, Named Entity Recogni-\ntion, and Semantic tagging. The assumption is that\nif a property is well encoded in the input represen-\ntation then it is easy for the decoder to predict that\nproperty. In practice, after training the MT sys-\ntem, we freeze the encoder parameters, and train\none decoder layer for each task. The decoder layer\nis simpler than the original one used for MT; it\nconsists only of one attention head and one feed-\nforward layer with ReLU activation. Moreover,\nin order to output the right amount of labels, the\ndecoder also has to learn implicitly the length of\nthe input sentence. Note that our goal is not to\nbeat the state of the art in a given task but rather\nto analyze the representation of an encoder trained\nfor MT on different tasks referring to different lin-\nguistic properties. Finally, to assess whether the\nknowledge captured within an encoder is general\nenough to also be used for other models, we test\na transfer learning scenario in which we use the\nencoder representation of a high resource language\npair to initialize the encoder of a low resource lan-\nguage pair. Here, we assume that a model is better\nat encoding abstract linguistic properties if it can\nshare useful information to enhance another weaker\nmodel.\n290\n4 Model setup\nWe trained Transformer models 3 from English\nto seven languages, Czech, German, Estonian,\nFinnish, Russian, Turkish and Chinese, using the\nparallel data provided by the WMT18 shared task\non news translation.4 The parallel data come from\ndifferent sources, mainly from Europarl (Koehn,\n2005), News Commentary (Tiedemann, 2012) and\nParaCrawl.5\nThe data sets are partially noisy, especially\nParaCrawl being on its ﬁrst release, and to ﬁl-\nter out potentially incorrect parallel sentences we\nused a language identiﬁer6 to tag each source and\ntarget sentence, discarding the sentences that do\nnot match across languages (Stymne et al., 2013;\nZarin ¸a et al., 2015). As development set we used\nthe provided newsdev data from the shared task,\nwhile using the newstest from WMT 2017 and 2018\nas test data. A widely used technique to allow an\nopen vocabulary is byte pair encoding (Sennrich\net al., 2016), in which the source and target words\nare split into subword units. However, in this work\nwe prefer to use the full word forms, allowing us\nto evaluate and compare the internal representation\non standard sequence labeling benchmarks tagged\nwith gold labels on the full word forms. Therefore,\nwe use a large vocabulary of 100K words per lan-\nguage. General statistics on the training data are\ngiven in Table 1. As can be seen, we ended up\nhaving an heterogeneous amount of data, ranging\nfrom 200K for Turkish up to 51M for Czech. We\ntrained each model for maximum 20 epochs, tak-\ning the best one according to the development set\nas model to evaluate. The BLEU score 7 of each\nmodel is shown in Table 2. Even though the scores\nseem low for the Transformer architecture for the\nMT task, we have to note that each model is trained\nusing full word forms in order to facilitate the anal-\nysis of the encoder representation (our results are\nin line with the comparison between subword units\nand full word forms done by Sennrich et al. (2016)).\n3We used the OpenNMT framework (Klein et al., 2017).\n4The provided data are already preprocessed and\nfreely available athttp://data.statmt.org/wmt18/\ntranslation-task/preprocessed/.\n5https://paracrawl.eu/\n6We used the fasttext language identiﬁer tool (Joulin\net al., 2016b,a) from https://fasttext.cc/docs/\nen/language-identification.html\n7We used the SACRE BLEU script (Post,\n2018), with signature BLEU+case.mixed+lang.en-\n{targetLanguage}+numrefs.1+smooth.exp+test.wmt{17,18}+\ntok.13a+version.1.3.0\nWe do not aim at beating the best system on the test\ndata, as our main point is to analyze different en-\ncoder representations across models with different\ntranslation quality and target language.\n5 Encoder Evaluation: Visualization\nOne of the most straightforward ways of under-\nstanding the weights of a neural network is by vi-\nsualizing them. In its base setting, the Transformer\nemploys 6 layers with 8 different attention heads\nfor each of them, making complete visualization\ndifﬁcult. Therefore, we focus only on attention\nweights with high scores that are visually inter-\npretable.\nWe discovered four different patterns shared\nacross models: paying attention to the word itself,\nto the previous and next word and to the end of\nthe sentence (Figure 2). We found that, usually on\nthe ﬁrst layer, i.e., layer 0, more attention heads\nfocus their weights on the word itself, while on the\nsubsequent layers the network moves the attention\nmore on other words, e.g., on the next and previous\nword, and to the end of the sentence. This suggests\nthat the transformer tries to ﬁnd long dependencies\nbetween words on higher layers whereas it tends to\nfocus on local dependencies in lower layers.\n6 Encoder Evaluation: Inducing Tree\nStructure\nThe architecture of the Transformer, linking each\nword with each other with an attention weight, can\nbe seen as a weighted graph in which the words\nare the nodes and from which tree structure can be\nextracted. Even though the models are not trained\nto produce any trees or to a speciﬁc syntax task, we\nused the attention weights in each layer to extract\na tree of the input sentences and inspect whether\nthey reﬂect a dependency tree.\nWe evaluated the induced trees on the English\nPUD treebank from the CoNLL 2017 Shared Task\n(Zeman et al., 2017). The PUD treebank consists\nof 1000 sentences randomly taken from on-line\nnewswire and Wikipedia. We measure the perfor-\nmance as Unlabeled Attachment Score (UAS) with\nthe ofﬁcial evaluation script8 from the shared task,\nusing gold segmentation and tokenization. Plus,\ngiven that our weights have no knowledge about\nthe root of the sentence, we decided to use the gold\nroot as starting node for the maximum spanning\n8conll17 ud eval.py (version 1.1)\n291\nen→cs en→de en→et en→ﬁ en→ru en→tr en→zh\nLayer 0\nattention head 0 15.06 10.67 8.79 31.63 17.13 10.99 13.00\nattention head 1 9.94 32.90 8.68 12.58 12.02 10.74 15.76\nattention head 2 15.84 10.62 9.60 10.12 12.08 13.69 15.50\nattention head 3 10.62 15.39 31.38 8.31 11.08 9.78 22.79\nattention head 4 17.25 18.12 7.76 25.10 11.75 13.20 10.28\nattention head 5 16.71 14.47 24.24 13.63 12.39 27.55 17.19\nattention head 6 30.26 26.28 11.76 10.43 11.55 9.90 33.26\nattention head 7 15.17 15.31 9.61 9.51 12.13 31.81 9.69\nLayer 1\nattention head 0 10.95 11.73 11.04 11.47 36.05 26.20 20.33\nattention head 1 10.91 10.65 27.58 10.88 12.66 11.23 10.72\nattention head 2 10.72 10.87 25.80 27.32 25.64 14.46 35.77\nattention head 3 12.21 15.06 15.06 20.90 10.45 14.04 9.62\nattention head 4 35.08 13.17 11.14 11.01 18.44 15.83 14.17\nattention head 5 29.04 10.69 10.85 12.51 33.23 27.41 10.84\nattention head 6 15.22 35.94 13.55 35.30 10.27 11.03 11.59\nattention head 7 22.64 35.89 35.07 10.10 13.59 11.82 24.09\nLayer 2\nattention head 0 35.46 12.33 7.40 9.01 35.07 20.53 11.02\nattention head 1 10.29 22.62 32.80 10.98 7.63 10.03 11.55\nattention head 2 19.74 9.02 33.16 9.00 20.92 9.52 29.40\nattention head 3 16.23 15.82 13.04 13.98 22.27 14.05 10.71\nattention head 4 23.23 11.07 12.58 29.43 35.53 10.85 12.98\nattention head 5 16.78 33.76 13.80 14.53 36.08 22.56 35.80\nattention head 6 10.17 22.15 10.23 11.30 12.54 19.38 15.16\nattention head 7 32.01 14.97 13.76 18.36 8.84 11.79 22.12\nLayer 3\nattention head 0 8.28 9.97 11.05 13.89 35.03 18.55 13.80\nattention head 1 35.20 24.76 7.99 13.72 20.64 21.53 13.03\nattention head 2 10.67 10.54 22.62 15.14 9.43 17.03 14.78\nattention head 3 31.13 17.36 12.14 27.24 9.27 15.67 11.20\nattention head 4 23.89 35.59 8.59 12.18 10.36 13.05 14.89\nattention head 5 14.94 10.12 12.37 7.78 12.62 7.18 19.80\nattention head 6 16.02 13.54 13.38 8.70 10.79 8.80 38.87\nattention head 7 13.44 11.81 13.02 14.96 29.10 17.83 9.02\nLayer 4\nattention head 0 14.45 27.88 20.86 11.63 12.84 25.40 13.34\nattention head 1 10.37 14.37 17.80 24.00 10.72 21.11 22.87\nattention head 2 15.06 10.69 11.82 9.52 13.20 11.36 25.25\nattention head 3 13.47 13.47 14.01 10.92 17.11 12.88 12.29\nattention head 4 29.66 17.31 19.45 11.82 10.87 11.76 13.55\nattention head 5 28.07 18.14 32.87 22.50 13.76 11.06 35.40\nattention head 6 13.35 11.27 9.95 15.49 27.68 25.13 11.56\nattention head 7 10.84 25.03 14.93 17.32 13.86 14.00 17.52\nLayer 5\nattention head 0 36.02 29.80 17.37 17.49 35.56 16.91 16.75\nattention head 1 28.02 27.23 16.68 28.25 13.04 28.23 17.71\nattention head 2 20.20 11.14 19.02 33.38 18.49 7.98 13.45\nattention head 3 11.86 8.30 22.45 14.71 19.17 15.76 19.16\nattention head 4 31.71 19.62 33.68 31.87 26.42 13.61 27.50\nattention head 5 13.55 15.20 30.73 17.35 11.98 23.13 26.70\nattention head 6 26.02 35.32 14.83 24.99 9.77 16.99 29.73\nattention head 7 18.63 10.33 15.71 11.01 12.59 25.67 14.79\nTable 3: UAS F1-score of the induced trees produced by the attention weights on the English PUD treebank\nfrom CoNLL 2017.\n292\nFigure 2: Four examples of the discovered patterns through visualization for the sentence: ”there is also an\neconomic motive .”.\nSample tree from the attention head 1, layer 3\n Sample tree from the attention head 4, layer 3\nFigure 3: Sample trees induced by the attention weights from the English-Czech model.\ntree algorithm. Speciﬁcally, we run the Chu-Liu-\nEdmonds algorithm (Chu, 1965; Edmonds, 1967)\nfor each attention head of each layer of the models\nto extract the maximum spanning trees. Table 3\nshows the F1-score of the induced structures. For\ncomparison purposes, in this dataset, a state of the\nart supervised parser (Dozat et al., 2017) reaches\n88.22 UAS F1-score and our random baseline, i.e.,\ninduced trees with random weights and gold root,\nachieves 10.1 UAS F1-score on average.9 Given\nour ﬁndings in Section 5, we also computed a left-\nand right- branching baseline (with golden root),\nobtaining 10.39 and 35.08 UAS F1-score respec-\ntively.\nAlthough our models are not trained to produce\ntrees, the best dependency trees induced on each\nlayer are far better than the random baseline, sug-\ngesting that the models are learning some syntac-\ntic relationships. However, the best scores do not\nachieve results much beyond the right branching\nbaseline, showing that it is difﬁcult to encode more\ncomplex and longer dependencies.\nOverall, for all language pairs we notice the\nsame performance trend across layers. Comparing\n9Even though not comparable in this setting, unsupervised\nsystems developed to build dependency trees achieve on an\nEnglish dataset UAS F1-score ranging from 27.9 to 51.4 when\nusing the output of a PoS tagger system (Alonso et al., 2017).\nour low resource language pair, English-Turkish,\nto the other high resource languages, we can see\nthat the models trained with larger dataset are\nable to induce better syntactic relationships, while\namong high resource languages all models are in\nthe same ballpark, without any speciﬁc correlation\nwith BLEU score, suggesting that it becomes more\ndifﬁcult to induce better dependency relations at a\ncertain point. Figure 3 shows some examples of\ninduced dependency trees. Interestingly enough,\nwe can see that the trees with higher scores fol-\nlow the patterns found in Section 5, in which each\nword is linked to the next one, so encoding most\ncompounds and multi-word expressions. From vi-\nsualizing other trees, even if they do not belong to\nthe best attention head, we can see that they try to\ncapture longer dependencies, as for dress and stuffy\nin the example in Figure 3.\n7 Encoder Evaluation: Probing\nSequence Labeling Tasks\nWe evaluated the encoder representation through\nfour different sequence labeling tasks: Part-of-\nSpeech (PoS) tagging, Chunking, Named Entity\nRecognition (NER) and Semantic tagging (SEM).\nIn this test bed we used the trained weights of the\nencoder, keeping them ﬁxed, training only one de-\n293\nen→cs en→de en→et en→ﬁ en→ru en→tr en→zh\nPOS\nlayer 0 91.13 / 7.70 91.06 / 8.2084.49 / 18.2086.88 / 25.0089.47 / 6.0068.47/ 52.10 90.81 / 12.20\nlayer 1 92.79 /2.90 93.12 / 4.6087.11/ 18.40 87.58/ 12.40 90.67 / 10.6067.53 / 47.0092.60/ 7.90\nlayer 2 93.20/ 5.40 93.18/ 4.50 84.99 /14.70 86.41 / 15.2091.86 /3.90 68.13 /45.40 91.68 / 13.30\nlayer 3 92.24 / 9.50 92.31 / 8.6084.51 / 16.6085.16 / 18.7091.46 / 6.0066.50 / 53.2089.52 / 19.00\nlayer 4 91.66 / 10.8090.85 / 13.7082.65 / 23.7083.46 / 24.4091.98/ 12.00 65.66 / 53.9086.47 / 22.10\nlayer 5 87.14 / 19.1087.83 / 24.1082.11 / 23.6080.41 / 33.3089.47 / 16.3062.80 / 54.8082.95 / 31.30\nCHUNK\nlayer 0 90.28 / 4.37 89.78 / 9.4986.98 / 13.4787.75 /8.90 88.12 / 6.6172.64/ 31.21 90.37 / 5.42\nlayer 1 92.98 /4.32 92.91 / 3.5888.00/ 11.78 88.92/ 10.19 91.16 /4.03 71.59 / 40.8192.76 /6.71\nlayer 2 93.56/ 6.56 93.92/ 3.53 88.00/ 12.28 88.65 / 13.2291.60 / 5.8270.25 / 37.3893.40/ 11.18\nlayer 3 93.46 / 12.3393.92/ 10.14 87.56 / 14.3687.41 / 19.9392.78/ 5.91 69.20 / 46.1790.83 / 16.90\nlayer 4 92.68 / 14.6692.83 / 12.7785.80 / 22.8186.60 / 20.1392.73 / 12.7268.54 / 51.0489.30 / 19.09\nlayer 5 90.87 / 14.4689.92 / 16.6085.34 / 19.8884.04 / 27.1490.95 / 15.1165.01 / 53.3382.82 / 31.71\nNER\nlayer 0 91.18 / 23.7592.71 / 12.0287.21 / 33.0389.38 / 29.5391.29 / 14.5886.49 / 39.4791.72 /11.05\nlayer 1 93.29 / 9.80 93.36 / 7.2788.65 /15.99 90.14 /20.77 92.22 / 10.0785.66 / 38.1492.93 / 11.13\nlayer 2 93.83/ 7.11 94.13 /11.13 87.46 / 37.3090.20 / 26.4793.20/ 8.12 86.52 / 43.0593.72/ 12.35\nlayer 3 93.23 / 16.5394.32/ 14.85 88.95/ 33.31 90.22/ 26.57 93.14 / 9.4286.82 /37.68 93.07 / 18.32\nlayer 4 93.72 / 11.8193.93 / 12.5188.57 / 40.5589.14 / 34.2892.02 / 12.6587.21/ 53.99 91.93 / 26.95\nlayer 5 92.62 / 21.6394.11 / 17.3587.64 / 30.1389.40 / 31.4992.33 / 13.9886.06 / 44.2592.35 / 30.08\nSEM\nlayer 0 83.99 / 13.5684.05 / 13.3581.87 / 14.7381.99 / 14.6983.36 / 14.0779.04 /16.87 84.08 / 13.63\nlayer 1 84.84 / 12.4885.27 / 12.1682.25 /14.11 82.70 / 13.9784.12 / 13.2678.80 / 17.1084.93 / 11.88\nlayer 2 85.17 / 11.9585.11 / 12.1682.28 / 14.2582.76 / 14.8584.09 / 13.0378.26 / 18.0985.40 / 11.74\nlayer 3 85.34 / 12.0284.77 / 11.4582.17 / 14.4182.82 / 14.0085.21/ 12.32 79.22/ 17.28 84.79 / 11.91\nlayer 4 85.29 /11.38 85.91/ 9.93 82.44/ 14.50 83.19/ 13.77 84.26 / 12.5078.36 / 19.2685.38 / 11.42\nlayer 5 86.27/ 11.68 85.71 / 10.7882.27 / 14.5582.96 / 13.8484.56 /11.79 78.67 / 18.7885.98/ 10.62\nTable 4: Results in terms of precision for each test set (↑, on the left side of each cell), together with the\nerror rate on the sentence length (↓, on the right side of each cell).\n#labels #training #testing average\nsentences sentences sent. length\nPoS 17 12543 1000 21.2\nChunk 22 8042 2012 23.5\nNER 9 14987 3684 12.7\nSEM 80 62739 4351 6.4\nTable 5: Statistics of the evaluation benchmarks\nused for the probing task.\ncoder layer using one attention head and one feed-\nforward layer. We then assess the quality of the\nencoder representation across stacked layers.\nEvaluation Benchmarks. We used a standard\nbenchmark for each task: the Universal Depen-\ndencies English Web Treebank v2.0 (Zeman et al.,\n2017) for PoS tagging, the CoNLL2000 Chunking\nshared task (Tjong Kim Sang and Buchholz, 2000),\nthe CoNLL2003 NER shared task (Tjong Kim Sang\nand De Meulder, 2003), and the annotated data\nfrom the Parallel Meaning Bank (PMB) for Seman-\ntic tagging (Abzianidze et al., 2017). Each bench-\nmark provides its own training, development and\ntest data, except chunking in which we use 10%\nof the training corpus as validation, and the PMB\nin which we used the silver portion for training\nand the gold portion for test and dev (following the\n80-20 split).10 Table 5 reports general statistics on\neach benchmark, regarding the granularity of each\ntask, the number of training and testing instances,\nand the average length of the test sentences.\nEvaluation Results. Table 4 reports the perfor-\nmance for each task and stacked layers, together\nwith the error rate for sentence length prediction.\nFor each language pair, we can see that the syntax\ninformation, i.e., the PoS task, is encoded mostly\nin the ﬁrst 3 layers, corroborating the results in Sec-\ntion 6, while moving towards more semantic tasks,\nas NER and SEM we can see that in general the\ndecoder needs more encoder layers to achieve bet-\nter results. Another interesting ﬁnding is provided\nby the length mismatch between the output of the\nmodels and the gold labels. Clearly the models\nencode the information about the sentence length\nin the ﬁrst three layers, and then the information\nstarts to vanish with an increase of the error rate.\nThe only exception is given by the SEM task, but\nas can be seen from the statistics in Table 5, the\n10We used the sem-0.1.0 version.\n294\nnewstest 2017newstest 2018\nEnglish→Turkish 6.93 6.22\nEnglish TL1→Turkish 8.72 7.93\nEnglish TL2→Turkish 7.82 6.91\nTable 6: BLEU score for the newstest2017 and new-\nstest2018 test data for the transfer learning experi-\nment.\naverage sentence length is very short and so it is\neasier to predict. Overall, comparing the perfor-\nmance reached on these probing tasks with the\nBLEU score of each model, we can see again that\nthe high resource language pairs achieve better re-\nsults compared to our low resource language pair.\nMoreover, we notice that in general higher BLEU\nscore correspond to higher probing results, conﬁrm-\ning the trend that encoding linguistic proprieties\nwithin the encoder representation go on par with\nbetter translation quality (Niehues and Cho, 2017;\nKiperwasser and Ballesteros, 2018).\n8 Encoder Evaluation: Transfer learning\nTo assess whether the knowledge encoded in the\nattention units can help other models in a low re-\nsource scenario, we additionally carried out an eval-\nuation of the encoder representation in a transfer\nlearning task. Similar to Zoph et al. (2016), we\nused the encoder weights from one high resource\nlanguage, i.e., English-German, to train a Trans-\nformer system for our low resource language pair,\nEnglish-Turkish. We provide two experiments: i)\ninitializing and ﬁne tuning the encoder weights\n(TL1), ii) initializing and keeping the encoder\nweights ﬁxed ( TL2). Table 6 shows the BLEU\nscores of the systems evaluated with and without\ntransferring the encoder parameters. Both transfer\nlearning settings are helpful to the decoder to reach\na better translation quality, with almost 2 BLEU\npoint more on the best scenario. Starting with a\nbetter encoder representation, taken from a high\nresource language pair, and then ﬁne tuning the pa-\nrameters on the low resource language achieves the\nbest result, matching and corroborating previous\nﬁndings on recurrent networks (Zoph et al., 2016).\n9 Related Work\nThe problem of interpreting and understanding neu-\nral networks is attracting more and more interest\nand work, with so many models and new architec-\ntures being published continuously each year. One\nof the ﬁrst techniques to examine a neural network\ninvolves the analysis of activation patterns of the\nhidden layers (Elman, 1991; Giles et al., 1992).\nNowadays, given its popularity, recurrent neural\nnetworks are the most evaluated networks, mainly\ninvestigated on the structures and linguistic proper-\nties they are encoding (Linzen et al., 2016; Engue-\nhard et al., 2017; Kuncoro et al., 2017; Gulordava\net al., 2018).\nTraditionally, a common way to inspect neural\nnetworks is by visualizing the hidden representa-\ntion trained for a speciﬁc task (Ding et al., 2017;\nStrobelt et al., 2018a,b), and to evaluate them by\nassessing the properties through downstream tasks\n(Chung et al., 2014; Greff et al., 2017).\nOther recent studies look for hidden linguistic\nunits that provide information on how the network\nworks (Karpathy et al., 2015; Qian et al., 2016;\nK´ad´ar et al., 2017), while another line of analysis\nprobes the representation learned by a neural net-\nwork as input to a classiﬁer of another task (Shi\net al., 2016; Adi et al., 2016; Belinkov et al., 2017a;\nTran et al., 2018).\nThe most closely related work is by Belinkov\net al. (2017b), in which they investigate the repre-\nsentation learned by the encoder of a sequence-to-\nsequence NMT system across different languages.\nUnlike them, we studied a neural network with-\nout any recurrent layers, which allows us to in-\nduce a tree representation from the input sentence,\nprobing the encoder representation towards more\ndownstream tasks, and showing that the attention\nweights can also be used to transfer knowledge to\nlow-resource languages.\n10 Conclusion\nIn this paper we investigated the kind of infor-\nmation that is captured by the encoder represen-\ntation of a Transformer model trained for the\ntask of Machine Translation. We analyzed and\ncompared experimentally different models across\nseveral languages, including the visualization of\nweights, building tree structure from each sen-\ntence, probing the representation to four different\nsequence-labeling tasks and by transferring the en-\ncoder knowledge to a low resource language. Un-\nlike most previous studies, where the analysis is\nmade only on RNNs, we examined an architecture\nbased on attention only. Our experimental eval-\nuation sheds lights on interesting ﬁndings about\ndependency relations and syntactic and semantic\n295\nbehavior across layers. In future work, we plan to\nextend the analysis with probing tasks to evaluate\nother linguistic properties (Conneau et al., 2018)\nas well as to a recent evaluation dataset (Sennrich,\n2017), tackling also the attention weights between\nthe encoder and the decoder.\nAcknowledgments\nThe work in this paper is supported by the Academy\nof Finland through project 314062 from the ICT\n2023 call on Computation, Machine Learning and\nArtiﬁcial Intelligence. We would also like to ac-\nknowledge NVIDIA and their GPU grant.\nReferences\nLasha Abzianidze, Johannes Bjerva, Kilian Evang,\nHessel Haagsma, Rik van Noord, Pierre Ludmann,\nDuc-Duy Nguyen, and Johan Bos. 2017. The par-\nallel meaning bank: Towards a multilingual corpus\nof translations annotated with compositional mean-\ning representations. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 242–247, Valencia, Spain. Association\nfor Computational Linguistics.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nTamer Alkhouli and Hermann Ney. 2017. Biasing\nattention-based recurrent neural networks using ex-\nternal alignment information. In Proceedings of the\nSecond Conference on Machine Translation , pages\n108–117.\nH´ector Mart´ınez Alonso, ˇZeljko Agi ´c, Barbara Plank,\nand Anders Søgaard. 2017. Parsing universal depen-\ndencies without training. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 1, Long\nPapers, volume 1, pages 230–240.\nPhilip Arthur, Graham Neubig, and Satoshi Nakamura.\n2016. Incorporating discrete translation lexicons\ninto neural machine translation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1557–1567, Austin,\nTexas. Association for Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In ICLR Workshop.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017a. What do neural ma-\nchine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), volume 1, pages 861–872.\nYonatan Belinkov, Llu ´ıs M `arquez, Hassan Sajjad,\nNadir Durrani, Fahim Dalvi, and James Glass.\n2017b. Evaluating layers of representation in neural\nmachine translation on part-of-speech and semantic\ntagging tasks. In Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , volume 1, pages\n1–10.\nLuisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and\nMarcello Federico. 2016. Neural versus phrase-\nbased machine translation quality: a case study. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pages\n257–267, Austin, Texas. Association for Computa-\ntional Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, et al. 2017. Findings of the 2017 confer-\nence on machine translation (wmt17). In Proceed-\nings of the Second Conference on Machine Transla-\ntion, pages 169–214.\nOndrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, et al. 2016. Findings of\nthe 2016 conference on machine translation. In\nACL 2016 First Conference on Machine Translation\n(WMT16), pages 131–198. The Association for Com-\nputational Linguistics.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nYoeng-Jin Chu. 1965. On the shortest arborescence of\na directed graph. Scientia Sinica, 14:1396–1400.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2126–2136. Association for Computa-\ntional Linguistics.\nYanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Visualizing and understanding neural\nmachine translation. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\n296\nLinguistics (Volume 1: Long Papers) , volume 1,\npages 1150–1159.\nTimothy Dozat, Peng Qi, and Christopher D Manning.\n2017. Stanford’s graph-based neural dependency\nparser at the conll 2017 shared task. Proceedings\nof the CoNLL 2017 Shared Task: Multilingual Pars-\ning from Raw Text to Universal Dependencies, pages\n20–30.\nJack Edmonds. 1967. Optimum branchings. Journal\nof Research of the National Bureau of Standards, B,\n71:233–240.\nJeffrey L Elman. 1991. Distributed representations,\nsimple recurrent networks, and grammatical struc-\nture. Machine learning, 7(2-3):195–225.\n´Emile Enguehard, Yoav Goldberg, and Tal Linzen.\n2017. Exploring the syntactic abilities of rnns with\nmulti-task learning. In Proceedings of the 21st Con-\nference on Computational Natural Language Learn-\ning (CoNLL 2017), pages 3–14.\nJonas Gehring, Michael Auli, David Grangier, and\nYann Dauphin. 2017a. A convolutional encoder\nmodel for neural machine translation. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), volume 1, pages 123–135.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017b. Convolu-\ntional sequence to sequence learning. arXiv preprint\narXiv:1705.03122.\nC Lee Giles, Clifford B Miller, Dong Chen, Guo-Zheng\nSun, Hsing-Hen Chen, and Yee-Chun Lee. 1992.\nExtracting and learning an unknown grammar with\nrecurrent neural networks. In Advances in neural in-\nformation processing systems, pages 317–324.\nKlaus Greff, Rupesh K Srivastava, Jan Koutn´ık, Bas R\nSteunebrink, and J ¨urgen Schmidhuber. 2017. Lstm:\nA search space odyssey. IEEE transactions on neu-\nral networks and learning systems , 28(10):2222–\n2232.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , volume 1, pages 1195–\n1205.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H ´erve J ´egou, and Tomas Mikolov.\n2016a. Fasttext.zip: Compressing text classiﬁcation\nmodels. arXiv preprint arXiv:1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nand Tomas Mikolov. 2016b. Bag of tricks\nfor efﬁcient text classiﬁcation. arXiv preprint\narXiv:1607.01759.\nAkos K ´ad´ar, Grzegorz Chrupała, and Afra Alishahi.\n2017. Representation of linguistic form and func-\ntion in recurrent neural networks. Computational\nLinguistics, 43(4):761–780.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nEliyahu Kiperwasser and Miguel Ballesteros. 2018.\nScheduled multi-task learning: From syntax to trans-\nlation. Transactions of the Association for Computa-\ntional Linguistics, 6:225–240.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander M. Rush. 2017. OpenNMT:\nOpen-source toolkit for neural machine translation.\nIn Proc. ACL.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT summit, vol-\nume 5, pages 79–86.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39.\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng\nKong, Chris Dyer, Graham Neubig, and Noah A.\nSmith. 2017. What do recurrent neural network\ngrammars learn about syntax? In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, pages 1249–1258, Valencia, Spain.\nAssociation for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nYang Liu and Mirella Lapata. 2018. Learning struc-\ntured text representations. Transactions of the Asso-\nciation for Computational Linguistics. To appear.\nThang Luong, Hieu Pham, and Christopher D Manning.\n2015a. Effective approaches to attention-based neu-\nral machine translation. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1412–1421.\nThang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\nand Wojciech Zaremba. 2015b. Addressing the rare\nword problem in neural machine translation. In Pro-\nceedings of the 53rd Annual Meeting of the Associa-\ntion for Computational Linguistics and the 7th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , volume 1, pages\n11–19.\nJan Niehues and Eunah Cho. 2017. Exploiting linguis-\ntic resources for neural machine translation using\nmulti-task learning. In Proceedings of the Second\nConference on Machine Translation, pages 80–89.\n297\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv preprint arXiv:1804.08771.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016.\nAnalyzing linguistic knowledge in sequential model\nof sentence. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 826–835.\nRico Sennrich. 2017. How grammatical is character-\nlevel neural machine translation? assessing mt qual-\nity with contrastive translation pairs. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, volume 2, pages 376–382.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1715–1725.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural mt learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1526–\n1534.\nHendrik Strobelt, Sebastian Gehrmann, Michael\nBehrisch, Adam Perer, Hanspeter Pﬁster, and\nAlexander M Rush. 2018a. Seq2seq-vis: A vi-\nsual debugging tool for sequence-to-sequence mod-\nels. arXiv preprint arXiv:1804.09299.\nHendrik Strobelt, Sebastian Gehrmann, Hanspeter Pﬁs-\nter, and Alexander M Rush. 2018b. Lstmvis: A tool\nfor visual analysis of hidden state dynamics in recur-\nrent neural networks. IEEE transactions on visual-\nization and computer graphics, 24(1):667–676.\nSara Stymne, Christian Hardmeier, J ¨org Tiedemann,\nand Joakim Nivre. 2013. Tunable distortion limits\nand corpus cleaning for smt. In WMT 2013; 8-9 Au-\ngust; Soﬁa, Bulgaria , pages 225–231. Association\nfor Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Proceedings of the Eight Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’12), Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA).\nErik F Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the conll-2000 shared task: Chunk-\ning. In Proceedings of the 2nd workshop on Learn-\ning language in logic and the 4th conference on\nComputational natural language learning-Volume 7,\npages 127–132. Association for Computational Lin-\nguistics.\nErik F Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the seventh conference on Natural language\nlearning at HLT-NAACL 2003-Volume 4, pages 142–\n147. Association for Computational Linguistics.\nKe Tran, Arianna Bisazza, and Christof Monz.\n2018. The importance of being recurrent for\nmodeling hierarchical structure. arXiv preprint\narXiv:1803.03585.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAdina Williams, Andrew Drozdov, and Samuel R Bow-\nman. 2018. Do latent tree learning models iden-\ntify meaningful structure in sentences? Transac-\ntions of the Association of Computational Linguis-\ntics, 6:253–267.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nIeva Zarin ¸a, P¯eteris N ¸ ikiforovs, and Raivis Skadin ¸ˇs.\n2015. Word alignment based parallel corpora eval-\nuation and cleaning using machine learning tech-\nniques. In Proceedings of the 18th Annual Confer-\nence of the European Association for Machine Trans-\nlation.\nDaniel Zeman, Martin Popel, Milan Straka, Jan Ha-\njic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,\nSampo Pyysalo, Slav Petrov, Martin Potthast, et al.\n2017. Conll 2017 shared task: multilingual parsing\nfrom raw text to universal dependencies. Proceed-\nings of the CoNLL 2017 Shared Task: Multilingual\nParsing from Raw Text to Universal Dependencies ,\npages 1–19.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1568–1575."
}