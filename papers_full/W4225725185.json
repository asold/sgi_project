{
    "title": "A Protein-Protein Interaction Extraction Approach Based on Large Pre-trained Language Model and Adversarial Training",
    "url": "https://openalex.org/W4225725185",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5016862420",
            "name": "Zhan Tang",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5076298659",
            "name": "Xuchao Guo",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5101814527",
            "name": "Zhao Bai",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5044284871",
            "name": "Lei Diao",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5091144053",
            "name": "Shuhan Lu",
            "affiliations": [
                "University of Michigan"
            ]
        },
        {
            "id": "https://openalex.org/A5100412866",
            "name": "Lin Li",
            "affiliations": [
                "China Agricultural University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2000982014",
        "https://openalex.org/W3038399516",
        "https://openalex.org/W2884165257",
        "https://openalex.org/W2121844933",
        "https://openalex.org/W2164881856",
        "https://openalex.org/W2766673096",
        "https://openalex.org/W2166111585",
        "https://openalex.org/W2370746078",
        "https://openalex.org/W2560748602",
        "https://openalex.org/W2465246611",
        "https://openalex.org/W2623425365",
        "https://openalex.org/W2556858158",
        "https://openalex.org/W2960293113",
        "https://openalex.org/W2773368817",
        "https://openalex.org/W2886936867",
        "https://openalex.org/W2904726360",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4233069070",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W2798812533",
        "https://openalex.org/W3172588403",
        "https://openalex.org/W6637162671",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W2556096924",
        "https://openalex.org/W2640329709",
        "https://openalex.org/W6659958353",
        "https://openalex.org/W2896303549",
        "https://openalex.org/W3098092829",
        "https://openalex.org/W2789804996",
        "https://openalex.org/W2166474856",
        "https://openalex.org/W1850865022",
        "https://openalex.org/W2097960255",
        "https://openalex.org/W4297809627",
        "https://openalex.org/W2512456704",
        "https://openalex.org/W2963706121",
        "https://openalex.org/W3022285953",
        "https://openalex.org/W2038385691",
        "https://openalex.org/W4293846201",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4293580221",
        "https://openalex.org/W4300996741",
        "https://openalex.org/W2964348125",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2169974160",
        "https://openalex.org/W4297814571",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W4294170691"
    ],
    "abstract": "Protein-protein interaction (PPI) extraction from original text is important for revealing the molecular mechanism of biological processes.With the rapid growth of biomedical literature, manually extracting PPI has become more time-consuming and laborious.Therefore, the automatic PPI extraction from the raw literature through natural language processing technology has attracted the attention of the majority of researchers.We propose a PPI extraction model based on the large pre-trained language model and adversarial training.It enhances the learning of semantic and syntactic features using BioBERT pre-trained weights, which are built on large-scale domain corpora, and adversarial perturbations are applied to the embedding layer to improve the robustness of the model.Experimental results showed that the proposed model achieved the highest F1 scores (83.93% and 90.31%) on two corpora with large sample sizes, namely, AIMed and BioInfer, respectively, compared with the previous method.It also achieved comparable performance on three corpora with small sample sizes, namely, HPRD50, IEPA, and LLL.",
    "full_text": "KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, Mar. 2022                                      771 \nCopyright â“’ 2022 KSII \n \nThis research was supported by  The National Key Research and Development Program of China , the China  \ngovernment [2016YFD0300710, Strategies and Technical Regulations for the Unified Prevention and Control of \nMajor Diseases and Pests in the Main Grain Producing Areas]. \n \nhttp://doi.org/10.3837/tiis.2022.03.002                                                                                                                ISSN : 1976-7277 \nA Protein-Protein Interaction Extraction \nApproach Based on Large Pre-trained \nLanguage Model and Adversarial Training \n \nZhan Tang1, Xuchao Guo1, Zhao Bai1, Lei Diao1, Shuhan Lu2 and Lin Li1* \n1 College of Information and Electrical Engineering, China Agricultural University \nBeijing, 100083 China \n[e-mail: tz_blues@163.com] \n2 School of Information, University of Michigan \nAnn Arbor, MI 48109 USA \n[e-mail: lilincau@126.com] \n*Corresponding author: Lin Li \n \nReceived July 13, 2021; revised October 7, 2021; revised October 28, 2021; revised December 20, 2021; \naccepted February 16, 2022; published March 31, 2022 \n \nAbstract \n \nProtein-protein interaction (PPI) extraction from original text is important for revealing the \nmolecular mechanism of biological processes. With the rapid growth of biomedical literature, \nmanually extracting PPI has become more time- consuming and laborious. Therefore, the \nautomatic PPI extraction from the raw literature through natural language processing \ntechnology has a ttracted the attention of the majority of researchers. We propose a PPI \nextraction model based on the large pre- trained language model and adversarial training. It \nenhances the learning of semantic and syntactic features using BioBERT pre-trained weights, \nwhich are built on large-scale domain corpora, and adversarial perturbations are applied to the \nembedding layer to improve the robustness of the model. Experimental results showed that the \nproposed model achieved the highest F1 scores (83.93% and 90.31%) on two corpora with \nlarge sample sizes, namely, AIMed and BioInfer, respectively, compared with the previous \nmethod. It also achieved comparable performance on three corpora with small sample sizes, \nnamely, HPRD50, IEPA, and LLL. \n \n \nKeywords: adversarial training, information extraction, natural language processing, pre-\ntrained language model, protein-protein interaction \n \n772                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \n1. Introduction \nProtein-protein interaction (PPI) is very important for understanding the molecular \nmechanism of biological processes [1]. The study of PPI has important reference significance \nfor biomedical work, such as the study of drug targets [2]  and analysis of signal proteins [3]. \nAbundant PPI information exists in biomedical literature in an unstructured form. Manual PPI \nextraction is costly and requires long time given the large number of published studies on PPI. \nTherefore, automatic PPI extraction from biomedical literature has become an important \nresearch field, which has attracted the attention of the majority of researchers. \nIn the previous work, extracting PPI based on co-occurrence and pattern matching is very \npopular [4-6]. However, in recent years, machine learning method with better performance has \nbecome the mainstream.  Machine learning method constructs feature set based on feature \nengineering and kernel method,  and then classified by support vector machine  or other \nclassifiers. The commonly used kernel functions are based on synchronous parse trees [7] and \ndependency parse trees [8]. Distributed smoothed tree kernel (DSTK) method proposed by [7] \nhas achieved significant improvement compared with other kernel methods and machine \nlearning methods. Some researchers also began to apply deep learning method s to PPI \nextraction and achieved satisfying performance  given the wide application of deep learning \ntechnology in natural language processing. The first attempt to introduce deep learning into \nthe field of PPI relation extraction, used feature engineering to construct feature sets and deep \nneural network (DNN) for modeling [9]. Although the first attempt to introduce deep learning \ntechnology to the PPI extraction problem use d a deep learning model, it is still essentially a \nfeature engineering-based method, which can be  improved. Methods based on c onvolution \nneural network (CNN) further improve the accuracy of PPI extraction. They usually use word \nembedding as input features [10]  and combine withshortest dependency path (SDP), lexical \nfeatures, and semantic f eatures. [11- 13]. The use of residual connection can increase the \nfeature extraction ability of CNN, with the  word embedding as input to obtain better results \nthan machine learning methods. The experimental results show that deeper structures can \ndirectly learn sufficient features from the text itself  [14]. Recurrent neural network  (RNN)-\nbased methods are also widely used [15-17]. The simple large capacity bidirectional long short \nterm memory (LSTM) model achieves satisfactory results on large sample size datasets [15]. \nIn addition, the t ransformer-based methods emphasize the integrity of global context \nrepresentation, such as Bert [18]  and BioBERT [19]. [20] added lexical features to the \ntransformer structure and proposed a n LBERT model based on lexical patterns to represent \nsentences. The accuracy of the LBERT  model has been greatly improved compared with \nBioBERT. However, its performance is still far behind that of LSTM and CNN-based methods \non large sample size datasets. Its fine-tuning methods from the official simple implementation \nare inapplicable to PPI extraction; thus the advantages of the large pre-trained language model \ncannot be fully expressed. \nIn general, the PPI extraction approach based on deep learning can be divided into three \nparts, namely, feature representation component, feature extraction component, and classifier \ncomponent, as shown in Fig. 1. \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  773 \n \nPositive, Negative\nLevels of PROTEIN1 were slightly increased following  PROTEIN2 treatment\nWord embedding, Lexical features, Semantic features, SDP, ...\nâ‘  Feature Representation\nDNN, CNN, RNN, Transformer, ...\nâ‘¡ Feature Extraction\nSoftmax, Sigmoid, ...\nâ‘¢ Classifier\n \nFig. 1.  Illustration of the PPI extraction approach based on deep learning \n \nThe feature representation component is used to convert text data into numerical data for \nsubsequent parts of the model, which has a crucial impact on the performance of the model. \nCommon feature representati on components include word embedding, lexical features, \nsemantic features, and SDP. The feature extraction component is used to model text, further \nabstract and extract text features, and generate vectors for final classification. Common feature \nextraction components include DNN, CNN, RNN, and transformer. In addition to feature \nextraction, a transformer can be used for feature representation. Relevant studies show that \ntransformer structure is superior to LSTM, CNN , and other structures [21] However, in PPI \nextraction, the existing transformer -based model  has not achieved significantly better \nperformance than LSTM, CNN , and other traditional structures. The classifier is used to \ngenerate the final predicted label. A linear transformation layer and a sigmoid or softmax \nfunction are added to the top of the model to generate conditional probabilities in the category \nspace. For two classification problems, using the sigmoid and the softmax function is roughly \nequivalent. \nMoreover, to increase the generalization of the PPI extraction model, the protein named \nentity in the text is usually replaced with the common entity name, as follows: â€œPROTEIN1,â€ \nâ€œPROTEIN2â€ or â€œPROTEIN,â€ where â€œPROTEIN1â€ and â€œPROTEIN2â€ are the target pairs , \nâ€œPROTEIN2â€ stands for other protein named entities. For example, the sentence â€œthymocyte \nactivation induces the association of phosphatidylinositol 3- kinase and pp120 with CD5.â€  \ncontains three protein named entities ,namely, â€œphosphatidylinositol 3-kinase,â€ â€œpp120,â€ and \nâ€œCD5.,â€ When considering the interaction between â€œphosphatidylinositol 3- kinaseâ€ and \nâ€œpp120,â€ the sentence is rewritten as follows: â€œthymocyte activation induces the association \nof PROTEIN1 and PROTEIN2 with PROTEIN.,â€ When we pay attention to the interaction \nbetween â€œpp120â€ and â€œCD5,â€ it is rewritten as â€œthymocyte activation induces the association \nof PROTEIN and PROTEIN1 with PROTEIN2.â€ It leads to many samples with small \ndifferences in the text data, seriously affecting the robustness of the PPI extraction model. \n \n \n \n774                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \nTo address these problems, based on the BERT  architecture, the adversarial training \nmethod is proposed, and the ADVBERT model and its two variant structures are proposed. In \nthe training process, the token embedding input is perturbed to improve the quality of the \nlearned word embedding and the overfitting problem. The main contributions of this study are \nas follows: \n(1) The limitation of the previous work is that the semantic and syntactic information \ncaptured is insufficient. This work fully uses the semantic and syntactic information of the \nlarge pre-trained language model, combined with the pre-training weights obtained from a lot \nof text in the field. In addition, the BERT architecture based on the small neural network is \nimproved, and a new state-of-the-art method on PPI extraction is achieved.  \n(2) The previous work should be improved in terms of robustness. This work introduces \nadversarial training into the PPI extraction approach and proposes an adversarial training \nmethod for PPI -related texts. The PPI extraction model obtains higher robustness in the \npresence of many small difference samples by applying the adversarial perturbations to the \nweight matrix of token embedding. \n2. Related work \n2.1 Pre-trained language model \nIn the field of natural language processing, a deep network structur e is trained using a large -\nscale unlabeled text corpus to obtain a set of model parameters. This deep network structure \nis usually called the pre-trained model. The pre-trained model can use general information in \nthe large-scale unlabeled text to improve the effectiveness of subsequent specific tasks and \nreduce the requirement for the amount of labeled data; it can also handle some scenes, where \nobtaining a large amount of labeled data is difficult [22]. \nMost early pre-trained models used static techniques, such as Word2Vec [23] and GloVe \n[24]. These methods can learn shallow representations of text, which can improve the  \ndownstream tasks. However, static pre -training technology cannot solve the problem of \npolysemous words that  often appear in natural language. In response to this problem, \nresearchers began to apply dynamic pre -training technology to pre-trained language models. \nELMo [25] uses two-layer two-way LSTM with residuals to train the language model, captures \ncontextual information, and solves the problem of ambiguity. [26] proposed a multistage \nmigration method and fine-tuning the pre-training model skills to provide important guidance \nfor the subsequent development of pre -training technology. Transformer [21] usually h as \nbetter results than LSTM, and has achieved effective results on tasks such as machine \ntranslation; it has been applied to pre- trained language models. The g enerative pre-training \n(GPT) model [27] stacks 12 transformer substructures, which have sufficient characterization \ncapabilities and significantly improve the effect of downstream tasks. The GPT model is still \nessentially a one-way language model, and its ability to model semantic information is limited. \nTo solve this problem, the BERT model [18] achieved the effect of a two-way language model \nby randomly covering part of the words in the input text sequence during pre -training. In \naddition, [28] proposed a model based on federated learning. The emergence of the BERT \nmodel has greatly promoted the development of the field of natural language processing and \nreached a new state- of-the-art in several natural language processing tasks. However,  the \nBERT pre-trained weights trained on the general corpus cannot achieve satisfactory results in \nsome cases due to the more complex textual sentence patterns in the biomedical field related \nto PPI and more professional terms . BioBERT [19] studied how to apply the pre -training \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  775 \n \nlanguage model BERT to the field of biomedicine. Through the almost identical architecture \non the task, after pre-training on the biomedical corpus, BioBERT is used in many biomedical \ntext mining tasks. It is much better than BERT and the previous SoTA models. However, [20] \napplied BioBERTâ€™s official code to PPI relation extraction and failed to achieve good results; \nthe potential of BioBERâ€™s pre-training weights should be further developed. \n2.2 Adversarial training methods \nAdversarial training was first applied in the field of computer vision, and adversarial samples \ngenerated based on adversarial disturbances were added to the training process to solve the \nsecurity problem of deep learning models. In recent years, with the development of deep \nlearning technology in the field of natural language processing, adversarial training based on \nfighting perturbations has also been used in the field of natural language processing. \n[29] found that after adding small perturbations that are invisible to the naked eye, the deep \nlearning model provides false predictions with high confidence. Thus, the adversarial sample \nattracts the attention of the majority of researchers. [30] believed that this impact is caused by \nthe linear behavior of deep neural networks in high dimensions given that small perturbations \non the input may greatly affect the deep learning model. In addition, a n adversarial objective \nfunction based on fast gradient sign method (FGSM), which enables adversarial training to be \ncarried out at the same time as model training , was proposed. Experiments have shown that \nthis regulation method is effective and can improve the robustness of deep learning models to \nsmall perturbations. [31] applied adversarial training to text p roblems, based on FGSM, \nremoved the sign function, used the L 2 norm to scale the gradient, and proposed the fast \ngradient method (FGM). Different from image data, text data are mostly in discrete form, and \ndirectly adding adversarial perturbations to the raw data is impossible. Therefore, FGM adds \nsmall perturbations to the embedding layer. Although the embedding layer with disturbance \ncannot correspond to the actual  text input, this method can still effectively improve the \nperformance of the model. [32] analyzed the adversarial training and summarized it as a \nminimumâ€“maximization problem: the disturbance that maximizes the loss of the original \nmodel is found,as well as the most robust parameters of the model when the data error rate is \nthe largest. \nTo the best of our knowledge, us e of  adversarial training to improve the effect of PPI \nextraction has not been explored. Many samples with small differences are  formed because \nthe protein named entity is replaced with the general entity name in the PPI extraction research, \nthereby affecting the robustness of the PPI model. Adversarial training can improve this \nlimitation to a certain extent. \n3. Method \nThe basic architecture of the adversarial bidirectional encoder representations from \ntransformers (ADVBERT)  model includes input embeddings, BERT a rchitecture, and \nclassifier, as shown in Fig. 2. \n \n776                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \n \np1 p2 p3 p4 p5 p6\ns0 s0 s0 s0 s0 s0\nt1 t2 t3 t4 t5 t6\np0\ns0\nt0\n+ + + + + + +\n+ + + + + + +\nToken\nEmbeddings\nPosition\nEmbeddings\nSegment\nEmbeddings\nBERT Architecture\nMulti-Head Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nÃ—  N\nh1 h2 h3 h4 h5 h6h0\nClassifier\nPerturbed \nEmbeddings\n \nFig. 2. Basic architecture of the ADVBERT model \n3.1 Input embeddings \n3.1.1 Tokenization \nThis work uses wordpiece [33] for word segmentation to reduce the size of the vocabulary. \nThe definition of the word is separated from the tense voice by breaking the word into smaller \nunits. This method can obtain streamlined and richer vocabulary. After the word segmentation, \na special token [CLS] for classification is added to the beginning of the sequence, and a special \ntoken [SEP] for identifying the end of a sentence is added to the end to satisfy the requirements \nof using pre-trained weights. For example, the word sequence of â€œLevels of PROTEIN1 were \nslightly increased following PROTEIN2 treatmentâ€ after tokenization is as follows: â€œ[CLS],â€ \nâ€œLevel,â€ â€œ##s,â€ â€œof,â€ â€œPR,â€ â€œ##OT,â€ â€œ##EI,â€ â€œ##N,â€ â€œ##1,â€ â€œwere,â€ â€œslightly,â€ \nâ€œincreased,â€ â€œfollowing,â€ â€œPR,â€ â€œ##OT,â€ â€œ##EI,â€ â€œ##N,â€ â€œ##2,â€ â€œtreatment,â€and â€œ[SEP].â€ \nThe word sequence is converted into the corresponding integer id sequence as the input of the \ntoken embeddings.  \n3.1.2 Input embeddings \nSegment embeddings are used to represent the sentence. For PPI extraction, only one segment \nis required; it is uniformly defined as  ğ’”ğ’”0. Position embeddings are used to encode position \ninformation. This work uses sine and cosine functions to construct position embeddings. The \nlength of the input sequence is ğ‘›ğ‘›, ğ‘–ğ‘– âˆˆ [0, ğ‘›ğ‘›) represent the i-th position in the sequence, and the \nembedding layer dimension is ğ‘‘ğ‘‘ğ‘’ğ‘’.Then, ğ’”ğ’”0 âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’, ğ’•ğ’•ğ‘–ğ‘– âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’, and the position embeddings can \nbe computed as follows: \nğ’‘ğ’‘ğ‘–ğ‘–\n(2ğ‘˜ğ‘˜) = sinï¿½ğ‘–ğ‘– 100002ğ‘˜ğ‘˜ ğ‘‘ğ‘‘ğ‘’ğ‘’â„â„ ï¿½     (1) \nğ’‘ğ’‘ğ‘–ğ‘–\n(2ğ‘˜ğ‘˜+1) = cosï¿½ğ‘–ğ‘– 100002ğ‘˜ğ‘˜ ğ‘‘ğ‘‘ğ‘’ğ‘’â„â„ ï¿½    (2) \nwhere ğ‘˜ğ‘˜ = ğ‘‘ğ‘‘ğ‘’ğ‘’/2, ğ’‘ğ’‘ğ‘–ğ‘–\n(2ğ‘˜ğ‘˜) represents the value of ğ’‘ğ’‘ğ‘–ğ‘– in the 2ğ‘˜ğ‘˜ dimension. \nToken embeddings is used to represent word embedding vectors. Adversarial perturbations \nare applied into token embeddings to increase the robustness and effectively  regulate the \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  777 \n \nmodel. The FGM method directly adds perturbations to the embeddings [31], but this method \nrequires disassembly and reconstruction of the model ; it is diffcult to implement. Perturbing \nthe embedding parameter matrix can also play a regularizing role  because the output of \nembeddings is calculated from the embedding parameter matrix. Adversarial perturbations are \nadded to the parameter matrix of token embeddings. Perturbation values are calculated by the \ngradient of the parameter matrix of token embeddings, and t he L2 norm of the parameter \nmatrix and an adjustable adversarial coefficient are used to scale the perturbation values. The \nparameter matrix of token embeddings is set to be ğ‘¾ğ‘¾ğ‘‡ğ‘‡, as follows: \nğ‘¹ğ‘¹ğ‘ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ = [ğ’“ğ’“0, ğ’“ğ’“1, â‹¯ğ’“ğ’“ğ‘›ğ‘›âˆ’1] = ğ›¼ğ›¼\nâˆ‡ğ‘¾ğ‘¾ğ‘‡ğ‘‡ğ¿ğ¿(ğœ½ğœ½,ğ’™ğ’™,ğ‘¦ğ‘¦)\nï¿½âˆ‡ğ‘¾ğ‘¾ğ‘‡ğ‘‡ğ¿ğ¿(ğœ½ğœ½,ğ’™ğ’™,ğ‘¦ğ‘¦)ï¿½2\n   (3) \nwhere ğ›¼ğ›¼ is the adversarial coefficient, ğ¿ğ¿ is the loss function, ğœ½ğœ½ is the model parameter, and ğ’™ğ’™, ğ‘¦ğ‘¦ \nare the input and output of the model, respectively. During training, ğ‘¾ğ‘¾ğ‘‡ğ‘‡ = ğ‘¾ğ‘¾ğ‘‡ğ‘‡ + ğ‘¹ğ‘¹ğ‘ğ‘ğ‘‘ğ‘‘ğ‘ğ‘, and \nthe perturbations are removed after parameter update, ğ‘¾ğ‘¾ğ‘‡ğ‘‡ = ğ‘¾ğ‘¾ğ‘‡ğ‘‡ âˆ’ğ‘¹ğ‘¹ğ‘ğ‘ğ‘‘ğ‘‘ğ‘ğ‘ . This type of \nperturbation is different from random noise. The randomly generated noise unnecessarily \nincreases the gradient, but the perturbations generated in this approach always increase the \ngradient. Thus, the regularization effect is stronger. \nThe output of the entire input embeddings ğ‘¬ğ‘¬ is the sum of the three embeddings obtained \nas follows: \nğ’†ğ’†ğ‘–ğ‘– = ğ’”ğ’”0 + ğ’•ğ’•ğ‘–ğ‘– + ğ’‘ğ’‘ğ‘–ğ‘–     (4) \nğ‘¬ğ‘¬ = [ğ’†ğ’†0, ğ’†ğ’†1, â‹¯ğ’†ğ’†ğ‘›ğ‘›âˆ’1] âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’    (5) \n3.2 BERT Architecture \n3.2.1 Transformer blocks \nAttention mechanism has not only been widely used in the field of computer vision [34-37], \nbut also in the field of natural language processing. Attention-based BERT architecture is \ncomposed of N transformer blocks. Transformer blocks mainly include two pa rts, namely, \nMulti-Head attention and Feed Forward. The attention calculation method is Scaled Dot -\nProduct Attention [18], as follows: \nğ´ğ´ğ‘‡ğ‘‡ğ‘‡ğ‘‡(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) = ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï¿½\nğ‘¸ğ‘¸ğ‘²ğ‘²ğ‘‡ğ‘‡\nâˆšğ‘‘ğ‘‘ğ‘˜ğ‘˜\nï¿½ğ‘½ğ‘½   (6) \nwhere ğ‘‘ğ‘‘ğ‘˜ğ‘˜ is the dimension of ğ‘²ğ‘². \nMulti-Head attention allows the model to focus on  the letters of different representation \nsubspaces from different positions. The calculation method of a single attention head adopts \nself-attention calculation. ğ‘¸ğ‘¸, ğ‘²ğ‘², and ğ‘½ğ‘½ are obtained by a linear function of the output of input \nembeddings ğ‘¬ğ‘¬, and Multi-Head attention is obtained by concatenating the attention output of \neach head, as follows: \nâ„ğ‘’ğ‘’ğ‘ ğ‘ ğ‘‘ğ‘‘ğ‘–ğ‘– = ğ´ğ´ğ‘‡ğ‘‡ğ‘‡ğ‘‡ï¿½ğ‘¬ğ‘¬ğ‘¾ğ‘¾ğ‘–ğ‘–\nğ‘„ğ‘„, ğ‘¬ğ‘¬ğ‘¾ğ‘¾ğ‘–ğ‘–\nğ¾ğ¾, ğ‘¬ğ‘¬ğ‘¾ğ‘¾ğ‘–ğ‘–\nğ‘‰ğ‘‰ï¿½, ğ‘–ğ‘– âˆˆ [1, ğ‘›ğ‘›ğ»ğ»]   (7) \nğ‘´ğ‘´ = ğ‘ğ‘ğ‘ ğ‘ ğ‘›ğ‘›ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘ (â„ğ‘’ğ‘’ğ‘ ğ‘ ğ‘‘ğ‘‘1, â‹¯, â„ğ‘’ğ‘’ğ‘ ğ‘ ğ‘‘ğ‘‘ğ»ğ»)ğ‘¾ğ‘¾ğ‘€ğ‘€ âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’   (8) \nwhere ğ‘›ğ‘›ğ»ğ» is the number of attention heads , ğ‘¾ğ‘¾ğ‘–ğ‘–\nğ‘„ğ‘„, ğ‘¾ğ‘¾ğ‘–ğ‘–\nğ¾ğ¾, ğ‘¾ğ‘¾ğ‘–ğ‘–\nğ‘‰ğ‘‰ âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’Ã—ğ‘‘ğ‘‘ğ‘¤ğ‘¤, ğ‘‘ğ‘‘ğ‘¤ğ‘¤ = ğ‘‘ğ‘‘ğ‘’ğ‘’/ğ‘›ğ‘›ğ»ğ», ğ‘¾ğ‘¾ğ‘€ğ‘€ âˆˆ\nğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’. When the output of the Multi-Head attention is connected with the input residual ly, \nthe layer normalization [33] is applied, as follows: \nğ‘¬ğ‘¬ = ğ¿ğ¿ğ‘ ğ‘ ğ‘¦ğ‘¦ğ‘’ğ‘’ğ¿ğ¿ğ¿ğ¿ğ‘ ğ‘ ğ¿ğ¿ğ‘ ğ‘ (ğ‘¬ğ‘¬+ ğ‘´ğ‘´) âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’   (9) \nwhere ğ¿ğ¿ğ‘ ğ‘ ğ‘¦ğ‘¦ğ‘’ğ‘’ğ¿ğ¿ğ¿ğ¿ğ‘ ğ‘ ğ¿ğ¿ğ‘ ğ‘  is layer normalization. \nThe Feed Forward operation consists of two linear transformations, with a GELU activation \nfunction in between; it also performs residual connection and layer normalization operations, \nas follows: \n778                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \nğ‘­ğ‘­ = ğºğºğºğºğ¿ğ¿ğºğº(ğ‘¬ğ‘¬ğ‘¾ğ‘¾1 + ğ’ƒğ’ƒ1)ğ‘¾ğ‘¾2 + ğ’ƒğ’ƒ2 âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’  (10) \nğ‘¬ğ‘¬ = ğ¿ğ¿ğ‘ ğ‘ ğ‘¦ğ‘¦ğ‘’ğ‘’ğ¿ğ¿ğ¿ğ¿ğ‘ ğ‘ ğ¿ğ¿ğ‘ ğ‘ (ğ‘¬ğ‘¬+ ğ‘­ğ‘­) âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’   (11) \nwhere ğ‘¾ğ‘¾1 âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’Ã—ğ‘‘ğ‘‘1, ğ‘¾ğ‘¾2 âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘1Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’, and ğ‘‘ğ‘‘1 is the intermediate size. \nThis operation is repeated  N times to obtain the final hidden layer vector s, ğ‘¯ğ‘¯ =\n[ğ’‰ğ’‰0, ğ’‰ğ’‰1, â‹¯ğ’‰ğ’‰ğ‘›ğ‘›âˆ’1] âˆˆ ğ‘…ğ‘…ğ‘›ğ‘›Ã—ğ‘‘ğ‘‘ğ‘’ğ‘’, and the ğ’‰ğ’‰0 corresponding to the classification identifier [CLS] is \ninput to the classifier for classification. \n3.2.2 Pre-trained weights \nThe BERT architecture uses a bidirectional transformer for encoding and a masked language \nmodel during pre -training, a small numb er of words are replaced with MASK or another \nrandom word with a small probability during training, thereby enhancing the context memory. \nThe PPI-related text is very closely related to the context, and fully integrat ing the context is \nnecessary to obtain sufficient semantic feature representation. Two open-source pre-trained \nweights are used for comparative research to use the information in the large-scale unlabeled \ncorpus. One is the BERT-Base cased released by Google Research (https://github.com/google-\nresearch/bert), and the other is BioBERT -Base v1.1 (+PubMed 1M) released by DMIS LAB \n(https://github.com/dmis-lab/biobert). The capacity, structure, configuration, and vocabulary \nof the two pre-trained weights are the same, as shown in Table 1. \n \nTable 1. Parameter description of BERT architecture \nParameter Description Value \nğ‘‘ğ‘‘ğ‘’ğ‘’ dimension of embeddings and hidden layer 768 \nN number of transformer blocks 12 \nğ‘›ğ‘›ğ»ğ» number of attention heads 12 \nğ‘‘ğ‘‘1 intermediate size 3072 \n \n3.2.3 Fine-tuning \nThe self -attention mechanism in the transformer allows BERT a rchitecture to fine -tune \nspecific downstream tasks by exchanging appropriate inputs and outputs. The knowledge \nobtained during pre -training on large -scale unlabeled corpus can be transferred to specific \ntasks during fine -tuning. PPI extraction relies on domain knowledge and a large number of \nsemantic and syntactic features. Fine-tuning on large-scale pre-training language models can \ngreatly improve the efficiency of PPI extraction. We consider the sentence after replacing the \nnamed entity as input, and whether PPI exists between the two proteins of interest as the binary \nclassification output. \n3.3 LSTM-based variant \nIn the original architecture of BERT, only the hidden layer vector corresponding to [CLS] is \nused, losing part of the text information. We design a variant structure based on LSTM, named \nADVBERT-LSTM, as shown in Fig. 3  to full y use of information in addition to  [CLS], \nconsidering the effectiveness of bidirectional LSTM in PPI extraction [15]. \n \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  779 \n \nh1h0 h2 h3 h4 h5 h6\nClassifier\n \nFig. 3. LSTM based variant structure \n \nThe hidden layer output vectors  except ğ’‰ğ’‰0 are processed by bidirectional LSTM and \nconcatenated with ğ’‰ğ’‰0 to generate the final vector for classification, as follows: \nğ’‰ğ’‰ğ‘™ğ‘™ï¿½ï¿½ï¿½âƒ— = ğ¿ğ¿ğ¿ğ¿ğ‘‡ğ‘‡ğ¿ğ¿(ğ’‰ğ’‰1 â†’ ğ’‰ğ’‰ğ‘›ğ‘›âˆ’1) âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘™ğ‘™    (12) \nğ’‰ğ’‰ğ‘™ğ‘™âƒ–ï¿½ï¿½ï¿½ = ğ¿ğ¿ğ¿ğ¿ğ‘‡ğ‘‡ğ¿ğ¿(ğ’‰ğ’‰ğ‘›ğ‘›âˆ’1 â†’ ğ’‰ğ’‰1) âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘™ğ‘™    (13) \nğ’‰ğ’‰0 = ğ‘ğ‘ğ‘ ğ‘ ğ‘›ğ‘›ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘ ğ‘’ğ‘’ğ‘›ğ‘›ğ‘ ğ‘ ğ‘ ğ‘ ğ‘’ğ‘’ï¿½ğ’‰ğ’‰0, ğ’‰ğ’‰ğ‘™ğ‘™ï¿½ï¿½ï¿½âƒ—, ğ’‰ğ’‰ğ‘™ğ‘™âƒ–ï¿½ï¿½ï¿½ï¿½ âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’+2Ã—ğ‘‘ğ‘‘ğ‘™ğ‘™   (14) \nwhere ğ‘‘ğ‘‘ğ‘™ğ‘™ is the dimension of the LSTM hidden units. \nThe LSTM structure  can model the text sequence satisfactorily and capture the long -\ndistance dependence in the text. The gating mechanism used in LSTM can effectively improve \nthe vanishing gradient problem. The combination of LSTM and BERT can furthe r improve \nthe performance of PPI extraction. \n3.4 CNN-based variant \nThe hidden layer output vector s obtained by the BERT architecture ha ve a higher level of \nabstraction and contain sufficient position information. Therefore, we  design another variant \nstructure based on CNN  to effectively capture other information besides [CLS], named \nADVBERT-CNN, as shown in Fig. 4. \n \nh1h0\nClassifier\nc0 c1 c2 c3\nh2 h3 h4 h5 h6\ncM\nMax \nPooling\n \nFig. 4.  CNN based variant structure \n \nA filter with a window size of 3 is used to perform convolution operations on the hidden \nlayer output vectors except ğ’‰ğ’‰0, as follows: \nğ‘ªğ‘ª = ğ¶ğ¶ğ¿ğ¿ğ¿ğ¿(ğ’‰ğ’‰1:3, ğ’‰ğ’‰2:4, â‹¯, ğ’‰ğ’‰ğ‘›ğ‘›âˆ’3:ğ‘›ğ‘›âˆ’1, ) âˆˆ ğ‘…ğ‘…(ğ‘›ğ‘›âˆ’3)Ã—ğ‘‘ğ‘‘ğ‘ğ‘  (15) \nwhere ğ‘‘ğ‘‘ğ‘ğ‘ is the dimension of the CNN hidden units. Then the features are further extracted \nthrough global max pooling and then concatenated with ğ’‰ğ’‰0 to generate the final vector for \nclassification, as follows: \nğ‘ªğ‘ªğ‘´ğ‘´ = ğºğºğºğºğ‘ ğ‘ ğºğºğ‘ ğ‘ ğºğºğ¿ğ¿ğ‘ ğ‘ ğ‘ ğ‘ ğºğºğ‘ ğ‘ ğ‘ ğ‘ ğºğºğ‘–ğ‘–ğ‘›ğ‘›ğºğº(ğ‘ªğ‘ª) âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘ğ‘   (16) \nğ’‰ğ’‰0 = ğ‘ğ‘ğ‘ ğ‘ ğ‘›ğ‘›ğ‘ğ‘ğ‘ ğ‘ ğ‘ ğ‘ (ğ’‰ğ’‰0, ğ‘ªğ‘ªğ‘´ğ‘´) âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’+ğ‘‘ğ‘‘ğ‘ğ‘   (17) \nCNN structure can identify indicative substructures in the input data and capture the local \nfeatures that contribute the most to the prediction task. BERT architecture generates features \nthat contain rich semantic and word meaning information through several transformer blocks. \nThe CNN structure further combines these features through convolution and pooling \noperations to better represent the entire text sequence. \n \n780                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \n3.5 Classifier \nA linear transformation layer and a softmax function are used to ğ’‰ğ’‰0, which is the final form \nfor classification, to generate conditional probabilities in the category space, as follows: \nğ©ğ© = ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ’‰ğ’‰0ğ‘¾ğ‘¾ğ‘ğ‘ + ğ’ƒğ’ƒğ‘ğ‘) âˆˆ ğ‘…ğ‘…2   (18) \nwhere ğ‘¾ğ‘¾ğ‘ğ‘ âˆˆ ğ‘…ğ‘…ğ‘‘ğ‘‘ğ‘’ğ‘’Ã—2. Then, the predicted label ğ‘¦ğ‘¦ï¿½ is outputted through the argmax function: \nğ‘¦ğ‘¦ï¿½ = ğ‘ ğ‘ ğ¿ğ¿ğºğºğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ’‘ğ’‘)    (19) \nThe loss function uses cross-entropy: \nğºğºğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  = âˆ’ğ‘¦ğ‘¦ğºğºğ‘›ğ‘›ğ‘ğ‘1 âˆ’(1 âˆ’ğ‘¦ğ‘¦) ğºğºğ‘›ğ‘›(1 âˆ’ğ‘ğ‘0)   (20) \nThe optimization goal of the model is to minimize the loss value of all training samples. \n4. Results \n4.1 Datasets \nVarious institutions provide a series of standard PPI extraction corpora to facilitate the \nmajority of researchers to study the problem of PPI relation extraction. The more widely used \napproach in the existing research are AIMed  [38], BioInfer [39], HPRD50 [40], IEPA [41], \nand LLL [42]. Each corpus contains multiple sentences f rom the biomedical literature, and \neach sentence contains one or more protein pairs. The protein named entity has also been \nannotated. Each protein pair is a sample ;whether each protein pair has a mutual relationship , \nthe interaction relationship is annotated by experts in the relevant field. The protein pairs that \ninteract with each other are positive samples, and the nonexistent protein pairs are negative \nsamples. \nSome annotation differences in the AIMed and BioInfer corpora  are processed following \nthe principle of effective length. After the annotation is completed, samples that do not contain \nPROTEIN1 and PROTEIN2 at the same time  are deleted. After processing, the sta tistics of \neach corpus are shown in Table 2. \n \nTable 2. The statistics of PPI extraction corpora \nCorpus Number of samples Positive samples Negative samples Ratio \nAIMed 5669 995 4674 0.21 \nBioInfer 10090 2425 7665 0.32 \nHPRD50 433 163 270 0.60 \nIEPA 817 335 482 0.70 \nLLL 330 164 166 0.99 \n \n4.2 Evaluation metrics \nWe use standard evaluation indicators, including precision (P), recall (R), and F1 score (F1) \nas metrics to evaluate the model performance. The calculation method of each metric is as \nfollows: \nğºğº =\nğ‘‡ğ‘‡ğ‘‡ğ‘‡\nğ‘‡ğ‘‡ğ‘‡ğ‘‡+ğ¹ğ¹ğ‘‡ğ‘‡ Ã— 100%     (21) \nğ‘…ğ‘… =\nğ‘‡ğ‘‡ğ‘‡ğ‘‡\nğ‘‡ğ‘‡ğ‘‡ğ‘‡+ğ¹ğ¹ğ¹ğ¹ Ã— 100%    (22) \nğ¹ğ¹1 =\n2ğ‘‡ğ‘‡ğ‘ƒğ‘ƒ\nğ‘‡ğ‘‡+ğ‘ƒğ‘ƒ Ã— 100%    (23) \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  781 \n \nwhere TP is the number of samples that are positive and classified as positive, FN is the \nnumber of samples that are positive and classified as negative, FP is the number  of samples \nthat are negative and classified as positive, and TN is the number of samples that are negative \nand classified as negative. \n4.3 Experimental settings \nSince these corpora do not officially divide the training and test sets, in this work , 10-fold \ncross-validation is used ; it is the most reliable evaluation method. Some hyperparameter \nsettings are shown in Table 3. \n \nTable 3. Hyperparameter settings \nHyperparameter Description Value \nepochs epochs of training 10 \nbatch size  batch size of training 8 \nlr Learning rate 2e-5 \nn maximum length of the input sequence 128 \nğ‘‘ğ‘‘ğ‘™ğ‘™ dimension of LSTM hidden units 256 \nğ‘‘ğ‘‘ğ‘ğ‘ dimension of CNN hidden units 128 \n \nWe trained our model on an NVIDIA GeForce GTX 1660 GPU with the Adam optimizers \nwith ğ›½ğ›½1 = 0.9 , ğ›½ğ›½1 = 0.98 , and ğœ€ğœ€ = 10âˆ’9 . We used open- source libraries TensorFlow \n(https://www.tensorflow.org/), bert4keras (https://github.com/bojone/bert4keras), and Keras \n(https://keras.io/) to implement our model under the Windows10 environment. The model is \ndirectly used for prediction after 10 epochs of training in the cross-validation of each fold. \n4.4 Experimental results \nThe proposed model is compared with state-of-the-art approaches in the traditional machine \nlearning methods DSTK, and other deep learning methods including DNN [9], MCCNN [10], \nsdpCNN [11], McDepCNN [12], LSTM [15], tLSTM [16], DEEPCNN [14], LBERT [20]. \nThe compared methods are briefly introduced as follows. The comparison results are shown \nin Table 4. The results of the ADVBERT-CNN and ADVBERT-LSTM models are obtained \nusing the BioBERT  pre-trained weights and the a dversarial coefficient ğ›¼ğ›¼ =0.25, and t he \nstandard deviation of the 10-fold cross-validation is in parentheses. \n \nTable 4. Comparison results (%) \nModel AIMed BioInfer HPRD50 IEPA LLL \nP R F1 P R F1 P R F1 P R F1 P R F1 \nDSTK 68.91 73.24 71.01 75.70 76.90 76.29 76.25 84.15 80.0 75.85 85.15 80.23 87.31 91.18 89.20 \nDNN 51.51 63.38 56.12 53.89 72.9 61.63 66.95 83.98 74.23 58.72 92.37 71.28 75.84 91.81 82.00 \nMCCNN 76.41 69.00 72.45 81.30 78.10 79.62 - - - - - - - - - \nsdpCNN 64.8 67.8 66.0 73.4 77.0 75.2 - - - - - - - - - \nMcDepCNN 67.3 60.1 63.5 62.7 68.2 65.3 - - - - - - - - - \nLSTM 78.8 75.2 76.9 87.0 87.4 87.2 - - - - - - - - - \ntLSTM 81.4 81.9 81.6 88.9 89.3 89.1 81.7 82.3 81.3 78.6 78.7 78.5 84.8 84.3 84.2 \nDEEPCNN 79.0 76.8 77.6 87.4 86.5 86.9 74.9 82.8 77.7 71.6 80.6 75.5 80.5 87.2 83.2 \n782                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \nLBERT 73.6 74.5 74.0 73.0 72.5 72.8 85.8 85.2 85.5 81.4 86.2 83.7 83.8 88.4 86.0 \nADVBERT-\nLSTM \n84.74 \n(4.12) \n83.39 \n(2.49) \n83.93 \n(1.19) \n89.94 \n(2.43) \n90.08 \n(2.43) \n89.97 \n(1.25) \n83.66 \n(8.99) \n87.17 \n(8.81) \n84.78 \n(5.61) \n85.34 \n(6.37) \n84.15 \n(5.75) \n84.48 \n(3.77) \n89.62 \n(9.26) \n88.31 \n(8.61) \n88.65 \n(7.45) \nADVBERT-\nCNN \n84.34 \n(3.89) \n81.40 \n(3.53) \n82.72 \n(1.93) \n91.37 \n(1.65) \n89.28 \n(2.30) \n90.31 \n(1.83) \n83.40 \n(6.44) \n85.40 \n(8.46) \n83.94 \n(4.12) \n84.13 \n(4.83) \n85.95 \n(7.02) \n84.88 \n(4.80) \n87.98 \n(8.81) \n89.60 \n(6.22) \n88.61 \n(6.56) \n \nThe proposed model has achieved the highest F1 scores on the three corpora of AIMed, \nBioInfer, and IEPA. T he comparison between ADVBERT -LSTM and the previous state- of-\nthe-art method indicates that, compared with tLSTM, the F1 score of the AIMed increased by \n2.3 percentage points , 0.8 percentage points  for the BioInfer corpus .The F1 score of the \nHPRD50 was 0.8 percentage points lower than that of LBERT, and that of the IEPA increased \nby 0.7 percentage points . The F1 score of the LLL  was 0.55 percentage points lower than \nDSTK. DSTK and LBERT performed poorly on the two large sample size corpora of AIMed \nand BioInfer . However , the proposed model has achieved significant improvement  on the \nAIMed and BioInfer corpora compared with the two models. For the three small sample size \ncorpora of HPRD50, IEPA, and LLL, the proposed model has also achieved a significant \nimprovement over tLSTM. The proposed model can achieve better performance on the  large \nsample size and small sample size corpora. \nThe use of large pre -trained language models can greatly improve the performance of \ndownstream tasks, given that the lexical and syntactic features required by the downstream \ntasks are learned from a large amount of corpus, especially the corpus in the field, in the pre-\ntraining process . Fine-tuning with the target corpus can complete the PPI extraction task \neffectively, and can improve the problem of the deep learning model being restricted by the \nsample size to a certain extent, because the features that cannot be fully learned on the small \nsample size data may have been effectively represented in the pre-training process. \n5. Discussion \n5.1 Comparisons between different methods \nThe advantage of the deep learning models over the traditional machine learning  models is \nthat manually designing feature engineering is unnecessary. The model itself can mine the \npotential features of the data, as much as possible, but it requires a sufficient amount of data \nto be effective. Therefore, on the two small -sample datasets of IEPA and LLL, the DSTK \nmethod using kernel function and machine learning model has certain advantages over the two \ndeep learning methods of tLSTM and DEEPCNN. However, for AIMed and BioInfer datasets \nwith large sample sizes, the advantages of deep le arning methods are highlighted. Deep \nlearning methods, such as LSTM, tLSTM, and DEEPCNN, have achieved significant \nimprovements compared with DSTK methods. The LBERT method that also uses a large-scale \npre-training language model is based on deep learning technology. However, it has learned \nsufficient lexical and syntactic features from a large amount of unlabeled corpus during the \npre-training process. Thus, it also obtained considerable performance on small sample size \ndatasets. \nThe proposed ADVBERTâ€“CNN and ADVBERTâ€“LSTM models use adversarial training to \nfurther exert the advantages of large- scale pre- training language models and enhance the \nrobustness of the models. Through the combination with CNN and LSTM structure, the ability \nto mine the potential fea tures of text data is improved, and the accuracy of text feature \nmodeling is enhanced. Therefore, the proposed model has achieved good performance on large \nsample size and small sample size data. In particular, the highest F1 scores were achieved on \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  783 \n \nthe three corpora, namely, AIMed, BioInfer, and IEPA. On the two other datasets, the F1 \nscores obtained are also very close to the best performance methods. \nIn summary, the proposed method further improves the performance of PPI extraction, and \nat the same time introduces large-scale pre-training language models and adversarial training \ntechniques into the field of PPI extraction. \n5.2 Performance with different pre-trained weights \nExperiments were performed using the general BERT (BERT-Base cased) pre-trained weights \nand the domain-related BioBERT (BioBERT-Base cased v1.1) pre-trained weights to explore \nthe impact of different pre- trained weights. The experimental results of the orig inal \nADVBERT model are shown in Fig. 5. \n \n(a) (b)  \nFig. 5. F1-score (%) of the original ADVBERT model using BERT and BioBERT pre-training  \nweights with adversarial training (a) and without adversarial training (b) \n \nThe experimental results of the LSTM based variant ADVBERT-LSTM model are shown \nin Fig. 6. \n \n(a) (b)  \nFig. 6. F1-score (%) of ADVBERT-LSTM model using BERT and BioBERT pre-training weights \nwith adversarial training (a) and without adversarial training (b) \n \n \n784                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \nThe experimental results of the CNN based variant ADVBERT-CNN model are shown in \nFig. 7. \n \n(a) (b)  \nFig. 7. F1 score (%) of the ADVBERT-CNN model using BERT and BioBERT pre-training weights \nwith adversarial training (a) and without adversarial training (b) \n \nRegardless of the variant structure used  and whether adversarial training is applied , the \nresults obtained by BioBERT pre-trained weights are better than those obtained by BERT pre-\ntrained weights. This finding shows that pre -training with corpus in the domain can help \nimprove the effect of downstream tasks. On the large  sample size corpora of AIMed and \nBioInfer, compared with the BERT pre-trained weight, using the BioBERT pre-trained weight \ncan usually achieve a 1 to 2 percentage point increase in F1 score. On the HPRD50, IEPA, and \nLLL with smaller sample sizes, the gap is even more evident. The sample size limits the effect \nof the deep learning model given that the large-capacity model cannot be fully trained with a \nsmall amount of data. However, the BioBERT pre-trained weights are constructed on abundant \ntext in the field . Thus, it contains more domain-related semantic and syntactic information, \nwhich can be well transformed to downstream tasks. \n5.3 Performance using different variants \nExperiments were carried out using the original ADVBERT model and the two variants based \non CNN and LSTM (using BioBERT pre-trained weights). The results are shown in Fig. 8. \n(a) (b)  \nFig. 8. F1-score (%) of the original ADVBERT model and its variants with adversarial training (a) \nand without adversarial training (b) \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  785 \n \n \nFig. 8 shows that regardless of whether adversarial training is used or not, the performance \nof original ADVBERT and the two variants on AIMed and BioInfer is unremarkably different. \nWhen adversarial training is unused, the LSTM based variant achieves the best performance \non the IEPA . However, the CNN -based variant achieves the best performance on the \nHPRD500 and LLL. When using adversarial training ( ğ›¼ğ›¼ =0.25), the LSTM -based variant \nachieved the best performance on the HPRD500 and LLL, and the CNN -based variant \nachieved the best performance on the IEPA dataset. In summary, the use of variant structures \ncan further utilize hidden layer information in addition to [CLS], and can slightly improve the \naccuracy of PPI extraction. \n \n5.4 Effect of the adversarial coefficient \nExperiments were carried out on the original ADVBERT model and its variants with different \nadversarial coefficient values and without adversarial training  (using BioBERT pre -trained \nweights), to illustrate the advantages of adversarial training and the effect  of adversarial \ncoefficient ğ›¼ğ›¼. \n \n \n \nFig. 9. Effect of different adversarial coefficients on the original ADVBERT model \n \nFig. 9 shows the effect  of different adversarial coefficients on the original ADVBERT \nmodel. Fig. 9 shows that all corpora, except the IEPA, achieved the highest F1 scores when ğ›¼ğ›¼ \n=0.25. When ğ›¼ğ›¼ exceeds 0.25, as ğ›¼ğ›¼ increases, F1-scores have declined to a certain  extent, but \noverall it is higher than that without adversarial training. \n \n\n786                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \n \nFig. 10. Effect of different adversarial coefficients on ADVBERT-LSTM model \n \nFig. 10 shows the effect of different adversarial coefficients on LSTM based variant \nADVBERT-LSTM. On the IEPA, ADVBERT-LSTM obtained the highest F1- score without \nusing adversarial training. The performance of using adversarial training on other corpora is \nstill better than not using the approach. On BioInfer and LLL, the F1 score increases with the \nincrease in ğ›¼ğ›¼. However, the F1 score still shows a downward trend after ğ›¼ğ›¼ exceeds 0.25 on \nAIMed and HPRD50. \n \n \nFig. 11. Effect of different adversarial coefficients on ADVBERT-CNN model \n \nFig. 11 shows the effect of different adversarial coefficients on CNN -based variant. For \nADVBERT-CNN model, the performance of using adversarial training is better than that of \nnot using it. In most corpora, the F1-score still shows a downward trend after ğ›¼ğ›¼ exceeds 0.25. \nIn summary, the performance of using adversarial training is better than not using \nadversarial training in most cases. Better performance can be achieved when ğ›¼ğ›¼ is 0.25, but \nextremely large ğ›¼ğ›¼ may also cause a decrease in performance. \n \n \n\nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  787 \n \n5.4 Error analysis \nIn this section, several typical error cases are selected for analysis from the prediction results \nof each corpus, which mainly includes three types, namely, protein named entity nesting, the \nsentence contains too many named entities of protein, and the annotation of ambiguity. \nThe largest proportion is the error caused by the protein named entity nesting, such as â€œA \nbacterially expressed 318 -amino acid fragment, PROTEIN1 (418 -736), containing the \namphipathic helix region, was able to bind PROTEIN2 alpha. â€ In this sentence, PROTEIN1 \nrefers to â€œHt 31â€ and PROTEIN2 refers to â€œRIIâ€, â€œHt 31â€ interacts with â€œRII alphaâ€ but not \nwith â€œRII,â€ the model inaccurately predicts it to be positive. \nSentences containing many named entities  of  protein , mostly in BioInfer . For instance, \nâ€œAbundance of PROTEIN , PROTEIN , PROTEIN and PROTEIN , PROTEIN1 PROTEIN , \nand PROTEIN is not affected by the differential PROTEIN2 expression . â€ A large number of \nnamed entities of protein leads to intricate interactions that should be predicted. These named \nentities of protein form many pair s, but the textual information of sentences is very limited. \nThe model can accurately  predicts most pair s, but it also inaccurately  predicts on a small \nnumber of pairs. \nThe annotation of ambiguity, such as â€œTransient expression of PROTEIN and a null mutant \nof PROTEIN1 (PTP-1CM) in COS cells resulted in an increase in tyrosyl phosphorylation of \nPROTEIN2 and its interaction with PTP-1CMâ€ in HPRD50, in which â€œCD22â€ appeared twice. \nFor PTP-1C and the previous CD22, the label given by the corpus is false. However, for PTP-\n1C and the latter CD22, the label given by the corpus is t rue, thereby causing interference to \nthe model. \n5.5 Cross-corpus evaluation \nCross-corpus evaluation indicates training on one corpus and testing on another corpus to \nevaluate the generalization of the PPI extraction model. We only use AIMed and BioInfer as \nthe training corpora  because the sample size of other corpora is extremely small. The \nexperimental results using AIMed as the training corpus are shown in Table 5 . The \nexperimental results with BioInfer as the training corpus are shown in Table 6. PIPE [43] is a \nknowledge-based state-of-the-art method for PPI extraction cross-corpus evaluation. \n \nTable 5. Cross-corpus evaluation results using AIMed as the training corpus (F1-score %) \nModel BioInfer HPRD50 IEPA LLL \nPIPE 58.2 69.4 69.0 75.2 \nMcDepCnn 48.0 - - - \nLSTM 49.3 - - - \ntLSTM 45.0 39.1 37.9 33.5 \nADVBERT-LSTM 43.40 74.81 26.02 37.86 \nADVBERT-CNN 41.29 69.37 30.85 26.45 \n \nTable 5  shows that when AIMed is used as the training corpus, the proposed model \nperforms poorly on most corpora . The training models on AIMed become over -fitting when \nlearning various complex situations  because the data in AIMed are the most complex. \nHowever, knowledge-based methods can avoid this limitation. \n \n \n788                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \n \nTable 6. Cross-corpus evaluation results using BioInfer as the training corpus \nModel AIMed HPRD50 IEPA LLL \nPIPE 52.1 71.3 72.3 78.5 \nMcDepCnn 49.9 - - - \nLSTM 50.7 - - - \ntLSTM 50.0 45.5 40.0 33.5 \nADVBERT-LSTM 54.55 78.93 75.89 78.05 \nADVBERT-CNN 54.20 77.69 78.21 79.88 \n \nTable 6 shows that when BioInfer was used as the training corpus, the proposed model \nachieved the highest F1 -scores in the remaining four corpora. The BioInfer corpus has the \nlargest amount of data without interference from a large number of complex cases; thus  the \nmodel performs better in cross-corpus evaluation. \n6. Conclusion \nThe ADVBERT model and its two variant structures are proposed for the PPI extraction task. \nThe BioBERT pre-trained weights constructed on the large -scale domain corpus are used to \nenhance the learning of the semantic features and syntactic features of the PPI -related text. \nAdversarial training  is introduced, and advers arial perturbations  are applied  to the token \nembedding of the model , thereby improving the robustness of the PPI extraction model.  \nExperiments on five PPI extracting public corpora show that the proposed approa ch has \nachieved significant improvement. In the future, we will continue to explore how to further \nutilize the advantages of the large pre -trained language models, and find  more effective \napproaches to generate adversarial perturbations to improve the generalization of the PPI \nextraction model. \nReferences \n[1] D. Kwon, J. H. Yoon, S.- Y . Shin, T.-H. Jang, H. -G. Kim, I. So, J.- H. Jeon and H. H. Park, â€œ A \ncomprehensive manually curated pr otein-protein interaction database for the Death Domain \nsuperfamily,â€ Nucleic Acids Research, vol. 40, pp. D331-D336, 2012. Article (CrossRef Link) \n[2] D. E. Gordon, G. M. Jang, M. Bouhaddou, J. W. Xu, K. Obernier, K. M. White, M. J. O'Meara, V . \nV . Rezelj, J. F. Z. Guo, D. L. Swaney et al, â€œA SARS-CoV-2 protein interaction map reveals targets \nfor drug repurposing,â€ Nature, vol. 583, pp. 459-468, 2020. Article (CrossRef Link) \n[3] M. Altmann, S. Altmann, P. A. Rodriguez, B. Weller, L. E. Vergara, J. Palme, N. M. de la Rosa, M. \nSauer, M. Wenig, J. A. Villaecija-Aguilar et al, â€œExtensive signal integration by the phytohormone \nprotein network,â€ Nature, vol. 583, pp. 271-276, 2020. Article (CrossRef Link) \n[4] K. Yu, P.-Y . Lung, T. Zhao, P. Zhao, Y .-Y . Tseng and J. Zhang, â€œAutomatic extraction of protein -\nprotein interactions using grammatical relationship graph ,â€ BMC Medical Informatics and \nDecision Making, vol. 18, 2018. Article (CrossRef Link) \n[5] S. Pyysalo, A. Airola, J. Heimonen, J. BjÃ¶rne, F. Ginter and T. Salakoski , â€œComparative analysis \nof five protein-protein interaction corpora,â€ BMC Bioinformatics, vol. 9, Article no. S6, 2008.  \nArticle (CrossRef Link) \n \n \n \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  789 \n \n[6] W. A. Baumgartner, Z. Lu, H. L. Johnson, J. G. Caporaso, J. Paquette, A. Lindemann, E. K. White, \nO. Medvedeva, K. B. Cohen and L. Hunter, â€œConcept recognition for extracting protein interaction \nrelations from biomedical text,â€ Genome Biology , vol. 9, Article no. S9 , 2008.  \nArticle (CrossRef Link) \n[7] G. Murugesan, S. Abdulkadhar and J. Natarajan , â€œDistributed smoothed tree kernel for protein-\nprotein interaction extraction from the biomedical literature ,â€ PLOS ONE, vol. 12, pp. e0187379, \n2017. Article (CrossRef Link) \n[8] A. Airola, S. Pyysalo, J. BjÃ¶rne, T. Pahikkala, F. Ginter and T. Salakoski, â€œAll-paths graph kernel \nfor protein -protein interaction extraction with evaluation of cross -corpus learning,â€ BMC \nBioinformatics, vol. 9, 2008, Article no. S2. Article (CrossRef Link) \n[9] Z. H. Zhao, Z. H. Yang, H. F. Lin, J. Wang and S. Gao , â€œA protein-protein interaction extraction \napproach based on deep neural network,â€ Int J Data Min Bioinform , vol. 15, pp. 145 -164, 2016. \nArticle (CrossRef Link) \n[10] C. Quan, L. Hua, X. Sun and W. Bai, â€œMultichannel Convolutional Neural Network for Biological \nRelation Extraction,â€ Biomed Res Int, vol. 2016, no. 1850404, 2016. Article (CrossRef Link) \n[11] L. Hua and C. Quan, â€œ A Shortest Dependency Path Based Convolutional Neural Network for \nProtein-Protein Relation Extraction,â€ Biomed Res Int, vol. 2016, no. 8479587, 2016.  \nArticle (CrossRef Link) \n[12] Y . Peng and Z. lu,  â€œDeep learning for extracting protein -protein interactions from biomedical \nliterature,â€ in Proc. of The BioNLP 2017 workshop, pp. 29-38, 2017. Article (CrossRef Link)  \n[13] S. P. Choi , â€œ Extraction of protein- protein interactions (PPIs) from the literature by deep \nconvolutional neural networks with various feature embeddings ,â€ J Inf Sci , vol. 44, pp. 60- 73, \n2018. Article (CrossRef Link) \n[14] H. Zhang, R. C. Guan, F. F. Zhou, Y . C. Liang, Z. H. Zhan, L. Huang and X. Y . Feng, â€œ Deep \nResidual Convolutional Neural Network for Protein- Protein interaction Extraction,â€ Ieee Access, \nvol. 7, pp. 89354-89365, 2019. Article (CrossRef Link) \n[15] Y . L. Hsieh, Y . C. Chang, N. W. Chang and W. L. Hsu, â€œIdentifying Protein-protein Interactions in \nBiomedical Literature using Recurrent Neural Networks with Long Short-Term Memory,â€ in Proc. \nof The 8th IJCNLP, pp. 240-245, 2017. Article (CrossRef Link) \n[16] M. Ahmed, J. Islam, M. R. Samee, and R. E. Mercer, â€œIdentifying Protein-Protein Interaction using \nTree LSTM and Structured Attention,â€ in Proc. of IEEE 13th ICSC, 2019. Article (CrossRef Link) \n[17] S. Yadav, A. Ekbal, S. Saha, A. Kumar and P. Bhattacharyya, â€œ Feature assisted stacked attentive \nshortest dependency path based Bi -LSTM model for protein- protein interaction ,â€ Knowledge-\nBased Syst, vol. 166, pp. 18-29, 2019. Article (CrossRef Link) \n[18] J. Devlin, M. -W. Chang, K. Lee and K. Toutanova, â€œ BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding,â€ in Proc. of NAACL, Minneapolis, Minnesota, USA, \npp. 4171-4186, 2019. \n[19] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So and J. Kang , â€œBioBERT: a pre -trained \nbiomedical language representation model for biomedical text mining ,â€ Bioinformatics, vol. 36, \npp. 1234-1240, 2020. Article (CrossRef Link) \n[20] N. Warikoo, Y . -C. Chang and W. -L. Hsu , â€œ LBERT: Lexically aware Transformer -based \nBidirectional Encoder Representation model for learning universal bio -entity relations ,â€ \nBioinformatics, vol. 37, pp. 404-412, 2021. Article (CrossRef Link) \n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser and I. \nPolosukhin, â€œAttention is all you need ,â€ in Proc. of 31st ICNIPS, Long Beach, California, USA, \npp. 6000â€“6010, 2017. \n[22] T. Yu, R. Jin, X. Han, J. Li and T. Yu , â€œReview of Pre- training Models for Natural Language \nProcessing,â€ CEA, vol. 56, no. 23, pp. 12-22, 2020. Article (CrossRef Link) \n[23] T. Mikolov, I. Sutskever, K. Chen, G. Corrado and J. Dean, â€œDistributed Representations of Words \nand Phrases and their Compositionality,â€ in Proc. of NIPS, 2013. Article (CrossRef Link) \n[24] J. Pennington, R. Socher and C. Manning,  â€œGlove: Global Vectors for Word Representation,â€ in \nProc. of EMNLP, 2014. Article (CrossRef Link) \n \n790                                                                                     Tang et al.: A Protein-Protein Interaction Extraction Approach Based on  \nLarge Pre-trained Language Model and Adversarial Training \n \n[25] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee and L. Zettlemoyer , â€œDeep \nContextualized Word Representations ,â€ in Proc. of NAACL, New Orleans, Louisiana, USA, pp. \n2227-2237, 2019. \n[26] J. Howard and S. Ruder, â€œUniversal Language Model Fine-tuning for Text Classification,â€ in Proc. \nof ACL, Melbourne, Australia, pp. 328-339, 2018. \n[27] R. Alec, N. Karthik, S. Tim and S. Ilya, â€œImproving Language Understanding by Generative Pre -\nTraining,â€ in Proc. of NLPIR, 2019. Article (CrossRef Link) \n[28] H. Yang, J. Yuan, C. Li, G. Zhao, Z. Sun, Q. Yao, B. Bao, A. V . Vasilakos and J. Zhang, â€œBrainIoT: \nBrain-Like Productive Services Provisioning wit h Federated Learning in Industrial IoT ,â€ IEEE \nInternet of Things Journal, vol. 9, pp. 2014-2024, 2022. Article (CrossRef Link) \n[29] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow and R. Fergus, â€œIntriguing \nproperties of neural networks,â€ in Proc. of ICLR, 2014. Article (CrossRef Link) \n[30] J. Goodfellow, J. Shlens and C. Szegedy , â€œExplaining and Harnessing Adversarial Examples,â€ in \nProc. of ICLR, 2015. Article (CrossRef Link). \n[31] T. Miyato, A. M. Dai and I. Goodfellow, â€œAdversarial Training Methods for Semi-Supervised Text \nClassification,â€ in Proc. of ICLR, 2017. Article (CrossRef Link) \n[32] A. Madry, A. Makelov, L. Schmidt, D. Tsipras and A. Vladu, â€œ Towards Deep Learning Models \nResistant to Adversarial Attacks,â€ in Proc. of ICLR, 2018. Article (CrossRef Link) \n[33] L. Ba, J. R. Kiros and G. E. Hinton, â€œLayer Normalization,â€ arxiv, 2016. Article (CrossRef Link) \n[34] M. Jian, K. M. Lam, J. Dong, et al, â€œ Visual-Patch-Attention-Aware Saliency Detection ,â€ IEEE \nTransactions on Cybernetics, vol. 45(8), pp.1575-1586, 2015. Article (CrossRef Link) \n[35] M. Jian, W. Zhang, H. Yu, et al, â€œSaliency Detection Based on Directional Patches Extraction and \nPrincipal Local Color Contrast ,â€ Journal of Visual Communication and Image Representation , \nvol.57, pp. 1-11, 2018. Article (CrossRef Link) \n[36] M. Jian, J. Wang, H. Yu, et al, â€œ Visual saliency detection by integrating spatial position prior of \nobject with background cues,â€ Expert Systems with Applications, vol. 168(11), pp. 114219, 2020. \nArticle (CrossRef Link) \n[37] M. Jian, Qi. Q ,J. Dong, et al, â€œIntegrating QDWD with pattern distinctness and local contrast for \nunderwater saliency detection,â€ Journal of Visual Communication and Image Representation, vol. \n53, pp. 31-41, 2018. Article (CrossRef Link) \n[38] R. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J. Mooney, A. K. Ramani and Y . W. Wong , \nâ€œComparative experiments on learning information extractors for proteins and their interactions ,â€ \nArtificial Intelligence in Medicine, vol. 33, pp. 139-155, 2005. Article (CrossRef Link) \n[39] S. Pyysalo, F. Ginter, J. Heimonen, J. BjoRne, J. Boberg, J. Rvinen and T. Salakoski, â€œBioInfer: a \ncorpus for information extraction in the biomedical domain,â€ BMC Bioinformatics, vol. 8, 2007.  \n[40] Fundel, R. Kueffner and R. Zimmer, â€œRelEx - Relation extraction using dependency parse trees,â€ \nBioinformatics, vol. 23, pp. 365-371, 2007. Article (CrossRef Link) \n[41] D. B. A'D, J. Da and D. Nb , â€œMining Medline: Abstracts, Sentences, Or Phrases?,â€ in Proc. of \nPacific Symposium on Biocomputing Pacific Symposium on Biocomputing, vol. 7, pp. 326- 337, \n2002.  \n[42] C. NÃ©dellec, â€˜â€˜Learning language in logic -genic interaction extraction challenge,â€™â€™ in Proc. of \nLearn. Lang. Logic Workshop, pp. 1â€“7, 2005. \n[43] Y.-C. Chang, C.-H. Chu, Y .-C. Su, C. C. Chen and W.-L. Hsu, â€œPIPE: a proteinâ€“protein interaction \npassage extraction module for BioCreative challenge,â€ Database, vol. 2016, no.  baw101, 2016. \nArticle (CrossRef Link) \n \nKSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS VOL. 16, NO. 3, March 2022                                  791 \n \nZHAN TAN received the M.S. degree in computer science from Shanghai Normal \nUniversity and his Bachelorâ€™s degree from Southwest Jiaotong University. He is currently \npursuing the Ph.D. degree with the College of Information and Electrical Engineering, China \nAgricultural University, Beijing, China. His research interests include natural language \nprocessing, text mining, and deep learning. \n \n \n \n \n \nZHAO BAI received a bachelor's degree in engineering from Anhui University of Science \nand Technology. He is currently  pursuing the M.S. degree with the College of Information \nand Electrical Engineering, China Agricultural University, Beijing, China. His research \ninterests is computer vision. \n \n \n \n \n \n \nLEI DIAO received a bachelor's degree in engineering from Anhui University of Science \nand Technology. He is currently pursuing the M.S . degree with the College of Information \nand Electrical Engineering, China Agricultural University, Beijing, China. His research \ninterests include natural language processing, text mining, and deep learning. \n \n \n \n \n \n \nXUCHAO GUO received his Bachelorâ€™s degree in Computer Science and his Masterâ€™s \ndegree in 2018 from Shandong Agricultural University. He is currently Ph.D. student in the \nCollege of Information and Electrical Engineering, China Agricultural University. He is \nmainly engaged in natural language processing and knowledge graph in agricultural fields. \nHis research interests include: deep learning, complex network analysis, data mining, \nmachine learning, and image processing. \n \n \n \n \nSHUHAN LU received her Bachelorâ€™s degree in Computer and Information Science in \n2020 at the Ohio State University. From 2020, she is currently studying Masterâ€™s in Health \nInformatics in University of Michigan, Ann Arbor. She has good experience in database \ncreating, database managing, and data analysis. Her research interests include: data mining, \ndatabase manage, machine learning and image processing. \n \n \n \n \n \nLIN LI is currently a Professor and a Doctoral Supervisor with the College of Information \nand Electrical Engineering (CIEE), China Agricultural University. Her main research \ninterests include knowledge engineering and machine learning. \n \n \n \n \n"
}