{
    "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling",
    "url": "https://openalex.org/W3174794493",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2108092137",
            "name": "Chuhan Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2142281011",
            "name": "Fangzhao Wu",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2040753340",
            "name": "Tao Qi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117581150",
            "name": "Yongfeng Huang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3006683367",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W3035164270",
        "https://openalex.org/W3098649723",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3019932981",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3034503922",
        "https://openalex.org/W3106298483",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4287812455",
        "https://openalex.org/W2142972908",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3092302658",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3170261818",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W4295838474"
    ],
    "abstract": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 848â€“853\nAugust 1â€“6, 2021. Â©2021 Association for Computational Linguistics\n848\nHi-Transformer: Hierarchical Interactive Transformer for Efï¬cient and\nEffective Long Document Modeling\nChuhan Wuâ€  Fangzhao Wuâ€¡ Tao Qiâ€  Yongfeng Huangâ€ \nâ€ Department of Electronic Engineering & BNRist, Tsinghua University, Beijing 100084, China\nâ€¡Microsoft Research Asia, Beijing 100080, China\n{wuchuhan15, wufangzhao, taoqi.qt}@gmail.com\nyfhuang@tsinghua.edu.cn\nAbstract\nTransformer is important for text modeling.\nHowever, it has difï¬culty in handling long\ndocuments due to the quadratic complexity\nwith input text length. In order to handle\nthis problem, we propose a hierarchical inter-\nactive Transformer (Hi-Transformer) for efï¬-\ncient and effective long document modeling.\nHi-Transformer models documents in a hierar-\nchical way, i.e., ï¬rst learns sentence represen-\ntations and then learns document representa-\ntions. It can effectively reduce the complexity\nand meanwhile capture global document con-\ntext in the modeling of each sentence. More\nspeciï¬cally, we ï¬rst use a sentence Trans-\nformer to learn the representations of each\nsentence. Then we use a document Trans-\nformer to model the global document context\nfrom these sentence representations. Next, we\nuse another sentence Transformer to enhance\nsentence modeling using the global document\ncontext. Finally, we use hierarchical pooling\nmethod to obtain document embedding. Exten-\nsive experiments on three benchmark datasets\nvalidate the efï¬ciency and effectiveness of Hi-\nTransformer in long document modeling.\n1 Introduction\nTransformer (Vaswani et al., 2017) is an effective\narchitecture for text modeling, and has been an es-\nsential component in many state-of-the-art NLP\nmodels like BERT (Devlin et al., 2019; Radford\net al., 2019; Yang et al., 2019; Wu et al., 2021). The\nstandard Transformer needs to compute a dense\nself-attention matrix based on the interactions be-\ntween each pair of tokens in text, where the compu-\ntational complexity is proportional to the square of\ntext length (Vaswani et al., 2017; Wu et al., 2020b).\nThus, it is difï¬cult for Transformer to model long\ndocuments efï¬ciently (Child et al., 2019).\nThere are several methods to accelerate Trans-\nformer for long document modeling (Wu et al.,\n2019; Kitaev et al., 2019; Wang et al., 2020; Qiu\net al., 2020). One direction is using Transformer\nin a hierarchical manner to reduce sequence length,\ne.g., ï¬rst learn sentence representations and then\nlearn document representations from sentence rep-\nresentations (Zhang et al., 2019; Yang et al., 2020).\nHowever, the modeling of sentences is agnostic to\nthe global document context, which may be subop-\ntimal because the local context within sentence is\nusually insufï¬cient. Another direction is using a\nsparse self-attention matrix instead of a dense one.\nFor example, Beltagy et al. (2020) proposed to\ncombine local self-attention with a dilated sliding\nwindow and sparse global attention. Zaheer et al.\n(2020) proposed to incorporate a random sparse\nattention mechanism to model the interactions be-\ntween a random set of tokens. However, these\nmethods cannot fully model the global context of\ndocument (Tay et al., 2020).\nIn this paper, we propose a hierarchical interac-\ntive Transformer (Hi-Transformer)1 for efï¬cient\nand effective long document modeling, which mod-\nels documents in a hierarchical way to effectively\nreduce the complexity and at the same time can\ncapture the global document context for sentence\nmodeling. In Hi-Transformer, we ï¬rst use a sen-\ntence Transformer to learn the representation of\neach sentence within a document. Next, we use a\ndocument Transformer to model the global docu-\nment context from these sentence representations.\nThen, we use another sentence Transformer to fur-\nther improve the modeling of each sentence with\nthe help of the global document context. Finally,\nwe use hierarchical pooling method to obtain the\ndocument representation. Extensive experiments\nare conducted on three benchmark datasets. The\nresults show that Hi-Transformer is both efï¬cient\nand effective in long document modeling.\n1https://github.com/wuch15/HiTransformer.\n849\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nHi-Transformer\nShop   the â€¦  without [CLS] The  royals   â€¦     is   [CLS] Prince Philip   â€¦      )   [CLS]\nSentence 1 Sentence 2 Sentence M\nDocument\nğ’†ğ’†1,1 ğ’†ğ’†1,2 ğ’†ğ’†1,ğ¾ğ¾ ğ’†ğ’†ğ‘ ğ‘ \nğ’‰ğ’‰1,1 ğ’‰ğ’‰1,2 ğ’‰ğ’‰1,ğ¾ğ¾ ğ’‰ğ’‰1\nğ‘ ğ‘ \nâ€¦\nâ€¦\nâ€¦\nğ’†ğ’†2,1 ğ’†ğ’†2,2 ğ’†ğ’†2,ğ¾ğ¾ ğ’†ğ’†ğ‘ ğ‘ \nğ’‰ğ’‰2,1 ğ’‰ğ’‰2,2 ğ’‰ğ’‰2,ğ¾ğ¾ ğ’‰ğ’‰2\nğ‘ ğ‘ \nâ€¦\nâ€¦\nâ€¦\nğ’†ğ’†ğ‘€ğ‘€,1 ğ’†ğ’†ğ‘€ğ‘€,2 ğ’†ğ’†ğ‘€ğ‘€,ğ¾ğ¾ ğ’†ğ’†ğ‘ ğ‘ \nğ’‰ğ’‰ğ‘€ğ‘€,1 ğ’‰ğ’‰ğ‘€ğ‘€,2 ğ’‰ğ’‰ğ‘€ğ‘€,ğ¾ğ¾ ğ’‰ğ’‰ğ‘€ğ‘€\nğ‘ ğ‘ \nWord & Position Embedding\n Word & Position Embedding\n Word & Position Embedding\n+\nğ’‘ğ’‘1\n+\nğ’‘ğ’‘2\n+\nğ’‘ğ’‘ğ‘€ğ‘€\nğ’“ğ’“1\nğ‘ ğ‘  ğ’“ğ’“2\nğ‘ ğ‘  ğ’“ğ’“ğ‘€ğ‘€\nğ‘ ğ‘ \nğ’…ğ’…1,1 ğ’…ğ’…1,2 ğ’…ğ’…1,ğ¾ğ¾ ğ’…ğ’…1\nğ‘ ğ‘  ğ’…ğ’…2,1 ğ’…ğ’…2,2 ğ’…ğ’…2,ğ¾ğ¾ ğ’…ğ’…2\nğ‘ ğ‘  ğ’…ğ’…ğ‘€ğ‘€,1 ğ’…ğ’…ğ‘€ğ‘€,2 ğ’…ğ’…ğ‘€ğ‘€,ğ¾ğ¾ ğ’…ğ’…ğ‘€ğ‘€\nğ‘ ğ‘ \nPooling\n Pooling\n Pooling\nâ€¦ğ’”ğ’”1 ğ’”ğ’”2 ğ’”ğ’”ğ‘€ğ‘€\nPooling\nNÃ—\nPosition\nEmbedding\nDocument Context-\naware Sentence\nRepresentations\nğ’…ğ’… Document \nEmbedding\nGlobal Context-aware \nSentence Embedding\nDocument Transformer\nSentence Transformer\nSentence Transformer\nSentence Transformer\nSentence Transformer\nSentence Transformer\nSentence Transformer\nFigure 1: The architecture of Hi-Transformer.\n2 Hi-Transformer\nIn this section, we introduce our hierarchical in-\nteractive Transformer (Hi-Transformer) approach\nfor efï¬cient and effective long document model-\ning. Its framework is shown in Fig. 1. It uses a\nhierarchical architecture that ï¬rst models the con-\ntexts within a sentence, next models the document\ncontexts by capturing the interactions between sen-\ntences, then employs the global document contexts\nto enhance sentence modeling, and ï¬nally uses hi-\nerarchical pooling techniques to obtain document\nembeddings. In this way, the input sequence length\nof each Transformer is much shorter than directly\ntaking the word sequence in document as input, and\nthe global contexts can be fully modeled. The de-\ntails of Hi-Transformer are introduced as follows.\n2.1 Model Architecture\nHi-Transformer mainly contains three modules, i.e.,\nsentence context modeling, document context mod-\neling and global document context-enhanced sen-\ntence modeling. The sentence-level context is ï¬rst\nmodeled by a sentence Transformer. Assume a doc-\nument contains M sentences, and the words in the\ni-th sentence are denoted as [wi,1, wi,2, ..., wi,K]\n(K is the sentence length). We insert a â€œ[CLS]â€ to-\nken (denoted as ws) after the end of each sentence.\nThis token is used to convey the contextual informa-\ntion within this sentence. The sequence of words in\neach sentence is ï¬rst converted into a word embed-\nding sequence via a word and position embedding\nlayer. Denote the word embedding sequence for the\ni-th sentence as [ei,1, ei,2, ...,ei,K, es]. Since sen-\ntence length is usually short, we apply a sentence\nTransformer to each sentence to fully model the in-\nteractions between the words within this sentence.\nIt takes the word embedding sequence as the input,\nand outputs the contextual representations of words,\nwhich are denoted as [hi,1, hi,2, ...,hi,K, hs\ni ]. Spe-\ncially, the representation hs\ni of the â€œ[CLS]â€ token\nis regarded as the sentence representation.\nNext, the document-level context is modeled by\na document Transformer from the representations\nof the sentences within this document. Denote the\n850\nembedding sequence of sentences in this document\nas [hs\n1, hs\n2, ...,hs\nM ]. We add a sentence position\nembedding (denoted as pi for the i-th sentence)\nto the sentence representations to capture sentence\norders. We then apply a document Transformer to\nthese sentence representations to capture the global\ncontext of document, and further learn document\ncontext-aware sentence representations, which are\ndenoted as [rs\n1, rs\n2, ...,rs\nM ].\nThen, we use the document context-aware sen-\ntence representations to further improve the sen-\ntence context modeling by propagating the global\ndocument context to each sentence. Motivated\nby (Guo et al., 2019), we apply another sentence\nTransformer to the hidden word representations\nand the document-aware sentence representation\nfor each sentence. It outputs a document context-\naware word representation sequence for each sen-\ntence, which is denoted as [di,1, di,2, ...,di,K, ds\ni ].\nIn this way, the contextual representations of words\ncan beneï¬t from both local sentence context and\nglobal document context.\nBy stacking multiple layers of Hi-Transformer,\nthe contexts within a document can be fully mod-\neled. Finally, we use hierarchical pooling (Wu\net al., 2020a) techniques to obtain the document em-\nbedding. We ï¬rst aggregate the document context-\naware word representations in each sentence into a\nglobal context-aware sentence embedding si, and\nthen aggregate the global context-aware embed-\ndings of sentence within a document into a uniï¬ed\ndocument embedding d, which is further used for\ndownstream tasks.\n2.2 Efï¬ciency Analysis\nIn this section, we provide some discussions on the\ncomputational complexity of Hi-Transformer. In\nsentence context modeling and document context\npropagation, the total computational complexity is\nO(M Â·K2 Â·d), where M is sentence number with a\ndocument, K is sentence length, andd is the hidden\ndimension. In document context modeling, the\ncomputational complexity is O(M2 Â·d). Thus, the\ntotal computational cost is O(M Â·K2 Â·d+M2 Â·d).2\nCompared with the standard Transformer whose\ncomputational complexity is O(M2 Â·K2 Â·d), Hi-\nTransformer is much more efï¬cient.\n2Note that Hi-Transformer can be combined with other\nexisting techniques of efï¬cient Transformer to further improve\nthe efï¬ciency for long document modeling.\n3 Experiments\n3.1 Datasets and Experimental Settings\nOur experiments are conducted on three bench-\nmark document modeling datasets. The ï¬rst one\nis Amazon Electronics (He and McAuley, 2016)\n(denoted as Amazon), which is for product review\nrating prediction.3 The second one is IMDB (Diao\net al., 2014), a widely used dataset for movie re-\nview rating prediction.4 The third one is the MIND\ndataset (Wu et al., 2020c), which is a large-scale\ndataset for news intelligence.5 We use the content\nbased news topic classiï¬cation task on this dataset.\nThe detailed dataset statistics are shown in Table 1.\nIn our experiments, we use the 300-dimensional\npre-trained Glove (Pennington et al., 2014) embed-\ndings for initializing word embeddings. We use\ntwo Hi-Transformers layers in our approach and\ntwo Transformer layers in other baseline methods.6\nWe use attentive pooling (Yang et al., 2016) to\nimplement the hierarchical pooling module. The\nhidden dimension is set to 256, i.e., 8 self-attention\nheads in total and the output dimension of each\nhead is 32. Due to the limitation of GPU memory,\nthe input sequence lengths of vanilla Transformer\nand its variants for long documents are 512 and\n2048, respectively. The dropout (Srivastava et al.,\n2014) ratio is 0.2. The optimizer is Adam (Bengio\nand LeCun, 2015), and the learning rate is 1e-4.\nThe maximum training epoch is 3. The models\nare implemented using the Keras library with Ten-\nsorï¬‚ow backend. The GPU we used is GeForce\nGTX 1080 Ti with a memory of 11 GB. We use\naccuracy and macro-F scores as the performance\nmetrics. We repeat each experiment 5 times and\nreport both average results and standard deviations.\n3.2 Performance Evaluation\nWe compare Hi-Transformer with several base-\nlines, including: (1) Transformer (Vaswani et al.,\n2017), the vanilla Transformer architecture; (2)\nLongformer (Beltagy et al., 2020), a variant of\nTransformer with local and global attention for\nlong documents; (3) BigBird (Zaheer et al., 2020),\nextending Longformer with random attention; (4)\nHI-BERT (Zhang et al., 2019), using Transformers\n3https://jmcauley.ucsd.edu/data/amazon/\n4https://github.com/nihalb/JMARS\n5https://msnews.github.io/\n6We also tried more Transformer layers for baseline meth-\nods but do not observe signiï¬cant performance improvement\nin our experiments.\n851\nDataset #Train #Val #Test Avg. #word Avg. #sent #Class\nAmazon 40.0k 5.0k 5.0k 133.38 6.17 5\nIMDB 108.5k 13.6k 13.6k 385.70 15.29 10\nMIND 128.8k 16.1k 16.1k 505.46 25.14 18\nTable 1: Statistics of datasets.\nMethods Amazon IMDB MIND\nAccuracy Macro-F Accuracy Macro-F Accuracy Macro-F\nTransformer 65.23 Â±0.38 42.23 Â±0.37 51.98 Â±0.48 42.76 Â±0.49 80.96 Â±0.22 59.97 Â±0.24\nLongformer 65.35 Â±0.44 42.45 Â±0.41 52.33 Â±0.40 43.51 Â±0.42 81.42 Â±0.25 62.68 Â±0.26\nBigBird 66.05 Â±0.48 42.89 Â±0.46 52.87 Â±0.51 43.79 Â±0.50 81.81 Â±0.29 63.44 Â±0.31\nHI-BERT 66.56 Â±0.32 42.65 Â±0.34 52.96 Â±0.46 43.84 Â±0.46 81.89 Â±0.23 63.63 Â±0.20\nHi-Transformer 67.24 Â±0.35 43.69 Â±0.32 53.78 Â±0.49 44.54 Â±0.47 82.51 Â±0.25 64.22 Â±0.22\nTable 2: The results of different methods on different datasets.\nMethod Complexity\nTransformer O(M2 Â·K2 Â·d)\nLongformer O(T Â·M Â·K Â·d)\nBigBird O(T Â·M Â·K Â·d)\nHI-BERT O(M Â·K2 Â·d + M2 Â·d)\nHi-Transformer O(M Â·K2 Â·d + M2 Â·d)\nTable 3: Complexity of different methods. K is sen-\ntence length, M is the number of sentences in a docu-\nment, T is the number of positions for sparse attention,\nand d is the hidden dimension.\nat both word and sentence levels. The results of\nthese methods on the three datasets are shown in Ta-\nble 2. We ï¬nd that Transformers designed for long\ndocuments like Hi-Transformer and BigBird out-\nperform the vanilla Transformer. This is because\nvanilla Transformer cannot handle long sequence\ndue to the restriction of computation resources, and\ntruncating the input sequence leads to the loss of\nmuch useful contextual information. In addition,\nHi-Transformer and HI-BERT outperform Long-\nformer and BigBird. This is because the sparse\nattention mechanism used in Longformer and Big-\nBird cannot fully model the global contexts within\na document. Besides, Hi-Transformer achieves the\nbest performance, and the t-test results show the\nimprovements over baselines are signiï¬cant. This\nis because Hi-Transformer can incorporate global\ndocument contexts to enhance sentence modeling.\nWe also compare the computational complexity\nof these methods in Table 3. The complexity of\nHi-Transformer is much less than the vanilla Trans-\nformer and is comparable with other Transformer\nvariants designed for long documents. These re-\nsults indicate the efï¬ciency and effectiveness of\nHi-Transformer.\n3.3 Model Effectiveness\nNest, we verify the effectiveness of the global doc-\nument contexts for enhancing sentence modeling\nin Hi-Transformer. We compare Hi-Transformer\nand its variants without global document contexts\nin Fig. 2. We ï¬nd the performance consistently\ndeclines when the global document contexts are\nnot encoded into sentence representations. This is\nbecause the local contexts within a single sentence\nmay be insufï¬cient for accurate sentence model-\ning, and global contexts in the entire document\ncan provide rich complementary information for\nsentence understanding. Thus, propagating the doc-\nument contexts to enhance sentence modeling can\nimprove long document modeling.\n3.4 Inï¬‚uence of Text Length\nThen, we study the inï¬‚uence of text length on the\nmodel performance and computational cost. Since\nthe documents in the MIND dataset are longest,\nwe conduct experiments on MIND to compare the\nmodel performance as well as the training time\nper layer of Transformer and Hi-Transformer un-\nder different input text length7, and the results are\nshown in Fig. 3. We ï¬nd the performance of both\nmethods improves when longer text sequences are\nused. This is intuitive because more information\ncan be incorporated when longer text is input to\nthe model for document modeling. However, the\ncomputational cost of Transformer grows very fast,\n7The maximum length of Transformer is 512 due to GPU\nmemory limitation.\n852\nAccuracy Macro-F65.0\n65.5\n66.0\n66.5\n67.0\n67.5\n68.0Accuracy\n66.61\n67.24\n41.0\n41.5\n42.0\n42.5\n43.0\n43.5\n44.0\nMacro-F\n42.70\n43.69\nHi-Transformer\n- Global document context\n(a) Amazon.\nAccuracy Macro-F51.5\n52.0\n52.5\n53.0\n53.5\n54.0\n54.5Accuracy\n53.13\n53.78\n42.0\n42.5\n43.0\n43.5\n44.0\n44.5\n45.0\nMacro-F\n43.92\n44.54\nHi-Transformer\n- Global document context\n(b) IMDB.\nAccuracy Macro-F80.0\n80.5\n81.0\n81.5\n82.0\n82.5\n83.0Accuracy\n81.92\n82.51\n62.0\n62.5\n63.0\n63.5\n64.0\n64.5\n65.0\nMacro-F\n63.56\n64.22\nHi-Transformer\n- Global document context\n(c) MIND.\nFigure 2: Effectiveness of global document context\npropagation in Hi-Transformer .\nwhich limits its maximal input text length. Dif-\nferent from Transformer, Hi-Transformer is much\nmore efï¬cient and meanwhile can achieve better\nperformance with longer sequence length. These re-\nsults further verify the efï¬ciency and effectiveness\nof Hi-Transformer in long document modeling.\n4 Conclusion\nIn this paper, we propose a Hi-Transformer ap-\nproach for both efï¬cient and effective long docu-\nment modeling. It incorporates a hierarchical ar-\nchitecture that ï¬rst learns sentence representations\nand then learns document representations. It can\neffectively reduce the computational complexity\nand meanwhile be aware of the global document\n64 128 256 512 1024 2048\nText Length\n75.0\n77.0\n79.0\n81.0\n83.0\n85.0Accuracy\nTransformer\nHi-Transformer\n(a) Accuracy.\n64 128 256 512 1024 2048\nText Length\n48.0\n52.0\n56.0\n60.0\n64.0\n68.0Macro-F\nTransformer\nHi-Transformer\n(b) Macro-F.\n64 128 256 512 1024 2048\nText Length\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Time per Iteration/s\nTransformer\nHi-Transformer\n(c) Training time per layer.\nFigure 3: Inï¬‚uence of input text length on performance\nand training time on the MIND dataset.\ncontexts in sentence modeling to help understand\ndocument content accurately. Extensive experi-\nments on three benchmark datasets validate the\nefï¬ciency and effectiveness of Hi-Transformer in\nlong document modeling.\nAcknowledgments\nThis work was supported by the National Natural\nScience Foundation of China under Grant numbers\nU1936216, U1936208, U1836204, and U1705261.\nWe are grateful to Xing Xie, Shaoyu Zhou, Dan\nShen, and Zhisong Wang for their insightful com-\nments and suggestions on this work.\n853\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nYoshua Bengio and Yann LeCun. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171â€“4186.\nQiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-\nder J Smola, Jing Jiang, and Chong Wang. 2014.\nJointly modeling aspects, ratings and sentiments for\nmovie recommendation (jmars). In KDD, pages\n193â€“202.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. In NAACL-HLT, pages 1315â€“1325.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative ï¬ltering. In WWW, pages\n507â€“517.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2019. Reformer: The efï¬cient transformer. In\nICLR.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532â€“1543.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise\nself-attention for long document understanding. In\nEMNLP: Findings, pages 2555â€“2565.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overï¬tting. JMLR, 15(1):1929â€“1958.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efï¬cient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998â€“6008.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang.\n2021. DA-transformer: Distance-aware transformer.\nIn NAACL-HLT, pages 2059â€“2068.\nChuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, and\nYongfeng Huang. 2020a. Attentive pooling with\nlearnable norms for text representation. In ACL,\npages 2961â€“2970.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2020b. Improving attention mechanism\nwith query-value interaction. arXiv preprint\narXiv:2010.03766.\nFangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan\nWu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,\nJianfeng Gao, Winnie Wu, et al. 2020c. Mind: A\nlarge-scale dataset for news recommendation. In\nACL, pages 3597â€“3606.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2019. Lite transformer with long-short range\nattention. In ICLR.\nLiu Yang, Mingyang Zhang, Cheng Li, Michael Ben-\ndersky, and Marc Najork. 2020. Beyond 512 to-\nkens: Siamese multi-depth transformer-based hier-\narchical encoder for long-form document matching.\nIn CIKM, pages 1725â€“1734.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS, pages 5753â€“\n5763.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiï¬cation. In\nNAACL-HLT, pages 1480â€“1489.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. Hi-\nbert: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In ACL, pages 5059â€“5069."
}