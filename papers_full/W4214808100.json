{
  "title": "A Comparative Study of Deep Learning Classification Methods on a Small Environmental Microorganism Image Dataset (EMDS-6): From Convolutional Neural Networks to Visual Transformers",
  "url": "https://openalex.org/W4214808100",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2095308973",
      "name": "Peng Zhao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2095600869",
      "name": "Chen Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2749535069",
      "name": "Md Mamunur Rahaman",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2097611263",
      "name": "Hao Xu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2974285487",
      "name": "Hechen Yang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2105351999",
      "name": "Hongzan Sun",
      "affiliations": [
        "China Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A1928628455",
      "name": "Tao Jiang",
      "affiliations": [
        "Chengdu University of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2006254856",
      "name": "Marcin Grzegorzek",
      "affiliations": [
        "University of Lübeck"
      ]
    },
    {
      "id": "https://openalex.org/A2095308973",
      "name": "Peng Zhao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2095600869",
      "name": "Chen Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2749535069",
      "name": "Md Mamunur Rahaman",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2097611263",
      "name": "Hao Xu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2974285487",
      "name": "Hechen Yang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2105351999",
      "name": "Hongzan Sun",
      "affiliations": [
        "China Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A1928628455",
      "name": "Tao Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2006254856",
      "name": "Marcin Grzegorzek",
      "affiliations": [
        "Lundbeck (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2096784155",
    "https://openalex.org/W2099355395",
    "https://openalex.org/W3089174329",
    "https://openalex.org/W2905230381",
    "https://openalex.org/W6771097684",
    "https://openalex.org/W7068054049",
    "https://openalex.org/W6728184133",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2744815531",
    "https://openalex.org/W2136804271",
    "https://openalex.org/W1527739722",
    "https://openalex.org/W2770456481",
    "https://openalex.org/W3143802587",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2311523351",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W2802802696",
    "https://openalex.org/W2777517892",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W6668105861",
    "https://openalex.org/W2079914758",
    "https://openalex.org/W6753767121",
    "https://openalex.org/W3135310702",
    "https://openalex.org/W607506750",
    "https://openalex.org/W2004410965",
    "https://openalex.org/W2982127434",
    "https://openalex.org/W6602002561",
    "https://openalex.org/W2782970864",
    "https://openalex.org/W6749781174",
    "https://openalex.org/W2165229257",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W6686164453",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3046220160",
    "https://openalex.org/W2398921440",
    "https://openalex.org/W2021716602",
    "https://openalex.org/W6788477181",
    "https://openalex.org/W3011667710",
    "https://openalex.org/W6811248998",
    "https://openalex.org/W3137963169",
    "https://openalex.org/W2744043610",
    "https://openalex.org/W2016070412",
    "https://openalex.org/W3146157593",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W4236965008",
    "https://openalex.org/W4287643567",
    "https://openalex.org/W4226414874",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4224324261",
    "https://openalex.org/W4239557128"
  ],
  "abstract": "In recent years, deep learning has made brilliant achievements in Environmental Microorganism (EM) image classification. However, image classification of small EM datasets has still not obtained good research results. Therefore, researchers need to spend a lot of time searching for models with good classification performance and suitable for the current equipment working environment. To provide reliable references for researchers, we conduct a series of comparison experiments on 21 deep learning models. The experiment includes direct classification, imbalanced training, and hyper-parameters tuning experiments. During the experiments, we find complementarities among the 21 models, which is the basis for feature fusion related experiments. We also find that the data augmentation method of geometric deformation is difficult to improve the performance of VTs (ViT, DeiT, BotNet, and T2T-ViT) series models. In terms of model performance, Xception has the best classification performance, the vision transformer (ViT) model consumes the least time for training, and the ShuffleNet-V2 model has the least number of parameters.",
  "full_text": "ORIGINAL RESEARCH\npublished: 02 March 2022\ndoi: 10.3389/fmicb.2022.792166\nFrontiers in Microbiology | www.frontiersin.org 1 March 2022 | Volume 13 | Article 792166\nEdited by:\nMohammad-Hossein Sarrafzadeh,\nUniversity of Tehran, Iran\nReviewed by:\nKaveh Kavousi,\nUniversity of Tehran, Iran\nHashem Asgharnejad,\nPolytechnique Montréal, Canada\n*Correspondence:\nChen Li\nlichen201096@hotmail.com\nTao Jiang\njiang@cuit.edu.cn\nSpecialty section:\nThis article was submitted to\nSystems Microbiology,\na section of the journal\nFrontiers in Microbiology\nReceived: 09 October 2021\nAccepted: 02 February 2022\nPublished: 02 March 2022\nCitation:\nZhao P , Li C, Rahaman MM, Xu H,\nYang H, Sun H, Jiang T and\nGrzegorzek M (2022) A Comparative\nStudy of Deep Learning Classiﬁcation\nMethods on a Small Environmental\nMicroorganism Image Dataset\n(EMDS-6): From Convolutional Neural\nNetworks to Visual Transformers.\nFront. Microbiol. 13:792166.\ndoi: 10.3389/fmicb.2022.792166\nA Comparative Study of Deep\nLearning Classiﬁcation Methods on a\nSmall Environmental Microorganism\nImage Dataset (EMDS-6): From\nConvolutional Neural Networks to\nVisual Transformers\nPeng Zhao1, Chen Li1*, Md Mamunur Rahaman1, Hao Xu1, Hechen Yang1, Hongzan Sun2,\nTao Jiang3* and Marcin Grzegorzek4\n1 Microscopic Image and Medical Image Analysis Group, College o f Medicine and Biological Information Engineering,\nNortheastern University, Shenyang, China, 2 Shengjing Hospital, China Medical University, Shenyang, C hina, 3 School of\nControl Engineering, Chengdu University of Information Tec hnology, Chengdu, China, 4 Institute of Medical Informatics,\nUniversity of Lübeck, Lübeck, Germany\nIn recent years, deep learning has made brilliant achieveme nts in Environmental\nMicroorganism (EM) image classiﬁcation. However, image classiﬁcation of small EM\ndatasets has still not obtained good research results. Ther efore, researchers need\nto spend a lot of time searching for models with good classiﬁc ation performance\nand suitable for the current equipment working environment . To provide reliable\nreferences for researchers, we conduct a series of comparis on experiments on\n21 deep learning models. The experiment includes direct cla ssiﬁcation, imbalanced\ntraining, and hyper-parameters tuning experiments. Durin g the experiments, we ﬁnd\ncomplementarities among the 21 models, which is the basis fo r feature fusion\nrelated experiments. We also ﬁnd that the data augmentation method of geometric\ndeformation is difﬁcult to improve the performance of VTs (V iT, DeiT, BotNet, and T2T -ViT)\nseries models. In terms of model performance, Xception has t he best classiﬁcation\nperformance, the vision transformer (ViT) model consumes t he least time for training,\nand the ShufﬂeNet-V2 model has the least number of parameter s.\nKeywords: deep learning, convolutional neural network, vi sual transformer, image classiﬁcation, small dataset,\nenvironmental microorganism\n1. INTRODUCTION\nWith the advancement of industrialization, industrial poll ution becomes increasingly serious.\nTherefore, ﬁnding eﬀective methods to control, reduce, or el iminate pollution is a top priority.\nBiological approaches have outstanding performance in solvin g environmental pollution problems.\nThe Biological approaches have four main advantages in enviro nmental treatment: no new\npollution, no additional energy consumption, gentle process, d ecomposition products can feedback\nto nature, and make a virtuous cycle of material changes (\nMcKinney, 2004 ). Microorganisms are\nall tiny creatures that are invisible to the naked eyes. They are tiny and simple in structure, and\nusually can only be seen with a microscope. Environmental Microorganisms (EMs) speciﬁcally\nrefer to those species of microorganisms that live in natural environments (such as mountains,\nZhao et al. Deep Learning Comparison on EMDS-6\nstreams, and oceans) and artiﬁcial environments (such orch ards\nand ﬁsh ponds). EMs play a vital role in whole nature\nfor better or worse. For example, lactic acid bacteria can\ndecompose some organic matter in the natural environment to\nprovide nutrients for plants; actinomycetes can digest organ ic\nwaste in sludge and improve water quality; microalgae can\nﬁx carbon dioxide in the air and be used as a raw material\nfor biodiesel (\nZhao et al., 2021 ); activated sludge composed\nof microorganisms has a strong ability to adsorb and oxidize\norganic matter and purify water (\nAsgharnejad and Sarrafzadeh,\n2020). Harmful rhizosphere bacteria can inhibit plant growth\nby producing phytotoxins ( Fried et al., 2000 ). Sludge bulking is\ncaused by bacterial proliferation and the accumulation of st icky\nmaterial, which poses a fundamental challenge for wastewate r\ntreatment ( Fan et al., 2017 ). Therefore, EMs research helps solve\nenvironmental pollution problems, and the classiﬁcation of E Ms\nis the cornerstone of related research ( Kosov et al., 2018 ).\nGenerally, the size of EMs is between 0.1 and 100 µ m,\nwhich is challenging to be identiﬁed and found. The traditio nal\nmicrobial classiﬁcation method typically uses the “morpholo gical\nmethod, ” which requires a skilled operator to observe the EMs\nunder a microscope. Then the results are given according to\nthe shape characteristics. This is very time-consuming and\nﬁnancial (\nPepper et al., 2011 ). In addition, if researchers do not\nrefer to the literature, even very experienced researchers c annot\nguarantee the accuracy and objectivity of the analysis resul ts.\nTherefore, using the computer-aided classiﬁcation of EM ima ges\ncan enable researchers to use the slightest professional kno wledge\nand the least time to make the most accurate judgments.\nCurrently, the analysis of EMs by computer vision is\nalready achieved. For example, RGB (Red, Green, Blue) color\nanalysis measures the number of microorganisms (\nFilzmoser\nand Todorov, 2011; Sarrafzadeh et al., 2015 ), and deep learning\nmethods are used to achieve the classiﬁcation and segmentat ion\nof EM images. Among them, the research of EM classiﬁcation\nusing deep learning methods obtains more and more attention .\nDeep learning is a new research direction in the ﬁeld of\nmachine learning, and it provides good performance for image\nclassiﬁcation (\nZhang et al., 2020 ). Traditional machine learning-\nbased EM classiﬁcation methods rely on feature extraction,\nwhich requires many human resources (\nÇayir et al., 2018 ).\nIn contrast, deep learning-based algorithms perform featur e\nextraction in an automated manner, allowing researchers to\nuse minimal domain knowledge and workforce to extract\nprominent features. Furthermore, the classiﬁcation result s of\ndeep learning are better than that of traditional machine le arning\nin the case of super-large training samples (\nWang et al., 2021 ).\nHowever, for small datasets, the performance of deep learnin g\nis limited. Because the collection of EMs is usually carried\nout outdoors, for some sensitive EMs, transportation, storag e,\nand observation during the period may aﬀect the quality of\nthe ﬁnal images. Therefore, it is diﬃcult to obtain enough\nhigh-quality images, and this case results in the problem of\nsmall datasets. Therefore, this paper compares the performance\nof various deep learning models on small data sets of EMs\nand aims to ﬁnd models with better performance on small\ndata sets.\nThis article compares a series of Convolutional Neural\nNetworks (CNNs), such as ResNet-18, 34, 50, 101 (\nHe et al.,\n2016), VGG11, 13, 16, 19 ( Simonyan and Zisserman, 2014 ),\nDenseNet-121, 169 ( Huang et al., 2017 ), Inception-V3 ( Szegedy\net al., 2016 ), Xception ( Chollet, 2017 ), AlexNet ( Krizhevsky\net al., 2012 ), GoogleNet ( Szegedy et al., 2015 ), MobileNet-\nV2 ( Sandler et al., 2018 ), ShuﬀeleNet-V2x0.5 ( Ma et al.,\n2018), Inception-ResNet-V1 ( Szegedy et al., 2017 ), and a\nseries of visual transformers (VTs), such as vision transfo rmer\n(ViT) ( Dosovitskiy et al., 2020 ), BotNet ( Srinivas et al., 2021 ),\nDeiT ( Touvron et al., 2020 ), T2T-ViT ( Yuan et al., 2021 ). The\npurpose is to ﬁnd deep learning models that are suitable for EM\nsmall datasets. The workﬂow diagram of this study is shown in\nFigure 1. Step (b) is to rotate the training set and validation set\nimages by 90 ◦, 180 ◦, 270 ◦, and mirror images up and down, left\nand right, augment the dataset by six times. Step (c) is unifo rm\nimage size to 224 × 224 to facilitate training and classiﬁcation.\nStep (d) is to input the processed data into diﬀerent network\nmodels for training. Step (e) is to input the test set into the tr ained\nnetwork for classiﬁcation, and step (f) is to calculate the Average\nPrecision (AP), accuracy, precision, recall, and F1-score based\non the classiﬁcation results to evaluate the performance of t he\nnetwork model.\nThe structure of this paper is as follows. Section 2 introduces\nthe related methods of deep learning in image classiﬁcation,\nthe impact of small datasets on image classiﬁcation, and the\nrelated work of deep learning models. Section 3 introduces th e\ndataset and experimental design in detail. Section 4 compares and\nsummarizes the experimental results. Section 5 summarizes th e\nwhole paper and looks forward.\n2. RELATED WORK\nThis section summarizes the impact of small datasets\non classiﬁcation, including basic deep learning image\nclassiﬁcation methods.\n2.1. The Impact of Small Datasets on\nImage Classiﬁcation\nIn rectal histopathology deep learning classiﬁcation resear ch,\na large number of labeled pathological images are needed.\nHowever, the preparation of large datasets requires expensive\nlabor costs and time costs, leading to the fact that existing s tudies\nare primarily based on small datasets. In addition, the lack of\nsuﬃcient data leads to overﬁtting problems during the traini ng\nprocess. A conditional sliding windows arithmetic is proposed\nin\nHaryanto et al. (2021) to solve this problem, which generates\nhistopathological images. This arithmetic successfully so lves the\nlimitation of rectal histopathological data.\nIn climate research, the use of deep learning in cloud layer\nanalysis often requires a lot of data. Therefore, classiﬁcati on in\nthe case of a small dataset cannot achieve higher accuracy. I n\norder to solve this problem, a classiﬁcation model with high\naccuracy on small datasets is proposed. The method improves\nfrom three aspects:\n1. A network model for a small dataset is designed.\nFrontiers in Microbiology | www.frontiersin.org 2 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFIGURE 1 | EM image classiﬁcation process. ViT, Vision Transformer; Bo TNet, Bottleneck Transformers; DeiT, Data-efﬁcient image T ransformers; T2T -ViT,\nTokens-to-Token Training Vision Transformer; ResNet, Res idual Network; VGG, Visual Geometry Group.\n2. A regularization technique to increase the generalizatio n ability\nof the model is applied.\n3. The average ensemble of models is used to improve the\nclassiﬁcation accuracy.\nTherefore, the model not only has higher accuracy but also ha s\nbetter robustness (\nPhung and Rhee, 2019 ).\nIn deep learning research, small datasets often lead to\nclassiﬁcation over-ﬁtting and low classiﬁcation accuracy .\nAccording to this problem, a kind of deep CNN based transfer\nlearning is designed to solve the problem of the small dataset .\nThis method mainly improves data and models. In terms of\ndata, the model transfers the feature layer of the CNN model\npre-trained on big sample dataset to a small sample dataset. In\nterms of model, the whole series average pooling is used inste ad\nof the fully connected layer, and Softmax is used for classiﬁ cation.\nThis method has a good classiﬁcation performance on small\nsample datasets (\nZhao, 2017).\nBecause of the limited training data, a two-phase classiﬁcat ion\nmethod using migration learning and web data augmentation\ntechnology is proposed. This method increases the number of\nsamples in the training set through network data augmentatio n.\nIn addition, it reduces the requirements on the number of\nsamples through transfer learning. This classiﬁer reduces t he\nover-ﬁtting problem while improving the generalization abili ty\nof the network (\nHan et al., 2018 ).\nIn radar image recognition, due to the complex environment\nand particular imaging principles, Synthetic Aperture Radar\n(SAR) images have the problem of sample scarcity. A target\nrecognition method of SAR image based on Constrained Naive\nGenerative Adversarial Networks and CNN is proposed to solve\nthis problem. This method combines Least Squares Generative\nAdversarial Networks and designs a shallow network structu re\nbased on the traditional CNNs model. The problem of high\nmodel complexity and over-ﬁtting caused by the deep network\nstructure is avoided, to improve the recognition performance .\nThis method can better solve the problems of few image samples\nand intense speckle noise (\nMao et al., 2021 ).\nLack of suﬃcient training data can seriously deteriorate\nthe performance of neural networks and other classiﬁers. Due\nto this problem, a self-aware multi-classiﬁer system suitab le\nfor “small data” cases is proposed. The system uses Neural\nNetwork, Support Vector Machines (SVMs) and Naive Bayes\nmodels as component classiﬁers. In addition, this system uses t he\nconﬁdence level as a criterion for classiﬁer selection. The s ystem\nperforms well in various test cases and is incredibly accurat e on\nsmall datasets (\nKholerdi et al., 2018 ).\nConvolutional Neural Networks are very eﬀective for face\nrecognition problems, but training such a network requires\na large number of labeled images. Such large datasets are\nusually not public and challenging to collect. According to t his\nsituation, a method based on authentic face images to synthe size\na vast training set is proposed. This method swaps the facial\ncomponents of diﬀerent face images to generate a new face.\nThis technology achieves the most advanced face recognitio n\nFrontiers in Microbiology | www.frontiersin.org 3 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nperformance on the Labeled Faces in the Wild (LFW) face\ndatabase (\nHu et al., 2017 ).\nThe eﬀectiveness of tuning the number of convolutional layer s\nto classify small datasets is proven in Chandrarathne et al. (2020) .\nIn addition, related experiments suggest that by employing a v ery\nlow learning rate (LR), the accuracy of classiﬁcation of smal l\ndatasets can be greatly increased.\nIn medical signal processing, very small datasets often lead to\nthe problems of model overﬁtting and low classiﬁcation accur acy.\nAccording to this situation, a method combining deep learni ng\nand traditional machine learning is proposed. This method\nuses the ﬁrst few layers of CNN for feature extraction. Then,\nthe extracted features are fed back to traditional supervise d\nlearning algorithms for classiﬁcation. This method can avo id\nthe overﬁtting problem caused by small datasets. In addition ,\nit has better performance than traditional machine learning\nmethods (\nAlabandi, 2017).\n2.2. Deep Learning Techniques\nDue to the excellent performance of AlexNet in the\nimage classiﬁcation competition (\nKrizhevsky et al., 2012 ),\nimprovements in the CNN architecture are very active. A serie s\nof CNN-based networks continue to appear, making CNN an\nirreplaceable mainstream method in the ﬁeld of computer vision .\nIn recent years, Transformer frequently appears in computer\nvision tasks and provides good performance, which is suﬃcient\nto attract the attention of researchers.\n2.2.1. Convolutional Neural Networks\nAlexNet is the ﬁrst large-scale CNN architecture to perform w ell\nin ImageNet classiﬁcation. The innovation of the network li es\nin the successful application of the Rectiﬁed Linear Unit (Relu)\nactivation function and the use of the Dropout mechanism and\ndata enhancement strategy to prevent overﬁtting. To improve t he\nmodel generalization ability, the network uses a Local Respon se\nNormalization layer. In addition, the maximum pooling of\noverlap is used to avoid the blurring eﬀect caused by average\npooling (\nKrizhevsky et al., 2012 ).\nThe Visual Geometry Group of Oxford proposes the\nVGG network. The network uses a deeper network structure\nwith depths of 11, 13, 16, and 19 layers. Meanwhile, VGG\nnetworks use a smaller convolution kernel (3 × 3 pixels)\ninstead of the larger convolution kernel, which reduces the\nparameters and increases the expressive power of the networks\n(\nSimonyan and Zisserman, 2014 ).\nGoogLeNet is a deep neural network model based on the\nInception module launched by Google. The network introduces\nan initial structure to increase the width and depth of the\nnetwork while removing the fully connected layer and using\naverage pooling instead of the fully connected layer to avoid the\ndisappearance of the gradient. The network adds two additiona l\nsoftmax to conduct the gradient forward (\nSzegedy et al., 2015 ).\nResNet solves the “degradation” problem of deep neural\nnetworks by introducing residual structure. ResNet network s use\nmultiple parameter layers to learn the representation of resid uals\nbetween input and output, rather than using parameter layers to\ndirectly try to learn the mapping between input and output as\nVGGs networks do. Residual networks are characterized by eas e\nof optimization and the ability to improve accuracy by adding\nconsiderable depth (\nHe et al., 2016 ).\nThe DenseNet network is inspired by the ResNet network.\nDenseNet uses a dense connection mechanism to connect all\nlayers. This connection method allows the feature map learn ed\nby each layer to be directly transmitted to all subsequent la yers as\ninput, so that the features and the transmission of the gradie nt is\nmore eﬀective, and the network is easier to train. The network\nhas the following advantages: it reduces the disappearance of\ngradients, strengthens the transfer of features, makes mor e\neﬀective use of features, and reduces the number of parameters\nto a certain extent (\nHuang et al., 2017 ).\nThe inception-V3 network is mainly improved in two aspects.\nFirstly, branch structure is used to optimize the Inception\nModule; secondly, the larger two-dimensional convolution\nkernel is unpacked into two one-dimensional convolution\nkernels. This asymmetric structure can deal with more\nand richer spatial information and reduce the computation\n(\nSzegedy et al., 2016 ).\nXception is an improvement of Inception-V3. The network\nproposes a novel Depthwise Separable Convolution allign them\nin column, the core idea of which lies in space transformation and\nchannel transformation. Compared with Inception, Xception ha s\nfewer parameters and is faster (\nChollet, 2017).\nMobileNets and Xception have the same ideas but diﬀerent\npursuits. Xception pursues high precision, but MobileNets\nis a lightweight model, pursuing a balance between model\ncompression and accuracy. A new unit Inverted residual with\nlinear bottleneck is applied in MobileNet-V2. The inverse\nresidual ﬁrst increases the number of channels, then perform s\nconvolution and then increases the number of channels. This can\nreduce memory consumption (\nSandler et al., 2018 ).\nShuﬄeNet makes some improvements based on MobileNet.\nThe 1 × 1 convolution used by MobileNet is a traditional\nconvolution method with a lot of redundancy. However,\nShuﬄeNet performs shuﬄe and group operations on 1 ×\n1 convolution. This operation implements channel shuﬄe\nand pointwise group convolution. In addition, this operation\ndramatically reduces the number of model calculations while\nmaintaining accuracy (\nMa et al., 2018 ).\nThe Inception-ResNet network is inspired by ResNet, which\nintroduces the residual structure of ResNet in the Inception\nmodule. Adding the residual structure does not signiﬁcantly\nimprove the model eﬀect. But the residual structure helps to\nspeed up the convergence and improve the calculation eﬃciency .\nThe calculation amount of Inception-ResNet-v1 is the same\nas that of Inception-V3, but the convergence speed is faster\n(\nSzegedy et al., 2017 ).\n2.2.2. Visual Transformers\nThe ViT model applies transformers in the ﬁeld of natural\nlanguage processing to the ﬁeld of computer vision. The main\ncontribution of this model is to prove that CNN is not the\nonly choice for image classiﬁcation tasks. Vision transform er\ndivides the input image into ﬁxed-size patches and then obtain s\npatch embedding through a linear transformation. Finally, t he\nFrontiers in Microbiology | www.frontiersin.org 4 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\npatch embeddings of the image are sent to the transformer to\nperform feature extraction to classiﬁcation. The model is mo re\neﬀective than CNN on super-large-scale datasets and has high\ncomputational eﬃciency (\nDosovitskiy et al., 2020 ).\nThe BoTNet is proposed by Srinivas. This network introduces\nself-attention into ResNet. Therefore, BoTNet has both the l ocal\nperception ability of CNN and the global information acquisit ion\nability of Transformer. The top-1 accuracy on ImageNet is as\nhigh as 84.7%, and the performance is better than models such\nas SENet and Eﬃcient-Net (\nSrinivas et al., 2021 ).\nT2T-ViT is an upgraded version of ViT. It proposes a novel\nTokens-to-Token mechanism based on the characteristics an d\nstructure of ViT. This mechanism allows the deep learning mo del\nto model local and global information. The performance of thi s\nmodel is better than ResNet in the ImageNet data test, and the\nnumber of parameters and calculations are signiﬁcantly redu ced.\nIn addition, the performance of its lightweight model is bett er\nthan that of MobileNet (\nYuan et al., 2021 ).\nDeiT is proposed by Touvron et al. The innovation of DeiT is\nproposes a new distillation process based on a distillation tok en,\nwhich has the same function as a class token. It is a token adde d\nafter the image block sequence. The output after the transfor mer\nencoder and the output of the teacher model calculates the los s\ntogether. The training of DeiT requires fewer data and fewer\ncomputing resources (\nTouvron et al., 2020 ).\n2.3. EM Image Classiﬁcation\nWith the development of technology, good results are achieve d\nusing computer-aided EM classiﬁcation. In\nKruk et al. (2015) ,\na system for automatic identiﬁcation of diﬀerent species of\nmicroorganisms in soil is proposed. The system ﬁrst separates\nmicroorganisms from the background using the Otsu. Then\nshape features, edge features, and color histogram features are\nextracted. Then the features are ﬁltered using a fast correl ation-\nbased ﬁlter. Finally, the random forest (RF) classiﬁer is use d for\nclassiﬁcation. This system frees researchers from the tedi ous task\nof microbial observation.\nIn\nAmaral et al. (1999) , a semi-automatic microbial\nidentiﬁcation system is proposed. The system can accurately\nidentify seven species of protozoa commonly found in\nwastewater. The system ﬁrst enhances the image to be processe d\nand then undergoes data collection and complex morphological\noperations to generate a 3D model of EMs. The 3D model is used\nto determine the species of protozoa. In\nAmaral et al. (2008) ,\na semi-automatic image analysis procedure is proposed. It is\nfound that geometric features have good recognition abilit y.\nIt is possible to detect the presence of two microorganisms,\nOpercularia and Vorticella, in wastewater plants. In\nChen\nand Li (2008) , an improved neural network classiﬁcation\nmethod based on microscopic images of sewage bacteria is\nproposed. The method uses principal component analysis to\nreduce the extracted EM features. Also, the method applies the\ndaptivate accelerated back propagation (BP) algorithm to lear n\nimage classiﬁcation.\nAn automatic classiﬁcation method with high robustness of\nEMs is suggested in\nLi et al. (2013) , which describes the shape\nof EMs in microscopic images by Edge Histograms, Extended\nGeometrical Features, etc. The support vector machine classiﬁ er\nis used to achieve the best classiﬁcation result of 89.7%. A sha pe-\nbased method for EM classiﬁcation is suggested in Yang et al.\n(2014), which introduces very robust two-dimensional feature\ndescriptors for EM shapes. The main process of this method is\nto separate EMs from the background. Then a new EM feature\ndescriptor is used and ﬁnally a SVM is used for classiﬁcation.\nA new method for automatic classiﬁcation of bacterial colony\nimages is proposed in\nNie et al. (2015) , which enables the\nclassiﬁcation of colonies in diﬀerent growth stages and cont exts.\nIn addition, the method mainly uses a multilayer middle layer\nCNN model for classiﬁcation and uses the patches segmented\nfrom the CDBN model as input. Finally, a voting scheme is used\nfor prediction. The results show that the method achieves res ults\nthat exceed the classical model.\n3. MATERIALS AND METHODS\nThis section explains the EMDS-6 dataset, data augmentation\nmethods, the distribution of the dataset, and the evaluatio n\nmetrics for classiﬁcation.\n3.1. Dataset\n3.1.1. Data Description\nThis experiment uses Environmental Microorganism Dataset\n6th Version (EMDS-6) to compare model performance. The\ndataset contains a total of 840 EM images of diﬀerent sizes.\nThese images contain a total of 21 types of EMs, each with\n40 images, namely: Actinophrys, Arcella, Aspidisca, Codosiga,\nColpoda, Epistylis, Euglypha, Paramecium, Rotifera, Vorticella,\nNoctiluca, Ceratium, Stentor, Siprostomum, K. Quadrala, Euglena,\nGymnodinium, Gymlyano, Phacus, Stylongchia, Synchaeta. Some\nexamples are shown in Figure 2 (\nZhao et al., 2021 ).\n3.1.2. Data Preprocessing\nIn order to improve the accuracy of the model and reduce\nthe degree of model overﬁtting, the images in EMDS-6 are\naugmented. Due to the security problem of data augmentation,\nthe only geometric transformation of the data is performed he re.\nThe geometric transformation includes rotation 90 ◦, 180 ◦, and\n270◦, up and down mirroring, and left and right mirroring.\nThese transformations do not break the EM label and ensure da ta\nsecurity. In addition, the image sizes in EMDS-6 is inconsiste nt,\nbut the input required by the deep learning models is the same.\nTherefore, all images in EMDS-6 are standardized to 224 ×\n224 pixels.\n3.1.3. Data Settings\nExperiment A: Randomly select 37.5% of the dataset as the\ntraining set, 25% as the validation set, and 37.5% as the test set.\nExperiment A is to directly perform classiﬁcation tasks on 21\ntypes of microorganisms through the deep learning model. The\ndetails of the training set, validation set, and test set are shown in\nTable 1.\nExperiment B: Randomly select 37.5% of the dataset\nas the training set, 25% as the validation set, and 37.5%\nas the test set. Speciﬁcally, 21 types of microorganisms\nFrontiers in Microbiology | www.frontiersin.org 5 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFIGURE 2 | Example images of EMDS-6. (a) Actinophrys, (b) Arcella, (c) Aspidisca, (d) Codosiga, (e) Colpoda, (f) Epistylis, (g) Euglypha, (h) Paramecium, (i)\nRotifera, (j) Vorticella, (k) Noctiluca, (l) Ceratium, (m) Stentor, (n) Siprostomum, (o) K. Quadrala, (p) Euglena, (q) Gymnodinium, (r) Gymlyano, (s) Phacus, (t)\nStylongchia, and (u) Synchaeta.\nare sequentially regarded as positive samples and the\nremaining 20 types of samples are regarded as negative\nsamples. In this way, 21 new datasets are generated.\nFor example, if Actinophrys images are used as positive\nsamples, the remaining 20 types of EMs such as Arcella and\nAspidisca are used as negative samples. Experiment B is\nimbalanced training to assist in verifying the performance o f\nthe model.\nBecause EMDS-6 is a very small dataset, the experimental\nresults are quite contingent. Therefore, 37.5% of the data i s used\nto test the performance of the model to increase the reliabili ty\nof the experiment. This also expresses our sincerity to the\nexperimental results.\n3.2. Evaluation Methods\nTo scientiﬁcally evaluate the classiﬁcation performance of\ndeep learning models, choosing appropriate indicators is a\ncrucial factor. Recall, Precision, Accuracy, F1-score, AP , and\nmean Average Precision (mAP) are commonly used evaluation\nindicators (\nXie et al., 2015 ). The eﬀectiveness of these indicators\nis proven. The Recall is the probability of being predicted\nto be positive in actual positive samples. Precision is the\nprobability of being actual positive in all predicted positive\nsamples. Average Precision refers to the average value of\nrecall rate from 0 to 1. The mAP is the arithmetic average\nof all AP. F1-score is the harmonic value of precision rate\nand recall rate. Accuracy refers to the percentage of correct\nresults predicted in the total sample (\nPowers, 2020 ). The\nspeciﬁc calculation methods of these indicators are shown in\nTable 2.\nIn Table 2, TN is the number of negative classes predicted\nas negative classes, FP represents the number of negative\nclasses predicted as positive classes, FN refers to the\nTABLE 1 | Dataset details of EMDS-6.\nClass\\Dataset Train Val Text Total\nActinophrys 15 10 15 40\nArcella 15 10 15 40\nAspidisca 15 10 15 40\nCodosiga 15 10 15 40\nColpoda 15 10 15 40\nEpistylis 15 10 15 40\nEuglypha 15 10 15 40\nParamecium 15 10 15 40\nRotifera 15 10 15 40\nVorticella 15 10 15 40\nNoctiluca 15 10 15 40\nCeratium 15 10 15 40\nStentor 15 10 15 40\nSiprostomum 15 10 15 40\nK.Quadrala 15 10 15 40\nEuglena 15 10 15 40\nGymnodinium 15 10 15 40\nGonyaulax 15 10 15 40\nPhacus 15 10 15 40\nStylongchia 15 10 15 40\nSynchaeta 15 10 15 40\nTotal 315 210 315 840\nnumber of positive classes predicted as negative classes,\nand TP is the number of positive classes predicted as\npositive classes.\nFrontiers in Microbiology | www.frontiersin.org 6 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 2 | Evaluation metrics for image classiﬁcation. Sample classiﬁc ation (K),\nnumber of positive samples (M).\nAssessments Formula\nPrecision ( P) TP\nTP+FP\nRecall ( R) TP\nTP+FN\nF1-score 2 × P×R\nP+R\nAccuracy TP+TN\nTP+TN+FP+FN\nAP 1\nM\n∑ M\ni=1 Precisionmax (i)\nmAP 1\nK\n∑ K\nj=1 AP (j)\nTABLE 3 | Computer hardware conﬁguration.\nHardware Product number\nCPU Intel Core i7-10700\nGPU NVIDIA Quadro RTX 4000\nMotherboard HP 8750 (LPC Controller-0697)\nRAM SAMSUNG DDR4 3200MHz\nSSD HP SSD S750 256GB\nTABLE 4 | Deep learning hyper-parameters.\nParameter Value\nBatch Size 32\nEpoch 100\nLearning 0.002\nOptimizer Adam\n4. COMPARISON OF CLASSIFICATION\nEXPERIMENTS\n4.1. Experimental Environment\nThis comparative experiment is performed on the local\ncomputer. The computer hardware conﬁguration is shown in\nTable 3. The computer software conﬁguration is as follows:\nWin10 Professional operating system, Python 3.6, and Pytorc h\n1.7.1. In addition, the code runs in the integrated developme nt\nenvironment Pycharm 2020 Community Edition.\nThis experiment mainly uses some classic deep learning\nmodels and some relatively novel deep learning models. The\nhyper-parameters uniformly set by these models are shown in\nTable 4.\n4.2. Experimental Results and Analysis\n4.2.1. The Classiﬁcation Performance of Each Model\non the Training and Validation Sets\nFigure 3 shows the accuracy and loss curves of the CNNs and\nVT series models. Table 5 shows the performance indicators of\ndiﬀerent deep learning models on the validation set. Accordi ng to\nFigure 3 and Table 5, the performance of diﬀerent deep learning\nmodels using small EM dataset cases is brieﬂy evaluated.\nAs shown in Figure 3, the accuracy rate of the training\nset is much higher than that of the validation set of each\nmodel. Densenet169, Googlenet, Mobilenet-V2, ResNet50, ViT,\nand Xception network models are particularly over-ﬁtted.\nIn addition, AlexNet, InceptionResnetV1, ShuﬄeNet-V2, and\nVGG11 network models do not show serious overﬁtting. Among\n21 models in Table 5, the accuracy rates of the Deit, ViT, and T2T-\nViT models are at the 10th, 12th, and 14th. The VT models are in\nthe middle and downstream position among the 21 models.\nThe Xception network model has the highest accuracy,\nprecision, and recall rates in the test set results, which are 4 0.32,\n49.71, and 40.33%. The AlexNet, ViT, and ShuﬄeNet-V2 network\nmodels require the shortest training time, which are 711.64,\n714.56, and 712.95 s. In addition, the ShuﬄeNet-V2 network\nmodel has the smallest parameter amount, which is 1.52 MB.\nVGG16 and VGG19 networks cannot converge in EMDS-6\nclassiﬁcation task. The VGG13 network model has the lowest\naccuracy, precision, and recall rates in the validation set r esults,\nwhich are 20.95, 19.23, and 20.95%. The VGG19 network model\nrequires the longest training time, which is 1036.68 s. In ad dition,\nthe VGG19 network model has the largest amount of parameters,\nwhich is 521 MB.\nXception is a network with excellent performance in\nEMDS-6 classiﬁcation. In the Xception network accuracy curve ,\nthe accuracy of the Xception network training set is rising\nrapidly, approaching the highest point of 90% after 80 epochs.\nMeanwhile, the accuracy of the validation set is close to the\nhighest point 45%, after 30 epochs. In addition, the Xception\nnetwork training set loss curve declines steadily and approach es\nits lowest point after 80 epochs. But the validation set loss\nbegins to approach the lowest point after 20 epochs and stops\nfalling. VGG13 is a network that performs poorly on EMDS-\n6 classiﬁcation. In the VGG13 network, the accuracy curve\nof the training set and the accuracy curve of the validation\nset have similar trends, and there are obvious diﬀerences\nafter 80 epochs. Meanwhile, the loss of the training set and\nthe loss of the validation set are also relatively close, and\nthere are obvious diﬀerences after 60 epochs. Networks such\nas Xception, ResNet34, and Googlenet are relatively high-\nperformance networks. The training accuracy of these networ ks\nis much higher than the validation accuracy. Furthermore,\nthe validation accuracy is close to the highest point in a few\nepochs. In addition, the training set loss of these networks i s\nusually lower than 0.3 at 100 epochs. VGG11 and AlexNet are\npoorly performing networks. These network training accuracy\ncurves are relatively close to the validation accuracy curv es.\nDisagreements usually occur after many epochs. In addition, the\ntraining set loss of these networks is usually higher than 0.3 a t\n100 epochs.\n4.2.2. The Classiﬁcation Performance of Each Model\non Test Set\nTable 6, shows the performance indicators of each model on\nthe test set, including precision, recall, F1-score, and accu racy.\nMoreover, the confusion matrix of the CNNs and VTs models are\nshown in Figure 4.\nIt is observed from the test set results that the accuracy\nranking of each model remains unchanged. The accuracy rate\nof the Xception network on the test set is still ranked ﬁrst, at\nFrontiers in Microbiology | www.frontiersin.org 7 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFIGURE 3 | The loss and accuracy curves of different deep learning netw orks on the training and validation sets. For example, AlexN et, Botnet, Densenet169,\nGooglenet, InceptionResnet-V1, Mobilenet-V2, ResNet50, S hufﬂeNet-V2, VGG11, VGG16, ViT, and Xception. train-accura te is the accuracy curve of the training set,\ntrain-accurate is the accuracy curve of the validation set, train-loss is the loss curve of the training set, and val-los s is the loss curve of the validation set.\nFrontiers in Microbiology | www.frontiersin.org 8 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 5 | Comparison of classiﬁcation results of different deep learn ing models on the validation set.\nModel Avg. R(%) Avg. P(%) Avg. F1_score(%) Accuracy(%) Para ms Size (MB) Time (s)\nXception 45.71 52.48 44.95 45.71 79.8 996\nResNet34 42.86 45.33 42.31 42.86 81.3 780\nGooglenet 41.90 42.83 40.49 41.91 21.6 772\nDensenet121 40.95 43.61 40.09 40.95 27.1 922\nDensenet169 40.95 43.62 39.89 40.95 48.7 988\nResNet18 40.95 45.55 41.05 40.95 42.7 739\nInception-V3 40.00 45.01 39.70 40.00 83.5 892\nMobilenet-V2 39.52 39.57 37.01 39.52 8.82 767\nInceptionResnetV1 39.05 41.54 37.96 39.05 30.9 800\nDeit 39.05 39.37 37.70 39.05 21.1 817.27\nResNet50 38.57 43.84 38.02 38.57 90.1 885\nViT 37.14 41.02 35.95 37.14 31.2 715\nResNet101 34.76 36.52 32.99 34.76 162 1021\nT2T -ViT 34.29 38.17 34.54 34.28 15.5 825.3\nShufﬂeNet-V2 33.81 33.90 31.68 33.81 1.52 713\nAlexNet 31.90 32.53 29.32 31.91 217 712\nVGG11 31.43 41.20 29.97 31.43 491 864\nBotNet 30.48 32.61 30.06 30.48 72.2 894\nVGG13 20.95 19.23 18.37 20.95 492 957\nVGG16 9.05 1.31 2.10 9.05 512 990\nVGG19 4.76 0.23 0.44 4.76 532 1036\nP denotes Precision, and R represents Recall. (Sort in descending or der of classiﬁcation accuracy).\n40.32%, and is 3.81% higher than the second. Meanwhile, the\naverage accuracy, average recall rate, and average F1-scor e of the\nXception network also remain in the ﬁrst place, at 40.32, 40.33 ,\nand 41.41%. Excluding the non-convergent VGG16 and VGG19\nnetworks, the accuracy of the VGG13 validation set is still r anked\nat the bottom, at 15.55%. However, the ranking of the T2T-ViT\nnetwork on the validation set accuracy rate changes dramati cally.\nThe accuracy rate of the T2T-ViT network is 34.28%, and the\nranking rose from 12th to 5th. In addition, the AP , average re call\nand average F1-score of the T2T-ViT network are 38.17, 34.29 ,\nand 34.54%. Judging from the time consumed for the models, the\nViT model consumes the least time at 3.77 s. On the other hand,\nthe Densenet169 model consumes the most time at 11.13 s.\nFigure 4 depicts the confusion matrix generated by part\nof the test dataset to more intuitively show the classiﬁcati on\nperformance of the CNNs and VTs models on small EM\ndatasets. In Table 6, Xception is the network with the best overall\nperformance, and VGG13 is the network with the worst overall\nperformance. In the confusion matrix of the Xception network,\n127 EM images out of 315 EM images are classiﬁed into the\ncorrect category. In addition, the 11th type of EM classiﬁcat ion\nperforms the best, with 12 EM images are correctly classiﬁed\nand three EM images are misclassiﬁed into other categories.\nMeanwhile, the Xception network performs the worst in the\n13th category of EM classiﬁcation results. Three EM images\nare correctly classiﬁed and 14 EM images are misclassiﬁed in to\nother categories. For the VGG13 network, 49 of the 315 EM\nimages are classiﬁed into the correct category. Among them,\nthe 16th EM classiﬁcation performs best. Six EM images are\ncorrectly classiﬁed, and 9 EM images are mistakenly classiﬁed\ninto other categories. Comparing the CNNs and VTs models, all\nof the models perform well on the 11th EM classiﬁcation and\nperform poorly on the 13th EM classiﬁcation. For example, the\nViT model correctly classiﬁes 9 EM images and 0 EM images in\nthe classiﬁcation of the 11th and 13th class EMs, respectively.\nFigure 4 shows that Xception better classiﬁes the 11th and\n16th types of EM images. ResNet is better at classifying tasks\nof the 11th and 16th types of images. Googlenet is better at\nclassifying the 9th, 17th, and 21st EMs. The overall classiﬁcat ion\nperformance of T2T-ViT is poor. However, there are still\noutstanding performances in the 16th EM classiﬁcation. The\nBotNet hybrid model is good at the 11th type of EM classiﬁcation .\nHowever, the classiﬁcation performance on the 12th and 13th\nimages is abysmal. ResNet is good at image classiﬁcation in the\n9th, 11th, and 17th categories. The ViT model is good at the 11 th,\n12th, and 17th EM image classiﬁcation. It is found from Figure 4\nthat the images that each model is good at classifying are not\nthe same. Therefore, there is a certain degree of complementa rity\namong diﬀerent deep learning models.\nFrom Figure 4, Xception and Googlenet are highly\ncomplementary. For example, Googlenet has a good performance\nin the classiﬁcation of EMs in classes 17 and 21, but Xception\nhas a poor performance in the classiﬁcation of EMs in classes\n17 and 21. In addition, Xception is better at classifying the 11 th\nclass of EM images than Googlenet. This result shows that the\nfeatures extracted by the two models are quite diﬀerent. Two\nnetworks can extract features that each other network canno t\nextract. Therefore, there is a strong complementarity betwe en\nFrontiers in Microbiology | www.frontiersin.org 9 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 6 | Comparison of classiﬁcation results of different deep learn ing models on the test set.\nModel Avg. R(%) Avg. P(%) Avg. F1_score(%) Accuracy(%) Para ms Size (MB) Time (s)\nXception 40.33 49.71 41.41 40.32 79.8 5.63\nResNet34 36.51 42.92 36.22 36.51 81.3 6.14\nGooglenet 35.23 37.70 34.21 35.24 21.6 5.97\nMobilenet-V2 34.29 38.21 33.07 34.29 8.82 5.13\nT2T -ViT 34.29 38.17 34.54 34.28 15.5 4.44\nDensenet169 33.65 36.55 33.79 33.65 48.7 11.13\nInceptionResnetV1 33.64 35.71 32.90 33.65 30.9 5.11\nResNet18 33.33 38.10 32.36 33.33 42.7 4.92\nResNet50 33.33 40.98 33.44 33.33 90.1 6.23\nDensenet121 33.01 39.20 33.79 33.02 27.1 9.27\nDeit 32.39 34.40 32.74 32.38 21.1 5.43\nViT 31.75 33.84 31.47 31.74 31.2 3.77\nInception-V3 31.11 34.84 31.32 31.11 83.5 7.49\nResNet101 27.94 34.59 28.31 27.94 162 8.83\nVGG11 27.61 29.64 26.00 27.62 491 4.98\nShufﬂeNet-V2 27.30 25.02 24.98 27.30 1.52 5.42\nBotNet 25.40 29.65 26.04 25.39 72.2 6.5\nAlexNet 24.44 23.98 22.65 24.44 217 3.9\nVGG13 15.55 15.18 14.38 15.55 492 5.28\nVGG16 8.26 1.28 1.93 8.25 512 5.79\nVGG19 4.76 0.23 0.44 4.76 532 6.42\nP denotes Precision, and R represents Recall. (Sort in descending or der of classiﬁcation accuracy).\nthe two features. In addition, although VGG11 performs poorly\nin the classiﬁcation of EMs. However, VGG11 is better at class 1\nand class 19 classiﬁcation tasks than Resnet34. Therefore, t here\nis still a certain complementarity between the features extr acted\nby the two models. This complementarity makes it possible to\nimprove model performance through feature fusion.\nIn the study, we combine 18 models in pairs. Regardless of the\nspeciﬁc feature fusion method or the possibility of a particula r\nimplementation, we calculate the ideal performance of the two\nmodels after fusion based on the current results. Part of the\nresults is shown in Table 7. All results of the table are in the\nappendix. In Table 7, the ideal accuracy rate of each combination\nis calculated by the following steps. For each combination, t he\nbest results of every model are ﬁrstly accumulated. Then, th e\naccumulated results are divided by the total number of image s\nin the test set, and the result is the ideal accuracy rate. For\nexample, the combination of Xception and Googlenet. In class\n1 EM classiﬁcation, Xception correctly classiﬁes four image s,\nand Googlenet correctly classiﬁes ﬁve images. Here, 5 are th e\nbest results. The other categories can be deduced by analogy.\nThe calculation method of model performance improvement\nis as follows: Use the ideal accuracy to subtract the highest\naccuracy of the two models to obtain the performance that can\nbe improved in the ideal state after the fusion. In Table 7, the\nfusion of Xception and Googlenet performs best on the EMDS-\n6, with a classiﬁcation accuracy of 46.03%. However, ResNet10 1\nand VGG11 are improved the most after the fusion, and the\ntwo models have the strongest complementarity. On the left\nside of Table 7, we can clearly see the ideal eﬀect of improving\nthe accuracy after the fusion of the two features. The improve d\naccuracy after fusion reﬂects the complementarity of the two\nmodels to some extent. This complementarity can provide some\nhelp to researchers who are engaged in feature fusion.\n4.3. Extended Experiments\n4.3.1. After Data Augmentation, the Classiﬁcation\nPerformance of Each Model on the Validation Set\nIn this section, we augment the dataset, and the performance\nindicators of the models are calculated and exhibited in\nTable 8, including precision, recall, F1-score, and accuracy. In\naddition, we compare the accuracy changes before and after da ta\naugmentation, as shown in Figure 5.\nAfter data augmentation, the time required for model traini ng\nalso increases signiﬁcantly. The training time of the ViT mo dels\nis the least, which is 902.27 s. Although the training set is\naugmented to six times, the training time of the ViT models is\nincreased by 187.27 s compared with the 715 s. The classiﬁcatio n\naccuracy of the Xception network ranks ﬁrst at 52.62%. The\nT2T-ViT network has the lowest classiﬁcation rate of 35.56%.\nAfter data augmentation, the classiﬁcation performance of\neach model is improved. Figure 5 shows the changes in the\naccuracy of each model after data augmentation. The validat ion\nset accuracy of the VGG16 network is increased the most, at\n28.41%. This is because the VGG16 network can converge on\nthe augmentation dataset. In addition, the validation set a ccuracy\nof VGG13 and VGG11 are improved signiﬁcantly, increasing\nby 21.59 and 16.67%, respectively. The accuracy of the VGG11\nvalidation set rose from 17th to 3th. The accuracy of the VGG1 3\nFrontiers in Microbiology | www.frontiersin.org 10 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFIGURE 4 | Confusion matrix comparison of different networks on test s et, Xception, Resnet34, Googlenet, T2T -ViT, BotNet, VGG13 ,ResNet18,ViT, and VGG11. (In\nthe confusion matrix, 01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11 , 12, 13, 14, 15, 16, 17, 18, 19, 20, 21 represent Actinophrys, Arcella, Aspidisca, Codosiga, Colpoda,\nEpistylis, Euglypha, Paramecium, Rotifera, Vorticella, N octiluca, Ceratium, Stentor, Siprostomum, K. Quadrala, Eu glena, Gymnodinium, Gymlyano, Phacus,\nStylongchia, and Synchaeta, respectively).\nFrontiers in Microbiology | www.frontiersin.org 11 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 7 | After fusing the two features, it has ideal precision and ide al performance improvement.\nModel Change (up) (%) Model Accuracy\nResNet101 VGG11 9.52 Googlenet Xception 46.03%\nInceptionResnetV1 ResNet18 7.94 Inception-V3 Xception 44.7 6%\nInception-V3 Shufﬂenet-V2 7.62 ResNet50 Xception 44.76%\nShufﬂenet-V2 VGG11 7.62 Deit Xception 44.44%\nDeit VGG11 7.30 Densenet161 Xception 44.13%\nInception-V3 VGG11 7.30 VGG11 Xception 44.13%\nResNet18 ResNet50 7.30 Densenet121 Xception 43.81%\nResNet34 ResNet50 7.30 Mobilenet-V2 Xception 43.81%\nResNet34 VGG11 7.30 ResNet34 ResNet50 43.81%\nResNet101 Shufﬂenet-V2 7.30 ResNet34 VGG11 43.81%\nGooglenet Mobilenet-V2 7.30 Densenet121 ResNet34 43.49%\nAlexnet T2T -ViT 6.98 Googlenet ResNet34 43.49%\nDeit Mobilenet-V2 6.98 InceptionResnetV1 Xception 43.49%\nDeit ViT -5 6.98 Mobilenet-V2 ResNet34 43.49%\nDensenet121 Googlenet 6.98 ResNet18 Xception 43.49%\nThe left side of the table shows the improved accuracy of feature fusion under ideal conditions, and the right side of the table shows the accuracy of fea ture fusion under ideal conditions.\nvalidation set rose from 19th to 11th. After data augmentati on,\nthe validation set accuracy of T2T-ViT, Densenet169, and\nViT are not improved signiﬁcantly, increasing by 1.28, 1.19,\nand 1.91%.\nFrom a speciﬁc series of models, the performance of VGG\nseries models is improved signiﬁcantly after data augmentat ion.\nThe performance improvement of the Densenet series models\nis not apparent. The accuracy of the Densenet121 and the\nDensenet169 validation sets are increased by 1.43 and 1.19%,\nrespectively. Meanwhile, the performance improvement of the\nVT series models is not apparent. The classiﬁcation accuracy o f\nthe T2T-ViT validation set is increased by 1.28%, ViT is incre ased\nby 1.91%, and Diet is increased by 4.28%. In the ResNet series\nmodels, ResNet18, ResNet34, and ResNet50 are increased by 3.49 ,\n3.25, and 3.65%, and the improvement is not obvious. However,\nthe classiﬁcation accuracy of the ResNet101 validation set i s\nincreased by 8.65%, which is obvious.\n4.3.2. After Data Augmentation, the Classiﬁcation\nPerformance of Each Model on the Test Set\nAfter data augmentation, the performance of each model on the\ntest set is shown in Table 9. In Table 9, the Xception network\nhas the highest accuracy of 45.71%. Meanwhile, the Xception\nnetwork has an excellent recall index of 50.43%. Excluding\nthe non-convergent VGG19, the VGG16 model has the worst\nperformance, with an accuracy of 24.76%. The ViT model\nconsumes the least time, which is 3.72 s. The Densenet169 mod el\nconsumes the most time, which is 11.04 s.\nFigure 6 shows the change of accuracy on the test set before\nand after the augmentation. In Figure 6, we can see that the\naccuracy of each deep learning model on the test set is genera lly\nincreased. Among them, the accuracy of the VGG series models\nis improved the most. VGG11 is increased by 9.25%, VGG13\nis increased by 21.28%, and VGG16 is increased by 16.51%.\nHowever, the accuracy of the VT series models test set is not\nsigniﬁcantly improved. The accuracy of some model test sets\neven drops. After data augmentation, the accuracy of the Diet\nnetwork validation set is not changed. The accuracy of the T2 T-\nViT network is dropped by 3.80%. The accuracy of the ViT\nmodel is dropped by 3.17%. However, the accuracy of BotNet,\na mixed model of CNN and VT, is improved signiﬁcantly,\nreaching 11.12%.\n4.3.3. In Imbalanced Training, After Data\nAugmentation, the Classiﬁcation Performance of\nEach Model on the Validation Set\nIn this section, we re-split and combine the data. Take each of the\n21 types of EMs as positive samples in turn and the remaining\n20 types of microorganisms as negative samples. In this way, we\nrepeat this process 21 times in our paper. The speciﬁc splitting\nmethod is shown in Section 3.1.3. The deep learning model\ncan calculate an AP after training each piece of data. Table 10\nshows the AP and mAP of each model validation set. We select\nthe classical VGG16, ResNet50, and Inception-V3 networks for\nexperiments. Furthermore, a relatively novel ViT model is al so\nselected. In addition, the Xception network, which has alway s\nperformed well above, is selected for experiments. Since the\nVGG16 network cannot converge at a LR of 0.0001, this part of\nthe experiment adjusts the LR of the VGG16 network to 0.00001.\nIt can be seen in Table 10 that the mAP of the Xception\nnetwork is the highest, which is 56.61%. The Xception network\nhas the highest AP on the 10th data, and the AP is 82.97%. The\nXception network has the worst AP on the 3rd data, with an AP of\n29.72%. As shown in Figure 7, the confusion matrix (d) is drawn\nby the 10th data. In (d), 46 of the 60 positive samples are classiﬁe d\ncorrectly, and 14 are mistakenly classiﬁed as negative sample s. In\nthe confusion matrix drawn by the third data, 8 of the 60 positiv e\nsamples are classiﬁed correctly, and 52 are incorrectly classi ﬁed\nas negative samples.\nFrontiers in Microbiology | www.frontiersin.org 12 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 8 | Comparison of classiﬁcation results of different deep learn ing models on the validation set.\nModel Avg. R(%) Avg. P(%) Avg. F1_score(%) Accuracy(%) Para ms Size (MB) Time (s)\nXception 52.62 52.05 50.63 52.62 79.80 2636.08\nMobilenet-V2 49.67 51.91 48.82 49.68 8.82 1237.49\nVGG11 48.10 52.40 48.44 48.10 491.00 1745.73\nResNet34 46.10 47.85 44.68 46.11 81.30 1335.87\nResNet18 44.44 51.87 43.03 44.44 42.70 1090.39\nGooglenet 44.29 47.16 43.50 44.29 21.60 1257.33\nInception-V3 43.97 50.78 43.41 43.97 83.50 2004.08\nAlexNet 43.58 45.02 43.05 43.57 217.00 951.27\nResNet101 43.41 46.08 43.33 43.41 162.00 2786.95\nDeit 43.34 46.62 43.29 43.33 21.10 1306.99\nVGG13 42.54 41.38 41.21 42.54 492.00 2307.04\nDensenet121 42.38 46.91 42.39 42.38 27.10 2169.11\nResNet50 42.22 47.76 42.10 42.22 90.10 1968.28\nDensenet169 42.14 48.04 42.79 42.14 48.70 2526.61\nInceptionResnetV1 41.66 47.83 41.68 41.67 30.90 1451.76\nViT 39.05 43.50 38.52 39.05 31.20 902.27\nShufﬂeNet-V2 37.62 39.37 36.84 37.62 1.52 965.81\nVGG16 37.47 38.21 36.80 37.46 512.00 2589.15\nBotNet 36.59 36.38 35.59 36.59 72.20 2000.17\nT2T -ViT 35.56 38.43 36.19 35.56 15.50 1385.62\nVGG19 4.76 0.23 0.44 4.76 532.00 1022.57\nP denotes Precision, and R represents Recall. The training set is a ugmented. (Sort in descending order of classiﬁcation accuracy).\nFIGURE 5 | In the validation set of different deep learning models, the a ccuracy difference between data augmentation and before da ta augmentation.\nFrontiers in Microbiology | www.frontiersin.org 13 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 9 | Comparison of classiﬁcation results of different deep learn ing models on the test set.\nModel Avg. R(%) Avg. P(%) Avg. F1_score(%) Accuracy(%) Para ms Size (MB) Time (s)\nXception 45.71 50.43 46.15 45.71 79.8 5.49\nMobilenet-V2 42.54 47.56 43.07 42.54 8.22 5.04\nResNet18 39.05 44.82 39.22 39.05 42.7 4.90\nDensenet121 38.73 40.28 38.20 38.73 27.1 8.98\nResNet34 38.73 42.25 37.84 38.73 81.3 6.07\nResNet50 38.10 41.56 36.97 38.10 90.1 6.20\nInception-V3 37.78 44.32 38.00 37.78 83.5 7.47\nGooglenet 37.46 43.55 37.92 37.46 21.6 6.03\nDensenet169 37.14 41.51 37.37 37.14 48.7 11.04\nVGG11 37.14 38.81 36.70 37.14 491 4.96\nInceptionResnetV1 36.82 41.47 36.75 36.83 30.9 5.11\nVGG13 36.82 38.46 36.25 36.83 492 5.28\nBotNet 36.50 39.12 36.35 36.51 72.2 6.44\nResNet101 35.23 38.01 35.44 35.24 162 8.85\nAlexNet 34.92 39.10 34.97 34.92 217 5.25\nDeit 32.39 34.40 32.74 32.38 21.1 4.41\nT2T -ViT 30.48 35.88 30.85 30.48 15.50 5.41\nShufﬂeNet-V2 28.57 35.64 29.41 28.57 1.52 5.42\nViT 28.58 29.63 27.86 28.57 31.2 3.72\nVGG16 24.77 25.53 24.11 24.76 512 5.79\nVGG19 4.76 0.23 0.44 4.76 532 6.36\nP denotes Precision, and R represents Recall. The training set is a ugmented. (Sort in descending order of classiﬁcation accuracy).\nFIGURE 6 | In the test set of different deep learning models, the accurac y difference between data augmentation and before data augm entation.\nThe mAp of the VGG16 network is the lowest at 34.69%. The\nVGG16 network performs best on the 10th data AP , with an AP\nof 76.12%. The VGG16 network performs the worst on the 21st\ndata AP , with an AP of 5.47%. Despite tuning the LR, the VGG16\nnetwork still fails to converge on the 3rd, 8th, 13th, 15th, a nd\n21st data.\nFrontiers in Microbiology | www.frontiersin.org 14 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 10 | AP and MAP of different deep learning models in imbalanced tr aining.\nModel/Sample 1 (%) 2 (%) 3 (%) 4 (%) 5 (%) 6 (%) 7 (%) 8 (%) 9 (%) 10 (% ) 11 (%)\nViT 30.77 44.99 18.43 48.51 74.47 76.17 50.98 15.32 31.12 60. 74 54.02\nXception 37.66 51.16 29.72 68.32 73.66 67.96 79.19 65.41 55. 84 82.97 55.91\nVGG16 48.38 41.43 9.63 51.05 52.61 42.23 76.92 5.97 27.57 76. 12 34.77\nResNet50 30.58 45.96 14.24 68.19 66.15 43.10 71.24 46.51 31. 87 62.19 36.79\nInception-V3 37.75 36.79 33.41 56.37 55.77 43.51 59.52 41.18 38.40 75.03 69.26\nModel/Sample 12 (%) 13 (%) 14 (%) 15 (%) 16 (%) 17 (%) 18 (%) 19 (%) 20 (%) 21 (%) mAPA\nViT 15.24 17.84 25.46 6.74 13.95 48.61 7.26 60.33 23.07 9.53 3 4.93\nXception 54.16 52.28 65.06 46.36 30.61 60.41 31.21 61.14 45. 50 74.36 56.61\nVGG16 24.06 16.22 63.90 5.80 10.49 33.87 24.77 44.00 33.14 5. 47 34.69\nResNet50 15.59 42.12 68.57 24.94 17.49 47.52 6.64 49.04 16.7 3 56.10 41.03\nInception-V3 15.09 49.09 64.11 37.91 15.00 43.98 15.84 54.40 10.78 60.38 43.50\n(In [%]).\nThe mAp of the ViT network and the VGG16 network\nare relatively close. The ViT network performs best on the\n6th data AP , with an AP of 76.17%. Among the 60 positive\nsamples, 35 are classiﬁed correctly, and 25 are classiﬁed as\nnegative samples. The ViT network performs the worst on the\n15th data AP , with an AP of 6.74%. Among the 60 positive\nsamples, 0 are classiﬁed correctly and 60 are classiﬁed as\nnegative samples.\nIn addition, Resnet50 performs the best on seven data AP\nand the worst on the 18th data AP. The Inception-V3 network\nperforms best on 10 data AP and the worst on the 16th data AP.\n4.3.4. Mis-classiﬁcation Analysis\nIn the extended experiments, we randomly divide EMDS-6 three\ntimes and train the data for each division. The results and\naccuracy errors of the three experiments are shown in Table 11\nand Figure 8.\nIn Table 11, under the original dataset, Xception has the best\nclassiﬁcation performance on 21 deep learning models. After\ndata augmentation, Xception still has the highest classiﬁca tion\nperformance. In Table 11, the performance of the VGG series\nnetwork has major changes compared to Table 9. In Figure 9,\nwe can clearly understand that VGG11, VGG13, VGG16, and\nVGG19 failed to converge at least once in the three experiment s.\nThis phenomenon causes the VGG series models to fall behind\nin average performance. Except for the VGG series models, the\nperformance of other models tends to be stable on the whole,\nand the errors are kept within ± 5% of the average of the\nthree experiments. Xception and Densenet169 networks show\ngood robustness in the classiﬁcation results before and aft er data\naugmentation. However, the classiﬁcation performance of th e\nAlexNet network ﬂuctuates greatly in the three experiments, and\nthe robustness is poor.\nIn Figure 9, after data augmentation, the performance of\nVGG13 improves the most, but this is mainly caused by\nthe failure of some experiments on the original dataset to\nconverge. In addition to the VGG13 network, the Mobilenet-V2 ,\nShuﬄeNet-V2, and Densenet121 models improve the most, with\naccuracy rates increase by 10.25, 9.52, and 8.89%. In additio n,\nthe performance improvement of ResNet34, ResNet18, and\nInceptionResnetV1 models is relatively small, and the accurac y\nare increase by 2.54, 2.96, and 3.5%. Generally speaking, after\ndata augmentation, the CNN series models have a very obvious\nimprovement in the precision, recall, F1-Score, and accuracy of\nthe test set. However, the opposite situation appeared in the\nVTs after data augmentation. Taking the Accuracy index as an\nexample, the accuracy of the ViT model in the test set has dropped\nby -2.5%, the Accuracy of the T2T-ViT model is equal to that\nbefore the augmentation, and the Accuracy of the Deit model h as\nonly increased by 1.16%.\nIn general, augmenting the dataset through geometric\ntransformation can eﬀectively improve the classiﬁcation\nperformance of the CNN series models. Nevertheless,\nfor the VTs, the method of geometric transformation to\naugment the dataset is diﬃcult to improve the classiﬁcation\nperformance of the VTs and even leads to a decrease in\nmodel performance.\n4.3.5. Comparison of Experimental Results After\nTuning Model Parameters\nIn this section, our extended experiments select representat ive\nmodels, namely the CNN-based Xception, the Transformer-\nbased ViT, and the BotNet hybrid model based on CNN and\nVT. This section of the experiment trains 100 epochs. The\npurpose of the study is to observe the eﬀect of changing two\nhyper-parameters, LR, and batch size (BS), on the experimental\nresults. The experimental results are shown in Table 12.\nUnder the same BS and diﬀerent LRs conditions, the\nmaximum ﬂuctuation of ViT training time is only 4.6 s, the\nmaximum ﬂuctuation of BotNet training time is 74.6 s, and\nthe maximum ﬂuctuation of Xceotion training time is 80.6 s.\nExperiments indicate that tuning LRs has little eﬀect on the\ntime required for training. However, the change of LRs greatl y\ninﬂuences the accuracy of experimental results. Taking the Vi T\nas an example, the accuracy of the model is 16.83% under the\nconditions of BS = 16 and LR = 2 × 10−5. Under the condition\nFrontiers in Microbiology | www.frontiersin.org 15 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFIGURE 7 | The confusion matrices (A–H) are drawn based on the Xception validation set results. Like wise, the confusion matrices (I–P) are drawn based on the ViT\nvalidation set results. (A–H) are generated from datasets 1, 3, 5, 7, 10, 11, 13, 15. (I–P) are generated from datasets 1, 3, 6, 7, 9, 11, 13, and 15, respe ctively.\n(Dataset segmentation is shown in 3.1.3 Experiment B).\nof LR = 2 × 10−4, the highest accuracy of the ViT can reach\n31.11%. In addition, the accuracy of the model is only 3.17%\nunder the condition of LR = 2 × 10−2. Experiments indicate\nthat the performance of the model decreases when using an\noversized LR (LR = 2 × 10−2) and an extremely small LR (LR\n= 2 × 10−5). An oversized LR may cause the network to fail\nto converge, which means the model lingered near the optimal\nvalue and could not reach the optimal solution. This leads to\nperformance degradation. The following two reasons explain t he\nperformance degradation when applying extremely small LRs.\nOn the one hand, an extremely small LR makes the network hard\nto converge fastly. The related experiments show that the mode l\nFrontiers in Microbiology | www.frontiersin.org 16 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nTABLE 11 | Comparison of different deep learning models on test set.\nMode1 Original data Augmented data\nRecall (%) Precision (%) F1-score (%) Accuracy (%) Recall (% ) Precision (%) F1-score (%) Accuracy (%)\nXception 39.37 44.25 39.07 39.37 44.76 47.97 44.53 44.76\nResNet34 37.14 41.96 36.93 37.14 39.68 43.15 39.54 39.68\nResNet18 35.24 40.53 34.33 35.24 38.20 42.64 38.38 38.20\nMobilenet-V2 34.50 37.24 33.86 34.50 44.75 48.31 44.82 44.7 5\nInceptionResnetV1 34.39 36.46 33.92 34.39 37.88 41.09 37.53 37.89\nGooglenet 34.07 36.89 33.48 34.07 40.32 44.59 40.37 40.32\nDeit 32.27 34.08 31.92 33.44 34.60 37.01 34.76 34.60\nInception-V3 33.33 33.78 32.26 33.33 39.79 43.17 39.69 39.79\nViT 33.24 34.92 32.63 33.23 30.69 32.49 30.08 30.69\nDensenet169 32.80 35.38 32.49 32.80 38.73 43.52 38.79 38.73\nResNet50 32.28 36.41 31.79 32.27 38.84 41.69 38.37 38.84\nDensenet121 31.11 35.66 31.25 31.11 40.00 43.02 39.75 40.00\nResNet101 30.90 35.29 30.97 30.90 36.61 38.34 36.01 36.61\nAlexNet 30.26 31.08 28.70 30.26 36.51 39.62 36.41 36.51\nT2T -ViT 29.10 32.84 29.17 29.10 29.10 32.19 29.13 29.10\nBotNet 29.00 31.11 28.46 28.99 33.02 34.29 32.45 33.02\nShufﬂeNet-V2 24.66 23.71 22.86 24.66 34.18 37.09 34.19 34.18\nVGG11 20.74 19.99 18.31 20.74 26.77 26.98 25.39 26.77\nVGG13 8.68 5.66 5.47 8.68 28.78 29.52 27.12 28.78\nVGG16 5.93 0.58 0.94 5.92 11.43 8.66 8.33 11.43\nVGG19 4.76 0.23 0.44 4.76 4.76 0.23 0.44 4.76\n[In (%)].\nFIGURE 8 | Error bar of accuracy on test set. The left ﬁgure shows the test set error bar before data augmentation. The ﬁgure on the right shows the error bar of the\ntest set after data augmentation.\nis diﬃcult to reach the optimal value within 100 epochs with an\nextremely small LR (2 × 10−5). On the other hand, an extremely\nsmall LR may cause the network to fall into an optimal local\nsolution, which leads to performance degradation.\nIn addition to the LR, the change of BS also dramatically\naﬀects the performance of the model. Diﬀerent models show\ndiﬀerent patterns at diﬀerent BS values. For example, the\naccuracy of the ViT model decreases rapidly with increasing\nBS at LR = 2 × 10−5. The accuracy of the BotNet increases\nsharply with increasing BS at LR = 2 × 10−5. However, the\nrelevant experiments show that BS does not seriously aﬀect the\nperformance of the model under large datasets (\nRadiuk, 2017 ).\nNevertheless, with small datasets, only a slight change in t he BS\nvalue can dramatically change the performance of the model.\nCompared to a large dataset, tuning the BS and LR on\na small dataset can signiﬁcantly change model performance.\nTherefore, ﬁnding the optimal parameters to improve the model\nperformance on small datasets is necessary.\nFrontiers in Microbiology | www.frontiersin.org 17 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFIGURE 9 | Error bar of accuracy on test set. The left ﬁgure shows the test set error bar before data augmentation. The ﬁgure on the right shows the error bar of the\ntest set after data augmentation.\nTABLE 12 | Comparison of training time consumption and test set accura cy of different networks.\nLR ViT (Times) ViT (Accuracy)\nBS 4 BS 8 BS 16 BS 32 BS 4 (%) BS 8 (%) BS 16 (%) BS 32 (%)\n2 × 10−5 530.70 793.56 760.76 760.99 28.25 21.59 16.83 14.92\n2 × 10−4 530.32 793.12 761.17 762.88 30.16 27.94 31.11 30.16\n2 × 10−3 530.30 792.43 760.87 761.88 11.43 15.87 20.63 17.14\n2 × 10−2 535.30 794.01 760.67 760.99 4.76 4.76 3.17 7.62\nLR Xception (Times) Xception (Accuracy)\nBS 4 BS 8 BS 16 BS 32 BS 4 (%) BS 8 (%) BS 16 (%) BS 32 (%)\n2 × 10−5 840.31 1106.94 1119.99 1074.65 38.10 37.46 37.14 37.78\n2 × 10−4 834.88 1107.95 1081.05 1088.86 51.43 50.48 41.90 38.73\n2 × 10−3 837.11 1113.56 1042.63 1042.75 23.49 34.29 28.25 30.48\n2 × 10−2 808.83 1086.24 1037.36 1073.62 14.29 16.83 20.00 17.46\nLR BotNet (Times) BotNet (Accuracy)\nBS 4 BS 8 BS 16 BS 32 BS 4 (%) BS 8 (%) BS 16 (%) BS 32 (%)\n2 × 10−5 806.80 1006.82 977.10 950.71 16.51 17.14 20.32 21.27\n2 × 10−4 778.31 990.82 1022.45 1011.02 12.70 24.76 26.67 27.94\n2 × 10−3 772.63 984.33 967.09 937.65 9.21 7.94 14.29 10.79\n2 × 10−2 774.33 985.43 968.46 936.39 7.94 10.79 7.62 16.83\nThe left side of the table shows the training time consumption, while the ri ght side of the table shows the accuracy of the test set. learning rate (LR), B atch Size (BS).\nFrontiers in Microbiology | www.frontiersin.org 18 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\n5. DISCUSSION\nThis experiment studies the classiﬁcation performance of 21 d eep\nlearning models on small EM dataset (EMDS-6). The comparison\nresults are obtained according to the evaluation indicator s, as\nshown in Tables 5, 6, 8, 9. Meanwhile, some models are selected\nfor imbalanced experiments to investigate the performance of\nthe models further. The results are shown in Table 10. In order\nto increase the reliability of the conclusions, this paper repe ats\nthe main experiment three times. The average value is shown\nin Table 11, and the errors of the three experiments are shown\nin Figure 8. In addition, this paper explores the impact of\nhyper-parameters on small dataset classiﬁcation, and the res ults\nare shown in Table 12.\nThe performance of the VGG network gradually\ndecreases as the number of network layers increases.\nEspecially the VGG16 and VGG19 networks cannot\nconverge on EMDS-6. This may be because the dataset is\ntoo small, and the gradient disappears in the process of a\ncontinuous deepening of the network layer, which aﬀects\nthe convergence.\nThe training time of the ViT network on EMDS-6 is very short,\nbut it does not make a signiﬁcant diﬀerence with other models.\nAfter the data augmentation of EMDS-6, the ViT network has\napparent advantages in the time of training the model, and\nthe time consumption is much less than other models. We can\nspeculate that the ViT model may further expand its advantage\nwhen trained on more training data.\nIn the experiments where the model parameters are tuned,\nslight changes in both the LR and BS parameters lead to\ndrastic changes in model performance. This does not happen\nif the experiment is based on a large-scale dataset. However, in\nsmall datasets, each class of EMs only accounts for a portion\nof the image, and most of the others are noise. Moreover,\nsome models that include batch normalization normalize the\nenvironmental noise at diﬀerent BS leading to ﬂuctuations in\nclassiﬁcation accuracy.\nAfter data augmentation, the accuracy of CNN series models\nimproves signiﬁcantly. However, the increase of VT series\nmodel accuracy is slight, and some of them even decrease.\nThe results are shown in Figure 6. To further prove the\nabove experimental results, this paper re-divides the dataset\nand conducts three experiments, and the results are shown\nin Figure 9. Experiments once again prove that the geometric\ndeformation augmented data method is diﬃcult to improve the\nperformance of the VT series models. This may be because\nour data augmentation method only makes geometric changes\nto the data. The geometric transformation is only changed th e\nspatial position of the feature. However, the VT series models\nuse attention to capture the global context information, and it\npays more attention to global information. Operations such as\nrotation and mirroring have little eﬀect on global informati on,\nand it is impossible to learn more global features. This makes\nthe performance of the VT series models unable to improve after\ndata augmentation signiﬁcantly. However, the performance o f\nBotNet, a hybrid model of CNN and VT, is signiﬁcantly improved\nafter data augmentation. This is because the BotNet network\nonly replaced three Bottlenecks with MHSA. The BotNet network\nis essentially more inclined to the feature extraction meth od\nof CNN.\n6. CONCLUSION AND FUTURE WORK\nThe classiﬁcation of small EM datasets are very challenging\nin computer vision tasks, which has attracted the attention o f\nmany researchers. Due to the development of deep learning,\nimage classiﬁcation of small datasets is developing rapidly.\nThis article uses 17 CNN models, three VT models, and\na hybrid CNN and VT model to test model performance.\nWe have performed several experiments, including direct\nclassiﬁcation of each model, classiﬁcation tasks after data\naugmentation, and imbalanced training tasks on some\nrepresentative models. The experimental results prove that\nthe Xception network is suitable for this kind of task. The\nViT models take the least time for training. Therefore, the\nViT model is suitable for large-scale data training. The\nShuﬄeNet-V2 network has the least number of parameters,\nalthough its classiﬁcation performance is average. Therefo re,\nShuﬄeNet-V2 is more suitable for occasions where high\nclassiﬁcation performance is not necessary and limited\nstorage space.\nThis study provides an analysis table of the diﬀerences\nbetween the 18 models. This result can help related research o n\nfeature fusion quickly ﬁnd models with signiﬁcant diﬀerence s\nand improve model performance. In addition, this study ﬁnds\nfor the ﬁrst time that the data augmentation method of geomet ric\ndeformation is extremely limited or even ineﬀective in improv ing\nthe performance of VT series models. This study and conclusion\ncan provide relevant researchers with a conclusion with suﬃc ient\nexperimental support. Our research and conclusions reduce the ir\nworkload in selecting experimental augmentation methods to a\ncertain extent. This has a signiﬁcant reference value.\nAlthough the augmentation method of geometric\ndeformation is eﬀective for the performance improvement of\nCNNs, it does not help much for the performance improvement\nof VTs. We can improve the VT networks performance by\nstudying new data augmentation methods in future work.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study. Thi s data\ncan be found here: https://ﬁgshare.com/articles/dataset/ EMDS-\n6/17125025/1.\nAUTHOR CONTRIBUTIONS\nPZ: experiment, result analysis, and paper writing. CL:\ndata preparation, method, result analysis, paper writing,\nproofreading, and funding support. MR: proofreading. HX and\nHY: experiment. HS: environmental microorganism knowledge\nsupport. TJ: result analysis and funding support. MG: method\nand result analysis. All authors contributed to the article a nd\napproved the submitted version.\nFrontiers in Microbiology | www.frontiersin.org 19 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nFUNDING\nThis work is supported by the National Natural Science\nFoundation of China (No. 61806047), the Scientiﬁc Research\nFund of Sichuan Provincial Science and Technology\nDepartment under Grant (No. 2021YFH0069), and\nProject supported by the State Key Laboratory of Robotics\n(No. 2019-O13).\nREFERENCES\nAlabandi, G. A. (2017). Combining Deep Learning With Traditional Machine\nLearning to Improve Classiﬁcation Accuracy on Small Datase ts. Thesis, Texas\nState University, San Marcos, TX.\nAmaral, A., Baptiste, C., Pons, M.-N., Nicolau, A., Lima, N., Ferreira , E., et al.\n(1999). Semi-automated recognition of protozoa by image analysis. Biotechnol.\nTechniq. 13, 111–118. doi: 10.1023/A:1008850701796\nAmaral, A., Ginoris, Y. P., Nicolau, A., Coelho, M., and Ferreira,\nE. (2008). Stalked protozoa identiﬁcation by image analysis and\nmultivariable statistical techniques. Anal. Bioanal. Chem. 391, 1321–1325.\ndoi: 10.1007/s00216-008-1845-y\nAsgharnejad, H., and Sarrafzadeh, M.-H. (2020). Development of digi tal image\nprocessing as an innovative method for activated sludge biomass q uantiﬁcation.\nFront. Microbiol. 11, 2334. doi: 10.3389/fmicb.2020.574966\nÇayir, A., Yenido ˘gan, I., and Da ˘g, H. (2018). “Feature extraction based\non deep learning for some traditional machine learning methods, ”\nin 2018 3rd International Conference on Computer Science and\nEngineering (UBMK). IEEE (Sarajevo, Bosnia and Herzegovina), 494–497.\ndoi: 10.1109/UBMK.2018.8566383\nChandrarathne, G., Thanikasalam, K., and Pinidiyaarachchi, A. ( 2020). “A\ncomprehensive study on deep image classiﬁcation with small dataset s, ” in\nAdvances in Electronics Engineering, Lecture Notes in Elec trical Engineering, Vol.\n619 (Singapore: Springer), 93–106. doi: 10.1007/978-981-15-1289- 6_9\nChen, C., and Li, X. (2008). “A new wastewater bacteria classiﬁc ation with\nmicroscopic image analysis, ” in Proceedings of the 12th WSEAS International\nConference on Computers (Heraklion), 915–921.\nChollet, F. (2017). “Xception: deep learning with depthwise separab le\nconvolutions, ” inProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (Honolulu, HI), 1251–1258. doi: 10.1109/CVPR.2017.195\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: transformers for imag e\nrecognition at scale. arXiv Preprint arXiv:2010.11929 .\nFan, N., Qi, R., Rossetti, S., Tandoi, V., Gao, Y., and Yang, M . (2017).\nFactors aﬀecting the growth of microthrix parvicella: batch tests u sing\nbulking sludge as seed sludge. Sci. Total Environ. 609, 1192–1199.\ndoi: 10.1016/j.scitotenv.2017.07.261\nFilzmoser, P., and Todorov, V. (2011). Review of robust multivariat e\nstatistical methods in high dimension. Anal. Chim. Acta 705, 2–14.\ndoi: 10.1016/j.aca.2011.03.055\nFried, J., Mayr, G., Berger, H., Traunspurger, W., Psenner, R., a nd Lemmer, H.\n(2000). Monitoring protozoa and metazoa bioﬁlm communities for ass essing\nwastewater quality impact and reactor up-scaling eﬀects. Water Sci. Technol.\n41, 309–316. doi: 10.2166/wst.2000.0460\nHan, D., Liu, Q., and Fan, W. (2018). A new image classiﬁcation me thod using\ncnn transfer learning and web data augmentation. Expert Syst. Appl. 95, 43–56.\ndoi: 10.1016/j.eswa.2017.11.028\nHaryanto, T., Suhartanto, H., Arymurthy, A. M., and Kusmardi, K. (202 1).\nConditional sliding windows: an approach for handling data limitat ion in\ncolorectal histopathology image classiﬁcation. Inform. Med. Unlock. 23, 100565.\ndoi: 10.1016/j.imu.2021.100565\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learn ing for image\nrecognition, ” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (Las Vegas, NV) 770–778. doi: 10.1109/CVPR.2016.90\nHu, G., Peng, X., Yang, Y., Hospedales, T. M., and Verbeek, J. (2017 ). Frankenstein:\nlearning deep face representations using small data. IEEE Trans. Image Process.\n27, 293–303. doi: 10.1109/TIP.2017.2756450\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2 017). “Densely\nconnected convolutional networks, ” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (Honolulu, HI), 4700–4708.\ndoi: 10.1109/CVPR.2017.243\nKholerdi, H. A., TaheriNejad, N., and Jantsch, A. (2018). “Enhan cement of\nclassiﬁcation of small data sets using self-awareness—an iris ﬂow er case-study, ”\nin 2018 IEEE International Symposium on Circuits and Systems (ISCAS ),IEEE\n(Florence), 1–5. doi: 10.1109/ISCAS.2018.8350992\nKosov, S., Shirahama, K., Li, C., and Grzegorzek, M. (2018). Envi ronmental\nmicroorganism classiﬁcation using conditional random ﬁelds and\ndeep convolutional neural networks. Pattern Recogn. 77, 248–261.\ndoi: 10.1016/j.patcog.2017.12.021\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagen et classiﬁcation\nwith deep convolutional neural networks. Adv. Neural Inform. Process. Syst.\n25:1097–1105. doi: 10.1145/3065386\nKruk, M., Kozera, R., Osowski, S., Trzci ´nski, P., Paszt, L. S., Sumorok, B.,\net al. (2015). “Computerized classiﬁcation system for the identi ﬁcation of soil\nmicroorganisms, ” inAIP Conference Proceedings , Vol. 1648 (Melville, NY: AIP\nPublishing LLC), 660018. doi: 10.1063/1.4912894\nLi, C., Shirahama, K., Grzegorzek, M., Ma, F., and Zhou, B. (2013) .\n“Classiﬁcation of environmental microorganisms in microscopic image s using\nshape features and support vector machines, ” in 2013 IEEE International\nConference on Image Processing , IEEE (Melbourne, VIC), 2435–2439.\ndoi: 10.1109/ICIP.2013.6738502\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. (2018). “Shuﬄenet v2 : practical\nguidelines for eﬃcient cnn architecture design, ” in Computer Vision – ECCV\n2018. ECCV 2018. Lecture Notes in Computer Science, Vol. 11218 , eds V.\nFerrari, M. Hebert, C. Sminchisescu, and Y. Weiss (Springer, Cham), 11 6–131.\ndoi: 10.1007/978-3-030-01264-9_8\nMao, C., Huang, L., Xiao, Y., He, F., and Liu, Y. (2021). Target rec ognition of SAR\nimage based on CN-GAN and CNN in complex environment. IEEE Access 9,\n39608–39617. doi: 10.1109/ACCESS.2021.3064362\nMcKinney, R. E. (2004). Environmental Pollution Control Microbiology: A Fifty-\nYear Perspective. Boca Raton, FL: CRC Press. doi: 10.1201/9780203025697\nNie, D., Shank, E. A., and Jojic, V. (2015). “A deep framework for bacterial image\nsegmentation and classiﬁcation, ” in Proceedings of the 6th ACM Conference on\nBioinformatics, Computational Biology and Health Informatic s (Atlanta, GA),\n306–314. doi: 10.1145/2808719.2808751\nPepper, I. L., Gerba, C. P., Gentry, T. J., and Maier, R. M. (2011) . Environmental\nMicrobiology. San Diego,CA: Academic Press.\nPhung, V. H., and Rhee, E. J. (2019). A high-accuracy model averag e ensemble\nof convolutional neural networks for classiﬁcation of cloud image patches on\nsmall datasets. Appl. Sci. 9, 4500. doi: 10.3390/app9214500\nPowers, D. M. (2020). Evaluation: from precision, recall and F-meas ure to ROC,\ninformedness, markedness and correlation. arXiv Preprint arXiv:2010.16061 .\nRadiuk, P. M. (2017). Impact of training set batch size on the perf ormance of\nconvolutional neural networks for diverse datasets. Inform. Technol. Manage.\nSci. 20, 20–24. doi: 10.1515/itms-2017-0003\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. ( 2018).\n“Mobilenetv2: inverted residuals and linear bottlenecks, ” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (Salt Lake City,\nUT), 4510–4520. doi: 10.1109/CVPR.2018.00474\nSarrafzadeh, M. H., La, H.-J., Lee, J.-Y., Cho, D.-H., Shin, S.-Y. , Kim, W.-J.,\net al. (2015). Microalgae biomass quantiﬁcation by digital image processing\nand rgb color analysis. J. Appl. Phycol. 27, 205–209. doi: 10.1007/s10811-014-\n0285-7\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutional n etworks for\nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 .\nSrinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and V aswani,\nA. (2021). Bottleneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605. doi: 10.1109/CVPR46437.2021.01625\nFrontiers in Microbiology | www.frontiersin.org 20 March 2022 | Volume 13 | Article 792166\nZhao et al. Deep Learning Comparison on EMDS-6\nSzegedy, C., Ioﬀe, S., Vanhoucke, V., and Alemi, A. (2017). “In ception-v4,\ninception-resnet and the impact of residual connections on learnin g, ” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligenc e, Vol. 31 (San\nFrancisco, CA).\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al.\n(2015). “Going deeper with convolutions, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (Boston, MA), 1–9.\ndoi: 10.1109/CVPR.2015.7298594\nSzegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., and Wojna, Z . (2016). “Rethinking\nthe inception architecture for computer vision, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (Las Vegas, NV),\n2818–2826. doi: 10.1109/CVPR.2016.308\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jé gou, H. (2020).\nTraining data-eﬃcient image transformers & distillation through at tention.\narXiv preprint arXiv:2012.12877 .\nWang, P., Fan, E., and Wang, P. (2021). Comparative analysis of ima ge\nclassiﬁcation algorithms based on traditional machine learning and deep\nlearning. Pattern Recogn. Lett. 141, 61–67. doi: 10.1016/j.patrec.2020.07.042\nXie, Y., Xing, F., Kong, X., Su, H., and Yang, L. (2015). Beyond clas siﬁcation:\nstructured regression for robust cell detection using convolution al neural\nnetwork. Med. Image Comput. Comput. Assist. Interv. 9351, 358-365.\ndoi: 10.1007/978-3-319-24574-4_43\nYang, C., Li, C., Tiebe, O., Shirahama, K., and Grzegorzek, M. (20 14).\n“Shape-based classiﬁcation of environmental microorganisms, ” in 2014 22nd\nInternational Conference on Pattern Recognition , IEEE (Stockholm), 3374–\n3379. doi: 10.1109/ICPR.2014.581\nYuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., et al. ( 2021). Tokens-\nto-token vit: Training vision transformers from scratch on imagene t. arXiv\npreprint arXiv:2101.11986.\nZhang, Z., Cui, P., and Zhu, W. (2020). Deep learning on graphs: a s urvey. IEEE\nTrans. Knowl. Data Eng. . 34, 249–270. doi: 10.1109/TKDE.2020.2981333\nZhao, P., Li, C., Rahaman, M., Xu, H., Ma, P., Yang, H., et al. (2021) .\nEMDS-6: Environmental microorganism image dataset sixth versio n\nfor image denoising, segmentation, feature extraction, classiﬁ cation\nand detection methods evaluation. arXiv Preprint arXiv: 2112.07111 .\n1–11.\nZhao, T., Liu, M., Zhao, T., Chen, A., Zhang, L., Liu, H., et al. ( 2021).\nEnhancement of lipid productivity in chlorella pyrenoidosa by collecting cells at\nthe maximum cell number in a two-stage culture strategy. Algal Res. 55, 102278.\ndoi: 10.1016/j.algal.2021.102278\nZhao, W. (2017). Research on the deep learning of the small sample dat a\nbased on transfer learning. AIP Confer. Proc. 1864, 020018. doi: 10.1063/1.49\n92835\nConﬂict of Interest: The authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2022 Zhao, Li, Rahaman, Xu, Yang, Sun, Jiang and Grzegor zek. This\nis an open-access article distributed under the terms of the Creative Commons\nAttribution License (CC BY). The use, distribution or repro duction in other forums\nis permitted, provided the original author(s) and the copyri ght owner(s) are credited\nand that the original publication in this journal is cited, i n accordance with accepted\nacademic practice. No use, distribution or reproduction is permitted which does not\ncomply with these terms.\nFrontiers in Microbiology | www.frontiersin.org 21 March 2022 | Volume 13 | Article 792166",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7365666031837463
    },
    {
      "name": "Computer science",
      "score": 0.7133487462997437
    },
    {
      "name": "Convolutional neural network",
      "score": 0.660295844078064
    },
    {
      "name": "Deep learning",
      "score": 0.6008879542350769
    },
    {
      "name": "Machine learning",
      "score": 0.5593957901000977
    },
    {
      "name": "Transformer",
      "score": 0.5177699327468872
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47986310720443726
    },
    {
      "name": "Artificial neural network",
      "score": 0.4603564143180847
    },
    {
      "name": "Feature extraction",
      "score": 0.4375261068344116
    },
    {
      "name": "Contextual image classification",
      "score": 0.4332139492034912
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.42491716146469116
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3521541357040405
    },
    {
      "name": "Data mining",
      "score": 0.3457202613353729
    },
    {
      "name": "Engineering",
      "score": 0.1538124680519104
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I91656880",
      "name": "China Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24201400",
      "name": "Chengdu University of Information Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I9341345",
      "name": "University of Lübeck",
      "country": "DE"
    }
  ],
  "cited_by": 55
}