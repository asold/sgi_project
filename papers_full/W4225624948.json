{
  "title": "LPViT: A Transformer Based Model for PCB Image Classification and Defect Detection",
  "url": "https://openalex.org/W4225624948",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2021459698",
      "name": "Kang An",
      "affiliations": [
        "Hangzhou Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097306761",
      "name": "Yanping Zhang",
      "affiliations": [
        "Gonzaga University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3204240075",
    "https://openalex.org/W6759044181",
    "https://openalex.org/W2132346196",
    "https://openalex.org/W3186516637",
    "https://openalex.org/W3171162369",
    "https://openalex.org/W3118710621",
    "https://openalex.org/W3109915642",
    "https://openalex.org/W3194737599",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2970546341",
    "https://openalex.org/W2975244163",
    "https://openalex.org/W2132971773",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W6640362995",
    "https://openalex.org/W6640425456",
    "https://openalex.org/W6729756640",
    "https://openalex.org/W2180612164",
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2963855133",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963389226",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3135799552",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2914154500",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W3101310341",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "PCB (printed circuit board) is an extremely important component of all electronic products, which has greatly facilitated human life. Meanwhile, tons of PCBs in the waste streams become a waste of resources, which puts the recycling and reuse of PCBs in urgent need. In the manufacturing and recycling of electronic products, the classification of PCBs, recognition of sub-components, and defect detection have been the key technology. Traditional manual detection and classification are subjective and rely on individuals&#x2019; experience. With the development of artificial intelligence, lots of research efforts have been dedicated to the automated detection and recognition of PCBs. In this paper, we propose a transformer-based model, LPViT, for defect detection and classification of PCBs. We conduct the defect detection task on the dataset DeepPCB, which consists of six different types of PCB defects. Defect detection benefits both manufacturing and recycling of PCBs. Among many electronic products, a group of affordable, general-purpose, and small-size PCBs is very popular, which are referred to as micro-PCBs. The classification and recognition of those PCBs will greatly facilitate the recycling and reuse process. We conduct the classification task on a dataset called <bold>micro-PCB</bold>, which includes 12 types of popular, general-purpose, affordable, and small PCBs. Through comparative experiments, our system demonstrates its advantage in both classification and defect detection tasks.",
  "full_text": "Received April 2, 2022, accepted April 11, 2022, date of publication April 20, 2022, date of current version April 27, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3168861\nLPViT: A Transformer Based Model for PCB\nImage Classification and Defect Detection\nKANG AN\n 1 AND YANPING ZHANG2\n1Qianjiang College, Hangzhou Normal University, Hangzhou 311121, China\n2Department of Computer Science, Gonzaga University, Spokane, WA 99258, USA\nCorresponding author: Kang An (q0070031@huqc.edu.cn)\nThis work was supported by the Hangzhou Normal University Science Foundation under Grant 2022QJJL08.\nABSTRACT PCB (printed circuit board) is an extremely important component of all electronic products,\nwhich has greatly facilitated human life. Meanwhile, tons of PCBs in the waste streams become a waste of\nresources, which puts the recycling and reuse of PCBs in urgent need. In the manufacturing and recycling\nof electronic products, the classiﬁcation of PCBs, recognition of sub-components, and defect detection\nhave been the key technology. Traditional manual detection and classiﬁcation are subjective and rely on\nindividuals’ experience. With the development of artiﬁcial intelligence, lots of research efforts have been\ndedicated to the automated detection and recognition of PCBs. In this paper, we propose a transformer-\nbased model, LPViT, for defect detection and classiﬁcation of PCBs. We conduct the defect detection task\non the dataset DeepPCB, which consists of six different types of PCB defects. Defect detection beneﬁts both\nmanufacturing and recycling of PCBs. Among many electronic products, a group of affordable, general-\npurpose, and small-size PCBs is very popular, which are referred to as micro-PCBs. The classiﬁcation and\nrecognition of those PCBs will greatly facilitate the recycling and reuse process. We conduct the classiﬁcation\ntask on a dataset called micro-PCB, which includes 12 types of popular, general-purpose, affordable, and\nsmall PCBs. Through comparative experiments, our system demonstrates its advantage in both classiﬁcation\nand defect detection tasks.\nINDEX TERMS Classiﬁcation, defect detection, label smooth, micro-PCB, DeepPCB, transformer, mask\npatch prediction, recognition.\nI. INTRODUCTION\nIn 1925, Charles Ducas of the United States printed out the\nline pattern on an insulated substrate and then plated it to\nbuild up the wires, which started a new era of the modern\nPCB (printed circuit board) technology. At the end of the 20th\ncentury, Rigid-Flex, buried resistance, buried capacitance,\nmetal substrates, and other new technologies continued to\nemerge. Right now, PCB has become an extremely important\ncomponent of all electronic products and plays a pivotal role\nin electronic industry. The electronic industry is driven by\nMoore’s Law. Products are becoming more and more pow-\nerful and integrated. The signal rate is getting faster and the\nproduct development cycle is getting shorter. As electronic\nproducts continue to miniaturize, PCB design faces various\nchallenges brought by high speed and high density.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Zhouyang Ren\n.\nOn the other hand, with the increasing usage of electronic\ndevices, more and more PCBs become wastes in the waste\nstreams. Many of them are functional or contain functional\ncomponents (e.x., tantalum capacitors and ceramic ﬁlters)\nthat are expensive and long-lasting. They also contain valu-\nable chemical elements (rare earth elements, gallium, etc)\nnecessary for producing electronic products. The recent auto\ncrisis during COVID-19 caused by the shortage of chips\nand small electronic goods makes those resources even more\nprecious. Therefore recycling and reuse of PCBs are getting\nmore attention.\nDuring the manufacturing, recycling, and reuse of elec-\ntronic products, the classiﬁcation of PCBs, recognition of\nelectronic components on PCBs, and defect detection have\nbeen the key technology. In practice, there are a massive\nnumber of different PCB makes and models. With the devel-\nopment of machine learning and deep learning technologies,\nthere has been plenty of research on classiﬁcation and recog-\nnition in the ﬁeld of computer vision using intelligent models.\n42542 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 10, 2022\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nThe classiﬁcation and recognition of PCBs have some simi-\nlarities with the problems in computer vision. However, there\nare much more diversities in PCBs. For example, compared\nto the human face, PCBs have electronic components (like\ncapacitors, resistors, and integrated circuits) organized fol-\nlowing various patterns. Even on a small PCB, there are\nmany more features and those features are quite similar.\nEspecially, some modern capacitors and resistors are small\nsurface-mounted, which almost look the same while eyes,\nnoses, and mouths are relatively easier to identify.\nThese years, lots of research efforts have been dedicated to\nthe automated classiﬁcation and recognition of components\non PCBs while most of the research is more focused on clas-\nsical datasets such as ImageNet [1]. However, as explained\nin [2], only a few PCB makes and models could be far more\ncommon. A typical candidate for these ‘‘far more common’’\nPCBs would be general-purpose, affordable, and small PCBs,\nwhich are referred to as micro-PCBs [2]. The dataset we\nuse in the classiﬁcation task is called micro-PCB [2]. If the\nmake and model of a PCB could be identiﬁed, we could\nconveniently identify the PCBs that we want to reuse. The\nsub-components used in it could also be directly recognized\nthrough a bill of materials. Meanwhile, defect detection has\nbeen necessary in real manufacturing and recycling process.\nIt’s challenging to conduct manual detection. To improve\nefﬁciency, it is necessary to develop an automated defect\ndetection system.\nIn this research, we propose a new transformer-based\nmodel called LPViT and its corresponding training strategy to\nachieve accurate classiﬁcation and defect detection on PCBs,\nand improve the robustness of the model. Our model can be\nused for both defect detection and classiﬁcation tasks. We\nconduct classiﬁcation tasks on the micro-PCB [2] dataset\nand the defect detection task on DeepPCB [3] dataset. The\nexperimental results demonstrate the advantage of our model,\nwhich is the current SOTA model.\nThe main contributions of this research are as follows:\n• Introducing the transformer structure to the classiﬁca-\ntion task of PCB image data;\n• Using a label to improve the robustness of the model\nsmooth strategy;\n• Driving the model to learn the relationship between\ndifferent patches by mask patch prediction to ensure its\nextraction of deep relationships.\nThe rest of this paper is organized as follows. Section II\nreviews related research work. Section III explains our pro-\nposed method. In Section IV, comprehensive experiments\nare conducted to evaluate the effectiveness of the proposed\nmethod. Finally, in Section V, we conclude the paper.\nII. RELATED WORKS\nA. DETECTION AND CLASSIFICATION OF PCBS\nThe defect detection and classiﬁcation of PCBs are critical for\nmodern electronic circuits, which greatly impact the precision\nof circuit performance. Image processing has been applied for\nPCB defect detection, classiﬁcation, and localization, which\ncounters difﬁculties and subjective aspects in manual inspec-\ntion and provides fast assessments. Malge and Nadaf [4]\nproposed a PCB defect detection and classiﬁcation system to\ndetect and classify defects in order to identify root causes. The\nsystem was based on a morphological image segmentation\nalgorithm and image processing theories. The system focused\non images of single-layer PCBs. Kamalpreet [5] proposed an\nalgorithm to classify 14 defects into ﬁve groups, which used\nMATLAB image processing operations. With the develop-\nment of machine learning and deep learning, more effective\nmodels have been developed. In [6], a deep learning-based\nadvanced PCB inspection system was proposed. The classi-\nﬁcation task of PCBs is often limited by a limited dataset.\nThe research applied image augmentation to get better per-\nformance. Ankit [7] created a deep learning model for Image\nClassiﬁcation for PCBs, which aimed to detect defective\nPCBs and classify them as Good or Bad. In [8], to extract PCB\ncircuits, machine learning and computer vision methods were\ninvestigated, designed, and tested with real-world PCB data.\nDeep learning networks (Faster R-CNN) and unsupervised\nmachine learning clustering (XOR-based Kmeans) were used\nto implement the detection and localization of electronic\ncomponents. However, all the above research aims to classify\nPCBs, identify electronic components, and detect defects that\ncould be harmful to the circuit performance. Massive patterns\nof electronic components, their combinations and locations\non PCBs, and limitations from the PCB image dataset make\nthe classiﬁcation and recognition way more complicated\nand less effective. In this research we are classifying micro\nPCBs based on their makes and models,which will greatly\nfacilitate downstream tasks by conveniently retrieving sub-\ncomponents through a bill of materials.\nMeanwhile, computer vision has made signiﬁcant\nprogress. Its application in object detection [9], [10] has\nadvanced the development of autonomous vehicles [11],\nrobotics [12], and many other practical applications. There is\na potential to adapt those implementations to other detection\nand classiﬁcation tasks, such as PCB images. However, large-\nscale datasets are required to train those object detection\nnetworks to obtain good performance. Unfortunately, for the\nPCB tasks, it is infeasible to build up a large-scale dataset to\ntrain those networks.\nB. TRANSFORMER MODELS\nThe transformer [13] mechanism was originally utilized\nin NLP (Natural Language Processing). Being applied in\nextended ﬁelds, it has demonstrated excellent performance in\ndifferent applications. The evolvement of transformer models\nincludes Bert [14], ALBERT [15], and RoBERTa [16]. Lead-\ning technology companies, like Google AI Lab, Amazon AI\nLab, Baidu’s ERNIE [17] and ERNIE2 [18] have been driven\nto devote resources to the research.\nSome researchers have attempted to migrate the trans-\nformer’s processing of input plenary data for use in the\nﬁeld of computer vision with great success, demonstrating\nVOLUME 10, 2022 42543\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nFIGURE 1. Overview of LPViT.\nthe general applicability and design rigor of such structures.\nIn the ﬁeld of computer vision, many transformer-based\narchitectures have been presented, each of which is tailored\nto the speciﬁc task. For example, DETR [19] was created for\nobject detection, and most AI researchers choose SETR [20]\nfor semantic segmentation.\nFor classiﬁcation tasks, ViT [21] and DeiT [22] have\nbeen proposed, which improve the metrics and become the\nbasis for complex tasks such as detection and segmenta-\ntion. Transformer-based strategies can also be well adapted\nto other applications. Taking the medical image classiﬁ-\ncation [23], [24] as an example, transformer-based mod-\nels can be adapted for tumor detection [25] and semantic\nsegmentation.\nHowever, for PCB classiﬁcation, there is still no speciﬁc\nclassiﬁcation model. The PCB image has a variety of colors\nand components. For accurate classiﬁcation, the features of\nPCB images need to be studied and accurately modeled.\nIII. METHOD\nThe capacitors, resistors, and other components on micro\nPCBs can vary signiﬁcantly under different illumination and\nviewing angles due to the reﬂection of light. The smaller scale\ncompared to ordinary PCBs further impacts on the results\nof classiﬁcations, which requires our proposed model to be\nextremely robust to eliminate the above interference.\nIn this research, we propose a new model, Label Robust\nand Patch Correlation Enhanced ViT (LPViT), which ensures\nthe robustness of the model while fully exploiting the rela-\ntionship between different regions of PCB images. Figure 1\nshows the architecture of our proposed model, LPViT. In our\nmodel, the input image is ﬁrst chunked to get different\npatches, and some of them are randomly masked or replaced,\nwhich aims to improve the ability of mutual understanding\nbetween different regions of the image. The operation is\ndesigned to drive the model to recover the lost information,\nand further extract features to get the patch embedding. With\nthe label smooth strategy for training to improve the robust-\nness, our model is able to achieve better results.\nA. TRANSFORMER\nFirst proposed by the Google team in 2017, Transformer\nis an architecture that has demonstrated its effectiveness in\nnatural language processing. Completely abandoning many\nof the previous approaches, its innovations allow for signiﬁ-\ncant improvement of the metrics of several natural language\nprocessing tasks at the time and laid the groundwork for the\nsubsequent models. Figure 2 shows the overall architecture\nof the Transformer.\n1) ATTENTION MECHANISM\nIt is widely believed that it is the Attention mechanism that\ncontributes to the success of the Transformer architecture,\nwhich enhances the feature extraction capability. Attention\nis a mechanism for improving the effectiveness of RNN[15]\n(LSTM or GRU)-based Encoder + Decoder models, com-\nmonly known as Attention Mechanism.\nFor example, in machine translation and speech recog-\nnition, every single word will be assigned a weight,\nwhich makes the learning of neural networks more ﬂex-\nible (soft). Attention can be utilized to explain what\nexactly the model has learned, as well as the alignment\nrelationship between the input and output sentences of\ntranslations.\nIn a transformer, the most important part is the Attention\nmechanism that is specially designed to promote the feature\nextraction function of the model for a better global and con-\ntextual understanding of the input information. To measure\nthe relative importance of input features, the cosine similarity,\nalso known as dot-product, is adopted. We use Dot-product\nAttention to describe the calculation of attention weights of\ndifferent features.\nA similar operation is used at all time steps, here as we get\ninput key noted as ki,i =1 ... N, the corresponding values\nvi,i =1 ... N and the query that we search, noted as q, we get\nformula 1 as follows:\nAttention(Q,K,V ) =\nN∑\ni=1\nqT kivi (1)\n42544 VOLUME 10, 2022\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nFIGURE 2. Transformer architecture.\nAs the length of the vector grows longer, the scale range of\nits dot product result grows wider. As a result, it will fall into\nthe saturation zone after going through the softmax operation,\nreducing the gradient during backpropagation. It makes the\nmodel optimization harder. To address this issue, the inner\nproduct value we obtain will be divided by the square root of\nits length, denoted as d, before executing softmax. Then we\nget fomula 2.\nAttention(Q,K,V ) =Softmax(QKT\n√\nd\n)V (2)\nIn the case that keys, values and queries share the\nsame vector value, the special attention structure is called\nself-attention.\n2) INPUT OF THE TRANSFORMER\nIn the Transformer, with the word Embedding and the posi-\ntional Embedding (Positional Encoding) added, the input\nrepresentation x of a word is constructed. Word Embedding\ninput representation can be produced in a variety of ways,\nsuch as pre-trained using Word2Vec, Glove, and so on. This\nis a common strategy in NLP applications.\nWe’ll also need a new embedding called positional embed-\nding to show the relative position of the word in the phrase.\nTransformer, unlike RNN, uses all global information but\nprioritizes the sequential information of words, which is crit-\nical for NLP. As a result, we employ Location Embedding\nto keep track of each word’s relative or absolute position in\nthe sequence. With the word Embedding and the position\nEmbedding of the word added, the representation vector x\nof the word is constructed, and x is used as the input for the\nTransformer.\n3) STRUCTURAL INNOVATION\nThe output of the Attention module can be multiplexed to\ncombine the information in numerous ways, thereby deﬁning\nthe existing characteristics from several perspectives, to thor-\noughly investigate the information needed for the model. This\nis referred to as the Multi-Head Attention Structure.\nThe Encoder block structure of the Transformer is shown\nin red, and it is made up of Multi-Head Attention, Add\nand Norm and Feed Forward. We just covered the Multi-\nHead Attention computing method. Here Add refers to X +\nMultiHeadAttention(X), it is a residual connection that is fre-\nquently used in ResNet to overcome the challenge of training\nmulti-layer networks by allowing the network to focus just on\nthe present section of the difference.\nLayer Normalization, which is commonly utilized in RNN\nstructures, is referred to as the Norm. Layer Normalization\nequalizes the mean-variance of the inputs of each layer of\nneurons, allowing for faster convergence.\nThe Feed Forward layer is a two-layer completely con-\nnected layer with ReLU as the ﬁrst layer’s activation function\nand no activation function in the second layer.\nReLU (ReLU (x) =max(0,x)) is one of the most widely\nused activation functions in the ﬁeld of deep learning, and the\nnonlinear mapping that it can provide is one of the important\nreasons why deep neural networks can produce excellent\nresults.\nB. VIT\nGradually, several researchers attempted to apply the trans-\nformer structure, which excels in NLP tasks, to vision tasks,\nresulting in Vision Transformer, or ViT. Figure 3 shows the\narchitecture of ViT.\nThe four primary aspects of the ViT process are as follows:\n1) Image chunking, that is, to create image patches\n2) Image patch embedding and position coding are both\navailable\n3) Encoder with Transformer\n4) Classiﬁcation with simple models using features from\nEncoder.\nWe will introduce ViT in terms of these four parts.\n1) IMAGE CHUNKING\nThe very ﬁrst step is pre-processing. Without any special\npre-processing, CNN convolves the image in two height and\nwidth dimensions natively. To use the Transformer structure,\nhowever, we must chunk it ﬁrst.\nIf the input is an image x in HWC, and it is now divided\ninto patches of P ×P ×C, the total number of patches is\nN =HW /P2. The dimension of data can be written in the\nVOLUME 10, 2022 42545\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nFIGURE 3. ViT architecture.\nshape of N ×P ×P ×C. After that, each patch is spread,\nand the data dimension corresponding to it can be expressed\nas N ×(P2 ×C). The length of the sequence from the input\nto the Transformer is N, the number of channels in the input\npicture is C, and the size of the image patches is P.\n2) IMAGE CHUNK EMBEDDING (PATCH EMBEDDING)\nBecause image chunking is simply a pre-processing step,\nwe must perform an image block embedding operation to\nconvert the vector dimension of N ×(P2 ×C) into a two-\ndimensional input of size N ×D. Block embedding, like\nword2vec in NLP, transforms a high-dimensional vector into\na low-dimensional representation. The embedding is given a\nparticular code, such as ’CLS’ in BERT, to be used as the\ncategory prediction result.\nAfter dimensionality reduction, the so-called picture block\nembedding is essentially a linear transformation, i.e., a fully\nlinked layer, of each spanned patch vector with dimension D.\nA position encoding vector must be added to the image\nblock embedding to maintain the spatial location information\nbetween the input picture patches. Instead of employing the\nupdated 2D position embedding approach, ViT’s position\nencoding employs a direct 1D learnable position embedding\nvariable, as the authors discovered that in trials, the 2D and\n1D embedding methods produced equivalent results.\n3) TRANSFORMER\nThe transformer module is used to extract features, which\nare then fed into the ﬁnal classiﬁer to produce the required\nclassiﬁcation results.\nDifferent investigations have proven that the transformer\narchitecture has a disruptive effect on various tasks by consid-\nerably improving the ability to mix features and fuse higher-\norder knowledge.\n4) CLASSIFIER\nAfter enough good features have been retrieved, all that is\nrequired for reliable classiﬁcation results is a simple network\ntopology, and a shallow network like MLP(multilayer percep-\ntron) can achieve incredibly high metrics.\nThe ﬁgures can be transformed into sequence information,\nwhich is more conducive to understanding the relationships\n42546 VOLUME 10, 2022\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nbetween them, thus increasing the metrics, by chunking them\nand then extracting the features independently.\nC. MASK PATCH PREDICTION\nThe most common front-to-back prediction performed on the\ninput sequence is called unidirectional prediction. In uni-\ndirectional prediction, the semantics of the entire utter-\nance is not the focus, which means it is not completely\nunderstood. So researchers further proposed bi-directional\nprediction [27].\nIn BERT [14], its authors argue that bi-directional pre-\ndiction still cannot understand the semantics of the whole\nutterance completely. A better approach is to use the full\ninformation of the context to predict [mask]. That is, a ran-\ndom part of the content is masked as the target of prediction.\nBERT uses a two-step operation to perform the training of the\nmask strategy.\nThe ﬁrst step is to mask 15% of the words in an article.\nAccording to the context, the model will omnidirectionally\npredict the masked words. For N articles, average W words\nin each article and randomly 15% of the words being masked,\nthe task of the model is to predict the 15% ×W ×N masked\nwords correctly. The transformer is trained to get appropriate\nparameters for the prediction purpose.\nThen, the second step is to continue training the parameters\nof the model. For example, from the above-mentioned N\narticles, select the utterances. These include two consecutive\ncontextual statements and a discrete pair of statements. The\ntransformer model will then identify consecutive and discon-\ntinuous utterances.\nIn ViT [21], we use a similar strategy called masked patch\nprediction for self-supervision which we believe will make\nthe model training more accurate and accelerate convergence.\nFor preliminary self-supervision investigations, we use the\nmasked patch prediction objective. To do this, we operate\npart of patch embeddings in the procedure. Among them all,\nwe replace the embeddings with a learnable [mask] embed-\nding by the amount of 80% while 10% will be a random\nanother patch embedding, and the remaining(10%) are left\nalone. In the end, we use the respective patch representations\nto predict the mean color of every color channel for each\ncorrupted patch to recover information.\nD. LABEL SMOOTH\nWith the widespread application of computer vision, there\nare many adversarial generation methods against the model,\nwhich seriously affects the effectiveness of the model and\nthe security of daily use, for example, FGSM [28], BIM\n[29], JSMA [30] and DeepFool [31]. For such problems,\nresearchers have proposed a series of defenses, and label\nsmooth [32] is one of the most widely used.\nIn classiﬁcation tasks, we often use one-hot codes to mark\nthe category information of the images. For an N-class classi-\nﬁcation task, the label Y is an N-d vector. If the corresponding\nlabel is K, then only the K-th element of Y is 1 and all others\nare 0.\nWhen using the label smooth strategy, we distribute a\nportion of the values corresponding to the true labels, denoted\nas α, equally to the components of each category, making the\ndifferences between categories smoother. In the classiﬁcation\ntask, the label corresponding to each sample is a vector\nof length equal to its number of categories, and when the\nnumber of categories is 4, a 4-dimensional (4d) vector is\nused for its representation. Taking N =4,α =0.2 as an\nexample, if the true category is 2, then the 4d vectors obtained\nbefore and after label smooth processing are [0, 1,0,0] and\n[0.05,0.85,0.05,0.05] respectively.\nThe advantages of label smoothing strategy can be\nexplained from several perspectives:\n1) In real scenarios, there could be noise especially when\nthere is a large amount of data. Label smoothing can be\nadded to prevent the model from incorrectly learning\nthe noise.\n2) Sometimes we train a model to give fairly high con-\nﬁdence, which may lead to other problems such as\nover-ﬁt. To address this issue, label smoothing can be\nintroduced to improve the learning difﬁculty of the\nmodel.\n3) For image classiﬁcation, there will be some ambiguous\ncases. Some pictures (like bottles and bowls) are using\nsoft-target instead of hard-target. Label smoothing can\nprovide a supervised effect for both categories. Here,\nwe denote the original one-hot label as the hard-target,\nand the result of its label smoothing as the correspond-\ning soft-target.\nWith label smoothing, our proposed model, LPViT can\nachieve more robust labeling and enhanced patch correlation.\nLPViT model, compared with other classiﬁcation models,\nhas the following characteristics, which are the reasons why\nit works better than traditional CNN models and other trans-\nformer structures:\n• The LPViT model, compared with other CNN-based\nmodels, focuses more on the information in the input and\nextracts the parts that are more relevant to the results,\nwhich optimizes the convergence process of the model.\n• Mask and label smooth strategies in LPViT are more\nrobust and can achieve better generalization than other\ntransformer-based models for unseen data distributions\nto obtain better results in different vision tasks, such as\ndetection and classiﬁcation.\nIV. EXPERIMENTS\nA. CLASSIFICATION TASK\n1) DATASET\nWe use a dataset called micro-PCB, which contains\n8,125 photos of 13 micro-PCBs with high resolution. The\naverage width and height of all photos are 1949 and\n2126 pixels.\nUnder optimal lighting circumstances, the micro-PCBs\nwere photographed in 25 various positions relative to the\ncamera. Each micro-PCB was photographed in 5 separate\nrotations in each position. As a result, each micro-PCB has\nVOLUME 10, 2022 42547\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nTABLE 1. PCBs and labels.\nFIGURE 4. Raspberry Pi A+sample image.\n125 different orientations to the camera. For every orienta-\ntion, a micro-PCB was photographed four times, which were\ncoded for training. After that, a single capture and testing of\na micro-PCB of the same make and model was performed.\nAn image of a micro-PCB utilized in training will not be used\nin testing.\nAlthough the micro-PCBs coded for training and testing\nare essentially similar, there are relatively minor changes in\nsome circumstances. Each micro-PCB in the dataset com-\nprises 500 training photos and 125 test images, resulting in\na 6,500/1,625 train/test split. Figures 4 - 7 demonstrate four\nsamples in the micro-PCB dataset.\nEach type of PCB and its category (Label) are listed in\nTable 1.\n2) MODEL TRAINING\nIn our experiments, we utilize Python and Pytorch1.8.0. The\nGPU GTX2080TI and CPU i7-10875h are the hardware\ndevices we employ to speed up the training.\nWe chose Adam [33] as our optimizer. The optimum\nparameters beta1 and beta2 are set to be 0.9 and 0.999,\nrespectively, with a learning rate of 0.0001. To prevent\nthe denominator from being 0, we use eps =1e-08.\nThe weight_decay equals 0, which means we do not\ntake them into the loss. With batch size of 64 we set\nthe binary classiﬁcation problem and choose binary_\nFIGURE 5. Arduino mega 2560 (black) sample image.\nFIGURE 6. Beaglebone black sample image.\nFIGURE 7. Arduino Leonardo sample image.\ncross_entropy_with_logits to be the loss function. The total\ntraining epochs are 75.\nTo further improve the training effect of the model, the\nlearning rate is dynamically adjusted, and we use a learning\nrate decay strategy to reduce the learning rate by 10% for\n42548 VOLUME 10, 2022\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nTABLE 2. Performance of different model Configs for classification on themicro-PCB dataset.\nFIGURE 8. Base model loss.\nFIGURE 9. Base model accuracy.\nevery 30 rounds of training. The initial learning rate is set\nto 0.0001.\nThe probability of using tokens in masked prediction tasks\nmask_prob is set to be 0.15, and the probability of randomly\nreplacing is set to be random_patch_prob =0.30, with\nreplace_prob =0.50, which means half of the tokens are set\nto be mask token.\n3) DATA AUGMENTATION\nWe utilize a random crop, a random horizontal ﬂip with a\nchance of 0.5, and scale the input photos to a ﬁxed size of\n224 ×224 for the training set. For test data, there is no data\naugmentation.\n4) PERFORMANCE METRICS\nIn this research, we use accuracy, precision, recall, and\nF1-score as the metrics to evaluate the performance of differ-\nent models. They are calculated using formulas 3, 4, 5, and 6.\nFIGURE 10. ViT loss.\nFIGURE 11. ViT accuracy.\nFIGURE 12. ViT label smooth loss.\nWe denote the total number of samples as N, the number\nof correctly classiﬁed samples as Nc, the predicted number\nof positive samples as ˆN+, where the true number of positive\nsamples is Nc\n+and the number of positive samples in the data\nVOLUME 10, 2022 42549\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nFIGURE 13. ViT label smooth accuracy.\nFIGURE 14. LPViT loss.\nFIGURE 15. LPViT accuracy.\nset is N+.\nAccuracy =Nc\nN (3)\nPrecision =Nc\n+\nˆN+\n(4)\nRecall =Nc\n+\nN+\n(5)\nF1 −score =2 ×Precision ×Recall\nPrecision +Recall (6)\n5) EXPERIMENTAL RESULTS\nWe choose ResNet50 [34] as the baseline model for this\nexperiment, which has shown excellent performance in many\nFIGURE 16. Error sample 1.\nFIGURE 17. Error sample 2.\nTABLE 3. Defect types and numbers.\nvision tasks. A comparison with it can effectively illustrate\nthe performance of the proposed model.\nWe use three different conﬁgurations for training.\n• ViT.\n• ViT + Label Smooth.\n• LPViT\nThe metrics of the baseline model and three ViT-based\nmethods are shown in Table 2. It can be found that our pro-\nposed model improves the accuracy rate by 1.36% compared\nto the baseline, which fully illustrates the effectiveness of the\nproposed model and its adaptability to the dataset.\nIn the ﬁrst model conﬁguration with the ViT only, the\nperformance has already exceeded the baseline model. After\nadding label smooth and MPP (mask patch prediction), the\naccuracy can be improved by another 0.62%. They reduce\n42550 VOLUME 10, 2022\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nFIGURE 18. Overview of LPViT for defect detection.\nTABLE 4. Metrics of different model Configs for DeepPCB dataset detection.\nthe risk of overﬁtting and enhance the smoothness of the\nboundary, which improves the model’s understanding of the\nrelationship between different regions of the images. Pre-\ncision, Recall, and F1-score demonstrate a similar pattern\nof improvement. We also compare our model with another\ntransformer Swin Transformer model on the classiﬁcation\ntask. As shown in Table 2, LPViT, outperforms other models\nby all metrics.\nFurthermore, the processing speed of our model is\n156.1FPS (frame per second), while the processing speed of\nthe Swin Transformer is 129.9FPS.\nFigures 8 - 15 demonstrate the accuracy and loss versus\nepochs for each model. It should be speciﬁcally noted that\nin all ViT-based methods, the accuracy on the training set\noutperforms the test set after a certain number of epochs of\ntraining. It indicates that the patch method of ViT can better\nextract the association of different regions in the image and\nobtain the underlying information in the recognition process\nto achieve better migration results on unseen new data.\n6) ERROR ANALYSIS\nFigures 16 and 17 provide two examples of misclassiﬁca-\ntion cases. With a further examination of the misclassiﬁed\nsamples, it can be seen that the images are blurred to a\ncertain extent compared to the correctly classiﬁed images.\nSome relevant details for classiﬁcation are lost, resulting in\nthe wrong classiﬁcation.\nIn the future, we can model the correlation between differ-\nent parts of the images and use it to recover the missing data\nof a certain place.\nB. DEFECT DETECTION\nIn order to fully investigate the effectiveness of our proposed\nmodel for different tasks, we use it to perform defect detec-\ntion tasks. It should be noted that in order to maintain the\nrelative position of the original images, we turn the output\npatch sequence back to the original two-dimensional arrange-\nment as the output of the backbone. Figure 18 illustrates the\noverview of the system for defect detection.\nFIGURE 19. PCB template sample.\nFIGURE 20. PCB defect annotation sample.\n1) DATASET\nHere we use a new dataset DeepPCB [3] for detection exper-\niments. It contains 1500 pairs of images. In each pair, there\nis one defect-free template image and an annotated defect\nimage. In DeepPCB, there are six different types of PCB\nVOLUME 10, 2022 42551\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nFIGURE 21. Detection result sample 1.\nFIGURE 22. Detection result sample 2.\ndefects: open, short, mousebite, spur, pin hole, and spuri-\nous copper. The details of each type are shown in Table 3.\nFigures 19 and 20 demonstrate an example pair of images.\n2) MODEL TRAINING\nWe continue to use Adam as the optimizer, with a learning\nrate of 0.0005 for the defect detection experiment. We set the\ntotal training epochs to be 2000.\n3) PERFORMANCE METRICS\nWe use mean Average Precision(mAP)as the metric for\nPCB defect detection task.\nWe arrange the detection results predicted by the trained\nmodel in decreasing order of the conﬁdence score. If the\npredicted box and the ground truth with the same pair get an\nIoU (intersection over union) greater than a certain threshold,\nwe believe it is a ‘‘match’’. We get mAP by fusing different\nAP values. Here we use AP, AP50, and AP75 which have dif-\nferent sampling and threshold values. The calculation process\nis the same and they can evaluate the effectiveness of the\nmodel from different perspectives.\n4) EXPERIMENTAL RESULT\nWe chose Faster RCNN [35] as our baseline model, which\nis a very widely used two-stage detector with outstanding\ndetection results. We modify its backbone step by step to be\nLPViT to test the effect of the model on the defect detection\ntask.\nAfter replacing the backbone with ViT, there is a signiﬁ-\ncant drop for each AP. It could be caused by the loss of some\nimportant interaction information during one-dimensional\nsequence processing, which makes the extracted features not\nthe most relevant part of the detection task. The drop is\nsigniﬁcantly improved by adding MPP, which demonstrates\nthe effect of the MPP strategy on model learning. MPP obvi-\nously improves the learning direction of the model and has\nsigniﬁcant effects on both classiﬁcation and detection tasks.\nThe addition of label smoothing further reduces some of the\nfalse positives and improves the performance of the model.\nTable 4 shows the comparative experimental results with the\nmaximum values in each column identiﬁed. LPViT improves\nall metrics by more than 6%. It also outperforms the current\nSOTA model (98.8% vs. 98.6%) that employs the group\npyramid pooling module [3]. Furthermore, the processing\nspeed of our model is 27.7FPS, while the processing speed\nin [3] is 24.6FPS. The experimental results fully demonstrate\nthe effectiveness of the proposed structure.\nV. CONCLUSION\nPCB devices have greatly facilitated human life. The classi-\nﬁcation and defect detection of PCB image data can greatly\naccelerate downstream tasks such as production and sorting,\neffectively improve manufacturing and recycling efﬁciency,\nand meet the growing demand for PCBs. Both industry and\nresearch institutions have devoted great efforts to the clas-\nsiﬁcation and defect detection of PCBs, which requires full\nconsideration of the small scale of the PCB model, the small\ndistinction between categories, and various imaging condi-\ntions such as image illumination angles. Among the massive\nPCBs, a big group of applications is using micro-PCBs man-\nufactured by several prevailing companies. Once the makes\nand models of PCBs are identiﬁed, the sub-components could\nbe easily retrieved through a bill of materials. In this research,\nwe propose a transformer-based classiﬁcation model, LPViT,\nwhich can be used to classify micro-PCBs based on their\nmakes and models and detect defects on PCBs. We also intro-\nduce mask patch prediction and label smooth to improve the\naccuracy and the robustness of the model. Through compar-\native experiments, our proposed model has achieved SOTA\nperformance on the micro-PCB dataset for classiﬁcaiton and\non DeepPCB dataset for defect detection.\nMeanwhile, there are some limitations in this research,\nwhich could be further studied in future research. Data aug-\nmentation can be achieved by different methods. In the future,\nwe could apply more complex data augmentation models to\nfurther enrich the dataset, which could potentially promote\nthe performance of our system. The dataset we use for clas-\nsiﬁcation is micro-PCBs. The classiﬁcation performance of\n42552 VOLUME 10, 2022\nK. An, Y. Zhang: LPViT: Transformer Based Model for PCB Image Classification and Defect Detection\nour system on large-scale PCBs needs to be further studied\nand veriﬁed.\nREFERENCES\n[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘Ima-\ngeNet: A large-scale hierarchical image database,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248–255, doi:\n10.1109/CVPR.2009.5206848.\n[2] A. Byerly, T. Kalganova, and A. J. Grichnik, ‘‘On the importance of\ncapturing a sufﬁcient diversity of perspective for the classiﬁcation of\nmicro-PCBs,’’ inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2021, pp. 209–219.\n[3] S. Tang, F. He, X. Huang, and J. Yang, ‘‘Online PCB defect detector on a\nnew PCB defect dataset,’’ 2019, arXiv:1902.06197.\n[4] P. S. Malge and R. S. Nadaf, ‘‘PCB defect detection, classiﬁcation\nand localization using mathematical morphology and image processing\ntools,’’ Int. J. Comput. Appl., vol. 87, no. 9, pp. 40–45, Feb. 2014, doi:\n10.5120/15240-3782.\n[5] K. Kamalpreet. (2014). PCB Defect Detection and Classiﬁcation Using\nImage Processing. [Online]. Available: https://www.semanticscholar.\norg/paper/PCB-Defect-Detection-and-Classiﬁcation-Using-Image-\nKamalpreet/592fd41d58ac50a38da9a8d9dd55ad34ec3e1a00\n[6] J. Kim, J. Ko, H. Choi, and H. Kim, ‘‘Printed circuit board defect\ndetection using deep learning via a skip-connected convolutional autoen-\ncoder,’’ Sensors, vol. 21, no. 15, p. 4968, Jul. 2021. [Online]. Available:\nhttps://www.mdpi.com/1424-8220/21/15/4968\n[7] U. Ankit. (Aug. 2021). Image Classiﬁcation of PCBs and Its Web Applica-\ntion (Flask). [Online]. Available: https://towardsdatascience.com/image-\nclassiﬁcation-of-pcbs-and-its-web-application-ﬂask-c2b26039924a\n[8] C. Yang. (2020). Machine Learning and Computer Vision for\nPCB Veriﬁcation. [Online]. Available: https://kth.diva-portal.\norg/smash/get/diva2:1529213/FULLTEXT01.pdf\n[9] J. Wang, L. Song, Z. Li, H. Sun, J. Sun, and N. Zheng, ‘‘End-to-end object\ndetection with fully convolutional network,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Nashville, TN, USA, Jun. 2021,\npp. 15849–15858.\n[10] T. Zhou, D. Fan, M. Cheng, J. Shen, and L. Shao, ‘‘RGB-D salient object\ndetection: A survey,’’ Comput. Vis. Media, vol. 2021, pp. 1–33, Jan. 2021.\n[11] D. Feng, A. Harakeh, S. L. Waslander, and K. Dietmayer, ‘‘A review\nand comparative study on probabilistic object detection in autonomous\ndriving,’’ IEEE Trans. Intell. Transp. Syst., early access, Aug. 30, 2021,\ndoi: 10.1109/TITS.2021.3096854.\n[12] Z. Zhou, L. Li, A. Fürsterling, H. J. Durocher, J. Mouridsen, and X. Zhang,\n‘‘Learning-based object detection and localization for a mobile robot\nmanipulator in SME production,’’ Robot. Comput.-Integr. Manuf., vol. 73,\nFeb. 2022, Art. no. 102229.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Conf.\nNeural Inf. Process. Syst. (NIPS), 2017, pp. 1–12.\n[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[15] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n‘‘ALBERT: A lite BERT for self-supervised learning of language repre-\nsentations,’’ 2019, arXiv:1909.11942.\n[16] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[17] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, ‘‘ERNIE: Enhanced\nlanguage representation with informative entities,’’ in Proc. 57th Annu.\nMeeting Assoc. Comput. Linguistics, 2019, pp. 1441–1451.\n[18] Y . Sun, S. Wang, Y . Li, S. Feng, H. Tian, H. Wu, and H. Wang, ‘‘ERNIE 2.0:\nA continual pre-training framework for language understanding,’’ 2019,\narXiv:1907.12412.\n[19] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Proc.\nEur. Conf. Comput. Vis. (ECCV), 2020, pp. 213–229.\n[20] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. S. Torr, and L. Zhang, ‘‘Rethinking semantic segmenta-\ntion from a sequence-to-sequence perspective with transformers,’’ 2021,\narXiv:2012.15840.\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[22] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efﬁcient image transformers & distillation through atten-\ntion,’’ 2020, arXiv:2012.12877.\n[23] T. Zhou, S. Ruan, and S. Canu, ‘‘A review: Deep learning for medical image\nsegmentation using multi-modality fusion,’’ Array, vols. 3–4, Sep. 2019,\nArt. no. 100004, doi: 10.1016/j.array.2019.100004.\n[24] Y . Shachor, H. Greenspan, and J. Goldberger, ‘‘A mixture of views net-\nwork with applications to multi-view medical imaging,’’ Neurocomputing,\nvol. 374, pp. 1–9, Jan. 2020, doi: 10.1016/j.neucom.2019.09.027.\n[25] V . Q. Joe and P. L. Westesson, ‘‘Tumors of the parotid gland: MR imaging\ncharacteristics of various histologic types,’’ Amer. J. Roentgenol., vol. 163,\nno. 2, pp. 433–438, Aug. 1994.\n[26] K. Cho et al., ‘‘Learning phrase representations using RNN encoder-\ndecoder for statistical machine translation,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process. (EMNLP), Oct. 2014, pp. 1724–1734.\n[27] Z. Huang, X. Wei, and Y . Kai, ‘‘Bidirectional LSTM-CRF models for\nsequence tagging,’’ 2015, arXiv:1508.01991.\n[28] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‘‘Explaining and harnessing\nadversarial examples,’’ 2014, arXiv:1412.6572.\n[29] A. Kurakin, I. Goodfellow, and S. Bengio, ‘‘Adversarial machine learning\nat scale,’’ 2016, arXiv:1611.01236.\n[30] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and\nA. Swami, ‘‘The limitations of deep learning in adversarial settings,’’\nin Proc. IEEE Eur. Symp. Secur. Privacy (EuroS&P), Mar. 2016,\npp. 372–387.\n[31] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, ‘‘DeepFool: A simple\nand accurate method to fool deep neural networks,’’ in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2574–2582.\n[32] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li, ‘‘Bag of\ntricks for image classiﬁcation with convolutional neural networks,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,\npp. 558–567.\n[33] D. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n2014, arXiv:1412.6980.\n[34] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[35] S. Ren, K. He, R. Girshick, and J. Sun, ‘‘Faster R-CNN: Towards real-\ntime object detection with region proposal networks,’’ IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\nKANG ANreceived the master’s degree in circuit\nand system from the Guilin University of Elec-\ntronic Technology, in 2007. He joined Hangzhou\nNormal University as a Lecturer. His research\ninterests include the Internet of Things technology\nand machine learning.\nYANPING ZHANG received the M.S. and Ph.D.\ndegrees in computer science from The University\nof Alabama, in 2009 and 2012, respectively. She\njoined the Department of Computer Science, Gon-\nzaga University, as an Assistant Professor, in 2012.\nShe is currently an Associate Professor of com-\nputer science at Gonzaga University. Her research\ninterests include but are not limited to the Internet\nof Things (IoT), smart and intelligent systems,\nmachine learning, wireless communication, and\ncomputer security.\nVOLUME 10, 2022 42553",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.549708366394043
    },
    {
      "name": "Transformer",
      "score": 0.4734344482421875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45390287041664124
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.426043838262558
    },
    {
      "name": "Computer vision",
      "score": 0.4119134545326233
    },
    {
      "name": "Engineering",
      "score": 0.14763233065605164
    },
    {
      "name": "Electrical engineering",
      "score": 0.1392505168914795
    },
    {
      "name": "Voltage",
      "score": 0.11698469519615173
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I163151501",
      "name": "Hangzhou Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210142152",
      "name": "ORCID",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I119888943",
      "name": "Gonzaga University",
      "country": "US"
    }
  ]
}