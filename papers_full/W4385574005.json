{
  "title": "What Language Model to Train if You Have One Million GPU Hours?",
  "url": "https://openalex.org/W4385574005",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3099917620",
      "name": "Teven Le Scao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166594679",
      "name": "Thomas Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2897607128",
      "name": "Daniel Hesslow",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2408638671",
      "name": "Stas Bekman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103258794",
      "name": "M. Saiful Bari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2936985360",
      "name": "Stella Biderman",
      "affiliations": [
        "Cape Eleuthera Institute",
        "Booz Allen Hamilton (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2765851221",
      "name": "Hady Elsahar",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A3113153300",
      "name": "Niklas Muennighoff",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2891256592",
      "name": "Jason Phang",
      "affiliations": [
        "Cape Eleuthera Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2765070949",
      "name": "Ofir Press",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2001631461",
      "name": "Colin Raffel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2901920096",
      "name": "Victor Sanh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111184121",
      "name": "Sheng Shen",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A3205834942",
      "name": "Lintang Sutawika",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3171351878",
      "name": "Jaesung Tae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3032358506",
      "name": "Zheng Xin Yong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2403313545",
      "name": "Julien Launay",
      "affiliations": [
        "Laboratoire de Physique de l'ENS"
      ]
    },
    {
      "id": "https://openalex.org/A2800204358",
      "name": "Iz Beltagy",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4283157303",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3199241049",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W4229506649",
    "https://openalex.org/W2950613642",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2971236147",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4286909525",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4223891676",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W3207699717",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W3187255235",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4320930577",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W3175526286",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W3036369012",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3212496002"
  ],
  "abstract": "Teven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, Iz Beltagy. Findings of the Association for Computational Linguistics: EMNLP 2022. 2022.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 765–782\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nWhat Language Model to Train if You Have One Million GPU Hours?\nThe BigScience Architecture & Scaling Group\nTeven Le Scao1∗ Thomas Wang1∗ Daniel Hesslow2∗ Lucile Saulnier1∗ Stas Bekman1∗\nM Saiful Bari3 Stella Biderman4,5 Hady Elsahar6 Niklas Muennighoff1 Jason Phang5 Ofir Press8\nColin Raffel1 Victor Sanh1 Sheng Shen9 Lintang Sutawika10 Jaesung Tae1 Zheng Xin Yong11\nJulien Launay2,12† Iz Beltagy13†\n1 Hugging Face 2 LightOn 3 NTU, Singapore 4 Booz Allen 5 EleutherAI 6 Naver Labs Europe 7 New York University\n8 University of Washington 9 Berkeley University 10 Big Science 11 Brown University 12 LPENS 13 Allen Institute for AI\nAbstract\nThe crystallization of modeling methods\naround the Transformer architecture has been a\nboon for practitioners. Simple, well-motivated\narchitectural variations can transfer across tasks\nand scale, increasing the impact of modeling\nresearch. However, with the emergence of state-\nof-the-art 100B+ parameters models, large lan-\nguage models are increasingly expensive to ac-\ncurately design and train. Notably, it can be dif-\nficult to evaluate how modeling decisions may\nimpact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone.\nIn the process of building BLOOM–the Big\nScience Large Open-science Open-access Mul-\ntilingual language model–our goal is to identify\nan architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours\nbudget. Specifically, we perform an ablation\nstudy at the billion-parameter scale compar-\ning different modeling practices and their im-\npact on zero-shot generalization. In addition,\nwe study the impact of various popular pre-\ntraining corpora on zero-shot generalization.\nWe also study the performance of a multilin-\ngual model and how it compares to the English-\nonly one. Finally, we consider the scaling be-\nhaviour of Transformers to choose the target\nmodel size, shape, and training setup. All our\nmodels and code are open-sourced at https:\n//huggingface.co/bigscience.\n1 Introduction\nRecent years have seen the advent of large language\nmodels characterized by emergent capabilities (e.g.,\nzero-shot generalization) arising from sheer scale\nalone (Radford et al., 2019; Brown et al., 2020).\nScaling LLMs results in a predictable increase in\nperformance: simple scaling laws connect the num-\nber of parameters, pretraining dataset size, and\ncompute budget (Kaplan et al., 2020; Ganguli et al.,\n2022; Hoffmann et al., 2022), providing a clear\n∗Equal contribution.\n†Equal supervision.\n10−2 100 102\nPF-days\n3 × 100\n4 × 100\n6 × 100\nLM-Loss\n125M\n350M\n760M\n1.3B\n13B\n3.01C−.046\nFigure 1: Smooth scaling of language modeling loss\nas compute budget and model size increase. We ob-\nserve a power-law coefficient αC ∼0.046, in-line with\nKaplan et al. (2020). We use this fit to estimate the\noptimal size and number of tokens to train on for the\nfinal model given the available budget.\npath towards more capable models. This paradigm\nshift has been fueled by the wide adoption of the\nTransformer (Vaswani et al., 2017), providing a\nscalable basis for practitioners to build upon.\nIn this paper, we design an architecture and\ntraining setup for a multilingual 100B+ parameters\nmodel (BLOOM, BigScience Workshop (2022)),\nseeking to best use a fixed 1,000,000 A100-hours\nbudget. Because of the costs involved with training\nlarge language models, we cannot exhaustively ex-\nplore the landscape of possible models. Instead, we\nposition ourselves as practitioners exploring \"off-\nthe-shelf\" solutions. We thus test promising addi-\ntions to the Transformer to attempt to reproduce\ntheir findings in a controlled, large-scale setting.\nAlthough our main goal was to prepare the archi-\ntecture and training setup of BLOOM, our findings\nare also valuable for practitioners building models\nin the 1-10B range, as they equally improve the per-\nformance of such smaller models. At variance with\nmajor works on large language models, we also\nmake a significant effort towards reproducibility\n765\nand openness: all of our pretrained models, code,\nand notes from our weekly meetings are made avail-\nable. See Appendix A for the relevant links.\nContributions. We first study the impact of pre-\ntraining corpora, positional embeddings, activation\nfunctions, and embedding norm on zero-shot gener-\nalization. We base our study on the popular GPT-2\narchitecture (Radford et al., 2019), with experi-\nments at the 1.3B parameters scale. We then con-\nsider the impact of massive multilinguality, show-\ning language-specific scaling laws in a multilingual\nsetting for the first time. Finally, we describe our\napproach to drafting an architecture for the final\n176B parameters BLOOM model.\n2 Methods\nWe first justify our choice to base our model on\nthe popular recipe of combining a decoder-only\nmodel with an autoregressive language modeling\nobjective, and introduce our experimental setup.\nWe then discuss our evaluation benchmarks, and\nmotivate our choice of zero-shot generalization as\nour key metric. Finally, we introduce the baselines\nwe compare to throughout the paper.\n2.1 Architecture and Pretraining Objective\nIn this paper, we base all models on a decoder-only\nTransformer pretrained with an autoregressive lan-\nguage modeling objective. This is a popular choice\nfor large language models (Brown et al., 2020; Rae\net al., 2021; Thoppilan et al., 2022), possibly be-\ncause it lends itself to zero-shot application to many\ndownstream tasks (Radford et al., 2019). Alterna-\ntives include encoder-decoder models trained with\na span-corruption objective (e.g., T5 Raffel et al.\n(2019)), as well as non-causal decoders models\nwith visibility over a prefix (so-called Prefix LMs,\nLiu et al. (2018); Dong et al. (2019)).\nOur decision is motivated by the findings\nof Wang et al. (2022), which showed that decoder-\nonly models combined with an autoregressive lan-\nguage modeling objective provide the best zero-\nshot generalization abilities immediately after pre-\ntraining. Although multitask finetuning (Sanh\net al., 2021; Wei et al., 2021) will instead favor\nan encoder-decoder with span corruption for best\nzero-shot generalization, Wang et al. (2022) found\na compromise between these two practices. Fol-\nlowing autoregressive pretraining, decoder-only\nmodels can be efficiently adapted into non-causal\ndecoders, simply by extending pretraining with\nspan corruption. This adaptation produces a sec-\nond model, which can provide excellent zero-shot\ngeneralization after multitask finetuning. Accord-\ningly, we follow their recommendation, and train\nan autoregressive decoder-only model first which\nwe will later consider adapting and finetuning.\n2.2 Experimental Setup\nWe follow the architectures GPT-2 (Radford et al.,\n2019) and the hyperparameters of GPT-3 (Brown\net al., 2020). For learning rate, we use a maxi-\nmum value of 2 ×10−4, with a linear warm-up\nover 375M tokens, followed by cosine decay to a\nminimum value of 1 ×10−5. We use a 1M tokens\nbatch size, with linear ramp-up over the first 4B\ntokens, and a sequence length of 2,048. We use\nthe Adam optimizer (Kingma and Ba, 2014), with\nβ1 = 0.9, β2 = 0.999, ϵ = 1 ×10−8, weight\ndecay 0.1, and gradient clipping to 1.0. We also\ntie the word embedding and softmax matrix (Press\nand Wolf, 2017). Unless noted otherwise, we con-\nduct our experiments with 1.3B parameters models,\npretraining on 112B tokens.\nWe picked this size and dataset size as a compro-\nmise between compute cost and the likelihood that\nour conclusions would transfer to the target 100B+\nmodel. Notably, we needed to be able to reliably\nmeasure zero-shot generalization above random\nchance. We note that training for 112B tokens 1.3B\nparameters models bring them significantly above\nthe optimality threshold of Kaplan et al. (2020),\nand of Hoffmann et al. (2022).\nThe main architectural difference with GPT-3 is\nthat all our layers use full attention, while GPT-3\nuses alternating sparse attention layers (Child et al.,\n2019). The main value of sparse attention layers is\nto save compute with long sequence lengths. How-\never, at the 100B+ scale, sparse attention layers\nprovide negligible compute savings, as the vast\nmajority of the compute is spent on the large feed-\nforward layers. Kaplan et al. (2020) estimated the\namount of compute per token to be:\nCforward = 2×(12nlayerd2 + nlayernctxd),\nwhere Cforward is the cost for the forward pass,\nnlayer is the number of layers, dis the hidden di-\nmension, and nctx is the sequence length. This\nmeans if 12d>>n ctx, the second nlayernctxdterm\nis negligible, which is the case for our final model\nwhere d> 10,000 and nctx = 2048.\n766\nModel Parameters Pretraining tokens\nDataset 112B 250B 300B\nOpenAI — Curie 6.7B 49.28\nOpenAI — Babbage 1.3B 45.30\nEleutherAI — GPT-Neo 1.3B The Pile 42.94\nOurs 13B OSCAR v1 47.09\nOurs\n1.3B The Pile 42.79 43.12 43.46\n1.3B C4 42.77\n1.3B OSCAR v1 41.72\nTable 1: Pretraining datasets with diverse cross-domain high-quality data improves zero-shot generalization.\nAverage accuracy on EAI harness (higher is better) using different pretraining corpora and comparison with baseline\nmodels. Bold is best 1.3B model for amount of tokens seen, underline is best overall.\nWhat is a FLOP exactly? We report throughput\nper GPU in FLOPS and total budgets in PF-days\n(i.e. one PFLOPS sustained for a day). It is im-\nportant to highlight that FLOPS are never directly\nmeasured, but always estimated, with widely dif-\nferent practices across papers. We refer to model\nFLOP the estimates based on the C = 6ND for-\nmula from Kaplan et al. (2020), where C is the\ntotal compute, N the model size, and Dthe num-\nber of tokens processed. These are the FLOP ac-\ntually used to train the model, and which are used\nfor scaling laws. We refer to hardware FLOP the\nestimates reported by our codebase, using the for-\nmula from Narayanan et al. (2021). This notably\nincludes gradient checkpointing, which trades ad-\nditionnal computations for reduced memory needs,\nand a more thorough accounting of operations.\n2.3 Evaluation Benchmarks\nWe measure upstream performance using the lan-\nguage modeling loss on an held out sample of the\npretraining dataset. However, it is not always pos-\nsible to compare losses across objectives and tok-\nenizers. Moreover, as upstream performance is not\nalways aligned with task performance (Tay et al.,\n2021), we must also measure downstream perfor-\nmance explicitly. We could use zero/few-shot gen-\neralization, with or without specific finetuning.\nSpecifically, we choose to measure zero-shot\ngeneralization on a diverse set of tasks. Few-shot\nand zero-shot results are strongly correlated: we\nfound a Pearson correlation coefficient of 0.93 be-\ntween zero-shot and few-shot performance across\nmodel sizes in Brown et al. (2020). We do not rely\non finetuning as it is not how the main final model\nis likely to be used, given its size and the challenges\nassociated with finetuning at the 100B+ scale.\nWe use the popular EleutherAI Language Model\nEvaluation Harness (EAI harness, Gao et al.\n(2021)), evaluating models across 27 diverse tasks\nthat are similar to those used in Brown et al. (2020)\n(see Appendix C for a list of tasks). Overall, the\nrandom baseline on our benchmark sits at 33.3%.\n2.4 Baselines\nWe use GPT-Neo (Black et al., 2021), a 1.3B\ndecoder-only autoregressive language model\ntrained on the Pile (Gao et al., 2020), and GPT-\n3 (Brown et al., 2020), accessed via the OpenAI\nAPI. We evaluate two models, Babbage and Curie1.\nBased on Gao (2021) and our own analysis, we\nassume Babbage is 1.3B while Curie is 6.7B based\non how close our computed results are to those re-\nported in the original paper. However, as details\nof the OpenAI API are kept secret, there is no way\nto make sure that the models are actually the ones\ndescribed in Brown et al. (2020) – the number of\npretraining tokens reported in Table 1 is thus to be\ntaken cautiously.\n3 Impact of Pretraining Data\nWe first study the impact of pretraining data on\nzero-shot generalization. More diverse pretraining\ndata, ideally curated from a cross-domain collec-\ntion of high-quality datasets, has been suggested to\nhelp with downstream task performance and zero-\nshot generalization (Rosset, 2020; Gao et al., 2020).\n1These models are now referred to as\ntext-babbage-001 and text-curie-001.\n767\n3.1 Corpora\nWe evaluate three possible corpora, all commonly\nused to train large language models:\n• OSCAR v1 (Ortiz Suárez et al., 2019)2, a mul-\ntilingual, filtered version of Common Crawl;\n• C4 (Raffel et al., 2019), specifically its repli-\ncation by AllenAI, a processed and filtered\nversion of Common Crawl;\n• The Pile (Gao et al., 2020), a diverse pre-\ntraining corpus that contains webscrapes from\nCommon Crawl in addition to high-quality\ndata from cross-domain sources such as aca-\ndemic texts and source code.\nFor each pretraining corpus, we train a 1.3B pa-\nrameter model for 112B tokens. For the Pile specif-\nically, motivated by good early results at 112B\ntokens, we train up to 300B tokens, to compare\nwith GPT-3 models and validate against GPT-Neo.\n3.2 Results\nEvaluation results are outlined in Table 1. We find\nthat training on the Pile produces models that are\nbetter at zero-shot generalization, with C4 a close\nsecond, and OSCAR significantly behind.\nImportantly, this finding transfers to larger\nscales: as part of engineering test runs, a 13B\nmodel was trained on OSCAR for 300B tokens.\nWe found this 13B model to underperform the 6.7B\nmodel from OpenAI API which we attribute to the\nlow quality of the English data in OSCAR.\nWe also note that our model trained on The Pile\noutperforms the 1.3B GPT-Neo trained on the same\ndataset. Finally, our 1.3B model still underper-\nforms the 1.3B model from the OpenAI API by\n1.6%. It seems most likely that the difference is\nthat of data, but we cannot investigate this further\nas the GPT-3 training dataset is neither publicly\navailable nor reproducible.\nFinding 1. Diverse cross-domain pretraining\ndata combining web crawls with curated high-\nquality sources improves zero-shot generaliza-\ntion over pretraining datasets constructed from\nCommon Crawl only.\n2The recent release of OSCAR v2 is a better dataset, but it\nwasn’t available when we started this project.\n4 Architecture Ablations\nWe now consider ablation studies to better identify\nthe best positional embedding, activation function,\nand embedding normalization placement.\n4.1 Positional Embeddings\nBackground Originally, both static sinusoidal\nposition embeddings and learned position embed-\ndings were proposed to capture positionnal infor-\nmation; the latter are popular in large language\nmodels (Brown et al., 2020). Su et al. (2021) pro-\nposed rotary embeddings, where the query and key\nrepresentations inside the self-attention mechanism\nare modified such that the attention captures rela-\ntive distances between them. Recently, Press et al.\n(2022) introduced a method which does not use em-\nbeddings, instead directly attenuating the attention\nscores based on how far away the keys/queries are.\nResults We compare learned, rotary, and ALiBi\nposition embeddings, and include a baseline with-\nout position embeddings. Our results are presented\nin Table 2. Although learned positional embed-\ndings outperform rotary embeddings, ALiBi yields\nsignificantly better results than all alternatives. We\nalso confirm the findings of Biderman (2021): a\nbaseline with no positional information exhibits\ncompetitive performance. While bidirectional mod-\nels require positional embeddings to determine the\nlocation of tokens, we find autoregressive models\ncan simply leverage the causal attention mask. We\nalso confirm the ability of ALiBi to extrapolate to\nlonger sequences than trained on in Figure 2. Note\nthat results in Table 2 do not use any extrapolation:\nALiBi embeddings are a better choice even without\ntaking into account their ability to extrapolate.\nFinding 2. ALiBi positional embeddings sig-\nnificantly outperforms other embeddings for\nzero-shot generalization.\nPositional Embedding Average EAI Results\nNone 41.23\nLearned 41.71\nRotary 41.46\nALiBi 43.70\nTable 2: ALiBi significantly outperforms other em-\nbeddings for zero-shot generalization. All models are\ntrained on the OSCAR dataset for 112 billion tokens.\n768\nFigure 2: ALiBi embeddings can effectively extrap-\nolate past the sequence length on which the model\nwas trained, while rotary embeddings can not. This\nis in line with the findings of Press et al. (2022).\n4.2 Activation Functions\nBackground. Large language models by and\nlarge still mostly use the GELU activation\n(Hendrycks and Gimpel, 2016). We evaluate a\nrecently proposed alternative, SwiGLU (Shazeer,\n2020), which combines both Gated Linear Units\n(Dauphin et al., 2016) with the Swish activation\nfunction (Ramachandran et al., 2017).\nSwiGLU uses 50% extra parameters in the feed-\nforward layers. As suggested in Shazeer (2020),\nwe compensate for this by reducing the hidden size\nof the feed-forward layer.\nResults. We present our results in Table 3.\nSwiGLU produces slightly better results than\nGELU. For our final model, we adopted GELU,\nas we initially observed a lower throughput for\nSwiGLU. However, further benchmarking identi-\nfied that this overhead was primarily associated\nwith the change in the hidden size of the feedfor-\nward network. Indeed, this new size, 5,456, is divis-\nible by neither the warp size of the GPU (Lashgar\net al., 2013) nor the number of streaming multipro-\ncessors, resulting in both tile and wave quantiza-\ntion. We accordingly recommend using SwiGLU\nfor future models.\nActivation function Average EAI Results\nGELU 42.79\nSwiGLU 42.95\nTable 3: SwiGLU slightly outperforms GELU for\nzero-shot generalization. Models trained on The Pile\nfor 112 billion tokens.\n4.3 Embedding Norm\nDettmers et al. (2021) suggests that greater stability\nof training can be achieved by including an extra\nlayer normalization (Ba et al., 2016) after the em-\nbedding layer. We evaluate the performance impact\nof such a modification in Table 4. We note that this\nincurs a significant reduction in the performance\nof the model. However, models above 100 billion\nparameters are notoriously unstable and require\nconsiderable engineering efforts in order to be kept\nstable. If this addition provides increased stability\nwhen training, it may be valuable.\nFinding 3. Adding layer normalization af-\nter the embedding layer incurs a significant\npenalty on zero-shot generalization.\n5 Multilinguality\nThe majority of 100B+ language models have been\ntrained in English, with notable exceptions in Chi-\nnese (Zeng et al., 2021; Wu et al., 2021) and Ko-\nrean (Kim et al., 2021) models. Smaller massively\nmultilingual models have seen wider adoption (Xue\net al., 2020), but these models are not suitable\nfor zero-shot. Recent results on large GPT-like\nmultilingual models show that English-only perfor-\nmance is usually disappointing (Lin et al., 2021).\nTraining data. We train a multilingual model\nto evaluate the effectiveness and potential impacts\nof this practice. We use the OSCAR dataset (Or-\ntiz Suárez et al., 2019), but here we include mul-\ntiple languages, not only English as in the ear-\nlier experiments. The languages we include are\nArabic, Basque, Bengali, Chinese, Catalan, En-\nglish, French, Hindi, Indonesian, Portuguese, Span-\nish, Urdu, and Vietnamese. We sample each lan-\nguage with a different probability that downsam-\nples the most frequent languages and upsamples\nthe least frequent ones, so that all languages are rep-\nresented. We estimate the sampling probabilities\nsimilar to Xue et al. (2021).\nEmbedding Norm Average EAI Results\nNo 43.46\nYes 42.24\nTable 4: Layer normalization after the embedding\nlayer diminishes performance significantly. Models\ntrained on The Pile for 300 billion tokens.\n769\nModel Size EN ZH ES FR VI AR HI UR Average\nXGLM (Lin et al.) 7.5B 54.5 45 38.2 50.7 47.5 47.5 43.4 42.7 46.19\nXGLM (reprod.) 7.5B 53.85 45.21 41.7 49.82 47.35 46.37 43.19 42.3 46.22\nXGLM 1.7B 49.68 44.63 37.39 47.94 42.75 45.65 44.35 43.19 44.45\nOurs 1.3B 49.9 44.53 36.77 46.51 45.75 43.41 45.95 42.91 44.47\nTable 5: Our multilingual 1.3B model achieves accuracy on zero-shot XNLI in line with XGLM Lin et al.\n(2021). First row is the reported XGLM results, and the second is our reproduction of their results to validate our\nmultilingual evaluation setup. Last two rows show that our multilingual model matches the XGLM results.\nEnglish-only evaluation. We first evaluate our\nmultilingual model on the same set of English\nbenchmarks we have used previously, in Table 6.\nMultilinguality significantly lowers accuracy on\nthe English benchmark, which is in line with the\nresults from Lin et al. (2021).\nMultilingual evaluation. Zero-shot multilingual\nevaluation is more challenging to setup because it\nrequires writing new prompts for each new lan-\nguage. Therefore, instead of manually writing\nprompts for each language, we follow the strat-\negy proposed by Lin et al. (2021), using English\nprompts for non-English examples–this can be\nviewed as cross-lingual zero-shot generalization.\nThey validated this strategy by demonstrating its\nability to achieve zero-shot performance on par\nwith (and sometimes even better than) human-\nwritten language-specific prompts. This strategy\nalso demonstrates cross-lingual abilities.\nWe evaluate on XNLI (Conneau et al., 2018), a\nmultilingual NLI dataset that covers 8 of the lan-\nguages we use for training. Our evaluation is differ-\nent from the zero-shot evaluation of the XTREME\nbenchmark (Hu et al., 2020). XTREME first fine-\ntunes the model on the English training data of\neach downstream task, then evaluates it on the non-\nEnglish dataset, attempting cross-lingual general-\nization. Our evaluation avoids any finetuning, and\ninstead relies entirely on zero-shot generalization.\nPretraining Average EAI Results\nEnglish-only 41.72\nMultilingual 38.55\nTable 6: Multilingual pretraining very significantly\ndiminishes English zero-shot generalization. Both\nmodels trained on OSCAR for 112B tokens.\nResults. Table 5 shows the XNLI results of\nour multilingual model and how it compares to\nXGLM (Lin et al., 2021). We were able to repro-\nduce the results of XGLM-7.5B which validates\nour evaluation setup. Furthermore, the table shows\nthat the performance of our 1.3B is in line with the\nXNLI 1.7B model, validating that our multilingual\nsetup achieves competitive results. It is worth not-\ning that our 1.3B model is trained on only 112B\ntokens from 13 languages while XGLM is trained\non 500B tokens from 30 languages. As far as we\nare aware, this is the first independent replication\nof the main results of Lin et al. (2021).\nLanguage-specific scaling laws. To explore how\nscale influences multilinguality, we train a wider\nrange of models (i.e. 0.3-6B parameters) on a\nlarger corpus of more than 300B tokens of text\ndrawn from a variety of languages (Laurençon\net al., 2022). In Figure 3, we show scaling laws for\nArabic, Catalan, Code, English, Spanish, Basque,\nFrench, Indonesian, Assamese, Bengali, Gujarati,\nHindi, Kannada, Malayalam, Marathi, Nepali,\nOdia, Punjabi, Tamil, Telugu, Urdu, aggregated\nNiger-Congo languages, Portuguese, Vietnamese,\nSimplified and Traditional Chinese.\nSmaller models struggle more with under-\nrepresented languages such as those in the Indic\nand Niger-Congo family. For example, the loss of\nthe sub-1 billion models goes up at the end of train-\ning for Malayalam, Odia, and Telugu. As data is\nnot repeated, it is unlikely that this effect is due to\noverfitting; we interpret this as insufficient capacity\nin the model to handle many language represen-\ntations, with data in the dominant language sets\ncausing catastrophic forgetting of less represented\nlanguages. In contrast, the largest model sees its\nloss decrease smoothly for every language: larger\nmodels handle multilinguality more easily. Overall,\nscaling laws coefficients are consistent across well-\nrepresented languages, only differing in offsets.\n770\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nArabic, 4.6%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nCatalan, 1.1%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nCode (family), 10.8%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nEnglish, 30.04%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nSpanish, 10.8%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nBasque, 0.15%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nFrench, 12.9%\n0.3B params.\n0.7B params.\n1.2B params.\n2.4B params.\n6.0B params.\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nIndonesian, 1.2%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nAssamese (Indic), 0.01%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nBengali (Indic), 0.5%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nGujarati (Indic), 0.04%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nHindi (Indic), 0.7%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nKannada (Indic), 0.06%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nMalayalam (Indic), 0.1%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nMarathi (Indic), 0.05%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nNepali (Indic), 0.07%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nOdia (Indic), 0.04%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nPunjabi (Indic), 0.05%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nT amil (Indic), 0.2%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nT elugu (Indic), 0.09%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nUrdu (Indic), 0.1%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nNiger-Congo (family), 0.03%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nPortuguese, 4.9%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nVietnamese, 2.7%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nChinese (simplified), 16.2%\n10 2\n 10 1\n 100 101 102\n2\n3\n4\n5\n6\n7\n8\nChinese (traditional), 0.05%\nCompute [PF-days]\nValidation loss\nFigure 3: Scaling laws across languages for the smaller BLOOM models . Black line is Pareto frontier of\noptimality (best loss at a given compute), dashed line is best fit. Fit coefficients are detailed in Appendix B. All\nsufficiently represented languages exhibit similar scaling behaviour, with mostly differences in loss offsets.\n6 Scaling to 176B parameters\nWe now detail how our previous findings influence\nour architecture and scaling decisions for the final\n176B BLOOM model.\nCompute allocation. We have been allocated 18\nweeks of dedicated use of partition with 52 nodes\nof 8x 80GB A100 GPUs on the Jean Zay supercom-\nputer. We set four nodes aside as spare, so that our\ncompute budget amounts to 1,161,216 A100-hours\nin total. Assuming a throughput of 100 model\nTFLOPS, approximately corresponding to state-of-\nthe-art hardware FLOPS of 150 (Narayanan et al.,\n2021), we have a compute budget of 4,838 PF-days\nfor the model training. We round this down to\n4,500 PF-days, this ∼10% safety margin account-\ning for potential downtime and inefficiencies (e.g.,\nbatch size ramp-up) during training. To put this\nnumber in perspective, this is ∼23% more than\nthe training budget of GPT-3. Given this compute\nbudget, our English-only scaling laws in 1 predict\nan optimal allocation for training a 392B parameter\nmodel for 165B tokens. We will use these as an up-\nper bound in size: the largest model we can afford\nis 392B parameters, and the minimum number of\ntokens to train on is 165B tokens.\nModel shape. Kaplan et al. (2020) studied the\ndependence of the loss with model shape, and\nfound only a limited impact within a wide range\nof feed-forward ratios dff /dmodel, aspect ratios\ndmodel/nlayer , and attention head dimensions.\nLevine et al. (2020) proposed a theoretically\nmotivated and empirically backed law describing\nthe optimal compromise between width and depth.\nThey predict that 100B+ parameters models such\nas GPT-3 are too deep, while models in the 10B or\nsmaller range are usually too shallow. For a GPT-\n3-sized model with 175B parameters, they predict\nan ideal depth of 80 layers.\n6.1 Final Model Architecture\nWe set three main guidelines for our final model:\n• 300-400B tokens. We want to guarantee our\nmodel will train on around 300-400B tokens of\ndata. This is in the upper range for models in\nthe size range we are pursuing, ensuring that low-\nresource languages will not be allocated too few\ntokens. Using the C = 6ND approximation\n(Kaplan et al., 2020), with C = 4,500 PF-days\nand D = 300-400B tokens, this constrains the\nmodel size to be around 160-200B parameters.\n771\nModel Size Pretraining Budget Layers Hidden dim. Attention heads\n[Bparams.] [Btokens] [PF-days] num. dim.\nLaMDA (Thoppilan et al., 2022) 137 432 4,106 64 8,192 128 64\nGPT-3 (Brown et al., 2020) 175 300 3,646 96 12,288 96 128\nJ1-Jumbo (Lieber et al., 2021) 178 300 3,708 76 13,824 96 144\nPanGu-α (Zeng et al., 2021) 207 42 604 64 16,384 128 128\nYuan (Wu et al., 2021) 245 180 3,063 76 16,384\nGopher (Rae et al., 2021) 280 300 4,313 80 16,384 128 128\nMT-530B (Smith et al., 2022) 530 270 9,938 105 20,480 128 160\nTable 7: State-of-the-art 100B+ models with publicly available details. Compute budget is expressed in model\nPF-days required for training the models, from the C = 6ND approximation of Kaplan et al. (2020). Number of\ntokens for LaMDA is inferred from reported compute budget and size. Yuan did not report attention head details.\nModel Size Layers Hidden dim. Attention heads Memory Performance\n[params.] num. dim. [GB] [sec/iter.] [TFLOPs]\n(1) 178 82 13,312 64 208 63 104 152\n(2) 178 82 128 104 60 109 146\n(3) 176 70 14,336 112 128 59 105 150\nTable 8: We choose configuration (3) as the final configuration for our 176B model. (1) was rejected because of\nhigh attention heads dimension, and (3) was favored over (2) because of higher throughput. Appendix D details all\n20 final configurations benchmarked, only the best three are displayed here.\n• 70-80 layers. From Levine et al. (2020) and the\nsize constraint above, we estimate that our model\nshould have between 70 and 80 layers.\n• Maximum throughput. Finally, we want the\nfinal architecture to have as high of a throughput\nper GPU as possible, as more compute will trans-\nlate directly into longer pretraining and thus a\nbetter model. Engineering constraints also come\ninto light here: wide shallow models are typically\neasier to parallelize across nodes, up to a point\nwhere excessive tensor paralellism becomes nec-\nessary due to memory constraints.\nWe detail in Table 7 the architectures of current\nstate-of-the-art 100B+ models. From these guide-\nlines, we benchmark 20 model configurations, de-\ntailed in Appendix D. Among these configurations,\nwe select three of particular interest, outlined in Ta-\nble 8. They best fit our guidelines above, and offer\nhigh throughput, maximizing our training budget.\nWe discard configuration (1), as its attention\nheads are much larger than other models in the\nliterature. Configuration (3) is shallower than rec-\nommended by Levine et al. (2020), but delivers\n3% higher throughput compared to (2). Thus, we\nchoose configuration (3) and its better throughput,\nand because a shallower model is easier to deal\nwith at inference time by introducing less latency.\n7 Limitations\nOptimal scaling. Concurrent to this work, Hoff-\nmann et al. (2022) identified more optimal scaling\nlaws. For our compute budget, they would sug-\ngest a 50B parameters model trained for a trillion\ntokens. Interestingly, even in hindsight, it would\nhave been difficult to follow this recommendation\nas we would have been limited by the limited avail-\nability of high-quality multilingual data and by the\nsize of the BigScience training dataset, ROOTS\n(Laurençon et al., 2022). Note that our Figure 1\nreproduces Kaplan et al. (2020) as we did not ac-\ncount for the learning rate schedule as suggested\nby Hoffmann et al. (2022).\nOther hyperparameters. In this work we have\nfocused on a subset of the available hyperparam-\neter space of large language models. We have in-\nvestigated architecture decisions around positional\nembeddings, activation functions and the embed-\nding norm. Alternative attention mechanisms (Tay\net al., 2020) or optimizers are examples of other\ndimensions that could be investigated, potentially\nleading to improved models.\nEfficient fine-tuning. Our study is focused on\nzero-shot use and does not consider efficient fine-\ntuning (Lester et al., 2021; Zaken et al., 2021),\nwhich is quite relevant for large language models,\nand which may lead to different conclusions.\n772\n8 Conclusion\nSeeking to establish the best possible model archi-\ntecture that can be accommodated within a fixed\n1,000,000 GPU-hours compute budget, we have\npresented an extensive study on principled model-\ning decisions for large language models.\nFirst, we have found that complimenting Com-\nmon Crawl data with high-quality cross-domain\ncurated data can boost zero-shot generalization,\nvalidating previous suggestions (Rosset, 2020; Gao\net al., 2020). Through an ablation study, we have\nidentified ALiBi as the position embedding of\nchoice, confirmed the potential of SwiGLU, and\nhighlighted that stabilizing techniques such as em-\nbedding normalization sometimes come at the ex-\npense of zero-shot generalization. Exploring multi-\nlinguality, we have found that multilingual models\nsignificantly underperform their monolingual coun-\nterparts on English zero-shot benchmarks, but that\nthey can learn under-resourced languages along\nwith larger ones if given enough scale. Finally,\nwe identified a candidate architecture for BLOOM\n176B, outlining the full reasoning behind every ar-\nchitectural parameter, including model shape.\nAt variance with previous 100B+ models, such\nas GPT-3 (Brown et al., 2020) or Gopher (Rae et al.,\n2021), this project was conducted in the open, and\nresulted in a number of open-access artefacts. No-\ntable similar projects conducted in parallel to this\none include OPT (Zhang et al., 2022) and GLM\n(Zeng et al., 2022), although they lacked the col-\nlaborative and massively multilingual components\nof this project.\nWe hope our work can help practitioners better\nunderstand modeling decisions, leading to better\nlanguage models, and that this transparency will\naccelerate future similar work.\nAcknowledgements\nThis work was granted access to the HPC resources\nof Institut du développement et des ressources en\ninformatique scientifique (IDRIS) du Centre na-\ntional de la recherche scientifique (CNRS) under\nthe allocation 2021-A0101012475 made by Grand\néquipement national de calcul intensif (GENCI).\nIn particular, all the trainings ran on the Jean-Zay\ncluster of IDRIS, and we want to thank the IDRIS\nteam for responsive support throughout the project,\nin particular Rémi Lacroix. Evaluations of GPT-3\nmodels were provided in part by the Allen Insti-\ntute for Artificial Intelligence. We thank Leo Gao\nfor his expertise and advice on language model\nevaluation.\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nStéphane Aroca-Ouellette, Cory Paik, Alessandro Ron-\ncone, and Katharina Kann. 2021. PROST: Physi-\ncal reasoning about objects through space and time.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 4597–4608,\nOnline. Association for Computational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nQiang Ning Ben Zhou, Daniel Khashabi and Dan Roth.\n2019. “going on a vacation” takes longer than “go-\ning for a walk”: A study of temporal commonsense\nunderstanding. In EMNLP.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533–1544.\n[@BlancheMinerva] Stella Biderman. 2021. You: Gee\nstella, #eleutherai sure hypes rotary embeddings a lot.\nare you sure that they’re that good? me:. Twitter.\nBigScience Workshop. 2022. Bloom (revision\n4ab0472).\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\n773\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\nYann N. Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2016. Language modeling with gated con-\nvolutional networks. CoRR, abs/1612.08083.\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke\nZettlemoyer. 2021. 8-bit optimizers via block-wise\nquantization.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. Advances in Neural Information Process-\ning Systems, 32.\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Nova\nDasSarma, Tom Henighan, Andy Jones, Nicholas\nJoseph, Jackson Kernion, Ben Mann, Amanda Askell,\net al. 2022. Predictability and surprise in large gener-\native models. arXiv preprint arXiv:2202.07785.\nLeo Gao. 2021. On the sizes of openai api models.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: an\n800GB dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roem-\nmele. 2012. SemEval-2012 task 7: Choice of plau-\nsible alternatives: An evaluation of commonsense\ncausal reasoning. In *SEM 2012: The First Joint\nConference on Lexical and Computational Seman-\ntics – Volume 1: Proceedings of the main conference\nand the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Eval-\nuation (SemEval 2012) , pages 394–398, Montréal,\nCanada. Association for Computational Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. ArXiv, abs/2003.11080.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First quora dataset release: Question pairs.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567–2577.\nMatt Gardner Johannes Welbl, Nelson F. Liu. 2017.\nCrowdsourcing multiple choice science questions.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\n774\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface:a challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL).\nBoseop Kim, Hyoungseok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Dong Hyeon Jeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim,\nDong Hyung Seo, Heungsub Lee, Minyoung Jeong,\nSungjae Lee, Minsub Kim, SukHyun Ko, Seokhun\nKim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-\nHyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin\nSuh, Sookyo In, Jinseong Park, Kyungduk Kim,\nHiun Kim, Jisu Jeong, Yong Goo Yeo, Dong hyun\nHam, Do-Hyoung Park, Min Young Lee, Jaewoo\nKang, Inho Kang, Jung-Woo Ha, Woo Chul Park,\nand Nako Sung. 2021. What changes can large-scale\nlanguage models bring? intensive study on hyper-\nclova: Billions-scale korean generative pretrained\ntransformers. ArXiv, abs/2109.04650.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nAhmad Lashgar, Amirali Baniasadi, and Ahmad Khon-\nsari. 2013. Warp size impact in gpus: large or small?\nIn GPGPU@ASPLOS.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro V on Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, Jörg\nFrohberg, Mario Šaško, Quentin Lhoest, Angelina\nMcMillan-Major, Gérard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Romero Muñoz,\nJian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid\nAlmubarak, Vu Minh Chien, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ife-\noluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas\nPai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-\ngaret Mitchell, Sasha Luccioni, and Yacine Jernite.\n2022. The bigscience ROOTS corpus: A 1.6TB\ncomposite multilingual dataset. In Thirty-sixth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth International Conference on the Principles of\nKnowledge Representation and Reasoning.\nYoav Levine, Noam Wies, Or Sharir, Hofit Bata, and\nAmnon Shashua. 2020. Limits to depth efficiencies\nof self-attention. Advances in Neural Information\nProcessing Systems, 33:22640–22651.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\n2021. Jurassic-1: Technical details and evaluation.\nTechnical report, AI21 Labs.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Ves Stoyanov,\nand Xian Li. 2021. Few-shot learning with multilin-\ngual language models. ArXiv, abs/2112.10668.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2020. Logiqa: A chal-\nlenge dataset for machine reading comprehension\nwith logical reasoning. CoRR, abs/2007.08124.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In International Conference on\nLearning Representations.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,\nJulie Bernauer, Bryan Catanzaro, et al. 2021. Ef-\nficient large-scale language model training on gpu\nclusters using megatron-lm. In Proceedings of the\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n15.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low resource infras-\ntructures. Proceedings of the Workshop on Chal-\nlenges in the Management of Large Corpora (CMLC-\n7) 2019. Cardiff, 22nd July 2019, pages 9 – 16,\nMannheim. Leibniz-Institut für Deutsche Sprache.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\n775\nMohammad Taher Pilehvar and os’e Camacho-\nCollados. 2018. Wic: 10, 000 example pairs for\nevaluating context-sensitive representations. CoRR,\nabs/1808.09121.\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. InProceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157–163, Valencia, Spain.\nAssociation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le.\n2017. Searching for activation functions. arXiv\npreprint arXiv:1710.05941.\nCorby Rosset. 2020. Turing-nlg: A 17-billion-\nparameter language model by microsoft.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2019. Winogrande: An ad-\nversarial winograd schema challenge at scale. arXiv\npreprint arXiv:1907.10641.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang A. Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M SAIFUL BARI, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal V . Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Févry, Ja-\nson Alan Fries, Ryan Teehan, Stella Rose Bider-\nman, Leo Gao, T. G. Owe Bers, Thomas Wolf, and\nAlexander M. Rush. 2021. Multitask prompted train-\ning enables zero-shot task generalization. ArXiv,\nabs/2110.08207.\nNoam Shazeer. 2020. Glu variants improve transformer.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yun-\nfeng Liu. 2021. Roformer: Enhanced transformer\nwith rotary position embedding. arXiv preprint\narXiv:2104.09864.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2020. Long\nrange arena: A benchmark for efficient transformers.\narXiv preprint arXiv:2011.04006.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fe-\ndus, Samira Abnar, Hyung Won Chung, Sharan\nNarang, Dani Yogatama, Ashish Vaswani, and Don-\nald Metzler. 2021. Scale efficiently: Insights from\npre-training and fine-tuning transformers. ArXiv,\nabs/2109.10686.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nThomas Wang, Adam Roberts, Daniel Hesslow,\nTeven Le Scao, Hyung Won Chung, Iz Beltagy, Julien\nLaunay, and Colin Raffel. 2022. What language\nmodel architecture and pretraining objective work\nbest for zero-shot generalization? arXiv preprint\narXiv:2204.05832.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\n776\nShaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang,\nChong Shen, Hongli Liu, Feng Li, Hong Zhu, Jian-\ngang Luo, Liang Xu, et al. 2021. Yuan 1.0: Large-\nscale pre-trained language model in zero-shot and\nfew-shot learning. arXiv preprint arXiv:2110.04725.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mt5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In NAACL.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitfit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, et al. 2021. Pangu-α: Large-\nscale autoregressive pretrained chinese language\nmodels with auto-parallel computation. arXiv\npreprint arXiv:2104.12369.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\n777\nA Open artefacts: models, code, and logs\nWe make public all artefacts produced as part of this work:\n• Models. All trained models are centralized at https://huggingface.co/bigscience;\n• Code. All code is available at https://github.com/bigscience-workshop/\nMegatron-DeepSpeed/tree/main/megatron;\n• Discussions and logbook. The notes from the weekly meetings of our work-\ning group are made available at https://docs.google.com/document/d/\n1qbIkhd6bvbOsJOWXL7SfKQ0jey3MWQYQb_SshqH1LII/.\nB Multilingual scaling laws\nLanguage Proportion [%] αc Cm\nArabic 4.6 0.057 1.16\nCatalan 1.1 0.057 1.11\nCode 10.8 0.054 0.94\nEnglish 30.0 0.051 1.08\nSpanish 10.8 0.050 1.01\nBasque 0.15 0.069 1.28\nFrench 12.9 0.047 1.06\nIndonesian 1.2 0.051 1.14\nAssamese 0.01 0.051 1.31\nBengali 0.5 0.037 1.15\nGujarati 0.04 0.051 1.30\nHindi 0.7 0.045 1.14\nKannada 0.06 0.046 1.26\nMalayalam 0.1 0.044 1.17\nMarathi 0.05 0.046 1.23\nNepali 0.07 0.055 1.25\nOdia 0.04 0.044 1.25\nPunjabi 0.05 0.043 1.20\nTamil 0.2 0.030 1.14\nTelugu 0.09 0.056 1.31\nUrdu 0.1 0.068 1.31\nNiger-Congo (family) 0.03 0.039 1.22\nPortuguese 4.9 0.049 1.05\nVietnamese 2.7 0.053 1.08\nChinese (simplified) 16.2 0.052 1.09\nChinese (traditionnal) 0.05 0.050 1.15\nTable 9: Best scaling law fit per language. We fit L(C) =CmC−αc to the runs reported in Figure 3. But for a\nhandful of languages which are poorly represented in the overall mixture (Basque, most of the Indic family, and\nNiger-Congo languages), scaling mostly different in offset Cm, not in exponent αc.\n778\nC Evaluation details\nTask Type Random baseline\nARC (Clark et al., 2018) Challenge Natural Language Inference 25.0\nEasy 25.0\nGLUE MRPC (Dolan and Brockett, 2005) Paraphrase Identification 50.0\nQQP (Iyer et al., 2017) Paraphrase Identification 50.0\nHellaSwag (Zellers et al., 2019) Sentence Completion 25.0\nLAMBADA (Paperno et al., 2016) Sentence Completion 0.0\nLogiQA (Liu et al., 2020) Multiple-Choice Question Answering 25.0\nMathQA (Amini et al., 2019) Multiple-Choice Question Answering 20.1\nMC-TACO (Ben Zhou and Roth, 2019) Multiple-Choice Question Answering 36.2\nOpenBookQA (Mihaylov et al., 2018) Multiple-Choice Question Answering 25.0\nPIQA (Bisk et al., 2020) Multiple-Choice Question Answering 50.0\nPROST (Aroca-Ouellette et al., 2021) Multiple-Choice Question Answering 25.0\nPudMedQA (Jin et al., 2019) Multiple-Choice Question Answering 33.3\nQNLI (Rajpurkar et al., 2016; Wang et al., 2019) Sentence Completion 50.0\nRace (Lai et al., 2017) Closed-Book Question Answering 25.0\nSciQ (Johannes Welbl, 2017) Multiple-Choice Question Answering 25.0\nSST (Socher et al., 2013) Sentiment 50.0\nSuperGLUE Boolq (Clark et al., 2019) Multiple-Choice Question Answering 50.0\nCOPA (Gordon et al., 2012) Sentence Completion 50.0\nMultiRC (Khashabi et al., 2018) Multiple-Choice Question Answering 5.8\nRTE (Dagan et al., 2005) Natural Language Inference 50.0\nWIC (Pilehvar and os’e Camacho-Collados, 2018) Word Sense Disambiguation 50.0\nWSC (Levesque et al., 2012) Word Sense Disambiguation 50.0\nTriviaQA (Joshi et al., 2017) Closed-Book Question Answering 0.0\nWebQuestions (Berant et al., 2013) Closed-Book Question Answering 0.0\nWinogrande (Sakaguchi et al., 2019) Coreference resolution 50.0\nWNLI (Sakaguchi et al., 2019) Natural Language Inference 50.0\nEAI harness 33.3\nTable 10: Evaluation tasks considered in the EAI harness and random baselines.\n779\nD Architecture details\nARCHITECTURE PARALLELISM PERFORMANCE\nSize Hidden dim. Layers Attention heads Data Tensor Pipeline MBS Memory Throughput\n[Bparams.] num. dim. [GB] [s/iter.] [TFLOPs]\n206 14,336 82 128 112 8 4 12 2 OOM\n203 13,312 94 128 104 8 4 12 2 67 124,1 146,1\n195 12,288 106\n128 96\n8 4 12\n2 67 121,4 143,7\n96 128 4 79 120,3 145,0\n128 2 65 118,8 146,9\n64 192 67 116,5 149,8\n184 12,288 100 64 192\n16 4\n6\n2 OOM\n1 OOM\n8 8 4 72 121,0 136,2\n2 61 140,0 117,9\n178 13,312 82\n128 104\n8 4\n12\n2 60 108,8 145,7\n104 128 62 123,7 128,1\n64 208 4 74 104,8 151,2\n4 8 52 111,8 141,8\n8 4 2 63 104,5 151,7\n176 14,336 70\n128 112\n8 4 12\n2 60 105,9 148,1\n112 128 59 104,5 150,1\n64 224\n4 73 102,3 153,3\n2 59 102,0 153,7\n4 8 12 40 121,6 128,9\nTable 11: Throughput and memory usage of considered models sizes. Note that pipeline parallelism here\nconsiders equal \"slots\" for embeddings and Transformer layers. This is important to optimize pipeline use, as our\nmultilingual embeddings are quite large (250k).\n780\nE All Results\nAblation Dataset Embedding Activation Embedding Norm Parameters 112GT 250GT 300GT\nEmbeddings OSCAR Learned GELU No 1.3B 41.71\nEmbeddings OSCAR None GELU No 1.3B 41.23\nEmbeddings OSCAR Rotary GELU No 1.3B 41.46\nEmbeddings OSCAR ALiBi GELU No 1.3B 43.70\nDataset The Pile Learned GELU No 1.3B 42.79 43.12 43.46\nDataset C4 Learned GELU No 1.3B 42.77\nDataset OSCAR Learned GELU No 1.3B 42.79\nActivation The Pile Learned GELU No 1.3B 42.79\nActivation The Pile Learned SwiGLU No 1.3B 42.95\nEmbedding Norm The Pile Learned GELU No 1.3B 42.79 43.12 43.46\nEmbedding Norm The Pile Learned GELU Yes 1.3B 42.24\nMultilinguality OSCAR-ML Learned GELU No 1.3B 38.55\nMultilinguality OSCAR Learned GELU No 1.3B 41.72\nScale OSCAR Learned GELU No 1.3B 41.72\nScale OSCAR Learned GELU No 13B 47.09\nTable 12: Summary of all results obtained in this study. The final three columns indicate the average EAI Harness\nresults at across different billion tokens trained. Some rows are duplicated for ease of reading.\n781\nPublic Name OpenAI: babbage Openai: curie gpt-neo 1.3B\nDataset C4 OSCAR The Pile The Pile The Pile The Pile The Pile OSCAR The Pile OSCAR OSCAR OSCAR OSCAR-ML\nEmbeddings Learned Learned Learned Learned Learned Learned Learned Learned Learned Rotary ALiBi None Learned\nActivation GELU GELU GELU GELU GELU GELU GELU GELU SwiGLU GELU GELU GELU GELU\nEmbedding Norm No No No No No No No No No No No No No\nParameters in billion 1.3 6.7 1.3 1.3 1.3 1.3 1.3 1.3 1.3 1.3 13 1.3 1.3 1.3 1.3 1.3\nTokens trained in billion 300 300 300 112 112 112 250 300 300 330 300 112 112 112 112 112\ntask metric\narc_challenge acc arc_challengeacc 0.276 0.334 0.231 0.243 0.249 0.258 0.264 0.260 0.242 0.250 0.322 0.247 0.236 0.252 0.249 0.212\narc_challenge acc_norm arc_challengeacc_norm 0.295 0.375 0.259 0.274 0.261 0.275 0.277 0.286 0.277 0.290 0.342 0.268 0.270 0.276 0.260 0.243\narc_easy acc arc_easyacc 0.597 0.685 0.562 0.561 0.560 0.556 0.569 0.601 0.568 0.582 0.681 0.557 0.554 0.575 0.537 0.484\narc_easy acc_norm arc_easyacc_norm 0.555 0.633 0.502 0.503 0.478 0.506 0.518 0.528 0.516 0.515 0.600 0.502 0.476 0.491 0.461 0.434\nboolq acc boolqacc 0.629 0.666 0.620 0.546 0.566 0.520 0.551 0.606 0.558 0.566 0.587 0.540 0.584 0.563 0.526 0.597\ncopa acc copaacc 0.810 0.850 0.690 0.700 0.720 0.710 0.710 0.730 0.690 0.690 0.880 0.660 0.690 0.780 0.680 0.710\nhellaswag acc hellaswagacc 0.429 0.504 0.387 0.422 0.404 0.374 0.385 0.405 0.378 0.380 0.542 0.379 0.410 0.422 0.395 0.340\nhellaswag acc_norm hellaswagacc_norm 0.545 0.664 0.489 0.551 0.515 0.464 0.486 0.521 0.477 0.476 0.716 0.475 0.524 0.549 0.495 0.424\nlambada acc lambadaacc 0.625 0.694 0.572 0.469 0.481 0.569 0.575 0.609 0.581 0.580 0.634 0.574 0.496 0.501 0.454 0.408\nlogiqa acc logiqaacc 0.201 0.215 0.197 0.206 0.237 0.210 0.218 0.203 0.217 0.223 0.232 0.215 0.210 0.215 0.237 0.218\nlogiqa acc_norm logiqaacc_norm 0.269 0.292 0.273 0.267 0.270 0.275 0.286 0.269 0.281 0.280 0.275 0.272 0.254 0.272 0.293 0.283\nmathqa acc mathqaacc 0.244 0.251 0.241 0.233 0.222 0.249 0.248 0.263 0.246 0.245 0.238 0.245 0.234 0.237 0.215 0.223\nmathqa acc_norm mathqaacc_norm 0.242 0.247 0.237 0.228 0.228 0.246 0.245 0.259 0.242 0.242 0.235 0.234 0.229 0.238 0.221 0.222\nmc_taco f1 mc_tacof1 0.458 0.484 0.493 0.361 0.293 0.485 0.488 0.494 0.487 0.489 0.497 0.493 0.461 0.337 0.477 0.387\nmrpc acc mrpcacc 0.578 0.684 0.684 0.684 0.588 0.684 0.684 0.684 0.679 0.679 0.677 0.684 0.684 0.684 0.679 0.302\nmrpc f1 mrpcf1 0.718 0.812 0.812 0.812 0.702 0.812 0.812 0.812 0.808 0.809 0.806 0.812 0.812 0.812 0.808 0.090\nmultirc acc multircacc 0.018 0.015 0.018 0.018 0.026 0.023 0.024 0.023 0.025 0.008 0.018 0.026 0.009 0.011 0.016 0.040\nopenbookqa acc openbookqaacc 0.224 0.290 0.216 0.220 0.200 0.190 0.196 0.222 0.194 0.208 0.294 0.214 0.212 0.224 0.210 0.170\nopenbookqa acc_norm openbookqaacc_norm 0.336 0.386 0.336 0.336 0.328 0.316 0.314 0.334 0.302 0.312 0.412 0.320 0.344 0.340 0.332 0.276\npiqa acc piqaacc 0.745 0.763 0.711 0.732 0.716 0.693 0.704 0.716 0.698 0.706 0.777 0.693 0.720 0.729 0.711 0.674\npiqa acc_norm piqaacc_norm 0.746 0.772 0.711 0.730 0.721 0.705 0.705 0.717 0.698 0.701 0.788 0.689 0.721 0.731 0.711 0.682\nprost acc prostacc 0.270 0.288 0.238 0.243 0.237 0.249 0.229 0.204 0.219 0.226 0.281 0.244 0.287 0.280 0.240 0.253\nprost acc_norm prostacc_norm 0.260 0.295 0.308 0.293 0.303 0.268 0.271 0.268 0.292 0.305 0.283 0.276 0.296 0.332 0.300 0.313\npubmedqa acc pubmedqaacc 0.611 0.622 0.544 0.573 0.438 0.563 0.589 0.662 0.612 0.612 0.615 0.589 0.507 0.514 0.486 0.412\nqnli acc qnliacc 0.512 0.529 0.499 0.476 0.507 0.505 0.506 0.505 0.499 0.499 0.517 0.498 0.493 0.481 0.493 0.493\nqqp acc qqpacc 0.372 0.441 0.382 0.396 0.384 0.381 0.370 0.375 0.371 0.369 0.368 0.435 0.370 0.423 0.370 0.389\nqqp f1 qqpf1 0.534 0.515 0.522 0.530 0.519 0.534 0.537 0.537 0.538 0.538 0.533 0.495 0.539 0.475 0.537 0.505\nrace acc raceacc 0.356 0.386 0.341 0.330 0.323 0.334 0.329 0.344 0.321 0.323 0.374 0.337 0.317 0.344 0.332 0.326\nrte acc rteacc 0.585 0.552 0.603 0.502 0.534 0.563 0.549 0.578 0.563 0.549 0.524 0.527 0.545 0.524 0.527 0.505\nsciq acc sciqacc 0.867 0.919 0.860 0.825 0.810 0.838 0.853 0.868 0.860 0.867 0.895 0.849 0.818 0.828 0.816 0.793\nsciq acc_norm sciqacc_norm 0.809 0.896 0.770 0.747 0.717 0.755 0.762 0.792 0.791 0.803 0.815 0.770 0.718 0.728 0.698 0.702\nsst acc sstacc 0.732 0.666 0.656 0.676 0.560 0.753 0.721 0.501 0.528 0.710 0.514 0.760 0.493 0.588 0.588 0.510\ntriviaqa acc triviaqaacc 0.115 0.195 0.052 0.027 0.025 0.056 0.065 0.058 0.047 0.049 0.133 0.050 0.031 0.039 0.028 0.021\nwebqs acc webqsacc 0.048 0.065 0.017 0.012 0.004 0.023 0.026 0.023 0.020 0.021 0.027 0.012 0.006 0.004 0.015 0.001\nwic acc wicacc 0.495 0.500 0.500 0.495 0.508 0.495 0.500 0.500 0.498 0.500 0.498 0.500 0.498 0.492 0.500 0.500\nwinogrande acc winograndeacc 0.595 0.648 0.551 0.564 0.565 0.536 0.552 0.560 0.533 0.543 0.647 0.538 0.564 0.583 0.543 0.519\nwsc acc wscacc 0.394 0.558 0.365 0.539 0.567 0.365 0.365 0.365 0.414 0.385 0.500 0.365 0.394 0.635 0.462 0.539\nAvg acc 45.30% 49.28% 42.94% 42.77% 41.72% 42.79% 43.12% 43.46% 42.24% 43.08% 47.09% 42.95% 41.45% 43.70% 41.23% 38.55%\n782",
  "topic": "STELLA (programming language)",
  "concepts": [
    {
      "name": "STELLA (programming language)",
      "score": 0.719771146774292
    },
    {
      "name": "Computer science",
      "score": 0.6344572901725769
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.3801906704902649
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35174477100372314
    },
    {
      "name": "Art history",
      "score": 0.34917789697647095
    },
    {
      "name": "Art",
      "score": 0.24033311009407043
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1332258667",
      "name": "Cape Eleuthera Institute",
      "country": "BS"
    },
    {
      "id": "https://openalex.org/I1322124587",
      "name": "Booz Allen Hamilton (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210129651",
      "name": "Laboratoire de Physique de l'ENS",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ],
  "cited_by": 32
}