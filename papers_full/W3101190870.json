{
  "title": "Examining the rhetorical capacities of neural language models",
  "url": "https://openalex.org/W3101190870",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5024250477",
      "name": "Zining Zhu",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5062022442",
      "name": "Chuer Pan",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5058570740",
      "name": "Mohamed Abdalla",
      "affiliations": [
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5056256317",
      "name": "Frank Rudzicz",
      "affiliations": [
        null,
        "University of Toronto",
        "Vector Institute",
        "St Michael's Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W2251293245",
    "https://openalex.org/W2979666134",
    "https://openalex.org/W2971141916",
    "https://openalex.org/W2295352645",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2516583532",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2080322628",
    "https://openalex.org/W2252267789",
    "https://openalex.org/W2565349465",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2877801623",
    "https://openalex.org/W2045738181",
    "https://openalex.org/W2251618894",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2154414741",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W1894075015",
    "https://openalex.org/W2963194097",
    "https://openalex.org/W2551276343",
    "https://openalex.org/W2044599851",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W1997210479",
    "https://openalex.org/W2962855015",
    "https://openalex.org/W2529194139",
    "https://openalex.org/W2892205701",
    "https://openalex.org/W2135336649",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W4248458756",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W1599880985",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2920663707",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3010386188",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W332673702",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W2251951653",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2740928030",
    "https://openalex.org/W2741164290",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970648896",
    "https://openalex.org/W1597655096",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W2416321009",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1669238674",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2950813464"
  ],
  "abstract": "Recently, neural language models (LMs) have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, there has been no analysis to date of the inter-sentential, rhetorical knowledge. In this paper, we propose a method that quantitatively evaluates the rhetorical capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory (RST). Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method shows an avenue towards quantifying the rhetorical capacities of neural LMs.",
  "full_text": "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 16–32\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\n16\nExamining the rhetorical capacities of neural language models\nZining Zhu1,2, Chuer Pan1, Mohamed Abdalla1,2, Frank Rudzicz1,2,3,4\n1: University of Toronto; 2: Vector Institute\n3: Li Ka Shing Knowledge Institute, St Michael’s Hospital\n4: Surgical Safety Technologies\nzining@cs.toronto.edu, chuer.pan@mail.utoronto.ca\n{msa, frank}@cs.toronto.edu\nAbstract\nRecently, neural language models (LMs) have\ndemonstrated impressive abilities in generat-\ning high-quality discourse. While many recent\npapers have analyzed the syntactic aspects en-\ncoded in LMs, to date, there has been no anal-\nysis of the inter-sentential, rhetorical knowl-\nedge. In this paper, we propose a method that\nquantitatively evaluates the rhetorical capaci-\nties of neural LMs. We examine the capaci-\nties of neural LMs understanding the rhetoric\nof discourse by evaluating their abilities to en-\ncode a set of linguistic features derived from\nRhetorical Structure Theory (RST). Our ex-\nperiments show that BERT-based LMs outper-\nform other Transformer LMs, revealing the\nricher discourse knowledge in their interme-\ndiate layer representations. In addition, GPT-\n2 and XLNet apparently encode less rhetor-\nical knowledge, and we suggest an explana-\ntion drawing from linguistic philosophy. Our\nmethod presents an avenue towards quantify-\ning the rhetorical capacities of neural LMs.\n1 Introduction\nIn recent years, neural LMs (especially contex-\ntualized LMs) have shown profound abilities to\ngenerate texts that could be almost indistinguish-\nable from human writings (Radford et al., 2019).\nNeural LMs could be used to generate concise\nsummaries (Song et al., 2019), coherent stories\n(See et al., 2019), and complete documents given\nprompts (Keskar et al., 2019). It is natural to ques-\ntion their source and extent of rhetorical knowl-\nedge: What makes neural LMs articulate, and\nhow? While some recent works query the linguistic\nknowledge (Hewitt and Manning, 2019; Liu et al.,\n2019a; Chen et al., 2019; Belinkov et al., 2017),\nthis open question remain unanswered. We hy-\npothesize that contextualized neural LMs encode\nrhetorical knowledge in their intermediate repre-\nsentations, and would like to quantify the extent\nthey encode rhetorical knowledge.\nTo verify our hypothesis, we hand-craft a set of\n24 rhetorical features including those used to exam-\nine rhetorical capacities of students (Mohsen and\nAlshahrani, 2019; Liu and Kunnan, 2016; Zhang,\n2013; Powers et al., 2001), and evaluate how well\nneural LMs encode these rhetorical features in the\nrepresentations while encoding texts.\nRecent work has started to evaluate encoded fea-\ntures from hidden representations. Among them,\nprobing (Alain and Bengio, 2017; Adi et al., 2017)\nhas been a popular choice. Previous work probed\nmorphological (Belinkov et al., 2017; Bisazza and\nTump, 2018), agreement (Giulianelli et al., 2018),\nand syntactic features (Hewitt and Manning, 2019;\nHewitt and Liang, 2019). Probing involves optimiz-\ning a simple projection model from representations\nto features. The loss of this optimization measures\nthe difﬁculty to decode features from the represen-\ntations.\nIn this work, we use a probe containing self at-\ntention mechanism. We ﬁrst project the variable-\nlength embeddings to a ﬁxed-length latent represen-\ntation per document. Then, we apply a simple diag-\nnostic classiﬁer to detect rhetorical features from\nthis latent representation. This design of probe re-\nduces the total number of parameters, and enable us\nto better understand each model’s ability to encode\nrhetorical knowledge. We ﬁnd that:\n•The BERT-based LMs encode more rhetorical\nfeatures, and in a more stable manner, than\nother models.\n•The semantics of non-contextualized embed-\ndings also pertain to some rhetorical features,\nbut less than most layers of contextualized\nlanguage models.\nThese observations allow us to investigate the\nmechanisms of neural LMs to better understand the\n17\ndegree to which they encode linguistic knowledge.\nWe demonstrate how discourse-level features can\nbe queried and analyzed from neural LMs. All of\nour code and parsed tree data will be available at\ngithub.\n2 Structural analysis of discourse\nVarious frameworks exist for “good discourse”\n(Lawrence and Reed, 2019; Irish and Weiss, 2009;\nToulmin, 1958), but most of them are inaccessi-\nble to quantitative analysis. In this work, we use\nRhetorical Structure Theory (Mann and Thomp-\nson, 1988; Mann et al., 1989) since it represents\nthe structures of discourse using trees, allowing\nstraightforward quantitative analysis. There are\ntwo components in an RST parse-tree:\n•Each leaf node represents an elementary dis-\ncourse unit (EDU). The role of an EDU in an\narticle is similar to that of a word in a sen-\ntence.\n•Each non-leaf node denotes a relation involv-\ning its two children. Often, one of the children\nis more dependent on the other, and less es-\nsential to the writer’s purpose. This child is\nreferred to as “satellite”, while the more cen-\ntral child is the “nucleus”.\nNS-Contrast\nSN-Attribution\nI didn’t\nknow\nthis is\nfrom C\nbut it is\nvery good!\nFigure 1: A portion of an RST tree, selected from\nIMDB (Maas et al., 2011) train/pos/1 7.txt,\nand parsed with Feng and Hirst (2014). Nodes with\nrectangle borders are discourse relations, and those\nwithout borders are individual EDUs. The “N” and “S”\npreﬁx for discourse relations stand for “nucleus” and\n“satellite” respectively.\nTree representations are clear, easy to under-\nstand, and allow us to compute features to numeri-\ncally depict the rhetorical aspects of documents.\n2.1 Rhetorical features\nPrevious work used RST features to analyze the\nquality of discourse, to assess writing abilities\n(Wang et al., 2019; Zhang, 2013), examine linguis-\ntic coherence (Feng et al., 2014; Abdalla et al.,\n2017), and to analyze arguments (Chakrabarty\net al., 2019). In this project, we extract similar\nRST features in the following three categories:\nDiscourse relation occurrences (Sig) We in-\nclude the number of relations detected in each\ndocument. There are 18 relations in this cate-\ngory1. Unfortunately, the relations adopted by\nopen-source RST parsers are not uniﬁed. To\nallow for comparison against other parsers, we\ndo not differentiate subtle differences between\nrelations, therefore grouping very similar rela-\ntions, following the approach in (Feng and Hirst,\n2012). (E.g., we consider both Topic-Shift and\nTopic-Drift to be a Topic-Change). Speciﬁ-\ncally, this approach does not differentiate between\nthe sequence of nucleus and satellite (e.g., NS-\nEvaluation and SN-Evaluation are both considered\nas an Evaluation).\nTree property features (Tree) We compute the\ndepth and the Yngve depth (the number of right-\nbranching in the tree) (Yngve, 1960) of each tree\nnode, and include their mean and variance as char-\nacteristic features, following previous work extract-\ning tree linguistic features (Li et al., 2019; Zhu\net al., 2019).\nEDU related features (EDU) We include the\nmean and variance of EDU lengths of each doc-\nument. We hypothesize the longer EDUs indicate\nhigher levels of redundancy in discourse, hence ex-\ntracting rhetorical features require memory across\nlonger spans.\nOverall, there are 24 features from three cate-\ngories. We normalize them to zero mean and unit\nvariance, and take these RST features for prob-\ning. The features are not independent of each other.\nSpeciﬁcally, the features of each group tend to de-\nscribe the same property from different aspects.2\n1The 18 relations are: Attribution, Background, Cause,\nComparison, Condition, Contrast, Elaboration, Enable-\nment, Evaluation, Explanation, Joint, Manner-Means, Topic-\nComment, Summary, Temporal, Topic-Change, Textual-\norganization, and Same-unit.\n2For example, Sig features describe the composition of\nthe document in a histogram. For the same document, if a re-\nlation is changed, e.g., from Contrast to Attribution,\nthen the occurrence of both Contrast and Attribution\nare affected.\n18\nFigure 2: RST relation occurrences per document. RST-DT contain longer documents than IMDB on av-\nerage. However, the distributions of frequencies between these two datasets are relatively consistent, with\nElaboration, Joint, and Attribution the most frequent signals.\n2.2 Probe\nOur probing method contains two weight parame-\nters, Wd and Wp. First, we embed a document with\nLtokens using a neural LM with Ddimensions to\nget a raw representation matrix X ∈RL×D. We\nuse a projection matrix Wd ∈RD×d to reduce the\nembedding dimension from D(e.g., D= 768for\nBERT and 2048 for XLM) to a much smaller one,\nd. Then, we use self attention similar to Lin et al.\n(2017) to collect the information spread across the\ndocument to a condensed form:\nA= (XWd)T (XWd) ∈Rd×d\nWe ﬂatten Ainto a vector with ﬁxed size: ˜A=\n(d2,1). We use a probing matrix Wp ∈Rd2×m\nto extract RST features v ∈Rm from attention,\nnormalize them to zero mean and unit variance,\nand optimize based on the expected L2 error:\nmin\nWd,Wp\nE||WT\np ˜A−v||2\nNote that the reduction from Dto dusing Wp\nis necessary, because it signiﬁcantly lowers the\nnumber of parameters of the probing model. If\nthere were no Wd (i.e., d = 768), then Wp alone\nwould require 7682mparameters to probe mfea-\ntures. Now, we let d= 10, then Wd and Wp com-\nbined have D×d+ d2m≈7680 + 100mparame-\nters. Considering m∈O(101), the total parameter\nsize is reduced from O(106) to O(103).\nThere is one more step before we can use this\nloss to measure the difﬁculty of probing rhetorical\nfeatures. L2 error scales linearly with the dimen-\nsion of features m, so it is necessary to normalize\nthe L2 error by m, to ensure that the losses can be\ncompared across linguistic feature sets. The dif-\nﬁculty of probing a group of mfeatures v ∈Rm\ntherefore is:\nDifﬁculty = 1\nmE\n⏐⏐⏐\n⏐⏐⏐WT\np ˜A−v\n⏐⏐⏐\n⏐⏐⏐\n2\n3 Experiments\n3.1 Data\nMost state-of-the-art rhetorical parsers are trained\non either Penn Discourse Treebank (Ji and Eisen-\nstein, 2014; Feng and Hirst, 2012) or RST-DT\n(Feng and Hirst, 2014; Joty et al., 2015; Surdeanu\net al., 2015; Heilman and Sagae, 2015; Li et al.,\n2016; Wang et al., 2017; Yu et al., 2018). Although\nthe documents contain accurate discourse annota-\ntions, RST-DT (Carlson et al., 2001) only has 385\ndocuments. The Penn Discourse Treebank (Prasad\net al., 2008) has 2,159 documents but their anno-\ntations do not follow the RST framework. So in\naddition to RST-DT, we extend the analysis to a\n100 times larger dataset, IMDB (Maas et al., 2011).\nIMDB contains 50,000 movie reviews without\ndiscourse annotations. In these reviews, the authors\nexplain and elaborate upon their opinions towards\ncertain movies and give ratings. We removed html\ntags, and attempt to parse all of them (i.e., both\ntrain and test data) using a two-pass parser from\nFeng and Hirst (2014). We discarded 1,977 docu-\nments that the RST parser generate ill-formatted\ntrees3. Of the remaining documents, we addition-\nally ﬁltered out those with sequence lengths greater\nthan 512 tokens4, resulting in 40,833 documents.\n3As determined by nltk.tree.\n4As determined by any one of the tokenizers, since these\n19\nFigure 3: Loss vs layer plot of six neural LMs on four RST feature sets on IMDB. The solid lines represent all\nRST features combined, while each dash-dotted line denotes one component (EDU, Sig, or Tree feature group for\nred, green, and blue respectively). In general, BERT-based LMs (BERT, BERT-multi, RoBERTa) encode rhetorical\nfeatures in a more stable and easy-to-probe manner than the rest.\nAfter parsing each document into an “RST-tree”,\nwe extracted the features mentioned in Section 2.1\nfrom these parsed trees. Figure 2 shows the occur-\nrence of the 18 RST relations per document, and\nTable 1 shows the statistics of remaining 6 features.\nIn addition, we include several examples of parsed\nRST trees in Appendix.\nFeature name Mean ±stdev\ntree depth mean 3.9 ±1.4\ntree depth var 4.6 ±4.2\ntree Yngve mean 9.2 ±8.8\ntree Yngve var 100.6 ±164.6\nedu len mean 8.6 ±1.4\nedu len var 21.8 ±16.0\nTable 1: Statistics of the 6 non-occurrence-based RST\nfeatures. The preﬁx “tree ” here refers to the parsed\n“RST-tree”.\nlanguage models come with their own tokenizers. Note that\nRoBERTa adds two special tokens, so this threshold becomes\n510 for RoBERTa.\n3.2 Language models\nWe considered the following popular neural LMs:\n•BERTBASE (Devlin et al., 2019) This LM with\n110M parameters is built with 12-layer Trans-\nformer encoder (Vaswani et al., 2017) with\n768 hidden dimensions. It is trained with\nmasked LM (i.e., cloze) and next sentence\nprediction objectives using 16GB text.\n•BERT-multi (Wolf et al., 2019) Same as\nBERT, BERT-multi is also a 12-layer Trans-\nformer encoder with 768 hidden dimensions\nand 110M parameters. Its difference from\nBERT is that, BERT-multi is trained on top\n104 languages with the largest Wikipedia.\n•RoBERTa (Liu et al., 2019b) is an enhanced\nversion of BERT with the same architecture,\nsimilar masked LM objectives, and 10 times\nlarger training corpus (over 160GB).\n•GPT-2 (Radford et al., 2019) is a 12-layer\nTransformer decoder with 768 hidden dimen-\nsions. There are 117M parameters in total.\nGPT-2 is pretrained on 40GB of text. Unlike\nBERT, GPT-2 is a uni-directional LM.\n20\nFigure 4: Probing loss, compared to those from non-contextualized baselines, for four feature groups, on IMDB.\nBERT-based neural LMs stably outperform the word embedding baselines in almost all layers.\n•XLM (Lample and Conneau, 2019) is 12-layer\nTransformer with 2048-hidden dimensions.\nWe use the English model trained with masked\nlanguage model (MLM) objective. Different\nfrom BERT (taking sentence pairs as input),\nXLM takes continuous streams of tokens as\ninput.\n•XLNet (Yang et al., 2019) is a 12-layer\nTransformer-XL (Dai et al., 2019) with two\nstreams of self attention and 768 hidden di-\nmensions and 110M parameters. The XLNet\nwe use is trained on 33GB texts using the “per-\nmutation language modeling” objective, with\nits LM factorization according to shufﬂed or-\nders, but its positional encoding correspond\nto the original sequence order. The permu-\ntation LM objective introduces diversity and\nrandomness to the context.\nTo make comparisons between models fair, we\nlimit to 12-layer neural LMs. The models are pre-\ntrained by Huggingface (Wolf et al., 2019).\n3.3 Implementation\nWe formulated probing as an optimization prob-\nlem, and implemented our solution with PyTorch\n(Paszke et al., 2019) and the Adam optimizer\n(Kingma and Ba, 2014) for 40 epochs. If the train-\ning loss stalls (i.e., does not change by ≥10−3),\nor if the training loss rises by more than 10% from\nthe previous epoch, we stop the optimization. All\noptimizations follow the same learning rate tuning\nschemas.\nIn our experiments, the representation dimension\ndis taken to be 10, while the LM dimensions Dis\n2048 for XLM and 768 for the rest.\n4 Results and Discussion\n4.1 Where do LMs encode RST features?\nFrom Figure 3, neural LMs encode RST features\nin different manners, depending on their structures.\nIn general, for BERT-based models, features seem\nto distribute evenly across layers. On GPT-2 and\nXLNet, lower layers seem to encode slightly more\nEDU and Sig features than higher levels, whereas\nTree features seem to be more concentrated in lay-\ners 2-6. The results on XLM are relatively noisy,\npossibly because the uni-language version does\nnot beneﬁt from the performance boost of cross-\nlanguage modeling.\nContrasting with previous work that suggested\n21\nFigure 5: Probing performances of averaging 12 layers for 6 neural LMs on 4 tasks in IMDB, compared to the\nthree non-contextual baselines. All LMs except GPT-2 outperform non-contextual LM baselines. Plots for RST-DT\n(Figure 6 in Appendix) reveal similar patterns.\nthat middle layers most contain syntactic features\n(Hewitt and Manning, 2019; Jawahar et al., 2019),\nour results indicate a less deﬁnitive localization\nfor discourse features, except for the ﬁrst and ﬁnal\nlayers. We suggest that the reason they encode less\ndiscourse information is that the ﬁrst layer focuses\non connections between “locations”, while the ﬁnal\nlayer focuses on extracting representations most\nrelevant to the ﬁnal task.\nAre RST features equally hard to probe?Fig-\nure 3 also shows the difﬁculty in probing features\nacross feature sets. In BERT-based models, EDU\nand Tree features are comparably easier to probe,\nwhereas the Sig feature groups is more challeng-\ning. However, GPT-2, XLNet, and XLM do not\nregard EDU or Tree features easier to probe than\nother groups. Nevertheless, the results on all fea-\ntures correlate more to the Sig features.\nHow about averaging layers?For comparison,\nwe also used the mean of all 12 layers for each\nneural LM. Figure 5 shows the probing results. Ex-\ncept GPT-2, other LMs show similar performances\nwhen the representations of layers are averaged. In\naddition, the performances show that Sig features\nare harder to probe than Tree and EDU features,\nwhereas the aggregation task (using all features)\nappears harder than each of its three component\nfeature groups.\n4.2 Deconstructing the probe\nWe perform ablation studies to illustrate the effec-\ntiveness of probing, deconstructing the language\nmodel probe step-by-step. First, we get rid of the\ncontextualization component in language model-\ning by using non-contextualized word embeddings,\nGloVe and FastText. Then, we discard the semantic\ncomponent of word embedding by mapping tokens\nto randomly generated vectors (RandEmbed). Fi-\nnally, we remove all information pertaining to the\ntext, leading to a random predictor for RST fea-\ntures, RandGuess.\nNon-contextualized word embeddings We\nconsider two popular word embeddings here:\n•GloVe (Pennington et al., 2014) contains\n2.2M vocabulary items and produces 300-\ndimensional word vectors. The GloVe embed-\nding we use is pretrained on Common Crawl.\n22\n•FastText (Bojanowski et al., 2017) is trained\non Wikipedia 2017 + UMBC (16B tokens)\nincluding subword information, and produces\n300-dimensional word vectors.\nWord embeddings map each token into a D-\ndimensional semantic space. Therefore, for a doc-\nument of length L, the embedded matrix also has\nshape L×D. The difference from the contextual-\nized neural LMs is that, the D-dimensional vectors\nof every word do not depend on their contexts.\nRandom embeddings In this step, we assign a\nnon-trainable random embedding vector per token\nin the vocabulary. This removes the semantic in-\nformation encoded by GloVe and FastText word\nembeddings.\nAs shown in Figure 4, 5: RandEmbed is worse\nthan GloVe and FastText (except for GloVe inSig\nfeatures task). This veriﬁes some semantic infor-\nmation is preserved in word embeddings.\nContextualized LMs against baselineFirst, the\nlack of context restrict the probing performance of\nnon-contextualized baselines. They are worse than\nmost layers in contextualized LMs (in Figure 4),\nand are worse than all except GPT-2 if we average\nthe layers (in Figure 5).\nSecond, it is impossible for any LM to have a\n“negative” rhetorical capacity. If the probing loss\nis worse than RandEmbed baseline, that means the\nRST probe can not detect rhetorical features of the\ngiven category encoded in the representations. This\nis what happens in some layers of GPT-2, XLM,\nand XLNet, and the mean of all layers of GPT-2.\nRandom guesser To measure the capacity of\nbaseline embeddings, we set up a random guesser\nas a “baseline-of-baseline”. The random guesser\noutputs the arithmetic mean of RST features\nplus a small Gaussian noise (with s.d. σ ∈\n{0,0.01,0.1,1.0}) The output of RandGuess is\ncompletely independent of the discourse. As shown\nin Table 2, the best of the four random guessers is\nmuch worse than any of the three word embedding\nbaselines, which is expected.\n4.3 Why are some LMs better?\nFrom probing experiments (Figure 3, 4, and 5) we\ncan see that BERT-based LMs have slightly better\nrhetorical capacities than XLNet, and much better\ncapacities than GPT-2. We present two hypotheses\nas following.\nConﬁg RST Feature Set\nAll EDU Sig Tree\nFastText .6987 .7215 .6911 .6889\nGloVe .7204 .7142 .7166 .6942\nRandEmbed .7238 .7365 .7077 .7034\nRandGuess 1101.5 128.9 3.1 6799.0\nTable 2: Comparison between RST probing losses of\nnon-contextual word embeddings (FastText, GloVe),\nrandom embedding (RandEmbed), and a trivial guessor\n(RandGuess).\nRhetorics favor contexts from both directions\nBERT-based LMs use Transformer encoders,\nwhereas GPT-2 use Transformer decoders. Their\nmain difference is that a Transformer encoder\nconsiders contexts from both “past” and “future”,\nwhile a Transformer decoder only conditions on\nthe context from the “past” (Vaswani et al., 2017).\nGPT-2 attends to uni-directional contexts. Appar-\nently both the “past” and “future” context would\ncontribute to the rhetorical features of words. With-\nout “future” contexts, GPT-2 would encode less\nrhetorical information.\nRandom permutation makes encoded rhetorics\nharder to decode The difference between XL-\nNet and other LMs is the permutation in context.\nWhile permutation increases the diversity in dis-\ncourse, they could also bring in new meaning to\nthe texts. For example, the sentence in Figure 1 (“I\ndidn’t know this is from C, but it is very good!”)\nhas several syntactically plausable factorization se-\nquences:\n•I didn’t know C ...\n•... this is C ...\n•I know it is very good ...\n•I didn’t know this is good ...\n•... didn’t this C good ...\nApparently such diversity in contexts makes the\nupper layers of XLNet contain harder-to-decode\nrhetorical features. If we average the represen-\ntations of all layers, XLNet has larger variance\nthan BERT-based LMs. We hypothesize that larger\nlayer-wise difference is a factor of such instability\nfor averaged representations.\n4.4 Limitations\nRST probing is not perfect. While we designed our\ncomparisons to be rigorous, there are still several\nlimitations to the RST probe, described below.\n23\n•RST signals are noisy. The RST relation\nclassiﬁcation task is less deﬁned than estab-\nlished tasks like POS tagging. Humans tend\nto disagree with the annotators, resulting in\na merely 65.8% accuracy in relation classi-\nﬁcation (i.e., the task introduced by Marcu\n(2000)). Regardless, state-of-the-art discourse\nparsers currently have performances slightly\nhigher than 60% (Feng and Hirst, 2014; Ji and\nEisenstein, 2014; Wang et al., 2017).\n•Train / test corpus discrepancy of RST parsers.\nMost available RST parsers are trained on\nRST-DT consisting of Wall Street Journal ar-\nticles. The results of parsers are affected by\nthe corpus. As shown in some examples in\nAppendix, the IMDB movie review dataset\ncontains less formal languages, introducing\nnoise in segmentations and relation signals.\nTo counteract noise of this type, we recom-\nmend evaluating LMs using a corpus similar\nto the scenario of applying the LM.\n•Only 12-layer LMs are involved, to compare\nacross various layers fairly. But our approach\nwould be applicable to 3-layer ELMo and\ndeeper LMs as well. Appropriate statistical\ncontrols would naturally need to be applied.\n•Not all documents can be analyzed. First, doc-\numents longer than 512 tokens cannot be en-\ncoded into one vector in our probing model.\nSecond, while RST provides elegant frame-\nworks for analyzing rhetorical structures of\ndiscourse, in practice, the RST pipeline does\nnot guarantee a successful analysis for an ar-\nbitrary document scraped online.\n5 Related work\nRecent work has considered the interpretability of\ncontextualized representations. For example, Jain\nand Wallace (2019) found attention to be uncorre-\nlated to gradient-based feature importance, while\nWiegreffe and Pinter (2019) suggested such ap-\nproaches allowed too much ﬂexibility to give con-\nvincing results. Similarly, Serrano et al. (2019)\nconsidered attention representations to be noisy\nindicators of feature importance.\nMany tasks in argument mining, similar to our\ntask of examining neural LMs, require understand-\ning the rhetorical aspects of discourse (Lawrence\nand Reed, 2019). This allows RST to be ap-\nplied in relevant work. For example, RST en-\nables understanding and analyzing argument struc-\ntures of monologues (Peldszus and Stede, 2016)\nand, when used with other discourse features, RST\ncan improve role-labelling in online arguments\n(Chakrabarty et al., 2019).\nProbing neural LMs is an emergent diagnostic\ntask on those models. Previous work probed mor-\nphological (Bisazza and Tump, 2018), agreement\n(Giulianelli et al., 2018), and syntactic features (He-\nwitt and Manning, 2019). Hewitt and Liang (2019)\ncompared different probes, and recommended lin-\near probes with as few parameters as possible, for\nthe purpose of reducing overﬁtting. Recently, Pi-\nmentel et al. (2020) argued against this choice\nfrom an information-theoretic point of view. V oita\nand Titov (2020) presents an optimization goal for\nprobes based on minimum description length.\nLiu et al. (2019a) proposed 16 diverse probing\ntasks on top of contextualized LMs including to-\nken labeling (e.g., PoS), segmentation (e.g., NER,\ngrammatical error detection) and pairwise relations.\nWhile LMs augmented with a probing layer could\nreach state-of-the-art performance on many tasks,\nthey found that LMs still lacked ﬁne-grained lin-\nguistic knowledge. DiscoEval (Chen et al., 2019)\nshowed that BERT outperformed traditional pre-\ntrained sentence encoders in encoding discourse\ncoherence features, which our results echo.\n6 Conclusion\nIn this paper, we propose a method to quantitatively\nanalyze the amount of rhetorical information en-\ncoded in neural language models. We compute fea-\ntures based on Rhetorical Structure Theory (RST)\nand probe the RST features from contextualized\nrepresentations of neural LMs. Among six popular\nneural LMs, we ﬁnd that contextualization helps\nto generally improve the rhetorical capacities of\nLMs, while individual models may vary in quality.\nIn general, LMs attending to contexts from both\ndirections (BERT-based) encode rhetorical knowl-\nedge in a more stable manner than those using uni-\ndirectional contexts (GPT-2) or permuted contexts\n(XLNet).\nOur method presents an avenue towards quantita-\ntively describing rhetorical capacities of neural lan-\nguage models based on unlabeled, target-domain\ncorpus. This method may be used for selecting\nsuitable LMs in tasks including rhetorical acts clas-\nsiﬁcations, discourse modeling, and response gen-\neration.\n24\nAcknowledgement\nWe thank the anonymous reviewers for feedback.\nRudzicz is supported by a CIFAR Chair in artiﬁ-\ncial intelligence. Abdalla is supported by a Vanier\nscholarship.\nReferences\nMohamed Abdalla, Frank Rudzicz, and Graeme Hirst.\n2017. Rhetorical structure and Alzheimer’s disease.\nAphasiology, 32(1):41–60.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. In ICLR, Toulon, France.\nGuillaume Alain and Yoshua Bengio. 2017. Under-\nstanding intermediate layers using linear classiﬁer\nprobes. In ICLR, Toulon, France.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017. What do Neural Ma-\nchine Translation Models Learn about Morphology?\nIn ACL, pages 861–872, Vancouver, Canada. Asso-\nciation for Computational Linguistics.\nArianna Bisazza and Clara Tump. 2018. The lazy en-\ncoder: A ﬁne-grained analysis of the role of mor-\nphology in neural machine translation. In EMNLP,\npages 2871–2876, Brussels, Belgium. Association\nfor Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. TACL, 5:135–146.\nLynn Carlson, Daniel Marcu, and Mary Ellen\nOkurowski. 2001. Building a Discourse-Tagged\nCorpus in the Framework of Rhetorical Structure\nTheory. In SIGDIAL Workshop.\nTuhin Chakrabarty, Christopher Hidey, Smaranda\nMuresan, Kathy McKeown, and Alyssa Hwang.\n2019. AMPERSAND: Argument Mining for PER-\nSuAsive oNline Discussions. In EMNLP, pages\n2933–2943, Hong Kong, China. Association for\nComputational Linguistics.\nMingda Chen, Zewei Chu, and Kevin Gimpel. 2019.\nEvaluation Benchmarks and Learning Criteria for\nDiscourse-Aware Sentence Representations. In\nEMNLP, pages 649–662, Hond Kong, China. Asso-\nciation for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models be-\nyond a Fixed-Length Context. In ACL, pages 2978–\n2988, Florence, Italy. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL. Association for Computa-\ntional Linguistics.\nVanessa Wei Feng and Graeme Hirst. 2012. Text-level\nDiscourse Parsing with Rich Linguistic Features. In\nACL, pages 60–68, Jeju Island, Korea. Association\nfor Computational Linguistics.\nVanessa Wei Feng and Graeme Hirst. 2014. A Linear-\nTime Bottom-Up Discourse Parser with Constraints\nand Post-Editing. In ACL, pages 511–521, Balti-\nmore, Maryland. Association for Computational Lin-\nguistics.\nVanessa Wei Feng, Ziheng Lin, and Graeme Hirst.\n2014. The Impact of Deep Hierarchical Discourse\nStructures in the Evaluation of Text Coherence. In\nCOLING, pages 940–949, Dublin, Ireland. Dublin\nCity University and Association for Computational\nLinguistics.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to investi-\ngate and improve how language models track agree-\nment information. In EMNLP BlackBoxNLP, pages\n240–248, Brussels, Belgium. Association for Com-\nputational Linguistics.\nMichael Heilman and Kenji Sagae. 2015. Fast rhetor-\nical structure theory discourse parsing. arXiv\npreprint 1505.02425.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In EMNLP,\npages 2733–2743, Hong Kong, China. Association\nfor Computational Linguistics.\nJohn Hewitt and Christopher D Manning. 2019. A\nStructural Probe for Finding Syntax in Word Repre-\nsentations. In NAACL, pages 4129–4138, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nRobert Irish and Peter Eliot Weiss. 2009. Engineering\ncommunication: from principles to practice. Oxford\nUniversity Press Canada.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In NAACL, pages 3543–3556, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In ACL, pages 3651–3657, Florence,\nItaly. Association for Computational Linguistics.\nYangfeng Ji and Jacob Eisenstein. 2014. Represen-\ntation learning for text-level discourse parsing. In\nACL, pages 13–24, Baltimore, Maryland. Associa-\ntion for Computational Linguistics.\n25\nShaﬁq Joty, Giuseppe Carenini, and Raymond T Ng.\n2015. CODRA: A Novel Discriminative Framework\nfor Rhetorical Analysis. Computational Linguistics,\n41(3):385–435.\nNitish Shirish Keskar, Bryan McCann, Lav Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCTRL - A Conditional Transformer Language\nModel for Controllable Generation. arXiv preprint\n1909.05858.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In ICLR, Banff,\nCanada.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual Language Model Pretraining. arXiv preprint\narXiv:1901.07291.\nJohn Lawrence and Chris Reed. 2019. Argument Min-\ning: A Survey. Computational Linguistics , pages\n1–54.\nBai Li, Yi-Te Hsu, and Frank Rudzicz. 2019. De-\ntecting dementia in Mandarin Chinese using trans-\nfer learning from a parallel corpus. In NAACL,\npages 1991–1997, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nQi Li, Tianshi Li, and Baobao Chang. 2016. Discourse\nParsing with Attention-based Hierarchical Neural\nNetworks. In EMNLP, pages 362–371, Austin,\nTexas. Association for Computational Linguistics.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. In ICLR, Toulon, France.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic Knowledge and Transferability of Contextual\nRepresentations. pages 1073–1094, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nSha Liu and Antony Kunnan. 2016. Investigating\nthe Application of Automated Writing Evaluation\nto Chinese Undergraduate English Majors: A Case\nStudy of WriteToLearn. CALICO, 33(1):71–91.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint 1907.11692.\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning Word Vectors for Sentiment Analysis. In\nACL, pages 142–150, Portland, Oregon, USA. Asso-\nciation for Computational Linguistics.\nWilliam C Mann, Christian M I M Matthiessen, and\nSandra A Thompson. 1989. Rhetorical structure the-\nory and text analysis. ISI Research Report.\nWilliam C. Mann and Sandra A. Thompson. 1988.\nRhetorical Structure Theory: Toward a functional\ntheory of text organization. Interdisciplinary Jour-\nnal for the Study of Discourse, 8(3):243–281.\nDaniel Marcu. 2000. The Theory and Practice of\nDiscourse Parsing and Summarization. MIT Press,\nCambridge, MA, USA.\nMohammed Ali Mohsen and Abdulaziz Alshahrani.\n2019. The Effectiveness of Using a Hybrid Mode of\nAutomated Writing Evaluation System on EFL Stu-\ndents’ Writing. Teaching English with Technology,\n19(1):118–131.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K ¨opf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. PyTorch: An\nImperative Style, High-Performance Deep Learning\nLibrary. In NeurIPS.\nAndreas Peldszus and Manfred Stede. 2016. Rhetori-\ncal structure and argumentation structure in mono-\nlogue text. In ArgMining Workshop, pages 103–\n112, Berlin, Germany. Association for Computa-\ntional Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. GloVe: Global Vectors for Word\nRepresentation. In EMNLP, pages 1532–1543,\nDoha, Qatar.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-Theoretic Probing for Linguis-\ntic Structure. Association of Computational Linguis-\ntics.\nDonald E Powers, Jill C Burstein, Martin Chodorow,\nMary E Fowles, and Karen Kukich. 2001. Stumping\nE-Rater: Challenging the Validity of Automated Es-\nsay Scoring. Technical report, Educational Testing\nService, Princeton, New Jersey.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bonnie\nWebber. 2008. The Penn Discourse TreeBank 2.0.\nIn LREC, Marrakech, Morocco. European Language\nResources Association (ELRA).\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D Manning. 2019. Do\nMassively Pretrained Language Models Make Bet-\nter Storytellers? In CoNLL, pages 843–861, Hong\nKong, China. Association for Computational Lin-\nguistics.\n26\nSoﬁa Serrano, Noah A Smith, and Paul G Allen.\n2019. Is Attention Interpretable? In ACL, pages\n2931–2951, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked Sequence to Se-\nquence Pre-training for Language Generation. In\nICML, Long Beach, California.\nMihai Surdeanu, Thomas Hicks, and Marco A\nValenzuela-Esc´arcega. 2015. Two Practical Rhetor-\nical Structure Theory Parsers. In NAACL, Denver,\nColorado.\nStephen Toulmin. 1958. The Uses of Argument. Cam-\nbridge University Press.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NeurIPS, Long Beach, California.\nElena V oita and Ivan Titov. 2020. Information-\nTheoretic Probing with Minimum Description\nLength. arXiv preprint arXiv:2003.12298.\nXinhao Wang, Binod Gyawali, James V Bruno,\nHillary R Molloy, Keelan Evanini, and Klaus Zech-\nner. 2019. Using Rhetorical Structure Theory to\nAssess Discourse Coherence for Non-native Spon-\ntaneous Speech. In DisRPT, pages 153–162, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nYizhong Wang, Sujian Li, and Houfeng Wang. 2017.\nA Two-Stage Parsing Method for Text-Level Dis-\ncourse Analysis. In ACL, pages 184–188, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention\nis not not Explanation. In EMNLP, pages 11–20,\nHong Kong, China. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv preprint 1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding.\nVictor H Yngve. 1960. A model and an hypothesis\nfor language structure. Proceedings of the American\nphilosophical society, 104(5):444–466.\nNan Yu, Meishan Zhang, and Guohong Fu. 2018.\nTransition-based neural RST parsing with implicit\nsyntax features. In COLING, pages 559–570, Santa\nFe, New Mexico, USA. Association for Computa-\ntional Linguistics.\nMo Zhang. 2013. Contrasting Automated and Human\nScoring of Essays. Technical report, Educational\nTesting Service.\nZining Zhu, Jekaterina Novikova, and Frank Rudzicz.\n2019. Detecting cognitive impairments by agreeing\non interpretations on linguistic features. In NAACL,\npages 1431–1441, Minnespolis, Minnesota. Associ-\nation for Computational Linguistics.\n27\nA Experiments on RST-DT\nAs a sanity check, we include experiments on RST-\nDT (Carlson et al., 2001) corpus with the same pre-\nprocessing and feature extraction procedures (i.e.,\nperform feature extraction and embedding on the\narticle level, and ignoring the overlength articles).\nAs shown in Figure 6, BERT-family and XLM out-\nperform GPT-2 and XLNet. Also, the noncontex-\ntualized embedding baselines show worse perfor-\nmances than contextualized embeddings in general,\nwith some exceptions (e.g., GPT-2 on EDU fea-\ntures). These are similar to the IMDB results.\nWhat are different is that the probing losses of\nRST-DT are lower than the IMDB experiments in\ngeneral. We consider two possible explanations.\nFirst, the IMDB signals contain more noise, so that\nprobing rhetorical features from IMDB would be\nnaturally more difﬁcult than probing from the RST-\nDT dataset. Second, it is possible that the probes\noverﬁt the much smaller RST-DT dataset.\nB Examples of parse trees\nWe include several examples of IMDB parse trees\nin Appendix here, including some examples where\nthe RST parser makes mistakes on a new domain,\nmovie review. For clarity of illustration, these ex-\namples are among the shorter movie reviews. More\nparse trees can be generated by our visualization\ncode, which is contained in our submitted scripts.\n28\nFigure 6: Probing performances of averaging 12 layers for 6 neural LMs on 4 tasks in RST-DT, compared to the\nthree non-contextual baselines.\nElaboration[N][S]\nThis movie is very good\n. <s> Contrast[N][N]\nThe screenplay is enchanting\n. <s> Elaboration[N][S]\nBut Meryl Streep is\nmost impressive . <s> Joint[N][N]\nHer performance is\nexcellent . <s>\nShe brings me to go into\nthe heart of her role\n. <P>\nFigure 7: IMDB train/pos/10348 8.txt. The <s> and <P> are appended automatically by the parser,\nmarking the end of sentences and paragraphs respectively.\n29\nElaboration[N][S]\nAttribution[S][N] Elaboration[N][S]\nA lot of people are saying Attribution[S][N]\nthat Al Pacino over\nacted Elaboration[N][S]\nbut I mean common obviously\nfor a movie role like\nthis\nAttribution[S][N]\n-- a cuban drug lord\nyou need a bit of over\nacting in this role\nwith that cuban accent\n. <s>\nThis movie overall\nwas a really good movie\nI\nAttribution[S][N]\nmyself rated a 10/10\nI would highly recommend\npeople to watch this\nmovie . <P>\nFigure 8: IMDB train/pos/11857 10.txt. There is an EDU segmentation error: the “I” is incorrectly\nassigned to the previous sentence “This movie overall was a really good movie”. Apparently some lexical cues the\nEDU segmentator relies on (e.g., sentence ﬁnishes with a period sign) is not always followed in IMDB.\nExplanation[N][S]\nI liked the ﬁlm . <s> Elaboration[N][S]\nElaboration[N][S] Elaboration[N][S]\nElaboration[N][S] Elaboration[N][S]\nSome of the action scenes\nwere very interesting\n, tense and well done\n. <s>\nElaboration[N][S]\nI especially liked\nthe opening scene\nwhich had a semi truck\nin it . <s>\nA very tense action\nscene\nthat seemed well done\n. <s>\nElaboration[N][S] I 'd give the ﬁlm an\n8 out of 10 . <P>\nElaboration[N][S] Attribution[S][N]\nSome of the transitional\nscenes were ﬁlmed\nin interesting ways\nsuch as time lapse photography\n, unusual colors , or\ninteresting angles\n. <s>\nAlso the ﬁlm is funny\nis several parts . <s> I also liked how the evil guy was\nportrayed too . <s>\nFigure 9: IMDB train/pos/1000 8.txt. The parser captures the key sentence of this review. All sentences\nfollowing the ﬁrst one act as reasons to explain how the reviewer liked the ﬁlm.\n30\nElaboration[N][S]\nElaboration[N][S] My rating : 8/10 <P>\nElaboration[N][S] Elaboration[N][S]\nSummary[N][S] Elaboration[N][S]\nBruce Almighty , one\nof Carrey 's best pictures\nsince . <s>\n. . well . <s> Elaboration[N][S] . . <s>\n. . a long time . <s> Elaboration[N][S]\nIt contains one of the\nfunniest scenes\nI have seen for a long\ntime too . <s>\nElaboration[N][S] Background[S][N]\nAttribution[S][N] Elaboration[N][S]\nMorgan Freeman plays Elaboration[N][S]\nGod well and even chips\nin a few jokes\nthat are surprisingly\nfunny . <s>\nIt contains one or two\nromantic moments\nthat are a bit boring\nbut over all a great\nmovie with some funny\nscenes . <s>\nThe best scene in , Attribution[S][N]\nit is\nwhere Jim is messing\nup the anchor man 's\nvoice . <s>\nFigure 10: IMDB train/pos/10301 8.txt. The interjection, “well”, is incorrectly identiﬁed as the satellite\nof the summary signal. This is likely caused by the discrepancy between the train (RST-DT) and test (IMDB)\ncorpus discrepancy for the RST parser. The RST-DT dataset contains news articles, which are more formal than\nthe online review in IMDB. The term “well” is therefore more likely to be identiﬁed as other senses.\nElaboration[N][S]\nElaboration[N][S] Elaboration[N][S]\nDo n't look for an overdeveloped\nplotline here . <s> Joint[N][N]\n... just sit back with\nsome popcorn\nand enjoy this one .\n<s>\nBackground[N][S]\nGreat fun for kids and\nadults alike . *** out\nof **** <P>\nA gallery of stars pop\nup same-unit[N][N]\nElaboration[N][S]\nnot to mention knockout\nperformances from\nBeatty , Madonna , and\nPacino . <s>\nas the classic cartoon\ncharacter 's villains\nin this live action\ncomedy ,\nwhich features incredible\nmakeup and set design\n,\nFigure 11: IMDB train/pos/11825 8.txt. One might suggest that the last EDU could be moved one level\nhigher (so that it summarizes the whole review), but this parsing is also reasonable, since the mention of kids\nelaborates the descriptions of the makeup and the views.\n31\nEvaluation[N][S]\nBackground[N][S] Condition[N][S]\nBig rock candy mountain\nis amazing . i watched\nit\nJoint[N][N]\nwhen i was little , and still do to this\nday . <s>\nsenior  in high school . same-unit[N][N]\nElaboration[N][S] Elaboration[N][S]\nif i could imagine heaven\n,\nthat is what it would\nlook like . i wish i\ncould live in big rock\ncandy mountain\nwhere candy grows on\ntrees . <s>\nFigure 12: IMDB train/pos/10788 10.txt. This is an example of the EDU segmentation contains mistake.\nThe “i wish i” should be merged with the subsequent EDU, “could live in big rock candy mountain”. Note that the\nsentence starts with two lowercase “i” (which should be uppercase). The non-standard usages like these are unique\nfor less formal texts like IMDB.\nElaboration[N][S]\nElaboration[N][S] It is really funnny\n<P>\nJoint[N][N] Contrast[N][N]\nTHis was a hilarious\nmovie\nand I would see it again\nand again . <s> Elaboration[N][S] Elaboration[N][S]\nIt is n't a movie for\nsomeone\nwho does n't have a fun\nsense of a humor , but for people\nwho enoy comedy like\nChris Rock its a perfect\nmovie in my opinion\n. <s>\nFigure 13: IMDB train/pos/11686 10.txt.\n32\nElaboration[N][S]\nElaboration[N][S]\nTruly one of the great\nones and deﬁnitely\nmy all-time favourite\n!!! <P>\nElaboration[N][S] Elaboration[N][S]\nJoint[N][N]\nIt just keeps getting\nbetter and better .\n<s>\nJust watched the ﬁlm\nfor the 3rd time Elaboration[N][S]\nand enjoyed Lindsay\nCrouse and the rest\nof the cast\njust as much as before\n. <s>\nYou simply have to marvel\nat the carefully measured\nway of speech , the slow\ndeliberate action\n, everything\nbeing exactly in place\n. <s>\nFigure 14: IMDB train/pos/10264 10.txt",
  "topic": "Rhetorical question",
  "concepts": [
    {
      "name": "Rhetorical question",
      "score": 0.8860267996788025
    },
    {
      "name": "Computer science",
      "score": 0.6700729727745056
    },
    {
      "name": "ENCODE",
      "score": 0.65276038646698
    },
    {
      "name": "Rhetoric",
      "score": 0.6028687953948975
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5344820022583008
    },
    {
      "name": "Linguistics",
      "score": 0.533638596534729
    },
    {
      "name": "Transformer",
      "score": 0.5137649774551392
    },
    {
      "name": "Rhetorical device",
      "score": 0.4219772219657898
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4035966992378235
    },
    {
      "name": "Natural language processing",
      "score": 0.40111425518989563
    },
    {
      "name": "Philosophy",
      "score": 0.10571834444999695
    },
    {
      "name": "Programming language",
      "score": 0.07174986600875854
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}