{
  "title": "Large Language Models with Controllable Working Memory",
  "url": "https://openalex.org/W4385570326",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103291722",
      "name": "Daliang Li",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2122381750",
      "name": "Ankit Singh Rawat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2631325715",
      "name": "Manzil Zaheer",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2190118489",
      "name": "Michal Lukasik",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1907049869",
      "name": "Andreas Veit",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2111117840",
      "name": "Felix Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103569061",
      "name": "Sanjiv Kumar",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2329937548",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W3211973371",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4221166832",
    "https://openalex.org/W1934970405",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2798836595",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W2168066834",
    "https://openalex.org/W4386290290",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2076420009",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3018732874",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W4229026446",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W2610916853",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4288099666",
    "https://openalex.org/W4293182797",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining.While many downstream applications provide the model with an informational context to aid its underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which then facilitates updating specific model predictions without frequently retraining the model. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size. As a solution, we propose a simple yet effective method – knowledge aware finetuning (KAFT) – to strengthen both controllability and robustness by injecting counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1774–1793\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models with Controllable Working Memory\nDaliang Li♠, Ankit Singh Rawat♠, Manzil Zaheer♥,\nXin Wang♠, Michal Lukasik♠, Andreas Veit♠, Felix Yu♠, Sanjiv Kumar♠\n♠Google Research New York♥Google DeepMind New York\n{daliangli, ankitsrawat, manzilzaheer}@google.com\n{wanxin, mlukasik, aveit, felixyu, sanjivk}@google.com\nAbstract\nLarge language models (LLMs) have led to\na series of breakthroughs in natural language\nprocessing (NLP), partly owing to the mas-\nsive amounts of world knowledge they mem-\norize during pretraining. While many down-\nstream applications provide the model with\nan informational context to aid its underlying\ntask, how the model’s world knowledge inter-\nacts with the factual information presented in\nthe context remains under explored. As a de-\nsirable behavior, an LLM should give prece-\ndence to the context whenever it contains task-\nrelevant information that conﬂicts with the\nmodel’s memorized knowledge. This enables\nmodel predictions to be grounded in the con-\ntext, which then facilitates updating speciﬁc\nmodel predictions without frequently retrain-\ning the model. By contrast, when the context\nis irrelevant to the task, the model should ig-\nnore it and fall back on its internal knowledge.\nIn this paper, we undertake a ﬁrst joint study\nof the aforementioned two properties, namely\ncontrollability and robustness, in the context\nof LLMs. We demonstrate that state-of-the-\nart T5 and PaLM models (both pretrained\nand ﬁnetuned) could exhibit low controlla-\nbility and robustness that does not improve\nwith increasing the model size. As a solu-\ntion, we propose a simple yet effective method\n– knowledge aware finetuning (KAFT) – to\nstrengthen both controllability and robustness\nby injecting counterfactual and irrelevant con-\ntexts to standard supervised datasets. Our com-\nprehensive evaluation showcases the utility of\nKAFT across model architectures and sizes.\n1 Introduction\nLarge language models (LLMs) pretrained on large\nscale datasets have shown promising results across\nnatural language tasks (Vaswani et al., 2017; Devlin\net al., 2019; Raffel et al., 2020a; Brown et al., 2020;\nRae et al., 2021; Chowdhery et al., 2022; Smith\net al., 2022). However, as models scale ever larger,\nthey become more expensive to train, making it\nunrealistic to frequently update model parameters.\nOn the other hand, many real world applications\noften necessitate adjusting model behavior. This\ndilemma is especially sharp in the case of factual\n(world) knowledge that plays important role in re-\nalizing impressive performance of LLMs. It is\nwell known that LLMs memorize large amounts\nof factual knowledge in their parameters (Petroni\net al., 2019; Roberts et al., 2020; Geva et al., 2021),\nwhich could potentially be out-dated or incorrect.\nEven for moderate-size models, it is prohibitively\nexpensive to retrain every time an update happens\nor a mistake is uncovered. Even if resources are\nample, it is difﬁcult to ensure that the modiﬁcation\nof model parameters do not affect unrelated skills\nor knowledge.\nIn human cognition, working memory (George\nA. Miller, 1960) provides the biological brain with\nthe ability to hold information temporarily to per-\nform tasks such as conversation, reasoning, and\nmathematics in a way that is adaptive to the ever\nchanging environment. As shown both experimen-\ntally and theoretically (Fuster, 1973; Ashby et al.,\n2005), working memory is stored in sustained ac-\ntivations of neurons, as opposed to the long term\nmemory which is stored in weights. Working mem-\nory is also the immediate information buffer that is\naccessed while performing conscious tasks. In par-\nticular, it is where the fusion of perceptual inputs\nand long term memory happens (Fukuda and Wood-\nman, 2017). This suggests that a potential method\nto solve LLMs’ pointwise knowledge update and\ncorrection problem is to control the working mem-\nory stored in activations, rather than editing the\nlong term memory stored in the model weights.\nAs demonstrated by their powerful in-context\nfew shot learning abilities (Brown et al., 2020),\nLLM could utilize different activation patterns re-\nsulting from different contexts during inference to\nsolve a diverse set of tasks without any changes in\n1774\nControllability Robustness\nQuestion Dave Gilmour and Roger Waters were in\nwhich rock group?\nHow hasBritish artsurvived in Normandy?\nContext George Roger Waters (born 6 September 1943)\nis an English singer, . . . Later that year, he re-\nunited withThe Rolling Stonesbandmates\nMason, Wright and David Gilmour...\nIn Britain,Norman artprimarily survives as\nstonework or metalwork, such as capitals and\nbaptismal fonts...\nKAFT (ours)The Rolling Stones (from context). In museums (irrelevant context).\nNoisy FT Pink Floyd stonework or metalwork\nUQA V2 11BPink Floyd stonework or metalwork, such as capitals and\nbaptismal fonts\nPretrained Pink Floyd As stonework and metalwork, such ascapi-tals\nand baptismal fonts\nTable 1: Examples of model outputs demonstrating that, in contrast with baselines, a model obtained by KAFT\nis characterized by both improved controllability by a context that contradicts its parametric knowledge, and im-\nproved robustness against an irrelevant context, compared to baseline methods. Here, Pretrained refers to a T5\nXXL model (Raffel et al., 2020b), which is also the underlying model for KAFT and Noisy Finetuning (FT). UQA\nV2 11B (Khashabi et al., 2022) is based on the T5 11B model.\nthe weights. It is natural to expect that the same\nwould be true with factual knowledge. In particular,\none could prepare a large list of natural language\nstatements covering desired knowledge updates and\ncorrections. At inference time, one can provide the\nrelevant statements as context along with the in-\nput and hope that the model would perform the\ntask based on the new knowledge presented in this\ncontext. Thus, if the model’s working memory is\nindeed controllable by context, then a single model\nwith static long term memory can produce different\nresults based on a varying set of facts available in\ndifferent contexts. However, we demonstrate that\nthis approach may fall short for existing LLMs as\nthey have great tendencies to ignore the context\nand stick to their own parametric knowledge – the\nworld knowledge stored in its model parameters.\nThis raises a natural question:\nIs it possible to design a mechanism to ensure that\nthe context can reliably inﬂuence the model’s\nworking memory?\nNote that any such mechanism has to take into\naccount the possibility of encountering noisy con-\ntexts. For example, any retrieval system that selects\nthe task-relevant context from a large collection of\ncontexts will be imperfect and occasionally provide\nirrelevant context. In such cases, it’s desirable that\nthe model prediction does not get swayed by an\nirrelevant context. Interestingly, we show that the\nstandard pretraining and ﬁnetuning methods do not\nensure this behavior either. In fact, we demonstrate\nthat it’s the noise encountered during the training\nthat often leads to the model ignoring the context.\nIn this work, we provide an afﬁrmative answer to\nthe aforementioned question and propose a novel\napproach – knowledge-aware finetuning (KAFT) –\nto make an LLM’s working memory controllable\nvia relevant context while being robust against irrel-\nevant context. Towards this, we aim to ensure that\nthe model utilizes different types of information at\nits disposal in the following order:\nrelevant context\n> model’s parametric knowledge (1)\n> irrelevant context, (2)\nwhere a > bindicates that a is prioritized over\nb. Thus, if the model decides that the context\nis relevant, it should ground its output in the\ncontext, ensuring the controllability of its working\nmemory by the context. This is crucial when the\ncontext is in conﬂict with the model’s parametric\nknowledge. On the other hand, when the context\nis irrelevant, the model should instead stick to its\nparametric knowledge; thus ensuring robustness of\nits working memory against noise.\nOur contributions. We develop ﬁrst LLMs that\nutilize different knowledge sources with a prede-\nﬁned order of priorities. Along the way, we develop\na systematic understanding of the working memo-\nries of LLMs and identify their shortcomings. Our\nkey contributions are summarized below.\n1775\nRobustness Controllability\nStandard (noisy) ﬁnetuning \u0017 \u0017\nCounterfactual ﬁnetuning\n(Longpre et al., 2021) \u0017 \u0013\nKAFT (our work) \u0013 \u0013\nTable 2: Summary of our contributions.\n1. We undertake a systematic joint study of both\ncontrollability and robustness of the working mem-\nory of LLMs. Focusing on question answering\n(QA) tasks, we deﬁne the context-question rele-\nvance based on whether the context entails an an-\nswer to the question. We create a novel benchmark\nto measure the controllability by including contexts\nthat imply an answer which contradicts the model’s\npretrained knowledge.1 Similarly, we benchmark\nrobustness against irrelevant contexts. We con-\nduct an extensive evaluation of LLMs with differ-\nent sizes across multiple architectures (encoder-\ndecoder and decoder-only) from T5 (Raffel et al.,\n2020b) and PaLM (Chowdhery et al., 2022) family.\nWe make the following key observations:\n(a) LLMs could exhibit low controllability. Our\nexperiments consistently show that both pre-\ntrained and QA ﬁnetuned LLMs tend to ignore a\ncontext when it contradicts with model’s world\nknowledge. We show that this problem persists\nand may intensify as the model becomes larger.\nWe further show that the noise in the (QA) ﬁne-\ntuning set plays an important role in emergence\nof this behavior (cf. Sec. 4.2).\n(b) LLMs may not be robust against context\nnoise. We demonstrate that both pretrained and\nQA ﬁnetuned models are strongly interfered\nby irrelevant contexts, especially the ones that\nare on the same general topic as the underlying\nquestion (cf. Sec. 4.3).\n2. We propose a novel method – knowledge\naware finetuning (KAFT) – to directly enhance\nboth controllability (Eq. 1) and robustness (Eq. 2)\nof an LLM. KAFT enhances the controllability by\ncreating counterfactual data augmentations where\nthe answer entity in the context is swapped to a\ndifferent but plausible entity, in conﬂict with the\nground truth (and potentially the model’s world\nknowledge). As for enhancing robustness, KAFT\nrequires that the model should predict its pretrained\nclosed-book answer rather than the ground truth\nanswer whenever the context is irrelevant.\n1We rely on in-context prompts in a closed book QA setup\nto measure the model’s parametric knowledge.\n3. Through extensive empirical evaluation, we\nshow that KAFT-based models successfully demon-\nstrate the coexistence of controllability and robust-\nness of model’s working memory (see Table 1 for\nan illustration).\n2 Related Works\nWorld knowledge in language models. Recent\nworks established that LLMs memorize factual\ninformation present in the pretraining corpus.\nE.g., Petroni et al. (2019) utilize language model\nanalysis (LAMA) probing to show that BERT\nmodels (Devlin et al., 2018) could act as knowl-\nedge bases. Roberts et al. (2020) reported similar\nﬁndings for T5 models. It is therefore common\npractice to employ LLMs in tasks like closed book\nQA (Chowdhery et al., 2022).\nKnowledge update in language models. Given\nthat factual knowledge is ever-evolving, outdated\nmemory of LLMs may lead to incorrect predic-\ntions (Lazaridou et al., 2021; Onoe et al., 2022).\nFurthermore, during deployment, one may unearth\nmistakes that need correction. Frequent retrain-\ning from scratch with an updated and corrected\ncorpus would be prohibitively expensive. Ideas\naround ﬁnetuning (Zhu et al., 2020) and continued\nlearning (Jang et al., 2022) train the model with\nless but still signiﬁcant resources. Multiple recent\nefforts have studied how these models store the\nfactual knowledge (Geva et al., 2021) and meth-\nods to update model parameters given new knowl-\nedge (De Cao et al., 2021; Dhingra et al., 2022;\nMitchell et al., 2022; Meng et al., 2022a,b). These\nstrategies change weights in response to single\nupdates, risking inadvertently affecting unrelated\nskills or knowledge and creating burden to poten-\ntially store multiple versions of LLMs. We focus\non updating the model behavior by providing a suit-\nable context and ensuring that the model’s working\nmemory is controllable by such contexts.\nContextual and parametric knowledge. Guu\net al. (2020); Joshi et al. (2020); Petroni et al.\n(2020) utilized retrieved context to assist language\nmodels in tasks such as QA. At the same time,\nLLMs memorize large amounts of knowledge in\ntheir parameters. Despite this dichotomy, only\na few studies have previously addressed the rela-\ntion between these two very different knowledge\nsources. Longpre et al. (2021) ﬁnd that larger mod-\nels have a greater tendency to ignore context in\n1776\nfavor of their own parametric knowledge, and that\nthe noise in the context in the ﬁnetuning set plays\na big role in causing this behavior. We incorporate\nthe algorithms proposed by Longpre et al. (2021)\nfor mitigating this problem as baselines in Sec. 4.4\n(the Relevant Only Finetuning approaches), where\nwe ﬁnd such baselines lack robustness against ir-\nrelevant contexts (Fig. 1, 2). Kassner and Schütze\n(2020) showed that language models tend to be eas-\nily misled by certain types of irrelevant contexts.\nWe observe similar phenomena in QA and show\nthat our proposed KAFT leads to more robust mod-\nels against irrelevant contexts. Finally, Pan et al.\n(2021) considers a scenario where some context\nsources may be less trustworthy than the model’s\nparametric knowledge. This scenario can be cap-\ntured by an extension of our framework Eq.(1-2).\nFor example, given three sources, one could en-\nforce the following precedence order: source1 >\nsource2 > model’s own knowledge > source3 >\nirrelevant contexts.\nNotions of controllability and robustness. In\ncontrol theory, controllability (Ogata, 1996) refers\nto the ability of using external inputs to manipulate\nsystem to reach all possible states. In the spirit\nof this deﬁnition, this paper measure the control-\nlability of an LM’s working memory by external\ncontexts. In the framework of controlled text gen-\neration (Zhang et al., 2022; Hu and Li, 2022), the\nnotion of controllability explored here is a special\ntype of ﬁne-grained semantic control of the model’s\nbehavior with the content of the context.\nNotions of robustness. (Liang et al., 2022; Omar\net al., 2022) survey many notions of robustness of\nlanguage models around the notion of the invari-\nance of model’s behaviors when the input is per-\nturbed (for example, expressing similar semantic\nmeanings in different ways). Our robustness bench-\nmark is an extreme and input-dependent version\nunder this framework. In our evaluations, the input\ncontains two parts: the context and the question. In\nthis work, a robust model’s response is invariant to\nlarge perturbations in the semantic content of the\ncontext, as long as these changes are not relevant\nto the question.\nDuring the preparation of this manuscript, we were\nmade aware of a parallel and independent investi-\ngation by Neeman et al. (2022) that shares some\nimportant aspects of our work.\nContext type Target sequence\nrelevant context ${ground truth answer}\n(from context)\nirrelevant context ${pretrained model’s answer}\n(irrelevant context)\nempty context ${pretrained model’s answer}\n(empty context)\ncounterfactual context${counterfactual answer}\n(from context)\nTable 3: The output formats of the KAFT model.\n3 Methods\nFor concreteness, consider a reading comprehen-\nsion QA task where the model takes question q\ntogether with a context c as its input. The question\nhas an answer label a. We also need a relevance\nlabel r denoting whether c entails a.\nStarting with a pretrained LM M, we would like\nto build a model M′ such that when the context c is\nrelevant, its answer is always grounded in c, when\nc is irrelevant, it sticks to the pretrained model’s\nanswer. In equations:\nr = 1 : M′(c + q) =a (3)\nr = 0 : M′(c + q) =M(q) (4)\nwhere + denotes string concatenation. This estab-\nlishes the priority order of knowledge sources as in\nEq. (1 & 2): if there is a conﬂict between a relevant\ncontext c and M’s parametric knowledge, then the\noutput should be consistent with c. In addition,\nirrelevant context should have no inﬂuence on the\nmodel’s output. Note that even though we are sepa-\nrating relevant vs irrelevant context here, the model\ndoes not know r a priori. It has to determine r\nbased on the semantics of c and q.\nIn the KAFT data, r = 1cases include relevant\nor counterfactual context, where a is the ground\ntruth or counterfactual answer, respectively; r = 0\ncases include empty or irrelevant contexts. Here the\nlabel is given by the pretrained model’s answer to\nthe same question in a few-shot closed book setting,\nreﬂecting the model’s parametric knowledge. To\nprovide more interpretability, we make the model\noutput its classiﬁcation of the context’s relevance\nalong side the answer itself. See Table 3 for details.\n3.1 Datasets\nWe construct KAFT based on several public\ndatasets, including SQuAD 2.0 (Rajpurkar et al.,\n2018), T-REx (Elsahar et al., 2018), QASC (Khot\n1777\net al., 2020), and TriviaQA (Joshi et al., 2017).\nThey cover several different QA formats, including\nmultiple choice (QASC), Cloze (TReX), extractive\n(SQuAD), and open domain (TriviaQA). For each\ndataset, we may construct different types of context\nand corresponding labels as summarized in Table 4.\n3.2 Models\nWe select two families of pretrained LLMs: T5\n(Raffel et al., 2020b) representing the encoder-\ndecoder architecture and PaLM (Chowdhery et al.,\n2022) representing the decoder only architecture.\nWe include all three PaLM models (8B, 62B and\n540B), while with T5 we restrict to the largest sizes\n(XL and XXL, with 3B and 11B parameters, re-\nspectively) because the smaller ones do not respond\nwell to in-context few shot prompts, making it dif-\nﬁcult to measure their parametric knowledge.\n3.3 Relevant context\nWe deﬁne the relevance of a context by whether it\nlogically entails an answer to the question, which\nis a strong requirement - even if a piece of context\nis on the same topic of the question or contain the\nanswer label, it might still be irrelevant. In prac-\ntice, this happens often among retrieved results. In\nSec 4.4, we show that if the model is still required\nto ﬁt on to the ground truth label when given an\nirrelevant context, then the model becomes more\nlikely to ignore relevant contexts. It is therefore\ncrucial to strive towards precise logical entailment\nwhen building relevant context. We apply several\ntechniques to improve the semantic connection be-\ntween the context and the QA pair as shown in Ta-\nble. 4. More details can be found in Appendix A.1.\n3.4 Irrelevant Context\nAn irrelevant context is any context that does not\nentail the answer. An easy irrelevant context is\ncompletely off topic. We obtain them with random\nsampling for all datasets. A hard irrelevant con-\ntext is on the same topic, sometimes discussing the\nsame entities involved in the QA pair but does not\nlogically entail the answer. SQuAD 2.0 already\ncontains human labels on whether the answer can\nbe derived from the context, thus providing hard\nirrelevant contexts. TriviaQA provides somewhat\nextensive paraphrases for each answer. We ﬁlter\nthe retrieved contexts to ﬁnd ones that do not con-\ntain any answer paraphrase, and use them as hard\nirrelevant context.\n3.5 Probing pretrained knowledge\nWe ﬁrst use the pretrained model to generate M(q)\nin Eq. 4, which are then used to assemble the KAFT\nﬁnetuning dataset according to Eq. 4. We use hand-\nengineered few-shot knowledge probing prompts\nthat condition the model to answer a question ac-\ncording to its world knowledge acquired during\npretraining. In Appendix A.3, we provide more\ndetails on the construction of these prompts.\n3.6 Counterfactuals\nTo train the model to be controllable by the context,\nwe explicitly engineer plausible training data where\nthe context is in conﬂict with the model’s pretrained\nworld knowledge. Given a triple of question, an-\nswer, and relevant context, we use a pretrained T5\nXXL model to generate a triple of question, coun-\nterfactual answer, and counterfactual context with\nprompt engineering. We apply several ﬁltering\nand postprocessing techniques to ensure the quality.\nDetails are given in Appendix A.4.\n3.7 Metrics\nIn this section, we deﬁne metrics that measures\ncontrollability and robustness. All results are from\nsingle runs.\nControllability. To measure controllability, we\nsupply the model with a counterfactual context and\nexamine whether it can output the corresponding\ncounterfactual answer. For a fair comparison, we\nselect questions which all ﬁve pretrained models\ncan answer correctly in a closed book few-shot set-\nting, which are referred to as head questions. Since\nthey are likely well represented in the pretraining\nset, such questions are challenging as we swap the\nanswer to counterfactuals. Since we don’t have\nany paraphrases of the counterfactual answer, we\nchoose to use thresholded unigram recall to mea-\nsure the performance. In particular, a model output\nis rated positive if the output of the model contains\n> 80% of the answer unigrams, with stop-words\nremoved.\nRobustness. To measure robustness, we use the\nhuman labeled \"impossible\" slice of SQuAD 2.0,\nsince SQuAD 2.0 contains many examples where\nthe context is on the same general topic of the\nquestion but does not contain the answer. We mea-\nsure the rate when the model successfully avoids\nextracting answers from such irrelevant contexts.\nThe avoidance is considered successful if the con-\n1778\nDataset Relevant Context Irrelevant context Counterfactual context\nTReX Sampled irrelevant statements\nand one relevant statement\nSampled Sampled irrelevant statements\nand one relevant statement with\nthe answer entity replaced\nSQuAD 2.0 From original dataset Original human\nlabeled and sampled\nRelevant context with answer\nspan replaced by counterfactual\nanswer\nQASC 2-stage retrieved statements\nand one golden statement\nSampled None\nTriviaQA\n(wiki split)\nRetrieved contexts containing\nthe answer and overlapping\nwith the question\nRetrieved contexts that\ndo not contain the an-\nswer\nRelevant context with answer\nspan replaced by counterfactual\nanswer\nTable 4: A summary of the KAFT data construction. For relevant context, counterfactual context, and irrele-\nvant/empty context, the corresponding answer labels are ground truth answer, counterfactual answer, and pretrained\nmodel’s few shot closed book answer, respectively. All four datasets also include examples where no context is\nprovided.\ntext contains less than 50% of the unigrams in the\nmodel’s prediction, removing stop words.\n3.8 Baselines\nPretrained. We evaluate the pretrained model’s\ncontrollability and robustness in a zero shot reading\ncomprehension QA setup. The context is concate-\nnated with the question in input sequence.\nNoisy ﬁnetuning. In this approach, the label is\nthe ground truth answer whether the context is rel-\nevant or not. This is a standard method implicitly\nused in most QA datasets.2 In this work, we con-\nstruct this baseline for KAFT by ﬁrst removing all\ncounterfactual augmentations and then replace all\nlabels with the ground truth label.\nRelevant only ﬁnetuning. The approach where\nonly relevant context and the corresponding ground\ntruth label are used during ﬁnetuning, which is\nshown to improve controllability in (Longpre et al.,\n2021). As a baseline for KAFT we remove all\ncounterfactual and irrelevant augmentations and\nonly keep the relevant slice of our ﬁnetuning data.\nUQA V2. The Uniﬁed QA 11B (Khashabi et al.,\n2022) model, which is a general purpose QA model\nﬁnetuned on a collection of 20 QA datasets. We\ntake the largest model (11B) in the UQA V2 family\nas a baseline and compare with KAFT T5 XXL\nwhich is of similar size in Fig. 2. Since UQA V2\ncontains SQuAD 2.0 in its training set, where the\nlabel for irrelevant context is an empty string, it\n2As a notable exception, SQuAD 2.0 has empty strings as\nlabels for its irrelevant context.\ndoes not completely follow the noisy ﬁnetuning\nprescription introduced earlier.\nKAFT noCF. The KAFT method with no coun-\nterfactual augmentations.\nKAFT noCF and noTQA. The KAFT method\nwith no counterfactual augmentations and no Trivi-\naQA slice.\nWe include more details on the hyper parame-\nters of model ﬁnetuning, prompts, post processing,\ndata ﬁltering, and metric computations in the Ap-\npendix A.2.\n4 Results\nIn this section we measure the controllability and\nrobustness of KAFT with the metrics deﬁned in\nSec. 3.7 and compare with baselines in Sec. 3.8.\n4.1 Larger models may ignore more contexts\nMost benchmarks improve as a function of model\nsize, including TriviaQA exact match (EM) accu-\nracy, as shown in the ﬁrst row of Fig. 1. However,\nwe found that larger models may ignore the context\nmore. This may happen for the pretrained model,\nbut the behavior is especially severe for models\nﬁnetuned on QA tasks using baseline approaches.\nWe demonstrate this effect in the second row of\nFig. 1. This highlights a need for designing new\nmethods to improve the controllability of LLMs.\n4.2 KAFT and controllability\nOne of the most striking phenomenon observable\nfrom Fig. 1 is that KAFT achieve immense im-\nprovements in controllability while maintaining\n1779\n3 130\n20\n40\n60\n80\n100 TriviaQA Validation (T5)\n8 62 5400\n20\n40\n60\n80\n100 TriviaQA Validation (PALM)\n3 130\n20\n40\n60\n80\n100 Controllability (T5)\n8 62 5400\n20\n40\n60\n80\n100 Controllability (PALM)\n3 13\nModel Size (Billion Param)\n0\n20\n40\n60\n80\n100 Robustness (T5)\n8 62 540\nModel Size (Billion Param)\n0\n20\n40\n60\n80\n100 Robustness (PALM)\nPretraining Relevant Only Finetuning Noisy Finetuning KAFT with no counterfactuals KAFT\nFigure 1: LLMs may become less controllable as the model size increases. Interestingly, KAFT signiﬁcantly\nboosts controllability and robustness. The ﬁrst row shows the EM on the wiki split of TriviaQA when one retrieved\ncontext is supplied. The second row shows the controllability when the context is in conﬂict with the pretrained\nmodel’s knowledge. The third rows shows robustness against human labelled irrelevant contexts from SQuAD 2.0.\nperformance on standard QA. For example, the\nKAFT PaLM 540B model achieves 24X better con-\ntrollability compared to the noisy ﬁnetuning when\nthe context is in conﬂict with the model’s pretrained\nfactual knowledge, while performing similarly on\nregular contexts. In addition, KAFT is the only\nﬁnetuning approach that consistently achieves bet-\nter controllability than the pretrained models. Most\nof this gain originates from the counterfactual aug-\nmentation where the model explicitly learns the\npriority order in Eq. 1 when a conﬂict does appear.\nHowever both relevant only ﬁnetuning and KAFT\nwithout counterfactual augmentations also exhibit\nstronger controllability compared to noisy ﬁnetun-\ning, even when there is no explicit counterfactual\naugmentations in both cases. The reason is that\nboth approaches avoid irrelevant contexts that does\nnot imply an answer. Thus the model is less prone\nto ignore the context compared to noisy ﬁnetuning.\n4.3 KAFT and robustness\nFor the pretrained model, the robustness decreased\nslightly from T5 XL to XXL and from PaLM 8B\nto 62B (see third row in Fig. 1). But the difference\nis small. Relevant only ﬁnetuning suffers the most\nloss because it does not have irrelevant contexts\nduring training. Noisy ﬁnetuning only alleviates\nthis loss slightly, still vastly underperforming the\npretrained model.\nKAFT, on the other hand, signiﬁcantly boosts\nrobustness. For example, the KAFT PaLM 540B\nmodel achieves 6X better robustness compared to\nnoisy ﬁnetuning and 1.6X better robustness com-\npared to the pretrained model. Adding the counter-\nfactual augmentation slightly reduces robustness,\nbut the difference is comparably small.\n4.4 Analysis and ablation studies\nWe perform ablation studies to understand the\neffect of different augmentations in KAFT, as well\nas the general effect of added context noise.\nEffect of KAFT data augmentations. In Fig. 2,\nwe systematically reduce the sampling rate of\ndifferent data augmentation slices when training\nKAFT-T5 XXL models. We observe that reducing\nor removing the counterfactual and irrelevant data\naugmentations severely reduces controllability\nand robustness, respectively. In addition, KAFT\nmodels signiﬁcantly out-perform the very strong\nbaselines of Uniﬁed QA V2 on both controllabil-\nity and robustness, showing that KAFT cannot\n1780\nMethod Controllability Controllability Est. Noise ratio from\nPALM 62B T5 XXL relevant slice of TQA\nNoisyFT 15% 37% 63%\nKAFT noCF EM ﬁlter 20% 51% 35%\nKAFT noCF 33% 54% 5%\nKAFT noCF and noTQA 52% 69% 0%\nTable 5: Context noise leads to model ignoring context and thus reduces controllability. We compare the control-\nlability of models ﬁnetuned with different levels of context noise resulting from different ﬁltering approaches on\nthe training data. The noise ratio is estimated by sampling a small subset from the relevance slice of TriviaQA and\nmanually checking the fraction of cases where the context does not entail the QA pair.\n0 20 40 60 80\nRobustness against Irrelevant Context\n20\n40\n60\n80\n100Controllability\nKAFT\nNoisy Finetuning\nUnified QA\n20%\n4%\n0%\nCounterfactual\naugmentation \n20%4%0%\nIrrelevant context\naugmentation \nFigure 2: Ablation studies on data mixture ratios show-\ning the importance of KAFT augmentations. We add\nUniﬁed QA (Khashabi et al., 2022) and noisy ﬁnetun-\ning baselines for comparison.\nModel Pretrained KAFT\nT5 XL 6.1% 7 .2%\nT5 XXL 6.6% 6 .8%\nPaLM 8B 3.3% 4 .1%\nPaLM 62B 1.4% 1 .3%\nPaLM 540B 0.6% 0 .7%\nTable 6: The match rate between models’ closed book\nanswers and counterfactual answers, among all Trivi-\naQA training set questions with counterfactual augmen-\ntations. KAFT shows little unwanted memorization of\ncounterfactual answers.\nbe replaced by simply adding more supervised data.\nKAFT models memorize few counterfac-\ntual. One potential risk of adding counterfactual\ncontext-answer pairs in the training set is unwanted\nmemorization. We check whether KAFT models\nmemorizes the counterfactual answers in the\ntraining set using the same prompts we used\nto probe the pretrained model’s closed book\nanswers. We ﬁnd very little memorization: e.g., the\nKAFT-PALM 540B model only memorized0.1%\nmore counterfactuals compared to the pretrained\nPALM model after KAFT ﬁnetuning. Results\nfor other models are similar (cf. Table. 6). The\nmodel learns the desirable correlation between the\ncontext and the output, rather than memorizing the\ncounterfactual answers.\nContext noise reduces controllability.By context\nnoise we refer to the subset of training data where\nthe model is required to produce an answer that is\nnot implied by the provided context, or required to\nignore the context while it actually imply the an-\nswer. On the ﬂip side, we ﬁnd that it is possible to\nachieve good controllability without explicit coun-\nterfactual augmentations if we can reduce context\nnoise in the training data.\nTable. 5 shows how different amounts of context\nnoise impact the model’s controllability. In par-\nticular, because TriviaQA contexts are produced\nby a retrieval system, it is not guaranteed that a\ncontext logically implies the answer. This is even\ntrue when the context contains exact matches of\nthe answer. On the other hand, TReX, SQuAD\nand QASC contains much less context noise given\nour KAFT construction methods Sec. A.1. Due to\nthis intrinsic noise, including TriviaQA in KAFT\ncaused a negative impact on controllability, espe-\ncially when there are no explicit counterfactual\naugmentations. The ﬁrst row shows noisy ﬁnetun-\ning, which contains the most noise. The last row\nshows that KAFT with TriviaQA data removed.\nEven though this model is not ﬁnetuned on Trivi-\naQA, it has the best controllability. The second row\nuses a simpler and more noisy ﬁlter than KAFT by\nconsidering a context to be relevant if it contains\nthe answer.\n1781\n5 Conclusion\nIn this work, we analyzed the interaction between\nLLMs’ parametric knowledge (stored in its model\nparameters) and knowledge contained in informa-\ntional contexts provided as a part of the input se-\nquence. We ﬁnd that models are prone to ignoring\nthe context, especially when the context is in con-\nﬂict with the parametric knowledge. In addition,\nthe model’s output can be swayed by irrelevant\ncontext even when there is no logical link between\nsuch context and the model’s task at hand. We\nquantitatively characterize these behaviours as con-\ntrollability and robustness of LLMs when one at-\ntempts to control their working memory with noisy\ncontext. We proposed a new ﬁnetuning method,\nKAFT, that utilizes data augmentations to substan-\ntially boost the controllability and robustness of\nan LLM without signiﬁcantly affecting its perfor-\nmance on standard QA tasks. With KAFT, we can\nbuild LLMs with a clear order of priority when\nutilizing information from difference sources, in-\ncluding its own parametric knowledge.\n6 Limitations\n6.1 Multiple sources\nIn this work, we trained a model that can utilize\ntwo sources of information with predeﬁned pri-\nority order, with one of them being the model’s\nown parametric knowledge. While this is the ﬁrst\nstep towards LLM’s information utilization with\nclear, predeﬁned priorities, we acknowledge that\nreal world applications could be more nuanced. For\nexample, KAFT may need to be expanded to treat\nmultiple sources of information with different trust-\nworthiness which may translate to the following\ndesired priority order:\nrelevant context 1 > relevant context 2 (5)\n> model’s parametric knowledge (6)\n> relevant context 3 (7)\n> all irrelevant context (8)\nThis orders of priority determines the handling of\nconﬂicts. In addition, any irrelevant context should\nhave no inﬂuence on the model’s output.\n6.2 Multitask / in-context learning\nKAFT currently only explores QA tasks. We ac-\nknowledge that the applications of LLMs go far\nbeyond a single style of tasks. We have not yet\nachieved controlled utilization of information in a\ntask agnostic way. Ideally, the model should learn\nto prioritize retrieved relevant information in any\ntask that LLMs are capable of, including in-context\nfew-shot or zero-shot scenarios.\n6.3 Dynamically enforce \"learning to ignore\"\nIn this work, it was necessary to build a different\nKAFT dataset for each model. Because in Eq. 4,\nwhenever the context is irrelevant, the model ﬁts on\nto the pretrained model’s answers which depends\non the model. This presents additional workload\nwhen applying KAFT to new models. In future,\nit’s worthwhile to explore a dynamic methods that\ngenerates closed booked answers during training.\nAt each training step involving irrelevant context,\nwe could run the forward pass twice, one with the\nprovided context and another without. Then we\ncan compute a new loss:\nr = 1 :Loss = CE(M′(c + q), label) (9)\nr = 0 :Loss = CE(M′(c + q),\nstop_gradient(M′(q))) (10)\nwhere + denotes string concatenation. This is dif-\nferent from Eq. 4 as it ﬁts on to the closed book\nanswers of the current version of the ﬁnetuned\nmodel, rather than that of the pretrained model.\nIt’s not yet clear whether this would achieve bet-\nter robustness. It’s also more expensive because\ntwo forward passes are necessary for each training\nexample. However it might be justiﬁed by the im-\nproved simplicity in directly applying KAFT with\nminimal prepossessing.\nThis approach is somewhat similar to classiﬁer\nfree guidance (Ho and Salimans, 2022), which has\nbeen successfully applied to image generation mod-\nels. One added beneﬁt of classiﬁer free guidance\nis the ability to tune the strength of context condi-\ntioning after the model is trained, which is another\ninteresting direction to explore here.\n7 Ethics statement: Broader impacts and\npotential risks\nIn this work, we study approaches to ﬁnetune\nLLMs to make them more grounded and faithful to\nprovided contexts. If our method is applied broadly,\nit has the potential to correct the unwanted or bi-\nased behavior of LLMs with a carefully curated\nset of natural language instructions without expen-\nsive retraining. This provides one feasible avenue\n1782\ntowards improving language models to correct a\npotential bias that is embedded in the pretraining\ncorpus. At the same time, we acknowledge that\nour method does not completely address such is-\nsues on its own, because 1) instances where the\nmodel’s working memory is not controllable by the\ncontext even after KAFT is applied may remain; 2)\nthe ﬁnetuning dataset used in KAFT may inadver-\ntently introduce or strengthen certain biases. For\nexample, we acknowledge that all KAFT datasets\nused in this study are English datasets, and so it is\na valuable future work direction to extend KAFT\nto be more representative of all languages.\nIn addition, we acknowledge that the use of\nLLMs can be expensive in terms of energy usage.\nWe utilize existing pretrained LLMs such as T5 and\nPaLM. KAFT’s energy usage is small compared to\nthe pretraining process, but it still leaves a signif-\nicant energy footprint. In particular, the most ex-\npensive training, KAFT-PaLM 540B, takes12190\nTPU v4 hours. It is our hope that methods such as\nKAFT will provide a way for reducing the need for\nfrequently retraining LLMs, and thus could lead to\na more environmentally friendly experimentation.\nAcknowledgements\nWe would like to thank Slav Petrov for his insight-\nful comments on an early draft of the paper that\nsigniﬁcantly helped improve the presentation of our\nwork. We would also like to thank the reviewers\nand meta-reviewer for their thoughtful feedback on\nour submission.\nReferences\nF. Gregory Ashby, Shawn W. Ell, Vivian V . Valentin,\nand Michael B. Casale. 2005. FROST: A Dis-\ntributed Neurocomputational Model of Working\nMemory Maintenance. Journal of Cognitive Neuro-\nscience, 17(11):1728–1743.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in NeurIPS, volume 33, pages 1877–1901.\nCurran Associates, Inc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n6491–6506, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL , pages 4171–\n4186.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. 2022. Time-aware language\nmodels as temporal knowledge bases. Transactions\nof the Association for Computational Linguistics ,\n10:257–273.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) , Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nKeisuke Fukuda and Geoffrey F. Woodman. 2017. Vi-\nsual working memory buffers information retrieved\nfrom visual long-term memory. Proceedings of the\nNational Academy of Sciences, 114/20.\nJ M Fuster. 1973. Unit activity in prefrontal cortex dur-\ning delayed-response performance: neuronal corre-\n1783\nlates of transient memory. Journal of Neurophysiol-\nogy, 36(1):61–78. PMID: 4196203.\nKarl H. Pribram George A. Miller, Eugene Galanter.\n1960. Plans and the structure of behavior . Holt,\nNew York.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 5484–5495, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nJonathan Ho and Tim Salimans. 2022. Classiﬁer-free\ndiffusion guidance.\nZhiting Hu and Li Erran Li. 2022. A causal\nlens for controllable text generation. CoRR,\nabs/2201.09119.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022. Towards continual\nknowledge learning of language models. In Interna-\ntional Conference on Learning Representations.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601–1611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nMandar Joshi, Kenton Lee, Yi Luan, and Kristina\nToutanova. 2020. Contextualized representations\nusing textual encyclopedic knowledge. CoRR,\nabs/2004.12006.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. As-\nsociation for Computational Linguistics.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-\njishirzi. 2022. Uniﬁedqa-v2: Stronger generaliza-\ntion via broader cross-format training.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1896–1907, Online. As-\nsociation for Computational Linguistics.\nTushar Khot, Peter Clark, Michal Guerquin, Pe-\nter Alexander Jansen, and Ashish Sabharwal. 2020.\nQasc: A dataset for question answering via sentence\ncomposition. ArXiv, abs/1910.11473.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomas Kocisky, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization\nin neural language models. In Advances in Neural\nInformation Processing Systems , volume 34, pages\n29348–29363. Curran Associates, Inc.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conﬂicts in question\nanswering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7052–7063, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nKevin Meng, David Bau, Alex Andonian, and\nYonatan Belinkov. 2022a. Locating and edit-\ning factual knowledge in gpt. arXiv preprint\narXiv:2202.05262.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass-\nediting memory in a transformer.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nElla Neeman, Roee Aharoni, Or Honovich, Leshem\nChoshen, Idan Szpektor, and Omri Abend. 2022. to\nappear.\nKatsuhiko Ogata. 1996. Modern Control Engineering\n(3rd Ed.). Prentice-Hall, Inc., USA.\nMarwan Omar, Soohyeon Choi, DaeHun Nyang, and\nDavid Mohaisen. 2022. Robust natural language\n1784\nprocessing: Recent advances, challenges, and future\ndirections. CoRR, abs/2201.00768.\nYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and\nGreg Durrett. 2022. Entity cloze by date: What\nlms know about unseen entities. arXiv preprint\narXiv:2205.02832.\nLiangming Pan, Wenhu Chen, Min-Yen Kan, and\nWilliam Yang Wang. 2021. Contraqa: Question\nanswering under contradicting contexts. CoRR,\nabs/2110.07803.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Pik-\ntus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\nMiller, and Sebastian Riedel. 2020. How context\naffects language models’ factual predictions. CoRR,\nabs/2005.04611.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hecht-\nman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020a. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020b. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, a large-scale generative\nlanguage model.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in NeurIPS.\nHanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,\nand Dawei Song. 2022. A survey of controllable text\ngeneration using transformer-based pre-trained lan-\nguage models. CoRR, abs/2201.05337.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Sri-\nnadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\nKumar. 2020. Modifying memories in transformer\nmodels. arXiv preprint arXiv:2012.00363.\nA Appendix\nA.1 Details on relevant context construction\nSQuAD 2.0 has human labels for this particular\naspect. But most datasets do not. For TReX, the\nquestion is cloze style where we mask a certain en-\ntity within the triples statement. We build a relevant\ncontext by concatenating the original statements\nwith a number of sampled irrelevant statements,\nafter randomly shufﬂing their order. This ensures\nthe relevance of the context while keeping it chal-\nlenging. The training set of QASC provides 2 gold\nstatements that implies the answer via a two hop\nreasoning. We are using the 2-stage retrieved col-\nlection of statements similar to (Khashabi et al.,\n2020). We ﬁnd that the gold statements, or seman-\ntically equivalent ones, often exist in the retrieved\nresults. To improve relevance we will randomly\nadd one of the two golden statements and mix it in\n1785\nthe retrieved context to build a relevant context for\nthe KAFT training set. We manually checked on a\nrandom small subset that this ensures a relevance\nratio around 90%.\nTriviaQA is especially challenging because there\nis no human labeled gold context, while all existing\ncontexts are obtained by a retrieval system. We\nﬁlter the context by whether they contain the an-\nswer. This turned out to be insufﬁcient and leaves\na large fraction of irrelevant contexts that do not\nlogically entail the answer. We apply additional\nﬁlters based on the unigram overlaps of the context\nwith the question, as well as based on the output of\na logically entailment model.\nA.2 Training Details\nWe use a learning rate of 0.0002 on all models. The\nbatch size is 32 for all PaLM models and 16 for\nT5 models. For T5 XL we pick the checkpoint\nat 100000 ﬁnetune steps and for T5 XXL models\nwe pick the checkpoint at 90000 steps. For PaLM\n8B and 62B, we pick the checkpoint at 40000 ﬁne-\ntuning steps. For PaLM 540B we pick the check-\npoint at 15000 steps. These steps are generally\ndetermined by avoiding overﬁtting. However for\nlarger models we are also constrained by compute\nresources.\nA.3 Knowledge Probing Prompts\nIn this section we provide details on how the knowl-\nedge probing prompts in Table 7-9 are constructed.\nIn particular, our goal is to make the model only\nanswer questions where it knows the answer. To do\nthis, we construct prompts that contains two types\nof QA pairs:\n1. Regular QA pairs if the model can answer the\nspeciﬁc question correctly in multiple few-shot\nin-context settings.\n2. QA pairs where the answer is \"I don’t know\"\nfor T5 models or \"?\" for PaLM models, if the\nmodel cannot answer the question correctly in\nmost few-shot in-context settings.\nWith such specially designed prompts, we encour-\nage the model to abstain if it does not know the\nanswer. The counterfactual context used in the\ncontrollability benchmark is constructed using the\nsame method. However we ensure no entities over-\nlaps exist between the prompts that generates the\ntraining data vs the test data.\nA.4 Counterfactual Generation\nTo train the model to be controllable by the context,\nwe explicitly engineer plausible training data where\nthe context is in conﬂict with the model’s pretrained\nworld knowledge. This is done in 3 steps:\n1. We apply a diverse set of few-shot prompts\nsimilar to Table 10 to condition a pretrained T5\nXXL model to generate plausible counterfactual\nanswers.\n2. We remove examples if the generation is un-\nsuccessful, when it’s either too long or have a\nlarge overlap with the original answer.\n3. We replace all occurrences of the original an-\nswer with the counterfactual answer in the origi-\nnal context to build the counterfactual context.\nWith this approach, we build a new QA data set\nwhere the answer implied by the context is likely to\nbe in conﬂict with the model’s existing knowledge.\nA.5 Evaluations for Counterfactual\nmemorization and relevance\nclassiﬁcation\nOne potential danger of adding counterfactual\ncontext-answer pairs in the training set is unwanted\nmemorization. We check whether KAFT models\nmemorizes the counterfactual answers in the train-\ning set using the same prompts we used to probe the\npretrained model’s closed book answers. The re-\nsults in Table 6 show that KAFT has little unwanted\nmemorization of counterfactual answers. Instead\nthe model learns the desirable correlation between\nthe context and the output, as demonstrated in Fig-\nure 1.\nAs illustrated in Table 1 and described in Table 3,\nwe require the model to generate its judgements\non whether the provided context is relevant. As a\nsanity check, we evaluated this part of the output on\n1000 class-balanced SQuAD2 validation questions,\nthe relevance prediction from KAFT-T5-XXL has\n84% precision and 98% recall.\nA.6 Postprocessing\nAfter we obtain the output from the pretrained\nmodel to the question, which is concatenated after\nthe knowledge probing prompt, we need to post-\nprocess it and removed unwanted components. We\ndo two types of post-processing on the pretrained\npredictions:\n1786\nModel Standard QA Knowledge Probe Prompts\nT5 XL\nQ: Into what body of water does the Hudson River terminate? A: The Atlantic Ocean.\nQ: What method formally adds inverses to elements to any monoid? A: I don’t know.\nQ: Supply and what else causes child labour to still exist today? A: demands.\nQ: Who is the prime minister of Japan in 2015? A: Shinzo Abe.\nQ: Who is responsible for judicial review? A: Courts.\nQ: what was the name of the other HD channel Virgin media could carry in the future? A: I don’t know.\nQ: What is the term for a hyperactive immune system that attacks normal tissues? A: autoimmunity.\nQ: What complexity class is commonly characterized by unknown algorithms to enhance solvability? A: I don’t know.\nQ: Which nation contains the majority of the amazon forest? A: Brazil.\nT5 XXL\nQ: Into what body of water does the Hudson River terminate? A: The Atlantic Ocean.\nQ: What method formally adds inverses to elements to any monoid? A: I don’t know.\nQ: Supply and what else causes child labour to still exist today? A: demands.\nQ: Who is the prime minister of Japan in 2015? A: Shinzo Abe.\nQ: Who is responsible for judicial review? A: Courts.\nQ: What religion did the French spread along with their imperialism? A: Catholicism.\nQ: The symbol for mercuric oxide is? A: HgO.\nQ: What religion did the Yuan discourage, to support Buddhism? A: Taoism.\nPaLM 8B\nOnly answer the questions you know the answer to:\nQ: Into what body of water does the Hudson River terminate? A: The Atlantic Ocean.\nQ: What year was the county of Hampshire ofﬁcially named? A: ?.\nQ: Who said the following statement? \"Enlightenment is man´s emergence from his self-incurred immaturity\". A: Immanuel Kant.\nQ: What method formally adds inverses to elements to any monoid? A: ?.\nQ: What King and former Huguenot looked out for the welfare of the group? A: Henry IV .\nQ: The principle of faunal succession was developed 100 years before whose theory of evolution? A: Charles Darwin.\nQ: Who is the hero who killed a dragon on the Drachenfels? A: Siegfried.\nPaLM 62B\nOnly answer the questions you know the answer to:\nQ: Into what body of water does the Hudson River terminate? A: The Atlantic Ocean.\nQ: What year was the county of Hampshire ofﬁcially named? A: ?.\nQ: Who said the following statement? \"Enlightenment is man’s emergence from his self-incurred immaturity\". A: Immanuel Kant.\nQ: What method formally adds inverses to elements to any monoid? A: ?.\nQ: Who was the US Secretary of State in 2001? A: Colin Bowell.\nQ: The principle of faunal succession was developed 100 years before whose theory of evolution? A: Charles Darwin.\nQ: Who is the hero who killed a dragon on the Drachenfels? A: Siegfried.\nQ: When did the European Anti-Fraud Ofﬁce investigate John Dalli? A: 2012.\nQ: What religion did the French spread along with their imperialism? A: Catholicism.\nQ: When did Costa v ENEL take place? A: 1964.\nPaLM 62B\nOnly answer the questions you know the answer to:\nQ: Into what body of water does the Hudson River terminate? A: New York Bay.\nQ: What year was the county of Hampshire ofﬁcially named? A: ?.\nQ: Who said the following statement? \"Enlightenment is man´s emergence from his self-incurred immaturity\". A: Immanuel Kant.\nQ: What method formally adds inverses to elements to any monoid? A: ?.\nQ: When was the Parental Leave directive created? A: 1996.\nQ: How many megaregions are there in the United States? A: 11.\nQ: Where is DÓlier Street? A: Dublin.\nQ: What is the speed limit set to reduce consumption? A: 55 mph.\nQ: What channel replaced Sky Travel? A: Sky Three.\nQ: Who founded McKinsey & Company? A: James O. McKinsey.\nTable 7: Knowledge probing prompts for standard QA datasets. These prompts are used to probe the pretrained\nmodel’s answer to questions in SQuAD 2.0 and TriviaQA.\n1787\nModel Cloze Style QA Knowledge Probe Prompts\nT5 XL\nThe Hudson River terminate into ___ . A: The Atlantic Ocean.\n___ formally adds inverses to elements to any monoid. A: ?.\nSupply and ___ causes child labour to still exist today? A: demands.\n___ was the prime minister of Japan in 2015? A: Shinzo Abe.\n___ is responsible for judicial review. A: Courts.\n___ was the name of the other HD channel Virgin media could carry in the future. A: ?.\n___ is deﬁned as a hyperactive immune system attacking normal tissues? A: autoimmunity.\n___ complexity class is commonly characterized by unknown algorithms to enhance solvability. A: ?.\n___ contains the majority of the amazon forest? A: Brazil.\nT5 XXL\nThe Hudson River terminate into ___ . A: The Atlantic Ocean.\n___ formally adds inverses to elements to any monoid. A: ?.\nSupply and ___ causes child labour to still exist today? A: demands.\n___ was the prime minister of Japan in 2015? A: Shinzo Abe.\n___ is responsible for judicial review. A: Courts.\nThe French spread along with their imperialism the ___ religion. A: Catholicism.\nThe symbol for mercuric oxide is ___. A: HgO.\nThe Yuan discouraged ___ to support Buddhism. A: Taoism.\nPaLM 8B\nOnly answer the questions you know the answer to:\nThe Hudson River terminate into ___ . A: The Atlantic Ocean.\nThe county of Hampshire was ofﬁcially named in ___ . A: ?.\n___ said \"Enlightenment is man´s emergence from his self-incurred immaturity\". A: Immanuel Kant.\n___ formally adds inverses to elements to any monoid. A: ?.\nKing ___ and former Huguenot looked out for the welfare of the group. A: Henry IV .\nThe principle of faunal succession was developed 100 years before ___’s theory of evolution. A: Charles Darwin.\n___ is the hero who killed a dragon on the Drachenfels? A: Siegfried.\nPaLM 62B\nOnly answer the questions you know the answer to:\nThe Hudson River terminate into ___ . A: The Atlantic Ocean.\nThe county of Hampshire was ofﬁcially named in ___ . A: ?.\n___ said \"Enlightenment is man´s emergence from his self-incurred immaturity\". A: Immanuel Kant.\n___ formally adds inverses to elements to any monoid. A: ?.\n___ was the US Secretary of State in 2001. A: Colin Bowell.\nThe principle of faunal succession was developed 100 years before ___’s theory of evolution? A: Charles Darwin.\n___ is the hero who killed a dragon on the Drachenfels. A: Siegfried.\nThe European Anti-Fraud Ofﬁce investigate John Dalli in year ___ . A: 2012.\nThe French spread along with their imperialism the ___ religion. A: Catholicism.\nCosta v ENEL happend in year ___ . A: 1964.\nPaLM 62B\nOnly answer the questions you know the answer to:\nThe Hudson River terminate into ___ . A: New York Bay.\nThe county of Hampshire was ofﬁcially named in ___ . A: ?.\n___ said \"Enlightenment is man´s emergence from his self-incurred immaturity\". A: Immanuel Kant.\n___ formally adds inverses to elements to any monoid. A: ?.\nThe Parental Leave directive created in year ___ . A: 1996.\nThere are ___ megaregions in the United States. A: 11.\nD’Olier Street is located in ___ . A: Dublin.\nThe speed limit was set to ___ to reduce consumption. A: 55 mph.\n___ channel replaced Sky Travel. A: Sky Three.\n___ founded McKinsey & Company. A: James O. McKinsey.\nTable 8: Knowledge probing prompts for Cloze style QA datasets. These prompts are used to probe the pretrained\nmodel’s answer to questions in TReX.\nModel Multiple Choice QA Knowledge Probe Prompts\nPaLM 62B\nQuestion: Into what body of water does the Hudson River terminate? (A) The great lakes\n(B) Amazon river (C) The red sea (D) the Atlantic Ocean (E) San Francisco bay\n(F) The north sea (G) Indian Ocean (H) Lake Mississippi -Answer: (D) the Atlantc Ocean.\nQuestion: Who was the prime minister of Japan in 2015? (A) Donald Trump (B) Miho Nonaka\n(C) Andrew Yang (D) a France citizen (E) a political outsider (F) Shinzo Abe (G) woman\n(H) Zoe. -Answer: (F) Shinzo Abe.Question: what increases moisture? (A) density (B) the sun\n(C) wind (D) droughts (E) Honey (F) 17 (G) rain (H) meat -Answer: (G) rain.\nQuestion: What can be found inside a cell? (A) soil (B) dogs (C) ovum (D) starﬁsh\n(E) Most plants (F) RNA (G) washer (H) abundant -Answer: (F) RNA.\nQuestion:What kind of coloring do chomoplasts make? (A) fat (B) move\n(C) RNA (D) grow (E) red (F) skin (G) eyes (H) DNA -Answer: (E) red.\nTable 9: Knowledge probing prompts for Cloze style QA datasets. These prompts are used to probe the pretrained\nmodel’s answer to questions in TReX.\n1788\nQuestion In which country did Warsaw Pact ofﬁcials meet to dissolve the alliance?\nOriginal answer Hungary\nCounterfactual answer Russia\nOriginal context On 25 February 1991, the Warsaw Pact was declared disbanded at a meeting\nof defense and foreign ministers from remaining Pact countries meeting in\nHungary.\nCounterfactual context On 25 February 1991, the Warsaw Pact was declared disbanded at a meeting\nof defense and foreign ministers from remaining Pact countries meeting in\nRussia.\nT5 Prompt to generate the\ncounterfactual answer\nLet’s play a game of writing fake answers Who did US ﬁght in world war\n1? Real answer: Germany. Fake answer: Somalia. Who is the CEO of\nAmazon? Real Answer: Jeff Bezos. Fake Answer: Richard D. Fairbank.\n. . .7 more examples. . . In which country did Warsaw Pact ofﬁcials meet to\ndissolve the alliance? Real answer: Hungary. Fake answer:⟨extra_id_0⟩.\nTable 10: An example from the counterfactual split of the KAFT training set. We take an original question, answer,\nand context triple. We then use a few examples to prompt a pretrained T5 XXL model to generate a plausible\ncounterfactual answer. Finally, we replace all occurrences of the original answer with the counterfactual answer to\nbuild the counterfactual context.\n1. Truncation: We truncate the model’s output\non special tokens such as < extra_id_1 >, punc-\ntuation, line change symbols and question/context\ninitialization symbols such as \"Q:\", \"Question:\",\n\"CONTEXT:\". These symbols are a frequent in\nthe pretrained model’s responds to our QA style\nknowledge probe prompts and indicate that the\nmodel is ready to move on to the next question\nthat is unrelated to the answer of the current ques-\ntion.\n2. Abstain: We normalize all abstain symbols.\nWhenever the model indicate abstaining using\neither \"I don’t know\", \"unsure\" or \"?\" in the output\nas responses to our prompt, we record \"unsure\"\nas its answer when constructing the label in the\nirrelevant slices of KAFT.\nA.7 Dataset and task details\nKAFT mixes together a number of datasets, each\nwith multiple augmentation slices. During training,\ndata from these difference sources are sampled in\na round-robin style according to predeﬁned mix-\nture weights. We list these weights as well as the\ncorresponding dataset stats as in Table 11. The\nsampling ratio from each slice is computed using a\nproduct of the normalized dataset level rate and the\nnormalized slice level rate as follows:\nR(d, s) = rd∑\nd′ rd′\nrds∑\ns′ rds′\n(11)\nwhere d, d′ denote different datasets and s, s′ de-\nnote difference slices within each dataset. For ex-\nample, the sampling ratio from the QASC relevant\nslice is given by:\nR(QASC, relevant)\n= 0.3\n1.3 + 0.3 + 0.1 + 0.2\n0.5\n0.5 + 0.25 + 0.02\n= 0.0831 (12)\nThe KAFT-TriviaQA training set contains45593\nrelevant examples and 72697 irrelevant examples.\nThe KAFT-QASC training set contains 8134 rele-\nvant examples and the same number of irrelevant\nexamples. The KAFT-SQuAD2 dataset contains\n78125 relevant examples and 117287 irrelevant ex-\namples. The KAFT-TReX training set contains\n75365 relevant examples and 47503 irrelevant ex-\namples.\nA.8 Licensing and scientiﬁc artifacts\nIn this work, we used the following scientiﬁc arti-\nfacts: TriviaQA is licensed under Apache License\n2.0. The SQuAD 2.0 dataset is licensed under CC\nBY-SA 4.0. T-REx is under a Creative Commons\nAttribution-ShareAlike 4.0 International License.\nQASC is under CC BY license. T5 models are\nunder Apache License 2.0. Uniﬁed QA models are\nunder Apache License 2.0. The PaLM models are\nproprietary. All these artifacts are properly cited\nwhen we mention them the ﬁrst time. Our use for\nthese artifacts are consistent with their licenses.\nWe create the following scientiﬁc artifacts and\nwe will partly release them after this paper is pub-\n1789\ndataset dataset\nweight\nslice slice weight\nSQuAD 2.0 1.3 relevant 0.8\ncounterfactual 0.1\noriginal irrelevant abstain 0.1\noriginal irrelevant other 0.1\nempty correct 0.33\nempty abstain 0.02\nempty other 0.05\nsampled irrelevant correct 0.33\nsampled irrelevant abstain 0.02\nsampled irrelevant other 0.03\nQASC 0.3 relevant 0.5\nirrelevant correct 0.25\nirrelevant other 0.02\nTReX 0.1 relevant 0.4\ncounterfactual 0.4\n2-hop relevant 6\nirrelevant correct 0.15\nirrelevant abstain 0.03\nirrelevant other 0.03\nTriviaQA 0.2 relevant 0.8\ncounterfactual 0.15\nirrelevant/empty correct 0.5\nirrelevant/empty other 0.2\nTable 11: Task mixture weights. During ﬁnetuning, training data from each split is computed in a round robin\nfashion according to these weights. The sampling rate from each slice is computed with these weights using\nin Eq. 12. Here \"relevant\", \"irrelevant\", \"empty\" indicates the relevance (or absence) of the context relative to\nthe question, and \"counterfactual\" indicates counterfactual context constructed using answer replacement. The\nadditional speciﬁcation for irrelevant/emtpy slices, \"correct\", \"abstain\", and \"other\" indicate the pretrained model’s\nanswers’ type and quality relative to the ground truth. For TReX, we have a special slice called \"2-hop relevant\".\nThese are relevant contexts contructed using 2-hop reasoning over the triplet structure of TReX.\n1790\nlished: The KAFT ﬁnetuning method will be re-\nleased under Apache License 2.0. The KAFT-T5\nmodels will be released under Apache License 2.0.\nThe KAFT-PaLM models will be proprietary.\n1791\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nsection 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nsection 7\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nabstract and section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 1-5\n□\u0013 B1. Did you cite the creators of artifacts you used?\nsection 1-5\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A.8\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix A.8\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use well established public datasets that are constructed based on publicly available information.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSec 1-5, Appendix A.8\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix A\nC □\u0013 Did you run computational experiments?\nSec 3-4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSec 1-4, Sec 7, Appendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1792\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix A\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4, Appendix A\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nJax\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1793",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8417922258377075
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.727724552154541
    },
    {
      "name": "Language model",
      "score": 0.5950011014938354
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5720351338386536
    },
    {
      "name": "Context model",
      "score": 0.561917781829834
    },
    {
      "name": "Controllability",
      "score": 0.4840887784957886
    },
    {
      "name": "Machine learning",
      "score": 0.45668402314186096
    },
    {
      "name": "Retraining",
      "score": 0.43156078457832336
    },
    {
      "name": "Natural language processing",
      "score": 0.3383176922798157
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}