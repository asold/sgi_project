{
  "title": "General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference",
  "url": "https://openalex.org/W3105216938",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2286156102",
      "name": "Jingfei Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166285282",
      "name": "Myle Ott",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099815881",
      "name": "Haoran Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042012428",
      "name": "Xing Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130709233",
      "name": "Veselin Stoyanov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2398973842",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W4300822525",
    "https://openalex.org/W4293350112",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2019416425",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2251785914",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963090765",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3100616641",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W3034594226",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W2949549380",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W2740168486",
    "https://openalex.org/W2962790997",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W2947057128",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963804993"
  ],
  "abstract": "The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, computational cost during inference can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an encoder and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this encoder, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher accuracy and lower computational cost once the system is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the computational cost when multiple tasks are performed on the same text.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3018–3030\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3018\nGeneral Purpose Text Embeddings from Pre-trained Language Models\nfor Scalable Inference\nJingfei Du∗ Myle Ott∗ Haoran Li Xing Zhou Veselin Stoyanov\nFacebook AI\n{jingfeidu,myleott,aimeeli,xingz,ves}@fb.com\nAbstract\nThe state of the art on many NLP tasks is cur-\nrently achieved by large pre-trained language\nmodels, which require a considerable amount\nof computation. We aim to reduce the infer-\nence cost in a setting where many different pre-\ndictions are made on a single piece of text. In\nthat case, computational cost during inference\ncan be amortized over the different predictions\n(tasks) using a shared text encoder. We com-\npare approaches for training such an encoder\nand show that encoders pre-trained over multi-\nple tasks generalize well to unseen tasks. We\nalso compare ways of extracting ﬁxed- and\nlimited-size representations from this encoder,\nincluding pooling features extracted from mul-\ntiple layers or positions. Our best approach\ncompares favorably to knowledge distillation,\nachieving higher accuracy and lower computa-\ntional cost once the system is handling around\n7 tasks. Further, we show that through bi-\nnary quantization, we can reduce the size of\nthe extracted representations by a factor of\n16 to store them for later use. The resulting\nmethod offers a compelling solution for using\nlarge-scale pre-trained models at a fraction of\nthe computational cost when multiple tasks are\nperformed on the same text.\n1 Introduction\nLarge pre-trained language models achieve state-\nof-the-art performance on many Natural Language\nProcessing (NLP) tasks (Peters et al., 2018; Rad-\nford et al., 2018; Devlin et al., 2019). However,\ninference for these models requires signiﬁcant\ncomputational resources, which limits their prac-\ntical use. Recent trends show that scaling models\nup (Liu et al., 2019b; Lan et al., 2020; Raffel et al.,\n2019; Li et al., 2020) in terms of computation still\nimproves end task performance, raising questions\n∗Equal contribution.\nabout whether and how the most accurate models\ncan be applied in real-world settings.\nThis computational burden is exacerbated by the\nneed to ﬁne-tune end-to-end a separate model for\neach task. Since each model has a new set of pa-\nrameters, none of the computation can be shared by\nmodels for different tasks during inference. This\nis particularly inefﬁcient in real-world settings that\nrequire multiple predictions about each input. For\nexample, given a news article, we may want to pre-\ndict its topic (Zhang et al., 2015), sentiment (Pang\nand Lee, 2004; Maas et al., 2011; Socher et al.,\n2013; Zhang et al., 2015), overall text quality (Pitler\nand Nenkova, 2008), whether it is humorous (Yang\net al., 2015) or offensive (Schmidt and Wiegand,\n2017; Zampieri et al., 2019) and so on.\nKnowledge Distillation (KD) is a way of reduc-\ning the computation required by large pre-trained\nLMs (Hinton et al., 2015; Sanh et al., 2019). How-\never, there is a sizeable gap in accuracy between the\nbest models using knowledge distillation and the\nfull ﬁne-tuned models. Another way of speeding up\ncomputation is through system optimizations such\nas quantization and operator fusion (Zafrir et al.,\n2019). These techniques can reduce the amount\nof computation signiﬁcantly, but may not be sufﬁ-\ncient by themselves and can be combined with the\nmethods we discuss.\nIn this paper we look at new ways to make infer-\nence computationally efﬁcient focusing on the case\nwhere different models (models for different tasks)\nare run over the same piece of text. We propose\nnew methods to run multiple task-speciﬁc models\nin a way that amortizes the computation over the\ndifferent tasks. The central idea is to compute the\nactivations for the full model once and use smaller\ntask-speciﬁc models on top of it. We explore three\npossible ways for sharing computation.\nThe ﬁrst solution is inspired by work on gen-\neral purpose text encoders (Kiros et al., 2015; Hill\n3019\n!\n❄\ntask i\nsingle-task encoder\nfeature extractor\n!\n!\ntask 1\nmulti-task encoder\nfeature extractor\n!\n!\ntask k-1...\nmulti-task encoder\ntask k\nfeature extractor (     or     )\nclassiﬁcation   \nhead i\n !\nclassif.  \n  head 1\n! classif.  \nhead k-1\n!\n... classiﬁcation   \nhead k\n !\n❄\n❄\n !\n(a) Single-task ﬁnetuning.\n!\n❄\ntask i\nsingle-task encoder\nfeature extractor\n!\n!\ntask 1\nmulti-task encoder\nfeature extractor\n!\n!\ntask k-1...\nmulti-task encoder\ntask k\nfeature extractor (     or     )\nclassiﬁcation   \nhead i\n !\nclassif.  \n  head 1\n! classif.  \nhead k-1\n!\n... classiﬁcation   \nhead k\n !\n❄\n❄\n ! (b) Multi-task pre-training.\n!\n❄\ntask i\nsingle-task encoder\nfeature extractor\n!\n!\ntask 1\nmulti-task encoder\nfeature extractor\n!\n!\ntask k-1...\nmulti-task encoder\ntask k\nfeature extractor (     or     )\nclassiﬁcation   \nhead i\n !\nclassif.  \n  head 1\n! classif.  \nhead k-1\n!\n... classiﬁcation   \nhead k\n !\n❄\n❄\n ! (c) Leave-one-task-out ﬁnetuning.\nFigure 1: An illustration of the ﬁnetuning approaches explored in this work. (a) In single-task ﬁnetuning , an\nencoder model is ﬁne-tuned end-to-end for a given task. (b) In multi-task pre-training, an encoder model is jointly\ntrained over k−1 tasks, each with their own classiﬁcation head. (c) In leave-one-task-out ﬁnetuning, a multi-task\nencoder is frozen and used to extract features for an unseen ( kth) task. Following Peters et al. (2019), we use\nand\n to denote components that are ﬁne-tuned for each task or frozen, respectively.\net al., 2016; Conneau et al., 2017; Subramanian\net al., 2018), which produce ﬁxed-size representa-\ntions (i.e., sentence embeddings) that can be shared\nacross tasks. We add only small task-speciﬁc layers\non top of these ﬁxed-size representations. Unfor-\ntunately, when evaluated on unseen tasks, we ﬁnd\nthat models that rely on ﬁxed-size representations\noften underperform single-task baselines by a large\nmargin, in agreement with past work (Subramanian\net al., 2018; Peters et al., 2019; Raffel et al., 2019;\nWang et al., 2019a).\nThe second solution is a multi-task system (Caru-\nana, 1997; Collobert and Weston, 2008; Ruder,\n2017), where a single model is jointly trained to\nhandle many tasks (see Figure 1b). If most layers\nare shared, the overall inference cost can be nearly\nk times less than for k separate single-task mod-\nels, while providing competitive task accuracy (Liu\net al., 2019a; Raffel et al., 2019; Wang et al., 2019a).\nHowever, multi-task systems work best when the\nset of tasks is known in advance. Adding new\ntasks requires retraining the multi-task model and\nre-incurring training costs, thus limiting the utility\nof this approach in real-world systems where new\nclassiﬁcation tasks may be introduced periodically.\nWe propose a third solution: a multi-task en-\ncoder that is shared across tasks and produces\nlimited-size representations that grow with the\nlength of the input, similar to contextualized word\nrepresentations (Peters et al., 2018). We evaluate\nour representations on 14 text classiﬁcation tasks\nusing a leave-one-task-out evaluation protocol (see\nFigure 1c), where a multi-task encoder model is\ntrained on k−1 tasks, frozen and used as a static\nfeature extractor for an unseen kth task.1 We ﬁnd\nan important ingredient to performing well on an\nunseen (kth) task is to extract features from multi-\nple layers and positions of the encoder. Ultimately,\nour general purpose encoders offer a better tradeoff\nbetween task accuracy and inference cost than ei-\nther ﬁxed-size representations or distilled models,\nwhile requiring minimal additional computation to\nhandle new tasks.\nWe also consider the case in which not all of the\npredictions can be done at the same time and inter-\nmediate representations have to saved. In that con-\ntext, we study the relationship between representa-\ntion size and end-task performance. We ﬁnd that\nfeatures extracted by our encoders are amenable\nto heavy quantization enabling a 16x reduction in\nthe size of the extracted features with negligible\nimpact on unseen task performance.\n2 Related Work\nSelf-supervised pre-training , typically through\nlanguage modeling, has advanced the state of the\nart for many NLP tasks (Peters et al., 2018; Rad-\nford et al., 2018; Devlin et al., 2019). There are\ntwo dominant ways of adapting pre-trained models\nto downstream tasks: (1) ﬁnetuning, which often\nresults in the best accuracy (Devlin et al., 2019);\nand (2) feature extraction, which can be signiﬁ-\ncantly more efﬁcient during inference when there\nare multiple end tasks. Peters et al. (2019) com-\npare these and ﬁnd ﬁnetuning outperforms feature\nextraction for BERT; however, they use features\nimmediately after pre-training, whereas we also\nconsider features after multi-task ﬁnetuning.\n1We consider a task to be synonymous with a dataset.\n3020\nMulti-task learning (MTL) has a rich history\nin machine learning (Caruana, 1997; Ruder, 2017)\nand NLP (Collobert and Weston, 2008; Luong et al.,\n2016). Multi-task models can potentially lever-\nage similarities across tasks to achieve higher end-\ntask accuracy than single-task models (Clark et al.,\n2019; Liu et al., 2019a; Phang et al., 2018; Wang\net al., 2019a). Compared to single-task models, a\nmulti-task model can also be more efﬁcient dur-\ning inference by sharing computation across tasks.\nMost work in multi-task learning assumes that the\nset of end-tasks is ﬁxed and known in advance and\ntraining is performed for all tasks together. This set-\nup can present challenges in the real world where\ntasks may require different retraining schedules and\nnew tasks may be frequently added or removed.\nGeneral purpose text encoders are usually\npre-trained with a mix of supervised and self-\nsupervised training objectives and produce ﬁxed-\nsize representations (Kiros et al., 2015; Hill et al.,\n2016; Conneau et al., 2017; Subramanian et al.,\n2018). Unlike multi-task learning, general purpose\ntext encoders are typically evaluated on unseen\ntasks, which is more representative of real-world\nsettings in which new tasks may be added peri-\nodically. Unfortunately, these approaches often\nunderperform single-task baselines (McCann et al.,\n2018; Liu et al., 2019a; Wang et al., 2019a).\nAnother line of work has explored adapting pre-\ntrained models by adding additional task-speciﬁc\ncapacity at each layer (Houlsby et al., 2019), how-\never these methods do not improve inference efﬁ-\nciency since there is no task-independent computa-\ntion that can be shared across tasks.\nKnowledge Distillation (Buciluˇa et al., 2006;\nHinton et al., 2015) is a technique where a more\nefﬁcient student model is trained to mimic the be-\nhaviour of a larger or ensembled teacher model. A\nknowledge distilled version of BERT (Sanh et al.,\n2019) has been proposed to reduce the computa-\ntion required by large pre-trained language models.\nDistilRoBERTa reaches 95% of RoBERTa-base’s\nperformance on GLUE while being twice faster.\nQuantization and other compression techniques\nhave been explored for word embeddings (Shu\nand Nakayama, 2017; Tissier et al., 2019) and\nsentence embeddings (Shen et al., 2019). Recent\nwork has also explored quantization for contex-\ntualized word representations, generally showing\nthat quantization-aware training is necessary to\nachieve reasonable end task performance (Zafrir\ntask type # train # dev # label\nMNLI NLI 393K 20K 3\nQNLI NLI 105K 5.4K 2\nQQP PP 364K 391K 2\nRTE NLI 2.5K 3K 2\nSST-2 SA 17K 1.8K 2\nMRPC PP 3.7K 1.7K 2\nCoLA LA 8.5K 1K 2\nAG-news DOC 120K 7.6K 4\nAmazon-5 SA 3M 650K 5\nAmazon-2 SA 3.6M 400K 2\nYelp-5 SA 650K 50K 5\nYelp-2 SA 560K 38K 2\nDBpedia DOC 560K 70K 14\nTable 1: Task statistics.\net al., 2019; Fan et al., 2020). Quantization is com-\nplementary to the approaches we consider and is\nexplored more in Section 5.\n3 Experimental Setup\nOur goal is to develop text encoders that produce\nrepresentations which achieve high accuracy for\nmultiple task with little task-speciﬁc processing.\nWe ﬁrst introduce our tasks, encoder models and\nﬁnetuning framework.\n3.1 Tasks\nWe consider 14 text classiﬁcation tasks, spanning\nsentiment analysis (SA), natural language inference\n(NLI ), paraphrase identiﬁcation (PP), document cat-\negorization (DOC ) and linguistic acceptability (LA).\nTasks are chosen for their diversity and usage in\nrecent related work, ensuring that our baselines are\nrepresentative of the state of the art.\nDetails about each task are given in Table 1. The\nSA, DOC and LA tasks consist of making predic-\ntions about a single text input, while NLI and PP\ntasks require classifying a pair of text inputs. For\npair tasks we concatenate the text with a special\nseparator token following Liu et al. (2019b). Since\nmany of our tasks are part of evaluation bench-\nmarks such as GLUE (Wang et al., 2019b) and\nthe test sets are not publicly available, we report\naccuracy on the corresponding development sets.\n3.2 Encoder models\nOur encoder models are based on RoBERTa (Liu\net al., 2019b), an optimized version of BERT (De-\nvlin et al., 2019) that achieves competitive\nperformance on most of the tasks considered\nin this work. We primarily use the public\n3021\nRoBERTaLARGE model consisting of 24 Trans-\nformer layers (Vaswani et al., 2017), 1024 dimen-\nsional representations and 355M parameters. We\nrefer the reader to Devlin et al. (2019) for more\ndetails about the BERT architecture and Liu et al.\n(2019b) for more details about RoBERTa.\nWe also consider a Knowledge Distilled (KD)\nversion of RoBERTa called DistilRoBERTa (Sanh\net al., 2019), which consists of 6 Transformer lay-\ners, 768-dim representations and 82M parameters.\nThe distilled model contains 1/4 as many parame-\nters and requires 1/7 as much computation (FLOPs)\nas the full model. We present a more detailed com-\nparison of the computational requirements for these\nencoder models in Section 6.5.\n3.3 Finetuning\nWe consider two methods for ﬁnetuning encoder\nmodels, illustrated in Figure 1. Finetuning hyper-\nparameters and other methodological details are\ngiven in the Appendix.\n3.3.1 Single-task ﬁnetuning\nSingle-task ﬁnetuning is the most common way of\nadapting pre-trained language models to a given\ntask (see Figure 1a). When applied to large pre-\ntrained models (e.g., RoBERTa) it often results\nin the best end-task accuracy, but requires the\nfull model to be run for every task and thus has\nthe highest inference costs for a set of k tasks.\nComputation can be reduced by using a smaller\npre-trained models—including knowledge distilled\nmodels (e.g., DistilRoBERTa).\nSingle-task ﬁnetuning serves as an upper bound.\nOur goal is to achieve similar accuracy as large\nsingle-task models with reduced inference costs.\n3.3.2 Leave-one-task-out ﬁnetuning\nWe also consider leave-one-task-out ﬁnetuning, il-\nlustrated in Figures 1b and 1c. We pre-train a multi-\ntask encoder on k−1 tasks and extract frozen fea-\ntures for a kth task. Freezing the encoder allows\nus to amortize the inference cost over tasks. The\nleave-one-task-out setup allows us to evaluate gen-\neralization on tasks unseen in the training of the\nencoder. This replicates the real-world setting of\nadding new tasks to an existing frozen encoder.\nLeave-one-task-out ﬁnetuning has two stages:\n1. Multi-task pre-training: We train a single\nmodel end-to-end over k −1 tasks (Figure 1b).\nThe majority of the encoder weights are shared\nacross tasks, except for a classiﬁcation head (see\nSection 3.4) that is unique to each task.\nIt is important for the multi-task model to prop-\nerly weight different tasks, so that larger tasks do\nnot dominate smaller ones (Raffel et al., 2019;\nWang et al., 2019a). We adopt a loss-reweighting\ntechnique inspired by Raffel et al. (2019). At each\nstep, we sample a batch of data for every task and\nupdate our model according to a sum of the losses,\nweighted by: αi = D( 1\nT )\ni /∑\nj D( 1\nT )\nj , where Di\nis the number of training examples for task iand\nT is a temperature controlling weight uniformity.\nWhen T = 1, task weights are proportional to data\nsize, and as T →0, task weights become uniform.\nWe use a ﬁxed temperature of T = 0.1, which\nperformed best in early experiments.\n2. Leave-one-task-out ﬁnetuning: In the sec-\nond stage, we freeze the multi-task encoder’s\nweights and use it as a feature extractor for an\nunseen kth task (see Figure 1c). The extracted fea-\ntures are fed to a new, randomly initialized classiﬁ-\ncation head, which is ﬁne-tuned over the training\ndata for the kth task. We repeat this process k\ntimes, with each task held out once, and report the\ncorresponding held-out task performance.\n3.4 Classiﬁcation heads\nEach task has a classiﬁcation head that takes fea-\ntures as input and makes a prediction. While related\nwork uses task-speciﬁc classiﬁcation layers (Peters\net al., 2018, 2019; Liu et al., 2019a), we adopt a\nuniﬁed architecture for all tasks. We follow the\noriginal BERT setup (Devlin et al., 2019) and use\na two-layer Multi-Layer Perceptron (MLP) with\ninner dimension equal to the pooled feature dimen-\nsion and a tanh activation function. The classiﬁ-\ncation head is always ﬁne-tuned for the end task.\n4 Feature extraction and pooling\nA common way to extract features from BERT-\nlike models is to take the representation in the\nlast Transformer layer corresponding to a special\nCLS token prepended to the input sequence (Devlin\net al., 2019). Recent work has also explored ex-\ntracting features from every position and layer, then\nlinearly combining the layers with task-speciﬁc\nweights (Peters et al., 2019; Tenney et al., 2019).\nWe propose a more general framework for ex-\ntracting features, shown in Figure 2. We extract\nfeatures from several layers of the encoder and\n3022\n!\n❄\nCLS ...w1 wn\n...\n...\n...\n...\n...\n...\n... ...\nclassiﬁcation head\nlayer-wise\npooler\n...\nposition-wise\npooler\nFigure 2: Features are extracted from multiple encoder\nlayers, pooled across layers, then positions, and ﬁnally\npassed to a task-speciﬁc classiﬁcation head. Some fea-\nture extraction and pooling approaches have additional\ntask-speciﬁc parameters that require ﬁnetuning.\n!\n❄\nK, V\ntask-speciﬁc   \nquery\n !\nclassiﬁcation   \nhead\n !\nmulti-head  \nattention\n !\nQ\n...\n❄\n ❄\n ❄\nFigure 3: Our proposed multi-head attention pooler.\nThe extracted features are frozen (\n ) and used as both\nthe keys ( K) and values ( V). Each task has its own\nquery (Q), multi-head attention module and classiﬁca-\ntion head, all of which are ﬁne-tuned (\n ).\nthen pool them, ﬁrst across layers and then across\npositions, before feeding them to a task-speciﬁc\nclassiﬁcation head. This framework subsumes both\nthe CLS token and weighted layer combination ap-\nproaches. We consider several ways of layer-wise\npooling and position-wise pooling:\nLayer-wise pooling approaches:\n•LAST -LAYER : only use the last layer. This set-\nting is used by Devlin et al. (2019).\n•LAYER -AVG: average the last mlayers. We tune\nmfor each setting, but ﬁnd that m= 16works\nbest in most cases.\n•LEARNED -COMB : learn a task-speciﬁc weighted\ncombination over all layers. This setting is used\nby Peters et al. (2019) and Tenney et al. (2019).\nPosition-wise pooling approaches:\n•CLS : extract features from the ﬁrst position. This\nsetting is used by Devlin et al. (2019).\n•POSITION -AVG: average features across posi-\ntions.\nlayer-wise position-wise quantization\npooling pooling fp16 int8 bool\nLAST-LAYER\nor\nLAYER-AVG\nCLSor 2K 1K 128POSITION-AVG\nMHA 100K 50K 6K\nLEARNED-COMB MHA 2.3M 1.2M 150K\nTable 2: Estimated storage cost (in bytes) to store fea-\ntures for a 50 token input.\n•MHA : pool features with a task-speciﬁc Multi-\nHead Attention (MHA) layer (Devlin et al.,\n2019). We learn a task-speciﬁc query and use\nfeatures as the keys and values (see Figure 3).\n5 Storage Considerations and\nQuantization\nIn a real-world settings it may be necessary to store\nextracted features for later use, such as when new\ntasks are introduced that require “backﬁlling” clas-\nsiﬁcations for older content (Shen et al., 2020).\nStorage costs quickly become impractical when\npooling over multiple hidden layers and positions,\nwith some approaches (Section 4) requiring fea-\ntures from every layer and position in the encoder.\nFor RoBERTaLARGE , with 24 layers and 1024 di-\nmension representations, a 50 token input would\nthus emit 50*24*1024 half-precision ﬂoating\npoint numbers and require 2.3MB of storage!\nWe consider quantization methods, described be-\nlow, for reducing the storage of extracted features.\nWith quantization, we replace ﬂoating point num-\nbers with alternative representation formats that\nhave reduced bit width. We will show in Section 6\nthat extracted features are surprisingly robust: they\nshow little degredation in end-task accuracy even\nwith binary quantization. Recent work has made\nsimilar observations in the context of 8-bit integer\nquantization for BERT model weights and activa-\ntions (Zafrir et al., 2019).\nWe explore both 8-bit (uint8) and 1-bit (boolean)\nquantization of extracted features (see Table 2). We\napply quantization prior to leave-one-task-out ﬁne-\ntuning (Section 3.3.2) to simulate a real-world set-\nting in which only quantized features are available.\nFor 8-bit quantization, we use PyTorch (Paszke\net al., 2019) to learn scale and zero-point parame-\nters which map ﬂoating point numbers to the range\n0-255. For 1-bit quantization, we apply the sign\nfunction to binarize each feature dimension.\n3023\n6 Results and Discussion\nTable 3 presents our main results for the 14 tasks\nintroduced in Section 3.1. Detailed results of all\ntasks are included in Table 5 in the Appendix.\n6.1 Baselines\nTable 3 (a) shows results for models ﬁne-tuned end-\nto-end on a single task. This approach yields the\nbest end-task accuracy but has the highest inference\ncosts (see discussion in Section 3.3.1).\nWe observe that DistilRoBERTa achieves\ncompetitive accuracy across many tasks with only\n1/4 as many parameters and 1/7 of the computa-\ntion of the full RoBERTa model. Multi-task pre-\ntraining (see Section 3.3.2) prior to single-task ﬁne-\ntuning improves results with an average gain of\n+0.2%. This is consistent with recent work (Liu\net al., 2019a; Wang et al., 2019a), but somewhat at\nodds with the ﬁndings of Raffel et al. (2019), who\nreport slightly worse performance with multi-task\npre-training. It remains an open question under\nwhat conditions multi-task pre-training improves\nend task accuracy for single-task models.\n6.2 Feature extraction and pooling\n6.2.1 Without multi-task pre-training\nTable 3 (b) shows results for single-task models\nwith a frozen encoder and ﬁne-tuned classiﬁcation\nhead. We observe that freezing the pre-trained\nRoBERTa model and extracting features from the\nlast layer’sCLS token performs poorly, with a 15%\ndrop in accuracy compared to the end-to-end ﬁne-\ntuned version (90.5% →75.5%). This is expected,\nsince the CLS token is not heavily used in the\nRoBERTa pre-training process (Liu et al., 2019b).2\nIf we instead average features across positions in\nthe last layer, we see slightly higher accuracy com-\npared to the CLS token alone (77.7% vs. 75.5%),\nwhile our multi-head attention (MHA) pooling fur-\nther improves accuracy to 83.3%, conﬁrming the\nimportance of task-speciﬁc position-wise pooling.\nWe next consider different layer-wise pooling\nstrategies, still using the MHA position-wise pool-\ning. Taking a simple average over the top 16 layers\nimproves accuracy by +2.2% compared to using\njust the last layer (85.5% vs. 83.3%). If we in-\nstead learn a task-speciﬁc weighted combination\nof layers, similar to Peters et al. (2019), we gain an\n2RoBERTa does not pre-train with a Next Sentence Predic-\ntion (NSP) objective, thus the CLS token is mostly unused.\nadditional +0.1% compared to using a simple av-\nerage. However, using a task-speciﬁc combination\nof layers introduces signiﬁcant storage costs (see\nTable 2), thus we focus on the LAYER -AVG pooling\napproach in the rest of our experiments.\n6.2.2 With multi-task pre-training\nTable 3 (c) presents results for leave-one-task-out\nmulti-task pre-training (Section 3.3.2), in which the\nencoder is ﬁne-tuned on k−1 tasks, then frozen.\nIn this setting, the last layer’sCLS token now en-\ncodes general task information, achieving a higher\naverage accuracy than any of the frozen encoders\nwhich did not have leave-one-task-out multi-task\npre-training (85.9% vs. 85.6%). As before, our\nmulti-head attention (MHA) position-wise pool-\ning strategy performs best, outperforming the CLS\napproach by +0.9% and the POSITION -AVG strat-\negy by +0.8%. Layer-wise pooling across multiple\nlayers provides an additional 1.6-1.7% gain.\n6.3 Quantization\nTable 3 (c) also shows the effect of feature quanti-\nzation on task accuracy. We quantize extracted\nfeatures after leave-one-task-out multi-task pre-\ntraining and use LAYER -AVG / MHA pooling, which\noffers the best balance between storage efﬁciency\nand accuracy. In early experiments, we considered\nwhether to quantize before or after layer-wise pool-\ning and found that quantization before layer-wise\npooling was slightly better for 1-bit quantization\nand had no impact on 8-bit quantization.\nWe observe no performance loss with 8-bit\nquantization. Surprisingly, 1-bit quantization\nonly reduces accuracy by 0.4%, still outperform-\ning distillation-based methods (88.0% vs. 87.1%),\nwhile reducing storage costs by a factor of 16 (to\n1024 bits per token; see Table 2).\nTo understand why quantization works so well,\nwe use a word-sense disambiguation (WSD) task to\nprobe if semantic information encoded in the origi-\nnal and quantized features is preserved. Following\nPeters et al. (2018), we apply a nearest neighbor\nclassiﬁer over word sense centroids, obtained by\naveraging features for each word sense over train-\ning data. We use the data and splits from Reif et al.\n(2019). We extract features from the 16th layer\nof the multi-task RoBERTa encoder (Table 3 (e)),\nwhich performed best in pilot experiments, and\ncompare them before and after 1-bit quantization.\nThe F1 scores shown in Table 4 show that\nRoBERTa features achieve similar results before\n3024\nmodel G FLOPs SA NLI PP DOC LA AVERAGE\n(a) Single-task ﬁnetuning (end-to-end):\nBERT - 86.8 83.1 89.7 97.1 83.1 ∗ 87.6\nXLNet - 87.6 89.2 90.5 97.4 84.5∗ 89.5\nDistilRoBERTa 61 86.6 80.7 89.6 97.1 84.3 87.1\nRoBERTa 430 88.2 91.3 91.8 97.4 86.3 90.5\n+ leave-one-task-out multi-task 430 88.2 91.6 92.1 97.4 87.2 90.7\npre-training\n(b) Single-task ﬁnetuning (frozen encoder):\nRoBERTa\n+ LAST -LAYER / CLS 31 80.8 58.8 68.2 94.9 69.1 75.5\n+ LAST -LAYER / POSITION -AVG 31 80.3 63.0 75.6 95.0 75.0 77.7\n+ LAST -LAYER / MHA 34 86.1 72.7 79.0 96.7 80.2 83.3\n+ LAYER -AVG / MHA 34 86.9 77.7 83.0 96.9 82.7 85.5\n+ LEARNED -COMB / MHA 34 87.0 78.0 82.8 96.9 82.5 85.6\n(c) Leave-one-task-out ﬁnetuning (frozen multi-task encoder):\nRoBERTa\n+ LAST -LAYER / CLS 31 87.4 82.8 81.8 94.9 76.4 85.9\n+ LAST -LAYER / POSITION -AVG 31 87.4 83.0 81.9 95.1 77.1 86.0\n+ LAST -LAYER / MHA 34 87.5 84.6 83.5 96.2 77.2 86.8\n+ LAYER -AVG / MHA 34 87.9 87.8 85.7 96.8 82.4 88.4\n+ LEARNED -COMB / MHA 34 87.9 87.9 85.7 96.9 82.3 88.5\nRoBERTa (8-bit quantization)\n+ LAYER -AVG / MHA 34 87.9 87.7 85.7 96.8 82.6 88.4\nRoBERTa (1-bit quantization)\n+ LAYER -AVG / MHA 34 87.8 87.1 84.6 96.6 81.3 88.0\n(d) Leave-one-task-group-out ﬁnetuning (frozen multi-task encoder):\nRoBERTa\n+ LAYER -AVG / MHA 31 87.0 81.3 85.3 96.7 82.4 86.6\n(e) Multi-task pre-training over all tasks (frozen multi-task encoder; no additional ﬁnetuning):\nRoBERTa\n+ LAST -LAYER / CLS 31 87.7 89.6 89.3 97.2 82.6 89.3\nTable 3: Results on 14 tasks, grouped by task type (see Section 3.1). We consider different layer-wise and position-\nwise pooling strategies introduced in Section 4. We also report the estimated inference cost for 14 tasks (in G\nFLOPs) for each strategy. Bold results indicate the most accurate method in each section. BERT results are from\nYang et al. (2019) and Sun et al. (2019). XLNet results are from Yang et al. (2019). DistilRoBERTa and RoBERTa\nresults are recomputed ourselves. Full results for each task is given in the Appendix. (*) we recomputed accuracy\nfor CoLA, since BERT and XLNet originally reported a different metric.\n3025\noriginal 1-bit quant.\nbaseline (most freq. sense) 64.8 -\nBERT (Reif et al., 2019) 71.1 -\nRoBERTa 71.2 71.1\nTable 4: WSD results (F1 score) for original and 1-bit\nquantized features.\nand after 1-bit quantization. Both results are com-\nparable to those from Reif et al. (2019), conﬁrming\nthat 1-bit quantization at least preserves word sense\ninformation in the extracted features.\n6.4 Generalization\nWe used the leave-one-task-out setting to evalu-\nate generalization to unseen tasks. We now con-\nsider the case where an entire task type (see Sec-\ntion 3.1) is held out during multi-task pre-training.\nFor example, we pre-train an encoder over non-\nNLI tasks and evaluate the frozen features on NLI\ntasks. Results presented in the fourth section (d) of\nTable 3 show that performance drops considerably\nfrom the corresponding leave-one-task-out setting\n(Table 3 (c)). Average accuracy decreases from\n88.4% to 86.6%. Accuracy on NLI tasks decreases\nthe most from 87.8% to 81.3%, consistent with\npast work showing positive transfer between NLI\ntasks (Phang et al., 2018). Thus, it is important to\npre-train the encoder over a variety of task types to\nmaximize generalization to new tasks.\nAnother alternative to leaving tasks out is to pre-\ntrain the encoder over all k tasks and evaluate it\non each task without additional ﬁnetuning. This\nsetting is useful when the set of all tasks is known\nin advance and does not change. Results (in the\nﬁnal section of Table 3 (e)) show that when models\nare part of the multi-task ﬁnetuning they perform\n3.4% better on average as opposed to when they\nare held out (89.3% vs. 85.9%).\n6.5 Computational cost during inference\nTable 3 reports cumulative inference cost (over\n14 tasks) for each method. Single-task ﬁnetun-\ning is the most accurate and the least efﬁcient ap-\nproach. Approaches using knowledge distillation\nand frozen encoders reduce FLOPs by an order of\nmagnitude.\nFigure 4 shows the number of FLOPs required\nfor inference as a function of the number of tasks\nperformed on the same text. While single-task ﬁne-\ntuning of the full model is never efﬁcient, distilled\n5 10 15 20\n# of tasks (T)\n0\n20\n40\n60\n80\n100\n120\n140\n160G FLOPs\nsingle-task ﬁnetuning (end-to-end)\nfrozen encoder + MHA\nfrozen encoder + CLS / POSITION -AVG\nDistilRoBERTa\nFigure 4: Estimated computational cost (in FLOPs) to\nrun RoBERTa inference forT tasks over a single input.\nThe cost for single-task models grows linearly with the\nnumber of tasks, whereas approaches based on a frozen\nencoder are much more efﬁcient. Distilled models are\nparticularly efﬁcient when the number of tasks is small,\nbut the cost scales linearly and becomes less efﬁcient\nthan a frozen encoder when the number of tasksT >7.\nmodels are the most efﬁcient for systems with 7 or\nfewer tasks. Frozen encoder approaches become\nthe most efﬁcient option when more than 7 tasks\nare prerformed on the same piece of text.\n7 Conclusion\nWe study how to improve the efﬁciency of large-\nscale pre-trained models so that can be used in\npractical settings. We show that when several tasks\nare performed on a single piece of text, the com-\nputation can be effectively amortized reducing the\namount of computation per task. Compared to dis-\ntillation, the shared computation method achieves\nhigher accuracy and reduces computational cost af-\nter 7 tasks need to be performed on the same piece\nof text. We show that the shared features can be\nquantized with very little loss in accuracy, which\nmeans that the intermediate computation can be\nstored for later use. In total, the techniques that\nwe present provide a compelling solution for run-\nning large-scale pre-trained models in applications\nwhere multiple predictions are made on the same\npiece of text.\nReferences\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the 12th ACM SIGKDD international\n3026\nconference on Knowledge discovery and data min-\ning, pages 535–541. ACM.\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D Manning, and Quoc V Le.\n2019. Bam! born-again multi-task networks for\nnatural language understanding. arXiv preprint\narXiv:1907.04829.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on Ma-\nchine learning, pages 160–167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL).\nAngela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, Remi Gribonval, Herve Jegou, and Armand\nJoulin. 2020. Training with quantization noise for\nextreme model compression.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\n2016. Learning distributed representations of sen-\ntences from unlabelled data. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1367–1377, San\nDiego, California. Association for Computational\nLinguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for\nnlp. International Conference on Machine Learning\n(ICML).\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems ,\npages 3294–3302.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. arXiv preprint arXiv:2002.11794.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint\narXiv:1901.11504.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2016. Multi-task se-\nquence to sequence learning. In International Con-\nference on Learning Representations (ICLR).\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. FAIRSEQ : A fast, extensible\ntoolkit for sequence modeling. In North American\nAssociation for Computational Linguistics (NAACL):\nSystem Demonstrations.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of\nthe 42nd annual meeting on Association for Compu-\ntational Linguistics, page 271. Association for Com-\nputational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems (NIPS), pages 8024–8035.\n3027\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In North American Association for Com-\nputational Linguistics (NAACL).\nMatthew Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. arXiv preprint\narXiv:1903.05987.\nJason Phang, Thibault Fvry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nEmily Pitler and Ani Nenkova. 2008. Revisiting read-\nability: A uniﬁed framework for predicting text qual-\nity. In Proceedings of the conference on empirical\nmethods in natural language processing, pages 186–\n195. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding with unsupervised learning. Technical\nreport, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems, pages 8592–8600.\nSebastian Ruder. 2017. An overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled ver-\nsion of bert: smaller, faster, cheaper and lighter. In\nNeurIPS EMC2 Workshop.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the Fifth International\nWorkshop on Natural Language Processing for So-\ncial Media, pages 1–10.\nDinghan Shen, Pengyu Cheng, Dhanasekar Sundarara-\nman, Xinyuan Zhang, Qian Yang, Meng Tang, Asli\nCelikyilmaz, and Lawrence Carin. 2019. Learning\ncompressed sentence representations for on-device\ntext processing. arXiv preprint arXiv:1906.08340.\nYantao Shen, Yuanjun Xiong, Wei Xia, and\nStefano Soatto. 2020. Towards backward-\ncompatible representation learning. arXiv preprint\narXiv:2003.11942.\nRaphael Shu and Hideki Nakayama. 2017. Compress-\ning word embeddings via deep compositional code\nlearning. arXiv preprint arXiv:1711.01068.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nSandeep Subramanian, Adam Trischler, Yoshua Ben-\ngio, and Christopher J Pal. 2018. Learning gen-\neral purpose distributed sentence representations via\nlarge scale multi-task learning. In International\nConference on Learning Representations.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\narXiv preprint arXiv:1905.05583.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nJulien Tissier, Christophe Gravier, and Amaury\nHabrard. 2019. Near-lossless binarization of word\nembeddings. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence , volume 33, pages\n7104–7111.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pap-\npagari, R Thomas McCoy, Roma Patel, Najoung\nKim, Ian Tenney, Yinghui Huang, Katherin Yu, et al.\n2019a. Can you tell me how to get past sesame\nstreet? sentence-level pretraining beyond language\nmodeling. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4465–4476.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nDiyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy.\n2015. Humor recognition and humor anchor extrac-\ntion. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2367–2376.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\n3028\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n5th Workshop on Energy Efﬁcient Machine Learning\nand Cognitive Computing.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. Semeval-2019 task 6: Identifying and catego-\nrizing offensive language in social media (offense-\nval). In Proceedings of the 13th International Work-\nshop on Semantic Evaluation, pages 75–86.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems (NIPS), pages 649–657.\n3029\nA Finetuning Methodology\nWe largely adopt the ﬁnetuning procedure and hy-\nperparameters from Liu et al. (2019b). We use\nthe Adam optimizer (Kingma and Ba, 2015) with\nβ1 = 0.9,β2 = 0.98,ϵ = 1e−6. We search\nover learning rates ∈{1,2,3}e-5 and batch sizes ∈\n{16,32}for each task. We ﬁnetune for 10 epochs3\nand report the best dev set accuracy for each task,\nwhich we measure at epoch boundaries. We lin-\nearly warm up the learning rate for the ﬁrst 6% of\nﬁnetuning updates and linearly decay the rate to 0\nfor the remaining updates. We apply dropout with\np= 0.1 and ﬁnetune with weight decay of 0.01.\nFor ﬁnetuning the multi-task encoders, we use\na learning rate of 1e-5 and batches consisting of\n4 samples from each task. For example, a leave-\none-task-out encoder for MNLI would be ﬁnetuned\non batches containing 4 samples from each of the\n(13) non-MNLI tasks, for a total batch size of 52.\nAs described in Section 3.3.1, the task losses are\nweighted with a mixing temperature α= 0.1 and\nsummed, following Raffel et al. (2019).\nWe perform our experiments using the FAIRSEQ\ntoolkit (Ott et al., 2019), PyTorch (Paszke et al.,\n2019) and Nvidia V100 GPUs. We train with\nmixed precision, following Liu et al. (2019b).\nB Detailed Results\nThe results for all setups over 14 tasks can be found\nin Table 5.\n3Due to the large size of the training sets, we ﬁnetune for\nonly a single epoch for the Amazon-{2,5} data, and for only\nﬁve epochs for the Yelp-{2,5} and DBpedia data.\n3030\nModel MNLI QNLI QQP RTE SST2 MRPC CoLA IMDB AG Amzn5 Amzn2 Yelp5 Yelp2 DBpd Avg\n(a)Single-task ﬁnetuning (end-to-end):\nBERT 86.6 92.3 91.3 70.4 93.2 88.0 83.1 95.5 94.8 65.8 97.4 70.7 98.1 99.4 87.6XLNet 89.8 93.9 91.8 83.8 95.6 89.2 84.5 96.2 95.5 67.7 97.4 70.7 98.1 99.4 89.5DistilRoBERTa 83.9 91.0 91.2 67.2 93.5 88.0 84.3 94.4 94.9 66.3 97.1 70.5 97.9 99.3 87.1RoBERTa 90.3 94.6 92.3 88.9 96.7 91.3 86.3 96.4 95.4 67.9 97.6 71.9 98.4 99.3 90.5+ leave-one-task-out multi-task 90.3 94.6 92.2 89.9 96.7 92.1 87.2 96.6 95.5 68.0 97.6 72.2 98.4 99.3 90.7pre-training\n(b)Single-task ﬁnetuning (frozen encoder):\nRoBERTa+LAST-LAYER/CLS 55.7 67.4 67.4 53.3 84.0 69.0 69.1 89.1 91.0 58.2 94.6 62.7 96.2 98.7 75.5+LAST-LAYER/POSITION-AVG 56.7 72.7 80.3 59.7 88.1 70.9 75.0 87.2 91.5 57.3 93.7 60.6 95.0 98.4 77.7+LAST-LAYER/MHA 75.7 81.6 86.0 60.7 92.5 71.9 80.3 94.3 94.1 65.1 96.9 69.9 98.0 99.3 83.3+LAYER-AVG/MHA 83.1 87.3 88.1 62.8 94.3 77.9 82.7 95.5 94.4 65.9 97.2 70.6 98.2 99.3 85.5+LEARNED-COMB/MHA 83.4 87.4 88.1 63.1 94.4 77.4 82.5 95.5 94.5 66.0 97.2 70.8 98.2 99.3 85.6ALBERT (Lan et al., 2020)+LAST-LAYER/CLS 67.2 72.4 78.3 53.2 86.8 71.6 69.9 90.1 92.2 60.1 93.7 66.6 98.0 97.8 78.4+LAST-LAYER/POSITION-AVG 64.6 77.9 79.2 59.4 86.9 75.0 69.9 89.1 93.2 59.2 93.7 66.8 98.0 98.0 79.4+LAST-LAYER/MHA 82.8 88.8 86.7 64.2 91.6 75.5 83.3 95.5 94.5 65.5 97.0 70.0 98.2 99.3 85.3+LAYER-AVG/MHA 84.2 89.1 88.4 67.8 95.2 80.1 84.7 95.5 94.4 65.9 97.2 70.8 98.2 99.3 86.5+LEARNED-COMB/MHA 84.4 90.2 88.9 73.1 95.8 84.1 86.5 95.5 94.4 65.9 97.2 70.8 98.2 99.3 87.5\n(c)Leave-one-task-out ﬁnetuning (frozen multi-task encoder):\nRoBERTa+LAST-LAYER/CLS 76.2 84.3 84.7 87.8 94.0 78.8 76.4 96.7 90.7 66.4 97.6 71.2 98.7 99.1 85.9+LAST-LAYER/POSITION-AVG 76.3 84.6 84.7 88.2 93.8 79.1 77.1 96.7 91.1 66.3 97.6 71.0 98.7 99.0 86.0+LAST-LAYER/MHA 79.8 87.6 86.2 86.5 94.1 80.8 77.2 96.7 93.2 66.5 97.6 71.5 98.7 99.2 86.8+LAYER-AVG/MHA 86.6 91.6 88.7 85.1 96.0 82.7 82.4 96.7 94.4 66.8 97.6 71.7 98.7 99.3 88.4+LEARNED-COMB/MHA 87.0 91.7 88.8 85.1 96.1 82.7 82.3 96.7 94.4 66.8 97.6 71.8 98.7 99.3 88.5\nRoBERTa (8-bit quantization)+LAYER-AVG/MHA 86.7 91.5 88.6 85.1 96.0 82.7 82.6 96.7 94.4 66.7 97.6 71.6 98.7 99.3 88.4\nRoBERTa (1-bit quantization)+LAYER-AVG/MHA 85.5 90.8 88.2 85.1 95.8 81.0 81.3 96.5 93.9 66.6 97.6 71.4 98.7 99.3 88.0\n(d)Leave-one-task-group-out ﬁnetuning (frozen multi-task encoder):\nRoBERTa+LAYER-AVG/MHA 85.2 90.4 88.7 68.3 94.3 82.0 82.4 95.6 94.2 65.8 97.2 70.6 98.2 99.3 86.6\n(e)Multi-task pre-training over all tasks (frozen multi-task encoder; no additional ﬁnetuning):\nRoBERTa+LAST-LAYER/CLS 89.3 93.7 89.6 85.9 94.8 89.0 82.6 96.7 95.1 66.5 97.5 71.9 98.6 99.3 89.3\nTable 5: Extended results table for all 14 tasks. See Table 3 for more details.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8015063405036926
    },
    {
      "name": "Encoder",
      "score": 0.739871621131897
    },
    {
      "name": "Inference",
      "score": 0.7147907614707947
    },
    {
      "name": "Pooling",
      "score": 0.6948525309562683
    },
    {
      "name": "Scalability",
      "score": 0.6325575113296509
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6091822385787964
    },
    {
      "name": "Computation",
      "score": 0.5436529517173767
    },
    {
      "name": "Machine learning",
      "score": 0.5236408710479736
    },
    {
      "name": "Language model",
      "score": 0.5074617266654968
    },
    {
      "name": "Fraction (chemistry)",
      "score": 0.47368624806404114
    },
    {
      "name": "Natural language processing",
      "score": 0.4512766897678375
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.4297037124633789
    },
    {
      "name": "Binary classification",
      "score": 0.41966724395751953
    },
    {
      "name": "Algorithm",
      "score": 0.16984635591506958
    },
    {
      "name": "Support vector machine",
      "score": 0.11057093739509583
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}