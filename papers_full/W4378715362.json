{
    "title": "Improving the quality of chemical language model outcomes with atom-in-SMILES tokenization",
    "url": "https://openalex.org/W4378715362",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4221239930",
            "name": "Umit V. Ucak",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A3189066220",
            "name": "Islambek Ashyrmamatov",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2112848657",
            "name": "Juyong Lee",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A4221239930",
            "name": "Umit V. Ucak",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A3189066220",
            "name": "Islambek Ashyrmamatov",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2112848657",
            "name": "Juyong Lee",
            "affiliations": [
                "Seoul National University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4322100081",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W4233998925",
        "https://openalex.org/W1964981516",
        "https://openalex.org/W2066676451",
        "https://openalex.org/W4394665659",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2968297680",
        "https://openalex.org/W2989615256",
        "https://openalex.org/W2973074478",
        "https://openalex.org/W2172216479",
        "https://openalex.org/W2895677720",
        "https://openalex.org/W3009321976",
        "https://openalex.org/W3045928028",
        "https://openalex.org/W2170973067",
        "https://openalex.org/W2405035126",
        "https://openalex.org/W2113373093",
        "https://openalex.org/W3138781613",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2803462582",
        "https://openalex.org/W2066532920",
        "https://openalex.org/W2908927563",
        "https://openalex.org/W2006607885",
        "https://openalex.org/W2137052779",
        "https://openalex.org/W2153693853",
        "https://openalex.org/W2146453976",
        "https://openalex.org/W2541750647",
        "https://openalex.org/W2580919858",
        "https://openalex.org/W2799620402",
        "https://openalex.org/W2621742623",
        "https://openalex.org/W2972608805",
        "https://openalex.org/W3088265803",
        "https://openalex.org/W3094771832",
        "https://openalex.org/W3010145447",
        "https://openalex.org/W3120024000",
        "https://openalex.org/W4220670676",
        "https://openalex.org/W2324964582",
        "https://openalex.org/W2606363443",
        "https://openalex.org/W2064535969",
        "https://openalex.org/W29374554",
        "https://openalex.org/W6977367121",
        "https://openalex.org/W1988037271",
        "https://openalex.org/W4225764472",
        "https://openalex.org/W4205618586",
        "https://openalex.org/W3114326827",
        "https://openalex.org/W4220931862",
        "https://openalex.org/W4225000967",
        "https://openalex.org/W3175782816",
        "https://openalex.org/W3009202547",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W3109470806"
    ],
    "abstract": null,
    "full_text": "Ucak et al. Journal of Cheminformatics (2023) 15:55 \nhttps://doi.org/10.1186/s13321-023-00725-9\nRESEARCH Open Access\n© The Author(s) 2023, corrected publication 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 \nInternational License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver \n(http:// creat iveco mmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a \ncredit line to the data.\nJournal of Cheminformatics\nImproving the quality of chemical language \nmodel outcomes with atom-in-SMILES \ntokenization\nUmit V. Ucak1†, Islambek Ashyrmamatov2† and Juyong Lee3* \nAbstract \nTokenization is an important preprocessing step in natural language processing that may have a significant influ-\nence on prediction quality. This research showed that the traditional SMILES tokenization has a certain limitation that \nresults in tokens failing to reflect the true nature of molecules. To address this issue, we developed the atom-in-SMILES \ntokenization scheme that eliminates ambiguities in the generic nature of SMILES tokens. Our results in multiple \nchemical translation and molecular property prediction tasks demonstrate that proper tokenization has a signifi-\ncant impact on prediction quality. In terms of prediction accuracy and token degeneration, atom-in-SMILES is more \neffective method in generating higher-quality SMILES sequences from AI-based chemical models compared to other \ntokenization and representation schemes. We investigated the degrees of token degeneration of various schemes and \nanalyzed their adverse effects on prediction quality. Additionally, token-level repetitions were quantified, and gener-\nated examples were incorporated for qualitative examination. We believe that the atom-in-SMILES tokenization has a \ngreat potential to be adopted by broad related scientific communities, as it provides chemically accurate, tailor-made \ntokens for molecular property prediction, chemical translation, and molecular generative models.\nKeywords Atom-in-SMILES, Tokenization, Repetition, Chemical language processing\nIntroduction\nTokenization is an essential preprocessing step for \nsequential data to train and use natural language pro -\ncessing (NLP) models. However, insufficient attention \nhas been devoted to its effects on chemical applica -\ntions. Tokenization can significantly influence prediction \nquality within the framework of text generation  [1]. In \nthe field of chemistry, it covers processes used to split \nlinear molecular representations into their constituent \nelements. As linear molecular representations are algo -\nrithmic abstractions, their partitioning can alter the per -\nception of molecules. Herein, tokenization refers to any \nlogical partitioning of molecular structures based on \nSMILES strings.\nIn general, a molecule can be perceived as an inher -\nent whole, owing to the internal relationships among its \natomic components. Simplified Molecular Input Line \nEntry System (SMILES) strings [2], the most commonly \nused molecular representation, are also defined to be \nmeaningful as a whole. They represent molecular objects, \nwhich are rigid bodies and completely different from \ntheir constituent atoms. Notably, any sensible partition -\ning of a molecule will produce meaningful fragments. \n†Umit V. Ucak and Islambek Ashyrmamatov have contributed equally to this \nwork.\n*Correspondence:\nJuyong Lee\nnicole23@snu.ac.kr\n1 Department of Molecular Medicine and Biopharmaceutical Sciences, \nGraduate School of Convergence Science and Technology, Seoul National \nUniversity, Seoul, Republic of Korea\n2 College of Pharmacy, Seoul National University, Seoul, Republic of Korea\n3 Research Institute of Pharmaceutical Science, Seoul National University, \nSeoul, Republic of Korea\nPage 2 of 13Ucak et al. Journal of Cheminformatics (2023) 15:55\nHowever, to some extent, the abstraction level of the \nprocess will get tangled because considering atoms in a \nmolecule is not a “realistic” approach. Likewise, SMILES \ntokens are merely sensible linear cuts of a string with \nreduced dimensionality.\nA typical tokenization of SMILES is performed atom-\nwise, i.e., character-wise. However, the SMILES represen-\ntation consists of a small number of distinct characters \nincluding atomic symbols, integers for ring closure, and \nspecial symbols for bonds and chirality. In other words, \nall atoms with the same atomic number are represented \nidentically. However, the characteristics of each atom \neven with the same atomic number may differ signifi -\ncantly based on its environment. This statement is simi -\nlar to the concept of atoms in molecules (AIM) [3], which \ndescribes the nature of molecules based on the electron \ndensity distribution attracted by each atom. Thus, the \nconventional atom-wise tokenization of SMILES may \nbe too abstract and chemically inaccurate and it may \nobscure the learning process of a model and the under -\nstanding of the results.\nHerein, we point out an analogy between the natural \n(spoken) language and constructed language of chemistry \n(see Table 1). The analogy provides a motivational ground \nfor the use of NLP methods in chemistry problems  [4]. \nThe intended analogy relies on the part-whole relation -\nship   [5, 6] and suggests that molecular substructures \n(typically composed of several atoms) can be considered \nas chemical “words” for the linguistic treatment of chem -\nical language. However, in practice, chemical words often \nbecome the tokens of SMILES that consists of atomic \nsymbols and characters representing topological charac -\nteristics, such as ring-closure or branches, which do not \ncorrespond to physical atoms. In this context, atoms are \npresent in molecules by neglecting an essential aspect of \nthe chemical reality. In the following paragraph, we ana -\nlyze the token characteristics of sentences and SMILES \nstrings for insight into the influence of the latter format \non the translation mechanic.\nAccording to the sentence length distribution of vari -\nous language corpora, a well-written sentence contains \n15–20 words on average [7]. The average sequence length \nof a SMILES string is typically three times longer than a \nnatural language, whereas the token space is at least 1000 \ntimes smaller than any developed language [8]. This is a \nconsequence of repetitive tokens observed in SMILES \nstrings. The most distinguishing feature of SMILES rep -\nresentation is the token repeat, which causes atoms of \nmolecules to be indistinguishable in the token space. The \nrepetitive nature of SMILES syntax adds to the more gen-\neral issue of neural machine translation (NMT) decoders, \nyielding degenerative outcomes [9, 10].\nToken order is another aspect of this comparison. \nAlthough the order of words in a sentence can be altered \nto enhance tone, meaning, or fluency, this cannot be \napplied to molecules. In fact, a single molecule can \nequally be represented by hundreds of SMILES enumera-\ntions depending on its topology (more if branches and \ncyclic fragments exist)  [11]. Canonical SMILES refers to \none of those many allowed permutations obtained by a \nunique and consistent atom numbering. In essence, while \nwords tend to retain their semantic significance as they \ntransition from isolated to contextual settings, with only \nminor semantic shifts that may occur over time, the same \ndoes not hold true for atoms within the realm of chem -\nistry. For example, the atomic symbols within a SMILES \nstring are treated equivalently to those in isolation, sig -\nnifying that the chemical significance of these symbols \nis upheld throughout the tokenization process. Thus, \ntokens such as carbon (C) may appear identical in differ -\nent molecules despite their actual differences in chemi -\ncal composition. However, it is important to distinguish \nbetween atom-in-SMILES (AIS, analogous to AIM) and \ncorresponding tokens, as atoms lose their identities when \nthey form molecules.\nInspired by the aforementioned comparative analysis out-\nlined in Table  1, we develop a tokenization framework by \nintroducing environmental information and show that it cor-\nroborates the chemical viewpoint. In recent decades, vari-\nous methods have been developed to enhance or extend the \nSMILES language. Few of these methods include BigSMILES \nfor describing macromolecules [12], CurlySMILES for supra-\nmolecular structures and nanodevices  [13], CXSMILES for \nstoring the special features of molecules [14], OpenSMILES \nspecification for specifying the stereochemistry and chiral-\nity  [15], DeepSMILES and SELFIES for machine learning \napplications  [16, 17], and canonicalization algorithms  [18, \n19]. The aforementioned approaches effectively solve par-\nticular problems originating from the internal structure of \nSMILES. In our approach, we do not treat syntactic prob-\nlems; rather, we redefine SMILES tokens by introducing \nenvironmental information. To consider local chemical \nenvironments, atom environments (AEs) are used, which \nare circular atom-centered topological molecular fragments \ncreated with predefined radii of covalent bonds. Hence, our \nTable 1 Comparison of the important aspects of natural and \nchemical languages within the NLP framework\n*practically less due to the rules of chemistry\nAspects Natural language SMILES language\nSequence length 15-20 words ∼ 3 times higher\nToken space >100K ∼ 1000 times smaller\nToken order Tone, meaning, fluency nC2  alternatives*\nMeaning-wise isolation ≡ context isolation ≡ context\nPage 3 of 13\nUcak et al. Journal of Cheminformatics (2023) 15:55\napproach entails utilizing AEs to produce environment-\naware atomic tokens analogous to atom-in-molecules. We \nterm this custom tokenization scheme Atom-in-SMILES, \nAIS.\nThe AIS tokenization integrates key aspects of SMILES \nand AEs  [20]. The proposed approach accommodates all \nrelevant information for a seamless bidirectional transfor-\nmation between the two representations, to ensure practi-\ncal implementation. We demonstrate that the AIS scheme \nperforms better in translating between equivalent string \nrepresentations of molecules using an exceptionally chal -\nlenging dataset. It was also observed that training with AIS \ntokenization leads to more accurate models for single-step \nretrosynthetic pathway prediction, and molecular prop -\nerty prediction tasks. We also evaluated prediction qualities \nby comparing AIS tokenization with the existing schemes: \ncanonical SMILES-based tokens, atom-wise and SMILES \npair encoding (SmilesPE)  [21], SELFIES, and DeepSMILES \ntokens. We show that AIS tokenization reflects the true \nchemical context, delivers better performance, and reduces \ntoken degeneration by 10%.\nImplementation\nAdvanced tokenization schemes have emerged as a result \nof the evolution of natural language processing. Figure  1 \nshows that state-of-the-art tokenization schemes, like \nBERT  [22], GPT-2  [23], and XLM  [24], divide words into \nsub-words to capture contextual relationships between \nthem while conventional tokenization schemes used to \nbreak down sentences into words or characters. In the field \nof cheminformatics, atom-wise tokenization of SMILES is \nprimarily used for training chemical language models. In \naddition to atom-wise SMILES tokenization, new molecu-\nlar representations have been introduced such as SELFIES \nand DeepSMILES, and specialized tokenization schemes \nlike SmilesPE imitating byte-pair encoding.\nIn the token space generated by atom-wise SMILES \ntokenization, all atoms with the identical atomic numbers \nare indistinguishable. As a toy example, in a glycine mol -\necule (Fig.  2) carbons are represented as two identical \ncarbon atoms following tokenization. Oxygen atoms are \nalso treated similarly. Hypothetical atomic constituents \nobtained by tokenization are often degenerated. This is an \nintrinsic feature of SMILES representation, which does not \ncorrespond to chemical reality.\nWe propose the AIS tokenization scheme that expresses. \nThe most natural formulation of this proposition is as fol-\nlows. Let T1 and T2 be the token spaces of SMILES and AIS, \nrespectively, and f : T 1 → T 2 be a mapping, which is one-\nto-one and onto; then,\nFor any SMILES string, the function f simply tweaks \neach atom by selecting it as the central atom of the \n(1)f(X ) =\n{\nAE |Xcentral if X is an atom\nX otherwise.\nFig. 1 Comparison of conventional and modern tokenization schemes in NLP and the tokenization methods in the chemical language domain\nPage 4 of 13Ucak et al. Journal of Cheminformatics (2023) 15:55\ncorresponding atomic environments (AEs); otherwise, it \nis an identity operator from the token space. The chiral -\nity and aromaticity information of the central atom are \npreserved through the above-described mapping. Bond \norder and hybridization are the two intrinsic dimen -\nsions of AIS tokens. As f is invertible, SMILES strings \ncan be fully recovered by the SMILES projection. The \nproposed algorithm for generating AIS tokenization can \nbe described through the presented pseudo-code (Algo -\nrithm 1). The algorithm works by iterating over the atoms \nin a SMILES string and generates rich, environment-\naware variants.\nIntroducing the neighboring atoms interacting with the \ncentral atom generates tokens with greater diversity. As \nshown in Fig.   2 carbon and oxygen atoms are well distin-\nguished according to their local chemical environments \n( C:1  ≡ C:2 , O:3  ≡ O:4 ). The token space stretches rela -\ntive to the atom-wise tokenization. As another example, \nshown in Fig. 3, we can consider the tokens of the follow-\ning aromatic molecules: Clc1csc2nc3ncncn3c12 and \nClc1csc2nc3nnccn3c12. The atom-wise tokens of these \nmolecules are identical. However, the set of the symmetric \ndifference of AIS tokens has three members, [cH;R;CN], \n[cH;R;NN], and [n;R;CN], rendering the carbon-nitro-\ngen swap recognizable.\nFig. 2 A toy example illustrating the major differences between AIS and conventional SMILES tokenizations. The formal description of AIS \ntokenization contains three primary elements, (i) central atom, (ii) ring information, and (iii) neighbor atoms information, interacting with the central \natom. The formalism ties everything together within a square bracket separated by a semi-colon. The chirality information can be attached to the \ncentral atom, which is labeled with either @ or @ @ suffixes. Aromaticity is reflected on the central atom with a lower case letter. Hydrogen atoms \nare explicitly specified on central atoms. The hybridization and bonding nature of organic elements can be easily deduced\nFig. 3 Token set comparison of two highly similar molecules. The \nmolecules differ only in the position of a carbon and nitrogen atom in \none of the rings\nPage 5 of 13\nUcak et al. Journal of Cheminformatics (2023) 15:55\nFigure  4 provides insight into the inherent proper -\nties of various molecular representations, revealing their \nexpressive power, token diversity, and chemical relevance. \nWe evaluated the distributions of tokens and normalized \nrepetition rates across a diverse set of molecular datasets \nwith a wide range of structural complexities and configu -\nrational changes, such as coordination compounds and \nligands (metal complexes from Crystallography Open \nDatabase  [25]), ring structures and functional groups \n(steroids  [26]), long-chain formations (phospholipids \nand ionizable lipids   [27]), complex and diverse struc -\ntures (natural products  [28]), small organic molecules \n(drugs  [29]), and configurational changes in molecular \nstructure (octane isomers). Single-token repetition can \nbe easily quanti fied as rep-l = ∑|s|\nt=1 [st ∈ st−w −1:t−1 ] , \nwhere s and |s| denote the prediction and token count \nrespectively [10]. We kept the number of considered pre -\nvious token w sufficiently large (as large as the maximum \nsequence length). Normalized repetition rates, which \nmeasure the ratio of single-token repetitions to sequence \nlength, is used to provide a meaningful measure of \nexpressiveness. Lower repetition rates indicate more \ndiverse and informative token sets that can alleviate the \nproblem of degeneracy observed in model outcomes.\nIn Fig.  4, AIS tokens exhibit consistently lower rep -\netition rates compared to SMILES, SELFIES, and Deep -\nSMILES, indicating a higher level of expressiveness. \nThis difference in expressive power is particularly evi -\ndent in drugs, natural products, and steroid datasets. \nHowever, in expressing long chains, as in the case of \nlipids, all tokenization schemes struggle. One limitation \nof AIS is that it lacks the ability to distinguish environ -\nmentally similar substructures or those with a symmetry \nplane since it only considers nearest neighborhoods. The \nSmilesPE representation exhibits a low-lying distribution \ndue to the relatively low number of pseudo-substruc -\ntures with fewer or zero repetitions. It is worth noting \nthat inherent repetitions in molecular representations \ncan exacerbate the repetition problem observed in NLP \nmodel outcomes, highlighting the importance of diverse \nand informative token sets.\nPage 6 of 13Ucak et al. Journal of Cheminformatics (2023) 15:55\nResults and discussion\nWe tested the AIS tokenization on three challenging \ntasks: (i) input–output equivalent mapping (SMILES \ncanonicalization), (ii) single-step retrosynthetic pre -\ndiction, and iii) molecular property prediction. For the \nfirst two functionality tests, we utilized NMT frame -\nwork that translates sequences from the source to the \ntarget domain with the most promising attention-based \ntransformer encoder-decoder architecture  [30, 31]. We \ntrained our models for 200,000 steps with the Adam opti-\nmizer, negative log-likelihood loss, and cyclic learning \nrate scheduler. For these tasks, we report the percentage \nof exact prediction.\nInput–output equivalent mapping\nFirst, we tested how the learning efficiency of an NMT \nmodel is affected by the choice of the tokenization \nscheme, on the task of converting non-canonical SMILES \nstrings into their canonical form. For rigorous test, we \ngenerated extremely confusing datasets consisting of \nmany similar strings. To generate the datasets, we used \nthe predefined subsets of the GDB-13  [32] database \nthat contains drug-like molecules with up to 13 heavy \natoms which consist of C, N, O, S, and Cl. The subsets \nwere generated by applying cumulative pre-defined con -\nstraints  [33, 34], which were named as follows: a: No \ncyclic HetHet Bond; b: No acyclic HetHet Bond; c: Sta-\nble FG; d: No cyclic C=C and C:C bonds; e: No acyclic \nC=C and C:C bonds; f: No small rings; g: Fragment-\nlike, and h: Scaffold-like. Our training dataset consisted \nof one million randomly sampled molecules taken from \nthe GDB-13, combined with 150K randomly sampled \nfrom the most stringent GDB-13 subset abcdefgh. \nWe augmented the subset at different levels ( ×10, ×30, \nand ×50) to make the training set more confusing. This \napproach resulted in training datasets with a high degree \nof similarity between the input (non-canonical instances) \nFig. 4 Comparison of expressiveness and normalized repetition rates across various molecular representations. Distributions showcasing the \ndistinct characteristics of tokenization schemes on representative datasets, each designed to test different facets of molecular structures such \nas coordination compounds, ligands (metal complexes), ring structures and functional groups (steroids), long-chain formations (phospholipids, \nionizable lipids), complex and diverse structures (natural products), small organic molecules (drugs), and configurational changes in molecular \nstructure (octane isomers). Each dataset contains one hundred members, with the exception of steroids (59 members) and octane isomers (18 \nmembers). The mean values of normalized repetitions and deviations from the mean are visually represented as horizontal and dashed vertical lines, \nrespectively, accompanying the distributions\nPage 7 of 13\nUcak et al. Journal of Cheminformatics (2023) 15:55\nand output (only canonical enumerations) SMILES \nstrings, making it difficult to discern variations.\nWe quantified the performance on large (20K) GDB-13 \ndisjoint test sets of varying constraints (see Table  2). To \nhighlight the benefits of our token design over SMILES \ntokenization, we utilized an approach shown in Fig.  5b. \nTable  2 and Fig.  5a, c demonstrate the limitations of \nSMILES tokenization and the characteristics of our \ntokens. The atom-in-SMILES scheme outperformed the \nSMILES atom-wise scheme on all subsets and augmen -\ntation levels, with increasing performance gaps for more \nrestrictive subsets (more similar). The highest prediction \naccuracy of 59.9% (x10) and 56.8% (x50) was achieved on \nthe subset abcdefg, compared to 50.9% (x10) and 50.0% \n(x50) for the atom-wise scheme.\nIn our experiments, we observed that the added com -\nplexity by data augmentation resulted in a degradation \nof performance, different from the typical degradation \nobserved in overly complex models (overfitting)  [35]. \nAtom-wise tokens struggled to handle the increas -\ning complexity, resulting in a performance deficit of up \nto 10.7% on the abcdef subset. Notably, as the level of \naugmentation increased, the model’s token-level prob -\nabilities decreased. However, we found that the AIS \ntokenization, trained on a dataset of extremely similar \nmolecules, was better equipped to handle this problem. \nThe greater string similarity led to consistent improve -\nments in predictive power, which we attribute to the \nricher and more expressive representation of AIS tokens.\nFig. 5 Performance of atom-wise (blue) and atom-in-SMILES (purple) tokenization schemes tested on various restricted GDB-13 test sets [33]. a Test \nresults of × 10 augmented training set. b Model overview. c Test results of × 50 augmented training set. The training is conducted with one million \nrandomly sampled molecules taken from the GDB-13, combined with 150K randomly sampled subset of the strictest cumulative abcdefgh data, \nwhich we augmented at different levels ( ×10, ×30, and ×50)\nTable 2 Performance of atom-wise and atom-in-SMILES tokenization schemes tested on various restricted GDB-13 test sets [33]\nThe training is conducted with one million randomly sampled molecules taken from the GDB-13, combined with 150K randomly sampled subset of the strictest \ncumulative abcdefgh data, which we augmented at different levels ( ×10, ×30, and ×50)\nGDB-13 subsets [33] \n(cumulative)\nPrediction accuracy (%)\nAtom-wise Atom-in-SMILES\nx10 x30 x50 x10 x30 x50\nab 34.2 34.3 33.2 37.3 35.9 34.1\nabc 31.0 30.8 29.6 33.7 32.1 30.4\nabcd 30.8 30.4 29.2 34.3 32.3 30.5\nabcde 48.7 47.6 45.5 53.6 50.0 47.0\nabcdef 41.8 40.6 39.1 52.5 49.6 46.9\nabcdefg 50.9 50.9 50.0 59.9 58.6 56.8\nPage 8 of 13Ucak et al. Journal of Cheminformatics (2023) 15:55\nSingle-step retrosynthesis and token degeneration\nRetrosynthetic prediction is a challenging task in organic \nsynthesis that involves breaking down a target mol -\necule into precursor molecules using a set of reaction \ntemplates. This process helps chemists identify poten -\ntial routes for synthesizing novel chemical structures. \nHowever, conventional template-based methods have \nlimitations such as coverage and template generation \nissues   [36, 37], and can be computationally expen -\nsive [38]. Additionally, atoms have not been successfully \nmapped between products and reactants in these meth -\nods  [39]. To address these challenges, we implemented \na template-free, direct translational method to suggest \nreactant candidates, which is extremely similar to the \nconcept proposed by several groups   [40–44]. These \napproaches can provide high-quality and complete rec -\nommendations without the need for hand-crafted tem -\nplates [45] or pre-existing reaction databases [46, 47].\nWe adopted the open-source ca. USPTO-50K reac -\ntion benchmark dataset that is widely used for a single-\nstep retrosynthesis prediction task. This dataset was a \nsubset of a larger collection from the U.S. patent litera -\nture obtained with a text-mining approach   [48, 49]. As \na preprocessing step, we removed sequences longer \nthan 150 tokens. The prediction quality is assessed by \ntop-1 accuracy, string match. Additionally, we reported \nTanimoto exactness (with hashed Morgan Fingerprint \nradius of 3 and bit size of 2048 [50]) since the predicted \nstructures might fail on the string match tests  [51], but \nstill can map to correct ground truth due to multiplic -\nity of SMILES representation. To determine the effect of \nthe tokenization on the prediction quality, we compared \nthe performance of the AIS tokenization with two other \nSMILES-based tokenization schemes, namely, atom-wise \nand SmilesPE, and two molecular representations Deep -\nSMILES and SELFIES.\nRepetition is a well-known issue in text generation \nmodels, where multiple tokens predict the same sub -\nsequent token with high probability  [9, 56], leading to \nthe generation of repetitive sequences. A sequence is \nsaid to have a repetition subsequence if and only if it \ncontains at least two adjacent identical continuous sub -\nsequences  [56]. Large-scale language models such as \nTransformer and GPT-2 have shown to exhibit this issue, \nresulting in a negative impact on the quality of generated \ntext. The Table  3 demonstrates different types of token \ndegeneration, including single-word repetition, phrase-\nlevel repetition, sentence-level repetition, structural \nrepetition, and subsequential repetition. The examples \nare drawn from a range of NLP tasks, such as sentence \ncompletion, summarization, generation from an initial \ntag line, product review generation, protein sequence \ngeneration, and molecule captioning. This emphasizes \nthe prevalence of token degeneration and highlights the \nimportance of addressing this issue to ensure the genera -\ntion of high-quality natural language text.\nHerein, we observed that molecular prediction tasks \nare also susceptible to token repetition. With the careful \nexamination of non-exact predictions, we were able to \nsummarize the common forms of problematic outcomes. \nFigure  6 displays six typical examples of token repeti -\ntion in SMILES predictions within an NMT retrosyn -\nthesis framework: long head and tail, repetitive rings and \nchains, and halogen repetitions on aliphatic and aromatic \ncarbons. These outcomes are considered to be the most \nprobable by the model and have a negative impact on the \nquality of predictions. The long head and tail result from \nthe repeated addition of identical or similar substruc -\ntures to a terminal, whereas repetitive rings and chains \noccur due to the repeated addition of the same substruc -\nture. The halogen repetitions on aliphatic and aromatic \ncarbons occur when the model repeats the same halo -\ngen substitution on similar carbons. Understanding and \naddressing these problematic outcomes is crucial for the \ndevelopment of accurate molecular prediction models.\nMethods for quantifying the propensity of subse -\nquent repetition are adapted from the recent studies by \nWelleck  [10] and Fu  [56] on neural text degeneration. \nWe focused more on token-level measure for repetition \nthan sequence-level repetition [9] because NMT typically \nuses a maximum log-likelihood training objective that is \nconcerned with optimizing next-token conditional dis -\ntributions. We used a token-level measure for repetition, \nrep-l, that counts the single-token repeats appearing in \nthe preceding tokens. As there are so many single-token \nrepeats appear in the ground truth SMILES, we reported \nthe number of predicted SMILES with repetition rate \nhigher than the ground truth.\nIn Table 4, top-1 string exact and Tanimoto exact accu-\nracy are listed for various tokenization schemes along \nwith the number of predicted SMILES with repeated \ntokens, rep-l|P − rep-l|GT ≥ 2 , where P and GT refer \nto prediction and ground truth. We observed perfor -\nmance gains using the AIS tokenization, outperform -\ning the baseline by 4.3% in string exacts and 2.9% in T c \nexacts. Our methodology, having the fewest single-token \nrepeats, alleviated the repetition problem by approxi -\nmately 10% compared to the atom-wise tokenization \nscheme. DeepSMILES exhibited the worst degenerate \nrepetition among all tokenization schemes, but its over -\nall accuracy in predicting retrosynthesis was 3.5% lower \nthan the baseline on average. Regardless of the repetition \nrate, SELFIES showed lower retrosynthesis prediction \naccuracy than the baseline of SMILES atom-wise tokeni -\nzation. The overall performance of SmilesPE was about \nonly half of the baseline. This clearly demonstrates that \nPage 9 of 13\nUcak et al. Journal of Cheminformatics (2023) 15:55\nthe pseudo-substructures obtained with SmilesPE were \nnot sufficient to capture the chemical change with ease.\nMolecular property prediction tasks\nThe Table  5 provides a comprehensive comparison of \nvarious tokenization schemes for molecular property \nprediction tasks, highlighting the significance of tokeni -\nzation in this domain. To evaluate the performance of \nthese schemes, we used the MoleculeNet benchmark \ndatasets  [ 58], which includes three regression tasks: \nESOL for estimating solubility, FreeSolv for hydration \nfree energies, and Lipophilicity for octanol/water distri -\nbution coefficient, logD at pH 7.4 and three binary clas -\nsification tasks: BBBP for barrier permeability, BACE \nfor predicting binding results for a set of inhibitors of \nhuman beta-secretase 1, and HIV for predicting inhibitor \nactivity. We trained random forest models with a 5-fold \ncross-validation strategy to ensure reproducible results. \nThe tokenized form of the molecules was converted into \none-hot encoding, which was used as the input feature \nrepresentation for training the models. The regression \ntasks were evaluated using the root mean squared error \n(RMSE) metric, while receiver operating characteristic \narea under the curve (ROC-AUC) was used for evaluat -\ning the classification tasks.\nOur findings indicate that the choice of tokenization \nscheme has a significant impact on the performance \nof molecular property prediction models. In terms of \nregression tasks, the AIS tokenization scheme dem -\nonstrated superior performance, achieving the lowest \nRMSE values on ESOL (0.553), FreeSolv (0.441), and \nLipophilicity (0.683) datasets. In terms of classifica -\ntion tasks, AIS performed strongly, with the highest \nROC-AUC values achieved on BBBP (0.885), and the \nsecond-highest value achieved on BACE (0.835) and \nHIV (0.729) datasets. In Table  5, the performances of \nSMILES, DeepSMILES, and SELFIES schemes were \nfound to be comparable across all the conducted tasks, \nwith SELFIES exhibiting a slightly more favorable per -\nformance than SMILES. However, SmilesPE exhibited \ninconsistent results compared to other tokenization \nschemes. While it performed the best on the HIV and \nBACE datasets and performed well on the BBBP and \nLipophilicity datasets, its performance was not consist -\nent across all datasets (worst in ESOL and FreeSolv). \nThis inconsistency could be attributed to the limited \nvocabulary size of SmilesPE, resulting in an inaccurate \nrepresentation of molecular substructures. Moreo -\nver, the representation of molecules as sequences of \npseudo-words in SmilesPE may not be suitable for cer -\ntain types of molecules, leading to poorer performance \non specific datasets.\nAIS as a fingerprint\nDue to the compositional nature of the AIS tokeniza -\ntion scheme, it can be readily converted to a fingerprint \nby simply removing non-atomic tokens. We considered \nthe frequency of each AIS token from a string as its fin -\ngerprint form. This is conceptually identical to counting \nthe frequency of each distinct fragment during ECFP \nTable 3 Degenerate repetition observed in various tasks from state-of-the-art large-scale language models\nThe examples contain single-word repetitions, phrase-level repetitions, sentence-level repetitions, structural repetitions where tokens may vary within a repeating \nphrase, and subsequential repetitions. The first repeated unit in each example is  emphasized in bold.\nTask [model] Context [Repetition pattern] output\nSentence completion \n[GPT2] ref. [10]\nPrefix: Lyrically the song has excerpts of differ-\nent languages including French, Spanish,\n[Single-word repetition] German, Italian, Portuguese, Spanish, Portu-\nguese, Portuguese, Portuguese, Portuguese, Portuguese, Portuguese, \nPortuguese, Portuguese, Portuguese, .\nSummarization [Trans-\nformer] ref. [52]\nArticle [Phrase-level repetition] ...cristiano ronaldo’s best footballer whereas \nbarcelona talisman lionel messi’s best footballer whereas barcelona’s \nbest footballer whereas barcelona talisman lionel lee whereas’s best \nfootballer’s. (Truncated)\nGeneration from an \ninitial tag line [GPT-2] \nref. [9]\nTag: So what’s new in my life? 09/11/18 - Just \ngot back from vacation\n[Sentence-level repetition] Just got back from vacation. 09/10/18 - Just \ngot back from vacation. Just got back from vacation. 09/09/18 - Just got \nback from vacation. Just got back from vacation. 09/08/18 - Just got back \nfrom vacation. Just got back from vacation.\nProduct review genera-\ntion [GPT-2] ref. [53]\nInitial context [Structural repetition] Great movie, although took a while to see at first it \nheld my interest and kept me interested, plus i thought it was extremely \ngood. also it was very good.\nProtein sequence \ngeneration [ProtGPT-2] \nref. [54]\nNo context [Subsequential repetition] MSNDTPTHDPTPPAPAPAPEPAPAPAPEPAPAPA-\nPEPAPAPAPEPAPAPAPEPAPAPAPEPAPAPAPEPAPAPAPEPAPAPAPEPAPAPAPE-\nPAPAPAPE.\nMolecule captioning \n[Transformer] ref. [55]\nSMILES: CC[N+](CC)=C1C=CC2=N \nC3=C(OC2=C1)C=C(N)C(C) =C3\n[Single-word repetition] the molecule is a deuterated compound that is \nis is is is an isotopologue of chloroform in which the four hydrogen atoms \nhave been replaced by deuterium. (Truncated)\nPage 10 of 13Ucak et al. Journal of Cheminformatics (2023) 15:55\nFig. 6 The most commonly occurring repetitive patterns in an NMT retrosynthetic framework. Copious repetitions are highlighted in SMILES and \nmolecular drawings. GT and P refer to the ground truth and prediction, respectively\nPage 11 of 13\nUcak et al. Journal of Cheminformatics (2023) 15:55\ngeneration  [50]. It should be noted that computing the \nmolecular similarity with AIS does not require any hash -\ning function. Thus, converting an AIS string to its finger -\nprint form requires much less computation than other \nfingerprint methods using hash functions. Based on this \ndefinition, we calculated the Tanimoto similarities of 2 \nmillion pairs generated by pairwise combination of 2000 \nrandomly chosen ChEMBL molecules using the AIS fin -\ngerprint and other widely used fingerprint schemes, and \ntheir probability densities are compared (Fig.  7). The \nmost probable similarity between a random pair of mol -\necules using the AIS fingerprint is 0.21. This is similar \nto those of HashAP and ECFP2 and lower than those of \nRDKit, Avalon, and MACCS. This indicates that the AIS \nfingerprint has better resolution power than MACCS, \nAvalon, and RDKit, and comparable to ECFP2 and \nHashAP .\nThis similarity between AIS and its fingerprint form \nmay enhance the learning process of various chemi -\ncal language models. In general, the loss functions of \nchemical translation and generation models are assessed \nthrough a token-wise comparison. However, few errors \nin a SMILES string may lead to an invalid or substan -\ntially different molecule. Consequently, the loss value and \nmolecular similarities may not be closely correlated. On \nthe contrary, AIS strings with a few token errors repre -\nsent similar molecules because of the fingerprint-like \nnature of AIS. Thus, loss values and dissimilarities of \nmolecules due to token errors are more closely correlated \nwith AIS than SMILES.\nIn a recent study, we established that fingerprint \nrepresentations, such as ECFP2, ECFP4, and atom \nenvironments, can be transformed back into their cor -\nresponding SMILES strings with minimal ambiguity [44]. \nThis suggests that fingerprint representations can serve \nas valuable and informative stand-alone representations. \nEmploying fingerprints as input representations simpli -\nfies the application of diverse AI models to chemistry, \nas bit vectors or straightforward token counts are more \nmanageable than character sequences and can be effort -\nlessly integrated with numerous existing algorithms. We \ncontend that the strong resemblance between AIS strings \nand their fingerprint counterparts holds significant \npotential for further development in this domain.\nConclusion\nThis study demonstrated that tokenization has a sig -\nnificant impact on the final prediction quality. We intro -\nduced atom-in-SMILES (AIS) tokenization as a proper \nand meaningful custom tokenization scheme to improve \nthe prediction quality in sequence prediction tasks \nachieving gains of up to 10.7% in equivalent SMILES \nmapping and 4.3% in a retrosynthetic prediction task. \nTable 4 Performance (top-1 accuracy) of various tokenization \nschemes on single-step retrosynthesis task and the number of \npredictions with token repetition\nTokenization \nschemes\nrep-l|P − rep-l|GT ≥ 2 Acc.(%) greedy\nString exact Tc exact\nAtom-wise  \nbaseline [57]\n– 42.00 –\nAtom-wise (ref. [57] is \nreproduced)\n801 42.05 44.72\nSmilesPE (ref. [21]) 821 19.82 22.74\nSELFIES (ref. [17]) 886 28.82 30.76\nDeepSMILES (ref. [16]) 902 38.63 41.20\nAtom-in-SMILES 727 46.32 47.62\nTable 5 Performance analysis of tokenization schemes for \nmolecular property prediction using MoleculeNet benchmark \nsuite\nComparison of Random Forest regression and classification models with 5-Fold \nCross-Validation. Bold emphasis  denotes the highest performing approach\nSMILES DeepSMILES SELFIES SmilesPE AIS\nRegression Datasets: RMSE\nESOL 0.628 0.631 0.675 0.689 0.553\nFreeSolv 0.545 0.544 0.564 0.761 0.441\nLip 0.924 0.895 0.938 0.800 0.683\nClassification Datasets: ROC-AUC \nBBBP 0.758 0.777 0.799 0.847 0.885\nBACE 0.740 0.774 0.746 0.837 0.835\nHIV 0.649 0.648 0.653 0.739 0.729\nFig. 7 Fingerprint nature of AIS. Pairwise similarity scores of 1 million \npairs of molecules are computed for the commonly used structural \nfingerprints and their probability density functions are plotted. The \nTanimoto coefficient is used to measure similarity scores\nPage 12 of 13Ucak et al. Journal of Cheminformatics (2023) 15:55\nAIS outperforms other tokenization methods in molecu -\nlar property prediction tasks and aligns more closely with \nchemical perspectives.\nWe investigated the resolution of the fingerprint aspect \nof AIS, revealing that it encompasses all essential infor -\nmation for seamless bidirectional transitions between \nSMILES and fingerprint representations, ensuring prac -\ntical implementation. The study addressed the repetition \nissue in molecular predictions, akin to natural language, \nwhich impeded the quality of predicted molecules. \nThe AIS tokenization scheme considerably diminished \nobstacles in repetitive loops (by around 10%) in the pre -\ndicted SMILES. As far as we are aware, no prior research \nhas examined token degeneration in AI-driven chemi -\ncal applications. The AIS tokenization method can be \nemployed by the broader community to deliver chemi -\ncally precise and customized tokens for molecular pre -\ndiction, property prediction, and generative models.\nAuthor contributions\nUVU, IA and JL conceived and designed the study. UVU and IA processed data, \ntrained the models and analyzed the results. UVU, IA., and JL discussed and \ninterpreted the results, wrote and reviewed the manuscript. All authors read \nand approved the final manuscript.\nFunding\nThis work was supported by the National Research Foundation of \nKorea (NRF) grants funded by the Korean government (MSIT) (Nos. \nNRF-2022M3E5F3081268, NRF-2020M3A9G7103933, and NRF-\n2022R1C1C1005080). This research was supported by the BK21FOUR \nProgram of the National Research Foundation of Korea(NRF) funded by the \nMinistry of Education(5120200513755). This work was also supported by \nthe Korea Environment Industry & Technology Institute (KEITI) through the \nTechnology Development Project for Safety Management of Household \nChemical Products, funded by the Korea Ministry of Environment (MOE) \n(KEITI:2020002960002 and NTIS:1485017120).\nAvailability of data and materials\nThe source code of this work is available via GitHub repo: https:// github. com/ \nsnu- lcbc/ atom- in- SMILES\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 16 January 2023   Accepted: 14 May 2023\nPublished: 29 May 2023\nReferences\n 1. Domingo M, Garcıa-Martınez M, Helle A, et al (2018) How Much Does \nTokenization Affect Neural Machine Translation? Arxiv. https:// doi. org/  \n10. 48550/ arxiv. 1812. 08621\n 2. Weininger D (1988) SMILES, a chemical language and information \nsystem. 1. Introduction to methodology and encoding rules. J Chem \nInf Comp Sci 28(1):31–36. https:// doi. org/ 10. 1021/ ci000 57a005\n 3. Bader RFW (1985) Atoms in molecules. Acc Chem Res 18(1):9–15. \nhttps:// doi. org/ 10. 1021/ ar001 09a003\n 4. Cadeddu A, Wylie EK, Jurczak J, Wampler-Doty M, Grzybowski BA (2014) \nOrganic chemistry as a language and the implications of chemical \nlinguistics for structural and retrosynthetic analyses. Angew Chem Int \nEd 53(31):8108–8112. https:// doi. org/ 10. 1002/ anie. 20140 3708\n 5. Lesniewski S (1927) O podstawach matematyki (on the foundations of \nmathematics). Przeglad filozoficzny 30:164–206\n 6. Varzi AC (1996) Parts, wholes, and part-whole relations: the prospects \nof mereotopology. Data Knowl Eng 20(3):259–286. https:// doi. org/ 10.  \n1016/ S0169- 023X(96) 00017-1\n 7. Borbély G, Kornai A (2019) Sentence Length. arXiv Preprint. https:// doi.  \norg/ 10. 48550/ arXiv. 1905. 09139\n 8. Bojar O, Chatterjee R, Federmann C, Graham Y, Haddow B, Huck M, \nJimeno Yepes A, Koehn P , Logacheva V, Monz C, Negri M, Névéol \nA, Neves M, Popel M, Post M, Rubino R, Scarton C, Specia L, Turchi \nM, Verspoor K, Zampieri M (2016) Findings of the 2016 conference \non machine translation. In: proceedings of the first conference on \nmachine translation: volume 2, shared task papers, pp. 131–198. Asso -\nciation for Computational Linguistics, Berlin, Germany . https:// doi. org/  \n10. 18653/ v1/ W16- 2301\n 9. Holtzman A, Buys J, Du L, Forbes M, Choi Y (2019) The curious case of \nneural text degeneration. arXiv Preprint. https:// doi. org/ 10. 48550/ arXiv. \n1904. 09751\n 10. Welleck S, Kulikov I, Roller S, Dinan E, Cho K, Weston J (2019) Neural text \ngeneration with unlikelihood training. arXiv Preprint. https:// doi. org/ 10. \n48550/ arXiv. 1908. 04319\n 11. Arús-Pous J, Johansson SV, Prykhodko O, Bjerrum EJ, Tyrchan C, Reymond \nJL, Chen H, Engkvist O (2019) Randomized SMILES strings improve \nthe quality of molecular generative models. J Cheminform 11(1):1–13. \nhttps:// doi. org/ 10. 1186/ s13321- 019- 0393-0\n 12. Lin T-S, Coley CW, Mochigase H, Beech HK, Wang W, Wang Z, Woods E, \nCraig SL, Johnson JA, Kalow JA, Jensen KF, Olsen BD (2019) Bigsmiles: a \nstructurally-based line notation for describing macromolecules. ACS Cent \nSci 5(9):1523–1531. https:// doi. org/ 10. 1021/ acsce ntsci. 9b004 76\n 13. Drefahl A (2011) CurlySMILES: a chemical language to customize and \nannotate encodings of molecular and nanodevice structures. J Chemin-\nform 3(1):1–7. https:// doi. org/ 10. 1186/ 1758- 2946-3-1\n 14. ChemAxon Extended SMILES and SMARTS - CXSMILES and CXSMARTS \n- Documentation. https:// docs. chema xon. com/ displ ay/ docs/ chema xon- \nsmiles- exten sions. md. Accessed: 10 Feb 2022\n 15. OpenSMILES. Home page http:// opens miles. org. Accessed: 10 Dec 2021\n 16. O’Boyle NM, Dalke A (2018) DeepSMILES: an adaptation of SMILES for use \nin machine-learning of chemical structures. ChemRxiv. https:// doi. org/ 10. \n26434/ chemr xiv. 70979 60. v1\n 17. Krenn M, Häse F, Nigam A, Friederich P , Aspuru-Guzik A (2020) Self-\nreferencing embedded strings (SELFIES): A 100% robust molecular string \nrepresentation. Mach Learn Sci Technol 1(4):045024. https:// doi. org/ 10. \n1088/ 2632- 2153/ aba947\n 18. O’Boyle NM (2012) Towards a universal SMILES representation - a \nstandard method to generate canonical SMILES based on the InChI. J \nCheminform 4(9):1–14. https:// doi. org/ 10. 1186/ 1758- 2946-4- 22\n 19. Schneider N, Sayle RA, Landrum GA (2015) Get your atoms in order-an \nopen-source implementation of a novel and robust molecular canonicali-\nzation algorithm. J Chem Inf Model 55(10):2111–2120. https:// doi. org/ 10. \n1021/ acs. jcim. 5b005 43\n 20. Hähnke VD, Bolton EE, Bryant SH (2015) PubChem atom environments. J \nCheminform 7(1):1–37. https:// doi. org/ 10. 1186/ s13321- 015- 0076-4\n 21. Li X, Fourches D (2021) SMILES pair encoding: a data-driven substructure \ntokenization algorithm for deep learning. J Chem Inf Model 61(4):1560–\n1569. https:// doi. org/ 10. 1021/ acs. jcim. 0c011 27\n 22. Devlin J, Chang M-W, Lee K, Toutanova K (2018) BERT: pre-training of \ndeep bidirectional transformers for language understanding. arXiv. \nhttps:// doi. org/ 10. 48550/ arXiv. 1810. 04805\n 23. Radford A, Wu J, Child R, Luan D, Amodei D & Sutskever I (2019) Language \nModels are Unsupervised Multitask Learners. OpenAI. https:// www. \nopenai. com/ blog/ better- langu age- models/\n 24. Lample G, Conneau A (2019) Cross-lingual language model pretraining. \narXiv. https:// doi. org/ 10. 48550/ arXiv. 1901. 07291\n 25. Quirós M, Graẑulis S, Girdzijauskaitė S, Merkys A, Vaitkus A (2018) Using \nSMILES strings for the description of chemical connectivity in the crystal-\nlography open database. J Cheminform 10(1):23. https:// doi. org/ 10. 1186/ \ns13321- 018- 0279-6\n 26. Hansen K, Mika S, Schroeter T, Sutter A, ter Laak A, Steger-Hartmann T, \nHeinrich N, Müller K (2009) Benchmark data set for in silico prediction of \nPage 13 of 13\nUcak et al. Journal of Cheminformatics (2023) 15:55\n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \names mutagenicity. J Chem Inform Model. https:// doi. org/ 10. 1021/ ci900 \n161g\n 27. O’Donnell VB, Dennis EA, Wakelam MJO, Subramaniam S (2019) LIPID \nMAPS: serving the next generation of lipid researchers with tools, \nresources, data, and training. Sci Signal 12(563):2964. https:// doi. org/ 10. \n1126/ scisi gnal. aaw29 64\n 28. Gu J, Gui Y, Chen L, Yuan G, Lu H-Z, Xu X (2013) Use of natural products \nas chemical library for drug discovery and network pharmacology. PLoS \nONE 8(4):62839. https:// doi. org/ 10. 1371/ journ al. pone. 00628 39\n 29. Knox C, Law V, Jewison T, Liu P , Ly S, Frolkis A, Pon A, Banco K, Mak C, \nNeveu V, Djoumbou Y, Eisner R, Guo AC, Wishart DS (2011) DrugBank 3.0: \na comprehensive resource for ‘Omics’ research on drugs. Nucleic Acids \nRes 39(suppl–1):1035–1041. https:// doi. org/ 10. 1093/ nar/ gkq11 26\n 30. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, \nPolosukhin I (2017) Attention is all you need. Adv. Neural Inf. Process Syst. \n2017–Decem:5999–6009\n 31. Bahdanau D, Cho KH, Bengio Y (2015) Neural machine translation by \njointly learning to align and translate. 3rd Int. Conf. Learn. Represent. ICLR \n2015 - Conf. Track Proc., 1–15\n 32. Blum LC, Reymond J-L (2009) 970 Million druglike small molecules for \nvirtual screening in the chemical universe database GDB-13. J Am Chem \nSoc 131(25):8732–8733. https:// doi. org/ 10. 1021/ ja902 302h\n 33. Blum LC, Deursen Rv, Reymond J-L (2011) Visualisation and subsets of \nthe chemical universe database GDB-13 for virtual screening. J Comput \nAided Mol Des 25(7):637–647. https:// doi. org/ 10. 1007/ s10822- 011- 9436-y\n 34. GDB-13 Database. Home page https:// gdb. unibe. ch/ downl oads/. \nAccessed: 02 Nov 2022\n 35. Ucak UV, Ji H, Singh Y, Jung Y (2016) A soft damping function for disper-\nsion corrections with less overfitting. J. Chem. Phys. 145(17):174104. \nhttps:// doi. org/ 10. 1063/1. 49658 18\n 36. Segler MHS, Waller MP (2017) Neural-symbolic machine learning for \nretrosynthesis and reaction prediction. Eur J Chem 23(25):5966–5971. \nhttps:// doi. org/ 10. 1002/ chem. 20160 5499\n 37. Jin W, Coley CW, Barzilay R, Jaakkola T (2017) Predicting organic reaction \noutcomes with weisfeiler-lehman network. Adv Neural Inf Process Syst \n2017–Decem:2608–2617\n 38. Coley CW, Green WH, Jensen KF (2018) Machine learning in computer-\naided synthesis planning. Acc Chem Res 51(5):1281–1289. https:// doi. \norg/ 10. 1021/ acs. accou nts. 8b000 87\n 39. Liu B, Ramsundar B, Kawthekar P , Shi J, Gomes J, Luu Nguyen Q, Ho S, \nSloane J, Wender P , Pande V (2017) Retrosynthetic reaction prediction \nusing neural sequence-to-sequence models. ACS Cent Sci 3(10):1103–\n1113. https:// doi. org/ 10. 1021/ acsce ntsci. 7b003 03\n 40. Karpov P , Godin G, Tetko IV (2019) A transformer model for retrosynthesis. \nIn: artificial neural networks and machine learning – ICANN 2019: work-\nshop and special sessions, pp. 817–830. Springer, Cham\n 41. Tetko IV, Karpov P , Van Deursen R, Godin G (2020) State-of-the-art aug-\nmented NLP transformer models for direct and single-step retrosynthesis. \nNat Commun 11(1):1–11. https:// doi. org/ 10. 1038/ s41467- 020- 19266-y\n 42. Schwaller P , Petraglia R, Zullo V, Nair VH, Haeuselmann RA, Pisoni R, Bekas \nC, Iuliano A, Laino T (2020) Predicting retrosynthetic pathways using \ntransformer-based models and a hyper-graph exploration strategy. Chem \nSci 11(12):3316–3325. https:// doi. org/ 10. 1039/ c9sc0 5704h\n 43. Ucak UV, Kang T, Ko J, Lee J (2021) Substructure-based neural machine \ntranslation for retrosynthetic prediction. J Cheminform 13(1):1–15. \nhttps:// doi. org/ 10. 1186/ s13321- 020- 00482-z\n 44. Ucak UV, Ashyrmamatov I, Ko J, Lee J (2022) Retrosynthetic reaction \npathway prediction through neural machine translation of atomic \nenvironments. Nat Commun 13(1):1186. https:// doi. org/ 10. 1038/ \ns41467- 022- 28857-w\n 45. Szymkuć S, Gajewska EP , Klucznik T, Molga K, Dittwald P , Startek M, Bajc-\nzyk M, Grzybowski BA (2016) Computer-assisted synthetic planning: the \nend of the beginning. Angew Chem Int Ed 55(20):5904–5937. https:// doi. \norg/ 10. 1002/ anie. 20150 6101\n 46. Coley CW, Barzilay R, Jaakkola TS, Green WH, Jensen KF (2017) Prediction \nof organic reaction outcomes using machine learning. ACS Cent Sci \n3(5):434–443. https:// doi. org/ 10. 1021/ acsce ntsci. 7b000 64\n 47. Law J, Zsoldos Z, Simon A, Reid D, Liu Y, Khew SY, Johnson AP , Major S, \nWade RA, Ando HY (2009) Route designer: a retrosynthetic analysis tool \nutilizing automated retrosynthetic rule generation. J Chem Inf Model \n49(3):593–602. https:// doi. org/ 10. 1021/ ci800 228y\n 48. Lowe DM (2012) Extraction of chemical structures and reactions from the \nliterature. PhD thesis, University of Cambridge. https:// doi. org/ 10. 17863/ \nCAM. 16293\n 49. Lowe D (2017) Chemical reactions from US patents (1976-Sep2016). \nFigshare. https:// doi. org/ 10. 6084/ m9. figsh are. 51048 73. v1\n 50. Rogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem Inf \nModel 50(5):742–754. https:// doi. org/ 10. 1021/ ci100 050t\n 51. Rajan K, Steinbeck C, Zielesny A (2022) Performance of chemical structure \nstring representations for chemical image recognition using transformers. \nDigit Discov 1(2):84–90. https:// doi. org/ 10. 1039/ d1dd0 0013f\n 52. Nair P , Singh AK (2021) On reducing repetition in abstractive summa-\nrization. In: proceedings of the student research workshop associated \nwith RANLP 2021, pp. 126–134. INCOMA Ltd., Online. Accessed 17 Apr \n2023  https:// aclan tholo gy. org/ 2021. ranlp- srw. 18 \n 53. Jawahar G, Abdul-Mageed M, Lakshmanan LVS (2020) Automatic \ndetection of machine generated text: A critical survey. In: proceedings \nof the 28th international conference on computational linguistics, pp. \n2296–2309. International Committee on Computational Linguistics, Bar-\ncelona, Spain (Online). Accessed 17 Apr 2023  https:// doi. org/ 10. 18653/ \nv1/ 2020. coling- main. 208.https:// aclan tholo gy. org/ 2020. coling- main. 208\n 54. Ferruz N, Schmidt S, Höcker B (2022) A deep unsupervised language \nmodel for protein design. BioRxiv. https:// doi. org/ 10. 1101/ 2022. 03. 09. \n483666\n 55. Edwards C, Lai T, Ros K, Honke G, Cho K, Ji H (2022) Translation between \nmolecules and natural language. arXiv. https:// doi. org/ 10. 48550/ arxiv. \n2204. 11817\n 56. Fu Z, Lam W, So AM-C, Shi B (2020) A theoretical analysis of the repetition \nproblem in text generation. arXiv. https:// doi. org/ 10. 48550/ arxiv. 2012. \n14660\n 57. Lin K, Xu Y, Pei J, Lai L (2020) Automatic retrosynthetic route planning \nusing template-free models. Chem Sci 11(12):3355–3364. https:// doi. org/ \n10. 1039/ c9sc0 3666k\n 58. Wu Z, Ramsundar B, Feinberg E, Gomes J, Geniesse C, Pappu AS, Leswing \nK, Pande V (2018) Moleculenet: a benchmark for molecular machine \nlearning. Chem Sci 9:513–530. https:// doi. org/ 10. 1039/ C7SC0 2664A\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}