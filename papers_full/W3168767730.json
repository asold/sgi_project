{
  "title": "How to Adapt Your Pretrained Multilingual Model to 1600 Languages",
  "url": "https://openalex.org/W3168767730",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2949023737",
      "name": "Abteen Ebrahimi",
      "affiliations": [
        "University of Colorado Boulder",
        "University of Colorado System"
      ]
    },
    {
      "id": "https://openalex.org/A2413443321",
      "name": "Katharina Kann",
      "affiliations": [
        "University of Colorado Boulder",
        "University of Colorado System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2016630033",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2981540061",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2891896107",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W3104723404",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2561995736",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W3098744621",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3093721400",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3029889931",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3092115807",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3098466758",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W2270364989",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W2998448492",
    "https://openalex.org/W2757931423",
    "https://openalex.org/W2762484717",
    "https://openalex.org/W2560939934",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W3027825632",
    "https://openalex.org/W3101498587"
  ],
  "abstract": "Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world's languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for over 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to $17.69\\%$ accuracy for part-of-speech tagging and $6.29$ F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 4555–4567\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4555\nHow to Adapt Your Pretrained Multilingual Model to 1600 Languages\nAbteen Ebrahimi and Katharina Kann\nUniversity of Colorado Boulder\n{abteen.ebrahimi,katharina.kann}@colorado.edu\nAbstract\nPretrained multilingual models (PMMs) en-\nable zero-shot learning via cross-lingual trans-\nfer, performing best for languages seen dur-\ning pretraining. While methods exist to im-\nprove performance for unseen languages, they\nhave almost exclusively been evaluated using\namounts of raw text only available for a small\nfraction of the world’s languages. In this paper,\nwe evaluate the performance of existing meth-\nods to adapt PMMs to new languages using\na resource available for over 1600 languages:\nthe New Testament. This is challenging for\ntwo reasons: (1) the small corpus size, and\n(2) the narrow domain. While performance\ndrops for all approaches, we surprisingly still\nsee gains of up to17.69% accuracy for part-of-\nspeech tagging and 6.29 F1 for NER on aver-\nage over all languages as compared to XLM-R.\nAnother unexpected ﬁnding is that continued\npretraining, the simplest approach, performs\nbest. Finally, we perform a case study to dis-\nentangle the effects of domain and size and to\nshed light on the inﬂuence of the ﬁnetuning\nsource language.\n1 Introduction\nPretrained multilingual models (PMMs) are a\nstraightforward way to enable zero-shot learning\nvia cross-lingual transfer, thus eliminating the need\nfor labeled data for the target task and language.\nHowever, downstream performance is highest for\nlanguages that are well represented in the pretrain-\ning data or linguistically similar to a well repre-\nsented language. Performance degrades as repre-\nsentation decreases, with languages not seen during\npretraining generally having the worst performance.\nIn the most extreme case, when a language’s script\nis completely unknown to the model, zero-shot per-\nformance is effectively random.\nWhile multiple methods have been shown to\nimprove the performance of transfer to underrep-\nFigure 1: The number of space-separated words in the\nBible and Wikipedia for six low-resource languages\nused in our experiments; plotted on a log scale.\nresented languages (cf. Section 2.3), previous\nwork has evaluated them using unlabeled data from\nsources available for a relatively small number of\nlanguages, such as Wikipedia or Common Crawl,\nwhich cover 3161 and 1602 languages, respectively.\nDue to this low coverage, the languages that would\nmost beneﬁt from these methods are precisely those\nwhich do not have the necessary amounts of mono-\nlingual data to implement them as-is. To enable\nthe use of PMMs for truly low-resource languages,\nwhere they can, e.g., assist language documentation\nor revitalization, it is important to understand how\nstate-of-the-art adaptation methods act in a setting\nmore broadly applicable to many languages.\nIn this paper, we ask the following question:Can\nwe use the Bible – a resource available for roughly\n1600 languages – to improve a PMM’s zero-shot\nperformance on an unseen target language? And,\nif so, what adaptation method works best? We\ninvestigate the performance of XLM-R (Conneau\n1https://en.wikipedia.org/wiki/List_\nof_Wikipedias\n2https://commoncrawl.github.io/\ncc-crawl-statistics/plots/languages\n4556\net al., 2020) when combined with continued pre-\ntraining (Chau et al., 2020), vocabulary extension,\n(Wang et al., 2020), and adapters (Pfeiffer et al.,\n2020b) making the following assumptions: (1) the\nonly text available in a target language is the New\nTestament, and (2) no annotated training data exists\nin the target language.\nWe present results on 2 downstream tasks – part-\nof-speech (POS) tagging and named entity recog-\nnition (NER) – on a typologically diverse set of\n30 languages, all of which are unseen during the\npretraining of XLM-R. We ﬁnd that, surprisingly,\neven though we use a small corpus from a narrow\ndomain, most adaptation approaches improve over\nXLM-R’s base performance, showing that the Bible\nis a valuable source of data for our purposes. We\nfurther observe that in our setting the simplest adap-\ntation method, continued pretraining, performs best\nfor both tasks, achieving gains of up to 17.69% ac-\ncuracy for POS tagging, and 6.29 F1 for NER on\naverage across languages.\nAdditionally, we seek to disentangle the effects\nof two aspects of our experiments on downstream\nperformance: the selection of the source language,\nand the restricted domain of the New Testament.\nTowards this, we conduct a case study focusing\non three languages with Cyrillic script: Bashkir,\nChechen, and Chuvash. In order to understand the\neffect of the choice of source language, we use a\nmore similar language, Russian, as our source of\nlabeled data. To explore the effect of the New Tes-\ntament’s domain, we conduct our pretraining exper-\niments with an equivalent amount of data sampled\nfrom the Wikipedia in each language. We ﬁnd that\nchanging the source language to Russian increases\naverage baseline performance by 18.96 F1, and we\nachieve the highest results across all settings when\nusing both Wikipedia and Russian data.\n2 Related Work\n2.1 Background\nPrior to the introduction of PMMs, cross-lingual\ntransfer was often based on word embeddings\n(Mikolov et al., 2013). Joulin et al. (2018) present\nmonolingual embeddings for 294 languages using\nWikipedia, succeeded by Grave et al. (2018) who\npresent embeddings for 157 languages trained on\nadditional data from Common Crawl. For cross-\nlingual transfer, monolingual embeddings can then\nbe aligned using existing parallel resources, or in a\ncompletely unsupervised way (Bojanowski et al.,\nCode Language Script Language Family Task\nace Acehnese Latin Austronesian NER\narz Egyptian Arabic Arabic Afro-Asiatic NER\nbak Bashkir Cyrillic Turkic NER\nbam Bambara Latin, N’ko Mande POS\nceb Cebuano Latin Austronesian NER\nche Chechen Cyrillic Northeast Caucasian NER\nchv Chuvash Cyrillic Turkic NER\ncop Coptic Coptic Ancient Egyptian POS\ncrh Crimean Turkish Cyrillic Turkic NER\nglv Manx Latin Indo-European POS\ngrc Ancient Greek Greek Indo-European POS\ngsw Swiss German Latin Indo-European POS\nhak Hakka Chinese Chinese Sino-Tibetan NER\nibo Igbo Latin Niger-Congo NER\nilo Iloko Latin Austronesian NER\nkin Kinyarwanda Latin Niger-Congo NER\nmag Magahi Devanagari Indo-Iranian POS\nmhr Eastern Mari Cyrillic Uralic NER\nmin Minangkabau Latin Austronesian NER\nmlt Maltese Latin Afro-Asiatic Both\nmri Maori Latin Austronesian NER\nmyv Erzya Cyrillic Uralic POS\nnds Low German Latin Indo-European NER\nory Odia Odia Indo-Iranian NER\nsco Scots Latin Indo-European NER\ntat Tatar Cyrillic Turkic NER\ntgk Tajik Cyrillic Indo-Iranian NER\nwar Waray Latin Austronesian NER\nwol Wolof Latin Niger-Congo Both\nyor Yoruba Latin Niger-Congo Both\nTable 1: Languages used in our experiments, none of\nwhich are represented in XLM-R’s pretraining data.\n2017; Artetxe et al., 2017; Lample et al., 2017;\nConneau et al., 2017; Artetxe et al., 2016). Al-\nthough they use transformer based models, Artetxe\net al. (2020) also transfer in a monolingual set-\nting. Another method for cross-lingual transfer in-\nvolves multilingual embeddings, where languages\nare jointly learned as opposed to being aligned\n(Ammar et al., 2016; Artetxe and Schwenk, 2019).\nFor a more in-depth look at cross-lingual word em-\nbeddings, we refer the reader to Ruder et al. (2019).\nWhile the above works deal with generally im-\nproving cross-lingual representations, task-speciﬁc\ncross-lingual systems often show strong perfor-\nmance in a zero-shot setting. For POS tagging,\nin a similar setting to our work, Eskander et al.\n(2020) achieve strong zero-shot results by using un-\nsupervised projection (Yarowsky et al., 2001) with\naligned Bibles. Recent work for cross-lingual NER\nincludes Mayhew et al. (2017) who use dictionary\ntranslations to create target-language training data,\nas well as Xie et al. (2018) who use a bilingual\ndictionary in addition to self-attention. Bharad-\nwaj et al. (2016) use phoneme conversion to aid\ncross-lingual NER in a zero-shot setting. More\nrecently, Bari et al. (2020) propose a model only\nusing monolingual data for each language, and Qi\net al. (2020) propose a language-agnostic toolkit\nsupporting NER for 66 languages. In contrast to\nthese works, we focus on the improvements offered\n4557\nby adaptation methods for pretrained models for\ngeneral tasks.\n2.2 Pretrained Multilingual Models\nPMMs can be seen as the natural extension of\nmultilingual embeddings to pretrained transformer-\nbased models. mBERT was the ﬁrst PMM, cover-\ning the 104 languages with the largest Wikipedias.\nIt uses a 110k byte-pair encoding (BPE) vocabu-\nlary (Sennrich et al., 2016) and is pretrained on\nboth a next sentence prediction and a masked lan-\nguage modeling (MLM) objective. Languages with\nsmaller Wikipedias are upsampled and highly rep-\nresented languages are downsampled. XLM is a\nPMM trained on 15 languages. XLM similarly\ntrains on Wikipedia data, using a BPE vocabulary\nwith 95k subwords and up- and downsamples lan-\nguages similarly to mBERT. XLM also introduces\ntranslation language modeling (TLM), a supervised\npretraining objective, where tokens are masked as\nfor MLM, but parallel sentences are concatenated\nsuch that the model can rely on subwords in both\nlanguages for prediction. Finally, XLM-R is an\nimproved version of XLM. Notable differences\ninclude the larger vocabulary of 250k subwords\ncreated using SentencePiece tokenization (Kudo\nand Richardson, 2018) and the training data, which\nis taken from CommonCrawl and is considerably\nmore than for mBERT and XLM. XLM-R relies\nsolely on MLM for pretraining and achieves state-\nof-the-art results on multiple benchmarks (Conneau\net al., 2020). We therefore focus solely on XLM-R\nin our experiments.\nDownstream Performance of PMMs While\nPires et al. (2019) and Wu and Dredze (2019) show\nthe strong zero-shot performance of mBERT, Wu\nand Dredze (2020) shine light on the difference in\nperformance between well and poorly represented\nlanguages after ﬁnetuning on target-task data.\nMuller et al. (2020) observe varying zero-shot\nperformance of mBERT on different languages\nnot present in its pretraining data. They group\nthem into ‘easy’ languages, on which mBERT per-\nforms well without any modiﬁcation, ‘medium’ lan-\nguages, on which mBERT performs well after addi-\ntional pretraining on monolingual data, and ‘hard’\nlanguages, on which mBERT’s performs poorly\neven after modiﬁcation. They additionally note\nthe importance of script, ﬁnding that transliterating\ninto Latin offers improvements for some languages.\nAs transliteration involves language speciﬁc tools,\nwe consider it out of scope for this work, and leave\nfurther investigation in how to best utilize translit-\neration for future work. Lauscher et al. (2020)\nfocus on PMM ﬁnetuning, and ﬁnd that for un-\nseen languages, gathering labeled data for few-shot\nlearning may be more effective than gathering large\namounts of unlabeled data.\nAdditionally, Chau et al. (2020), Wang et al.\n(2020), and Pfeiffer et al. (2020b) present the adap-\ntation methods whose performance we investigate\nhere in a setting where only the Bible is available.\nWe give a general overview of these methods in the\nremainder of this section, before describing their\napplication in our experiments in Section 3.\n2.3 Adaptation Methods\nContinued Pretraining In a monolingual set-\nting, continued pretraining of a language represen-\ntation model on an MLM objective has shown to\nhelp downstream performance on tasks involving\ntext from a domain distant from the pretraining cor-\npora (Gururangan et al., 2020). In a multilingual\nsetting, it has been found that, given a target lan-\nguage, continued pretraining on monolingual data\nfrom that language can lead to improvements on\ndownstream tasks (Chau et al., 2020; Muller et al.,\n2020).\nVocabulary Extension Many pretrained mod-\nels make use of a subword vocabulary, which\nstrongly reduces the issue of out-of-vocabulary to-\nkens. However, when the pretraining and target-\ntask domains differ, important domain-speciﬁc\nwords may be over-fragmented, which reduces per-\nformance. In the monolingual setting, Zhang et al.\n(2020) show that extending the vocabulary with in-\ndomain tokens yields performance gains. A similar\nresult to that of continued pretraining holds in the\nmultilingual setting: downstream performance of\nan underrepresented language beneﬁts from addi-\ntional tokens in the vocabulary, allowing for better\nrepresentation of that language. Wang et al. (2020)\nﬁnd that extending the vocabulary of mBERT with\nnew tokens and training on a monolingual corpus\nyields improvements for a target language, regard-\nless of whether the language was seen or unseen.\nChau et al. (2020) have similar results, and intro-\nduce tiered vocabulary augmentation, where new\nembeddings are learned with a higher learning rate.\nWhile both approaches start with a random initial-\nization, they differ in the amount of new tokens\nadded: Wang et al. (2020) limit new subwords to\n4558\n30,000, while Chau et al. (2020) set a limit of 99,\nselecting the subwords which reduce the number\nof unknown tokens while keeping the subword-to-\ntoken ratio similar to the original vocabulary.\nAdapters Adapters are layers with a small num-\nber of parameters, injected into models to help\ntransfer learning (Rebufﬁ et al., 2017). Houlsby\net al. (2019) demonstrate the effectiveness of task-\nspeciﬁc adapters in comparison to standard ﬁne-\ntuning. Pfeiffer et al. (2020b) present invertible\nadapters and MAD-X, a framework utilizing them\nalong with language and task adapters for cross-\nlingual transfer. After freezing model weights, in-\nvertible and language adapters for each language,\nincluding English, are trained together using MLM.\nThe English-speciﬁc adapters are then used along\nwith a task adapter to learn from labeled English\ndata. For zero-shot transfer, the invertible and lan-\nguage adapters are replaced with those trained on\nthe target language, and the model is subsequently\nevaluated.\n3 Experiments\n3.1 Data and Languages\nUnlabeled Data We use the Johns Hopkins Uni-\nversity Bible Corpus (JHUBC) from McCarthy\net al. (2020), which contains 1611 languages, pro-\nviding verse-aligned translations of both the Old\nand New Testament. However, the New Testament\nis much more widely translated: 86% of transla-\ntions do not include the Old Testament. We there-\nfore limit our experiments to the New Testament,\nwhich accounts to about 8000 verses in total, al-\nthough speciﬁc languages may not have transla-\ntions of all verses. For the 30 languages we con-\nsider, this averages to around 402k subword tokens\nper language. The speciﬁc versions of the Bible we\nuse are listed in Table 5.\nLabeled Data For NER, we use the splits of\nRahimi et al. (2019), which are created from the\nWikiAnn dataset (Pan et al., 2017). For POS tag-\nging, we use data taken from the Universal Depen-\ndencies Project (Nivre et al., 2020). As XLM-R uti-\nlizes a subword vocabulary, we perform sequence\nlabeling by assigning labels to the last subword\ntoken of each word. For all target languages, we\nonly ﬁnetune on labeled data in English.\nLanguage Selection To select the languages for\nour experiments, we ﬁrst compile lists of all lan-\nguages for which a test set exists for either down-\nstream task and we have a Bible for. We then\nﬁlter these languages by removing those present\nin the pretraining data of XLM-R. See Table 1 for\na summary of languages, their attributes, and the\ndownstream task we use them for.\n3.2 PMM Adaptation Methods\nOur goal is to analyze state-of-the-art PMM adap-\ntion approaches in a true low-resource setting\nwhere the only raw text data available comes from\nthe New Testament and no labeled data exists at\nall. We now describe our implementation of these\nmethods. We focus on theBase version of XLM-R\n(Conneau et al., 2020) as our baseline PMM.\nContinued Pretraining We consider three mod-\nels based on continued pretraining. In the sim-\nplest case, +MLM, we continue training XLM-R\nwith an MLM objective on the available verses of\nthe New Testament. Additionally, as Bible trans-\nlations are a parallel corpus, we also consider a\nmodel, +TLM, trained using translation language\nmodeling. Finally, following the ﬁndings of Lam-\nple and Conneau (2019), we also consider a model\nusing both TLM and MLM, +{M|T}LM. For this\nmodel, we alternate between batches consisting\nsolely of verses from the target Bible and batches\nconsisting of aligned verses of the target-language\nand source-language Bible. For NER, we pretrain\n+MLM and +TLM models for 40 epochs, and pre-\ntrain +{M|T}LM models for 20 epochs. For POS\ntagging, we follow a simlar pattern, training +MLM\nand +TLM for 80 epochs, and +{M|T}LM for 40\nepochs.\nVocabulary Extension To extend the vocabulary\nof XLM-R, we implement the process of Wang et al.\n(2020). We denote this as +Extend. For each tar-\nget language, we train a new SentencePiece (Kudo\nand Richardson, 2018) tokenizer on the Bible of\nthat language with a maximum vocabulary size of\n30,000.3 To prevent adding duplicates, we ﬁlter\nout any subword already present in the vocabulary\nof XLM-R. We then add additional pieces repre-\nsenting these new subwords into the tokenizer of\nXLM-R, and increase XLM-R’s embedding matrix\naccordingly using a random initialization. Finally,\nwe train the embeddings using MLM on the Bible.\nFor NER, we train+Extend models for 40 epochs,\nand for POS tagging, we train for 80 epochs.\n3We note that for many languages, the tokenizer cannot\ncreate the full 30,000 subwords due to the limited corpus size.\n4559\nAdapters For adapters, we largely follow the full\nMAD-X framework (Pfeiffer et al., 2020b), using\nlanguage, invertible, and task adapters. This is\ndenoted as +Adapters. To train task adapters,\nwe download language and invertible adapters for\nthe source language from AdapterHub (Pfeiffer\net al., 2020a). We train a single task adapter for\neach task, and use it across all languages. We train\nlanguage and invertible adapters for each target\nlanguage by training on the target Bible with an\nMLM objective. As before, for NER we train for\n40 epochs, and for POS we train for 80 epochs.\n3.3 Hyperparameters and Training Details\nFor ﬁnetuning, we train using 1 Nvidia V100 32GB\nGPU, and use an additional GPU for adaptation\nmethods. Experiments for NER and POS take\naround 1 and 2 hours respectively, totalling to 165\ntotal training hours, and 21.38 kgCO 2eq emitted\n(Lacoste et al., 2019). All experiments are run us-\ning the Huggingface Transformers library (Wolf\net al., 2020). We limit sequence lengths to 256\ntokens.\nWe select initial hyperparameters for ﬁnetuning\nby using the English POS development set. We\nthen ﬁx all hyperparameters other than the number\nof epochs, which we tune using the 3 languages\nwhich have development sets, Ancient Greek, Mal-\ntese, and Wolof. We do not use early stopping. For\nour ﬁnal results, we ﬁnetune for 5 epochs with a\nbatch size of 32, and a learning rate of 2e-5. We\nuse the same hyperparameters for both tasks.\nFor each task and adaptation approach, we\nsearch over {10, 20, 40, 80 } epochs, and select\nthe epoch which gives the highest average perfor-\nmance across the development languages. We use\nthe same languages as above for POS. For NER\nwe use 4 languages with varying baseline perfor-\nmances: Bashkir, Kinyarwanda, Maltese, and Scots.\nWe pretrain with a learning rate of 2e-5 and a batch\nsize of 32, except for +Adapters, for which we\nuse a learning rate of 1e-4 (Pfeiffer et al., 2020b).\n4 Results\nWe present results for NER and POS tagging in Ta-\nbles 2 and 3, respectively. We additionally provide\nplots of the methods’ performances as compared to\nthe XLM-R baseline in Figures 2 and 3, showing\nperformance trends for each model.\nNER We ﬁnd that methods based on our most\nstraightforward approach, continued pretraining\nLang. XLM-R +MLM +TLM +{M|T}LM +Extend +Adapters\nace 31.95 38.10 37.29 38.06 37.54 33.56\narz 49.80 54.05 51.94 53.33 44.53 36.07\nbak 30.34 36.10 32.28 37.99 29.97 33.80\nceb 51.64 53.48 54.12 52.90 49.31 52.51\nche 14.84 16.26 15.08 13.72 14.47 18.93\nchv 46.90 64.34 66.67 59.23 48.78 54.36\ncrh 39.27 36.56 43.77 36.69 43.01 32.28\nhak 31.36 36.07 43.95 40.36 27.53 28.67\nibo 44.88 50.19 48.06 50.39 43.48 42.96\nilo 56.25 61.47 63.89 66.06 61.95 52.63\nkin 57.39 61.21 60.87 56.54 48.95 56.13\nmhr 45.74 48.39 46.29 43.44 36.78 40.75\nmin 44.13 42.91 46.76 41.43 42.49 41.70\nmlt 48.80 60.00 62.84 58.48 45.67 35.26\nmri 11.95 31.93 48.28 28.27 20.89 14.74\nnds 62.75 63.37 70.88 68.53 64.63 59.21\nory 31.49 25.64 24.24 28.95 22.13 25.44\nsco 76.40 74.51 73.56 74.42 74.90 65.87\ntat 36.63 51.52 50.23 50.56 33.78 43.70\ntgk 22.92 32.68 36.59 33.98 35.65 36.86\nwar 57.87 63.11 70.22 64.00 65.79 64.32\nyor 52.91 38.79 36.67 35.29 41.41 33.87\nAvg. 43.01 47.30 49.29 46.94 42.44 41.07\n∆Avg. 0.00 4.29 6.29 3.93 -0.57 -1.94\nTable 2: F1 score for all models on NER.\n(+MLM, +TLM, +{M|T}LM), perform best, with\n3.93 to 6.29 F1 improvement over XLM-R. Both\n+Extend and +Adapters obtain a lower aver-\nage F1 than the XLM-R baseline, which shows that\nthey are not a good choice in our setting: either\nthe size or the domain of the Bible causes them\nto perform poorly. Focusing on the script of the\ntarget language (cf. Table 1), the average perfor-\nmance gain across all models is higher for Cyrillic\nlanguages than for Latin languages. Therefore,\nin relation to the source language script, perfor-\nmance gain is higher for target languages with a\nmore distant script from the source. When consid-\nering approaches which introduce new parameters,\n+Extend and +Adapters, performance only in-\ncreases for Cyrillic languages and decreases for\nall others. However, when considering continued\npretraining approaches, we ﬁnd a performance in-\ncrease for all scripts.\nLooking at Figure 2, we see that the lower the\nbaseline F1, the larger the improvement of the adap-\ntion methods on downstream performance, with all\nmethods increasing performance for the language\nfor which the baseline is weakest. As baseline per-\nformance increases, the beneﬁt provided by these\nmethods diminishes, and all methods underperform\nthe baseline for Scots, the language with the high-\nest baseline performance. We hypothesize that at\n4560\nFigure 2: NER results (F1). Trendlines are created using linear regression.\nthis point the content of the Bible offers little to no\nextra knowledge for these languages compared to\nthe existing knowledge in the pretraining data.\nPOS Tagging Our POS tagging results largely\nfollow the same trend as those for NER, with con-\ntinued pretraining methods achieving the highest\nincrease in performance: between 15.81 and 17.61\npoints. Also following NER and as shown in Fig-\nure 3, the largest performance gain can be seen for\nlanguages with a low baseline performance, and,\nas the latter increases, the beneﬁts obtained from\nadaptation become smaller. However, unlike for\nNER, all methods show a net increase in perfor-\nmance, with +Adapters, the lowest performing\nadaptation model, achieving a gain of 9.01 points.\nWe hypothesize that a likely reason for this is the\ndomain and style of the Bible. While it may be\ntoo restrictive to signiﬁcantly boost downstream\nNER performance, it is still a linguistically rich re-\nsource for POS tagging, a task that is less sensitive\nto domain in general.\nAdditionally, there is a notable outlier language,\nCoptic, on which no model performs better than\nrandom choice (which corresponds to 6% accu-\nracy). This is because the script of this language\nis almost completely unseen to XLM-R, and prac-\ntically all non-whitespace subwords map to the\nunknown token: of the 50% of non-whitespace to-\nkens, 95% are unknown. While +Extend solves\nthis issue, we believe that for a language with a\ncompletely unseen script the Bible is not enough\nto learn representations which can be used in a\nzero-shot setting.\nLang. XLM-R +MLM +TLM +{M|T}LM +Extend +Adapters\nbam 32.44 60.59 60.91 63.13 53.40 54.74\ncop 4.31 4.02 4.03 4.03 4.35 3.70\nglv 33.12 59.78 57.91 59.05 43.43 50.88\ngrc 53.79 58.07 55.42 54.21 52.35 34.86\ngsw 47.78 61.98 59.14 58.38 56.72 61.70\nmag 51.09 60.77 57.30 58.52 54.57 55.81\nmlt 30.18 59.60 56.00 53.95 50.50 44.17\nmyv 46.62 66.63 68.48 66.55 58.62 57.05\nwol 37.97 65.52 62.97 59.30 55.56 54.52\nyor 31.34 48.54 49.40 48.31 47.71 41.29\nAvg. 36.86 54.55 53.16 52.54 47.72 45.875\n∆Avg. 0.00 17.69 16.29 15.68 10.86 9.01\nTable 3: POS tagging accuracy.\n5 Case Study\nAs previously stated, using the Bible as the cor-\npus for adaptation is limiting in two ways: the ex-\ntremely restricted domain as well as the small size.\nTo separate the effects of these two aspects, we\nrepeat our experiments with a different set of data.\nWe sample sentences from the Wikipedia of each\ntarget language to simulate a corpus of similar size\nto the Bible which is not restricted to the Bible’s\ndomain or content. To further minimize the effect\n4561\nFigure 3: POS results (Accuracy). Trendlines are created by ﬁtting a 2nd order, least squares polynomial.\nof domain, we focus solely on NER, such that the\ndomain of the data is precisely that of the target\ntask. Additionally, we seek to further investigate\nthe effect on the downstream performance gains of\nthese adaptation methods when the source language\nis more similar to the target language. To this end,\nwe focus our case study on three languages written\nin Cyrillic: Bashkir, Chechen, and Chuvash. We\nbreak up the case study into 3 settings, depending\non the data used. In the ﬁrst setting, we change\nthe language of our labeled training data from En-\nglish to Russian. While Russian is not necessarily\nsimilar to the target languages or mutually intelligi-\nble, we consider it to be more similar than English;\nRussian is written in the same script as the target\nlanguages, and there is a greater likelihood for lexi-\ncal overlap and the existence of loanwords. In the\nsecond setting, we pretrain using Wikipedia and\nin the third setting we use both Wikipedia data as\nwell as labeled Russian data.\nTo create our Wikipedia training data, we ex-\ntract sentences with WikiExtractor (Attardi, 2015)\nand split them with Moses SentenceSplitter (Koehn\net al., 2007). To create a comparable training set\nfor each language, we ﬁrst calculate the total num-\nber of subword tokens found in the New Testament,\nand sample sentences from Wikipedia until we have\nan equivalent amount. In the setting where we use\ndata from the New Testament and labeled Russian\ndata, for +TLM and +{M|T}LM we additionally\nsubstitute the English Bible with the Russian Bible.\nWhen using Wikipedia, we omit results for +TLM\nand +{M|T}LM, as they rely on a parallel corpus.\nSetting Model bak che chv Avg.\nB-E XLM-R 30.34 14.84 46.90 30.69\nBest 37.99 16.26 66.67 40.30\nB-R\nXLM-R 53.84 43.94 51.16 49.65\n+MLM 58.46 38.67 55.09 50.74\n+TLM 53.58 27.78 50.96 44.10\n+{M|T}LM 57.99 31.64 57.14 48.92\n+Extend 39.86 27.16 46.32 37.78\n+Adapters 48.03 21.64 36.11 35.26\nW-E\n+MLM 43.12 18.18 74.82 45.37\n+Extend 30.26 20.93 35.62 28.94\n+Adapters 41.88 41.15 71.74 51.59\nW-R\n+MLM 61.19 55.89 67.42 61.50\n+Extend 36.84 44.08 35.46 38.79\n+Adapters 56.39 28.65 73.12 52.72\nTable 4: Case study: Cyrillic NER (F1). Setting de-\nscribes the source of data for adaptation, either the\n(B)ible or (W)ikipedia, as well as the language of the\nﬁnetuning data, (E)nglish or (R)ussian.\n5.1 Results\nWe present the results of our case study in Table\n4. In the sections below, we refer to case study\nsettings as they are described in the table caption.\nEffects of the Finetuning Language We ﬁnd\nthat using Russian as the source language (the “Rus-\nsian baseline”; B-R w/ XLM-R) increases perfor-\nmance over the English baseline (B-E w/ XLM-R)\nby 18.96 F1. Interestingly, all of the adaptation\nmethods utilizing the Bible do poorly in this set-\n4562\nting (B-R), with +MLM only improving over the\nRussian baseline by 1.09 F1, and all other methods\ndecreasing performance. We hypothesize that when\nadaptation data is limited in domain, as the source\nlanguage approaches the target language in similar-\nity, the language adaptation is mainly done in the\nﬁnetuning step, and any performance gain from the\nunlabeled data is minimized. This is supported by\nthe previous NER results, where we ﬁnd that, when\nusing English as the source language, the adapta-\ntion methods lead to higher average performance\ngain over the baseline for Cyrillic languages, i.e.,\nthe more distant languages, as opposed to Latin lan-\nguages. The adaptation methods show a larger im-\nprovement when switching to Wikipedia data (W-\nR), with +MLM improving performance by 11.85\nF1 over the Russian baseline. Finally, the perfor-\nmance of +Extend when using Russian labeled\ndata is similar on average regardless of the adap-\ntation data (B-R, W-R), but noticeably improves\nover the setting which uses Wikipedia and English\nlabeled data.\nEffects of the Domain Used for Adaptation\nFixing English as the source language and chang-\ning the pretraining domain from the Bible to\nWikipedia (W-E) yields strong improvements, with\n+Adapters improving over the English base-\nline by 20.9 F1 and +MLM improving by 14.68\nF1. However, we note that, while the average of\n+Adapters is higher than that of +MLM, this is\ndue to higher performance on only a single lan-\nguage. When compared to the best performing\npretraining methods that use the Bible (B-E), these\nmethods improve by 11.29 F1 and 5.30 F1 respec-\ntively. When using both Wikipedia and Russian\ndata, we see the highest overall performance, and\n+MLM increases over the English baseline by 30.81\nF1 and the Russian baseline by 11.85 F1.\n6 Limitations\nOne limitation of this work – and other works\nwhich involve a high number of languages – is task\nselection. While part-of-speech tagging and named\nentity recognition 4 are important, they are both\nlow-level tasks largely based on sentence structure,\nwith no requirement for higher levels of reasoning,\nunlike tasks such as question answering or natural\nlanguage inference. While XTREME (Hu et al.,\n4We also note that the WikiANN labels are computer gen-\nerated and may suffer from lower recall when compared to\nhand-annotated datasets.\n2020) is a great, diverse benchmark covering these\nhigher level tasks, the number of languages is still\nlimited to only 40 languages, all of which have\nWikipedia data available. Extending these bench-\nmarks to truly low resource languages by introduc-\ning datasets for these tasks will further motivate\nresearch on these languages, and provide a more\ncomprehensive evaluation for their progress.\nAdditionally, while the Bible is currently avail-\nable in some form for 1611 languages, the available\ntext for certain languages may be different in terms\nof quantity and quality from the Bible text we use in\nour experiments. Therefore, although we make no\nlanguage-speciﬁc assumptions, our ﬁndings may\nnot fully generalize to all 1611 languages due to\nthese factors. Furthermore, this work focuses on\nanalyzing the effects of adaptation methods for\nonly a single multilingual transformer model. Al-\nthough we make no model-speciﬁc assumptions in\nour methods, the set of unseen languages differs\nfrom model to model. Moreover, although we show\nimprovements for the two tasks, we do not claim to\nhave state-of-the-art results. In a low-resource set-\nting, the best performance is often achieved through\ntask-speciﬁc models. Similarly, translation-based\napproaches, as well as few-shot learning may offer\nadditional beneﬁts over a zero-shot setting. We\nalso do not perform an extensive analysis of the tar-\nget languages, or an analysis of the selected source\nlanguage for ﬁnetuning. A better linguistic under-\nstanding of the languages in question would allow\nfor a better selection of source language, as well as\nthe ability to leverage linguistic features potentially\nleading to better results.\nFinally, by using a PMM, we inherit all of that\nmodel’s biases. The biases captured by word em-\nbeddings are well known, and recent work has\nshown that contextual models are not free of biases\neither (Caliskan et al., 2017; Kurita et al., 2019).\nThe use of the Bible, and religious texts in general,\nmay further introduce additional biases. Last, we\nacknowledge the environmental impact from the\ntraining of models on the scale of XLM-R (Strubell\net al., 2019).\n7 Conclusion\nIn this work, we evaluate the performance of contin-\nued pretraining, vocabulary extension, and adapters\nfor unseen languages of XLM-R in a realistic low-\nresource setting. Using only the New Testament,\nwe show that continued pretraining is the best per-\n4563\nforming adaptation approach, leading to gains of\n6.29 F1 on NER and 17.69% accuracy on POS tag-\nging. We therefore conclude that the Bible can be\na valuable resource for adapting PMMs to unseen\nlanguages, especially when no other data exists.\nFurthermore, we conduct a case study on three\nlanguages written in Cyrillic script. Changing the\nsource language to one more similar to the target\nlanguage reduces the effect of adaptation, but the\nperformance of the adaptation methods relative to\neach other is preserved. Changing the domain of\nthe adaptation data to one more similar to the tar-\nget task while keeping its size constant improves\nperformance.\nAcknowledgments\nWe would like to thank the ACL reviewers for\ntheir constructive and insightful feedback as well\nas Yoshinari Fujinuma, St´ephane Aroca-Ouellette,\nand other members of the CU Boulder’s NALA\nGroup for their advice and help.\nReferences\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A. Smith.\n2016. Massively multilingual word embeddings.\nArXiv, abs/1602.01925.\nM. Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In ACL.\nM. Artetxe and Holger Schwenk. 2019. Massively mul-\ntilingual sentence embeddings for zero-shot cross-\nlingual transfer and beyond. Transactions of the As-\nsociation for Computational Linguistics, 7:597–610.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2289–2294, Austin, Texas. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 451–462,\nVancouver, Canada. Association for Computational\nLinguistics.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nM Saiful Bari, Shaﬁq R. Joty, and Prathyusha Jwalapu-\nram. 2020. Zero-resource cross-lingual named en-\ntity recognition. In AAAI.\nAkash Bharadwaj, David Mortensen, Chris Dyer, and\nJaime Carbonell. 2016. Phonologically aware neural\nmodel for named entity recognition in low resource\ntransfer settings. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1462–1472, Austin, Texas. Asso-\nciation for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual bert, a small corpus, and a\nsmall treebank. ArXiv, abs/2009.14124.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, E. Grave, Myle Ott, Luke Zettlemoyer, and\nVeselin Stoyanov. 2020. Unsupervised cross-lingual\nrepresentation learning at scale. In ACL.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv ´e J´egou. 2017.\nWord translation without parallel data. arXiv\npreprint arXiv:1710.04087.\nRamy Eskander, Smaranda Muresan, and Michael\nCollins. 2020. Unsupervised cross-lingual part-of-\nspeech tagging for truly low-resource scenarios. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4820–4831, Online. Association for Computa-\ntional Linguistics.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings\nof the International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks.\nArXiv, abs/2004.10964.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799,\nLong Beach, California, USA. PMLR.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. CoRR, abs/2003.11080.\n4564\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov,\nHerv´e J ´egou, and Edouard Grave. 2018. Loss in\ntranslation: Learning bilingual word mapping with a\nretrieval criterion. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180, Prague, Czech Republic. As-\nsociation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations.\nAlexandre Lacoste, Alexandra Luccioni, Victor\nSchmidt, and Thomas Dandres. 2019. Quantifying\nthe carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In NeurIPS.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2017. Unsupervised ma-\nchine translation using monolingual corpora only.\narXiv preprint arXiv:1711.00043.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaˇs. 2020. From zero to hero: On the lim-\nitations of zero-shot language transfer with multilin-\ngual Transformers. pages 4483–4499.\nStephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.\nCheap translation for cross-lingual named entity\nrecognition. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2536–2545, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nArya D. McCarthy, Rachel Wicks, Dylan Lewis, Aaron\nMueller, Winston Wu, Oliver Adams, Garrett Nico-\nlai, Matt Post, and David Yarowsky. 2020. The\nJohns Hopkins University Bible corpus: 1600+\ntongues for typological exploration. In Proceed-\nings of the 12th Language Resources and Evaluation\nConference, pages 2884–2892, Marseille, France.\nEuropean Language Resources Association.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, volume 26, pages 3111–3119. Curran As-\nsociates, Inc.\nB. Muller, Antonis Anastasopoulos, Benoˆıt Sagot, and\nDjam´e Seddah. 2020. When being unseen from\nmbert is just the beginning: Handling new lan-\nguages with multilingual language models. ArXiv,\nabs/2010.12858.\nJoakim Nivre, Marie-Catherine de Marneffe, F. Gin-\nter, Jan Hajivc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nD. Zeman. 2020. Universal dependencies v2: An ev-\nergrowing multilingual treebank collection. ArXiv,\nabs/2004.10643.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nJonas Pfeiffer, Andreas R ¨uckl´e, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterhub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 46–54.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. Mad-x: An adapter-based\nframework for multi-task cross-lingual transfer. In\nEMNLP.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 101–\n108, Online. Association for Computational Linguis-\ntics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\n4565\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems, volume 30, pages 506–\n516. Curran Associates, Inc.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual word embedding\nmodels. Journal of Artiﬁcial Intelligence Research ,\n65:569–631.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in nlp.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and\nDan Roth. 2020. Extending multilingual BERT to\nlow-resource languages. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 2649–2656, Online. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all lan-\nguages created equal in multilingual bert? In\nRepL4NLP@ACL.\nJiateng Xie, Zhilin Yang, Graham Neubig, Noah A.\nSmith, and Jaime Carbonell. 2018. Neural cross-\nlingual named entity recognition with minimal re-\nsources. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 369–379, Brussels, Belgium. Association for\nComputational Linguistics.\nDavid Yarowsky, Grace Ngai, and Richard Wicen-\ntowski. 2001. Inducing multilingual text analysis\ntools via robust projection across aligned corpora. In\nProceedings of the First International Conference on\nHuman Language Technology Research.\nRong Zhang, Revanth Gangi Reddy, Md Arafat Sul-\ntan, Vittorio Castelli, Anthony Ferritto, Radu Flo-\nrian, Efsun Sarioglu Kayi, Salim Roukos, Avi Sil,\nand Todd Ward. 2020. Multi-stage pre-training for\nlow-resource domain adaptation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5461–\n5468, Online. Association for Computational Lin-\nguistics.\n4566\nA Appendix\nIn Table 5, we provide the number of subwords\ncreated by the XLM-R tokenizer from the New\nTestament of each target language, in addition to\nthe speciﬁc version of the Bible we use, as found\nin the JHU Bible Corpus. In Table 7 and 6 we\nprovide the relative performance of all adaptation\nmethods as compared to baseline performance.\nLang. Bible Version Bible Size (thousands)\nace ace-x-bible-ace-v1 536k\narz arz-x-bible-arz-v1 828k\nbak bak-BAKIBT 453k\nbam bam-x-bible-bam-v1 429k\nceb ceb-x-bible-bugna2009-v1 384k\nche che-x-bible-che-v1 523k\nchv chv-CHVIBT 519k\ncop cop-x-bible-bohairic-v1 259k\ncrh crh-CRHIBT 347k\neng eng-x-bible-kingjames-v1 461k\nglv glv-x-bible-glv-v1 196k\ngrc grc-x-bible-textusreceptusV AR1-v1 322k\ngsw gsw-x-bible-alemannisch-v1 351k\nhak hak-x-bible-hak-v1 598k\nibo ibo-x-bible-ibo-v1 458k\nilo ilo-x-bible-ilo-v1 378k\nkin kin-x-bible-bird-youversion-v1 344k\nmag mag-MAGSSI 388k\nmhr mhr-x-bible-mhr-v1 398k\nmin min-x-bible-min-v1 505k\nmlt mlt-x-bible-mlt-v1 389k\nmri mri-x-bible-mri-v1 411k\nmyv myv-x-bible-myv-v1 463k\nnds nds-x-bible-nds-v1 333k\nory ory-x-bible-ory-v1 386k\nrus rus-x-bible-kulakov-v1 283k\nsco sco-x-bible-sco-v1 30k\ntat tat-TTRIBT 438k\ntgk tgk-TGKIBT 233k\nwar war-x-bible-war-v1 401k\nwol wol-x-bible-wol-v1 383k\nyor yor-x-bible-yor-v1 450k\nAvg. 402k\nTable 5: Size of the New Testaments of each language,\nalong with the speciﬁc Bible version. Size is calcu-\nlated in subword units using the base XLM-Roberta to-\nkenizer.\n4567\nLang.XLM-R∆MLM∆TLM∆{M|T}LM∆Extend∆Adapters\nbam 32.44 28.15 28.47 30.69 20.96 22.30\ncop 4.31 -0.29 -0.28 -0.28 0.04 -0.61\nglv 33.12 26.66 24.79 25.93 10.31 17.76\ngrc 53.79 4.28 1.63 0.42 -1.44 -18.93\ngsw 47.78 14.20 11.36 10.60 8.94 13.92\nmag 51.09 9.68 6.21 7.43 3.48 4.72\nmlt 30.18 29.42 25.82 23.77 20.32 13.99\nmyv 46.62 20.01 21.86 19.93 12.00 10.43\nwol 37.97 27.55 25.00 21.33 17.59 16.55\nyor 31.34 17.20 18.06 16.97 16.37 9.95\nAvg. 36.86 17.69 16.29 15.68 10.86 9.01\nTable 6: Accuracy deltas for POS tagging compared to baseline\nLang.XLM-R∆MLM∆TLM∆{M|T}LM∆Extend∆Adapters\nace 31.95 6.15 5.34 6.11 5.59 1.61\narz 49.80 4.25 2.14 3.53 -5.27 -13.73\nbak 30.34 5.76 1.94 7.65 -0.37 3.46\nceb 51.64 1.84 2.48 1.26 -2.33 0.87\nche 14.84 1.42 0.24 -1.12 -0.37 4.09\nchv 46.90 17.44 19.77 12.33 1.88 7.46\ncrh 39.27 -2.71 4.50 -2.58 3.74 -6.99\nhak 31.36 4.71 12.59 9.00 -3.83 -2.69\nibo 44.88 5.31 3.18 5.51 -1.40 -1.92\nilo 56.25 5.22 7.64 9.81 5.70 -3.62\nkin 57.39 3.82 3.48 -0.85 -8.44 -1.26\nmhr 45.74 2.65 0.55 -2.30 -8.96 -4.99\nmin 44.13 -1.22 2.63 -2.70 -1.64 -2.43\nmlt 48.80 11.20 14.04 9.68 -3.13 -13.54\nmri 11.95 19.98 36.33 16.32 8.94 2.79\nnds 62.75 0.62 8.13 5.78 1.88 -3.54\nory 31.49 -5.85 -7.25 -2.54 -9.36 -6.05\nsco 76.40 -1.89 -2.84 -1.98 -1.50 -10.53\ntat 36.63 14.89 13.60 13.93 -2.85 7.07\ntgk 22.92 9.76 13.67 11.06 12.73 13.94\nwar 57.87 5.24 12.35 6.13 7.92 6.45\nyor 52.91 -14.12 -16.24 -17.62 -11.50 -19.04\nAvg. 43.01 4.29 6.29 3.93 -0.57 -1.94\nTable 7: F1 deltas for NER compared to baseline",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8347600102424622
    },
    {
      "name": "Natural language processing",
      "score": 0.600136399269104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5506472587585449
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5010905265808105
    },
    {
      "name": "Language model",
      "score": 0.4958692491054535
    },
    {
      "name": "Mathematics",
      "score": 0.07071718573570251
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802236040",
      "name": "University of Colorado System",
      "country": "US"
    }
  ],
  "cited_by": 6
}