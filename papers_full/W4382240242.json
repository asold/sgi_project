{
    "title": "Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method",
    "url": "https://openalex.org/W4382240242",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1977132994",
            "name": "Tao Wang",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A2324159050",
            "name": "Kaihao Zhang",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2783359674",
            "name": "Tianrun Shen",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A2251619090",
            "name": "Wenhan Luo",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2107541040",
            "name": "Bjorn Stenger",
            "affiliations": [
                "Rakuten (Japan)",
                "Institut Teknologi PLN"
            ]
        },
        {
            "id": "https://openalex.org/A2097484344",
            "name": "Tong Lu",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A1977132994",
            "name": "Tao Wang",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A2324159050",
            "name": "Kaihao Zhang",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2783359674",
            "name": "Tianrun Shen",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A2251619090",
            "name": "Wenhan Luo",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2107541040",
            "name": "Bjorn Stenger",
            "affiliations": [
                "Rakuten (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2097484344",
            "name": "Tong Lu",
            "affiliations": [
                "Nanjing University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2025328853",
        "https://openalex.org/W2783573276",
        "https://openalex.org/W6750972829",
        "https://openalex.org/W3034552680",
        "https://openalex.org/W2063784584",
        "https://openalex.org/W2412926690",
        "https://openalex.org/W2468596194",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W3002804594",
        "https://openalex.org/W2566376500",
        "https://openalex.org/W2950335850",
        "https://openalex.org/W2139375301",
        "https://openalex.org/W3134649899",
        "https://openalex.org/W3107113662",
        "https://openalex.org/W3111901135",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2254039850",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W2128926607",
        "https://openalex.org/W6644873565",
        "https://openalex.org/W2054814429",
        "https://openalex.org/W3166368936",
        "https://openalex.org/W6754146604",
        "https://openalex.org/W3212228063",
        "https://openalex.org/W3202580978",
        "https://openalex.org/W1600176996",
        "https://openalex.org/W2943838036",
        "https://openalex.org/W2767030724",
        "https://openalex.org/W3119525307",
        "https://openalex.org/W4225672218",
        "https://openalex.org/W4230472795",
        "https://openalex.org/W2949187370",
        "https://openalex.org/W2150721269",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3013338555",
        "https://openalex.org/W1978461177",
        "https://openalex.org/W3035731588",
        "https://openalex.org/W3174792937",
        "https://openalex.org/W3121661546",
        "https://openalex.org/W4312812783",
        "https://openalex.org/W2767618624",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4288359455",
        "https://openalex.org/W4394666973"
    ],
    "abstract": "As the quality of optical sensors improves, there is a need for processing large-scale images. In particular, the ability of devices to capture ultra-high definition (UHD) images and video places new demands on the image processing pipeline. In this paper, we consider the task of low-light image enhancement (LLIE) and introduce a large-scale database consisting of images at 4K and 8K resolution. We conduct systematic benchmarking studies and provide a comparison of current LLIE algorithms. As a second contribution, we introduce LLFormer, a transformer-based low-light enhancement method. The core components of LLFormer are the axis-based multi-head self-attention and cross-layer attention fusion block, which significantly reduces the linear complexity. Extensive experiments on the new dataset and existing public datasets show that LLFormer outperforms state-of-the-art methods. We also show that employing existing LLIE methods trained on our benchmark as a pre-processing step significantly improves the performance of downstream tasks, e.g., face detection in low-light conditions. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLFormer.",
    "full_text": "Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and\nTransformer-Based Method\nTao Wang1, Kaihao Zhang2, Tianrun Shen1, Wenhan Luo3*, Bjorn Stenger4, Tong Lu1*\n1State Key Lab for Novel Software Technology, Nanjing University, China\n2 Australian National University, Australia\n3 Shenzhen Campus of Sun Yat-sen University, China\n4 Rakuten Institute of Technology, Japan\n{taowangzj, super.khzhang, whluo.china}@gmail.com, tiruns@yeah.net, bjorn@cantab.net, lutong@nju.edu.cn\nAbstract\nAs the quality of optical sensors improves, there is a need\nfor processing large-scale images. In particular, the ability of\ndevices to capture ultra-high definition (UHD) images and\nvideo places new demands on the image processing pipeline.\nIn this paper, we consider the task of low-light image enhance-\nment (LLIE) and introduce a large-scale database consisting of\nimages at 4K and 8K resolution. We conduct systematic bench-\nmarking studies and provide a comparison of current LLIE\nalgorithms. As a second contribution, we introduce LLFormer,\na transformer-based low-light enhancement method. The core\ncomponents of LLFormer are the axis-based multi-head self-\nattention and cross-layer attention fusion block, which sig-\nnificantly reduces the linear complexity. Extensive experi-\nments on the new dataset and existing public datasets show\nthat LLFormer outperforms state-of-the-art methods. We also\nshow that employing existing LLIE methods trained on our\nbenchmark as a pre-processing step significantly improves\nthe performance of downstream tasks, e.g., face detection in\nlow-light conditions. The source code and pre-trained models\nare available at https://github.com/TaoWangzj/LLFormer.\nIntroduction\nImages taken in low-light conditions typically show notice-\nable degradation, such as poor visibility, low contrast, and\nhigh noise levels. To alleviate these effects, a number of\nlow-light image enhancement (LLIE) methods have been\nproposed to transform a given low-light image into a high-\nquality image with appropriate brightness. Traditional LLIE\nmethods are mainly based on image priors or physical models\nfrom other tasks, such as histogram equalization-based meth-\nods (Kim 1997; Stark 2000), retinex-based methods (Kimmel\net al. 2003; Wang et al. 2014) and dehazing-based meth-\nods (Dong et al. 2011; Zhang et al. 2012). Recently, many\nlearning-based LLIE methods have been introduced, making\nuse of large-scale synthetic datasets and achieving significant\nimprovements in terms of performance and speed (Wei et al.\n2018; Guo et al. 2020; Lim and Kim 2020; Jiang et al. 2021;\nLi, Guo, and Loy 2021; Liu et al. 2021b).\nMost existing datasets, e.g., LOL (Wei et al. 2018) and\nSID (Chen et al. 2018), consist of lower resolution images\n*Corresponding authors.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(1K or less). Thus, LLIE methods trained on these datasets\nare naturally constrained to low-resolution images. Sensors\non modern mobile devices are able to capture images of reso-\nlutions of 4K or 8K, creating a need for algorithms designed\nfor processing Ultra-High-Definition (UHD) images. It is dif-\nficult for existing LLIE methods to simultaneously reconcile\ninference efficiency and visual enhancement on UHD images.\nIn this paper, we focus on the task of Ultra-High Definition\nLow-Light Image Enhancement (UHD-LLIE). We first build\na large-scale benchmark dataset containing UHD images in\nLOw-Light conditions (UHD-LOL) to explore and evaluate\nimage enhancement algorithms. UHD-LOL includes two sub-\nsets, UHD-LOL4K and UHD-LOL8K, containing 4K and\n8K-resolution images, respectively. The UHD-LOL4K subset\ncontains\n8, 099 image pairs, 5, 999 for training and 2, 100 for\ntesting. The subset of UHD-LOL8K includes 2, 966 image\npairs, 2, 029 for training and 937 for testing. Example 4K\nand 8K low-light images are shown in Fig. 1.\nUsing this dataset, we conduct extensive benchmarking\nstudies to compare existing LLIE methods and highlight\nsome shortcomings in the UHD setting. We propose a novel\ntransformer-based method named Low-Light Transformer-\nbased Network (LLFormer) for the UHD-LLIE task. LL-\nFormer is composed of two basic units, an efficient axis-based\ntransformer block and a cross-layer attention fusion block.\nWithin the axis-based transformer block, the axis-based self-\nattention unit performs the self-attention mechanism on the\nheight and width axes of features across the channel dimen-\nsion to capture non-local self-similarity and long-range de-\npendencies with less computational complexity. Moreover,\nafter the axis-based self-attention, we design a novel dual\ngated feed-forward network, which employs a dual gated\nmechanism to focus on useful features. The cross-layer atten-\ntion fusion Block learns attention weights across features in\ndifferent layers and adaptively fuses features with the learned\nweights to improve feature representation. The LLFormer\nadopts a hierarchical structure, which greatly alleviates the\ncomputational bottleneck for the UHD-LLIE task.\nThe contributions of this paper are summarized as fol-\nlows. (1) We build a benchmark dataset of 4K and 8K UHD\nimages, UHD-LOL, to explore and evaluate image enhance-\nment algorithms. To the best of our knowledge, this is the first\nlarge-scale UHD low-light image enhancement dataset in the\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2654\n(a) 4K images from UHD-LOL4K subset. (b) 8K images from UHD-LOL8K subset.\nFigure 1: Low-light images sampled from the proposed UHD-LOL dataset.\nliterature. (2) Based on UHD-LOL, we benchmark existing\nLLIE algorithms to show the performance and limitations of\nthese methods, offering new insights. (3) We propose a novel\ntransformer model, LLFormer, for the UHD-LLIE task. In\nboth quantitative and qualitative aspects, LLFormer achieves\nstate-of-the-art performance on the public LOL and MIT-\nAdobe FiveK datasets, and our UHD-LOL benchmark.\nRelated Work\nLow-Light Image Datasets. Lore et al.(Lore, Akintayo, and\nSarkar 2017) synthesized 422, 500 low-light image patches\nfrom 169 images. (Shen et al. 2017) created an LLIE dataset\nof 10, 000 image pairs. (Chen et al. 2018) built the See-in-\nthe-Dark (SID) dataset. It contains5, 094 short-exposure low-\nlight raw images and their corresponding long-exposure ones.\n(Cai, Gu, and Zhang 2018) synthesized the SICE dataset\nfrom 589 image sequences with multi-exposure image fusion\n(MEF) or a high dynamic range (HDR) algorithm. (Wei et al.\n2018) created the LOw-Light (LOL) dataset, which consists\nof 485 image pairs for training and 15 for testing. Based on\nthe LOL dataset, Liuet al.(Liu et al. 2021a) created VE-LOL-\nL for training and evaluating LLIE methods, which includes\n2, 100 images for training and400 for evaluation. MIT-Adobe\nFiveK (Bychkovsky et al. 2011) consists of 5, 000 images\ncaptured of various indoor and outdoor scenes.\nLow-Light Image Enhancement Methods. Data-driven\nmethods have been successfully applied to the LLIE task. For\nexample, RetinexNet in (Wei et al. 2018) combines Retinex\ntheory and deep CNNs in a unified end-to-end learning frame-\nwork. Recently, data-driven methods based on transformers\nhave been applied to low-level tasks: Uformer (Wang et al.\n2022) uses a modified Swin transformer block (Liu et al.\n2021c) to build a U-shaped network, showing good perfor-\nmance in image restoration. Restormer (Zamir et al. 2022) in-\ntroduces modifications of the transformer block for improved\nfeature aggregation for image restoration. While transformers\nwork well in many tasks, their potential for low-light image\nenhancement remains unexplored. In this work, we focus on\ndesigning a transformer for UHD LLIE.\nBenchmark and Methodology\nBenchmark Dataset\nWe create a new large-scale UHD-LLIE dataset called UHD-\nLOL to benchmark the performance of existing LLIE meth-\nods and explore the UHD-LLIE problem. UHD-LOL is com-\nposed of 4K images of 3, 840 × 2, 160 resolution and 8K\nimages of 7, 680 × 4, 320 resolution, respectively. To build\nthis dataset of image pairs, we use normal-light 4K and 8K\nimages from public data (Zhang et al. 2021). These UHD\nimages were crawled from the web and captured by vari-\nous devices. Images contain both indoor and outdoor scenes,\nincluding buildings, streets, people, animals, and natural land-\nscapes. We synthesize corresponding low-light images fol-\nlowing (Wei et al. 2018), which takes both the low-light degra-\ndation process and natural image statistics into consideration.\nSpecifically, we first generate three random variables X, Y ,\nZ, uniformly distributed in (0, 1). We use these variables\nto generate parameters provided by the Adobe Lightroom\nsoftware. The parameters include exposure (−5+5X2), high-\nlights (50 min{Y, 0.5} + 75), shadows (−100 min{Z, 0.5}),\nvibrance (−75 + 75X2), and whites ( 16(5 − 5X2)). The\nsynthesized low-light and normal-light images make up our\nUHD-LOL, which consists of two subsets: UHD-LOL4K\nand UHD-LOL8K. The UHD-LOL4K subset contains 8, 099\npairs of 4K low-light/normal-light images. Among them,\n5, 999 pairs of images are used for training and 2, 100 for\ntesting. The UHD-LOL8K subset includes 2, 966 pairs of\n8K low-light/normal-light images, which are split into 2, 029\npairs for training and 937 for testing. Example images are\nshown in Fig. 1.\nLLFormer Architecture\nAs illustrated in Fig. 2, the overall architecture of LLFormer\nis a hierarchical encoder-decoder structure. Given a low-\nlight image I ∈ RH×W×3, LLFormer first employs a 3 × 3\nconvolution as a projection layer to extract shallow feature\nF0 ∈ RH×W×C. Next, F0 is fed into three sequential trans-\nformer blocks to extract deeper features. More specifically,\nintermediate features outputted from transformer blocks\nare denoted as\nF1, F2, F3 ∈ RH×W×C. These features\nF1, F2, F3 pass through the proposed cross-layer attention\nfusion block to be aggregated and transformed into the en-\nhanced image features F4. Second, four stages in an encoder\nare used for deep feature extraction on F4. To be specific,\neach stage contains one downsampling layer and multiple\ntransformer blocks. From top to bottom stages, the number\nof transformer blocks increases. We use the pixel-unshuffle\noperation (Shi et al. 2016) to downscale the spatial size and\ndouble the channel number. Therefore, features in the i-th\nstage of the encoder can be denoted as Xi ∈ R\nH\n2i ×W\n2i ×2iC\nand i = 0, 1, 2, 3 corresponding to the four stages. Subse-\nquently, the low-resolution latent feature X3 passes through\na decoder which contains three stages and takes X3 as input\nand progressively restores the high-resolution representations.\nEach stage is composed of an upsampling layer and multiple\ntransformer blocks. Features in the i-th stage of decoder are\n2655\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlockW\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nTransformer\nBlock\nW\nW\nW\nTransformer\nBlock\nNormalization\nAttention Fusion \nBlockCross-layer\nDownSample\nUpSample\nW Connection with Weights\nConvolution\nConvolution\nDual Gated \nFeed-Forward Network\nDGFN\nNorm\nInput Output\n2023.3.29_v2\nHeight Axis Multi-\nhead Attention\nWidth Axis Multi-\nhead Attention\nNorm\nDGFN\nNorm\nAxis-based Multi head Self-Attention\nAxis-based Transformer Block\nFigure 2: LLFormer architecture. The core design of LLFormer includes an axis-based transformer block and a cross-layer\nattention fusion block. In the former, axis-based multi-head self-attention performs self-attention on the height and width axis\nacross the channel dimension sequentially to reduce the computational complexity, and a dual gated feed-forward network\nemploys a gated mechanism to focus more on useful features. The cross-layer attention fusion block learns the attention weights\nof features in different layers when fusing them.\ndenoted as X′\ni ∈ R\nH\n2i × H\n2i ×2i+1C, i= 0,1, 2. We apply the\npixel-shuffle operation (Shi et al. 2016) for upsampling. To\nalleviate the information loss in the encoder and for features\nto be well recovered in the decoder, we use the weighted\nskip connection with a 1 × 1 convolution for feature fusion\nbetween the encoder and decoder, which can flexibly adjust\nthe contributions of the features from encoder and decoder.\nThird, after the decoder, the deep feature F in turn passes\nthrough three transformer blocks and a cross-layer attention\nfusion block to generate the enhanced features for image re-\nconstruction. Finally, LLFormer applies a 3 × 3 convolution\non the enhanced features to yield the enhanced images ˆI. We\noptimize LLFormer using a smooth L1 loss (Girshick 2015).\nAxis-Based Transformer Block\nTransformers were shown to have advantages in modeling\nnon-local self-similarity and long-range dependencies com-\npared to CNNs. However, as discussed in (Vaswani et al.\n2017; Liu et al. 2021c), the computational cost of the stan-\ndard transformer is quadratic with respect to the spatial size\nof input feature maps (H × W). Moreover, it often becomes\ninfeasible to apply transformers to high-resolution images\nespecially UHD images. To address this problem, we propose\nan axis-based multi-head self-attention (A-MSA) mechanism\nin the transformer block. The computational complexity of\nA-MSA is linear in spatial size, which greatly reduces the\ncomputational complexity. Further, we introduce a dual gated\nmechanism in the plain transformer feed-forward network\nand propose the dual gated feed-forward network (DGFN)\nto capture more important information in features. We inte-\ngrate our A-MSA and DGFN with the plain transformer units\nto build the axis-based transformer block (ATB). As shown\nin Fig. 2, an ATB contains an A-MSA, a DGFN, and two\nnormalization layers. The formula of ATB is:\nF′ = A-MSA (LN (Fin)) +Fin,\nFout = DGFN (LN (F′)) +F′, (1)\nwhere Fin denotes the input of ATB. F′ and Fout are the\noutputs of A-MSA and DGFN, respectively. LN is the layer\nnormalization (Ba, Kiros, and Hinton 2016). In the following,\nwe provide details of A-MSA and DGFN.\nAxis-Based Multi-head Self-Attention. The computa-\ntional complexity of the standard self-attention is quadratic\nwith the resolution of input, i.e., O\n\u0000\nW2H2\u0001\nfor H × W\nfeature maps. Instead of computing self-attention globally,\nwe propose A-MSA, as illustrated in Fig. 2, to compute self-\nattention on the height and width axes across the channel\ndimension sequentially. Thanks to this operation, the com-\nplexity of our A-MSA is reduced to linear. Moreover, to\nalleviate the limitation of transformers in capturing local\ndependencies, we employ depth-wise convolutions to help\nA-MSA focus on the local context before computing a feature\nattention map (Zamir et al. 2022; Wang et al. 2022). Since\nthe mechanisms of height and width axis multi-head self-\nattention are similar, we thus only introduce the details of\nheight axis multi-head self-attention for ease of illustration.\nFor height axis multi-head attention, as shown in Fig. 3 (a),\ngiven feature X ∈ RH×W×C output from the normalization\nlayer, we at first apply1×1 convolutions to enhance the input\nfeature X, and use 3 × 3 depth-wise convolutions to obtain\nfeatures with enriched local information. Then, the output\nfeatures from 3×3 depth-wise convolutions are queryQ, key\n2656\n/gid00019\nReshape\nMatrix Multiplication\nElement-wise Multiplication\nElement-wise Sum\nK\nQ\n/gid00050\n/gid00030\nw\nw\nH\nc\nw\nH\nc\nH\nc\nw\nc\nH\nw\nVw\nH\nc\nc\nw\n/gid00018\nw\nc\nw\nc\nw\nw\nc\nw\nw\nc\nw\nc\n/gid00018\nw\nc\n/gid00050\n/gid00009\n/gid00030\n/gid00050\n/gid00030\nw\nc\nK\nQ\nV\n   Height Axis \nAttention Map\n   Width Axis \nAttention Map\nDW  DW  DW  \nDW  DW  DW  \n(a) Height Axis Multi-head Self-Attention (b) Width Axis Multi-head Self-Attention\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nDW  DW  \nGELU\nPath2Path1\nGELU\n/gid00019\n/gid00019\n/gid00019\n/gid00019\n/gid00019\n/gid00019\n/gid00019\n/gid00019\n/gid00019\n/gid00019\nReshape\nMatrix Multiplication\nElement-wise Multiplication\nSum\n   Input\n   Input\n/gid00001/gid00001/gid00001/gid00016/gid00048/gid00047/gid00043/gid00048/gid00047   Output\n(c) Dual Gated Feed-Forward Network\nFigure 3: The architecture of our axis-based multi-head self-attention and dual gated feed-forward network. From left to right,\nthe components are height axis multi-head attention, width axis multi-head attention, and dual gated feed-forward network.\nK, and value V, as Q = WQ\n3×3WQ\n1×1X,K = WK\n3×3WK\n1×1X\nand V = WV\n3×3WV\n1×1X, where W1×1 and W3×3 denote\n1 × 1 convolution and 3 × 3 depth-wise convolution, re-\nspectively. After that, the query and key are reshaped for\nconducting dot-product to generate height axis attention map\nA ∈ RH×H×W . To achieve multi-head self-attention, we\nsplit the reshaped ˆQ, ˆK and ˆV into k heads along the fea-\nture channel dimension respectively, as ˆQ = [ˆq1, . . . ,ˆqk],\nˆK =\nh\nˆk1, . . . ,ˆkk\ni\n, ˆV = [ˆv1, . . . ,ˆvk], where the dimension\nof each head is dk = C/k. The height axis multi-head self-\nattention for the j-th head can be formulated as:\nSA\n\u0010\nˆqj, ˆkj, ˆvj\n\u0011\n= ˆvjsoftmax\n\u0010\nˆqjˆkj/α\n\u0011\n, (2)\nwhere ˆqj ∈ RH×dk×W , ˆkj ∈ Rdk×H×W and ˆvj ∈\nRdk×H×W denote the j-th head of ˆQ, ˆK and ˆV, respectively.\nα is a scale factor. The output feature X′ can be obtained by:\nX′ = W1×1Concatk\nj=0\n\u0010\nSA\n\u0010\nˆqj, ˆkj, ˆvj\n\u0011\u0011\n, (3)\nwhere Concat represents the concatenation operation. Fi-\nnally, we reshape X′ to obtain the output feature Xout ∈\nRH×W×C of height axis multi-head attention. Xout is for-\nwarded to the width axis multi-head attention (see Fig. 3 (b))\nto compute self-attention along the width axis.\nDual Gated Feed-Forward Network. Previous work sug-\ngests that Feed-Forward Networks (FFN) demonstrate a limi-\ntation in capturing local context (Vaswani et al. 2017; Doso-\nvitskiy et al. 2021). For efficient feature transformations,\nwe introduce a dual gated mechanism and local information\nenhancement in FFN, and propose a novel dual gated feed-\nforward network (DGFN). As shown in Fig. 3 (c), for the dual\ngated mechanism, we first apply dual GELU and element-\nwise product in two parallel paths to filter the less informative\nfeatures and then fuse useful information from two paths with\nan element-wise sum. Further, we apply a 1 × 1 convolution\n(W1×1) and a 3 × 3 depth-wise convolution (W3×3) in each\npath to enrich the local information. Given Y ∈ RH×W×C\nas input, the complete DGFN is formulated as:\nDG = ϕ\n\u0000\nW1\n3×3W1\n1×1Y\n\u0001\n⊙ (W2\n3×3W2\n1×1Y)\n+ (W1\n3×3W1\n1×1Y) ⊙ ϕ\n\u0000\nW2\n3×3W2\n1×1Y\n\u0001\n,\nˆY = W1×1DG(Y) +Y,\n(4)\nwhere ˆY ∈ RH×W×C represents the output features, DG\ndenotes the dual gated mechanism, ⊙ is the element-wise\nmultiplication operation, and ϕ is the GELU activation func-\ntion.\nCross-Layer Attention Fusion Block\nRecent transformer-based methods adopt feature connections\nor skip connections to combine features from different lay-\ners (Zamir et al. 2022; Wang et al. 2022). However, these\noperations do not fully exploit dependencies across differ-\nent layers, limiting the representation capability. To address\nthis, we propose a novel cross-layer attention fusion block\n(CAFB), which adaptively fuses hierarchical features with\nlearnable correlations among different layers. The intuition\nbehind CAFB is that activations at different layers are a re-\nsponse to a specific class, and feature correlations can be\nadaptively learned using a self-attention mechanism.\nThe CAFB architecture is shown in Fig. 4. Given concate-\nnation features ( Fin ∈ RN×H×W×C) from N successive\nlayers (N = 3in the experiments), we first reshape Fin into\nˆFin with dimensions H × W × NC . Like self-attention in\nATB, we employ 1 × 1 convolutions to aggregate pixel-wise\ncross-channel context followed by 3 × 3 depth-wise convolu-\ntions to yield Q, K and V. We then reshape the query and\nkey into 2D matrices of dimensions N × HWC ( ˆQ) and\nHWC × N ( ˆK) to calculate the layer correlation attention\nmatrix A of size N × N. Finally, we multiply the reshaped\nvalue ˆV ∈ RHWC ×N by the attention matrix A with a scale\nfactor α, and add the input features Fin. The CAFB process\nis formulated as:\nˆFout = W1×1Layer_Attention ( ˆQ, ˆK, ˆV) +ˆFin,\nLayer_Attention ( ˆQ, ˆK, ˆV) = ˆVsoftmax( ˆQ ˆK/α),\n(5)\nwhere ˆFout is the output feature that focuses on informative\nlayers of the network. In practice, we place the proposed\nCAFB in the symmetric position of the head and tail in the\nnetwork, so that CAFB helps capture long-distance depen-\ndencies among hierarchical layers in both feature extraction\nand image reconstruction processes.\n2657\n/gid00019\nV\n/gid00019\n/gid00019\n/gid00019\nK\nQ\nSkip Connection\n    Correlation \nAttention Matrix\n/gid00019\nReshape\nMatrix Multiplication\nDepth-wise Convolution\nElement-wise Sum\nConvolution\nDW  DW  DW  \nInput Feature Out FeatureCross-layer Attention Fusion Block\nFeature From N Layers \n/gid00019\nFigure 4: The architecture of the proposed cross-layer attention fusion block.\nExperiments and Analysis\nImplementation Details\nThe LLFormer is trained on 128 × 128 patches with a batch\nsize of 12. For data augmentation, we adopt horizontal and\nvertical flips. We use the Adam optimizer with an initial\nlearning rate of 10−4 and decrease it to 10−6 using cosine\nannealing. The numbers of encoder blocks in the LLFormer\nfrom stage 1 to stage 4 are {2, 4, 8, 16}, and the number\nof attention heads in A-MSA are {1, 2, 4, 8}. The numbers\ncorresponding to decoders from stage 1 to 3 are {2, 4, 8}\nand {1, 2, 4}. For benchmarking, we compare 16 representa-\ntive LLIE methods, including seven traditional non-learning\nmethods (BIMEF (Ying, Li, and Gao 2017), FEA (Dong\net al. 2011), LIME (Guo, Li, and Ling 2016), MF (Fu et al.\n2016a), NPE (Wang et al. 2013), SRIE (Fu et al. 2016b),\nMSRCR (Jobson, Rahman, and Woodell 1997)), three super-\nvised CNN-based methods (RetinexNet (Wei et al. 2018),\nDSLR (Lim and Kim 2020), KinD (Zhang, Zhang, and\nGuo 2019)), two unsupervised CNN-based methods (EL-\nGAN (Jiang et al. 2021), RUAS (Liu et al. 2021b)), two\nzero-shot learning-based methods (Z_DCE (Guo et al. 2020),\nZ_DCE++ (Li, Guo, and Loy 2021) ) and two supervised\ntransformer-based methods (Uformer (Wang et al. 2022),\nRestormer (Zamir et al. 2022)). For each method, we use the\npublicly available code and train each learning-based method\nfor 300 epochs. For ELGAN, we directly use its pre-trained\nmodel for testing. Performance is evaluated with the PSNR,\nSSIM, LPIPS, and MAE metrics.\nBenchmarking Study for UHD-LLIE\nUHD-LOL4K Subset. We test 16 different state-of-the-art\nLLIE methods and our proposed LLFormer on the UHD-\nLOL4K subset. The quantitative results are reported in Table\n1. According to Table 1, we can find that traditional LLIE\nalgorithms (BIMEF, FEA, LIME, MF, NPE, SRIE, MSRCR)\ngenerally do not work well on UHD-LOL4K. Among them,\nthe quantitative scores (PSNR, SSIM, LPIPS, MAE) of some\nmethods are even worse than those of unsupervised learn-\ning methods (RUAS, ELGAN). The results of CNN-based\nsupervised learning methods (see RetiunexNet, DSLR, and\nKID) are better than unsupervised learning-based and zero-\nshot learning-based methods, which is expected. Among the\nCNN-based methods, DSLR obtains the best performance in\nterms of PSNR, SSIM, LPIPS, and MAE. Compared with\nCNN-based supervised learning methods, the performances\nof transformer-based supervised learning methods (Uformer,\nRestormer, and LLFormer) are greatly improved. Among\nthese, the proposed LLFormer obtains the best performance,\nachieving a 0.42 dB improvement in PSNR compared to\nRestormer. A visual comparison is shown in Fig. 5. The\nimage recovered by LLFormer contains vivid colors and is\ncloser to the ground truth.\nUHD-LOL8K Subset. We also conduct benchmarking\nexperiments on the UHD-LOL8K subset by partitioning each\n8K image into 4 patches of 4K resolution. The last four\ncolumns of Table 1 show the evaluation results. Deep learn-\ning methods RetinexNet, DSLR, Uformer, Restormer, and\nLLFormer achieve better performance on both pixel-wise and\nperceptual metrics. Transformer-based methods achieve top\nranks for all evaluation metrics with LLFormer outperform-\ning other methods. As shown in Fig. 5, LLFormer produces\nvisually pleasing results with more details.\nImproving Downstream Tasks. To verify whether LLIE\nis beneficial for downstream tasks, we randomly select\n300 images from the DARK FACE dataset (Yang et al.\n2020) and pre-process these images using the top three\nmethods in our benchmark study. We then detect faces us-\ning RetinaFace (Deng et al. 2020). When using the pre-\nprocessing step, the average precision (AP) values for\nUformer, Restormer, and LLFormer improve by 67.06%,\n68.11%, and 71.2%, respectively. Visual results are shown\nin Fig. 6. Pre-trained LLIE models not only generate im-\nages with adequate color balance, but also help improve the\nperformance of downstream tasks.\nComparison Results on Public Datasets\nWe benchmark LLFormer on the LOL (Wei et al. 2018) and\nMIT-Adobe FiveK (Bychkovsky et al. 2011) datasets, com-\nparing it with 14 methods specifically designed for LLIE and\ntwo transformer-based methods. We use published code to re-\ntrain Uformer and Restormer on these datasets, respectively.\nResults are shown in Table 2. LLFormer achieves signifi-\ncantly higher performance on the LOL dataset, obtaining\nhigher PSNR, SSIM, and MAE scores than Restormer. In\nterms of LPIPS, LLFormer ranks in second place. On the\nMIT-Adobe FiveK dataset, transformer-based methods rank\n2658\nMethods UHD-LOL4K UHD-LOL8K\nPSNR ↑ SSIM ↑ LPIPS ↓ MAE ↓ PSNR ↑ SSIM ↑ LPIPS ↓ MAE ↓\ninput images 11.9439 0.5295 0.3125 0.2591 13.7486 0.6415 0.3104 0.2213\nBIMEF† (Ying,\nLi, and Gao 2017) 18.1001 0.8876 0.1323 0.1240 19.5225 0.9099 0.1825 0.1048\nFEA† (Dong et\nal. 2011) 18.3608 0.8161 0.2197 0.0986 15.3301 0.7699 0.3696 0.1700\nLIME† (Guo, Li,\nand Ling 2016) 16.1709 0.8141 0.2064 0.1285 13.5699 0.7684 0.3055 0.2097\nMF† (Fu et\nal. 2016a) 18.8988 0.8631 0.1358 0.1111 18.2474 0.8781 0.2158 0.1258\nNPE† (Wang\net al. 2013) 17.6399 0.8665 0.1753 0.1125 16.2283 0.7933 0.3214 0.1506\nSRIE† (Fu et\nal. 2016b) 16.7730 0.8365 0.1495 0.1416 19.9637 0.9140 0.1813 0.0975\nMSRCR† (Jobson, Rahman,\nand Woodell 1997) 12.5238 0.8106 0.2136 0.2039 12.5238 0.7201 0.4364 0.2352\nRetinexNet‡ (W\nei et al. 2018) 21.6702 0.9086 0.1478 0.0690 21.2538 0.9161 0.1792 0.0843\nDSLR‡ (Lim and\nKim 2020) 27.3361 0.9231 0.1217 0.0341 21.9406 0.8749 0.2661 0.0805\nKinD‡ (Zhang, Zhang,\nand Guo 2019) 18.4638 0.8863 0.1297 0.1060 17.0200 0.7882 0.1739 0.1538\nZ_DCE§ (Guo et\nal. 2020) 17.1873 0.8498 0.1925 0.1465 14.1593 0.8141 0.2847 0.1914\nZ_DCE++§ (Li, Guo,\nand Loy 2021) 15.5793 0.8346 0.2223 0.1701 14.6837 0.8348 0.2466 0.1904\nRU\nAS△ (Liu et al. 2021b) 14.6806 0.7575 0.2736 0.1690 12.2290 0.7903 0.3557 0.2445\nELGAN△ (Jiang et\nal. 2021) 18.3693 0.8642 0.1967 0.1011 15.2009 0.8376 0.2293 0.1713\nUformer⋆ (Wang\net al. 2022) 29.9870 0.9804 0.0342 0.0262 28.9244 0.9747 0.0602 0.0344\nRestormer⋆ (Zamir et\nal. 2022) 36.9094 0.9881 0.0226 0.0117 35.0568 0.9858 0.0331 0.0195\nLLFormer⋆ 37.3340 0.9889 0.0200 0.0116 35.4313 0.9861 0.0267 0.0194\nTable 1: Benchmarking study on the UHD-LOL4K and UHD-LOL8K subsets. †, ‡, §, △ and ⋆ indicate the traditional methods,\nsupervised CNN-based methods, unsupervised CNN-based methods, zero-shot methods and transformer-based methods.\nInput RetinexNet DSLR ELGAN Uformer Restormer LLFormer GT\nFigure 5: Visual results on the UHD-LOL. The top and bottom rows are from the UHD-LOL4K and UHD-LOL4K subsets.\nRetinaFace\n+ input image RetinaFace + Uformer\nRetinaFace\n+ Restormer RetinaFace + LLFormer\nFigure 6: Enhanced visual results and face detection results.\nat the top and LLFormer achieves the best results on all met-\nrics. Among the best three transformer-based methods, the\noverhead (parameters and multiply-accumulate operations)\nfor Uformer, Restormer and LLFormer are 38.82M/76.67G,\n26.10M/140.99G and 24.52M/22.52G (measured on 256 ×\n256 images), respectively. This shows that the proposed LL-\nFormer achieves the best performance with efficient use of\nresources. This is due to the design of LLFormer, where the\naxis-based multi-head self-attention and hierarchical struc-\nture help to decrease the computational complexity.\nAblation Studies\nWe conduct ablation studies by measuring the contributions\nof the following factors: (1) Axis-based Multi-head Self At-\ntention; (2) Dual Gated Feed-Forward Network; (3) Weighted\nskip connection; (4) Cross-layer Attention Fusion Block; (5)\nWidth and depth of the network. Experiments are performed\non the UHD-LOL4K subset, and models are trained on image\npatches of size 128 × 128 for 100 epochs.\nA. Axis-Based Transformer Block. We measure the im-\npact of the proposed axis-based multi-head self attention and\ndual gated feed-forward network (FFN), see Table 3. Com-\npared with the base model using Resblock (Lim et al. 2017),\nour A-MSA (either height or width) and DGFN significantly\ncontribute to the improvements. When using depth-wise con-\nvolution to enhance locality in self-attention (compare Table\n3 (d) and (h) or the feed-forward network (compare Table 3\n(f) and (h)), the improvements in terms of PSNR are 0.89,\n0.75, respectively. By applying the dual gated mechanism,\nPSNR and SSIM are improved by3.42 and 0.0081 (see Table\n3 (g), (h)). Using the dual gated mechanism together with lo-\n2659\nMethods LOL MIT-Adobe\nFiveK\nPSNR ↑SSIM ↑LPIPS ↓MAE ↓ PSNR ↑SSIM ↑LPIPS ↓MAE ↓\nBIMEF (Y\ning, Li, and Gao 2017) 13.8752 0.5950 0.3264 0.2063 17.9683 0.7972 0.1398 0.1134\nFEA (Dong\net al. 2011) 16.7165 0.4784 0.3847 0.1421 15.2342 0.7161 0.1949 0.1512\nLIME (Guo,\nLi, and Ling 2016) 16.7586 0.4449 0.3945 0.1200 13.3031 0.7497 0.1319 0.2044\nMF (Fu\net al. 2016a) 16.9662 0.5075 0.3796 0.1416 17.6271 0.8143 0.1204 0.1194\nNPE (W\nang et al. 2013) 16.9697 0.4839 0.4049 0.1290 17.3840 0.7932 0.1320 0.1224\nSRIE (Fu\net al. 2016b) 11.8552 0.4954 0.3401 0.2571 18.6273 0.8384 0.1047 0.1030\nMSRCR (Jobson,\nRahman, and Woodell 1997)13.1728 0.4615 0.4350 0.2067 13.3149 0.7515 0.1767 0.1993\nRetinexNet\n(Wei et al. 2018) 16.7740 0.4250 0.4739 0.1256 12.5146 0.6708 0.2535 0.2068\nDSLR (Lim\nand Kim 2020) 14.9822 0.5964 0.3757 0.1918 20.2435 0.8289 0.1526 0.0880\nKinD (Zhang,\nZhang, and Guo 2019) 17.6476 0.7715 0.1750 0.1231 16.2032 0.7841 0.1498 0.1379\nZ_DCE (Guo\net al. 2020) 14.8607 0.5624 0.3352 0.1846 15.9312 0.7668 0.1647 0.1426\nZ_DCE++ (Li,\nGuo, and Loy 2021) 14.7484 0.5176 0.3284 0.1801 14.6111 0.4055 0.2309 0.1539\nRU\nAS (Liu et al. 2021b) 16.4047 0.5034 0.2701 0.1534 15.9953 0.7863 0.1397 0.1426\nELGAN (Jiang\net al. 2021) 17.4829 0.6515 0.3223 0.1352 17.9050 0.8361 0.1425 0.1299\nUformer (W\nang et al. 2022) 18.5470 0.7212 0.3205 0.1134 21.9171 0.8705 0.0854 0.0702\nRestormer (Zamir\net al. 2022) 22.3652 0.8157 0.1413 0.0721 24.9228 0.9112 0.0579 0.0556\nLLFormer 23.6491 0.8163 0.1692 0.0635 25.7528 0.9231 0.0447 0.0505\nTable 2: Comparison results on LOL and MIT-Adobe FiveK datasets.\nVariant Component MACs Params PSNR/SSIM\nBase (a) Resblock 11.90G 13.87M 31.92/0.9771\nMulti-head\nattention\n(b) A-MSA (Height)\n+\nDGFN 13.60G 14.77M 35.15/0.9836\n(c) A-MSA (W\nidth)\n+ DGFN 13.60G 14.77M 34.98/0.9832\n(d)A-MSA\n+ DGFN 16.26G 19.78M 35.31/0.9843\n(e) A-MSA\n+ FFN 18.90G 20.62M 23.33/0.9111\nFFN (f) A-MSA\n+ DGFN 21.47G 24.18M 35.83/0.9846\n(g)A-MSA\n+ DGFN 25.52G 24.52M 32.78/0.9786\nLLFormer (h)A-MSA +\nDGFN 22.52G 24.52M 36.20/0.9867\nTable 3: Ablation study on Transformer Block. (a) refers to\nmodel with Resblock, (d) refers to A-MSA without depth-\nwise convolution, (f) is DGFN without depth-wise convolu-\ntion, and (g) is DGFN without the dual gated mechanism.\ncality yields the best results. In contrast, combining A-MSA\nwith the conventional FFN (Vaswani et al. 2017), degrades\nthe performance (Table 3 (e)). This indicates that designing\nan appropriate FFN is critical for the transformer block.\nB. Skip Connection and Fusion Block. To validate the\nweighted connection and cross-layer attention fusion block,\nwe conduct ablation studies by progressively removing the\ncorresponding components: (1) skip, (2) 1 × 1 convolution,\n(3) skip with 1 × 1 convolution, (4) head CAFB, (5) tail\nCAFB, (6) all CAFBs. Table 4 shows the results in terms of\nPSNR and SSIM, which indicate that each component helps\nimprove the results. The model improves significantly when\nincluding CAFB and weighted skip connections. We observe\na minor gain when applying 1 × 1 convolutions.\nC. Wider vs. Deeper. To study the effect of width and\ndepth in the network, we conduct experiments to increase\nthe width (channels) and depth (number of encoder stages)\nof LLFormer. Table 5 shows the results. The results demon-\nstrate that LLFormer strikes the best trade off between per-\nformance and complexity (36.20/0.9867, 22.52G, 24.52M,\nVariant Component MACs Params PSNR/SSIM\nSkip\nw/o skip 22.52G 24.52M 35.45/0.9844\nw/o con\nv 22.47G 24.50M 35.90/0.9852\nw/o skip+con\nv 22.47G 24.50M 35.12/0.9847\nCAFB\nw/o head\nCAFB 21.76G 24.51M 35.57/0.9847\nw/o tail\nCAFB 21.76G 24.51M 35.81/0.9852\nw/o CAFB 21.00G 24.50M 35.10/0.9835\nLLFormer contain all 22.52G 24.52M 36.20/0.9867\nTable 4: Ablation study on connection and fusion.\nVariant W/D MACs Params PSNR/SSIM Speed\nLLFormer 16/4 22.52G 24.52M 36.20/0.9867 0.063 s\nWider\n32/4 81.92G 95.63M 36.91/0.9871 0.120 s\n48/4 111.22G 114.49M 37.44/0.9880 0.152 s\n64/4 311.16G 377.64M 38.00/0.9881 0.193 s\nDeeper\n16/3 14.88G 3.51M 20.19/0.9432 0.054s\n16/5 29.53G 106.77M 36.09/0.9844 0.142 s\n16/6 36.45G 432.25M 35.62/0.9847 0.181 s\n16/7 43.32G 1727.08M 35.41/0.9845 0.217 s\nTable 5: \"Wider vs. Deeper\" analysis.\n0.063s), compared to its wider or deeper counterparts.\nConclusion\nIn this paper, we build the first large-scale low-light UHD im-\nage enhancement benchmark dataset, which consists of UHD-\nLOL4K and UHD-LOL8K subsets. Based on this dataset, we\nconduct comprehensive experiments for UHD-LLIE. To the\nbest of our knowledge, this is the first attempt to specifically\naddress the UHD-LLIE task. We propose the first transformer-\nbased baseline network called LLFormer for UHD-LLIE.\nExtensive experiments show that LLFormer significantly out-\nperforms other state-of-the-art methods.\n2660\nAcknowledgements\nThis work was supported in part by the National Natural Sci-\nence Foundation of China (Grant No. 61672273, 61832008),\nin part by Shenzhen Science and Technology Program (No.\nJSGG20220831093004008, JCYJ20220818102012025).\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer normal-\nization. arXiv preprint arXiv:1607.06450.\nBychkovsky, V .; Paris, S.; Chan, E.; and Durand, F. 2011.\nLearning photographic global tonal adjustment with a\ndatabase of input/output image pairs. In CVPR, 97–104.\nCai, J.; Gu, S.; and Zhang, L. 2018. Learning a deep single\nimage contrast enhancer from multi-exposure images. IEEE\nTIP, 27(4): 2049–2062.\nChen, C.; Chen, Q.; Xu, J.; and Koltun, V . 2018. Learning to\nsee in the dark. In CVPR, 3291–3300.\nDeng, J.; Guo, J.; Ververas, E.; Kotsia, I.; and Zafeiriou, S.\n2020. Retinaface: Single-shot multi-level face localisation in\nthe wild. In CVPR, 5203–5212.\nDong, X.; Wang, G.; Pang, Y .; Li, W.; Wen, J.; Meng, W.;\nand Lu, Y . 2011. Fast efficient algorithm for enhancement of\nlow lighting video. In ICME, 1–6.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2021. An image is worth 16x16\nwords: Transformers for image recognition at scale. In ICLR.\nFu, X.; Zeng, D.; Huang, Y .; Liao, Y .; Ding, X.; and Pais-\nley, J. 2016a. A fusion-based enhancing method for weakly\nilluminated images. Signal Processing, 129: 82–96.\nFu, X.; Zeng, D.; Huang, Y .; Zhang, X.-P.; and Ding, X.\n2016b. A weighted variational model for simultaneous re-\nflectance and illumination estimation. In CVPR, 2782–2790.\nGirshick, R. 2015. Fast r-cnn. In ICCV, 1440–1448.\nGuo, C.; Li, C.; Guo, J.; Loy, C. C.; Hou, J.; Kwong, S.; and\nCong, R. 2020. Zero-reference deep curve estimation for\nlow-light image enhancement. In CVPR, 1780–1789.\nGuo, X.; Li, Y .; and Ling, H. 2016. LIME: Low-light image\nenhancement via illumination map estimation. IEEE TIP,\n26(2): 982–993.\nJiang, Y .; Gong, X.; Liu, D.; Cheng, Y .; Fang, C.; Shen, X.;\nYang, J.; Zhou, P.; and Wang, Z. 2021. Enlightengan: Deep\nlight enhancement without paired supervision. IEEE TIP, 30:\n2340–2349.\nJobson, D. J.; Rahman, Z.-u.; and Woodell, G. A. 1997. A\nmultiscale retinex for bridging the gap between color images\nand the human observation of scenes. IEEE TIP, 6(7): 965–\n976.\nKim, Y .-T. 1997. Contrast enhancement using brightness\npreserving bi-histogram equalization. IEEE TCE, 43(1): 1–8.\nKimmel, R.; Elad, M.; Shaked, D.; Keshet, R.; and Sobel,\nI. 2003. A variational framework for retinex. IJCV, 52(1):\n7–23.\nLi, C.; Guo, C. G.; and Loy, C. C. 2021. Learning to Enhance\nLow-Light Image via Zero-Reference Deep Curve Estimation.\nIEEE TPAMI, 44(8): 4225–4238.\nLim, B.; Son, S.; Kim, H.; Nah, S.; and Mu Lee, K. 2017.\nEnhanced deep residual networks for single image super-\nresolution. In CVPRW, 136–144.\nLim, S.; and Kim, W. 2020. Dslr: Deep stacked laplacian\nrestorer for low-light image enhancement. IEEE TMM, 23:\n4272–4284.\nLiu, J.; Xu, D.; Yang, W.; Fan, M.; and Huang, H. 2021a.\nBenchmarking low-light image enhancement and beyond.\nIJCV, 129(4): 1153–1184.\nLiu, R.; Ma, L.; Zhang, J.; Fan, X.; and Luo, Z. 2021b.\nRetinex-inspired unrolling with cooperative prior architecture\nsearch for low-light image enhancement. In CVPR, 10561–\n10570.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021c. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012–10022.\nLore, K. G.; Akintayo, A.; and Sarkar, S. 2017. LLNet:\nA deep autoencoder approach to natural low-light image\nenhancement. PR, 61: 650–662.\nShen, L.; Yue, Z.; Feng, F.; Chen, Q.; Liu, S.; and Ma, J.\n2017. Msr-net: Low-light image enhancement using deep\nconvolutional network. arXiv preprint arXiv:1711.02488.\nShi, W.; Caballero, J.; Huszár, F.; Totz, J.; Aitken, A. P.;\nBishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-time\nsingle image and video super-resolution using an efficient\nsub-pixel convolutional neural network. In CVPR, 1874–\n1883.\nStark, J. A. 2000. Adaptive image contrast enhancement\nusing generalizations of histogram equalization. IEEE TIP,\n9(5): 889–896.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention\nis all you need. In NeurIPS, 5998–6008.\nWang, L.; Xiao, L.; Liu, H.; and Wei, Z. 2014. Variational\nBayesian method for retinex. IEEE TIP, 23(8): 3381–3396.\nWang, S.; Zheng, J.; Hu, H.-M.; and Li, B. 2013. Natu-\nralness preserved enhancement algorithm for non-uniform\nillumination images. IEEE TIP, 22(9): 3538–3548.\nWang, Z.; Cun, X.; Bao, J.; and Liu, J. 2022. Uformer: A\ngeneral u-shaped transformer for image restoration. InCVPR,\n17683–17693.\nWei, C.; Wang, W.; Yang, W.; and Liu, J. 2018. Deep retinex\ndecomposition for low-light enhancement. In BMVC.\nYang, W.; Yuan, Y .; Ren, W.; Liu, J.; Scheirer, W. J.; Wang,\nZ.; Zhang, T.; Zhong, Q.; Xie, D.; Pu, S.; et al. 2020. Ad-\nvancing image understanding in poor visibility environments:\nA collective benchmark study. IEEE TIP, 29: 5737–5752.\nYing, Z.; Li, G.; and Gao, W. 2017. A bio-inspired multi-\nexposure fusion framework for low-light image enhancement.\narXiv preprint arXiv:1711.00591.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient Transformer for\nHigh-Resolution Image Restoration. In CVPR, 5728–5739.\nZhang, K.; Li, D.; Luo, W.; Ren, W.; Stenger, B.; Liu, W.;\nLi, H.; and Yang, M.-H. 2021. Benchmarking Ultra-High-\nDefinition Image Super-resolution. In ICCV, 14769–14778.\n2661\nZhang, X.; Shen, P.; Luo, L.; Zhang, L.; and Song, J. 2012.\nEnhancement and noise reduction of very low light level\nimages. In ICIP, 2034–2037.\nZhang, Y .; Zhang, J.; and Guo, X. 2019. Kindling the dark-\nness: A practical low-light image enhancer. In ACMMM,\n1632–1640.\n2662"
}