{
  "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
  "url": "https://openalex.org/W4385572331",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4281418779",
      "name": "Myles Foley",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A2737338445",
      "name": "Ambrish Rawat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118647311",
      "name": "Taesung Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185188078",
      "name": "Yufang Hou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2026787086",
      "name": "Gabriele Picco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2626178311",
      "name": "Giulio Zizzo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385572879",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2793187629",
    "https://openalex.org/W4386076098",
    "https://openalex.org/W2964412937",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W2937447982",
    "https://openalex.org/W4383533118",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3100880133",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3169141681",
    "https://openalex.org/W3013242664",
    "https://openalex.org/W2952607215",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W2879765882",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2579318729",
    "https://openalex.org/W4283210053",
    "https://openalex.org/W3210109040",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4281609260",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3106051020",
    "https://openalex.org/W2768064608",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4281764334"
  ],
  "abstract": "Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco, Giulio Zizzo. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7423–7442\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMatching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large\nLanguage Models\nMyles Foley ∗1, Ambrish Rawat 2, Taesung Lee 2,\nYufang Hou 2, Gabriele Picco 2, Giulio Zizzo 2\n1Imperial College London, 2IBM Research\nm.foley20@imperial.ac.uk\n{ambrish.rawat, yhou}@ie.ibm.com\n{taesung.lee, gabriele.picco, giulio.zizzo2}@ibm.com\nAbstract\nThe wide applicability and adaptability of gen-\nerative large language models (LLMs) has en-\nabled their rapid adoption. While the pre-\ntrained models can perform many tasks, such\nmodels are often ﬁne-tuned to improve their\nperformance on various downstream applica-\ntions. However, this leads to issues over viola-\ntion of model licenses, model theft, and copy-\nright infringement. Moreover, recent advances\nshow that generative technology is capable of\nproducing harmful content which exacerbates\nthe problems of accountability within model\nsupply chains. Thus, we need a method to in-\nvestigate how a model was trained or a piece of\ntext was generated and what their pre-trained\nbase model was. In this paper we take the ﬁrst\nstep to address this open problem by tracing\nback the origin of a given ﬁne-tuned LLM to its\ncorresponding pre-trained base model. We con-\nsider different knowledge levels and attribution\nstrategies, and ﬁnd that we can correctly trace\nback 8 out of the 10 ﬁne tuned models with our\nbest method.\n1 Introduction\nRecent advancements in pre-trained large language\nmodels (LLMs) have enabled the generation of\nhigh quality texts that humans have difﬁculty iden-\ntifying as machine generated ( Wahle et al. , 2022).\nWhile these pre-trained models can perform many\ntasks in the zero-shot or few-shot settings ( Brown\net al. , 2020; Schick and Schütze , 2021), such mod-\nels are often ﬁne-tuned to improve their perfor-\nmance on various downstream applications ( Peters\net al. , 2019; Pfeiffer et al. , 2020). As of May 2023,\nthere are more than 209,000 models hosted on Hug-\ngingface1 and more than 12,000 of them belong to\nthe “ text generation ” category. Many generation\nmodels are ﬁne-tuned from the open-access pre-\ntrained base models such as XLNet (\nYang et al. ,\n∗ ∗Work done during internship at IBM Research.\n1https://huggingface.co/models\n2019), BART ( Lewis et al. , 2020), or GPT-J ( Wang\nand Komatsuzaki , 2021) whose training typically\nrequires signiﬁcant computational resources.\nWhile the proliferation of text generation mod-\nels has led to the performance improvement for\na wide range of downstream applications such as\ntext summarization and dialogue systems, it has\nalso been repeatedly shown that these pre-trained\nor ﬁne-tuned LLMs can facilitate the creation and\ndissemination of misinformation at scale ( Wei-\ndinger et al. , 2021), and the manipulation of public\nopinion through false “majority opinions” ( Mann,\n2021). In response, laws like the EU’s Digital\nServices Act (DSA) 2 aim at tackling these issues\nby enforcing procedural accountability and trans-\nparency for responsible use of AI-based technolo-\ngies. These growing demands for AI forensics re-\nquire the development of methods for establishing\nmodel ownership, protecting intellectual property,\nand analyzing the accountability of any violations.\nIn this work, we systematically investigate LLM\nattribution, a novel task recently proposed at the\nﬁrst “Machine Learning Model Attribution Chal-\nlenge (MLMAC)” 3, which aims to link an arbitrary\nﬁne-tuned LLM to its pre-trained base model using\ninformation such as generated responses from the\nmodels. Through LLM attribution, regulatory bod-\nies can trace instances of intellectual property theft\nor inﬂuence campaigns back to the base model.\nHowever, determining attribution for ﬁne-tuned\nLLMs can be challenging as base models often\nhave similar architectures and overlapping training\ndata. For instance, T HEPILE (Gao et al. , 2020a),\na large data set that consists of 22 smaller, high-\nquality datasets, with a total size of 825 GB, was in-\ncluded into the training data for both GPT-J ( Wang\nand Komatsuzaki , 2021) and OPT ( Zhang et al. ,\n2022).\n2https://eur-lex.europa.eu/eli/reg/\n2022/2065/oj\n3https://mlmac.io/\n7423\n7424\naccess to the models in B and F and the amount of\nresources available for developing the method. In\ngeneral, we assume that the developer of an attri-\nbution system can only query the LLMs as a black\nbox to obtain the generated responses, and has lim-\nited access to models in F. We speculate this to\nbe true for real-world settings where the producers\nof pre-trained base models, maintainers of model\nzoos, or an external auditor are incentivised to de-\nvelop such attribution systems. In such scenarios,\nthey may only have limited access to the API of\nﬁne-tuned models which will typically be owned\nby a third-party. Other constraints may arise from\nthe amount of resources available for developing at-\ntribution systems. For instance, an external auditor\nmay not have the domain expertise or computation\nresources to beneﬁt from the insights from other\nﬁne-tuning pipelines. Similarly, the developer is\nassumed to have no knowledge of the ﬁne-tuning\ndatasets and approaches used to obtain the models\nin F, as in these cases attribution may be easily\nachieved by replicating the setup locally and com-\nparing the obtained models with those in F. In\naddition to these assumptions, we consider the fol-\nlowing two knowledge levels available with the\ndeveloper of an attribution system.\n• Universal knowledge KU : This allows the\ndeveloper access to universal knowledge\nabout models in\nB. This allows the analysis\nby a human expert, as well as computing the\nperplexity of the input. Moreover, the devel-\noper can build an additional set of ﬁne-tuned\nmodels A, or even the capability to train such\nmodels. This enables building a supervised\nattributor using A as a training dataset.\n• Restricted knowledge KR: We do not have\naccess to A, and can only query the models in\nB as a black box to get the responses.\n4 Attribution Methods\nWe approach the LLM attribution problem as a\nclassiﬁcation task. Essentially, LLM attribution re-\nquires identifying the certain robust or latent char-\nacteristics of a pre-trained base model within the\ngiven ﬁne-tuned model. The ﬁne-tuned model may\nretain unique aspects in the pre-training data like\nevents and vocabulary of a speciﬁc time period, or\nthe ﬂuency of the base model in a certain domain.\nIn particular, as shown in Figure 2 we build\na classiﬁer hmb testing a response for each pre-\nInput \nRepresentation\nPrompts\np1, …, pK\nResponses\nmf(p1), …, mf(pK)\nFine-tuned \nmodel\nmf\nResponses\nmb(p1), …, mb(pK)\nPre-trained \nBase model\nmb\nClassifier\nhmb\nFigure 2: An example conﬁguration of a one-vs-rest\nclassiﬁer hmb using both base model mb and ﬁne-tuned\nmodel mf .\ntrained base model mb to decide if a given ﬁne-\ntuned model mf retains the characteristics of mb,\nfollowing the one-vs-rest ( mb or others) scheme.\nThen, we aggregate the result to pick the top-1 base\nmodel with the majority voting method. In other\nwords, we take mf such that ∑\np∈P hmb (mf (p))\nis maximized, where P is a set of prompts.\nThe task can be further broken down into two\nsteps for each base model mb and its classiﬁer\nhmb including (1) characterizing the target base\nmodel mb and representing the input to the classi-\nﬁer (Section 4.1.1), (2) selecting the prompts (Sec-\ntion 4.1.2), and (3) designing the classiﬁer (Sec-\ntion 4.2).\n4.1 Model Characterization and Input\nRepresentation\nIn this step, we characterize an LLM (ﬁne-tuned or\nbase model), and prepare the input to the classiﬁer\nhmb . One piece of evidence of attribution lies in ex-\nploiting the artefacts of a pre-trained LLM that are\nexpected to persist through the ﬁne-tuning process\nand are inherited by their ﬁne-tuned counterparts.\nFor instance, a distinctive feature of RoBERTa ( Liu\net al. , 2019) is the sequence length limit of 512\nwhich is often inherited by its ﬁne-tuned versions.\nThe task characteristics and associated training data\nmay also help distinguish different LLMs. For ex-\nample, LLMs trained for speciﬁc tasks like chat\nbots or code generation will have characteristically\ndifferent output spaces. They may also have unique\naspects in their training data like a speciﬁc lan-\nguage or markers such as data collected over spe-\nciﬁc time period.\nWhile feature engineering can extract a usable\nset of features, it is prone to bias, and less adaptable,\nand it also requires deep knowledge about\nB. Thus,\nwe leverage the embeddings of the prompts and\nresponses to learn and exploit such knowledge.\n7425\n4.1.1 Input Representation\nOur goal is to train a classiﬁer to capture the corre-\nlations between an arbitrary response and the base\nmodel mb. For example, with a prompt p, this\ncould capture the relationship between a response\nmb(p) and mb. Similarly, we can capture the rela-\ntionship between a response mf (p) and mb where\nmf is obtained by ﬁne-tuning mb. Assuming that\nsuch correlations are preserved in a base model and\nﬁne-tuned model pair, we use it to determine the\nattribution of a ﬁne-tuned LLM.\nGiven a set of prompts p1, . . . , pK, there are\nmultiple ways to prepare them for the classiﬁer.\nWe can apply the target base model, or ﬁne-tuned\nmodel to get the responses, and concatenate the\nprompt and its response. Speciﬁcally, we list the\ninput representations we consider as follows:\n• Base model only ( IB): “ pi mb(pi)”\n• Fine-tuned model only ( IF): “ pi mf (pi)”\n• Base model + ﬁne-tuned model ( IB+F): “ pi\nmb(pi) <SEP> pi mf (pi)”\n• Separate embeddings for base model and ﬁne-\ntuned model.\nWe embed these concatenated sentences\nusing BERT computed by a best-base-\nmultilingual-cased model5 except for\nthe last approach that embeds the components\nseparately for margin-based classiﬁer TripletNet\ndescribed in Section 4.2. Note that all reference to\na ﬁne-tuned model mf during training are actually\nsampled from another set A of ﬁne-tuned models\nunder KU assumption as we assume only sparse\naccess to mf . Also, IB takes the responses from\nmf during prediction to test if the responses share\nthe same characteristics that this classiﬁer learned\nabout mb.\n4.1.2 Prompt Selection\nWhile many corpora to pre-train LLMs provide\nprompts, they might not be all useful to predict\nthe base model. Thus, we aim to test and se-\nlect prompts with more distinctive outcome. Our\nprompt selection strategy is driven to help best\ncharacterise base models. We ﬁrst collect the list\nof datasets used in training each base model, identi-\nfying unique aspects of datasets that can help iden-\ntify a base model. Intuitively, one might expect\n5https://huggingface.co/bert-base-multilingual-cased\nsuch unique prompts or ‘edge cases’ to bring out\nthe distinctive aspects in the subsequent ﬁne-tuned\nmodels. Speciﬁcally, we ﬁrst identify the unique\ncategories of prompts (e.g. different languages)\npresent in different datasets and sample from this\nset.6\nMore speciﬁcally, we consider three approaches:\na small set ( P1) of edge cases that are distinct to\neach corpus, a naive collection ( P2) of prompts,\nand reinforcement learning to select a subset ( P3)\nfrom the edge cases.\nWhile the naive collection of the 10,000 prompts\nfrom ThePile corpus and manually selecting a set\nof prompts unique to each training dataset is clear,\nwe can also use reinforcement learning to optimize\nthe selection using the classiﬁcation result. More\nspeciﬁcally, we train an agent for each hmb that can\nsupply prompts for attribution inference. During\nthe training episodes, the agent is rewarded for\nprompts whose responses lead to correct attribution.\nThe reinforcement learning setup for this problem\nis deﬁnes as follows:\n• State. A feature space consisting of the\nclassiﬁcation of the prompt, and an embed-\nding of the prompt response computed by\nbest-base-multilingual-cased.\n• Action. Selecting one of the prompts from\nP1.\n• Reward. Using a sparse reward function we\nreward (+1) for correct classiﬁcation and pe-\nnalise (-10) for incorrect classiﬁcation.\n• Episode. 20 possible actions.\nAt the start of each episode we are able to randomly\nselect one of the models that the classiﬁer was\ntrained on, thus the RL agent learns to generalise\nto a variety of different models. We implement the\nRL agent using the Proximal Policy Optimisation\n(PPO) method ( Schulman et al. , 2017).\nWe can use these collected prompts in a few\ndifferent ways. A simplistic approach is using each\nset P1, P2 or P3 individually. Another approach\nP1+P2 trains the classiﬁer with P2, and then ﬁne-\ntune with P1 to leverage both of them ( P3 is already\na subset of P2) and we ﬁnd this is promising in our\nexperiments. See Appendix D for details of the\nprompts used from T HEPILE for this combination\napproach.\n6Prompt selection is complex and the numerous dimen-\nsions of fairness and robustness of such schemes are useful for\nfurther investigation of the LLM attribution problem. How-\never, we believe them to be out of scope of this ﬁrst systematic\nstudy on LLM attribution.\n7426\n4.2 Classiﬁer Architecture\nWe consider a one vs rest setup where for each base\nmodel mb we train a binary classiﬁer hmb : Σ M →\n{0, 1} which takes as input a response s ∈ ΣN , op-\ntionally with additional tokens, and predicts a score\nthat reﬂects its association to the based model mb.\nSingle embeddings prepared in Section 4.1.1 can\nbe straightforwardly used in a simple classiﬁer. We\nﬁne-tune the BERT model used for the embedding\nto make the binary prediction with cross-entropy\nloss. Given the one-vs-rest approach the positive\nsamples for an hmb are repurposed as negative ones\nfor the rest of the classiﬁers hml for ml ∈ B\\{mb}.\nThe best average score thus obtained is used to es-\ntablish the attribution for mf .\nWe also consider TripletNet ( Wei et al. , 2021)\nbased classiﬁers that use a margin-based loss func-\ntion using the separate embeddings of the base\nand ﬁne-tuned model responses. The TripletNet\nis able to make predictions by taking in a single\nsentence, computing the output embedding, and\nﬁnding the closest embedding from the training set\nand using the label of the training sentence as a\nprediction. The cosine distance between the anchor\ninput, positive example, and negative example are\nthen computed as the loss. We adopt the margin\nparameter 0.4 from the original paper (\nWei et al. ,\n2021).\n5 Experiments\n5.1 Experiment Setup\nFor training the attribution models hmb we make\nuse of popular text corpora including: GitHub,\nThe BigScience ROOTS Corpus ( Laurençon et al. ,\n2022), CC-100 ( Conneau et al. , 2020), Reddit\n(Hamilton et al. , 2017), and T HEPILE (Gao et al. ,\n2020b).\nWe also use a variety of prompt sizes for attribu-\ntion (150 to 10,000), and datasets (IMDB Reviews\n(Maas et al. , 2011), GLUE ( Wang et al. , 2018),\nTajik OSCAR ( Abadji et al. , 2022), and Amazon\nMultilingual ( Keung et al. , 2020).\nTo provide a wide didactic range of models for\nour approaches we utilise 10 pre-trained LLMs\nto create\nB and corresponding ﬁne-tuned mod-\nels (Table 1): Bloom ( Scao and et al. , 2022),\nOPT ( Zhang et al. , 2022), DialoGPT ( Zhang et al. ,\n2020), DistilGPT2 ( Sanh et al. , 2020), GPT2\n(Radford et al. , 2019), GPT2-XL ( Radford et al. ,\n2019), GPT-NEO ( Black et al. , 2021), CodeGen\n(Nijkamp et al. , 2023), XLNET, MultiLingual-\nm# Base Model Fine-tuning dataset\n0 bloom-350m common_gen ( Lin et al. , 2020)\n1 OPT-350M Pike, CYS, Manga-v1\n2 DialoGPT-\nlarge\nPersuasion For Good Dataset\n(Wang et al. , 2019)\n3 distilgpt2 wikitext2 ( Merity et al. , 2016)\n4 GPT2-XL the Wizard of Wikipedia dataset\n(Dinan et al. , 2019)\n5 gpt2 Wikipedia dump, EU Bookshop\ncorpus, Open Subtitles, Com-\nmonCrawl, ParaCrawl and News\nCrawl.\n6 GPT-Neo-\n125m\nCmotions - Beatles lyrics\n7 xlnet-base-\ncased\nIMDB ( Maas et al. , 2011)\n8 multilingual-\nMiniLM-L12-\nv2\nUnknown\n9 codegen-350M Zhu et al. (2022)\nTable 1: Fine-tuned models, their original base models\nand the datasets they are ﬁne-tuned on.\nMiniLM ( Wang et al. , 2021). These models provide\ndifferent architectures, parameter sizes, and tasks\nto offer a variety of model behaviors.\nWe consider a one-to-one mapping from B to F\n(and A), thus F and A contain ten models each. We\nutilise open-source models that are implemented\nin the Huggingface library to form the sets of\nF\nand A. We select A and F such that the ﬁne-tuning\ndataset, and model conﬁguration are known to us,\nof these we select the most popular by number of\ndownloads. We provide further details of these in\nAppendix B.\nWe take the top-1 result for each mb as men-\ntioned in Section 4 and check its correctness. We\nuse F1 and ROC curves as additional metrics.\nThese are calculated using prompt-level attribu-\ntion calculated per mb (as in Figure 8), and we use\nan average per hmb (as in Figure 3). Each of the\nattributors hmb described is ran once to determine\nthe attribution of mf to mb. Training is conducted\nusing a single NVIDIA A100 GPU.\n5.2 Compared Approaches\nWe consider different conﬁgurations for BERT clas-\nsiﬁers based on the input representations IB, IF or\nIB+F, and the prompts used P1, P2, P3 or P1+P2\ndescribed in Section 4.1.1.\nWe also consider the margin classiﬁer TripleNet\n(Section 4.2), and the following heuristic ap-\nproaches.\n• Perplexity: A measure of how conﬁdent a\nmodel is at making predictions, this can be\n7427\nleveraged for measuring attribution by com-\nputing the perplexity of mb relative to the re-\nsponse of mf to prompt p.\n• Heuristic Decision Tree (HDT) : Using KU\nwe can use knowledge of B to create a series\nof discriminative heuristics to categorise F\nas used by the winning solution to the ﬁrst\nMLMAC7.\n• Exact Match : Attribute responses mf to mb\nwhen both models respond the same to a\nprompt. Using the argmax of these attribu-\ntions to attribute mf to mb.\nFor detailed descriptions of the heuristic ap-\nproaches, please refer to Appendix A.\n5.3 Attribution Accuracy\nHere, we examine the attribution abilities of the\ncompared approaches shown in Table 2. Under KU\nconditions the baselines of Perplexity and HDT are\nonly able to correctly attribute 1 and 5 models re-\nspectively. Perplexity fails to capture the subtly of\nattribution, as repetitive responses lead to lower per-\nplexity and so incorrect attribution. The HDT par-\nticularly fails to account for overlap in pre-training\nand ﬁne-tuning. For instance, DialoGPT-Large and\nmf3 (ﬁne-tuned version of distilgpt2) respond in\nsimilar short sentences that leads to incorrect attri-\nbution. The TripletNet baseline performs poorly,\nonly correctly attributing 3 of the models. Both\nBERT based attributors are able to attribute more\nmodels correctly in comparison to the baselines.\nExamining the models at KR shows similar per-\nformance. The exact match correctly attributes\n5 models and BERT+\nIB identiﬁes 6 models.\nBERT+IB+P1 + P2 attributor is the most success-\nful by correctly attributing 8 models. Note that this\nmodel is the most expensive to train as we have to\nquery a large number of prompts.\nWe compare the ROC curves for BERT based\nattributor deﬁned under each K in Figure 3. We\nprovide plots of hmb in each variant in Appendix C.\nIt is interesting to note that the models under KR\nhave shallower curves than their KU counterparts,\nyet these KR models lead to the same or higher\nnumber of correct attributions. This is likely due\nto the ‘noise’ that gets added to responses of\nA\nfrom their separate ﬁne-tuning task, TA. This noise\nmoves the responses of ma further from mf (and\n7https://pranjal2041.medium.com/identifying-pretrained-\nmodels-from-ﬁnetuned-lms-32ceb878898f\n\u0000 ✁ ✂ ✄ ☎ ✆ ✝ ✞ ✟\n✠\n✞ ✡ ☛ ☞ ✌ ✍ ✎ ✌ ✏\n\u0000 ✁ ✂ ✄ ☎ ✆ ✝ ✞ ✟\n✑ ✒ ✠\n✞ ✡ ☛ ☞ ✌ ✍ ✓ ✎ ✔\n\u0000 ✁ ✂ ✄ ☎ ✆ ✝ ✞ ✟\n✑\n✞ ✡ ☛ ☞ ✌ ✍ ✓ ☛\n\u0000 ✁ ✂ ✄ ☎ ✆ ✝ ✞ ✟\n✑\n✞ ✡ ☛ ✞ ✡\n✕\n☞ ✌ ✍ ✓\n✕ ✖\nFigure 3: Average ROC plots for each classiﬁer at each\nknowledge level.\nAttribution\nMethod K m# TP0 1 2 3 4 5 6 7 8 9\nHDT KU ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✗ ✓ 5\nPerplexity KU ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ 1\nTripletNet + P1 KU ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✓ 3\nBERT + IF + P1 KU ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ 6\nBERT + IB+F + P1 KU ✗ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ ✓ 6\nExact matching KR ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✓ 5\nBERT + IB + P1 KR ✓ - ✓ - ✗ ✗ ✓ ✓ ✓ ✓ 6\nBERT + IB + P3 KR ✓ ✗ - ✗ ✗ ✓ ✓ ✓ - ✓ 5\nBERT + IB+P1+P2 KR ✓ ✓ ✓ - ✗ ✓ ✓ ✓ ✓ ✓ 8\nTable 2: Model Attributions on m# from the different\nmethods. Dashes (–) are used when multiple models\n(mf ) are attributed to mb. TP denotes True Positives.\nby extent mb). As such responses from mb are\ncloser to mf than ma. This makes the attributors\npredict more negative samples correctly under KU\nas there is greater disparity in response between\nma and mf , leading to a higher AUC; but also to\nmiss-attribution of mf at inference. Hence, it is\nunsurprising that the pretrained KR has the low-\nest AUC of any model, yet it leads to the highest\nattribution accuracy in Table\n2 as it is trained on\nresponses of mb which is closer in the latent space\nto responses of mf than ma.\nLesson Learned:Even under reduced knowl-\nedge level, pre-training was found to be the\nfactor that contributed to the highest attribu-\ntion performance.\n5.4 Effects of Prompt usage\nThe number of prompts available to an attributor for\nclassiﬁcation can have an inﬂuence on the attribu-\ntion performance: we hypothesize that increasing\nthe number of prompts used results in a clearer\nsignal as to the ﬁnetuned to base model attribution.\nWe train BERT attributors under the KR con-\n7428\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC pile150: 0.7\nAUC pile500: 0.609\nAUC pile1k: 0.679\nAUC pile2k: 0.763\nAUC pile4k: 0.767\nAUC pile6k: 0.754\nAUC pile8k: 0.773\nAUC pile10k: 0.771\nFigure 4: Mean ROC for varying quantities of prompts.\nNumber\nof Prompts\nm# TP0 1 2 3 4 5 6 7 8 9\n150 ✗ ✓ ✗ - - - ✓ ✗ - ✓ 3\n500 ✗ ✗ ✗ - - - - - ✓ - 1\n1000 ✗ ✗ - - - - - ✗ ✓ - 1\n2000 ✗ ✓ ✓ ✓ - - - ✓ ✓ ✓ 6\n4000 ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✓ ✓ 5\n6000 ✗ ✓ ✓ - - - - ✗ ✓ ✓ 4\n8000 ✗ ✓ ✗ - - - - ✗ ✓ - 2\n10000 - ✓ ✗ - - - - ✓ ✓ ✓ 4\nBERT + IB + P1 +P2 ✓ ✓ ✓ - ✗ ✓ ✓ ✓ ✓ ✓ 8\nTable 3: Model Attributions on F using a varying num-\nber of prompts from The Pile.\ndition, as the KR pretrained model performed the\nstrongest. For these experiments we do not use RL\nprompt selection.\nThe results of this experiment are shown in Fig-\nure 4. By increasing the number of prompts that\na classiﬁer is able to use for classiﬁcation, we see\nthat there is an improvement in the AUC, with di-\nminishing returns from 6,000 prompts onward.\nIncreasing the number of prompts improves the\nAUC, yet does not lead to direct improvement in\nterms of the attribution accuracy as shown in Ta-\nble\n3. In fact, increasing the number of prompts\nused for classiﬁcation leads to a highly variable\nperformance. None of the models that directly use\nthese prompts (150 - 10K prompts from the pile)\nare able to improve or even match that of the pre-\ntrained model using 150 prompts from Table 1.\nLesson Learned: Increasing the number of\nprompts for attribution does not lead to reli-\nable improvements in the number of models\ncorrectly attributed.\n5.5 Effects of pretraining attributors\nWe next aim to investigate how the size of the pre-\ntraining data effects the performance of the attri-\nNumber of\nPrompts\nm# TP0 1 2 3 4 5 6 7 8 9\n150 ✗ - ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✓ 5\n500 ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ 6\n1000 ✗ ✓ ✓ ✓ ✗ ✓ ✗ - ✓ ✓ 6\n2000 ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ ✓ 7\n4000 ✓ ✓ ✓ - ✓ ✓ ✓ ✓ ✗ ✓ 8\n6000 ✓ ✓ ✓ - ✗ ✓ ✓ ✓ ✓ ✓ 8\n8000 ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ ✓ 7\n10000 ✓ ✓ ✓ - ✗ ✓ ✓ ✓ ✓ ✓ 8\nBERT + IB + P1 +P2 ✓ ✓ ✓ - ✗ ✓ ✓ ✓ ✓ ✓ 8\nTable 4: Model Attributions on F from the models\npretrained on different portion from P2, and then ﬁne-\ntuned with P1.\nbution, as while using increasingly large data for\ndirect attribution may not improve performance,\nSection 5.3 shows that using it as pretraining data\ndoes improve attribution.\nTo this end each model discussed in Section 5.4\nis ﬁnetuned under KR, varying the size of pretrain-\ning data from 150 prompt responses to 10,000.\nWe report the results of the experiment in Fig-\nure 5. In Figure 5a we see that the ﬁnetuned models\nare able to improve over the equivalent models in\nFigure 4. Yet they do not improve on the AUC of\nmodels trained under KU conditions.\nWe see from Figure 5b that increasing the num-\nber of prompts minimally improves the precision\nand recall of attribution, with little correlation be-\ntween number of prompts, even of a varied set like\nTHEPILE . Whilst these pretrained-ﬁnetuned attrib-\nutors are able to improve on the precision of the\nattributor using manual selected prompts, however\nthey are unable to improve on the recall.\nWhat is most important for this task, however, is\nthe ability of attribution, hence we also determine\nthe model attributions for each model in Table 4.\nThe models that have been pretrained on a larger\nnumber are able to outperform the KR model of\nSection 5.3 attributing 8 models correctly in the the\nmodels pretrained on 4k and 6k prompts.\nLesson Learned: Pretraining attributors is\nvital to improve the attribution performance.\nHowever, this has to diminishing returns in\nterms of correct attributions and AUC.\n5.6 Effects of Finetuning on Attribution\nThe type and duration of the ﬁnetuning conducted\non a base model B can effect attribution perfor-\nmance. To investigate this we use of two base\nmodels: distilgpt2 and Multilingual-MiniLM and\nﬁnetune them using three datasets: IMDB ( Maas\net al. , 2011), GLUE ( Wang et al. , 2018), Amazon\n7429\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC pile150: 0.783\nAUC pile500: 0.773\nAUC pile1k: 0.785\nAUC pile2k: 0.787\nAUC pile4k: 0.775\nAUC pile6k: 0.765\nAUC pile8k: 0.801\nAUC pile10k: 0.778\n(a) ROC and AUC of attributors with pretrained models\nusing different pretraining data sizes.\n0 2000 4000 6000 8000 10000\nNumber of Prompts\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1\nPile prompts\nPile and Manual Prompts\nManual Prompts\n(b) F1 score of attributors with pretrained models using\ndifferent pretraining data sizes. Manual=150 selected.\nFigure 5\n1 2 3 4 5 6 7 8 9 10\nNumber  of F inetuning Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1\nDISTILGPT2 IMDB\nDISTILGPT2 GL UE\nDISTILGPT2 AMAZON\nDISTILGPT2 OSCAR\nMLMINI IMDB\nMLMINI GL UE\nMLMINI AMAZON\nMLMINI OSCAR\nFigure 6: F1 scores of DistilGPT2 and MLMINI attrib-\nutors.\nreviews Multilingual ( Keung et al. , 2020), and the\nTajik language subset of OSCAR ( Abadji et al. ,\n2022).\nUsing such datasets more closely models the re-\nalistic attack scenario where common pre-training\nprompt sets are used in an attempt to determine at-\ntribution, and ﬁne-tuning datasets are often propri-\netary and/or unique to the application. Conducting\nexperiments in this scenario in a controlled setting\nallows us to study the effect of ﬁnetuning on attri-\nbution in terms of (a) number of epoch and (b) size\nof dataset.\nEffect of Finetuning Epochs: Firstly, we study\nthe effect of the number of ﬁnetuning epochs has\non attribution. Figure 6 shows the F1 score of the\nMLMini and distilgpt2 attributors when trying to\nattribute the ﬁnetuned base models.\nThe MLMini attributor is greatly affected ini-\ntially by MLMini being ﬁnetuned on IMDB, how-\never as with the model ﬁnetuned on Amazon re-\nviews there is an increase in attribution perfor-\nmance with increasing ﬁnetuning epochs. Con-\nversely, the MLMini model ﬁnetuned on GLUE\nMNLI had minimal change in performance only\nwith anomalous increased F1 score at epoch 6.\nHowever, when trying to attribute MLMINI ﬁne-\ntuned with the Tajik subset of OSCAR we see that\nthe F1 score is signiﬁcantly worse. We speculate\nthat AMAZON and IMDB datasets are similar to\nthe pretraining dataset of MLMini (CC-100) and\nthat the AMAZON reviews, with its 6 languages,\nare the most similar to this. In fact, the CC-100\nis likely to have an overlap in the data distribu-\ntion of all three of these datasets as all are openly\navailable. As there is no Tajik in CC-100 it is out-\nof-distribution (OOD) of MLMINI’s pretraining\ndataset, which leads to the poor performance in\nattribution.\nWith the attributor for distilgpt2 there is poor\nperformance in all datasets regardless of the num-\nber of epochs. This follows due to the ﬁnetuning\ndatasets being OOD relative the the pretraining\ndata of distilgpt2 which used the OpenWebTextCor-\npus. As OpenWebTextCorpus is mostly in English,\nﬁnetuning in other languages such as those in the\nAMAZON dataset, makes attribution harder.\nLesson Learned:The attribution performance\nis dominated by the similarity of the ﬁne-\ntuning dataset to the pre-training dataset,\nrather than the amount of ﬁne-tuning con-\nducted.\nEffects of Dataset Size: In addition to the num-\nber of ﬁnetuning epochs we consider the overall\n7430\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n% of Dataset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1\nDISTILGPT2 IMDB\nDISTILGPT2 GL UE\nDISTILGPT2 AMAZON\nDISTILGPT2 OSCAR\nMLMINI IMDB\nMLMINI GL UE\nMLMINI AMAZON\nMLMINI OSCAR\nFigure 7: F1 of DistilGPT2 and MLMINI attributors\nunder varying dataset size, relative to original dataset.\nsize of the ﬁnetuning set on attribution. We report\nthe results of using a ﬁxed 10 epochs and varying\nthe ﬁnetuning dataset size in Figure 7. We can\nsee similar effects as in Figure 6, that the OOD\ndatasets for Distilgpt2 lead to poor F1 scores, and\nconsequently, poor attribution results.\nFor MLMINI we see similar performance on\nIMDB and AMAZON (two of the in-distibution\ndatasets) with an increased F1 as the dataset size\nincreases. When ﬁnetuning on OSCAR and GLUE\nthe F1 score shows a minimal correlation with\ndataset size. This again follows from Figure\n6.\nOSCAR is OOD for MLMINI, which makes attri-\nbution signiﬁcantly harder. Similarly GLUE offers\nthe most varied dataset making attribution harder\nand giving lower F1.\nLesson Learned:Training on a richer dataset\nbroadly improves results if it is within distri-\nbution.\nEffects of Dataset: Across Figures 6 and 7 we\nsee the effect of different ﬁnetuning datasets on the\nability to attribute to base models.\nWe can observe the effect of the ﬁnetuning\ndatasets on the ability to attribute to base models in\nFigures\n6 and 7. These ﬁgures show the distribution\nof the dataset greatly affects attribution. Finetuning\ndatasets that are completely out of distribution in\nrelation to the original pre-training dataset severely\nimpact attribution performance. This is particularly\napparent in MLMINI where ﬁnetuning on OSCAR\nleads to poor attribution performance in Figure 6\nand 7.\nBoth base models ﬁnetuned with GLUE also\nmake attribution harder. We reason that this is due\nto the broad range of prompts that are not typical\nof a ﬁnetuning dataset. This leads the model to\nproduce generic responses to the targeted prompts\nused for attribution.\nLesson Learned:The most signiﬁcant impact\non attribution is the distribution and variety\nof the ﬁnetuing dataset.\n6 Conclusion\nIn this work we have taken initial steps in the LLM\nattribution problem. We study LLM attribution in\nKU and KR settings which limit access to B and\nF to different levels. We argue this prevents trivial\nsolutions in white-box settings, and provides an\ninteresting and realistic study of LLM attribution.\nWe have considered a variety of different LLMs\nthat are trained on different datasets, and for dif-\nferent purposes. We postulate that the 10 differ-\nent LLMs provide a didactic range of models for\nLLM attribution. In our experiments, we have used\npre-existing LLMs that have been ﬁne-tuned by the\nopen-source community to demonstrate the applica-\nbility of our methodology. To mitigate the potential\nfor bias this causes, we have tried out best to ensure\nthe ﬁne-tuning task and dataset of such models is\nknown. In addition, we ﬁne-tune a subset of these\nmodels in an ablation study, which demonstrates\nthe effect that such ﬁne-tuning has on LLM attri-\nbution in a controlled environment. Our ablation\nstudy also studies the effect that OOD ﬁne-tuning\ndatasets have on attribution. This mitigates the ef-\nfect of only ﬁne-tuning within distribution (of the\npre-training data).\nOverall, our work contributes to the growing\nunderstanding of LLM attribution, laying the foun-\ndation for future advancements and developments\nin this domain.\n7431\nLimitations\nWe have considered a variety of different LLMs\nin order to study attribution. However we have\nonly considered a small sample of the different\nLLM architectures and training strategies. This\nhas been with a view to using a small but diverse\nset of LLMs. Of these 10 base models, we tested\nour approach to attribution on a controlled set of\nﬁne-tuned models. While a study that considers a\nwider variety and larger scale of ﬁne-tuned models\nwould be beneﬁcial to the problem of attribution,\nthe computation resources limited our study.\nFurthermore, in our assumptions in this work\nwe consider that there is a one-to-one mapping be-\ntween mf and mb. However, this is not necessarily\nthe case. There could be an m-to-n mapping and\nalso a model may be present in one set, but not the\nother.\nWe believe there is rich space for further research\nin this area that can address these limitations, and\nfurther develop the problem of attribution.\nEthics Statement\nIn the discussion we have highlighted how the tech-\nniques for attributing ﬁne-tuned models to their\npre-trained large language models can be used as a\ntool to mitigate issues such as violation of model\nlicenses, model theft, and copyright infringement,\nbut this is only a subset of the issues related to\nauthorship attribution. The increasing quality and\ncredibility of LLM generated text has recently high-\nlighted ethical issues such as plagiarism 8 or the\nbanning of users for submitting AI generated re-\nsponses to answer questions. 9 Even within the sci-\nentiﬁc community discussions are arising related\nto topics such as the authorship of papers or codes,\nwho owns what is it generated? Many AI con-\nferences have banned the submission of entirely\nself-generated scientiﬁc papers. 10\nThese are some examples of controversial sit-\nuations, but the use of AI-generated content has\nethical implications in several domains that depend\non the speciﬁc context and application. It is there-\nfore crucial, as a ﬁrst step to tackle these ethical\nissues, to ensure that any AI-generated contents\n8New bot ChatGPT will force colleges to get creative to\nprevent cheating, experts say\n9AI-generated answers temporarily banned on coding\nQ&A site Stack Overﬂow\n10Top AI conference bans use of ChatGPT and AI language\ntools to write academic papers\nare clearly labeled as such and are not presented as\noriginal work without proper attribution (whether\nit’s a person or a base model).\nAcknowledgements\nThis work was supported by European Union’s\nHorizon 2020 research and innovation programme\nunder grant number 951911 – AI4Media.\n7432\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBenoît Sagot. 2022. Towards a cleaner document-\noriented multilingual crawled corpus . In Proceedings\nof the Thirteenth Language journal and Evaluation\nConference, pages 4344–4355, Marseille, France. Eu-\nropean Language Resources Association.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorﬂow.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners . In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.\nExtracting Training Data from Large Language Mod-\nels. In Proceedings of the 30th USENIX Security\nSymposium. arXiv.\nHuili Chen, Bita Darvish Rouhani, Cheng Fu, Jishen\nZhao, and Farinaz Koushanfar. 2019. DeepMarks: A\nSecure Fingerprinting Framework for Digital Rights\nManagement of Deep Learning Models . In Proceed-\nings of the 2019 on International Conference on Mul-\ntimedia Retrieval , ICMR ’19, pages 105–113, New\nYork, NY , USA. Association for Computing Machin-\nery.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting\n. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020.\nUnsupervised\nCross-lingual Representation Learning at Scale . In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nNathan Coooper, Artashes Arutiunian, Santiago\nHincapié-Potes, Ben Trevett, Arun Raja, Erfan Hos-\nsami, and Mrinal Mathur. 2021. Code Clippy Data:\nA large dataset of code data from Github for research\ninto code language models .\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof Wikipedia: Knowledge-Powered Conversational\nAgents.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020a. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 .\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020b.\nThe Pile: An\n800GB Dataset of Diverse Text for Language Model-\ning. ArXiv:2101.00027 [cs].\nShivali Goel, Rishi Madhok, and Shweta Garg. 2018.\nProposing Contextually Relevant Quotes for Im-\nages. In Advances in Information Retrieval , Lecture\nNotes in Computer Science, pages 591–597, Cham.\nSpringer International Publishing.\nWilliam L. Hamilton, Rex Ying, and Jure Leskovec.\n2017. Inductive representation learning on large\ngraphs. In Proceedings of the 31st International Con-\nference on Neural Information Processing Systems ,\nNIPS’17, page 1025–1035, Red Hook, NY , USA.\nCurran Associates Inc.\nSorami Hisamoto, Matt Post, and Kevin Duh. 2020.\nMembership Inference Attacks on Sequence-to-\nSequence Models: Is My Data In Your Machine\nTranslation System?\nTransactions of the Associa-\ntion for Computational Linguistics , 8:49–63. Place:\nCambridge, MA Publisher: MIT Press.\nDorjan Hitaj, Briland Hitaj, and Luigi V . Mancini. 2019.\nEvasion Attacks Against Watermarking Techniques\nfound in MLaaS Systems . In 2019 Sixth Inter-\nnational Conference on Software Deﬁned Systems\n(SDS), pages 55–63.\nPhillip Keung, Yichao Lu, György Szarvas, and Noah A.\nSmith. 2020. The multilingual Amazon reviews cor-\npus. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4563–4568, Online. Association for\nComputational Linguistics.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro V on Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, Jörg\nFrohberg, Mario Šaško, Quentin Lhoest, Angelina\nMcMillan-Major, Gérard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben Allal, Francesco De Toni,\n7433\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Romero Muñoz,\nJian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid\nAlmubarak, Vu Minh Chien, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ife-\noluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas\nPai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-\ngaret Mitchell, Sasha Luccioni, and Yacine Jernite.\n2022.\nThe BigScience ROOTS Corpus: A 1.6TB\nComposite Multilingual Dataset . In Proceedings of\nthe 36st International Conference on Neural Infor-\nmation Processing Systems .\nErwan Le Merrer, Patrick Pérez, and Gilles Trédan.\n2020. Adversarial frontier stitching for remote neu-\nral network watermarking . Neural Computing and\nApplications, 32(13):9233–9244.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A Constrained Text Gen-\neration Challenge for Generative Commonsense Rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 .\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach . ArXiv:1907.11692 [cs].\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis .\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nChristopher B. Mann. 2021. Can conversing with a com-\nputer increase turnout? mobilization using chatbot\ncommunication\n. Journal of Experimental Political\nScience, 8(1):51–62.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nat McAleese.\n2022. Teaching language models to support answers\nwith veriﬁed quotes . ArXiv:2203.11147 [cs].\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer Sentinel Mixture Mod-\nels. In Proceedings of the 5th International Confer-\nence on Learning Representations .\nFatemehsadat Mireshghallah, Archit Uniyal, Tianhao\nWang, David Evans, and Taylor Berg-Kirkpatrick.\n2022. Memorization in NLP Fine-tuning Methods .\nArXiv:2205.12506 [cs].\nNur Azmina Mohamad Zamani, Jasy Suet Yan Liew,\nand Ahmad Muhyiddin Yusof. 2022. XLNET-\nGRU sentiment regression model for cryptocurrency\nnews in English and Malay . In Proceedings of\nthe 4th Financial Narrative Processing Workshop\n@LREC2022, pages 36–42, Marseille, France. Euro-\npean Language Resources Association.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight,\nBenjamin Chess, and John Schulman. 2022.\nWe-\nbGPT: Browser-assisted question-answering with hu-\nman feedback . ArXiv:2112.09332 [cs].\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nICLR.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales . In Proceedings of the\n43rd Annual Meeting of the Association for Compu-\ntational Linguistics (ACL’05) , pages 115–124, Ann\nArbor, Michigan. Association for Computational Lin-\nguistics.\nMatthew E. Peters, Sebastian Ruder, and Noah A. Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks . In Proceedings of\nthe 4th Workshop on Representation Learning for\nNLP (RepL4NLP-2019) , pages 7–14, Florence, Italy.\nAssociation for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer .\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nErwin Quiring, Daniel Arp, and Konrad Rieck. 2018.\nForgotten Siblings: Unifying Attacks on Machine\nLearning and Digital Watermarking\n. In 2018 IEEE\nEuropean Symposium on Security and Privacy (Eu-\nroS&P), pages 488–502.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\n7434\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. 2022. Measuring Attribution in Natural Lan-\nguage Generation Models . ArXiv:2112.12870 [cs].\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter .\nArXiv:1910.01108 [cs].\nTeven Le Scao and et al. 2022. BLOOM: A\n176B-Parameter Open-Access Multilingual Lan-\nguage Model . ArXiv:2211.05100 [cs].\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners\n. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms . CoRR, abs/1707.06347.\nR. Shokri, M. Stronati, C. Song, and V . Shmatikov. 2017.\nMembership inference attacks against machine learn-\ning models . In 2017 IEEE Symposium on Security\nand Privacy (SP) , pages 3–18, Los Alamitos, CA,\nUSA. IEEE Computer Society.\nGowthami Somepalli, Vasu Singla, Micah Goldblum,\nJonas Geiping, and Tom Goldstein. 2022. Diffusion\nArt or Digital Forgery? Investigating Data Replica-\ntion in Diffusion Models . ArXiv:2212.03860 [cs].\nKai-Wen Tuan, Yi-Jyun Chen, Yi-Chien Lin, Chun-Ho\nKwok, Hai-Lun Tu, and Jason S. Chang. 2021. Learn-\ning to ﬁnd translation of grammar patterns in parallel\ncorpus. In Proceedings of the 33rd Conference on\nComputational Linguistics and Speech Processing\n(ROCLING 2021) , pages 301–309, Taoyuan, Taiwan.\nThe Association for Computational Linguistics and\nChinese Language Processing (ACLCLP).\nYusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and\nShin’ichi Satoh. 2017. Embedding Watermarks into\nDeep Neural Networks . In Proceedings of the 2017\nACM on International Conference on Multimedia\nRetrieval, ICMR ’17, pages 269–277, New York, NY ,\nUSA. Association for Computing Machinery.\nJan Philip Wahle, Terry Ruas, Frederic Kirstein, and\nBela Gipp. 2022. How large language models are\ntransforming machine-paraphrase plagiarism. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing . Association for\nComputational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding . In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355, Brussels, Belgium. Association for\nComputational Linguistics.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nTianhao Wang and Florian Kerschbaum. 2019. Attacks\non Digital Watermarks for Deep Neural Networks .\nIn ICASSP 2019 - 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 2622–2626. ISSN: 2379-190X.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021. MiniLMv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers . In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021 ,\npages 2140–2151, Online. Association for Computa-\ntional Linguistics.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,\nSijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-\nsuasion for good: Towards a personalized persuasive\ndialogue system for social good . In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 5635–5649, Florence,\nItaly. Association for Computational Linguistics.\nJason Wei, Chengyu Huang, Soroush V osoughi,\nYu Cheng, and Shiqi Xu. 2021. Few-Shot Text Clas-\nsiﬁcation with Triplet Networks, Data Augmentation,\nand Curriculum Learning\n. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 5493–5500, Online.\nAssociation for Computational Linguistics.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia\nHaas, Laura Rimell, Lisa Anne Hendricks, William\nIsaac, Sean Legassick, Geoffrey Irving, and Iason\nGabriel. 2021. Ethical and social risks of harm from\nlanguage models .\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding . In Advances in Neural Infor-\nmation Processing Systems , volume 32. Curran Asso-\nciates, Inc.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor\nMihaylov, Myle Ott, Sam Shleifer, Kurt Shuster,\nDaniel Simig, Punit Singh Koura, Anjali Sridhar,\nTianlu Wang, and Luke Zettlemoyer. 2022.\nOPT:\nOpen Pre-trained Transformer Language Models .\nArXiv:2205.01068 [cs].\n7435\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-scale\ngenerative pre-training for conversational response\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations , pages 270–278, Online. As-\nsociation for Computational Linguistics.\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravin-\ndran, Sindhu Tipirneni, and Chandan K. Reddy. 2022.\nXLCoST: A Benchmark Dataset for Cross-lingual\nCode Intelligence . ArXiv:2206.08474 [cs].\nA Heuristic Approaches\nA.1 Perplexity\nUsing the response of F we can calculate the per-\nplexity of B relative to F. This can then be used\nas a measure of how conﬁdent B is in predicting\nF, where a lower perplexity would indicate higher\nconﬁdence and attribution. In our initial experi-\nments, we found this to be loose approximation of\nsimilarity between models in B and F. Moreover,\nthis approach assumed stronger access which is\ntypically not available in real-world settings as we\ndiscussed in Section 3.\nPerplexity is a measure of how well a model is\nable to predict a sample. It has previously been\nused in analogous settings for extracting training\ndata from language models (\nCarlini et al. , 2021;\nMireshghallah et al. , 2022) to determine if a model\nis conﬁdent in its prediction of a sample. It is pos-\nsible to leverage this for the purpose of attributing\nF to B. By collecting responses of F to prompts\nwe can calculate the perplexity of B relative to F.\nThus we can take the perplexity score as a measure\nof how conﬁdent B is in predicting the response of\nF, we would expect lower perplexity to be an in-\ndication of higher conﬁdence and therefore higher\nchances of attribution.\nA.2 Heuristic Decision Tree\nWhen it comes to generalisation, many LLMs share\nan equal footing owing to the massive size and in-\ntensive training backing their capabilities. How-\never, when examined closely there are distinctive\nfeatures that set them apart which can be detected\nvia static or dynamic inspection of the model. For\ninstance, LLMs with a larger number of parameters\ntend to take longer for inference. Similarly, length\nof response varies across LLMs, and some are\nprone to repetition (such as XLNET ( Mohamad Za-\nmani et al. , 2022)). The task characteristics and\nassociated training data may also help distinguish\ndifferent LLMs. For example, LLMs trained for\nspeciﬁc tasks like chat bots or code generation\nwill have characteristically different output spaces.\nThey may also have unique aspects in their training\ndata like a speciﬁc language or markers such as\ndata collected over speciﬁc time period. Much like\nwatermarking, these can be used to craft prompts\nthat can help reveal these unique artefacts 11.\nWhile in principle many of these heuristics can\nbe used for attribution, the practical development\nof such systems faces a range of challenges. First,\nthese properties may not be preserved across the\nﬁne tuning process and therefore provide no mean-\ningful insight for attribution. Second, these heuris-\ntics require a high level of expertise and knowledge\nwhich may not always be available. An external\nauditor working with the restricted knowledge of\nKR may not be able to develop such solutions.\nThird, many of the properties of models in F can\nbe easily obfuscated by the exposed API. For exam-\nple it is fairly easy to normalise response times or\npost-process the responses to account for repetition.\nMoreover, an API may be simultaneously backed\nby multiple different models which would make the\nattribution even more challenging. Finally, LLMs\noften have overlapping datasets which can dilute\nmany of the subtleties underlying these heuristics.\nThis limits the applicability and scalability of such\napproaches for larger collections of B and F.\nB Fine-tuned model Details\nHere we provide details of the ﬁne-tuned LLMs we\nuse in sets A and F. Each of the LLMs is an open\nsource implementation hosted on the Huggingface,\nwe provide the link to the ﬁne-tuned model. In\nTable 5 we show set F as FT models 0-9 inclusive,\nand set A from 10-19 inclusive. For each model\nwe also provide the dataset used to ﬁne-tune each\nof the LLMs.\nC AUC Curves\nWe provide the ﬁnegrained plots of how each indi-\nvidual hmb did in each experiment. Figure 8 shows\nthe results from the experiment that measures the\nattribution accuracy under different K as discussed\nin Section 5.3. Figure 9 details the effect of using\na different number of prompts for attribution under\nKR, as discussed in Section 5.4. Finally Figure 10\nshows the effect of varying the number of prompts\nfor pretaining hmb (Section 5.5).\n11Winning solution to the ﬁrst MLMAC\n7436\nFT model Base Model FT dataset\n0 bloom-350m common_gen ( Lin et al. , 2020)\n1 OPT-350M Pike, CYS, Manga-v1\n2 DialoGPT-large Persuasion For Good Dataset ( Wang et al. , 2019)\n3 distilgpt2 wikitext2 ( Merity et al. , 2016)\n4 GPT2-XL the Wizard of Wikipedia dataset ( Dinan et al. , 2019)\n5 gpt2 Wikipedia dump, EU Bookshop corpus,\nOpen Subtitles, CommonCrawl, ParaCrawl and News Crawl.\n6\nGPT-Neo-125m Cmotions - Beatles lyrics\n7 xlnet-base-cased IMDB ( Maas et al. , 2011)\n8 multilingual-MiniLM-L12-v2 Unknown\n9 codegen-350M Zhu et al. (2022)\n10 bloom-350m Cmotions - Beatles lyrics\n11 OPT-350M GLUE ( Wang et al. , 2018)\n12 DialoGPT-large The complete works of Sir Arthur Conan Doyle\n13 distilgpt2 Quotes-500K ( Goel et al. , 2018)\n14 GPT2-XL OSCAR ( Abadji et al. , 2022)\n15 gpt2 IMDB ( Maas et al. , 2011)\n16 GPT-Neo-125m Code Clippy Data dataset ( Coooper et al. , 2021)\n17 xlnet-base-cased Rotten Tomatoes ( Pang and Lee , 2005)\n18 multilingual-MiniLM-L12-v2 https://www.tensorﬂow.org/datasets/catalog/wikipedia\n#wikipedia20200301bn\n19 codegen-350M BigPython dataset\nTable 5: Fine-tuned models, their original base models and the datasets they are ﬁne-tuned on.\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.71\nAUC opt-350m:0.82\nAUC DialoGPT -large:0.87\nAUC distilgpt2:0.77\nAUC gpt2-xl:0.62\nAUC gpt2:0.74\nAUC gpt-neo:0.82\nAUC xlnet:0.97\nAUC ML-MiniLM:0.83\nAUC codegen:0.89\n(a) BERT + IB + P1\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.77\nAUC opt-350m:0.8\nAUC DialoGPT -large:0.86\nAUC distilgpt2:0.83\nAUC gpt2-xl:0.68\nAUC gpt2:0.73\nAUC gpt-neo:0.79\nAUC xlnet:0.78\nAUC ML-MiniLM:0.78\nAUC codegen:0.87\n(b) BERT + IB+F + P1\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.7\nAUC opt-350m:0.6\nAUC DialoGPT -large:0.54\nAUC distilgpt2:0.68\nAUC gpt2-xl:0.65\nAUC gpt2:0.84\nAUC gpt-neo:0.53\nAUC xlnet:0.98\nAUC ML-MiniLM:0.74\nAUC codegen:0.84\n(c) BERT + IB + P1\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.66\nAUC opt-350m:0.68\nAUC DialoGPT -large:0.62\nAUC distilgpt2:0.6\nAUC gpt2-xl:0.68\nAUC gpt2:0.77\nAUC gpt-neo:0.59\nAUC xlnet:0.98\nAUC ML-MiniLM:0.83\nAUC codegen:0.84\n(d) BERT + IB + P1+P2\nFigure 8: ROC of Individual base model classiﬁers, hmb, under different K\n7437\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.67\nAUC opt-350m:0.96\nAUC DialoGPT -large:0.7\nAUC distilgpt2:0.54\nAUC gpt2-xl:0.5\nAUC gpt2:0.57\nAUC gpt-neo:0.66\nAUC xlnet:0.68\nAUC ML-MiniLM:0.93\nAUC codegen:0.79\n(a) pile150\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.54\nAUC opt-350m:0.7\nAUC DialoGPT -large:0.66\nAUC distilgpt2:0.46\nAUC gpt2-xl:0.38\nAUC gpt2:0.54\nAUC gpt-neo:0.65\nAUC xlnet:0.6\nAUC ML-MiniLM:0.98\nAUC codegen:0.58\n(b) pile 500\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.62\nAUC opt-350m:0.95\nAUC DialoGPT -large:0.64\nAUC distilgpt2:0.51\nAUC gpt2-xl:0.59\nAUC gpt2:0.48\nAUC gpt-neo:0.67\nAUC xlnet:0.65\nAUC ML-MiniLM:0.98\nAUC codegen:0.7\n(c) pile 1k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.75\nAUC opt-350m:0.97\nAUC DialoGPT -large:0.64\nAUC distilgpt2:0.76\nAUC gpt2-xl:0.71\nAUC gpt2:0.52\nAUC gpt-neo:0.79\nAUC xlnet:0.82\nAUC ML-MiniLM:0.92\nAUC codegen:0.75\n(d) pile 2k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.75\nAUC opt-350m:0.96\nAUC DialoGPT -large:0.68\nAUC distilgpt2:0.77\nAUC gpt2-xl:0.71\nAUC gpt2:0.59\nAUC gpt-neo:0.81\nAUC xlnet:0.82\nAUC ML-MiniLM:0.76\nAUC codegen:0.82\n(e) pile 4k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.73\nAUC opt-350m:0.97\nAUC DialoGPT -large:0.67\nAUC distilgpt2:0.66\nAUC gpt2-xl:0.61\nAUC gpt2:0.52\nAUC gpt-neo:0.8\nAUC xlnet:0.79\nAUC ML-MiniLM:0.98\nAUC codegen:0.81\n(f) pile 6k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.73\nAUC opt-350m:0.97\nAUC DialoGPT -large:0.68\nAUC distilgpt2:0.72\nAUC gpt2-xl:0.69\nAUC gpt2:0.57\nAUC gpt-neo:0.79\nAUC xlnet:0.81\nAUC ML-MiniLM:0.97\nAUC codegen:0.8\n(g) pile 8k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.73\nAUC opt-350m:0.96\nAUC DialoGPT -large:0.67\nAUC distilgpt2:0.71\nAUC gpt2-xl:0.71\nAUC gpt2:0.57\nAUC gpt-neo:0.79\nAUC xlnet:0.83\nAUC ML-MiniLM:0.93\nAUC codegen:0.81\n(h) pile 10k\nFigure 9: ROC of Individual base model classiﬁers, hmb, with different number of prompts used for attribution\nunder KR\n7438\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.67\nAUC opt-350m:0.96\nAUC DialoGPT -large:0.7\nAUC distilgpt2:0.54\nAUC gpt2-xl:0.5\nAUC gpt2:0.57\nAUC gpt-neo:0.66\nAUC xlnet:0.68\nAUC ML-MiniLM:0.93\nAUC codegen:0.79\n(a) pile150\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.54\nAUC opt-350m:0.7\nAUC DialoGPT -large:0.66\nAUC distilgpt2:0.46\nAUC gpt2-xl:0.38\nAUC gpt2:0.54\nAUC gpt-neo:0.65\nAUC xlnet:0.6\nAUC ML-MiniLM:0.98\nAUC codegen:0.58\n(b) pile 500\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.68\nAUC opt-350m:0.88\nAUC DialoGPT -large:0.77\nAUC distilgpt2:0.8\nAUC gpt2-xl:0.64\nAUC gpt2:0.75\nAUC gpt-neo:0.82\nAUC xlnet:0.98\nAUC ML-MiniLM:0.67\nAUC codegen:0.86\n(c) pile 1k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.77\nAUC opt-350m:0.89\nAUC DialoGPT -large:0.76\nAUC distilgpt2:0.64\nAUC gpt2-xl:0.61\nAUC gpt2:0.78\nAUC gpt-neo:0.86\nAUC xlnet:0.97\nAUC ML-MiniLM:0.79\nAUC codegen:0.8\n(d) pile 2k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.72\nAUC opt-350m:0.85\nAUC DialoGPT -large:0.83\nAUC distilgpt2:0.56\nAUC gpt2-xl:0.7\nAUC gpt2:0.79\nAUC gpt-neo:0.8\nAUC xlnet:0.98\nAUC ML-MiniLM:0.69\nAUC codegen:0.83\n(e) pile 4k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.61\nAUC opt-350m:0.86\nAUC DialoGPT -large:0.8\nAUC distilgpt2:0.69\nAUC gpt2-xl:0.55\nAUC gpt2:0.78\nAUC gpt-neo:0.84\nAUC xlnet:0.98\nAUC ML-MiniLM:0.71\nAUC codegen:0.83\n(f) pile 6k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.76\nAUC opt-350m:0.89\nAUC DialoGPT -large:0.8\nAUC distilgpt2:0.62\nAUC gpt2-xl:0.7\nAUC gpt2:0.84\nAUC gpt-neo:0.84\nAUC xlnet:0.98\nAUC ML-MiniLM:0.76\nAUC codegen:0.82\n(g) pile 8k\n0.0 0.2 0.4 0.6 0.8 1.0\nF alse  P ositive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nT rue P ositive Rate\nAUC bloom:0.66\nAUC opt-350m:0.68\nAUC DialoGPT -large:0.62\nAUC distilgpt2:0.6\nAUC gpt2-xl:0.68\nAUC gpt2:0.77\nAUC gpt-neo:0.59\nAUC xlnet:0.98\nAUC ML-MiniLM:0.83\nAUC codegen:0.84\n(h) pile 10k\nFigure 10: ROC of Individual base model classiﬁers, hmb using a varying number of prompts for pretraining hmb.\n7439\nDataset Percentage of prompts in 10,000 subset of the Pile\nPile-CC 25.24\nOpenWebText2 15.20\nPubMed Abstracts 14.23\nStackExchange 13.99\nGithub 8.55\nWikipedia (en) 7.79\nUSPTO Backgrounds 5.14\nPubMed Central 2.59\nFreeLaw 2.41\nNIH ExPorter 1.04\nDM Mathematics 0.99\nArXiv 0.91\nHackerNews 0.81\nEnron Emails 0.47\nOpenSubtitles 0.27\nYoutubeSubtitles 0.11\nBooks3 0.09\nEuroParl 0.06\nPhilPapers 0.05\nBookCorpus2 0.02\nUbuntu IRC 0.02\nGutenberg (PG-19) 0.02\nTable 6: Distribution of the original datasets present in the 10,000 prompt subset of The Pile\nD The Pile subset\nWe make use of a 10,000 prompt subset of The\nPile ( Gao et al. , 2020b), in Table 6 we report the\ndistrubtion of the smaller datasets present in The\nPile.\n7440\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□\u0013 A2. Did you discuss any potential risks of your work?\n7\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n5\n□\u0013 B1. Did you cite the creators of artifacts you used?\n5, B\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo, the artifacts used all have open source licences, and are freely avaliable at their respective\ncitations.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe make use of data used to train/ﬁne-tune language models, this is consistent with their intended\nuse. We don’t discuss this use explicity relative to the intended us, but do discuss the use of these\nartifacts for ﬁnetuning purposes in Section 5, B.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo as none of the data used contains information that names or uniquely identiﬁes individual people\nor offensive content.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nB,D\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nB\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7441\nC □\u0013 Did you run computational experiments?\n5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n5\n□\u0017 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe utilised the BERT using default parameter values used in the original paper and then used this\nmodel to develop a classiﬁer.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n5, B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n7442",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6704397201538086
    },
    {
      "name": "Foley",
      "score": 0.6648229956626892
    },
    {
      "name": "Matching (statistics)",
      "score": 0.5588029026985168
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5087676644325256
    },
    {
      "name": "Natural language processing",
      "score": 0.45274975895881653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41518133878707886
    },
    {
      "name": "Language model",
      "score": 0.41352713108062744
    },
    {
      "name": "Statistics",
      "score": 0.14277541637420654
    },
    {
      "name": "Mathematics",
      "score": 0.14035293459892273
    },
    {
      "name": "Physics",
      "score": 0.07957407832145691
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    }
  ]
}