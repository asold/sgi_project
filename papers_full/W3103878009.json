{
  "title": "Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation",
  "url": "https://openalex.org/W3103878009",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5100415705",
      "name": "Pei Zhang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5019715118",
      "name": "Boxing Chen",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5016546383",
      "name": "Niyu Ge",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5055225377",
      "name": "Kai Fan",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2767019613",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W2970316683",
    "https://openalex.org/W2798931235",
    "https://openalex.org/W2982588609",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W4300428972",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963206679",
    "https://openalex.org/W2964289193",
    "https://openalex.org/W2983108239",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3102507836",
    "https://openalex.org/W2982644924",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2971347700",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2962943802"
  ],
  "abstract": "Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1081–1087,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n1081\nLong-Short Term Masking Transformer: A Simple but Effective Baseline\nfor Document-level Neural Machine Translation\nPei Zhang, Boxing Chen, Niyu Ge, Kai Fan∗\nAlibaba Group Inc.\n{xiaoyi.zp,boxing.cbx,niyu.ge,k.fan}@alibaba-inc.com\nAbstract\nMany document-level neural machine transla-\ntion (NMT) systems have explored the utility\nof context-aware architecture, usually requir-\ning an increasing number of parameters and\ncomputational complexity. However, few at-\ntention is paid to the baseline model. In this pa-\nper, we research extensively the pros and cons\nof the standard transformer in document-level\ntranslation, and ﬁnd that the auto-regressive\nproperty can simultaneously bring both the ad-\nvantage of the consistency and the disadvan-\ntage of error accumulation. Therefore, we\npropose a surprisingly simple long-short term\nmasking self-attention on top of the standard\ntransformer to both effectively capture the\nlong-range dependence and reduce the propa-\ngation of errors. We examine our approach\non the two publicly available document-level\ndatasets. We can achieve a strong result in\nBLEU and capture discourse phenomena.\n1 Introduction\nRecent advances in deep learning have led to signif-\nicant improvement of Neural Machine Translation\n(NMT) (Sutskever et al., 2014; Bahdanau et al.,\n2014; Luong et al., 2015; Vaswani et al., 2017).\nParticularly, the performance on the sentence-level\ntranslation of both low- and high- resource lan-\nguage pairs is dramatically improved (Kudugunta\net al., 2019; Lample et al., 2018; Lample and Con-\nneau, 2019). However, when translating text with\nlong-range dependencies, such as in conversations\nor documents, the original mode of translating one\nsentence at a time ignores the discourse phenom-\nena (V oita et al., 2019a,b), introducing undesirable\nbehaviors such as inconsistent pronouns across dif-\nferent translated sentences.\nDocument-level NMT, as a more realistic transla-\ntion task in these scenarios, has been systematically\n∗corresponding author.\ninvestigated in the machine translation community.\nMost literatures focused on looking back a ﬁxed\nnumber of previous source or target sentences as\nthe document-level context (Tu et al., 2018; V oita\net al., 2018; Zhang et al., 2018; Miculicich et al.,\n2018; V oita et al., 2019a,b). Some latest works\ninnovatively attempted to either get the most out of\nthe entire document context or dynamically select\nthe suitable context (Maruf and Haffari, 2018; Yang\net al., 2019a; Maruf et al., 2019; Jiang et al., 2019).\nBecause of the scarcity of document training data,\nthe beneﬁt gained from such an approach, as re-\nﬂected in BLEU, is usually limited. We therefore\nelect to pay attention to the context in the previous\nnsentences only where nis a small number and\nusually does not cover the entire document.\nAlmost all of the latest studies chose the standard\ntransformer model as their baseline which trans-\nlates each sentence in the document with the model\ntrained on the sentence-level data. The cohesion\nand consistency are in general poor. A more rea-\nsonable baseline is to train the transformer with the\ncontext prepended, and this modiﬁcation could be\nsimply implemented via data preprocessing. Baw-\nden et al. (2018) conducted a detailed analysis of\nRNN-based NMT models on the topic of whether\nor not to include the extended context. Consistency\nand precision is often viewed as a trade-off of each\nother. We conduct a detailed analysis of the effect\nof document context on consistency in transformer\narchitecture accepting multi-sentence input.\nWhen it comes to leveraging the contextual in-\nformation, the common approach is to model the\ninteraction between the sentence and its context\nwith specially designed attention modules (Kim\net al., 2019). Such works tend to include more\nthan one encoder or decoder, with a substantial\nnumber of parameters and additional computations.\nIn our work, we reduce the contextual and regu-\nlar attention modules into one single encoder and\n1082\ndecoder. Our idea is motivated by the one trans-\nformer decoder with the two-stream self-attention\n(Yang et al., 2019b). In particular, we maintain\ntwo different sets of hidden states and employ two\ndifferent masking matrices to capture the long and\nshort term dependencies.\nThe contributions of this paper are threefold:\ni) we extensively research the performance of\nthe standard transformer in the setting of multi-\nsentence input and output; ii) we propose a simple\nbut effective modiﬁcation to adapting the trans-\nformer for document NMT with the aim of ame-\nliorating the effect of error accumulation; iii) our\nexperiments demonstrate that even the simple base-\nline can achieve comparable results.\n2 The Proposed Approach\nThe standard transformer NMT follows the typical\nencoder-decoder architecture with using stacked\nself-attention, pointwise fully connected layers,\nand the encoder-decoder attention layers. The self-\nattention in the decoder allows only those positions\nfrom the left up to the current one to be attended to,\npreventing information ﬂow to the right beyond the\ncurrent target and preserving the auto-regressive\nproperty. The illegal connections will be masked\nout by setting as −∞before the softmax operation.\nThe attention probability can be succinctly written\nin a uniﬁed formulation.\nA= Softmax\n(\nQK⊤\n√\nd/h\n+ M\n)\n(1)\nwhere the matricesQ,K represent queries and keys\nin attention module (Vaswani et al., 2017), and\nM is the masking matrix. For the encoder self-\nattention and the encoder-decoder attention, M =\n0. For the decoder self-attention, M is an upper\ntriangular matrix with zero on the diagonal and\nnon-zero (−∞≈− 109) everywhere else.\n2.1 Long-Short Term Masking Transformer\nThe basic setup in this work is multi-sentence in-\nput and output, denoted as k-to-kmodel. In other\nwords, both the encoder and decoder need to con-\nsume k sentences during training and inference.\nTherefore, in our modiﬁed transformer, the reg-\nular self-attention is substituted by the long-short\nterm masking self-attention (illustrated in Figure 1).\nWhile the idea of most context-aware model is to\nintroduce another isolated attention module, we\npropose to maintain two stream attentions via the\nJulia            saw                a                cat             <eos>                It                 was          hungry.<sos>          Julia              saw               a                cat                <eos>               It              was           hungry\nwas\nMulti-head AttentionMulti-head AttentionQQK, V K, V\nJuliasawacat<eos>Itwashungry.\nJuliasawacat<eos>Itwashungry.\nlocal attention maskfor the encoder local attention maskfor the decoder <sos>Juliasawacat<eos>Itwashungryinput\noutput\nFigure 1: Illustration of the Long-Short Term Mask-\ning Self-Attention. Green nodes: global self-attention,\nwhich is the same as the standard self-attention. Pink\nnodes: local self-attention, which does not have access\nto the information from the document context. The red\ndash lines is removed in the decoder attention.\nlocal and global representations, but the parameters\nto calculate queries, keys and values are shared.\nThe global self-attention, simply following the\ncalculation in Eq (1), serves a similar role to the\nstandard hidden states in transformer. The keys and\nvalues can broadly look around from the ﬁrst token\nto the last one, and the global hidden state of the\nnext layer will summarize the information of both\nthe context and current sentence. The query vector\ndirectly comes from the global hidden states of the\nprevious layer via a fully connect layer.\nThe local self-attention only accesses the infor-\nmation of the current sentence, where the contex-\ntual information from the previous sentence(s) is\nblocked when computing the keys and values. Sim-\nilar to the masking strategy of the transformer de-\ncoder, the implementation of the local self-attention\nis to mask out the tokens of the context via −∞in-\nside the scaled dot-product operation. Figure 1\ndepicts the masking matrices of the local self-\nattention for the encoder and decoder respectively.\nThey are both diagonal block matrices, where each\nblock represents the local self-attention of cur-\nrent sentence and the blank and maroon dots de-\nnote value 0 and −∞. When calculating attention\nweights, we only need to replace the M in Eq (1)\nwith the block masking matrices.\nFor the two sets of hidden representations in\nthe ﬁnal layer, we can either aggregate them with\nelement-wise operation (such as summation or con-\ncatenation) or directly use global states to pre-\ndict the distribution of target language model. In\nour work, we adopt the concatenation, and subse-\n1083\nSrc “在死之前，我想种一棵树” “在死之前，我想过隐居生活”\n“在死之前，我想在抱她一次”\nRef “Before I die, I want to plant a tree.” “Before I die, I want to live\noff the grid.” “Before I die, I want to hold her one more time.”\nSys0 “Before death, I want a tree.” “Before I die, I want to live lives.”\n“Before death, I want to hug her again.”\nSys1 I want to be a tree before I die. “Before death, I want to become\ninvisible.” “Before death, I want to hug her again.”\nSys2 “I want to create a tree before I die.” “Before I die, I want to\nlive a hidden life.” “Before I die, I want to hug her again.”\nSrc 在左边你能看到一个小船。这是一个约15英尺的船。我想让\n你们注意冰山的形状它在水面上的变形。\nRef\nYou can see on the left side a small boat. That’s about a 15 foot\nboat. And I’d like you to pay attention to the shape of the iceberg\nand where it is at the waterline.\nSys0 On the left you see a small boat. It’s about 15 feet. I want you to\nlook at the shape of the iceberg that it deformed on the water.\nSys1\nOn the left you can see a small boat. This is a ship about 15 feet.\nI want you to notice the shape of the iceberg which is distorted\non the water.\nSys2\nOn the left you see a small boat. This is a 15 foot boat. I want\nyou to pay attention to the shape of the iceberg that’s distorted\non the surface of the water.\nTable 1: Examples of translation results. Sys0: 1-to-1\ntransformer. Sys1: 3-to-3 transformer. Sys2: 3-to-3\nlong-short term masking transformer.\nquently transform them via a fully connected layer\nto reduce dimensionality.\n3 Experiments\nExperimental Setup\nWe carry out experiments with the Chinese-\nEnglish IWSLT TED talks dataset 1 and English-\nRussian open-subtitle dataset2. The widely used\nZh-En IWSLT dataset contains around 200K train-\ning sentence pairs divided into 1713 documents.\nAs is the convention, dev2010 and tst2010-2013\nare used for validation and testing respectively. The\nEn-Ru subtitle dataset contains around 1.5M con-\nversations, where each conversation includes ex-\nactly 4 sentences. Two randomly selected subsets\nof 10,000 instances from movies not included in\nthe training are used for development and test3.\nThe BPE tokenization is separately learnt with\n32K operations for each language in the dataset.\nThe resulting source / target vocabulary sizes for\nEn-Zh and En-Ru datasets are 10296 / 16018 and\n12273 / 22642, respectively. The token-level batch\nsizes are 8192 and 16384 for training the Zh-En\nand En-Ru datasets on two and four P-100 GPUs.\nThe model hyper-parameters and the optimizer\nof standard transformer baseline follow the base\nsetting in (Vaswani et al., 2017). We set the layers\nin encoder and decoder to 6, and the attention heads\nto 8. The dimensionality of input and output is 512.\nIn addition, we add a feed-forward layer before the\n1https://wit3.fbk.eu\n2https://github.com/lena-voita/good-translation-wrong-in-\ncontext\n3http://data.statmt.org/acl18 contextnmt data/\ndecoder output layer, with dimensionality 1024, to\ncombine the local and global stream. We use the\nAdam optimizer with β1 = 0.9, β2 = 0.98 and ϵ\n= 10−9, with 16000 warm-up steps and scale of 4.\nThe batch size for each GPU is 4000.\nBLEU score is calculated with the script\nmteval-v13a.pl in Moses4. All reported val-\nues are evaluated on the test set with the best check-\npoint on the development set.\n3.1 Evaluation on BLEU\nWe ﬁrst conduct a detailed analysis on the k-to-k\ntranslation model with respect to the IWSLT Zh-En\ndataset. In this scenario, the k source and target\nsentences are concatenated as the input and out-\nput to train the transformer. During inference, for\nevery consecutive k source sentences, the model\nproduces ktarget sentences. To translate a test set\nin a k−to−kmodel, we keep a sliding window of\nsize k. Each sentence is translated ktimes (excpet\nfor the ﬁrst k−1 sentences), each time as a jth\n(j ≤k) sentence. For example, in a 4-to-4 model,\nsentence 5 is translated 4 times – the 1st time as\nthe last sentence in the chunk s2,s3,s4,s5, the 2nd\ntime as the 3rd sentence in the chunk s3,s4,s5,s6,\nand so on. We thus can assemble different versions\nof the ﬁnal translated test set where each sentence\nis translated as the jth sentence (j ≤ k) in the\ntranslation process. Each of these ﬁnal documents\nis evaluated separately. The results are illustrated\nin Figure 2.\nWe can make two inferences from the results.\nFirst, with the Standard transformer, the 1st sen-\ntence BLEU always the highest (Figure 2(a)). This\nis likely the results of error propagation to subse-\nquent sentences from the auto-regressive property\nmentioned above. Second, larger k, i.e. more con-\ntextual information will not necessarily result in\nbetter BLEU score. In this case, k= 2or 3 is bet-\nter than k= 4. We hypothesize that training with\nlonger sentences requiring learning longer range\ndependencies is fundamentally difﬁcult, especially\nfor such a small dataset.\nWhen we compare the results of our model with\nthe standard transformer, we have two other ﬁnd-\nings. First, the BLEU scores of our k-to-kmodel\noutperform those of the standard transformer, and\nfor the j-th sentence BLEU, the score does not\ndecline as much as in the standard transformer.\n4https://github.com/moses-\nsmt/mosesdecoder/blob/master/scripts/generic/mteval-\nv13a.pl\n1084\n17\n17.5\n18\n18.5\n19\n1-to-1 2-to-2 3-to-3 4-to-4\n17.33\n17.22\n17.54\n17.61\n17.85\n17.75\n17.76\n18.6\n17.89\n17.67\n1st sent BLEU 2nd sent BLEU\n3rd sent BLEU 4th sent BLEU\n(a) Standard Transformer\n17\n17.5\n18\n18.5\n19\n2-to-2 3-to-3 4-to-4\n18.21\n18.26\n18.05\n18.57\n18.72\n18.11\n18.48\n18.81\n18.61\n1st sent BLEU 2nd sent BLEU\n3rd sent BLEU 4th sent BLEU (b) Our Approach\n17\n17.5\n18\n18.5\n19\n[1] [2] [3] [4]\n18.65\n18.24\n17.79\n17.32\nBLEU (c) Other works\nFigure 2: Zh-En: The j-th sentence BLEU of k-2-kmodel, where it means the average BLEU on thej-th sentence.\n[1] (Tu et al., 2018) [2] (Miculicich et al., 2018) [3] (V oita et al., 2018) [4] (Jiang et al., 2019)\nModels Model Beam multi-bleu mteval-v13a Deixis Lexical Ellipsis Ellipsis\nSize Cohesion (VP) (Inﬂ.)\ns-hier-to-2.tied (Bawden et al., 2018) NA 4 26.68 NA 60.9 % 48.9% 65.6% 66.4%\nSentence baseline (V oita et al., 2019b) 256M 4 32.40 NA 50.0% 45.9% 28.9% 53.0%\nConcat Baseline (V oita et al., 2019b) 256M 4 31.56 NA 83.5% 47.5% 76.2% 76.6%\nCADec (V oita et al., 2019b) 458M 4 32.38 NA 81.6% 58.1% 80.0% 72.2%\nConcat Baseline (Jean et al., 2019) 256M 8 NA 31.00 83.4% 48.9% 73.8% 76.0%\nPartial Copy (Jean et al., 2019) 256M 8 NA 31.60 86.6% 74.9% 77.9% 75.5%\nOur Approach (4-to-4) 262M 4 31.84 32.60 91.0% 46.9% 78.2% 82.2%\n8 32.02 32.80\nOur Approach (4-to-4) 262M 4 31.31 32.28 90.5% 73.9% 81.0% 80.6%\n+ Partial Copy 8 31.60 32.17\nTable 2: En-Ru: The comparison on the accuracy of four consistency metrics. i) multi-bleu are as reported in the\noriginal paper. We opt for mteval-v13a because it does not depend on tokenization. ii) Beam size won’t affect the\nvalues of consistency metrics. iii) Concat Baseline means standard transformer with 4-to-4.\nWe believe that our long-short term masking self-\nattention can, to some extent, relieve the effect\nof error accumulation. Second, when document\ninformation is used (i.e., k >1), decoding each\nsentence as the last sentence (ie. using all previ-\nous context) achieves higher BLEU scores than\ndecoding each sentence individually in the stan-\ndard transformer. We pay more attention to the\nlast sentence because presumably it has the richest\ncontextual information; this is also the setting for\nthe results in the next section.\nTwo qualitative examples are shown in Table 3\n(more examples can see in the supplementary mate-\nrials). In the ﬁrst case, compared to Sys0 and Sys1,\nSys2 is more consistent in the segments “Before I\ndie” and “I want to” of three sentences. In the sec-\nond case, the translation of “boat” in Sys1 or Sys0\nis either omitted or inconsistent in the second sen-\ntence, while Sys2 performs better in consistency.\n3.2 Evaluation on Consistency\nThe publicly available open-subtitle En-Ru dataset\nhas a special test data to evaluate consistency of\ndocument-level translation systems. The details of\nthe data can be found in V oita et al. (2019b). The\ncontext of the training and test data contains exactly\n3 sentences, so we mainly adopt a 4-to-4 model in\nour experiments and each sentence is translated as\nthe last sentence in a chunk of 4 sentences. In this\nsection, we follow previous works to focus on the\naccuracy of Deixis, Lexical cohesion, Verb phrase\nellipsis and Ellipsis (inﬂection) 5.\nIn Table 2, we summarize the results of BLEU as\nwell as consistency performance. s-hier-to-2.tied\n(Bawden et al., 2018) is an RNN-based NMT, so\nits performance is relatively worse than the other\ntransformer-based models. In contrast, our ap-\nproach can achieve better performance with respect\nto both BLEU and consistency, except for lexical\ncohesion. Especially the accuracy of lexical co-\nhesion of Partial Copy (Jean et al., 2019) exceeds\nours by a large margin. Jean et al. (2019) ﬁlled the\nmissing context with partial copy strategy, since\nthe repetition can naturally enhance the lexical co-\nhesion. Therefore, when we also apply the partial\ncopy trick to our model, the lexical cohesion can\nboost by 27% but the BLEU is sacriﬁced. The\nLexical Cohesion of CADec (V oita et al., 2019b)\n5See a short introduction in the supplementary materials.\n1085\nis a bit higher than our approach without partial\ncopy. Considering that CADec is almost double-\nsized of our standard transformer and complicated\narchitecture with the backbone of the deliberation\nnetworks (Xia et al., 2017), the gain over baseline\nis much higher cost than ours. In summary, our\nmodel can achieve a strong result in both BLEU\nand consistency with few extra model parameters.\n4 Discussions and Conclusions\nIn this work, we present a simple but effective varia-\ntion with the long-short term masking strategy, and\nwe performed comparative studies with the k-to-k\ntranslation model of the standard transformer. Just\nas the big, complex neural network architectures\nwith great many parameters has its power, small\nbut efﬁcient modiﬁcation like ours to the classical\ntransformer has its unique appeals. Other examples\nof simple but impactful ideas are data augmenta-\ntion and the round-trip back-translation (V oita et al.,\n2019a), to name just a few. Big or small, complex\nor simple, each has its distinct advantages. We’re\nencouraged by our ﬁndings that in tandem with the\ngreat machinery that could bring powerful results,\nsimplistic approaches could be just as efﬁcacious.\nAcknowledgments\nThis work is partly supported by National Key\nR&D Program of China (2018YFB1403202).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phenom-\nena in neural machine translation. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1304–1313.\nS´ebastien Jean, Ankur Bapna, and Orhan Firat. 2019.\nFill in the blanks: Imputing missing sentences for\nlarger-context neural machine translation. arXiv\npreprint arXiv:1910.14075.\nShu Jiang, Rui Wang, Zuchao Li, Masao Utiyama, Ke-\nhai Chen, Eiichiro Sumita, Hai Zhao, and Bao-liang\nLu. 2019. Document-level neural machine transla-\ntion with inter-sentence attention. arXiv preprint\narXiv:1910.14528.\nYunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019.\nWhen and why is document-level context useful in\nneural machine translation? In Proceedings of the\nFourth Workshop on Discourse in Machine Transla-\ntion (DiscoMT 2019), pages 24–34.\nSneha Kudugunta, Ankur Bapna, Isaac Caswell, and\nOrhan Firat. 2019. Investigating multilingual nmt\nrepresentations at scale. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1565–1575.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1412–1421.\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1275–\n1284.\nSameen Maruf, Andr ´e FT Martins, and Gholamreza\nHaffari. 2019. Selective attention for context-aware\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3092–3102.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2947–2954.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang.\n2018. Learning to remember translation history with\na continuous cache. Transactions of the Association\nfor Computational Linguistics, 6:407–420.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\n1086\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 876–885.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019b.\nWhen a good translation is wrong in context:\nContext-aware machine translation improves on\ndeixis, ellipsis, and lexical cohesion. arXiv preprint\narXiv:1905.05979.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In Advances in Neural Information Process-\ning Systems, pages 1784–1794.\nZhengxin Yang, Jinchao Zhang, Fandong Meng,\nShuhao Gu, Yang Feng, and Jie Zhou. 2019a. En-\nhancing context modeling with a query-guided cap-\nsule network for document-level translation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1527–1537.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 533–542.\nA Appendix\nA.1 Evaluation Metrics of Consistency\nBLEU is a commonly used metric to evaluate the\nprecision-based quality of the translation in terms\nof n-gram, but it is not ﬁt to evaluate discourse\nphenomena, because n-gram precision does not\nspeciﬁcally reﬂect the cohesion and consistency in\nthe long-range dependencies. Deixis addresses the\nerror related to personal pronouns, speciﬁcally gen-\nder marks and informal/formal distinction. Lexical\ncohesion is refers to the consistency of a word or\nphrase when it occurs multiple times. Ellipsis is\nthe omission of words that are understood from\nthe context and it sometimes involves replacement\nof generic term for a speciﬁc term (such as ’did’\nfor ’saw’ in English). Since the target language is\nRussian, we care about both the verb and inﬂection.\nA.2 Code in TensorFlow\nWe present the code snippet for generating local\nmasking matrix for transformer encoder. The ma-\ntrix for transformer decoder is simply add the above\nencoder matrix to the regular decoder self-attention\nmasking matrix.\n1 def generate_masking(inputs, sentence_sep_id):\n2 \"\"\"GE N E R A T ELO N GSH O R TTE R MMA S K I N G\n3 AR G S:\n4 I N P U T S: A D E N S E V E C T O R[B A T C H, L E N G T H] O F\nS O U R C E O R T A R G E T W O R D I D S\n5 S E N T E N C E_S E P_I D: T H E I D O F T H E S E N T E N C E\nS E P A R A T I O N T O K E N\n6 \"\"\"\n7 shape = tf.shape(inputs)\n8 length = shape[1]\n9 sentence_sep_id_matrix = sentence_sep_id *\ntf.ones(shape, dtype=inputs.dtype)\n10 sentence_end = tf.cast(tf.equal(inputs,\nsentence_sep_id), tf.float32)\n11 sentence_end_mask = tf.cumsum(sentence_end,\naxis = -1)\n12 sentence_end_mask_expand_row = tf.\nexpand_dims(sentence_end_mask, -1)\n13 sentence_end_mask_expand_row = tf.tile(\nsentence_end_mask_expand_row, [1, 1,\nlength])\n14 sentence_end_mask_expand_column = tf.\nexpand_dims(sentence_end_mask, -2)\n15 sentence_end_mask_expand_column = tf.tile(\nsentence_end_mask_expand_column, [1,\nlength, 1])\n16 mask = tf.cast(tf.equal(\nsentence_end_mask_expand_row,\nsentence_end_mask_expand_column), tf.\nfloat32)\n17 mask = -1e9 * (1.0 - mask)\n18 mask = tf.reshape(mask, [-1, 1, length,\nlength])\n19\n20 return mask\nA.3 More Examples\nWe randomly selected three translation examples\nand illustrated in Table 3. For Example 1, the pro-\nposed system learnt “And” at the beginning of the\ntranslation, which is a side effect of document-level\ntraining. For Example 2, whether using “love” or\n“love to” is consistency in the proposed system and\n1-to-1 baseline transformer. It seems that 1-to-1\nbaseline can approximately translate “极” to “radi-\ncal”, which does not even appear in the reference.\nI personally think “extremely” is a better trans-\nlation. For Example 3, the reference seems not\nconsistency in “how are we” and “how do we”, but\nour proposed system prefers to keep in consistency\nusing “how do we”.\n1087\nSrc\n养殖金枪鱼的饲料转换率是15比1。这个意思是说，每生产\n1磅金枪鱼肉耗费15磅用其他野生鱼类做的饲料。这可不是\n很具有可持续发展性。\nRef It’s got a feed conversion ratio of 15 to one. That means it takes\nﬁfteen pounds of wild ﬁsh to get you one pound of farm tuna.\nNot very sustainable.\nSys0\nFeeding tuna is 15 to one. That means that every pound of tunas\ncosts 15 pounds to feed feed on other wild ﬁsh. It’s not\nsustainable.\nSys1\nIt’s 15 to 1. What that means is that every pound-pound tuna\nproduces 15 pounds of feed on every other wild ﬁsh. It’s not\nsustainable.\nSys2\nAnd the shift rate of breeding tuna is 15 to one. That means, for\nevery one pound of tuna, it takes 15 pounds of feeding on other\nwild ﬁsh. It’s not very sustainable.\nSrc 我们爱极了革新我们爱技术，我们爱创造我们爱娱乐\nRef We love innovation. We love technology. We love creativity.\nWe love entertainment.\nSys0 We love radical innovation. We love technology. We love\ncreation. We love entertainment.\nSys1 We love to be innovative. We love technology. We love to\ncreate. We love entertainment.\nSys2 We love innovation. We love technology. We love creating.\nWe love entertainment.\nSrc\n想要喂饱这个世界？让我们开始问：我们怎么去喂养我们\n自己？或者更好的，我们怎么去建立一种环境它可以让每\n一个团体去养活自己？\nRef\nWant to feed the world? Let’s start by asking: how are we\ngoing to feed ourselves? Or better: how can we create\nconditions that enable every community to feed itself?\nSys0\nDo you want to feed the world? So let’s start asking: how\ndo we feed ourselves? Or better, how can we build an\nenvironment that allows every group to feed themselves?\nSys1\nHow do we feed the world? So let’s start asking: how do\nwe feed ourselves? Or even better, how do we build an\nenvironment that will feed itself?\nSys2\nWant to feed the world? Let’s start asking: how\ndo we feed ourselves? Or better, how do we build an\nenvironment that allows every single group to feed itself?\nTable 3: Examples of translation results. Sys0: 1-to-1\ntransformer. Sys1: 3-to-3 transformer. Sys2: 3-to-3\nlong-short term masking transformer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8166735172271729
    },
    {
      "name": "Transformer",
      "score": 0.7243379950523376
    },
    {
      "name": "Machine translation",
      "score": 0.6653831601142883
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6454051733016968
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4869452714920044
    },
    {
      "name": "Machine learning",
      "score": 0.43645384907722473
    },
    {
      "name": "Speech recognition",
      "score": 0.3671758770942688
    },
    {
      "name": "Natural language processing",
      "score": 0.32972824573516846
    },
    {
      "name": "Voltage",
      "score": 0.10269054770469666
    },
    {
      "name": "Engineering",
      "score": 0.07742384076118469
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ]
}