{
  "title": "Greening Large Language Models of Code",
  "url": "https://openalex.org/W4386614449",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222417459",
      "name": "Shi, Jieke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104523023",
      "name": "Yang Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222667302",
      "name": "Kang, Hong Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969616922",
      "name": "Xu, Bowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222417460",
      "name": "He, Junda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744541287",
      "name": "Lo, David",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4376122476",
    "https://openalex.org/W4313547593",
    "https://openalex.org/W4225878270",
    "https://openalex.org/W4221016682",
    "https://openalex.org/W4285490465",
    "https://openalex.org/W3100985894",
    "https://openalex.org/W2065053490",
    "https://openalex.org/W3021206621",
    "https://openalex.org/W3166095789",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W1571917368",
    "https://openalex.org/W4386080900",
    "https://openalex.org/W2138428785",
    "https://openalex.org/W4386081573",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W2954141573",
    "https://openalex.org/W3034560159",
    "https://openalex.org/W3121707215",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W2772626348",
    "https://openalex.org/W3156869386",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4382490810",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W3217001695",
    "https://openalex.org/W2911546748",
    "https://openalex.org/W2954451301",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3135013702",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2972135640",
    "https://openalex.org/W3015810308",
    "https://openalex.org/W2769879317",
    "https://openalex.org/W4221153682",
    "https://openalex.org/W1921848483",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2316373884",
    "https://openalex.org/W4386794865",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2888483609",
    "https://openalex.org/W4392609567",
    "https://openalex.org/W3123509436",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W4283751459",
    "https://openalex.org/W4384345728",
    "https://openalex.org/W1438627575",
    "https://openalex.org/W4389208909",
    "https://openalex.org/W4220722393",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2803426915",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4308351360",
    "https://openalex.org/W2376557535",
    "https://openalex.org/W3133951772",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W4380568733"
  ],
  "abstract": "Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers' devices becomes essential. To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness. The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160$\\times$ smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184$\\times$ less), carbon footprint (up to 157$\\times$ less), and inference latency (up to 76$\\times$ faster), with only a negligible loss in effectiveness (1.67\\% on average).",
  "full_text": "Greening Large Language Models of Code\nJieke Shiâ™¦, Zhou Yangâ™¦, Hong Jin Kangâ™ , Bowen Xuâ™£, Junda Heâ™¦, and David Loâ™¦\nâ™¦School of Computing and Information Systems, Singapore Management University, Singapore\nâ™ Department of Computer Science, University of California, Los Angeles, USA\nâ™£Department of Computer Science, North Carolina State University, Raleigh, USA\n{jiekeshi, zyang, jundahe, davidlo}@smu.edu.sg, hjkang@cs.ucla.edu, bxu22@ncsu.edu\nABSTRACT\nLarge language models of code have shown remarkable effective-\nness across various software engineering tasks. Despite the avail-\nability of many cloud services built upon these powerful models,\nthere remain several scenarios where developers cannot take full\nadvantage of them, stemming from factors such as restricted or un-\nreliable internet access, institutional privacy policies that prohibit\nexternal transmission of code to third-party vendors, and more.\nTherefore, developing a compact, efficient, and yet energy-saving\nmodel for deployment on developersâ€™ devices becomes essential.\nTo this aim, we propose Avatar, a novel approach that crafts a\ndeployable model from a large language model of code by optimiz-\ning it in terms of model size, inference latency, energy consumption,\nand carbon footprint while maintaining a comparable level of ef-\nfectiveness (e.g., prediction accuracy on downstream tasks). The\nkey idea of Avataris to formulate the optimization of language\nmodels as a multi-objective configuration tuning problem and solve\nit with the help of a Satisfiability Modulo Theories (SMT) solver\nand a tailored optimization algorithm. The SMT solver is used to\nform an appropriate configuration space, while the optimization\nalgorithm identifies the Pareto-optimal set of configurations for\ntraining the optimized models using knowledge distillation. We\nevaluate Avatarwith two popular language models of code, i.e.,\nCodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulner-\nability prediction and clone detection. We use Avatarto produce\noptimized models with a small size (3 MB), which is 160Ã—smaller\nthan the original large models. On the two tasks, the optimized\nmodels significantly reduce the energy consumption (up to 184Ã—\nless), carbon footprint (up to 157Ã—less), and inference latency (up\nto 76Ã—faster), with only a negligible loss in effectiveness (1.67%).\nKEYWORDS\nLanguage Models of Code, Configuration Tuning, Multi-Objective\nOptimization\nACM Reference Format:\nJieke Shiâ™¦, Zhou Yangâ™¦, Hong Jin Kangâ™ , Bowen Xuâ™£, Junda Heâ™¦, and David\nLoâ™¦. 2024. Greening Large Language Models of Code. InSoftware Engineering\nin Society (ICSE-SEISâ€™24), April 14â€“20, 2024, Lisbon, Portugal. ACM, New York,\nNY, USA, 12 pages. https://doi.org/10.1145/3639475.3640097\nâ€ Zhou Yang is the corresponding author.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal\nÂ© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0499-4/24/04.\nhttps://doi.org/10.1145/3639475.3640097\nLAY ABSTRACT\nLarge language models of code have proven to be highly effective\nfor various software engineering tasks, such as spotting program\ndefects and helping developers write code. While many cloud ser-\nvices built on these models (e.g., GitHub Copilot) are now acces-\nsible, several factors, such as unreliable internet access (e.g., over\n20% of GitHub Copilotâ€™s issues are related to network connectiv-\nity [22]) and privacy concerns (e.g., Apple has banned the internal\nuse of external AI tools to protect confidential data [ 53]), hinder\ndevelopers from fully utilizing these services. Therefore, deploy-\ning language models of code on developersâ€™ devices like laptops\nappears promising. However, local deployment faces challenges:\n(1) Consumer-grade personal devices typically lack sufficient mem-\nory and the high-performance CPUs/GPUs required for efficient\nmodel execution; (2) Even if the hardware requirements are met,\ndeploying the models on many devices can result in considerable\nenergy consumption and carbon emissions, negatively impacting\nenvironmental sustainability.\nTo address these challenges, we present Avatar, an innovative\napproach that optimizes large language models of code and enables\ntheir deployment on consumer-grade devices. Avatarcan opti-\nmize two popular models from a large size of 481 MB to a compact\nsize of 3 MB, resulting in significant reductions in inference time,\nenergy consumption, and carbon emissions by hundreds of times.\nOur technique effectively lowers the entry barrier for leveraging\nlarge language models of code, making them available to ordi-\nnary developers without the need for high-performance computing\nequipment. Furthermore, it also contributes to a more sustainable\nand user-friendly software development environment.\n1 INTRODUCTION\nRecent years have seen a remarkable surge in Artificial Intelligence\n(AI)-powered services for software engineering, such as GitHub\nCopilot [23] and GitLab Auto DevOps [12]. This surge has brought\na new level of automation to the software development process,\nsignificantly improving developerâ€™s productivity and the quality\nof software products. According to an economic analysis report\nreleased by GitHub, AI-powered services for software development\ncould boost the global GDP by over $1.5 trillion by 2030 [13].\nThe foundation of these AI-powered services lies in large lan-\nguage models of code [35, 49, 55, 56, 82]. These models have shown\nsuperior performance in various software engineering tasks such\nas vulnerability detection [7, 33] and code completion [9, 47]. How-\never, the services that utilize language models of code are typi-\ncally hosted in the cloud, giving rise to several issues such as data\nleakage concerns [36, 48, 57, 80] and poor user experience due to\nnetwork fluctuations [22]. Therefore, there is a growing need for\narXiv:2309.04076v3  [cs.SE]  12 Jan 2024\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo\ndeploying these models within the integrated development envi-\nronments (IDEs) on developersâ€™ local machines. However, recent\nstudies [65, 75] have highlighted several challenges associated with\ndeploying language models of code, including their large size, long\ninference latency, high energy consumption, and considerable car-\nbon footprint.\nTypically, language models of code are large-sized with numer-\nous parameters. For example, CodeBERT [ 18] and GraphCode-\nBERT [26], two popular language models of code, both have 125\nmillion parameters, resulting in a file size of about 500 megabytes\n(MB). The recently released Code Llama model is even larger at over\n130 gigabytes (GB) [58]. However, real-world deployment experi-\nences, as observed by the Visual Studio team in deploying IDEs, have\nemphasized a preference for compact models, which are typically\naround 3 MB in size and can seamlessly function as IDE compo-\nnents or editor plug-ins even on low-end hardware devices [ 70].\nMeanwhile, language models perform billions of floating-point op-\nerations (FLOPs) during inference. These massive computations\ncause long inference latency, often taking over 1.5 seconds to re-\nturn a prediction [65]. Such delays can disrupt developersâ€™ work-\nflow, ultimately resulting in a suboptimal user experience. Previous\nstudies [4, 70] suggest that for a model deployed in IDEs to offer\ndevelopers instantaneous assistance, its inference latency should\nideally be within a few tens of milliseconds at most. The inability\nof language models of code to meet the above requirements gives\nrise to usability issues, consequently impeding their widespread\ndeployment within developersâ€™ IDEs.\nFurthermore, and perhaps even more importantly, the billions\nof FLOPs during inference entail significant energy consumption\nand carbon footprint, raising concerns about environmental and\nclimate sustainability. Considering a CodeBERT deployed in IDEs,\na developer typically needs to run it thousands of times per day,\nwhich is a common usage amount [31]. Such intensive usage results\nin an energy consumption of 0.32 kilowatt-hours (kWh), while a\ntypical consumer-grade laptop has a battery capacity of around 70\nwatt-hours [40], i.e., 0.07 kWh. Consequently, a laptopâ€™s battery\ncan only support a developer running CodeBERT for 0.22 hours,\nwhich is far from sufficient for a typical workday. This would frus-\ntrate developers and also hinder their ability to work flexibly in\nmobile environments. Moreover, the above energy cost of 0.32 kWh\ncan translate into a considerable carbon footprint, amounting to\napproximately 0.14 kilograms of CO2 emissions. This carbon foot-\nprint is comparable to the emissions generated by driving a car\nfor 0.6 miles.1 With the expected widespread adoption of language\nmodels of code by many software developers in the near future, the\ncumulative carbon footprint stemming from model inference will\nbecome an increasingly pressing issue.\nTo date, few approaches have emerged to address the above\nissues [65, 75]. Shi et al. [ 65] propose Compressor, the state-of-\nthe-art approach that can compress language models of code down\nto 3 MB and thereby improve their inference latency. Compres-\nsor adopts the knowledge distillation technique [ 34] to transfer\nknowledge from a large model to a tiny one with a well-crafted\n1All of these calculations on energy consumption and carbon footprint are based\non the Machine Learning Emissions Calculator: https://mlco2.github.io/impact.\narchitecture searched by their proposed genetic algorithm. How-\never, while Compressor excels at optimizing the model size and\ninference latency, it does not encompass the optimization of two\nother critical aspects, i.e., energy consumption and carbon footprint.\nAdditionally, Compressorâ€™s search space for small model architec-\ntures is limited solely to hyperparameters related to model size, like\nthe number of network layers. This limited scope excludes configu-\nrations that can significantly affect a modelâ€™s effectiveness, like the\nchoice of tokenizer [39]. Consequently, it falls short of identifying\nthe optimal small model. These limitations necessitate our work.\nOur work still follows the idea of using knowledge distillation to\noptimize language models for the sake of size and inference latency,\nbut offers a novel take on simultaneously addressing the issues of\nenergy consumption and carbon footprint.\nThis paper proposes Avatar, a novel approach aimed at optimiz-\ning language models of code for real-world deployment. Avatar\naccomplishes this by formulating the seeking of an optimal model\nas a multi-objective configuration tuning problem, where the op-\ntimization objectives include the simultaneous minimization of\nmodel size, inference latency, energy consumption, and carbon\nfootprint, while maintaining effectiveness (e.g., prediction accu-\nracy) on downstream tasks.\nAvatarstarts by identifying the key configurations within lan-\nguage models that impact the above objectives. It then innovatively\ncombines a Satisfiability Modulo Theories (SMT) solver with a tai-\nlored multi-objective optimization algorithm to solve the configura-\ntion tuning problem. The SMT solver is used to construct a config-\nuration space that adheres to the 3 MB model size constraint, while\nthe multi-objective optimization algorithm identifies the Pareto-\noptimal set of configurations, i.e., the set of configurations that\ncannot be improved in one objective without making sacrifices in\nanother, thereby achieving the best trade-off among all objectives.\nTo efficiently obtain the effectiveness of models during optimization\nwithout the need for expensive training and evaluation processes,\nAvatarbuilds a regression model serving as an effectiveness indi-\ncator. This indicator estimates a modelâ€™s effectiveness solely based\non its configurations, facilitating the quick identification of the\nPareto-optimal configurations. Finally, Avatarleverages knowl-\nedge distillation to train a compact and environmentally-friendly\nmodel using the configurations from the Pareto-optimal set.\nWe evaluate Avatarusing the same settings as the baseline\nmethod [65]. Our evaluation focuses on optimizing two represen-\ntative language models of code: CodeBERT [18] and GraphCode-\nBERT [26]. We utilize two datasets for popular automated software\nengineering tasks: vulnerability prediction and clone detection.\nWith Avatar, we produce optimized models with a compact size of\n3 MB, a reduction of 160Ã—compared to the original large language\nmodels. Across both tasks, these optimized models show a remark-\nable improvement in various aspects. They reduce inference latency\nby up to 76Ã—compared to the original models, optimize energy con-\nsumption by up to 184Ã—less, and reduce carbon footprint by up to\n157Ã—less. Importantly, these optimizations incur only a negligible\nloss in effectiveness, averaging 1.67%. Notably,Avataroutperforms\nthe baseline method, Compressor, across all metrics. On average,\nAvatarachieves a 0.75% higher prediction accuracy. Additionally,\nit exhibits significant improvements in terms of inference latency\n(44Ã—faster on average), energy consumption (up to 8Ã—less), and\nGreening Large Language Models of Code ICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal\n1 {\n2 \"tokenizer\": \"Byte-Pair Encoding\",\n3 \"vocab_size\": 50265,\n4 \"num_hidden_layers\": 12,\n5 \"hidden_size\": 768,\n6 \"hidden_act\": \"GELU\",\n7 \"hidden_dropout_prob\": 0.1,\n8 \"intermediate_size\": 3072,\n9 \"num_attention_heads\": 12,\n10 \"attention_probs_dropout_prob\": 0.1,\n11 \"max_sequence_length\": 512,\n12 \"position_embedding_type\": \"absolute\",\n13 \"learning_rate\": 1e-4,\n14 \"batch_size\": 32\n15 }\nListing 1: Typical tunable configurations of language models\nof code.\ncarbon footprint (up to 7Ã—less). Moreover, we also highlight the\nbenefits of Avatarin the context of cloud deployment, showing\nthat the optimized models can process up to 9.7Ã—more queries per\nsecond than the original large language models of code.\nThe contributions of this paper are summarized as follows:\nâ€¢Insight: We are the first to propose optimizing language models\nof code in terms of the energy consumption and carbon footprint\nby tuning their configurations.\nâ€¢Technique: We propose and implement Avatar, a novel ap-\nproach that uses an SMT solver and a tailored multi-objective\noptimization algorithm to optimize language models of code in\nterms of model size, inference latency, energy consumption, and\ncarbon footprint, while maintaining effectiveness.\nâ€¢Evaluation: We perform a thorough evaluation of Avatar, and\nthe results show thatAvatareffectively optimizes language mod-\nels of code, greatly outperforming the state-of-the-art approach.\n2 PRELIMINARIES\nLanguage Models of Code and Their Configurations. The re-\ncent development and adoption of language models of code have\nenabled state-of-the-art results to be achieved on code-related\ntasks [35, 49, 55, 56]. These powerful models are mainly built upon\nthe Transformer architecture [74] and trained on large datasets of\nsource code from various programming languages. Among these\nmodels, a notable category is encoder-only models such as Code-\nBERT [18] and GraphCodeBERT [26], which utilize solely the en-\ncoder component of Transformer and are specialized for program\nunderstanding tasks such as vulnerability detection [7] and code\nsearch [85]. These encoder-only models represent the software engi-\nneering communityâ€™s early efforts at language models of code [35].\nDue to their pioneering status, these models have long been used\nin various real-world applications like the Akvelon code search\nengine [2]. This has led to widespread popularity and social impact\nand thus motivated our study to focus on these models.\nTypically, encoder-only language models of code have a number\nof configurations that can be tuned to achieve varying levels of\nmodel performance. Listing 1 shows an example of tunable config-\nurations from the Hugging Faceâ€™s implementation [15], with a total\n1 input ğ‘€: language model of code (teacher model)\n2 input ğ‘: small model (student model)\n3 input ğ·: training dataset\n4 input ğ‘‡: temperature parameter\n5 for ğ‘‘in ğ·:\n6 ğ‘, ğ‘= ğ‘€(ğ‘‘), ğ‘(ğ‘‘)\n7 ğ‘™ğ‘œğ‘ ğ‘  = softmax(ğ‘\nğ‘‡ )âˆ— log \u0000softmax(ğ‘\nğ‘‡ )\u0001 âˆ—ğ‘‡2\n8 ğ‘.update(ğ‘™ğ‘œğ‘ ğ‘ )\n9 return ğ‘\nListing 2: Algorithm of knowledge distillation.\nnumber of 13. Six of these configurations directly impact model\nsize and inference latency, including the number of hidden layers,\nhidden size (i.e., the dimension of hidden layers), number of atten-\ntion heads, vocabulary size, intermediate size (i.e., the dimension of\nfeed-forward layers), and maximum sequence length. Larger values\nin these configurations tend to result in larger model sizes and\nlonger inference latency, while smaller values may compromise\nmodel effectiveness (e.g., prediction accuracy). Compressor [65]\nfocuses solely on tuning these configurations to optimize model\nsize and inference latency at the cost of effectiveness.\nHowever, there exist seven additional configurations that also\ncontribute to model effectiveness. These include the choice of tok-\nenizer, activation function for hidden layers, type of position embed-\ndings, dropout rates for hidden layers and attention heads, learning\nrate, and batch size. For example, the choice of a tokenizer can affect\na modelâ€™s ability to capture the semantics of source code [39, 42, 64],\nthus impacting its overall effectiveness. In this study, we aim to\ntune all 13 configurations to achieve the best trade-off between\nmodel effectiveness and efficiency. We discuss the tuning space of\nthese configurations and how to tune them in Section 3.\nKnowledge Distillation. Knowledge distillation has proven to\nbe an effective technique for optimizing large language models in\nterms of model size [41, 59, 65]. It compresses a large model (referred\nto as the teacher model) by training a small model (the student\nmodel) to mimic the behaviors of the large one (i.e., produces the\nsame output given the same input) [5, 24, 34].\nIn line with recent work [65], our study leverages a task-specific\ndistillation method introduced by Hinton et al. [ 34] to optimize\nlanguage models of code. The algorithm of this method is shown\nin Listing 2. Specifically, given a language model of code that is\nfine-tuned for a specific task and a small model to be trained, we\ninput training data into both models, collect the resulting output\nprobability values (line 15), and then update the parameters of the\nsmall model (line 8) to minimize the training loss computed by the\nfunction shown in line 7. The intuition behind minimizing this loss\nfunction is to bring the outputs of the language and small models\ncloser together. ğ‘ğ‘– and ğ‘ğ‘– in this function denote the outputs of the\nlarge and small models, respectively. ğ‘‡ is the softmax functionâ€™s\ntemperature parameter, as Hinton et al. [34] introduced. Note that\nthe language model producing ğ‘ğ‘– is fixed during the distillation\nprocess, while the small model producing ğ‘ğ‘– is trained.\nNote that the above loss function does not necessitate ground-\ntruth labels, only requiring the modelâ€™s outputs. Thus, we follow\nCompressor [65] to use unlabeled data for training. This choice is\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo\n1 {\n2 \"tokenizer\": [\"Byte-Pair Encoding\", \"WordPiece\",\nâ†©â†’\"Unigram\", \"Word\"],\n3 \"vocab_size\": range(1000, 50265),\n4 \"num_hidden_layers\": range(1, 12),\n5 \"hidden_size\": range(16, 768),\n6 \"hidden_act\": [\"GELU\", \"ReLU\", \"SiLU\", \"GELU_new\"],\n7 \"hidden_dropout_prob\": [0.1, 0.2, 0.3, 0.4, 0.5],\n8 \"intermediate_size\": range(16, 3072),\n9 \"num_attention_heads\": range(1, 12),\n10 \"attention_probs_dropout_prob\": [0.1, 0.2, 0.3, 0.4,\nâ†©â†’0.5],\n11 \"max_sequence_length\": range(256, 512),\n12 \"position_embedding_type\":[\"absolute\", \"relative_key\",\nâ†©â†’\"relative_key_query\"],\n13 \"learning_rate\": [1e-3, 1e-4, 5e-5],\n14 \"batch_size\": [16, 32, 64]\n15 }\nListing 3: The configuration space of small models. It\ncontains around 4.5 Ã—1019 plausible sets of configurations.\ndriven by the practical consideration that obtaining labeled data is\ntypically costly and challenging, while ample unlabeled data can be\nreadily collected from open-source software platforms like GitHub.\n3 METHODOLOGY\n3.1 Problem Formulation\nAs introduced in Section 1, we aim to optimize the model size,\ninference latency, energy consumption, and carbon footprint of\nlanguage models of code while maintaining their effectiveness (e.g.,\nprediction accuracy on downstream tasks). Among these objectives,\nthe inference latency, energy consumption, and carbon footprint\nare all related to the modelâ€™s computational cost during inference.\nWe use floating-point operations (FLOPs) to measure computational\ncost, following prior studies [29, 61, 65]. FLOPs count how many\nmultiply and accumulate operations the model performs for each\nprediction. The more FLOPs a model has, the more time it will take\nto make a prediction, the more energy it will consume, and the\nmore CO2 it will emit [61]. Therefore, we use FLOPs as the proxy\nfor these three objectives. Then, combined with the model size and\neffectiveness, we formulate our optimization problem as follows:\nminğ‘ {size(ğ‘),FLOPs(ğ‘),âˆ’effectiveness(ğ‘)}\ns.t. ğ‘ âˆˆC (1)\nwhere ğ‘ is a set of configurations, and Cdefines the configuration\nspace, as illustrated in Listing 3. Most of these configurations offer\na range of adjustable integer or decimal values. For instance, the vo-\ncabulary size is adjustable to any integer value ranging from 1,000\nto 50,265. Some others involve selecting from predefined options.\nThe tokenizer requires a choice among four popular tokenization\nmethods: Byte-Pair Encoding [62], WordPiece [76], Unigram [45],\nand Word [42]. Additionally, we set the hidden activation function\nand position embedding type as tunable configurations following\nthe Hugging Faceâ€™s implementation [ 15], which includes a few\nmore advanced options than the original implementation of lan-\nguage models. The hidden activation function requires a choice\nSMU Classification: Restricted\nConfigurationSpace(Listing 3)\nPruned Space(Listing 4)\nPruning(Section 3.3)\nEffectiveness Indicator(Section 3.4)\nMulti-ObjectiveOptimization(Section 3.5)Sampling & Training(Listing 5)\nLarge Language Model of Code\nCompact and Green Model of Code\nPareto-optimal Configurations\nKnowledge Distillation (Listing 2)\nConfiguration Tuning(Listing 6)\nFigure 1: The workflow of Avatar.\nfrom four options: Gaussian Error Linear Unit (GELU) [32], Recti-\nfied Linear Unit (ReLU) [30], Sigmoid Linear Unit (SiLU) [14], and\na new GELU implementation (GELU_new) [15]. The position em-\nbedding type offers three choices: absolute, relative_key [63], and\nrelative_key_query [37]. In total, the configuration space contains\nabout 4.5 Ã—1019 possible sets of configurations, which is much\nlarger than the one used by Compressor that only tunes 5 configu-\nrations. Our configuration space is also extensible to include more\nconfigurations or more options for existing configurations, such as\nmore tokenizer choices. Here we focus on the configuration space\nshown in Listing 3 as studies [ 39, 65] and Hugging Faceâ€™s imple-\nmentation [15] have explicitly shown that these configurations and\noptions have a significant impact on model effectiveness.\nSolving the problem posed by Equation 1 is challenging for three\nreasons: (1) the tuning space of configurations is quite huge, which\nmakes brute force impractical since evaluating all configurations is\ncomputationally infeasible; (2) utilizing off-the-shelf Satisfiability\nModulo Theories (SMT) solvers that support solving constrained\noptimization problems is not a viable approach for solving this\nproblem. This is because obtaining model effectiveness necessitates\ntraining and testing the model. Such a process cannot be formulated\nas a mathematical function of configurations that SMT solvers\ncan handle; (3) this multi-objective optimization problem comes\nwith objectives that conflict with others. For example, a larger\nmodel typically has better effectiveness on downstream tasks but\nincurs higher FLOPs. Thus, solving Equation 1 involves finding a\nPareto-optimal solution set, i.e., a set of trade-off solutions where\nno solution can be improved in one objective without degrading\nother objectives [10], rather than finding a single, unique solution.\n3.2 Approach Overview\nPursuant to the above challenges, our approach, Avatar, is de-\nsigned to solve the problem through a multi-step process outlined\nin Figure 1. First, we prune the configuration space using an SMT\nsolver, with the 3 MB model size constraint suggested by prior\nstudies [65, 70] as the pruning criterion (Section 3.3). This initial\nstep removes configurations that are irrelevant to our objectives,\nthereby facilitating the subsequent identification of Pareto-optimal\nconfigurations. Next, we sample a small number of configurations\nfrom the pruned space and use them to train a regression model that\ncan predict the effectiveness of a model initialized by a given set\nof configurations, i.e., build an effectiveness indicator (Section 3.4).\nSubsequently, we use a multi-objective optimization algorithm,\nGreening Large Language Models of Code ICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal\n1 {\n2 \"tokenizer\": [\"Byte-Pair Encoding\", \"WordPiece\",\nâ†©â†’\"Unigram\", \"Word\"],\n3 \"vocab_size\": range(1000, 46000),\n4 \"num_hidden_layers\": range(1, 12),\n5 \"hidden_size\": range(16, 256),\n6 \"hidden_act\": [\"GELU\", \"ReLU\", \"SiLU\", \"GELU_new\"],\n7 \"hidden_dropout_prob\": [0.1, 0.2, 0.3, 0.4, 0.5],\n8 \"intermediate_size\": range(32, 3072),\n9 \"num_attention_heads\": range(1, 12),\n10 \"attention_probs_dropout_prob\": [0.1, 0.2, 0.3, 0.4,\nâ†©â†’0.5],\n11 \"max_sequence_length\": range(256, 512),\n12 \"position_embedding_type\":[\"absolute\", \"relative_key\",\nâ†©â†’\"relative_key_query\"],\n13 \"learning_rate\": [1e-3, 1e-4, 5e-5],\n14 \"batch_size\": [16, 32, 64]\n15 }\nListing 4: The pruned configuration space. It contains around\n1.3 Ã—1019 sets of configurations, 28.9% of the original one. The\nunderlined entries are pruned (Section 3.3).\nassisted by the effectiveness indicator, to identify the set of Pareto-\noptimal configurations within the pruned space (Section 3.5). Fi-\nnally, we train a compact and environmentally-friendly model with\nthe configurations from the Pareto-optimal set using the knowl-\nedge distillation technique that we have introduced in Section 2.\nWe describe these steps in detail below.\n3.3 Pruning Configuration Space\nThe predefined configuration space shown in Listing 3 is incredibly\nlarge, with quintillions of possible configuration sets. However, only\na fraction of them adhere to the constraints outlined in Section 1. For\nexample, setting the vocabulary size to its maximum value of 50,265\nwill result in a model size that exceeds the 3 MB constraint, even\nwith all other configurations minimized. Such configurations are\nthus considered irrelevant to our objectives and should be omitted\nfrom the configuration space to facilitate the subsequent process of\nidentifying Pareto-optimal configurations.\nWe prune the configuration space by formulating and solving\na constraint satisfaction problem using Microsoft Z3 [11], a state-\nof-the-art SMT solver known for efficiently handling nonlinear\nconstrained optimization problems [ 6, 21]. While Z3 cannot di-\nrectly solve our primary optimization problem, it performs well\nat identifying and excluding configurations that violate specified\nconstraints. One crucial constraint is related to model size, as in-\ntroduced in Section 1, which specifies that the model size cannot\nexceed 3 MB. This constraint is only explicit one suggested by prior\nstudies [65, 70] while acceptable standards for other objectives have\nnot been empirically specified. We formulate the constraint satis-\nfaction problem as follows, where Crepresents the configuration\nspace, and ğ‘ denotes a set of configurations:\nsize(ğ‘)â‰¤ 3 MB s.t. ğ‘ âˆˆC (2)\nSolving this constraint satisfaction problem yields multiple sets of\nconfigurations that satisfy the model size constraint, which can\nthen be merged to craft a new configuration space.\n1 input C: pruned configuration space\n2 input ğ‘€: language model of code (teacher model)\n3 input ğ·: training dataset\n4 input ğ‘‰: validation dataset\n5 input ğ‘‡: temperature parameter\n6 input ğ‘˜: number of sampled configurations\n7 ğ‘ = sample(C,ğ‘˜), ğ‘’ = { }\n8 for ğ‘–in ğ‘˜:\n9 ğ‘ğ‘– = initialize(ğ‘ğ‘– )\n10 ğ‘ğ‘– = knowledge-distillation(ğ‘€,ğ‘ğ‘–,ğ·,ğ‘‡ )\n11 ğ‘’ğ‘– = test(ğ‘ğ‘–,ğ‘‰)\n12 return Bayesian-Ridge-Regression({ğ‘,ğ‘’})\nListing 5: Algorithm for building an effectiveness indicator.\nAs pointed out in Section 2, a language model typically offers a\nhandful of tunable configurations that directly determine the model\nsize. Letğ‘£denote the vocabulary size,ğ‘™denote the number of hidden\nlayers, â„denote the hidden size, ğ‘– denote the intermediate size, ğ‘\ndenote the number of attention heads, and ğ‘  denote the maximum\nsequence length. Then the model size can be calculated as follows:\nsize(ğ‘)= 4(ğ‘£+ğ‘ +3)â„\n1024 Ã—1024 # embedding layer\n+4(4â„2 +(9 +2ğ‘–)â„+ğ‘–)ğ‘™\n1024 Ã—1024 # transformer layers\n+2â„2 +4â„+2\n1024 Ã—1024 # classifier layer\n(3)\nThe above formula follows the official implementation of Com-\npressor [65] to calculate the actual file size of a model in MB. It\nbreaks down a language model of code into three components: the\nembedding, transformer, and classifier layers. By summing these\ncomponents, the formula calculates the total model size. Note that\nthis formula only considers the six configurations that directly\naffect model size, while excluding other configurations like the\ntokenizer from our constraint satisfaction problem-solving process.\nWe then use the above formula and the raw configuration space\nas inputs to Z3, to find the configurations for which the formula\nevaluates to a value less than 3 MB. Considering that solving with\nZ3 can slow down significantly when dealing with an overly large\nconfiguration space [21, 73], we run Z3 by partitioning the configu-\nration space into several smaller subspaces and processing them in\nparallel. Taking the vocabulary size as an example, we can partition\nthe original range of 1,000 to 50,265 into 50 subranges, i.e., 1,000\nto 2,000, 2,000 to 3,000, etc. These 50 subranges are then combined\nwith the tuning ranges of other configurations, forming 50 sub-\nspaces. Each subspaceâ€™s constraint satisfaction problem is treated\nas an independent task and solved in parallel using separate Z3\nthreads. Once all tasks are completed, we aggregate the results to\nform a new, pruned configuration space, as shown in Listing 4. The\nunderlined entries, i.e., the vocabulary size, hidden size, and inter-\nmediate size, have been pruned. This process significantly reduces\nthe configuration space from4.5Ã—1019 to 1.3Ã—1019, which accounts\nfor only 28.9% of the original space. Notably, the pruned configura-\ntion space still contains a broad and diverse range of configurations,\nproviding sufficient space to identify Pareto-optimal solutions.\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo\n3.4 Effectiveness Indicator\nWhen tuning configurations, assessing the effectiveness of a model\nthat has a given set of configurations is essential to determine\nwhether it qualifies as a Pareto-optimal solution. However, obtain-\ning model effectiveness through training and testing is computa-\ntionally expensive. Inspired by recent work in leveraging machine\nlearning techniques to predict the runtime performance of soft-\nware [20, 27, 28], we propose to construct a regression model as a\nproxy for the training and testing process. Specifically, the regres-\nsion model builds a computationally efficient function that maps a\nmodelâ€™s configurations to its effectiveness, enabling us to estimate\na modelâ€™s effectiveness using only the provided configuration as\ninput. Consequently, this approach eliminates the need for resource-\nintensive model training and testing. We consider this regression\nmodel as an effectiveness indicator.\nWe follow the procedures outlined in Listing 5 to develop an\neffectiveness indicator. First, we randomly sample a set of configura-\ntions from the pruned configuration space (line 7). Next, we utilize\nthe knowledge distillation technique introduced in Section 2 to train\na model for each of these sampled configurations (line 10). We then\nevaluate the effectiveness of these models on the validation dataset\n(line 11), which has a similar distribution to the test dataset, but re-\nmains distinct and is not used for training. Subsequently, we use the\nsampled configurations and the corresponding effectiveness values\nto train a regression model that serves as our effectiveness indicator\n(line 12). For this purpose, we employ Bayesian Ridge Regression\n(BRR) [72]. BRR is a statistical regression method that combines\nBayesian principles [ 51] with linear regression techniques [ 67].\nIt trains regression models by minimizing the squared difference\nbetween predicted and actual target values. BRR is particularly valu-\nable when dealing with limited data points, which is the case for\nour effectiveness indicator since we have only a few sampled con-\nfigurations. Note that the regression model usually takes numbers\nas inputs, while some of our configurations are strings. For these\nconfigurations, we use their corresponding indices in the tuning\nrange as inputs to the regression model. For example, the tokenizer\nhas four options, so we use 0, 1, 2, and 3 to represent them.\n3.5 Multi-Objective Configuration Tuning\nWith the pruned configuration space and effectiveness indicator, we\nare now ready to introduce our innovative multi-objective configu-\nration tuning algorithm, which is specifically designed to identify\nthe set of Pareto-optimal configurations in terms of size, FLOPs,\nand effectiveness for optimizing large language models of code.\nAs presented in Listing 6, our algorithm takes the pruned con-\nfiguration space, the effectiveness indicator, and the number of\ngenerations as inputs. It starts by generating an initial population\nof configuration sets by an adaptive random initialization method\n(line 5). These configurations are then assessed in terms of the three\nobjectives (line 6): the size and FLOPs are calculated with the imple-\nmentation of Compressor [65], while the effectiveness indicator\npredicts the effectiveness. The algorithm maintains an archive to\nstore the Pareto-optimal configurations (line 7). This archive is ini-\ntialized as an empty set and is updated throughout the algorithmâ€™s\nexecution. Subsequently, it enters an iterative loop that runs for a\nspecified number of generations. At each iteration, the algorithm\n1 input C: pruned configuration space\n2 input ğ¼: effectiveness indicator\n3 input ğ‘”: number of generations\n4 input ğ‘: population size\n5 ğ‘ƒ = adaptive-random-initialization(C,ğ‘)\n6 ğ‘Š = calculate-objectives(ğ‘ƒ,ğ¼ )\n7 ğ´= update-archive(ğ‘ƒ,ğ‘Š, âˆ…)\n8 for ğ‘– = 0 to ğ‘”:\n9 ğ‘„ = two-point-crossover(ğ‘ƒ)\n10 ğ‘„ = boundary-random-mutation(ğ‘„)\n11 ğ‘„ = correction(ğ‘„)\n12 ğ‘Š = calculate-objectives(ğ‘„,ğ¼ )\n13 ğ´= update-archive(ğ‘„,ğ‘Š,ğ´ )\n14 ğ‘ƒ = tournament-selection(ğ‘ƒâˆªğ‘„)\n15 return ğ´\nListing 6: Algorithm of multi-objective configuration tuning.\napplies three operators, i.e., two-point crossover, boundary random\nmutation, and correction, to generate new offspring from the popu-\nlation (lines 9 to 11). These offspring are then evaluated, and the\narchive of Pareto-optimal configurations is updated accordingly\n(lines 12 to 13). The next generation of population is selected from\nthe current population and the offspring by a tournament selection\nmethod (line 14). After the loop terminates, the algorithm returns\nthe archive of Pareto-optimal configurations (line 15). The main\noperators and steps are described in detail below.\nAdaptive Random Initialization. We aim to assemble an initial\npopulation of highly diverse configuration sets, which can facilitate\nmore efficient exploration of the configuration space. To achieve\nthis, we employ adaptive random initialization [1, 50], an extension\nof naive random search that attempts to maximize the Euclidean\ndistance between the selected configurations in the population.\nConcretely, this method first randomly selects a configuration set\nğ‘ from the configuration space. It then randomly selects another\nconfiguration set ğ‘â€²and compares the Euclidean distance betweenğ‘\nand ğ‘â€²with the distance between ğ‘and the other configuration sets\nalready present in the population. If the distance between ğ‘ and ğ‘â€²\nexceeds those between ğ‘and other configuration sets,ğ‘â€²is added to\nthe population. Otherwise, ğ‘â€²is discarded. This process continues\nuntil the population reaches the desired size. Importantly, when\ncalculating the Euclidean distance, as when training the effective-\nness indicator, we replace the configuration in the form of strings\nwith its corresponding numerical index within the tuning range.\nTwo-Point Crossover. This operator, commonly used in meta-\nheuristic algorithms such as genetic algorithms to solve optimiza-\ntion problems [44, 66], aims to combine two parent configurations\nto generate new offspring configurations. It begins by randomly\nselecting two parent configurations and two crossover points. Sub-\nsequently, it swaps the values of the two parent configurations\nbetween these two crossover points to create two offspring con-\nfigurations. For instance, if the two parent configurations are de-\nnoted as ğ‘1 and ğ‘2, and the selected crossover points are ğ‘1 and\nğ‘2, the resulting offspring configurations are computed as follows:\nğ‘1 [0 :ğ‘1]+ğ‘2 [ğ‘1 : ğ‘2]+ğ‘1 [ğ‘2 :]and ğ‘2 [0 :ğ‘1]+ğ‘1 [ğ‘1 : ğ‘2]+ğ‘2 [ğ‘2 :].\nHere, ğ‘1 [0 :ğ‘1]represents the values of ğ‘1 before ğ‘1, and ğ‘1 [ğ‘2 :]\nGreening Large Language Models of Code ICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal\nrepresents the values of ğ‘1 from ğ‘2 to the end. The generated off-\nspring configurations are then added to the population.\nBoundary Random Mutation. This operator introduces random\nmodifications to the values of a configuration set, resulting in a new\noffspring configuration. Following recent work utilizing genetic al-\ngorithms for optimization problems [65, 79], we employ the bound-\nary random mutation operator to generate offspring configurations.\nThe process begins by randomly selecting a configuration from the\npopulation. Subsequently, for each configuration value within this\nselected configuration, a mutation rate ğ‘Ÿ is randomly chosen from\nthe range of [0,1]. If ğ‘Ÿ falls below a predefined threshold, the se-\nlected configuration value is set to a random value within its tuning\nrange, while ensuring that the modified solution remains within\nthe feasible configuration space, i.e., the boundary. The resulting\noffspring configuration is then incorporated into the population.\nCorrection. The above crossover and mutation operators may\nproduce invalid offspring configurations that are unusable for ini-\ntializing models. For example, according to the implementation\nof Hugging Face [15], a modelâ€™s hidden size must be divisible by\nthe number of attention heads; otherwise, the model will fail to\ninitialize due to dimension misalignment errors. To address such\ncases and rectify them, our tuning algorithm employs correction\noperators. When it encounters invalid offspring configurations, it\ndiscards their values and proceeds to randomly select new values\nuntil the offspring configuration becomes valid.\nTournament Selection. The selection operator plays a key role\nin constructing the next generation from the existing population\nand the newly generated offspring. Using the tournament selection\nmethod [17], a well-established technique in metaheuristic algo-\nrithms, a fixed number of configurations are randomly selected\nfrom the combined pool of the current population and offspring.\nThen, the Pareto-optimal ones are selected from these configura-\ntions and added to the next generation, ensuring that the most\npromising candidates are retained for the next iteration.\nAs mentioned above, the algorithm manages and continuously\nupdates an archive of Pareto-optimal configurations throughout\nits execution. When evaluating a configuration set, the algorithm\ncompares it with the configurations already present in the archive.\nIf the evaluated configuration set is not dominated by any other\nconfiguration set in the archive, it secures its place within the\narchive. Additionally, if any configuration set in the archive is found\nto be dominated by the new configuration set, it will be excluded\nfrom the archive. This process ensures the archive contains only\nnon-dominated configurations, i.e., Pareto-optimal solutions. The\nalgorithm terminates when the specified number of generations\nis reached, at which point it returns the archive of Pareto-optimal\nconfigurations. We then select a configuration set from the archive\nto train a compact and green model using knowledge distillation.\n4 EMPIRICAL EVALUATION\nOur evaluation aims to answer the following research questions:\nâ€¢RQ1 (Effectiveness): How effective is Avatarin optimizing\nlanguage models of code?\nâ€¢RQ2 (Comparison): How does Avatarcompare to the state-of-\nthe-art method in optimizing language models of code?\nTable 1: Overview of datasets used in our experiments.\nDataset Labeled/Unlabeled\nVal/Test Language Source\nDevign [86] 10,927/10,927\n2,732/2,732 C FFmpeg\nQemu\nBigCloneBench [69] 45,051/45,051\n4,000/4,000 Java SourceForge\nGoogle Code\n4.1 Experimental Setup\nTasks and Datasets. Following the evaluation settings in the prior\nwork [65], we assess the performance of Avataron two popular\nsoftware engineering tasks: vulnerability prediction and clone de-\ntection. Table 1 provides an overview of the datasets used in our\nexperiments. These datasets encompass different programming lan-\nguages and sizes, allowing for a thorough evaluation of Avatar.\nMore details on the tasks and datasets are provided below.\nThe vulnerability prediction task involves determining whether\na given code snippet is vulnerable or not. Integrating vulnerability\nprediction models into an IDE can significantly assist developers in\nidentifying critical program defects early, thus enhancing software\nquality and reducing maintenance costs. For our experiment, we\nutilize the Devign dataset [86], which was released by Zhou et al. It\ncontains 27,318 functions from two popular open-source C libraries,\ni.e., FFmpeg and Qemu. The dataset was constructed by manually\nannotating whether these functions contain vulnerabilities or not.\nWe first follow the CodeXGLUE [49] benchmark for dataset splitting,\nallocating 80% for training, 10% for validation, and 10% for testing.\nTo facilitate knowledge distillation, which requires unlabeled data,\nwe follow Compressor [65] to evenly divide the training set into\ntwo mutually exclusive halves. One half is used for fine-tuning the\nlanguage models, while the other, with erased labels, serves to train\nthe model with configurations generated by Avatar.\nThe clone detection task aims to identify whether two given\nfunctions are code clones, assisting in recognizing redundant im-\nplementations of the same functionalities during software mainte-\nnance. For evaluating Avatarâ€™s effectiveness in clone detection, we\nselect the widely-used BigCloneBench dataset [69]. This dataset is\ncollected by mining the clones of specific functionalities in 25,000\nJava projects sourced from SourceForge and Google Code plat-\nform. It includes over 6,000,000 pairs of cloned Java methods, along\nwith 260,000 non-clone pairs. We follow recent studies [65, 79] to\nrandomly select 90,102 examples (i.e., 10% of the original training\ndataset) for training and reserve 4,000 for validation and testing.\nThen, we divide the training data into labeled and unlabeled por-\ntions of equal size, which are for fine-tuning large models and\ntraining optimized models, respectively.\nLanguage Models of Code. To evaluate Avatar, we follow Shi et\nal. [65] to use two popular encoding-only language models of code:\nCodeBERT [18] and GraphCodeBERT [26]. These two models share\nthe same architecture and have been language on the CodeSearch-\nNet dataset [38]. CodeBERT undergoes pre-training with two tasks:\nmasked language modeling, which predicts masked tokens in in-\nput texts, and replaced token detection, which identifies whether\na token in a given input has been replaced. GraphCodeBERT also\nuses masked language modeling, but also incorporates code graph\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo\nTable 2: Results of Avatarand the original language models on the two tasks. â€œCBâ€ and â€œGCBâ€ denote CodeBERT and\nGraphCodeBERT, respectively. â€œACCâ€ is the prediction accuracy. â€œLATâ€ is the inference latency. â€œEâ€ is the energy consumption.\nâ€œCO2â€ is the CO 2 emission, i.e., the carbon footprint.\nModel Vulnerability Prediction Clone Detection\nACC (%) LAT (ms) E (kWh) CO 2 (kg) GFLOPs ACC (%) LAT (ms) E (kWh) CO 2 (kg) GFLOPs\nCB (481 MB) 61.82 1394 0.32 0.14 138.4 96.10 1963 0.65 0.28 138.4\nCB-Avatar(3 MB) 60.87 (-0.95) 29 (48Ã—) 0.006 (53Ã—) 0.003 (47Ã—) 0.64 (216Ã—) 93.69 (-2.41) 19 (103Ã—) 0.006 (108Ã—) 0.003 (93 Ã—) 1.14 (121 Ã—)\nGCB (481 MB) 61.57 1139 0.26 0.11 138.4 96.85 1539 0.52 0.22 138.4\nGCB-Avatar(3 MB) 61.12 (-0.45) 15 (76Ã—) 0.005 (52Ã—) 0.002 (55Ã—) 0.67 (207Ã—) 94.00 (-2.85) 10 (154Ã—) 0.002 (260Ã—) 0.001 (220Ã—) 0.80 (173Ã—)\nAverage Loss/Gain -0.70 62Ã— 53Ã— 51Ã— 212Ã— -2.63 129Ã— 184Ã— 157Ã— 147Ã—\nstructure information by predicting masked nodes in data flow\ngraphs during pre-training. After pre-training, both CodeBERT and\nGraphCodeBERT can be fine-tuned on downstream tasks, enabling\nthem to achieve state-of-the-art performance [49, 56, 82].\nTo fine-tune CodeBERT, we use the hyperparameter settings\nfrom the CodeXGLUE benchmark [49]. In the case of GraphCode-\nBERT, we follow the hyperparameter settings described in the\nGraphCodeBERT paper [26]. All models deliver results comparable\nto those reported in the previous study [82].\nEvaluation Metrics. After obtaining the model trained with config-\nurations tuned by Avatar, we compare it with the language model\nand the model generated by our baseline method, Compressor, us-\ning six metrics: effectiveness, model size, inference latency, energy\nconsumption, carbon footprint, and Giga floating-point operations\n(GFLOPs). Effectiveness is evaluated by prediction accuracy on the\ntwo downstream tasks, following prior studies [65, 79]. Model size\nis quantified in megabytes (MB). For inference latency, which is\nmeasured in milliseconds (ms), we standardize experimental con-\nditions by limiting all models to use only 8 CPU cores, simulating\nrunning on a typical consumer-grade laptop. The testing datasets\nare used to query the models, and the average inference latency is\ncalculated for each data example. Note that we use a batch size of\n1 to replicate real-world scenarios where models are deployed on\nlaptops and only process a single input at a time.\nTo evaluate energy consumption and carbon footprint, we use\nthe Machine Learning Emissions Calculator2, developed by Lacoste\net al. [46]. The tool requires the total running time of a model as\ninput and outputs the energy consumption and carbon footprint,\nmeasured in kilowatt-hours (kWh) and kilograms (kg), respectively.\nWe record the total running time of the models on the testing\ndatasets as input to the tool, and consistent with our inference\nlatency evaluation, we use a batch size of 1. Additionally, as men-\ntioned in Section 3, GFLOPs are commonly used to quantify the\ncomputational cost of a model, which is closely related to energy\nconsumption and carbon footprint. Thus, we also report GFLOPs to\nillustrate how Avatarcontributes to environmental sustainability\nby reducing the computational cost of language models of code.\nImplementation. We run all experiments on an Ubuntu 18.04\nserver equipped with an Intel Xeon E5-2698 CPU, 504 GB of RAM,\nand 8 Tesla V100 GPUs. To prune the configuration space with Z3,\nwe partition it into 25,600 subspaces and execute Z3 in parallel\nacross 80 CPU cores. For training the effectiveness indicator, we\nsample 20 sets of configurations from the pruned configuration\n2https://mlco2.github.io/impact/#compute\nspace. In the multi-objective tuning algorithm, we configure the\npopulation size to be 20, with 50 generations. The crossover and\nmutation rates were set to 0.6 and 0.1, respectively.\n4.2 Effectiveness of Avatar(RQ1)\nAfter obtaining the Pareto-optimal configurations using Avatar,\nwe select the configuration with a model size closest to 3 MB for\ntraining the optimized model. This results in a model that is ap-\nproximately 160Ã—smaller than the original language model of code\nfor each task. Table 2 shows the experimental results comparing\nthe optimized models with the original ones. On the two tasks,\nthe optimized models exhibit an average decrease in accuracy of\nonly 1.67% (â‰ˆ(0.70% +2.63%)/2) compared to the original large\nmodels. This accuracy result illustrates that Avatarsignificantly\noptimizes model size with only a negligible loss in effectiveness\non downstream tasks. Furthermore, the inference latency of the\noptimized models sees a substantial reduction on both tasks, with\nan average reduction of 62Ã—for vulnerability detection and 129Ã—\nfor clone detection. Prior research [52] has suggested that software\npractitioners are willing to accept a small sacrifice in effectiveness\nin exchange for a significant improvement in usability. Therefore,\nwe consider the reduced accuracy of the optimized models to be\nacceptable in practical applications.\nTable 2 also presents results of optimizing language models in\nterms of environmental sustainability. We employ the Machine\nLearning Emissions Calculator [ 46] to calculate the energy con-\nsumption and carbon footprint of the optimized models, comparing\nthem to the original ones. Note that these results are calculated\nusing a single NVIDIA Tesla V100 GPU and encompass the cost of\nrunning the entire testing dataset rather than a single query. On\nboth tasks, the energy consumption of the optimized models sees\na significant reduction, averaging 53Ã—and 184Ã—less, respectively.\nThis reduction extends to a corresponding decrease in carbon foot-\nprint, ranging from 51Ã—to 157Ã—less. Additionally, we observe a\nnotable reduction in GFLOPs for the optimized models, with an\naverage reduction of 212Ã—and 147Ã—on the two tasks, respectively.\nThese results underscore the sustainability benefits that the opti-\nmized models can offer in real-world deployments.\nAnswers to RQ1: Avatareffectively optimizes language mod-\nels of code in terms of model size (160 Ã—smaller), inference\nlatency (up to 76Ã—faster), energy consumption (up to 184Ã—\nless), and carbon footprint (up to 157Ã—less), with only a negli-\ngible loss in effectiveness (1.67% on average).\nGreening Large Language Models of Code ICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal\nTable 3: Results of Avatarand Compressor on the two tasks. â€œCBâ€ and â€œGCBâ€ denote CodeBERT and GraphCodeBERT,\nrespectively. â€œACCâ€ is the prediction accuracy. â€œLATâ€ is the inference latency. â€œEâ€ is the energy consumption. â€œCO 2â€ is the CO 2\nemission, i.e., the carbon footprint.\nModel Vulnerability Prediction Clone Detection\nACC (%) LAT (ms) E (kWh) CO 2 (kg) GFLOPs ACC (%) LAT (ms) E (kWh) CO 2 (kg) GFLOPs\nCB-Compressor(3 MB) 59.11 521 0.012 0.006 2.25 95.40 601 0.02 0.01 2.25\nCB-Avatar(3 MB) 60.87 (+1.76) 29 (18Ã—) 0.006 (2Ã—) 0.003 (2Ã—) 0.64 (4 Ã—) 93.69 (-1.71) 19 (32Ã—) 0.006 (3 Ã—) 0.003 (3 Ã—) 1.14 (2 Ã—)\nGCB-Compressor(3 MB) 59.99 702 0.016 0.007 2.25 92.15 747 0.025 0.011 2.25\nGCB-Avatar(3 MB) 61.12 (+1.13) 15 (47Ã—) 0.005 (3Ã—) 0.002 (4Ã—) 0.67 (3 Ã—) 94.00 (+1.85) 10 (75Ã—) 0.002 (13Ã—) 0.001 (11Ã—) 0.80 (3 Ã—)\nAverage Loss/Gain +1.45 33 Ã— 3Ã— 4Ã— 4Ã— +0.07 54 Ã— 8Ã— 7Ã— 3Ã—\n4.3 Avatarvs. Compressor (RQ2)\nAs the baseline for our experiments, we employ the approach,Com-\npressor, proposed by Shi et al. [65]. To ensure a fair comparison,\nwe directly utilize the models available in the official repository of\nCompressor. The models produced usingCompressor and Avatar\nhave a similar size at 3 MB. The evaluation results comparing these\napproaches are presented in Table 3.\nCompared to the models optimized by Compressor, the models\nproduced by Avatarexhibit a slightly higher accuracy, with an av-\nerage improvement of 0.75% (â‰ˆ(1.45% +0.07%)/2) on the two tasks.\nThese results suggest that Avatarcan optimize language models\nof code more effectively without compromising effectiveness as\nmuch as Compressor. More importantly, the models optimized by\nAvatardemonstrate significant improvements in inference latency\non both tasks. Compressor produces models with an inference\nlatency in the hundreds of milliseconds range, while the optimized\nmodels obtained by our approach have a maximum latency of 29\nms. On average, the inference latency of the models optimized by\nAvataris 44Ã—(â‰ˆ(33 +54)/2) faster than that of the ones produced\nby Compressor, which highlights the effectiveness of Avatarin\nenhancing the usability of language models compared to the state-\nof-the-art approach.\nAvataralso improves the energy consumption of the optimized\nmodels by 3Ã—and 8Ã—compared to Compressor on vulnerability\nprediction and clone detection, respectively. These reductions also\ntranslate into a corresponding decrease in carbon footprint, with\nreductions of 4Ã—and 7Ã—on the two tasks. Overall, except for model\nsize, the models optimized by Avataroutperform the ones opti-\nmized by Compressor across all metrics.\nAnswers to RQ2: Avatarsignificantly outperformsCompres-\nsor (i.e., the state-of-the-art approach) in terms of prediction\naccuracy (0.75% on average), inference latency (44 Ã—faster\non average), energy consumption (up to 8Ã—less), and carbon\nfootprint (up to 7Ã—less).\n5 DISCUSSIONS\n5.1 Efficiency of Avatar\nWe investigate the time taken by Avatarto optimize language\nmodels of code, breaking it down into four parts: pruning the con-\nfiguration space, building the effectiveness indicator, executing the\nconfiguration tuning algorithm, and training optimized models.\nIn our experimental setup, the parallel execution of pruning the\nconfiguration space takes just 5 minutes to complete. After that,\nTable 4: Usefulness of Avatarin cloud deployment. The\nresults show how many queries that the models can process\nper second when deployed on a cloud server.\nModel Vulnerability Prediction Clone Detection\nCodeBERT 58 64\nCodeBERT-Avatar 171 (2.9Ã—) 476 (7.4 Ã—)\nGraphCodeBERT 79 48\nGraphCodeBERT-Avatar 390 (4.9Ã—) 570 (11.9 Ã—)\nAverage Improvements 3.9Ã— 9.7Ã—\nAvataruses a single 16 GB Tesla V100 GPU to train 20 models for\nconstructing the effectiveness indicator, consuming approximately\n10 hours. Note that this overhead is only rarely incurred, e.g., the\nfirst time optimizing a language model for deployment, which may\noccur only on a monthly or yearly basis. Because of the carefully\npruned configuration space and the specialized optimization algo-\nrithm, Avatarefficiently returned Pareto-optimal configurations\nin about 2 minutes. Subsequently, the knowledge distillation phase\nrequired more time, withAvatartaking an average of 14.9 and 18.3\nminutes to train an optimized model for the vulnerability prediction\nand clone detection tasks, respectively. These results underscore\nthe fact that Avatarcan produce well-performing optimized mod-\nels with much less time cost than fine-tuning or pre-training large\nlanguage models, which often takes a few hours or days [65].\n5.2 Usefulness in Cloud Deployment\nThe primary goal of Avataris to optimize language models of code\nfor deployment on developersâ€™ personal devices like laptops. As\nmentioned in Section 1, we hold this perspective due to privacy\nconcerns [36, 48, 57, 80] and the need for use under poor network\nconditions. Deploying models on cloud servers may not be a viable\noption because it requires sending code to third-party vendors,\nwhich is prohibited by some companies that consider code bases to\nbe important intelligent properties. Also, cloud deployment may\nresult in more inference latency for developers in some regions with\npoor bandwidth or Internet coverage. However, we acknowledge\nthat cloud deployment is a common practice today, offering more\ncomputing resources and scalability to support a larger user base.\nTherefore, it would be worthwhile to also discuss the benefits of\noptimized models in the context of cloud deployments.\nWe run experiments assuming that the models process queries in\nbatch mode with a batch size of 100. These experiments are run on a\nserver equipped with a Tesla V100 GPU. We send the queries directly\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo\nfrom the GPUâ€™s host machine to eliminate any potential impact\nfrom network fluctuations, and then measure how many queries the\nmodels can process per second. The experimental results, presented\nin Table 4, show that compared to the original language models\nof code, the optimized models can process on average 3.9 Ã—and\n9.7Ã—more queries per second on the two tasks, respectively. These\nresults highlight the advantages of using Avatarfor deploying\nlarge language models of code in cloud servers.\n5.3 Threats to Validity\nOne potential threat to internal validity is the randomness inherent\nin the configuration tuning algorithms used in our experiments.\nTo address this concern, we have run each experiment 10 times\nand reported the average results, as recommended by Arcuri and\nBriand [3]. Regarding external validity, a potential threat is that our\nresults may not be generalizable to other models and tasks beyond\nthe ones we have studied. To ensure the generalizability of our\nwork, we have carefully selected two representative encoder-only\nlanguage models of code and two popular downstream tasks with\ndifferent characteristics for our evaluation. This ensures that our\nresults are unbiased and our method potentially applies to a broad\ncontext. While we have not yet applied our method to other types\nof language models, such as decoder-only models, which have also\nrecently gained popularity, we plan to extend our study on those\nmodels to further validate our workâ€™s generalizability in the future.\nOne threat to construct validity is that the evaluation metrics may\nnot fully capture the performance of our Avatarand the baseline\nin enhancing the usability and sustainability of language models of\ncode. To mitigate it, we use a total of five widely-used evaluation\nmetrics to compare the effectiveness of Avatarand the baseline\nfrom a comprehensive set of perspectives.\n6 RELATED WORK\nIn recent years, both the natural language processing and software\nengineering communities have dedicated their efforts to optimizing\nlanguage models. However, unlike our work, which seeks to simul-\ntaneously optimize multiple aspects of language models of code,\nmost existing studies focus on reducing model size only, thereby\nindirectly mitigating other related issues such as inference latency.\nThese existing studies typically fall into three main categories:\nmodel pruning, model quantization, and knowledge distillation.\nModel pruning and quantization involve directly altering model\nparameters to reduce model size. Model pruning replaces certain\nparameters with zeros, or removes network components like hid-\nden layers [16, 54]. Model quantization converts a modelâ€™s 32-bit\nfloating-point parameters into lower-bit fixed-point values [19, 43,\n81]. These techniques have proven effective in reducing model size\nto a level suitable for deployment in scenarios with less stringent\nrequirements. A recent study has also demonstrated their potential\nto reduce the computational cost and carbon footprint of language\nmodels of code [75], offering a promising avenue for future research.\nHowever, these techniques fall short of meeting the 3 MB model\nsize recommendation put forth by Svyatkovskiy et al. [70] within\nthe context of software engineering. As a result, we have chosen\nnot to include them in our pipeline and comparison experiments.\nWe have introduced knowledge distillation in Section 2, an es-\nsential step in Avatarand the baseline. While several knowledge\ndistillation methods have been proposed, most of them typically\nresult in models ranging from 100 to 200 MB [41, 60, 68, 77]. Some\nstudies [8, 71, 78, 84] have successfully optimized language models\ninto sizes ranging from 20 to 40 MB. Notably, onlyCompressor [65]\nhas achieved the remarkable feat of optimizing a large language\nmodel of around 500 MB into a compact 3 MB model. Therefore,\nwe only compare Avatarwith Compressor in our experiments.\nThe software engineering research community has also explored\nalternative methods for optimizing language models of code. For\nexample, Grishina et al. [25] propose using only the initial layers of\nlanguage models during inference to reduce resource consumption.\nAdditionally, Zhang et al. [83] introduce a technique to simplify\nthe input programs for CodeBERT, significantly reducing compu-\ntational cost without compromising model performance. Despite\nthese efforts, there are still gaps in optimizing language models\nof code to simultaneously improve usability and environmental\nsustainability. To the best of our knowledge, our study is the first\nto address both aspects concurrently.\n7 CONCLUSION AND FUTURE WORK\nThis paper proposes Avatar, a novel approach that can optimize\nlarge language models of code in terms of model size, inference\nlatency, energy consumption, and carbon footprint without sacri-\nficing effectiveness (e.g., prediction accuracy on downstream tasks)\nby much, thereby improving the usability and environmental sus-\ntainability of language models of code. The key idea of Avatar\nis to formulate the optimization of language models as a multi-\nobjective configuration tuning problem and solve it with the help\nof SMT solvers and a tailored optimization algorithm. We evaluate\nAvatarwith two state-of-the-art language models, i.e., CodeBERT\nand GraphCodeBERT, on two popular tasks, i.e., vulnerability pre-\ndiction and clone detection. We use Avatarto produce optimized\nmodels with a small size (3 MB), which is 160 Ã—smaller than the\noriginal large models. On the two tasks, the optimized models\ncan significantly reduce the energy consumption (up to 184Ã—less),\ncarbon footprint (up to 157 Ã—less), and inference latency (up to\n76Ã—faster), with only a negligible loss in effectiveness (1.67% on\naverage). Compared with the state-of-the-art approach, Avatar\noptimizes language models of code more effectively in all metrics.\nIn the future, we plan to further investigate the effectiveness\nand efficiency of our proposed approach Avatarby experimenting\nwith more large language models of code beyond those considered\nin this paper, such as the generative language models of code.\nReplication Package: The code, datasets, and documentation\nfor this work, along with all obtained models, are available\nvia this link: https://github.com/soarsmu/Avatar.\nACKNOWLEDGMENTS\nThis research / project is supported by the National Research Foun-\ndation, under its Investigatorship Grant (NRF-NRFI08-2022-0002).\nAny opinions, findings and conclusions or recommendations ex-\npressed in this material are those of the author(s) and do not reflect\nthe views of National Research Foundation, Singapore.\nGreening Large Language Models of Code ICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal\nREFERENCES\n[1] Raja Ben Abdessalem, Annibale Panichella, Shiva Nejati, Lionel C Briand, and\nThomas Stifter. 2018. Testing autonomous cars for feature interaction failures\nusing many-objective search. In Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering . 143â€“154.\n[2] Akvelon. 2023. Code Search: a Closer Look at Akvelonâ€™s Source Code Search\nEngine â€” akvelon.com. https://akvelon.com/code-search-a-closer-look-at-\nakvelons-source-code-search-engine/. [Accessed 28-09-2023].\n[3] Andrea Arcuri and Lionel Briand. 2011. A practical guide for using statistical\ntests to assess randomized algorithms in software engineering. In Proceedings of\nthe 33rd international conference on software engineering . 1â€“10.\n[4] Gareth Ari Aye and Gail E Kaiser. 2020. Sequence model design for code comple-\ntion in the modern IDE. arXiv preprint arXiv:2004.05249 (2020).\n[5] Lei Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep?. In\nProceedings of the 27th International Conference on Neural Information Processing\nSystems-Volume 2. 2654â€“2662.\n[6] Nikolaj BjÃ¸rner. 2013. SMT in verification, modeling, and testing at microsoft.\nIn Hardware and Software: Verification and Testing: 8th International Haifa Verifi-\ncation Conference, HVC 2012, Haifa, Israel, November 6-8, 2012. Revised Selected\nPapers 8 . Springer, 3â€“3.\n[7] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. 2021.\nDeep learning based vulnerability detection: Are we there yet. IEEE Transactions\non Software Engineering (2021).\n[8] Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding,\nHongbo Deng, Jun Huang, Wei Lin, and Jingren Zhou. 2020. AdaBERT: Task-\nAdaptive BERT Compression with Differentiable Neural Architecture Search.\nIn Proceedings of the Twenty-Ninth International Joint Conference on Artificial\nIntelligence, IJCAI 2020 . 2463â€“2469.\n[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).\n[10] Tao Chen and Miqing Li. 2023. The weights can be harmful: Pareto search versus\nweighted search in multi-objective search-based software engineering. ACM\nTransactions on Software Engineering and Methodology 32, 1 (2023), 1â€“40.\n[11] Leonardo De Moura and Nikolaj BjÃ¸rner. 2008. Z3: An efficient SMT solver. In\nInternational conference on Tools and Algorithms for the Construction and Analysis\nof Systems . Springer, 337â€“340.\n[12] GitLab Auto DevOps. 2023. Top 10 ways machine learning may help De-\nvOps â€” about.gitlab.com. https://about.gitlab.com/blog/2022/02/14/top-10-ways-\nmachine-learning-may-help-devops/. [Accessed 22-09-2023].\n[13] Thomas Dohmke, Marco Iansiti, and Greg Richards. 2023. Sea Change in Software\nDevelopment: Economic and Productivity Analysis of the AI-Powered Developer\nLifecycle. arXiv preprint arXiv:2306.15033 (2023).\n[14] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units\nfor neural network function approximation in reinforcement learning. Neural\nnetworks 107 (2018), 3â€“11.\n[15] Hugging Face. 2023. Configurations of Encoder-only Models â€” hug-\ngingface.co. https://huggingface.co/docs/transformers/model_doc/roberta#\ntransformers.RobertaConfig. [Accessed 25-09-2023].\n[16] Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing Transformer\nDepth on Demand with Structured Dropout. In 2020 8th International Conference\non Learning Representations .\n[17] Yongsheng Fang and Jun Li. 2010. A review of tournament selection in ge-\nnetic programming. In International symposium on intelligence computation and\napplications. Springer, 181â€“192.\n[18] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\nLinjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:\nA Pre-Trained Model for Programming and Natural Languages. InFindings of the\nAssociation for Computational Linguistics: EMNLP 2020 . Association for Computa-\ntional Linguistics, Online, 1536â€“1547.\n[19] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan\nSajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021. Compressing\nLarge-Scale Transformer-Based Models: A Case Study on BERT. Transactions of\nthe Association for Computational Linguistics 9 (09 2021), 1061â€“1080.\n[20] Yanjie Gao, Xianyu Gu, Hongyu Zhang, Haoxiang Lin, and Mao Yang. 2023.\nRuntime performance prediction for deep learning models with graph neural\nnetwork. In 2023 IEEE/ACM 45th International Conference on Software Engineering:\nSoftware Engineering in Practice (ICSE-SEIP) . IEEE, 368â€“380.\n[21] Yanjie Gao, Yonghao Zhu, Hongyu Zhang, Haoxiang Lin, and Mao Yang. 2021.\nResource-guided configuration space reduction for deep learning models. In 2021\nIEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE,\n175â€“187.\n[22] GitHub. 2023. GitHub Copilot Community. https://github.com/orgs/community/\ndiscussions/categories/copilot?discussions_q=category%3ACopilot+network.\n[Accessed 03-10-2023].\n[23] GitHub. 2023. GitHub Copilot Â· Your AI pair programmer â€” github.com. https:\n//github.com/features/copilot/. [Accessed 22-09-2023].\n[24] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowl-\nedge distillation: A survey. International Journal of Computer Vision 129, 6 (2021),\n1789â€“1819.\n[25] Anastasiia Grishina, Max Hort, and Leon Moonen. 2023. The EarlyBIRD Catches\nthe Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code\nClassification. arXiv preprint arXiv:2305.04940 (2023).\n[26] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long\nZhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and\nMing Zhou. 2021. GraphCode{BERT}: Pre-training Code Representations with\nData Flow. In 2021 9th International Conference on Learning Representations .\n[27] Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel\nValov, Krzysztof Czarnecki, Andrzej Wasowski, and Huiqun Yu. 2018. Data-\nefficient performance learning for configurable systems. Empirical Software\nEngineering 23 (2018), 1826â€“1867.\n[28] Huong Ha and Hongyu Zhang. 2019. DeepPerf: Performance prediction for\nconfigurable software with deep sparse neural network. In 2019 IEEE/ACM 41st\nInternational Conference on Software Engineering (ICSE) . IEEE, 1095â€“1106.\n[29] Mirazul Haque, Yaswanth Yadlapalli, Wei Yang, and Cong Liu. 2022. EREBA:\nBlack-Box Energy Testing of Adaptive Neural Networks. In Proceedings of the\n44th International Conference on Software Engineering (Pittsburgh, Pennsylvania)\n(ICSE â€™22) . Association for Computing Machinery, New York, NY, USA, 835â€“846.\n[30] Kazuyuki Hara, Daisuke Saito, and Hayaru Shouno. 2015. Analysis of function of\nrectified linear unit used in deep learning. In 2015 international joint conference\non neural networks (IJCNN) . IEEE, 1â€“8.\n[31] Vincent J Hellendoorn, Sebastian Proksch, Harald C Gall, and Alberto Bacchelli.\n2019. When code completion fails: A case study on real-world completions. In\n2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE,\n960â€“970.\n[32] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 (2016).\n[33] David Hin, Andrey Kan, Huaming Chen, and M Ali Babar. 2022. LineVD:\nStatement-level vulnerability detection using graph neural networks. In Pro-\nceedings of the 19th International Conference on Mining Software Repositories .\n596â€“607.\n[34] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowledge\nin a Neural Network. In 2015 NIPS Deep Learning and Representation Learning\nWorkshop.\n[35] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu\nLuo, David Lo, John Grundy, and Haoyu Wang. 2023. Large Language Mod-\nels for Software Engineering: A Systematic Literature Review. arXiv preprint\narXiv:2308.10620 (2023).\n[36] Yizhan Huang, Yichen Li, Weibin Wu, Jianping Zhang, and Michael R Lyu. 2023.\nDo Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code\nCompletion Tools. arXiv preprint arXiv:2309.07639 (2023).\n[37] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. 2020. Improve Trans-\nformer Models with Better Relative Position Embeddings. Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 (2020).\n[38] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc\nBrockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic\ncode search. arXiv preprint arXiv:1909.09436 (2019).\n[39] Yasir Hussain, Zhiqiu Huang, Yu Zhou, Izhar Ahmed Khan, Nasrullah Khan,\nand Muhammad Zahid Abbas. 2023. Optimized Tokenization Process for Open-\nVocabulary Code Completion: An Empirical Study. In Proceedings of the 27th\nInternational Conference on Evaluation and Assessment in Software Engineering .\n398â€“405.\n[40] Apple Inc. 2023. MacBook Pro 14- and 16-inch - Tech Specs â€” apple.com. https:\n//www.apple.com/sg/macbook-pro-14-and-16/specs/. [Accessed 03-10-2023].\n[41] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang\nWang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language Un-\nderstanding. In Findings of the Association for Computational Linguistics: EMNLP\n2020. 4163â€“4174.\n[42] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and\nAndrea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for\nsource code. In Proceedings of the ACM/IEEE 42nd International Conference on\nSoftware Engineering . 1073â€“1085.\n[43] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer.\n2021. I-BERT: Integer-only BERT Quantization. In Proceedings of the 38th Interna-\ntional Conference on Machine Learning (Proceedings of Machine Learning Research,\nVol. 139). PMLR, 5506â€“5518.\n[44] Padmavathi Kora and Priyanka Yadlapalli. 2017. Crossover operators in genetic\nalgorithms: A review. International Journal of Computer Applications 162, 10\n(2017).\n[45] Taku Kudo. 2018. Subword Regularization: Improving Neural Network Transla-\ntion Models with Multiple Subword Candidates. InProceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics.\nICSE-SEISâ€™24, April 14â€“20, 2024, Lisbon, Portugal Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, and David Lo\n[46] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.\n2019. Quantifying the Carbon Emissions of Machine Learning. arXiv preprint\narXiv:1910.09700 (2019).\n[47] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task learning based pre-\ntrained language model for code completion. InProceedings of the 35th IEEE/ACM\nInternational Conference on Automated Software Engineering . 473â€“485.\n[48] David Lo. 2023. Trustworthy and Synergistic Artificial Intelligence for Software\nEngineering: Vision and Roadmaps. arXiv preprint arXiv:2309.04142 (2023).\n[49] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio\nBlanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong\nZhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan\nDuan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie LIU. 2021.\nCodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\nand Generation. In 35th Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 1) .\n[50] Sean Luke. 2009. Essentials of metaheuristics .\n[51] David JC MacKay. 1992. Bayesian interpolation. Neural Computation 4, 3 (1992),\n415â€“447.\n[52] Irene Manotas, Christian Bird, Rui Zhang, David Shepherd, Ciera Jaspan, Caitlin\nSadowski, Lori Pollock, and James Clause. 2016. An Empirical Study of Practi-\ntionersâ€™ Perspectives on Green Software Engineering. In 2016 IEEE/ACM 38th\nInternational Conference on Software Engineering (ICSE) . 237â€“248.\n[53] Ivan Mehta. 2023. Apple reportedly limits internal use of AI-powered tools\n| TechCrunch. https://techcrunch.com/2023/05/19/apple-reportedly-limits-\ninternal-use-of-ai-powered-tools-like-chatgpt-and-github-copilot. [Accessed\n03-10-2023].\n[54] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\nBetter than One?. In Advances in Neural Information Processing Systems , Vol. 32.\nCurran Associates, Inc.\n[55] Changan Niu, Chuanyi Li, Bin Luo, and Vincent Ng. 2022. Deep Learning Meets\nSoftware Engineering: A Survey on Pre-Trained Models of Source Code. In Pro-\nceedings of the Thirty-First International Joint Conference on Artificial Intelligence,\nIJCAI 2022, Vienna, Austria, 23-29 July 2022 , Luc De Raedt (Ed.). ijcai.org, 5546â€“\n5555.\n[56] Changan Niu, Chuanyi Li, Vincent Ng, Dongxiao Chen, Jidong Ge, and Bin Luo.\n2023. An Empirical Comparison of Pre-Trained Models of Source Code. In 45th\nIEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne,\nAustralia, May 14-20, 2023 . IEEE, 2136â€“2148.\n[57] Liang Niu, Shujaat Mirza, Zayd Maradni, and Christina PÃ¶pper. 2023.\n{CodexLeaks}: Privacy Leaks from Code Generation Language Models in\n{GitHub}Copilot. In 32nd USENIX Security Symposium (USENIX Security 23) .\n2133â€“2150.\n[58] Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-\nqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, et al. 2023. Code\nllama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).\n[59] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.arXiv\npreprint arXiv:1910.01108 (2019).\n[60] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.arXiv\npreprint arXiv:1910.01108 (2019).\n[61] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai.\nCommun. ACM 63, 12 (2020), 54â€“63.\n[62] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine\nTranslation of Rare Words with Subword Units. In54th Annual Meeting of the As-\nsociation for Computational Linguistics . Association for Computational Linguistics\n(ACL), 1715â€“1725.\n[63] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with\nRelative Position Representations. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) . Association for Computational\nLinguistics.\n[64] Jieke Shi, Zhou Yang, Junda He, Bowen Xu, and David Lo. 2022. Can identifier\nsplitting improve open-vocabulary language model of code?. In 2022 IEEE Inter-\nnational Conference on Software Analysis, Evolution and Reengineering (SANER) .\nIEEE, 1134â€“1138.\n[65] Jieke Shi, Zhou Yang, Bowen Xu, Hong Jin Kang, and David Lo. 2022. Compress-\ning pre-trained models of code into 3 mb. In Proceedings of the 37th IEEE/ACM\nInternational Conference on Automated Software Engineering . 1â€“12.\n[66] Seung Yeob Shin, Shiva Nejati, Mehrdad Sabetzadeh, Lionel C Briand, and Frank\nZimmer. 2018. Test case prioritization for acceptance testing of cyber physical\nsystems: a multi-objective search-based approach. In Proceedings of the 27th acm\nsigsoft international symposium on software testing and analysis . 49â€“60.\n[67] Jeffrey M Stanton. 2001. Galton, Pearson, and the peas: A brief history of linear\nregression for statistics instructors. Journal of Statistics Education 9, 3 (2001).\n[68] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient Knowledge Dis-\ntillation for BERT Model Compression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP) . Association for\nComputational Linguistics, Hong Kong, China, 4323â€“4332.\n[69] Jeffrey Svajlenko, Judith F. Islam, Iman Keivanloo, Chanchal K. Roy, and Moham-\nmad Mamun Mia. 2014. Towards a Big Data Curated Benchmark of Inter-project\nCode Clones. In 2014 IEEE International Conference on Software Maintenance and\nEvolution. 476â€“480.\n[70] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vi-\ncente Franco, and Miltiadis Allamanis. 2021. Fast and Memory-Efficient Neural\nCode Completion. In 2021 IEEE/ACM 18th International Conference on Mining\nSoftware Repositories (MSR) . 329â€“340.\n[71] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\n2019. Distilling task-specific knowledge from bert into simple neural networks.\narXiv preprint arXiv:1903.12136 (2019).\n[72] Michael E Tipping. 2001. Sparse Bayesian learning and the relevance vector\nmachine. Journal of machine learning research 1, Jun (2001), 211â€“244.\n[73] Takahisa Toda and Takehide Soh. 2016. Implementing efficient all solutions SAT\nsolvers. Journal of Experimental Algorithmics (JEA) 21 (2016), 1â€“44.\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc.\n[75] Xiaokai Wei, Sujan Gonugondla, Shiqi Wang, Wasi Ahmad, Baishakhi Ray,\nHaifeng Qian, Xiaopeng LI, Varun Kumar, Zijian Wang, Yuchen Tian, Qing Sun,\nBen Athiwaratkun, Mingyue Shang, Murali Krishna Ramanathan, Parminder\nBhatia, and Bing Xiang. 2023. Towards greener yet powerful code generation via\nquantization: An empirical study. In ESEC/FSE 2023 .\n[76] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Googleâ€™s neural machine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).\n[77] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. BERT-\nof-Theseus: Compressing BERT by Progressive Module Replacing. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguistics, Online, 7859â€“7869.\n[78] Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu.\n2021. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with\nNeural Architecture Search. In Proceedings of the 27th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining (Virtual Event, Singapore). Association\nfor Computing Machinery, New York, NY, USA, 1933â€“1943.\n[79] Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022. Natural Attack for Pre-\nTrained Models of Code. In Proceedings of the 44th International Conference on\nSoftware Engineering (Pittsburgh, Pennsylvania) (ICSE â€™22) . Association for Com-\nputing Machinery, New York, NY, USA, 1482â€“1493.\n[80] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun\nHan, and David Lo. 2023. What Do Code Models Memorize? An Empirical Study\non Large Language Models of Code. arXiv preprint arXiv:2308.09932 (2023).\n[81] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020.\nGobo: Quantizing attention-based nlp models for low latency and energy ef-\nficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO). IEEE, 811â€“824.\n[82] Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, and Ling-\nming Zhang. 2022. An Extensive Study on Pre-Trained Models for Program\nUnderstanding and Generation. In Proceedings of the 31st ACM SIGSOFT Interna-\ntional Symposium on Software Testing and Analysis (Virtual, South Korea) (ISSTA\n2022). Association for Computing Machinery, New York, NY, USA, 39â€“51.\n[83] Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong Gu. 2022. Diet code\nis healthy: Simplifying programs for pre-trained models of code. In Proceedings\nof the 30th ACM Joint European Software Engineering Conference and Symposium\non the Foundations of Software Engineering . 1073â€“1084.\n[84] Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou. 2021. Extremely\nSmall BERT Models from Mixed-Vocabulary Training. In Proceedings of the 16th\nConference of the European Chapter of the Association for Computational Linguis-\ntics: Main Volume. Association for Computational Linguistics, Online, 2753â€“2759.\n[85] Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing generalizability of\ncodebert. In 2021 IEEE International Conference on Software Maintenance and\nEvolution (ICSME) . IEEE, 425â€“436.\n[86] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-\nvign: Effective Vulnerability Identification by Learning Comprehensive Program\nSemantics via Graph Neural Networks. In Advances in Neural Information Pro-\ncessing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox,\nand R. Garnett (Eds.), Vol. 32. Curran Associates, Inc.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8290489912033081
    },
    {
      "name": "Solver",
      "score": 0.5228365063667297
    },
    {
      "name": "Code generation",
      "score": 0.5113201141357422
    },
    {
      "name": "Satisfiability modulo theories",
      "score": 0.5020806789398193
    },
    {
      "name": "Energy consumption",
      "score": 0.45873135328292847
    },
    {
      "name": "Code (set theory)",
      "score": 0.4525737464427948
    },
    {
      "name": "Distributed computing",
      "score": 0.4309332072734833
    },
    {
      "name": "Avatar",
      "score": 0.4161493182182312
    },
    {
      "name": "Computer engineering",
      "score": 0.35067930817604065
    },
    {
      "name": "Key (lock)",
      "score": 0.328612357378006
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.3159865736961365
    },
    {
      "name": "Theoretical computer science",
      "score": 0.26877346634864807
    },
    {
      "name": "Programming language",
      "score": 0.24702033400535583
    },
    {
      "name": "Computer security",
      "score": 0.12609532475471497
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I137902535",
      "name": "North Carolina State University",
      "country": "US"
    }
  ],
  "cited_by": 2
}