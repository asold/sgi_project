{
  "title": "End-to-end Speech Recognition with Word-based RNN Language Models",
  "url": "https://openalex.org/W2949079381",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2231538697",
      "name": "Hori Takaaki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993303458",
      "name": "Cho, Jaejin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2575256866",
      "name": "Watanabe, Shinji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953291251",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2775766866",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W1600744878",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2739883972",
    "https://openalex.org/W2795935804",
    "https://openalex.org/W2530876040",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2787663903",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2951327905",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W2962988733",
    "https://openalex.org/W2949190276",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2950903920",
    "https://openalex.org/W6908809"
  ],
  "abstract": "This paper investigates the impact of word-based RNN language models (RNN-LMs) on the performance of end-to-end automatic speech recognition (ASR). In our prior work, we have proposed a multi-level LM, in which character-based and word-based RNN-LMs are combined in hybrid CTC/attention-based ASR. Although this multi-level approach achieves significant error reduction in the Wall Street Journal (WSJ) task, two different LMs need to be trained and used for decoding, which increase the computational cost and memory usage. In this paper, we further propose a novel word-based RNN-LM, which allows us to decode with only the word-based LM, where it provides look-ahead word probabilities to predict next characters instead of the character-based LM, leading competitive accuracy with less computation compared to the multi-level LM. We demonstrate the efficacy of the word-based RNN-LMs using a larger corpus, LibriSpeech, in addition to WSJ we used in the prior work. Furthermore, we show that the proposed model achieves 5.1 %WER for WSJ Eval'92 test set when the vocabulary size is increased, which is the best WER reported for end-to-end ASR systems on this benchmark.",
  "full_text": "END-TO-END SPEECH RECOGNITION WITH WORD-BASED RNN LANGUAGE MODELS\nTakaaki Hori1, Jaejin Cho2, Shinji Watanabe2\n1Mitsubishi Electric Research Laboratories (MERL)\n2Center for Language and Speech Processing, Johns Hopkins University\nthori@merl.com, {jcho52, shinjiw}@jhu.edu\nABSTRACT\nThis paper investigates the impact of word-based RNN language\nmodels (RNN-LMs) on the performance of end-to-end automatic\nspeech recognition (ASR). In our prior work, we have proposed\na multi-level LM, in which character-based and word-based RNN-\nLMs are combined in hybrid CTC/attention-based ASR. Although\nthis multi-level approach achieves signiﬁcant error reduction in the\nWall Street Journal (WSJ) task, two different LMs need to be trained\nand used for decoding, which increase the computational cost and\nmemory usage. In this paper, we further propose a novel word-\nbased RNN-LM, which allows us to decode with only the word-\nbased LM, where it provides look-ahead word probabilities to pre-\ndict next characters instead of the character-based LM, leading com-\npetitive accuracy with less computation compared to the multi-level\nLM. We demonstrate the efﬁcacy of the word-based RNN-LMs us-\ning a larger corpus, LibriSpeech, in addition to WSJ we used in the\nprior work. Furthermore, we show that the proposed model achieves\n5.1 %WER for WSJ Eval’92 test set when the vocabulary size is\nincreased, which is the best WER reported for end-to-end ASR sys-\ntems on this benchmark.\nIndex Terms— End-to-end speech recognition, language mod-\neling, decoding, connectionist temporal classiﬁcation, attention de-\ncoder\n1. INTRODUCTION\nAutomatic speech recognition (ASR) is currently a mature set of\nwidely-deployed technologies that enable successful user interface\napplications such as voice search [1]. However, current systems\nlean heavily on the scaffolding of complicated legacy architec-\ntures that grew up around traditional techniques, including hidden\nMarkov models (HMMs), Gaussian mixture models (GMMs), hy-\nbrid HMM/deep neural network (DNN) systems, and sequence\ndiscriminative training methods [2]. These systems also require\nhand-made pronunciation dictionaries based on linguistic assump-\ntions, extra training steps to derive context-dependent phonetic\nmodels, and text preprocessing such as tokenization for languages\nwithout explicit word boundaries. Consequently, it is quite difﬁ-\ncult for non-experts to develop ASR systems for new applications,\nespecially for new languages.\nEnd-to-end ASR has the goal of simplifying the above module-\nbased architecture into a single-network architecture within a deep\nlearning framework, in order to address these issues. End-to-end\nASR methods typically rely only on paired acoustic and language\ndata without linguistic knowledge, and train the model with a single\nalgorithm. Therefore, the approach makes it feasible to build ASR\nsystems without expert knowledge. There are several types of end-\nto-end architecture for ASR such as connectionist temporal classi-\nﬁcation (CTC) [3], recurrent neural network (RNN) transducer [4],\nattention-based encoder decoder [5], and their hybrid models [6, 7].\nRecently, the use of external language models has shown sig-\nniﬁcant improvement of accuracy in neural machine translation [8]\nand end-to-end ASR [9, 10]. This approach is called shallow fu-\nsion, where the decoder network is combined with an external lan-\nguage model in log probability domain for decoding. In our previous\nwork [9], we have shown the impact of recurrent neural network lan-\nguage models (RNN-LMs) in Japanese and Mandarin Chinese tasks,\nreaching a comparable or higher accuracy to those of state-of-the-art\nDNN/HMM systems. Since the Japanese and Chinese systems were\ndesigned to output character sequences, the RNN-LM was also de-\nsigned as a character-based LM, and effectively combined with the\ndecoder network to jointly predict the next character.\nA character-based architecture achieves high-accuracy ASR for\nlanguages with a large set of characters such as Japanese and Chi-\nnese. It also enables open vocabulary ASR, in contrast to word-based\narchitectures, which suffer from the out-of-vocabulary (OOV) prob-\nlem. However, the character-based LMs generally under-perform\nrelative to word LMs for languages with a phonogram alphabet\nusing fewer distinct characters, such as English, because of the\ndifﬁculty of modeling linguistic constraints across long sequences\nof characters. Actually, English sentences are much longer than\nJapanese and Chinese sentences in the length of character sequence.\nTo overcome this problem, we have further extended end-to-end\nASR decoding with LMs at both the character and word levels\n[11]. During the beam search decoding, Hypotheses are ﬁrst scored\nwith the character-based LM until a word boundary is encountered.\nKnown words are then re-scored using the word-based LM, while\nthe character-based LM provides for out-of-vocabulary scores. This\napproach exploits the beneﬁts of both character and word level ar-\nchitectures, and enables high-accuracy open-vocabulary end-to-end\nASR.\nMore speciﬁcally, the character-based LM yields the following\nbeneﬁts in the decoding process with the word-based LM:\n1. Character-based LM can help correct hypotheses survive until\nthey are rescored at word boundaries during the beam search.\nBefore the hypothesis reaches the boundary, the identity of\nthe last word is unknown and its word probability cannot be\napplied. Hence, good character-level prediction is important\nto avoid pruning errors for hypotheses within a word.\n2. Character-based LM can predict character sequences even for\nOOV words not included in the vocabulary of the word-based\nLM. Since the word-based LM basically cannot predict un-\nseen character sequences, good character-level prediction is\nimportant for open-vocabulary ASR.\nHowever, the multi-level LM approach has a problem that it re-\nquires two different RNN-LMs. To build the two LMs, we need to\narXiv:1808.02608v1  [cs.CL]  8 Aug 2018\nDeep CNN (VGG net) \nBLSTM \nAttention Decoder RNN-LM CTC \ncl cl-1 \nxt x1 xT \n…… …… \n…… …… \nShared \nEncoder \nJoint \nDecoder \nFig. 1. Hybrid attention/CTC network with LM extension: the\nshared encoder contains a VGG net followed by BLSTM layers and\ntrained by both CTC and attention model objectives simultaneously.\nThe joint decoder predicts an output label sequence by the CTC, at-\ntention decoder and RNN-LM.\ntake additional time and effort, almost twice of them, for training\nthe models. Moreover, the two LMs also increase the computational\ncost and memory usage for decoding. Inherently, RNN-LMs need\na lot of computation for training and decoding compared with con-\nventional N-gram LMs. In addition, text corpora for training LMs\nare usually much larger than paired acoustic and text data for train-\ning end-to-end ASR models. Considering this situation, solving the\nabove problem is crucial for better end-to-end ASR.\nIn this paper, we propose a novel strategy for language modeling\nand decoding in end-to-end ASR to solve the problem. The proposed\nmethod allows us to decode with only a word-based RNN-LM in\naddition to the encoder decoder, leading a competitive accuracy and\nless computation in the decoding process compared to the multi-level\nLM approach. This method employs look-ahead word probabilities\nto predict next characters instead of the character-based LM. Al-\nthough our approach is similar to old fashioned lexical-tree search al-\ngorithms including language model look-ahead [12, 13], it provides\nan efﬁcient way of dynamically computing the look-ahead probabil-\nities for end-to-end ASR with a word-based RNN-LM, which does\nnot exist in the prior work. We demonstrate the efﬁcacy of the pro-\nposed LMs on standard Wall Street Journal (WSJ) and LibriSpeech\ntasks.\n2. END-TO-END ASR ARCHITECTURE\nThis section explains the hybrid CTC/attention network [6, 7] we\nused for evaluating the proposed language modeling and decoding\napproach. But the proposed LMs can also be applied to standard\nattention-based encoder decoders for ASR.\n2.1. Network architecture\nFigure 1 shows the latest architecture of the CTC/attention network\n[9]. The encoder has deep convolutional neural network (CNN)\nlayers with the VGG net architecture [14], which are followed by\nstacked bidirectional long short-term memory (BLSTM) layers. The\ndecoder network has a CTC network, an attention decoder network,\nand an RNN-LM, which jointly predict the next label. Given input\nsequence X = x1,...,x T, the encoder network accepts Xand out-\nputs hidden vector sequenceH = h1,..., hT′, where T′= T/4 by\nusing two max-pooling steps in the deep CNN. The decoder network\niteratively predicts a single label cl based on the hidden vectors H\nand the label context c1,...,c l−1, and generates L-length label se-\nquence C = {cl ∈U|l = 1,··· ,L}, where Uis a set of labels. In\nthis work, we assume Uis a set of distinct characters or alphabet of\nthe target language.\nThe hybrid attention/CTC network utilizes both beneﬁts of CTC\nand attention during training and decoding by sharing the same\nCNN/ BLSTM encoder with CTC and attention decoder networks\nand training them jointly. Unlike the solitary attention model, the\nforward-backward algorithm of CTC can enforce monotonic align-\nment between speech and label sequences during training. That\nis, rather than solely depending on the data-driven attention mech-\nanism to estimate the desired alignments in long sequences, the\nforward-backward algorithm in CTC helps to speed up the process\nof estimating the desired alignment. The objective to be maximized\nis a logarithmic linear combination of the CTC and attention-based\nposterior probabilities pctc(C|X) and patt(C|X):\nLMTL = λlog pctc(C|X) + (1−λ) logpatt(C|X), (1)\nwith a tunable parameter λ: 0≤λ≤1.\n2.2. Decoding with external language models\nThe inference step of CTC/attention-based speech recognition is per-\nformed by output-label synchronous decoding with a beam search.\nAlthough the decoding algorithm is basically the same as the method\nfor standard attention-based encoder decoders, it also considers the\nCTC and LM probabilities to ﬁnd a better hypothesis. The decoder\nﬁnds the most probable character sequence ˆCgiven speech input X,\naccording to\nˆC = arg max\nC∈U∗{λlog pctc(C|X) + (1−λ) logpatt(C|X)\n+γlog plm(C)}, (2)\nwhere LM probability plm(C) is added with scaling factor γ to the\nCTC/attention probability in the log probability domain.\nIn the beam search process, the decoder computes a score of\neach partial hypothesis, which is deﬁned as the log probability of\nthe hypothesized character sequence. The joint score α(g) of each\npartial hypothesis his computed by\nα(h) =λαctc(h) + (1−λ)αatt(h) +γαlm(h), (3)\nwhere αctc(h), αatt(h), and αlm(h) are CTC, attention, and LM\nscores, respectively.\nWith the attention model, score αatt(h) can be obtained recur-\nsively as\nαatt(h) =αatt(g) + logpatt(c|g,X), (4)\nwhere g is an existing partial hypothesis, and cis a character label\nappended to gto generate h, i.e., h = g·c. The score for his ob-\ntained as the addition of the original scoreαatt(g) and the conditional\nlog probability given by the attention decoder. LM score αlm(h) is\nalso obtained similarly to the attention model as\nαlm(h) =αlm(g) + logplm(c|g). (5)\nOn the other hand, CTC score αctc(h) is obtained differently\nfrom the other scores, where we compute the CTC preﬁx probability\n[15] deﬁned as the cumulative probability of all label sequences that\nhave has their preﬁx:\np(h,... |X) =\n∑\nν∈(U∪{<eos>})+\nP(h·ν|X), (6)\nand use it as the CTC score:\nαctc(h) ≜ log p(h,... |X), (7)\nwhere ν represents all possible label sequences except the empty\nstring, and <eos> indicates the end of sentence.\nDuring the beam search, the number of partial hypotheses for\neach length is limited to a predeﬁned number, called a beam width,\nto exclude hypotheses with relatively low scores, which dramatically\nimproves the search efﬁciency.\n3. INCORPORATING WORD-BASED RNN-LMS\nIn this section, we explain the basic approach to incorporate word-\nbased LMs into a character-based end-to-end ASR, and present two\nword-based RNN-LMs, one is a multi-level LM we have already\nproposed and the other is a look-ahead word LM we propose in this\npaper.\n3.1. Basic approach\nIn most end-to-end ASR systems, a ﬁnite lexicon and an N-gram\nlanguage model are compiled into a Weighted Finite-State Trans-\nducer (WFST), and used for decoding [16, 17]. The WFST frame-\nwork efﬁciently handles frame-synchronous or label-synchronous\ndecoding with the optimized search network and reduces the word\nerror rate [18, 19]. However, this approach is not suitable for RNN-\nLMs because an RNN-LM cannot be represented as a static state\nnetwork.\nIn this paper, we extend the character-based decoding to enable\nopen-vocabulary end-to-end ASR with a word-based RNN-LM. We\nconsider that the character-based systems can predict space charac-\nters between words as well as letters within the word. Note that the\nspace character has an actual character code, which is different from\nthe CTC’s blank symbol. With the space characters, it is possible\nto deterministically map any character sequence to a word sequence,\ne.g., character sequence\na <space> c a t <space> e a t s\nis mapped to a unique word sequence\na cat eats\nwhere <space> formally represents the space character. Accord-\ningly, only when the decoder hypothesizes a space character, it com-\nputes the probability of the last word using the word-level RNN-LM\nand simply accumulates it to the hypothesis score. No special treat-\nment is necessary for different types of homonyms: words with the\nsame spelling but different pronunciation are handled in a context-\ndependent way by the word language model, whereas words with the\nsame pronunciation but different spellings are automatically handled\nas different word hypotheses in the beam search. Similarly, ambigu-\nous word segmentations are automatically handled as different de-\ncoding hypotheses.\n3.2. Multi-level RNN-LM\nThe multi-level RNN-LM contains character-level and word-level\nRNN-LMs, but it can be implemented as a function that performs\ncharacter-level prediction. Let Vbe the vocabulary of the word-level\nRNN-LM and be including an abstract symbol of OOV word such as\n<UNK>. We compute the conditional character probabilities with\nplm(c|g) =\n\n\n\npwlm(wg|ψg)\npclm(wg|ψg) if c∈S,wg ∈V\npwlm(<UNK>|ψg) ˜β if c∈S,wg ̸∈V\npclm(c|g) otherwise\n(8)\nwhere S denotes a set of labels that indicate the end of word, i.e.,\nS = {<space>,<eos>}, wg is the last word of the character se-\nquence g, and ψg is the word-level history, which is the word se-\nquence corresponding to gexcluding wg. For the above example, g,\nwg, and ψg are set as\ng= a,<space>,c,a,t,<space>,e,a,t,s\nwg = eats\nψg = a,cat.\n˜βis a scaling factor used to adjust the probabilities for OOV words.\nThe ﬁrst condition on the right-hand side of Eq. (8) is applied\nwhen the ghas reached the end of a word. In this case, the word-level\nprobability pwlm(wg|ψg) is computed using the word-level RNN-\nLM. The denominator pclm(wg|ψg) is the probability of wg obtained\nby the character-level RNN-LM and used to cancel the character-\nlevel LM probabilities accumulated for wg. The probability can be\ncomputed as\npclm(wg|ψg) =\n|wg|∏\ni=1\npclm(wg,i|ψgwg,1 ···wg,i−1), (9)\nwhere |wg|is the length of word wg in characters and wg,i indicates\nthe i-th character ofwg. The second term, pwlm(<UNK>|ψg) acts as a\nweight on the character-level LM and ensures that the combined lan-\nguage model is normalized over character sequences both at word\nboundaries and in-between. If wg is an OOV word as in the sec-\nond condition, we assume that a word-level probability for the OOV\nword can be computed with the word and character-level RNN-LMs\nas\npoov(wg|ψg) =pwlm(<UNK>|ψg)pclm(wg|<UNK>,ψg). (10)\nSince the character-level probability satisﬁes\npclm(wg|<UNK>,ψg) ∝pclm(wg|ψg) (11)\nand\npclm(wg|<UNK>,ψg) =β(ψg) pclm(wg|ψg), (12)\nwe approximate β(ψg) ≈˜βand use ˜βas a tunable parameter. In the\nsecond condition of Eq. (8), character-based probabilitypclm(wg|ψg)\nis eliminated since it is already accumulated for the hypothesis.\nThe third case gives the character-level LM probabilities to the\nhypotheses within a word. Although the character-level LM proba-\nbilities are canceled at the end of every known word hypothesis and\nso are only used to score OOV words, they serve another impor-\ntant role in keeping the correct word hypotheses active in the beam\nsearch until the end of the word where the word-level LM probability\nis applied.\n<UNK> \nA \nAN \nAND   \nANNA \nAT \nBIT \nBY \n \n \n \n \nZIP \nZOO \n: \n: \n: \n(a) Vocabulary with word Ids \nA \nB \nZ \nN \nT \nI \nY \nT \nD \nN \n A \nO O \nI P \nRoot \nnode \n(b) Prefix tree \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n \n \n \n \n999 \n1000 \nFig. 2. Preﬁx tree representation of a vocabulary. (a) vocabulary\nincluding word strings and id numbers. (b) preﬁx tree of the vocab-\nulary, where the shaded circle indicates the root node, each white\ncircle is a node representing a character and each path from the\nroot node represents a character sequence of each word, where each\ndouble-circle node corresponds to a word end.\nFinally, the log probability of sentence-end label <eos> is\nadded to the log probability of each complete hypothesis g′as\nα(g′) =α(g) +γlog pwlm(<eos>|ψgwg) (13)\nin the beam search process.\n3.3. Look-ahead word-based RNN-LM\nThe look-ahead word-based RNN-LM enables us to decode with\nonly a word-based RNN-LM in addition to the encoder decoder. This\nmodel predicts next characters using a look-ahead mechanism over\nthe word probabilities given by the word-based LM, while the multi-\nlevel LM uses a character-level LM until the identity of the word is\ndetermined.\nTo compute look-ahead probabilities efﬁciently, we use a preﬁx\ntree representation as shown in Fig. 2. This example shows a vo-\ncabulary and its preﬁx tree representation. During decoding, each\nhypothesis holds a link to a node, which indicates where the hypoth-\nesis is arriving in the tree. Suppose a set of anticipated words at each\nnode has already been obtained in advance. A look-ahead probabil-\nity at node ncan be computed as the sum of the word probabilities\nof all the anticipated words as\npla(n|ψ) =\n∑\nw∈wset(n)\npwlm(w|ψ), (14)\nwhere wset(n) denotes the set of anticipated words at node n, and\npwlm(w|ψ) is the original word probability given by the underlying\nword-based RNN-LM for word-level context ψ.\nThe character-based LM probability with the look-ahead mech-\nanism is computed as\nplm(c|g) =\n\n\n\npwlm(wg|ψg)/pla(ng|ψg) if ng ∈F,c ∈S\npla(ng·c|ψg)/pla(ng|ψg) if ng ̸= null,c ∈ξ(ng)\npwlm(<UNK>|ψg)η if ng ̸= null,c ̸∈ξ(ng)\n1 otherwise\n(15)\nwhere F denotes a set of word end nodes, ng is the node that ghas\narrived, ng·c is a succeeding node of ng determined by c, ξ(ng) is a\nset of succeeding nodes from ng, and ηis a scaling factor for OOV\nword probabilities, which is a tunable parameter.\nThe ﬁrst case of Eq. (15) gives the word probability at a word-\nend node, where pwlm(wg|ψg) needs to be normalized bypla(ng|ψg)\nto cancel the already accumulated look-ahead probabilities. The sec-\nond case computes the look-ahead probability when making a tran-\nsition from node ng to ng·c. The third case gives the OOV word\nprobability, where character cis not accepted, which means the hy-\npothesis is going to an OOV word. The last one handles the case\nthat ng is null, which means that the hypothesis is already out of the\ntree, and it returns 1 since the OOV probability was already applied\nin the third case. In the above procedure, we assume that whenever\nthe hypothesis is extended by <space> label, the new hypothesis\npoints the root node of the tree.\nAlthough this approach is similar to conventional ASR systems\nbased on a preﬁx tree search including a LM look-ahead mechanism,\nthe look-ahead probabilities needs to be computed on the ﬂy using\nthe word-based RNN-LM unlike conventional approaches with uni-\ngram or bigram LM or weight pushing over a static WFST.\nTo compute the sum of probabilities in Eq. (14), we assume that\nthe Id numbers are assigned in alphabetical order in the vocabulary.\nIn this case, the Id numbers should be consecutive in each set as a\nproperty of preﬁx tree. Accordingly, we can compute the sum using\nthe cumulative sums over the word probability distribution by\npla(n|ψ) =sψ[max id(n)] −sψ[min id(n) −1], (16)\nwhere sψ[·] denotes an array of the cumulative sums given context\nψ, which is obtained as\nsψ[i] =\ni∑\nk=0\npwlm(w(k)|ψ) for i= 0,..., |V|, (17)\nand max id(n) and min id(n) are the maximum and minimum Id\nnumbers in the set of anticipated words at node n. w(k) denotes the\nk-th word in the vocabulary. Once the cumulative sums are com-\nputed right after the softmax operation, we can quickly compute the\nlook-ahead probabilities.\n4. RELATED WORK\nThere are some prior work, that incorporates word units into end-\nto-end ASR. One major approach is acoustic-to-word CTC [20, 21,\n22], where the input acoustic feature sequence is directly mapped to\nthe word sequence using CTC. However, this approach essentially\nrequires a large amount of paired acoustic and text data to learn\nacoustic mapping to a large number of words. For example, [20]\nused 125,000 hours of transcribed audio data to train the word CTC\nmodel.\nOur approach, in contrast, is specially designed for end-to-end\nASR using a character or subword-based encoder decoder and an\nexternal RNN language model trained with a large text corpus. Thus,\nthis architecture is more suitable for low-resource languages, where\nthe amount of parallel data is limited but large text data are available.\nSubword units [23, 24] are also available as an intermediate rep-\nresentation of character and word, where the unit set is automatically\nobtained by byte-pair encoding [25] or some chunking techniques.\nHowever, this approach needs to select an appropriate number of\nsubword units using training data. Increasing the number of units\nwill lead more acoustically expressive units but make it more difﬁ-\ncult to train the encoder decoder using a limited amount of data. In\naddition, it assumes to use the subword units for both the encoder\ndecoder and the language model, but the appropriate units can be\ndifferent for the encoder decoder and the language model. Our ap-\nproach basically employs characters and words, but it is also possible\nto combine a character-based encoder decoder with a subword-based\nLM or a subword-based encoder decoder with a word-based LM.\n5. EXPERIMENTS\nWe evaluate the proposed language models with the Wall Street Jour-\nnal (WSJ) and LibriSpeech corpora. WSJ is a well-known English\nclean speech database [26, 27] including approximately 80 hours\ndata while LibriSpeech is a larger data set of read speech from au-\ndiobooks, which contains 1000 hours of audios and transcriptions\n[28].\n5.1. Evaluation with WSJ\nWe used the si284 data set for training, the dev93 data set for val-\nidation, and the eval92 data set for evaluation. The data sets are\nsummarized in Table 1.\nTable 1. WSJ data sets used for evaluation\n# utterances Length (h)\nTraining (WSJ1 si284) 37,416 80\nValidation (dev93) 503 1.1\nEvaluation (eval92) 333 0.7\nAs input features, we used 80 mel-scale ﬁlterbank coefﬁcients\nwith pitch features and their delta and delta delta features for the\nCNN/BLSTM encoder [29]. Our encoder network is boosted by\nusing deep CNN, which is discussed in Section 2.1. We used a 6-\nlayer CNN architecture based on the initial layers of the VGG-like\nnetwork [14] followed by eight BLSTM layers in the encoder net-\nwork. In the CNN architecture, the initial three input channels are\ncomposed of the spectral features, delta, and delta delta features. In-\nput speech feature images are downsampled to (1/4 ×1/4) images\nalong with the time-frequency axes through the two max-pooling\nlayers. The BLSTM layers had 320 cells in each layer and direction,\nand the linear projection layer with 320 units is followed by each\nBLSTM layer.\nWe used the location-based attention mechanism [5], where the\n10 centered convolution ﬁlters of width 100 were used to extract the\nconvolutional features. The decoder was a one-layer unidirectional\nLSTM with 300 cells. We used only 32 distinct labels: 26 English\nletters, apostrophe, period, dash, space, noise, and sos/eos tokens.\nThe AdaDelta algorithm [30] with gradient clipping [31] was\nused for the optimization. We also applied a unigram label smooth-\ning technique [32] to avoid over-conﬁdence predictions. In the hy-\nbrid attention/CTC architecture, we used the λ = 0.1 for training\nand the λ = 0.2 for decoding. The beam width was set to 30 in\ndecoding under all conditions. The joint CTC-attention ASR was\nimplemented by using the Chainer deep learning toolkit [33].\nCharacter and word-based RNN-LMs were trained with the WSJ\ntext corpus, which consisted of 37M words from 1.6M sentences.\nThe character-based LM had a single LSTM layer with 800 cells\nand a 32-dimensional softmax layer while the word-based LM had\na single LSTM layer with 1000 cells. We trained word-based RNN-\nLMs for 20K, 40K and 65K vocabularies, where the softmax layer\nhad 20K, 40K or 65K-dimensional output in each LM. We used the\nstochastic gradient descent (SGD) to optimize the RNN-LMs.\nTable 2. Word error rate (WER) with different language models on\nWSJ.\nLanguage models (vocab. size) dev93 eval92\nNo LM 17.3 13.4\nChararacter LM 12.3 7.7\nWord LM (20K) 17.1 12.6\nMulti-level LM (20K) 9.6 5.6\nLook-ahead LM (20K) 9.5 6.1\nMulti-level LM (40K) 9.3 5.3\nLook-ahead LM (40K) 8.6 5.3\nMulti-level LM (65K) 9.0 5.4\nLook-ahead LM (65K) 8.4 5.1\n0.0\t\r  \n0.2\t\r  \n0.4\t\r  \n0.6\t\r  \n0.8\t\r  \n1.0\t\r  \n1.2\t\r  \n1.4\t\r  \n1.6\t\r  \n1.8\t\r  \n2.0\t\r  \nNo\t\r  LM\t\r  \nCharacter\t\r  LM\t\r  Mul6-­‐level\t\r  20K\t\r  Look-­‐ahead\t\r  20K\t\r  Mul6-­‐level\t\r  40K\t\r  Look-­‐ahead\t\r  40K\t\r  Mul6-­‐level\t\r  65K\t\r  Look-­‐ahead\t\r  65K\t\r  \nDecoding\t\r  *me\t\r  ra*o\t\r  \nWord\t\r  LM\t\r  \nChararacter\t\r  LM\t\r  \nEncoder\t\r  decoder\t\r  \nFig. 3. Decoding time ratio in the beam search using different lan-\nguage models. Every elapsed time was divided by that of the No LM\ncase.\nThe ﬁrst experiment evaluates the contributions of language\nmodels. Table 2 shows word error rate (WER) with different lan-\nguage models. The WERs for no language model (No LM), char-\nacter LM, word LM (20K) and multi-level LM (20K) were already\nreported in [11], where the multi-level LM (20K) performed the\nbest in our prior work. When we simply applied the word-based\nLM without any character-level LMs or look-ahead mechanism, the\nWER reduction was very small due to the pruning errors discussed\nin Introduction.\nAfter that, we conducted experiments with the look-ahead LM\n(20K), where the WER for eval92 test set increased from 5.6 % to\n6.1 %. We analyzed the recognition results, and found that the in-\ncreased errors mainly came from OOV words. This could be be-\ncause the look-ahead LM did not use a strong character-based LM\nfor predicting OOV words. To mitigate this problem, we increased\nthe vocabulary size to 40K and 65K. Then, we obtained a large im-\nprovement reaching 5.1 % WER for the look-ahead LM. The rea-\nson why the 65K-look-ahead LM achieved lower WERs than those\nof multi-level LMs is probably that the look-ahead mechanism pro-\nvided better character-level LM scores consistent with the word LM\nprobabilities, which were helpful in the beam search process.\nNext, we investigated the decoding time when using different\nlanguage models. Figure 3 shows the decoding time ratio, where\neach decoding time was normalized by the pure end-to-end decod-\ning time without language models, i.e., the case of No LM, where\nwe only used a single CPU for each decoding process1. The charac-\nter LM and 20K-word multi-level LM increased the decoding time\n1Since the beam search-based decoding was implemented in Python, the\ndecoding speed has not been optimized sufﬁciently.\nTable 3. Comparison with other end-to-end ASR systems reported\non WSJ.\nEnd-to-end ASR systems dev93 eval92\nseq2seq [17] - 9.3\nCTC [3] - 8.2\nCTC [16] - 7.3\nseq2seq [32] 9.7 6.7\nMulti-level LM (20K) [11] 9.6 5.6\nLook-ahead LM (65K) [this work] 8.4 5.1\nTable 4. LibriSpeech data sets used for evaluation\n# utterances Length (h)\nTrain set 281,231 960\ndev clean 2,703 5.3\ndev other 2,864 5.1\ntest clean 2,620 5.4\ntest other 2,939 5.3\nby 40% and 65%, respectively, while the 20K-word look-ahead LM\nincreased it by 38%. Even when we used the 65K-word LMs, the\ndecoding time for the look-ahead LM was still 55%, which was less\nthan that of the 20K multi-level LM. Thus, the proposed look-ahead\nLM has a higher accuracy to multi-level LMs with less decoding\ntime.\nFinally, we compare our result with other end-to-end systems\nreported on the WSJ task. Table 3 summarizes the WER numbers\nobtained from other articles and this work. Since the systems in\nthe table have different network architectures from each other, it is\ndifﬁcult to compare these numbers directly. However, we conﬁrmed\nthat our system has achieved the best WER in the state-of-the-art\nsystems on the WSJ benchmark.\n5.2. Evaluation with LibriSpeech\nWe conducted additional experiments using LibriSpeech to examine\nthe performance of RNN-LMs including character LM, multi-level\nLM and look-ahead word LM for a larger corpus. The data sets are\nsummarized in Table 4. All the experiments for LibriSpeech were\nperformed using ESPnet, the End-to-End Speech Processing Toolkit\n[34], and the recipe for a baseline LibriSpeech setup with PyTorch\nbackend [35]. According to the baseline recipe, we trained an 8-layer\nBLSTM encoder including 320 cells in each layer and direction, and\nthe linear projection layer with 320 units followed by each BLSTM\nlayer. The second and third bottom LSTM layers of the encoder\nread every second state feature in the network below, reducing the\nutterance length by a factor of four, i.e.,T/4. We also used location-\nbased attention with a similar setting to the WSJ model. The decoder\nwas a one-layer unidirectional LSTM with 300 cells. We also trained\ndifferent language models as prepared for WSJ task, where we used\nonly transcription of audio data including 9.4M words. The both\ncharacter and word RNN-LMs had 2 LSTM layers and 650 cells per\nlayer. The beam width was set to 20 for decoding.\nTable 5 shows word error rate (WER) with different language\nmodels. We obtained consistent error reduction with WSJ’s results\nin Table 2, where the both multi-level and look-ahead LMs provided\nsigniﬁcant error reduction when the vocabulary size was increased\nto 65K. In this case, the look-ahead LM had competitive WERs to\nthe multi-level LM. However, the look-ahead LM still has the speed\nbeneﬁt similar to the results in Fig. 3 and the other beneﬁt that we\ncan completely exclude the training process of the character LM.\nTable 5. Word error rate (WER) with different language models on\nLibriSpeech\nLanguage models (vocab. size) dev dev test test\nclean other clean other\nNo LM 7.7 21.1 7.7 21.9\nCharacter LM 6.6 18.3 6.6 19.1\nMulti-level LM (20K) 5.7 16.0 5.9 16.8\nLook-ahead LM (20K) 6.3 16.6 6.4 17.4\nMulti-level LM (40K) 5.4 15.6 5.5 16.5\nLook-ahead LM (40K) 5.6 15.8 5.7 16.7\nMulti-level LM (65K) 5.4 15.6 5.5 16.6\nLook-ahead LM (65K) 5.4 15.6 5.5 16.5\n6. CONCLUSION\nIn this paper, we proposed a word-based RNN language model\n(RNN-LM) including a look-ahead mechanism for end-to-end au-\ntomatic speech recognition (ASR). In our prior work, we com-\nbined character-based and word-based language models in hybrid\nCTC/attention-based encoder decoder architecture. Although the\nLM with both the character and word levels achieves signiﬁcant\nerror reduction, two different LMs need to be trained and used\nfor decoding, which increase the computational cost and mem-\nory usage. The proposed method allows us to decode with only a\nword-based RNN-LM, which leads competitive accuracy and less\ncomputation in the beam search process compared to the multi-level\nLM approach. Furthermore, it can completely exclude the training\nprocess for the character-level LM. We have shown the efﬁcacy\nof the proposed method on standard Wall Street Journal (WSJ) and\nLibriSpeech tasks in terms of computational cost and recognition ac-\ncuracy. Finally, we demonstrated that the proposed method achieved\n5.1 %WER for WSJ Eval’92 test set when the vocabulary size was\nincreased, which is the best WER reported for end-to-end ASR\nsystems on this benchmark.\n7. REFERENCES\n[1] Tara N. Sainath, Oriol Vinyals, Andrew Senior, and Hasim Sak,\n“Convolutional, Long Short-Term Memory, Fully Connected\nDeep Neural Networks,” in IEEE International Conference on\nAcoustics, Speech and Signal Processing, 2015.\n[2] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-\nget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr\nMotlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg\nStemmer, and Karel Vesely, “The Kaldi speech recognition\ntoolkit,” in IEEE Workshop on Automatic Speech Recognition\nand Understanding (ASRU), Dec. 2011.\n[3] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech\nrecognition with recurrent neural networks,” in International\nConference on Machine Learning (ICML), 2014, pp. 1764–\n1772.\n[4] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton,\n“Speech recognition with deep recurrent neural networks,” in\nAcoustics, speech and signal processing (icassp), 2013 ieee in-\nternational conference on. IEEE, 2013, pp. 6645–6649.\n[5] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio, “Attention-based mod-\nels for speech recognition,” inAdvances in Neural Information\nProcessing Systems (NIPS), 2015, pp. 577–585.\n[6] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint CTC-\nattention based end-to-end speech recognition using multi-\ntask learning,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), 2017, pp. 4835–\n4839.\n[7] Takaaki Hori, Shinji Watanabe, and John R. Hershey, “Joint\nCTC/attention decoding for end-to-end speech recognition,” in\nProceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (ACL): Human Language Tech-\nnologies: long papers, 2017.\n[8] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho,\nLoic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio, “On using monolingual corpora in neural\nmachine translation,” arXiv preprint arXiv:1503.03535, 2015.\n[9] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan,\n“Advances in joint CTC-attention based end-to-end speech\nrecognition with a deep CNN encoder and RNN-LM,” in IN-\nTERSPEECH, 2017.\n[10] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath,\nZhifeng Chen, and Rohit Prabhavalkar, “An analysis of in-\ncorporating an external language model into a sequence-to-\nsequence model,” arXiv preprint arXiv:1712.01996, 2017.\n[11] Takaaki Hori, Shinji Watanabe, and John R Hershey, “Multi-\nlevel language modeling and decoding for open vocabulary\nend-to-end speech recognition,” inAutomatic Speech Recogni-\ntion and Understanding Workshop (ASRU), 2017 IEEE. IEEE,\n2017, pp. 287–293.\n[12] Stefan Ortmanns, Hermann Ney, and Andreas Eiden,\n“Language-model look-ahead for large vocabulary speech\nrecognition,” in Spoken Language, 1996. ICSLP 96. Proceed-\nings., Fourth International Conference on. IEEE, 1996, vol. 4,\npp. 2095–2098.\n[13] Fil Alleva, Xuedong Huang, and Mei-Yuh Hwang, “Improve-\nments on the pronunciation preﬁx tree search organization,”\nin Acoustics, Speech, and Signal Processing, 1996. ICASSP-\n96. Conference Proceedings., 1996 IEEE International Con-\nference on. IEEE, 1996, vol. 1, pp. 133–136.\n[14] Karen Simonyan and Andrew Zisserman, “Very deep convo-\nlutional networks for large-scale image recognition,” arXiv\npreprint arXiv:1409.1556, 2014.\n[15] Alex Graves, “Supervised sequence labelling with recur-\nrent neural networks,” PhD thesis, Technische Universit¨at\nM¨unchen, 2008.\n[16] Yajie Miao, Mohammad Gowayyed, and Florian Metze,\n“EESEN: End-to-end speech recognition using deep RNN\nmodels and WFST-based decoding,” inIEEE Workshop on Au-\ntomatic Speech Recognition and Understanding (ASRU), 2015,\npp. 167–174.\n[17] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Phile-\nmon Brakel, and Yoshua Bengio, “End-to-end attention-\nbased large vocabulary speech recognition,” in IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2016, pp. 4945–4949.\n[18] Mehryar Mohri, Fernando Pereira, and Michael Riley,\n“Weighted ﬁnite-state transducers in speech recognition,”\nComputer Speech & Language, vol. 16, no. 1, pp. 69–88, 2002.\n[19] Takaaki Hori and Atsushi Nakamura, “Speech recognition al-\ngorithms using weighted ﬁnite-state transducers,” Synthesis\nLectures on Speech and Audio Processing, vol. 9, no. 1, pp.\n1–162, 2013.\n[20] Hagen Soltau, Hank Liao, and Hasim Sak, “Neural speech\nrecognizer: Acoustic-to-word lstm model for large vocabulary\nspeech recognition,” arXiv preprint arXiv:1610.09975, 2016.\n[21] Kartik Audhkhasi, Bhuvana Ramabhadran, George Saon,\nMichael Picheny, and David Nahamoo, “Direct acoustics-to-\nword models for english conversational speech recognition,”\narXiv preprint arXiv:1703.07754, 2017.\n[22] Jinyu Li, Guoli Ye, Rui Zhao, Jasha Droppo, and Yifan\nGong, “Acoustic-to-word model without oov,” in Automatic\nSpeech Recognition and Understanding Workshop (ASRU),\n2017 IEEE. IEEE, 2017, pp. 111–117.\n[23] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mo-\nhammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan\nCao, Qin Gao, Klaus Macherey, et al., “Google’s neural ma-\nchine translation system: Bridging the gap between human and\nmachine translation,” arXiv preprint arXiv:1609.08144, 2016.\n[24] Kanishka Rao, Has ¸im Sak, and Rohit Prabhavalkar, “Ex-\nploring architectures, data and units for streaming end-to-\nend speech recognition with rnn-transducer,” in Automatic\nSpeech Recognition and Understanding Workshop (ASRU),\n2017 IEEE. IEEE, 2017, pp. 193–199.\n[25] Philip Gage, “A new algorithm for data compression,” The C\nUsers Journal, vol. 12, no. 2, pp. 23–38, 1994.\n[26] Linguistic Data Consortium, “CSR-II (wsj1) complete,” Lin-\nguistic Data Consortium, Philadelphia , vol. LDC94S13A,\n1994.\n[27] John Garofalo, David Graff, Doug Paul, and David Pal-\nlett, “CSR-I (wsj0) complete,” Linguistic Data Consortium,\nPhiladelphia, vol. LDC93S6A, 2007.\n[28] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev\nKhudanpur, “Librispeech: an asr corpus based on public do-\nmain audio books,” in Acoustics, Speech and Signal Process-\ning (ICASSP), 2015 IEEE International Conference on. IEEE,\n2015, pp. 5206–5210.\n[29] Yu Zhang, William Chan, and Navdeep Jaitly, “Very deep\nconvolutional networks for end-to-end speech recognition,” in\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing, 2017.\n[30] Matthew D Zeiler, “Adadelta: an adaptive learning rate\nmethod,” arXiv preprint arXiv:1212.5701, 2012.\n[31] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, “On the\ndifﬁculty of training recurrent neural networks,”arXiv preprint\narXiv:1211.5063, 2012.\n[32] Jan Chorowski and Navdeep Jaitly, “Towards better decoding\nand language model integration in sequence to sequence mod-\nels,” arXiv preprint arXiv:1612.02695, 2016.\n[33] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton,\n“Chainer: a next-generation open source framework for deep\nlearning,” in Proceedings of Workshop on Machine Learning\nSystems (LearningSys) in NIPS, 2015.\n[34] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki\nHayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta So-\nplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al.,\n“Espnet: End-to-end speech processing toolkit,”arXiv preprint\narXiv:1804.00015, 2018.\n[35] Nikhil Ketkar, “Introduction to pytorch,” in Deep Learning\nwith Python, pp. 195–208. Springer, 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7633821964263916
    },
    {
      "name": "Speech recognition",
      "score": 0.6878861784934998
    },
    {
      "name": "Language model",
      "score": 0.6865348815917969
    },
    {
      "name": "Word (group theory)",
      "score": 0.6785393953323364
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6157965660095215
    },
    {
      "name": "Vocabulary",
      "score": 0.5779048204421997
    },
    {
      "name": "Decoding methods",
      "score": 0.5224363207817078
    },
    {
      "name": "Word error rate",
      "score": 0.519679605960846
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4815090596675873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.435366690158844
    },
    {
      "name": "Task (project management)",
      "score": 0.4301976263523102
    },
    {
      "name": "Algorithm",
      "score": 0.14274656772613525
    },
    {
      "name": "Artificial neural network",
      "score": 0.10953933000564575
    },
    {
      "name": "Mathematics",
      "score": 0.09349417686462402
    },
    {
      "name": "Linguistics",
      "score": 0.07758146524429321
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": []
}