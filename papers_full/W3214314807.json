{
    "title": "Probing Pre-trained Language Models for Semantic Attributes and their Values",
    "url": "https://openalex.org/W3214314807",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5051998921",
            "name": "Meriem Beloucif",
            "affiliations": [
                "Universität Hamburg"
            ]
        },
        {
            "id": "https://openalex.org/A5021287757",
            "name": "Chris Biemann",
            "affiliations": [
                "Universität Hamburg"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2998696444",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2964242047",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W3034685497",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W4213168938",
        "https://openalex.org/W2963906317",
        "https://openalex.org/W3099668342",
        "https://openalex.org/W2103095311",
        "https://openalex.org/W2115792525",
        "https://openalex.org/W2004763266",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W2250817354",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2605717780"
    ],
    "abstract": "Pretrained Language Models (PTLMs) yield state-of-the-art performance on many Natural Language Processing tasks, including syntax, semantics and commonsense reasoning. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g. the relation between rich and high net worth. We use PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLMs encode semantic attributes along with their values. Such inferences based on semantics are intuitive for us humans as part of our language understanding. Since PTLMs are trained on large amounts of Wikipedia data, we would assume that they can generate similar predictions. However, our findings reveal that PTLMs perform still much worse than humans on this task. We show an analysis which explains how to exploit our methodology to integrate better context and semantics into PTLMs using knowledge bases.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2554–2559\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2554\nProbing Pretrained Language Models for\nSemantic Attributes and their Values\nMeriem Beloucif and Chris Biemann\nLanguage Technology Group, Dept. of Informatics, MIN Faculty, Universität Hamburg\n{beloucif, biemann}@informatik.uni-hamburg.de\nAbstract\nPretrained Language Models (PTLMs) yield\nstate-of-the-art performance on many Natural\nLanguage Processing tasks, including syntax,\nsemantics and commonsense reasoning. In\nthis paper, we focus on identifying to what\nextent do PTLMs capture semantic attributes\nand their values, e.g. the relation between rich\nand high net worth. We use PTLMs to predict\nmasked tokens using patterns and lists of items\nfrom Wikidata in order to verify how likely\nPTLMs encode semantic attributes along with\ntheir values. Such inferences based on seman-\ntics are intuitive for us humans as part of our\nlanguage understanding. Since PTLMs are\ntrained on large amount of Wikipedia data, we\nwould assume that they can generate similar\npredictions. However, our ﬁndings reveal that\nPTLMs perform still much worse than humans\non this task. We show an analysis which ex-\nplains how to exploit our methodology to inte-\ngrate better context and semantics into PTLMs\nusing knowledge bases.\n1 Introduction\nGiven the ability of pretrained language models\n(PTLMs), such as BERT (Devlin et al., 2019), to\ncreate useful text representations, they have be-\ncome the standard choice when building NLP ap-\nplications (Peters et al., 2018a; Devlin et al., 2019;\nRadford and Narasimhan, 2018). However, there\nhas recently been a rising amount of research that\nuses probes to understand the level of linguistics\nPTLMs encode. Different probing experiments\nhave been proposed to study the drawbacks of\nPTLMs in areas such as the biomedical domain (Jin\net al., 2019), syntax (Hewitt and Manning, 2019),\nsemantic and syntactic sentence structures (Ten-\nney et al., 2019; Peters et al., 2018b), prenomial\nanaphora (Sorodoc et al., 2020), linguistics (Be-\nlinkov et al., 2017; Clark et al., 2020; Tenney et al.,\n2019) and commonsense knowledge (Petroni et al.,\n2019; Davison et al., 2019; Talmor et al., 2020).\nFigure 1: In FrameNet, adjectives are lexical units that\nevoke other frames: “rich” is a lexical unit that evokes\nthe frame wealthiness, while “old\" evokes age. The re-\nlation between rich and high net worth could be deﬁned\nas a value-attribute pair, where rich is the value of an\nexpression that represents an attribute: wealthiness/net\nworth. LU stands for lexical unit, FN1_Sent represents\nand Finished_Initial refer to which FrameNet version\nthe lexical unit is from.\nIn this paper, we expand on this line of research\nby probing PTLMs to investigate if they cover se-\nmantic attributes and their values. The closest work\nto ours has been proposed by Ribeiro et al. (2020),\nwhere they investigate if PTLMs capture check-\nlists such as: red, green and yellow. In contrast to\nthem, we focus on ﬁnding out whether pretrained\nlanguage models capture the correlation between\nsemantic attributes and their values.\nAn example of semantic attributes and their val-\nues is the relation that exists between old, age and\ndate of birth, or the relation between rich, wealth\nand net worth. Looking up rich on FrameNet (Fill-\nmore, 1982; Baker et al., 1998) would not result in a\nframe by itself, but would evoke the semantic frame\nwealthiness (Figure 1). In WordNet (Fellbaum,\n1998) these associations are called an attribute-\nvalue relation, where the attribute is a noun for\nwhich adjectives express values. For instance, the\n2555\nPatterns\nA, [MASK] and B.\nWhat’s [V ALUE] or[MASK], A or B?\nWhat’s [V ALUE] and[MASK], A or B?\nA is [MASK], thus they have a [V ALUE]\nA is [V ALUE], thus it has more[MASK] per m2.\nTo know which is [V ALUE], A or B, you need[MASK].\nWhat’s [V ALUE], thus has a higher[MASK], A or B ?\nWe need the [MASK] to know what’s [V ALUE], A or B.\nWe need the [V ALUE] to know who is[MASK] A or B\nA is famous for [V ALUE], thus it has more[MASK].\nTable 1: The list of all patterns created for collecting probing\ndata from Wikidata (Vrandeˇci´c and Krötzsch, 2014).\nnoun weight is an attribute, for which the adjectives\nlight and heavy express values. Another example\nof these kind of associations is rich, which could\nbe associated with wealthiness and net worth.\nKnowledge bases (KBs), such as Wikidata\n(Vrandeˇci´c and Krötzsch, 2014), constitute a valu-\nable resource for collecting attributes and their val-\nues. In general, KBs have been shown to help\nimprove multiple NLP application as they con-\ntain structured information (Annervaz et al., 2018;\nNakashole and Mitchell, 2015; Rahman and Ng,\n2011; Ratinov and Roth, 2009). As matter of fact,\nit is fairly simple to answer factoid questions such\nas “How old is Joe Biden?” using Wikidata, by\nsimply looking up his date of birth on Wikidata.\nAn important step to make this happen is to match\nold and date of birthto each other. Similarly, to\ncheck “how rich is Jeff Bezos?\", we only need to\nextract his net worthfrom Wikidata, and identify\nthat rich and net worth are related to each other.\nHowever, a simple task like this one requires us to\nhave tools that can identify the relation between\nthe attribute net worthand its qualitative value rich.\nDespite this task being straightforward to perform\nmanually, it is not yet solved automatically due to\nlinguistic challenges. In the previous example, rich\nand high net worth are not synonyms, and do not\nshare the same part-of-speech tag since the former\nis an adjective and the latter is a noun. However,\nthey tend to appear together in text: net worth oc-\ncurs 34 times in the Wikipedia page of Jeff Bezos,\nwhile rich/-er/-est appears 35 times.\nIn this paper, we conduct a case study to iden-\ntify to what extent do language models that are\npretrained on massive amounts of data learn these\nattribute-value correlations. In other words, we ask\nthe question: “do PTLMs understand that a given\nvalue is associated with a speciﬁc attribute?\". We\nassume that PTLMs should learn these relations\ngiven that they are trained on everyday’s online\ndata, which contains a wide amount of attribute-\nvalue pairs. Our goal is to see our work aspire\nfuture works by: a) enabling more efﬁcient com-\nparative QA and factoid QA such as “Q: Who’s\nolder Obama or Trump”, b) improving PTLMs’\nabilities to faithfully capture attribute-value rela-\ntions, and c) a step towards ﬁnding resources to\nﬁne-tune PTLMs towards semantic objectives.\n2 Methodology\nIn this paper, we aim at showing to what degree do\nPTLMs contain abstract semantically-based rela-\ntions like attributes and their values. For instance,\nwhen asking “which is denser, New York or Hong\nKong?\", humans automatically link density in this\nspeciﬁc case to urban cities and population. In or-\nder to show what PTLMs are able to understand\nwhen it comes to attribute-value pairs, we start by\ndeﬁning and collecting data from the predeﬁned set\nof patterns shown in Table 1. Next, we randomly\nselect a sample from the collected data to probe\nthree different pretrained language models: BERT\n(Devlin et al., 2019), RoBERTa (Liu et al., 2019)\nand XLNet (Yang et al.).\nPatterns The Wikipedia page of Jeff Bezos con-\ntains the statement: Bezos was named the richest\nman in modern history after his net worth increased\nto X. This type of sentences appears frequently on\nthe web, which is why we construct a dataset based\non similar patterns. To the best of our knowledge,\nthere is no available dataset that contains state-\nments of attribute-value pairs. In order to come up\nwith patterns that are likely to happen, we decided\non two types of patterns: 1) a single object state-\nment, and 2) a comparative statement between two\nobjects. Table 1 shows a sample of patterns used\nfor data collection from Wikidata. Our initial pat-\nterns include: 1) basic linguistic phenomena such\nas hypernyms and hyponyms (A, B and C); 2) sin-\ngle object statements such as A is [VALUE], hence\nit has more [MASK]and A is [MASK], hence it\nhas more [ATTRIBUTE]; and 3) comparative state-\nments containing two objects, to test if having two\nobjects would increase the likelihood of predicting\nthe correct entity or value. The integrality of our\npatterns include 15 different ones: 2 hypernyms\nand hyponyms, the 5 comparatives from Table 1\nwith the masked attributes, the 5 same ones with\n2556\nDomain Example\nPeople (20%) Gates is richer, thus has a higher net worth.\nChemistry (23%) We need the mass to know if gold is heavier.\nGeography (21%) Lima is denser, and thus has more people in a given areas.\nPolitics (26%) Obama is younger than Trump, hence he was born after.\nArt (10%) Paris is famous for art, thus it has more museums.\nTable 2: Statistics about the data extracted from Wikidata.\nthe masked values, 3 based on the single subject.\nData Collection After choosing the patterns, we\nconstruct the dataset based on the previously de-\nscribed patterns by extracting objects from Wiki-\ndata. First, we vary object-entity-value and object1-\nobject2-entity-value to include statements such\nas gold-mass-heavy and gold-silver-mass-heavy\nor Obama-date_of_birth-age and Obama-Trump-\ndate_of_birth-age. We constructed different sen-\ntences that include several patterns on how we\ncould possibly invoke these object-entity-value\ntriplet, using single object statements and compara-\ntive statements. Next, we extract different objects\nfrom Wikidata (Vrande ˇci´c and Krötzsch, 2014).\nWikidata is a collaborative knowledge base, con-\ntaining triplets (entity_id, property_id,\nvalue_id) that deﬁne a type of relation holding\nbetween an entity and a value. For instance, the\nWikidata page of Barack Obama contains multiple\ntriplets such as instance of humanwhere instance\nwhich has P31 is a property_id and human is a\nvalue with a value_id equal to Q5. For our task,\nand in one speciﬁc case among many others, we\niterate over all US presidents and create all possible\ncombinations for the (date_of_birth-age) attribute-\nvalue pair. Our dataset contains some statements\nthat are not valid from a commonsense perspec-\ntive, but this does not affect this speciﬁc task, since\nwe are not judging if a statement is true, but only\nif an attribute-value pair happens simultaneously.\nTable 2 contains our data distribution per subject.\nOur goal is to determine what is the prediction that\nPTLMs yield for each case, and whether the predic-\ntion changes if we blindly change the object. Our\nﬁnal dataset1 contains 18,327 English statements\non 15 different target masks.\nProbing PTLMs We probe three different\nPTLMs, including BERT, RoBERTa and XLNET\non a randomly selected test set. We use the Hug-\n1https://github.com/uhh-lt/\nsemantic-probing\nFigure 2: Another instance is related to geography,\nwhere we collect all the cities and create a statement\nabout population-high_density pair: “[CITY] is denser,\nthus it has more [MASK] in a given area\". We ex-\npected predictions like inhabitants, residents, popula-\ntion, which only appear 4 out of 50 predictions.\ngingFace code2(Wolf et al., 2020) to probe three\nPTLMs on attributes and their values by using the\npredictions of PTLMs’ Masked Language Mod-\nelling (MLM) head. We randomly extract 10 sen-\ntences from object-entity-value set of our data to\nmake sure that our ﬁnal test set (120 samples) is\nuniformly distributed.\n3 Results\nWe evaluate the predictions from the PTLMs man-\nually using heuristics based on the relatedness be-\ntween the predicted attribute-value pairs 3. If an\nentity-value pair are semantically related, we give\na score of 1, else we give a score of 0. For instance,\nin Table 3, examples 1, 2, 3, 6 and 7 receive a 1,\nwhile 4 and 5 receive a 0. The results of our man-\nual evaluation are summarized in Figure 3. We\nasked one person to predict the masked word for\n2https://huggingface.co/\nbert-base-uncased\n3The results reported here have been last checked on the\n17th of July, 2021.\n2557\nMasked Sentence Prediction\n1. What’s heavier or [MASK], gold or silver ? lighter, softer, thinner, heavier, light\n2. What’s heavier and [MASK], gold or silver ? lighter, softer, stronger, heavier, brighter\n3. What’s heavier, and thus has a higher [MASK], gold or silver? value, price, worth, weight, content\n4. To know which is heavier, gold or silver, you need [MASK] :,to’, . ,’ , ;\n5. We need the [MASK], to determine which is heavier, gold or silver formula, alloy, ratio, weight, elements\n6. [MASK] such as gold, silver and copper metals, elements, metal, commodities, materials\n7. Gold, [MASK] and iron silver, copper, tin, nickel, platinum\nTable 3: BERT is good at hypernyms and hyponyms, like in gold, silver and iron, but it fails to accurately predict the right\nattribute-value pair.\nCity BERT RoBERTa XLNET\nSingapore vegetation, density, moisture density, people, moisture it, thus, (\nSoﬁa density, layer, leaves volume, energy, density it, to, d\nNew Delhi vegetation, forests, soils density, space, people New, it, to\nBuenos Aires vegetation, forests, density people, density, inhabitant in, off, <eop>\nLa Paz density, vegetation, layers space, density, tree La, more, than\nPanama City vegetation, forests, trees people, inhabitant, residents Panama, Y , P\nAddis Ababa vegetation, leaves, density density, space, people and, more, ness\nMbabane vegetation, soils, forests energy, density, carbon ., M, S\nAsunción vegetation, density, inhabitants inhabitants , density, trees it, as, in\nFreetown inhabitants, forests, density people, density, trees thus, it, density\nTable 4: We probe different PTLMs on the sentence: [CITY] is denser, thus it has more [MASK].\nFigure 3: Comparing BERT and RoBERTa to human\npredictions. The human prediction was collected from\nonly one person, the accuracy was high since this kind\nof task is simple. We excluded XLNet from this evalu-\nation, since its predictions were completely random.\nthe sake of comparison. We note from the ﬁgure\nthat the human prediction outperforms the ones\nfrom BERT and RoBERTa by a very large margin.\nHowever, the predictions from RoBERTa outper-\nform BERT. To dig deeper into the model’s predic-\ntions, we looked into concrete examples (Table 4)\nto compare the three most likely predictions for the\ninput sentence: [City] is denser, thus it has more\n[MASK] in a given area. We note that RoBERTa\nhas better predictions than BERT, while XLNET\nhas a very poor set of predictions. However, there is\nmore randomness in all predictions: BERT only has\n2 out of 30 predictions relating to density, whereas\nRoBERTa has 10 out 30 predictions. We also note\nthat RoBERTa has a complete different prediction\nvocabulary set when the city is Mbabane (the pre-\ndicted words are more energy and carbon oriented),\nwhich reﬂects that PTLMs learn to generalize from\nthe data they are trained on, but do aspire to the\nlevel of abstraction that would be required for a\nﬁrm grip of attribute-value pairs.\n4 Discussion and Future Work\nWe consistently observed throughout multiple ex-\namples that PTLMs are vulnerable to slight changes\nsuch as the name of the city or the name of the el-\nement that we are targeting. Figure 2 shows ten\nrandomly selected cities from the input sentence:\n[City] is denser, thus it has more [MASK] in a\ngiven area, and the ﬁve most likely predictions from\nBERT. The examples show some random predic-\ntions (in red), such as moisture, layer and vowels.\nWhat we found interesting in our case study, is that\nchanging only the city, changes the distribution of\nthe predicted words, but in what seems to be a ran-\ndom fashion. We note from the ﬁgure that only\ntwo cities, namely Freetown and Asunción, trigger\npredictions related to inhabitants and population.\nWhile we initially conjectured that Freetown has\nalso ’town’ in its name, we were surprised that this\ndid not apply for Panama City.\nWe argue that even though BERT is trained on\nWikipedia content, the huge amount of data and the\nbiased distribution of other linguistic phenomena\nmakes it difﬁcult to capture attribute-value rela-\n2558\ntions. For that reason, it could be possible to train\na robust semantically-aware PTLM by ﬁne-tuning\nthe current PTLMs to FrameNet content, and a\nﬁrst step would be to integrate Wikidata entities\nand their values according to the semantic frames\nof each entity and every word that is evoked by\na value. One advantage of PTLMs is their capa-\nbility to perform well on speciﬁc tasks and do-\nmains that were not part of their training regime\nvia ﬁne-tuning, i.e. the retraining of a pretrained\nmodel with domain-speciﬁc examples. We argue\nthat for entities and their values, resources such as\nFrameNet and WordNet, while paired with mas-\nsive resources such as Wikidata could be used to\nﬁne-tune PTLMs towards more semantically-based\nobjectives, as a complementary work to ERNIE\n(Zhang et al., 2019), which showed that ﬁne-tuning\nPTLMs towards knowledge graphs helps enhancing\nlanguage representation with external knowledge.\n5 Conclusion\nWe demonstrated that PTLMs are unable to capture\nsemantic similarity between different words that re-\nfer to the same concepts. While PTLMs have been\nshown to improve the quality of many tasks and\nare not easy to train, our probing experiments show\nthat an improvement is necessary. All the examples\nwe extracted from Wikidata show that by enabling\nPTLMs to capture more semantically-based infor-\nmation by ﬁne-tuning towards more semantically-\nbased objectives like the ones found in FrameNet.\nAll our examples are extracted from Wikidata to\nshow that, resources such as Wikidata are rich and\ncould be used as a resource for ﬁne-tuning BERT\ntowards more high-level semantics.\n6 Acknowledgements\nThis work was supported by the DFG through the\nproject “ACQuA: Answering Comparative Ques-\ntions with Arguments” (grants BI 1544/7- 1 and HA\n5851/2-1) as part of the priority program “RATIO:\nRobust Argumentation Machines” (SPP 1999).\nReferences\nK M Annervaz, Basu Roy Chowdhury Somnath, and\nDukkipati Ambedkar. 2018. Learning beyond\ndatasets: Knowledge graph augmented neural net-\nworks for natural language processing. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n313–322, New Orleans, LA, USA.\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. In Proceed-\nings of the 17th International Conference on Com-\nputational Linguistics, COLING ’98, pages 86–90,\nNantes, France.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017. What do neural ma-\nchine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 861–\n872, Vancouver, Canada.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 1173–\n1178, Hong Kong, China.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4171–4186, Minneapolis, MN,\nUSA.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database. Bradford Books.\nCharles J. Fillmore. 1982. Frame semantics. In Lin-\nguistics in the Morning Calm, pages 111–137, Seoul,\nSouth Korea.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4129–4138, Minneapolis, MN,\nUSA.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embed-\ndings from language models. In Proceedings of the\n3rd Workshop on Evaluating Vector Space Repre-\nsentations for NLP, pages 82–89, Minneapolis, MN,\nUSA.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR.\nNdapandula Nakashole and Tom M. Mitchell. 2015. A\nknowledge-intensive model for prepositional phrase\nattachment. In Proceedings of the 53rd Annual\n2559\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing, pages 365–375,\nBeijing, China.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2227–2237, New Orleans, LA,\nUSA.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1499–1509, Brussels, Belgium.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 2463–\n2473, Hong Kong, China.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nAltaf Rahman and Vincent Ng. 2011. Coreference res-\nolution with world knowledge. In Proceedings of\nthe 49th Annual Meeting of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 814–824, Portland, OR, USA.\nLev Ratinov and Dan Roth. 2009. Design challenges\nand misconceptions in named entity recognition. In\nProceedings of the Thirteenth Conference on Com-\nputational Natural Language Learning, pages 147–\n155, Boulder, CO, USA.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online.\nIonut-Teodor Sorodoc, Kristina Gulordava, and\nGemma Boleda. 2020. Probing for referential\ninformation in language models. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4177–4189,\nOnline.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, pages 743–\n758.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy.\nDenny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commu-\nnications of the Association for Computing Machin-\nery, pages 78–85.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. Xl-\nnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, pages 5753–5763.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1441–1451, Florence, Italy."
}