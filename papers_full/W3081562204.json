{
  "title": "Multi-channel Transformers for Multi-articulatory Sign Language Translation",
  "url": "https://openalex.org/W3081562204",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2751291759",
      "name": "Camgoz, Necati Cihan",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A3112366713",
      "name": "Koller Oscar",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2744606106",
      "name": "Hadfield, Simon",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2753526026",
      "name": "Bowden, Richard",
      "affiliations": [
        "University of Surrey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2024229306",
    "https://openalex.org/W2038975019",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2981531407",
    "https://openalex.org/W4211001381",
    "https://openalex.org/W2607014458",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W2799020610",
    "https://openalex.org/W2017030536",
    "https://openalex.org/W1992016704",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2551572271",
    "https://openalex.org/W2746301562",
    "https://openalex.org/W2908497602",
    "https://openalex.org/W2788495578",
    "https://openalex.org/W2965778995",
    "https://openalex.org/W2788334925",
    "https://openalex.org/W6768970301",
    "https://openalex.org/W2964253156",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2954798773",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W2188882108",
    "https://openalex.org/W203196793",
    "https://openalex.org/W2243738093",
    "https://openalex.org/W2463640844",
    "https://openalex.org/W2755802490",
    "https://openalex.org/W2587277634",
    "https://openalex.org/W2970756316",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2127091905",
    "https://openalex.org/W2620855276",
    "https://openalex.org/W3127711005",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2138057876",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2102564687",
    "https://openalex.org/W581810680",
    "https://openalex.org/W2146221819",
    "https://openalex.org/W2161919149",
    "https://openalex.org/W4234518198",
    "https://openalex.org/W2066601700",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2050596847",
    "https://openalex.org/W2896487916",
    "https://openalex.org/W2968383879",
    "https://openalex.org/W2997931247",
    "https://openalex.org/W2952746495",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W3003261425",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2578229578",
    "https://openalex.org/W3106076771",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1568697048",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2899675781",
    "https://openalex.org/W2976132101",
    "https://openalex.org/W94699809",
    "https://openalex.org/W1497212834",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W3015075688",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2091636093",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2402205665",
    "https://openalex.org/W2109752280"
  ],
  "abstract": null,
  "full_text": "Multi-channel Transformers for\nMulti-articulatory Sign Language Translation\nNecati Cihan Camgoz1, Oscar Koller2, Simon Hadﬁeld1, and Richard Bowden1\n1 CVSSP, University of Surrey, UK, {n.camgoz, s.hadﬁeld, r.bowden}@surrey.ac.uk,\n2 Microsoft, Munich, Germany, oscar.koller@microsoft.com\nAbstract. Sign languages use multiple asynchronous information chan-\nnels (articulators), not just the hands but also the face and body, which\ncomputational approaches often ignore. In this paper we tackle the multi-\narticulatory sign language translation task and propose a novel multi-\nchannel transformer architecture. The proposed architecture allows both\nthe inter and intra contextual relationships between diﬀerent sign artic-\nulators to be modelled within the transformer network itself, while also\nmaintaining channel speciﬁc information. We evaluate our approach on\nthe RWTH-PHOENIX-Weather-2014T dataset and report competitive\ntranslation performance. Importantly, we overcome the reliance on gloss\nannotations which underpin other state-of-the-art approaches, thereby\nremoving the need for expensive curated datasets.\nKeywords: sign language translation, multi-channel, sequence-to-sequence\n1 Introduction\nSign languages are the main medium of communication of the Deaf. Every\ncountry typically has its own sign language and although some grammatical\nstructures are shared, as are signs that rely upon heavy iconicity, diﬀerent sign\nlanguage have unique vocabularies [57,59]. Contrary to spoken and written lan-\nguages, sign languages are visual. This makes automatic sign language under-\nstanding a novel research ﬁeld where computer vision and natural language pro-\ncessing meet with a view to understanding and translating the spatio-temporal\nlinguistic constructs of sign [7].\nSigners use multiple channels to convey information [61]. These channels,\nalso known as articulators in linguistics [46], can be grouped under two main\ncategories with respect to their role in conveying information, namely manual\nand non-manual features [8]. Manual features include the hand shape and its\nmotion. Although manual features can be considered as the dominant part of\nthe sign morphology, they alone do not encapsulate the full context of the con-\nveyed information. To give clarity, emphasis and additional meaning, signers\nuse non-manual features, such as facial expressions, mouth gestures, mouthings3\nand body pose. Furthermore, both manual and non-manual features eﬀect each\nother’s meaning when used together.\n3 Mouthings are lip patterns that accompany a sign.\narXiv:2009.00299v1  [cs.CV]  1 Sep 2020\n2 N. C. Camgoz et al.\nFig. 1.An overview of the proposed Multi-channel Transformer architecture applied\nto the multi-articulatory SLT task.\nTo date, the literature in the ﬁeld has predominantly focused on using the\nmanual features to realize sign language recognition and translation [50,39,13],\nthus ignoring the rich and essential information contained in the non-manual\nfeatures. This focus on the manual features is partially responsible for the com-\nmon misconception that sign language recognition and translation problems are\nspecial sub-tasks of the gesture recognition ﬁeld [54]. Sign language is as rich\nand complex as any spoken language. However, the multi-channel nature adds\nadditional complexity as channels are not synchronised.\nIn contrast to much of the existing literature, in this paper we model sign\nlanguage by incorporating both manual and non-manual features into Sign Lan-\nguage Translation (SLT). To achieve this, we utilize multiple channels which cor-\nrespond to the articulatory subunits of the sign, namely hand shape, upper body\npose and mouthings. We explore several approaches to combine the information\npresent in these channels using both early and late fusion in a transformer archi-\ntecture. Based on these ﬁndings we then introduce a novel deep learning archi-\ntecture, the Multi-channel Transformer. This approach incorporates both inter\nand intra channel contextual relationships to learn meaningful spatio-temporal\nrepresentations of asynchronous sign articulators, while also preserving chan-\nnel speciﬁc information by using anchoring losses. Although this approach was\ndesigned speciﬁcally for SLT, we believe it can also be used to tackle other\nmulti-channel sequence-to-sequence learning tasks, such as audio-visual speech\nrecognition [1]. An overview of the Multi-channel Transformer in the context of\nSLT can be seen in Figure 1.\nWe evaluate our approach on the challenging RWTH-PHOENIX-Weather-\n2014T (PHOENIX14T) dataset which provides both sign gloss 4 annotations\nand spoken language translations. Previous approaches [13,12] on PHOENIX14T\nheavily relied upon sign gloss annotations, which are labor intensive to obtain.\nWe aim to remove this dependency on gloss annotation, by utilizing channel\nspeciﬁc features obtained from related tasks, such as human pose estimation\napproaches [14,27] to represent upper body pose channel or lip reading fea-\ntures [19,3] to represent mouthings [38,37]. Removing the dependency on man-\n4 Glosses can be considered as the minimal lexical items of the sign languages.\nMulti-channel Transformers for Multi-articulatory SLT 3\nual annotation allows our approach to be scaled beyond what is possible with\nprevious techniques, potentially using huge collections of un-annotated data.\nWe empirically show that by integrating multiple articulator channels into our\nmulti-channel transformer, it is possible to achieve competitive SLT performance\nwhich is on par with models trained using additional gloss annotation.\nThe contributions of the paper can be summarized as: (1) We overcome the\nneed for expensive gloss-level supervision by combining multiple articulatory\nchannels with anchoring losses to achieve competitive continuous SLT perfor-\nmance on the PHOENIX14 T dataset. (2) We propose a novel multi-channel\ntransformer architecture that supports multi-channel, asynchronous, sequence-\nto-sequence learning and (3) We use this to introduce the ﬁrst successful ap-\nproach to multi-articulatory SLT, which models the inter and intra relationship\nof manual and non-manual channels.\n2 Literature Review\nComputational processing of sign languages is an important ﬁeld and expected\nto have tremendous impact on language deprivation of Deaf children, accessibil-\nity, sign linguistics and human-computer interaction in general. Its ﬁrst attempt\ndates back more than thirty years: A patent describing a hardwired electronic\nglove that recognized American Sign Language (ASL) ﬁnger spelling from hand\nconﬁgurations [24]. In the early days the ﬁeld moved slowly, focusing ﬁrst on\nisolated [60], then continuous sign language recognition [55]. With the rise of\ndeep learning, enthusiasm was revived and accelerated the ﬁeld [10,39,41]. The\nrecognition of limited domain but continuous real-life sign language became feasi-\nble [11,20,28,40,21]. Driven by linguistic evidence [5,65,52], the ﬁeld realized that\nsign language recognition needs to focus on more than just the hands. Earlier\nworks looked at several modalities separately, such as the face in general [63,36],\nhead pose [45], the mouth [2,37,38], eye-gaze [15], and body pose [53,17]. More\nrecently, multi-stream architectures showed strong performance [35,68].\nNevertheless, sign recognition only addresses part of the communication bar-\nrier between Deaf and hearing people. Sign languages follow a distinct grammar\nand are not word by word translations of spoken languages. After successful\nrecognition, reordering and mapping into the target spoken language complete\nthe communication pipeline. In early works, recognition and translation were\ntreated as two independent processing steps. Isolated single signs were recog-\nnized and subsequently translated [16]. Often, existing work exclusively consid-\nered the problem as a text-to-text translation problem [9,56], despite the visual\nnature of sign language and the lack of a written representation.\nGenerally speaking, much of the available sign translation literature falsely\ndeclares sign recognition as sign translation [22,64,26,25]. Camgoz et al. [13]\nwere the ﬁrst to release a joint recognition and translation corpus with videos,\nglosses and translations to spoken language, covering real-life sign language,\nrecorded from the broadcast news. They proposed to tackle the task based on a\nNeural Machine Translation (NMT) framework relying on input tokenization of\n4 N. C. Camgoz et al.\nthe videos and subsequent sequence-to-sequence networks with attention. Their\nbest performing tokenization method was based on strong sign recognition mod-\nels trained using gloss annotations with full video frames and achieved an 18.1\nBLEU-4 score, while a simple tokenization scheme (not trained with glosses) only\nreached 9.6 BLEU-4 on the test set of PHOENIX14T. Orbay and Akarun [48] in-\nvestigated diﬀerent tokenization methods on the same corpus and showed again\nthat a pretrained hand shape recognizer [39] outperforms simpler approaches\nand reaches 14.6 BLEU-4. While they also investigated transformer architec-\ntures and multiple hands as input, the results underperformed. Ko et al. [34]\ndescribe a non-public dataset covering sign language videos, gloss annotation\nand translation. Their method relies on detected body keypoints only. It hence\nmisses the important appearance based characteristics of sign. More recently,\nCamgoz et al. [12] proposed Sign Language Transformers, a joint end-to-end\nsign language recognition and translation approach. They used pre-trained gloss\nrepresentations as inputs to their networks and trained transformer encoders\nusing gloss annotations to learn meaningful spatio-temporal representations for\nSLT. Their approach is the current state-of-the-art on PHOENIX14 T. They re-\nport 20.2 BLEU-4 for pre-trained gloss features to spoken language translation,\nand 21.3 BLEU-4 with the additional gloss recognition supervision.\nOverall, previous work in the space of SLT has two major short-comings,\nwhich we intend to address with this paper: (1) The beauty of translations is\nthe abundance of available training data, as they can be created in real-time\nby interpreters. Glosses are expensive to create and limit data availability. No\nprevious work was able to achieve competitive performance while not relying on\nglosses. (2) So far SLT has never considered multiple articulators.\n3 Background on Neural Machine Translation\nThe objective of machine translation is to learn the conditional probability\np(Y|X) where X = ( x1,...,x T ) is a sentence from the source language with\nT tokens and Y= (y1,...,y U ) is the desired corresponding translation of said\nsentence in the target language. To learn this mapping using neural networks,\nKalchbrenner et al. [32] proposed using an encoder-decoder architecture, where\nthe source sentence is encoded into a ﬁxed sized “context” vector which is then\nused to decode the target sentence. Cho et al. [18] and Sutzeker et al. [58] fur-\nther improved this approach by assigning the encoding and decoding stages of\ntranslation to individual specialized Recurrent Neural Networks (RNNs).\nThe main drawback of RNN-based approaches are long term dependency is-\nsues. Although there have been practical solutions to this, such as source sentence\nreversing [58], the context vector is still of ﬁxed size, and thus cannot perfectly\nencode arbitrarily long input sequences. To overcome the information bottleneck\nimposed by using the last hidden state of the RNN as the context vector, Bah-\ndanau et al. [4] proposed an attention mechanism, which was a breakthrough\nin the ﬁeld of NMT. The idea behind the attention mechanism is to use a soft-\nsearch over the encoder outputs at each step of target sentence decoding. This\nMulti-channel Transformers for Multi-articulatory SLT 5\nwas realized by conditioning target word prediction on a context vector which is\na weighted sum of the source sentence representations. The weighting in turn is\ndone by a learnt scoring function which measures the relevance of the decoders\ncurrent hidden state and the encoder outputs. Luong et al. [44] further improved\nthis approach by proposing a dot product attention (scoring) function as:\ncontext = softmax\n(\nQKT )\nV (1)\nwhere queries, Q, correspond to the hidden state of the decoder at a given time\nstep, and keys, K, and values, V, represent the encoder outputs.\nMore recently, Vaswaniet al. [62] introduced self-attention mechanisms, which\nreﬁne the source and target token representations by looking at the context they\nhave been used in. Combining encoder and decoder self-attention layers with\nencoder-decoder attention, Vaswaniet al. proposed Transformer networks, a fully\nconnected network (as opposed to being RNN-based) which has revolutionized\nthe ﬁeld of machine translation. In contrast to RNN-based models, transformers\nobtain Q, K and V values by using individually learnt linear projection matri-\nces at each attention layer. Vaswani et al. also introduced “scaled” dot-product\nattention as:\ncontext = softmax\n(QKT\n√dm\n)\nV (2)\nwhere dm is the number of hidden units of the model. The motivation behind\nthe scaling operation is to counteract the eﬀect of gradients becoming extremely\nsmall in cases where the number of hidden units is high and in-turn, the dot\nproducts grow large [62].\nIn this work we extend the transformer network architecture and adapt it\nto the task of multi-channel sequence-to-sequence learning. We propose a multi-\nchannel attention layer to reﬁne the representations of each source channel in the\ncontext of other source channels, while maintaining channel speciﬁc information\nusing anchoring losses. We also adapt the encoder-decoder attention layer to be\nable to use multiple source channel representations.\n4 Multi-channel Transformers\nIn this section we introduce Multi-channel Transformers, a novel architecture for\nsequence-to-sequence learning problems where the source information is embed-\nded across several asynchronous channels. Given source sequencesX= (X1,...,X N ),\nwhere Xi is the ith source channel with a cardinality of Ti, our objective is to\nlearn the conditional probability p(Y|X), where Y = ( y1,...,y U ) is the target\nsequence with U tokens. In the application domain of SLT, these channels cor-\nrespond to representations of the manual and non-manual features of the sign.\nAn overview of the multi-channel transformer can be seen in Figure 1, while\nindividual attention modules introduced in this paper are visualized in Figure 2.\nTo keep the formulation simple, and to focus the readers attention on the diﬀer-\nentiating factors of our architecture, we omit the multi-headed attention, layer\nnormalization and residual connections from our equations, which are the same\nas the original transformer networks [62].\n6 N. C. Camgoz et al.\nLayer Norm\nLayer Norm\nFeed Forward\nChannel-wise Self-Attention\nLayer Norm\nFeed Forward\nLayer Norm\nLayer Norm\nFeed Forward\nLayer Norm\nLayer Norm\nFeed Forward\nLayer Norm\nMulti-channel Encoder Attention\nMean\nLayer Norm\nFeed Forward\nLayer Norm\nMulti-channel Decoder Attention\n  \n         \n         \n  \n   \n  \n  \n   \n  \n   \n     \n     \n  \n  \n  \n  \n  \n    \n  \n    \n  \n   \n     \n     \n  \n   \n   \n     \n  \n  \n  \n  \n    \n  \n  \n  \n  \n  \n    \n   \n    \n  \n \n  \n \n  \n \nFig. 2.A detailed overview of the introduced attention modules: (left) Channel-wise\nSelf-Attention, (middle) Multi-channel Encoder Attention, and (right) Mult-channel\nDecoder Attention\n4.1 Channel and Word Embeddings\nAs with other machine translation tasks, we start by projecting both the source\nchannel features and the one-hot word vectors into a denser embedding where\nsimilar inputs lie close to one-another. To achieve this we use linear layers. We\nemploy normalization and activation layers to change the scale of the embedded\nchannel features and give additional non-linear representational capability to the\nmodel. The transformer networks do not have an implicit structure to model the\nposition of a token within the sequence. To overcome this, we employ positional\nencoding [62] to add temporal ordering to the embedded representations. The\nembedding process for an input feature xi,t coming from the ith channel at time\nt can be formalized as:\nˆxi,t = Activ (Norm (xi,tWce\ni + bce\ni )) + PosEnc(t) (3)\nwhere Wce\ni and bce\ni are channel speciﬁc learnt parameters of the linear projection\nlayers. Similarly, the word embedding is as follows:\nˆyu = yuWwe + bwe + PosEnc(u) (4)\nwhere Wwe and bwe are the weights of a linear layer which are either learned\nfrom scratch or pretrained on a large corpus [6,31].\n4.2 Multi-Channel Encoder Layer\nChannel-wise Self Attention (cs):Each multi-channel encoder layer starts\nby learning the contextual relationships within a single channel by utilizing indi-\nvidual self-attention layers (See Figure 2 (left)). As per the original transformer\nimplementation, we use the scaled dot product scoring function in the atten-\ntion mechanisms. Given embedded source channel representations, ˆXi, we obtain\nMulti-channel Transformers for Multi-articulatory SLT 7\nQueries, Keys and Values for the channel i as5:\nQcs\ni = ˆXiWcs,q\ni + bcs,q\ni\nKcs\ni = ˆXiWcs,k\ni + bcs,k\ni\nVcs\ni = ˆXiWcs,v\ni + bcs,v\ni\n(5)\nwhich are then passed to the channel-wise self attention function to have their\nintra channel contextual relationship modeled as:\nhcs\ni = softmax\n(\nQcs\ni (Kcs\ni )T\n√dm\n)\nVcs\ni (6)\nwhere hCS\ni is the spatio-temporal representation of the ith source channel and\ndm is the hidden size of the model. We also utilize individual feed forward layers\nas described in [62] for each channel as:\nFF(x) = max\n(\n0,xWﬀ\n1 + b1\n)\nWﬀ\n2 + b2 (7)\nBy feeding the contextually modeled channel representations through feed for-\nward layers, we obtain the ﬁnal outputs of the channel-wise attention layer of\nour multi-channel encoder layer as:\nˆhcs\ni = FFcs\ni (hcs\ni ) (8)\nMulti-channel Encoder Attention (me):We now introduce the multi-channel\nencoder attention, which learns the contextual relationship between the self-\nattended channel representations (See Figure 2 (middle)). As we are using dot\nproduct attention, we start by obtaining Q, K and V for each source as:\nQme\ni = ˆhcs\ni Wme,q\ni + bme,q\ni\nKme\ni = ˆhcs\ni Wme,k\ni + bme,k\ni\nVme\ni = ˆhcs\ni Wme,v\ni + bme,v\ni\n(9)\nThese values are then passed to the multi-channel attention layers where the\nQueries of each channel are used to estimate the scores over the concatenated\nKeys of the other channels. These scores are then used to calculate the channel-\nfused representations by taking a weighted sum over the other channels’ con-\ncatenated Values. More formally, multi-channel attention can be deﬁned as:\nhme\ni = softmax\n(\nQme\ni\n([\n∀Kme\nj where j ̸= i\n])T\n√dm\n)\n[\n∀Vme\nj where j ̸= i\n]\n(10)\nWe would like to note that, the concatenation operation ([ ]) is performed\nover the time axis, thus making our approach applicable to tasks where the\n5 Note that we use a vectorized formulation in our equations. All softmax and bias\naddition operations are done row-wise.\n8 N. C. Camgoz et al.\nsource channels have a diﬀerent numbers of tokens. We then pass multi-channel\nattention outputs to individual feed forward layers to obtain the ﬁnal outputs\nof the multi-channel encoder layer as:\nˆhme\ni = FFme\ni (hme\ni ) (11)\nSeveral multi-channel encoder layers can be stacked to form the encoder\nnetwork with the aim of learning more complex multi-channel contextual rep-\nresentations, he = (he\n1,...,h e\nN ), where he\ni is the output corresponding to the ith\nsource channel.\n4.3 Multi-channel Decoder Layer\nTransformer networks utilize a masked self attention and an encoder-decoder\nattention in each decoder layer. The subsequent masking on self-attention is es-\nsential, as the target tokens’ successors will not be available at inference time. In\nour approach, we also employ the masked self-attention to model the contextual\nrelationship between target tokens’ and its predecessors. However, we replace\nencoder-decoder attention with multi-channel decoder attention, which is modi-\nﬁed to work with multiple source channel representations (See Figure 2 (right)).\nGiven the word embeddings ˆYof a sentence Y, we ﬁrst obtain the masked self-\nattention (sa) outputs ˆhsa using the generic approach [62], which are then in\nturn passed to our multi-channel decoder attention.\nMulti-channel Decoder Attention (md):In generic transformers, encoder-\ndecoder attention Queries are obtained from the decoder self-attention estimates,\nˆhsa, while Keys and Values are calculated from the ﬁnal encoder layer outputs,\nhe. In order to incorporate information coming from multiple channels using\ntransformer models, we propose the multi-channel decoder attention module.\nWe ﬁrst obtain the Q, K and V as:\nQmd = ˆhsaWmd,q + bmd,q\nKmd\ni = he\ni Wmd,k\ni + bmd,k\ni\nVmd\ni = he\ni Wmd,v\ni + bmd,v\ni\n(12)\nNote that each source channel ihas their own learned Key and Value matrices,\nWmd,k\ni and Wmd,v\ni respectively.\nThese are then passed to the multi-channel decoder attention module where\nthe Queries of each target token are scored against all channel Keys. Channel\nscores are then used to calculate the weighted average of their respectiveValues.\nIndividual channel outputs are averaged to obtain the ﬁnal output of the multi-\nchannel decoder attention module. This process can be formalized as:\nhmd = 1\nN\nN∑\ni=1\n(\nsoftmax\n(\nQmd (\nKmd\ni\n)T\n√dm\n)\nVmd\ni\n)\n(13)\nMulti-channel Transformers for Multi-articulatory SLT 9\nThe attention module outputs are then passed through a feed forward layer\nto obtain the ﬁnal representations of the multi-channel decoder layer as:\nˆhmd = FFmd(hmd) (14)\nLike the multi-channel encoder layer, multiple decoder layers can be stacked\nto improve representation capabilities of the decoder network. The output of the\nstacked decoder is denoted as hd = (h1,...,h U ) which is used to condition target\ntoken generation.\n4.4 Loss Functions\nWe propose training multi-channel transformers using two types of loss function,\nnamely a Translation Loss, which is commonly used in machine translation, and\na Channel Anchoring Loss, which aims to preserve channel speciﬁc information\nduring encoding.\nTranslation Loss:Although diﬀerent loss functions have been used to train\ntranslation models, such as a mixture-of-softmaxes [66], token level cross-entropy\nloss is the most common approach to learn network parameters. Given a source-\ntarget pair, the translation loss, LT , is calculated as the accumulation of the\nerror at each decoding step u, which is estimated using a classiﬁcation loss over\nthe target vocabulary as:\nLT = 1 −\nU∏\nu=1\nG∑\ng=1\np(yg\nu)p(ˆyg\nu) (15)\nwhere p(yg\nu) and p(ˆyg\nu) represent the ground truth and the generation proba-\nbilities of the target yg at decoding step u, respectively, and G is the target\nlanguage vocabulary size. In our networks, the probability of generating target\ntoken yu at the decoding step uis conditioned on the hidden state of the decoder\nnetwork hd\nu at the corresponding time step, p(ˆyu) = p(ˆyu|hd\nu). Softmaxed linear\nprojection of hd\nu is used to model the probability of producing tokens over the\nwhole target vocabulary as:\np(ˆyu|hd\nu) = softmax(hd\nuWo + bo) (16)\nwhere Wo and bo are the trainable parameters of a linear layer.\nChannel Anchoring Loss:For source channels, where we have access to a\nrelevant classiﬁer, we use an anchoring loss to preserve channel speciﬁc informa-\ntion. Predictions of these classiﬁers are used as ground truth to calculate token\nlevel cross entropy losses in the same manner as the translation loss. Given the\nclassiﬁer outputs corresponding to the ith channel, Ci = (ci,1,...,c i,Ti ), and the\nhidden state of the encoder, he, we ﬁrst calculate the prediction probabilities\nover the target channel classes as:\np(ˆci,t|he) = softmax(heWo\ni + bo\ni ) (17)\n10 N. C. Camgoz et al.\nwhere p(ˆci,t|he) represent the prediction probabilities over the ith channel’s clas-\nsiﬁer vocabulary, while Wo\ni and bo\ni are the weights and biases of the linear layer\nused for the ith channel, respectively. We then use a modiﬁed version of Equa-\ntion 15 to calculate the ith channel’s anchoring loss, LA,i, as:\nLA,i = 1 −\nT∏\nt=1\nGi∑\ng=1\np(cg\ni,t)p(ˆcg\ni,t|he\nt ) (18)\nwhere p(cg\ni,t) and p(ˆcg\ni,t) represent the classiﬁer output and the predicted prob-\nabilities of the class cd\ni at the encoders tth step, respectively, while Gi is the\nnumber of target classes of the classiﬁer corresponding to channel i. For exam-\nple, one can use a hand shape classiﬁer’s convolutional layer as as input channel\nand the same classiﬁer’s predictions as ground truth for hand channel anchor-\ning loss to preserve the hand shape information, as well as to regularize the\ntranslation loss.\nTotal Loss: We use a weighed combination of Translation loss, LT , and\nAnchoring losses, LA = (LA,1,..., LA,N ), during training as:\nL= λT LT + λA\nN∑\ni=1\nLA,i (19)\nwhere λT and λA decide the importance of each loss function during training.\n5 Implementation and Evaluation Details\nDataset: We evaluate our model on the challenging PHOENIX14 T [13]\ndataset, which is currently the only publicly available large vocabulary con-\ntinuous SLT dataset aimed at vision based sign language research.\nSign Channels: We use three diﬀerent articulators/channels to represent\nthe manual and non-manual features of the sign, namely hand shapes, mouthings\nand upper body pose. We employ the models proposed and used in [35] and\nfrom these networks extracted 1024 dimensional Convolutional Neural Network\n(CNN) features for each frame (last layer before the fully connected layer) for\nhand shape and mouthing channels. We use the class prediction from the same\nnetwork to anchor the channel representations. Although these networks were\ntrained on 61 and 40 hand shapes and mouthing classes respectively (including\ntransition/silence class), the predictions only contained 52 and 36 classes. Hence,\nour anchoring losses are calculated over the predicted number of classes.\nTo represent the upper body pose of the signers, we extract 2D skeletal pose\ninformation using the OpenPose library [14]. We then employ a 2D-to-3D lifting\napproach designed speciﬁcally for sign language production to obtain the ﬁnal\n3D joint positions of 50 upper body pose joints [67]. As there were no prior\nsubunit classes for the upper body pose on PHOENIX14 T, we do not utilize an\nanchoring loss on the pose channel.\nMulti-channel Transformers for Multi-articulatory SLT 11\nTraining and Network Details:Our networks are trained using the Py-\nTorch framework [51] with a modiﬁed version of the JoeyNMT library [42].\nWe use Adam [33] optimizer with a batch size of 32, a learning rate of 10 −3\n(β1 = 0.9,β2 = 0.998) and a weight decay of 10 −3. We utilize Xavier [23] ini-\ntialization and train all networks from scratch. We do not apply dropout and\nonly use a single headed scaled dot-product attention to reduce the number of\nhyper-parameters in our experiments.\nDecoding: During training we use a greedy search to evaluate development\nset translation performance. At inference, we employ beam search decoding with\nthe beam width ranging from 0 to 10. We also employ a length penalty as\nproposed by [30] with α values ranging from 0 to 5. We use the development\nset to ﬁnd the best performing beam width and α, and use these during test set\ninference for ﬁnal results.\nPerformance Metrics:We use BLEU [49] and ROUGE [43] scores to mea-\nsure the translation performance. To give the reader a better understanding of\nthe networks behaviour, we repeat each experiment 10 times and report mean\nand standard deviation of BLEU-4 and ROUGE scores on both development and\ntest sets. We also report our best result for every setup based on the BLEU-4\nscore as per the development set. BLEU-4 score is also used as the validation\nscore for our learning scheduler and for early stopping.\n6 Experiment Results\nIn this section we propose several multi-channel SLT experimental setups and\nreport our quantitative results. We start by sharing single channel SLT perfor-\nmance using diﬀerent network architectures, varying the number of hidden units,\nboth to set a baseline for our multi-channel approaches and to ﬁnd the opti-\nmal network size. After that, we propose two naive channel fusion approaches,\nnamely early fusion and late fusion, to set a fusion benchmark for our novel\nMulti-channel Transformer architecture. Finally, we report the performance of\nthe multi-channel transformer approach with and without the channel anchoring\nlosses and compare our results against the state-of-the-art.\nWe apply batch normalization [29] and a soft-sign activation function [47] to\ninput channel embeddings before passing them to our networks. See supplemen-\ntary material for empirical justiﬁcation for this choice.\n6.1 Single Channel Baselines\nIn the ﬁrst set of experiments, we train single channel SLT models. The main\nobjective of these experiments is to set translation baselines for all future multi-\nchannel fusion models. However, we would also like to examine the relative in-\nformation presented in each channel by comparing their translation performance\nagainst one another. In addition, we wish to identify the optimal network setup\nfor each channel to guide the future experiments. Therefore, we conduct experi-\nments with four network setups for all three articulators with sizes varying from\n12 N. C. Camgoz et al.\nTable 1.Single channel SLT baselines using diﬀerent network architectures.\nDev Set Test Set\nBLEU-4 ROUGE BLEU-4 ROUGE\nChannel HSxFFBest mean±std - mean ±std - mean ±std - mean ±std\nHand 32x64 14.54 14.05±0.4238.47 38.49±0.4513.88 13.80±0.6338.05 38.04±0.49\nHand 64x12816.4415.70±0.4140.79 40.45±0.6416.18 15.63±0.6540.62 40.07±0.80\nHand 128x25616.3215.91±0.4341.87 41.08±0.7316.76 16.02±0.8841.85 40.67±0.93\nHand 256x51216.06 15.41±0.4641.46 40.13±0.8315.43 15.57±0.6040.48 39.88±0.71\nMouthing 32x6411.70 11.24±0.3433.26 32.84±0.6010.77 11.01±0.3433.51 33.05±0.34\nMouthing 64x12812.91 12.55±0.3036.22 35.17±0.7112.83 12.62±0.5035.30 35.04±0.72\nMouthing 128x25613.74 13.08±0.4137.20 35.96±0.7913.77 13.50±0.3737.24 36.60±0.72\nMouthing 256x51212.86 12.40±0.4234.13 34.63±0.6413.25 12.34±0.5135.53 34.83±0.47\nPose 32x64 9.64 8.91±0.4230.27 29.92±0.679.64 8.55±0.6230.03 29.13±0.79\nPose 64x128 10.64 10.28±0.2631.06 31.31±0.479.97 9.88±0.1829.63 30.44±0.60\nPose 128x25611.02 10.52±0.3132.22 31.85±0.4210.26 10.03±0.4630.44 30.79±0.82\nPose 256x51210.06 9.50±0.5131.03 30.11±0.759.51 8.62±0.7029.92 28.65±0.99\nGloss 32x64 17.21 16.03±0.4942.20 41.26±0.6415.45 15.68±0.4341.21 40.71±0.42\nGloss 64x128 18.50 18.16±0.2344.99 43.87±0.6818.14 17.89±0.5643.57 43.02±0.69\nGloss 128x25619.4319.14±0.3646.10 45.17±0.6319.5219.08±0.4845.32 44.52±0.80\nGloss 256x51219.5218.36±0.5045.97 44.16±0.7919.6118.60±0.6345.29 43.92±0.98\n32x64 to 256x512 (hidden size (HS) x number of feed forward (FF) units). All\nnetworks were built using two encoder and decoder layers.\nAs can be seen in Table 1,Hand is the best performing channel in all network\nsetups. Furthermore, using a network setup of 128x256 outperforms all of the\nalternatives. We believe this is closely related to the limited number of training\nsamples we have and the over-ﬁtting issues that come with it. Therefore, for the\nrest of our experiments we use 128x256 parameters for each channel.\nWe further train a Gloss single channel network to set a baseline for our\nmulti-channel approaches to compare against. As shown in Table 1, using CNN\nfeatures that were trained using gloss level annotations outperforms all single\nsign articular based models (19.52 vs. 16.44 dev BLEU-4 score). Although the\n256x512 network setup obtained the best individual development and test set\ntranslation performances, the mean performance of the 128x256 network was\nbetter, encouraging us to utilize this setup going forward.\n6.2 Early and Late Fusion of Sign Channels\nTo set another benchmark for our multi-channel transformer, we propose two\nnaive multi-channel fusion approaches, namely early and late fusion. In the early\nfusion setup, features from diﬀerent channels are concatenated to create a fused\nrepresentation of each frame. These representations are then used to train SLT\nmodels, as if they were features coming from a single channel. Hence, the contex-\ntual relationship is performed in an implicit manner by the transformer archi-\ntecture. In our second, late fusion setup, individual SLT models are built which\nare then fused at the decoder output level, i.e. hd, by concatenation. The fused\nrepresentation is then used to generate target tokens using a linear projection\nlayer. Compared to early fusion, this approach’s capability to learn more ab-\nstract relationships is limited as the fusion is only done by a single linear layer.\nWe examine all four possible fusions of the three channels. Network setup is set\nto linearly scale with respect to the number of channels that are fused together\nwith a factor of 128x256 per channel.\nMulti-channel Transformers for Multi-articulatory SLT 13\nTable 2.SLT performance of early and late channel fusion approaches.\nDev Set Test SetBLEU-4 ROUGE BLEU-4 ROUGEFusion Channels HSxFFBest mean±std - mean±std - mean±std - mean±stdEarlyH+M 2*(128x256)17.25 16.73±0.5742.04 41.72±0.6117.37 16.73±0.8242.35 41.76±0.80EarlyH+P 2*(128x256)16.17 15.70±0.3240.51 40.28±0.4615.75 15.83±0.3040.54 40.34±0.70EarlyM+P 2*(128x256)13.57 12.91±0.3036.43 35.88±0.4013.23 13.02±0.4235.66 36.01±0.72EarlyH+M+P3*(128x256)15.69 15.08±0.5539.78 39.38±0.7315.19 15.19±0.4939.99 39.43±0.80\nLateH+M 2*(128x256)17.03 16.36±0.4841.6941.58±0.7916.8116.67±0.4941.6941.69±0.54LateH+P 2*(128x256)16.61 16.16±0.3341.54 41.18±0.5815.90 16.07±0.7640.81 40.50±0.99LateM+P 2*(128x256)14.22 13.55±0.3136.44 37.03±0.6414.11 13.65±0.4936.25 36.95±0.65LateH+M+P3*(128x256)17.00 16.35±0.3842.0941.29±0.4216.9516.50±0.4742.1241.53±0.52\nAs can be seen in Table 2, fusion of Hands and Mouth yields slightly better\nresults than single channel translation models (excluding gloss). However, unlike\nlate fusion, which saw improvement in all scenarios, early fusion’s performance\ngets worse as more features are added to the network. As this means having more\nparameters in our networks, we believe this is due to the natural propensity of\nthe transformers to over-ﬁt on small training datasets, like ours.\n6.3 Multi-channel Transformers\nIn this set of experiments we examine the translation performance of the pro-\nposed multi-channel transformer architecture for multi-articulatory SLT. We ﬁrst\nstart by investigating the eﬀects of the anchoring loss. We then compare our\nbest performing method against other fusion options, gloss based translation\nand other state-of-the-art methods. As with other fusion experiments, we exam-\nine all possible fusion combinations. In addition to using the 128x256 network\nsetup, we also evaluate having a larger network to see if the additional anchoring\nlosses help with over-ﬁtting by regularizing the translation loss.\nAs can be seen in the ﬁrst row of Table 3, while using the same number\nof parameters as the early and late fusion setups, our proposed Multi-Channel\nTransformer approach outperforms both conﬁgurations. However, doubling the\nnetwork size does eﬀect the direct application of multi-channel attention nega-\ntively. To counteract this issue and to examine the eﬀects of the anchoring loss,\nwe run experiments with both 128x256 and 256x512 setups. We normalize our\nlosses on the sequence level instead of token level and we set the anchoring loss\nweight, λA, to 0.15 to counteract diﬀerent source (video) and target (sentence)\nsequence lengths. Using the anchoring losses not only improves the performance\nof the 128x256 models but also allows the 256x512 networks to achieve simi-\nlar translation performance to using gloss features. We believe this is due to\ntwo main factors. Firstly, the anchoring loss forces the encoder channels to pre-\nserve the channel speciﬁc information while being contextually modeled against\nother articulators. Secondly, it acts as a regularizer for the translation loss and\ncounteracts the over-ﬁtting previously discussed.\nCompared to the state-of-the-art, our best multi-channel transformer model\nsurpasses the performance of several previous models [48,13], some of which\nare heavily reliant on gloss annotation. Furthermore, our multi-channel mod-\nels perform on par with our single gloss channel model, and yields competitive\n14 N. C. Camgoz et al.\nTable 3.Multi-channel Transformer based multi-articulatory SLT results.\nDev Set Test SetBLEU-4 ROUGE BLEU-4 ROUGEChannels Anchoring Loss HSxFF Best mean±std - mean±std - mean±std - mean±std\nH+M \u0017 2*(128x256)17.71 16.97±0.5343.43 42.02±0.8617.72 17.19±0.7342.70 41.95±0.85H+P \u0017 2*(128x256)17.20 16.36±0.5842.15 41.23±0.4616.41 16.25±0.6640.56 40.87±0.67M+P \u0017 2*(128x256)14.17 13.50±0.4036.82 36.62±0.5213.43 13.93±0.4437.03 37.43±0.65H+M+P \u0017 3*(128x256)17.98 16.89±0.5944.01 41.85±0.9317.15 16.85±0.6542.38 41.83±0.85H+M \u0017 2*(256x512)15.95 15.46±0.3441.01 40.31±0.5015.87 15.80±0.3640.30 40.40±0.70H+P \u0017 2*(256x512)15.41 14.95±0.3340.10 39.22±0.5315.91 15.14±0.5940.24 39.11±0.69M+P \u0017 2*(256x512)13.39 12.60±0.4935.70 35.01±0.7313.38 12.74±0.4836.89 35.39±0.79H+M+P \u0017 3*(256x512)15.87 14.97±0.5140.53 39.65±0.7916.02 15.17±0.8140.15 39.61±1.12H+M \u0013 2*(128x256)18.52 17.93±0.3944.56 43.25±0.5717.93 17.76±0.4943.21 42.91±0.49H+P \u0013 2*(128x256)17.70 16.53±0.6743.19 41.26±0.9916.93 16.41±0.4842.62 40.80±0.82M+P \u0013 2*(128x256)15.14 14.60±0.3238.57 38.45±0.4515.32 15.05±0.7238.47 38.66±0.76H+M+P \u0013 3*(128x256)18.80 17.81±0.6844.24 43.17±0.8118.30 17.75±0.5843.65 42.90±0.62H+M \u0013 2*(256x512)19.05 18.07±0.4445.04 43.76±0.8219.2117.71±0.7245.0543.29±0.99H+P \u0013 2*(256x512)16.80 16.29±0.3641.15 40.86±0.4216.68 16.29±0.4841.34 40.68±0.76M+P \u0013 2*(256x512)15.14 14.60±0.2639.45 38.47±0.6215.36 15.13±0.4440.06 39.09±0.62H+M+P \u0013 3*(256x512)19.51 18.66±0.5245.90 44.30±0.9218.5118.31±0.5743.5743.75±0.63Gloss – 128x256 19.4319.14±0.3646.10 45.17±0.6319.5219.08±0.4845.32 44.52±0.80Gloss – 256x51219.5218.36±0.5045.97 44.16±0.7919.6118.60±0.6345.29 43.92±0.98Orbayet al. [48] – – – – – – 14.56 – 38.05 –Sign2Gloss→Gloss2Text [13] – – 17.89 – 43.76 – 17.79 – 43.45 –Sign2Gloss2Text [13] – – 18.40 – 44.14 – 18.13 – 43.80 –(Gloss) Sign2Text [12] – 3x(512x2048)20.69 – – – 20.17 – – –(Gloss) Sign2(Gloss+Text) [12] –3x(512x2048)22.38 – – – 21.32 – – –\ntranslation performance to the state-of-the-art transformer based approaches\n[12], which utilize larger models and uses gloss supervision on several levels (pre-\ntrained Gloss CNN features and transformer encoder supervision). However, due\nto their dependence on gloss annotations, such models [13,12] can not be scaled\nto larger un-annotated datasets, which is not a limiting factor for the proposed\nmulti-channel transformer approach. See supplementary material for qualitative\ntranslation examples from our best multi-articulatory translation model.\n7 Conclusion\nThis paper presented a novel approach to Neural Machine Translation in the con-\ntext of sign language. Our novel multi-channel transformer architecture allows\nboth the inter and intra contextual relationship between diﬀerent asynchronous\nchannels to be modelled within the transformer network itself. Experiments on\nRWTH-PHOENIX-Weather-2014T dataset demonstrate the approach achieves\non par or competitive performance against the state-of-the-art. More impor-\ntantly, we overcome the reliance on gloss information which underpins other\nstate-of-the-art approaches. Now we have broken the dependency upon gloss in-\nformation, future work will be to scale learning to larger dataset where gloss\ninformation is not available, such as broadcast footage.\nAcknowledgements\nThis work received funding from the SNSF Sinergia project ‘SMILE’ (CRSII2160811),\nthe European Union’s Horizon2020 research and innovation programme under\ngrant agreement no. 762021 ‘Content4All’ and the EPSRC project ‘ExTOL’\n(EP/R03298X/1). This work reﬂects only the authors view and the Commission\nis not responsible for any use that may be made of the information it contains.\nWe would also like to thank NVIDIA Corporation for their GPU grant.\nMulti-channel Transformers for Multi-articulatory SLT 15\nReferences\n1. Afouras, T., Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Deep Audio-\nvisual Speech Recognition. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI) (2018)\n2. Antonakos, E., Pitsikalis, V., Rodomagoulakis, I., Maragos, P.: Unsupervised Clas-\nsiﬁcation of Extreme Facial Events Using Active Appearance Models Tracking for\nSign Language Videos. In: Proceedings of the IEEE International Conference on\nImage Processing (ICIP) (2012)\n3. Assael, Y.M., Shillingford, B., Whiteson, S., De Freitas, N.: Lipnet: End-to-end\nSentence-level Lipreading. In: GPU Technology Conference (2017)\n4. Bahdanau, D., Cho, K., Bengio, Y.: Neural Machine Translation by Jointly Learn-\ning to Align and Translate. In: Proceedings of the International Conference on\nLearning Representations (ICLR) (2015)\n5. Bellugi, U., Fischer, S.: A comparison of sign language and spoken language. Cog-\nnition 1(2) (1972)\n6. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching Word Vectors with\nSubword Information. Transactions of the Association for Computational Linguis-\ntics (ACL) 5 (2017)\n7. Bragg, D., Koller, O., Bellard, M., Berke, L., Boudrealt, P., Braﬀort, A., Caselli, N.,\nHuenerfauth, M., Kacorri, H., Verhoef, T., Vogler, C., Morris, M.R.: Sign Language\nRecognition, Generation, and Translation: An Interdisciplinary Perspective. In:\nProceedings of the International ACM SIGACCESS Conference on Computers\nand Accessibility (ASSETS) (2019)\n8. Brentari, D.: Sign Language Phonology. Cambridge University Press (2019)\n9. Bungeroth, J., Ney, H.: Statistical Sign Language Translation. In: Proceedings of\nthe Workshop on Representation and Processing of Sign Languages at International\nConference on Language Resources and Evaluation (LREC) (2004)\n10. Camgoz, N.C., Hadﬁeld, S., Koller, O., Bowden, R.: Using Convolutional 3D Neural\nNetworks for User-Independent Continuous Gesture Recognition. In: Proceedings\nof the IEEE International Conference on Pattern Recognition Workshops (ICPRW)\n(2016)\n11. Camgoz, N.C., Hadﬁeld, S., Koller, O., Bowden, R.: SubUNets: End-to-end Hand\nShape and Continuous Sign Language Recognition. In: Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) (2017)\n12. Camgoz, N.C., Hadﬁeld, S., Koller, O., Bowden, R.: Sign Language Transformers:\nJoint End-to-end Sign Language Recognition and Translation. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)\n13. Camgoz, N.C., Hadﬁeld, S., Koller, O., Ney, H., Bowden, R.: Neural Sign Language\nTranslation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2018)\n14. Cao, Z., Hidalgo, G.M., Simon, T., Wei, S., Sheikh, Y.: OpenPose: Realtime Multi-\nPerson 2D Pose Estimation using Part Aﬃnity Fields. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence (TPAMI) (2019)\n15. Caridakis, G., Asteriadis, S., Karpouzis, K.: Non-Manual Cues in Automatic Sign\nLanguage Recognition. Personal and ubiquitous computing 18(1) (2014)\n16. Chai, X., Li, G., Lin, Y., Xu, Z., Tang, Y., Chen, X., Zhou, M.: Sign Language\nRecognition and Translation with Kinect. In: Proceedings of the International Con-\nference on Automatic Face and Gesture Recognition (FG) (2013)\n16 N. C. Camgoz et al.\n17. Charles, J., Pﬁster, T., Everingham, M., Zisserman, A.: Automatic and Eﬃcient\nHuman Pose Estimation for Sign Language Videos. International Journal of Com-\nputer Vision (IJCV) 110(1) (2014)\n18. Cho, K., Van Merri¨ enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning Phrase Representations Using RNN Encoder-Decoder for\nStatistical Machine Translation. In: Proceedings of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP) (2014)\n19. Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Lip Reading Sentences in the\nWild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2017)\n20. Cui, R., Liu, H., Zhang, C.: Recurrent Convolutional Neural Networks for Contin-\nuous Sign Language Recognition by Staged Optimization. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)\n21. Cui, R., Liu, H., Zhang, C.: A Deep Neural Framework for Continuous Sign Lan-\nguage Recognition by Iterative Training. IEEE Transactions on Multimedia (2019)\n22. Fang, B., Co, J., Zhang, M.: DeepASL: Enabling Ubiquitous and Non-Intrusive\nWord and Sentence-Level Sign Language Translation. In: Proceedings of the ACM\nConference on Embedded Networked Sensor Systems (SenSys) (2017)\n23. Glorot, X., Bengio, Y.: Understanding the Diﬃculty of Training Deep Feedforward\nNeural Networks. In: Proceedings of the International Conference on Artiﬁcial\nIntelligence and Statistics (2010)\n24. Grimes, G.J.: Digital Data Entry Glove Interface Device (1983), uS Patent\n4,414,537\n25. Guo, D., Wang, S., Tian, Q., Wang, M.: Dense Temporal Convolution Network for\nSign Language Translation. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence (2019)\n26. Guo, D., Zhou, W., Li, H., Wang, M.: Hierarchical LSTM for Sign Language Trans-\nlation. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence (2018)\n27. Hidalgo, G., Raaj, Y., Idrees, H., Xiang, D., Joo, H., Simon, T., Sheikh, Y.:\nSingle-Network Whole-Body Pose Estimation. In: IEEE International Conference\non Computer Vision (ICCV) (2019)\n28. Huang, J., Zhou, W., Zhang, Q., Li, H., Li, W.: Video-based Sign Language Recog-\nnition without Temporal Segmentation. In: Proceedings of the AAAI Conference\non Artiﬁcial Intelligence (2018)\n29. Ioﬀe, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. In: Proceedings of the International Conference\non Machine Learning (ICML) (2015)\n30. Johnson, M., Schuster, M., Le, Q.V., Krikun, M., Wu, Y., Chen, Z., Thorat, N.,\nVi´ egas, F., Wattenberg, M., Corrado, G., et al.: Googles Multilingual Neural Ma-\nchine Translation System: Enabling Zero-Shot Translation. Transactions of the\nAssociation for Computational Linguistics 5 (2017)\n31. Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of Tricks for Eﬃcient Text\nClassiﬁcation. In: Proceedings of the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics (ACL) (2017)\n32. Kalchbrenner, N., Blunsom, P.: Recurrent Continuous Translation Models. In: Pro-\nceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (2013)\n33. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In: Proceed-\nings of the International Conference on Learning Representations (ICLR) (2014)\n34. Ko, S.K., Kim, C.J., Jung, H., Cho, C.: Neural Sign Language Translation based\non Human Keypoint Estimation. Applied Sciences 9(13) (2019)\nMulti-channel Transformers for Multi-articulatory SLT 17\n35. Koller, O., Camgoz, N.C., Bowden, R., Ney, H.: Weakly Supervised Learning with\nMulti-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign Lan-\nguage Videos. IEEE Transactions on Pattern Analysis and Machine Intelligence\n(TPAMI) (2019)\n36. Koller, O., Forster, J., Ney, H.: Continuous Sign Language Recognition: Towards\nLarge Vocabulary Statistical Recognition Systems Handling Multiple Signers.\nComputer Vision and Image Understanding (CVIU) 141 (2015)\n37. Koller, O., Ney, H., Bowden, R.: Read My Lips: Continuous Signer Independent\nWeakly Supervised Viseme Recognition. In: European Conference on Computer\nVision (ECCV) (2014)\n38. Koller, O., Ney, H., Bowden, R.: Deep Learning of Mouth Shapes for Sign Lan-\nguage. In: Proceedings of the IEEE International Conference on Computer Vision\nWorkshops (ICCVW) (2015)\n39. Koller, O., Ney, H., Bowden, R.: Deep Hand: How to Train a CNN on 1 Million\nHand Images When Your Data Is Continuous and Weakly Labelled. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(2016)\n40. Koller, O., Zargaran, S., Ney, H.: Re-Sign: Re-Aligned End-to-End Sequence Mod-\nelling with Deep Recurrent CNN-HMMs. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) (2017)\n41. Koller, O., Zargaran, S., Ney, H., Bowden, R.: Deep Sign: Hybrid CNN-HMM for\nContinuous Sign Language Recognition. In: Proceedings of the British Machine\nVision Conference (BMVC) (2016)\n42. Kreutzer, J., Bastings, J., Riezler, S.: Joey NMT: A minimalist NMT toolkit for\nnovices. In: Proceedings of the Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP): System Demonstrations (2019)\n43. Lin, C.Y.: ROUGE: A Package for Automatic Evaluation of Summaries. In: Pro-\nceedings of the Annual Meeting of the Association for Computational Linguistics,\nText Summarization Branches Out Workshop (2004)\n44. Luong, M.T., Pham, H., Manning, C.D.: Eﬀective Approaches to Attention-based\nNeural Machine Translation. In: Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) (2015)\n45. Luzardo, M., Karppa, M., Laaksonen, J., Jantunen, T.: Head Pose Estimation for\nSign Language Video. Image Analysis (2013)\n46. Malaia, E., Borneman, J.D., Wilbur, R.B.: Information Transfer Capacity of Ar-\nticulators in American Sign Language. Language and Speech 61(1) (2018)\n47. Nwankpa, C., Ijomah, W., Gachagan, A., Marshall, S.: Activation Functions: Com-\nparison of trends in practice and research for deep learning. arXiv:1811.03378\n(2018)\n48. Orbay, A., Akarun, L.: Neural Sign Language Translation by Learning Tokeniza-\ntion. arXiv:2002.00479 (2020)\n49. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: BLEU: A Method for Automatic\nEvaluation of Machine Translation. In: Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL) (2002)\n50. Parton, B.S.: Sign Language Recognition and Translation: A Multidisciplined Ap-\nproach From the Field of Artiﬁcial Intelligence. The Journal of Deaf Studies and\nDeaf Education 11(1) (2005)\n51. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\nDesmaison, A., Antiga, L., Lerer, A.: Automatic Diﬀerentiation in PyTorch. In:\nProceedings of the Advances in Neural Information Processing Systems Workshops\n(NIPSW) (2017)\n18 N. C. Camgoz et al.\n52. Pfau, R., Quer, J.: Nonmanuals: Their Grammatical and Prosodic Roles. In: Sign\nLanguages. Cambridge University Press (2010)\n53. Pﬁster, T., Charles, J., Everingham, M., Zisserman, A.: Automatic and Eﬃcient\nLong Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts.\nIn: Proceedings of the British Machine Vision Conference (BMVC) (2012)\n54. Pigou, L., Dieleman, S., Kindermans, P.J., Schrauwen, B.: Sign Language Recogni-\ntion using Convolutional Neural Networks. In: European Conference on Computer\nVision Workshops (ECCVW) (2014)\n55. Starner, T., Weaver, J., Pentland, A.: Real-time American Sign Language Recog-\nnition using Desk and Wearable Computer Based Video. IEEE Transactions on\nPattern Analysis and Machine Intelligence (TPAMI) 20(12) (1998)\n56. Stein, D., Schmidt, C., Ney, H.: Sign Language Machine Translation Overkill. In:\nInternational Workshop on Spoken Language Translation (2010)\n57. Stokoe, W.C.: Sign Language Structure. Annual Review of Anthropology 9(1)\n(1980)\n58. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to Sequence Learning with Neu-\nral Networks. In: Proceedings of the Advances in Neural Information Processing\nSystems (NIPS) (2014)\n59. Sutton-Spence, R., Woll, B.: The Linguistics of British Sign Language: An Intro-\nduction. Cambridge University Press (1999)\n60. Tamura, S., Kawasaki, S.: Recognition of Sign Language Motion Images. Pattern\nRecognition 21(4) (1988)\n61. Valli, C., Lucas, C.: Linguistics of American Sign Language: An Introduction.\nGallaudet University Press (2000)\n62. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is All You Need. In: Proceedings of the Advances in\nNeural Information Processing Systems (NIPS) (2017)\n63. Vogler, C., Goldenstein, S.: Facial Movement Analysis in ASL. Universal Access in\nthe Information Society 6(4) (2008)\n64. Wang, S., Guo, D., Zhou, W.g., Zha, Z.J., Wang, M.: Connectionist Temporal\nFusion for Sign Language Translation. In: Proceedings of the ACM International\nConference on Multimedia (2018)\n65. Wilbur, R.B.: Phonological and Prosodic Layering of Nonmanuals in American\nSign Language. The Signs of Language Revisited: An Anthology to Honor Ursula\nBellugi and Edward Klima (2000)\n66. Yang, Z., Dai, Z., Salakhutdinov, R., Cohen, W.W.: Breaking the Softmax Bot-\ntleneck: A High-Rank RNN Language Model. In: Proceedings of the International\nConference on Learning Representations (ICLR) (2018)\n67. Zelinka, J., Kanis, J., Salajka, P.: NN-Based Czech Sign Language Synthesis. In:\nInternational Conference on Speech and Computer (2019)\n68. Zhou, H., Zhou, W., Zhou, Y., Li, H.: Spatial-Temporal Multi-Cue Network for\nContinuous Sign Language Recognition. In: Proceedings of the AAAI Conference\non Artiﬁcial Intelligence (2020)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7685366272926331
    },
    {
      "name": "Transformer",
      "score": 0.6825729012489319
    },
    {
      "name": "Architecture",
      "score": 0.6226468682289124
    },
    {
      "name": "Sign language",
      "score": 0.5177994966506958
    },
    {
      "name": "Asynchronous communication",
      "score": 0.4564708173274994
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4304332733154297
    },
    {
      "name": "Natural language processing",
      "score": 0.37710779905319214
    },
    {
      "name": "Speech recognition",
      "score": 0.3392142951488495
    },
    {
      "name": "Linguistics",
      "score": 0.16420313715934753
    },
    {
      "name": "Engineering",
      "score": 0.1201280951499939
    },
    {
      "name": "Telecommunications",
      "score": 0.0909777581691742
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 13
}