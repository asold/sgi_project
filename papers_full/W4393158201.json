{
  "title": "Bootstrapping Cognitive Agents with a Large Language Model",
  "url": "https://openalex.org/W4393158201",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2548513172",
      "name": "Feiyu Zhu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2130920929",
      "name": "Reid Simmons",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384918756",
    "https://openalex.org/W3174357804",
    "https://openalex.org/W4296706399",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W2966636162",
    "https://openalex.org/W2779578326",
    "https://openalex.org/W4296414573",
    "https://openalex.org/W4283803468",
    "https://openalex.org/W6672752436",
    "https://openalex.org/W4288043535",
    "https://openalex.org/W4297161808",
    "https://openalex.org/W6677916085",
    "https://openalex.org/W4313530526",
    "https://openalex.org/W4383046944",
    "https://openalex.org/W4293566037",
    "https://openalex.org/W2745881055",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4381252103",
    "https://openalex.org/W4301914006",
    "https://openalex.org/W4312807436",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4385374425",
    "https://openalex.org/W4292958252",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4323870803",
    "https://openalex.org/W4384111647",
    "https://openalex.org/W4377130745",
    "https://openalex.org/W4225834215",
    "https://openalex.org/W4388720459",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4214717370",
    "https://openalex.org/W4389665836",
    "https://openalex.org/W4394828156",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W2884565639",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4385436553",
    "https://openalex.org/W4380353811",
    "https://openalex.org/W4389116286",
    "https://openalex.org/W4307125074",
    "https://openalex.org/W4378768661",
    "https://openalex.org/W4380558503",
    "https://openalex.org/W4385567216",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4320559489",
    "https://openalex.org/W1928882148",
    "https://openalex.org/W4383108457"
  ],
  "abstract": "Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. In contrast cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent entirely based on large language models. Our experiments also indicate that the cognitive agent bootstrapped using this framework can generalize to novel environments and be scaled to complex tasks.",
  "full_text": "Bootstrapping Cognitive Agents with a Large Language Model\nFeiyu Zhu, Reid Simmons\nCarnegie Mellon University\nfeiyuz@andrew.cmu.edu, rsimmons@andrew.cmu.edu\nAbstract\nLarge language models contain noisy general knowledge of\nthe world, yet are hard to train or fine-tune. In contrast cogni-\ntive architectures have excellent interpretability and are flexi-\nble to update but require a lot of manual work to instantiate. In\nthis work, we combine the best of both worlds: bootstrapping\na cognitive-based model with the noisy knowledge encoded\nin large language models. Through an embodied agent doing\nkitchen tasks, we show that our proposed framework yields\nbetter efficiency compared to an agent entirely based on large\nlanguage models. Our experiments also indicate that the cog-\nnitive agent bootstrapped using this framework can generalize\nto novel environments and be scaled to complex tasks.\nIntroduction\nLarge language models (LLM) such as GPT-4 (OpenAI\n2023), have shown emerging capabilities after training on\ninternet-scale text data with human feedback, and have been\nemployed in robot planning (Huang et al. 2022), animal be-\nhavior analysis (Ye et al. 2023), human proxies (Zhang and\nSoh 2023), and many more. However, they have also been\ncriticized for being susceptible to adversarial attacks (Zou\net al. 2023), hallucination (Casper et al. 2023), and having\ndiminishing returns for scaling (OpenAI 2023).\nCognitive architectures are another approach in the pur-\nsuit of AI that attempts to model human cognition computa-\ntionally (Newell 1994). Despite the variety of architectures\ndeveloped, most of them share the same central components,\nconsisting of declarative memory reflecting knowledge of\nthe world, procedural memory dictating the agent’s behav-\nior, and short-term working memory that assists reasoning\nand planning (Laird, Lebiere, and Rosenbloom 2017).\nThe procedural memory is represented by a set of produc-\ntion rules, each with a precondition and an effect. Agents op-\nerate in perceive-plan-act cycles, dynamically matching rel-\nevant features of the environment to the production rules and\napplying their effects. Unlike operators in symbolic plan-\nning, production rules do not represent alternative actions\nbut instead reflect different contextual knowledge (Laird\n2022). These rules can be reinforced and modified through-\nout the agent’s learning process. Despite some pioneering\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n: AND →\n⇒\nLLM\nWorld\nKnowledge:\n(tomatoes belong\nin the fridge)\nEnvt\nKnowledge:\n(gripper empty\n& table empty)\nProductions:\n(goal) (subtask: find\na tomato)\nTask\nStack:\nslice a tomato find a tomato\npush\nquerygenerate\nobserve\nact\nFigure 1: Overview of agent framework. It shows the agent\nexecuting the production of attending to a new subtask of\nfinding a tomato when the original task is to slice a tomato\nand the tomato is not in the gripper nor on the table. Dotted\nlines represent the information a production rule may condi-\ntion on. Solid lines represent information flow.\nwork on data-driven cognitive model creation (Hake, Sibert,\nand Stocco 2022), almost all previous work generate their\ninitial set of production rules manually, limiting their appli-\ncation to simple environments such as blocks world or psy-\nchology experiments (Park et al. 2023).\nIn this work, we combine the two approaches in a com-\nplementary fashion (Figure 1). LLMs encode the common\nsense knowledge of the world (Madaan et al. 2022) that can\nbe used in place of human labor for constructing agents in\nthe cognitive architecture. The reasoning and learning capa-\nbilities in the cognitive architecture can identify and filter the\nnoise in LLMs while converting the knowledge in language\nto actionable productions of an embodied agent.\nThis combined framework separates knowledge genera-\ntion and knowledge application, and this modularity is the\nkey to generalization. The LLM is responsible only for gen-\nerating general knowledge, such as “if the task is to find an\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n655\nobject, the agent should explore the places where that object\nis commonly stored”. Since such knowledge can be applied\nto almost all objects and environments, the LLM needs to\ngenerate these only once, and it is the role of the cognitive ar-\nchitecture to dynamically match the environment to the gen-\nerated knowledge. This is significantly different from using\nLLMs to generate plans directly, as the plans are grounded\nto the specific instance of the task (e.g., finding a specific ob-\nject in the specific environment), and are non-trivial to gen-\neralize to novel environments without re-generation.\nThe contribution of this paper is threefold: 1) we propose\nan agent framework that combines LLMs with customized\ncognitive architecture, 2) we demonstrate how it can learn\nto perform various kitchen tasks from bootstrapping, and 3)\nwe show that, when applied to new environments, it requires\nsignificantly fewer tokens than querying LLM for actions. 1\nRelated Work\nLearning Through Program Synthesis\nInteractive Task Learning (ITL) (Laird et al. 2017) aims at\nteaching robots new skills in a one-shot fashion. Previous\nwork implements this in the SOAR cognitive architecture\nand has shown effective task and environment transferabil-\nity in domains such as board games (Kirk and Laird 2019)\nand embodied agents (Mininger and Laird 2022). To reduce\nthe need for extensive human input, recent research explores\nusing LLM as the knowledge source (Lindes and Peter 2023;\nKirk et al. 2023), shifting human labor from specifying the\ngoal conditions to answering yes/no questions. In contrast,\nour approach uses strategic prompting and self-reflection\nmechanisms to eliminate the need for human supervision.\nOur work shares some high-level ideas with DreamCoder\n(Ellis et al. 2021), which learns to solve new problems by\nprogram generation and reflection. Instead of formulating it\nas an informed search problem, we accelerate this process\nby querying LLMs for their existing knowledge.\nMadaan et al. (2022) extract common-sense knowledge\nfrom LLMs into code form similar to how we extract pro-\nductions. But they only address the general task decomposi-\ntion, not applying the information to an embodied agent.\nLarge Language Model for Embodied Agents\nMany studies have explored using LLMs to generate code\nthat performs robotics tasks (Liang et al. 2023; Singh et al.\n2023; Vemprala et al. 2023) and game environments (Wang\net al. 2023), which is similar to the procedural memory in\nthe cognitive architectures. Other works explored generat-\ning PDDL specifications (Liu et al. 2023a; Xie et al. 2023).\nUnlike the situation-grounded code produced by these meth-\nods, our approach generates abstract productions with learn-\nable weights. This allows more generalization capabilities\nand choosing the best plan among multiple applicable plans.\nOthers let LLMs select the action directly (Di Palo et al.\n2023; Vemprala et al. 2023) with the help of other auxil-\niary components such as affordance evaluation (Ahn et al.\n1Code at github.com/zfy0314/cognitive-agents\n2022), memory stream (Park et al. 2023), visual summa-\nrization (Qiu et al. 2023), and knowledge base (Zhu et al.\n2023). Some others explored multi-modal foundation mod-\nels tailored for embodied agents (Driess et al. 2023; Xiang\net al. 2023). As LLMs are non-trivial to update from a sin-\ngle instance, using more explicit production systems in our\napproach enables persistent one-shot updates and more in-\nterpretability. As we will show in our experiments, relying\non LLMs for every action is also not very cost-effective.\nMethod\nArchitecture Overview\nFigure 1 illustrates the architecture and workflow of the\nagent. The agent has four main components. Aworld knowl-\nedge base that contains general knowledge, such as “Toma-\ntoes are commonly stored in the Fridge”. Environment\nknowledge that reflects what the agent knows about the envi-\nronment from past observations, including both information\nabout the agent itself (e.g., the gripper is empty) and about\nthe external world (e.g., the table is clear). These two com-\nponents form the declarative memories of the agent.\nAnother essential component is the procedural memory\nthat contains all the production rules. In our work, however,\nwe integrate the working memory into each production by\nexploiting the Python class structure, so there is no cen-\ntralized working memory. And, finally, inspired by the goal\nmodule of ACT-R (Anderson 2009) and the impasse mecha-\nnism of SOAR (Laird 2022), the agent manages atask stack.\nAt each time step, the agent searches in its procedural\nmemory for any applicable production rule, considering the\ncurrent task and environment knowledge. If there is no pro-\nduction applicable, the agent will summarize the current\nknowledge and query the LLM for both an action sugges-\ntion and a corresponding production rule, such that the agent\nknows what to do in similar scenarios in the future. When at\nleast one production is applicable, it will sample an appli-\ncable production rule, based on its utility, and execute the\nproposed action, which can be either in the environment or\ninternally, such as adding a subtask to its task stack.\nBootstrapping Procedures\nThe bootstrapping process starts with acurriculum. We took\ninspiration from (Wang et al. 2023), which uses an LLM\nto automatically construct the curriculum for Minecraft. As\nthe simulator we use is not as popular as Minecraft and has\nsome specific constraints (e.g., can only hold one object at a\ntime), we find it better to specify the curriculum manually.\nUnlike previous work that requires human input on the next\nsteps and/or goal condition for the tasks (Mininger and Laird\n2022), we require only the names of the task families, so\ndesigning the curriculum is not very labor intensive.\nAnother difference is that our curriculum consists of fam-\nilies of tasks (e.g., find a/an <object>) instead of\nspecific instances (e.g., find a/an egg). We follow the\nSOAR syntax and keep all variables in angle brackets.\nWith a given curriculum, the following steps are used\nto bootstrap a single task in the curriculum (using find\na/an <object> as an example).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n656\n1. Fill in the variables randomly from the environment to\ninstantiate a concrete task (e.g., find a/an Egg);\n2. Attempt the task with the existing production rules;\n3. (Action Selection) If there is no production rule for a\nstate, or there is a cycle detected through the production\napplication, query an LLM for an action;\n4. (Production Generation) Generate the corresponding\nproduction rule to the action, and load it into the agent;\n5. Repeat steps 1-4 sufficient times until the robot can per-\nform the task with only production rules;\n6. (Production Improvement) Use a critic to summarize\nthe end condition of the task for future use and improve\nthe generated productions.\nThe above procedures are repeated for all task families in\nthe curriculum. While the agent might not fully learn every\nscenario of a task before moving on to the next one, it can\nstill query the LLM later on to generate a production rule for\na previously learned task. The training of a task is considered\ncomplete as long as the agent has sufficient experience with\nthe task to generate a reasonable end condition such that fu-\nture tasks can reuse the previously learned tasks.\nAction Selection\nThe LLM is prompted with the current task, a summary\nof the current state, and a list of options available to the\nrobot, which include both motor actions on the environ-\nment (e.g., move to a specific location) and internal ac-\ntions (e.g., attend to a new subtask). For each previously\ntrained subtask, we provide the end condition generated by\nthe critic for the LLM to evaluate its relevance. Like the\ntask names, the actions can also be parameterized (e.g.,\nmove to <receptacle>), and the LLM can replace\n<receptacle> with anything as it sees fit.\nWe use chain-of-thought prompting (Wei et al. 2022),\nwhich explicitly instructs the LLM to respond to the prompt\nin a step-by-step manner, probing it to make the most in-\nformed decision. The LLM is instructed to reflect on com-\nmon strategies for approaching the task, analyze the current\nsituation, and evaluate the usefulness of each action before\nsuggesting one option for the robot to take. The LLM is also\nprompted to state the purpose of the chosen action, which\nwill inform the production rule generation later.\nProduction Generation\nAlthough the production rules are generated based on the\ncurrent state, we represent them not as plans for the current\ntask, but instead as underlying decision-making principles\nfor all similar scenarios. For example, if the current task is\nto find a/an egg, instead of suggesting the action se-\nquence of exploring every cabinet in the current environ-\nment, a desirable production rule would suggest “whenever\nyou need to find something, you should first explore the un-\nexplored places where that object is commonly stored”. This\nis a systematic generalization that can be applied to finding\nany objects, not just eggs, and also can be applied to novel\nenvironments with different layouts and receptacle types.\nListing 1: Production interface\n1 class GeneratedProduction(Production):\n2 def precondition(self, agent) -> bool:\n3 # Returns whether the production is\napplicable given the agent\n4 # Set variables as side-effects\n5 def apply(self) -> str:\n6 # Returns the effect\n7 # Based on the variable bindings\nTo generate desired production rules, we use a two-step\nprocess. The first step summarizes the action selection pro-\ncess and generates the English description of the produc-\ntion rule; the second step then converts it into executable\nPython code (Listing 1). This separation is inspired by how\nhuman beginners are instructed to build cognitive models\n(Laird 2017), and has two benefits: 1) it allows each query to\nthe LLM to be of reasonable length (∼ 5k tokens), prevent-\ning LLMs from losing focus on lengthy prompts (Liu et al.\n2023b); and 2) it facilitates a modular design, which en-\nables generating code from English descriptions generated\nfrom other sources, including human feedback and post-\ngeneration self-reflection.\nFor each step, we also use the chain-of-thought prompt-\ning technique. For English description generation, the LLM\nis given the entire history of the action selection process, and\nis instructed to take four steps: 1) identify relevant informa-\ntion that leads to choosing the action; 2) generate a specific\nproduction rule that describes the current situation; 3) iden-\ntify the potentially generalizable components in the specific\nrule and how they can be generalized; and 4) replace the\ncomponents to form the generalized production description.\nFor code generation, the LLM is given the Python inter-\nface of querying declarative memory and the current task,\nand is instructed to take another four steps: 1) plan what\nvariable bindings are needed; and how their values should\nbe assigned, 2) analyze the predicates in the precondition\nand associate them with relevant variables; 3) plan how each\npredicate should be tested using the provided function inter-\nfaces; and 4) fill in the production template. The code snip-\npet is parsed from the response and imported into the agent.\nProduction Improvement\nWe use three mechanisms to monitor and improve the\ncommon interface mismatch, over-constraining, and over-\ngeneralization problems of the LLM-generated productions.\nSimilar to the iterative prompting design in V oyager\n(Wang et al. 2023), the agent replays the generated produc-\ntion rule on the state from which it was generated, and en-\nsures that its precondition check passes the current condi-\ntions. This fixes most function interface mismatches, as the\ngenerated production has to comply with a specific naming\nscheme and the interface of the declarative knowledge.\nHowever, passing the precondition test for a single in-\nstance does not guarantee that production is ideal. As the\nLLM has access to accumulated observations from the past\nduring the action selection process, it might include unnec-\nessary conditions that happen to be true in the production’s\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n657\nprecondition, over-constraining it. This is handled by a critic\nLLM that summarizes the end condition of the task and pro-\nvides suggestions on the existing productions.\nThe critic LLM is given the name of the task family\n(e.g., find a/an <object>), and the English descrip-\ntions, generated by the production LLM, of the existing\nproduction rules for that task. The critic LLM is instructed\nto first analyze all the production rules whose effect is the\ndone action, and summarize the end condition of the given\ntask (e.g., the robot is holding the desired\nobject in its gripper). These end conditions\nsummarize the behavior of the previously learned tasks\nto inform the action selection process for future tasks.\nThis summary will be added to the prompt when querying\nfor tasks later in the curriculum to incentivize reusing\npreviously learned tasks. Next, for each production rule, the\nLLM either keeps it as is, removes it entirely, or modifies it.\nThe modifications are in the English description space for\nthe critic, and we make use of the two-step modularity of\nproduction generation to update the production rules.\nOver-generalization happens when important features are\nleft out of the production’s precondition. For example, for\nthe pick and place task, the LLM might generate a\nproduction rule that says:\nIF task is pick and place <object> AND\n<object> in field of view AND\ngripper is empty\nTHEN pick <object>\nThis will make the robot pick up the object even when the\nobject is already in the target receptacle. To prevent the agent\nfrom being stuck in an infinite loop, it will keep a state\ntransition graph during the execution process and query the\nLLM for an alternative action once a cycle is detected using\na depth-first search on the transition graph. Coupled with the\nproduction reinforcement (described below), the agent will\nprioritize loop-breaking productions.\nProduction Reinforcement\nFollowing previous work in visual navigation (Anderson\net al. 2018), the agent has to explicitly choose the special\ndone action to indicate that it has completed the current\ntask. We further extend this and give the agent a quit op-\ntion to indicate that it believes the given task is impossible\nin the given environment. This is important as we allow the\narchitecture to choose to attend to any subtask as it wants,\nand it should be able to realize when a task is impossible.\nAs we do not pre-define the goal condition during the\nbootstrapping process, we give a unit reward whenever the\nagent decides it is done with the current task. The reward\npropagates back through the shortest path to the starting\nstate. For example, if the state transition is\nS0\nP1\n= =⇒ S1\nP2\n= =⇒ S2\nP3\n= =⇒ S0\nP4\n= =⇒ S4\nP5\n= =⇒ S5\nPdone\n= = =⇒\nwhere S0 is the start state and Pdone is the production that\nyields the done action. Then the shortest path is\nS0\nP4\n= =⇒ S4\nP5\n= =⇒ S5\nPdone\n= = =⇒\nTherefore only P4, P5, Pdone will receive a utility update,\nusing the bellman backup (Sutton and Barto 2018).\nUafter(P) ← 1\nN(P) + 1\n\u0000\nN(P) · Ubefore(P) + γ∆t\n\u0001\n(1)\nWhere U(P) is the utility of production P, N(P) is the\nnumber of times P gets applied, ∆t is the time difference\nfrom production application to thedone action, and γ is the\ndiscount factor (which is set to 0.95 for our experiments).\nWhen a subtask is involved, the utility is updated with\nrespect to each task. For example, if the state transition is\nA0\nP1\n= =⇒ A1\nP2\n= =⇒ B3\nQ3\n= =⇒ B4\nQ4\n= =⇒ B5\nQdone\n= = =⇒\n| {z }\na subtask initiated by P2\nA6\nPdone\n= = =⇒\nWhere A and P correspond to the states and productions\nof the original task respectively and B and Q correspond to\nthe states and productions of the subtask respectively. This\nwill be treated as two separate utility update pathways\nA0\nP1\n= =⇒ A1\nP2\n= =⇒ A6\nPdone\n= = =⇒ and B3\nQ3\n= =⇒ B4\nQ4\n= =⇒ B5\nQdone\n= = =⇒\nIf a subtask ends up with quit then there will be no util-\nity update, not even negative ones. Because the task might\nbe impossible due to environmental constraints, which has\nnothing to do with the production rules.\nIntuitively, the closer a production brings the agent to\nchoose done for its current task, the higher its utility is.\nThis process is not provided to the LLM, so it has no incen-\ntive to “cheat” by proposing the done action all the time.\nWe also explicitly tell the LLM to avoid selecting done or\nquit action unless it is “absolutely certain” about it. This\nworked empirically in our experiments.\nThis utility update process helps reduce the impact of hal-\nlucination in LLMs, as the knowledge is aggregated. For\nexample, when tasked with “explore the countertops”, the\nLLM may hallucinate and propose a production Pbad that\nkeeps the agent exploring the cabinets after all countertops\nhave been explored, instead of proposing the done action,\nas it should. However, when tasked with “explore the sink“\nin the same bootstrapping section, the LLM may generate\na production Pgood that correctly identifies the termination\ncondition and proposes done when all receptacles of the\ndesired type have been explored. Then later, when the agent\nneeds to explore all the countertops (potentially as a sub-\ntask of another task) and all of the countertops have been\nexplored, both Pbad and Pgood will be applicable. The agent\nwill prioritize Pgood because it is guaranteed to have a higher\nutility value than Pbad. On the other hand, if we use LLM to\ngenerate plans for each task, we may get a correct plan for\nthe sink but an incorrect one for the countertops.\nWhen multiple productions are applicable given the same\nenvironment knowledge, we resolve the conflict using the\ndefinition of noisy-optimal in previous works (Tian et al.\n2023), where the probability of productionPi being selected\nand applied, given the current knowledge K, is\nP(Pi | K) ∝ IK(Pi) · exp(U(Pi)) (2)\nwhere IK(p) indicates that the preconditions of production\np hold, given knowledge K.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n658\nWorld Knowledge Base\nFor the sake of simplicity, we implemented the world knowl-\nedge of the agent as a dictionary that maps natural language\nstatements to either true or false. Unlike many existing cog-\nnitive architectures that assume an absence of knowledge\nimplies the negation, we explicitly differentiate between not\nknowing and knowing to be false. In the future, it can also\nbe replaced with a real-valued vector database.\nWhen a production rule is conditioned on a statement not\npreviously known to the agent, the LLM is used to evaluate\nwhether the statement is true, and the result will be saved\nto the knowledge base to be reused later. For instance, when\nbootstrapping the task of finding an egg, the agent will learn\nthe production rule that says “If there is an unexplored re-\nceptacle where the object is commonly stored, explore that\nreceptacle”. But the agent does not know whether “egg is\ncommonly stored in the fridge” is true or not initially, so it\nwill query the LLM and memorize the positive response in\nits world knowledge base. Later when the agent is tasked to\nput things in their common storage place, the agent can reuse\nthe knowledge and place eggs into the fridge. In addition to\ntransferring to new tasks, the knowledge can be applied to\nnew environments as well (e.g., eggs are commonly stored\nin fridges in most American households).\nThis knowledge base could be easily replaced by connect-\ning it to an existing knowledge graph or ontology. But for the\npurpose of this paper, we are bootstrapping it from scratch.\nExperiments\nSetup\nFollowing previous works in the embodied agents domain\n(Sarch et al. 2022; Trabucco et al. 2023), we evaluate\nour method in kitchen environments (see Figure 2) in the\nAI2THOR simulator (Kolve et al. 2017). As shown in Fig-\nure 2d, the agent has access to classification labels and at-\ntributes (e.g., “is opened”) for objects that are close enough\n(within 1.6m) or large enough (more than 5% of the frame).\nWe also assume the agent already knows the names and lo-\ncations of the large receptacles (e.g., cabinets, fridges, etc.)\nbut does not know what objects are in the receptacles until it\nactively explores them.\nWe use three different tasks for evaluation:\n• find a/an <object>: the goal is to have the spec-\nified object in the robot’s field of view. This is a funda-\nmental skill that is often overlooked or directly assumed\nin many of the previous works (Singh et al. 2023). We\nwant to show that our framework can bootstrap very ba-\nsic skills, in addition to composite actions.\n• slice a/an <object>: the goal is to use a knife\nto slice an object. Because the robot can hold at most\none item at a time, slicing involves a sequence of actions\nincluding finding the target object and the knife, putting\nthem in the same place, and the final slice action. We\nwant to show that our framework can handle tasks that\ninvolve multiple steps and tool use.\n• clear the countertops: the goal is to have all\nthe objects on the countertops moved to suitable storage\n(a) training floor plan\n (b) testing floor plan\n(c) ego-centric view\n (d) instance segmentation\nFigure 2: Screenshots of the AI2THOR simulator\nplaces. This is a common household task that has also\nbeen investigated in previous work (Andrew et al. 2022;\nSarch et al. 2022). We want to show that our framework\ncan handle tasks that involve repeating similar subtasks.\nThe goal conditions listed above are used only for evaluation\npurposes, but are not provided to the LLM during training\nor testing. The LLM has to infer the goal condition from the\ntask description only.\nFor find and slice, 5 target objects are chosen for\neach task, and we run 3 trials for each object where the ini-\ntial locations of the objects are shuffled. For clear the\ncountertops we run 3 trials each with 5 objects on the\ncountertops that need to be put away. The specific objects\nand locations vary between trials, and the success of the\nagent is evaluated based on how many objects originally on\nthe countertops have been relocated to other places. This re-\nsults in 15 specific goal instances for each task family.\nWe use GPT4-0613 (OpenAI 2023) for our experiments\nas previous works have shown that GPT3.5 is insufficient for\ncode generation (Olausson et al. 2023; Wang et al. 2023). We\nset temperature to the 0 for the most deterministic response.\nConditions\nFor the experimental condition, we bootstrapped our agent\nwith the following curriculum in the training floor plan:\n1. explore <receptacle>\n2. find a/an <object>\n3. pick and place a/an <object>\nin/on a/an <receptacle>\n4. slice a/an <object>\n5. put things on the countertop away\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n659\nThis process generated 27 production rules in total. During\ntest time, the agent can query the LLM for an immediate ac-\ntion if it does not have an applicable production rule for the\ncurrent situation, but it cannot learn new production rules.\nFor the baseline condition of using LLMs to query only\nthe actions, we omit the production generation steps and\nonly use the action selection process within our framework.\nThis ensures the prompts used by both conditions are the\nsame, so LLM should suggest actions of similar quality. If\nthe action proposed by the LLM leads to an affordance er-\nror, we query the LLM another two times, and if none of the\nactions are viable by the agent, then it raises a failure.\nAlthough many works address the rearrangement task\n(Sarch et al. 2022; Wu et al. 2023a), they are not appropri-\nate baselines as their architectures already encode the gen-\neral strategies (e.g., first determine the target receptacle for\neach object, then navigate to the target area, etc.) while our\napproach bootstraps everything from scratch. Similarly, a\nhand-coded cognitive agent by human experts may perform\neven better but that defeats the purpose of eliminating the\nneed for manual coding of knowledge. Other code gener-\nation works cannot handle multiple instances of the same\nkind (Singh et al. 2023) or understand the slicing precondi-\ntions (Song et al. 2022) without non-trivial modifications.\nResults\nTable 1 shows the quantitative results of different types\nof agents performing each kitchen task. The action-only\nbaseline successfully completes all tasks but one, where it\nassumes find a/an mug is equivalent to find a/an\ncup, and ends the search pre-maturely without exploring\nthe sink where the mug is actually located. On the other\nhand, our bootstrapped agent is able to finish most tasks\ncompletely using its learned production rule. The only ex-\nceptions are when it is tasked to find an object that was not\npart of its training environment. But with very limited addi-\ntional queries, the bootstrapped agent is able to successfully\ncomplete those tasks as well. This shows that the knowledge\nin the bootstrapped agent can be easily transferred to new\nobjects in new environments.\nThe success rate and number of query tokens show two\nadvantages of our framework. First, it is verifiable such that\nit does not make false assumptions (e.g., confusing mugs\nwith cups). Second, it is much more efficient to be deployed\ninto new environments as the production rules it learns can\nbe easily transferred and require minimal further assistance\nfrom the LLM, saving computations and costs.\nWe use a paired sample t-test to compare the number of\nsteps taken by both agents. No significant evidence suggests\nthat the two agents perform differently in find or slice\ntasks (p-values 0.446 and 0.347, respectively). This is not\nsurprising as the knowledge source of both agents is the\nsame LLM.\nHowever, the bootstrapped agent is taking longer in the\nclearing task with significance (p-value 0.001), which re-\nsults from a stylistic difference between the two agents.\nAs shown in Figures 3a and 3b, the bootstrapped agent\nplaces everything into an individual cabinet while the base-\nline places multiple objects in the same cabinets. This is\n(a) bootstrapped clearing\n (b) action-only clearing\n(c) bootstrapped slicing\n (d) action-only slicing\nFigure 3: Examples of task execution. The first row shows\nthe bootstrapped agent put each object in their own cabinet\nwhile the baseline agent put multiple objects in the same\ncabinet. The second row shows the bootstrapped agent sliced\nthe apple on the countertop while the baseline agent sliced\nthe apple at its current location.\nbecause one of the productions generated is “if there is an\nobject on the countertop and there is an empty receptacle,\nattend to the subtask pick up the object, and place it into the\nempty receptacle”. This production gets reused repeatedly,\nrequiring the agent to seek a unique empty receptacle be-\nfore placing each object instead of putting every object in\nthe same cabinet. By contrast, the baseline agent is making\ndecisions on a case-by-case basis, so it does not enforce that\nthe target receptacle has to be empty.\nA similar difference is also found in the slice task\nwhere the bootstrapped agent always moves the objects to\nthe countertops before slicing while the baseline agent slices\nobjects at their current location (Figures 3c and 3d).\nProduction Analysis\nThe following are some learned productions:\n• IF the current task is to find a/an <object> AND the\n<object> is located on <location> AND the robot\nis not at <location> THEN choose motor action:\nmove to <location>.\n• IF the current task is to slice a/an <sliceable>\nAND the robot is holding a/an <sliceable> AND\nthere is no <tool> in the spatial knowledge or object\nknowledge THEN choose ’attend to subtask: find a/an\n<tool>’.\n• IF the current task is to clear objects from\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n660\nTask Agent Success ↑ Success w/o LLM ↑ Steps ↓ Tokens ↓\nfind a/an <object> action-only 14/15 - 15.67 54754.20\nbootstrapped (ours) 15/15 12/15 15.80 916.87\nslice a/an <object> action-only 15/15 - 28.20 102806.60\nbootstrapped (ours) 15/15 15/15 29.13 0.00\nclear the countertops action-only 15/15 - 5.13 18924.87\nbootstrapped (ours) 15/15 15/15 7. 47 0.00\nTable 1: Result of experiments on household tasks. Completion steps and tokens are averaged over all task instances\nclose open mov\ne to location pick up put do\nwn\nexplore\nfind\nslice in\nview pick and\nplace\nclear countertops slice\nFigure 4: The hierarchy of tasks learned. Gray nodes denote\nthe built-in functions of the robot, and white nodes represent\nthe tasks learned from the curriculum. For built-in actions\nthat involve an object (e.g., close), the object has to be within\nthe field of view for the action to be taken. Special actions\n(i.e., done and quit) are omitted due to space constraints.\na/an <receptacle\ntype> AND all the\n<receptacle type> are empty THEN choose\nspecial action: ’done’.\nThese show that the agent is able to represent different as-\npects of the given tasks using production rules. The first rep-\nresents a common strategy for finding things, namely how\nto find things with a known location. The second represents\ndecomposing complex tasks and reusing previously learned\ntasks. The third is a correct termination condition for the ex-\nploration task that is generated directly by the LLM.\nFigure 4 shows the task hierarchy learned by the agent\nafter training on the given curriculum. It shows how pre-\nviously learned tasks are used to perform new tasks. This\nreduces the number of queries needed for the LLM, fosters\ngenerality, and ensures the scalability of our approach.\nDiscussion\nExplainability\nOur framework touches upon all three aspects of explain-\nability as defined by Milani et al. (2022). The preconditions\nof the productions directly specify the feature that is being\nused (feature importance). Each production rule corresponds\nto a specific scenario during the bootstrapping process when\nit is created, which helps determine the training points that\ninfluence the learned policy (learning process). Lastly, the\nproduction application process can be easily converted to a\nverifiable decision tree by merging the precondition checks\nof productions (policy-level explainability). As the produc-\ntion rules can be formally verified, they are preferable to\nblack-box LLM models in safety-critical situations.\nLimitations\nIn this work, we explore only the high-level decision-making\nprocess of the agent and rely heavily on having a well-\ndefined interface for low-level actions, such as navigation\nand object manipulation. There will likely be a considerable\nsim-to-real gap when applying this to physical agents.\nAdditionally, the English description generation step re-\nquires the decision-making process to be articulable to be\nconverted to production rules. This is hard for skills that can-\nnot be fully expressed using language (e.g., sculpting).\nFuture Work\nThere are more learning opportunities in cognitive archi-\ntectures such as updating the preconditions of productions\nor using separate productions for conflict resolutions. Also,\nlarge vision models can be used to generate production rules\nwithout separate perception modules (Wu et al. 2023b).\nAdditionally, it is well-acknowledged that human values\nand preferences are hard to represent with reward functions\n(Casper et al. 2023). However, the production rules are in-\nterpretable and can be modified to suit each individual with-\nout extensive computation. As they are also modular, updat-\ning one specific production rule does not affect the others.\nIt would interesting to examine whether this framework will\nfacilitate personalization in human-AI collaboration tasks.\nSpecifically, the user can iteratively update the production\nrules to fit their preference without having to worry about\nthe agent forgetting about how to perform the task.\nConclusion\nThis paper presents a framework for bootstrapping a cog-\nnitive architecture from the existing noisy knowledge in\nLLMs, with minimal human inputs. We demonstrated how\nsuch an agent could efficiently learn to perform kitchen tasks\nand be applied to new environments. This work generalizes\nusing LLMs to generate plans and provides an alternative to\npurely data-driven foundation models. And finally, we shed\nlight on how it will benefit personalized agents in the future.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n661\nAcknowledgments\nThe authors would like to thank Michelle Zhao and Daphne\nChen for their feedback on the manuscript, and the artists\nfrom the Noun Project for distributing the icons in Fig-\nure 1 under the Creative Commons License. This work was\npartially supported by a CMU SURF grant and by the AI-\nCARING Institute (NSF IIS-2112633).\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Fu, C.; Gopalakrishnan, K.; Hausman,\nK.; et al. 2022. Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691.\nAnderson, J. R. 2009. How can the human mind occur in the\nphysical universe? Oxford University Press.\nAnderson, P.; Chang, A.; Chaplot, D. S.; Dosovitskiy, A.;\nGupta, S.; Koltun, V .; Kosecka, J.; Malik, J.; Mottaghi, R.;\nSavva, M.; et al. 2018. On evaluation of embodied naviga-\ntion agents. arXiv preprint arXiv:1807.06757.\nAndrew, S.; Karmesh, Y .; Alex, C.; Vincent-Pierre, B.;\nAaron, G.; Angel, C.; Manolis, S.; Zsolt, K.; and Dhruv,\nB. 2022. Habitat Rearrangement Challenge 2022. https:\n//aihabitat.org/challenge/rearrange\n2022. Accessed: 2023-\n01-02.\nCasper, S.; Davies, X.; Shi, C.; Gilbert, T. K.; Scheurer, J.;\nRando, J.; Freedman, R.; Korbak, T.; Lindner, D.; Freire, P.;\net al. 2023. Open Problems and Fundamental Limitations\nof Reinforcement Learning from Human Feedback. arXiv\npreprint arXiv:2307.15217.\nDi Palo, N.; Byravan, A.; Hasenclever, L.; Wulfmeier, M.;\nHeess, N.; and Riedmiller, M. 2023. Towards a unified agent\nwith foundation models. In Workshop on Reincarnating Re-\ninforcement Learning at ICLR 2023.\nDriess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.;\nIchter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; et al.\n2023. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378.\nEllis, K.; Wong, C.; Nye, M.; Sabl ´e-Meyer, M.; Morales,\nL.; Hewitt, L.; Cary, L.; Solar-Lezama, A.; and Tenenbaum,\nJ. B. 2021. Dreamcoder: Bootstrapping inductive program\nsynthesis with wake-sleep library learning. In Proceedings\nof the 42nd acm sigplan international conference on pro-\ngramming language design and implementation, 835–850.\nHake, H. S.; Sibert, C.; and Stocco, A. 2022. Inferring a\nCognitive Architecture from Multitask Neuroimaging Data:\nA Data-Driven Test of the Common Model of Cognition Us-\ning Granger Causality. Topics in Cognitive Science, 14(4):\n845–859.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In International Con-\nference on Machine Learning, 9118–9147. PMLR.\nKirk, J. R.; and Laird, J. E. 2019. Learning Hierarchi-\ncal Symbolic Representations to Support Interactive Task\nLearning and Knowledge Transfer. In IJCAI, 6095–6102.\nKirk, J. R.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2023.\nIntegrating Diverse Knowledge Sources for Online One-shot\nLearning of Novel Tasks. arXiv:2208.09554.\nKolve, E.; Mottaghi, R.; Han, W.; VanderBilt, E.; Weihs, L.;\nHerrasti, A.; Deitke, M.; Ehsani, K.; Gordon, D.; Zhu, Y .;\net al. 2017. Ai2-thor: An interactive 3d environment for vi-\nsual ai. arXiv preprint arXiv:1712.05474.\nLaird, J. E. 2017. SOAR 9.6.0 Tutorial. https:\n//soar.eecs.umich.edu/articles/downloads/soar-suite/228-\nsoar-tutorial-9-6-0. Accessed: 2023-01-02.\nLaird, J. E. 2022. Introduction to Soar. arXiv preprint\narXiv:2205.03854.\nLaird, J. E.; Gluck, K.; Anderson, J.; Forbus, K. D.; Jenkins,\nO. C.; Lebiere, C.; Salvucci, D.; Scheutz, M.; Thomaz, A.;\nTrafton, G.; et al. 2017. Interactive task learning. IEEE\nIntelligent Systems, 32(4): 6–21.\nLaird, J. E.; Lebiere, C.; and Rosenbloom, P. S. 2017. A\nstandard model of the mind: Toward a common computa-\ntional framework across artificial intelligence, cognitive sci-\nence, neuroscience, and robotics. Ai Magazine, 38(4): 13–\n26.\nLiang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,\nB.; Florence, P.; and Zeng, A. 2023. Code as policies:\nLanguage model programs for embodied control. In 2023\nIEEE International Conference on Robotics and Automation\n(ICRA), 9493–9500. IEEE.\nLindes, J. R.; and Peter, W. 2023. Improving Knowledge\nExtraction from LLMs for Robotic Task Learning through\nAgent Analysis. arXiv preprint arXiv:2306.06770.\nLiu, B.; Jiang, Y .; Zhang, X.; Liu, Q.; Zhang, S.; Biswas, J.;\nand Stone, P. 2023a. LLM+ P: Empowering Large Language\nModels with Optimal Planning Proficiency. arXiv preprint\narXiv:2304.11477.\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,\nM.; Petroni, F.; and Liang, P. 2023b. Lost in the middle:\nHow language models use long contexts. arXiv preprint\narXiv:2307.03172.\nMadaan, A.; Zhou, S.; Alon, U.; Yang, Y .; and Neubig, G.\n2022. Language models of code are few-shot commonsense\nlearners. arXiv preprint arXiv:2210.07128.\nMilani, S.; Topin, N.; Veloso, M.; and Fang, F. 2022. A\nsurvey of explainable reinforcement learning.arXiv preprint\narXiv:2202.08434.\nMininger, A.; and Laird, J. E. 2022. A Demonstration of\nCompositional, Hierarchical Interactive Task Learning. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 36, 13203–13205.\nNewell, A. 1994. Unified theories of cognition. Harvard\nUniversity Press.\nOlausson, T. X.; Inala, J. P.; Wang, C.; Gao, J.; and Solar-\nLezama, A. 2023. Demystifying GPT Self-Repair for Code\nGeneration. arXiv:2306.09896.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPark, J. S.; O’Brien, J. C.; Cai, C. J.; Morris, M. R.;\nLiang, P.; and Bernstein, M. S. 2023. Generative agents:\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n662\nInteractive simulacra of human behavior. arXiv preprint\narXiv:2304.03442.\nQiu, J.; Xu, M.; Han, W.; Moon, S.; and Zhao, D. 2023. Em-\nbodied Executable Policy Learning with Language-based\nScene Summarization. arXiv preprint arXiv:2306.05696.\nSarch, G.; Fang, Z.; Harley, A. W.; Schydlo, P.; Tarr,\nM. J.; Gupta, S.; and Fragkiadaki, K. 2022. Tidee: Tidy-\ning up novel rooms using visuo-semantic commonsense pri-\nors. In European Conference on Computer Vision, 480–496.\nSpringer.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\nProgPrompt: Generating Situated Robot Task Plans using\nLarge Language Models. In International Conference on\nRobotics and Automation (ICRA).\nSong, C. H.; Wu, J.; Washington, C.; Sadler, B. M.; Chao,\nW.-L.; and Su, Y . 2022. Llm-planner: Few-shot grounded\nplanning for embodied agents with large language models.\narXiv preprint arXiv:2212.04088.\nSutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-\ning: An introduction. MIT press.\nTian, R.; Tomizuka, M.; Dragan, A. D.; and Bajcsy, A. 2023.\nTowards Modeling and Influencing the Dynamics of Human\nLearning. In Proceedings of the 2023 ACM/IEEE Interna-\ntional Conference on Human-Robot Interaction, 350–358.\nTrabucco, B.; Sigurdsson, G. A.; Piramuthu, R.; Sukhatme,\nG. S.; and Salakhutdinov, R. 2023. A Simple Approach\nfor Visual Room Rearrangement: 3D Mapping and Semantic\nSearch. In The Eleventh International Conference on Learn-\ning Representations.\nVemprala, S.; Bonatti, R.; Bucker, A.; and Kapoor, A. 2023.\nChatGPT for Robotics: Design Principles and Model Abili-\nties. Technical Report MSR-TR-2023-8, Microsoft.\nWang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.;\nZhu, Y .; Fan, L.; and Anandkumar, A. 2023. V oyager: An\nOpen-Ended Embodied Agent with Large Language Mod-\nels. arXiv:2305.16291.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.; Song,\nS.; Bohg, J.; Rusinkiewicz, S.; and Funkhouser, T. 2023a.\nTidybot: Personalized robot assistance with large language\nmodels. arXiv preprint arXiv:2305.05658.\nWu, W.; Yao, H.; Zhang, M.; Song, Y .; Ouyang, W.; and\nWang, J. 2023b. GPT4Vis: What Can GPT-4 Do for Zero-\nshot Visual Recognition? arXiv preprint arXiv:2311.15732.\nXiang, J.; Tao, T.; Gu, Y .; Shu, T.; Wang, Z.; Yang, Z.;\nand Hu, Z. 2023. Language Models Meet World Models:\nEmbodied Experiences Enhance Language Models. arXiv\npreprint arXiv:2305.10626.\nXie, Y .; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023.\nTranslating natural language to planning goals with large-\nlanguage models. arXiv preprint arXiv:2302.05128.\nYe, S.; Lauer, J.; Zhou, M.; Mathis, A.; and Mathis,\nM. W. 2023. AmadeusGPT: a natural language interface\nfor interactive animal behavioral analysis. arXiv preprint\narXiv:2307.04858.\nZhang, B.; and Soh, H. 2023. Large Language Models\nas Zero-Shot Human Models for Human-Robot Interaction.\narXiv:2303.03548.\nZhu, X.; Chen, Y .; Tian, H.; Tao, C.; Su, W.; Yang, C.;\nHuang, G.; Li, B.; Lu, L.; Wang, X.; et al. 2023. Ghost in\nthe Minecraft: Generally Capable Agents for Open-World\nEnviroments via Large Language Models with Text-based\nKnowledge and Memory. arXiv preprint arXiv:2305.17144.\nZou, A.; Wang, Z.; Kolter, J. Z.; and Fredrikson, M. 2023.\nUniversal and Transferable Adversarial Attacks on Aligned\nLanguage Models. arXiv:2307.15043.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n663",
  "topic": "Bootstrapping (finance)",
  "concepts": [
    {
      "name": "Bootstrapping (finance)",
      "score": 0.8359336853027344
    },
    {
      "name": "Computer science",
      "score": 0.48649829626083374
    },
    {
      "name": "Cognition",
      "score": 0.47623947262763977
    },
    {
      "name": "Psychology",
      "score": 0.386868417263031
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3687102198600769
    },
    {
      "name": "Natural language processing",
      "score": 0.33632147312164307
    },
    {
      "name": "Econometrics",
      "score": 0.20966163277626038
    },
    {
      "name": "Mathematics",
      "score": 0.13019272685050964
    },
    {
      "name": "Neuroscience",
      "score": 0.06170576810836792
    }
  ]
}