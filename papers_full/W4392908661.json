{
  "title": "Network Intrusion Detection Based on Feature Image and Deformable Vision Transformer Classification",
  "url": "https://openalex.org/W4392908661",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2124296415",
      "name": "Kan He",
      "affiliations": [
        "Shenyang University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": [
        "Shenyang University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1965322206",
      "name": "Xuejun Zong",
      "affiliations": [
        "Shenyang University of Chemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098014420",
      "name": "Lian Lian",
      "affiliations": [
        "Shenyang University of Chemical Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3044970631",
    "https://openalex.org/W4379875301",
    "https://openalex.org/W4281386867",
    "https://openalex.org/W3203436613",
    "https://openalex.org/W4321503132",
    "https://openalex.org/W2967545389",
    "https://openalex.org/W4291743636",
    "https://openalex.org/W2762776925",
    "https://openalex.org/W3157331533",
    "https://openalex.org/W4318962769",
    "https://openalex.org/W4311493355",
    "https://openalex.org/W4308422437",
    "https://openalex.org/W4205559658",
    "https://openalex.org/W3120973779",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3123908207",
    "https://openalex.org/W4225151257",
    "https://openalex.org/W4323022372",
    "https://openalex.org/W4310034732",
    "https://openalex.org/W4213061225",
    "https://openalex.org/W2807786182",
    "https://openalex.org/W3167979994",
    "https://openalex.org/W3214029542",
    "https://openalex.org/W4382468150",
    "https://openalex.org/W4376480418",
    "https://openalex.org/W2944992190",
    "https://openalex.org/W4312401446",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2789828921",
    "https://openalex.org/W2296509296",
    "https://openalex.org/W3118241656",
    "https://openalex.org/W3171364527",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3122239467"
  ],
  "abstract": "Network intrusion detection technology has always been an indispensable protection mechanism for industrial network security. The rise of new forms of network attacks has resulted in a heightened demand for these technologies. Nevertheless, the current models&#x2019; effectiveness is subpar. We propose a new Deformable Vision Transformer (DE-VIT) method to address this issue. DE-VIT introduces a new deformable attention mechanism module, where the positions of key-value pairs in the attention mechanism are selected in a data-dependent manner, allowing it to focus on relevant areas, capture more informative features, and avoid excessive memory and computational costs. In addition to using deformable convolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a sliding window mechanism is also employed to utilize edge information fully. In Parallel, we use a layered focal loss function to improve classification performance and address data imbalance issues. In summary, DE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations on the public intrusion detection datasets, and the accuracy of the enhanced intrusion detection model surpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It reaches 99.5&#x0025; and 97.5&#x0025; on the CIC IDS2017 and UNSW-NB15 datasets, exhibiting an increase of 8.5&#x0025; and 9.1&#x0025;, respectively.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nNetwork Intrusion Detection Based on Feature \nImage and Deformable Vision Transformer \nClassification \nKan He1, Wei Zhang2, Xuejun Zong3, Lian Lian3 \n1Associate Professor, School of Information Engineering, Shenyang University of Chemical Technology , Shenyang 110142, China \n2College of Information Engineering, Shenyang University of Chemical Technology, Shenyang 110142, China  \n3Key Laboratory of Information Security for Petrochemical Industry in Liaoning Province,  Shenyang 110142, China \nCorresponding author: Xuejun Zong (xuejun_zong@syuct.edu.cn). \nThis work was supported by the Liaoning Science and Technology Innovation Platform Construction Program Project ([2022] No. 36)  and the National \nGuiding Local Science and Technology Development Funds Program Project ([2023] No. 7). \nABSTRACT Network intrusion detection technology has always been an indispensable protection \nmechanism for industrial network security. The rise of new forms of network attacks has resulted in a \nheightened demand for these technologies. Nevertheless, the current mod els' effectiveness is subpar. We \npropose a new Deformable Vision Transformer (DE -VIT) method to address this issue. DE-VIT introduces \na new deformable attention mechanism module, where the positions of key -value pairs in the attention \nmechanism are selected in a data -dependent manner, allowing it to focus on relevant areas , capture more \ninformative features, and avoid excessive memory and computational costs. In addition to using deformable \nconvolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a \nsliding window mechanism i s also employed to utilize edge information fully . In Parallel, we use a layered \nfocal loss function to improve classification performance and address data imbalance issues. In summary, \nDE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations \non the public intrusion detection d atasets, and the accuracy of the enhanced intrusion detection model \nsurpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It \nreaches 99.5% and 97.5% on the CIC IDS2017 and UNSW -NB15 datasets, exhibiting an increa se of 8.5% \nand 9.1%, respectively.  \nINDEX TERMS  Network intrusion detection, deformable vision transformer,  deformable convolution , \ndeformable attention mechanism, vision transformer. \nI. INTRODUCTION \nWith the rapid development of the Internet, massive \nmultimedia information is being disseminated, which poses \na significant  challenge to Internet security due to the attacks \nof hackers and information leaks. Therefore, preventing \nnetwork intrusion has become the central issue of network \nsecurity. To address this problem, the development of \nintrusion detection technology has become necessary. Our \nmain objective is to create a network intrusion detection \nsystem that is both secure and effective. To date, numerous \nIntrusion Detection Systems (IDS) have been designed \nutilizing various technologies, including traditional visual \nanalysis [1] and machine learning [2]. \nSupervised algorithms like Naive Bayes (NB) [4] and \nSupport Vector Machine (SVM) [3] are commonly \nemployed in traditional machine learning for solving \nclassification problems. However, they must repeatedly \nlabel a substantial amount of data, which is time-consuming \nand laborious. On the other hand, unsupervised algorithms \nlike Principal Component Analysis (PCA) [6] and Gaussian \nMixture Model (GMM) [5] can discover hidden structures \nand information in the data without data labeling. However, \ntheir prediction accuracy is low, and evaluating the model's \nperformance is challenging.  \nVisual analysis can transform abstract information into \nunderstandable image data, making it easier for people to \ndiscover patterns in network security data and alarm \ninformation. However, personal subjective factors can lead \nto different interpretations o f the same data.  \nDeep learning algorithms such as Convolutional Neural \nNetwork (CNN) [7], Recurrent Neural Network (RNN) [8], \nand Generative Adversarial Network (GAN) [9] have their \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nlimitations. RNN is helpful for processing sequence data \nand can capture dependencies between sequences. \nHowever, in some cases involving long -term dependencies, \ndeep learning models may encounter issues with gradient \nexplosion and vanishing during training . It  is not parallel \ncomputing, which affects training speed. GAN can solve \nimbalanced data problems in intrusion detection by using a \ngenerator and a discriminator. When the network layer is \ndeep enough in CNN , it can parallelly compute and capture \nremote features. However, deep convolution's [10] effect is \nnot always ideal, as learning remote dependency \nrelationships mainly depends on the length of the path \nthrough which forward and backward signals pass in the \nnetwork [11]. \nVision Transformer (VIT) [12] addresses the issues of \nmodeling global dependencies in inputs and outputs \nthrough attention mechanisms while avoiding recurrent \nstructures and enabling parallel computing capabilities. By \ncalculating the correlation between any two positions \nregardless o f distance, the attention mechanism allows for \nbetter and faster feature extraction. Additionally, the VIT's \nmulti-head attention mechanism obtains richer feature \ninformation by forming multiple subspaces and extensively \nutilizing information from differen t representation \nsubspaces at different locations.  \nBased on the advantages above , we propose a new \nintrusion detection model called Deformable Vision \nTransformer (DE -VIT) tailored to the task . To achieve this, \nwe have enhanced the VIT model , which has several \nadvantages. Specifically, we have addressed the limitation \nof VIT ignoring local structures when directly segmenting \nimages into fixed -length tokens by suggesting using a \nsliding window mechanism to segment the data. This \napproach improves edge information preservation. \nAdditionally, we have introduced a deformable attention \nmechanism to avoid excessive memory and computational \ncosts resulting from using dense self -attention in VIT. \nFurthermore, features may be affected by irrelevant parts \nbeyond the region of interest. Specifically, having too many \nkeys for each query patch will result in high computational \ncosts, slow convergence, and an incr eased risk of \noverfitting.  \nIn contrast, the convolution module [13] uses deformable \nconvolution to enlarge the receptive field of each patch \nwithout increasing the convolution kernel. The model uses \na hierarchical focal loss function to assign weights to \nsamples with different categories [14]. This approach \neffectively addresses prediction errors and reduces the \nimpact of outliers on the model, thereby enhancing the \nclassification performance. The simulation experiments \nconducted on the CIC IDS2017 [33] and UNSW-NB15 [34] \ndatasets show that our proposed intrusion detection method  \nhas significantly improved accuracy.  \nOverall, the contributions of this paper are as follows:   \nFirstly, we propose DE -VIT, which improves upon \ntraditional neural networks like CNN by addressing their \npoor performance in global information perception [15]. \nThis improvement bolsters the detection model's \ngeneralization ability.  \nSecondly, we use a small convolution kernel and stride \ninstead of the original hard segmentation . This enables \nbetter correlation modeling between each patch and its \nadjacent patches, allowing for better modeling of local \ninformation such as edges and lines.  \nThirdly, we introduce a deformable attention mechanism \n[16] for computing the correlation between two positions. \nSo that the model can extract features more effectively,  \nfocus on relevant regions, capture more information \nfeatures, and reduce computation. Additionally, \ndeformable convolution [17] increases the receptive field \nof each patch without increasing the convolution kernel \nsize. This allows each patch to contain more information, \nresulting in a more efficient feature extraction process.  \nFourthly, we add sine and cosine positional encoding to \nassign position information to each feature. This helps the \nmodel understand their relationships since different \npositions represent different features.  \nFinally, we use the hierarchical focal loss function to \nsolve the data imbalance problem and improve the \nclassification performance by reducing the impact of \noutliers on the model.  \nThe rest of this article follows: Part II introduces the \nrelevant information to be consulted. Part III proposes an \nimproved DE -VIT intrusion detection model. Part IV \nprovides binary and multiclass experimental results and \ncompares them with other methods. Part V is the summary \nof the full text . \nII. RELATED WORK \nSecurity issues have recently plagued Network-based \nIntrusion Detection Systems (NIDS) . Our focus is on \nfinding solutions to these problems. Traditional machine \nlearning and deep learning, particularly deep learning, are \nthe preferred solutions. Deep learning outperforms \ntraditional machine learning algorithms in processing data \nin different fields , such as natural language, image, and \nspeech. It has also proven to be highly effective in intrusion \ndetection. However, machine learning has its limitations. \nFor instance, Mishra and colleagues [37] conducted a \ncomprehensive study and survey of different machine -\nlearning algorithms used in intrusion detection. They \noutlined the limitations of various technologies. For \nexample, If the data contains misclassified instances, \nSupport Vector Machines (SVM ) [18] may fail to find the \nseparating hyperplane. The Naive Bayes classifier [19] \ntypically assumes attribute independence, which may not \nhold in practical applications. Decision trees [20] can be \nprone to overfitting if the model becomes overly complex \nand fits too closely to the training data, resulting in poor \ngeneralization ability to new data. Random forests [21] may \nhave high computational costs and perform poorly on \nimbalanced datasets. Additionally, the K -Nearest \nNeighbors (KNN) algorithm [22] is affected by the \nselection of the distance measure and the value of K [23]. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nNowadays, the rapid growth and expansion of computer \nnetworks and related applications have led to significant \nachievements in various network security tasks through \ndeep learning. For instance, Khan et al.  [24] proposed a \nnew framework for Network Intrusion Detection System \n(NIDS) based on deep convolutional neural networks. They \nutilized network spectrum images generated by short -time \nFourier transform. The objective of this solution is to \novercome the challeng e of high False Alarm Rates (FAR) \nencountered when detecting new attacks, which has been a \npersistent issue for existing NIDS solutions. To address this \nchallenge, Lopez -Martin et al.  [25] introduced a novel \nmethod for network intrusion detection. Their approach \nutilizes an extended radial basis function neural network \nand offline reinforcement learning. Specifically, they \nemploy an extended Radial Basis Function Neural Network \n(RBFNN) in offline reinforcement learning algorithms as a \npolicy network . This allows for the end -to-end learning of \nall parameters and network weights of RBF functions \nthrough gradient descent  without needing external \noptimization. The proposed method was tested on five \ncommon intrusion detection datasets (UNSW -NB15, NSL-\nKDD, AWID, CIC IDS2017, and CIC DDOS2019)  and \nachieved good performance indicators. However, this \nmethod may become slow when dealing with large -scale \ndata. Additionally, offline reinforcement learning methods \nrequire storing historical data for training, which may \nrequire a large amount of stora ge space. Furthermore, since \nhistorical data may not fully cover all cases, the trained \nmodel may not adapt to all situations.  \nRecently, Xu et al.  [26] introduced the Multilayer \nIntrusion Detection Model that employs Stacked \nBidirectional Long Short -Term Memory (LSTM), \nConvolutional Neural Networks (CNN), Convolutional \nDenoising Autoencoders (SCDAE), and self -attention \nmechanisms to improve the accuracy of the model. The \nmodel also utilizes attention mechanisms to extract features \nand classify network traffic data. However, this model's \npracticality and scalability in real -world scenarios may be \nlimited due to its requirement for a significant amount of \ntraining data and computing resources to achieve high \naccuracy. In another study [27], this paper explores using \nGraph Neural Networks (GNN) in network security \nintrusion detection. It focuses explicitly  on applying graph \nrepresentation learning techniques in intrusion detection. \nGNN can learn effective representations from graph -\nstructured data without external domain knowledge, which \nis crucial for effective detection. However, GNN's \ncomputational cost may be high, and it may perform  poorly \non small or sparse graphs because they require a certain degree \nof connectivity to learn effective representations. Further \nimprovement is still needed. \nYang et al. [28] proposed an improved wireless network \nintrusion detection method based on a convolutional neural \nnetwork. Compared with traditional models, this method has a \nlower false positive rate, higher true positive rate, and higher \ndetection accuracy. However, this method is based on specific \ndatasets and has not been further tested and validated, which \nmay not be suitable for real-world scenarios or other datasets. \nKim et al.  [29] proposed a method for early detection of \nnetwork intrusion using a type of classifier based on \nGenerative Adversarial Networks (GAN). This method takes \nthe packet data as the input feature and judges whether the data \npacket is malicious traffic through the model, enabling real -\ntime and accurate intrusion detection without the delay time of \nsession termination or collecting a certain number of packets. \nNonetheless, the problem of dilution of  intrusion features \ncaused by timeouts and traffic outside the intrusion cycle still \nexists. Despite the emergence of numerous new deep learning \nmodels, most still operate by processing data on request. \nHowever, this approach overlooks the potential correlation \nbetween data requests in adjacent time frames. \nIn 2017, the Google team proposed the Transformer [30], \nwhich quickly became popular and was widely applied in \nnatural language processing. The subsequent BERT [31] \nmodel has been commonly used in machine translation, text \ngeneration, and other tasks. Previous neural network models \noften have their limitations, such as convolutional neural \nnetwork (CNN) and recurrent neural network (RNN), which \nstruggled with processing sequential data due to issues like \nRNNs' gradient vanishing problem and CNNs' weak local \nperception ability, the Transformer overcomes these \nshortcomings. It supports parallel computing and better global \ninformation representation and  has long-sequence solid  \nmodeling capabilities. In 2020, the Vision Transformer (VIT) \n[12] emerged, applying the Transformer to the field of vision \nand achieving excellent performance in computer vision tasks. \nHowever, in the traditional attention mechanism, each \nfeature calculates the relationship with the entire feature map, \nso using the dense self-attention mechanism in VIT will cause \nexcessive memory and computational costs, and the features \nmay be affected beyond the area of interest. Due to the impact \nof irrelevant parts, too many keys per query patch will incur \nhigh computational costs and slow convergence, increasing \nthe risk of overfitting. Therefore, a deformable attention \nmechanism [16] method is proposed. Each feature only \ncalculates the relationship with surrounding features, which \nreduces the computing power and avoids the influence of \nirrelevant feature areas.  \nAt the same time, compared with other traditional neural \nnetworks, it also has the advantage  of focusing on key \ninformation and processing long sequences.  For example, \ncompared with convolutional neural networks, the deformable \nattention mechanism is better able to capture global \ninformation because it can pay attention to all positions in the \ninput sequence simultaneously, thereby better understanding \nthe relationship between contexts, and can directly process the \nentire sequence without losing position information, so it is \nmore suitable for handling dependencies between different \npositions.  Compared with recurrent neural networks, the \ndeformable attention mechanism allows the output of all \npositions to be calculated simultaneously  to achieve better \nparallel computing and improve the training and inference \nefficiency of the model.  Compared with graph neural  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nCharacter \ndigitization\nDataset as \ncsv file\nData \nnormalization\nData reshape \nto image\nData Preprocessing\nEmbedded\nPatches\nNorm\nMulti-head \ndeformable attention\nDeformable \nconvolution\nNorm\nCLS\nMLP\nfully connected\nSoftmax\nOut\nLayer classification \nprediction\n \nFIGURE 1. Architecture Diagram of Intrusion Detection Model  \n \nnetworks, in addition to global information capture and \nparallel computing, the deformable attention mechanism can \nprocess graph-structured data and handle other types of inputs, \nsuch as sequence and set data. Therefore, it has broader \napplication capabilities in various tasks and fields  without \nbeing limited by data type. In summary, the application of the \ndeformable attention mechanism in network intrusion \ndetection is very suitable, and the current application of this \nmethod in network intrusion detection is scarce. It provides a \nreference for other colleagues to conduct further research. \nIII. METHODS \nFig. 1 presents the flow chart of the complete model \nframework. The model has been enhanced based on the \noriginal VIT, forming a new DE -VIT model. The overall \nsteps of the model are as follows:  \nStep 1: In the original datasets, the CIC IDS2017 and \nUNSW-NB15 datasets each consist of 80 and 48 features, \nrespectively, represented in numerical and textual formats. \nNon-numeric attributes are subjected to one-hot encoding to \nfacilitate the model's handling of these features. Additionally, \nrows containing Not a Number  (NaN) and infinity (INF) \nvalues and irrelevant numerical columns  are excluded. \nFollowing this preprocessing step, we obtain a row of 64 \nfeature sequences of numerical values. This sequence is then \nreshaped into an 8x8 image matrix. \nStep 2: After obtaining the feature images, undergo \nprocessing through a deformable convolution layer with a \nsmall stride of 1 and 3x3 convolution kernel to extract image \nfeatures. The extracted features are then passed through an \nembedding layer for encoding, incorporating position \nencoding. Finally, the encoded features are fed into a \ndeformable attention layer, enabling the model to focus on \nspecific regions of interest, thereby improving the model's \ncomprehension of image information. After training, the \nmodel parameters  retain memorized image information \ncorresponding to each category, facilitating subsequent data \nclassification. \nA. FEATURE EXTRACTION \nThe earliest transformers were used to process one -\ndimensional sequence data. After the appearance of the Vision \nTransformer (VIT) [12], the application of the transformer in \nthe image field became a reality. To achieve this, the image \nğ‘‹ âˆˆ â„ğ»Ã—ğ‘ŠÃ—ğ¶  is partitioned into blocks to obtain ğ‘‹ğ‘ƒ âˆˆ\nâ„ğ‘Ã—(ğ‘ƒÃ—ğ‘ƒÃ—ğ¶) . However, this approach may cause loss of \nstructural information at the segmentation boundary, which is \nnot conducive to modeling features such as image edges. A \nsliding window mechanism is proposed to segment the data to \naddress this issue. Moreover, deformable convolution is used \ninstead of regular 2D convolution to increase the receptive \nfield of each patch. This has three benefits: \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \n1. The proposed method utilizes adjacent position and edge \ninformation, effectively enhancing the correlation between \neach patch and establishing a connection with its neighbors. \nThis approach enables the model to understand global \ninformation better and improve overall performance. \n2. The input size constraints of the VIT model can be \nrelaxed by using a sliding window approach to segment the \nimage instead of strictly dividing it into equal -sized blocks. \nThis method eliminates the requirement of dividing the image \ninto ğ‘ = ğ» Ã— ğ‘Š/(ğ‘ƒ Ã— ğ‘ƒ) blocks of equal size, as specified \nby the standard VIT model. \n3. Deformable convolution [17] allows the convolution \nkernel to have an offset so that it can have a larger receptive \nfield and better perceive global information without increasing \nthe size of the kernel. \nIn traditional natural language processing tasks, word \nposition information is included, whereas in Transformer -\nbased models, this information is discarded. However, patch \nposition order contains important structural information for \nimages that must be preserved. Therefore, a sinusoidal \npositional encoding is proposed that embeds position \ninformation into a texture that periodically varies with the \ndimension index (as shown in Fig. 2). Since different positions \nrepresent different features, adding positional encoding [32] is \nessential for accurate feature modeling. \nğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–) = ğ‘ ğ‘–ğ‘›(ğ‘ğ‘œğ‘ / 10000\n2ğ‘–\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ )\nğ‘ƒğ¸(ğ‘ğ‘œğ‘ ,2ğ‘–+1) = ğ‘ğ‘œğ‘ (ğ‘ğ‘œğ‘ / 10000\n2ğ‘–\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ )\n(1) \n \nğ‘ğ‘œğ‘   is the location of patch, ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™  is the embedding \ndimension of each patch, ğ‘–  is the location of embedding \ndimension. \n \nFIGURE 2. Visualization of sin-cosine positional encoding \nB. DEFORMABLE CONVOLUTION \nThere are limitations to modeling large, unknown shape \ntransformations using traditional CNNs. Since different \nlocations may correspond to objects of various scales or sizes, \nthe convolutional unit's fixed sampling of the input feature \nmap can result in all activation units having the same receptive \nfield. This can potentially hinder the precise localization of \nobjects. To address this issue, deformable convolution is \nintroduced to improve deformation modeling ability . This \nmethod is based on a parallel network that learns offset, \ncausing the convolutional kernel to shift at the sampling points \nof the input feature map, thereby focusing on the region or \ntarget of interest. Fig. 3 illustrates this process.  \n(a) (b)\n \nFIGURE 3. Ordinary convolution and deformable convolution \n \nSpecifically, the traditional Transformer model utilizes 2D \nconvolution for feature extraction, employing large \nconvolutional kernels and strides. This approach is necessary \nbecause, for image data, even smaller kernels and strides \nwould still yield feature maps with sizes reaching tens of \nthousands of pixels. Calculating attention mechanisms for \neach pixel would thus lead to an extremely high computational \ncomplexity. However, the features are considerably smaller \nfor network intrusion detection than for image data. Therefore, \nemploying small strides and convolutional kernels with \ncontinuous sliding windows can adequately extract edge \ninformation, resulting in more reliable model outcomes. \nIn traditional convolution, sampling is done on the regular \ngrid ğ‘… of the input feature map ğ‘¦, and the sum of weighted \nsampling values by ğ‘¤. ğ‘… represents the relative coordinates of \neach point in the output feature map, ğ‘0 is a point on feature \nmap y corresponding to the center point of the convolution \nkernel, and ğ‘ğ‘›  is each offset of ğ‘0  within the convolutional \nkernel range. \n \nğ‘… = {(âˆ’1, âˆ’1), (âˆ’1,0), â€¦ , (0,1), (1,1)} (2) \nğ‘¦(ğ‘0) = âˆ‘ ğ‘¤\nğ‘ğ‘›âˆˆğ‘…\n(ğ‘ğ‘›) â‹… ğ‘¥(ğ‘0 + ğ‘ğ‘›) (3) \nDeformable convolution introduces an offset âˆ†ğ‘ğ‘› for each \npoint based on traditional convolution, which is generated by \nanother convolution with the input feature map and is usually \na decimal. \nğ‘¦(ğ‘0) = âˆ‘ ğ‘¤\nğ‘ğ‘›âˆˆğ‘…\n(ğ‘ğ‘›) â‹… ğ‘¥(ğ‘0 + ğ‘ğ‘› + âˆ†ğ‘ğ‘›) (4) \n \nSince the offset position after addition is not an integer and \ndoes not correspond to the actual pixel points on the feature \nmap, interpolation is needed to obtain the offset pixel value, \nusually using bilinear interpolation, where ğ‘ = ğ‘0 + ğ‘ğ‘› +\nâˆ†ğ‘ğ‘›, and ğ‘ enumerates all integral spatial positions in the  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nFeature map\nQuery feature Zq\nReference point Pq  (Pqx,Pqy)\nPq  \n(Pqx,Pqy)\n \n \n \n \n \n \n \n \n \nLinear Linear\nSoftmax\n0.5\n0.3\n0.2\n0.4\n0.4\n0.2\n0.3\n0.3\n0.4\nAggregate\nAggregate\nAggregate\nSampling Offsets       }\nHead1 Head2 Head3\nValues {   m }\nHead1\nHead2\nHead3\nAttention Weights {    }\nHead1 Head2 Head3\nAggregated Sampled Values\nHead1 Head2 Head3 Linear\nLinear\nOutput\n \nFIGURE 4. Deformable attention module.  \nfeature map ğ‘¥. ğº(Â· ï¼ŒÂ·) represents the bilinear interpolation \nkernel. \nğ‘¥(ğ‘) = âˆ‘ ğº(ğ‘, ğ‘) â‹… ğ‘¥(ğ‘)\nğ‘\n(5) \nC. SELF ATTENTION \nLet's examine the attention mechanism [30] of the Vision \nTransformer. The input is a flattened feature map ğ‘‹ âˆˆ â„ğ‘Ã—ğ¶ , \nand it is processed by a multi -head self-attention (MHSA) \nblock with ğ‘€ heads. The MHSA block can be represented as \nfollows: \n \nğ‘ = ğ‘¥ğ‘Šğ‘, ğ‘˜ = ğ‘¥ğ‘Šğ‘˜, ğ‘£ = ğ‘¥ğ‘Šğ‘£ (6) \nğ‘§(ğ‘š) = ğœ(ğ‘(ğ‘š)ğ‘˜(ğ‘š)ğ‘‡/ âˆšğ‘‘)ğ‘£(ğ‘š)    ğ‘š = 1, â€¦ , ğ‘€ (7) \nğ‘§ =  ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘§(1), ğ‘§(2), â€¦ , ğ‘§(ğ‘€))ğ‘Šğ‘œ (8) \nHere, ğœ(âˆ™) denotes the softmax function, and ğ‘‘ =  ğ¶/ğ‘€ \nrepresents the dimension for each head. ğ‘§(ğ‘š) represents the \nembedding output of the m -th attention head, while \nğ‘(ğ‘š), ğ‘˜(ğ‘š), ğ‘£(ğ‘š) âˆˆ â„ğ‘Ã—ğ‘‘ represent the query embedding, key \nembedding, and value embedding respectively. The projection \nmatrices ğ‘Šğ‘ , ğ‘Šğ‘˜ , ğ‘Šğ‘£  , ğ‘Šğ‘œ âˆˆ â„ğ¶Ã—ğ¶  are used in the MHSA \nblock. To introdu ce non-linearity, a Transformer block \ntypically includes an MLP block with two linear \ntransformations and a GELU activation.  \nBy incorporating normalization layers and identity shortcut \nconnections, the ğ‘™-th transformer block can be expressed as  \nfollows: \nğ‘§ğ‘™\nâ€² = ğ‘€ğ»ğ‘†ğ´(ğ¿ğ‘(ğ‘§ğ‘™âˆ’1)) + ğ‘§ğ‘™âˆ’1 (9) \nğ‘§ğ‘™ = ğ‘€ğ¿ğ‘ƒ(ğ¿ğ‘(ğ‘§ğ‘™\nâ€²)) + ğ‘§ğ‘™\nâ€² (10) \nD. DEFORMABLE ATTENTION \nIn traditional self-attention mechanisms, obtaining the feature \nvector at each position involves the weighted summing of the \nentire feature map. However, in deformable attention \nmechanisms, the feature vector at each position is obtained by \nweighted summing of the feature vectors of adjacent small \nblocks. That can be translated, rotated, scaled, and transformed \nin different directions. This deformable approach enables the \nmodel to better adapt to objects of different shapes and sizes \nand to capture interactions between objects more accurately. \nWhy do traditional attention mechanisms consume more \nmemory and computational costs? One major challenge with \nusing Transformer attention for image feature mapping is that \nit considers all possible spatial positions, leading to \ncomputational inefficiency. To overcome this limitation, a \ndeformable attention mechanism has been proposed as an \nalternative to the traditional attention mechanism. This \napproach draws inspiration from deformable convolution and \nfocuses on a small number of key sampling points around the \nreference point, as illustrated in Fig. 4. By assigning a small \nfixed number of ke ys to each query, not only can the \ncomputational resources of the model be reduced, but also the \nproblems related to convergence and feature space resolution \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \ncan be alleviated. To enhance comprehension, Fig. 4 displays \nthe input data as an image. However, the input image is \nflattened into a sequence input.  \nSpecifically, the first step is to determine the number of \nyour sampling points, perform two full connections on the \nfeature map, and determine the offsets and weights relative to \nthe sampling points around the feature points. The weights and \noffsets are random at the beginning; during the model training, \nthe obtained weights and offsets are more and more consistent \nwith the optimization results, focusing on the area of interest \nand ignoring non -correlated areas so that each feature only \ncalculates the relationship between a small number of \nsurrounding feature points, which not only reduces It reduces \ncomputing power and memory, and avoids the impact of \nirrelevant areas on the results. \nThe input feature map ğ‘¥ âˆˆ â„ğ»Ã—ğ‘ŠÃ—ğ¶  is given. we use ğ‘ to \nindex a query element with content feature ğ‘§ğ‘  and a two -\ndimensional reference point ğ‘ƒğ‘ . The deformable attention \nmechanism can be calculated using the following formula: \nğ·ğ‘’ğ‘“ğ‘œğ‘Ÿğ‘šğ´ğ‘¡ğ‘¡ğ‘›(ğ‘§ğ‘, ğ‘ğ‘, ğ‘¥) =\nâˆ‘ ğ‘Šğ‘š\nğ‘€\nğ‘š=1\n[âˆ‘ ğ´ğ‘šğ‘ğ‘˜\nğ¾\nğ‘˜=1\nâ‹… ğ‘Šğ‘š\nâ€² ğ‘¥(ğ‘ğ‘ + ğ›¥ğ‘ğ‘šğ‘ğ‘˜ )] (11) \n \nIn the equation, we denote the total number of attention \nheads as M and the total number of sampled keys as ğ¾, where \nğ¾ is always less than ğ»ğ‘Š. Additionally, we use ğ›¥ğ‘ğ‘šğ‘ğ‘˜  and \nğ´ğ‘šğ‘ğ‘˜ to denote the sampling offsets and attention weights of \nthe m-th attention head and k-th sampled point, respectively. \nThe attention weight ğ´ğ‘šğ‘ğ‘˜ ranges from 0 to 1, âˆ‘ ğ´ğ‘šğ‘ğ‘˜\nğ¾\nğ‘˜=1 =\n1 . ğ›¥ğ‘ğ‘šğ‘ğ‘˜ âˆˆ â„  is a two -dimensional unconstrained real \nnumber. As ğ‘ƒğ‘ + ğ›¥ğ‘ğ‘šğ‘ğ‘˜  is a fractional order, bilinear \ninterpolation is utilized to compute the precise value \nfollowing the offset. ğ›¥ğ‘ğ‘šğ‘ğ‘˜  and ğ´ğ‘šğ‘ğ‘˜ are both derived from \nlinear projection on the query feature ğ‘ğ‘, with ğ‘Šğ‘š and ğ‘Šğ‘š\nâ€²  \nserving as projection matrices. Specifically, the query feature \nğ‘ğ‘  is inputted into a linear projection operator with 3ğ‘€ğ¾ \nchannels. The first 2ğ‘€ğ¾  channels encode the sampling \noffset Î”p_mqk, while the remaining ğ‘€ğ¾  channels are \ndirected to the Softmax operator to obtain the attention \nweight ğ´ğ‘šğ‘ğ‘˜.  \nThis is because every sampling point in each attention \nhead possesses ğ¾  sampling keys. Each sampling key \nnecessitates the prediction of horizontal and vertical \ncoordinates for the offset, while the weight solely requires \nthe prediction of a singular value. \nThe procedure entails the addition of offset ğ´ğ‘šğ‘ğ‘˜  to the \ntwo-dimensional reference point ğ‘ƒğ‘  to acquire the offset \ncoordinates. The value of this offset is subsequently found in \nfeature map x, based on these coordinates. The offset \ncoordinates are then multiplied by attention weight ğ´ğ‘šğ‘ğ‘˜ \nand ultimately output through a fully connected layer.  \nE. LOSS FUNCTION \nTo address the issue of imbalanced intrusion detection \ndatasets, we propose a layered focal loss function, called L -\nFocal loss. firstly, we start with the binary classification \ncross-entropy loss and gradually introduce the L -Focal loss \nfunction.  \nwhere ğ‘¦ is the label of the sample, and ğ‘¦â€² âˆˆ [0,1] is the \nestimated probability of the model for the class with label \nğ‘¦ = 1. ğ‘ is the final probability \nğ‘ = { ğ‘¦â€²,             ğ‘–ğ‘“ ğ‘¦ = 1,\n1 âˆ’ ğ‘¦â€²,         ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’. (12) \nğ¶ğ¸(ğ‘, ğ‘¦) = ğ¶ğ¸(ğ‘) = âˆ’ log(ğ‘) (13) \n \nIn intrusion detection datasets, there is an imbalance in the \ndifferent categories of data, with normal traffic samples \nbeing much more abundant than other types of attack \nsamples. Negative samples account for the dominant part of \nthe total loss. The model predicts attack traffic as normal \ntraffic to reduce loss and improve accuracy . This makes \nmodel optimization unsatisfactory. This violates our original \nintention, but the L-Focal loss function assigns the weight of \npositive and negative samples in the total loss by varying the \nweight factor Î±. The weight decreases as the number of \nsamples increases, resulting in a more minor impact on the \nloss function. \nğµğ¶ğ¸(ğ‘, ğ‘¦, ğ›¼) = { âˆ’ğ›¼log (ğ‘),            ğ‘–ğ‘“ ğ‘¦ = 1,\nâˆ’(1 âˆ’ ğ›¼)log (ğ‘),    ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’. (14) \n \nWhen too many negative samples are easily distinguished, \nthe entire training revolves around these negative samples, \noverwhelming positive samples and causing significant loss. \nTherefore, we introduce a modulation factor ğ›¾ to focus on \ncomplex samples. In the L -Focal loss function, ğ›¾ is utilized \nas a modulation factor to amplify the loss of challenging -to-\nclassify samples and reduce the impact of easily classifiable \nsamples on the model. However, entering different \nmodulation factor parameters results in significant \ndifferences in output results, leading to overfitting problems \nwhen there are outliers. \nIt is not uncommon for an intrusion detection model to \nfind rare types of attacks in a dataset [37]. However, directly \nusing (1 âˆ’ ğ‘)ğ›¾ as a modulation factor for the loss function \nduring the model training process can lead to continuous loss \nreduction without improving the model's accuracy. This may \nnot be conducive to model development and research. The \nL-Focal loss function, on the oth er hand, classifies samples \ninto different levels of classification, with a focus on \naccurately classifying samples near ğ‘ =  0.5 , rather than \noveremphasizing highly classifiable samples with ğ‘ â‰«  0.5 \nor outliers with ğ‘ â‰ª  0.5. \nğ‘”(ğ‘) = {(1 âˆ’ ğ‘)ğ›¾1 ,          |ğ‘ âˆ’ 0.5| < ğ‘,\n(1 âˆ’ ğ‘)ğ›¾2 ,              ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’, (15) \nğ¿ğ¹ğ‘œğ‘ğ‘ğ‘™ = { âˆ’ğ›¼ğ‘”(ğ‘)log ğ‘,            ğ‘–ğ‘“ ğ‘¦ = 1,\nâˆ’(1 âˆ’ ğ›¼)ğ‘”(ğ‘)log ğ‘,    ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’. (16) \n \nTo achieve this, the L -Focal loss function introduces a \nmodulation factor, ğ‘”(ğ‘), and a weighting factor, ğ›¼ âˆˆ [0,1], \nwhile ğ‘ âˆˆ [0,1] represents the estimated probability of the \nmodel. By setting a value to classify samples in the L -Focal  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nFIGURE 5. The number of attack types comes from CIC IDS2017 and UNSW -NB15. \n \nBenign Benign\nDDos\nBenign\nDDos DDos\nPatatorPatatorPatator\n \nFIGURE 6. CIC IDS2017 dataset attack type data visualization.  \n \nloss function, we can focus on samples between  [0.5 âˆ’\nğ‘, 0.5 +  ğ‘] during training. This approach helps prevent \noverfitting or large cumulative sample losses that can \ndominate gradients and adversely affect model optimization. \nTo ensure that the model focuses on appropriate samples \nduring training, we set 0 â‰¤ ğ‘ â‰¤ 0.5 and 0 â‰¤ ğ›¾1 < ğ›¾2 â‰¤ 5 \nin the L-Focal loss function.  \nIV. EXPERIMENT \n \n \n \nNormal Normal\nDos\nNormal\nDos Dos\nGenericGenericGeneric\n \nFIGURE 7. UNSW-NB15 dataset attack type data visualization. \nA. DATASET  \nIn this study, we utilized two datasets, CIC IDS2017 [33] and \nUNSW-NB15 [34], due to their diverse attack types and \nrelevant features essential for server identification and arrival \ntime.  \nThe CIC IDS2017 and UNSW-NB15 datasets are publicly \navailable datasets downloaded from official websites. The \nCIC IDS2017 data set is provided in CSV format and \nincludes timestamps, source and destination IP addresses, \nsource and destination ports, protocol, and other information.  \n0 20000 40000 60000 80000 100000 120000\nBenign/Normal\nDos/Generic\nHeartbleed/Exploits\nDDos/Fuzzers\nPatator/Dos\nWeb/Reconn\nBot/Analysis\nPortScan /Backdoor\nInfiltration/Shellcode\nWorms\nCIC IDS2017 and UNSW-NB15 dataset attack type distribution\nCIC IDS2017 UNSW-NB15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nTransformer Encoder\nBenign Malicious\nCSV file\n \nFIGURE 8. The overall structure of our system \n \nTABLE 1. VIT and DE-VIT Model Comparison \nModel Sine-Cosine Position Encoding cosine annealing L-Focal loss DeformConv2d Deformable Attention \nVIT Ã— Ã— Ã— Ã— Ã— \nDE-VIT âˆš âˆš âˆš âˆš âˆš \n \nThese datasets encompass benign daily activities as well as \nvarious types of attacks, such as Benign, DoS (Denial of \nService), Heartbleed, DDoS (Distributed Denial of Service), \nPatator, Web attack, Bot, PortScan, and Infiltration.  The \nUNSW-NB15 dataset is provided in CSV format and \nincludes timestamps, source and destination IP addresses, \nsource and destination ports, protocol, and response speed, \namong other details. These datasets encompass benign daily \nactivities and attacks, such as Normal, DoS (Denial of \nService), Generic, Reconnaissance, Analysis, Exploits, \nFuzzers, Backdoor, Shellcode, and Worms. \nAs stated in the beginning of part III, CIC IDS2017 has 80 \nfeatures, while UNSW-NB15 has 48 features. To preprocess \nthe data, we one -hot encoded the characters, removed \ncolumns with identical values, cleaned some dirty data, \nprocessed it into 64 numerical features, normalized it, used \nthe matplotlib library to visualize images, reshaped it into \n8x8 image data as shown in Fig. 6 and Fig. 7, and finally \nclassified it using a model. \nTable 1 presents  the positive and negative sample \nclassification of CIC IDS2017 [33] and UNSW-NB15 [34] \ndatasets. While Fig. 5 shows the distribution of various data \ntypes in these datasets. \nTABLE 2. The number of positive and negative samples comes from \nCIC IDS2017 and UNSW-NB15 datasets \n \nDataset Benign Malicious \nUNSW-NB15 56000 119341 \nCIC IDS2017 97718 127993 \nB. EVALUATION \nThere are several metrics available to evaluate NIDS \nperformance, including False Positive (FP), False Negative \n(FN), True Negative (TN), and True Positive (TP). We will \nuse the following metrics to test NIDS based on these \nparameters: \nPrecision (P): This metric calculates the percentage of \nattack samples correctly identified out of all samples \nclassified as attacks. \nğ‘ƒ = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ (17) \n \nRecall Rate (RC): This metric calculates the percentage of \nattack samples that were correctly identified across the entire \nattack sample \nğ‘…ğ¶ = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ + ğ¹ğ‘ (18) \n \nAccuracy (ACC): This represents the percentage of \ncorrectly classified samples among all normal and abnormal \nerror instances. \nğ´ğ¶ğ¶ = ğ‘‡ğ‘ + ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ + ğ‘‡ğ‘ƒ + ğ¹ğ‘ + ğ¹ğ‘ƒ (19) \n \nFalse positive Rate (FPR): This measures the proportion \nof normal samples judged as attacks among all normal \nsamples. \nğ¹ğ‘ƒğ‘… = ğ¹ğ‘ƒ\nğ¹ğ‘ƒ + ğ‘‡ğ‘ (20) \nC. EXPERIMENTAL RESULTS  \nThe DE-VIT model has an input image size of 8 Ã— 8, with a \npatch size of 3 Ã— 3, 12 heads for multi -head self -attention, \nand a hidden layer size of 768. It consists of 12 layers and an \nMLP size of 3072. The deformable attention mechanism \nsamples 8 points surrounding each reference point, and the \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \ndeformable convolutional kernel size is 3 Ã— 3 with a stride \nof 1; therefore, the number of patches is 36. The learning rate \nuses a cosine annealing decay strategy.  \nIn the L-Focal loss function, ğ›¼ = 0.65ï¼Œğ›¾1 = 2ï¼Œğ›¾2 = 3, \nand ğ‘ = 0.3. The model is compared with a regular VIT \nmodel in experiments, The structural improvements are \nshown in Table 2. \nTABLE 3. The final accuracy, false positive rates and recall rates \n \n VIT DE-VIT \nDataset ACC FPR Recall ACC FPR Recall \nUNSW-\nNB15 \n95% 4.2% 93.6% 97.2% 1.4% 94.89% \nCIC \nIDS2017 \n97.4% 1.9% 96.6% 99.5% 0.3% 99.3% \n \nDuring the study, the data was classified as benign and \nmalicious. The dataset was divided into 20% for testing, 10% \nfor validation, and 70% for training. Fig. 9 shows the training \ncurve of the DE -VIT model on the CIC IDS2017 dataset, \nwhile Table 3 presents the False Positive Rate (FPR) and \nAccuracy (ACC) results for both the CIC IDS2017 [33] and \nUNSW-NB15 [34] datasets. \nBased on the experimental results, it has been \ndemonstrated that DE -VIT outperforms VIT on various \ndatasets regarding recall rate, accuracy, and precision. \nTherefore, DE-VIT is considered a superior model compared \nto VIT. In addition, the results suggest that DE -VIT can \nprovide better performance in various computer vision tasks. \nThese findings are significant for resear chers and \npractitioners in computer vision, as they can utilize DE -VIT \nto improve their models' accuracy and enhance their \napplications' performance. \n \nFIGURE 9. DE-VIT training curve on CIC IDS2017 dataset. \nD. BINARY CLASSIFICATION EXPERIMENT \nTable 4 provides a comparison of our proposed method with \nother NIDS methods that use machine learning or deep \nlearning for binary classification tasks , such as BP [35], \nCNN [36], SVM [25], DBN [35], Deep Belief Network with \nImproved Kernel -Based Extreme Learning (DBN-KELM) \n[35], RNN [24], and LSTM [24]. The confusion matrix for   \nTABLE 4. Comparison with other methods for binary classification.  \n \nDataset UNSW-NB15 CIC IDS2017 \n Precision Accuracy Precision Accuracy \nBP [35] 59.90% 70.89% 63.24% 72.56% \nCNN [36] 81.29% 80.11% 83.49% 81.22% \nSVM [25] 75.2% 81.5% 74.5% 98.9% \nDBN [35] 73.13% 81.69% 75.68% 82.39% \nDBN-KELM [35] 80.61% 88.21% 83.69% 90.95% \nRNN [24]   95.86% 95.83% \nLSTM [24]   95.82% 95.79% \nVIT   92.1% 95.0% 97.5% 97.4% \nDE-VIT 97.1% 97.25% 99.59% 99.5% \n \nbinary classification on the UNSW -NB15 [33] and CIC \nIDS2017 [34] datasets for both VIT and DE -VIT \narchitectures is shown in Fig. 11 and Fig. 1 2, respectively. \nWith the utilization of 20% of the data for testing. \n \n \n \nFIGURE 10. The Graphical comparison accuracy ratio of binary \nclassification. \n \nFurthermore, the accuracy comparison graph for binary \nclassification is presented in Fig. 10. We found that our \nmethod utilizing the DE-VIT classifier outperforms all other \nmethods in terms of accuracy and precision on both the \nUNSW-NB15 [34] and CIC IDS2017  [33] datasets. \nSpecifically, we achieved 99.5% accuracy on the CIC \nIDS2017 dataset and 97.25% on the UNSW -NB15 dataset. \nAdditionally, our VIT model utilizing this method also \nshowed promising results.  \nDE-VIT can outperform other algorithms ; firstly , the \nsuperior performance of DE -VIT in binary classification \nprimarily stems from the optimization of the model. In Part \nII, we conducted sufficient research on related work, \ndiscussing the shortcomings of most current deep learning \nalgorithms and highlig hting the advantages of our adopted \ndeformable attention mechanism module. Additionally, we \nupgraded the model's feature extraction module by \nemploying deformable convolutions, which better fulfills the  \n0.00%\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\nThe Graphical comparison accuracy ratio of \nbinary classification.\nUNSW-NB15 CIC IDS2017\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nThe confusion matrix using DE-ViT as classifier\n The confusion matrix using ViT as classifier\n \nFIGURE 11. The confusion matrices using the VIT and DE-VIT architectures for binary classification on the UNSW -NB15 dataset \nThe confusion matrix using DE-ViT as classifier\n The confusion matrix using ViT as classifier\n \nFIGURE 12. The confusion matrices using the VIT and DE-VIT architectures for binary classification on the CIC IDS2017 dataset \nrequirements of feature extraction. Furthermore, we \noptimized the loss function to achieve a better balance in the \ndataset. Secondly, regarding the achieved accuracies of 99.5% \n(CIC IDS2017) and 97.25% (UNSW -NB15), the CIC \nIDS2017 dataset is relatively cle aner, and experts have \nestablished more reasonable feature extraction criteria based \non traffic indicators. Therefore, overall, the accuracy is \nhigher than that of the UNSW-NB15 dataset. \nTo visualize our findings, we have included the Receiver \nOperating Characteristic (ROC) curves for both the VIT and \nDE-VIT classifiers on both datasets in Fig. 13 and Fig. 14. \nOverall, Thanks to the optimization of the model structure by \nDE-VIT, satisfactory results are achieved on both datasets. \nOur research demonstrates the effectiveness of our approach \nin accurately classifying data in these two datasets. \nE. MULTI-CLASSIFICATION EXPERIMENT \nWe conducted multi -class classification experiments on the \nUNSW-NB15 and CIC IDS2017 datasets [33], which have \nimbalanced generated image datasets. In the UNSW -NB15 \ndataset [34], Normal accounts for 31% of the data, while in the \nCIC IDS2017 dataset, benign accounts for 43%. This indicates \nthat normal data types make up most, while attack data types \nare relatively rare. \nTable 5 displays the performance of the DE-VIT classifier \non the multi-class test set of the UNSW-NB15 dataset, which \nhas ten categories: Normal, Dos, Generic, Reconnaissance, \nAnalysis, Exploits, Fuzzers, Backdoor, Shellcode, and Worms. \nThe test set constitutes 20% of the entire dataset. Following \none-hot encoding of the categories, a multi -class regression \ntest is performed to ascertain the number of accurately \nclassified instances. Subsequently, evaluation metrics are \naggregated for comprehensive statistical analysis. Each data \ncategory's recall rate, precision, and accuracy are listed. As \nNormal samples have the largest quantity, the accuracy is  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nReceiver Operating Characteristic for DE-VIT Receiver Operating Characteristic for VIT\n \nFIGURE 13. Receiver Operating Characteristic Curve Classifiers in the UNSW -NB15 dataset \nReceiver Operating Characteristic for DE-VIT Receiver Operating Characteristic for VIT\n \nFIGURE 14. Receiver Operating Characteristic Curve Classifiers in the CIC IDS2017 dataset\nTABLE 5. Performance of the DE-VIT classifier on the test set for multi-class classification on the UNSW-NB15 dataset \n \n Normal Generic Exploits Fuzzers Dos \nNumbers  12936 7729 5132 2062 3089 \nRecall 96.6% 98.9% 94.9% 88.3% 89.4% \nPrecision 98.3% 96.9% 96.9% 90.2% 92.5% \nAccuracy 97.1% 95.1% 95.1% 92.4% 93.6% \n \nReconnaissance Analysis Backdoor Shellcode Worms \nNumbers  2496 577 483 378 44 \nRecall 97.1% 84.6% 85.4% 79.6% 70.2% \nPrecision 98.1% 82.5% 90.1% 71.1% 78.3% \nAccuracy 97.8% 89.7% 92.3% 80.6% 72.7% \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nTABLE 6. Comparison with other methods for multi-class classification. \n \n Precision Acc \n Normal Generic Exploits Fuzzers Dos  \nCNN [36] 82.1%                                75.8% 77.3% 81.6% 74.6% 80.51% \nDBN [35] 78.9% 90.2% 85.6% 64.4% 25.8% 68.6% \nDBN-KELM [35] 89.0% 92.3% 86.9% 65.2% 30.9% 75.6% \nVIT 93.6% 92.9% 91.1% 89.5% 90.3% 90.9% \nDE-VIT 98.3% 96.9% 96.9% 90.2% 92.5% 95.4% \n Reconnaissance Analysis Backdoor Shellcode Worms  \nCNN [36] 71.3% 75.2% 68.5% 65.5% 66.3%  \nDBN [35] 66.9% 25.8% 0.5% 7.9% 2.2%  \nDBN-KELM [35] 67.6% 36.8% 0.0% 11.3% 2.5%  \nVIT 85.3% 77.4% 69.6% 73.2% 70.6%  \nDE-VIT 98.1% 82.5% 90.1% 71.1% 78.3%  \nhighest, whereas categories with fewer samples have slightly \nlower accuracy. Table 6 compares the overall accuracy and \ncategory precision of DE -VIT with other multi -class \nalgorithms on the UNSW -NB15 dataset. DE -VIT has an \naccuracy 15% higher than the second -ranked CNN and \nsignificantly outperforms other algorithms. \nTable 7 shows the performance of the DE-VIT classifier on \nthe multi-class test set of the CIC IDS2017 dataset, which has \nnine categories: Benign, Dos, Heartbleed, DDos, Patator, Web \nattack, Bot, PortScan, and Infiltration. The recall rate, \nprecision, and accuracy of each data category are listed, and it \ncan be seen that each category has a higher accuracy. Table 8 \ncompares the overall accuracy and category precision of DE-\nVIT with other multi -class algorithms on the CIC IDS2017 \ndataset [33]. The overall accuracy is 6.5% higher than the \nsecond-ranked DBN -KELM [35] and superior  to other \nalgorithms. However, precision and accuracy decrease rapidly \nin categories with relatively few samples. Although we have \ntaken measures to address the imbalance issue in the data and \nachieved fairly good results, it is not perfect and still needs \nfurther improvement. \nV. CONCLUSION \nThis paper presents a cutting -edge approach, DE -VIT, to \naddress the challenge of intrusion detection. By transforming \nintrusion detection data into image data, DE -VIT employs \nimage classification algorithms to resolve the issue effectively. \nThe method refines the original VIT and incorporates a more \nadvanced deformable attention mechanism instead of self -\nattention. An enhanced sliding window mechanism is \nintroduced to optimize the model further for superior edge \ninformation extraction . At the same time,  deformable \nconvolution is utilized to extract edge information and expand \nthe receptive field of each patch. Sine and cosine positional \nencoding are integrated to represent diverse features at varying \npositions. This results  in a texture imbued with position \ninformation as the position embedding changes gradually with \nthe dimension sequence number. \nDE-VIT introduces an innovative deformable attention \nmechanism module that selects the position of key-value pairs \nin a data -dependent manner. This agile solution enables the \ndeformable attention mechanism module to concentrate on \npertinent areas and captu re more information features, \ncircumventing excessive memory and computational costs. To \ntackle the issue of imbalanced datasets, a layered focal loss \nfunction, dubbed the L -Focal loss function, is proposed to \ncenter attention on challenging -to-classify samples, thereby \nenhancing classification accuracy. \nExperimental results demonstrate DE -VIT's superiority \nover other algorithms. In binary classification experiments, we \nattained 99.5% accuracy on the CIC IDS2017 dataset [33] and \n97.25% on the UNSW -NB15 dataset [34], outperforming \nmost mainstream algorithms such as CNN, LSTM, and DBN-\nKELM. Our algorithm surpassed others in multi-classification \nexperiments, achieving commendable accuracy and precision \nacross most categories. The accuracy of small samples did not \nsignificantly decline under the constraint of the loss function, \nculminating in satisfactory experimental results. \nSeveral promising research directions emerge based on the \nDE-VIT approach proposed in this paper. Firstly, exploring its \napplicability in diverse image -based classification tasks \nbeyond intrusion detection, such as medical imaging or \nsatellite analysis, can provide valuable insights into its \nversatility. Additionally, refining the deformable attention \nmechanism for dynamic focus adjustment and incorporating \nreinforcement learning techniques can enhance adaptability. \nExtending the application of the L-Focal loss function beyond \nimbalanced datasets or investigating novel loss functions can \nimprove model robustness.  Moreover, integrating DE -VIT \nwith advanced techniques like graph neural networks or self- \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nTABLE 7. Performance of the DE-VIT classifier on the test set for multi-class classification on the CIC IDS2017 dataset  \n \n Benign Dos Heartbleed DDos Patator \nNumbers  15525 9524 7238 3424 2941 \nRecall 97.5% 97.9% 98.1% 96.3% 98.6% \nPrecision 99.0% 98.3% 97.5% 95.0% 98.9% \nAccuracy 98.8% 98.2% 98.6% 97.6% 98.8% \n \nWeb attack Bot PortScan Infiltration \n \nNumbers  2344 2045 1549 552  \nRecall 92.1% 89.4% 79.8% 75.6%  \nPrecision 93.5% 92.2% 81.3% 77.5%  \nAccuracy 92.4% 89.7% 87.5% 86.9%  \n \nTABLE 8. Comparison with other methods for multi-class classification. \n \n Precision Acc \n Benign Dos Heartbleed DDos Patator  \nDBN [35] 75.9% 80.6% 82.3% 69.2% 75.2% 84.9% \nDBN-KELM [35] 77.8% 86.4% 85.9% 62.4% 79.4% 90.8% \nVIT 95.6% 93.8% 92.3% 91.7% 95.5% 93.8% \nDE-VIT 99.0% 98.3% 97.5% 95.0% 98.9% 97.3% \n Web attack Bot PortScan Infiltration   \nDBN [35] 82.4% 64.7% 73.2% 73.4%   \nDBN-KELM [35] 79.9% 73.8% 76.4% 76.3%   \nVIT 90.2% 86.8 80.9% 73.6%   \nDE-VIT 93.5% 92.2% 81.3% 77.5%   \nsupervised learning methods may synergistically enhance \nintrusion detection performance. Exploring these directions \nhas the potential to advance state-of-the-art intrusion detection \nand beyond. \nREFERENCES \n[1] S. Yoo, J. Jo, B. Kim, and J. Seo, \"Hyperion: A visual analytics tool \nfor an intrusion detection and prevention system,\" IEEE Access, vol. \n8, pp. 133865-133881, 2020.  \n[2] P. V. Pandit, S. Bhushan, and P. V. Waje, \"Implementation of \nIntrusion Detection System Using Various Machine Learning \nApproaches with Ensemble learning,\" in 2023 International \nConference on Advancement in Computation & Computer \nTechnologies (InCACCT), 2023: IEEE, pp. 468-472. \n[3] R. Zhang, Y. Song, and X. Wang, \"Network Intrusion Detection \nScheme Based on IPSO-SVM Algorithm,\" in 2022 IEEE Asia-Pacific \nConference on Image Processing, Electronics and Computers (IPEC), \n2022: IEEE, pp. 1011-1014. \n[4] T. Wisanwanichthan and M. Thammawichai, \"A double -layered \nhybrid approach for network intrusion detection system using \ncombined naive bayes and SVM,\" IEEE Access, vol. 9, pp. 138432 -\n138450, 2021. \n[5] T. Y. Zhang, W. Chen, Y. X. Liu, and L. F. Wu, \"An intrusion \ndetection method based on stacked sparse autoencoder and improved \ngaussian mixture model,\" Computers & Security, vol. 128, May 2023, \nArt no. 103144. \n[6] J. Camacho, R. TherÃ³ n, J. M. GarcÃ­ a-GimÃ© nez, G. MaciÃ¡-FernÃ¡ ndez, \nand P. GarcÃ­ a-Teodoro, \"Group-wise principal component analysis for \nexploratory intrusion detection,\" IEEE Access, vol. 7, pp. 113081 -\n113093, 2019. \n[7] L. Mohammadpour, T. C. Ling, C. S. Liew, and A. Aryanfar, \"A \nsurvey of CNN-based network intrusion detection,\" Applied Sciences, \nvol. 12, no. 16, p. 8162, 2022. \n[8] C. Yin, Y. Zhu, J. Fei, and X. He, \"A deep learning approach for \nintrusion detection using recurrent neural networks,\" Ieee Access, vol. \n5, pp. 21954-21961, 2017. \n[9] G. Andresini, A. Appice, L. De Rose, and D. Malerba, \"GAN \naugmentation to deal with imbalance in imaging -based intrusion \ndetection,\" Future Generation Computer Systems, vol. 123, pp. 108 -\n127, 2021. \n[10] J. He, X. Wang, Y. Song, and Q. Xiang, \"A multiscale intrusion \ndetection system based on pyramid depthwise separable convolution \nneural network,\" Neurocomputing, vol. 530, pp. 48-59, 2023. \n[11] Y. Almaghthawi, I. Ahmad, and F. E. Alsaadi, \"Performance Analysis \nof Feature Subset Selection Techniques for Intrusion Detection,\" \nMathematics, vol. 10, no. 24, p. 4745, 2022. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n[12] A. Dosovitskiy et al., \"An image is worth 16x16 words: Transformers \nfor image recognition at scale,\" arXiv preprint arXiv:2010.11929, \n2020. \n[13] M. Y. Aldarwbi, A. H. Lashkari, and A. A. Ghorbani, \"The sound of \nintrusion: A novel network intrusion detection system,\" Computers \nand Electrical Engineering, vol. 104, p. 108455, 2022. \n[14] M. Hassan, M. E. Haque, M. E. Tozal, V. Raghavan, and R. Agrawal, \n\"Intrusion detection using payload embeddings,\" IEEE Access, vol. 10, \npp. 4015-4030, 2021. \n[15] M. R. Ayyagari, N. Kesswani, M. Kumar, and K. Kumar, \"Intrusion \ndetection techniques in network environment: a systematic review,\" \nWireless Networks, vol. 27, pp. 1269-1285, 2021. \n[16] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \"Deformable detr: \nDeformable transformers for end -to-end object detection,\" arXiv \npreprint arXiv:2010.04159, 2020. \n[17] J. Dai et al., \"Deformable convolutional networks,\" in Proceedings of \nthe IEEE international conference on computer vision, 2017, pp. 764-\n773.  \n[18] M. Mohammadi et al., \"A comprehensive survey and taxonomy of the \nSVM-based intrusion detection systems,\" Journal of Network and \nComputer Applications, vol. 178, p. 102983, 2021. \n[19] S. Singh, \"Poly Logarithmic Naive Bayes Intrusion Detection System \nUsing Linear Stable PCA Feature Extraction,\" Wireless Personal \nCommunications, vol. 125, no. 4, pp. 3117-3132, 2022. \n[20] L. Zou, X. Luo, Y. Zhang, X. Yang, and X. Wang, \"HC-DTTSVM: A \nNetwork Intrusion Detection Method Based on Decision Tree Twin \nSupport Vector Machine and Hierarchical Clustering,\" IEEE Access, \nvol. 11, pp. 21404-21416, 2023. \n[21] C. Zhang, W. Wang, L. Liu, J. Ren, and L. Wang, \"Three -branch \nrandom forest intrusion detection model,\" Mathematics, vol. 10, no. \n23, p. 4460, 2022. \n[22] G. Liu, H. Zhao, F. Fan, G. Liu, Q. Xu, and S. Nazir, \"An enhanced \nintrusion detection model based on improved kNN in WSNs,\" Sensors, \nvol. 22, no. 4, p. 1407, 2022. \n[23] P. Mishra, V. Varadharajan, U. Tupakula, and E. S. Pilli, \"A detailed \ninvestigation and analysis of using machine learning techniques for \nintrusion detection,\" IEEE communications surveys & tutorials, vol. \n21, no. 1, pp. 686-728, 2018. \n[24] A. S. Khan, Z. Ahmad, J. Abdullah, and F. Ahmad, \"A spectrogram \nimage-based network anomaly detection system using deep \nconvolutional neural network,\" IEEE Access, vol. 9, pp. 87079-87093, \n2021. \n[25] M. Lopez-Martin, A. Sanchez-Esguevillas, J. I. Arribas, and B. Carro, \n\"Network intrusion detection based on extended RBF neural network \nwith offline reinforcement learning,\" IEEE Access, vol. 9, pp. 153153-\n153170, 2021. \n[26] H. Xu, L. Sun, G. Fan, W. Li, and G. Kuang, \"A Hierarchical Intrusion \nDetection Model Combining Multiple Deep Learning Models With \nAttention Mechanism,\" IEEE Access, 2023. \n[27] T. Bilot, N. El Madhoun, K. Al Agha, and A. Zouaoui, \"Graph Neural \nNetworks for Intrusion Detection: A Survey,\" IEEE Access, 2023. \n[28] H. Yang and F. Wang, \"Wireless network intrusion detection based on \nimproved convolutional neural network,\" Ieee Access, vol. 7, pp. \n64366-64374, 2019. \n[29] T. Kim and W. Pak, \"Early Detection of Network Intrusions Using a \nGAN-Based One-Class Classifier,\" IEEE Access, vol. 10, pp. 119357-\n119367, 2022. \n[30] A. Vaswani  et al., \"Attention is all you need,\" Advances in neural \ninformation processing systems, vol. 30, 2017. \n[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training \nof deep bidirectional transformers for language understanding,\" arXiv \npreprint arXiv:1810.04805, 2018. \n[32] J. Spencer, H. Kahraman, and E. Beyersmann, \"Positional encoding of \nmorphemes in visual word recognition,\" Journal of Experimental \nPsychology: Learning, Memory, and Cognition, 2023. \n[33] I. Sharafaldin, A. H. Lashkari, and A. A. Ghorbani, \"Toward \ngenerating a new intrusion detection dataset and intrusion traffic \ncharacterization,\" ICISSp, vol. 1, pp. 108-116, 2018. \n[34] N. Moustafa and J. Slay, \"UNSW-NB15: a comprehensive data set for \nnetwork intrusion detection systems (UNSW -NB15 network data \nset),\" in 2015 military communications and information systems \nconference (MilCIS), 2015: IEEE, pp. 1-6. \n[35] Z. Wang, Y. Zeng, Y. Liu, and D. Li, \"Deep belief network integrating \nimproved kernel -based extreme learning machine for network \nintrusion detection,\" IEEE Access, vol. 9, pp. 16062-16091, 2021. \n[36] I. Al-Turaiki and N. Altwaijry, \"A convolutional neural network for \nimproved anomaly-based network intrusion detection,\" Big Data, vol. \n9, no. 3, pp. 233-252, 2021. \n[37] P. Mishra, V. Varadharajan, U. Tupakula, and E. S. Pilli, \"A detailed \ninvestigation and analysis of using machine learning techniques for \nintrusion detection,\" IEEE communications surveys & tutorials, vol. \n21, no. 1, pp. 686-728, 2018. \n[38] J. Spencer, H. Kahraman, and E. Beyersmann, \"Positional encoding of \nmorphemes in visual word recognition,\" Journal of Experimental \nPsychology: Learning, Memory, and Cognition, 2023. \n \nKan He  received his B.S. and M.S. degrees in \nautomation from the Shenyang Institute of Chemical \nTechnology. \nHe serves as an associate professor and master's \ntutor, and his research interests include information \nsecurity and artificial intelligence. Over the years, as \nthe project leader and the main completer, he has \nundertaken more than 20 horizontal and vertical \nscientific research projects; he has published many \npapers, two patents, and more than ten software \ncopyrights. \nWei Zhang is studying for a bachelor's degree in \nengineering from the School of Information \nEngineering, Shenyang University of Chemical \nTechnology. The main research directions are \nartificial intelligence and network security. \n \n \n \n \nXuejun Zong received his B.S. degree from the \nShenyang Institute of Chemical Technology in \n1991, majoring in automation. In 1996, he studied \nat LinkÃ¶ ping University, Sweden, majoring in \nmeasurement technology, and M.S. in computer \nscience and technology majors at Northeastern \nUniversity in 2003.  \nSince 2015, he has been a third-level professor \nat the School of Information Engineering, \nShenyang University of Chemical Technology. He \nis the director of the \"Liaoning Provincial Key \nLaboratory of Petrochemical Information Security .\" Since 2019, he has \nserved as the Dean of the Graduate School of Shenyang University of \nChemical Technology. He has long been engaged in teaching and scientific \nresearch in control theory, engineering, and industrial information security. \nAs the first person to complete the project, he won the second prize in the \nLiaoning Province Science and Technology Progress Award, the second \nprize in the China Petroleum and Chemical Industry Federation Science and \nTechnology Progress Award, and the second prize in the Shenyang Science \nand Technology Progress Award. He has four invention patents and seven \nsoftware copyrights. He has published over 40 papers in SCI, EI, and \nChinese core journals. His research interests include control science and \nengineering, artificial intelligence, and industrial information security. \nLian Lian  received her Ph.D. degree from \nNortheastern University. \nAs a core Liaoning Provincial Key Laboratory \nof Petrochemical Information Security and \nIndustrial Control and Information Security \nTechnology Innovation Team member , she is \ncurrently engaged in scientific research in control \ntheory, control engineering, artificial intelligence, \nand industrial information security. In recent \nyears, she has presided over two provincial and \nhorizontal projects. She has published nine papers \nin international journals, core journals, and essential conferences at home \nand abroad. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3376434\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8607726097106934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6040269732475281
    },
    {
      "name": "Intrusion detection system",
      "score": 0.5883524417877197
    },
    {
      "name": "Transformer",
      "score": 0.4737352132797241
    },
    {
      "name": "Sliding window protocol",
      "score": 0.46496182680130005
    },
    {
      "name": "Embedding",
      "score": 0.4505692720413208
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4497910141944885
    },
    {
      "name": "Focus (optics)",
      "score": 0.44122976064682007
    },
    {
      "name": "Machine learning",
      "score": 0.43264222145080566
    },
    {
      "name": "Deep learning",
      "score": 0.431481271982193
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4184423089027405
    },
    {
      "name": "Data mining",
      "score": 0.3430160880088806
    },
    {
      "name": "Computer vision",
      "score": 0.3307077884674072
    },
    {
      "name": "Window (computing)",
      "score": 0.12575176358222961
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I48780066",
      "name": "Shenyang University of Chemical Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I166846921",
      "name": "Liaoning Shihua University",
      "country": "CN"
    }
  ]
}