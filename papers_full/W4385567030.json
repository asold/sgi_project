{
  "title": "Probing Structural Knowledge from Pre-trained Language Model for Argumentation Relation Classification",
  "url": "https://openalex.org/W4385567030",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108469540",
      "name": "Yang Sun",
      "affiliations": [
        "Novel (United States)",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1926107870",
      "name": "Bin Liang",
      "affiliations": [
        "Novel (United States)",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3103801846",
      "name": "Jianzhu Bao",
      "affiliations": [
        "Novel (United States)",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2038824294",
      "name": "Min Yang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099613179",
      "name": "Ruifeng Xu",
      "affiliations": [
        "Harbin Institute of Technology",
        "Novel (United States)",
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979666134",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2097794246",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2952597188",
    "https://openalex.org/W2252164999",
    "https://openalex.org/W3102577836",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3139812857",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W2951602301",
    "https://openalex.org/W3088366812",
    "https://openalex.org/W3034808961",
    "https://openalex.org/W3154430186",
    "https://openalex.org/W2759690420",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2964164368",
    "https://openalex.org/W2890465489",
    "https://openalex.org/W2963157208",
    "https://openalex.org/W3167369375",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3210828003",
    "https://openalex.org/W2252115865",
    "https://openalex.org/W3174902437",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2899315526",
    "https://openalex.org/W4283733706",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2399503976",
    "https://openalex.org/W3035152150",
    "https://openalex.org/W3175244212",
    "https://openalex.org/W3198003283"
  ],
  "abstract": "Extracting fine-grained structural information between argumentation component (AC) pairs is essential for argumentation relation classification (ARC). However, most previous studies attempt to model the relationship between AC pairs using AC level similarity or semantically relevant features. They ignore the complex interaction between AC pairs and cannot effectively reason the argumentation relation deeply.Therefore, in this paper, we propose a novel dual prior graph neural network (DPGNN) to jointly explore the probing knowledge derived from pre-trained language models (PLMs) and the syntactical information for comprehensively modeling the relationship between AC pairs. Specifically, we construct a probing graph by using probing knowledge derived from PLMs to recognize and align the relational information within and across the argumentation components. In addition, we propose a mutual dependency graph for the AC pair to reason the fine-grained syntactic structural information, in which the syntactical correlation between words is set by the dependency information within AC and mutual attention mechanism across ACs. The knowledge learned from the probing graph and the dependency graph are combined to comprehensively capture the aligned relationships of AC pairs for improving the results of ARC. Experimental results on three public datasets show that DPGNN outperforms the state-of-the-art baselines by a noticeable margin.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3605–3615\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nProbing Structural Knowledge from Pre-trained Language Model for\nArgumentation Relation Classification\nYang Sun1,2, Bin Liang1,2, Jianzhu Bao1,2, Min Yang3∗, and Ruifeng Xu1,2,4∗\n1 Harbin Institute of Technology, Shenzhen, China\n2 Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies\n3 SIAT, Chinese Academy of Sciences, Shenzhen, China\n4 Peng Cheng Laboratory, Shenzhen, China\nsy95@mail.ustc.edu.cn, bin.liang@stu.hit.edu.cn\njianzhubao@gmail.com, min.yang@siat.ac.cn, xuruifeng@hit.edu.cn\nAbstract\nExtracting fine-grained structural information\nbetween argumentation component (AC) pairs\nis essential for argumentation relation classi-\nfication (ARC). However, most previous stud-\nies attempt to model the relationship between\nAC pairs using AC level similarity or semanti-\ncally relevant features. They ignore the com-\nplex interaction between AC pairs and can-\nnot effectively reason the argumentation rela-\ntion deeply. Therefore, in this paper, we pro-\npose a novel dual prior graph neural network\n(DPGNN) to jointly explore the probing knowl-\nedge derived from pre-trained language mod-\nels (PLMs) and the syntactical information for\ncomprehensively modeling the relationship be-\ntween AC pairs. Specifically, we construct a\nprobing graph by using probing knowledge de-\nrived from PLMs to recognize and align the\nrelational information within and across the\nargumentation components. In addition, we\npropose a mutual dependency graph for the\nAC pair to reason the fine-grained syntactic\nstructural information, in which the syntactical\ncorrelation between words is set by the depen-\ndency information within AC and the mutual\nattention mechanism across ACs. The knowl-\nedge learned from the probing graph and the\ndependency graph are combined to comprehen-\nsively capture the aligned relationships of AC\npairs for improving the results of ARC. Exper-\nimental results on three public datasets show\nthat DPGNN outperforms the state-of-the-art\nbaselines by a noticeable margin.\n1 Introduction\nArgumentation relation classification (ARC) is the\nmost challenging subtask of argumentation mining,\nwhich requires the model to understand complex\nlinguistic interactions between argumentation com-\nponents (Lawrence and Reed, 2020). The goal of\nARC is to identify the argumentation relation (i.e.,\n∗ Min Yang and Ruifeng Xu are corresponding authors\nAR: Support\nAR: Attack\nAC 3: Electric cars should be a priority to fight global warming.\nAC 4: Public transportation is a better idea than electric cars.\nAC 1: Affirmative action is good public policy.\nAC 2: Diversity improves group decision-making. \nFigure 1: Examples of argumentation relation over AC\npairs , where the words with different colors represent\nthe structure within AC and the solid directed lines\ndenote the fine-grained alignment of structure within\nAC pair.\nSUPPORT or ATTACK) between argumentation com-\nponents (AC) pairs. As shown in Figure 1, there is\nan example with support relation and an example\nwith attack relation, where AC2 supports AC1 and\nAC4 attacks AC3.\nARC needs the model to effectively explore\nstructural knowledge within and cross ACs so as to\nbetter infer the argumentation relation between AC\npairs. Intuitively, the semantic structure can capture\nthe correlations among semantically similar words,\nwhile the syntactic structure can capture the syn-\ntactical constraint (e.g., dependency information)\nfor syntactically relevant words. Taking Figure 1\nas an example, the words “affirmative action” and\n“is good” in AC1 can be aligned with “diversity”\nand “improves” in AC2 respectively for identify-\ning the support relation. However, most previous\nefforts (Palau and Moens, 2009; Peldszus, 2014;\nCocarascu and Toni, 2017; Galassi et al., 2018)\nmerely focus on the AC-level similarity between\nAC pairs and result in sub-optimal performance.\nAlthough recent works (Gemechu and Reed, 2019;\nJo et al., 2021) model the fine-grained semantically\nrelevant features (such as words) between AC pair\nby introducing external knowledge, they ignore the\ncomplex interactions of AC pair and cannot effec-\ntively reason the argumentation relation deeply. For\ninstance, in Figure 1, previous works only empha-\n3605\nsize the superficial similarity between individual\nwords “better” and “priority” and fail to capture the\noverall opposition relation between AC3 and AC4\nsince “electric cars” is the object in AC3 but the\nsubject in AC4. To our best knowledge, the struc-\ntural knowledge within or across ACs has not been\nsimultaneously investigated for the Argumentation\nRelation Classification.\nIn this paper, we propose a dual prior graph\nneural network (DPGNN) to leverage two prior\nknowledge, i.e., dependency information and prob-\ning knowledge from pre-trained language models\n(PLMs), by constructing dual graph modules for\nARC. By combining the information from the two\ngraph modules, DPGNN can more accurately cap-\nture the fine-grained relations between AC pairs.\nSpecifically, we construct the mutual dependency\ngraph from two perspectives (i.e., intra-AC perspec-\ntive and inter-AC perspective) to gain the syntacti-\ncal structure information with dependency parsing\nand attention mechanisms. The intra-AC graph\nconstructs the syntactical structure within ACs in\nthe mutual graph, and the inter-AC graph aligns\nand builds the structure between AC pair to explore\nthe argumentation relation. Despite the effective-\nness of the learned dependency information, it is\ndifficult to recognize semantically relevant words\nsuch as synonyms and antonyms by only relying\non the dependency information.\nTo complement the dependency information for\nARC, we probe the relational knowledge from\nPLMs, such as BERT (Kenton and Toutanova,\n2019) and RoBERTa (Liu et al., 2020), which con-\ntain rich semantic knowledge for ARC, such as the\ncounterpart relationship between “public policy”\nand “group decision-making” as shown in Figure 1.\nConcretely, we first probe token representations\nand attention matrices from PLMs, where the to-\nken representations form the nodes of the probing\ngraph and the attention matrices form the edges\nbetween nodes. In particular, to obtain useful prob-\ning knowledge with respect to ARC, we propose\nthree different levels of probes1 (i.e., word-, AC-\nand pair-level) with adaptive attention mechanisms\nfor probing token representations and attention ma-\ntrices. The key idea behind this probing technique\nis motivated by the observation that the knowledge\nstored in PLMs contains rich linguistic and rela-\ntional knowledge (Petroni et al., 2019; Zhong et al.,\n1A probe is a simple neural network that develops the\nfeatures (i.e. hidden states and attention weights) from PLM\nfor specific task (Wu et al., 2020)\n2021) about words (e.g., synonyms and antonyms)\nin PLMs (Jawahar et al., 2019; Clark et al., 2019).\nWe disentangle each probed attention matrix into\nfour attention sub-matrices according to the span\nof each AC within AC pair to construct the probing\ngraph from intra-AC and inter-AC perspectives, re-\nspectively. Finally, we combine the graph represen-\ntations from the dependency graph and the probing\ngraph with a biaffine function (Morio et al., 2020)\nfor argumentation relation prediction.\nOur main contributions are three-fold. (1) We\npropose a dual prior graph neural network for ARC,\nwhich jointly explores the probed knowledge de-\nrived from PLMs and the syntactical information to\ncomprehensively model the relationship between\nAC pairs. (2) We probe rich relation knowledge\nfrom PLMs in terms of AC-pair level, which elicit\nrelations to robustly capture the semantic corre-\nspondences between ACs and AC pairs. (3) We\nconduct extensive experiments on three benchmark\nARC datasets. Experimental results show that our\nmethod significantly outperforms previous meth-\nods and achieves new state-of-the-art results.\n2 Related Work\n2.1 Argumentation Relation Classification\nEarly works (Palau and Moens, 2009; Wyner et al.,\n2010; Cabrio and Villata, 2012; Peldszus, 2014;\nPeldszus and Stede, 2015) focused on argumen-\ntative relation classification based on several dis-\ncrete features involving grammar and text statis-\ntics for ARC in specific corpus. Previous studies\nhave mostly employed traditional methods such\nas support vector machines, naive Bayes classi-\nfiers and maximum entropy classifiers. With the\nwidespread usage of deep learning, Cocarascu and\nToni (2017) proposed a deep learning architecture\nbased on Long-Short Term Memory (LSTM) net-\nworks for ARC. Galassi et al. (2018) explored resid-\nual networks for ARC. Previous research works\nonly focused on the content of argumentation com-\nponent pairs to determine relationships between AC\npairs. To capture more precise semantic similarity\nbetween AC pair, recent works develop external\nknowledge and designed feature for ARC. For ex-\nample, Gemechu and Reed (2019) designed four\nfine-grained features and identifies argumentation\nrelations by exploiting the similarity among the\nfour fine-grained features. Paul et al. (2020) ex-\ntracted the relevant knowledge from the general\nknowledge resource ConceptNet and encoded ACs\n3606\nand knowledge using two BiLSTM with attention\nmechanism for ARC. Jo et al. (2020) used BERT\nwith four characteristics regarding the sentence’s\ncontent, proposition types, tone, and an external\nengineering knowledge source for detecting attack-\nable sentences. Jo et al. (2021) classified argumen-\ntative relation based on multiple designed features\nincluding factual consistency, sentiment coherence,\ncausal relation and normative relation between two\nACs. In this paper, we emphasize the ARC task\nbased on the form of AC pairs. Different from pre-\nvious works, we do not explicitly introduce exter-\nnal knowledge, but integrate the probed knowledge\nelicited from PLMs to learn fine-grained reasoning\nfor ARC.\n2.2 Probing Knowledge from PLMs\nThe success of PLMs has led to a large number\nof studies eliciting the rich knowledge that PLMs\nlearn implicitly during pre-training (Jawahar et al.,\n2019; Clark et al., 2019; Wu et al., 2020). Some\nworks probe PLMs with a small amount of learn-\nable parameters considering a variety of linguistic\nproperties, such as morphology (Belinkov et al.,\n2017), word sense (Reif et al., 2019), syntax (He-\nwitt and Manning, 2019; Dai et al., 2021). There\nare also some works (Petroni et al., 2019; Zhong\net al., 2021) that seek to answer to what extent the\nPLMs store factual, relational and commonsense\nknowledge. Wang et al. (2022) elicited relational\nstructures from PLMs via a probing procedure and\nutilized the induced relations to augment the graph-\nbased text-to-SQL parsers for better schema link-\ning. Different from previous studies, we aim to\nprobe rich relational knowledge for ARC.\n2.3 Graph Neural Network for NLP\nThe recent success of graph neural networks (GNN)\nhas boosted research in natural language processing\n(NLP) tasks, such as fact verification (Zhong et al.,\n2020) and aspect-based sentiment analysis (Liang\net al., 2022). Recently, HARGAN (Huang et al.,\n2021) introduces the argumentation relation infor-\nmation for persuasiveness prediction with a GNN-\nbased model. The previous works do not fully\nexplore the fine-grained graph structure in ARC. In\nthis paper, we propose a dual prior GNN to align\nthe fine-grained structure information between AC\npair by introducing probing knowledge from PLM\nand dependency information.\n3 Methodology\nTask Definition Following previous work (Jo\net al., 2021), we assume an AC pair (P,Q) are\ngiven in an argumentative text, where the argu-\nmentation components P = (p1,p2,...,p m) and\nQ = (q1,q2,...,q n) consist of m and n tokens,\nrespectively. The goal of ARC is to predict the\nrelation type y(P,Q) (i.e., Support or Attack).\nModel Overview Figure 2 illustrates the architec-\nture of our DPGNN model. Our method leverages\ntwo complementary structural knowledge to con-\nstruct dual graphs for learning the alignment of\nfine-grained structures within ACs and between\nAC pairs for ARC. Concretely, we propose three\nprobes to elicit knowledge from hidden states and\nattention matrices in BERT. The probed hidden\nstates and attention matrices are employed to con-\nstruct a probing graph for reasoning the relation\nbetween the AC pair. In addition, we develop the\ndependency graph to gain the syntactical structure\ninformation with dependency parsing and attention\nmechanisms. Finally, a biaffine module is devised\nto combine the probing graph and the mutual de-\npendency graph for improving the performance of\nARC.\n3.1 Probing Knowledge from PLMs\n3.1.1 Probing Knowledge\nPre-trained language models (PLMs) contain a rich\nhierarchy of linguistic information in internal vec-\ntor representations (i.e., hidden states) (Jawahar\net al., 2019). The self-attention layers in BERT\ncontain not only long-distance dependencies be-\ntween words within each AC but also rich rela-\ntional knowledge (Clark et al., 2019) for reason-\ning. In this paper, we aim to probe hidden states\nand attention matrices in PLM (i.e., BEER) to cap-\nture the alignment of structures between each AC\npair. Specifically, we develop three probes from\nthe word-level, AC-level, and AC pair-level respec-\ntively, where each word, AC, and AC pair represent\nthe probing units (smallest units) for the three cor-\nresponding probes. The word-, AC- and AC pair-\nlevel probes elicit the hidden states and attention\nmatrices from PLMs.\nThe input of BERT is the AC pair (P,Q), which\nis formulated as “[CLS]P[SEP]Q[SEP]”. We de-\nfine the hidden states of the AC pair (P,Q) in\nthe BERT layers (i.e., 12 layers in the BERT-\nbase version) as B = {HB\n1 ,...,H B\n12}, where\n3607\nBERT\nͳʹൈ\nAttention \nMatrix \nProbe\nHidden \nState\nProbe\nAC pair (P,Q) \nInter-AC Graph\nIntra-AC \nGraph\nMutual Graph\nDependency \nGraph\nDependency \nParser\nBiaffine\nModule\n\b\n\b୔୕ \b୕୕\n\b୔୔ \b୕୔\nInter-AC \nAdjacency \nMatrices\nொ\n۵۲ۻ۵۾ \n\u0007\n1 =0\nFeed \nForward\nMulti-Head\nAttention\nHidden \nState\n(P, Q)\nAttention \nMatrix\nIntra-AC \nAdjacency \nMatrices\nFigure 2: The architecture of DPGNN, where \"PG\" and \"MDG\" represent probing graph and mutual dependency\ngraph, respectively.\nHB\ni = {hB\ni,1,..., hB\ni,m+n}is a concatenation of\nthe hidden states HP\ni = {hP\ni,1,..., hP\ni,m}and\nHQ\ni = {hQ\ni,1,..., hQ\ni,n}of P and Qrespectively.\nIn a similar way, we denote the attention ma-\ntrices in the BERT layers (i.e., 12 layers) as\nA= {EA\n1 ,...,E A\n12}with the input of the AC pair\n(P,Q), where EA\ni = {eA\ni,1,..., eA\ni,m+n}is the av-\nerage value of the original attention matrix along\nwith the head dimension and eA\ni,j ∈Rm+n. Here,\nEA\ni = [EP\ni ; EQ\ni ] where EP\ni = {eP\ni,1,..., eP\ni,m}\nand EQ\ni = {eQ\ni,1,..., eQ\ni,n}represent the attention\nmatrices of P and Qrespectively.\nAC Pair-Level Probe We propose an AC pair-\nlevel probe to capture the interaction and align-\nment between a pair of ACs by using the AC\npair as the probing unit. In particular, we first\ncalculate the representation for the AC pair as\nhAPL\ni = 1\nm\n∑m+n\nj=1 hB\ni,j in the i-th layer by apply-\ning the average pooling over HB\ni . In addition, we\nalso calculate the attention vector eAPL\ni for the AC\npair (P,Q) in the i-th layer via average pooling:\neAPL\ni = 1\nm\n∑m\nj=1 eA\ni,j.\nProbing Knowledge across Layers To adap-\ntively choose the important features across layers,\nwe apply a soft attention mechanism with two layer\nfeedback neural network (Wang et al., 2019) to\nlearn the combined representations and attention\nmatrices. Formally, we first calculate the learned\nweight αAPL\ni of representation hAPL\ni of AC pair\n(P,Q) in the i-th layer as follows:\nαAPL\ni = aα ·tanh(WαhAPL\ni + bα) (1)\nwhere Wα, bα and aα are learnable parameters.\nAfter that, we normalize the attention weights via\nsoftmax function and get the normalized coefficient\n˜αAPL\ni that is easily comparable across different lay-\ners. Although the coefficient ˜αAPL\ni is formulated\nby the AC pair representation, our goal is to gain\nthe fine-grained word representations. Thus, all\nwords in AC pair (P,Q) share the same coefficient\n˜αAPL\ni in the i-th layer. With the learned weights\nas coefficients, we fuse the representations of j-th\nword in all layers to obtain the probing representa-\ntion as follows:\nhj =\n12∑\ni=1\n˜αAPL\ni hB\ni,j (2)\nFinally, we obtain the probing representations\nHP = {hP\n1 ,..., hP\nm}and HQ = {hQ\n1 ,..., hQ\nn}\nof AC pair (P,Q). The similar equations 1-2 is\nalso applied in probing attention matrices, and we\nobtain the probing attention vectors of all words\nin AC pair to form the probing attention matrix\nE = {e1,..., em+n}, where ei ∈Rm+n.\n3.1.2 Probing Graph Construction\nWe construct the probing graph using probing\nknowledge including representations and an atten-\ntion matrix. The probing graph takes the unique\nwords in the AC pair as vertices and the embed-\ndings of the vertices are initialized with the prob-\ning representations. To effectively construct and\nalign the relational structure within ACs and be-\ntween AC pairs, we propose intra-AC and inter-AC\ngraphs by using the probing attention matrix to\nbuild the weighted edges within ACs and between\nAC pairs in the probing graph separately.\nThere are four attention sub-matrices in the prob-\ning attention matrix Eof AC pair (P,Q) as shown\nin the left part of Figure 2, which represent the word\n3608\ncorrelation within AC and between AC respectively.\nThus, we separate the probing attention matrix E\ninto four attention sub-matrices EPP ∈Rm×m,\nEQQ ∈Rn×n, EPQ ∈Rm×n and EQP ∈Rn×m\nof AC pair (P,Q) to build and align the relational\nstructure within ACs and between ACs, where\nEPP and EQQ are denoted by intra-AC adjacency\nmatrices, and EPQ and EQP are denoted by inter-\nAC adjacency matrices.\nIntra-AC Graph Construction Intuitively, the\nrelational structure within AC can build the correla-\ntion among semantically similar words within AC\nand help the alignment between AC pair for ARC.\nThus, we first design an intra-AC graph to build\nthe relational structure based on the intra-AC adja-\ncency matrix for each AC in the probing graph. The\nintra-AC adjacency matrix has captured semanti-\ncally related terms of each word in AC. Specifically,\ngiven the AC P, we built an intra-AC graph Gpro\nP\nwith the node representations HP. The intra-AC\nadjacency matrix EPP is set to the initial weighted\nedges to form the relational structure within the AC.\nThen we normalize the intra-AC adjacency matrix\nEPP of AC P via softmax function as the nor-\nmalized adjacency matrix ˜EPP = softmax(EPP )\nwithin AC P for easily comparable across differ-\nent words because the intra-AC adjacency matrices\nare segmented from an attention matrix: Then the\nupdated node representations SP = {sP\ni ,..., sP\nm}\ncan be obtained by the following equation:\nsP\ni = ˜EPP\ni HP (3)\nwhere ˜EPP\ni is the i-th row in ˜EPP . In this way,\nthe structure information within AC could be ex-\ntracted into the node representations via the intra-\nAC graph. The same equations can be applied for\nAC Qto acquire the updated node representations\nSQ = {sQ\ni ,..., sQ\nm}.\nInter-AC Graph Construction To explore the\ncomplex interaction and relationship between the\nAC pair, we utilize inter-AC adjacency matrices\nwith prior relational knowledge to build an intra-\nAC graph to align the fine-grained node represen-\ntations between AC pairs. Formally, given the\nintra-AC graph Gpro\nP as query and the intra-AC\ngraph Gpro\nQ as value, we produce the normalized\nadjacency matrix ˜EPQ = softmax(EPQ) for the\nquery and value pair. After that, we calculate Gpro\nP -\nspecific node representations CP = {cP\n1 ,..., cP\nm}\nare formulated as:\ncP\ni = ˜EPQ\ni SQ (4)\nwhere ˜EPQ\ni is the i-th row in ˜EPQ. We apply\nalignment function (Shen et al., 2018) to perform\nfine-grained node-to-node alignment and calculate\naligned node representations VP = {vP\n1 ,..., vP\nm}\nof intra-AC graph Gpro\nP , where vP\ni is calculated as:\nvP\ni = Wv[sP\ni ,cP\ni ,sP\ni −cP\ni ,sP\ni ⊙cP\ni ] (5)\nwhere Wv is a weight matrix and ⊙denotes\nelement-wise multiplication.\nSimilarly, we take the intra-AC graph Gpro\nQ as\nquery and the intra-AC graph Gpro\nP as value. By\nusing Equations 4-5, we can calculate the aligned\nnode representations VQ = {vQ\n1 ,..., vQ\nn}of the\nintra-AC graph Gpro\nQ . Then, we apply mean pool-\ning for the aligned vector VP and VQ to calculate\nthe relation-specific intra-AC graph representations\ngpro\nP for Gpro\nP and gpro\nQ for Gpro\nQ by:\ngpro\nP = 1\nm\nm∑\ni=1\nvP\ni , gpro\nQ = 1\nn\nn∑\nj=1\nvQ\nj (6)\n3.2 Mutual Dependency Graph Construction\nThe syntax is the grammatical structure of the text\n(e.g., dependency tree), whereas semantics repre-\nsent the meaning being conveyed. Thus, the syntac-\ntic structure can help the model capture the long-\nterm and syntactically relevant contextual words\nas clues to reason argumentation relations, which\nare difficult to be learned by semantic-based meth-\nods. To construct the syntactic structure within sen-\ntences, Liang et al. (2021a) propose a dependency\ngraph neural network, but they cannot capture the\ncomplex interaction between AC pairs. Thus, we\npropose a mutual dependency graph from the intra-\nAC perspective (i.e., dependency graph) and inter-\nAC perspective (i.e., mutual graph) which aims to\nbuild and align the syntactic structure within AC\nand between AC pairs by using syntactic depen-\ndency information and attention mechanism. We\nalso construct the mutual dependency graph with\nunique words in AC pair as vertices initialized with\nthe probing representations.\n3.2.1 Dependency Graph Construction\nThe goal of the dependency graph in the mutual\ndependency graph is to build a syntactic structure\nwithin AC by developing the syntactical depen-\ndency. Compared with the intra-AC graph in the\nprobing graph, the dependency graph only empha-\nsizes the crucial syntactic word relations and evades\nthe inconsequential ones in each AC, which can be\n3609\nDataset Train Dev Test\nDN 11098 472 707\nDC 6581 496 330\nPE 2697 362 773\nTable 1: The statistics of the evaluated datasets\na hard bound of structure within ACs to mutually\ncompensate the error word dependencies with the\nprobing graph.\nTo build the syntactic structure within each AC,\nwe construct dependency graph Gdep over the de-\npendency tree for each AC. Given a dependency\ngraph Gdep\nP with the node representations HP of\nAC P, the discrete adjacency matrix D ∈Rm×m\nfor Gdep\nP is a binary matrix and can be derived from\nthe dependency tree 2 where Di,j = 1represents\nthat i-th word is connected to j-th word in the de-\npendency tree of the AC.\nAfter that, we feed the node representations\nand the adjacency matrix D into the graph at-\ntention network (GAT) (Veli ˇckovi´c et al., 2018)\nto update the representation of each word node\nand erect the intrinsic structure by aggregating in-\nformation from its neighbors in Gdep\nP and Gdep\nQ .\nThen we achieve the updated node representations\nZP = {zP\n1 ,..., zP\nm}and ZQ = {zQ\n1 ,..., zQ\nn}\nwith syntactic structure for the nodes within AC P\nand Q.\n3.2.2 Mutual Graph Construction\nWe try to directly apply the inter-AC graph in the\nprobing graph for aligning the syntactic structure\nbetween AC pairs but find a decrease in model\nperformance. We suspect the reason is that an inter-\nAC perspective module is hard to capture heteroge-\nneous relational information between AC pairs. To\neffectively coordinate the syntactic structure infor-\nmation within ACs, we construct a mutual graph\nto align the AC pairs at a fine-grained level. To\nthe end, we employ a mutual attention mechanism\nattβ(·) with dot product to learn the importance\nscore βi,j for each node iin Gdep\nP from each node\njin Gdep\nQ as follows:\nβi,j = attβ(WPzP\ni ,WQzQ\nj ) (7)\nwhere WP and WQ are weight matrices. Then,\nwe normalize βi,j across all nodes in Gdep\nQ us-\ning the softmax function to get ˜βi,j. Next,\nthe Gdep\nP -specific node representations UP =\n2In this work, we use spaCy toolkit for generating depen-\ndency tree of the input sentence: https://spacy.io/.\n{uP\n1 ,..., uP\nm}is obtained by using the weighted\nsum over ZQ, where uP\ni is computed by uP\ni =∑n\nj=1 ˜βi,jzQ\nj . Finally, the alignment function\nand mean pooling operation in Equations 5-6 are\napplied to calculate the relation-specific depen-\ndency graph representations gdep\nP for Gdep\nP in here.\nThe similar procedures is processed to obtain the\nrelation-specific dependency graph representations\ngdep\nQ for Gdep\nQ from Gdep\nP .\n3.3 Biaffine Module\nTo harmonize the information from dual graphs,\nwe use the concatenation operation on the relation-\nspecific graph representations of the probing graph\nand mutual dependency graph to get the compre-\nhensive relation-specific graph representations rP\nand rQ of AC P and Q:\nrP = gpro\nP ||gdep\nP , rQ = gpro\nQ ||gdep\nQ (8)\nWe then apply a biaffine operation (Morio et al.,\n2020) to capture the bidirectional property of AC\npair and a softmax function to produce the relation\nlabel probability p(y):\np(y(P,Q)|rP,rQ) = softmax\n(\nϱ(rP,rQ)\n)\n(9)\nwhere y(P,Q) is the ground-truth relation label of\nAC pair, ϱ(x,y) = [x\n1 ]⊤Wϱy and Wϱ is learn-\nable weights.\n3.4 Loss Function\nOur training goal is to minimize the following total\nobjective function:\nL= −\n∑\nD\nlnp(y(P,Q)) +λ||θ||2 (10)\nwhere Ddenotes the training dataset, θrepresents\nall trainable parameters, and λis the coefficient of\nthe regularization term.\n4 Experimental Setup\n4.1 Datasets\nIn order to evaluate the performance of our\nDPGNN model, we conduct experiments on three\npublic benchmark datasets including debatepedia-\nnormative (DN), debatepedia-casual (DC) (Jo et al.,\n2021) and PE (Stab and Gurevych, 2017) and fol-\nlow their official train/dev/test split. The detailed\nstatistics of three datasets are shown in Table 1.\n3610\nModel DN DC\nACC Macro Support-F1 Attack-F1 ACC Macro Support-F1 Attack-F1\nBiLSTM 71.0 71.0 71.3 70.7 68.5 68.3 71.0 65.5\nLSTM-ATT 71.6 71.5 70.1 72.9 70.3 70.3 71.2 69.4\nHybrid Net 67.2 67.2 68.1 66.3 59.7 58.8 64.5 53.2\nBERT 79.1 79.4 79.8 79.0 80.7 80.7 81.4 79.9\nBERT+LX 78.4 78.4 79.2 77.5 81.6 81.5 82.3 80.8\nBERT+MT 79.6 79.6 80.0 79.1 77.6 77.5 77.5 78.9\nLogBERT 81.0 80.7 81.1 80.4 81.2 80.8 81.7 80.0\nDPGNN 82.9 82.9 82.3 83.5 84.2 84.1 85.6 82.6\nTable 2: Performance comparison on DN and DC datasets. Our improvements over baselines are statistically\nsignificant with p < 0.05.\nModel ACC Macro Support-F1 Attack-F1\nBiLSTM 93.8 55.5 96.8 14.2\nLSTM-ATT91.7 55.7 95.6 15.8\nHybrid Net 92.9 55.8 96.3 15.4\nBERT 93.3 60.0 96.5 23.5\nDPGNN 94.5 63.8 96.6 31.0\nTable 3: Performance comparison on PE dataset\n4.2 Evaluation Metrics\nWe apply the same evaluation metrics with previous\nworks (Bao et al., 2021; Jo et al., 2021; Liang et al.,\n2021b), including accuracy (ACC), per-class F1\n(denoted as Support-F1 and Attack-F1), and macro\naveraged score (denoted as Macro). Concretely, the\nmacro averaged score is calculated by averaging\nall the per-class F1 scores.\n4.3 Baselines\nWe compare our model with state-of-the-art base-\nlines:\n• BiLSTM (Cocarascu and Toni, 2017): This\nmodel optimizes ARC using two BiLSTMs to\nencode AC pair, respectively.\n• LSTM-ATT (Ma et al., 2017): It employs\ntwo LSTMs and an interaction attention to\ngenerate representations for AC pairs.\n• Hybrid-Net (Chen et al., 2018): It encodes\nthe input using BiLSTM and uses self- and\ncross-attention between words for ARC.\n• BERT (Kenton and Toutanova, 2019): This\nmodel uses vanilla BERT model by feeding\nthe AC pair and using the representation of\n[CLS] for predictions.\n• BERT+LX (Jo et al., 2021): This model\nemploys BERT as encoder and latent cross\nto incorporate external features, such as fac-\ntual consistency and sentiment coherence, for\nARC.\n• BERT+MT (Jo et al., 2021): It uses multi-\ntask learning to train the ARC and other logic\ntasks, such as textual entailment and sentiment\nclassification, simultaneously.\n• LogBERT (Jo et al., 2021): This model ap-\nplies BERT as encoder to pre-train in logic\ntasks and fine-tune on the target dataset for\nARC finally.\nFor the PE dataset, we do not compare our model\nwith BERT-LX, BERT-MT, and LogBERT since\nthey require a large number of external engineer-\ning features and annotations that cannot be easily\nacquired.\n4.4 Implementation Details\nWe use PyTorch to implement the proposed model\non an NVIDIA GeForce RTX 3080 GPU. We use\nthe uncased BERT base model3 as our PLM. Our\nmodel is optimized using AdaW (Loshchilov and\nHutter, 2018) with the learning rates of 1e-5 on\nthe BERT layers and 1e-3 on other layers on all\ndatasets. We set the size of word embedding as 768.\nThe default setup in probing representation and\nattention matrix is a pair-level probe in all datasets\nbecause of their excellent performance. For all\ndatasets, we set the batch size as 32 and the weight\ndecay λas 1e-3. We adopt dropout with a dropout\nrate of 0.1 to avoid overfitting. The training process\nstops if the accuracy score does not increase for 5\nepochs on the validation data. The code and data\nare available 4.\n3We implement BERT using huggingface toolkit:\nhttps://huggingface.co/\n4https://github.com/HITSZ-HLT/DPGNN\n3611\nModel DN DC\nMacro ∇ Macro ∇\nDPGNN 82.9 - 84.2 -\n-w/o MDG 81.6 -1.3 83.0 -1.2\n-w/o DI 82.3 -0.6 82.1 -2.1\n-w/o PG 81.9 -1.0 82.4 -1.8\n-w/o PR 82.4 -0.5 83.6 -0.6\nTable 4: The ablation results in terms of removing differ-\nent components in DPGNN on the DN and DC datasets\n5 Experimental Results\n5.1 Performance Comparison\nWe report the results of DPGNN and compared\nbaselines in Table 2. We can observe that our\nDPGNN model achieves the best performance on\nall the datasets. On the DC and DN datasets, our\nmodel outperforms the best performing baseline by\n2.6 and 2.6, and 1.9 and 2.2 on the ARC task re-\nspectively in terms of accuracy, per-class F1 score,\nand macro averaged score. In addition, we also\nobserve that LSTM-based baselines (i.e., BiLSTM,\nLSTM-ATT, and Hybrid Net) generally perform\nworse than BERT-based models. This may be be-\ncause the pre-trained BERT contains rich knowl-\nedge learned from the large-scale corpora. The\nBERT-based methods that integrate multiple ex-\nternal features (i.e., BERT+LX and LogBERT)\nachieve slightly better performance than the origi-\nnal BERT model. Furthermore, DPGNN performs\nbetter than all the BERT-based models by lever-\naging the probing knowledge from PLMs and de-\npendency knowledge to effectively capture the re-\nlational features between AC pairs. We observe\nsimilar trends on the PE dataset in Table 3.\n5.2 Ablation Study\nEffectiveness of Different Components To in-\nvestigate the effectiveness of different components\nin DPGNN, we conduct an ablation study in terms\nof removing the mutual dependency graph (w/o\nMDG), removing dependency information (w/o\nDI), removing the probing graph (w/o PG), and\nremoving probing representation (w/o PR) respec-\ntively. It is noteworthy that removing the probing\ngraph is equal to removing the probing attention\nmatrix. As shown in Table 4, the full model of\nDPGNN has the best performance. We also ob-\nserve that w/o MDG and w/o PG achieve similar\nperformance and perform worse than DPGNN, ver-\nifying the effectiveness of the probing knowledge\nand the dependency knowledge that can comple-\nment each other for capturing the fine-grained struc-\nProbe DN DC\nACC Macro ACC Macro\nWord-Level 81.6 81.6 82.7 82.6\nAC-Level 82.2 82.1 83.6 83.6\nAC Pair-Level 82.9 82.9 84.2 84.1\nTable 5: The ablation results in terms of applying differ-\nent probes in DPGNN on the DN and DC datasets\nAffirmative\naction\nis\ngood\npublic\npolicy\nDiversity\nimproves\ngroup\ndecision\n-\nmaking\n(a) intra-AC Graph\nAffirmative\naction\nis\ngood\npublic\npolicy\nDiversity\nimproves\ngroup\ndecision\n-\nmaking (b) Mutual Graph\nFigure 3: The attention maps of an example in Figure 1\nwhen reasoning the argumentation relation between AC1\nand AC2.\ntural information within ACs and between AC pairs.\nThe performance of DPGNN w/o DI slightly de-\ncreases on the DN dataset, while there is a huge\nperformance drop on the DC dataset. This may\nbe because the DN dataset relies on the relational\nstructure while the syntactic structure contributes\nmore to DC dataset including longer and more syn-\ntactically sensitive samples.\nEffectiveness of Different Probes We also ex-\nplore two additional probes, including the word-\nlevel probe and AC-level probe. The probing unit\nfor the word-level probe is individual words. Thus,\nwe treat each learned hidden state and attention vec-\ntor as word-level probing knowledge. The AC-level\nprobe leverages the global information of each AC\nto elicit the AC-level knowledge by applying the\naverage operation over the word representations to\nobtain the AC representation in each layer. Similar\nto AC pair-level probe, we can acquire the probing\nrepresentations and attention matrices of the AC\npair in the word- and AC-level probes. We report\nthe results of our DPGNN with the three different\nprobes. As shown in Table 5, the word-level probe\nalways performs worse than the AC-level and AC\npair-level probes on both DC and DN datasets. This\nmay be because the word-level probe does not cap-\nture the global information of each AC and each\nAC pair. By contrast, the pair-level probe achieves\nthe best performance among the three probes on the\n3612\ntwo datasets, since the pair-level probe can better\ncapture the established association of words over\nAC pair, rather than merely learn the knowledge\nwithin each AC.\n5.3 Case Study\nWe use a case study to visualize the attention maps\nof the inter-AC probing graph and the mutual de-\npendency graph when predicting the relation from\nAC2 to AC1. The attention maps are shown in Fig-\nure 1. The color depth indicates the importance\ndegree of the word. As shown in Figure 3a, the\nimportant words such as “affirmative” and “good”\nin AC1 are aligned with the words “diversity” and\n“improve” in AC2. Meanwhile, we can also ob-\nserve that the attention weights capture the impor-\ntant alignment between “public policy” and “de-\ncision making”. This verifies that the inter-AC\ngraph and mutual graph can align the rich structure\nbetween AC pairs. By combining heterogeneous\ninformation from dual graphs, our model can ob-\ntain comprehensive complementary information for\neffective ARC.\n6 Conclusion\nIn this paper, we proposed a graph-based model\nDPGNN with two prior knowledge, i.e., probing\nknowledge elicited from PLM and syntactical de-\npendency information, to model the relational and\nsyntactic structures within ACs and between AC\npairs for ARC. To effectively capture the useful\nprobing knowledge from BERT, we propose three\nprobes to elicit word-, AC- and pair-level knowl-\nedge. In addition, DPGNN integrated the probing\ngraph with decoupled probing attention matrices\nand the mutual dependency graph with syntactic de-\npendency information to make our model more ef-\nfective to utilize the heterogeneous structure within\nACs and between AC pairs. Experimental results\non three benchmark datasets demonstrated that\nDPGNN outperformed the strong baselines.\nAcknowledgements\nWe thank the anonymous reviewers for their\nvaluable suggestions to improve the quality\nof this work. This work was partially sup-\nported by the National Key R&D Program\nof China (No. 2019YFB2102500), the Na-\ntional Natural Science Foundation of China\n(62006062, 62176076), Shenzhen Foundational\nResearch Funding (JCYJ20200109113441941,\nJCYJ20210324115614039), The Major Key\nProject of PCL2021A06, Guangdong Provincial\nKey Laboratory of Novel Security Intelligence\nTechnologies (2022B1212010005).\nLimitations\nTo better understand the limitations of the proposed\nmodel, we carry out an analysis of the errors made\nby DPGNN. Specifically, we randomly select 100\ninstances that are incorrectly predicted by DPGNN\nand summarize the primary types of error. The first\ntype of error is caused by failing to classify ACs\nthat contain latent opinions or require deep compre-\nhension. For example, for an AC pair “Affirmative\naction is good public policy.” and “Predominantly\nblack schools offer fewer AP classes.” DPGNN\ntends to align “good public policy” with “fewer\nAP classes”, resulting in attack relation which is\nwrongly predicted.\nThe second error category is caused by vague\nwords. For example, DPGNN cannot correctly pre-\ndict the argumentation relation between the AC\npair High speed rail development is generally good\npolicy.” and ”Upgrading existing lines is an inef-\nfective solution.”, This may be because the con-\ntext information is not sufficient enough such that\nDPGNN cannot capture the opposite semantic be-\ntween ”High speed rail development” and ”Upgrad-\ning existing lines”.\nThird, another error category occurs when the\nAC pair exists with multiple aligned antonyms.\nThe argumentation relation is misled by multiple\naligned antonyms between the AC pair. For ex-\nample, the argumentation relation of the AC pair\n“Free trade and economic globalization is good for\nthe world” and “Protectionism is discriminatory” is\nwrongly predicted as an attack by considering the\ntwo antonymous alignments between “Free trade\nand economic globalization” and “Protectionism”,\nand between “good” and “discriminatory”. It sug-\ngests that certain alignment method needs to be\ndevised in the future so as to better infer argumen-\ntation relation. For example, we may leverage a\ngraph neural network over the AC-specific node\nrepresentations to guide the learning of relation-\nspecific features.\nReferences\nJianzhu Bao, Chuang Fan, Jipeng Wu, Yixue Dang, Ji-\nachen Du, and Ruifeng Xu. 2021. A neural transition-\nbased model for argumentation mining. In Proceed-\n3613\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6354–6364.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017. \"what do neural\nmachine translation models learn about morphology?\nIn ACL.\nElena Cabrio and Serena Villata. 2012. Generating\nabstract arguments: A natural language approach. In\nCOMMA, pages 454–461.\nDi Chen, Jiachen Du, Lidong Bing, and Ruifeng\nXu. 2018. Hybrid neural attention for agree-\nment/disagreement inference in online debates. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 665–\n670.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n276–286.\nOana Cocarascu and Francesca Toni. 2017. Identifying\nattack and support argumentative relations using deep\nlearning. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1374–1379.\nJunqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, and\nXipeng Qiu. 2021. Does syntax matter? a strong\nbaseline for aspect-based sentiment analysis with\nroberta. In EMNLP.\nAndrea Galassi, Marco Lippi, and Paolo Torroni. 2018.\nArgumentative link prediction using residual net-\nworks and multi-objective learning. In Proceedings\nof the 5th Workshop on Argument Mining, pages 1–\n10.\nDebela Gemechu and Chris Reed. 2019. Decomposi-\ntional argument mining: A general purpose approach\nfor argument graph construction. In 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 516–526. Association for Computa-\ntional Linguistics.\nJohn Hewitt and Christopher D Manning. 2019. A struc-\ntural probe for finding syntax in word representations.\nIn NAACL.\nKuo-Yu Huang, Hen-Hsen Huang, and Hsin-Hsi Chen.\n2021. Hargan: Heterogeneous argument attention\nnetwork for persuasiveness prediction. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 13045–13054.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does bert learn about the structure of\nlanguage? In ACL 2019-57th Annual Meeting of the\nAssociation for Computational Linguistics.\nYohan Jo, Seojin Bang, Emaad Manzoor, Eduard\nHovy, and Chris Reed. 2020. Detecting attack-\nable sentences in arguments. arXiv preprint\narXiv:2010.02660.\nYohan Jo, Seojin Bang, Chris Reed, and Eduard Hovy.\n2021. Classifying argumentative relations using logi-\ncal mechanisms and argumentation schemes. Trans-\nactions of the Association for Computational Linguis-\ntics, 9:721–739.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171–4186.\nJohn Lawrence and Chris Reed. 2020. Argument min-\ning: A survey. Computational Linguistics, 45(4):765–\n818.\nBin Liang, Yonghao Fu, Lin Gui, Min Yang, Jiachen Du,\nYulan He, and Ruifeng Xu. 2021a. Target-adaptive\ngraph for cross-target stance detection. In Proceed-\nings of the Web Conference 2021, pages 3453–3464.\nBin Liang, Hang Su, Lin Gui, Erik Cambria, and\nRuifeng Xu. 2022. Aspect-based sentiment anal-\nysis via affective knowledge enhanced graph con-\nvolutional networks. Knowledge-Based Systems ,\n235:107643.\nBin Liang, Rongdi Yin, Jiachen Du, Lin Gui, Yulan\nHe, Min Yang, and Ruifeng Xu. 2021b. Embed-\nding refinement framework for targeted aspect-based\nsentiment analysis. IEEE Transactions on Affective\nComputing.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017. Interactive attention networks for\naspect-level sentiment classification. arXiv preprint\narXiv:1709.00893.\nGaku Morio, Hiroaki Ozaki, Terufumi Morishita, Yuta\nKoreeda, and Kohsuke Yanai. 2020. Towards bet-\nter non-tree argument mining: Proposition-level bi-\naffine parsing with task-specific parameterization. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3259–\n3266.\nRaquel Mochales Palau and Marie-Francine Moens.\n2009. Argumentation mining: the detection, classifi-\ncation and structure of arguments in text. In Proceed-\nings of the 12th international conference on artificial\nintelligence and law, pages 98–107.\n3614\nDebjit Paul, Juri Opitz, Maria Becker, Jonathan Kobbe,\nGraeme Hirst, and Anette Frank. 2020. Argumenta-\ntive relation classification with background knowl-\nedge. In Computational Models of Argument, pages\n319–330. IOS Press.\nAndreas Peldszus. 2014. Towards segment-based recog-\nnition of argumentation structure in short texts. In\nProceedings of the First Workshop on Argumentation\nMining, pages 88–97.\nAndreas Peldszus and Manfred Stede. 2015. Towards\ndetecting counter-considerations in text. In Proceed-\nings of the 2nd Workshop on Argumentation Mining,\npages 104–109.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B.\nViégas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In NeurIPS.\nDinghan Shen, Xinyuan Zhang, Ricardo Henao, and\nLawrence Carin. 2018. Improved semantic-aware\nnetwork embedding with fine-grained word align-\nment. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1829–1838.\nChristian Stab and Iryna Gurevych. 2017. Parsing argu-\nmentation structures in persuasive essays. Computa-\ntional Linguistics, 43(3):619–659.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In International\nConference on Learning Representations.\nLihan Wang, Bowen Qin, Binyuan Hui, Bowen Li, Min\nYang, Bailin Wang, Binhua Li, Jian Sun, Fei Huang,\nLuo Si, et al. 2022. Proton: Probing schema linking\ninformation from pre-trained language models for\ntext-to-sql parsing. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and\nData Mining, pages 1889–1898.\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang\nYe, Peng Cui, and Philip S Yu. 2019. Heterogeneous\ngraph attention network. In The world wide web\nconference, pages 2022–2032.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting bert. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176.\nAdam Wyner, Raquel Mochales-Palau, Marie-Francine\nMoens, and David Milward. 2010. Approaches to\ntext mining arguments from legal cases. In Semantic\nprocessing of legal texts, pages 60–79. Springer.\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu,\nNan Duan, Ming Zhou, Jiahai Wang, and Jian Yin.\n2020. Reasoning over semantic-level graph for fact\nchecking. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6170–6180.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033.\n3615",
  "topic": "Argumentation theory",
  "concepts": [
    {
      "name": "Argumentation theory",
      "score": 0.8527399301528931
    },
    {
      "name": "Computer science",
      "score": 0.7174551486968994
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6173993349075317
    },
    {
      "name": "Graph",
      "score": 0.5639917850494385
    },
    {
      "name": "Natural language processing",
      "score": 0.5347112417221069
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5311160087585449
    },
    {
      "name": "Dependency graph",
      "score": 0.46422454714775085
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4401456415653229
    },
    {
      "name": "Relation (database)",
      "score": 0.4379333555698395
    },
    {
      "name": "Mutual information",
      "score": 0.4272667169570923
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4156380295753479
    },
    {
      "name": "Machine learning",
      "score": 0.3362812399864197
    },
    {
      "name": "Theoretical computer science",
      "score": 0.31982678174972534
    },
    {
      "name": "Data mining",
      "score": 0.2803013324737549
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210102458",
      "name": "Novel (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ],
  "cited_by": 9
}