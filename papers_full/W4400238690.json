{
  "title": "A Comprehensive Analysis of Various Tokenizers for Arabic Large Language Models",
  "url": "https://openalex.org/W4400238690",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3006884876",
      "name": "Faisal F. Qarah",
      "affiliations": [
        "Taibah University"
      ]
    },
    {
      "id": "https://openalex.org/A2794456465",
      "name": "Tawfeeq alsanoosy",
      "affiliations": [
        "Taibah University"
      ]
    },
    {
      "id": "https://openalex.org/A3006884876",
      "name": "Faisal F. Qarah",
      "affiliations": [
        "Taibah University"
      ]
    },
    {
      "id": "https://openalex.org/A2794456465",
      "name": "Tawfeeq alsanoosy",
      "affiliations": [
        "Taibah University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3166339217",
    "https://openalex.org/W2807629530",
    "https://openalex.org/W4377989177",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2998353611",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2905510703",
    "https://openalex.org/W3120253119",
    "https://openalex.org/W2189831162",
    "https://openalex.org/W6601894380",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W3183430109",
    "https://openalex.org/W2621199241",
    "https://openalex.org/W3095629350",
    "https://openalex.org/W6680762825",
    "https://openalex.org/W6759471380",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4389518703",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W4385572802",
    "https://openalex.org/W4385570293",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4366548693",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3167528419",
    "https://openalex.org/W3045600569",
    "https://openalex.org/W4385567046",
    "https://openalex.org/W4287888944",
    "https://openalex.org/W3155561744",
    "https://openalex.org/W3088728183",
    "https://openalex.org/W6746200681",
    "https://openalex.org/W2625475744",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W2948589047",
    "https://openalex.org/W6607331989",
    "https://openalex.org/W4206897013",
    "https://openalex.org/W2791757336",
    "https://openalex.org/W3119349118",
    "https://openalex.org/W3153962697",
    "https://openalex.org/W6763361438",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2250988285",
    "https://openalex.org/W785060174",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W1697700638",
    "https://openalex.org/W1728883788",
    "https://openalex.org/W3032746405",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2770803436"
  ],
  "abstract": "Pretrained language models have achieved great success in various natural language understanding (NLU) tasks due to their capacity to capture deep contextualized information in text using pretraining on large-scale corpora. Tokenization plays a significant role in the process of lexical analysis. Tokens become the input for other natural language processing (NLP) tasks, like semantic parsing and language modeling. However, there is a lack of research on the evaluation of the impact of tokenization on the Arabic language model. Therefore, this study aims to address this gap in the literature by evaluating the performance of various tokenizers on Arabic large language models (LLMs). In this paper, we analyze the differences between WordPiece, SentencePiece, and BBPE tokenizers by pretraining three BERT models using each tokenizer while measuring the performance of each model on seven different NLP tasks using 29 different datasets. Overall, the model pretrained with text tokenized using the SentencePiece tokenizer significantly outperforms the other two models that utilize WordPiece and BBPE tokenizers. The results of this paper will assist researchers in developing better models, making better decisions in selecting the best tokenizers, improving feature engineering, and making models more efficient, thus ultimately leading to advancements in various NLP applications.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8637176752090454
    },
    {
      "name": "Lexical analysis",
      "score": 0.7957371473312378
    },
    {
      "name": "Natural language processing",
      "score": 0.7535477876663208
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6790764331817627
    },
    {
      "name": "Parsing",
      "score": 0.6557585000991821
    },
    {
      "name": "Arabic",
      "score": 0.4939756989479065
    },
    {
      "name": "Process (computing)",
      "score": 0.4667017459869385
    },
    {
      "name": "Language model",
      "score": 0.461901992559433
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43182235956192017
    },
    {
      "name": "Linguistics",
      "score": 0.24089738726615906
    },
    {
      "name": "Programming language",
      "score": 0.0965329110622406
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}