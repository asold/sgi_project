{
  "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models",
  "url": "https://openalex.org/W3120253119",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3021587178",
      "name": "Phillip Rust",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2655736194",
      "name": "Jonas Pfeiffer",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140581490",
      "name": "Sebastian Ruder",
      "affiliations": [
        "University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3047738520",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2574640638",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2905510703",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3214161538",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3155682407",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3101601200",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W3105421296",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W2768172751",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3209051700",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W3101260801",
    "https://openalex.org/W2969250152",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2950342398",
    "https://openalex.org/W3088049945",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2806081754",
    "https://openalex.org/W3088382025",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2810095012",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W3033940819",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972340899",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2996580882"
  ],
  "abstract": "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3118–3135\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3118\nHow Good is Your Tokenizer?\nOn the Monolingual Performance of Multilingual Language Models\nPhillip Rust∗1†, Jonas Pfeiffer∗1,\nIvan Vuli´c2, Sebastian Ruder3, Iryna Gurevych1\n1Ubiquitous Knowledge Processing Lab, Technical University of Darmstadt\n2Language Technology Lab, University of Cambridge\n3DeepMind\nwww.ukp.tu-darmstadt.de\nAbstract\nIn this work, we provide a systematic and\ncomprehensive empirical comparison of pre-\ntrained multilingual language models versus\ntheir monolingual counterparts with regard to\ntheir monolingual task performance. We study\na set of nine typologically diverse languages\nwith readily available pretrained monolingual\nmodels on a set of ﬁve diverse monolingual\ndownstream tasks. We ﬁrst aim to establish,\nvia fair and controlled comparisons, if a gap\nbetween the multilingual and the correspond-\ning monolingual representation model of that\nlanguage exists, and subsequently investigate\nthe reason for any performance difference. To\ndisentangle conﬂating factors, we train new\nmonolingual models on the same data, with\nmonolingually and multilingually trained tok-\nenizers. We ﬁnd that while the pretraining data\nsize is an important factor, a designated mono-\nlingual tokenizer plays an equally important\nrole in the downstream performance. Our re-\nsults show that languages that are adequately\nrepresented in the multilingual model’s vocab-\nulary exhibit negligible performance decreases\nover their monolingual counterparts. We fur-\nther ﬁnd that replacing the original multilin-\ngual tokenizer with the specialized monolin-\ngual tokenizer improves the downstream per-\nformance of the multilingual model for almost\nevery task and language.\n1 Introduction\nFollowing large transformer-based language mod-\nels (LMs, Vaswani et al., 2017) pretrained on large\nEnglish corpora (e.g., BERT, RoBERTa, T5; Devlin\net al., 2019; Liu et al., 2019; Raffel et al., 2020),\nsimilar monolingual language models have been in-\ntroduced for other languages (Virtanen et al., 2019;\n∗Both authors contributed equally to this work.\n†PR is now afﬁliated with the University of Copenhagen.\nOur code is available at https://github.com/Adapter-Hub/hgiyt.\nAntoun et al., 2020; Martin et al., 2020, inter alia),\noffering previously unmatched performance in all\nNLP tasks. Concurrently, massively multilingual\nmodels with the same architectures and training\nprocedures, covering more than 100 languages,\nhave been proposed (e.g., mBERT, XLM-R, mT5;\nDevlin et al., 2019; Conneau et al., 2020; Xue et al.,\n2021).\nThe “industry” of pretraining and releasing new\nmonolingual BERT models continues its operations\ndespite the fact that the corresponding languages\nare already covered by multilingual models. The\ncommon argument justifying the need for mono-\nlingual variants is the assumption that multilingual\nmodels—due to suffering from the so-called curse\nof multilinguality (Conneau et al., 2020, i.e., the\nlack of capacity to represent all languages in an eq-\nuitable way)—underperform monolingual models\nwhen applied to monolingual tasks (Virtanen et al.,\n2019; Antoun et al., 2020; R¨onnqvist et al., 2019,\ninter alia). However, little to no compelling em-\npirical evidence with rigorous experiments and fair\ncomparisons have been presented so far to support\nor invalidate this strong claim. In this regard, much\nof the work proposing and releasing new mono-\nlingual models is grounded in anecdotal evidence,\npointing to the positive results reported for other\nmonolingual BERT models (de Vries et al., 2019;\nVirtanen et al., 2019; Antoun et al., 2020).\nMonolingual BERT models are typically eval-\nuated on downstream NLP tasks to demonstrate\ntheir effectiveness in comparison to previous mono-\nlingual models or mBERT (Virtanen et al., 2019;\nAntoun et al., 2020; Martin et al., 2020, inter alia).\nWhile these results do show that certain monolin-\ngual models can outperform mBERT in certain\ntasks, we hypothesize that this may substantially\nvary across different languages and language prop-\nerties, tasks, pretrained models and their pretrain-\ning data, domain, and size. We further argue that\n3119\nconclusive evidence, either supporting or refuting\nthe key hypothesis that monolingual models cur-\nrently outperform multilingual models, necessitates\nan independent and controlled empirical compari-\nson on a diverse set of languages and tasks.\nWhile recent work has argued and validated that\nmBERT is under-trained (R¨onnqvist et al., 2019;\nWu and Dredze, 2020), providing evidence of im-\nproved performance when training monolingual\nmodels on more data, it is unclear if this is the only\nfactor relevant for the performance of monolin-\ngual models. Another so far under-studied factor is\nthe limited vocabulary size of multilingual models\ncompared to the sum of tokens of all corresponding\nmonolingual models. Our analyses investigating\ndedicated (i.e., language-speciﬁc) tokenizers reveal\nthe importance of high-quality tokenizers for the\nperformance of both model variants. We also shed\nlight on the interplay of tokenization with other\nfactors such as pretraining data size.\nContributions. 1) We systematically compare\nmonolingual with multilingual pretrained language\nmodels for 9 typologically diverse languages on 5\nstructurally different tasks. 2) We train new mono-\nlingual models on equally sized datasets with differ-\nent tokenizers (i.e., shared multilingual versus ded-\nicated language-speciﬁc tokenizers) to disentangle\nthe impact of pretraining data size from the vocabu-\nlary of the tokenizer. 3) We isolate factors that con-\ntribute to a performance difference (e.g., tokenizers’\n“fertility”, the number of unseen (sub)words, data\nsize) and provide an in-depth analysis of the im-\npact of these factors on task performance. 4) Our\nresults suggest that monolingually adapted tokeniz-\ners can robustly improve monolingual performance\nof multilingual models.\n2 Background and Related Work\nMultilingual LMs.The widespread usage of pre-\ntrained multilingual Transformer-based LMs has\nbeen instigated by the release of multilingual BERT\n(Devlin et al., 2019), which followed on the success\nof the monolingual English BERT model. mBERT\nadopted the same pretraining regime as mono-\nlingual BERT by concatenating the 104 largest\nWikipedias. Exponential smoothing was used when\ncreating the subword vocabulary based on Word-\nPieces (Wu et al., 2016) and a pretraining corpus.\nBy oversampling underrepresented languages and\nundersampling overrepresented ones, it aims to\ncounteract the imbalance of pretraining data sizes.\nThe ﬁnal shared mBERT vocabulary comprises a\ntotal of 119,547 subword tokens.\nOther multilingual models followed mBERT,\nsuch as XLM-R (Conneau et al., 2020). Con-\ncurrently, many studies analyzed mBERT’s and\nXLM-R’s capabilities and limitations, ﬁnding that\nthe multilingual models work surprisingly well for\ncross-lingual tasks, despite the fact that they do not\nrely on direct cross-lingual supervision (e.g., par-\nallel or comparable data, translation dictionaries;\nPires et al., 2019; Wu and Dredze, 2019; Artetxe\net al., 2020; Hu et al., 2020; K et al., 2020).\nHowever, recent work has also pointed to some\nfundamental limitations of multilingual LMs. Con-\nneau et al. (2020) observe that, for a ﬁxed model\ncapacity, adding new languages increases cross-\nlingual performance up to a certain point, after\nwhich adding more languages results in perfor-\nmance drops. This phenomenon, termed the curse\nof multilinguality, can be attenuated by increas-\ning the model capacity (Artetxe et al., 2020; Pfeif-\nfer et al., 2020b; Chau et al., 2020) or through\nadditional training for particular language pairs\n(Pfeiffer et al., 2020b; Ponti et al., 2020). Another\nobservation concerns substantially reduced cross-\nlingual and monolingual abilities of the models\nfor resource-poor languages with smaller pretrain-\ning data (Wu and Dredze, 2020; Hu et al., 2020;\nLauscher et al., 2020). Those languages remain un-\nderrepresented in the subword vocabulary and the\nmodel’s shared representation space despite over-\nsampling. Despite recent efforts to mitigate this\nissue (e.g., Chung et al. (2020) propose to cluster\nand merge the vocabularies of similar languages,\nbefore deﬁning a joint vocabulary across all lan-\nguages), the multilingual LMs still struggle with\nbalancing their parameters across many languages.\nMonolingual versus Multilingual LMs. New\nmonolingual language-speciﬁc models also\nemerged for many languages, following BERT’s\narchitecture and pretraining procedure. There are\nmonolingual BERT variants for Arabic (Antoun\net al., 2020), French (Martin et al., 2020), Finnish\n(Virtanen et al., 2019), Dutch (de Vries et al.,\n2019), to name only a few. Pyysalo et al. (2020)\nreleased 44 monolingual WikiBERT models\ntrained on Wikipedia. However, only a few\nstudies have thus far, either explicitly or implicitly,\nattempted to understand how monolingual and\nmultilingual LMs compare across languages.\n3120\nNozza et al. (2020) extracted task results from\nthe respective papers on monolingual BERTs to\nfacilitate an overview of monolingual models and\ntheir comparison to mBERT.1 However, they have\nnot veriﬁed the scores, nor have they performed a\ncontrolled impartial comparison.\nVuli´c et al. (2020) probed mBERT and monolin-\ngual BERT models across six typologically diverse\nlanguages for lexical semantics. They show that\npretrained monolingual BERT models encode sig-\nniﬁcantly more lexical information than mBERT.\nZhang et al. (2020) investigated the role of pre-\ntraining data size with RoBERTa, ﬁnding that the\nmodel learns most syntactic and semantic features\non corpora spanning 10M–100M word tokens, but\nstill requires massive datasets to learn higher-level\nsemantic and commonsense knowledge.\nMulcaire et al. (2019) compared monolingual\nand bilingual ELMo (Peters et al., 2018) LMs\nacross three downstream tasks, ﬁnding that contex-\ntualized representations from the bilingual models\ncan improve monolingual task performance relative\nto their monolingual counterparts.2 However, it is\nunclear how their ﬁndings extend to massively mul-\ntilingual LMs potentially suffering from the curse\nof multilinguality.\nR¨onnqvist et al. (2019) compared mBERT to\nmonolingual BERT models for six languages\n(German, English, Swedish, Danish, Norwegian,\nFinnish) on three different tasks. They ﬁnd that\nmBERT lags behind its monolingual counterparts\nin terms of performance on cloze and generation\ntasks. They also identiﬁed clear differences among\nthe six languages in terms of this performance gap.\nThey speculate that mBERT is under-trained with\nrespect to individual languages. However, their set\nof tasks is limited, and their language sample is\ntypologically narrow; it remains unclear whether\nthese ﬁndings extend to different language families\nand to structurally different tasks.\nDespite recent efforts, a careful, systematic study\nwithin a controlled experimental setup, a diverse\nlanguage sample and set of tasks is still lacking.\nWe aim to address this gap in this work.\n3 Controlled Experimental Setup\nWe compare multilingual BERT with its monolin-\ngual counterparts in a spectrum of typologically\n1https://bertlang.unibocconi.it/\n2Mulcaire et al. (2019) clearly differentiate between mul-\ntilingual and polyglot models. Their deﬁnition of polyglot\nmodels is in line with what we term multilingual models.\ndiverse languages and across a variety of down-\nstream tasks. By isolating and analyzing crucial\nfactors contributing to downstream performance,\nsuch as tokenizers and pretraining data, we can\nconduct unbiased and fair comparisons.\n3.1 Language and Task Selection\nOur selection of languages has been guided by sev-\neral (sometimes competing) criteria: C1) typologi-\ncal diversity; C2) availability of pretrained mono-\nlingual BERT models; C3) representation of the\nlanguages in standard evaluation benchmarks for a\nsufﬁcient number of tasks.\nRegarding C1, most high-resource languages be-\nlong to the same language families, thus sharing\na majority of their linguistic features. Neglecting\ntypological diversity inevitably leads to poor gener-\nalizability and language-speciﬁc biases (Gerz et al.,\n2018; Ponti et al., 2019; Joshi et al., 2020). Fol-\nlowing recent work in multilingual NLP that pays\nparticular attention to typological diversity (Clark\net al., 2020; Hu et al., 2020; Ponti et al., 2020, in-\nter alia), we experiment with a language sample\ncovering a broad spectrum of language properties.\nRegarding C2, for computational tractability, we\nonly select languages with readily available BERT\nmodels. Unlike prior work, which typically lacks\neither language (R ¨onnqvist et al., 2019; Zhang\net al., 2020) or task diversity (Wu and Dredze,\n2020; Vuli´c et al., 2020), we ensure that our ex-\nperimental framework takes both into account, thus\nalso satisfying C3. We achieve task diversity and\ngeneralizability by selecting a combination of tasks\ndriven by lower-level syntactic and higher-level\nsemantic features (Lauscher et al., 2020).\nFinally, we select a set of 9 languages from 8\nlanguage families, as listed in Table 1.3 We evalu-\nate mBERT and monolingual BERT models on ﬁve\ndownstream NLP tasks: named entity recognition\n(NER), sentiment analysis (SA), question answer-\ning (QA), universal dependency parsing (UDP),\nand part-of-speech tagging (POS).4\n3Note that, since we evaluate monolingual performance\nand not cross-lingual transfer performance, we require train-\ning data in the target language. Therefore, we are unable to\nleverage many of the available multilingual evaluation data\nsuch as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al.,\n2020), or XNLI (Conneau et al., 2018). These evaluation sets\ndo not provide any training portions for languages other than\nEnglish. Additional information regarding our selection of\npretrained models is available in Appendix A.1.\n4Information on which datasets are associated with which\nlanguage and the dataset sizes (examples per split) are pro-\nvided in Appendix A.4.\n3121\nLanguage ISO Language Family Pretrained BERT Model\nArabic AR Afroasiatic AraBERT (Antoun et al., 2020)English EN Indo-European BERT (Devlin et al., 2019)Finnish FI Uralic FinBERT (Virtanen et al., 2019)IndonesianID Austronesian IndoBERT (Wilie et al., 2020)JapaneseJA Japonic Japanese-char BERT5\nKorean KO Koreanic KR-BERT (Lee et al., 2020)RussianRU Indo-European RuBERT (Kuratov and Arkhipov, 2019)Turkish TR Turkic BERTurk (Schweter, 2020)ChineseZH Sino-Tibetan Chinese BERT (Devlin et al., 2019)\nTable 1: Overview of selected languages and their re-\nspective pretrained monolingual BERT models.\nNamed Entity Recognition (NER).We rely on:\nCoNLL-2003 (Tjong Kim Sang and De Meulder,\n2003), FiNER (Ruokolainen et al., 2020), Chi-\nnese Literature (Xu et al., 2017), KMOU NER, 6\nWikiAnn (Pan et al., 2017; Rahimi et al., 2019).\nSentiment Analysis (SA). We employ: HARD\n(Elnagar et al., 2018), IMDb Movie Reviews\n(Maas et al., 2011), Indonesian Prosa (Purwari-\nanti and Crisdayanti, 2019), Yahoo Movie Re-\nviews,7 NSMC,8 RuReviews (Smetanin and Ko-\nmarov, 2019), Turkish Movie and Product Reviews\n(Demirtas and Pechenizkiy, 2013), ChnSentiCorp.9\nQuestion Answering (QA).We use: SQuADv1.1\n(Rajpurkar et al., 2016), KorQuAD 1.0 (Lim et al.,\n2019), SberQuAD (Eﬁmov et al., 2020), TQuAD,10\nDRCD (Shao et al., 2019), TyDiQA-GoldP (Clark\net al., 2020).\nDependency Parsing (UDP).We rely on Univer-\nsal Dependencies (Nivre et al., 2016, 2020) v2.6\n(Zeman et al., 2020) for all languages.\nPart-of-Speech Tagging (POS).We again utilize\nUniversal Dependencies v2.6.\n3.2 Task-Based Fine-Tuning\nFine-Tuning Setup. For all tasks besides UDP,\nwe use the standard ﬁne-tuning setup of Devlin\net al. (2019). For UDP, we use a transformer-based\nvariant (Glavaˇs and Vuli´c, 2021) of the standard\ndeep biafﬁne attention dependency parser (Dozat\nand Manning, 2017). We distinguish between fully\nﬁne-tuning a monolingual BERT model and fully\nﬁne-tuning mBERT on the task. For both settings,\nwe average scores over three random initializations\non the development set. On the test set, we report\n5https://github.com/cl-tohoku/bert-japanese\n6https://github.com/kmounlp/NER\n7 https://github.com/dennybritz/sentiment-analysis\n8 https://www.lucypark.kr/docs/2015-pyconkr/#39\n9https://github.com/pengming617/bert classiﬁcation\n10https://tquad.github.io/turkish-nlp-qa-dataset/\nLg Model NER SA QA UDP POSTest Test Dev Test TestF1 Acc EM / F1 UAS / LAS Acc\nAR Monolingual91.1 95.9 68.3/82.4 90.1/85.6 96.8mBERT 90.0 95.4 66.1 / 80.6 88.8 / 83.8 96.8\nEN Monolingual91.5 91.6 80.5 / 88.092.1/89.7 97.0mBERT 91.2 89.8 80.9/88.4 91.6 / 89.1 96.9\nFI Monolingual92.0 —– 69.9/81.6 95.9/94.4 98.4mBERT 88.2 —– 66.6 / 77.6 91.9 / 88.7 96.2\nID Monolingual 91.096.0 66.8 / 78.1 85.3 / 78.1 92.1mBERT 93.5 91.4 71.2/82.1 85.9/79.3 93.5\nJA Monolingual 72.488.0 —– / —– 94.7/93.0 98.1mBERT 73.4 87.8 —– / —– 94.0 / 92.3 97.8\nKO Monolingual88.8 89.7 74.2/91.1 90.3/87.2 97.0mBERT 86.6 86.7 69.7 / 89.5 89.2 / 85.7 96.0\nRU Monolingual91.0 95.2 64.3/83.7 93.1/89.9 98.4mBERT 90.0 95.0 63.3 / 82.6 91.9 / 88.5 98.2\nTR Monolingual 92.888.8 60.6/78.1 79.8/73.2 96.9mBERT 93.8 86.4 57.9 / 76.4 74.5 / 67.4 95.7\nZH Monolingual76.5 95.3 82.3/89.3 88.6/85.6 97.2mBERT 76.1 93.8 82.0 / 89.3 88.1 / 85.0 96.7\nAVGMonolingual87.4 92.4 70.8/84.0 90.0/86.3 96.9mBERT 87.0 91.0 69.7 / 83.3 88.4 / 84.4 96.4\nTable 2: Performance on Named Entity Recognition\n(NER), Sentiment Analysis (SA), Question Answering\n(QA), Universal Dependency Parsing (UDP), and Part-\nof-Speech Tagging (POS). We use development (dev)\nsets only for QA. Finnish ( FI) SA and Japanese ( JA)\nQA lack respective datasets.\nthe results of the initialization that achieved the\nhighest score on the development set.\nEvaluation Measures. We report F1 scores for\nNER, accuracy scores for SA and POS, unlabeled\nand labeled attachment scores (UAS & LAS) for\nUDP, and exact match andF1 scores for QA.\nHyper-Parameters and Technical Details.We\nuse AdamW (Kingma and Ba, 2015) in all experi-\nments, with a learning rate of3e−5.11 We train for\n10 epochs with early stopping (Prechelt, 1998).12\n11Preliminary experiments indicated this to be a well per-\nforming learning rate. Due to the large volume of our exper-\niments, we were unable to tune all the hyper-parameters for\neach setting. We found that a higher learning rate of 5e − 4\nworks best for adapter-based ﬁne-tuning (see later) since the\ntask adapter parameters are learned from scratch (i.e., they are\nrandomly initialized).\n12We evaluate a model every 500 gradient steps on the\ndevelopment set, saving the best-performing model based on\nthe respective evaluation measures. We terminate training if\nno performance gains are observed within ﬁve consecutive\nevaluation runs (= 2,500 steps). For QA and UDP, we use the\nF1 scores and LAS, respectively. For FI and ID QA, we train\nfor 20 epochs due to slower convergence. We train with batch\nsize 32 and max sequence length 256 for all tasks except QA.\nIn QA, the batch size is 24, max sequence length 384, query\nlength 64, and document stride is set to 128.\n3122\n3.3 Initial Results\nWe report our ﬁrst set of results in Table 2.13 We\nﬁnd that the performance gap between monolingual\nmodels and mBERT does exist to a large extent,\nconﬁrming anecdotal evidence from prior work.\nHowever, we also notice that the score differences\nare largely dependent on the language and task at\nhand. The largest performance gains of monolin-\ngual models over mBERT are found for FI, TR, KO,\nand AR. In contrast, mBERT outperforms the In-\ndoBERT (ID) model in all tasks except SA, and\nperforms competitively with the JA and ZH mono-\nlingual models on most datasets. In general, the\ngap is particularly narrow for POS tagging, where\nall models tend to score high (in most cases north of\n95% accuracy). ID aside, we also see a clear trend\nfor UDP, with monolingual models outperforming\nfully ﬁne-tuned mBERT models, most notably for\nFI and TR. In what follows, we seek to understand\nthe causes of this behavior in relation to different\nfactors such as tokenizers, corpora sizes, as well as\nlanguages and tasks in consideration.\n4 Tokenizer versus Corpus Size\n4.1 Pretraining Corpus Size\nThe size of the pretraining corpora plays an impor-\ntant role in the performance of transformers (Liu\net al., 2019; Conneau et al., 2020; Zhang et al.,\n2020, inter alia ). Therefore, we compare how\nmuch data each monolingual model was trained on\nwith the amount of data in the respective language\nthat mBERT has seen during training. Given that\nmBERT was trained on entire Wikipedia dumps,\nwe estimate the latter by the total number of words\nacross all articles listed for each Wiki. 14 For the\nmonolingual LMs, we extract information on pre-\ntraining data from the model documentation. If no\nexact numbers are explicitly stated, and the pretrain-\ning corpora are unavailable, we make estimations\nbased on the information provided by the authors.15\nThe statistics are provided in Figure 1a. For EN, JA,\nRU, and ZH, both the respective monolingual BERT\nand mBERT were trained on similar amounts of\nmonolingual data. On the other hand, monolingual\nBERTs of AR, ID, FI, KO, and TR were trained on\nabout twice (KO) up to more than 40 times (TR) as\nmuch data in their language than mBERT.\n13See Appendix Table 8 for the results on development sets.\n14Based on the numbers from\nhttps://meta.m.wikimedia.org/wiki/List of Wikipedias\n15We provide further details in Appendix A.2.\n4.2 Tokenizer\nCompared to monolingual models, mBERT is sub-\nstantially more limited in terms of the parameter\nbudget that it can allocate for each of its 104 lan-\nguages in its vocabulary. In addition, monolingual\ntokenizers are typically trained by native-speaking\nexperts who are aware of relevant linguistic phe-\nnomena exhibited by their target language. We\nthus inspect how this affects the tokenizations of\nmonolingual data produced by our sample of mono-\nlingual models and mBERT. We tokenize examples\nfrom Universal Dependencies v2.6 treebanks and\ncompute two metrics ( ´Acs, 2019).16 First, the sub-\nword fertility measures the average number of sub-\nwords produced per tokenized word. A minimum\nfertility of 1 means that the tokenizer’s vocabu-\nlary contains every single word in the text. We\nplot the fertility scores in Figure 1b. We ﬁnd that\nmBERT has similar fertility values as its mono-\nlingual counterparts for EN, ID, JA, and ZH. In\ncontrast, mBERT has a much higher fertility for\nAR, FI, KO, RU, and TR, indicating that such lan-\nguages are over-segmented. mBERT’s fertility is\nthe lowest for EN; this is due to mBERT having\nseen the most data in this language during training,\nas well as English being morphologically poor in\ncontrast to languages such as AR, FI, RU, or TR.17\nThe second metric we employ is the proportion\nof words where the tokenized word is continued\nacross at least two sub-tokens (denoted by contin-\nuation symbols ##). Whereas the fertility is con-\ncerned with how aggressively a tokenizer splits,\nthis metric measures how often it splits words. In-\ntuitively, low scores are preferable for both metrics\nas they indicate that the tokenizer is well suited to\nthe language. The plots in Figure 1c show similar\ntrends as with the fertility statistic. In addition to\nAR, FI, KO, RU, and TR, which already displayed\ndifferences in fertility, mBERT also produces a pro-\nportion of continued words more than twice as high\nas the monolingual model for ID.18\n16We provide further details in Appendix A.3.\n17The JA model is the only monolingual BERT with a fertil-\nity score higher than mBERT; its tokenizer is character-based\nand thus by design produces the maximum number of sub-\nwords.\n18We discuss additional tokenization statistics, further high-\nlighting the differences (or lack thereof) between the indi-\nvidual monolingual tokenizers and the mBERT tokenizer, in\nAppendix B.1.\n3123\nar en fi id ja ko ru tr zh0\n1\n2\n3\n4Pretraining corpus size [1B words]\n(a) Pretraining corpus size\nar en fi id ja ko ru tr zh0.0\n0.5\n1.0\n1.5\n2.0Fertility (b) Subword fertility\nar en fi id ja ko ru tr zh0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Proportion of continued words\nModel\nMono\nmBERT (c) Proportion of continued words\nFigure 1: Comparison of monolingual models with mBERT w.r.t. pretraining corpus size (measured in billions of\nwords), subword fertility (i.e., the average number of subword tokens produced per tokenized word ( ´Acs, 2019)),\nand proportion of continued words (i.e., words split into multiple subword tokens ( ´Acs, 2019)).\n4.3 New Pretrained Models\nThe differences in pretraining corpora and tok-\nenizer statistics seem to align with the variations\nin downstream performance across languages. In\nparticular, it appears that the performance gains of\nmonolingual models over mBERT are larger for\nlanguages where the differences between the re-\nspective tokenizers and pretraining corpora sizes\nare also larger ( AR, FI, KO, RU, TR vs. EN, JA,\nZH).19 This implies that both the data size and\nthe tokenizer are among the main driving forces of\ndownstream task performance. To disentangle the\neffects of these two factors, we pretrain new mod-\nels for AR, FI, ID, KO, and TR (the languages that\nexhibited the largest discrepancies in tokenization\nand pretraining data size) on Wikipedia data.\nWe train four model variants for each language.\nFirst, we train two new monolingual BERT models\non the same data, one with the original monolingual\ntokenizer (MONO MODEL -MONO TOK) and one with\nthe mBERT tokenizer (MONO MODEL -MBERTT OK).20\nSecond, similar to Artetxe et al. (2020), we re-\ntrain the embedding layer of mBERT, once with the\nrespective monolingual tokenizer (MBERTM ODEL -\nMONO TOK) and once with the mBERT tokenizer\n(MBERTM ODEL -MBERTT OK). We freeze the trans-\nformer and only retrain the embedding weights,\nthus largely preserving mBERT’s multilingual-\nity. The reason we retrain mBERT’s embed-\nding layer with its own tokenizer is to further\neliminate confounding factors when comparing\nto the version of mBERT with monolingually\nretrained embeddings. By comparing models\n19The only exception is ID, where the monolingual model\nhas seen signiﬁcantly more data and also scores lower on the\ntokenizer metrics, yet underperforms mBERT in most tasks.\nWe suspect this exception is because IndoBERT is uncased,\nwhereas the remaining models are cased.\n20The only exception isID; instead of relying on the uncased\nIndoBERT tokenizer by Wilie et al. (2020), we introduce a\nnew cased tokenizer with identical vocabulary size (30,521).\ntrained on the same amount of data, but with\ndifferent tokenizers ( MONO MODEL -MONO TOK vs.\nMONO MODEL -MBERTT OK, MBERTM ODEL -MBERTT OK\nvs. MBERTM ODEL -MONO TOK), we disentangle the\neffect of the dataset size from the tokenizer, both\nwith monolingual and multilingual LM variants.\nPretraining Setup.We pretrain new BERT mod-\nels for each language on its respective Wikipedia\ndump.21 We apply two preprocessing steps to\nobtain clean data for pretraining. First, we use\nWikiExtractor (Attardi, 2015) to extract text pas-\nsages from the raw dumps. Next, we follow\nPyysalo et al. (2020) and utilize UDPipe (Straka\net al., 2016) parsers pretrained on UD data to seg-\nment the extracted text passages into texts with\ndocument, sentence, and word boundaries.\nFollowing Liu et al. (2019); Wu and Dredze\n(2020), we only use the masked language mod-\neling (MLM) objective and omit the next sen-\ntence prediction task. Besides that, we largely\nfollow the default pretraining procedure by De-\nvlin et al. (2019). We pretrain the new monolin-\ngual LMs ( MONO MODEL -*) from scratch for 1M\nsteps.22 We enable whole word masking (Devlin\net al., 2019) for the FI monolingual models, follow-\ning the pretraining procedure for FinBERT (Virta-\nnen et al., 2019). For the retrained mBERT mod-\nels (MBERTM ODEL -*), we train for 250,000 steps\nfollowing Artetxe et al. (2020). 23 We freeze all\nparameters outside the embedding layer.24\nResults. We perform the same evaluations on\ndownstream tasks for our new models as described\n21We use Wiki dumps from June 20, 2020 (e.g., ﬁwiki-\n20200720-pages-articles.xml.bz2 for FI).\n22The batch size is 64; the sequence length is 128 for the\nﬁrst 900,000 steps, and 512 for the remaining 100,000 steps.\n23We train with batch size 64 and sequence length 512,\notherwise using the same hyper-parameters as for the mono-\nlingual models.\n24For more details see Appendix A.5.\n3124\nLg Model NER SA QA UDP POSTest Test Dev Test TestF1 Acc EM /F1 UAS / LAS Acc\nAR\nMonolingual 91.1 95.9 68.3/82.4 90.1/85.6 96.8\nMONOMODEL-MONOTOK 91.7 95.6 67.7/ 81.6 89.2/ 84.4 96.6MONOMODEL-MBERTTOK 90.0 95.5 64.1 / 79.4 88.8 / 84.097.0\nMBERTMODEL-MONOTOK 91.2 95.4 66.9/ 81.8 89.3/ 84.5 96.4\nMBERTMODEL-MBERTTOK 89.7 95.6 66.3 / 80.7 89.1 / 84.2 96.8\nmBERT 90.0 95.4 66.1 / 80.6 88.8 / 83.8 96.8\nFI\nMonolingual 92.0 —– 69.9/81.6 95.9/94.4 98.4\nMONOMODEL-MONOTOK 89.1 —– 66.9 / 79.5 93.7/ 91.5 97.3MONOMODEL-MBERTTOK 90.0 —– 65.1 / 77.0 93.6 / 91.597.0\nMBERTMODEL-MONOTOK 88.1 —– 66.4/ 78.3 92.4/ 89.6 96.6\nMBERTMODEL-MBERTTOK 88.1 —– 65.9 / 77.3 92.2 / 89.4 96.7\nmBERT 88.2 —– 66.6 / 77.6 91.9 / 88.7 96.2\nID\nMonolingual 91.0 96.0 66.8 / 78.1 85.3 / 78.1 92.1\nMONOMODEL-MONOTOK 92.5 96.0 73.1/ 83.6 85.0/ 78.5 93.9MONOMODEL-MBERTTOK 93.2 94.8 67.0 / 79.2 84.9 / 78.693.6\nMBERTMODEL-MONOTOK 93.9 94.6 74.1/83.8 86.4/80.2 93.8\nMBERTMODEL-MBERTTOK 93.9 94.6 71.9 / 82.7 86.2 / 79.6 93.7\nmBERT 93.5 91.4 71.2 / 82.1 85.9 / 79.3 93.5\nKO\nMonolingual 88.8 89.7 74.2/91.1 90.3/87.2 97.0\nMONOMODEL-MONOTOK 87.1 88.8 72.8/ 90.3 89.8/ 86.6 96.7MONOMODEL-MBERTTOK 85.8 87.2 68.9 / 88.7 88.9 / 85.6 96.4\nMBERTMODEL-MONOTOK 86.6 88.1 72.9/ 90.2 90.1/ 87.0 96.5\nMBERTMODEL-MBERTTOK 86.2 86.6 69.3 / 89.3 89.2 / 85.9 96.2\nmBERT 86.6 86.7 69.7 / 89.5 89.2 / 85.7 96.0\nTR\nMonolingual 92.8 88.8 60.6/78.1 79.8/73.2 96.9\nMONOMODEL-MONOTOK 93.4 87.0 56.2/ 73.7 76.1/ 68.9 96.3MONOMODEL-MBERTTOK 93.3 84.8 55.3 / 72.5 75.3 / 68.3 96.5\nMBERTMODEL-MONOTOK 93.7 85.3 59.4/ 76.7 77.1/ 70.2 96.3\nMBERTMODEL-MBERTTOK 93.8 86.1 58.7 / 76.6 76.2 / 69.2 96.3\nmBERT 93.8 86.4 57.9 / 76.4 74.5 / 67.4 95.7\nAVG\nMonolingual 91.1 92.6 68.0/82.3 88.3/83.7 96.2\nMONOMODEL-MONOTOK 90.8 91.9 67.3/ 81.7 86.8/ 82.0 96.2MONOMODEL-MBERTTOK 90.5 90.6 64.1 / 79.4 86.3 / 81.6 96.1\nMBERTMODEL-MONOTOK 90.7 90.9 68.0/ 82.2 87.1/ 82.3 95.9\nMBERTMODEL-MBERTTOK 90.3 90.7 66.4 / 81.3 86.6 / 81.7 95.9\nmBERT 90.4 90.0 66.3 / 81.2 86.1 / 81.0 95.6\nTable 3: Performance of our new MONO MODEL -* and\nMBERTM ODEL -* models (see §A.5) ﬁne-tuned for the\nNER, SA, QA, UDP, and POS tasks (see §3.1), com-\npared to the monolingual models from prior work and\nfully ﬁne-tuned mBERT. We group model counterparts\nw.r.t. tokenizer choice to facilitate a direct comparison\nbetween respective counterparts. We use development\nsets only for QA. Bold denotes best score across all\nmodels for a given language and task. Underlined de-\nnotes best score compared to its respective counterpart.\nin §3, and report the results in Table 3.25\nThe results indicate that the models trained with\ndedicated monolingual tokenizers outperform their\ncounterparts with multilingual tokenizers in most\ntasks, with particular consistency for QA, UDP,\nand SA. In NER, the models trained with multilin-\ngual tokenizers score competitively or higher than\nthe monolingual ones in half of the cases. Over-\nall, the performance gap is the smallest for POS\ntagging (at most 0.4% accuracy). We observe the\n25Full results including development set scores are available\nin Table 9 of the Appendix.\nlargest gaps for QA (6.1 EM / 4.4 F1 in ID), SA\n(2.2% accuracy in TR), and NER (1.7 F1 in AR).\nAlthough the only language in which the monolin-\ngual counterpart always comes out on top is KO,\nthe multilingual counterpart comes out on top at\nmost 3/10 times (for AR and TR) in the other lan-\nguages. The largest decrease in performance of a\nmonolingual tokenizer relative to its multilingual\ncounterpart is found for SA in TR (0.8% accuracy).\nOverall, we ﬁnd that for 38 out of 48 task, model,\nand language combinations, the monolingual tok-\nenizer outperforms the mBERT counterpart. We\nwere able to improve the monolingual performance\nof the original mBERT for 20 out of 24 languages\nand tasks by only replacing the tokenizer and, thus,\nleveraging a specialized monolingual version. Sim-\nilar to how the chosen method of tokenization af-\nfects neural machine translation quality (Domingo\net al., 2019), these results establish that, in fact,\nthe designated pretrained tokenizer plays a funda-\nmental role in the monolingual downstream task\nperformance of contemporary LMs.\nIn 18/24 language and task settings, the mono-\nlingual model from prior work (trained on more\ndata) outperforms its corresponding MONO MODEL -\nMONO TOK model. 4/6 settings in which our\nMONO MODEL -MONO TOK model performs better are\nfound for ID, where IndoBERT uses an uncased\ntokenizer and our model uses a cased one, which\nmay affect the comparison. Expectedly, these re-\nsults strongly indicate that data size plays a major\nrole in downstream performance and corroborate\nprior research ﬁndings (Liu et al., 2019; Conneau\net al., 2020; Zhang et al., 2020, inter alia).\n4.4 Adapter-Based Training\nAnother way to provide more language-speciﬁc ca-\npacity to a multilingual LM beyond a dedicated to-\nkenizer, thereby potentially making gains in mono-\nlingual downstream performance, is to introduce\nadapters (Pfeiffer et al., 2020b,c; ¨Ust¨un et al.,\n2020), a small number of additional parameters at\nevery layer of a pretrained model. To train adapters,\nusually all pretrained weights are frozen, while only\nthe adapter weights are ﬁne-tuned.26 The adapter-\nbased approaches thus offer increased efﬁciency\nand modularity; it is crucial to verify to which ex-\ntent our ﬁndings extend to the more efﬁcient and\n26Pfeiffer et al. (2020b) propose to stack task-speciﬁc\nadapters on top of language adapters and extend this approach\nin Pfeiffer et al. (2020c) by additionally training new embed-\ndings for the target language.\n3125\nLg Model NER SA QA UDP POSTest Test Dev Test TestF1 Acc EM /F1 UAS / LAS Acc\nAR\nmBERT 90.0 95.4 66.1 / 80.6 88.8/83.8 96.8+ ATask 89.6 95.6 66.7 / 81.1 87.8 / 82.696.8+ ATask+ ALang 89.795.766.9 / 81.0 88.0 / 82.896.8+ ATask+ ALang+MONOTOK 91.1 95.7 67.7/82.188.5 / 83.4 96.5\nFI\nmBERT 88.2 —– 66.6 / 77.6 91.9 / 88.7 96.2+ ATask 88.5—– 65.2 / 77.3 90.8 / 87.0 95.7+ ATask+ ALang 88.4 —– 65.7 / 77.1 91.8 / 88.5 96.6+ ATask+ ALang+MONOTOK 88.1 —– 66.7/79.0 92.8/90.1 97.3\nID\nmBERT 93.591.4 71.2 / 82.185.9/79.3 93.5+ ATask 93.590.6 70.6 / 82.5 84.8 / 77.4 93.4+ ATask+ ALang 93.593.6 70.8 / 82.2 85.4 / 78.1 93.4+ ATask+ ALang+MONOTOK 93.493.8 74.4/84.485.1 / 78.393.5\nKO\nmBERT 86.686.7 69.7 / 89.589.2/85.796.0+ ATask 86.2 86.5 69.8 / 89.7 87.8 / 83.9 96.2+ ATask+ ALang 86.2 86.3 70.0 / 89.8 88.3 / 84.3 96.2+ ATask+ ALang+MONOTOK 86.587.9 73.1/90.488.9 / 85.296.5\nTR\nmBERT 93.8 86.457.9 / 76.4 74.5 / 67.4 95.7+ ATask 93.0 83.9 55.3 / 75.1 72.4 / 64.1 95.7+ ATask+ ALang 93.5 84.8 56.9 / 75.8 73.0 / 64.7 95.9+ ATask+ ALang+MONOTOK 92.7 85.360.0/77.0 75.7/68.1 96.3\nAVG\nmBERT 90.490.0 66.3 / 81.2 86.0 /81.095.6+ ATask 90.2 89.2 65.5 / 81.1 84.7 / 79.0 95.6+ ATask+ ALang 90.3 90.1 66.1 / 81.2 85.3 / 79.7 95.8+ ATask+ ALang+MONOTOK 90.4 90.7 68.4/82.6 86.2/81.0 96.0\nTable 4: Performance on the different tasks leveraging\nmBERT with different adapter components (see §4.4).\nmore versatile adapter-based ﬁne-tuning setup.\nWe evaluate the impact of different adapter com-\nponents on the downstream task performance and\ntheir complementarity with monolingual tokenizers\nin Table 4.27 Here, +ATask and +ALang implies\nadding task- and language-adapters respectively,\nwhereas +MONO TOK additionally includes a new\nembedding layer. As mentioned, we only ﬁne-tune\nadapter weights on the downstream task, leveraging\nthe adapter architecture proposed by Pfeiffer et al.\n(2021). For the +ATask + ALang setting we lever-\nage pretrained language adapter weights available\nat AdapterHub.ml (Pfeiffer et al., 2020a). Lan-\nguage adapters are added to the model and frozen\nwhile only task adapters are trained on the target\ntask. For the +ATask +ALang+ MONO TOK we train\nlanguage adapters and new embeddings with the\ncorresponding monolingual tokenizer equally as de-\nscribed in the previous section (e.g. MBERTM ODEL -\nMONO TOK), task adapters are trained with a learning\nrate of 5e− 4 and 30 epochs with early stopping.\nResults. Similar to previous ﬁndings, adapters im-\nprove upon mBERT in 18/24 language, and task\nsettings, 13 of which can be attributed to the im-\nproved MBERTM ODEL -MONO TOK tokenizer. Figure 2\nillustrates the average performance of the different\nadapter components in comparison to the mono-\nlingual models. We ﬁnd that adapters with dedi-\ncated tokenizers reduce the performance gap con-\n27See Appendix Table 10 for the results on dev sets.\nNER SA UDP POS80\n90\n100Scores\nQA60\n65\n70\nModel\nMono\nmBERT\n+ ATask\n+ ATask + ALang\n+ ATask + ALang + MonoTok\nFigure 2: Task performance averaged over all lan-\nguages for different models: fully ﬁne-tuned mono-\nlingual ( Mono), fully ﬁne-tuned mBERT ( mBERT),\nmBERT with task adapter ( +ATask), with task and\nlanguage adapter ( +ATask + ALang), with task and\nlanguage adapter and embedding layer retraining\n(+ATask + ALang+ MONO TOK).\nsiderably without leveraging more training data,\nand even outperform the monolingual models in\nQA. This ﬁnding shows that adding additional\nlanguage-speciﬁc capacity to existing multilingual\nLMs, which can be achieved with adapters in a\nportable and efﬁcient way, is a viable alternative to\nmonolingual pretraining.\n5 Further Analysis\nAt ﬁrst glance, our results displayed in Table 2\nseem to conﬁrm the prevailing view that mono-\nlingual models are more effective than multilin-\ngual models (R¨onnqvist et al., 2019; Antoun et al.,\n2020; de Vries et al., 2019, inter alia). However,\nthe broad scope of our experiments reveals certain\nnuances that were previously undiscovered. Un-\nlike prior work, which primarily attributes gaps\nin performance to mBERT being under-trained\n(R¨onnqvist et al., 2019; Wu and Dredze, 2020),\nour disentangled results (Table 3) suggest that a\nlarge portion of existing performance gaps can be\nattributed to the capability of the tokenizer.\nWith monolingual tokenizers with lower fertil-\nity and proportion-of-continued-words values than\nthe mBERT tokenizer (such as for AR, FI, ID,\nKO, TR), consistent gains can be achieved, irre-\nspective of whether the LMs are monolingual (the\nMONO MODEL -* comparison) or multilingual (a com-\nparison of MBERTM ODEL -* variants).\nWhenever the differences between monolingual\nmodels and mBERT with respect to the tokenizer\nproperties and the pretraining corpus size are small\n(e.g., for EN, JA, and ZH), the performance gap is\ntypically negligible. In QA, we even ﬁnd mBERT\nto be favorable for these languages. Therefore, we\nconclude that monolingual models are not superior\nto multilingual ones per se, but gain advantage in\ndirect comparisons by incorporating more pretrain-\ning data and using language-adapted tokenizers.\n3126\nAll NER POS QA SA UDP\nTask\nCont. Proportion\nFertility\nPre-Train Size\nMetric\n0.36 0.28 0.09 0.69 0.37 0.48\n0.36 0.28 0.09 0.69 0.37 0.48\n0.38 0.21 0.50 0.62 0.39 0.65 −1\n0\n1\nFigure 3: Spearman’s ρ correlation of a relative de-\ncrease in the proportion of continued words (Cont. Pro-\nportion), a relative decrease in fertility, and a rela-\ntive increase in pretraining corpus size with a relative\nincrease in downstream performance over fully ﬁne-\ntuned mBERT. For the proportion of continued words\nand the fertility, we consider fully ﬁne-tuned mBERT,\nthe MONO MODEL -* models, and the MBERTM ODEL -*\nmodels. For the pretraining corpus size, we consider\nthe original monolingual models and the MONO MODEL -\nMONO TOK models. We exclude the ID models (see Ap-\npendix B.2 for the clariﬁcation).\nCorrelation Analysis.To uncover additional pat-\nterns in our results (Tables 2, 3, 4), we perform\na statistical analysis assessing the correlation be-\ntween the individual factors (pretraining data size,\nsubword fertility, proportion of continued words)\nand the downstream performance. Although our\nframework may not provide enough data points\nto be statistically representative, we argue that the\ncorrelation coefﬁcient can still provide reasonable\nindications and reveal relations not immediately\nevident by looking at the tables.\nFigure 3 shows that both decreases in the propor-\ntion of continued words and the fertility correlate\nwith an increase in downstream performance rel-\native to fully ﬁne-tuned mBERT across all tasks.\nThe correlation is stronger for UDP and QA, where\nwe ﬁnd models with monolingual tokenizers to\noutperform their counterparts with the mBERT to-\nkenizer consistently. The correlation is weaker for\nNER and POS tagging, which is also expected,\nconsidering the inconsistency of the results.28\nOverall, we ﬁnd that the fertility and the pro-\nportion of continued words have a similar effect\non the monolingual downstream performance as\nthe corpus size for pretraining; This indicates that\nthe tokenizer’s ability of representing a language\nplays a crucial role; Consequently, choosing a sub-\noptimal tokenizer typically results in deteriorated\ndownstream performance.\n28For further information, see Appendix B.2.\n6 Conclusion\nWe have conducted the ﬁrst comprehensive em-\npirical investigation concerning the monolingual\nperformance of monolingual and multilingual lan-\nguage models (LMs). While our results support the\nexistence of a performance gap in most but not all\nlanguages and tasks, further analyses revealed that\nthe gaps are often substantially smaller than what\nwas previously assumed. The gaps exist in certain\nlanguages due to the discrepancies in1) pretraining\ndata size, and 2) chosen tokenizers, and the level\nof their adaptation to the target language.\nFurther, we have disentangled the impact of pre-\ntrained corpora size from the inﬂuence of the tok-\nenizers on the downstream task performance. We\nhave trained new monolingual LMs on the same\ndata, but with two different tokenizers; one being\nthe dedicated tokenizer of the monolingual LM\nprovided by native speakers; the other being the\nautomatically generated multilingual mBERT tok-\nenizer. We have found that for (almost) every task\nand language, the use of monolingual tokenizers\noutperforms the mBERT tokenizer.\nConsequently, in line with recent work by Chung\net al. (2020), our results suggest that investing more\neffort into 1) improving the balance of individ-\nual languages’ representations in the vocabulary\nof multilingual LMs, and 2) providing language-\nspeciﬁc adaptations and extensions of multilingual\ntokenizers (Pfeiffer et al., 2020c) can reduce the\ngap between monolingual and multilingual LMs.\nAnother promising future research direction is com-\npletely disposing of any (language-speciﬁc or mul-\ntilingual) tokenizers during pretraining (Clark et al.,\n2021).\nOur code, pretrained models, and adapters are\navailable at https://github.com/Adapter-Hub/hgiyt.\nAcknowledgments\nJonas Pfeiffer is supported by the LOEWE initia-\ntive (Hesse, Germany) within the emergenCITY\ncenter. The work of Ivan Vuli ´c is supported by\nthe ERC Consolidator Grant LEXICAL: Lexical\nAcquisition Across Languages (no 648909).\nWe thank Nils Reimers, Prasetya Ajie Utama,\nand Adhiguna Kuncoro for insightful feedback and\nsuggestions on a draft of this paper.\n3127\nReferences\nJudit ´Acs. 2019. Exploring BERT’s Vocabulary. Blog\nPost.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nGiusepppe Attardi. 2015. Wikiextractor. GitHub\nRepository.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4536–4546, Online. Association for Computational\nLinguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. CANINE: Pre-training an efﬁcient\ntokenization-free encoder for language representa-\ntion. arXiv preprint.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nErkin Demirtas and Mykola Pechenizkiy. 2013. Cross-\nlingual polarity detection with machine translation.\nIn Proceedings of the Second International Work-\nshop on Issues of Sentiment Discovery and Opinion\nMining (WISDOM ’13), pages 9:1–8, Chicago, USA.\nAssociation for Computing Machinery.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMiguel Domingo, Mercedes Garcıa-Martınez, Alexan-\ndre Helle, Francisco Casacuberta, and Manuel Her-\nranz. 2019. How much does tokenization affect neu-\nral machine translation? arXiv preprint.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In Proceedings of the 5th International Confer-\nence on Learning Representations (ICLR) , Toulon,\nFrance. OpenReview.net.\nPavel Eﬁmov, Andrey Chertok, Leonid Boytsov, and\nPavel Braslavski. 2020. SberQuAD – Russian Read-\ning Comprehension Dataset: Description and analy-\nsis. In CLEF 2020: Experimental IR Meets Multilin-\nguality, Multimodality, and Interaction, pages 3–15.\nSpringer, Cham, Switzerland.\nAshraf Elnagar, Yasmin S. Khalifa, and Anas Einea.\n2018. Hotel Arabic-Reviews Dataset Construction\nfor Sentiment Analysis Applications. In Intelligent\nNatural Language Processing: Trends and Applica-\ntions, pages 35–52. Springer, Cham, Switzerland.\nDaniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Roi\nReichart, and Anna Korhonen. 2018. On the relation\nbetween linguistic typology and (limitations of) mul-\ntilingual language modeling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 316–327, Brussels, Bel-\ngium. Association for Computational Linguistics.\nGoran Glavaˇs and Ivan Vuli´c. 2021. Is supervised syn-\ntactic parsing beneﬁcial for language understanding\ntasks? an empirical investigation. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 3090–3104, Online. Association for\nComputational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\n3128\nConference on Machine Learning, pages 4411–4421,\nVirtual. PMLR.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual BERT: an empirical study. In Proceedings of\nthe 8th International Conference on Learning Rep-\nresentations (ICLR), Addis Ababa, Ethiopia. Open-\nReview.net.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the 3rd International Conference on Learning\nRepresentations (ICLR), San Diego, CA, USA.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation\nof deep bidirectional multilingual transformers for\nrussian language. arXiv preprint.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glava ˇs. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nSangah Lee, Hansol Jang, Yunmee Baik, Suzi Park,\nand Hyopil Shin. 2020. KR-BERT: A small-scale\nKorean-speciﬁc language model. arXiv preprint.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee.\n2019. KorQuAD1.0: Korean QA dataset for ma-\nchine reading comprehension. arXiv preprint.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proceedings of the\n7th International Conference on Learning Represen-\ntations (ICLR) , New Orleans, LA, USA. OpenRe-\nview.net.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: A tasty French language\nmodel. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7203–7219, Online. Association for Computa-\ntional Linguistics.\nPhoebe Mulcaire, Jungo Kasai, and Noah A. Smith.\n2019. Polyglot contextual representations improve\ncrosslingual transfer. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3912–3918, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajiˇc, Christopher D. Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman.\n2016. Universal Dependencies v1: A multilingual\ntreebank collection. In Proceedings of the Tenth In-\nternational Conference on Language Resources and\nEvaluation (LREC’16), pages 1659–1666, Portoro ˇz,\nSlovenia. European Language Resources Associa-\ntion (ELRA).\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4034–4043, Mar-\nseille, France. European Language Resources Asso-\nciation.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020.\nWhat the [MASK]? Making sense of language-\nspeciﬁc BERT models. arXiv preprint.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n3129\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 487–503, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Andreas R ¨uckl´e, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterHub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 46–54, Online. Asso-\nciation for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020c. UNKs Everywhere: Adapt-\ning Multilingual Language Models to New Scripts.\narXiv preprint.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nEdoardo Maria Ponti, Goran Glava ˇs, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nEdoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak,\nIvan Vuli´c, Roi Reichart, Thierry Poibeau, Ekaterina\nShutova, and Anna Korhonen. 2019. Modeling lan-\nguage variation and universals: A survey on typo-\nlogical linguistics for natural language processing.\nComputational Linguistics, 45(3):559–601.\nLutz Prechelt. 1998. Early stopping-but when? In\nNeural Networks: Tricks of the Trade , pages 55–69.\nSpringer, Berlin, Germany.\nAyu Purwarianti and Ida Ayu Putu Ari Crisdayanti.\n2019. Improving Bi-LSTM performance for Indone-\nsian sentiment analysis using paragraph vector. In\nProceedings of the 2019 International Conference of\nAdvanced Informatics: Concepts, Theory and Appli-\ncations (ICAICTA), pages 1–5, Yogyakarta, Indone-\nsia. IEEE.\nSampo Pyysalo, Jenna Kanerva, Antti Virtanen, and\nFilip Ginter. 2020. WikiBERT models: Deep trans-\nfer learning for many languages. arXiv preprint.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nTeemu Ruokolainen, Pekka Kauppinen, Miikka Silfver-\nberg, and Krister Lind ´en. 2020. A Finnish news\ncorpus for named entity recognition. Language Re-\nsources and Evaluation, 54(1):247–272.\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual BERT ﬂu-\nent in language generation? In Proceedings of the\nFirst NLPL Workshop on Deep Learning for Natural\nLanguage Processing, pages 29–36, Turku, Finland.\nLink¨oping University Electronic Press.\nStefan Schweter. 2020. BERTurk - BERT models for\nTurkish. Zenodo.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2019. DRCD: a Chinese machine\nreading comprehension dataset. arXiv preprint.\nSergey Smetanin and Michail Komarov. 2019. Sen-\ntiment analysis of product reviews in Russian us-\ning convolutional neural networks. In Proceedings\nof the 2019 IEEE 21st Conference on Business In-\nformatics (CBI) , pages 482–486, Moscow, Russia.\nIEEE.\nMilan Straka, Jan Hajiˇc, and Jana Strakov´a. 2016. UD-\nPipe: Trainable pipeline for processing CoNLL-U\nﬁles performing tokenization, morphological analy-\nsis, POS tagging and parsing. In Proceedings of\nthe Tenth International Conference on Language Re-\nsources and Evaluation (LREC’16) , pages 4290–\n4297, Portoro ˇz, Slovenia. European Language Re-\nsources Association (ELRA).\n3130\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147, Edmonton, Canada. Association for Com-\nputational Linguistics.\nAhmet ¨Ust¨un, Arianna Bisazza, Gosse Bouma, and\nGertjan van Noord. 2020. UDapter: Language adap-\ntation for truly Universal Dependency parsing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2302–2315, Online. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008, Long Beach,\nCA, USA. Curran Associates, Inc.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBERT for Finnish. arXiv preprint.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. arXiv preprint.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaˇs, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata,\nSamuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,\nSidik Soleman, Rahmad Mahendra, Pascale Fung,\nSyafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:\nBenchmark and resources for evaluating Indonesian\nnatural language understanding. In Proceedings of\nthe 1st Conference of the Asia-Paciﬁc Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural Lan-\nguage Processing, pages 843–857, Suzhou, China.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. arxiv preprint.\nJingjing Xu, Ji Wen, Xu Sun, and Qi Su. 2017. A\ndiscourse-level named entity recognition and rela-\ntion extraction dataset for Chinese literature text.\narXiv preprint.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 483–498, Online. Association for Computa-\ntional Linguistics.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia\nAckermann, No¨emi Aepli, ˇZeljko Agi´c, Lars Ahren-\nberg, Chika Kennedy Ajede, Gabriel ˙e Aleksan-\ndraviˇci¯ut˙e, Lene Antonsen, Katya Aplonova, An-\ngelina Aquino, Maria Jesus Aranzabe, Gashaw Aru-\ntie, Masayuki Asahara, Luma Ateyah, Furkan At-\nmaca, Mohammed Attia, Aitziber Atutxa, Lies-\nbeth Augustinus, Elena Badmaeva, Miguel Balles-\nteros, Esha Banerjee, Sebastian Bank, Verginica\nBarbu Mititelu, Victoria Basmov, Colin Batchelor,\nJohn Bauer, Kepa Bengoetxea, Yevgeni Berzak, Ir-\nshad Ahmad Bhat, Riyaz Ahmad Bhat, Erica Bi-\nagetti, Eckhard Bick, Agn ˙e Bielinskien ˙e, Rogier\nBlokland, Victoria Bobicev, Lo¨ıc Boizou, Emanuel\nBorges V¨olker, Carl B¨orstell, Cristina Bosco, Gosse\nBouma, Sam Bowman, Adriane Boyd, Kristina\nBrokait˙e, Aljoscha Burchardt, Marie Candito,\nBernard Caron, Gauthier Caron, Tatiana Cavalcanti,\nG¨uls ¸en Cebiro˘glu Eryi˘git, Flavio Massimiliano Cec-\nchini, Giuseppe G. A. Celano, Slavom ´ır ˇC´epl¨o,\nSavas Cetin, Fabricio Chalub, Ethan Chi, Jinho\nChoi, Yongseok Cho, Jayeol Chun, Alessandra T.\n3131\nCignarella, Silvie Cinkov´a, Aur´elie Collomb, C ¸ a˘grı\nC ¸¨oltekin, Miriam Connor, Marine Courtin, Eliza-\nbeth Davidson, Marie-Catherine de Marneffe, Vale-\nria de Paiva, Elvis de Souza, Arantza Diaz de Ilar-\nraza, Carly Dickerson, Bamba Dione, Peter Dirix,\nKaja Dobrovoljc, Timothy Dozat, Kira Droganova,\nPuneet Dwivedi, Hanne Eckhoff, Marhaba Eli, Ali\nElkahky, Binyam Ephrem, Olga Erina, Toma ˇz Er-\njavec, Aline Etienne, Wograine Evelyn, Rich ´ard\nFarkas, Hector Fernandez Alcalde, Jennifer Fos-\nter, Cl ´audia Freitas, Kazunori Fujita, Katar ´ına\nGajdoˇsov´a, Daniel Galbraith, Marcos Garcia, Moa\nG¨ardenfors, Sebastian Garza, Kim Gerdes, Filip\nGinter, Iakes Goenaga, Koldo Gojenola, Memduh\nG¨okırmak, Yoav Goldberg, Xavier G ´omez Guino-\nvart, Berta Gonz ´alez Saavedra, Bernadeta Grici ¯ut˙e,\nMatias Grioni, Lo ¨ıc Grobol, Normunds Gr ¯uz¯ıtis,\nBruno Guillaume, C ´eline Guillot-Barbance, Tunga\nG¨ung¨or, Nizar Habash, Jan Haji ˇc, Jan Haji ˇc jr.,\nMika H ¨am¨al¨ainen, Linh H `a M ˜y, Na-Rae Han,\nKim Harris, Dag Haug, Johannes Heinecke, Oliver\nHellwig, Felix Hennig, Barbora Hladk ´a, Jaroslava\nHlav´aˇcov´a, Florinel Hociung, Petter Hohle, Jena\nHwang, Takumi Ikeda, Radu Ion, Elena Irimia,\nO. l´aj´ıd´e Ishola, Tom ´aˇs Jel ´ınek, Anders Johannsen,\nHildur J ´onsd´ottir, Fredrik Jørgensen, Markus Juu-\ntinen, H ¨uner Kas ¸ıkara, Andre Kaasen, Nadezhda\nKabaeva, Sylvain Kahane, Hiroshi Kanayama,\nJenna Kanerva, Boris Katz, Tolga Kayadelen, Jes-\nsica Kenney, V ´aclava Kettnerov ´a, Jesse Kirchner,\nElena Klementieva, Arne K ¨ohn, Abdullatif K ¨oksal,\nKamil Kopacewicz, Timo Korkiakangas, Natalia\nKotsyba, Jolanta Kovalevskait˙e, Simon Krek, Sooky-\noung Kwak, Veronika Laippala, Lorenzo Lam-\nbertino, Lucia Lam, Tatiana Lando, Septina Dian\nLarasati, Alexei Lavrentiev, John Lee, Phuong\nLˆe H`ˆong, Alessandro Lenci, Saran Lertpradit, Her-\nman Leung, Maria Levina, Cheuk Ying Li, Josie\nLi, Keying Li, KyungTae Lim, Yuan Li, Nikola\nLjubeˇsi´c, Olga Loginova, Olga Lyashevskaya,\nTeresa Lynn, Vivien Macketanz, Aibek Makazhanov,\nMichael Mandl, Christopher Manning, Ruli Ma-\nnurung, C ˘at˘alina M ˘ar˘anduc, David Mare ˇcek, Ka-\ntrin Marheinecke, H ´ector Mart´ınez Alonso, Andr ´e\nMartins, Jan Ma ˇsek, Hiroshi Matsuda, Yuji Mat-\nsumoto, Ryan McDonald, Sarah McGuinness, Gus-\ntavo Mendonc ¸a, Niko Miekka, Margarita Misir-\npashayeva, Anna Missil ¨a, C ˘at˘alin Mititelu, Maria\nMitrofan, Yusuke Miyao, Simonetta Montemagni,\nAmir More, Laura Moreno Romero, Keiko Sophie\nMori, Tomohiko Morioka, Shinsuke Mori, Shigeki\nMoro, Bjartur Mortensen, Bohdan Moskalevskyi,\nKadri Muischnek, Robert Munro, Yugo Murawaki,\nKaili M ¨u¨urisep, Pinkey Nainwani, Juan Igna-\ncio Navarro Hor ˜niacek, Anna Nedoluzhko, Gunta\nNeˇspore-B¯erzkalne, Luong Nguy˜ ˆen Thi., Huy` ˆen\nNguy˜ˆen Thi. Minh, Yoshihiro Nikaido, Vitaly Niko-\nlaev, Rattima Nitisaroj, Hanna Nurmi, Stina Ojala,\nAtul Kr. Ojha, Ad ´edayo. Ol´u`okun, Mai Omura,\nEmeka Onwuegbuzia, Petya Osenova, Robert\n¨Ostling, Lilja Øvrelid, S ¸aziye Bet ¨ul ¨Ozates ¸, Arzu-\ncan ¨Ozg¨ur, Balkız ¨Ozt¨urk Bas ¸aran, Niko Partanen,\nElena Pascual, Marco Passarotti, Agnieszka Pate-\njuk, Guilherme Paulino-Passos, Angelika Peljak-\nŁapi ´nska, Siyao Peng, Cenel-Augusto Perez, Guy\nPerrier, Daria Petrova, Slav Petrov, Jason Phelan,\nJussi Piitulainen, Tommi A Pirinen, Emily Pitler,\nBarbara Plank, Thierry Poibeau, Larisa Ponomareva,\nMartin Popel, Lauma Pretkalnin ¸a, Sophie Pr ´evost,\nProkopis Prokopidis, Adam Przepi ´orkowski, Ti-\nina Puolakainen, Sampo Pyysalo, Peng Qi, An-\ndriela R ¨a¨abis, Alexandre Rademaker, Loganathan\nRamasamy, Taraka Rama, Carlos Ramisch, Vinit\nRavishankar, Livy Real, Petru Rebeja, Siva Reddy,\nGeorg Rehm, Ivan Riabov, Michael Rießler,\nErika Rimkut ˙e, Larissa Rinaldi, Laura Rituma,\nLuisa Rocha, Mykhailo Romanenko, Rudolf Rosa,\nValentin Ros, ca, Davide Rovati, Olga Rudina, Jack\nRueter, Shoval Sadde, Beno ˆıt Sagot, Shadi Saleh,\nAlessio Salomoni, Tanja Samard ˇzi´c, Stephanie\nSamson, Manuela Sanguinetti, Dage S ¨arg, Baiba\nSaul¯ıte, Yanin Sawanakunanon, Salvatore Scarlata,\nNathan Schneider, Sebastian Schuster, Djam ´e Sed-\ndah, Wolfgang Seeker, Mojgan Seraji, Mo Shen,\nAtsuko Shimada, Hiroyuki Shirasu, Muh Shohibus-\nsirri, Dmitry Sichinava, Aline Silveira, Natalia Sil-\nveira, Maria Simi, Radu Simionescu, Katalin Simk´o,\nM´aria ˇSimkov´a, Kiril Simov, Maria Skachedubova,\nAaron Smith, Isabela Soares-Bastos, Carolyn Spa-\ndine, Antonio Stella, Milan Straka, Jana Strnadov ´a,\nAlane Suhr, Umut Sulubacak, Shingo Suzuki, Zsolt\nSz´ant´o, Dima Taji, Yuta Takahashi, Fabio Tam-\nburini, Takaaki Tanaka, Samson Tella, Isabelle\nTellier, Guillaume Thomas, Liisi Torga, Marsida\nToska, Trond Trosterud, Anna Trukhina, Reut Tsar-\nfaty, Utku T ¨urk, Francis Tyers, Sumire Uematsu,\nRoman Untilov, Zde ˇnka Ure ˇsov´a, Larraitz Uria,\nHans Uszkoreit, Andrius Utka, Sowmya Vajjala,\nDaniel van Niekerk, Gertjan van Noord, Viktor\nVarga, Eric Villemonte de la Clergerie, Veronika\nVincze, Aya Wakasa, Lars Wallin, Abigail Walsh,\nJing Xian Wang, Jonathan North Washington, Max-\nimilan Wendt, Paul Widmer, Seyi Williams, Mats\nWir´en, Christian Wittern, Tsegay Woldemariam,\nTak-sum Wong, Alina Wr ´oblewska, Mary Yako,\nKayo Yamashita, Naoki Yamazaki, Chunxiao Yan,\nKoichi Yasuoka, Marat M. Yavrumyan, Zhuoran Yu,\nZdenˇek ˇZabokrtsk´y, Amir Zeldes, Hanzhi Zhu, and\nAnna Zhuravleva. 2020. Universal Dependencies\n2.6. LINDAT/CLARIAH-CZ digital library at the\nInstitute of Formal and Applied Linguistics (´UFAL),\nFaculty of Mathematics and Physics, Charles Uni-\nversity.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R. Bowman. 2020. When do you need bil-\nlions of words of pretraining data? arXiv preprint.\n3132\nA Reproducibility\nA.1 Pretrained Models\nAll of the pretrained language models we use are\navailable on the HuggingFace model hub 29 and\ncompatible with the HuggingFace transformers\nPython library (Wolf et al., 2020). Table 5 displays\nthe model hub identiﬁers of our selected models.\nA.2 Estimating the Pretraining Corpora\nSizes\nSince mBERT was pretrained on the entire\nWikipedia dumps of all languages it covers (De-\nvlin et al., 2019), we estimate the language-speciﬁc\nshares of the mBERT pretraining corpus by word\ncounts of the respective raw Wikipedia dumps, ac-\ncording to numbers obtained from Wikimedia 30:\n327M words for AR, 3.7B for EN, 134M for FI,\n142M for ID, 1.1B for JA, 125M for KO, 781M for\nRU, 104M for TR, 482M for ZH.31 Devlin et al.\n(2019) only included text passages from the arti-\ncles, and used older Wikipedia dumps, so these\nnumbers should serve as upper limits, yet be rea-\nsonably accurate. For the monolingual models, we\nrely on information provided by the authors.32\nA.3 Data for Tokenizer Analyses\nWe tokenize the training and development splits\nof the UD (Nivre et al., 2016, 2020) v2.6 (Zeman\net al., 2020) treebanks listed in Table 6.\nA.4 Fine-Tuning Datasets\nWe list the datasets we used, including the number\nof examples per dataset split, in the Table 7.\nA.5 Training Procedure of New Models\nWe pretrain our models on single Nvidia Tesla\nV100, A100, and Titan RTX GPUs with 32GB,\n40GB, and 24GB of video memory, respectively.\nTo support larger batch sizes, we train in mixed-\nprecision (fp16) mode. Following Wu and Dredze\n(2020), we only use masked language modeling\n(MLM) as pretraining objective and omit the next\nsentence prediction task as Liu et al. (2019) ﬁnd it\ndoes not yield performance gains. We otherwise\n29https://huggingface.co/models\n30https://meta.m.wikimedia.org/wiki/List of Wikipedias\n31We obtained the numbers for ID and TR on Dec 10, 2020\nand for the remaining languages on Sep 10, 2020.\n32For JA, RU, and ZH, the authors do not provide exact word\ncounts. Therefore, we estimate them using other provided\ninformation (RU, ZH) or scripts for training corpus reconstruc-\ntion (JA).\nmostly follow the default pretraining procedure by\nDevlin et al. (2019).\nWe pretrain the new monolingual models\n(MONO MODEL -*) from scratch for 1M steps with\nbatch size 64. We choose a sequence length of\n128 for the ﬁrst 900,000 steps and 512 for the\nremaining 100,000 steps. In both phases, we\nwarm up the learning rate to 1e− 4 over the ﬁrst\n10,000 steps, then decay linearly. We use the\nAdam optimizer with weight decay (AdamW)\n(Loshchilov and Hutter, 2019) with default\nhyper-parameters and a weight decay of 0.01. We\nenable whole word masking (Devlin et al., 2019)\nfor the FI monolingual models, following the\npretraining procedure for FinBERT (Virtanen et al.,\n2019). To lower computational requirements for\nthe monolingual models with mBERT tokenizers,\nwe remove all tokens from mBERT’s vocabulary\nthat do not appear in the pretraining data. We,\nthereby, obtain vocabularies of size 78,193 ( AR),\n60,827 (FI), 72,787 (ID), 66,268 (KO), and 71,007\n(TR), which for all languages reduces the number\nof parameters in the embedding layer signiﬁcantly,\ncompared to the 119,547 word piece vocabulary of\nmBERT.\nFor the retrained mBERT models (i.e.,\nMBERTM ODEL -*), we run MLM for 250,000\nsteps (similar to Artetxe et al. (2020)) with batch\nsize 64 and sequence length 512, otherwise using\nthe same hyper-parameters as for the monolingual\nmodels. In order to retrain the embedding layer,\nwe ﬁrst resize it to match the vocabulary of\nthe respective tokenizer. For the MBERTM ODEL -\nMBERTT OK models, we use the mBERT tokenizers\nwith reduced vocabulary as outlined above. We\ninitialize the positional embeddings, segment\nembeddings, and embeddings of special tokens\n([CLS], [SEP], [PAD], [UNK], [MASK]) from\nmBERT, and reinitialize the remaining embeddings\nrandomly. We freeze all parameters outside the\nembedding layer. For all pretraining runs, we set\nthe random seed to 42.\nA.6 Code\nOur code with usage instructions for ﬁne-\ntuning, pretraining, data preprocessing, and cal-\nculating the tokenizer statistics is available at\nhttps://github.com/Adapter-Hub/hgiyt. The repos-\nitory also contains further links to a collection of\nour new pretrained models and language adapters.\n3133\nB Further Analyses and Discussions\nB.1 Tokenization Analysis\nIn our tokenization analysis in§4.2 of the main text,\nwe only include the fertility and the proportion of\ncontinued words as they are sufﬁcient to illustrate\nand quantify the differences between tokenizers. In\nsupport of the ﬁndings in §4.2 and for complete-\nness, we provide additional tokenization statistics\nhere.\nFor each tokenizer, Table 5 lists the respective\nvocabulary size and the proportion of its vocabu-\nlary also contained in mBERT. It shows that the\ntokenizers scoring lower in fertility (and accord-\ningly performing better) than mBERT are often not\nadequately covered by mBERT’s vocabulary. For\ninstance, only 5.6% of the AraBERT (AR) vocabu-\nlary is covered by mBERT.\nFigure 4 compares the proportion of unknown\ntokens ([UNK]) in the tokenized data. It shows that\nthe proportion is generally extremely low, i.e., the\ntokenizers can typically split unknown words into\nknown subwords.\nSimilar to the work by ´Acs (2019), Figure 5\ncompares the tokenizations produced by the mono-\nlingual models and mBERT with the reference to-\nkenizations provided by the human dataset anno-\ntators with respect to their sentence lengths. We\nﬁnd that the tokenizers scoring low in fertility and\nthe proportion of continued words typically exhibit\nsentence length distributions much closer to the\nreference tokenizations by human UD annotators,\nindicating they are more capable than the mBERT\ntokenizer. Likewise, the monolingual models’ and\nmBERT’s sentence length distributions are closer\nfor languages with similar fertility and proportion\nof continued words, such as EN, JA, and ZH.\nB.2 Correlation Analysis\nTo uncover some of the hidden patterns in our re-\nsults (Tables 2, 3, 4), we perform a statistical analy-\nsis assessing the correlation between the individual\nfactors (pretraining data size, subword fertility, pro-\nportion of continued words) and the downstream\nperformance.\nFigure 6b shows that both decreases in the pro-\nportion of continued words and the fertility corre-\nlate with an increase in downstream performance\nrelative to fully ﬁne-tuned mBERT across all tasks.\nThe correlation is stronger for UDP and QA, where\nwe found models with monolingual tokenizers to\noutperform their counterparts with the mBERT to-\nkenizer consistently. The correlation is weaker for\nNER and POS tagging, which is also expected,\nconsidering the inconsistency of the results.\nSomewhat surprisingly, the tokenizer metrics\nseem to be more indicative of high downstream\nperformance than the size of the pretraining cor-\npus. We believe that this in parts due to the overall\npoor performance of the uncased IndoBERT model,\nwhich we (in this case unfairly) compare to our\ncased ID-MONO MODEL -MONO TOK model. Therefore,\nwe plot the same correlation matrix excluding ID\nin Figure 3.\nCompared to Figure 6b, the overall correlations\nfor the proportion of continued words and the fer-\ntility remain mostly unaffected. In contrast, the\ncorrelation for the pretraining corpus size becomes\nmuch stronger, conﬁrming that the subpar perfor-\nmance of IndoBERT is indeed an outlier in this\nscenario. Leaving out Indonesian also strengthens\nthe indication that the performance in POS tagging\ncorrelates more with the data size than with the\ntokenizer, although we argue that this indication\nmay be misleading. The performance gap is gen-\nerally very minor in POS tagging. Therefore, the\nSpearman correlation coefﬁcient, which only takes\nthe rank into account, but not the absolute score\ndifferences, is particularly sensitive to changes in\nPOS tagging performance.\nFinally, we plot the correlation between the three\nmetrics and the downstream performance under\nconsideration of all languages and models, includ-\ning the adapter-based ﬁne-tuning settings, to gain\nan understanding of how pronounced their effects\nare in a more “noisy” setting.\nAs Figure 6a shows, the three factors still corre-\nlate with the downstream performance in a similar\nmanner even when not isolated. This correlation\ntells us that even when there may be other factors\nthat could have an inﬂuence, these three factors\nare still highly indicative of the downstream perfor-\nmance.\nWe also see that the correlation coefﬁcients for\nthe proportion of continued words and the fertility\nare nearly identical, which is expected based on\nthe visual similarity of the respective plots (seen in\nFigures 1b and 1c).\n3134\nC Full Results\nFor compactness, we have only reported the perfor-\nmance of our models on the respective test datasets\nin the main text.33 For completeness, we also in-\nclude the full tables, including development (dev)\ndataset performance averaged over three random\ninitializations, as described in §3. Table 8 shows\nthe full results corresponding to Table 2 (initial\nresults), Table 9 shows the full results correspond-\ning to Table 3 (results for our new models), and\nTable 10 shows the full results corresponding to\nTable 4 (adapter-based training).\nLang Model Reference V . Size % Voc\nMULTIbert-base-multilingual-cased Devlin et al. (2019) 119547 100\nAR aubmindlab/bert-base-arabertv01 Antoun et al. (2020) 64000 5.6EN bert-base-cased Devlin et al. (2019) 28996 66.4FI TurkuNLP/bert-base-ﬁnnish-cased-v1 Virtanen et al. (2019) 50105 14.3ID indobenchmark/indobert-base-p2 Wilie et al. (2020) 30521 40.5JA cl-tohoku/bert-base-japanese-char5 4000 99.1KO snunlp/KR-BERT-char16424 Lee et al. (2020) 16424 47.4RU DeepPavlov/rubert-base-cased Kuratov and Arkhipov (2019) 119547 21.1TR dbmdz/bert-base-turkish-cased Schweter (2020) 32000 23.0ZH bert-base-chinese Devlin et al. (2019) 21128 79.4\nTable 5: Selection of pretrained models used in our ex-\nperiments. We display the respective vocabulary sizes\nand the proportion of tokens that are also covered by\nmBERT’s vocabulary.\nLang Treebank # Words\nAR PADT 254192EN LinES, EWT, GUM, ParTUT 449977FI FTB, TDT 324680ID GSD 110141JA GSD 179571KO GSD 390369RU GSD, SynTagRus, Taiga 1130482TR IMST 47830ZH GSD, GSDSimp 222558\nTable 6: UD v2.6 (Zeman et al., 2020) treebanks used\nfor our tokenizer analyses. We use training and devel-\nopment portions only and display the total number of\nwords per language.\nar en fi id ja ko ru tr zh0.0\n0.004\n0.008\n0.012\n0.016Proportion UNK\nModel\nMono\nmBERT\nFigure 4: Proportion of unknown tokens in respective\nmonolingual corpora tokenized by monolingual models\nvs. mBERT.\n33Except for QA, where we do not use any test data\nTask Lang Dataset Reference Train / Dev / Test\nNER\nAR WikiAnn Pan et al. (2017); Rahimi et al. (2019) 20000 / 10000 / 10000EN CoNLL-2003 Tjong Kim Sang and De Meulder (2003) 14041 / 3250 / 3453FI FiNER Ruokolainen et al. (2020) 13497 / 986 / 3512ID WikiAnn Pan et al. (2017); Rahimi et al. (2019) 20000 / 10000 / 10000JA WikiAnn Pan et al. (2017); Rahimi et al. (2019) 20202 / 10100 / 10113KO KMOU NER6 23056 / 468 / 463RU WikiAnn Pan et al. (2017); Rahimi et al. (2019) 20000 / 10000 / 10000TR WikiAnn Pan et al. (2017); Rahimi et al. (2019) 20000 / 10000 / 10000ZH Chinese Literature Xu et al. (2017) 24270 / 1902 / 2844\nSA\nAR HARD Elnagar et al. (2018) 84558 / 10570 / 10570EN IMDb Movie Reviews Maas et al. (2011) 20000 / 5000 / 25000FI — — —ID Indonesian Prosa Purwarianti and Crisdayanti (2019) 6853 / 763 / 409JA Yahoo Movie Reviews7 30545 / 3818 / 3819KO NSMC 8 120000 / 30000 / 50000RU RuReviews Smetanin and Komarov (2019) 48000 / 6000 / 6000TR Movie & Product Reviews Demirtas and Pechenizkiy (2013) 13009 / 1627 / 1629ZH ChnSentiCorp9 9600 / 1200 / 1200\nQA\nAR TyDiQA-GoldP Clark et al. (2020) 14805 / 921EN SQuAD v1.1 Rajpurkar et al. (2016) 87599 / 10570FI TyDiQA-GoldP Clark et al. (2020) 6855 / 782ID TyDiQA-GoldP Clark et al. (2020) 5702 / 565JA — — —KO KorQuAD 1.0 Lim et al. (2019) 60407 / 5774RU SberQuAD Eﬁmov et al. (2020) 45328 / 5036TR TQuAD 10 8308 / 892ZH DRCD Shao et al. (2019) 26936 / 3524\nUD\nAR PADT (Zeman et al., 2020) 6075 / 909 / 680EN EWT (Zeman et al., 2020) 12543 / 2002 / 2077FI FTB (Zeman et al., 2020) 14981 / 1875 / 1867ID GSD (Zeman et al., 2020) 4477 / 559 / 557JA GSD (Zeman et al., 2020) 7027 / 501 / 543KO GSD (Zeman et al., 2020) 4400 / 950 / 989RU GSD (Zeman et al., 2020) 3850 / 579 / 601TR IMST (Zeman et al., 2020) 3664 / 988 / 983ZH GSD (Zeman et al., 2020) 3997 / 500 / 500\nTable 7: Named entity recognition (NER), sentiment\nanalysis (SA), question answering (QA), and universal\ndependencies (UD) datasets used in our experiments\nand the number of examples in their respective train-\ning, development, and test portions. UD datasets were\nused for both universal dependency parsing and POS\ntagging experiments.\n0 50 100 150 200\nSentence length [Tokens]\n0.000\n0.005\n0.010\n0.015\nProportion\nReference\nMono\nmBERT\n(a) AR\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.01\n0.02\n0.03\nProportion\nReference\nMono\nmBERT (b) EN\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.02\n0.04\n0.06\n0.08\nProportion\nReference\nMono\nmBERT (c) FI\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.01\n0.02\n0.03\n0.04\nProportion\nReference\nMono\nmBERT\n(d) ID\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.01\n0.02\n0.03\nProportion\nReference\nMono\nmBERT (e) JA\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.02\n0.04\n0.06\nProportion\nReference\nMono\nmBERT (f) KO\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.01\n0.02\n0.03\n0.04\nProportion\nReference\nMono\nmBERT\n(g) RU\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.02\n0.04\n0.06\n0.08\nProportion\nReference\nMono\nmBERT (h) TR\n0 50 100 150 200\nSentence length [Tokens]\n0.00\n0.01\n0.02\n0.03\n0.04\nProportion\nReference\nMono\nmBERT (i) ZH\nFigure 5: Sentence length distributions of monolin-\ngual UD corpora tokenized by respective monolingual\nBERT models and mBERT, compared to the reference\ntokenizations by human UD treebank annotators.\n3135\nAll NER POS QA SA UDP\nTask\nCont. Proportion\nFertility\nPre-Train Size\nMetric\n0.39 0.28 0.30 0.61 0.41 0.48\n0.42 0.31 0.34 0.64 0.40 0.52\n0.20 0.20 0.13 0.32 0.21 0.23 −1\n0\n1\n(a) We consider all languages and models.\nAll NER POS QA SA UDP\nTask\nCont. Proportion\nFertility\nPre-Train Size\nMetric\n0.34 0.20 0.18 0.67 0.32 0.45\n0.35 0.23 0.20 0.65 0.33 0.48\n0.23 0.06 0.27 0.34 0.31 0.43 −1\n0\n1\n(b) For the proportion of continued words and the fertility, we\nconsider fully ﬁne-tuned mBERT, the MONO MODEL -* mod-\nels, and the MBERTM ODEL -* models. For the pretraining\ncorpus size, we consider the original monolingual models and\nthe MONO MODEL -MONO TOK models.\nFigure 6: Spearman’s ρ correlation of a relative de-\ncrease in the proportion of continued words (Cont. Pro-\nportion), a relative decrease in fertility, and a rela-\ntive increase in pretraining corpus size with a relative\nincrease in downstream performance over fully ﬁne-\ntuned mBERT.\nLg Model NER SA QA UDP POSDev Test Dev Test Dev Dev Test Dev TestF1 F1 Acc Acc EM /F1 UAS / LAS UAS / LAS Acc Acc\nAR Monolingual91.5 91.1 96.1 95.9 68.3/82.4 89.4/85.0 90.1/85.6 97.5 96.8mBERT 90.3 90.0 95.8 95.4 66.1 / 80.6 87.8 / 83.0 88.8 / 83.8 97.296.8\nEN Monolingual 95.491.5 91.6 91.680.5 / 88.092.6/90.3 92.1/89.7 97.1 97.0mBERT95.791.2 90.1 89.880.9/88.492.1 / 89.6 91.6 / 89.1 97.0 96.9\nFI Monolingual93.3 92.0—– —– 69.9/81.6 95.7/93.9 95.9/94.4 98.1 98.4mBERT 90.9 88.2 —– —– 66.6 / 77.6 91.1 / 88.0 91.9 / 88.7 96.0 96.2\nID Monolingual 90.9 91.094.6 96.066.8 / 78.1 84.5 / 77.4 85.3 / 78.1 92.0 92.1mBERT93.7 93.593.1 91.471.2/82.1 85.0/78.4 85.9/79.3 93.3 93.5\nJA Monolingual 72.1 72.4 88.788.0—– / —–96.0/94.7 94.7/93.0 98.3 98.1mBERT73.4 73.4 88.887.8 —– / —– 95.5 / 94.2 94.0 / 92.3 98.1 97.8\nKO Monolingual88.6 88.8 89.8 89.7 74.2/91.1 88.5/85.0 90.3/87.2 96.4 97.0mBERT 87.3 86.6 86.7 86.7 69.7 / 89.5 86.9 / 83.2 89.2 / 85.7 95.8 96.0\nRU Monolingual91.9 91.0 95.2 95.2 64.3/83.7 92.4/90.1 93.1/89.9 98.6 98.4mBERT 90.2 90.095.295.0 63.3 / 82.6 91.5 / 88.8 91.9 / 88.5 98.4 98.2\nTR Monolingual 93.1 92.889.3 88.8 60.6/78.1 78.0/70.9 79.8/73.2 97.0 96.9mBERT93.7 93.886.4 86.4 57.9 / 76.4 72.6 / 65.2 74.5 / 67.4 95.5 95.7\nZH Monolingual77.0 76.5 94.8 95.3 82.3/89.3 88.1/84.9 88.6/85.6 96.6 97.2mBERT 76.0 76.1 93.1 93.8 82.0 /89.387.1 / 83.7 88.1 / 85.0 96.1 96.7\nAVGMonolingual88.2 87.4 92.5 92.4 70.8/84.0 89.5/85.8 90.0/86.3 96.9 96.9mBERT 87.9 87.0 91.2 91.0 69.7 / 83.3 87.7 / 83.8 88.4 / 84.4 96.4 96.4\nTable 8: Full Results - Performance on Named Entity\nRecognition (NER), Sentiment Analysis (SA), Ques-\ntion Answering (QA), Universal Dependency Parsing\n(UDP), and Part-of-Speech Tagging (POS). We use de-\nvelopment (dev) sets only for QA. Finnish (FI) SA and\nJapanese (JA) QA lack respective datasets.\nLg Model NER SA QA UDP POSDev Test Dev Test Dev Dev Test Dev TestF1 F1 Acc Acc EM /F1 UAS / LAS UAS / LAS Acc Acc\nAR\nMonolingual 91.5 91.196.1 95.9 68.3/82.4 89.4/85.0 90.1/85.6 97.596.8\nMONOMODEL-MONOTOK 88.691.796.095.667.7/ 81.688.4/ 83.789.2/ 84.497.3 96.6MONOMODEL-MBERTTOK 90.190.0 95.9 95.5 64.1 / 79.4 87.8 / 83.2 88.8 / 84.0 97.497.0\nMBERTMODEL-MONOTOK 91.991.295.995.4 66.9/ 81.888.2/ 83.589.3/ 84.597.2 96.4MBERTMODEL-MBERTTOK90.0 89.7 95.8 95.666.3 / 80.7 87.8 / 83.0 89.1 / 84.2 97.396.8\nmBERT 90.3 90.0 95.8 95.4 66.1 / 80.6 87.8 / 83.0 88.8 / 83.8 97.2 96.8\nFI\nMonolingual93.3 92.0—– —–69.9/81.6 95.7/93.9 95.9/94.4 98.1 98.4\nMONOMODEL-MONOTOK 91.989.1 —– —– 66.9/ 79.593.6/ 91.093.7/ 91.597.097.3MONOMODEL-MBERTTOK 91.8 90.0—– —– 65.1 / 77.0 93.1 / 90.6 93.6 / 91.596.2 97.0\nMBERTMODEL-MONOTOK 91.0 88.1—– —– 66.4/ 78.392.2/ 89.392.4/ 89.696.3 96.6MBERTMODEL-MBERTTOK92.088.1—– —– 65.9 / 77.3 92.1 / 89.2 92.2 / 89.4 96.696.7\nmBERT 90.9 88.2 —– —– 66.6 / 77.6 91.1 / 88.0 91.9 / 88.7 96.0 96.2\nID\nMonolingual 90.9 91.094.6 96.066.8 / 78.1 84.5 / 77.4 85.3 / 78.1 92.0 92.1\nMONOMODEL-MONOTOK 93.0 92.5 93.996.073.1/ 83.683.4 / 76.8 85.0/ 78.593.693.9MONOMODEL-MBERTTOK 93.393.293.994.8 67.0 / 79.2 84.0/ 77.484.9 / 78.693.4 93.6\nMBERTMODEL-MONOTOK 93.893.994.494.674.1/83.885.5/78.886.4/80.293.593.8MBERTMODEL-MBERTTOK93.993.993.7 94.671.9 / 82.7 85.3 / 78.6 86.2 / 79.6 93.4 93.7\nmBERT 93.7 93.5 93.1 91.4 71.2 / 82.1 85.0 / 78.4 85.9 / 79.3 93.3 93.5\nKO\nMonolingual88.6 88.8 89.8 89.7 74.2/91.1 88.5/85.0 90.3/87.2 96.4 97.0\nMONOMODEL-MONOTOK 87.987.189.088.872.8/ 90.387.9/ 84.289.8/ 86.696.496.7MONOMODEL-MBERTTOK 86.9 85.8 87.3 87.2 68.9 / 88.7 86.9 / 83.2 88.9 / 85.6 96.1 96.4\nMBERTMODEL-MONOTOK 87.986.688.288.172.9/ 90.287.9/ 83.990.1/ 87.096.296.5MBERTMODEL-MBERTTOK86.7 86.2 86.6 86.6 69.3 / 89.3 87.2 / 83.3 89.2 / 85.9 95.9 96.2\nmBERT 87.3 86.6 86.7 86.7 69.7 / 89.5 86.9 / 83.2 89.2 / 85.7 95.8 96.0\nTR\nMonolingual 93.1 92.889.3 88.8 60.6/78.1 78.0/70.9 79.8/73.2 97.0 96.9\nMONOMODEL-MONOTOK 93.593.487.587.056.2/ 73.774.4/ 67.376.1/ 68.995.9 96.3MONOMODEL-MBERTTOK 93.2 93.3 85.8 84.8 55.3 / 72.5 73.2 / 66.0 75.3 / 68.3 96.496.5\nMBERTMODEL-MONOTOK 93.5 93.7 86.185.3 59.4/ 76.774.7/ 67.677.1/ 70.296.196.3MBERTMODEL-MBERTTOK93.993.886.0 86.158.7 / 76.6 73.2 / 66.1 76.2 / 69.2 95.9 96.3\nmBERT 93.7 93.886.4 86.4 57.9 / 76.4 72.6 / 65.2 74.5 / 67.4 95.5 95.7\nAVG\nMonolingual 91.591.1 92.5 92.6 68.0/82.3 87.2/82.4 88.3/83.7 96.2 96.2\nMONOMODEL-MONOTOK 91.0 90.891.691.967.3/ 81.785.5/ 80.686.8/ 82.096.096.2MONOMODEL-MBERTTOK 91.190.5 90.7 90.6 64.1 / 79.4 85.0 / 80.1 86.3 / 81.6 95.9 96.1\nMBERTMODEL-MONOTOK 91.690.791.290.968.0/ 82.285.7/ 80.687.1/ 82.395.995.9MBERTMODEL-MBERTTOK91.3 90.3 90.5 90.7 66.4 / 81.3 85.1 / 80.0 86.6 / 81.7 95.8 95.9\nmBERT 91.2 90.4 90.5 90.0 66.3 / 81.2 84.7 / 79.6 86.1 / 81.0 95.6 95.6\nTable 9: Full Results - Performance of our new\nMONO MODEL -* and MBERTM ODEL -* models (see\n§A.5) ﬁne-tuned for the NER, SA, QA, UDP, and POS\ntasks (see §3.1), compared to the monolingual mod-\nels from prior work and fully ﬁne-tuned mBERT. We\ngroup model counterparts w.r.t. tokenizer choice to fa-\ncilitate a direct comparison between respective counter-\nparts. We use development sets only for QA. Bold de-\nnotes best score across all models for a given language\nand task. Underlined denotes best score compared to\nits respective counterpart.\nLg Model NER SA QA UDP POSDev Test Dev Test Dev Dev Test Dev TestF1 F1 Acc Acc EM /F1 UAS / LAS UAS / LAS Acc Acc\nAR\nmBERT 90.3 90.0 95.8 95.4 66.1 / 80.687.8/83.0 88.8/83.897.296.8+ ATask 90.0 89.696.195.6 66.7 / 81.1 86.7 / 81.6 87.8 / 82.697.3 96.8+ ATask+ ALang 90.2 89.796.1 95.766.9 / 81.0 87.0 / 81.9 88.0 / 82.897.3 96.8+ ATask+ ALang+MONOTOK91.5 91.196.095.7 67.7/82.187.7 / 82.8 88.5 / 83.497.396.5\nFI\nmBERT 90.9 88.2 —– —– 66.6 / 77.6 91.1 / 88.0 91.9 / 88.7 96.0 96.2+ ATask 91.288.5—– —– 65.2 / 77.3 90.2 / 86.3 90.8 / 87.0 95.8 95.7+ ATask+ ALang 91.688.4 —– —– 65.7 / 77.1 91.1 / 87.7 91.8 / 88.5 96.3 96.6+ ATask+ ALang+MONOTOK90.8 88.1 —– —–66.7/79.0 92.8/89.9 92.8/90.1 96.9 97.3\nID\nmBERT 93.7 93.593.1 91.4 71.2 / 82.185.0/78.4 85.9/79.393.393.5+ ATask 93.393.592.9 90.6 70.6 / 82.5 83.7 / 76.5 84.8 / 77.4 93.5 93.4+ ATask+ ALang 93.693.593.1 93.6 70.8 / 82.2 84.3 / 77.4 85.4 / 78.1 93.6 93.4+ ATask+ ALang+MONOTOK93.0 93.494.5 93.8 74.4/84.484.6 / 77.6 85.1 / 78.393.7 93.5\nKO\nmBERT 87.3 86.686.7 86.7 69.7 / 89.5 86.9 /83.2 89.2/85.795.8 96.0+ ATask 87.1 86.2 86.7 86.5 69.8 / 89.7 85.5 / 81.1 87.8 / 83.9 95.9 96.2+ ATask+ ALang 87.3 86.2 86.6 86.3 70.0 / 89.8 85.9 / 81.6 88.3 / 84.3 96.0 96.2+ ATask+ ALang+MONOTOK87.786.587.9 87.9 73.1/90.4 87.0/ 82.7 88.9 / 85.296.3 96.5\nTR\nmBERT 93.7 93.8 86.4 86.457.9 / 76.4 72.6 / 65.2 74.5 / 67.4 95.5 95.7+ ATask 93.0 93.0 86.1 83.9 55.3 / 75.1 70.4 / 62.0 72.4 / 64.1 95.5 95.7+ ATask+ ALang 93.3 93.5 86.2 84.8 56.9 / 75.8 71.1 / 63.0 73.0 / 64.7 96.0 95.9+ ATask+ ALang+MONOTOK92.7 92.7 86.1 85.360.0/77.0 73.5/65.6 75.7/68.1 96.4 96.3\nAVG\nmBERT 91.2 90.490.5 90.0 66.3 / 81.2 84.7 / 79.6 86.0 /81.095.6 95.6+ ATask 90.9 90.2 90.5 89.2 65.5 / 81.1 83.3 / 77.5 84.7 / 79.0 95.6 95.6+ ATask+ ALang 91.290.3 90.5 90.1 66.1 / 81.2 83.9 / 78.3 85.3 / 79.7 95.8 95.8+ ATask+ ALang+MONOTOK91.190.4 91.1 90.7 68.4/82.6 85.1/79.7 86.2/81.0 96.1 96.0\nTable 10: Full Results - Performance on the different\ntasks leveraging mBERT with different adapter compo-\nnents (see §4.4).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7796734571456909
    },
    {
      "name": "Task (project management)",
      "score": 0.7551599740982056
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6872730255126953
    },
    {
      "name": "Vocabulary",
      "score": 0.6761313676834106
    },
    {
      "name": "Multilingualism",
      "score": 0.5648161172866821
    },
    {
      "name": "Natural language processing",
      "score": 0.5618206262588501
    },
    {
      "name": "Representation (politics)",
      "score": 0.5470626354217529
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.5005788803100586
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42545557022094727
    },
    {
      "name": "Linguistics",
      "score": 0.34808897972106934
    },
    {
      "name": "Political science",
      "score": 0.054937779903411865
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}