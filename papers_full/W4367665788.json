{
    "title": "DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer",
    "url": "https://openalex.org/W4367665788",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2671061575",
            "name": "Li ZongRen",
            "affiliations": [
                "Xinjiang University"
            ]
        },
        {
            "id": "https://openalex.org/A1865918890",
            "name": "Wushouer Silamu",
            "affiliations": [
                "Xinjiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2099351573",
            "name": "Wang Yu-zhen",
            "affiliations": [
                "82th Hospital of Pla"
            ]
        },
        {
            "id": "https://openalex.org/A2169217893",
            "name": "Wei Zhe",
            "affiliations": [
                "82th Hospital of Pla"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6790275670",
        "https://openalex.org/W3140620488",
        "https://openalex.org/W4285505553",
        "https://openalex.org/W6786346207",
        "https://openalex.org/W4309746201",
        "https://openalex.org/W3150587331",
        "https://openalex.org/W4312757628",
        "https://openalex.org/W3148741900",
        "https://openalex.org/W4285507510",
        "https://openalex.org/W4285506639",
        "https://openalex.org/W6785795694",
        "https://openalex.org/W3142717613",
        "https://openalex.org/W6777881666",
        "https://openalex.org/W4286543538",
        "https://openalex.org/W3185052070",
        "https://openalex.org/W2160382843",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W3165957853",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W3146749430",
        "https://openalex.org/W4220872673",
        "https://openalex.org/W3015788359",
        "https://openalex.org/W4302363625",
        "https://openalex.org/W6795435739",
        "https://openalex.org/W4286544248",
        "https://openalex.org/W4221163766",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3201430889",
        "https://openalex.org/W4291511624",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4313573052",
        "https://openalex.org/W3009386672",
        "https://openalex.org/W4286544490",
        "https://openalex.org/W4221150837",
        "https://openalex.org/W4286541714",
        "https://openalex.org/W6743645564",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W6755875945",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W1815337875",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3171424841",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2963046541",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2751909359",
        "https://openalex.org/W3142164167",
        "https://openalex.org/W3151682712",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W3028279406"
    ],
    "abstract": "Aiming at the task of automatic brain tumor segmentation, this paper proposes a new DenseTrans network. In order to alleviate the problem that convolutional neural networks(CNN) cannot establish long-distance dependence and obtain global context information, swin transformer is introduced into UNet++ network, and local feature information is extracted by convolutional layer in UNet++. then, in the high resolution layer, shift window operation of swin transformer is utilized and self-attention learning windows are stacked to obtain global feature information and the capability of long-distance dependency modeling. meanwhile, in order to alleviate the secondary increase of computational complexity caused by full self-attention learning in transformer, deep separable convolution and control of swin transformer layers are adopted to achieve a balance between the increase of accuracy of brain tumor segmentation and the increase of computational complexity. on BraTs2021 data validation set, model performance is as follows: the dice dimilarity score was 93.2&#x0025;,86.2&#x0025;,88.3&#x0025; in the whole tumor,tumor core and enhancing tumor, hausdorff distance(95&#x0025;) values of 4.58mm,14.8mm and 12.2mm, and a lightweight model with 21.3M parameters and 212G flops was obtained by depth-separable convolution and other operations. in conclusion, the proposed model effectively improves the segmentation accuracy of brain tumors and has high clinical value.",
    "full_text": "Received 28 March 2023, accepted 24 April 2023, date of publication 1 May 2023, date of current version 5 May 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3272055\nDenseTrans: Multimodal Brain Tumor\nSegmentation Using Swin Transformer\nLI ZONGREN\n 1, WUSHOUER SILAMU\n 1, WANG YUZHEN\n 2, AND WEI ZHE\n2\n1College of Information Science and Engineering, Xinjiang University, Ürümqi 830047, China\n2Information Research and Development Center, 940th Hospital of the PLA Joint Logistic Support Force, Lanzhou 730050, China\nCorresponding author: Wushouer Silamu (lzr18993156814@163.com)\nThis work was supported by the National Natural Science Foundation of China under Grant 61433012.\nABSTRACT Aiming at the task of automatic brain tumor segmentation, this paper proposes a new\nDenseTrans network. In order to alleviate the problem that convolutional neural networks(CNN) cannot\nestablish long-distance dependence and obtain global context information, swin transformer is introduced\ninto UNet++ network, and local feature information is extracted by convolutional layer in UNet++. then,\nin the high resolution layer, shift window operation of swin transformer is utilized and self-attention learning\nwindows are stacked to obtain global feature information and the capability of long-distance dependency\nmodeling. meanwhile, in order to alleviate the secondary increase of computational complexity caused by full\nself-attention learning in transformer, deep separable convolution and control of swin transformer layers are\nadopted to achieve a balance between the increase of accuracy of brain tumor segmentation and the increase\nof computational complexity. on BraTs2021 data validation set, model performance is as follows: the dice\ndimilarity score was 93.2%,86.2%,88.3% in the whole tumor,tumor core and enhancing tumor, hausdorff\ndistance(95%) values of 4.58mm,14.8mm and 12.2mm, and a lightweight model with 21.3M parameters and\n212G flops was obtained by depth-separable convolution and other operations. in conclusion, the proposed\nmodel effectively improves the segmentation accuracy of brain tumors and has high clinical value.\nINDEX TERMS Brain tumor segmentation, convolutional neural networks, swin transformer, UNet++.\nI. INTRODUCTION\nAs one of the tumors with high fatality rate, brain tumor\nhas become an important factor endangering human life and\nhealth. from the perspective of tracing, brain tumors are\nusually divided into primary tumors and secondary tumors.\nprimary tumors refer to the tumors initially appearing in\nthe intracranial, which originate from the central nervous\nsystem and originate from intracranial neuroepithelial tis-\nsue, meningeal tissue cells and pineal cells. secondary brain\ntumors are relative to primary brain tumors. secondary brain\ntumors originate from the lungs, digestive tract, mammary\ngland, uterus and other organs of the body, and metastasize\nfrom other organs to the brain, usually with a relatively high\ndegree of malignancy. gliomas are the most common form\nof brain tumor, caused by cancerous glial cells in the brain\nand spinal cord. gliomas are classified into low grade (LGG)\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Liangxiu Han\n.\nand high grade (HGG) subtypes [1]. high grade gliomas\nare aggressive, grow rapidly, have poor survival prognosis,\nand usually require surgery and radiotherapy. as a reliable\ndiagnostic tool, magnetic resonance imaging (MRI) can real-\nize human examination by radiating energy signals from\ninternal substances to the surrounding environment through\nhigh-frequency magnetic field in vitro, which plays an impor-\ntant role in the analysis and detection of brain tumors.\nthere are usually four common 3D modes T1-weighted(T1),\nT1-weighted with gadolinium contrast enhancement(T1-Gd\nor T1c), T2-weighted(T2), and Fluid Attenuated Inversion\nRecovery (FLAIR). different MRI modes can effectively\ncomplement each other and fully subdivide the tumor in\nrelated areas, thus effectively improving the accuracy of seg-\nmentation. as can be seen from Figure 1, MRI data of differ-\nent morphologies captured different pathological features of\ntumors.\nMRI is the primary method of clinical detection of\nbrain tumors. segmentation of brain tumor regions from\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 42895\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nFIGURE 1. An example of multimodal MRI volumes for brain tumor segmentation. The green, blue, and yellow regions in the ground truth indicate\nedema (ED), non-enhancing tumor and necrosis (NCR/NET), and enhancing tumor (ET), respectively.\nmultimodal MRI images is helpful for treatment examination,\npost-diagnosis monitoring and effect evaluation of patients\n[2]. However, the segmentation of brain tumors in MRI was\nperformed manually by experienced radiographers in the\npast, which is time-consuming and may lead to inconsistent\nsegmentation results, because artificial segmentation mainly\ndepends on experience. the same medical image is segmented\nby different technicians with different results. therefore, many\nresearchers try to solve this problem by using the method\nof computer aided diagnosis to achieve semi-automatic seg-\nmentation. with the rapid development of machine learn-\ning technology, various automatic segmentation methods for\nbrain tumors emerge in an endless stream, while traditional\nsegmentation methods based on threshold, edge detection,\nclustering, region and registration have gradually faded out of\npeople’s attention due to their high complexity and low seg-\nmentation accuracy [3]. machine learning algorithms based\non feature selection and classification, such as random for-\nest, adaboost, kmeans clustering, support vector machines,\netc., still have limited segmentation performance. moreover,\nautomatic brain tumor segmentation remains a challenge due\nto extreme intrinsic heterogeneity in appearance, shape, and\nhistology.\nIn order to solve the above problems, and with the vig-\norous development of deep learning technology, researchers\nbegan to use computer deep learning technology to segment\nbrain tumors, and has achieved obvious advantages in seg-\nmentation accuracy. Since the birth of Fully Convolutional\nNetworks(FCN) architecture [4] and UNet architecture [5]\nin 2015. Due to their excellent encoder-decoder architecture,\nFCN and UNet have become increasingly popular in the field\nof brain tumor segmentation [6], [7], [8]. Myronenko et al. [9]\nproposed a multi-modal semantic segmentation method for\n3D brain tumors, which followed the UNet encoder decoder\narchitecture, and added variable autoencoder (V AE) branches\nto the network to reconstruct input images together with the\nsegmentation, so as to regularise the shared encoder [40].\nJiang et al. [10] designed a new two-level cascade UNet\nto segment brain tumors. the substructure of brain tumors\nwas trained end-to-end from coarse to fine. after that, the\ncrude segmentation map and the original image are input\ninto the second stage UNet, through which a more accu-\nrate segmentation map with more network parameters can\nbe provided. In recent years, Isensee et al. [11] applied\nnnU-Net network to brain tumor segmentation, and made\nspecific modifications by integrating brain tumor segmenta-\ntion, including post-processing, region-based training, etc.,\neffectively improving the accuracy of brain tumor segmen-\ntation. Luu et al. [12] applied the extended nnU-Net network\nto brain tumor segmentation, improved on the basis of nnU-\nNet, replaced batch normalization with group normaliza-\ntion, and improved nnU-Net by using axial attention in the\ndecoder, which further improved the accuracy of brain tumor\nsegmentation.\nHowever, because the current segmentation methods based\non brain tumors mostly rely on CNN and its variants, although\nCNN has achieved excellent performance, it cannot learn\nglobal and remote semantic information interaction well due\nto the locality of convolutional operation [13], [14], [15],\nlacks the ability to model long-term dependencies explicitly.\nlater, although some researchers have introduced the brilliant\ntransformer architecture in the field of Natural Language\nProcessing(NLP) into the field of image segmentation, trans-\nformer and its variants, such as vision transformer, require\na large number of data sets for pre-training. However, the\nlack of current medical image data sets prevents Trans-\nformer from further deepening in the field of medical image\nsegmentation.Under the scarcity restriction, many models\nmay under-perform in capturing meaningful patterns in the\ndata [44].\nIn view of this, this paper proposes a network architecture\nbased on DenseTrans, which can effectively solve the above\nproblems by combining the improvement of UNet++ and\nswin transformer. the main contributions of this paper are as\nfollows:\n(1) Combine swin transformer with the improved UNet++\nnetwork innovatively. firstly, extract features through CNN\nencoder, and then transfer extracted features into swin trans-\nformer through patch embedding in the high-resolution\nlayer of UNet++. use the swin transformer layer to learn\nlong-range dependencies and global context information.\nmoreover, due to the introduction of swin transformer,\n42896 VOLUME 11, 2023\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nlong-term dependency modeling using transformer no longer\nrequires a large number of medical image data sets for pre-\ntraining, which solves the problem that global dependency\nmodeling cannot be carried out when medical image data sets\nare scarce.\n(2) In order to make full use of the information of each\nmode in MRI, T1-weighted(T1), T1-weighted with gadolin-\nium contrast enhancement(T1-Gd or T1c), T2-weighted(T2),\nand Fluid Attenuated Inversion Recovery (FLAIR) modes\nwere used for pixel level fusion. the predictive ability of each\nmode to Enhancing Tumor(ET), Tumor Core(TC) and Whole\nTumor is explored to better perform pixel-level classification.\n(3) In order to alleviate the increasing complexity of net-\nwork structure and the increasing number of parameters\nand computational complexity, deep separable convolution\nwas introduced into DenseTrans network architecture, which\neffectively improved the efficiency of image segmentation\nand reduced the number of model parameters and computa-\ntional complexity.\n(4) Further optimize the training model through deep\nsupervision, assist the model to conduct pixel-level classifi-\ncation of brain tumors, and improve the segmentation accu-\nracy of brain tumors. More importantly, this paper uses deep\nsupervision to carry out hierarchical pruning of the model,\nso that the computational complexity of the deep network\nwith a large number of parameters can be greatly reduced\nwithin the acceptable accuracy range. Experimental results\non the BraTS 2021 and BraTS 2020 data sets demonstrate\nthe effectiveness of the method.\nThe specific sections of the article are arranged as follows:\nthe second section introduces the relevant work of the article,\nand the third section elaborates it in detail The improvement\nmethod and the concrete implementation process proposed\nin this paper, the fourth section through the experiment to\nverify the proposed method, the fifth section of this paper is\nsummarized.\nII. RELATED WORK\nA. UNet AND ITS VARIANTS\nSince the UNet network was proposed in 2015, it has effec-\ntively improved the accuracy of brain tumor segmentation.\nIn addition, the original purpose of UNet network is to solve\nthe problem of medical image segmentation. This network is\nmainly composed of Encoder layer, decoder layer and skip\nconnection layer. Encoder layer consists of 3 ×3 convolution\nlayer, Rectified Linear Unit(RELU) layer and maxPooling\nlayer. feature extraction was carried out by subsampling. The\ndecoder layer consists of the transposed convolution layer\nand Rectified Linear Unit(RELU) to form the upper sam-\npling layer. skip connection concat the features of encoder\nand decoder, providing multi-scale and multi-level informa-\ntion for the later image segmentation, effectively alleviating\nthe problem of space loss caused by downsampling in the\nstructure of single encoder, thus improving the accuracy of\nimage segmentation. traditional UNet designed a four-layer\nstructure, but the researchers speculated that a three-layer or\nfive-layer structure for different data sets could also improve\nthe segmentation accuracy. Zhou et al. [16] proposed the\nUNet++ network, designed a dense jump connection layer\non the basis of UNet, introduced a built-in UNet set of differ-\nent depths, thus improving the segmentation performance of\nobjects of different sizes, which is equivalent to integrating\nseveral UNet networks to train image segmentation at the\nsame time. by pruning the model of UNet ++ through deep\nsupervision, the pruned UNet ++ model achieved signif-\nicant acceleration, but the performance was only slightly\ndecreased. Milletari et al. [17] proposed a kind of UNet\nnetwork VNet, which introduced residual connections in the\ncontraction layer and the expansion layer, optimized the\nconvolutional neural network through residual connections,\nand used the convolution layer to replace the pooling layer\nduring downsampling. VNet replaced the pooling layer with\nthe convolution layer to further optimize the weights. Finally,\nthe network proposed the dice loss function of dynamic\nadjustment. when the sample categories were unbalanced, the\nformula was used to adjust the weights dynamically, and there\nwas no need to reweight the samples during training [43]. the\nsample imbalance is improved. Huang et al. [18] believed that\nthe skip connection layer of UNet++ fused the low-level fea-\ntures of the encoder and the high-level features of the decoder,\nwhich would lead to feature loss. therefore, the UNet3+\nnetwork with full-scale feature fusion was proposed. the\nfeatures of the decoder are fused with the lower and sibling\nfeatures of the encoder and the higher features of the decoder.\nQin et al. [19] proposed an improved UNet3+ network. In the\nencoder stage, phase residual network was adopted to replace\nthe original convolution layer, which improved the perfor-\nmance of network feature extraction to a certain extent and\navoided the phenomenon of gradient disappearance. In addi-\ntion, the batch normalization layer is replaced by the Filter\nResponse Normalization(FRN) layer [44], and the impact of\nbatch size on the network is avoided.\nB. TRANSFORMER AND ITS VARIANTS IN MEDICAL\nIMAGE SEGMENTATION\nAlthough CNN has achieved great success in the field of med-\nical image segmentation, due to the limitation of receptive\nfield in convolution operation, these methods are unable to\nestablish long-range dependence and global context connec-\ntion. In view of this, some researchers have introduced the\ntransformer architecture in the field of Natural Language Pro-\ncessing(NLP) to medical image segmentation. Vaswani et al.\n[20] believe that Recurrent Neural Networks(RNN) and other\nmodels are difficult to realize parallelization, and Recurrent\nNeural Networks(RNN) is difficult to remember long dis-\ntance features. In view of this, Transformer model is pro-\nposed, which uses weighted attention to make the model\ncan see all inputs. The multi-head attention mechanism is\nintroduced in order to project onto multiple Spaces to obtain\nmulti-scale features similar to the channels of convolutional\nneural networks. In the training process, a group of query\nfunctions with multiple attention headers are calculated at the\nVOLUME 11, 2023 42897\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nsame time, and then they are encapsulated in the matrix Q,\nthe functions matched by Q are encapsulated in the matrix K,\nthe corresponding key value is in V , d k represents the matrix\ndimension, and the output calculation matrix is:\nAttention(Qi, Ki, Vi) = softmax(QiKT\ni√\ndk\n)Vi (1)\nLong distance dependencies and global context informa-\ntion can be effectively captured through the Transformer\nmodel, but the Transformer is primarily used in the NLP\ndomain. Dosovitskiy et al. [21] applies transformer to the\nfield of computer vision and puts forward a Vision trans-\nformer model, which divides the input image into patch\nblocks and uses patch blocks to classify Transformer. patch\nblocks are taken as 1D input sequence and corresponding\nposition information is added. medical images are classi-\nfied at pixel level by transformer encoder and MLP layer.\nhowever, this model requires a large number of data sets\nfor pre-training, but medical image data sets are scarce.\nLiu et al. [22] proposed a new architecture based on\ntransformer, swin transformer. the model is a layered trans-\nformer, and its representation is shifted by the shifted win-\ndow. the shifted window scheme improves efficiency by\nlimiting self-focused calculations to non-overlapping local\nwindows while also allowing cross-window connections.\nCao et al. [23] proposed a pure swin transformer model\nof UNet. the model draws on UNet and includes encoder,\ndecoder, jump connection, connection layer, etc. the\nmulti-scale features are extracted by swin transformer from\nthe encoder. the patch expanding module is creatively pro-\nposed to increase the image resolution and reduce the feature\nchannel at the decoder layer, thus improving the image\nsegmentation accuracy. Wu et al. [24] proposes a transformer\nmodel based on 3DUNet architecture, which proposes a\nnew local attention (LSM) and global attention mechanism\n(GSM). GS-MSA mimics the vacuous convolution model,\nand selects a patch every fixed distance to form a global atten-\ntion unit, and the rest to form a unit, so as to extract global\nfeature information. Hatamizadeh et al. [25] introduced swin\ntransformers into the UNet network, used swin transformers\nas the encoder to extract features, and then input the extracted\nfeatures into the convolutional network to restore images\nthrough upsampling. The encoder features are passed to the\ndecoder through jump connections during upsampling, and\nresidual blocks are utilized at each layer of the decoder.\nIII. METHOD\nA. OVERALL ARCHITECTURE OF DenseTrans\nAlthough some researchers combine UNet with Transformer\nin an attempt to solve the problem that CNN cannot establish\nlong-distance dependency and extract global context infor-\nmation, directly combining CNN with Transformer cannot\nachieve the expected effect. the transformer model and its\nvariants have a weak ability to extract local information and\nshallow features, and transformer requires a large number of\ndata sets for pre-training.\nAs shown in Figure 2, DenseTrans is an improved dense\nhybrid model of UNet++ and swin transformer. DenseTrans\ncontains encoder,decoder dense jump connection layer. first\nof all, the MRI section X ∈ RH×W ×D×C , which H×W\nrepresents spatial resolution, D represents dimension, and C\nrepresents the number of channels. firstly, 3D CNN is used\nto extract local shallow features and context information.\nsince transformer cannot directly flatline pixels into 1D for\nattention calculation, it needs to calculate the attention weight\nafter Patch of the input image, and partial local features will\nbe ignored when patch is used as a unit for calculation.\ntherefore, for shallow features, 3D CNN is still used for\nfeature extraction. after two downsampling with 3D CNN, the\nobtained features are then introduced into swin transformer\nfor long-distance dependency modeling and global context\ninformation acquisition. the features processed by swin trans-\nformer encoder are converted into 3D dimensions through\nthe feature mapping layer. finally, the spatial resolution of\nthe features is restored through the decoder layer, and the\npixel-level classification is performed by the SoftMax layer to\ngenerate segmentation results. at the same time, the improved\nUNet++ model is adopted in the model, which is differ-\nent from the traditional UNet, which generally adopts four\ntimes of downsampling for spatial resolution restoration. after\ndownsampling twice with 3D CNN, swin transformer blocks\nare added to each layer. 3D CNN and swin transformer are\nused to capture features of different layers, connect them by\nsuperposition, and integrate more shallow UNet. The scale\ndifference of the feature map during fusion is smaller, and\ndeep supervision is added to each shallow UNet output,\nso that the complex depth network can greatly reduce the\nnumber of parameters within the acceptable accuracy range.\nMore importantly, in order to reduce the computational com-\nplexity and the number of parameters, we use depth-separable\nconvolution when using CNN downsampling.\nB. NETWORK ENCODER\nDifferent from pure swin transformer [23] to extract features\nby layered transformer construction of encoder, we took\nan MRI section of the input, the use of 3D CNN for two\ndown sampling to extract the local characteristics of shal-\nlow, alleviate the problem of weak extraction of local fea-\nture information in transformer. we adopted depth-separable\nconvolution to carry out the convolution operation. first, the\ndepth information was separated by 3 × 3×1 convolution,\nand then the channel fusion was carried out by 1 × 1×1\nconvolution. such operation can reduce the number of model\ntraining parameters and reduce the computational complex-\nity. after convolution, the maximum pooling is used for sam-\npling reduction, and the input image is gradually encoded\ninto low-resolution/high-order feature representation G ∈\nRK× H\n16 × W\n16 × D\n16 (the last layer of Encoder K=256). In this\ncase, G is integrated with rich local context information. the\nintegrated information is then passed into swin transformer\nto learn the long distance dependencies and global context\ninformation.\n42898 VOLUME 11, 2023\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nFIGURE 2. Overall architecture of the proposed DenseTrans.\nSwin Transformer Block:The model uses swin transformer\ninstead of transformer or vision transformer because the latter\nrequires a large data set for pre-training. some improvements\nbased on traditional techches transformer, first using the patch\nlayer partition the input image into a patch block, to convert\nthe characteristics of the channel, through the linear embed-\nding layer after converted into X ∈ RH×W ×D×K , unlike\ntraditional swin transformer, patch merging of image resolu-\ntion and adding feature channels is not used in the following\nmonths. because the model in this paper has already layered\nthe input image, subsampling and layered feature extraction\ndo not need to be done again. after swin transformer core\nW-MSA, its essence is to carry out transformer in a fixed\nwindow, and its computational complexity is as follows:\n(W − MSA) = 4hwD2 + 2hwM2D (2)\nW-MSA makes transformer for the local information\nwithin the small window, but it still needs to obtain the global\ninformation. SW-MSA is used to obtain the information\nbetween the windows, and then the windows are moved in\nVOLUME 11, 2023 42899\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nSW-MSA. cyclic shift and mask are used to make transformer\nfor the pixels within the moving window. In this way, the pixel\ninformation between the surrounding windows is obtained,\nand the global context information is obtained along with the\nmovement of the window, and the long-distance dependence\nrelationship is established. as the window moves, swin trans-\nformer is calculated as:\n∧\nZ l = W − MSA(LN(zl−1)) + zl−1 (3)\nzl = MLP(LN(∧\nZ l)) + ∧\nZ l (4)\n∧\nZ l + 1 = SW − MSA(LN(zl )) + zl (5)\nzl+1 = MLP(LN(∧\nZ l + 1)) + ∧\nZ l + 1 (6)\nLN represents the layer normalization, and z l represents\nthe swin transformer output of Layer L. the algorithm flow is\nshown in Figure 3.\nFIGURE 3. Swin transformer block.\nRelative Position Bias: swin transformer differs from\nvision transformer in the use of non-overlapping patches,\nwhich reduces the computational load and complexity. how-\never, when W-MSA fixes the local window, it cannot learn the\nglobal features. The information between SW-MSA learning\nwindows was introduced, which led to difficulties in position\nlearning. then, relative position coding was used to learn\nrelative position information, and the following was followed\nwhen calculating similarity and self-attention:\nAttention(Q, K, V) = SoftMax(QKT √\nd + T )V (7)\nwhere, Q, K and V respectively represent query coefficients,\ncorresponding key and value pairs queried, Q, K, V ∈\nRM2×M2\nand M2 represent the number of patches in the input\nimage, and represent relative position offset, T ∈ RM2×M2\n,\nwhich is used to represent the relative position information\nbetween patches.\nC. NETWORK DECODER\nCorresponding to the encoder is the symmetric decoder of\nthe DenseTrans block, in order to get the segmentation result\nof the original input image (H×W×D). We use 3D CNN\ntranspose convolution in the decoder for upsampling and\npixel-level segmentation of the extracted depth features. Sec-\nondly, the model class in this paper integrates multiple UNet,\nso depth supervision is added to each layer of UNet, so that\nthe depth network with a huge number of parameters can\nreduce the number of parameters significantly within the\nacceptable accuracy range.\nFeature Mapping. encoder extracts features, CNN is used\nfor subsampling, and then swin transformer is used for global\nfeature learning. before learning the features extracted by\nCNN, swin transformer will first input the feature graph\nPatchEmbedding, so the features processed by swin trans-\nformer cannot be directly used for upsampling of 3D CNN.\na feature mapping module is designed in this paper. ZL ∈\nRd×N of swin transformer is mapped to Xint ∈ Rd× H\n16 × W\n16 × D\n16 ,\nand then the features processed by feature mapping are used\nfor up-sampling.\n1) COMBINED FEATURE UPSAMPLING\nEach level of UNet in the model performs up-sampling after\nfeature mapping is completed. during the up-sampling pro-\ncess, concatenate the features after transposed convolution\nand the features transmitted by jump connections, so as to\nfurther improve the segmentation accuracy and obtain more\nabundant semantic information of global context. finally,\npixel-level segmentation is performed by SoftMax.\n2) DEEP SUPERVISION\nDeep Supervision is added into the model, aiming to make\nthe complex and redundant deep network significantly reduce\nthe number of parameters within the acceptable accuracy\nrange.1×1 convolution kernel is added to each level of super-\nvision after L 1,L2,L3,L4, which is equivalent to supervising\nthe output of each level or each UNet and analyzing the output\nresults, so as to reduce the number of model parameters and\ncomputational complexity within an acceptable range.\nIV. EXPERIMENTAL RESULTS AND DISCUSSION\nA. DATASET AND PRE-PROCESSING\nThe model uses BraTS 2021 as the benchmark data set to\ndemonstrate the proposed method. the BraTS 2021 dataset,\nprovided by the Brain Tumor Segmentation (BraTS) chal-\nleng, contained MRI scans from a total of 2000 patients.\namong them, there were 1251 cases of training set, 219 cases\nof verification set and 530 cases of test set. The train-\ning set contained the original image and the correspond-\ning annotations. the verification set was used to adjust the\nmodel but did not provide corresponding annotations. users\nneed to upload data segment to https://ipp.cbica.upenn.edu/\nfor assessment. each training sample contains data of four\nmodes: they are native T1-weighted(T1), T1-weighted with\n42900 VOLUME 11, 2023\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\ngadolinium contrast enhancement(T1-Gd or T1c), T2-\nweighted(T2), and Fluid Attenuated Inversion Recovery\n(FLAIR). All data sets were manually segmented by one to\nfour raters following the same protocol, and their markings\nwere approved by experienced board-certified neuroradiol-\nogists. Annotations included four categories: GD enhances\ntumor (ET-label 4), peritumoral edema/infiltrating tissue\n(ED-label 2), and necrotic tumor core (NCR-label 1), back-\nground (label 0). since the annotations of verification set and\ntest set are not published, the training set (1251) of BraTS\n2021 data set is divided into training set and test set in\nthe training stage. the dimension of each sample is 240 ×\n240×155, in which there are most background voxels. we cut\nthe sample into 128 ×128×128, and use contrast processing,\nnoise reduction processing and other pretreatment methods.\nthe second 3D data set for experimental verification is the\nBraTS 2020 data set, which is similar to the BraTS 2021 data\nset, both of which are data sets of MICCAI brain tumor\nsegmentation competition. the training set of this data set\ncontains 369 cases, and the validation set contains 125 cases.\nthe validation set and the test set are used for online evaluation\nwithout publishing corresponding annotations.\nB. IMPLEMENTATION AND EVALUATION METRICS\nThe method adopted in our experiment is consistent with\nmost previous experiments in the field. In the training stage,\nthe training set is divided according to 8:2 to conduct model\ntraining and adjustment, and the verification set is used to\nevaluate the model performance in the Inference stage.the\nproposed model is run in PyTorch framework with 8 NVIDIA\nRTX A5000 graphics cards (each with 24G of memory) for\n7000 epochs using a batch size of 12. For the optimizer,\nwe set the adam optimizer with an initial learning rate of\n0.0004. during optimization, the initial rate decays by a power\nof 0.8 in each iteration. in the processing process, images\nof the four modes of MRI were combined into a 4D image\n(C×H×W×D), where C=4. In addition, the following data\nenhancement techniques are applied in the processing pro-\ncess: (1) the original image (240 × 240×155) is randomly\ncropped to (128 × 128×128); (2) the image is simply rotated\nat the Angle {90,180,270}; (3) Contrast processing and Gaus-\nsian denoising.\n1) IMPLEMENTATION DETAILS\nOur model is trained from scratch in PyTorch, using Dense-\nTrans Net as a split network and adding a swin transform to\nthe encoder’s skip connections at a high level stage. In Dense-\nTrans Net, the contraction path has five layers, including bot-\ntleneck, each Layer is composed of 3 ×3×1 deep convolution\nand 1×1×1 channel convolution as well as Layer Normaliza-\ntion and reLu activation. the number of feature channels set in\nthe the first encoder is 16. then, the maximum pooling layer\nof 2 × 2×2 is used for downsampling, and the stride is 2.the\nnumber of channels is set to 32 in the second encoder, and the\nnumber of channels at the third and fourth layers as well as\nbottleneck layers is doubled in turn. in the skip connections,\nfeatures are flattened through the patch embedding layer, and\nthen swin transform is used to learn long-term dependency\nand global context information. at the decoder stage, feature\nmapping layer is used to convert the features processed by\nswin transform into Xint ∈ Rd× H\n16 × W\n16 × D\n16 . after that, trilineal\ninterpolation followed by 3 × 3×1 deep convolution and\n1 × 1×1 channel convolution are used for up-sampling. the\nlast layer is composed of 1 × 1×1 convolution, and the\nnumber of output channels is 3. The segmentation result is\nthen generated.\n2) EVALUATION METRICS\nPreviously, the commonly used evaluation metrics for brain\ntumor segmentation include (1) the Dice similarity coeffi-\ncient (DSC): DSC is used to measure the overlap between\nthe segmentation contour obtained by the proposed tumor\nsegmentation method and the manual contour described by\nexperienced doctors.\nDSC = 2 × |X∩ Y |\n|X| + |Y | (8)\n(2) the Hausdorff distance (HD): The surface area difference\nbetween the segmented profile and the manual profile was\nmeasured. HD is more sensitive to boundary segmentation\nand is used to assess the maximum difference between the\nsurface area of the segmented contour S and the correspond-\ning manual contour M.\nHausdorff 95distance\n= P95\n{\nSupx∈X d(x, Y ), Supy∈Y d(X, y)\n}\n(9)\nIn this paper, Dice and Hausdorff distance are used to eval-\nuate the segmentation results of the proposed model. to some\nextent, Hausdorff distance is the complement of Dice coef-\nficient, which can measure the maximum distance between\ntwo contour edges. raise the weight of outliers and punishes\noutliers.therefore, the combination of Dice coefficient and\nHausdorff distance is noisier and has stronger generalization\nand robustness than pure Dice metrics.\n3) OPTIMIZER\nIn order to obtain the global minimum in the model training\nstage, we conducted various experiments in the optimization\nof the back propagation loss function, including optimization\nalgorithms such as stochastic gradient descent (SGD), Adam\nand Momentum. Initially, we tried the SGD optimizer, but\nbecause it was sensitive to the learning rate of hyperpa-\nrameters, If the learning rate is too small, the convergence\nspeed will be too slow, and if the learning rate is too large,\nthe extreme point will be crossed, and the algorithm will\nbe easily stuck at the saddle point in the iterative process.\ntherefore, in the end, we choose Adam optimizer, which is an\noptimizer algorithm with momentum term, and uses gradient\nfirst-order matrix estimation and second-order matrix estima-\ntion to dynamically adjust the learning rate of parameters.\nsince the learning rate of each iteration of Adam has a certain\nVOLUME 11, 2023 42901\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nTABLE 1. Comparison of the proposed model with the classical brain tumor segmentation method on the BraTS 2021 data validation set.\nrange, the parameters are relatively stable, and the step size\nannealing process can be naturally realized, so it is more\nsuitable for large-scale data scenarios.\ngt ← ∇wft (wt−1) (10)\nwe first calculate the gradient, and the formula is shown\nin (10), where g represents the gradient and f represents the\nnoisy objective function. our goal is to calculate the expected\nvalue of the function f(w). after that, we update biased first\nmoment estimate.\nmt ← β1 · mt−1 + (1 − β1) · gt (11)\nβ1 coefficient is exponential decay rate and controls weight\ndistribution.After that, we update biased second raw moment\nestimate, and calculate the first order moment estimate and\nsecond order matrix estimate of deviation correction respec-\ntively, and finally update the parameters.\nwt ← wt−1 − α ·\n⋀\nm t(√⋀\nv t + ε) (12)\nwhere,\n⋀\nm t represents the first moment estimate after calculat-\ning the deviation, and\n⋀\nv t represents the second moment esti-\nmate after calculating the deviation.we set the initial learning\nrate to 0.0004, and the initial rate decays 0.8 powers in each\niteration.\nC. RESULT AND COMPARISONS\n1) BraTS 2021\nThe model proposed by us is different from the ordinary\nUNet model and also fundamentally different from the pure\ntransform model. In Table 1, the DenseTrans model proposed\nby us is compared with excellent models in recent years.\nthe segmentation accuracy of our model is 93.23%(DSC)\nand 4.58(HD) on WT, 86.2%(DSC) and 14.8(HD) on TC,\nand 88.3%(DSC) and 12.2(HD) on ET. comparing our pro-\nposed method with the Brats2021 Challenge champion model\nmethod (Extending nn-UNet), our segmentation accuracy on\nWT is improved by 0.4%, that on TC is reduced by 1.8%\nand that on ET is improved by 3.8%. on another evalua-\ntion metrics, Hausdorff distance metric, our model is better\n42902 VOLUME 11, 2023\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nTABLE 2. Comparison of the proposed model with the classical brain tumor segmentation method on the BraTS 2020 data validation set.\nthan Extending nn-UNet model. Extending nn-UNet model\nintegrates automatic segmentation of a variety of models,\nand axial attention mechanism is added to the Decoder pro-\ncess, which makes the model have an obvious effect on the\naccuracy of TC segmentation. our model obviously outper-\nforms the Extending nn-UNet model on both WT and ET,\ndemonstrating the benefits of applying the swin transform\nof the attention mechanism to establish global dependencies.\ncompared with the classical Swin Unter model, this model\nused Swin Transform to construct the Unet-like model for\nbrain tumor segmentation, which played a leading role in the\ncombination of UNet and swin transform. we improved the\nsegmentation accuracy by 0.4% on WT, 2.3% on TC, and\n2.5% on ET. on The whole, the DenseTrans model proposed\nby us has achieved good results in terms of the Dice scores,\nand improved the accuracy on both WT and ET. compared\nwith the Extending nn-UNet model and Swin Unter model,\nOn TC, our segmentation accuracy decreased slightly, mainly\nbecause the model set a lower weight for T1GD mode when\nthe initial input original image was used for multi-mode\nfusion, and T1GD mode was more suitable for detecting TC\nregion. In terms of Hausdorff distance metric, the Dense-\nTrans also demonstrated better performance. the experimen-\ntal results show that combining swin transform with improved\nUNet++ is helpful for long-term dependency modeling and\nglobal context information acquisition.\n2) QUANTITATIVE ANALYSIS\nThe segmentation results of our model are compared with the\nlatest segmentation results, and the quantitative comparison\nresults are shown in table 5. since our model combines CNN\nand Transformer for brain tumor segmentation, we will com-\npare them with models using CNN and Transformer alone.\nusing the same data set and the same input modes, all methods\nwere compared and performance was quantitatively assessed.\ncompared with the classical 3DUNet in convolutional neural\nnetworks, our original model has a great improvement in\nperformance, but also a significant increase in computational\ncomplexity. however, when the number of layers was reduced\nfrom L4 to L3 by combining deep separable convolution\nand pruning assisted by deep supervision, our computational\ncomplexity was significantly reduced. the pruning strategy\nwe adopted is shown in Figure 2. in the training stage of our\nmodel, because there are both forward and back propagation,\neach layer from L1 to L4 is used for weight updating. while\nin the Inference phase, the input image will only propagate\nforward, reducing the level of the model will not hinder the\noutput of the model. due to the use of deep supervision, each\nlayer from L1 to L4 produces corresponding segmentation\nresults. The experimental results show that the number of\nparameters in L3 layer is reduced more than that in L4 layer,\nand the performance is only slightly decreased. Compared\nwith TransBTS, a classical model using transformer, our\nVOLUME 11, 2023 42903\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nTABLE 3. Fold cross validation results on Brats2021 validation set.\nTABLE 4. Fold cross validation results on Brats2020 validation set.\nmodel not only improves segmentation accuracy but also\nreduces computational complexity.\n3) QUALITATIVE ANALYSIS\nIn FIG 4, we visualized the segmentation results of the pro-\nposed model on the Brats2021 data set, and conducted a 5 fold\ncross validation evaluation of the proposed method on the\nvalidation set. by comparing other experimental methods in\ntable 1 and table 2, our model has achieved great advantages\nin the Brats2021 data set, especially in WT and ET, and\nDice and Hausdorff metrics are significantly better than other\nsimilar methods. moreover, it can be seen from Figure 4 that\nour model more accurately describes brain tumors, and has\nobvious advantages for edge segmentation. by modeling the\nlong-term dependence between each volume through Swin\nTransform, a better segmentation mask can be generated.\ntable 3 and table 4 respectively show the cross-validation\nresults from the first to the fifth fold of our model on the\nBrats2021 and Brats2020 data sets and the average results of\nthe integration of five models. It can be seen from table 3\nthat on the Brats2021 data set, the 5-fold cross-validation\nmodel integrated by us has the best performance. the segmen-\ntation accuracy of the fourth-fold and fifth-fold is close to\nthat of the integrated model, but fluctuates greatly, while the\nperformance of Hausdorff95 is poor. similarly, the stability\nof the third and fifth fold in Table 4 is poor, and there are\npeaks. however, the segmentation accuracy of the model in\nour average integration 5 is higher, and the Hausdorff95 index\nalso has the best performance.\n4) BraTS 2020\nWe also conducted experimental verification on the\nBraTS 2020 data set. Since the modes and corresponding\nannotations of the BraTS 2020 data set are consistent with\nthose of BraTS 2021, we directly applied the hyperparameters\nof the BraTS 2021 data set to the BraTS 2020 data set\nfor verification. as the number of instances is about 1/5\nof the BraTS 2021 data set, the segmentation accuracy is\n42904 VOLUME 11, 2023\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nFIGURE 4. Visualized predicted images of different models. The green, red, and blue regions indicate edema (ED), non-enhancing tumor and\nnecrosis (NCR/NET), and enhancing tumor (ET), respectively.\nslightly reduced compared with the BraTS 2021 data set. the\nsegmentation accuracy was 91.4%(DSC) and 6.32(HD) on\nWT, 85.3%(DSC) and 16.9(HD) on TC, 82.3%(DSC) and\n15.2(HD) on ET. compared with classical networks such as\nTransBTS Net, 3DU-net, Dual-Path UNet and Scale Atten-\ntion Unet, our model has achieved significant advantages\nin two evaluation metrics, and the segmentation accuracy\nhas been significantly improved. This indicates that the deep\nfusion method of CNN and swin transfrom adopted by us has\na remarkable effect. compared with the traditional 3DU-net,\nit can be obviously seen that the segmentation accuracy of\nour model is increased by 2.8%(DSC) on WT, 1.0%(DSC)\non TC and 3.8%(DSC) on ET. meanwhile, the Hausdorff\ndistance is also lower than that of 3DU-net. this demonstrates\nthe importance of our improved swin transfrom learning\nlong-term dependencies and global context information.\nD. MODEL COMPLEXITY ANALYSIS\nSince the model proposed in this paper combines Swin\nTransform with the improved UNet ++, both the number of\nVOLUME 11, 2023 42905\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\nTABLE 5. Performance comparison with other state-of-art models.\ntraining parameters and computational complexity are\nimproved, with 51.82M parameters and 384GFlops, which is\na medium-scale model. later, we used deep separable convo-\nlution and deep supervision to reduce the number of layers\nin swin transformer during the model downsampling process\nto alleviate this problem. by reducing swin transformer from\nL4 to L3, we get a relatively lightweight DenseTrans model\nwith 21.3M parameters and 212GFlops, with only a slight\nperformance drop, due to the depth separable convolution and\nthe reduction in the number of swin transformer layers. com-\npared with TransBTS net model, the number of parameters is\nreduced by 21%, and the performance is greatly improved.\ncompared with 16.21M parameter and 1670GFlops of the\n3D UNet model, the number of parameters is not much\ndifferent, while the performance improvement is more obvi-\nous. our lightweight DenseTrans model has significant\nadvantages in terms of complexity and segmentation accu-\nracy compared to brain tumor segmentation models using\nTransformer.\nIn general, the DenseTrans model proposed in this paper,\nafter the introduction of swin transformer layer reduction\nand depth-separable convolution, obtains a lightweight net-\nwork. the number of network parameters and computa-\ntional complexity are shown in table5. compared with 3D\nUNet, TransBTS and Swin UNETR models, this lightweight\nmodel has fewer parameters and computational complex-\nity.the Dice Similarity Score was 92.8%, 85.8%, 87.2% in\nthe whole tumor,tumor core and enhancing tumor, Hausdorff\nDistance(95%) values of 5.32mm,15.8mm and 13.6mm. the\nperformance is obviously better than previous similar excel-\nlent models.\nE. DISCUSSION\nThe experimental results from Brats2021 and Brats2020 data\nsets show that the segmentation accuracy of the proposed\nmodel is significantly improved on both WT and ET by\ncomparing with the traditional excellent models Extend-\ning nn-UNet, Swin Unter, TransBTS Net and 3DU-net. the\nresults show the effectiveness and feasibility of the model,\nand further prove that the combination of swin transform\nand improved UNet++ is helpful for long-term dependency\nmodeling and global context information acquisition. as for\nthe slight decline in the segmentation accuracy on TC, this\nis due to the low weight setting of the T1GD mode in\nthe initial input multi-mode fusion, and the T1GD mode is\nmore suitable for the detection of TC region. according to\nthe experimental verification, if the weight of T1GD mode\nis adjusted during the multi-mode fusion, the segmentation\naccuracy of WT and ET regions will be affected. therefore,\nthe model is set according to the current superparameter to\nensure the optimization of segmentation accuracy. In terms of\ndetails, compared with Swin Unter and Extending nn-UNet,\nalthough the performance of our model is obviously improved\nin WT and extending nn-unet, the Dice coefficient in TC\nis reduced by 2.3% and 1.8% respectively. the more direct\nreason is that we combine the features extracted by CNN with\nthe feature layer of swin transformer in the high-resolution\nfilter layer of input features, and swin transformer adopts\n(4 × 4)patch as the self-attention weight of Token in a unit\nlearning window. then, the information between local win-\ndows is exchanged by shift window and stack operation to\nobtain global feature information, but this operation limits\nthe ability to obtain local semantic feature information. there-\nfore, compared with Swin Unter and Extending nn-UNet, the\nnecrotic and non-enhancing tumor parts in the TC region are\nlimited in the ability of our model to obtain local informa-\ntion, thus reducing the segmentation accuracy in this region.\nIn general, through comparative experimental analysis, our\nmodel has the following advantages over the best brain tumor\nsegmentation models: (a) It greatly improves the segmenta-\ntion accuracy of the whole tumor (WT) and the enhancing\ntumor (ET) regions. (b) compared with other brain tumor\n42906 VOLUME 11, 2023\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\ntransformer segmentation models, our lightweight model has\nfewer parameters.\nF. LIMITATIONS AND FUTURE WORK\nThe advantages of our proposed model are that it improves the\nsegmentation accuracy of multimodal brain tumors and effec-\ntively alleviates the problem of secondary increase of compu-\ntational complexity caused by the introduction of transformer\nin the field of medical images.\n1) LIMITATIONS\n(a) Problems of generality and uncertainty. our model is\ndesigned based on a specific brain tumor model, so it does not\nhave a good universality for other tumors. Secondly, multi-\nmodal fusion was carried out in model setting and experi-\nmental configuration, but it was only carried out for different\nsequences in MRI, without attempting to fuse CT, MRI and\nimage text.finally, because our experiment was conducted in\nMRI of brain tumors, there is uncertainty in our model for\nother tumors or other medical imaging methods such as CT.\n(b) Computational complexity. we use deep separable con-\nvolution and other operations to reduce the complexity of\nthe model. compared with similar brain tumor segmentation\nmodels in transformer, the complexity of our model is much\nlower. however, due to the introduction of full self-attention\nmechanism in transformer, the complexity of our model is\nstill higher than that of pure CNN.\n2) FUTURE WORK\n(a) Universality.we have collected CT, MRI images and\nimage reports of 45 cancer patients, and plan to establish a\ngeneral model to accurately segment tumors in different sites\nin the future.\n(b) Computational complexity.we will try to take mea-\nsures including concurrent multi-head self-attention learning\nmechanism to balance the increase of receptive field and\nthe secondary increase of computational complexity in trans-\nformer self-attention learning window.\n(c) Clinical practicability. Inspired by the work of [39],\n[41], and [42] and in order to improve the generality and\nrobustness of the model, we applied the feature extraction\nframework of the proposed model to the detection and classi-\nfication of brain tumors to build an excellent computer-aided\ndiagnosis system and provide accurate and reliable reference\nbasis for clinical practice.\nV. CONCLUSION\nIn this paper, we introduce the DenseTrans model, which\nis a new multimodal brain tumor segmentation model com-\nbined with improved UNet++ and Swin Transformer. the\nmodel uses the improved Encoder of UNet++ to extract\nlocal features, and then each layer in nested UNet transfers\nthe extracted features to swin transformer for learning the\nlong-distance dependency and obtaining the global context\ninformation. combining advantages of CNN and transformer,\nthe experimental results on the Brats2021 and Brats2020\ndataset show that the model can effectively improve the accu-\nracy of brain tumor segmentation. In future work, we will\ncontinue to study the lightweight aspect of DenseTrans model\nto achieve an efficient semantic segmentation model.\nREFERENCES\n[1] D. N. Louis, H. Ohgaki, O. D. Wiestler, W. K. Cavenee, P. C. Burger,\nA. Jouvet, B. W. Scheithauer, and P. Kleihues, ‘‘The 2007 WHO classifi-\ncation of tumours of the central nervous system,’’ Acta Neuropathologica,\nvol. 114, no. 2, pp. 97–109, Aug. 2007.\n[2] X. Guan, G. Yang, J. Ye, W. Yang, X. Xu, W. Jiang, and X. Lai, ‘‘3D AGSE-\nVNet: An automatic brain tumor MRI data segmentation framework,’’\nBMC Med. Imag., vol. 22, no. 1, pp. 1–18, Dec. 2022.\n[3] J. Liu, M. Li, J. Wang, F. Wu, T. Liu, and Y . Pan, ‘‘A survey of MRI-based\nbrain tumor segmentation methods,’’ Tsinghua Sci. Technol., vol. 19, no. 6,\npp. 578–595, Dec. 2014.\n[4] J. Long, E. Shelhamer, and T. Darrell, ‘‘Fully convolutional networks\nfor semantic segmentation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2015, pp. 3431–3440.\n[5] O. Ronneberger, P. Fischer, and T. Brox, ‘‘U-Net: Convolutional networks\nfor biomedical image segmentation,’’ in Proc. Int. Conf. Med. Image Com-\nput. Comput.-Assist. Intervent.Cham, Switzerland: Springer, Oct. 2015,\npp. 234–241.\n[6] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[7] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ‘‘Densely\nconnected convolutional networks,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jul. 2017, pp. 4700–4708.\n[8] G. Wang, W. Li, S. Ourselin, and T. Vercauteren, ‘‘Automatic brain tumor\nsegmentation using cascaded anisotropic convolutional neural networks,’’\nin Proc. Int. MICCAI Brainlesion Workshop, Sep. 2017, pp. 178–190.\n[9] A. Myronenko, ‘‘3D MRI brain tumor segmentation using autoencoder\nregularization,’’ in Proc. Int. MICCAI Brainlesion Workshop, Sep. 2018,\npp. 311–320.\n[10] Z. Jiang, C. Ding, M. Liu, and D. Tao, ‘‘Two-stage cascaded U-Net: 1st\nplace solution to brats challenge 2019 segmentation task,’’ in Proc. Int.\nMICCAI Brainlesion Workshop, Oct. 2019, pp. 231–241.\n[11] F. Isensee, P. F. Jäger, P. M. Full, P. V ollmuth, and K. H. Maier-Hein, ‘‘nnU-\nNet for brain tumor segmentation,’’ in Proc. Int. MICCAI Brainlesion\nWorkshop, Oct. 2020, pp. 118–132.\n[12] H. M. Luu and S. H. Park, ‘‘Extending nn-UNet for brain tumor\nsegmentation,’’ in Proc. Int. MICCAI Brainlesion Workshop, 2022,\npp. 173–186.\n[13] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille,\nand Y . Zhou, ‘‘TransUNet: Transformers make strong encoders for medical\nimage segmentation,’’ 2021, arXiv:2102.04306.\n[14] T. Magadza and S. Viriri, ‘‘Brain tumor segmentation using partial depth-\nwise separable convolutions,’’ IEEE Access, vol. 10, pp. 124206–124216,\n2022.\n[15] S. Liang, Z. Hua, and J. Li, ‘‘Transformer-based multi-scale feature fusion\nnetwork for remote sensing change detection,’’ J. Appl. Remote Sens.,\nvol. 16, no. 4, Nov. 2022, Art. no. 046509.\n[16] Z. Zhou et al., ‘‘UNet++: A nested U-Net architecture for medical\nimage segmentation,’’ in Proc. 4th Int. Workshop, DLMIA, 8th Int. Work-\nshop, ML-CDS, Conjunct. MICCAI. Granada, Spain: Springer, Sep. 2018,\npp. 3–11.\n[17] F. Milletari, N. Navab, and S.-A. Ahmadi, ‘‘V-Net: Fully convolutional\nneural networks for volumetric medical image segmentation,’’ in Proc. 4th\nInt. Conf. 3D Vis. (3DV), Oct. 2016, pp. 565–571.\n[18] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y . Iwamoto, X. Han,\nY .-W. Chen, and J. Wu, ‘‘UNet 3+: A full-scale connected UNet for medi-\ncal image segmentation,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess. (ICASSP), May 2020, pp. 1055–1059.\n[19] C. Qin, Y . Wu, W. Liao, J. Zeng, S. Liang, and X. Zhang, ‘‘Improved\nU-Net3+ with stage residual for brain tumor segmentation,’’ BMC Med.\nImag., vol. 22, no. 1, pp. 1–15, Dec. 2022.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\nVOLUME 11, 2023 42907\nL. Zongren et al.: DenseTrans: Multimodal Brain Tumor Segmentation Using Swin Transformer\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[22] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n‘‘Swin transformer: Hierarchical vision transformer using shifted win-\ndows,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 10012–10022.\n[23] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,\n‘‘Swin-UNet: UNet-like pure transformer for medical image segmenta-\ntion,’’ 2021, arXiv:2105.05537.\n[24] Y . Wu, K. Liao, J. Chen, J. Wang, D. Z. Chen, H. Gao, and J. Wu,\n‘‘D-former: A U-shaped dilated transformer for 3D medical image seg-\nmentation,’’Neural Comput. Appl., vol. 35, pp. 1931–1944, Oct. 2022.\n[25] A. Hatamizadeh, V . Nath, Y . Tang, D. Yang, H. R. Roth, and D. Xu,\n‘‘Swin UNETR: Swin transformers for semantic segmentation of brain\ntumors in MRI images,’’ in Proc. Int. MICCAI Brainlesion Workshop,\n2022, pp. 272–284.\n[26] H. Peiris, Z. Chen, G. Egan, and M. Harandi, ‘‘Reciprocal adversarial\nlearning for brain tumor segmentation: A solution to BraTS challenge 2021\nsegmentation task,’’ 2022, arXiv:2201.03777.\n[27] H. Jia, C. Bai, W. Cai, H. Huang, and Y . Xia, ‘‘HNF-Netv2 for brain tumor\nsegmentation using multi-modal MR imaging,’’ 2022, arXiv:2202.05268.\n[28] A. S. Akbar, C. Fatichah, and N. Suciati, ‘‘UNet3D with multiple atrous\nconvolutions attention block for brain tumor segmentation,’’ in Proc. Int.\nMICCAI Brainlesion Workshop, 2022, pp. 182–193.\n[29] Z. Li, Z. Shen, J. Wen, T. He, and L. Pan, ‘‘Automatic brain tumor seg-\nmentation using multi-scale features and attention mechanism,’’ in Proc.\nInt. MICCAI Brainlesion Workshop, 2022, pp. 216–226.\n[30] P. Ahmad, S. Qamar, L. Shen, S. Q. A. Rizvi, A. Ali, and G. Chetty,\n‘‘MS UNet: Multi-scale 3D UNet for brain tumor segmentation,’’ in Proc.\nInt. MICCAI Brainlesion Workshop, 2022, pp. 30–41.\n[31] K. Pawar, S. Zhong, D. S. Goonatillake, G. Egan, and Z. Chen,\n‘‘Orthogonal-nets: A large ensemble of 2D neural networks for 3D brain\ntumor segmentation,’’ in Proc. Int. MICCAI Brainlesion Workshop, 2022,\npp. 54–67.\n[32] J. Roth, J. Keller, S. Franke, T. Neumuth, and D. Schneider, ‘‘Multi-plane\nUNet++ ensemble for glioblastoma segmentation,’’ in Proc. Int. MICCAI\nBrainlesion Workshop, 2022, pp. 285–294.\n[33] Y . Wang, Y . Zhang, F. Hou, Y . Liu, J. Tian, and C. Zhong, and Z. He,\n‘‘Modality-pairing learning for brain tumor segmentation,’’ in Proc. Int.\nMICCAI Brainlesion Workshop, 2021, pp. 230–240.\n[34] T. Henry, A. Carré, M. Lerousseau, T. Estienne, C. Robert, N. Paragios,\nand E. Deutsch, ‘‘Brain tumor segmentation with self-ensembled, deeply-\nsupervised 3D U-Net neural networks: A BraTS 2020 challenge solution,’’\nin Proc. Int. MICCAI Brainlesion Workshop, 2020, pp. 327–339.\n[35] W. Jun, X. Haoxiang, and Z. Wang, ‘‘Brain tumor segmentation using dual-\npath attention U-Net in 3D MRI images,’’ in Proc. Int. MICCAI Brainlesion\nWorkshop, 2021, pp. 183–193.\n[36] C. Liu, W. Ding, L. Li, Z. Zhang, C. Pei, L. Huang, and X. Zhuang, ‘‘Brain\ntumor segmentation network using attention-based fusion and spatial rela-\ntionship constraint,’’ in Proc. Int. MICCAI Brainlesion Workshop, 2021,\npp. 219–229.\n[37] M. D. Cirillo, D. Abramian, and A. Eklund, ‘‘V ox2V ox: 3D-GAN for brain\nTumour segmentation,’’ inProc. Int. MICCAI Brainlesion Workshop, 2021,\npp. 274–284.\n[38] Y . Yuan, ‘‘Automatic brain tumor segmentation with scale attention net-\nwork,’’ in Proc. Int. MICCAI Brainlesion Workshop, 2021, pp. 285–294.\n[39] V . Rajinikanth, S. Kadry, and Y . Nam, ‘‘Convolutional-neural-network\nassisted segmentation and SVM classification of brain tumor in clinical\nMRI slices,’’ Inf. Technol. Control, vol. 50, no. 2, pp. 342–356, Jun. 2021.\n[40] S. Kadry, R. Damasevicius, D. Taniar, V . Rajinikanth, and I. A. Lawal,\n‘‘U-Net supported segmentation of ischemic-stroke-lesion from brain MRI\nslices,’’ in Proc. 7th Int. Conf. Bio Signals, Images, Instrum. (ICBSII),\nMar. 2021, pp. 1–5.\n[41] S. Maqsood, R. Damaševičius, and R. Maskeli ¯unas, ‘‘Multi-modal brain\ntumor detection using deep neural network and multiclass SVM,’’ Medic-\nina, vol. 58, no. 8, p. 1090, Aug. 2022.\n[42] S. Maqsood, R. Damasevicius, and F. M. Shah, ‘‘An efficient approach\nfor the detection of brain tumor using fuzzy logic and U-NET CNN\nclassification,’’ in Proc. Int. Conf. Comput. Sci. Appl.Cham, Switzerland:\nSpringer, Sep. 2021, pp. 105–118.\n[43] M. Frank, D. Drikakis, and V . Charissis, ‘‘Machine-learning methods for\ncomputational science and engineering,’’ Computation, vol. 8, no. 1, p. 15,\nMar. 2020.\n[44] K. Poulinakis, D. Drikakis, I. W. Kokkinakis, and S. M. Spottswood,\n‘‘Machine-learning methods on noisy and sparse data,’’ Mathematics,\nvol. 11, no. 1, p. 236, Jan. 2023.\nLI ZONGREN received the Ph.D. degree and\nthe master’s degree in software engineering from\nLanzhou Jiaotong University, Lanzhou, China, in\n2013 and 2019, respectively. He is currently pursu-\ning the Ph.D. degree in computer science and tech-\nnology with Xinjiang University, Ürümqi, China,\nin 2020. His current research interests include\nmedical image analysis and medical data security.\nWUSHOUER SILAMU was born in Greenwich\nWushuer Lamu, Yili, Xinjiang, in October 1941.\nHe is currently a Multilingual Information Pro-\ncessing Expert, an Academician of the Chinese\nAcademy of Engineering, and a Professor and a\nDoctoral Supervisor with Xinjiang University.\nWANG YUZHEN received the master’s degree.\nShe is currently a Senior Engineer, the Deputy\nDirector of the Information Department, 940th\nHospital of the PLA Joint Logistic Support Force,\nand a Medical Information Field Expert, long-\nterm engaged in medical information manage-\nment, technology research and development, and\nscientific research.\nWEI ZHE was born in Baoji, Shanxi, in October\n1963. He is currently pursuing the Ph.D. degree.\nHe is also a master’s supervisor with 30 years\nof experience in biomedical engineering and infor-\nmation engineering.\n42908 VOLUME 11, 2023"
}