{
  "title": "Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?",
  "url": "https://openalex.org/W4376139957",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5052970724",
      "name": "Eunice Yiu",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5053383859",
      "name": "Eliza Kosoy",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5062253481",
      "name": "Alison Gopnik",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2892734197",
    "https://openalex.org/W4212844288",
    "https://openalex.org/W1970243396",
    "https://openalex.org/W4247417260",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W2995859172",
    "https://openalex.org/W2074028900",
    "https://openalex.org/W6980851464",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2097625105",
    "https://openalex.org/W2103152357",
    "https://openalex.org/W6628920117",
    "https://openalex.org/W4200466014",
    "https://openalex.org/W2170333515",
    "https://openalex.org/W2037889091",
    "https://openalex.org/W2890250492",
    "https://openalex.org/W2736660860",
    "https://openalex.org/W2021157664",
    "https://openalex.org/W1797402740",
    "https://openalex.org/W4223947928",
    "https://openalex.org/W2582860102",
    "https://openalex.org/W2895999345",
    "https://openalex.org/W7075718831",
    "https://openalex.org/W2136530632",
    "https://openalex.org/W3162021500",
    "https://openalex.org/W4287817350",
    "https://openalex.org/W2132573777",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4283080831",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W1873606094",
    "https://openalex.org/W2060993602",
    "https://openalex.org/W2098065033",
    "https://openalex.org/W6948936239",
    "https://openalex.org/W3099990773",
    "https://openalex.org/W2156834836",
    "https://openalex.org/W2171960331",
    "https://openalex.org/W1980862600",
    "https://openalex.org/W2089915875",
    "https://openalex.org/W2154167968",
    "https://openalex.org/W4285105218",
    "https://openalex.org/W4211219997",
    "https://openalex.org/W1582069037",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W2094101853",
    "https://openalex.org/W2003767505",
    "https://openalex.org/W1996613012",
    "https://openalex.org/W6999939688",
    "https://openalex.org/W4200057797",
    "https://openalex.org/W2255750102",
    "https://openalex.org/W2048269309",
    "https://openalex.org/W2016092929",
    "https://openalex.org/W1811781384"
  ],
  "abstract": "Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. We argue that these artificial intelligence models are cultural technologies that enhance cultural transmission in the modern world, and are efficient imitation engines. We explore what AI models can tell us about imitation and innovation by evaluating their capacity to design new tools and discover novel causal structures, and contrast their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. Critically, our findings suggest that machines may need more than large scale language and images to achieve what a child can do.",
  "full_text": " 1 \n \n \n \nImitation versus Innovation: What children can do that large language and \nlanguage-and-vision models cannot (yet)? \n \n \n \n \nEunice Yiu1, Eliza Kosoy1, & Alison Gopnik1 \n \n \n \n \n \n \n \n \n \n \n1 Department of Psychology, University of California - Berkeley, 2121 Berkeley Way, Berkeley, \nCA, 94704, U.S.A.  \n \nCorresponding author: Eunice Yiu, University of California - Berkeley, 2121 Berkeley Way, \nBerkeley, CA, 94704, U.S.A.  \nEmail: ey242@berkeley.edu \n \n 2 \nAbstract \nMuch discussion about large language models and language-and-vision models has \nfocused on whether these models are intelligent agents. We present an alternative perspective. \nWe argue that these artificial intelligence models are cultural technologies that enhance cultural \ntransmission in the modern world, and are efficient imitation engines. We explore what AI \nmodels can tell us about imitation and innovation by evaluating their capacity to design new \ntools and discover novel causal structures, and contrast their responses with those of human \nchildren. Our work serves as a first step in determining which particular representations and \ncompetences, as well as which kinds of knowledge or skill, can be derived from particular \nlearning techniques and data. Critically, our findings suggest that machines may need more than \nlarge scale language and images to achieve what a child can do.  \n \nKeywords \ninnovation, imitation, tool use, causal learning, children, large language models  \n \n \n \n \n \n \n \n \n \n 3 \nIntroduction \nRecently, large language and language-and-vision models, such as OpenAI’s GPT and \nDALL-E, have sparked much interest and discussion. These systems are trained on an \nunprecedentedly large amount of image and text data and are built with billions of parameters. \nThe systems generate novel text or images in response to prompts. Typically, they are pretrained \nwith a relatively simple objective such as predicting the next item in a string of text correctly. In \nmore recent systems, they are also fine-tuned through reinforcement learning methods – humans \njudge the texts and images the systems generate, and so further shape what the systems produce.  \nA common way of thinking about these systems is to treat them as individual agents, and \nthen debate how intelligent those agents are. The phrase “an AI” rather than “AI” or “AI \nsystem,” implying individual agency, is frequently used. Some have claimed that these models \ncan tackle complex commands, perform abstract reasoning, such as inferring theory of mind \n(e.g., Kosinski, 2023) and demonstrate creativity (e.g., Summers-Stay et al., 2023). \nWe argue that this framing is wrong. Instead, we argue that the best way to think of these \nsystems is as powerful new cultural technologies, analogous to earlier technologies like writing, \nprint, libraries, internet search and even language itself. Large language and vision models \nprovide a new method for easy and effective access to the vast amount of text that others have \nwritten and images that others have shaped. These AI systems offer a new means for cultural \nproduction and evolution, allowing information to be passed efficiently from one group of people \nto another (Bolin, 2012; Boyd & Richerson, 1988; Henrich, 2018).  \nFurthermore, we argue that large language and vision models provide us with an \nopportunity to discover which representations and cognitive capacities, in general, human or \nartificial, can be acquired purely through cultural transmission itself and which require \n 4 \nindependent contact with the external world.  One central question in cognitive science is how \nmuch the meaning of a word, an object or a concept can be learned from the distribution of \ndisembodied and amodal symbols in language (Grand et al., 2022; Landauer & Dumais, 1997; \nPiantadosi, 2023), and how much meaning depends on grounded perceptual and motor \ninteractions with the world (Barsalou, 2008; Gibson, 1979).  \n This contrast extends beyond perceptual and motor representations themselves. For \nexample, the kinds of causal representations that are embodied in theories, either formal \nscientific theories or intuitive theories are the result of truth-seeking epistemic processes (e.g., \nGopnik & Wellman, 2012; Harris et al., 2018). They are evaluated with respect to an external \nworld and make predictions about and shape actions in that world. New evidence from that world \ncan radically revise them. Representations like these are designed to solve “the inverse problem” \n– the problem of reconstructing the structure of a novel, changing, external world from the data \nthat we receive from that world. Although such representations may be very abstract, as in \nscientific theories, they ultimately depend on perceptual and motor abilities – they depend on \nbeing able to perceive the world and act on it in new ways. \nThese truth-seeking epistemic processes contrast with the processes that allow faithful \ntransmission of representations from one agent to another, regardless of the relation between \nthose representations and the external world. Such transmission may be crucial for abilities like \nlanguage learning and social coordination. There is considerable evidence that mechanisms for \nthis kind of faithful transmission are in place early in development and play a particularly \nimportant role in human cognition and culture (Meltzoff & Moore, 1977). LLM’s enable and \nfacilitate this kind of transmission. \n 5 \nThis contrast between transmission and truth is in turn closely related to the \nimitation/innovation contrast in discussions of cultural evolution (Tomasello et al., 1993; Legare \nand Nielsen, 2015, Boyd & Richerson, 1988; Henrich, 2018). Cultural evolution depends on the \nbalance between these two different kinds of cognitive mechanisms. Imitation allows the \ntransmission of knowledge or skill from one agent to another (Boyd et al., 2011; Henrich, 2016). \nInnovation produces novel knowledge or skill through contact with a changing world (Derex, \n2022). Imitation means that each individual agent does not have to innovate – they can take \nadvantage of the cognitive discoveries of others. But imitation by itself would be useless if some \nagents did not also have the capacity to innovate. It is the combination of the two that allows \ncultural and technological progress.  \nHowever, in any given case, it may be difficult to determine which kinds of cognitive \nmechanisms produced a particular kind of representation or behavior, knowledge or skill. For \nexample, my answer to an exam question in school might simply reflect the fact that I have \nremembered what I was taught. Or it might indicate that I have knowledge that would allow me \nto make novel predictions about or perform novel actions on the external world.  Probing the \nresponses of large models may give us a tool to help answer that question – at least, in principle. \nIf large models can reproduce particular competencies, for example, producing grammatical text \nin response to a prompt, that suggests that those abilities can be developed through imitation – \nextracting existing knowledge encoded in the minds of others. If not, that suggests that these \ncapacities may require innovation – extracting knowledge from the external world. \nIn this paper, we explore what state-of-the-art large language and language-and-vision \nmodels can contribute to our understanding of imitation and innovation. We contrast the \n 6 \nperformance of models trained on a large corpus of text data, or text and image data, with that of \nchildren.   \nLarge language and language-and-vision models as imitation engines. \nImitation refers to the behavior of copying or reproducing features or strategies \nunderlying a model’s behavior (Heyes, 2001; Tomasello, 1990). By observing and imitating \nothers, individuals acquire the skills, knowledge, and conventions that are essential to effectively \nparticipate in their cultural groups, promoting cultural continuity over time. An assortment of \ntechnological innovations such as writing, print, internet search and we would argue, LLM’s, \nhave made this imitation much more effective over time.  \nMoreover, cultural technologies not only allow access to information, but they also \ncodify, summarize, and organize that information in ways that enable and facilitate transmission. \nLanguage itself works by compressing information into a digital code. Writing and print \nsimilarly abstract and simplify from the richer information stream of spoken language, at the \nsame time that they allow wider temporal and spatial access to that information. Print, in \naddition, allows many people to receive the same information at the same time, and this is, of \ncourse, highly amplified by the internet. At the same time, devices such as indexes, catalogs, and \nlibraries, and more recently, Wiki’s and algorithmic search engines, allow humans to quickly \nfind relevant text and images and use those texts and images as a springboard to generate \nadditional text and images. \nDeep learning models trained on large datasets today excel at imitation in a way that far \noutstrips earlier technologies and so represent a new phase in the history of cultural technologies. \nLarge language models such as Anthropic’s Claude and OpenAI’s GPT can use the statistical \npatterns in the text in their training sets to generate a variety of new text, from emails and essays \n 7 \nto computer programs and songs. GPT-3 can imitate both natural human language patterns and \nparticular styles of writing close to perfectly. It arguably does this better than many people \n(Zhang and Li, 2021). Strikingly and surprisingly, the syntactic structure of the language \nproduced by these systems is accurate. There is some evidence that large language models can \neven grasp language in more abstract ways and imitate human figurative language understanding \n(e.g., Jeretic et al., 2020; Stowe et al., 2022). This suggests that finding patterns in large amounts \nof human text may be enough to pick up many features of language, independent of any \nknowledge about the external world. Similarly, large diffusion-based-text-to-image vision and \nlanguage models such as DALLE-2 impressively create images that imitate a plethora of existing \nartistic styles and concepts (Fig. 1) from textual descriptions. \n(a)                                              (b)              (c)  \n                       \nFig. 1. Sample output images from DALLE-2 for the captions (a) “Mona Lisa painted by \nVincent van Gogh,” (b) “a computer from the 90s in the style of vaporwave” and (c) “high \nquality photo of a monkey astronaut.” \n \nIn turn, this raises the possibility that human children learn features of language or \nimages in a similar way. In particular, this discovery has interesting connections to the large \nbody of empirical literature showing that infants are sensitive to the statistical structure of \nlinguistic strings and visual images from a very young age (e.g., Kirkham, Slemmer & Johnson, \n2002; Saffran, Aslin & Newport, 1996). The LLMs suggest that this may enable much more \npowerful kinds of learning than we might have thought, such as the ability to learn complex \nsyntax.  \n\n 8 \nOn the other hand, although these systems, like children, are skilled imitators, their \nimitation may differ from that of children in important ways. There are debates in the \ndevelopmental literature about how much childhood imitation simply reflects faithful cultural \ntransmission (as in the phenomenon of over-imitation, where children reproduce unnecessary \ndetails of an action) and how much it is shaped by and in the service of broader truth-seeking \nprocesses such as understanding the goals and intentions of others. Children can meaningfully \ndecompose observed visual and motor patterns in relation to the agent, target object, movement \npath and other salient features of events (Bekkering, Wohlschlager & Gattis, 2000; Gergely, \nBekkering & Király, 2002). Moreover, children distinctively copy intentional actions (Meltzoff, \n1995), discarding apparently failed attempts, mistakes, and causally inefficient actions (Schulz, \nHooppell and Jenkins, 2008; Buchsbaum et al., 2011) when they seek to learn skills from \nobserving other people (Over & Carpenter, 2013). While the imitative behavior of large language \nand vision models can be viewed as the abstract mapping of one pattern to another, human \nimitation appears to be mediated by goal representation and the understanding of causal structure \nfrom a young age. It would be interesting to see if large models also replicate these features of \nhuman imitation. \nCan large language and language-and-vision models innovate? \nI. Can LLM’s design new tools? \n The most ancient representative of the human genus is called Homo habilis (“handy \nman”) due to their ability to design and use novel stone tools. Tool use is one of the best \nexamples of the advantages of cultural transmission, and of the balance between imitation and \ninnovation. Imitation allows a novice to observe a model and reproduce their actions to bring \n 9 \nabout a particular outcome, even without really understanding the causal properties of the tool. \nTechniques like “behavior cloning” in AI and robotics use a similar approach. \nAgain however, the ability to imitate existing tools depends on the parallel ability to \ndesign new tools. Tool innovation is an indispensable part of human lives, and it has also been \nobserved in a variety of nonhuman animals such as crows (Von Bayern et al., 2009) and \nchimpanzees (Whiten, Horner & de Waal, 2005). Tool innovation has often been taken to be a \ndistinctive mark of intelligence in biological systems (Emery & Clayton, 2004; Reader & \nLaland, 2002). \nTool use can then be an interesting point of comparison between large models and \nchildren. We might predict that these models will generate text and images that capture familiar \ntool uses – for example, predicting appropriately that a hammer should be used to bang in a nail. \nHowever, these systems might have more difficulty producing the right responses for tool \ninnovation, which depends on discovering and using new causal properties and affordances. We \nmight, however, also wonder whether young children can themselves perform this kind of \ninnovation, or whether it depends on explicit instruction and experience. In an ongoing study of \ntool innovation (Yiu & Gopnik, in press), we have investigated whether human children and \nadults can insightfully use familiar objects in new ways to accomplish particular outcomes, and \ncompared the results to the output of large deep learning models such as GPT-3 and DALLE-2.  \nTool innovation can involve designing new tools, but it can also refer to using old tools in \nnew ways to solve novel problems (Rawlings and Legare, 2021). Our experiment examines the \nlatter type of tool innovation. Physically building a new tool from scratch and then executing a \nseries of actions that lead to a desired goal is a difficult task for young children (Beck et al., \n2011). So instead, we study the ability to recognize new functions in everyday objects and to \n 10 \nselect appropriate object substitutes in the absence of typical tools to solve various physical \ntasks.  \nOur study has two components: an “imitation” component and an “innovation” \ncomponent. In the innovation part of the study, we present a series of problems in which a goal \nhas to be executed in the absence of the typical tool (e.g., holding a hot cup of coffee in the \nabsence of a mug sleeve). We then provide alternative objects for participants to select: (1) an \nobject that is more superficially similar to the typical tool, that is the mug sleeve, and is \nassociated with it but is not functionally relevant to the context (e.g., a coffee lid), (2) an object \nthat is superficially dissimilar but has the same affordances and causal properties as the typical \ntool (e.g., a plant pot), and (3) a totally irrelevant object (e.g., a succulent). In the imitation part \nof the study, we present the same sets of objects but ask participants to select which of the object \noptions would “go best” with the typical tool (e.g., a mug sleeve and a coffee lid should go \ntogether).  \nSo far, we have found that both children aged three to seven years old (n = 42, Mage = \n5.71 years, SD = 1.24) and adults (n = 30, Mage = 27.80 years, SD = 5.54) can recognize common \nsuperficial relationships between objects when they are asked which objects should go together \n(Mchildren = 88.4%, SEchildren = 2.82%; Madults = 84.9%, SEadults = 3.07%). But they can also \ndiscover new functions in everyday objects to solve novel physical problems and so select the \nsuperficially unrelated but functionally relevant object. (Mchildren = 85.2%, SEchildren = 3.17%; \nMadults = 95.7%, SEadults = 1.04%). \nUsing exactly the same questions that we used to test our human participants, we queried \nOpenAI’s gpt-3.5-turbo and text-davinci-003 models, Anthropic’s Claude, and Google’s FLAN-\nT5 (XXL) and BigScience’s Bloomz. As we predicted we found that these large language \n 11 \nmodels are almost as capable of identifying superficial commonalities between objects as \nhumans are. They are sensitive to the associations between the objects, and they excel at our \nimitation tasks – they respond that the coffee lid goes with the hot coffee. However, they are \nmuch less capable than both adults and children when they are asked to select a novel functional \ntool to solve a problem – they again choose the coffee lid rather than the plant pot to hold the hot \ncoffee. This suggests that simply learning from large amounts of existing language may not be \nsufficient to achieve grounded tool innovation. Discovering novel functions in everyday tools is \nnot about finding the statistically nearest neighbor from lexical co-occurrence patterns. Rather, it \nis about appreciating the more abstract functional analogies and causal relationships between \nobjects that do not necessarily belong to the same category or are associated in text. Compared to \nhumans, large language models are not as successful at this type of innovation task. On the other \nhand, they excel at generating responses that simply demand some abstraction from existing \nknowledge. \n Perhaps this reflects the fact that these tasks involve visual and spatial information rather \nthan just text. To explore this possibility, we tested whether large vision and language models \nsuch as DALLE-2 can depict how common physical problems like those in our experiment can \nbe solved. We looked both at how these systems represent the use of typical tools versus the \ninnovative tools that our human participants selected. DALLE-2 was reasonably good at \nproducing images that capture how typical tools are commonly used (Fig. 2(a), (c), (e)). \nHowever, when a more unconventional tool was mentioned in the caption, the system could not \nappropriately piece together how these objects can be applied to achieve the described goals \n(Fig. 2(b), (d), (f)). \n 12 \n(a) using a cake knife to cut a cake                            (b) using a hanger to cut a cake \n        \n(c) using a water can to water plants    (d) using a ziplock bag to water plants \n        \n(e) using a mug sleeve to hold a hot cup of coffee (f) using an empty plant pot to hold a hot cup of \ncoffee \n              \ntypical tool       novel tool \nFig. 2. Examples of outputs by DALLE-2 for solving physical problems using typical tools and \nnovel tools.  \n \nAll in all, creativity and innovation refer to the production of ideas and products that are \nboth novel and useful given the constraints of a certain context (Barron, 1955; Runco & Jaeger, \n2012). This does not seem to be consistently achievable in current large language models yet. \nThis reflects the lack of relation between the model representations and the external world.  \nII. Can LLM’s discover novel causal relationships and use them to design \ninterventions?  \n Designing novel tools depends on being able to infer a novel causal relationship, such as \nthe insulating relationship between the plant pot and the coffee cup. A substantial literature \nshows that even very young children excel at discovering such relationships. Information about \ncausal structure can be conveyed through imitation and cultural transmission. In fact, from a very \nyoung age, even infants will reproduce an action they have observed in order to bring about an \neffect (Waismeyer, Meltzoff & Gopnik, 2015). However, very young children can also infer \nnovel causal structure by observing complex statistical relations among events, and ,most \nsignificantly, by acting on the world themselves to bring about effects like a scientist performing \nexperiments (Cook, Goodman & Schulz, 2011; Gopnik et al., 2004, 2017; Gopnik & \n\n 13 \nTenenbaum, 2007; Schulz, Bonawitz & Griffiths, 2007). Causal discovery is a particularly good \nexample of a cognitive process that is directed at solving an inverse problem and discovering \nnew truths through perception and action.  \nIn another line of research (Kosoy et al., 2022, in prep) we have explored whether LLM’s \nand other AI models can discover and use novel causal structure. In these studies we use a virtual \n“blicket detector” – a machine that lights up and plays music when you put some objects on it \nbut not others. The blicket detector can work on different abstract principals or “over-\nhypotheses”, individual blocks may make it go or you may need a combination of blocks to do \nso. An over-hypothesis refers to an abstract principal that reduces a hypothesis space at a less \nabstract level (Kemp, Performs & Tenenbaum, 2007), and a causal over-hypothesis refers to \ntransferable abstract hypotheses about sets of causal relationships (Kosoy et al., 2022).  If  you \nknow that it takes two blocks to make the machine go, you will generate different  specific \nhypotheses about which blocks are blickets.  \nThe blicket detector tasks intentionally involve a new artifact, described with new words, \nso that the participants cannot easily use past culturally transmitted information, such as the fact \nthat flicking a light switch makes a bulb go on.  In these experiments, we simply ask children to \nfigure out how the machines work and allow them to freely explore and act to solve the task and \ndetermine which blocks are blickets. Even four-year-old children spontaneously acted on the \nsystems and discovered their structure – they figured out which ones were blickets and used \nthem to make the machine go. \nWe then gave a variety of LLM’s, including OpenAI’s GPT, Google’s PaLM and most \nrecently LaMDA the same data that the children produced, described in language (e.g., “I put  \nthe blue one and the red one on the machine and the machine lit up” and prompted the systems to \n 14 \nanswer questions about the causal structure of the machine (e.g., Is the red one a blicket?). \nMachine learning systems struggled to model, understand and extract causal over-hypotheses \nfrom their data. Young children learned novel causal over-hypotheses from only a handful of \nobservations, including the outcome of their own experimental interventions, and applied the \nlearned structure to novel situations. In contrast, large language models and vision-and-language \nmodels, as well as both deep reinforcement learning algorithms and behavior cloning struggled \nto figure out the relevant causal structures. This is consistent with other recent studies: LLM’s \nproduce the correct text in cases like causal vignettes, where the patterns are available in the \ntraining data, but often fail when they are asked to make inferences that involve novel events or \nrelations (e.g., Binz and Schulz, 2022; Ullman, 2023), even when these involve superficially \nslight changes to the training data.  \nChallenges of Studying Large Language and Language-and-Vision Models: The Questions \nLeft Unanswered \nIt is difficult to escape the language of individual agency, for example, to ask whether an \nLLM can or cannot innovate or solve a causal problem, or even can or cannot be sentient or \nintelligent. A great deal of the discussion about AI has this character. But we emphasize again \nthat the point of this work is neither to decide whether or not LLM’s are intelligent agents, nor to \npresent some crucial comparative gotcha test that would determine the answer to such questions. \nInstead, the research projects we have briefly described here are a first step in determining which \nrepresentations and competences, as well as which kinds of knowledge or skill, can be derived \nfrom which learning techniques and data. Which kinds of knowledge can be extracted from large \nbodies of text and images, and which depend on actively seeking the truth about an external \nworld?  \n 15 \n We think that there is a great deal of scope for research that uses developmental \npsychology techniques to investigate AI systems and vice-versa. Developmentalists have long \nrealized that superficially similar behaviors can have very different psychological origins and can \nbe the result of very different learning techniques and data. As a result, we have put considerable \nmethodological energy into trying to solve this problem. A particular conversation with a child, \nhowever compelling, is just the start of a proper research program including novel, carefully \ncontrolled tests like our tests of tool innovation and causal inference.  The conversation may \nreflect knowledge that has come through imitation, statistical pattern recognition, reinforcement \nfrom adults or conceptual understanding and the job of the developmental psychologist is to \ndistinguish these possibilities. This should also be true of our assessments of AI systems. \n At the same time AI systems have their own properties that need to be considered when \nwe compare their output to that of humans. These can sometimes be problematic, for example, \nonce a particular cognitive test is explicitly described in internet text it then becomes part of the \nsystem’s training sample – we found that in some cases the systems referred to our earlier \npublished blicket detector papers as the source for their answers! The more recent versions of \nGPT, GPT-4 and GPT-3.5, have also been fine-tuned through reinforcement learning from \nhuman feedback. This also raises problems, human reinforcement may be opaque and variable, \nand may simply edit out the most obvious mistakes and errors. But on the other hand, these \nsystems also have the advantage that we know more about their data and learning techniques \nthan we do about those of human children. For example, we know that the data for GPT systems \nis internet text and that the training function involves predicting new text from earlier text. We \nknow that large language models and language-and-vision models are built on deep neural \nnetworks and trained on immense amounts of unlabeled text or text-image pairings. \n 16 \nThese kinds of techniques may indeed contribute to some kinds of human learning as \nwell. Children do learn through cultural transmission and statistical generalizations from data. \nBut human children also learn in very different ways. Though we don't know the details of the \nchild’s learning algorithms or data, we do know that, unlike large language and language-and-\nvision models, they are curious, active, self-supervised and intrinsically motivated. They are \ncapable of extracting novel and abstract structures from the environment beyond statistical \npatterns, spontaneously making over-hypotheses and generalizations, and applying these insights \nto new situations. A child’s learning is also embodied in the physical world; changes in the brain, \nbody, environment, and experiences promote behavioral flexibility and multimodal exploration, \nwhich leads to cascades of development in various psychological domains (Adolph and Hoch, \n2019; Iverson, 2021).  \nSince performance in large deep learning models has been steadily improving with \nincreasing model size on various tasks, some have advocated that simply scaling up language \nmodels could allow task-agnostic, few-shot performance (e.g., Brown et al., 2020). But a child \ndoes not interact with the world better by increasing their brain capacity. Is building the tallest \ntower the ultimate way to reach the moon? Putting scale aside, what are the mechanisms that \nallow us humans to be effective and creative learners? What in a child’s “training data” and \nlearning capacities is critically effective and different from that of LLM’s? Can we design new \nAI systems that use active, self-motivated, exploration of the real external world as children do? \nAnd what we might expect the capacities of such systems to be? Comparing these systems in a \ndetailed and rigorous way can provide important new insights about both natural and artificial \nintelligence. \nConclusion \n 17 \nLarge language models such as ChatGPT are valuable cultural technologies. They can \nimitate millions of human writers, summarizing long texts, translating between languages, \nanswering questions and coding programs. Imitative learning is critical for promoting and \npreserving knowledge, artifacts, and practices faithfully within social groups. Moreover, changes \nin cultural technologies can have transformative effects on human societies and cultures – for \ngood or ill. There is a good argument that the initial development of printing technology \ncontributed to the Protestant reformation. Later improvements in printing technology in the 18th \ncentury were responsible for both the best parts of the American Revolution and the worst parts \nof the French one (Darnton, 1982). Large language and language-and-vision models may well \nhave equally transformative effects in the 21st century. \nHowever, cultural evolution depends on a fine balance between imitation and innovation.  \nThere would be no progress without innovation, the ability to expand, create, change, abandon, \nevaluate and improve on existing knowledge and skills. Whether this means recasting existing \nknowledge in new ways or creating something entirely original, innovation challenges the status \nquo and questions the conventional wisdom that is the training corpus for artificially intelligent \nsystems. Large language models can help us acquire information that is already known more \nefficiently, even though they are not innovators themselves. Moreover, accessing existing \nknowledge much more effectively can stimulate more innovation among humans and perhaps the \ndevelopment of more advanced AI. But ultimately, machines may need more than large scale  \nlanguage and images to match the achievements of every human child.  \nReferences \nAdolph, K. E., & Hoch, J. E. (2019). Motor development: Embodied, embedded, enculturated,  \nand enabling. Annual review of psychology, 70, 141-164. \n 18 \nBarron, F. (1955). The disposition toward originality. The Journal of Abnormal and Social  \nPsychology, 51(3), 478. https://doi.org/10.1146/annurev-psych-010418-102836 \nBarsalou, L. W. (2008). Grounded cognition. Annu. Rev. Psychol., 59, 617-645.  \nhttps://doi.org/10.1146/annurev.psych.59.103006.093639 \nBeck, S. R., Apperly, I. A., Chappell, J., Guthrie, C., & Cutting, N. (2011). Making tools isn’t  \nchild’s play. Cognition, 119(2), 301-306. https://doi.org/10.1016/j.cognition.2011.01.003 \nBekkering, H., Wohlschlager, A., & Gattis, M. (2000). Imitation of gestures in children is goal- \ndirected. The Quarterly Journal of Experimental Psychology: Section A, 53(1), 153-164. \nhttps://doi.org/10.1080/713755872 \nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand GPT-3. Proceedings of  \nthe National Academy of Sciences, 120(6), e2218523120. \nhttps://doi.org/10.1073/pnas.2218523120 \nBolin, G. (2012). Introduction: Cultural technologies in cultures of technology. In Cultural  \nTechnologies (pp. 1-15). Routledge. \nBoyd, R., Richerson, P. J., & Henrich, J. (2011). The cultural niche: Why social learning is  \nessential for human adaptation. Proceedings of the National Academy of Sciences, \n108(supplement_2), 10918-10925. https://doi.org/10.1073/pnas.1100290108 \nBoyd, R., & Richerson, P. J. (1988). Culture and the evolutionary process. University of  \nChicago Press. \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D.  \n(2020). Language models are few-shot learners. Advances in neural information  \nprocessing systems, 33, 1877-1901. https://doi.org/10.48550/arXiv.2005.14165 \nBuchsbaum, D., Gopnik, A., Griffiths, T. L., & Shafto, P. (2011). Children’s imitation of causal  \n 19 \naction sequences is influenced by statistical and pedagogical evidence. Cognition, 120(3), \n331-340. https://doi.org/10.1016/j.cognition.2010.12.001 \nCook, C., Goodman, N. D., & Schulz, L. E. (2011). Where science starts: Spontaneous  \nexperiments in preschoolers’ exploratory play. Cognition, 120(3), 341-349.  \nhttps://doi.org/10.1016/j.cognition.2011.03.003 \nDarnton, R. (1982). What is the History of Books?. Daedalus, 65-83. \nDerex, M. (2022). Human cumulative culture and the exploitation of natural phenomena.  \nPhilosophical Transactions of the Royal Society B, 377(1843), 20200311. \nhttps://doi.org/10.1098/rstb.2020.0311 \nEmery, N. J., & Clayton, N. S. (2004). The mentality of crows: convergent evolution of  \nintelligence in corvids and apes. Science, 306(5703), 1903-1907. \nhttps://doi.org/10.1126/science.1098410 \nGergely, G., Bekkering, H., & Király, I. (2002). Rational imitation in preverbal infants. Nature,  \n415(6873), 755-755. https://doi.org/10.1038/415755a \nGibson, James J. \"The Ecological Approach to Visual Perception: Classic Edition.\" (1979). \nGopnik, A., O’Grady, S., Lucas, C. G., Griffiths, T. L., Wente, A., Bridgers, S., ... & Dahl, R. E.  \n(2017). Changes in cognitive flexibility and hypothesis search across human life history \nfrom childhood to adolescence to adulthood. Proceedings of the National Academy of \nSciences, 114(30), 7892-7899. https://doi.org/10.1073/pnas.1700811114 \nGopnik, A., & Wellman, H. M. (2012). Reconstructing constructivism: causal models, Bayesian  \nlearning mechanisms, and the theory theory. Psychological bulletin, 138(6), 1085. \nhttps://doi.org/10.1037/a0028044 \nGopnik, A., & Tenenbaum, J. B. (2007). Bayesian networks, Bayesian learning and cognitive  \n 20 \ndevelopment. Developmental science, 10(3), 281-287. https://doi.org/10.1111/j.1467- \n7687.2007.00584.x \nGopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E., Kushnir, T., & Danks, D. (2004). A theory  \nof causal learning in children: causal maps and Bayes nets. Psychological review, 111(1),  \n3. https://doi.org/10.1037/0033-295X.111.1.3 \nGrand, G., Blank, I. A., Pereira, F., & Fedorenko, E. (2022). Semantic projection recovers rich  \nhuman knowledge of multiple object features from word embeddings. Nature human \nbehaviour, 6(7), 975-987. https://doi.org/10.1038/s41562-022-01316-8 \nHarris, P. L., Koenig, M. A., Corriveau, K. H., & Jaswal, V. K. (2018). Cognitive foundations of  \nlearning from testimony. Annual Review of Psychology, 69, 251-273. \nhttps://doi.org/10.1146/annurev-psych-122216-011710 \nHenrich, J. (2018). Human cooperation: The hunter-gatherer puzzle. Current Biology, 28(19),  \nR1143-R1145. https://doi.org/10.1016/j.cub.2018.08.005 \nHenrich, J. (2016). The secret of our success: How culture is driving human evolution,  \ndomesticating our species, and making us smarter. Princeton University Press. \nHeyes, C. (2001). Causes and consequences of imitation. Trends in cognitive sciences, 5(6), 253- \n261. https://doi.org/10.1016/S1364-6613(00)01661-2 \nIverson, J. M. (2021). Developmental variability and developmental cascades: Lessons from  \nmotor and language development in infancy. Current Directions in Psychological \nScience, 30(3), 228-235. https://doi.org/10.1177/0963721421993822 \nJeretic, P., Warstadt, A., Bhooshan, S., & Williams, A. (2020). Are natural language inference  \nmodels IMPPRESsive? Learning IMPlicature and PRESupposition. arXiv preprint \narXiv:2004.03066. https://doi.org/10.48550/arXiv.2004.03066 \n 21 \nKemp, C., Perfors, A., & Tenenbaum, J. B. (2007). Learning overhypotheses with hierarchical  \nBayesian models. Developmental science, 10(3), 307-321. https://doi.org/10.1111/j.1467-\n7687.2007.00585.x \nKirkham, N. Z., Slemmer, J. A., & Johnson, S. P. (2002). Visual statistical learning in infancy:  \nEvidence for a domain general learning mechanism. Cognition, 83(2), B35-B42.  \nhttps://doi.org/10.1016/S0010-0277(02)00004-5 \nKosinski, M. (2023). Theory of mind may have spontaneously emerged in large language  \nmodels. arXiv preprint arXiv:2302.02083. https://doi.org/10.48550/arXiv.2302.02083 \nKosoy, E., Chan, D. M., Liu, A., Collins, J., Kaufmann, B., Huang, S. H., ... & Gopnik, A.  \n(2022). Towards understanding how machines can learn causal overhypotheses. arXiv  \npreprint arXiv:2206.08353. https://doi.org/10.48550/arXiv.2206.08353 \nKosoy, E., Cobb, D. K., Reagan, E. R.,  Lai, L., Jenkins, M., & Gopnik, A. How old is LaMDA?  \nSocial Intuition and Intelligence: Teaching LaMDA to Play. Manuscript in preparation.  \nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic  \nanalysis theory of acquisition, induction, and representation of knowledge. Psychological \nreview, 104(2), 211. https://doi.org/10.1037/0033-295X.104.2.211 \nLegare, C. H., & Nielsen, M. (2015). Imitation and innovation: The dual engines of cultural  \nlearning. Trends in cognitive sciences, 19(11), 688-699.  \nhttps://doi.org/10.1016/j.tics.2015.08.005 \nMeltzoff, A. N. (1995). Understanding the intentions of others: re-enactment of intended acts by  \n18-month-old children. Developmental psychology, 31(5), 838. \nhttps://doi.org/10.1037/0012-1649.31.5.838 \nMeltzoff, A. N., & Moore, M. K. (1977). Imitation of facial and manual gestures by human  \n 22 \nneonates. Science, 198(4312), 75-78. https://doi.org/10.1126/science.198.4312.75 \nOver, H., & Carpenter, M. (2013). The social side of imitation. Child development perspectives,  \n7(1), 6-11. https://doi.org/10.1111/cdep.12006 \nPiantadosi, S. T. (2023). Modern language models refute Chomsky’s approach to language.  \nLingbuzz Preprint, lingbuzz/007180. \nRawlings, B., & Legare, C. H. (2021). Toddlers, tools, and tech: The cognitive ontogenesis of  \ninnovation. Trends in cognitive sciences, 25(1), 81-92. \nhttps://doi.org/10.1016/j.tics.2020.10.006 \nReader, S. M., & Laland, K. N. (2002). Social intelligence, innovation, and enhanced brain size  \nin primates. Proceedings of the National Academy of Sciences, 99(7), 4436-4441.  \nhttps://doi.org/10.1073/pnas.062041299 \nRunco, M. A., & Jaeger, G. J. (2012). The standard definition of creativity. Creativity research  \njournal, 24(1), 92-96. https://doi.org/10.1080/10400419.2012.650092 \nSaffran, J. R., Aslin, R. N., & Newport, E. L. (1996). Statistical learning by 8-month-old  \ninfants. Science, 274(5294), 1926-1928. https://doi.org/10.1126/science.274.5294.1926 \nSchulz, L. E., Hooppell, C., & Jenkins, A. C. (2008). Judicious imitation: Children differentially  \nimitate deterministically and probabilistically effective actions. Child Development, \n79(2), 395-410.  \nSchulz, L. E., Bonawitz, E. B., & Griffiths, T. L. (2007). Can being scared cause tummy aches?  \nNaive theories, ambiguous evidence, and preschoolers' causal inferences. Developmental \npsychology, 43(5), 1124. https://doi.org/10.1037/0012-1649.43.5.1124 \nStowe, K., Utama, P., & Gurevych, I. (2022, May). IMPLI: Investigating NLI models’  \nperformance on figurative language. In Proceedings of the 60th Annual Meeting of the  \n 23 \nAssociation for Computational Linguistics (Volume 1: Long Papers) (pp. 5375-5388). \nhttps://doi.org/10.18653/v1/2022.acl-long.369 \nSummers-Stay, D., Voss, C. R., & Lukin, S. M. (2023). Brainstorm, then Select: a Generative  \nLanguage Model Improves Its Creativity Score. In The AAAI-23 Workshop on Creative  \nAI Across Modalities. \nTomasello, M., Kruger, A., & Ratner, H. (1993). Cultural learning. Behavioral and Brain  \nSciences, 16, 495-552. https://doi.org/10.1017/S0140525X0003123X \nTomasello, M. (1990). Cultural transmission in the tool use and communicatory signaling of  \nchimpanzees? In S. T. Parker & K. R. Gibson (Eds.), \"Language\" and intelligence in  \nmonkeys and apes: Comparative developmental perspectives (pp. 274–311). Cambridge  \nUniversity Press. https://doi.org/10.1017/CBO9780511665486.012 \nParker & K. Gibson (Eds.). Language and intelligence in monkeys and apes: Comparative  \ndevelopmental perspectives, 274-311. Cambridge, UK: Cambridge University Press. \nhttps://doi.org/10.1017/CBO9780511665486.012 \nSchulz, L. E., Bonawitz, E. B., & Griffiths, T. L. (2007). Can being scared cause tummy aches?  \nNaive theories, ambiguous evidence, and preschoolers' causal inferences. Developmental \npsychology, 43(5), 1124. \nUllman, T. (2023). Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks.  \narXiv preprint arXiv:2302.08399. https://doi.org/10.48550/arXiv.2302.08399 \nVon Bayern, A. M., Heathcote, R. J., Rutz, C., & Kacelnik, A. (2009). The role of experience in  \nproblem solving and innovative tool use in crows. Current Biology, 19(22), 1965-1968.  \nhttps://doi.org/10.1016/j.cub.2009.10.037 \nWaismeyer, A., Meltzoff, A. N., & Gopnik, A. (2015). Causal learning from probabilistic events  \n 24 \nin 24‐month‐olds: an action measure. Developmental science, 18(1), 175-182.  \nhttps://doi.org/10.1111/desc.12208 \nWhiten, A., Horner, V., & de Waal, F. (2005). Conformity to cultural norms of tool use in  \nchimpanzees. Nature, 437(7059), 737-740. https://doi.org/10.1038/nature04047 \nYiu, E., & Gopnik, A. (in press). Discovering New Functions in Everyday Tools by Children,  \nAdults and LLM’s. In Proceedings of the Annual Meeting of the Cognitive Science  \nSociety. \nZhang, M., & Li, J. (2021). A commentary of GPT-3 in MIT Technology Review 2021.  \nFundamental Research, 1(6), 831-833. https://doi.org/10.1016/j.fmre.2021.11.011 \n ",
  "topic": "Imitation",
  "concepts": [
    {
      "name": "Imitation",
      "score": 0.8466822504997253
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.6252953410148621
    },
    {
      "name": "Computer science",
      "score": 0.6247150897979736
    },
    {
      "name": "Contrast (vision)",
      "score": 0.508934497833252
    },
    {
      "name": "Cognitive science",
      "score": 0.4943532943725586
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4622369408607483
    },
    {
      "name": "Language model",
      "score": 0.4385618567466736
    },
    {
      "name": "Scale (ratio)",
      "score": 0.41035956144332886
    },
    {
      "name": "Cognitive psychology",
      "score": 0.40185537934303284
    },
    {
      "name": "Natural language processing",
      "score": 0.32721972465515137
    },
    {
      "name": "Psychology",
      "score": 0.310396671295166
    },
    {
      "name": "Social psychology",
      "score": 0.09439742565155029
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}