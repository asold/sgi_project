{
  "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
  "url": "https://openalex.org/W4280618035",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281227731",
      "name": "Lekshmi Chandrika Reghunath",
      "affiliations": [
        "A P J Abdul Kalam Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2228201541",
      "name": "Rajeev Rajan",
      "affiliations": [
        "A P J Abdul Kalam Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A4281227731",
      "name": "Lekshmi Chandrika Reghunath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228201541",
      "name": "Rajeev Rajan",
      "affiliations": [
        "A P J Abdul Kalam Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2410860376",
    "https://openalex.org/W2008066450",
    "https://openalex.org/W6602989878",
    "https://openalex.org/W1590102137",
    "https://openalex.org/W2604509013",
    "https://openalex.org/W2903539539",
    "https://openalex.org/W3004440566",
    "https://openalex.org/W2902583091",
    "https://openalex.org/W3015591047",
    "https://openalex.org/W2065147844",
    "https://openalex.org/W2108148744",
    "https://openalex.org/W2101304342",
    "https://openalex.org/W2096623359",
    "https://openalex.org/W6610768942",
    "https://openalex.org/W2990912900",
    "https://openalex.org/W2619329613",
    "https://openalex.org/W2890267272",
    "https://openalex.org/W3046491228",
    "https://openalex.org/W3037702327",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2889387262",
    "https://openalex.org/W2593528643",
    "https://openalex.org/W2047583614",
    "https://openalex.org/W1505110664",
    "https://openalex.org/W2478051194",
    "https://openalex.org/W2993245626",
    "https://openalex.org/W2988491440",
    "https://openalex.org/W3130327629",
    "https://openalex.org/W6610385870",
    "https://openalex.org/W2516037800",
    "https://openalex.org/W3032019894",
    "https://openalex.org/W2997592942",
    "https://openalex.org/W2160537075",
    "https://openalex.org/W2772743710",
    "https://openalex.org/W2591436322",
    "https://openalex.org/W6609736445",
    "https://openalex.org/W2094833092",
    "https://openalex.org/W875533737",
    "https://openalex.org/W1766888123",
    "https://openalex.org/W3098670224",
    "https://openalex.org/W3102659268"
  ],
  "abstract": "Abstract Multiple predominant instrument recognition in polyphonic music is addressed using decision level fusion of three transformer-based architectures on an ensemble of visual representations. The ensemble consists of Mel-spectrogram, modgdgram, and tempogram. Predominant instrument recognition refers to the problem where the prominent instrument is identified from a mixture of instruments being played together. We experimented with two transformer architectures like Vision transformer (Vi-T) and Shifted window transformer (Swin-T) for the proposed task. The performance of the proposed system is compared with that of the state-of-the-art Han’s model, convolutional neural networks (CNN), and deep neural networks (DNN). Transformer networks learn the distinctive local characteristics from the visual representations and classify the instrument to the group where it belongs. The proposed system is systematically evaluated using the IRMAS dataset with eleven classes. A wave generative adversarial network (WaveGAN) architecture is also employed to generate audio files for data augmentation. We train our networks from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from the variable-length test audio file without any sliding window analysis and aggregation strategy as in existing algorithms. The ensemble voting scheme using Swin-T reports a micro and macro F1 score of 0.66 and 0.62, respectively. These metrics are 3.12% and 12.72% relatively higher than those obtained by the state-of-the-art Han’s model. The architectural choice of transformers with ensemble voting on Mel-spectro-/modgd-/tempogram has merit in recognizing the predominant instruments in polyphonic music.",
  "full_text": "ReghunathandRajan EURASIPJournalonAudio,Speech,andMusic\nProcessing         (2022) 2022:11 \nhttps://doi.org/10.1186/s13636-022-00245-8\nEMPIRICAL RESEARCH OpenAccess\nTransformer-basedensemblemethod\nformultiplepredominantinstruments\nrecognitioninpolyphonicmusic\nLekshmiChandrikaReghunath * and RajeevRajan\nAbstract\nMultiplepredominantinstrumentrecognitioninpolyphonicmusicisaddressedusingdecisionlevelfusionofthree\ntransformer-basedarchitecturesonanensembleofvisualrepresentations.Theensembleconsistsof\nMel-spectrogram,modgdgram,andtempogram.Predominantinstrumentrecognitionreferstotheproblemwhere\ntheprominentinstrumentisidentifiedfromamixtureofinstrumentsbeingplayedtogether.Weexperimentedwith\ntwotransformerarchitectureslikeVisiontransformer(Vi-T)andShiftedwindowtransformer(Swin-T)fortheproposed\ntask.Theperformanceoftheproposedsystemiscomparedwiththatofthestate-of-the-artHan’smodel,\nconvolutionalneuralnetworks(CNN),anddeepneuralnetworks(DNN).Transformernetworkslearnthedistinctive\nlocalcharacteristicsfromthevisualrepresentationsandclassifytheinstrumenttothegroupwhereitbelongs.The\nproposedsystemissystematicallyevaluatedusingtheIRMASdatasetwithelevenclasses.Awavegenerative\nadversarialnetwork(WaveGAN)architectureisalsoemployedtogenerateaudiofilesfordataaugmentation.Wetrain\nournetworksfromfixed-lengthmusicexcerptswithasingle-labeledpredominantinstrumentandestimatean\narbitrarynumberofpredominantinstrumentsfromthevariable-lengthtestaudiofilewithoutanyslidingwindow\nanalysisandaggregationstrategyasinexistingalgorithms.TheensemblevotingschemeusingSwin-Treportsamicro\nandmacroF1scoreof0.66and0.62,respectively.Thesemetricsare3.12%and12.72%relativelyhigherthanthose\nobtainedbythestate-of-the-artHan’smodel.Thearchitecturalchoiceoftransformerswithensemblevotingon\nMel-spectro-/modgd-/tempogramhasmeritinrecognizingthepredominantinstrumentsinpolyphonicmusic.\nKeywords: Predominant,Modifiedgroupdelay,Mel-spectrogram,Modgdgram,Tempogram,Shiftedwindow\n1 Introduction\nMusic information retrieval (MIR) is a growing field\nof research with lots of real-world applications and is\nappliedwellincategorizing,manipulating,andsynthesiz-\ningmusic.AnimportantMIRtaskofpredominantinstru-\nment recognition is addressed in this paper. Predominant\ninstrument recognition refers to the problem where the\nprominent instrument is identified from a mixture of\ninstrumentsbeingplayedtogether[ 1].\n*Correspondence:clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,India\nThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nfering partials in the orchestral background. The auditory\nsceneproducedbyamusicalcompositioncanberegarded\nas a multi-source environment, where different sound\nsources are played at various pitches and loudness, and\neven the spatial position of a given sound source may\nvary with respect to time [2]. Automatic identification of\nlead instruments is important, since the performance of\nthe source separation can be improved significantly by\nknowing the type of the instrument [1]. If the instrument\ninformation is included in the tags, it allows people to\nsearch for music with the specific instrument they want.\nAudio enhancement based on instrument-specific equal-\n©TheAuthor(s).2022 OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,which\npermitsuse,sharing,adaptation,distributionandreproductioninanymediumorformat,aslongasyougiveappropriatecredit\ntotheoriginalauthor(s)andthesource,providealinktotheCreativeCommonslicence,andindicateifchangesweremade. The\nimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unlessindicated\notherwiseinacreditlinetothematerial. Ifmaterialisnotincludedinthearticle’sCreativeCommonslicenceandyourintended\nuseisnotpermittedbystatutoryregulationorexceedsthepermitteduse,youwillneedtoobtainpermissiondirectlyfromthe\ncopyrightholder. Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/.\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page2of14\nization is also in high demand in music processing. It also\nhelpstoenhancefundamentalMIRtaskslikeauto-tagging\n[3],andautomaticmusictranscription[ 4].\nAn extensive review of approaches for isolated musi-\ncal instrument classification can be found in [5]. Non-\nnegative matrix factorization (NMF) model [6], end-to-\nend model [7], fusion model with spectral, temporal,\nand modulation features [8] can be referred to as initial\nattempts for the proposed task in a polyphonic environ-\nment. More recent works deal with instrument recogni-\ntion in polyphonic music, which is a more demanding\nand challenging problem. A method for automatic recog-\nnition of predominant instruments with support vector\nmachine (SVM) classifiers trained with features extracted\nfrom real musical audio signals is proposed in [2]. Bosch\net al. improved this algorithm with source separation\nin a preprocessing step [9]. Han et al. [1]d e v e l o p e da\ndeep CNN for instrument recognition based on Mel-\nspectrogram inputs and aggregation of multiple outputs\nfrom sliding windows over the audio data. Pons et al. [10]\nanalyzed the architecture of Han et al. in order to for-\nmulate an efficient design strategy to capture the relevant\ninformation about timbre. Both approaches were trained\nand validated by the IRMAS dataset of polyphonic music\nexcerpts. Detecting the activity of music instruments\nusing a deep neural network (DNN) through a temporal\nmax-poolingaggregationisaddressedin[ 11].DongyanYu\net al. [12] employed a network with an auxiliary classifi-\ncation scheme to learn the instrument categories through\nmultitask learning. Gomez et al. [13]i n v e s t i g a t edt h er o l e\nof two source separation algorithms as pre-processing\nsteps to improve the performance in the context of pre-\ndominant instrument detection tasks. It was found that\nboth source separation and transfer learning could signif-\nicantly improve the recognition performance, especially\nfor a small dataset composed of highly similar musical\ninstruments.In[ 14],theHilbert-Huangtransform(HHT)\nisemployedtomapone-dimensionalaudiodataintotwo-\ndimensional matrix format, followed by CNN to learn\nthe affluent and effective features for the task. The pro-\nposedworkin[ 15]employedanattentionmechanismand\nmultiple-instance learning (MIL) framework to address\nthe challenge of weakly labeled instrument recognition in\nthe OpenMIC dataset.\nThe modified group delay feature (MODGDF) is pro-\nposed for pitched musical instrument recognition in\nan isolated environment in [16]. While the commonly\napplied Mel-frequency cepstral coefficients (MFCC) fea-\nture is capable of modeling the resonances introduced by\nthe filter of the instrument body, it neglects the spec-\ntral characteristics of the vibrating source, which also\nplay their role in human perception of musical sounds\n[17]. Incorporating phase information attempts to pre-\nservethisneglectedcomponent.Ithasalreadybeenestab-\nlished in the literature that the modified group delay\nfunction emphasizes peaks in spectra well [18]. It has\nalso been shown in [19] that sinusoids in noise can be\nestimated well-using group delay function. Furthermore,\nit was shown that even for shorter windows, the phase\nspectrum could contribute as much as the magnitude\nspectrumtospeechintelligibility[ 20].Inourwork,weare\nintroducingphase-basedmodgdgramasacomplementary\nfeature to magnitude-based spectrogram in recognizing\npredominant instruments from a polyphonic environ-\nment. The source information is completely suppressed\nin the modgdgram compared to the spectrogram, and the\nsystem-specific information is retained, which is a vital\nclueininstrumentidentification.\nTempo-based features are employed in various music\ninformation retrieval tasks. Grosche et al. point out the\npotential of integrating the concept of tempo represen-\ntation into music structural segmentation [21]. Tempo-\nbased features have also been used for cross-version nov-\nelty detection in [22]. In [23], an ensemble of VGG-like\nCNN classifiers were trained on non-augmented, pitch-\nsynchronized, tempo-synchronized, and genre-similar\nexcerptsofIRMASfortheproposedtask.Theyemployed\ntempo-syncing as one of the data augmentation tech-\nniques and achieved better results than the baseline\nmodel.\nThe fusion of multiple modalities can offer significant\nperformance gains over using a modality alone and is\nwidely used in recent music processing applications [24–\n26]. The performance of the various features depends on\ntheinstrumentcharacteristicsandotherunknownfactors,\nand no one feature consistently outperforms all others.\nConsequently, researchers have investigated the possibil-\nity of fusing multiple features to take advantage of their\nstrengths. In our work, we utilize transformer architec-\ntures to learn instrument-specific characteristics using\nMel-spectro-/modgd-/tempogram to estimate predomi-\nnant instruments from polyphonic music. Transformer-\nbased systems have outperformed previous approaches\nfor various natural language processing (NLP) and com-\nputervisiontasks[ 27],[28].\n2 Contributions\nThe major contributions of the proposed experiment can\nbesummedupas:\n1 Introducingmodgdgramandtempogramas\ncomplementaryfeaturestotheconventional\nMel-spectrogramrepresentationforpredominant\ninstrumentrecognition.The proposedensemble\nvotingtechniquemakesuse ofthepotentialofthree\nvisual representationsinmakingafinaldecisionon\nrecognizingpredominantinstrumentsina\npolyphonicenvironment.\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page3of14\n2 Wepresentahighcapacitytransformermodelfor\nMel-spectrograminputs.Our modelis derivedfrom\n[29]withsomesignificantchangesasdescribedin\nSection4,anditoutperformstheexistingmodels,\nincluding[1].The efficacyoftransformermodelsand\nattentionmechanismsaredemonstratedby\ncomparisonwithCNNandDNNarchitectures.\n3 Weexplorethetime-domainstrategyofsynthetic\nmusicaudio generationfordataaugmentationusing\nWaveGAN.The proposedtaskisaddressedwithand\nwithoutdataaugmentation.\n4 Inthedevelopmentphase,theperformanceis\nevaluatedusingvariousschemes like\nMel-spectrogram,modgdgram,andtempogram\nfollowedbyensemblevoting.\nThe outline of the rest of the paper is as follows.\nSection 3explainstheproposedsystem.Themodelarchi-\ntectures are described in Section 4. The performance\nevaluation is explained in Section5 followed by the anal-\nysis of results in Section6. The paper is concluded in\nSection 7.\n3 Systemdescription\nThe proposed method of Vision transformer (Vi-T) and\nShiftedwindowtransformer(Swin-T)areshowninFigs. 2\nand 3 respectively. In the proposed model, transform-\ners are used to learn the distinctive characteristics of\nMel-spectro/modgd/tempo-gram to identify the leading\ninstrumentinapolyphoniccontext.Asapartofdataaug-\nmentation, additional training files are generated using\nWaveGAN (Fig.1). The probability values reported at the\nnodes of the trained model are mapped as the scores\nfor a test file input. The final decision on the test file is\nbased on soft voting. Soft voting involves summing the\npredicted probabilities for class labels (from three net-\nworks) followed by thresholding. The candidates above\nthe particular threshold were considered as predominant\ninstruments. The performance of the proposed system is\ncompared with that of the state-of-the-art Han’s model\nandaDNNmodel.Adetaileddescriptionofeachphaseis\ngivenint hef ollowingsubsections.\n3.1 Featureextraction\n3.1.1 Mel-spectrogram\nMel-spectrogramiswidelyusedinspeechandmusicpro-\ncessing applications [30],[31]. Mel-spectrogram approxi-\nmates how the human auditory system works and can be\nseen as the spectrogram smoothed, with high precision\nin the low frequencies and low precision in the high fre-\nquencies [32]. All audio files in the IRMAS dataset are\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz. The time-domain waveform is converted to a\ntime-frequency representation using a short-time Fourier\ntransform (STFT) with a frame size of 50 ms and hop\nsize of 10 ms. Then the linear frequency scale obtained\nspectrogram is converted to a Mel-scale using 128 for the\nnumberofMel-frequencybins.\nFig.1 Visualrepresentationofanaudioexcerptwithacousticguitarasleading,UpperpanerepresentstheMel-spectrogram,modgdgram,and\ntempogramoftheoriginalaudiofileandlowerpanerepresentstheWaveGANgeneratedfiles\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page4of14\n3.1.2 Modifiedgroupdelayfunctionsandmodgdgram\nGroup delay features are being employed in numerous\nspeech and music processing applications [18],[33]. The\ngroup delay function is defined as the negative derivative\noftheunwrappedFouriertransformphasewithrespectto\nfrequency. Group delay functions,τ(ejω) are mathemati-\ncallydefinedas\nτ(ejω) =− d{arg(X(ejω))}\ndω (1)\nwhere X(ejω) is the Fourier transform of the signalx[n]\nand arg(X(ejω)) is the phase function. It can be computed\ndirectlyfromthesignal, x[n]by[ 34],\nτ(ejω) = XR(ejω)YR(ejω) + YI(ejω)XI(ejω)\n|X(ejω)|2 (2)\nwhere the subscriptsR and I denote the real and imag-\ninary parts andX(ejω) and Y(ejω) are the Fourier trans-\nforms ofx[n] and n.x[n] (signal multiplied with index),\nrespectively.Thespikynatureofthegroupdelayspectrum\nduetozerosthatarelocatedclosetotheunitcirclecanbe\nsuppressed by replacing the term|X(ejω)| in the denom-\ninator of Eq. (2) with its cepstrally smoothed version,\nS(ejω) therebyresultinginmodifiedgroupdelayfunctions\n(MODGD) [18]. The modified group delay functions are\nobtainedby,\nτm(ejω) =\n( τc(ejω)\n|τc(ejω)|\n)\n(|τc(ejω)|)α,( 3 )\nwhere,\nτc(ejω) = XR(ejω)YR(ejω) + YI(ejω)XI(ejω)\n|S(ejω)|2γ .( 4 )\nTwo new parameters, α and γ (0 <α ≤ 1a n d0 <\nγ ≤ 1) are introduced to control the dynamic range of\nMODGD[ 18].Modgdgramisthevisualrepresentationof\nMODGD with time and frequency in the horizontal and\nverticalaxis,respectively.Inathirddimension,theampli-\ntude of the group delay function at a particular time is\nrepresented by the intensity or color of each point in the\nimage.Modgdgramsarecomputedwithaframesizeof50\nms and a hop size of 10 ms. The parametersα andγ have\nbeen empirically chosen as 0.9 and 0.5, respectively. Mel-\nspectrograms and modgdgrams are implemented using\nMATLAB.\nTypically in spectrograms, we can see pitch compo-\nnents and their harmonics as striations along with for-\nmantstructure.Butsystem-specificinformation(formant\ntracks) is enhanced in modgdgram by suppressing the\nsource information. In music, the body of the musical\ninstrument is the counterpart of the vocal tract (system)\nin speech. Davis et al. [35] claim that timbres are prop-\nerties of musical instruments which rely on the physical\ncharacteristics of the instrument. Thus, timbre makes a\nparticular musical instrument or the human voice and\nproduces a different sound from another, even when they\nplayorsingthesamenote.\n3.1.3 Tempogram\nA tempogram is a time-pulse representation of an audio\nsignal laid out such that it indicates the variation of pulse\nstrength over time given a specific time lagl or a beats\nper minute (BPM) value.[36]. It is a time-tempo repre-\nsentation that encodes the local tempo of a music signal\nover time. The calculation of the tempogram is based on\nthe assumption that music exhibits coherent and locally\nperiodicpatterns.Thesepatternsmaybecharacterizedby\npeaks in the autocorrelation function (ACF) of the onset\ndetection function [36] at certain time lags. The train-\ning and testing audio files are read and processed using\ntheLibrosaframework.Theprincipleofautocorrelationis\nusedtoestimatethetempoateverysegmentinthenovelty\nfunction[37].Autocorrelationtempogramsarecomputed\nwith librosa.feature.tempogram using a 2048 point FFT\nwindowandahopsizeof512.\n4 Modelarchitectures\n4.1 DNN\nA DNN framework on musical texture features (MTF)\nis experimented with to examine the performance of\ndeeplearningmethodologyonhandcraftedfeatures.MTF\nincludesMFCC(13dim),spectralcentroid,spectralband-\nwidth, root mean square energy, spectral roll-off, and\nchroma STFT. The features are computed with a frame\nsizeof40msandahopsizeof10msusingLibrosaframe-\nwork1.TheDNNconsistsofsevenlayers,withincreasing\nunits from 8 to 512. Regarding the activation function,\nReLU has been chosen for hidden layers and softmax for\ntheoutputlayer.Theapproachattemptedin[ 38]hasbeen\ncustomized for multi-label classification and has been\nexperimentedwithtoanalyzetheroleofmachinelearning\ntechniques,especiallyusingtheMTF-SVMframework.\n4.2 CNN\nCNN uses a deep architecture with repeated convolu-\ntions followed by max-pooling. A total of five layers are\nused with the number of filters starting from 32 to 512\nforMel-spectrogramprocessing.Thefirsttwolayersused\n5×5 filters, and the remaining layers used 3×3f i l t e r s .\nUsing filters of different shapes seems an efficient way\nof learning spectrogram-based CNNs [10]. To achieve\nthe best performance, the optimal filter size is usually\nchosen empirically by either experimental validation or\nvisualization for each convolutional layer [39]. The ini-\ntial layers help to extract general features and also help in\nnoise reduction. The last convolutional layers used 3×3\nfilters as later layers reveal more specific and complex\n1https://librosa.org/doc/latest/tutorial.html\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page5of14\nFig.2 (a)BlockdiagramoftheproposedmethodofVisiontransformer,(b)Internalarchitectureoftransformerencoder\npatterns and final layers activations help to recognize the\npredominant instruments from accompaniments. Global\nmax-pooling is adopted in the final max-pooling layer,\nwhich is then fed to a fully connected layer. For mod-\ngdgram processing, we used six convolutional layers with\nthenumberoffiltersincreasingfrom8to256,followedby\n2×2 max pooling. We used filters of size 3×3i na l ls i x\nlayers with a fixed stride size of one. For tempogram pro-\ncessing, we used the same model as Mel-spectrogram. A\ndropoutof0.5isintroducedafterthefullyconnectedlayer\nto avoid overfitting in all processing. Leaky ReLU with\nα = 0.33 in hidden layers has been empirically chosen\nfor optimum performance in Mel-spectrogram process-\ning. But in modgdgram and tempogram processing, the\nbestperformanceisobtainedforReLU.Softmaxisusedas\ntheactivationfunctionfortheoutputlayer.\n4.3 Vi-T\nInspiredbythesuccessofTransformer[ 27]invariousnat-\nurallanguageprocessingtasks,VisionTransformers(ViT)\n[40] constitute the first pure transformer-based architec-\nture that can achieve good performance on the image\nrecognition task. Figure2 shows the architecture of our\nproposedmethod.AsshowninFig. 2(a),theinputimage x\nϵ RHXWXL,whereH,W,andLrepresenttheheight,width,\nand the number of channels of the imagex. The input\nimage is partitioned into non-overlapping patches, called\ntokens. In our work, we chooseM =6 ×6, whereM is\nthe size of a patch. Then each patch is linearly projected\nto a dimension of 64, along with position embeddings,\nand feeds the resulting sequence of vectors to a standard\ntransformerencoder.Thenumberofpatches,P= HW/M2.\nThe various hyperparameters selected for our proposed\nmethod are shown in Table1. Position embeddings are\nadded to the patch embeddings to retain positional infor-\nmation. The Transformer encoder is shown in Fig.2(b)\nand consists of alternating layers of multi-headed self-\nattention(MSA)witheightattentionheadsandtwomulti\nlayer perceptron (MLP) layers with 2048 and 1024 nodes\nwith Gaussian error linear unit (GELU) nonlinearity in\nbetween. Layernorm (LN) is applied before every MSA\nand MLP layer, and residual connections are placed after\neachmodule.MSAisdefinedin[ 27]as\nMSA(Q,K,V) = Concat(head1,head2, ... head8) WO\n(5)\nwhere,\nheadi = Attention\n(\nQWQ\ni ,KWK\ni ,VWV\ni\n)\n(6)\nAttention (Q,K,V) = softmax\n(\nQKT\n√\ndk\n)\nV (7)\nTable1 VarioushyperparameterschosenforVi-TandSwin-T\nHyperparameter Vi-T Swin-T\nImagesize 72 × 72 72 × 72\nPatchdimension 6 × 64 × 4\nHyperparameter(C) 64 96\nNumberofheads 8 8\nNumberofwindows NA 4\nNumberofMLPnodes 2048,1048 256,256\nMinibatch-size 256 32\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page6of14\nwhere Q, K, V represents the query key and value vec-\ntor respectivelyandWQ\ni ,WK\ni ,WV\ni and WO aretheweight\nmatrices of query, key, value, and output vectors, respec-\ntively [27], whiledk is the dimension of the query vector.\nThe outputs from all 8 attention heads are concatenated\nto form a single output vector before passing it through\nthe feed-forward network. The model is then trained on\ninstrumentclassificationinasupervisedmanner.\n4.4 Swin-T\nThemaindrawbackofViTisthatitproducesfeaturemaps\nof a single low resolution and has quadratic computation\ncomplexitytoinputimagesizeduetocomputationofself-\nattention globally. Also, the tokens are of fixed scale and\nare thus unsuitable for vision applications. Unlike other\ntransformers Swin-T [29] has a hierarchical architecture\nandhaslinearcomputationalcomplexitythroughthepro-\nposed shifted window-basedself-attention approach. The\ncomputationalcomplexityofVi-Tisgivenby[ 29]\n/Omega1(MSA) = 4hwC2 + 2(hw)2C (8)\n/Omega1(W − MSA) = 4hwC2 + 2M2hwC (9)\nThe computational complexity drops for Swin-T as per\nthe Eq. (9) above. MSA has quadratic computational\ncomplexity to patch numberhw, while W-MSA has lin-\near computational complexity due to the shifted window\napproach [29]. Figure 3 shows the architecture of our\nproposed method. The input image is partitioned into\nnon-overlapping patches, called tokens during patch par-\ntitioning. In our work, we chooseM =4 ×4, where M\nis the size of a patch. The second step is linear embed-\nding, in which the eigenvalues in the feature map are\nprojected to a C dimensional vector. The hyperparame-\nter C has been empirically chosen as 96 for our work.\nThe various selected hyperparameters for our proposed\nmethod are shown in Table1. The output of the patch\nembeddinglayerleadstotwoSwinTransformernetworks.\nThe output of the second Swin-T network is applied to\na patch merging layer. Patch merging works in a similar\nway to CNN’s pooling layer by concatenating the features\nof each group of neighboring patches and applying a lin-\near embedding layer to change the output dimension to\n2C. Hence the output of patch merging layer is(H\n8 x W\n8 x\n2C) and is followed by global average pooling and a dense\nlayer with 11 nodes and a softmax activation function.\nFigure 3(b) shows the internal architecture of the Swin-T\nblock. Shifted windows approach is used in the encoder\nto address the multi-head self-attention (MSA) scheme.\nThe output of the patch embedding layer is divided into\nnon-overlapping windows (in our work, we chooseN =\n4, whereN i st h en u m b e ro fw i n d o w s ) .H e r et oc o m p u t e\nthe self-attention of a given patch within that window,\nwe ignore the rest of the patches in other windows. As\nillustrated in Fig.3(b), W-MSA is the windowed multi-\nhead self-attention in which we divide the patched image\ninto non-overlapping windows and compute attention for\npatches within the window. In SW-MSA, the window is\nstride forwarded by two patches just like the kernel strid-\ninginCNNandcomputing attentionwithinthatwindow.\nFor the empty patches, the process was repeated after\nzero padding. W-MSA and SW-MSA are followed by a\n2-layer MLP each with 256 nodes and GELU nonlinear-\nity in between. LN is applied before each MSA module\nand MLP, and a residual connection is applied after each\nmodule.Themodifiedequationforattention[ 29]is\nAttention (Q,K,V) = softmax\n(\nQKT\n√\ndk + B\n)\nV (10)\nwhereBistherelativepositionofwindow.\n5 Performanceevaluation\n5.1 Dataset\nThe performance of the proposed system is evaluated\nusing the IRMAS (Instrument Recognition in Musical\nAudio Signals) dataset, developed by the Music Technol-\nogy Group (MTG) of Universitat Pompeu Fabra (UPF). It\nconsists of more than 6000 musical audio excerpts from\nvariousstyleswithannotationsofthepredominantinstru-\nments present. All audio files in the IRMAS dataset are\nFig.3 (a)BlockdiagramoftheproposedmethodofSwintransformer,(b)InternalarchitectureofSwin-Tblock\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page7of14\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz. IRMAS dataset [2] contains separate train-\ning and testing set of eleven classes. The classes include\ncello(Cel),clarinet(Cla),flute(Flu),acousticguitar(Gac),\nelectric guitar (Gel), organ (Org), piano (Pia), saxophone\n(Sax), trumpet (Tru), violin (Vio), and human singing\nvoice (Voice). The training data are single-labeled and\nconsist of 6705 audio files with excerpts of 3 s from more\nthan2000distinctrecordings.Ontheotherhand,thetest-\ning data are multi-labeled and consist of 2874 audio files\nwith lengths between 5 and 20 s and contain multiple\npredominantinstruments.Thisdatasethastwodisadvan-\ntages when training models. First, the number of audio\nfiles available for certain instruments like cello, clarinet,\nand flute is lessthan500,andthe modelstrained with the\ndata are hardly generalizable. Second, the dataset is not\nwell balanced in terms of either musical genre or instru-\nmentation. However, this may not be a problem if the\ndatasets were larger and the distribution represented the\nrealworld.Dataaugmentationoffersanexcellentsolution\nto this issue. Data augmentation means training the deep\nnetwork with additional diverse data. This increases the\ngeneralization capability of the network and thus reduces\noverfitting.\n5.2 DataaugmentationusingWaveGAN\nGenerative adversarial networks (GAN) have been suc-\ncessfully applied to a variety of problems in image gen-\neration [41] and style transfer [42]. WaveGAN architec-\nture is similar to deep convolutional GAN (DCGAN),\nwhich is used for Mel-spectrogram generation in vari-\nous music processing applications. The DCGAN gener-\nator uses transposed convolution to iteratively upsample\nlow-resolutionfeaturemapsintoahigh-resolutionimage.\nIn WaveGAN architecture, the transposed convolution\noperation is modified to widen its receptive field. Specifi-\ncally, longer one-dimensional filters of length 25 are used\ninstead of two-dimensional filters of size 5×5a n dt h e\nintermediate representation is upsampled by a factor of\nfour instead of two at each layer. The input to the genera-\ntor is a random sample taken from a uniform distribution\nbetween −1 and 1 and is projected and reshaped to the\ndimension 16×1024. This is followed by six transpose\nconvolution layers that upsample the input feature map\nto a fine and detailed output. The output of the genera-\ntor is 65,536 samples (corresponding to 4.01 s of audio at\n16 kHz). It is also capable to produce 1.49 s of audio at\n44.1khzbychoosingtheslicelengthof65536samples.The\noutput of the generator is directly applied to the input of\nthe discriminator. The discriminator is an efficient CNN\nthat discriminates between real and generated samples.\nThediscriminatorisalsomodifiedsimilarly,usinglength-\n25 filters in one dimension and increasing stride from\ntwo to four which results in WaveGAN architecture [43].\nThe transposed convolution in the generator produces\ncheckerboard artifacts [43]. To ensure that the discrimi-\nnator does not learn these artifacts, we use phase shuffle\noperation(withhyperparametern=2)assuggestedin[ 43].\nReLUisusedastheactivationfortransposedconvolution\nlayers and LReLU withα = 0.2 is chosen for convolution\noperation. Finally, the system is trained using the Wasser-\nstein GAN with gradient penalty (WGAN-GP) strategy\n[44]totacklethevanishinggradientproblemandenhance\ntraining stability. For training, the WaveGAN optimizes\nWGAN-GP using Adam for both generator and discrim-\ninator. A constant learning rate of 0.0001 is used with\nβ1 = 0.5and β2 = 0.9.\nWaveGAN is trained for 2000 epochs on the three-sec\naudio files of each class to generate similar audio files\nbased on a similarity metric (s)[ 45] with an acceptance\ncriterion ofs > 0.1. The values of parameters and hyper-\nparameters associated with WaveGAN for our experi-\nments are listed in Table2. A total of 6585 audio files\nwith cello (625), clarinet (482), flute (433), acoustic guitar\n(594), electric guitar (732), organ (657), piano (698), sax-\nophone (597), trumpet (521), violin (526), and voice (720)\nare generated. Training files available in the corpus are\ndenoted byTrainDB and the generated files are added to\nthe available training corpus, and the augmented corpus\nis denoted byTrainAugDB. Mel-spectrogram, modgdgram,\nand tempogram of natural and generated audio files for\nacousticguitarareshowninFig. 1.Theexperimentdetails\nand a few audio files can be accessed at https://sites.\ngoogle.com/view/audiosamples-2020/home/instrument.\nThe quality of generated files is evaluated using a per-\nceptualtest.Itisconductedwithtenlistenerstoassessthe\nquality of generated files for 275 files covering all classes.\nListeners are asked to grade the quality by choosing one\namongthefiveopiniongradesvaryingfrompoortoexcel-\nlent quality (scores, 1 to 5). A mean opinion score (MOS)\nof 3.64 is obtained. This value is comparable to the MOS\nscoreobtainedin[ 43]and[ 46]usingW a veGAN .\n5.3 Experimentalset-up\nTheexperimentisprogressedinfourphases,namelyMel-\nspectrogram-based, modgdgram-based, and tempogram-\nTable2 VarioushyperparameterschosenforWaveGAN\nName Value\nWavGANLatentdimension 100\nNumberofchannels 1\nWavGANdimension 32\nTrainingbatchsize 64\nKernellength 25\nGenerationlength 65,536samples\nLoss WGAN-GP( λ =10)\nDupdatesperGupdates 5\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page8of14\nbased, followed by soft voting. Hard or majority voting\nis not used in our method since the presence of simulta-\nneously occurring partials degrades its performance [1].\nHan’s model [1] is implemented with 1 s slice length\nfor performance comparison. In their approach, sigmoid\noutputs obtained by sliding window analysis on Mel-\nspectrogram inputs were aggregated followed by thresh-\nolding, and the candidates above that particular thresh-\nold were considered as predominant instruments. In our\nproposed method of soft voting, the predicted proba-\nbilities from three networks are summed followed by\nthresholding. We choose a threshold value of 0.5 empir-\nically as it helps to recognize most of the predominant\ninstruments[ 1].\n5.3.1 Trainingconfiguration\nThe DNN network is trained with categorical cross-\nentropy loss function using Adam optimizer with a learn-\ning rate of 0.001 and a mini- batch size of 128. For CNN\nnetworks, we choose a batch size of 128 and an Adam\noptimizer with a categorical cross-entropy loss function.\nFor Vi-T, we used categorical cross-entropy loss function\nusing Adam optimizer, with a learning rate of 0.001 and\nweightdecayof0.0001,andthemini-batchsizewassetto\n256. For Swin-T we used categorical cross-entropy using\ntheAdamoptimizer,withalearningrateof0.001andgra-\ndient clip value of 0.5, and the mini-batch size was set to\n32. 20% of training data is used for tuning the hyperpa-\nrametersduringvalidationforallthemodels.Thetraining\nwas stopped when the validation loss did not decrease for\nmorethantwoepochs.\n5.3.2 Testingconfiguration\n2874 polyphonic files of variable length with multiple\npredominant instruments are used for the testing phase.\nSince the number of annotations for each class was not\nequal,wecomputedprecision,recall,andF1measuresfor\nboth the micro and the macro averages. For the micro\naverages, we calculated the metrics globally, thus giving\nmore weight to the instrument with a higher number of\nappearances. On the other hand, we calculated the met-\nrics for each label and found their unweighted average for\nthemacroaverages.\n6 Resultsandanalysis\nTheoverallperformanceofdifferentphasesoftheSwin-T\nexperiment with data augmentationTrainAugDB is tabu-\nlated in Table3. Our proposed method of Voting-Swin-\nT achieved micro and macro F1 measures of 0.66 and\n0.62, respectively, which are 3.12% and 12.72% relatively\nhigher than those obtained for Mel-spectrogram-based\nHan’s model. The performance of the various features\ndepends on the instrument characteristics and other\nunknown factors, and none of the features consistently\noutperforms all others. The proposed Mel-spectrogram-\nSwin-Tframeworkshowssuperiorperformanceforseven\ninstrument classes than Han’s model. Our proposed\nModgdgram-Swin-T framework shows a competing per-\nformancewiththestate-of-the-artHan’smodel.Whilethe\nHanmodelreportsamacro-F1scoreof0.55,ourproposed\nModgdgram-Swin-Tgives0.51.Inthecaseofmodgdgram\nprocessing, instruments like the electric guitar, organ,\nsaxophone, trumpet, and violin show enhanced perfor-\nmance over the Mel-spectrogram-Swin-T. They showed\nimproved performance for four instruments than Han’s\nmodel. It shows the promise of the image processing\naspectofmodgdgramforpredominantinstrumentrecog-\nnition. Also, our proposed Tempogram-Swin-T shows\nsimilar performance as that of the Mel-spectrogram\nTable3 Precision(P),recall(R),andF1scorefortheSwin-TexperimentsandHan’smodelwithdataaugmentation\nClass Han’sModel Mel-spectrogram Modgdgram Tempogram Voting\nPRF 1PRF 1PRF 1PRF 1PRF 1\nCel 0.55 0.55 0.55 0.52 0.58 0.55 0.27 0.40 0.32 0.52 0.46 0.49 0.61 0.62 0.61\nCla 0.11 0.65 0.18 0.47 0.76 0.58 0.24 0.50 0.33 0.44 0.79 0.56 0.36 0.77 0.49\nFlu 0.33 0.61 0.43 0.81 0.83 0.82 0.52 0.63 0.57 0.81 0.80 0.80 0.57 0.80 0.66\nGac 0.84 0.63 0.72 0.43 0.62 0.51 0.64 0.47 0.54 0.30 0.64 0.41 0.59 0.60 0.59\nGel 0.69 0.69 0.69 0.70 0.52 0.60 0.57 0.55 0.56 0.78 0.42 0.55 0.73 0.59 0.66\nOrg 0.45 0.46 0.45 0.59 0.53 0.56 0.44 0.55 0.49 0.67 0.53 0.59 0.53 0.58 0.55\nPia 0.76 0.61 0.67 0.61 0.54 0.57 0.71 0.47 0.56 0.51 0.50 0.51 0.81 0.51 0.63\nSax 0.62 0.61 0.61 0.68 0.55 0.61 0.53 0.57 0.55 0.78 0.48 0.59 0.61 0.51 0.56\nTru 0.47 0.42 0.44 0.59 0.68 0.63 0.50 0.72 0.59 0.62 0.66 0.64 0.58 0.74 0.65\nVio 0.41 0.57 0.48 0.53 0.59 0.56 0.40 0.63 0.49 0.56 0.55 0.56 0.59 0.73 0.65\nVoice 0.94 0.78 0.85 0.70 0.79 0.75 0.57 0.59 0.58 0.77 0.80 0.78 0.69 0.90 0.78\nMacro 0.56 0.60 0.55 0.60 0.63 0.61 0.49 0.55 0.51 0.62 0.60 0.59 0.61 0.67 0.62\nMicro 0.64 0.64 0.64 0.62 0.62 0.62 0.54 0.54 0.54 0.58 0.58 0.58 0.66 0.66 0.66\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page9of14\nFig.4 Instrument-wiserecallforexperimentswithdataaugmentation\nnetwork and reports a better macro score than Han’s\nmodel. It shows superior performance for five instru-\nmentclassesthanHan’smodelandthreeinstrumentsover\nour proposed Mel-spectrogram-Swin-T network. Thus\nour proposed voting-Swin-T and Mel-spectrogram-Swin-\nTshowedimprovedperformancethanthestate-of-the-art\nHan’s model.\n6.1 Analysisofinstrument-wiseidentification\nperformance\nThe instrument-wise recall for all our voting experiments\nwith data augmentation is shown in Fig.4.T h ep r o p o s e d\nVoting frameworks showed superior performance to the\nstate-of-the-art Han’s model. In the case of ensemble vot-\ning using CNN, instruments like the clarinet, electric\nguitar, piano, and trumpet show improved performance\nover Han’s model. In the case of voting using transform-\ners, seven instruments showed improved performance\nover Han’s model. For all the voting techniques, the voice\nreports a high recall due to its distinct spectral character-\nistic[1].\n6.2 Effectofdataaugmentation\nF o rd e e pl e a r n i n g ,t h en u m b e ro ft r a i n i n ge x a m p l e si s\ncritical for the performance compared to the case of\nusing hand-crafted features because it aims to learn a\nfeature from the low-level input data [1]. The problem\nwith small datasets is that models trained with them\ndo not generalize well from the validation and test set\n[47]. Han’s model using (TrainDB) reports a low F1 score\nof about 0.20 for cello, and they suggest that it is due\nto the insufficient number of training samples [1]. The\nsameexperimentwhenrepeatedusing TrainAugDB andour\nMel-spectrogram-Swin-T showed an improved F1 score\nvalida t est hec laimin[1].\nThe significance of data augmentation in the proposed\nmodel can be analyzed from Table4. While the proposed\nmethod of Voting-Swin-T, without data augmentation\n(TrainDB), reports micro and macro F1 score of 0.59 and\n0.60, respectively, the metrics improved to 0.66 and 0.62,\nrespectively, for the data augmentation scheme. It shows\nan improvement of 11.86% and 3.33% relatively higher\nthan that obtained for experiments withTrainDB. Simi-\nlarperformanceimprovementisobservedforHan’smodel\nandMTF-SVMandDNNframeworks.\n6.3 Effectoftransformerarchitectureandattention\nThe instrument-wise F1 scores for all the Mel-\nspectrogram experiments are shown in Fig.5.T h em o d e l\nusing CNN alone does not show improved performance\nas expected; this is mainly because of the difficulty in\npredicting the multiple predominant instruments from\nthe variable-length testing file while training with single\npredominant fixed-length training files. Only the instru-\nments with distinct spectral characteristics and voice\nshow good performance. On the other hand, experiments\nwith transformer architecture showed improved perfor-\nmance for all the instruments. This is mainly because\nthe transformer architecture with a multi-head attention\nmechanism helps to focus or attend to specific regions of\nthe visual representation for predominant instruments\nrecognition. Another important point is that it requires\nvery few trainable parameters to learn the model, which\nTable4 Effectofdataaugmentation.Thehighestvaluesare\nhighlighted\nSL.No Model TrainDB TrainAugDB\nMicroF1 MacroF1 MicroF1 MacroF1\n1 HanModel[ 1] 0.60 0.50 0.64 0.55\n2 K.Racharlaetal.\n[38](MTF-SVM)\n0.22 0.19 0.25 0.23\n3 MTF-DNN 0.32 0.28 0.38 0.35\n4 Proposed\nmethod\n-Voting-Swin-T\n0.59 0.60 0.66 0.62\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page10of14\nFig.5 Instrument-wiseF1scoresforMel-spectrogramexperimentswithdataaugmentation\nhelps to reach convergence faster than the models\nemployingCNNalone.Forpolytimbralmusicinstrument\nrecognition attention model focuses on specific time\nsegments in the audio relevant to each instrument label.\nThe ability of the attention model to weigh relevant\nand suppress irrelevant predictions for each instrument\nleads to better classification accuracy [15]. Compared to\nself-attention multi-head attention gives the attention\nlayer multiple representation subspaces, and as the image\npassesthroughdifferentheads,predictionsaboutthepre-\ndominant instruments are more refined than employing\nsingle head self-attention. In the case of ViT, we have to\ncompute the self-attention for a given patch with all the\notherpatchesinaninputimage.Ontheotherhand,Swin-\nT with shifted window scheme gives the effect of kernel\nstriding in CNN which along with multi-head attention\nhelps to recognize multiple predominant instruments\nwithlinearcomputationalcomplexity.\nWealsoconductedanablationstudyofthearchitecture\nin order to gain a better understanding of the network’s\nbehavior. We investigated the performance by changing\nthe number of heads, patch size, projection dimension,\nand the number of MLP nodes. The results are tabulated\ninTable 5.TheoptimalparametersobtainedthroughMel-\nspectrogram analysis are applied to the modgdgram and\ntempogramarchitecturesthroughasimilarablationstudy.\nTo summarize, the results show the potential of Swin-T\narchitecture and the promise of alternate visual represen-\ntationsotherthantheconventionalMel-spectrogramsfor\npredominantinstrumentsrecognitiontasks.\n6.4 Effectofvotingandablationstudyofensemble\nSeveral studies [48, 49] have demonstrated that by con-\nsolidating information from multiple sources, better per-\nformance can be achieved compared to uni-modal sys-\ntems which motivated us to perform the ensemble voting\nmethod. We also conducted the ablation study of the\nensemble to evaluate the contribution of the individ-\nual parts in the proposed ensemble classification frame-\nworkforpredominantinstrumentrecognition.Sincethere\nare three visual representations, we have experimented\nwith different fusion schemes as shown in Table 6.\nTable 6 reports F1 measures for different fusion strate-\ngies trained withTrainAugDB.S p e c t ,M o d g d ,a n dT e m p o\nrefer to Mel-spectrogram-Swin-T, Modgdgram-Swin-T,\nandTempogram-Swin-Trespectively.\nIt is important to note that Spect + Modgd and Modgd\n+ Tempo show improvement in macro measures com-\nparedtoMel-spectrogram-basedHan’smodel.Thisshows\nthe importance of phase information in the proposed\nTable5 AblationstudyoftheMel-spectrogramarchitectureshowingtheeffectofnumberofheads,patchsize,projectiondimension,\nandnumberofMLPnodes.Highestvaluesarehighlighted\nSL.No ArchitectureSpec. Option Vi-T Swin-T\nF1Micro F1Macro F1Micro F1Macro\n1 Numberofheads 4 0.58 0.52 0.62 0.53\n8 0.59 0.57 0.62 0.61\n2 Patchsize 4x4 0.55 0.50 0.62 0.61\n6x6 0.59 0.57 0.60 0.53\n3 Projectiondimension 64 0.59 0.57 0.56 0.51\n96 0.58 0.51 0.62 0.61\n4 NumberofMLPnodes 2048,1024 0.59 0.57 0.62 0.55\n256,256 0.57 0.51 0.62 0.61\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page11of14\nTable6 Ablationstudyofensemblewithdataaugmentation.\nSpect,Modgd,andTemporefertoMel-spectrogram-Swin-T,\nModgdgram-Swin-T,andTempogram-Swin-T,respectively.+\ndenotessoftvoting\nSl.No Ensemble F1Micro F1Macro\n1 Spect+Modgd. 0.59 0.60\n2 Spect+Tempo. 0.64 0.60\n3 Modgd+Tempo. 0.57 0.55\n4 Spect+Modgd+Tempo. 0.66 0.62\ntask. Conventionally, the spectrum-related features used\nin instrument recognition take into account merely the\nmagnitude information. However, there is often addi-\ntional information concealed in the phase, which could\nbe beneficial for recognition as seen in [16]. In the case\nof tempogram, Spect + Tempo showed improved per-\nformance over Han’s model. The advantage of onsets\nin extracting informative cues about musical instrument\nrecognitionisproposedin[ 50].Humanlistenerscaneasily\nidentify instrument sounds from onset portions com-\npared to other portions of the sound. Cemgil et al. [51]\ndefine the “tempogram” which induces a probability dis-\ntribution over the pairs (pulse period, pulse phase) given\nthe onsets. In most automated tempo and beat tracking\napproaches, the first step is to estimate the positions of\nnote onsets within the music signal. Results of the exper-\niments described in [52] suggested that the presence of\nonsetswasbeneficial,inparticularforinstrumentsounds.\nSince onset detection is the primary step in comput-\ning tempogram, it can provide useful information about\npredominant instruments. The experimental results val-\nidate the claim in [52]. The advantage of voting is that\nit is unlikely that all classifiers will make the same mis-\ntake, as long as every error is made by a minority of the\nclassifiers, an optimal classification can be achieved [53].\nSince the ensemble soft voting of three representations\nresults in better performance, we opted for the same as\nthe final scheme and our proposed ensemble frameworks\noutperformthestate-of-the-artHan’smodel.\n6.5 Comparisontoexistingalgorithms\nThe performance metrics for various algorithms on the\nIRMAS corpus are reported in Table7. The number of\ntrainableparametersisalsoindicated.\nBosch et al. [9] modified the Fuhrmann’s algorithm [2]\nand used typical hand-made timbral audio features with\ntheir frame-wise mean and variance statistics to train\nSVMs with a source separation technique called flexible\naudio source separation framework (FASST) in a prepro-\ncessing step. It reports a micro and macro F1 score of\n0.50 and 0.43 respectively, and it is evident that the pro-\nposedensembleframeworksoutperformthehand-crafted\nTable7 PerformancecomparisononIRMASdataset.Thebest\nresultintheproposedschemeishighlightedinred\nSl.No Model/parameters F1Micro F1Macro\n1 Boschetal.[ 9] 0.50 0.43\n2 Hanetal.[ 1]/1446k 0.60 0.50\n3P o n s e t a l . [ 10]/743k 0.59 0.52\n4 Proposedmethod-\nVoting-CNN/2040k\n0.63 0.57\n5 Proposedmethod-\nVoting-Vi-T/1079k\n0.65 0.60\n6 Proposedmethod-\nVoting-Swin-T/350k\n0.66 0.62\nfeatures. The MTF-SVM approach [38]h a sn o ts h o w n\ngoodperformanceasexpected.Thestate-of-the-artHan’s\nmodel (TrainDB)[1] reports micro and macro F1 score\nof 0.60 and 0.50 respectively. Han Model (TrainAugDB)\nreportsmicroandmacroF1scoreof0.64and0.55respec-\ntively. The proposed voting model using Swin-T reports\nmicro and macro F1 scores of 0.66 and 0.62 respectively.\nThese values are 3.12% and 12.72% relatively higher than\nthe state-of-the-art Han’s model. Han et al. [1]d e v e l o p e d\na deep CNN for instrument recognition based on Mel-\nspectrogram inputs and aggregation of multiple outputs\nfrom sliding windows over the audio data. Pons et al.\n[10] customized the architecture of Han et al. and intro-\nduced two models, namely, single-layer and multi-layer\napproaches. They used the same aggregation strategy as\nthat of Han’s model by averaging the softmax predictions\nand finding the candidates with a threshold of 0.2. As dif-\nferentfromtheexistingapproaches,weestimatedthepre-\ndominant instrument using the entire Mel-spectrogram\nwithout sliding window and aggregation analysis. Better\nmicro and macro measures show that it is possible to\npredict multiple instruments from the visual represen-\ntations without any sliding window analysis. Also, our\nproposed Swin-T for Mel-spectrogram requires approxi-\nmately four times fewer trainable parameters than Han’s\nmodel [54]. In [15], the usage of an attention layer was\nshown to improve classification results in the OpenMIC\ndataset when applied to a set of Mel-spectrogram fea-\ntures extracted from a pre-trained VGG net. While the\nworkfocusesonMel-spectrogram,weexperimentedwith\nthe effect of phase and tempo information along with\nmagnitude information. Our proposed ensemble voting\ntechniqueoutperformedexistingalgorithmsandtheMTF\nDNNandSVMframeworkontheIRMASdatasetforboth\nthemicroandthemacroF1measure.\n7C o n c l u s i o n\nWe presented a transformer-based predominant instru-\nment recognition system using multiple visual repre-\nsentations. Transformer models are used to capture\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page12of14\nthe instrument-specific characteristics and then do fur-\nther classification. We experimented with Vi-T and the\nrecentSwin-Tarchitectureswithadetailedablationstudy\nand our proposed experiments using Swin-T outperform\nexistingalgorithmswithverylesstrainableparameters.\nWe introduced an alternate visual representation to\nconventionally used Mel-spectrograms. Our study shows\nthat visual representation in terms of modgdgram can\nbe explored in many applications. We believe that opti-\nmum parameters may potentially lead to a better visual\nrepresentation for modified group delay functions. It is\nworth noting that many recent deep learning schemes\nin image processing such as transfer learning, atten-\ntion mechanism, and transformers are transferable to the\naudio processing domain. Modified group delay func-\ntions can be computed directly from the music sig-\nnal and also from the flattened music spectrum. It is\nknown as direct-modgdgram (or simply “modgdgram”)\nand source-modgdgram, respectively. Direct modgdgram\nemphasizes system information and source-modgdgram\nprovides information about the multiple sources present\nin the music signal [55]. Source-modgdgram has been\neffectivelyusedformelodyextraction[ 56]andmulti-pitch\nestimation [57]. Since we need system information to\ntrack the presence of instruments, we employ the direct-\nmodgdgramforthetaskofinstrumentrecognition.\nThe proposed method is evaluated using the IRMAS\ndataset. As observed in many music information retrieval\ntasks, the data augmentation strategy has also shown its\npromise in the proposed task. The time-domain strat-\negy of synthetic music generation for data augmentation\nusing WaveGAN is explored. WaveGAN data augmenta-\ntion for instrument detection is probably a new attempt\nin predominant instrument recognition. As future work,\nwe would like to focus on synthesizing high-quality audio\nfiles using recent high fidelity audio synthesis approaches\ndiscussed in [58] and to compare the pipeline of tradi-\ntional audio augmentations used in many tasks [23]w i t h\nadversarial audio synthesis. The ensemble voting frame-\nworkoutperformstheexistingstate-of-the-artalgorithms\nand music texture features DNN and SVM frameworks.\nThe results show the potential of the ensemble vot-\ning technique in predominant instrument recognition in\npolyphonicmusic.\nAcknowledgements\nTheauthorswouldliketoacknowledgeJuanJ.Bosch,FerdinandFuhrmann,\nandPerfectoHerrera(MusicTechnologyGroup-UniversitatPompeuFabra)\nfordevelopingtheIRMASdatasetandmakingitpubliclyavailable.\nAuthors’contributions\nLCRandRRjointlydesigned,implemented,andinterpretedthecomputer\nsimulations.RRimplementedthemodgdgramalgorithm.Allauthors\ncontributedtowritingthemanuscriptandfurtherreadandapprovedthefinal\nmanuscript.\nFunding\nNotapplicable.\nAvailabilityofdataandmaterials\nThedatasetsgeneratedand/oranalyzedduringthecurrentstudyareavailable\ninthezenodorepository( https://www.upf.edu/web/mtg/irmas)andare\npubliclyavailable.\nDeclarations\nCompetinginterests\nTheauthorsdeclarethattheyhavenocompetinginterests.\nReceived:2December2021 Accepted:22April2022\nReferences\n1. Y.Han,J.Kim,K.Lee,Deepconvolutionalneuralnetworksfor\npredominantinstrumentrecognitioninpolyphonicmusic.IEEE/ACM\nTrans.Audio,SpeechLang.Process. 25(1),208–221(2017)\n2. F.Fuhrmann,P.Herrera,in Proc.of13thInternationalConferenceonDigital\nAudioEffects(DAFx10)Graz,Austria,September6-10,2010 .Polyphonic\ninstrumentrecognitionforexploringsemanticsimilaritiesinmusic,\n(2010),pp.1–8\n3. J.-Y.Liu,Y.-H.Yang,in Proc.ofthe24thACMMultimediaConference\nAmsterdam,NetherlandsOctober15-19,2016 .Eventlocalizationinmusic\nauto-tagging(AssociationforComputingMachinery,NewYork,2016),\npp.1048–1057\n4. Z.Duan,J.Han,B.Pardo,Multi-pitchstreamingofharmonicsound\nmixtures.IEEE/ACMTransactionsonAudio.SpeechLang.Process. 22(1),\n138–150(2013)\n5. G.Peeters,B.L.Giordano,P.Susini,N.Misdariis,S.McAdams,Thetimbre\ntoolbox:Extractingaudiodescriptorsfrommusicalsignals.J.Acoust.Soc.\nAm.130(5),2902–2916(2011)\n6. P.Smaragdis,J.C.Brown,in ProcofIEEEWorkshoponApplicationsofSignal\nProcess.AudioAcoust.,NewPaltz,NY,2003 .Non-negativematrix\nfactorizationforpolyphonicmusictranscription,(2003),pp.177–180\n7. P.Li,J.Qian,T.Wang,Automaticinstrumentrecognitioninpolyphonic\nmusicusingconvolutionalneuralnetworks.arXivpreprint\narXiv:1511.05520(2015)\n8. T.Kitahara,M.Goto,K.Komatani,T.Ogata,H.G.Okuno,Instrument\nidentificationinpolyphonicmusic:Featureweightingtominimize\ninfluenceofsoundoverlaps.EURASIPJ.Adv.SignalProc. 2007,1–15\n(2006)\n9. J.J.Bosch,J.Janer,F.Fuhrmann,P.Herrera,in Proc.ofthe13th\nInternationalSocietyforMusicInformationRetrievalConference,ISMIR,Porto,\nPortugal.Acomparisonofsoundsegregationtechniquesfor\npredominantinstrumentrecognitioninmusicalaudiosignals,(2012),\npp.552–564. https://doi.org/10.5281/zenodo.1416076\n10. J.Pons,O.Slizovskaia,R.Gong,E.Gomez,X.Serra,in Proc.of25thEuropean\nSignalProcessingConferenceKosInternationalConventionCentre(KICC),\nPsalidi,KosIsland,August28toSeptember2,2017 .Timbreanalysisofmusic\naudiosignalswithconvolutionalneuralnetworks(IEEE,2017),\npp.2744–2748\n11. S.Gururani,C.Summers,A.Lerch,in Proc.of19thInternationalSocietyfor\nMusicInformationRetrievalConferenceParis,France.September23-27,2018 .\nInstrumentactivitydetectioninpolyphonicmusicusingdeepneural\nnetworks,(2018),pp.569–576. https://doi.org/10.5281/zenodo.1492479\n12. D.Yu,H.Duan,J.Fang,B.Zeng,Predominantinstrumentrecognition\nbasedondeepneuralnetworkwithauxiliaryclassification.IEEE/ACM\nTrans.AudioSpeechLang.Process. 28,852–861(2020)\n13. J.S.G’omez,J.Abeßer,E.Cano,in Proc.ofthe19thInternationalSocietyfor\nMusicInformationRetrievalConference,ISMIR,Paris,FranceSeptember23-27,\n2018.Jazzsoloinstrumentclassificationwithconvolutionalneural\nnetworks,sourceseparation,andtransferlearning,(2018),pp.577–584.\nhttps://doi.org/10.5281/zenodo.1492481\n14. X.Li,K.Wang,J.Soraghan,J.Ren,in ProcofInternationalConferenceon\nComputationalIntelligenceinMusicSoundArtandDesign(PartofEvoStar) .\nFusionofhilberthuangtransformanddeepconvolutionalnetworkfor\npredominantmusicalinstrumentsrecognitionvol.12103ofLecture\nNotesinComputerScience(Springer,2020),pp.80–89\n15. K.Watcharasupat,S.Gururani,A.Lerch,Visualattentionformusical\ninstrumentrecognition.arXivpreprintarXiv:2006.09640(2020)\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page13of14\n16. A.Diment,P.Rajan,T.Heittola,T.Virtanen,in Proc.ofthe10thInternational\nSymposiumonComputerMusicMultidisciplinaryResearch,Marseille,France,\nOctober15-18,2013 .Modifiedgroupdelayfeatureformusicalinstrument\nrecognition(LMA,2013),pp.431–438. http://www.cmmr2013.cnrs-mrs.fr/\nDocs/CMMR2013Proceedings.pdf\n17. F.Fuhrmann,etal., Automaticmusicalinstrumentrecognitionfrom\npolyphonicmusicaudiosignals .(PhDthesis,UniversitatPompeuFabra,\n2012)\n18. H.A.Murthy,B.Yegnanarayana,Groupdelayfunctionsandits\napplicationsinspeechtechnology.Sadhana. 36(5),745–782(2011)\n19. B.Yegnanarayana,H.A.Murthy,Significanceofgroupdelayfunctionsin\nspectrumestimation.IEEETrans.SignalProcess. 40(9),2281–2289(1992)\n20. K.K.Paliwal,L.D.Alsteris,Ontheusefulnessofstftphasespectrumin\nhumanlisteningtests.SpeechCommun. 45(2),153–170(2005)\n21. P.Grosche,M.Muller,F.Kurth,in Proc.ofIEEEInternationalConferenceon\nAcoustics,SpeechandSignalProcessing,2010-Mar15-19,Dallas,Texas,USA .\nCyclictempogram—amid-leveltemporepresentationformusicsignals,\n(2010),pp.5522–5525. https://doi.org/10.1109/ICASSP.2010.5495219\n22. M.Muller,T.Pratzlich,J.Driedger,in Proc.of13thInternationalSocietyfor\nMusicInformationRetrievalConference(ISMIR),Porto,Portugal,October\n8th-12th,2012.Across-versionapproachforstabilizingtempo-based\nnoveltydetection,(2012),pp.427–432\n23. A.Kratimenos,K.Avramidis,C.Garoufis,A.Zlatintsi,P.Maragos,in Proc.of\n28thEuropeanSignalProcessingConference(EUSIPCO2020),Virtual,January\n18-22,2021.Augmentationmethodsonmonophonicaudiofor\ninstrumentclassificationinpolyphonicmusic,(2021),pp.156–160.\nhttps://doi.org/10.23919/Eusipco47968.2020.9287745\n24. O.Slizovskaia,E.G’omez,G.Haro,in Proc.ofthe2017ACMonInternational\nConferenceonMultimediaRetrievalICMR’17,June6-9,2017,Bucharest,\nRomania.Musicalinstrumentrecognitioninuser-generatedvideosusing\namultimodalconvolutionalneuralnetworkarchitecture,(2017),\npp.226–232. https://doi.org/10.1145/3078971.3079002\n25. S.Oramas,F.Barbieri,O.NietoCaballero,X.Serra,Multimodaldeep\nlearningformusicgenreclassification.Trans.Int.Soc.MusicInf.Retr. 1,\n4–21(2018). https://doi.org/10.5334/tismir.10\n26. C.Chen,Q.Li,Amultimodalmusicemotionclassificationmethodbased\nonmultifeaturecombinednetworkclassifier.Math.Probl.Eng. 2020\n(2020).https://doi.org/10.1155/2020/4606027\n27. A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.\nKaiser,I.Polosukhin,in Proc.of31stConferenceonNeuralInformation\nProcessingSystems(NIPS2017)LongBeach,CA,USA .Attentionisallyou\nneed(CurranAssociates,Inc,pp.5998–6008. http://arxiv.org/abs/1706.\n03762\n28. T.Zhong,S.Zhang,F.Zhou,K.Zhang,G.Trajcevski,J.Wu,Hybridgraph\nconvolutionalnetworkswithmulti-headattentionforlocation\nrecommendation.WorldWideWeb. 23(6),3125–3151(2020)\n29. Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,B.Guo,Swin\ntransformer:Hierarchicalvisiontransformerusingshiftedwindows.arXiv\npreprintarXiv:2103.14030(2021)\n30. M.Sukhavasi,S.Adapa,Musicthemerecognitionusingcnnand\nself-attention.arXivpreprintarXiv:1911.07041(2019)\n31. D.Ghosal,M.H.Kolekar,in Proc.ofInterspeech,Hyderabad,India,September\n2-6,2018.Musicgenrerecognitionusingdeepneuralnetworksand\ntransferlearning,(2018),pp.2087–2091. https://doi.org/0.21437/\nInterspeech.2018-2045\n32. W.J.Poser, Douglaso’shaughnessy,speechcommunication:Humanand\nmachine.(Addison-wesleypublishingcompany,Reading,Massachusetts,\n1987),pp.52–54\n33. R.Rajan,H.A.Murthy,Two-pitchtrackinginco-channelspeechusing\nmodifiedgroupdelayfunctions.SpeechComm. 89,37–46(2017)\n34. A.V.Oppenheim,R.W.Schafer, DiscreteTimeSignalProcessing .(Prentice\nHall,Inc,NewJersey,1990)\n35. S.Davies,Perceivingmelodiesandperceivingmusicalcolors.Rev.Philos.\nPsychol.1,19–39(2009). https://doi.org/10.1007/s13164-009-0007-2,\nhttps://psycnet.apa.org/doi/10.1007/s13164-009-0007-2\n36. M.Tian,G.Fazekas,D.A.Black,M.Sandler,in Proc.ofIEEEInternational\nConferenceonAcoustics,SpeechandSignalProcessing(ICASSP) .Ontheuse\nofthetempogramtodescribeaudiocontentanditsapplicationtomusic\nstructuralsegmentation,(2015),pp.419–423\n37. M.Muller, FundamentalsofMusicProcessingAudio,Analysis,Algorithms,\nApplications,vol.5.(SpringerInternationalPublishing,Cham,2015)\n38. K.Racharla,V.Kumar,C.B.Jayant,A.Khairkar,P.Harish,in Proc.of7th\nInternationalConferenceonSignalProcessingandIntegratedNetworks\n(SPIN),Noida,India .Predominantmusicalinstrumentclassificationbased\nonspectralfeatures,(2020),pp.617–622. https://doi.org/10.1109/\nSPIN48934.2020.9071125\n39. M.D.Zeiler,R.Fergus,in Proc.ofEuropeanconferenceoncomputervision\n(ECCV).Tvisualizingandunderstandingconvolutionalnetworks(Springer\nInternationalPublishing,Switzerland,2014),pp.818–8331\n40. A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.\nUnterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,etal.,in Proc.\nof9thInternationalConferenceonLearningRepresentations(ICLR)-Virtual\nmodefromMay3-7(2021) .Animageisworth16x16words:Transformers\nforimagerecognitionatscale,(2021),pp.1–21.OpenReview.net\n41. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A.\nAaronCourville,Y.Bengio,Generativeadversarialnets.Adv.NeuralInf.\nProcess.Syst. 27,2672–2680(2014)\n42. T.Kim,M.Cha,H.Kim,J.K.Lee,J.Kim,in Proc.of34thInternational\nconferenceonmachinelearning,Sydney,Australia.06–11August2017 .\nLearningtodiscovercross-domainrelationswithgenerativeadversarial\nnetworks,vol.70(PMLR,2017),pp.1857–1865. https://proceedings.mlr.\npress/v70/kim17a.html\n43. C.Donahue,J.J.McAuley,M.Puckette,in Proc.of7thInternational\nConferenceonLearningRepresentations,ICLR2019,NewOrleans,LA,USA,\nMay6-9,2019 .Adversarialaudiosynthesis,(2019),pp.1–16.\nOpenReview.net\n44. I.Gulrajani,F.Ahmed,M.Arjovsky,V.Dumoulin,A.Courville,in Proc.ofthe\n31stInternationalConferenceonNeuralInformationProcessingSystems,\nLongBeachCaliforniaUSADecember4-9,2017 .Improvedtrainingof\nwassersteinGANs(CurranAssociatesInc.,MorehouseLaneRedHookNY,\n2017)\n45. A.Madhu,S.Kumaraswamy,in Proc.of27thEuropeanSignalProcessing\nConference(EUSIPCO),2-6September2019inACoruña,Spain .Data\naugmentationusinggenerativeadversarialnetworkforenvironmental\nsoundclassification,(2019),pp.1–5\n46. G.Atkar,P.Jayaraju,Speechsynthesisusinggenerativeadversarial\nnetworkforimprovingreadabilityofhindiwordstorecuperatefrom\ndyslexia.NeuralComput.Applic. 33,9353–9362(2021). https://doi.org/10.\n1007/s00521-021-05695-3\n47. L.Perez,J.Wang,Theeffectivenessofdataaugmentationinimage\nclassificationusingdeeplearning.arXivpreprintarXiv:1712.04621(2017)\n48. M.Lee,J.Lee,J.-H.Chang,Ensembleofjointlytraineddeepneural\nnetwork-basedacousticmodelsforreverberantspeechrecognition.\nDigit.Sig.Process. 85,1–9(2019)\n49. L.Nanni,G.Maguolo,S.Brahnam,M.Paci,Anensembleofconvolutional\nneuralnetworksforaudioclassification.arXivpreprintarXiv:2007.07966\n(2020)\n50. K.Siedenburg,M.R.Schadler,D.Hulsmeier,Modelingtheonset\nadvantageinmusicalinstrumentrecognition.J.Acoust.Soc.Am. 146(6),\n523–529(2019)\n51. A.T.Cemgil,B.Kappen,P.Desain,H.Honing,Ontempotracking:\nTempogramrepresentationandkalmanfiltering.J.NewMusic.Res. 29(4),\n259–273(2000)\n52. M.Ogg,L.R.Slevc,W.J.Idsardi,Thetimecourseofsoundcategory\nidentification:insightsfromacousticfeatures.J.Acoust.Soc.Am. 142(6),\n3459–3473(2017)\n53. M.S.MohdAzmi,M.N.Sulaiman,Accelerator-basedhumanactivity\nrecognitionusingvotingtechniquewithnbtreeandmlpclassifiers.Int.J.\nAdv.Sci.Eng.Inf.Technol. 7(1),146–152(2017)\n54. S.Paul,P.-Y.Chen,Visiontransformersarerobustlearners.arXivpreprint\narXiv:2105.07581(2021)\n55. R.Rajan, EstimationofPitchinSpeechandMusicUsingModifiedGroupdelay\nFunctions.(Ph.D.thesis,SubmittedtoIndianInstituteofTechnology,\nMadras,2017). http://compmusic.upf.edu/system/files/static_files/Rajan-\nRajeev-PhD-thesis-2017.pdf\n56. R.Rajan,H.A.Murthy,in Proc.ofIEEEInternationalConferenceonAcoustics,\nSpeechandSignalProcessing,ICASSP2013,Vancouver,BC,Canada,May\n26-31,2013.Groupdelay-basedmelodymonopitchextractionfrommusic\n(Groupdelay-basedmelodymonopitchextractionfrommusic,2013),\npp.186–190\n57. R.Rajan,H.A.Murthy,Two-pitchtrackinginco-channelspeechusing\nmodifiedgroupdelayfunctions.SpeechCommun.(2017).\n89.10.1016/j.specom.2017.02.004\nReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page14of14\n58. J.Kong,J.Kim,J.Bae,in Proc.of34thConferenceonNeuralInformation\nProcessingSystems(NeurIPS2020),Vancouver,Canada .HiFi-GAN:\nGenerativeAdversarialNetworksforEfficientandHighFidelitySpeech\nSynthesis,vol.33(CurranAssociates,Inc,2020),pp.17022–17033\nPublisher’sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaffiliations.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7303486466407776
    },
    {
      "name": "Computer science",
      "score": 0.6529937982559204
    },
    {
      "name": "Polyphony",
      "score": 0.5968685150146484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47293829917907715
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.460030198097229
    },
    {
      "name": "Convolutional neural network",
      "score": 0.412320077419281
    },
    {
      "name": "Speech recognition",
      "score": 0.37298595905303955
    },
    {
      "name": "Engineering",
      "score": 0.12016558647155762
    },
    {
      "name": "Voltage",
      "score": 0.08727675676345825
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3129367976",
      "name": "A P J Abdul Kalam Technological University",
      "country": "IN"
    }
  ],
  "cited_by": 24
}