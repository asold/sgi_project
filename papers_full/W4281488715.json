{
  "title": "DeepStruct: Pretraining of Language Models for Structure Prediction",
  "url": "https://openalex.org/W4281488715",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2137691572",
      "name": "Chenguang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097346119",
      "name": "Xiao Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2160382611",
      "name": "Zui Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3171201324",
      "name": "Haoyun Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098032575",
      "name": "Jie Tang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2131252044",
      "name": "Dawn Song",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034328552",
    "https://openalex.org/W2308486447",
    "https://openalex.org/W2103076621",
    "https://openalex.org/W2578454709",
    "https://openalex.org/W2295434202",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W3198490223",
    "https://openalex.org/W4320013820",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3035375600",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2915816387",
    "https://openalex.org/W2250225327",
    "https://openalex.org/W2152380671",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2963066655",
    "https://openalex.org/W2509043545",
    "https://openalex.org/W3121525843",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W2971136144",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2507397568",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2935052563",
    "https://openalex.org/W4288288848",
    "https://openalex.org/W3201244947",
    "https://openalex.org/W3034862440",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W2838122361",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W3035219457",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W4288364064",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2804778516",
    "https://openalex.org/W3090145439",
    "https://openalex.org/W3045703328",
    "https://openalex.org/W2563163303",
    "https://openalex.org/W3034797437",
    "https://openalex.org/W3175225269",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W4288373939",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1604644367",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3106340866",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4288104771",
    "https://openalex.org/W2902619431",
    "https://openalex.org/W3035625205",
    "https://openalex.org/W2963022746",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W1566346388",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W3015465981",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2945475330",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W1482328859",
    "https://openalex.org/W2015707701",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3173617765",
    "https://openalex.org/W3137214022",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W2251913848",
    "https://openalex.org/W2251199578",
    "https://openalex.org/W3037624666",
    "https://openalex.org/W2963009325",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.",
  "full_text": "DEEP STRUCT : Pretraining of Language Models for Structure Prediction\nChenguang Wang∗†, Xiao Liu¶†, Zui Chen¶†, Haoyun Hong¶, Jie Tang¶, Dawn Song∗\n∗UC Berkeley, ¶Tsinghua University\n{chenguangwang,dawnsong}@berkeley.edu, jietang@tsinghua.edu.cn\n{liuxiao21,chenzui19,honghy17}@mails.tsinghua.edu.cn\nAbstract\nWe introduce a method for improving the struc-\ntural understanding abilities of language mod-\nels. Unlike previous approaches that finetune\nthe models with task-specific augmentation,\nwe pretrain language models on a collection\nof task-agnostic corpora to generate structures\nfrom text. Our structure pretraining enables\nzero-shot transfer of the learned knowledge\nthat models have about the structure tasks. We\nstudy the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks\nincluding open information extraction, joint en-\ntity and relation extraction, named entity recog-\nnition, relation classification, semantic role la-\nbeling, event extraction, coreference resolution,\nfactual probe, intent detection, and dialogue\nstate tracking. We further enhance the pretrain-\ning with the task-specific training sets. We\nshow that a 10B parameter language model\ntransfers non-trivially to most tasks and ob-\ntains state-of-the-art performance on 21 of 28\ndatasets that we evaluate.1\n1 Introduction\nPretrained language models (LMs) have revolu-\ntionized NLP over the last few years (Peters et al.,\n2018; Devlin et al., 2019; Radford et al., 2019b),\nincreasingly adept in performing the flexible and\ntask-agnostic downstream transfer. Their transfer\nperformance is less studied in structure prediction\ntasks, however. Well-studied tasks mainly focus\non understanding one particular aspect of the text,\nsuch as predicting the next word that comes after\nas in language modeling. Unlike those downstream\ntasks, structure prediction requires the structural un-\nderstanding of the text for further integrating multi-\nple relevant aspects into a structure. For instance,\na typical structure prediction task, called open in-\nformation extraction, seeks the entire structural in-\n†\nEqual contribution.\n1The code and datasets are available at https://\ngithub.com/cgraywang/deepstruct.\nOpen information extraction \n(Iago; Born in; 1951)  \n \nJoint entity and relation extraction\n(Iago; instance of; person) \n(Iago; city_of_birth; Tbilisi) \nRelation classification\n(Iago; city_of_birth; Tbilisi) \nMulti-task corpora (optional)\ninput: jer conll04: An art exhibit is at the       \n           Haka Theatre \n(Haka Theatre; instance of; theatre)output:\nner genia: Japan began the\ndefence of their Asian Cup title \ninput:\noutput:\nNamed entity recognition\n(Iago; instance of; person)\n \n(Asian Cup; instance of; race)\n(Japan; instance of; location) (art exhibit; located in; Haka Theatre) \n(Iago; is a; Georgian artist) \nBorn in 1951 in Tbilisi, Iago is a Georgian artist\nDeepStruct\ninput:  triple: The couple have a daughter      output:  (couple; have; a daughter)  \nTask-agnostic corpora\n......\ninput:  entity: He played for FIFA      \n(Structure pretraining)\ninput:  relation: The book Fly is in English    output:  (Fly; language; English)\noutput:  (He; instance of; human)\n(FIFA; instance of; club)  \n......\nFigure 1: Summary of our approach and results. Upper: an\noverview of DEEP STRUCT and the proposed structure pretrain-\ning. Lower: performance of our 10B DEEP STRUCT zero-shot\nand multi-task, compared with 175B GPT-3 zero-shot.\nformation in a sentence (Figure 2). Different from\ntraditional NLP tasks, structure prediction takes\none step further and serves as a natural testbed for\nthe structural understanding competence of LMs.\nIt is non-trivial to transfer LMs to downstream\nstructure prediction tasks. While the structure pre-\ndiction requires structural understanding, the LMs\nare pretrained to understand an independent aspect.\nFor example, GPT-3 (Brown et al., 2020) is trained\nto predict the next word, and BERT (Devlin et al.,\n2019) is trained to recover the masked tokens. Re-\ncent work has made efforts in bridging the gap\nin transferring pretrained models to structure pre-\ndiction tasks with a focus on two directions. As\nshown in Figure 3, first, task-specific architectures\nare proposed to model the structures for different\nstructure prediction tasks (Stanovsky et al., 2018;\nSoares et al., 2019). Second, task-specific data aug-\nmentation (Paolini et al., 2021; Wang et al., 2021;\nWei et al., 2021) is introduced, aiming to enrich\ntext format with structure information. These ap-\nproaches involve custom-designed task augmenta-\ntions, impeding their usability in general structure\nprediction tasks.\n(1) \nBorn in 1951 in Tbilisi, Iago is a Georgian artist\nStructural \nUnderstanding\nIago\nBorn in\n1951\ndate_of_birth\ninstance of\nperson is a\nGeorgian artist\n(2) \nTraditional \nUnderstanding\nartist\nBorn in 1951 in Tbilisi, Iago is a Georgian\nPredicts an independent aspect \n(e.g., word(s) and label(s))Predicts a structure that integrates multiple relevant aspects\nFigure 2: Comparison between structural understanding and\ntraditional understanding of text.\nIn this paper, we improve the structural under-\nstanding capabilities of LMs. In contrast to previ-\nous approaches relying on task augmentations, we\nintroduce structure pretraining, which systemati-\ncally teaches LMs to better understand structures\nof text beyond independent aspects in a pretraining\nphase (Figure 1). This enables the zero-shot trans-\nfer of knowledge that LMs learned about structures\nduring our pretraining to downstream structure pre-\ndiction tasks. For example, our zero-shot 10B pa-\nrameter LM significantly outperforms the zero-shot\nGPT-3 (175B) on a structure prediction benchmark\ndataset (Figure 1). We accomplish this by reformu-\nlating structure prediction as a series of unit tasks–\ntriple prediction tasks. We then train LMs on a col-\nlection of task-agnostic structural corpora to gener-\nate triples from text. The design of triple represen-\ntation is important: it unifies a wide set of standard\nstructure prediction tasks into the same task format.\nWe apply our pretrained model DEEP STRUCT to\n28 datasets spanning 10 structure prediction tasks,\nincluding open information extraction, joint entity\nand relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event\nextraction, coreference resolution, factual probe,\nintent detection, and dialogue state tracking. We\nfurther enhance the pretraining with multiple down-\nstream structure prediction training sets and obtain\nstate-of-the-art performance on 21 of 28 datasets.\nOur contributions are as follows:\n• We improve structural understanding abilities of\npretrained LMs. Compared to traditional NLP\ntasks that only consider the understanding of an\nindependent aspect of the text, structural under-\nstanding takes a step further that requires the\nability to integrate multiple relevant aspects into\na structure. We argue that it is important for LMs\nto go beyond traditional understanding toward\nstructural understanding, as it requires a higher\nlevel of intelligent competence and is more chal-\nlenging. It can also benefit a wide spectrum of\nNLP tasks that require structure-level understand-\ning capability.\n• We propose structure pretraining, which pretrains\nthe LMs to understand structures in the text. The\nbasic intuition is that the standard pretraining\nhelps LMs to understand individual aspects of\nthe information in the text, our method learns to\nintegrate those individual aspects into structures.\nCompared to existing approaches, this method\nenables the zero-shot transfer of LMs to structure\nprediction tasks. For instance, our 10B LM pro-\nduces superior zero-shot performance compared\nto 175B GPT-3 on a representative structure pre-\ndiction task.\n• We further equip our pretraining with multi-task\nlearning and apply our method to 28 structure\nprediction datasets across 10 tasks. We achieve\nstate-of-the-art performance on 21 of 28 datasets\nthat we evaluate. We hope this can help facilitate\nthe structural understanding research in the NLP\ncommunity.\n2 Structure Pretraining\nPretrained \nLM\n(1) Task augmented pretrain-finetune\nTask augmentation Finetune on\ntask 0\nInference on\ntask 0\nPretrained \nLM\nStructure-pretrain on\ntask-agnostic tasks\nInference on\ntask 0,1,2,3,...\n(2) Structure pretraining (ours)\nRequires task-specific\narchitectures or data\naugmentation\nRequires task-specific\nexamples\nA task-specific model\nfor each task\nLM learns to generate\nstructural triples from text\nInference on\nmultiple tasks\nZero-shot\nPretrained \nLM\nStructure-pretrain on\ntask-agnostic tasks and\nmultiple tasks: 0,1,2,3,...\nInference on\ntask 0,1,2,3,...\nLM learns to generate structural\ntriples from text with additional\ntask-specific examples\nMulti-task\nInference on\nmultiple tasks\nFigure 3: Comparing structure pretraining with standard\npretrain-finetune paradigm.\nThe goal of our method is to improve the struc-\ntural understanding capabilities of language models\n(LMs), i.e., understanding the structures of text. As\nshown in Figure 3, instead of using the standard\npretrain-finetune paradigm for each task, we intro-\nduce structure pretraining that aims to teach LMs\nto correspond to structures in a wide spectrum of\ntasks at the same time. We evaluate the structural\nunderstanding ability on multiple structure predic-\ntion tasks.\n2.1 Generative Pretraining\nWhile the LM is pretrained to understand a single\naspect of the text, structural understanding aims to\nrecover the entire structure of the text (Figure 2).\nStructure pretraining is designed to bridge the gap\nvia guiding LMs to produce structures from the text.\nIt is ideal to generate arbitrary structures as needed.\nHowever, this is infeasible due to the highly com-\nplex nature of such structures.\nAs an alternative, we reformulate the structure\nprediction as a combination of triple generation\ntasks. We refer to a triple as (head entity; relation;\ntail entity) describing relations between entities.\nWe design three pretraining tasks with a focus on\npredicting the entities, relations, and triples respec-\ntively. As shown in Figure 1, (i) entity prediction\naims to output triples regarding the entities and\ntheir types in an input sentence. We implement\nthis via prepending “entity:” as a prefix in the\ninput. (ii) Relation prediction aims to recover the\nrelations and corresponding types in the input as a\ntriple. Similarly, we add “relation:” including\na task separator “:” to each input. (iii) Triple pre-\ndiction outputs the entire triple structure from the\ninput. We attach “triple:” to indicate this task.\nThese pretraining tasks are task-agnostic to down-\nstream tasks, enabling the zero-shot downstream\ntransfer (Sec. 2.3).\nAlthough the triple formulation is straightfor-\nward, we find that it is very flexible and able to\nmodel all structure prediction tasks we consider.\nA structure prediction task can be generally de-\ncomposed into generating the entities, relations,\nor triples. For example, named entity recognition\npredicts the entities and their types. It can be nat-\nurally represented as an entity prediction problem.\nBesides, traditional structure prediction tasks fo-\ncusing on relations (e.g., relation classification) or\ntriples (e.g., open information extraction) can be\nformulated as relation or triple prediction tasks re-\nDataset #Sent. #Rel.\n(#Tri.)Task\nT-REx (ElSahar et al., 2018) 6.2M 11.1M entity, relation\nTEKGEN (Agarwal et al., 2021) 7.9M 16M entity, relation\nKELM (Agarwal et al., 2021) 18M 45M entity, relation\nWebNLG (Gardent et al., 2017) 85K 261K relation\nConceptNet (Speer and Havasi, 2012) 610K 610K relation\nOPIEC (Gashteovski et al., 2019) 26.8M 104M triple\nTable 1: Pretraining dataset statistics and corresponding\npretraining tasks. #Sent. and #Rel. denote the number\nof sentences and relations respectively.\nDeepstruct \nIago is<s> born in 1951 (Iago;instanceof;entity: person)\nIago is <e>bornin 1951(Iago;instanceof;entity: person)\nFigure 4: Summary of training procedure.\nspectively. A summary of all downstream tasks is\ndescribed in Sec. 2.2.\nWe frame the pretraining as a conditional genera-\ntion task where the input corresponds to text x, and\nthe output y is a sequence of triples. Our pretrain-\ning can be expressed as estimating a conditional\ndistribution p(y|x) in a probabilistic framework.\nWe use an autoregressive LM to modelp(y|x).\nPretraining Data We train the model on a col-\nlection of task-agnostic corpora including pre-\nbuilt large-scale alignments between text and\ntriples. In particular, we use T-REx (ElSahar\net al., 2018), TEKGEN and KELM (Agarwal et al.,\n2021), WebNLG (Gardent et al., 2017), Concept-\nNet (Speer and Havasi, 2012). These corpora align\ntext to triples consisting of high-quality entities\nand relations in knowledge graphs (e.g., Wikidata),\nwhich are used for entity and relation prediction\ntasks. In addition, for triple prediction tasks, we\nuse OPIEC (Gashteovski et al., 2019) that provides\nopen schema triples. The pretraining data statistics\nand the corresponding pretraining tasks are shown\nin Table 1. Appendix A.1 shows additional details\nof our pretraining data.\nFigure 4 shows an example of the training pro-\ncedure for the entity prediction task based on the\ninput and output sample below.\nInput entity: Iago is born in 1951\nOutput (Iago; instance of; person)\nwhere the input text and output triple are aligned,\nand the alignment is provided by our pretraining\ndata. Tokens are predicted autoregressively start-\ning with <s> token and ending with <e> token.\nThe head entity (i.e., Iago) and the tail entity (i.e.,\nOpen information extraction\n(4 datasets)\noie  oie2016: Born in 1951 in\nTbilisi, Iago is a Georgian artist \nOIE2016WEB NYT PENN\ninput\n(Iago; Born in; 1951)\noutput\n(Iago; is a; Georgian artist) \nRelation classification\n(2 datasets)\nrc  tacred: Alice is Bob's\nmother. The relationship\nbetween Alice and Bob is \nTACRED\ninput\n(Alice; mother; Bob) \noutput\nFewRel 1.0\nFactual probe\n(2 datasets)\nfp  t-rex: Daniel, born in\n1970, is an Astralian author \nGoogle-RE\ninput\noutput\nT-REx\n(Daniel; date_of_birth; 1970) \nSemantic role labeling\n(3 datasets)\nsrl conll05: Scotty [accepted] the\ndecision with indifference and did not\nenter the arguments \nCoNLL05 WSJ\ninput\noutput\n(Scotty; instance of; subject) \nCoNLL05 BrownCoNLL12\n(decision; instance of; object) \nEvent extraction\n(4 datasets)\nee ace2005: Barack Obama won the 44th\nPresident of the United States \nACE2005 Trigger Id\ninput\noutput\n(Obama; instance of; president)\nJoint entity and relation extraction\n(4 datasets)\nCoNLL04ADE NYT ACE2005\nCoreference resolution\n(1 datasets)\nCoNLL12\nDialogue state tracking\n(1 datasets)\nMultiWOZ 2.1\nIntent detection\n(2 datasets)\nATIS SNIPS\nNamed entity recognition\n(4 datasets)\nCoNLL03OntoNotes GENIAACE2005\ncr conll12: Deterrents\ndoesn't work terribly well\nwhen an enemy values your\ndeath more than his life \ninput\noutput\n(an enemy; refer to; his) \njer ade: The Davao Medical Center\nis a regional government hospital \ninput\noutput\n(hospital; part of; government) \n(Davao Medical Center; instance of; hospital)\ndst multiwoz: [User]: I am looking\nfor a place to to stay that is cheap\nin a type of hotel. [Agent]: Okay \ninput\noutput\n([User]; hotel price range; cheap)\n([User]; hotel type; hotel)\nner genia: Japan began the\ndefence of their Asian Cup title \ninput\noutput\n(Japan; instance of; location)\n(Asian Cup; instance of; race)\nid atis: Play the song\nlittle robin redbreast \ninput\noutput\n(intent; is; play music)\nACE2005 Trigger CI\nACE2005 Argument Id\nACE2005 Argument CI\nFigure 5: Summary of tasks and datasets. Blue: entity prediction task; Red: relation prediction task; Purple: entity and relation\nprediction task; Yellow: triple prediction task.\nperson) of the output triple then serve as the predic-\ntions (i.e., entity mention and entity type) of named\nentity recognition.\n2.2 Tasks\nIt is resource-intensive to create large-scale struc-\ntural understanding datasets from scratch. There-\nfore, we collect existing datasets in the field of\nstructure prediction for evaluation. We consider 28\ndatasets spanning 10 structure prediction tasks as\nshown in Figure 5. Detailed descriptions and sizes\nof datasets are shown in Appendix A.\n2.3 Zero-Shot\nThe zero-shot DEEP STRUCT refers to the setting\nwhere the pretrained model is used without any\ntask-specific training at inference time. This differs\nfrom prior fully supervised methods. This setting\nis challenging as it might be difficult for humans to\nunderstand the tasks without prior examples. For\nexample, if we are asked about “semantic role la-\nbeling” that aims to recover the predicate-argument\nstructure, it is hard to understand what this really\nmeans. Nevertheless, the existing zero-shot setting\nresonates with human behaviors. For example, for\nnamed entity recognition, a human can understand\nand follow the instruction.\nWe enable the zero-shot transfer to structure\nprediction tasks via converting the downstream\ntasks to one or a combination of the pretraining\ntasks. As shown in Figure 5, at inference time,\nseven structure prediction tasks are formulated as\nentity prediction with the prefix “ entity:” at-\ntached to the input example (in blue), while one\ntask is cast as relation prediction with the prefix\n“relation:” (in red). In addition, open infor-\nmation extraction is a triple prediction task with\nthe prefix “ triple:” (in yellow). Joint entity\nand relation extraction (JER) is a combination of\nentity and relation prediction (in purple). For the\nentity and relation prediction, we use the prefix\n“entity:” and “relation:” respectively. Be-\nsides, for each dataset, we build a schema align-\nment between the pretraining dataset and down-\nstream dataset (details are described in Sec. 5). The\noutput triples are then decoded as corresponding\nstructure predictions based on the pre-built schema\nalignment.\n2.4 Multi-Task\nHowever, the distribution of the pretraining data is\nnot perfectly aligned with the distribution of down-\nstream datasets. This results in a shift in the output\ndistribution of the pretrained model. The zero-shot\nsetting cannot perform at its best on several out-of-\ndistribution tasks including dialogue state tracking.\nThe reason is that its desired output is a dialogue\nstate, which is lacking in our task-agnostic pretrain-\ning corpora. To mitigate this, we integrate multiple\nstructure prediction datasets into the pretraining\ncorpora, and train our method on the mixture of\nthe datasets. We list an example input and output\nformat for each task in Figure 5. For all datasets\nof a particular task, we adopt the same input and\noutput format. We also add task name and dataset\nname followed by the separator “:” as a prefix to\neach input example. For example, we add “ jer\nade:” to indicate one of the JER datasets, ADE.\nMore examples of each task and dataset are shown\nin Table 16. In contrast to fully pretrain-finetuned\nmodels that store a copy of parameters for each\ntask, this setting is a lightweight alternative and\nproduces a single model for all tasks, improving\nparameter sharing.\nAfter multi-task training, we further finetune our\nmethod on the task-specific dataset for each task.\nThe intuition is that finetuning is the de facto way\nto leverage pretrained LMs to perform downstream\ntasks. We aim to test an upper bound of the trans-\nfer performance of our structure pretraining via the\nadditional finetuning phase. For both multi-task set-\ntings, we use the same data format with the training\nat test time. Basically, we add the task name and\ndataset name followed by the separator to the input\nexample.\n3 Experiments\nIn this section, we show that DEEP STRUCT suc-\ncessfully transfers to the structure prediction tasks\nconsidered and obtain state-of-the-art results on 21\nof 28 datasets we evaluate. All results are obtained\nvia structure pretraining a pretrained 10B param-\neter LM, GLM (Du et al., 2021). The details of\nthe experimental setup, datasets, and comparison\nmethods are described in Appendix A.\n3.1 Main Results\nWe have two settings as described in Sec. 2: zero-\nshot and multi-task. We also finetune the multi-task\nversion on each downstream dataset. In total, we\nhave three versions of DEEP STRUCT . For compari-\nson: we report the performance of TANL (Paolini\net al., 2021) when available. We also show the best\nperformance among the task-specific models that\nare described in Appendix A. Table 2 reports the\nresults.\nWith the zero-shot setting, a single model is\nused to perform on multiple tasks without the need\nof any task-specific training. This is in contrast\nto previous approaches that rely on task-specific\nmodels and datasets for each task. In Table 3, we\nalso report the zero-shot performance of GPT-3\n175B (Brown et al., 2020) on CoNLL04 and ADE\n(JER) via formulating the task as a question an-\nswering problem through prompting (details of\nthe formulation are described in Sec. 5). JER re-\nquires the model to extract a set of entities and\na set of relations between pairs of entities from\nthe input text. Each predicted entity or relation\nhas to be also assigned to an entity or a relation\ntype. Zero-shot DEEP STRUCT 10B outperforms\nzero-shot GPT-3 175B by a large margin without\nany prompt engineering. As shown in Table 2,\noverall, DEEP STRUCT ’s zero-shot performance is\nstill far from that of task-specific supervised mod-\nels on most tasks. The only exception is that the\nzero-shot setting obtains the new state-of-the-art\nperformance on the factual probe with averaging\n20% P@1 improvement. This is because the task-\nspecific method is also zero-shot. Note that we\nhave removed the overlapped sentences with the\nT-REx test sets (factual probe) from our pretrain-\ning data. The result indicates that the structure\npretraining is able to adapt the LM knowledge to\nthe tasks by making LM aware of symbolic knowl-\nedge in the pretraining corpora. Besides, the zero-\nshot approach generalizes well to all task-agnostic\npretraining tasks including entity prediction (e.g.,\nnamed entity recognition), relation prediction (e.g.,\nrelation classification), and triple prediction (e.g.,\nopen information extraction).\nSimilar to the zero-shot setup, we only train a\nsingle model to conduct all the downstream tasks\nunder the multi-task setting. This is different from\nthe supervised models that use task-specific models\nand parameters. We achieve state-of-the-art perfor-\nmance on three datasets. For the other datasets,\nwe obtain a competitive performance within a few\npoints of the best-compared methods. Notably,\nmost structure prediction tasks show a large gain\nfrom zero-shot to multi-task. This suggests that\nmost tasks are out-of-distribution of our structure\npretrained model. Nevertheless, our method ap-\npears to be able to adapt to the downstream distribu-\ntions, presenting a fair and strong performance with\nmulti-task learning. Another explanation is that\nmulti-task examples help the model better under-\nstand the downstream tasks, such as the output for-\nmat of each task. We also observe strong multi-task\nperformance on FewRel, which is a low-resource\nstructure prediction benchmark. This suggests that\nthe multi-task setting is beneficial in low-resource\nregimes via transferring knowledge from similar\ntasks. For our multi-task training, we leave out the\nACE2005 named entity recognition dataset due to\nthe overlap between train and test splits for differ-\nent tasks. After finetuning, we obtain state-of-the-\nart performance on 21 datasets. For instance, we\nobtain a +8.0 absolute improvement and a +2.9 ab-\nsolute improvement on CoNLL05 Brown (semantic\nrole labeling) and TACRED (relation classification)\ncompared to the state-of-the-art methods.\nAll above results are based on a pretrained 10B\nparameter LM, GLM. GLM is an autoregressive\nLM. In addition, the context x is encoded by a\nTask Dataset Metric Task-specific model TANL\nDEEPSTRUCT\nzero-shot multi-task\nw/ finetune\nOpen\ninformation\nextraction\nOIE2016\nF1\n67.0(Stanovsky et al., 2018) - 28.1 71.2 71.3\nWEB 58.9(Stanovsky et al., 2016) - 43.8 50.8 49.1\nNYT 38.3(Saha and Mausam, 2018) - 28.9 43.6 45.0\nPENN 42.6(OpenIE43) - 51.0 54.5 45.1\nRelation\nclassification\nTACRED\nF1\n73.9(Sainz et al., 2021) 71.9 36.1 74.9 76.8\nFewRel 1.0\n5-way 1-shot 90.1(Soares et al., 2019) 93.6±5.4 72.4±6.9 93.6±6.0 98.4±2.8\n5-way 5-shot 89.5(Gao et al., 2019) 97.6±3.2 70.8±8.0 96.4±4.2 100.0±0.0\n10-way 1-shot 83.4(Soares et al., 2019) 82.2±5.1 67.6±4.5 92.2±6.4 97.8±2.0\n10-way 5-shot 81.8(Gao et al., 2019) 89.8±3.6 66.4±6.3 94.6±3.6 99.8±0.6\nJoint entity\nand relation\nextraction\nCoNLL04\nF1\n(Ent.\nRel. )\n88.9(Zhao et al., 2020) 90.3 48.3 87.4 90.7\n71.9 71.4 25.8 69.6 78.3\nADE 89.3(Eberts and Ulges, 2020) 91.2 60.7 90.2 91.1\n78.8 83.8 10.6 83.7 83.8\nNYT - (Yuan et al., 2020) 94.9 60.5 95.4 95.9\n84.6 90.8 28.6 93.9 93.3\nACE2005 88.4(Luan et al., 2019) 88.9 31.8 87.8 90.0\n63.2 63.7 5.3 54.0 66.8\nEvent\nextraction ACE2005\nTrigger Id\nF1\n72.5(Nguyen and Nguyen, 2019) 72.9 - 71.7 73.5\nTrigger Cl 69.8(Nguyen and Nguyen, 2019) 68.5 - 67.9 69.8\nArgument Id 59.9(Nguyen and Nguyen, 2019) 50.1 - 54.9 59.4\nArgument Cl 52.5(Wadden et al., 2019) 48.5 - 52.7 56.2\nCoreference\nresolution CoNLL12\nMUC 86.3\n(Wu et al., 2020)\n81.0 - 63.9 74.9\nB3 77.6 69.0 - 57.7 71.3\nCEAFϕ4 75.8 68.4 - 60.2 73.1\nAve. F1 79.9 72.8 - 60.6 73.1\nIntent\ndetection\nATIS F1 97.8(E et al., 2019) 97.6 - 66.6 97.8\nSNIPS 97.4 98.7 - 78.4 97.3\nSemantic\nrole\nlabeling\nCoNLL05 WSJ\nF1\n88.8\n(Shi and Lin, 2019)\n89.3 - 95.6 95.2\nCoNLL05 Brown 82.0 84.1 - 92.0 92.1\nCoNLL12 86.5 87.7 - 97.6 96.0\nNamed\nentity\nrecognition\nCoNLL03\nF1\n93.5(Yu et al., 2020b) 91.7 44.4 93.1 93.0\nOntoNotes 90.4(Yan et al., 2021) 89.9 2.5 87.6 87.8\nGENIA 80.5 (Yu et al., 2020b) 76.4 47.2 80.2 80.8\nACE2005 86.9(Li et al., 2020a) 84.9 28.1 - 86.9\nDialogue state\ntracking MultiWOZ 2.1 Joint\nAcc. 55.7(Hosseini-Asl et al., 2020) 51.4 - 53.5 54.2\nFactual probeGoogle-RE P@1 78.0(Petroni et al., 2020) - 97.9 90.3 -\nT-REx 62.6 - 85.0 71.0 -\nTable 2: Results on all tasks. All evaluation scores are higher the better. TANL is introduced in (Paolini et al., 2021).\nThe bold denotes the best, and the underline indicates the second best. Detailed results are included in Appendix A.\nModel CoNLL04 ADE\nEnt. Rel. Ent. Rel.\nGPT-3 175B zero-shot 34.7 18.1 5.8 1.3\nzero-shot 48.3 25.8 60.7 10.6\nDEEP STRUCT multi-task 87.4 69.6 90.2 83.7\nw/ finetune 90.7 78.3 91.1 83.8\nTable 3: Compare DEEP STRUCT to GPT-3 (Brown et al.,\n2020) 175B zero-shot on CoNLL04 and ADE datasets\n(joint entity and relation extraction). Ent. and Rel.\ndenote entity F1 and relation F1 respectively.\nbidirectional encoder. In principle, generative LMs,\nsuch as T5 (Raffel et al., 2019), BART (Lewis et al.,\n2020) and GPT-3 (Brown et al., 2020), can also be\nused with the proposed structure pretraining for the\nstructure prediction tasks as well. We leave this as\none of the future investigations.\n3.2 Ablation Studies\nPretraining Strategies As the key question of\nour work is to investigate how structure pretrain-\ning improves the structural understanding ability of\nLMs, we examine how different pretraining strate-\ngies impact the downstream performance. We eval-\nuate the below settings on the CoNLL04 (JER). The\nfirst two settings examine the relative importance of\nthe pretraining data: (i) With example-proportional\nmixing: We follow (Raffel et al., 2019) with a mix-\ning rate maximum of 10K to balance the different\nsizes of datasets. All other components are kept the\nsame with DEEP STRUCT multi-task with finetun-\ning. (ii) With entity and relation augmentation: We\nadd special tokens “[]” to indicate the positions\nof the entities and relations in a sentence. Addi-\ntional details are shown in Appendix A.5. (iii) No\npretrain, finetune: We remove structure pretrain-\ning, and only finetune the LM on CoNLL04. (iv)\nZero-shot: We only use the task-agnostic datasets\nand exclude the multi-task datasets in the pretrain-\ning. (v) Multi-task: We use the multi-task model\nwithout finetuning. (iv) and (v) are the same with\nthe zero-shot and multi-task settings in Sec. 2. (vi)\nFinetune: The multiple downstream datasets are\nexcluded in the structure pretraining, but the model\nis finetuned on CoNLL04.\nTable 4 shows the results. First, the distribution\nof pretraining data does not significantly shift from\nthat of most tasks. This limits the impact of the bal-\nanced strategy (i). The data augmentation (ii) does\nnot bring additional benefits to the downstream per-\nformance. This confirms that the key to the success\nof structure prediction is our formulation that nar-\nrows down a complex structure to a set of triple\nprediction tasks. This allows the pretraining to cap-\nture the entities and relations that are important\nfor tasks. Second, removing the structure pretrain-\ning (iii) provides the most direct ablation of how\nmuch structure pretraining helps. Structure pre-\ntraining significantly improves the LM in structure\nprediction. This is due to the gap between LM pre-\ntraining and downstream structural understanding.\nFor example, the distribution of structure predic-\ntion datasets is different from or is considered as\nout-of-distribution for the pretraining data. Struc-\nture pretraining improves the adaptation to those\ndatasets. Next, similar to the findings in Table 2,\nwe find that both task-agnostic training sets (iv) and\nmulti-tasks datasets (v) contribute to the strength\nof structure pretraining. In particular, finetuning is\nstill very important to improve the downstream per-\nformance (IV et al., 2021). However, it produces a\ntask-specific model for each dataset instead of a uni-\nfied model for all tasks as in our zero-shot or multi-\ntask setup. Compared to only finetuning the model\non a downstream dataset (vi), the multi-task setting\nobtains sizable improvements. This is because if\nthe downstream dataset sizes are small such as of\nCoNLL04, multi-task learning can be extremely\nhelpful in the low-resource regimes (Paolini et al.,\n2021). We conduct the above ablation studies us-\ning a base version of DEEP STRUCT with 220M\nparameters.\nScaling Laws As it is often the case that larger\nmodels substantially improve the transferring ca-\npabilities of LMs (Brown et al., 2020; Wei et al.,\n2021), we explore how model scaling benefits the\nstructure pretraining. We evaluate the effect on\nmodels with 110M, 220M, 2B, 10B parameters on\nMethod Ent. Rel.\nDEEP STRUCT 220M multi-task finetune 90.7 75.7\nWith example-proportional mixing 88.0 73.1\nWith entity and relation augmentation 88.6 74.9\nNo pretrain 220M, finetune 84.7 63.5\nDEEP STRUCT 220M zero-shot 51.5 22.9\nDEEP STRUCT 220M multi-task 76.9 55.2\nDEEP STRUCT 220M finetune 87.4 70.4\nTable 4: Ablation over different facets of structure pretraining\non CoNLL04 test set (joint entity and relation extraction). Ent.\nand Rel. indicate entity F1 and relation F1 respectively.\nJER datasets with multi-task and multi-task fine-\ntuned DEEP STRUCT (Figure 6).\nAs expected, average performance across the\ndatasets improves as models grow larger. We find\nthat when the models reach the order of 10B pa-\nrameters, structure pretraining obtains the best per-\nformance. The 10B parameter model significantly\nimproves the results compared to the 110M param-\neter model. One reason is that for small-scale mod-\nels, learning across 28 structure prediction datasets\nduring the structure pretraining may exceed the\nmodel capacity. For larger models, structure pre-\ntraining fully utilizes the model capacity and also\nteaches the models to generate triples according\nto the downstream tasks, allowing them to gener-\nalize well to most tasks with the rest capacity. It\nis also interesting that the performance does not\nseem to significantly saturate, indicating that the\nperformance may further improve with larger-scale\nmodels. Under both setups, we observe similar\ntrends. We also see that the model size matters\nmore to the multi-task setting than to the finetuned\nversion, suggesting finetuning is able to help specif-\nically adapt to a task given a limited model size.\nThe main pitfall is its generalization to more tasks.\n4 Related Work\nPretrained LMs (Devlin et al., 2019; Radford et al.,\n2019b; Yang et al., 2019) are the key ingredi-\nents in contemporary NLP. Sequence-to-sequence\n(seq2seq) LMs target conditional generation, such\nas T5 (Raffel et al., 2019), BART (Lewis et al.,\n2020) and GLM (Du et al., 2021). These models\nhave benefited a wide range of nature language gen-\neration tasks such as summarization (Zhang et al.,\n2020) and text infilling (Zhu et al., 2019; Shen\net al., 2020). Recent attempts of generative pre-\ndiction (Paolini et al., 2021; Schick and Schütze,\n2021; Lester et al., 2021) have found that seq2seq\nmodels are able to provide a unified solution for\nmodeling a wide set of NLP tasks. While existing\n/uni00000014/uni00000014/uni00000013/uni00000030/uni00000015/uni00000015/uni00000013/uni00000030/uni00000015/uni00000025/uni00000014/uni00000013/uni00000025\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000006/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni0000000c\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013\n/uni0000001b/uni00000018\n/uni0000001c/uni00000013\n/uni0000001c/uni00000018\n/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003Entity/uni00000003/uni00000029/uni00000014/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002d/uni00000028/uni00000035/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni0000001b/uni0000001a/uni00000011/uni00000014\n/uni0000001c/uni00000013/uni00000011/uni00000017\n/uni00000019/uni0000001a/uni00000011/uni00000015\n/uni0000001b/uni00000018/uni00000011/uni00000018\n/uni0000001b/uni0000001c/uni00000011/uni00000018\n/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000003/uni00000050/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000057/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051e/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000003/uni00000050/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000010/uni00000057/uni00000044/uni00000056/uni0000004e\n/uni00000014/uni00000014/uni00000013/uni00000030/uni00000015/uni00000015/uni00000013/uni00000030/uni00000015/uni00000025/uni00000014/uni00000013/uni00000025\n/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000006/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni0000000c\n/uni00000017/uni00000013\n/uni00000017/uni00000018\n/uni00000018/uni00000013\n/uni00000018/uni00000018\n/uni00000019/uni00000013\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013\n/uni0000001b/uni00000018\n/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000029/uni00000014/uni00000003/uni00000052/uni00000051/uni00000003/uni0000002d/uni00000028/uni00000035/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni0000001a/uni00000013/uni00000011/uni00000016\n/uni0000001a/uni0000001a/uni00000011/uni00000018\n/uni0000001a/uni0000001c/uni00000011/uni0000001a/uni0000001b/uni00000013/uni00000011/uni00000019\n/uni00000017/uni00000016/uni00000011/uni00000018\n/uni00000019/uni00000013/uni00000011/uni00000013\n/uni0000001a/uni00000016/uni00000011/uni0000001c/uni0000001a/uni00000018/uni00000011/uni00000016\n/uni0000001c/uni00000014/uni00000011/uni0000001a\n/uni0000001c/uni00000013/uni00000011/uni00000015\n/uni0000001c/uni00000014/uni00000011/uni0000001c\n/uni0000001a/uni0000001a/uni00000011/uni00000018\n/uni0000001a/uni0000001c/uni00000011/uni0000001a/uni0000001b/uni00000013/uni00000011/uni00000019\n/uni0000001a/uni00000018/uni00000011/uni00000016\nFigure 6: Model scaling results on joint entity and relation extraction (JER) datasets. Left: entity F1; Right: relation F1.\napproaches focus on text-to-text generation, DEEP -\nSTRUCT aims to perform text-to-triple generation.\nMulti-task learning (Caruana, 1997) aims to train\na model for multiple tasks simultaneously. In deep\nlearning, it is usually categorized into hard weight\nsharing and soft weight constraints (Ruder, 2017).\nIn the context of NLP, weight sharing has been\nadopted in (Collobert and Weston, 2008; Yang\net al., 2016; Liu et al., 2020). Since the emergence\nof large pretrained LMs (Radford et al., 2019a;\nDevlin et al., 2019; Yang et al., 2019), multi-task\ntraining has been shown effective to enhance LMs’\ntransferability to downstream tasks (Raffel et al.,\n2019). Recent studies (Wei et al., 2021) also show\nthat pretrained models finetuned with abundant\ndownstream tasks can conduct effective zero-shot\nlearning. The main difference is that DEEP STRUCT\ntrains across multiple structure prediction datasets\nin structure pretraining with task-agnostic corpora,\nwhere we cast all datasets into triple formats.\nStructure prediction is a long-standing challenge\nthat relates to many NLP applications such as open\ninformation extraction (Gashteovski et al., 2019),\nnamed entity recognition (Sang and Meulder, 2003;\nWeischedel et al., 2013), and relation classifica-\ntion (Zhang et al., 2017; Han et al., 2018; Gao\net al., 2019). To handle different structure pre-\ndiction problems, prior work presents a variety of\ntask-specific models in the form of sequence tag-\nging (Stanovsky et al., 2018; Li et al., 2019), ma-\nchine reading comprehension (Zhao et al., 2020)\nand text classification (Soares et al., 2019), which\nhinders the knowledge transfer across different\ntasks. TANL (Paolini et al., 2021) proposes a\ntranslation-based approach to unify different struc-\nture prediction tasks with task-specific data aug-\nmentation. By contrast, our DEEP STRUCT unifies\nmore structure prediction tasks via a single model\nand a uniform data format.\n5 Discussion\nRelated Models Recent studies have provided\nunified solutions for structural prediction tasks.\nWe focus on the comparison between our DEEP -\nSTRUCT to the state-of-the-art TANL (Paolini\net al., 2021) and DeepEx (Wang et al., 2021).\nTANL (Paolini et al., 2021) proposes task-specific\ndata augmentation (i.e., augmented natural lan-\nguage) that annotates task information and predic-\ntions in the input and output respectively for each\nstructure prediction task. The main difference is\nthat DEEP STRUCT decomposes the structure pre-\ndiction tasks into a collection of triple generation\ntasks. The triple format serves as the unified rep-\nresentation for all considered structure prediction\ntasks without the need of introducing new data aug-\nmentation as in TANL. While TANL mainly works\nin the multi-task setting, we additionally enable the\nzero-shot transfer via the task-agnostic structure\npretraining. DeepEx (Wang et al., 2021) explores\nthe attention matrices of pretrained LMs via beam\nsearch to generate triples for information extraction\ntasks. Following the search, DeepEx introduces an\nextra ranking stage to improve the quality of the\ntriples. Differently, DEEP STRUCT aims to generate\nthe triples for a wide set structure prediction tasks\nin an end-to-end fashion thanks to the proposed\nstructure pretraining.\nBesides, both TANL and DeepEx explore rela-\ntively small-scale pretrained LMs. Instead, DEEP -\nSTRUCT scales up to 10 billion parameters. Fig-\nure 6 shows that the performance improvements\nfollow the scaling law (Raffel et al., 2019; Lester\net al., 2021; Wei et al., 2021; Sanh et al., 2021; Liu\nquery entity: \n(Iago; instance of; person)\nBorn in 1951 in Tbilisi , Iago is a Georgian artist .\nQ: Does the entity \"Iago\" in the above sentence\nbelongs to type \"person\"?\nA: Yes.\nQ: Does the relation between \"Iago\" and \"Tbilisi\" in\nthe above sentence  belongs to type \"lives in\"?\nA: Yes.\nquery relation: \n(Iago; lives in; Tbilisi)\npredict entity: \nperson\npredict relation: \nlives in\nFigure 7: An example of GPT-3 zero-shot setting. To predict\nentities, we convert the gold entity triple (Iago; instance of;\nperson) to an entity based true-or-false question. Similarly, to\npredict relations, the gold relation triple (Iago; lives in; Tbilisi)\nis turned into a relation based true-or-false question. The task\npredictions are correct if the answers are “yes”.\net al., 2021). Based on our results, DEEP STRUCT\ngeneralizes better to more structure prediction tasks\ncompared to TANL and DeepEx.\nZero-Shot Setup For our zero-shot setup, we fol-\nlow the zero-shot usage in recent pretrained LM\nstudies (Brown et al., 2020; Wei et al., 2021; Sanh\net al., 2021). It refers to the setting where a pre-\ntrained model is used to directly perform down-\nstream tasks without including downstream train-\ning sets in its own pretraining data. For DEEP -\nSTRUCT , our pretraining data is task-agnostic. For\neach task, we build an offline alignment between\nthe schema of the pretraining data and the task\ndataset based on co-occurrence information in the\npretraining data and downstream data (Angeli et al.,\n2015). We then manually curate the alignment.\nThe resulting schema alignment is part of our re-\nlease 1. At test time, we convert each task to one\nor a combination of the pretraining tasks based on\nFigure 5: entity, relation, or triple prediction. After\nproducing the triples, we use the pre-built schema\nalignment to obtain the task predictions.\nFor GPT-3 zero-shot setting, we follow the\nprompting method introduced by GPT-3 (Brown\net al., 2020). In more details, we aim to test the\nupper bound performance of GPT-3 for structure\nprediction, in particular the JER task. Therefore, in-\nstead of using standard prompts in the form of ques-\ntion answering, we design the prompts for “true-\nor-false” questions based on the ground truth. In\nthis case, GPT-3 only answers with “yes” or “no”\nto produce a task prediction (Figure 7), which is\napparently an easier task compared to generating\nthe structures from scratch.\n6 Conclusion\nWe improve structural understanding capabilities\nof language models. We evaluate it on a wide set\nof structure prediction tasks including 10 tasks and\n28 datasets, and successfully transfer pretrained\nlanguage models to them through the proposed\nstructure pretraining, which teaches language mod-\nels to output triples from the text. We enable both\nzero-shot and multi-task transfer learning. DEEP -\nSTRUCT obtains state-of-the-art results on 21 of 28\ndatasets. The result shows that pretrained language\nmodels can handle higher-level understanding (e.g.,\nstructural understanding), which may benefit more\nNLP tasks. We hope it will foster future research\nalong the language structural understanding direc-\ntion.\n7 Ethical Considerations\nWe hereby acknowledge that all of the co-authors\nof this work are aware of the provided ACM Code\nof Ethics and honor the code of conduct. This\nwork is mainly about the pretraining and multi-\ntask learning of LMs for structural prediction. The\nfollowings give the aspects of both our ethical con-\nsiderations and our potential impacts to the com-\nmunity. This work uses LMs, for which the risks\nand potential harms are discussed in (Brown et al.,\n2020). There are potential undesirable biases that\nexisted in task-agnostic data (e.g., from Wikipedia)\nand multi-task downstream datasets (mostly cre-\nated from news articles). We do not anticipate the\nproduction of harmful outputs, especially towards\nvulnerable populations, after using our model or\ntraining NLP models on our datasets.\n8 Environmental Considerations\nWe adopt the pretrained LMs from the (Du et al.,\n2021), whose energy cost and carbon footprint dur-\ning pretraining were 80.6 MWh and 4.6 tCO2e,\nrespectively. Additionally, the structure pretraining\ntakes less than 5% gradient-steps of the number of\npretraining steps of LMs, and thus the estimated\nauxiliary cost for energy is comparatively smaller.\nIn addition, training and tuning pretrained LMs\non a wide range of tasks and datasets consume a\nplenitude of energy and increase emissions of car-\nbon dioxide. To alleviate the problem, in this work\nwe make efforts to study the multi-task training,\nwhich only involves training on a combination of\nall datasets at once. Our results (e.g., Figure 6)\nshow that, despite the gap between multi-task and\nmulti-task finetune on smaller models, the perfor-\nmance gap becomes minor when the model size\nscales up to 10 billion parameters. This indicates\nthat we can reduce energy consumption when train-\ning large pretrained models via employing multi-\ntask training.\nAcknowledgement\nWe would like to thank the anonymous reviewers\nfor their suggestions and comments. This material\nis in part based upon work supported by Berkeley\nDeepDrive and Berkeley Artificial Intelligence Re-\nsearch. Xiao Liu, Zui Chen, Haoyun Hong, and Jie\nTang are supported by the NSFC for Distinguished\nYoung Scholar (61825602) and NSFC (61836013).\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In NAACL-HLT, pages 3554–\n3565.\nGabor Angeli, Melvin Jose Johnson Premkumar, and\nChristopher D. Manning. 2015. Leveraging linguistic\nstructure for open domain information extraction. In\nACL, pages 344–354.\nBen Athiwaratkun, Cícero Nogueira dos Santos, Jason\nKrone, and Bing Xiang. 2020. Augmented natu-\nral language for generative sequence labeling. In\nEMNLP, pages 375–385.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. In EMNLP-\nIJCNLP, pages 5359–5368.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz - A\nlarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling. In EMNLP, pages\n5016–5026.\nXavier Carreras and Lluís Màrquez. 2005. Introduction\nto the conll-2005 shared task: Semantic role labeling.\nIn CoNLL, pages 152–164.\nRich Caruana. 1997. Multitask learning. Mach. Learn.,\npages 41–75.\nJun Chen, Robert Hoehndorf, Mohamed Elhoseiny, and\nXiangliang Zhang. 2020. Efficient long-distance re-\nlation extraction with dg-spanbert. CoRR.\nJanara Christensen, Stephen Soderland, and Oren Et-\nzioni. 2011. An analysis of open information extrac-\ntion based on semantic role labeling. In Proceedings\nof the sixth international conference on Knowledge\ncapture, pages 113–120.\nRonan Collobert and Jason Weston. 2008. A unified\narchitecture for natural language processing: deep\nneural networks with multitask learning. In ICML.\nLuciano Del Corro and Rainer Gemulla. 2013. Clausie:\nclause-based open information extraction. In WWW,\npages 355–366.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calt-\nagirone, Thibaut Lavril, Maël Primet, and Joseph\nDureau. 2018. Snips voice platform: an embedded\nspoken language understanding system for private-\nby-design voice interfaces. CoRR.\nFilipe de Sá Mesquita, Jordan Schmidek, and Denilson\nBarbosa. 2013. Effectiveness and efficiency of open\nrelation extraction. In EMNLP-ACL, pages 447–457.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. All\nNLP tasks are generation tasks: A general pretraining\nframework. CoRR.\nHaihong E, Peiqing Niu, Zhongfu Chen, and Meina\nSong. 2019. A novel bi-directional interrelated\nmodel for joint intent detection and slot filling. In\nACL, pages 5467–5471.\nMarkus Eberts and Adrian Ulges. 2020. Span-based\njoint entity and relation extraction with transformer\npre-training. In ECAI-PAIS, pages 2006–2013.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Frédérique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In LREC-ELRA.\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyang Gao, Adarsh Kumar,\nAnuj Kumar Goyal, Peter Ku, and Dilek Hakkani-Tür.\n2020. Multiwoz 2.1: A consolidated multi-domain\ndialogue dataset with state corrections and state track-\ning baselines. In LREC, pages 422–428.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2019. Fewrel 2.0:\nTowards more challenging few-shot relation classifi-\ncation. In EMNLP-IJCNLP, pages 6249–6254.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The webnlg chal-\nlenge: Generating text from RDF data. In INLG,\npages 124–133.\nKiril Gashteovski, Sebastian Wanner, Sven Hertling,\nSamuel Broscheit, and Rainer Gemulla. 2019.\nOPIEC: an open information extraction corpus.\nCoRR.\nPankaj Gupta, Hinrich Schütze, and Bernt Andrassy.\n2016. Table filling multi-task recurrent neural net-\nwork for joint entity and relation extraction. In COL-\nING, pages 2537–2547.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius, and\nLuca Toldo. 2012. Development of a benchmark\ncorpus to support the automatic extraction of drug-\nrelated adverse effects from medical case reports. J.\nBiomed. Informatics, pages 885–892.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,\nZhiyuan Liu, and Maosong Sun. 2018. Fewrel: A\nlarge-scale supervised few-shot relation classification\ndataset with state-of-the-art evaluation. In EMNLP,\npages 4803–4809.\nLuheng He, Mike Lewis, and Luke Zettlemoyer. 2015.\nQuestion-answer driven semantic role labeling: Us-\ning natural language to annotate natural language. In\nEMNLP, pages 643–653.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, USA, June 24-27, 1990.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A sim-\nple language model for task-oriented dialogue. In\nNeurIPS.\nRobert L. Logan IV , Ivana Balazevic, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2021. Cutting down on prompts and parameters:\nSimple few-shot learning with language models.\nCoRR.\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and\nDaniel S. Weld. 2019. BERT for coreference res-\nolution: Baselines and analysis. In EMNLP-IJCNLP,\npages 5802–5807.\nPaul Kingsbury and Martha Palmer. 2003. Propbank:\nthe next level of treebank. In Proceedings of Tree-\nbanks and lexical Theories, volume 3. Citeseer.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nfine inference. In NAACL-HLT, pages 687–692.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP, pages 3045–3059.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In ACL, pages 7871–7880.\nXiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong\nHan, Fei Wu, and Jiwei Li. 2020a. A unified MRC\nframework for named entity recognition. In ACL,\npages 5849–5859.\nXiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang,\nFei Wu, and Jiwei Li. 2020b. Dice loss for data-\nimbalanced NLP tasks. In ACL, pages 465–476.\nZuchao Li, Shexia He, Hai Zhao, Yiqing Zhang, Zhu-\nosheng Zhang, Xi Zhou, and Xiang Zhou. 2019. De-\npendency or span, end-to-end uniform semantic role\nlabeling. In AAAI, pages 6730–6737.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR.\nXiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng,\nXueyun Zhu, Emmanuel Awa, Pengcheng He,\nWeizhu Chen, Hoifung Poon, Guihong Cao, and Jian-\nfeng Gao. 2020. The microsoft toolkit of multi-task\ndeep neural networks for natural language under-\nstanding. In ACL, pages 118–126.\nYi Luan, Dave Wadden, Luheng He, Amy Shah, Mari\nOstendorf, and Hannaneh Hajishirzi. 2019. A general\nframework for information extraction using dynamic\nspan graphs. In NAACL-HLT, pages 3036–3046.\nNafise Sadat Moosavi and Michael Strube. 2016. Which\ncoreference evaluation metric do you trust? a pro-\nposal for a link-based entity aware metric. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 632–642.\nTrung Minh Nguyen and Thien Huu Nguyen. 2019. One\nfor all: Neural joint modeling of entities and events.\nIn AAAI, pages 6851–6858.\nTomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.\nThe genia corpus: An annotated research abstract\ncorpus in molecular biology domain. In HLT, page\n82–86.\nHarinder Pal et al. 2016. Demonyms and compound\nrelational nouns in nominal open ie. In Proceedings\nof the 5th Workshop on Automated Knowledge Base\nConstruction, pages 35–39.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai,\nCícero Nogueira dos Santos, Bing Xiang, and Stefano\nSoatto. 2021. Structured prediction as translation be-\ntween augmented natural languages. In ICLR.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT, pages 2227–2237.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus,\nTim Rocktäschel, Yuxiang Wu, Alexander H. Miller,\nand Sebastian Riedel. 2020. How context affects\nlanguage models’ factual predictions. CoRR.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models\nas knowledge bases? In EMNLP 2019, pages 2463–\n2473. Association for Computational Linguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Björkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards robust\nlinguistic analysis using ontonotes. In CoNLL, pages\n143–152.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2019a. Improving language under-\nstanding by generative pre-training. OpenAI blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019b. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, page 9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR.\nOsman Ramadan, Pawel Budzianowski, and Milica Ga-\nsic. 2018. Large-scale multi-domain belief tracking\nwith knowledge sharing. In ACL, pages 432–437.\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling relations and their mentions without\nlabeled text. In ECML-PKDD, pages 148–163.\nDan Roth and Wen-tau Yih. 2004. A linear program-\nming formulation for global inference in natural lan-\nguage tasks. In HLT-NAACL, pages 1–8.\nSebastian Ruder. 2017. An overview of multi-task learn-\ning in deep neural networks. CoRR.\nSwarnadeep Saha and Mausam. 2018. Open informa-\ntion extraction from conjunctive sentences. In COL-\nING, pages 2288–2299.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, An-\nder Barrena, and Eneko Agirre. 2021. Label verbal-\nization and entailment for effective zero and few-shot\nrelation extraction. In EMNLP, pages 1199–1212.\nErik F. Tjong Kim Sang and Fien De Meulder. 2003.\nIntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In HLT-\nNAACL, pages 142–147.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M. Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault Févry, Jason Alan Fries, Ryan Tee-\nhan, Stella Biderman, Leo Gao, Tali Bers, Thomas\nWolf, and Alexander M. Rush. 2021. Multitask\nprompted training enables zero-shot task generaliza-\ntion. CoRR.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In NAACL-HLT, pages 2339–2352.\nTianxiao Shen, Victor Quach, Regina Barzilay, and\nTommi S. Jaakkola. 2020. Blank language models.\nIn EMNLP, pages 5186–5198.\nPeng Shi and Jimmy Lin. 2019. Simple BERT models\nfor relation extraction and semantic role labeling.\nCoRR.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling,\nand Tom Kwiatkowski. 2019. Matching the blanks:\nDistributional similarity for relation learning. InACL,\npages 2895–2905.\nRobyn Speer and Catherine Havasi. 2012. Representing\ngeneral relational knowledge in conceptnet 5. In\nLREC, pages 3679–3686.\nGabriel Stanovsky and Ido Dagan. 2016. Creating a\nlarge benchmark for open information extraction. In\nEMNLP, pages 2300–2305.\nGabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav\nGoldberg. 2016. Getting more out of syntax with\nprops. CoRR.\nGabriel Stanovsky, Julian Michael, Luke Zettlemoyer,\nand Ido Dagan. 2018. Supervised open information\nextraction. In NAACL-HLT, pages 885–895.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn EMNLP-IJCNLP, pages 5783–5788.\nC. Walker and Linguistic Data Consortium. 2005. ACE\n2005 Multilingual Training Corpus. Linguistic Data\nConsortium.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2021. Zero-shot informa-\ntion extraction as a unified text-to-triple translation.\nIn EMNLP, pages 1225–1238.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs.\nCoRR.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Finetuned\nlanguage models are zero-shot learners.\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Ed-\nuard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-\nwen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-\nchini, et al. 2013. Ontonotes release 5.0 ldc2013t19.\nLinguistic Data Consortium, Philadelphia, PA.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl,\nCaiming Xiong, Richard Socher, and Pascale Fung.\n2019. Transferable multi-domain state generator for\ntask-oriented dialogue systems. In ACL, pages 808–\n819.\nWei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei\nLi. 2020. Corefqa: Coreference resolution as query-\nbased span prediction. In ACL, pages 6953–6963.\nYing Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,\nand Denilson Barbosa. 2013. Open information ex-\ntraction with tree kernels. In HLT-NAACL, pages\n868–877.\nHang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng\nZhang, and Xipeng Qiu. 2021. A unified gen-\nerative framework for various NER subtasks. In\nACL/IJCNLP, pages 5808–5822.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS, pages 5754–5764.\nZhilin Yang, Ruslan Salakhutdinov, and William W. Co-\nhen. 2016. Multi-task cross-lingual sequence tagging\nfrom scratch. CoRR.\nBowen Yu, Zhenyu Zhang, Xiaobo Shu, Tingwen Liu,\nYubin Wang, Bin Wang, and Sujian Li. 2020a. Joint\nextraction of entities and relations based on a novel\ndecomposition strategy. In ECAI-PAIS, pages 2282–\n2289.\nJuntao Yu, Bernd Bohnet, and Massimo Poesio. 2020b.\nNamed entity recognition as dependency parsing. In\nACL, pages 6470–6476.\nYue Yuan, Xiaofei Zhou, Shirui Pan, Qiannan Zhu,\nZeliang Song, and Li Guo. 2020. A relation-specific\nattention network for joint entity and relation extrac-\ntion. In IJCAI, pages 4054–4060.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. Multiwoz 2.2 : A dialogue dataset with addi-\ntional annotation corrections and state tracking base-\nlines. CoRR.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. PEGASUS: pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn ICML, pages 11328–11339.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nEMNLP, pages 35–45.\nTianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun\nLi. 2020. Asking effective and diverse questions: A\nmachine reading comprehension based framework\nfor joint entity-relation extraction. In IJCAI, pages\n3948–3954.\nWanrong Zhu, Zhiting Hu, and Eric P. Xing. 2019. Text\ninfilling. CoRR.\nA Experimental Setup\nA.1 Implementation Details\nModel Architecture We leverage the General-\nized Language Model (GLM) (Du et al., 2021)\nas our base language model pretrained on autore-\ngressive blank infilling objectives. GLM follows\nan adaptive encoder-decoder architecture. It im-\nproves the pretrain-finetune consistency via cloze-\nstyle finetuning. GLM adopts the Byte Pair Encod-\ning (Radford et al., 2019b), covering 50,257 tokens.\nIn this work, we leverage the models in four dif-\nferent scales: 110M, 220M, 2B, and 10B 2. The\n110M model is pretrained over English Wikipedia\nand BookCorpus, and the others are pretrained over\nthe Pile corpora (Gao et al., 2021). The Pile cor-\npora are regarded as the similar corpora for training\nGPT-3. GLM outperforms T5 on text summariza-\ntion, which shares a similar nature with structure\nprediction tasks. Compared to GPT-3, GLM is a\nbidirectional model and is able to perform autore-\ngressive generation.\nStructure Pretraining Procedure\nTask-Agnostic Pretraining We conduct the pre-\ntraining on 8 NVIDIA DGX-A100 machines using\nan Adam optimizer with a 5e-6 learning rate and\n0.1 weight decay. We train the model with batch\nsize 4 per GPU for 3 epochs and use the checkpoint\nof the last iteration.\nMulti-Task Training We conduct the multi-task\ntraining on 8 NVIDIA DGX-A100 machines using\nan Adam optimizer with a 5e-6 learning rate and\n0.1 weight decay. We train the model with batch\nsize 4 per GPU for 6 epochs and use the checkpoint\n2https://github.com/THUDM/GLM\nwith the best performance on the corresponding\nvalidation set for each dataset.\nInference During the inference, length penalty\nand minimum target length are the most important\nhyperparameters. Length penalty is a float between\n0 and 1 to control the GLM’s generation length.\nThe larger the length penalty is, the longer the gen-\neration length is. In general, for entity prediction\ntasks (e.g., NER, SRL, event extraction), a larger\nlength penalty is used. For entity and relation pre-\ndiction or triple prediction tasks (e.g., JER and\nOIE), a smaller one is used. For other tasks that\nrequire a specific number of output triples (e.g., re-\nlation classification, intent detection, factual probe),\nwe trim the generation results according to the re-\nquirements of different tasks. We show details of\nthe task-specific hyperparameters in Appendix A.2\nto Appendix A.11.\nPretraining Data We apply the task-agnostic\npretraining data presented in Sec. 2.1 during struc-\nture pretraining. A small portion of T-REx (El-\nSahar et al., 2018) is used in the factual probe\ntask (Petroni et al., 2020). To avoid the test leak-\nage, we (i) sample a small portion of the T-REx\nas our pretraining data, and (ii) remove samples\nthat appeared in the T-REx dataset of the factual\nprobe task from the pretraining data. We integrate\nWebNLG 2.1 and 3.0 into a WebNLG dataset. For\nOPIEC, We use its OPIEC-clean version. Similar\nto T-REx, we also sample a portion of the OPIEC\nfor our pretraining due to its large size.\nThe following sections introduce the dataset for-\nmats, comparison methods, and training details for\nall 10 structure prediction tasks. We show addi-\ntional input and output examples on all datasets in\nTable 16.\nA.2 Open Information Extraction\nFor OIE, given a sentence, we are asked to extract\ntriples consisting of arguments and predicates. An\nexample of the input and output format is as fol-\nlows.\nInput and Output Format\nInput Born in 1951 in Tbilisi, Iago is a Geor-\ngian artist.\nOutput (Iago; Born in; 1951) (Iago; is a;\nGeorgian artist)\nwhere we extract arguments (e.g., Iago) and their\npredicates (e.g., Born in) in the form of triples as\nTask Dataset #SentsTrain Dev Test\nOpen information extraction\nOIE2016 2,278 571 589WEB - - 920NYT - 300 149PENN - - 51\nRelation classificationTACRED 68,124 22,631 15,509FewRel 1.0 56,000 1,120 –\nJoint entity and relation extraction\nCoNLL04 922 231 288ADE 3,845 – 427NYT 56,195 5,000 5,000ACE2005 7,477 1,789 1,517\nEvent extraction ACE2005 Trigger 11,178 649 642ACE2005 Argument 4,450 531 612\nCoreference resolutionCoNLL12 3,991 2,359 2,421\nIntent detection ATIS 4,478 500 893SNIPS 13,084 700 700\nSemantic role labeling\nCoNLL05 39,832 3,206 –CoNLL05 WSJ 39,832 3,206 5,221CoNLL05 Brown 39,832 3,206 779CoNLL12 89,549 32,397 21,499\nNamed entity recognition\nCoNLL03 14,041 3,250 3,453OntoNotes 59,924 8,528 8,262GENIA 14,824 1,855 1,854ACE2005 7,299 971 1,060\nDialogue state trackingMultiWOZ 2.1 62,367 7,371 7,368\nFactual probe Google-RE - - 552T-REx - - 3,403\nTable 5: Statistics of downstream datasets.\noutputs from the input sentence.\nDatasets We evaluate the performance of the\ncompared approaches on OIE benchmark datasets\nincluding OIE2016 (Stanovsky and Dagan, 2016),\na dataset converted from QA-SRL (He et al.,\n2015) based on Newswire and Wikipedia; three\ndatasets transformed from news corpus, in-\ncluding NYT (de Sá Mesquita et al., 2013),\nWEB (de Sá Mesquita et al., 2013), PENN (Xu\net al., 2013). The statistics of all datasets are shown\nin Table 5.\nComparison Methods We compare our method\nDEEP STRUCT to the following OIE systems\npresented in (Stanovsky et al., 2018): (i)\nClausIE (Corro and Gemulla, 2013): which lever-\nages linguistic and grammatical knowledge to split\nclauses in a sentence and identify their roles, (ii)\nOpenIE 4 3: which integrates SRLIE (Christensen\net al., 2011) and Relnoun (Pal et al., 2016) systems,\n(iii) PropS (Stanovsky et al., 2016): which focuses\non prepositional phrases structure in sentences for\nOIE, (iv) RnnOIE (Stanovsky et al., 2018): a su-\npervised recurrent neural network based approach\nthat views the OIE task as a sequence tagging prob-\nlem. We additionally compare to MAMA using\nBERTLARGE from (Wang et al., 2020), which pro-\nposes to leverage knowledge stored in attention\nmatrices for OIE.\n3https://github.com/dair-iitd/OpenIE-standalone\nTraining Details We train our model on the\nOIE2016 training set for 5 epochs during multi-task\nfinetuning. The per GPU batch size is 4. During\ninference, for OIE2016, we choose a length penalty\nof 0.8. WEB, NYT, and PENN only contain the\ntest sets. For these datasets, we use a length penalty\nof 0.5 and trim the outputs to only contain the first\ntriple. We adopt the preprocessed OIE datasets\nprovided by Stanovsky et al. (2018).\nAdditional Results A detailed comparison be-\ntween DEEP STRUCT and compared approaches is\nshown in Table 6. On OIE2016, NYT, and PENN\ndatasets, DEEP STRUCT presents significant im-\nprovements compared to the OIE systems. While\non WEB, PropS (Stanovsky et al., 2016) outper-\nforms our method. The reason is that the arguments\nof WEB are very short and concise (e.g., “( google\n; assimilates ; youtube )”), which aligns better with\nthe phrase extraction paradigm of PropS. We also\nobserve that finetuning hurts the performance on\nWEB and PENN. This is because we are only able\nto finetune DEEP STRUCT on the OIE2016 training\nset and this can lead to overfitting.\nA.3 Relation Classification\nGiven head and tail entities in the target sentence,\nwe seek to identify the relation between them. An\nexample is as below.\nInput and Output Format\nInput The 1976 Thomas Cup was the tenth\nedition of Thomas Cup, the world cham-\npionship of men’s international team bad-\nminton (its female counterpart is the Uber\nCup). The relationship between Uber Cup\nand badminton is\nOutput (Uber Cup; sport; badminton)\nwhere “Uber Cup” and “badminton” are the cor-\nresponding head and tail entities, and “sport” is a\nrelation from a predefined category. In addition, we\naugment the input sentence with the task-specific\nsuffix “The relationship between [head entity] and\n[tail entity] is” following (Paolini et al., 2021) as\nshown in the above example.\nDatasets We evaluate on FewRel (Han et al.,\n2018) and TACRED (Zhang et al., 2017).\n• FewRel is a few-shot N-way K-shot relation\nclassification dataset for meta learning. For all\n100 relations, train (64 relations), validation\n(16 relations), and test set (20 relations) are\nconstructed accordingly. We report the results\non the dev set.\n• TACREDis a large-scale benchmark includ-\ning over 100K samples and 41 relation types.\nWe select the checkpoint on the dev set, and\nreport results on the test set.\nWe show the dataset statistics in Table 5. We use F1\nto evaluate the results. We parse every relation type\nand the corresponding head and tail entities from\nevery original sample and formulate it as the input\nand output format as shown in the above example.\nComparison Methods The compared models are\nas follows: (i) BERT-PAIR (Gao et al., 2019) is a\nsentence-level pairwise model that optimizes the\nsimilarity between sentences with the same relation.\n(ii) BERTEM+Matching the Blanks (MTB) (Soares\net al., 2019) proposes continual pretraining for\nrelations over a large-scale entity-linked corpus.\n(iii) TANL (Paolini et al., 2021) is a sequence-to-\nsequence generation model using task-augmented\nnatural languages.\nTraining Details We train our model on training\nsets of TACRED and FewRel for 20 epochs during\nmulti-task finetuning respectively. The per GPU\nbatch size is 4. As shown in the above input and\noutput format, we use the prompt of “The relation-\nship between [head entity] and [tail entity] is” to\nquery the model to generate the relation. For the\nzero-shot setting, the model is also provided with\nthe prefix “( [head entity];” to generate the relation\nand tail entity. The prediction is correct only if both\nthe relation and tail entity are correct. The length\npenalty equals 0.5.\nAdditional Results We show the results in Ta-\nble 7. DEEP STRUCT outperforms all supervised\nmethods on both TACRED and FewRel. We find\nthat our task-agnostic pretraining can significantly\nhelp improve relation classification. This is vital to\nfew-shot settings (FewRel), where DEEP STRUCT\ncan achieve almost perfect F1 scores. We also no-\ntice that multi-task DEEP STRUCT outperforms all\ncompared approaches except for the 5-5 FewRel\nsetting.\nA.4 Factual Probe\nGiven an input sentence, and a gold head entity and\nrelation, the task is to predict the missing tail entity\nin the following.\nInput and Output Format\nInput Daniel Bowen, born in 1970, is a Mel-\nbourne resident best known as the author of\nthe blog, Diary of an Average Australian.\nOutput (Daniel Bowen; date_of_birth; 1970)\nwhere “(Daniel Bowen; date of birth; ” is provided\nin the output, and the model is asked to generate\n“1970)”.\nDatasets We use the Google-RE dataset consist-\ning of 3 relations (“place of birth”, “place of death”,\nand “date of birth”), and T-REx with 41 relations\nfrom the LAMA benchmark (Petroni et al., 2019).\nThe task is evaluated using mean precision at one\n(P@1).\nComparison Methods We compare to the fol-\nlowing approaches: (i) LAMA (Petroni et al., 2019)\nuses only the head and relation to form the query\nwithout using the oracle context, and (ii) LAMA-\nOracle (Petroni et al., 2020) takes (at most) five\ngold sentences as additional context in the query.\nBoth methods are based on BERTLARGE and the\nquery is constructed based on natural language\ntemplates. For example, the Wikidata relation\n“place_of_birth” is aligned with a template “was\nborn in”.\nTraining Details As the factual probe task is usu-\nally performed without training sets, we only report\nDEEP STRUCT ’s results in the zero-shot and multi-\ntask setting (without finetuning). We follow the\ntask format of LAMA-Oracle (Petroni et al., 2020),\nwhich appends the query to five oracle context sen-\ntences. We use the relation labels (e.g., “place of\nbirth”) rather than the templates (e.g., “was born\nin”) as they align better with our task-agnostic pre-\ntraining. Note that we have removed the T-REx\ndata in the LAMA benchmark from the pretraining\ndata.\nAdditional Results Table 8 shows the results.\nDEEP STRUCT significantly outperforms compared\napproaches, which is attributed to the larger model\nsize and knowledge-intensive task-agnostic pre-\ntraining. Multi-task setting actually hurts the per-\nformance due to the difference between the relation\nschema of downstream datasets and task-agnostic\npretraining datasets.\nA.5 Joint Entity and Relation Extraction\nThe goal of the task is to extract entities and re-\nlations (with their type information) from a given\nsentence. We formulate the task as two unit tasks:\nthe first task is entity prediction to generate the en-\ntities, while the second task is relation prediction\nto generate the relations. Our task formalization is\ndifferent compared to traditional JER, where our\ntwo unit tasks are independent. An example is as\nfollows.\nInput and Output Format\nInput Blackstone already holds a 50 percent\nstake in the two parks that make up Universal\nOrlando.\nEntity Output (Blackstone; instance of; or-\nganization) (parks; instance of; organization)\n(Universal Orlando; instance of; organization)\nRelation Output (Blackstone; employer;\nparks)\nwhere the entity output contains entity predictions\nand relation output contains relation predictions.\nThe entity mentions are detected following the pro-\ncedure below.\nEvaluation Details The conventional entity pre-\ndiction evaluation is based on extractive span\nmatching. To ensure a fair comparison in situations\nwhere there are multiple entities with the same sur-\nface, we adopt the following strategy: we match the\nspans of the generated entities from left to right in\nthe original sentence when they are first mentioned.\nIf there are duplicated entities, they are matched\nsequentially. For example, the first generated one\nmatches the first mention span, while the second\none matches the second mention span, etc. This\nstrategy applies to all tasks that involve entity men-\ntion detection such as named entity recognition.\nDatasets We experiment on the following\ndatasets:\n• The CoNLL04 (Roth and Yih, 2004) dataset:\nCoNLL04 consists of four types of enti-\nties (“location”, “organization”, “person”,\n“other”) and five types of relations (“work for”,\n“kill”, “organization based in“, “live in”, “lo-\ncated in”) on sentences taken from WSJ, AP,\netc, containing 922 samples for training, 231\nsamples for validating, and 288 samples for\ntesting. We use the same split as in (Gupta\net al., 2016). We use the same type names\nof entities and relations with TANL (Paolini\net al., 2021).\n• The ADE (Gurulingappa et al., 2012) dataset:\nADE contains annotated documents for drug-\nrelated adverse effects over medical case re-\nports corpus. It consists of two entity types\n(“Adverse-Effect” and “Drug”) and one rela-\ntion type (“(Has-)Adverse-Effect”). We use\nthe same type names as in (Paolini et al.,\n2021).\n• The NYT (Riedel et al., 2010) dataset: NYT is\na distantly-supervised joint entity and relation\nextraction dataset based on New York Times\ncorpus. The dataset consists of three entity\ntypes (“PER”, “ORG”, and “LOC”) and 24\nFreebase relations. We use a preprocessed ver-\nsion of this dataset from (Yu et al., 2020a) and\nuse the same type names with TANL (Paolini\net al., 2021).\n• The ACE2005 (Walker and Consortium,\n2005) dataset: ACE2005 is based on the ACE\n2005 Multilingual Training Corpus. We use a\npreprocessed version of this dataset in (Luan\net al., 2019). We make use of seven entity\ntypes and six relation types with the same\ntype names as in TANL.\nThe dataset statistics are shown in Table 5.\nComparison Methods We compare our method\nDEEP STRUCT on the four datasets to the following\nJER methods: (i) SpERT (Eberts and Ulges, 2020):\nThe BERT-based model first conducts named entity\nrecognition formulated as sequence tagging, and\nperforms relation classification between recognized\nentities; (ii) DyGIE (Luan et al., 2019): The gen-\neral information extraction framework organizes\ndynamic spans into graphs; (iii) MRC4ERE (Zhao\net al., 2020): The model formulates the joint en-\ntity and relation extraction task as machine reading\ncomprehension; (iv) RSAN (Yuan et al., 2020):\nThe work presents a relation-specific attention net-\nwork to jointly extract entities and relations; (v)\nTANL (Paolini et al., 2021): It is a sequence-to-\nsequence extraction model using augmented natu-\nral languages.\nTraining Details We train our model on JER\ntraining sets during multi-task finetuning for (i) 10\nepochs on CoNLL04, (ii) 10 epochs on ADE, (iii)\n3 epochs on NYT, and (iv) 10 epochs on ACE2005.\nWe employ less number of epochs on NYT, as its\nsize is much larger compared to other datasets. We\nfind that the relation prediction task and the en-\ntity prediction task need different length penalties.\nTherefore, we split the training sets corresponding\nto the two tasks. We choose a length penalty of\n0.8 for entity prediction and 0.3 for relation predic-\ntion during inference. We use the same evaluation\nscripts as in (Paolini et al., 2021). As multi-task\ntraining and finetuning for 10 folds on ADE is too\nexpensive for DEEP STRUCT 10B, only the first\nsplit of ADE is included in the multi-task training\nand finetuning. In the ablation study (Sec. 3.2), we\nalso present a model variant with entity and rela-\ntion augmentation. For this setting, we augment\nthe output with entity boundary information. For\nexample, for the example in Figure 1, “([Iago]; in-\nstance of; person) ([Iago]; city_of_birth; [Tbilisi])”\nare the augmented outputs.\nAdditional Results Table 9 presents the results.\nDEEP STRUCT outperforms or is competitive com-\npared to state-of-the-art supervised approaches.\nWe also find the multi-task DEEP STRUCT per-\nforms competitively with previous task-specific ap-\nproaches on both ADE and NYT. This indicates\nthat multi-task trained models are cost-effective\nalternatives to per-task finetuned models.\nA.6 Named Entity Recognition\nCompared to joint entity and relation extraction,\nnamed entity recognition only focuses on predict-\ning the entities and their corresponding types in the\ntarget sentence. We show an example below.\nInput and Output Format\nInput What we need to do is to make sure\nthat state boards, number one, have adequate\nfunding.\nOutput (we; instance of; human) (state; in-\nstance of; geographical entity) (state boards;\ninstance of; organization)\nwhere the head entities of these triples are the entity\nmentions in the given sentence, and tail entities are\nfrom a predefined list of entity types.\nDatasets We experiment on the following\ndatasets:\n• The CoNLL03 (Sang and Meulder, 2003)\ndataset: CoNLL03 (English) data was taken\nfrom the Reuters Corpus, containing 14,041\ntraining samples, 3,250 validating samples\nand 3,453 testing samples. It consists four\nentity types (“LOC”, “ORG”, “PER”, and\n“MISC”). We use the preprocessed version of\nthis dataset from (Li et al., 2020a).\n• The OntoNotes (Pradhan et al., 2013) dataset:\nOntoNotes contains 59,924 training samples,\n8,528 validating samples, and 8,262 testing\nsamples. It consists 18 different entity types\n(e.g., “ORG”, “PER”). We use the preprocess-\ning scripts provided by (Luan et al., 2019).\n• The GENIA (Ohta et al., 2002) dataset:\nGENIA consists of compiled and annotated\nbiomedical literature, which contains 14,824\ntraining samples, 1,855 validating samples,\nand 1,854 testing samples. It consists five\nentity types (“DNA”, “RNA”, “cell_line”,\n“cell_type”, and “protein”). We use a prepro-\ncessed version of this dataset (Li et al., 2020a).\n• The ACE2005 (Walker and Consortium,\n2005) dataset: ACE2005 contains 7,299 train-\ning samples, 971 validating samples, and\n1,060 testing samples. Note that it is also pro-\ncessed based on the ACE2005 corpus but with\ndifferent data splits compared to that of the\nACE2005 JER dataset. It includes seven en-\ntity types. We use the preprocessed version of\nthis dataset in (Li et al., 2020a), and exclude\nthis dataset in the DEEP STRUCT ’s multi-task\nsetting due to its overlap with the ACE2005\nJER dataset.\nComparison Methods We compare our method\nDEEP STRUCT on the four datasets to the following\nNER methods: (i) BERT-MRC (Li et al., 2020a):\nthis method formulates NER as a machine reading\ncomprehension problem, (ii) BERT-MRC+DSC (Li\net al., 2020b): this model is a dice-loss enhanced\nversion of BERT-MRC, (iii): Cloze-CNN (Baevski\net al., 2019): the model leverages cloze-style pre-\ntraining on convolutional neural networks for natu-\nral languages, (iv) GSL (Athiwaratkun et al., 2020):\nthe method uses augmented language for intent de-\ntection, slot filling, and named entity recognition,\n(v) BiaffineLSTM (Yu et al., 2020b): the model\ntransforms NER into dependency parsing using bi-\naffine LSTMs, (vi) TANL (Paolini et al., 2021):\nit presents a sequence-to-sequence extraction ap-\nproach using augmented natural languages.\nTraining Details We train our model on NER\ntraining sets for 15 epochs on every dataset during\nmulti-task finetuning with early stopping. The per\nGPU batch size is 4. We choose a length penalty\nof 0.8 during inference. Since some datasets may\ncontain null predictions, we set the minimum target\nlength to 0.\nAdditional Results Table 10 shows the results.\nDEEP STRUCT achieves comparable performance\nto task-specific supervised approaches, except for\nOntoNotes. We suppose that OntoNotes contains\na relatively large number of entity types, making\nit more challenging for models to use labels for\nconsidering their semantic meanings. On GENIA\nand ACE2005, DEEP STRUCT outperforms state-of-\nthe-art task-specific methods.\nA.7 Semantic Role Labeling\nIn semantic role labeling, we seek to identify the\ncorresponding arguments in the form of spans (or\nthe semantic roles) given a certain predicate. Con-\nsider an example as follow.\nInput and Output Format The predicate is\nmarked in the input. The model then yields ar-\nguments according to the predicate in the output\nwith their corresponding argument types from a\npredefined set.\nInput Scotty [ accepted ] the decision with\nindifference and did not enter the arguments.\nOutput (Scotty; instance of; subject) (deci-\nsion; instance of; object)\nwhere “[ accepted ]” is the given predicate, and\narguments such as “Scotty”, “the decision” and\ntheir corresponding types are generated in the form\nof triples.\nDatasets We experiment on the following\ndatasets: CoNLL05 WSJ, CoNLL05 Brown (Car-\nreras and Màrquez, 2005) and CoNLL12 (Pradhan\net al., 2013). Table 5 shows the dataset statistics.\n• The CoNLL05 WSJ and CoNLL05 Brown\ndatasets: CoNLL05 WSJ and CoNLL05\nBrown datasets share the same train and val-\nidation splits. They have different test sets.\nFor CoNLL05 WSJ and CoNLL05 Brown,\nthe corresponding test datasets are taken from\nthe WSJ and Brown corpus respectively. The\ndatasets consist of seven different types includ-\ning “V” (verb), “A0” (subject), “A1” (object),\n“A2”, “A3”, “AM-MOD”, and “AM-NEG”.\nWe use the same type names as in (Paolini\net al., 2021).\n• The CoNLL12 dataset: CoNLL12 dataset is\nbuilt upon OntoNotes dataset including 39\nargument types. We leverage the same type\nnames as in (Paolini et al., 2021).\nComparison Methods We compare our method\nDEEP STRUCT on the datasets to the following\nSRL models: (i) Dep and Span (Li et al., 2019):\nthis model formulates semantic role labeling as\nan end-to-end dependency parsing task, (ii) BERT\nSRL (Shi and Lin, 2019): it is a sequence-tagging\nversion of BERT, (iii) TANL (Paolini et al., 2021):\nthis is a sequence-to-sequence extraction model\nusing augmented natural languages.\nTraining Details During multi-task finetun-\ning, for CoNLL05, the model is trained on\nCoNLL05 WSJ’s training set, and evaluated on\nboth CoNLL05 WSJ and CoNLL05 Brown test\nsets. We train DEEP STRUCT on CoNLL05 WSJ\nand CoNLL12 for 5 epochs respectively. The per\nGPU batch size equals 4. The length penalty is 0.8.\nEvaluation Details Sentences with multiple tar-\nget predicates are duplicated during data prepro-\ncessing. So, each sentence is only related to one\ntarget predicate that is marked by “[]”. We adopt\nthe same evaluation scripts as in (Paolini et al.,\n2021).\nAdditional Results Table 11 shows the results.\nBoth multi-task and multi-task finetuned DEEP -\nSTRUCT outperform task-specific models by a\nlarge margin. An important reason is that Prop-\nBank (Kingsbury and Palmer, 2003) is included in\nthe multi-task training. The knowledge of Prop-\nBank transfers well to other SRL datasets. We\nfind that the performance gain is significant since\nthe large-scale model has the capacity to capture\nthe PropBank knowledge. We also observe a mi-\nnor performance drop from multi-task to multi-task\nfinetuned DEEP STRUCT on CoNLL05 WSJ and\nCoNLL12 datasets. This might be attributed to\noverfitting. Besides, the issue can be relieved if a\nbetter hyperparameter combination is used in the\nmulti-task finetuning setting.\nA.8 Event Extraction\nThis task contains two sequential subtasks: (i)\nevent triggers identification and classification: this\nsubtask first identifies the trigger words in target\nsentences that refer to certain types of events, and\n(ii) trigger arguments identification and classifica-\ntion: this subtask then extracts arguments from the\ntarget sentences that can be mapped to certain roles\nin the event from (i).\nInput and Output Format\nTrigger Input But the Saint Petersburg sum-\nmit ended without any formal declaration on\nIraq .\nTrigger Output (summit; instance of; meet)\nArgument Input But the Saint Petersburg [\nsummit ] ended without any formal declara-\ntion on Iraq .\nArgument Output (Saint Petersburg; in-\nstance of; place)\nwhere “summit” is an extracted trigger and “meet”\nis its corresponding trigger event. Then, based on\nthe “summit” event, we can further extract the role\nof “place” in this event as “Saint Petersburg”.\nDatasets We experiment on the ACE2005\ndataset. For detailed dataset statistics, please refer\nto Table 5.\n• The ACE2005 dataset (Walker and Consor-\ntium, 2005): ACE2005 contains 33 types of\nevent triggers, and each of them corresponds\nto a set of argument roles. We follow the pre-\nprocessing in TANL (Paolini et al., 2021) and\nuse the same evaluation scripts.\nComparison Methods We compare our method\nDEEP STRUCT on the dataset to the following meth-\nods: (i) J3EE (Nguyen and Nguyen, 2019): This\nmethod presents a joint model based on a recurrent\nneural network to first extract mention spans for\ntriggers and arguments and then perform pairwise\nclassification, (ii) DyGIE++ (Wadden et al., 2019):\nthe method leverages BERT for sequence tagging\nto identify mention spans and then classify each\nmention with triggers in pair for argument roles,\n(iii) TANL (Paolini et al., 2021): this is a sequence-\nto-sequence extraction approach using augmented\nnatural languages.\nTraining Details We train our model on\nACE2005 event trigger and argument training sets\nfor 20 epochs during multi-task finetuning. The per\nGPU batch size is 4. During inference, we choose\na length penalty of 0.8. The argument prediction\ntask requires triggers as input to make predictions.\nAn example is shown in Table 16. For the argu-\nment prediction task, we first generate all trigger\npredictions using our 10B model. If there is more\nthan one trigger in a sentence, we will duplicate\nthe sentence to make sure that every sample corre-\nsponds to a single trigger. The ACE2005 dataset is\nprocessed similarly to the named entity recognition\ntask.\nAdditional Results Table 12 presents the results.\nDEEP STRUCT is competitive with the state-of-the-\nart task-specific supervised models on trigger iden-\ntification and classification, as well as the argument\nidentification task. In the meantime, DEEP STRUCT\noutperforms the comparison methods on the argu-\nment classification task.\nA.9 Coreference Resolution\nThe coreference resolution aims to identify and\ncluster mentions in a document that refers to the\nsame entity. An example is as follows.\nInput and Output Format\nInput And deterrents don’t work well when\nan enemy values your death more than his life.\nOutput (an enemy; refer to; his)\nwhere “an enemy” appears as the target entity and\n“his” is the mention it refers to. “refer to” is pro-\nvided as part of the output triple.\nDatasets We experiment on the CoNLL12\n(Pradhan et al., 2013) dataset constructed from\nOntoNotes corpus. The dataset statistics are pre-\nsented in Table 5.\nComparison Methods We compare our method\nDEEP STRUCT on the dataset to the following\nmethods: (i) Higher-order c2f-coref (Lee et al.,\n2018): this method proposes a fully differentiable\nformulation of coreference resolution via itera-\ntive refinement using attention mechanism, (ii)\nBERT+c2f-coref (Joshi et al., 2019): this model\nreplaces original recurrent neural network back-\nbone in (Lee et al., 2018) with BERT, (iii) Core-\nfQA+SpanBERT (Wu et al., 2020): the model\nformulates coreference resolution as question an-\nswering, (iv) TANL (Paolini et al., 2021): this is a\nsequence-to-sequence extraction model using aug-\nmented natural languages.\nTraining Details We train our model on\nCoNLL12 coreference resolution training set for\n40 epochs during multi-task finetuning. The per\nGPU batch size is 4. During inference, we choose\na length penalty of 0.8. CoNLL12 coreference res-\nolution has different evaluation metrics compared\nto other structure prediction tasks, including: (i)\nMUC: a link-based metric that reflects the mini-\nmum number of missing mentions in the response\nchain (Moosavi and Strube, 2016), (ii) B3: a single-\nmention based metric which computes the macro\nF1 of all entity mentions, and (iii) CEAFϕ4: a simi-\nlarity metric based on the assumption that the coref-\nerence map should be one-to-one. Due to the lim-\nited maximum sequence length of language models,\nthe dataset is chunked with a fixed size of 512 dur-\ning data preprocessing. Following TANL (Paolini\net al., 2021), only intra-chunk coreferences are pre-\nserved. We also use the same evaluation scripts\nwith (Paolini et al., 2021).\nAdditional Results Table 13 shows the results.\nDEEP STRUCT presents better results compared\nto TANL and classic task-specific supervised ap-\nproaches. However, DEEP STRUCT fails when com-\npared with the state-of-the-art coreference method.\nThe main reason is that this task requires task-\nspecific model architectures. In the meantime, we\nargue that it is promising to employ a unified frame-\nwork for multiple structure prediction tasks.\nA.10 Dialogue State Tracking\nWe are presented with a dialogue between a user\nand an agent to identify what information is known\ngiven a list of slots by the end of each round of the\nconversation. An example is as follows.\nInput and Output Format\nInput [User]: I would like a taxi from\nSaint Johns College to Pizza Hut Fen Ditton.\n[Agent]: What time do you want to leave and\nwhen you want to arrive? [User]: I want to\nleave after 17:15.\nOutput ([User]; taxi arrive by; not given)\n([User]; taxi departure; Saint Johns College)\n([User]; taxi destination; Pizza Hut Fen Dit-\nton) ([User]; taxi leave at; 17:15)\nin which by the end of this conversation, we know\nthat the user wants to get to Pizza Hut Fen Ditton\nfrom Saint Johns College, leaving at 17:15, while\nthe taxi’s arrival time is unknown. The slots “taxi\narrive by”, “taxi departure”, “taxi destination”, and\n“taxi leave at” are provided for the output.\nDatasets We use the MultiWOZ 2.1\n(Budzianowski et al., 2018; Ramadan et al.,\n2018; Eric et al., 2020; Zang et al., 2020), which\nis a daily dialogue dataset for task-oriented\nconversations. We follow the preprocessing in\n(Wu et al., 2019). Following TANL (Paolini\net al., 2021), the “police” and “hospital” slots are\nexcluded from the training set as they are absent\nin the test set. The resulting training set contains\n7,904 samples. Dataset statistics are presented in\nTable 5.\nComparison Methods We compare our method\nwith: (i) TRADE (Wu et al., 2019): It is a transfer-\nable multi-domain generative dialogue state track-\ning model, (ii) SimpleTOD (Hosseini-Asl et al.,\n2020): This is a state-of-the-art task-specific model\nfor dialogue state tracking based on GPT-2 (Rad-\nford et al., 2019b). In addition, we also compare\nour method with TANL.\nTraining Details We finetune for 20 epochs. The\nmaximum sequence length is 512, and the per GPU\nbatch size is 4. Given a domain and all possible\nslots, DEEP STRUCT generates triples regarding the\nslots: if the information is not yet provided, the\ntail should be “not given”. We use the same type\nnames with (Paolini et al., 2021).\nAdditional Results Table 14 shows the results.\nWhile DEEP STRUCT does not outperform the state-\nof-the-art SimpleTOD, it is still competitive com-\npared to the task-specific supervised models. This\ndemonstrates the effectiveness of DEEP STRUCT\nin dealing with different structure prediction tasks\nunder the same architecture.\nA.11 Intent Detection\nIntent detection identifies the user’s intent in the\nconversation with the agent based on a predefined\nlist of slots. It resonates with the classical sentence\nclassification task. Below is an example.\nInput and Output Format\nInput Show flight and prices from Kansas\nCity to Chicago next Wednesday arriving in\nChicago by 7 pm.\nOutput (intent; is; flight and airfare)\nwhere our prediction is the “flight and airfare”. The\nhead entity “intent” and predicate “is” are given for\nall outputs.\nDatasets We use two datasets, the ATIS dataset\n(Hemphill et al., 1990), which contains flight and\nairline-related conversation and queries, and the\nSNIPS dataset (Coucke et al., 2018), which consists\nof daily queries from the interaction between the\nusers and dialogue agents. The dataset statistics are\nshown in Table 5.\nComparison Methods We compare our method\nto SF-ID (E et al., 2019) and TANL (Paolini et al.,\n2021) in this task.\nTraining Details We formulate the label of every\nsample as “(intent; is; [label])”. We parse every\nintent from every original sample and formulate it\ninto the input and output format as shown above.\nWe finetune for 20 epochs. The maximum sequence\nlength is 512, and the per GPU batch size is 4. We\nreport F1 for this task.\nAdditional Results Table 15 shows the results.\nDEEP STRUCT is comparable to task-specific super-\nvised approaches on both ATIS and SNIPS datasets.\nFor TANL, we produce the results using the re-\nleased code 4.\nB Error Analysis\nWe analyze the errors of DEEP STRUCT 10B multi-\ntask in recall on the CoNLL04 (JER) dataset. We\nspecifically investigate the errors in the relation\noutputs. Table 17 shows the error cases. We find\nthat most errors are caused by minor differences\nbetween ground truth entities and predicted entities\nfrom entity outputs. For example, the predicated\nentity has almost the same span as the ground truth\nentity (e.g., “U.S.” and “the U.S.”). Besides, we\nobserve some false-positive errors that are due to\nthe noise in the datasets. In such cases, our predic-\ntions are reasonable while they are missing due to\nthe incompleteness of human annotations.\n4https://github.com/amazon-research/tanl\nOIE2016 WEB NYT PENN\nClausIE (Corro and Gemulla, 2013) 58.8 44.9 29.6 34.6\nOpenIE 4 59.6 55.7 38.3 42.6\nPropS (Stanovsky et al., 2016) 55.6 58.9 37.2 39.1\nRnnOIE (Stanovsky et al., 2018) 67.0 58.1 28.3 34.5\nMAMA (Wang et al., 2020) 36.6 54.3 32.9 33.0\nDEEP STRUCT\nzero-shot 28.1 43.8 28.9 51.0\nmulti-task 71.2 50.8 43.6 54.5\nw/ finetune 71.3 49.1 45.0 45.1\nTable 6: Results on open information extraction.\nTACRED FewRel 1.0\n5-1 5-5 10-1 10-5\nBERTEM (Soares et al., 2019) 70.1 88.9 - 82.8 -\nBERTEM+MTB (Soares et al., 2019) 71.5 90.1 - 83.4 -\nDG-SpanBERT (Chen et al., 2020) 71.5 - - - -\nBERT-PAIR (Gao et al., 2019) 85.7 89.5 76.8 81.8\nNLI-DeBERTa (Sainz et al., 2021) 73.9\nTANL (Paolini et al., 2021) 71.9 93.6±5.4 97.6±3.2 82.2±5.1 89.8±3.6\nTANL (multitask) (Paolini et al., 2021) 69.1 - - - -\nDEEP STRUCT\nzero-shot 36.1 72.4±6.9 70.8±8.0 67.6±4.5 66.4±6.3\nmulti-task 74.9 93.6±6.0 96.4±4.2 92.2±6.4 94.6±3.6\nw/ finetune 76.8 98.4±2.8 100±0.0 97.8±2.0 99.8±0.6\nTable 7: Results on relation classification.\nGoogle-RE T-Rex\nLAMA (Petroni et al., 2019) 10.5 32.3\nLAMA-Oracle (Petroni et al., 2020) 74.3 66.0\nDEEP STRUCT zero-shot 97.9 85.0\nmulti-task 90.3 71.0\nTable 8: Results on factual probe.\nCoNLL04 ADE NYT ACE2005\nEnt Rel Ent Rel Ent Rel Ent Rel\nSpERT (Eberts and Ulges, 2020) 88.9 71.5 89.3 78.8\nDyGIE (Luan et al., 2019) 88.4 63.2\nMRC4ERE (Zhao et al., 2020) 88.9 71.9 85.5 62.1\nRSAN (Yuan et al., 2020) 84.6\nTANL (Paolini et al., 2021) 89.4 71.4 90.2 80.6 94.9 90.8 88.9 63.7\nTANL (multitask) (Paolini et al., 2021) 90.3 70.0 91.2 83.8 94.7 90.7 - -\nDEEP STRUCT\nzero-shot 48.3 25.8 60.7 10.6 60.5 28.6 31.8 5.3\nmulti-task 87.4 69.6 90.2 83.7 95.4 93.9 87.8 54.0\nw/ finetune 90.7 78.3 91.1 83.8 95.9 93.3 90.0 66.8\nTable 9: Results on joint entity and relation extraction.\nCoNLL03 OntoNotes GENIA ACE2005\nBERT-MRC (Li et al., 2020a) 93.0 91.1 - 86.9\nBERT-MRC+DSC (Li et al., 2020b) 93.3 92.1\nCloze-CNN (Baevski et al., 2019) 93.5\nGSL (Athiwaratkun et al., 2020) 90.7 90.2\nBiaffineLSTM (Yu et al., 2020b) 93.5 91.3 80.5 85.4\nTANL (Paolini et al., 2021) 91.7 89.8 76.4 84.9\nTANL (multitask) (Paolini et al., 2021) 91.7 89.4 76.4 -\nDEEP STRUCT\nzero-shot 44.4 42.5 47.2 28.1\nmulti-task 93.1 87.6 80.2 -\nw/ finetune 93.0 87.8 80.8 86.9\nTable 10: Results on named entity recognition.\nCoNLL05 WSJ CoNLL05 Brown CoNLL12\nDep and Span (Li et al., 2019) 86.3 76.4 83.1\nBERT SRL (Shi and Lin, 2019) 88.8 82.0 86.5\nTANL (Paolini et al., 2021) 89.3 82.0 87.7\nTANL (multitask) (Paolini et al., 2021) 89.1 84.1 87.7\nDEEP STRUCT multi-task 95.6 92.0 97.6\nw/ finetune 95.2 92.1 96.0\nTable 11: Results on semantic role labeling.\nTrigger Id Trigger Cl Argument Id Argument Cl\nJ3EE (Nguyen and Nguyen, 2019) 72.5 69.8 59.9 52.1\nDyGIE++ (Wadden et al., 2019) 69.7 55.4 52.5\nTANL (Paolini et al., 2021) 72.9 68.4 50.1 47.6\nTANL (multitask) (Paolini et al., 2021) 71.8 68.5 48.5 48.5\nDEEP STRUCT multi-task 71.7 67.9 54.9 52.7\nw/ finetune 73.5 69.8 59.4 56.2\nTable 12: Results on event extraction (ACE2005).\nCoNLL12\nMUC B 3 CEAFϕ4 Avg. F1\nHigher-order c2f-coref (Lee et al., 2018) 80.4 70.8 67.6 73\nBERT+c2f-coref (Joshi et al., 2019) 81.4 71.7 68.8 73.9\nCorefQA+SpanBERT (Wu et al., 2020) 86.3 77.6 75.8 79.9\nTANL (Paolini et al., 2021) 81.0 69.0 68.4 72.8\nTANL (multitask) (Paolini et al., 2021) 78.7 65.7 63.8 69.4\nDEEP STRUCT multi-task 63.9 57.7 60.2 60.6\nw/ finetune 74.9 71.3 73.1 73.1\nTable 13: Results on coreference resolution.\nMultiWOZ 2.1\nTRADE (Wu et al., 2019) 45.6\nSimpleTOD (Hosseini-Asl et al., 2020) 55.7\nTANL (Paolini et al., 2021) 50.5\nTANL (multitask) (Paolini et al., 2021) 51.4\nDEEP STRUCT multi-task 53.5\nw/ finetune 54.2\nTable 14: Results on dialogue state tracking.\nATIS SNIPS\nSF-ID (E et al., 2019) 97.8 97.4\nTANL (Paolini et al., 2021) 97.6 98.7\nDEEP STRUCT multi-task 66.6 78.4\nw/ finetune 97.8 97.3\nTable 15: Results on intent detection.\nTask Dataset Input Output\nOpen InformationExtraction OIE2016 oie oie2016: But for now, at least, Americans are far better atmaking PCs and the software that runs them.(Americans; making; PCs and the software that runs them) (PCs; runs;software)WEB oie web: Finally google bought youtube. (google; bought; youtube)NYT oie nyt: Now in its 58th final, the United States is pursuing a 30thcup title. (United States; pursuing; cup)\nPENN oie penn: Samsung already owns korea first advertising co., thatcountry’s largest agency. ( Samsung; owns; korea first advertising co. )\nRelation Classifi-cation TACRED rc tacred: Donald Wildmon , the founder and head of the AmericanFamily Association , is asking its members to petition Congress toend all funding for PBS . The relationship between Donald Wildmonand American Family Association is\n( Donald Wildmon; employee of; American Family Association )\nFewRel 1.0 rc fewrel: Boott was elected an Associate Fellow of the AmericanAcademy of Arts and Sciences in 1835 . The relationship betweenBoott and American Academy is\n( Boott; member of; American Academy )\nFactual Probe Google-REfp google-re: Eldon Coombe (born c 1941) is a Canadian curlerfrom Ottawa, Canada. (Eldon Coombe; date of birth; 1941)\nT-REx fp t-rex: Kurt Schwertsik (born 25 June 1935, Vienna) is an Aus-trian contemporary composer. (Kurt Schwertsik; place of birth; Vienna)\nJoint Entity andRelation Extrac-tion\nCoNLL04 jer conll04: An art exhibit at the Hakawati Theatre in Arab eastJerusalem was a series of portraits of Palestinians killed in the rebel-lion .\n( Hakawati Theatre; instance of; organization ) ( Arab; instance of;other ) ( Jerusalem; instance of; location ) ( Palestinians; instance of;other ) ( Hakawati Theatre; organization based in; Jerusalem )ADE jer ade: Lethal anuria complicating high dose ifosfamidechemotherapy in a breast cancer patient with an impaired renal func-tion .\n( Lethal anuria; instance of; disease ) ( ifosfamide; instance of; drug )( Lethal anuria; effect; ifosfamide )\nNYT jer nyt: Mary L. Schapiro , who earlier this year became the newhead of NASD , was more amenable to fashioning a deal to the NewYork Exchange ’s liking than her predecessor , Robert R. Glauber .\n( NASD; instance of; organization ) ( Robert R. Glauber; instance of;human ) ( Robert R. Glauber; company; NASD )\nACE2005 jer ace2005: The Davao Medical Center , a regional governmenthospital , recorded 19 deaths with 50 wounded .( Davao Medical Center; instance of; organization ) ( government;instance of; geographical entity ) ( hospital; instance of; organization) ( 50; instance of; human ) ( hospital; part of; government )\nNamed EntityRecognitionCoNLL03 ner conll03: Japan began the defence of their Asian Cup title witha lucky 2-1 win against Syria in a Group C championship match onFriday .\n( Japan; instance of; location ) ( Asian Cup; instance of; miscellaneous) ( Syria; instance of; location )\nOntoNotes ner ontonotes: Relevant departments from Beijing Municipalitypromptly activated emergency contingency plans .( Beijing Municipality; instance of; country city state )\nGENIA ner genia: Human T and B lymphocytes demonstrate an early andtransient hyperpolarization after ligand binding .( Human T and B lymphocytes; instance of; cell type )\nACE2005 ner ace2005: BEGALA Dr . Palmisano , again , thanks for stayingwith us through the break . ( Dr; instance of; human ) ( Dr . Palmisano; instance of; human ) ( us;instance of; human )\nSemantic RoleLabeling CoNLL05 WSJ srl conll05: But while the New York Stock Exchange did n’t [ fall] apart Friday as the Dow Jones Industrial Average plunged 190.58points – most of it in the final hour – it barely managed to stay thisside of chaos .\n( the New York Stock Exchange; instance of; second argument ) ( n’t;instance of; negation )\nCoNLL05 Brown srl conll05: His father [ tried ] to make the food a topic .( His father; instance of; first argument ) ( to make the food a topic;instance of; second argument )CoNLL12 srl conll12: Dear viewers , the China News program will [ end ]here . ( the China News program; instance of; second argument ) ( will;instance of; modal ) ( here; instance of; location )\nEvent Extraction ACE2005 Trigger ee ace2005 trg: The European Union held a summit in Brussels. ( summit; instance of; meet )ACE2005 Argumentee ace2005 arg: The European Union held a [ summit ] in Brussels.( Brussels; instance of; place )\nCoreference Res-olution CoNLL12 cr conll12: And deterrents does n’t work terribly well when anenemy values your death more than his life .( an enemy; refer to; his )\nDialogue StateTracking MultiWOZ 2.1dst multiwoz: [User]: I am looking for a place to to stay that hascheap price range it should be in a type of hotel. [Agent]: Okay , doyou have a specific area you want to stay in? [User]: No , I just needto make sure it s cheap. Oh, and I need parking. [Agent]: I found 1cheap hotel for you that include parking. Do you like me to book it?[User]: Yes, please. 6 people 3 nights starting on Tuesday.\n([User]; hotel area; not given) ([User]; hotel book day; Tuesday)([User]; hotel book people; 6) ([User]; hotel book stay; 3) ([User];hotel internet; not given) ([User]; hotel name; not given) ([User];hotel parking; yes) ([User]; hotel price range; cheap) ([User]; hotelstars; not given) ([User]; hotel type; hotel)\nIntent Detection ATIS id atis: Please give me a list of all the flights between Dallas andBaltimore and their cost. (intent; is; flight and airfare)\nIntent Detection SNIPS id snips: Play the song little robin redbreast. (intent; is; play music)\nTable 16: Input and output examples for every dataset.\nError type Percentage Input Ground Truth Ours Prediction\nClose Entity 65.3%Locations containing suitable federally owned land were listed as : Fort Wainwright annex ,Fairbanks , Alaska ; (Fort Wainwright annex ; located in ; Fairbanks) (Fort Wainwright annex ; located in ; Alaska)\nTotally Missing 26.4%Judith C. Toth says she returned for a fourth term in Maryland ’s House of Delegates becauseshe couldn ’t find a better job . (House of Delegates ; organization based in ; Maryland)(Judith C. Toth ; lives in ; Maryland)\nWrong Relation 4.2%After buying the shawl for $1 , 600 , Darryl Breniser of Blue Ball , said the approximately 2-by-5foot shawl was worth the money . (Darryl Breniser ; lives in ; Blue Ball) (Darryl Breniser ; works for ; Blue Ball)\nDifferent Focus 1.7%An architect of President Nixon ’s unsuccessful executive-privilege Watergate defense is a topprospect for the post of U.S. solicitor in the new Bush administration .( Bush ; lives in ; U.S. ) ( Nixon ; lives in ; U.S. )\nTable 17: Analysis of recall errors of DEEP STRUCT on CoNLL04 joint entity and relation extraction task. For each error type,\nwe list the percentage of missing triples caused by this particular type of error, and an example of this type of error taken from\nthe CoNLL04 dataset.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8793849945068359
    },
    {
      "name": "Coreference",
      "score": 0.7473520636558533
    },
    {
      "name": "Natural language processing",
      "score": 0.7100026607513428
    },
    {
      "name": "Relationship extraction",
      "score": 0.7014798521995544
    },
    {
      "name": "Task (project management)",
      "score": 0.6768971681594849
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6743741035461426
    },
    {
      "name": "Language model",
      "score": 0.6056145429611206
    },
    {
      "name": "Code (set theory)",
      "score": 0.5009756088256836
    },
    {
      "name": "Event (particle physics)",
      "score": 0.49666935205459595
    },
    {
      "name": "Information extraction",
      "score": 0.48818498849868774
    },
    {
      "name": "Relation (database)",
      "score": 0.42636987566947937
    },
    {
      "name": "Task analysis",
      "score": 0.4226202964782715
    },
    {
      "name": "Resolution (logic)",
      "score": 0.3563239574432373
    },
    {
      "name": "Data mining",
      "score": 0.1747831404209137
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 54
}