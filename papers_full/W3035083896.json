{
    "title": "Towards Fully 8-bit Integer Inference for the Transformer Model",
    "url": "https://openalex.org/W3035083896",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2098738808",
            "name": "Ye Lin",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2148565904",
            "name": "Yanyang Li",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2970016700",
            "name": "Tengbo Liu",
            "affiliations": [
                "Northeastern University",
                "Institute of Psychology, Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A1983914940",
            "name": "Tong Xiao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2163293007",
            "name": "Tongran Liu",
            "affiliations": [
                "Northeastern University",
                "Institute of Psychology, Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2496766346",
            "name": "Jingbo Zhu",
            "affiliations": [
                "Northeastern University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6754905691",
        "https://openalex.org/W2947946877",
        "https://openalex.org/W2803739089",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2793416812",
        "https://openalex.org/W2777406049",
        "https://openalex.org/W6803771590",
        "https://openalex.org/W2997751980",
        "https://openalex.org/W2981910001",
        "https://openalex.org/W2797162333",
        "https://openalex.org/W2948223045",
        "https://openalex.org/W2604319603",
        "https://openalex.org/W6864014924",
        "https://openalex.org/W2948798935",
        "https://openalex.org/W6795876147",
        "https://openalex.org/W2955646770",
        "https://openalex.org/W2963122961",
        "https://openalex.org/W2899063892",
        "https://openalex.org/W2763421725",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W4288337707",
        "https://openalex.org/W3164612223",
        "https://openalex.org/W2965046076",
        "https://openalex.org/W4295262505",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2962944188",
        "https://openalex.org/W2963809228"
    ],
    "abstract": "8-bit integer inference, as a promising direction in reducing both the latency and storage of deep neural networks, has made great progress recently. On the other hand, previous systems still rely on 32-bit floating point for certain functions in complex models (e.g., Softmax in Transformer), and make heavy use of quantization and de-quantization. In this work, we show that after a principled modification on the Transformer architecture, dubbed Integer Transformer, an (almost) fully 8-bit integer inference algorithm Scale Propagation could be derived. De-quantization is adopted when necessary, which makes the network more efficient. Our experiments on WMT16 En&lt;-&gt;Ro, WMT14 En&lt;-&gt;De and En-&gt;Fr translation tasks as well as the WikiText-103 language modelling task show that the fully 8-bit Transformer system achieves comparable performance with the floating point baseline but requires nearly 4x less memory footprint.",
    "full_text": "Towards Fully 8-bit Integer Inference for the Transformer Model\nYe Lin1\u0003, Yanyang Li1\u0003, Tengbo Liu1 , Tong Xiao1;2y, Tongran Liu3 and Jingbo Zhu1;2\n1Natural Language Processing Lab., Northeastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\n3CAS Key Laboratory of Behavioral Science, Institute of Psychology, CAS, Beijing, China\nflinye2015, blamedrleeg@outlook.com, tengboliu@stumail.neu.edu.cn,\nfxiaotong, zhujingbog@mail.neu.edu.cn, liutr@psych.ac.cn\nAbstract\n8-bit integer inference, as a promising direction in\nreducing both the latency and storage of deep neu-\nral networks, has made great progress recently. On\nthe other hand, previous systems still rely on 32-\nbit ﬂoating point for certain functions in complex\nmodels (e.g., Softmax in Transformer), and make\nheavy use of quantization and de-quantization. In\nthis work, we show that after a principled mod-\niﬁcation on the Transformer architecture, dubbed\nInteger Transformer, an (almost) fully 8-bit inte-\nger inference algorithm Scale Propagation could\nbe derived. De-quantization is adopted when nec-\nessary, which makes the network more efﬁcient.\nOur experiments on WMT16 En$Ro, WMT14\nEn$De and En!Fr translation tasks as well as the\nWikiText-103 language modelling task show that\nthe fully 8-bit Transformer system achieves compa-\nrable performance with the ﬂoating point baseline\nbut requires nearly 4\u0002less memory footprint.\n1 Introduction\nIn recent years, the self-attention-based Transformer model\n[Vaswani et al., 2017 ] has shown promising improvements\nin a wide variety of tasks, e.g., machine translation [Li et\nal., 2020] and language modelling [Baevski and Auli, 2019].\nThe superior performance of these systems is mostly achieved\nby using very large neural networks, which are accompa-\nnied by the great demands on computation, storage and en-\nergy [Strubell et al., 2019]. As a side effect, deploying such\nmodels on small devices is challenging as they have limited\nstorage space and computation power. For example, practical\nsystems often run on CPUs where the 32-bit ﬂoating point\ncomputation capability is much lower than that of GPUs.\nOne appealing solution to these issues is to reduce the nu-\nmerical precision used in the model at hand [Hubara et al.,\n2016; Micikevicius et al., 2018 ], where both the parameters\nand the activations are represented with fewer bits. For in-\nstance, employing 8-bit integer (INT8) potentially consumes\n4\u0002less storage space but is up to 6\u0002 faster [Quinn and\n\u0003Authors contributed equally.\nyCorresponding author.\nfx; sg OP1\n\b\nx0; s0\t\nOP2\n\b\nx00; s00\t\n(a) Ideal INT8 Inference\nr Q() OP1 D() r0 Q() OP2 D() r00\n(b) Practical INT8 Inference\nFigure 1: Ideal vs. Practical INT8 inference (OP: operation).\nBallesteros, 2018]. Beyond this, INT8 is 10\u0002 more energy\nefﬁcient [Johnson, 2018] and saves much less chip area than\nthe commonly used 32-bit ﬂoating point (FP32) in hardware\ndesign [Sze et al., 2017 ]. Also, the low-precision approach\nis orthogonal to other existing compression and acceleration\nmethods, e.g., efﬁcient network design [Xiao et al., 2019].\nIn general, we need two additional components to adapt\nFP32 algorithms to INT8 algorithms: quantization and de-\nquantization [Gong et al., 2018]. Quantization can be seen as\na function that transforms a rational tensor r into an integer\ntensor xwith the scale s[Wu, 2020]:\nQ(r;s) = bs\u0001re (1)\nwhere b\u0001erepresents rounding to the nearest integer. As a re-\nverse process, de-quantization approximates the rational ten-\nsor rwith its quantized form x:\nD(x;s) = x=s (2)\nIdeally, the INT8-based inference process is as follow: the\nrational input (FP32) tensor r is ﬁrst quantized to an INT8\ntensor xwith the scale s. Then all succeeding operations are\nperformed on INT8 tensors and corresponding scales simul-\ntaneously. De-quantization is employed at the end of the pro-\ncess or there appears an overﬂow1.\nThis method is efﬁcient because quantization and de-\nquantization functions are used only when necessary. Unfor-\ntunately, previous INT8-based models are much more expen-\nsive, as every operation in it is sandwiched between a pair of\nquantization and de-quantization (see Fig. 1). The heavy use\nof quantization and de-quantization blocks the efﬁcient ﬂow\n1For intermediate tensors produced by these operations, we per-\nform de-quantization and quantization with s = 2p\u00001\nmax(jrj) immedi-\nately if the overﬂow happens. The bit-precision p is 7 for INT8.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3759\nof INT8 throughout the network, and somehow prevents fully\n8-bit integer models. The problem lies in two facts:\n\u000fScale Incompatibility: INT8 tensors with different\nscales are incomparable because we cannot use the same\nFP32-to-INT8 mapping to process them in a single op-\neration. For example, let x1 and x2 be INT8 tensors that\nare quantized from FP32 tensors r1 and r2 with differ-\nence scales s1 and s2. Adding x1 and x2 is obviously\nproblematic because x1 + x2 is not the INT8 form of\nr1 + r2, i.e., r1 + r2 6= (x1 + x2)=s1 6= (x1 + x2)=s2.\n\u000fINT8 Incompatibility: some functions in complex net-\nworks are not INT8 friendly and we have to resort to\nFP32 computation in this case. The most representative\nexamples are the exponential function in the attention\nmechanism and the square root function in layer nor-\nmalization [Vaswani et al., 2017].\nIn this work, we take a further step towards fully INT8-\nbased transformer models. We choose Transformer for study\nbecause it is one of the most popular models in natural lan-\nguage processing. We present Scale Propagation, which\nbounds INT8 tensors with associated scales, and propagates\nthem throughout the network during inference. It addresses\nthe scale incompatibility issue by matching the input scales\nif necessary, allowing each operation to manipulate the INT8\ntensor and its scale simultaneously. Moreover, we propose\nInteger Transformer in responding to the INT8 incompati-\nbility issue. To make full use of INT8 in Transformer, we\nreplace the exponential function in the standard attention by\nthe polynomial function, and replace the square root func-\ntion in the layer normalization with the absolute value func-\ntion. Our extensive experiments on several machine transla-\ntion and language modelling tasks show that integer Trans-\nformer achieves competitive INT8 performance with approx-\nimately 4\u0002less storage and 3.47\u0002speed-up on average.\n2 Background: Transformer\nWe start with the description of Transformer. Transformer\n[Vaswani et al., 2017] is mainly composed of a stack of lay-\ners. Each layer consists of a self-attention and a feed-forward\nnetwork. The self-attention takes three tensors, Q, K and V,\nas inputs and produces a tensor with the same size as the out-\nput. It is formulated as:\nAttention(Q;K;V ) = SoftMax(QKT\npdm\n)V (3)\nwhere dm is the dimension of the hidden representation.\nSoftMax is a function that casts its input to a distribution:\nSoftMax(xi) = exi\nP\nj exj\n(4)\nThe feed-forward network is built on top of two linear pro-\njections with the ReLU activation in between:\nFFN(x) = ReLU( xW1 + b1)W2 + b2 (5)\nReLU(x) = max(0 ;x) (6)\nThese modules are coupled with the residual connection\n[He et al., 2016 ], i.e., y = f(x) + x where f is either the\nr\nd2\nd1\ns\n1\nd1\n2p\u00001\nmax(jrj)\n(a) Scale\nInitialization\ns1\nd2\ns2\nd1\n1\n\u0016s1\u0016s2\nd1\nd1\nM(x; s;\nd2)\n\u0016s1 \u0002\u0016sT\n2\n(b) Scale\nMatMul (along d2)\nFigure 2: Examples of initializing scale and multiplying scale in\nMatMul.\nself-attention or the feed-forward network. The Layer Nor-\nmalization is after the residual connection:\nLN(x) = g\f( x\u0000\u0016p\n\u001b2 + \"\n) + b (7)\nwhere \u0016and \u001bare the mean and variance of xalong the hid-\nden dimension, and \"is a ﬁxed small number to prevent di-\nviding 0. g and b are two learnable parameters. For more\ndetails, we refer the reader to [Vaswani et al., 2017].\n3 Scale Propagation\n3.1 Bounding Tensors & Scales\nAs discussed in Section 1, the necessity of de-quantization\ncomes from the fact that input INT8 tensors might not be pro-\nduced by the same mapping that converts FP32 to INT8, e.g.,\nnot multiplied by the same scale in our case, and therefore\nforces us to compute the correct result by rolling back to the\nFP32 mode.\nInspired by this fact, the ideal INT8-based inference should\npropagate not only the tensor but also the mapping. If mul-\ntiple INT8 tensors are inputted, unifying their mappings is\nnecessary so that we can perform the succeeding operation\non INT8 tensors directly. In our case, the mapping is deﬁned\nas the scale that indicates to what extent the current INT8 ten-\nsor deviates from its FP32 counterpart. This scale can be ob-\ntained through s = 2p\u00001\nmax(jrj) initially, where ris the network\ninput. In practice, max(jrj) is performed along the hidden\ndimension, producing a scale with the same shape as r ex-\ncept the dimension that the maximization operates is 1. Fig.\n2(a) shows how to initialize the scale from an FP32 tensor.\nAlthough we introduce extra operations to manipulate FP32\nscales, it is cheap to maintain them and the cost is negligible.\n3.2 Manipulating Tensors & Scales\nExtending the common FP32 operations to INT8 tensors and\nassociated scales is non-trivial. Two questions naturally arise:\n1) how can we calibrate mappings? 2) how to operate both the\nINT8 tensors and scales for a given FP32 operation?\nFor the ﬁrst question, we note that the mapping here is a\nscale that gets multiplied in the quantization. Thus having an\nidentical mapping across tensors is as to ﬁnd a unique multi-\nplier for every input tensor in our case. For each input tensor\nxi with the scale si, we do Scale Matching:\nM(xi;si) = fxi=dsi=\u0016se;\u0016sg (8)\nwhere \u0016s = min( s1;\u0001\u0001\u0001 ;sn). Choosing the minimum of\nscales \u0016sas the unique multiplier guarantees that the result of\nEq. 8 does not overﬂow.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3760\nFP32 OP INT8 Equivalent\n[r1;r2] f[x1;x2] ;[s1;s2]g\nrT\n1 fxT\n1 ;sT\n1 g\nr1 \u0001r2 fx1 \u0001x2;s1 \u0001s2g\nr1 + r2 fxi;\u0016sg= M(xi;si);i 2f1; 2g\nfx1 + x2;\u0016sg;\u0016s2Rm\u0002n\nMatMul(r1;rT\n2 ) fxi;\u0016sig= M(xi;si;d2);i 2f1; 2g\nfx1 \u0002xT\n2 ;\u0016s1 \u0002\u0016sT\n2 g;\u0016si 2Rm\u00021\nrn\n1 fxn\n1 ;sn\n1 g\njr1j\nfjx1j;s1g\nReLU(r1) fReLU(x1);s1g\nTable 1: FP32 operations in INT8. ri = D(xi; si), i 2 f1; 2g,\nri 2Rm\u0002n, xi 2Zm\u0002n, si 2Rm\u0002n. [] denotes the concatenation.\n\u0001denotes the element-wise multiplication.\nAlgorithm 1SCALE PROPAGATION PROTOCOL\nInput: Operation OP; INT8 Tensors x1:::n; Scales s1:::n\nOutput: INT8 Tensor x; Scale s\n1: fx;sg= OP(fx1:::n;s1:::ng) fStore xin INT32g\n2: if x> 2p \u00001 then\n3: fx;sg= R(x;s) fRe-scalingg\n4: end if\n5: Convert (INT32) xto INT8\n6: return x;s\nHaving the scale matching, it is handy to induce the INT8\nform for any FP32 tensor operation, as shown in Table 1.\nFor tensor shape transformations, such as concatenation and\ntranspose, the same transformation is applied to the INT8 ten-\nsor and its scale simultaneously, since they do not change the\nvalues. For element-wise multiplication, we multiply tensors\nand scales independently, as the quantization is just another\nelement-wise multiplication. For addition, we ﬁrst match the\ninput scales via Eq. 8, then add tensors as usual.\nHandling matrix multiplication (MatMul) is more sophis-\nticated. MatMul is an element-wise multiplication with an\naddition along the last dimension. We therefore ﬁrst match\nthe input scales along that dimension, then perform MatMul\nto the tensors and scales independently. To match the input\nscale along a speciﬁc dimension, we employ the same idea\nof scale matching by treating it as matching scales of multi-\nple sub-tensors splitted from that dimension. It is denoted as\nM(x;s;d ), where xis the INT8 tensor, sis its scale and dis\nthe dimension that we would like to match scales. Fig. 2(b)\nshows an example of how MatMul works on scales, which\nmatches the scales on the dimension d2 and multiplies them.\nFor element-wise non-linear functions, we assume that\nthey satisfy the distribution law, i.e., OP(r) = OP( x=s) =\nOP(x)=OP(s). Then, we have:\nOP(fx;sg) = fOP(x);OP(s)g (9)\nwhere xis the INT8 tensor and sis its scale. This assump-\ntion holds for the polynomial function rn where nis a ﬁxed\ninteger, since rn = (x=s)n = xn=sn. It also holds for the ab-\nsolute value function, because jrj= jx=sj= jxj=sas s >0.\nfx1;s1g\nD()\nLN\nQ()\nN\nReLU\nD()\nQ()\nN\nD()\nL\nQ()\nfx12;s12g\nQ()W1\nQ()W2\nD()\nr2\nr3\nfx4; s4g\nfx5; s5g\nfx6; s6g\nr7\nfx8; s8g\nfx9; s9g\nr10\nr1\nr11\n(a) Canonical Inference\nfx1;s1g\nLN\nR()\nN\nReLU\nR()\nN\nL\nfx9;s9g\nQ()W1\nQ()W2\nScale Matching\nfx2; s2g\nfx3; s3g\nfx4; s4g\nfx5; s5g\nfx6; s6g\nfx7; s7g\nfx8; s8g\nfx1; s1g\n(b) Scale Propagation\nFigure 3: The comparison of INT8 inferences in the FFN layer.\nThe same is for ReLU(fx;sg) when entries of x have the\nmaximum value(s) exceeded 0, which is always true other-\nwise it will face the ‘dying ReLU’ problem[He et al., 2015].\n3.3 The General Protocol\nNote that addition and multiplication operations may pro-\nduce results that are out of the INT8 range. These results\nare thereby stored in data types with more bits in practical\nimplementations, e.g., INT32. We need to project the result\nback to INT8 before the succeeding operations. We call it\nRe-scaling:\nR(x;s) = fx=^s;s=^sg (10)\nwhere ^s =\nl\nmax(jxj)\n2p\u00001\nm\n. The protocol of extending an FP32\noperation to INT8 tensors and their scales is summarized in\nAlg. 1: we directly apply the INT8 form of this operation\nto update fx;sg, and then use re-scaling to project xback to\nINT8 if necessary. Once this protocol for INT8 operations\nis deﬁned, the routine for the INT8 forward propagation is\nas straightforward as the FP32 one, except that FP32 opera-\ntions are replaced by their INT8 equivalents. This gives us\nthe Scale Propagation. As shown in Fig. 3, scale propagation\ngets rid of de-quantization and only INT8 tensors are propa-\ngated in the whole forward propagation.\n4 Integer Transformer\n4.1 Polynomial Attention\nApplying scale propagation to the Transformer model is not\nimmediately available. As discussed in the previous section,\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3761\nV\nK\nQ\nx= QKT\npdm\nx= Poly(x) x= P\nj xj\ny= xV y= y=x\ndm\n1dm\n4 0 0\n1\n1\ndm\n4 dm\n1\ndm\n1\nFigure 4: A running example of Polynomial Attention, where Q 2R1\u0002dm, K; V2R4\u0002dm. Different colors indicate different shapes.\nscale propagation assumes the element-wise functions sat-\nisfy the distribution law, which is not held for the exponen-\ntial function ex in the SoftMax of the attention functions, as\nex=s 6= ex=es. Besides, the exponential function does not\nproduce an integer output given an integer input.\nTo enable scale propagation, we choose ReLU as an al-\nternative of the exponential function here, since it not only\nproduces positive results as the exponential function but also\nis compatible with INT8. A bias term is added in advance to\nrule out entries that are below a learnt threshold.\nOne downside of ReLU is its linear nature. The exponen-\ntial function has the property that larger input values become\nmore signiﬁcant after the transformation, as its gradient ex is\nlarger than 1 in the positive number ﬁeld. To achieve a sim-\nilar effect, we introduce the polynomial function xn, whose\ngradient n\u0001xn\u00001 is exponential while it always produces in-\nteger outputs given integer inputs. Note that we only apply\nthis polynomial function after ReLU, since an even degree n\nwill mess up with the order of scores: a large negative number\nwill be ranked in front instead of behind.\nPutting all these pieces together, we have:\nPoly(x) = [ReLU(x+ b)]n + j\u000ej (11)\nwhere b is the bias term, n is the degree of the polynomial\nfunction and \u000eis another learnable parameter. j\u000ejensures that\nthe worst case of the attention, i.e., producing all 0 results, is\na simple average instead of nothing.\nLastly, we multiplyPoly(x) with V and then divide the re-\nsult by P\nj Poly(xj), otherwise the integer division will incur\nall 0 results because Poly(xi) \u0014P\nj Poly(xj):\nPolyAttn(Q;K;V ) =\nPoly(QKT\npdm\n)V\nP\nj Poly(\nQKT\njpdm\n)\n(12)\nThis way sidesteps the previous issue as the multiplication\nresults are usually not smaller than the sum. We call Eq. 12\nPolynomial Attention. Fig. 4 shows a running example of it.\n4.2 L1 Layer Normalization\nAnother component that hinders Transformer INT8 inference\nis the square root function for computing the standard devia-\ntion inside the layer normalization, which does not guaran-\ntee the integer outputs given the integer inputs. Hoffer et\nal. [2018] proposes L1 Batch Normalization, which approxi-\nmates the standard deviation with its L1-norm equivalent:\nL1LN(x) = g\f( x\u0000\u0016\nC\u0001k x\u0000\u0016k1 =n) + b (13)\nwhere g and bare two parameters, \u0016is the mean of xalong\nthe batch dimension, C =\np\n\u0019=2 and nis the batch size. This\nway replaces the square root function in the L2-norm by the\nabsolute value function in the L1-norm.\nWe extend a similar idea of L1 batch normalization to our\ncase, that we compute the mean \u0016 along the hidden dimen-\nsion instead of the dimension along the batch. We call thisL1\nLayer Normalization. The replacement of layer normaliza-\ntion as well as the attention gives us the Integer Transformer\nthat supports fully INT8 inference.\n5 Experiments\n5.1 Setup\nWe evaluate our methods on three machine translation (MT)\ntasks and a language modelling (LM) task, including the\nWMT16 English-Roman (En$Ro), the WMT14 English-\nGerman (En$De), the WMT14 English-French (En!Fr)\nand the WikiText-103 LM tasks. For En$Ro (610K pairs),\nwe use newsdev-2016 and newstest-2016 as the validation and\ntest sets respectively. For En$De (4.5M pairs), newstest-\n2013 is the validation set and newstest-2014 is the test set.\nFor En!Fr (36M pairs), we validate the system on the com-\nbination of newstest-2012 and newstest-2013, and test it on\nnewstest-2014. We tokenize every sentence using a script\nfrom Moses and segment every word into subword units us-\ning byte-pair encoding. The number of the BPE merge op-\nerations is set to 32K. We report case-sensitive tokenized\nBLEU scores. In addition, the results are the average of three\nidentical runs with different random seeds for En$Ro and\nEn$De. The WikiText-103 dataset contains a training set of\n103 million words. Both the validation and test sets contain\n0.2 million words. For the LM task, we report the perplexity.\nFor the machine translation tasks, we experiment with the\nTransformer-base (base) setting [Wang et al., 2019]. We ad-\nditionally run the Transformer-big (big) setting on En$De\nand En!Fr. Both settings consist of a 6-layer encoder and\na 6-layer decoder. The embedding size is set to 512 for\nTransformer-base and 1,024 for Transformer-big. The num-\nber of heads is 8/16 for Transformer-base/big. The hidden\nsize equals to 4\u0002embedding size in both settings. For train-\ning, we use Adam optimizer with \f1 = 0:9 and \f2 = 0:997.\nWe adopt the inverse square root learning rate schedule with\n8K warmup steps and the learning rate = 0 :001/0:0007 for\nTransformer-base/big.\nFor the language modelling task, we follow the lm-base\nand lm-big architectural choices and training details de-\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3762\nEntry System BLEU Storage Estimated\nSpeed-upFP32 INT8\nbase\nEn!Ro Baseline 32.55 - 318M 1\u0002\nOurs 32.60 32.54 80M 3:53\u0002\nRo!En Baseline 32.85 - 306M 1\u0002\nOurs 33.04 32.95 77M 3:59\u0002\nEn!De Baseline 26.95 - 302M 1\u0002\nOurs 27.08 26.91 76M 3:24\u0002\nDe!En Baseline 32.19 - 302M 1\u0002\nOurs 32.43 32.26 76M 3:31\u0002\nEn!Fr Baseline 40.88 - 425M 1\u0002\nOurs 40.64 40.00 107M 3:03\u0002\nbig\nEn!De Baseline 28.72 - 939M 1\u0002\nOurs 28.93 28.71 236M 3:60\u0002\nDe!En Baseline 33.07 - 939M 1\u0002\nOurs 33.53 33.46 236M 3:68\u0002\nEn!Fr Baseline 42.37 - 1243M 1\u0002\nOurs 42.46 41.59 311M 3:51\u0002\nTable 2: BLEU scores [%], storage (megabytes) and speed-up.\nEntry valid test Storage Estimated\nSpeed-upFP32 INT8 FP32 INT8\nbase\nBaseline 29.61 - 31.18 - 596M 1\u0002\nOurs 29.49 30.28 30.79 31.61 150M 3:43\u0002\nbig\nBaseline 18.22 - 18.86 - 944M 1\u0002\nOurs 17.49 17.55 18.16 18.23 280M 3:78\u0002\nTable 3: WikiText-103 PPL, storage (megabytes) and speed-up.\nscribed in [Baevski and Auli, 2019 ]. The embedding size\nis 512 for lm-base and 1024 for lm-big. The hidden size\nequals to 4\u0002embedding size. The number of heads is 8 for\nboth lm-base and lm-big. The number of layers is set to 6/16\nfor lm-base/big. For the lm-base model, we train it with the\nsame setting as in the machine translation tasks. As for the\nlm-big training, we use the Nesterov’s accelerated gradient.\nWe adopt the cosine learning rate schedule with 16K warmup\nsteps and the maximum learning rate 1. All experiments are\nrun on 8 NVIDIA TITAN V GPUs.\n5.2 Results\nTable 2 summarizes the results on various translation tasks.\nCompared to the vanilla Transformer, integer Transformer\nobtains competitive or even superior FP32 performance by\n0.1\u00180.4 BLEU points in either the base or big setup. When\ninteger Transformer is decoded with INT8, it shows only\nabout a decrease of 0.3 BLEU points on average except in\nEn!Fr, where it underperforms the baseline by more than 1\nBLEU point. In Section 5.3, we will show that it is mainly\ndue to the last residual connection and layer normalization,\nwhich suffer from greater loss with lower bits representations.\nExperiments on the WikiText-103 language modelling task in\nTable 3 show a similar trend as those in machine translation\ntasks, where integer Transformer beats the baseline with the\nsame setup as in MT.\nBoth Table 2 and Table 3 show that using INT8 indeed\nsaves nearly 4\u0002storage space. Since we need to store both\nSystem BLEU PPL\nEn!Ro En!De En!Fr valid test\nBaseline 32.55 26.95 40.99 29.58 31.28\n+Poly 32.56 27.13 40.90 29.54 31.20\n+L1LN 32.55 26.94 40.67 29.61 31.18\nTable 4: The ablation study of Integer Transformer.\nSystem BLEU PPL\nEn!Ro En!De En!Fr valid test\nOurs (FP32) 32.60 27.08 40.99 29.49 30.79\n+BScale 32.54 21.84 39.45 30.88 32.25\n+B\u0002T Scale 32.54 26.91 40.00 30.28 31.61\nTable 5: INT8 performance vs. different sized scales.\nthe parameters and their scales, we are unable to reach exactly\n4\u0002less storage. Employing INT8 also runs about 3.5\u0002faster\non average. Note that we estimate this speed-up by collecting\nthe time consumption of each operation and their correspond-\ning speed-up (6\u0002) in INT8, as modern CPUs have limited\nsupports of INT8 arithmetics, e.g., MatMul only. We ﬁnd that\nthis speed-up is more obvious if the output sequence is longer,\ne.g., translations in Ro!En is longer than those in En !Fr\nand thus higher speed-up in Ro!En is observed. This phe-\nnomenon arises from the fact that operations that beneﬁt from\nINT8 such as MatMul occupy a higher portion when generat-\ning long sequences, while other ﬁxed time operations such as\ndata preparation become marginal.\n5.3 Analysis\nWe show an ablation study of integer Transformer in Table 4.\nWe can see that replacing the standard attention by polyno-\nmial attention generally improve the FP32 result and L1 layer\nnormalization has the close performance to standard layer\nnormalization. These observations imply that either the poly-\nnomial attention or the L1 layer normalization is a good al-\nternative to its counterpart in the baseline transformer model.\nSection 3.1 has described how to obtain the initial scale\nby taking the maximum of the hidden dimension in the FP32\ninput. This method can be extended to the case of multiple\ndimensions. In Table 5, we test it on maximizing on T \u0002C\nand C given the input of the size T \u0002B \u0002C, resulting a\nsized B and B \u0002T scale respectively. Here T is the input\nsequence length, B is the batch size and C is the number of\nthe hidden units. The results reveal that using a scale with\nmore entries better preserves the performance, yet the one\nwith fewer entries lowers the computation budget.\nAlso, we plot how hyper-parameters relate to performance.\nWe can see from the left of Fig. 5 that n> 1 results in much\nbetter performance than n= 1 in all tasks, indicating the ne-\ncessity of non-linearity in the attention. But higherndoes not\nnecessarily lead to better results, where n = 3 performs the\nbest in En!De and WikiText-103. The right of Fig. 5 shows\nthat adding a few bits can recover most of the performance,\nespecially for those suffer from great loss in INT8 inference,\ne.g., En!Fr. Moreover, we observe that the performance of\nEn!Ro decreases slightly with 6 bit, which suggests that fur-\nther speed-up might be available.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3763\nEn-Ro En-De En-Fr WikiText-103\n1 2 3 4 5\n\u00000:7\n\u00000:4\n\u00000:2\n0:1\n0:4\nn\n4BLEU\n\u00000:4\n\u00000:2\n0:1\n0:4\n0:6\n6 7 8 9 10\n\u000012\n\u000010\n\u00007\n\u00005\n\u00002\n0\n#Bits\n\u000050\n\u000040\n\u000030\n\u000020\n\u000010\n0\n4PPL\nFigure 5:\nSensitivity analysis (n: the degree of the polynomial func-\ntion; #Bits: the number of bits used in the inference).\nWe next investigate which factor has a signiﬁcant impact\non the performance by presenting details on which module is\nresponsible for the INT8 performance loss. As can be seen in\nFig. 6, if the performance drop is not signiﬁcant, each mod-\nule contribute similarly, otherwise a few modules should be\nblamed for. This fact suggests that poor INT8 performance is\nmainly led by one or two crucial points, e.g., the layer nor-\nmalization and the residual connection in En!Fr.\nAs implied by Fig. 6, we make an in-depth analysis to see\nwhether the high precision loss connects to the poor perfor-\nmance of applying INT8 to the layer normalization and the\nresidual connection in En!Fr. To evaluate the precision loss,\nwe choose the mean square error between the FP32 activa-\ntions and the de-quantized INT8 ones as the proxy. Fig. 7\nshows that there exists a positive relationship between preci-\nsion loss and performance loss, i.e., 4BLEU. Interestingly,\nmost loss occurs in the last layer. Noting that the residual\nconnection is the sum of all outputs of the residual branches\nin previous layers, the last residual connection will produce\nthe result with large values, which might suffer from greater\nprecision loss through the quantization.\n6 Related Work\nEmploying the low precision data type for neural networks\nto accelerate the network computation or save storage space\nhas a long history. Early work has shown that training and\ninference with the ternary (2-bit) or even binary (1-bit) net-\nwork is possible [Hubara et al., 2016]. But these results have\nrestricted to simple architectures, such as the feed-forward\nnetworks. Recent work mainly focuses on training a sophis-\nticated network with higher precision, such as 32-bit (FP32)\nand 16-bit ﬂoating point (FP16) [Micikevicius et al., 2018 ]\nbut attempts to inference with fewer bits, such as 8-bit ﬁxed\npoint (INT8) [Jacob et al., 2018 ]. However, most of them\nhave limited to computer vision and only a few of them dis-\ncuss how to leverage low precision to infer the complicate\nTransformer model in natural language processing.\nBhandare et al. [2019] ﬁrst demonstrates that Transformer\ncan be inferred with INT8. But some operations are still per-\nformed in FP32 and its INT8 performance is not evaluated\nby common metrics, e.g., BLEU. Though more recent work\n[Prato et al., 2019; Wu, 2020 ] share the same limitation of\npartially relying on FP32, they report better INT8 results by\n4BLEU/-4PPL 0\n0:2\n0:4\nEmb Attn FFN LN Res Proj\n\u00001:2\n\u00001\n\u00000:8\nEn!Ro En!De\nEn!Fr WikiText-103\nFigure 6: Performance improvement (> 0) and loss (< 0) of ap-\nplying INT8 to modules (Emb: the embedding; Attn: the attention;\nFFN: the feed-forward network; LN: the layer normalization; Res:\nthe residual connection; Proj: the output projection.)\n1 2 3 4 5 6 Out\n\u00000:8\n\u00000:6\n\u00000:4\n\u00000:2\n0\n0:2\nDecoder Layer (LN)\n4BLEU\n0\n0:2\n0:4\n0:6\n0:8\n1\n4BLEU Loss\n1 2 3 4 5 6\n\u00000:8\n\u00000:6\n\u00000:4\n\u00000:2\n0\n0:2\nDecoder Layer (Res)\n0\n7\n14\n21\n28\n35\nPrecision Loss\n4BLEU Loss\nFigure 7:\nPrecision loss vs. Performance loss (En!Fr, Out: the last\nlayer normalization before the output projection).\ntailoring the training as well as the quantization method to\nthe Transformer model. This work, on the other hand, takes a\nstep toward fully INT8 inference without any FP32 operation\nfor the Transformer model. The forward propagation ﬂows\npurely on INT8 and shows competitive performance without\nmodifying the training process.\n7 Conclusion\nIn this work, we present an (almost) fully INT8 inference al-\ngorithm Scale Propagation, which propagates the INT8 ten-\nsor and its scale to resolve the scale incompatibility prob-\nlem. Moreover, we propose Integer Transformer to address\nthe INT8 incompatibility issue in the Transformer model,\nwhich replaces the exponential function and the square root\nfunction by the polynomial function and the absolute value\nfunction respectively. Our experiments show that our method\nachieves competitive INT8 performance in machine transla-\ntion and language modelling tasks.\nAcknowledgments\nThis work was supported in part by the National Science\nFoundation of China (Nos. 61876035 and 61732005), the\nNational Key R&D Program of China (No. 2019QY1801)\nand the Opening Project of Beijing Key Laboratory of Inter-\nnet Culture and Digital Dissemination Research. The authors\nwould like to thank anonymous reviewers for their comments.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3764\nReferences\n[Baevski and Auli, 2019] Alexei Baevski and Michael Auli.\nAdaptive input representations for neural language model-\ning. In 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019, 2019.\n[Bhandare et al., 2019] Aishwarya Bhandare, Vamsi Sri-\npathi, Deepthi Karkada, Vivek Menon, Sun Choi, Kushal\nDatta, and Vikram Saletore. Efﬁcient 8-bit quantization\nof transformer neural machine language translation model.\nCoRR, abs/1906.00532, 2019.\n[Gong et al., 2018] Jiong Gong, Haihao Shen, Guoming\nZhang, Xiaoli Liu, Shane Li, Ge Jin, Niharika Mahesh-\nwari, Evarist Fomenko, and Eden Segal. Highly efﬁ-\ncient 8-bit low precision inference of convolutional neu-\nral networks with intelcaffe. In Proceedings of the\n1st on Reproducible Quality-Efﬁcient Systems Tourna-\nment on Co-designing Pareto-efﬁcient Deep Learning, Re-\nQuEST@ASPLOS 2018, Williamsburg, VA, USA, March\n24, 2018, page 2, 2018.\n[He et al., 2015] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Delving deep into rectiﬁers: Surpass-\ning human-level performance on imagenet classiﬁcation.\nIn 2015 IEEE International Conference on Computer Vi-\nsion, ICCV 2015, Santiago, Chile, December 7-13, 2015,\npages 1026–1034, 2015.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 770–778, 2016.\n[Hoffer et al., 2018] Elad Hoffer, Ron Banner, Itay Golan,\nand Daniel Soudry. Norm matters: efﬁcient and accurate\nnormalization schemes in deep networks. In Advances in\nNeural Information Processing Systems 31: Annual Con-\nference on Neural Information Processing Systems 2018,\nNeurIPS 2018, 3-8 December 2018, Montr ´eal, Canada,\npages 2164–2174, 2018.\n[Hubara et al., 2016] Itay Hubara, Matthieu Courbariaux,\nDaniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bina-\nrized neural networks. In Advances in Neural Informa-\ntion Processing Systems 29: Annual Conference on Neu-\nral Information Processing Systems 2016, December 5-10,\n2016, Barcelona, Spain, pages 4107–4115, 2016.\n[Jacob et al., 2018] Benoit Jacob, Skirmantas Kligys,\nBo Chen, Menglong Zhu, Matthew Tang, Andrew G.\nHoward, Hartwig Adam, and Dmitry Kalenichenko.\nQuantization and training of neural networks for efﬁcient\ninteger-arithmetic-only inference. In 2018 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018, pages\n2704–2713, 2018.\n[Johnson, 2018] Jeff Johnson. Rethinking ﬂoating point for\ndeep learning. CoRR, abs/1811.01721, 2018.\n[Li et al., 2020] Yanyang Li, Qiang Wang, Tong Xiao, Ton-\ngran Liu, and Jingbo Zhu. Neural machine translation with\njoint representation. In Proceedings of the Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, 2020.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan\nNarang, Jonah Alben, Gregory F. Diamos, Erich Elsen,\nDavid Garc´ıa, Boris Ginsburg, Michael Houston, Oleksii\nKuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track\nProceedings, 2018.\n[Prato et al., 2019] Gabriele Prato, Ella Charlaix, and Mehdi\nRezagholizadeh. Fully quantized transformer for im-\nproved translation. CoRR, abs/1910.10485, 2019.\n[Quinn and Ballesteros, 2018] Jerry Quinn and Miguel\nBallesteros. Pieces of eight: 8-bit neural machine transla-\ntion. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018,\nVolume 3 (Industry Papers), pages 114–120, 2018.\n[Strubell et al., 2019] Emma Strubell, Ananya Ganesh, and\nAndrew McCallum. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th Con-\nference of the Association for Computational Linguistics,\nACL 2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 3645–3650, 2019.\n[Sze et al., 2017] Vivienne Sze, Yu-Hsin Chen, Tien-Ju\nYang, and Joel S. Emer. Efﬁcient processing of deep neu-\nral networks: A tutorial and survey. Proceedings of the\nIEEE, 105(12):2295–2329, 2017.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems 30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017, Long Beach,\nCA, USA, pages 5998–6008, 2017.\n[Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo\nZhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation.\nIn Anna Korhonen, David R. Traum, and Llu ´ıs M`arquez,\neditors, Proceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Papers,\npages 1810–1822. Association for Computational Linguis-\ntics, 2019.\n[Wu, 2020] Ephrem Wu. Learning accurate integer\ntransformer machine-translation models. CoRR,\nabs/2001.00926, 2020.\n[Xiao et al., 2019] Tong Xiao, Yinqiao Li, Jingbo Zhu,\nZhengtao Yu, and Tongran Liu. Sharing attention weights\nfor fast transformer. In Sarit Kraus, editor, Proceedings of\nthe Twenty-Eighth International Joint Conference on Arti-\nﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-\n16, 2019, pages 5292–5298. ijcai.org, 2019.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3765"
}