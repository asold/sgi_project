{
  "title": "ITrans: generative image inpainting with transformers",
  "url": "https://openalex.org/W4390954029",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2043220055",
      "name": "Miao Wei",
      "affiliations": [
        "University of Jyväskylä",
        "Dalian University of Technology",
        "Dalian University"
      ]
    },
    {
      "id": "https://openalex.org/A1538671269",
      "name": "Wang Li-Jun",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2227143521",
      "name": "Lu, Huchuan",
      "affiliations": [
        "Dalian University of Technology",
        "Dalian University"
      ]
    },
    {
      "id": "https://openalex.org/A2360804979",
      "name": "Huang Kai-ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2369240907",
      "name": "Shi Xin-chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134632113",
      "name": "Liu Bo-cong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2187250581",
    "https://openalex.org/W2105038642",
    "https://openalex.org/W3176674719",
    "https://openalex.org/W4240726888",
    "https://openalex.org/W1993120651",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W2738588019",
    "https://openalex.org/W2798365772",
    "https://openalex.org/W2991377405",
    "https://openalex.org/W2964148878",
    "https://openalex.org/W2982763192",
    "https://openalex.org/W6600223405",
    "https://openalex.org/W3109174731",
    "https://openalex.org/W3034419329",
    "https://openalex.org/W6603094881",
    "https://openalex.org/W3175375202",
    "https://openalex.org/W3121667743",
    "https://openalex.org/W3190220420",
    "https://openalex.org/W3034482833",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W6602254124",
    "https://openalex.org/W3199003182",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W6600258949",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6600042225",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6601473709",
    "https://openalex.org/W6600302053",
    "https://openalex.org/W3206082266",
    "https://openalex.org/W3136958399",
    "https://openalex.org/W4312771828",
    "https://openalex.org/W2145023731",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2572730214",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963800363",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963037581",
    "https://openalex.org/W2103559027",
    "https://openalex.org/W2981408470",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963521568",
    "https://openalex.org/W2962785568"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nMultimedia Systems (2024) 30:21 \nhttps://doi.org/10.1007/s00530-023-01211-w\nREGULAR PAPER\nITrans: generative image inpainting with transformers\nWei Miao1,4 · Lijun Wang2 · Huchuan Lu1 · Kaining Huang3 · Xinchu Shi3 · Bocong Liu3\nReceived: 15 June 2023 / Accepted: 8 December 2023 / Published online: 17 January 2024 \n© The Author(s) 2024\nAbstract\nDespite significant improvements, convolutional neural network (CNN) based methods are struggling with handling long-\nrange global image dependencies due to their limited receptive fields, leading to an unsatisfactory inpainting performance \nunder complicated scenarios. To address this issue, we propose the Inpainting Transformer (ITrans) network, which combines \nthe power of both self-attention and convolution operations. The ITrans network augments convolutional encoder–decoder \nstructure with two novel designs, i.e. , the global and local transformers. The global transformer aggregates high-level image \ncontext from the encoder in a global perspective, and propagates the encoded global representation to the decoder in a multi-\nscale manner. Meanwhile, the local transformer is intended to extract low-level image details inside the local neighborhood \nat a reduced computational overhead. By incorporating the above two transformers, ITrans is capable of both global rela-\ntionship modeling and local details encoding, which is essential for hallucinating perceptually realistic images. Extensive \nexperiments demonstrate that the proposed ITrans network outperforms favorably against state-of-the-art inpainting methods \nboth quantitatively and qualitatively.\nKeywords Convolutional neural network · Image inpainting · Global transformer · Local transformer\n1 Introduction\nImage inpainting, also known as image completion, is the \ntask of filling in missing pixels in an image with fine image \ncontent. This task finds applications in various image editing \ndomains, including object removal [1], image restoration [2], \nphoto retouching [3], etc. The solution of image inpainting \nis to understand image structures and perform image syn-\nthesis. Prior to the deep learning era, this subject was mainly \nperformed by using existing image patches to fill in masked \nregions [2, 4, 5]. However, due to the lack of semantic under-\nstanding, these methods have been replaced by deep neural \nnetworks [6 –14] and adversarial learning [15– 19]. Deep \nlearning-based methods treat inpainting as a generation task \nthat involves end-to-end learning using convolutional neural \nnetworks (CNNs). CNNs are known for their remarkable \ncapacity to generate fine details, thanks to their inductive \nbiases [20], including locality and weight sharing, which \nensure them efficient models across domains. Nevertheless, \nthe limited receptive fields of CNNs are insufficient to access \nthe necessary information for generating quality inpaintings \nCommunicated by B. Bao.\n * Wei Miao \n mdd98@mail.dlut.edu.cn\n Lijun Wang \n ljwang@dlut.edu.cn\n Huchuan Lu \n lhchuan@dlut.edu.cn\n Kaining Huang \n huangkaining@meituan.com\n Xinchu Shi \n shixinchu@meituan.com\n Bocong Liu \n liubocong@meituan.com\n1 School of Information and Communication Engineering, \nDalian University of Technology, No. 2 Linggong Road, \nDalian 116023, Liaoning, China\n2 School of Artificial Intelligence, Dalian University \nof Technology, No. 2 Linggong Road, Dalian 116023, \nLiaoning, China\n3 Meituan Group, No. 4 Wangjing East Road, Chaoyang \nDistrict, Beijing 100102, China\n4 Faculty of Information Technology, University of Jyväskylä,  \nSeminaarinkatu 15, 40014 Jyväskylä, Finland\n W. Miao et al.\n21 Page 2 of 12\nunder complex scenarios, leading to unwanted artifacts and \nblurry results. More recently, transformers [21] have dem-\nonstrated record-breaking performance in various computer \nvision tasks with strong capability in modeling long-range \ndependencies. While transformers provide an alternative to \nCNNs, their lack of inductive biases presents a challenge \nfor processing images. Although transformers have a higher \nperformance ceiling than CNNs, their complex costly pre-\ntraining requirements make them more difficult to learn [20]. \nAs a result, using transformers for image inpainting is still \nrelatively uncommon in literature.\nWe propose the Inpainting Transformer (ITrans) network \nto integrate the benefits of both CNNs and transformers. To \nleverage the inductive bias of CNNs, we design a convolu-\ntional encoder–decoder network for feature extraction and \nimage generation, respectively. Additionally, we introduce \na global and local transformer module to enhance the flex-\nibility of transformers. The global transformer module aims \nto achieve high-level perception of the input image from \na holistic view and connects the encoder with decoder via \nskip paths in a multi-scale manner. The local transformer \nmodule is designed to ensure image consistency and enhance \nlocal details at a lower computational cost. Our ITrans net-\nwork incorporates these novel designs, resulting in greater \nrepresentational power than pure CNNs and more efficient \nlearning than pure transformers. Consequently, our approach \nachieves superior performance on various image inpainting \nbenchmarks. Our main contributions are outlined below:\n• We propose ITrans network for image inpainting, which \nbenefits from the built-in inductive biases of CNNs and \nthe strong expressive power transformers. And ITrans is \nthe first approach to train CNN-transformer in an end-to-\nend way.\n• We design the global and local transformer module, \nwhich learn to capture image context from multiple per-\nspectives and significantly improve image generation of \nmissing regions.\n• Our method sets new state-of-the-art performance on \nvarious benchmark datasets. Extensive experiments also \nverify the major insight of our ITrans network.\nTo the best of our knowledge, we are among the first to \ninvestigate hybrid architectures for image inpainting by \nmerging CNNs and transformers in an end-to-end scheme. \nOur source code and models will be made available upon \nacceptance.\n2  Related work\n2.1  Image inpainting\nDeep generative networks for image inpainting Tradi-\ntional patch-based inpainting methods typically rely \non propagating images from remaining areas or other \nsources. For instance, in [2 ], redundant image patches \nwere employed to determine the priority of each pixel \nbased on gradient variation. Pixels sharing greater simi-\nlarity with missing pixels were used to fill in the areas. \nHays et al. [4 ] searched through numerous image patches \non the Internet to locate a suitable patch to fill missing \nareas. Another typical patch-based technique is patch \nmatching [5 ], which looked for identical patches in vari-\nous source pictures. This method split the image into \nsmall patches and selected the most comparable one to \nfill in the holes. Although these traditional methods work \nwell for small, tiny holes, and homogeneous background \nregions, they lack the crucial generating capacity needed \nto handle massive missing regions.\nPathak et al. [6 ] proposed a deep-learning approach \ncalled context encoders for image inpainting tasks, which \nis the pilot study of this area. Built on an encoder–decoder \narchitecture, the encoder extracts low-resolution features \nfrom the corrupted image, and the decoder enlarges and \nreconstructs the image. However, the approach often \nresults in visual artifacts and blurriness in the recovery \nregions. To address this issue, Iizuka et al. [7 ] reduced the \nnumber of downsampling layers, and [22] included dilated \nconvolution layers in the bottleneck. Meanwhile, recent \nwork LaMa [23] employed Fourier convolutions to enlarge \nthe receptive filed and inductive bias. The U-Net structure \n[24] is widely applied to extract low-level features well-\nreserved in encoder layers. Liu et al. [8 ] introduced partial \nconvolution to prevent the feature maps from capturing \ntoo many zeros, thereby smoothing the output image by \nfiltering redundant zeros while traversing over missing \nregions. Additionally, Yu et al. [11] implemented gated \nconvolution in both encoder and decoder layers, which \nlearns a dynamic feature selection mechanism for channel-\nwise spatial placement across all layers, improving color \nconsistency and inpainting quality on free-form masks.\nAttention mechanism Attention mechanisms have \nrecently been applied to improve inpainting tasks. Yu et al. \n[25] first introduced contextual attention, demonstrating \nthe attention process with dilated convolutions. The con-\ntextual attention model operates in two stages. The first \nstage generates a coarse inpainting result, while the second \nstage refines the image using patch-similarity-based con-\ntextual attention. Kim et al. [26] made further progress by \nintroducing the texture transform attention (TTA) module. \nITrans: generative image inpainting with transformers  \n Page 3 of 12 21\nWith the TTA module, high-level features are reassembled \nfrom low-level features and sent to the decoder, improving \nthe inclusion of texture information in the reconstructed \nregions. Transformers are all about attention, and the \ninclusion of an attention mechanism reminds us of the \npotential of transformers in image inpainting.\n2.2  Vision transformers\nTransformers Attention-based models, particularly \ntransformers [ 21] have emerged as the de facto stand-\nard approach in natural language processing (NLP) [27]. \nTransformers also show great performance in computer \nvision fields. Vision transformer (ViT) [28] is a convo-\nlution-free transformer that outperforms previous CNN-\nbased models [29] in image recognition tasks. ViT pro-\ncesses images as a sequence of 16 × 16 words, allowing \nfor robust representation. The transformer architecture’s \neffectiveness in ViT has been demonstrated through pre-\ntraining on massive datasets. Subsequently, DeiT [ 30] \nadapts ViT for better sample efficiency through an inno-\nvative knowledge distillation technique. ViT has also been \napplied to other computer vision tasks, such as object \ndetection [31, 32], and semantic segmentation [33– 35].\nTransformers in inpainting Image inpainting can be \nconsidered as a form of image generation task, and there \nare various approaches that use transformers and convolu-\ntion layers. Parmer et al. [36] first suggested that image \ngeneration be viewed as autoregressive sequence genera-\ntion using a transformer architecture. Generative models \nsuch as [36, 37] employ autoregressive learning and GPT-\n3-based techniques [27]. In contrast, transformer-based \ngenerative adversarial networks (GANs) have not received \nmuch attention until recently. TransGAN [38] introduced \na pure transformer-based GAN that employs grid self-\nattention, a variant of self-attention, to scale for varying \nimage sizes. ViTGAN [39] modified the normalization \nlayers and output mapping layers of ViT in the encoder to \nfill in missing regions. To ensure Lipschitzness, ViTGAN \nutilizes L2 attention [40].\nIn the field of image inpainting, [41 ] employed a GPT-\nbased [42] bilateral transformer as the bottleneck model, \nwith convolution-based encoder and decoder for feature \nextraction. The bilateral transformer is applied to non-\npredicted tokens, while an autoregressive model is used \nin predicted tokens to avoid information leakage. This \nenables the model to simultaneously obtain bilateral con-\ntext and generate output. ICT [ 43] is a pluralistic image \ncompletion model that consists of two stages. In the first \nstage, a bi-directional transformer is used to generate a \nprobability distribution for the missing regions. In the \nsecond stage, a guided upsampling network is employed \nto reconstruct the images. T-Fill [ 44] employed a restric -\ntive CNN for individual weighted token representation, \nwhich is used in long-range transformer interactions. \nNotably, to the best of our knowledge, no other works \nhave attempted to apply the ViT structure in image \ninpainting.\n3  Approach\nOur goal is to generate a realistic image Ip from a masked \nimage Im that has missing regions, indicated by a binary \nmask M . Following the idea of [9 ], we divide the inpaint-\ning process into two stages: edge generation and image \ninpainting. Specifically, we first generate an edge map with \nthe Canny edge detector [45] and complete the edge map \nas the image’s structure prior. Subsequently, we stack edge \nmap and masked image Im together as a four-channel input \nto our ITrans network to yield the inpainting result.\nOur ITrans network is an end-to-end network that \nincorporates the CNN network with transformer modules, \nglobal and local transformer, to enhance the quality of \ninpainting. Figure  2 shows the structure of the ITrans net-\nwork. The global transformer focuses on global high-level \ncontext modeling in the encoder, and serves in the skip \nlayer to enhance inpainting performance. The local trans-\nformer is applied to an additional neighborhood branch to \nacquire low-level details. These two transformer modules \nintroduce additional attention in inpainting. The network \nis trained on places and human face datasets with ran-\ndomly generated irregular masks for free-form inpaint-\ning. The architecture details of our ITrans network will \nbe discussed in depth in Sect.  3.1. The two transformer \nmodules in ITrans will be introduced in Sects.  3.2 and 3.3, \nrespectively. The loss functions will be shown in Sect.  3.4.\n3.1  ITrans network\nThe whole inpainting network consists of two stages: edge \ncompletion and image inpainting. We have utilized the same \nedge generation model as [9 ]. The edge completion model \ntakes the masked grayscale image Ig , masked edge Em , and \nCanny-generated edges together as the input to construct the \nfull edge, considered as an image structure prior.\nIn the image inpainting stage, we introduce ITrans \nnetwork. ITrans’s primary structure is a CNN-based \nencoder–decoder network, with 8 ResBlocks [29] used to \ngenerate missing pixels in the bottleneck. The architecture \nof ITrans leverages the inductive bias of CNN networks \n W. Miao et al.\n21 Page 4 of 12\nto efficiently learn cross-domain information from vari-\nous images. Since encoder features typically contain \nmore unique image structures than decoder features, we \nbelieve it is essential to aggregate both types of features \nfor inpainting. To achieve this, we employ the global trans-\nformer in the skip-connection structure to merge these two \nfeatures.\nMoreover, we also incorporate an extra branch with \nfour convolutional layers for the local transformer. This \nfeature map is then passed through the local transformer \nto extract local details. Finally, the concatenation of the \nResBlock bottleneck and local transformer outputs is sent \nto the decoder. This decoder progressively upsamples the \nfeature maps to generate the final image.\nThe ITrans network is a generative model that trains \nunder the GAN framework [46], using PatchGAN [47] \nstructure for the discriminators. In particular, we choose \ndiffernent normalization approaches in different modules. \nSpectral normalization [48] is applied in all discriminators to \nstabilize training by scaling down weight matrices. Instance \nnormalization [49] is used in the encoder and decoder for \nstructure generating, while layer normalization [50] is imple-\nmented in all transformer layers.\n3.2  Global transformer\nThe global transform00er performs global self-attention \non feature maps in order to enhance the quality of image \ninpainting. Based on the concept of vision transformer (ViT) \n[51], which treats images as word sequences in natural lan-\nguage processing, our global transformer splits the input \nimage into fixed-size patches and adds class tokens into \npatches. Position embedding is then used to maintain posi-\ntional information, and the concatenated sequence is sent to \nthe transformer encoder. To avoid overfitting, dropout layers \n[52] are also implemented.\nIn image inpainting tasks, retaining all pixels through-\nout the process is necessary to preserve key textural clues \nin the background. Therefore, in the global transformer for \nimage inpainting, we remove all dropout layers to maintain \nall features and pixels for higher inpainting quality, applied \nin both position embedding and the transformer encoder. To \ninduce self-attention of the input, a multi-head layer (MLP) \nis inserted after the transformer encoder. The MLP layer \nenhances the generation performance of the global trans-\nformer and stabilizes training. Following that, a classifica-\ntion vector is obtained, representing categories of all pixels \nin feature maps. However, instead of a 1-D vector, we want \na self-attention map for the decoder. The obtained vector is \nsent to a rearrange module, which reshapes it into the size of \ninput feature map. Each pixel in this self-attention map has a \nclassification. Finally, we add a convolution layer to recover \ninput channels and smoothing the attention map. And this is \nthe output of the global transformer, which comprises classi-\nfication categories from the input feature map. The structure \nof our global transformer is depicted in Fig. 3.\n3.3  Local transformer\nIn general, convolution layers focus on the local area within \nthe convolution kernel, while the ViT module concentrates \non global attention and precise details on local areas. How -\never, the global receptive field of ViT can result in the loss \nof some details. Therefore, to address this issue, we propose \nthe local transformer, which primarily concerns low-level \nimage details in deeper layers. To the best of our knowledge, \nthere have been relatively few attempts to use transformers \nto extract local fine details. The structure of our local trans-\nformer is depicted in Fig. 4.\nInitially, we consider the sequences for attention com-\nputation, which are query (Q ), key (K) and value (V ). We \napply a sequence extracting convolution layer instead of \npatching procedures to obtain the sequences. The sequences \nare defined as:\nwhere X is the input feature map; f (⋅) , g(⋅) , and h(⋅) are \ndifferent convolution layers. Then, query (Q ), key (K) and \nvalue (V) sequences are sent to the kernel-sized self-atten-\ntion layer. The self-attention layer in our local transformer \nfocuses on attention with a sequence size that extracts con-\nvolution kernels. To ensure efficient computation, we adopt \na dynamic multi-headed dimension choosing mechanism in \nthe attention layer. For multi-headed layers, we use different \nhead numbers for distinct feature channels to save compu-\ntational costs. The number of head dimensions depends on \nthe number of input feature channels. The head dimension is \nsmall for low-level features and large for high-level features, \nresulting in reduced computation costs across a spectrum of \ninput sizes. The self-attention head is defined as:\nwhere \n√\ndh is the feature dimension for each head. Finally, \nan MLP layer is added to restore the missing pixels and to \ngenerate the final local attention map.\nTo reduce computational costs, we omit the use of posi-\ntion embedding and class tokens in our local transformer \ndesign. Attention sequences are generated using convolution \nkernels, which preserve the order of original features. There-\nfore, it is unnecessary to retain position information through \nposition embedding. In highly detailed contexts, there are \nmore pixel categories than in the original image, and class \n(1)\nQ = Reshape(f(X)),\nK = Reshape(g(X)),\nV = Reshape(h(X)),\n(2)Attention(X)= softmax(QKT ∕\n√\ndh)V,\nITrans: generative image inpainting with transformers  \n Page 5 of 12 21\ntokens become less significant in local areas while consum-\ning more time. To address this issue, we add a skip layer to \nthe local transformer, which combines the input feature map \nwith the local attention map for upsampled decoding. The \noutput of our local transformer is defined as:\nwhere F(⋅) denotes convolution operation.\n3.4  Training losses\nInpainting tasks are inherently ambiguous, especially \nwhen dealing with extensive missing regions, and multiple \nplausible fillings may be appropriate for the same region. \nTo address the complexity of this task, we will introduce \nall of our proposed losses.\nIn the edge completion stage, we apply adversarial loss \nand feature-matching loss [53]:\nThe loss weight /u1D706adve and /u1D706FM are to 1 and 10, respectively. \nAdversarial loss ensures the generated details are naturally \nlooking ones, which is defined as:\nwhere G1 and D1 denote edge generator and discriminator, \nrespectively; EGT indicates ground truth edges; Ep indi-\ncates predicted completed edges; and Ig indicates grayscale \nimages.\nFeature-matching loss compares activation maps in spe-\ncific discriminator layers, which is similar to perception \nloss [54– 56] and it is defined as:\nwhere L is the final convolution layer of discriminator, N i is \nthe number of elements of the i ’th activation layer, and D (i)\n1  \nis the i’th layer of discriminator.\nIn inpainting stage, the input is incomplete image \nIm = IGT ⋅ (1 − M ) , where masked areas are set to 0, along \nwith completed edge map Ec = EGT ⋅ (1 − M)+ Ep ⋅ M . \nThe predicated image Ip is generated from the incomplete \nimage and the completed edge. L1 loss, adversarial loss, \nstyle loss, perceptual loss, and total variation loss are all \nincluded in training loss. L1 loss is normalized by mask \n(3)Output= F (Concat(X, Attention(X))),\n(4)min\nG 1\nmax\nD 1\nLG 1\n= min\nG 1\n(/u1D706advemax\nD 1\n(Ladve)+ /u1D706FM LFM ).\n(5)\nLadve = /u1D53C(EGT ,Ig) log D 1 (EGT ,Ig)\n+ /u1D53CIg\nlog [1 − D 1 (Ep,Ig)],\n(6)LFM = /u1D53C\n� L�\ni=1\n1\nNi\n‖D(i)\n1 (EGT )− D(i)\n1 (Ep)‖1\n�\n,\nsize to guarantee a proper scaling. Adversarial loss is simi-\nlar to Eq. (5 ):\nPerceptual loss [54] evaluates the distance between features \nof the predicted and original images on a pre-trained net-\nwork. It does not require the exact reconstruction, allow -\ning for variances in the predicted image. Perceptual loss is \ndefined as:\nwhere Φi is the i’th activation layer of VGG-19 pre-trained \nnetwork on ImageNet [57].\nStyle loss is shown by Sajjadi et al. [58] as an effective \nway to deal with “checkerboard” artifacts caused by trans-\npose convolution [59]. Style loss adopts the same activation \nlayers as perceptual loss and is defined as:\nwhere G Φi\nj  is the Gram matrix of activation map Φi . Total \nvariation loss [60] is used for smoothing the output spatially \nand compacting the possible noise in the decoder. Total vari-\nation loss for an H × W × C feature map is defined as:\nThe final training loss for ITrans network is:\nIn the training settings, we set /u1D706l1 = 1 , /u1D706advi = 0.1 , /u1D706p = 0.1 , \n/u1D706s = 250 , and /u1D706TV = 0.01.\n4  Experiments\n4.1  Implementation details\nThe ITrans network is implemented in PyTorch [ 62]. We \nuse Adam [63] optimizer with /u1D6FD1 =0 and /u1D6FD2=0.9. The learn-\ning rate of the generator is set to 10−4 learning rate initially, \nand decreases to 10−5 until convergence. The discriminator’s \nlearning rate is one-tenth that of the generator. In the edge \n(7)\nLadvi= /u1D53C(IGT ,E c) log D 2 (IGT ,E c)\n+ /u1D53CE c\nlog [1 − D 2 (Ip,E c)].\n(8)Lp = /u1D53C\n�\n�\ni\n1\nNi\n‖Φi(IGT )−Φ i(Ip)‖1\n�\n,\n(9)Ls = /u1D53Cj\n�\n‖G Φi\nj (Ip)−G Φi\nj (IGT )‖1\n�\n,\n(10)LTV = 1\nHWC\n/uni2211.s1\ni,j,k\n/uni221A.s1\n(Ii,j+1,k − Ii,j,k)2 +( Ii+1,j,k − Ii,j,k)2 .\n(11)\nLinpaint = /u1D706l1 Ll1 + /u1D706adviLadvi + /u1D706p Lp\n+ /u1D706sLs + /u1D706TV LTV .\n W. Miao et al.\n21 Page 6 of 12\nFig. 1  Inpainting results by the \nproposed ITrans network for \ndiverse scenes and human faces\nFig. 2  ITrans Network. In the \nITrans network, we adopt an \nencoder–decoder structure \nalong with ResBlock bottle-\nneck. The global transformer \nis added as skip layer to gather \nencoder and decoder features \nand self-attention together. We \nadd another branch specifically \nfor the local transformer, which \naims at extracting fine image \ndetails\nFig. 3  Global transformer. In \nglobal transformer, we do not \nadopt dropout layer in order to \nkeep all pixels in the feature \nmap. We believe it is essential \nto hold all pixels in our atten-\ntion map\n\nITrans: generative image inpainting with transformers  \n Page 7 of 12 21\ncompletion stage, the initial edge is generated by the Canny \nedge detector [45].\n4.2  Training datasets\nOur ITrans network is trained on the MS-COCO [64], \nPlaces2 [65] datasets for places inpainting and CelebA \n[66] dataset for human faces. Places2 is an image inpaint-\ning dataset that contains over 8 million images with more \nthan 365 places categories, while CelebA has over 200 \nthousand celebrity faces. To improve free-form inpaint-\ning performance, we mix NVIDIA-ALDR datasets [8 ] \nand Google-Quick-Draw!-based QD-IMD [67] together, \nas well as randomly produced square masks. Both of these \ndatasets include randomly drawn stripes to simulate the \nartifacts present in real-world inpainting tasks. The resolu-\ntion of training images is set to 512 × 512 and all models \nare trained for 1 million iterations with a batchsize of 8.\n5  Results\nIn our experiments, we use Places2 and CelebA for places \nand human faces tests respectively, and NVIDIA-ALDR test \nsets are used for different mask regions.\nFig. 4  Local transformer. We \nobtain transformer sequences \nwith sequence extraction \nconvolution. Self-attention is \ncomputed by a kernel-sized \nattention layer. And we add a \nskip layer at the output stage to \ncombine input and self-attention \ntogether for feature aggregating\n(a) GT\n (b) Input\n (c) FRRN\n (d)E C\n (e)I CT\n (f)I Trans\nFig. 5  Qualitative comparison with current models. a Ground truth, b masked images, c FRRN [61], d EdgeConnect [9], e ICT [43], f ITrans\n W. Miao et al.\n21 Page 8 of 12\n5.1  Qualitative comparison\nFigure  1 shows inpainting results obtained by our ITrans \nnetwork. Our ITrans network produces visually realistic \nresults when the missing area is extensive. In Fig.  5, we \ncompare images generated by our model to those generated \nby other inpainting approaches. ITrans works well on fine \ndetails, demonstrating the efficacy of our network structure. \nWith the use of edge maps, ITrans network could specifically \nconcentrate on pixel generation with the transformer-based \nself-attention.\n5.2  Quantitative comparison\nWe use four quantitative metrics to evaluate inpainting \nqualities: (1) relative L1 (MAE); (2) structural similar -\nity index (SSIM) [68]; (3) peak signal-to-noise ratio \n(PSNR); (4) Frechet inception distance (FID) [69]. \nTable 1  Quantitative results on Places2\nWe compare our Inpainting Transformer network (ITrans) with \nFRRN [61], EdgeConnect [9] and ICT [43]. On evaluating metrics, \nPSNR and SSIM are higher the better, while MAE and FID are lower \nthe better. The best metrics are boldfaced\nMask ratio FRRN EC ICT Itrans\nPSNR 0–10% 31.33 31.92 26.60 32.15\n10–20% 27.73 27.60 24.11 27.88\n20–30% 24.53 24.70 21.98 24.98\n30–40% 22.16 22.50 20.25 22.79\n40–50% 20.06 20.69 18.83 21.05\nAll 23.83 24.36 21.66 24.68\nSSIM 0–10% 0.9677 0.9712 0.9191 0.9726\n10–20% 0.9358 0.9342 0.8719 0.9375\n20–30% 0.8795 0.8808 0.8090 0.8868\n30–40% 0.8077 0.8132 0.7369 0.8222\n40–50% 0.7199 0.7324 0.6563 0.7456\nAll 0.8113 0.8203 0.7582 0.8288\nMAE(%) 0–10% 1.78 1.56 3.37 1.54\n10–20% 2.22 2.28 4.17 2.22\n20–30% 3.23 3.26 5.25 3.14\n30–40% 4.51 4.46 6.51 4.25\n40–50% 6.19 5.91 7.98 5.61\nAll 4.86 4.36 6.14 4.17\nFID 0–10% 4.49 4.08 15.03 4.06\n10–20% 11.52 10.69 20.14 10.31\n20–30% 22.94 20.43 26.52 19.14\n30–40% 38.04 34.77 33.67 31.84\n40–50% 58.73 53.94 43.99 49.02\nAll 17.10 16.24 14.37 15.10\nTable 2  Quantitative results on Celeb-HQ\nThe best metrics are boldfaced\nMask ratio FRRN EC ICT Itrans\nPSNR 0–10% 35.23 35.29 32.45 35.67\n10–20% 30.72 30.89 29.78 31.19\n20–30% 27.56 27.73 27.34 27.97\n30–40% 24.86 24.90 25.15 25.28\n40–50% 22.16 22.75 23.50 23.30\nAll 26.18 26.70 26.55 27.28\nSSIM 0–10% 0.9804 0.9802 0.9674 0.9815\n10–20% 0.9558 0.9563 0.9462 0.9584\n20–30% 0.9182 0.9185 0.9153 0.9214\n30–40% 0.8640 0.8662 0.8711 0.8742\n40–50% 0.7901 0.8007 0.8283 0.8098\nAll 0.8547 0.8629 0.8704 0.8791\nMAE(%) 0–10% 1.09 1.09 1.63 1.06\n10–20% 1.58 1.57 2.03 1.50\n20–30% 2.26 2.27 2.60 2.14\n30–40% 3.28 3.21 3.34 3.00\n40–50% 4.82 4.44 4.16 4.03\nAll 3.84 3.32 3.23 2.99\nFID 0–10% 3.32 3.21 15.19 2.61\n10–20% 8.67 7.77 17.87 7.04\n20–30% 15.97 14.74 21.16 14.16\n30–40% 25.14 25.06 25.02 23.34\n40–50% 37.87 36.46 27.98 34.79\nAll 14.17 12.56 14.07 11.04\nTable 3  Inpainting results with and without global transformer\nStatistics are based on 2000 images on Places2 and 500 images on \nCeleb-HQ, respectively\nDataset Places CelebA\nMask w/o G-Trans w/ G-Trans w/o G-Trans w/ G-Trans\nPSNR\n20–30% 24.70 24.73 27.73 27.79\n30–40% 22.50 22.53 24.86 25.10\nSSIM\n20–30% 0.8808 0.8824 0.9185 0.9211\n30–40% 0.8132 0.8150 0.8640 0.8704\nMAE\n20–30% 3.26 3.23 2.27 2.21\n30–40% 4.46 4.40 3.28 3.14\nFID\n20–30% 20.43 20.23 14.74 14.66\n30–40% 34.77 34.73 25.06 25.00\nITrans: generative image inpainting with transformers  \n Page 9 of 12 21\nPixel-wise metrics measures the accuracy (MAE), struc-\nture (SSIM) and color (PSNR) of inpainting images with \nground truths. FID measures perceptually accuracy due \nto its feature-based characteristic, which is based on the \nInception-V3 model [70] for superior perception perfor -\nmance than humans [71– 73].\nTable  1 shows our experimental results on Places2, \nand Table  2 shows our testing results on Celeb-HQ. The \nPlaces2 dataset includes 12,000 images, with each mask \nratio consisting of 2000 masks. Celeb-HQ comprises 500 \nimages for each mask ratio and 2000 images for all mask \nregions. We compare the ITrans network with FRRN \n[61], EdgeConnect [9 ] and ICT [43]. We obtain statistics \nusing available codes and pretrained weights. Our experi-\nments demonstrate that our ITrans network exceeds other \napproaches on the majority of metrics. However, it should \nbe noted that ICT outperforms better than ITrans in terms \nof large masks, especially on human faces. We believe \nthat this is because visual plausibility is more essential \nthan restoring the original images in large masks.\n5.3  Ablation study\nIn this section, we will turn to our key contributions: the \nglobal and local transformer. We will demonstrate their \nefficacy through the following ablation studies.\nGlobal transformer Skip layers are widely used in the \nencoder–decoder structure. However, traditional skip lay -\ners simply combine the encoder and decoder without any \nextra structure. In contrast, our global transformer aggre-\ngates encoder attention and decoder features in the skip \nlayers. Table 3 shows the inpainting performance with and \nwithout global transformer. The results reveal that our global \ntransformer outperforms the network without a skip layer. \nThis suggests that our global transformer performs well \non inpainting tasks and demonstrates the efficacy of global \nattention.\nLocal transformer The local transformer is the next focus \nof our research in the ITrans network, as it contains both \nlocal and global transformers. Having already demonstrated \nthe effectiveness of the global transformer, we are now gain-\ning experience with the local transformer. We compare the \nperformance of the network with and without the local trans-\nformer, and the results are shown in Table  4. Our findings \ndemonstrate that our proposed local transformer module \neffectively enhances inpainting performance for both places \nand faces, with the additional branch of local attention prov-\ning highly valuable. This additional self-attention branch \nhighlights the importance of detailed local self-attention in \nimproving the network’s inpainting ability.\n5.4  Limitations\nFailure cases are shown in Fig.  6. Blurriness and artifacts \nappear when the inpainting mask is large or complicated. A \nbetter edge completion model and a better network struc-\nture, we believe, might improve performance. Moreover, the \ncurrent generation performance of transformers is relatively \npoor, we need to discover a solution to enhance their gen-\nerating ability. Even though a 512 × 512 image is sufficient, \nour model still need to be experimented on higher resolution \nto enhance the utility of our ITrans network.\n5.5  Future works\nThe current ITrans network has significant scope for \nimprovement. For example, the network needs to be trained \non a wider variety of datasets. Although the current dataset \nprovides simulation of various contexts, it is still insufficient \nfor real-world inpainting tasks. Additionally, the transformer \nstructure remains computationally expensive during training \nand requires a lighter version. Recently, diffusion models ?? \nhave become popular in generative tasks, and a combination \nTable 4  Inpainting results with and without local transformer\nStatistics are based on 2000 images on Places2 and 500 images on \nCeleb-HQ, respectively\nDataset Places CelebA\nMask w/o L-Trans w/ L-Trans w/o L-Trans w/ L-Trans\nPSNR\n20–30% 24.73 24.97 27.79 27.88\n30–40% 22.53 22.76 25.10 25.26\nSSIM\n20–30% 0.8824 0.8858 0.9211 0.9214\n30–40% 0.8150 0.8204 0.8704 0.8735\nMAE\n20–30% 3.23 3.17 2.21 2.14\n30–40% 4.40 4.28 3.14 3.02\nFID\n20–30% 20.23 19.57 14.66 14.16\n30–40% 34.73 32.57 25.00 23.56\nFig. 6  Failure cases. Artifacts appear in huge missing holes\n W. Miao et al.\n21 Page 10 of 12\nof CNNs, transformers, and diffusion models could hold \ngreat promise in this field.\n6  Conclusion\nThrough multiple experiments, we have evaluated the \nend-to-end ITrans network’s ability to perform well in \nvarious inpainting scenarios. The ITrans network leverages \nthe inductive bias of CNNs while adding flexibility with \nits global and local transformers. The global transformer \nprovides global semantic self-attention for encoder feature \nmaps, which are then utilized in the decoder. The local trans-\nformer extracts local feature details to enhance the inpaint-\ning results further. Finally, future enhancements of the gen-\nerating ability are expected to improve overall performance.\nAuthor contributions All authors reviewed the manuscript. WM wrote \nthe main manuscript text, performed all experiments and prepared all \nfigures. LW wrote the abstract and rounded off the manuscript text. HL \nprovided experiment equipment.\nFunding Open Access funding provided by University of Jyväskylä \n(JYU).\nData availability The data mentioned in this article is free to use.\nDeclarations \nConflict of interest The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Criminisi, A., Perez, P., Toyama, K.: Object removal by exemplar-\nbased inpainting. In: 2003 IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition, 2003. Proceedings., \nvol. 2, p. 2003. IEEE\n 2. Criminisi, A., Pérez, P., Toyama, K.: Region filling and object \nremoval by exemplar-based image inpainting. IEEE Trans. Image \nProcess. 13(9), 1200–1212 (2004)\n 3. Liang, J., Zeng, H., Cui, M., Xie, X., Zhang, L.: Ppr10k: a large-\nscale portrait photo retouching dataset with human-region mask \nand group-level consistency. In: Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, pp. \n653–661 (2021)\n 4. Hays, J., Efros, A.A.: Scene completion using millions of photo-\ngraphs. ACM Trans. Graph. (ToG) 26(3), 4 (2007)\n 5. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patch-\nMatch: a randomized correspondence algorithm for structural \nimage editing. ACM Trans. Graph. 28(3), 24 (2009)\n 6. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: \nContext encoders: feature learning by inpainting. In: Proceedings \nof the IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp. 2536–2544 (2016)\n 7. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Globally and locally con-\nsistent image completion. ACM Trans. Graph. (ToG) 36(4), 1–14 \n(2017)\n 8. Liu, G., Reda, F.A., Shih, K.J., Wang, T.-C., Tao, A., Catanzaro, \nB.: Image inpainting for irregular holes using partial convolutions. \nIn: Proceedings of the European Conference on Computer Vision \n(ECCV), pp. 85–100 (2018)\n 9. Nazeri, K., Ng, E., Joseph, T., Qureshi, F., Ebrahimi, M.: Edge-\nConnect: structure guided image inpainting using edge prediction. \nIn: Proceedings of the IEEE/CVF International Conference on \nComputer Vision Workshops, pp. 0–0 (2019)\n 10. Xiong, W., Yu, J., Lin, Z., Yang, J., Lu, X., Barnes, C., Luo, \nJ.: Foreground-aware image inpainting. In: Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition \n(ICCV), pp. 5840–5848 (2019)\n 11. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form \nimage inpainting with gated convolution. In: Proceedings of the \nIEEE/CVF international conference on computer vision (ICCV), \npp. 4471–4480 (2019)\n 12. Song, Y., Yang, C., Shen, Y., Wang, P., Huang, Q., Kuo, C.-C.J.: \nSPG-Net: segmentation prediction and guidance network for \nimage inpainting. arXiv preprint arXiv: 1805. 03356 (2018)\n 13. Zeng, Y., Lin, Z., Yang, J., Zhang, J., Shechtman, E., Lu, H.: High-\nresolution image inpainting with iterative confidence feedback \nand guided upsampling. In: European Conference on Computer \nVision, pp. 1–17. Springer (2020)\n 14. Yi, Z., Tang, Q., Azizi, S., Jang, D., Xu, Z.: Contextual residual \naggregation for ultra high-resolution image inpainting. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, pp. 7508–7517 (2020)\n 15. Zhao, S., Cui, J., Sheng, Y., Dong, Y., Liang, X., Chang, E.I., Xu, \nY.: Large scale image completion via co-modulated generative \nadversarial networks. arXiv preprint arXiv: 2103. 10428 (2021)\n 16. Liu, H., Wan, Z., Huang, W., Song, Y., Han, X., Liao, J.: PD-\nGAN: probabilistic diverse GAN for image inpainting. In: Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, pp. 9371–9381 (2021)\n 17. Liu, R., Wang, X., Lu, H., Wu, Z., Fan, Q., Li, S., Jin, X.: \nSCCGAN: style and characters inpainting based on CGAN. Mob. \nNetw. Appl. 26(1), 3–12 (2021)\n 18. Wang, L., Zhang, S., Gu, L., Zhang, J., Zhai, X., Sha, X., Chang, \nS.: Automatic consecutive context perceived transformer GAN \nfor serial sectioning image blind inpainting. Comput. Biol. Med. \n136, 104751 (2021)\n 19. Zhao, L., Mo, Q., Lin, S., Wang, Z., Zuo, Z., Chen, H., Xing, \nW., Lu, D.: UCTGAN: diverse image inpainting based on unsu-\npervised cross-space translation. In: Proceedings of the IEEE/\nCVF Conference on Computer Vision and Pattern Recognition, \npp. 5741–5750 (2020)\n 20. d’ Ascoli, S., Touvron, H., Leavitt, M.L., Morcos, A.S., Biroli, \nG., Sagun, L.: ConViT: improving vision transformers with soft \nconvolutional inductive biases. In: International Conference on \nMachine Learning, pp. 2286–2296. PMLR (2021)\n 21. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., \nGomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you \nneed. In: Advances in Neural Information Processing Systems, \npp. 5998–6008 (2017)\nITrans: generative image inpainting with transformers  \n Page 11 of 12 21\n 22. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated \nconvolutions. arXiv preprint arXiv: 1511. 07122 (2015)\n 23. Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., \nAshukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., Lem-\npitsky, V.: Resolution-robust large mask inpainting with fourier \nconvolutions. In: Proceedings of the IEEE/CVF Winter Con-\nference on Applications of Computer Vision, pp. 2149–2159 \n(2022)\n 24. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional \nnetworks for biomedical image segmentation. In: International \nConference on Medical Image Computing and Computer-\nassisted Intervention, pp. 234–241. Springer (2015)\n 25. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Genera-\ntive image inpainting with contextual attention. In: Proceed-\nings of the IEEE Conference on Computer Vision and Pattern \nRecognition, pp. 5505–5514 (2018)\n 26. Kim, Y., Cheon, M., Lee, J.: Texture transform attention for \nrealistic image inpainting. arXiv preprint arXiv: 2012. 04242  \n(2020)\n 27. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dha-\nriwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: \nLanguage models are few-shot learners. arXiv preprint arXiv: \n2005. 14165 (2020)\n 28. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, \nX., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., \nGelly, S., et al.: An image is worth 16x16 words: transformers \nfor image recognition at scale. arXiv preprint arXiv: 2010. 11929 \n(2020)\n 29. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for \nimage recognition. In: Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition, pp. 770–778 (2016)\n 30. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., \nJégou, H.: Training data-efficient image transformers & distilla-\ntion through attention. In: International Conference on Machine \nLearning, pp. 10347–10357. PMLR (2021)\n 31. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., \nZagoruyko, S.: End-to-end object detection with transformers. In: \nEuropean Conference on Computer Vision, pp. 213–229. Springer \n(2020)\n 32. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable \nDETR: deformable transformers for end-to-end object detection. \narXiv preprint arXiv: 2010. 04159 (2020)\n 33. Wang, H., Zhu, Y., Adam, H., Yuille, A., Chen, L.-C.: Max-Dee-\npLab: end-to-end panoptic segmentation with mask transformers. \nIn: Proceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition, pp. 5463–5474 (2021)\n 34. Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, \nH.: End-to-end video instance segmentation with transformers. In: \nProceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition, pp. 8741–8750 (2021)\n 35. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., \nFeng, J., Xiang, T., Torr, P.H., et al.: Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with transformers. \nIn: Proceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition, pp. 6881–6890 (2021)\n 36. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, \nA., Tran, D.: Image transformer. In: International Conference on \nMachine Learning, pp. 4055–4064. PMLR (2018)\n 37. Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., Sutsk-\never, I.: Generative pretraining from pixels. In: International Confer-\nence on Machine Learning, pp. 1691–1703. PMLR (2020)\n 38. Jiang, Y., Chang, S., Wang, Z.: TransGAN: two transformers can \nmake one strong GAN. arXiv preprint arXiv: 2102. 07074 (2021)\n 39. Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., Liu, C.: ViTGAN: \ntraining GANs with vision transformers. arXiv preprint arXiv: 2107. \n04589 (2021)\n 40. Kim, H., Papamakarios, G., Mnih, A.: The Lipschitz constant of \nself-attention. In: International Conference on Machine Learning, \npp. 5562–5571. PMLR (2021)\n 41. Yu, Y., et al.: Diverse image inpainting with bidirectional and autore-\ngressive transformers. arXiv (2021)\n 42. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., \net al.: Language models are unsupervised multitask learners. Ope-\nnAI Blog 1(8), 9 (2019)\n 43. Wan, Z., Zhang, J., Chen, D., Liao, J.: High-fidelity pluralistic \nimage completion with transformers. In: Proceedings of the IEEE/\nCVF International Conference on Computer Vision, pp. 4692–4701 \n(2021)\n 44. Zheng, C., Cham, T.-J., Cai, J., Phung, D.: Bridging global context \ninteractions for high-fidelity image completion. In: Proceedings of \nthe IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 11512–11522 (2022)\n 45. Canny, J.: A computational approach to edge detection. IEEE Trans. \nPattern Anal. Mach. Intell. 6, 679–698 (1986)\n 46. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, \nD., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. \nAdv. Neural Inf. Process. Syst. 27 (2014)\n 47. Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A.: Image-to-image transla-\ntion with conditional adversarial networks. In: Proceedings of the \nIEEE Conference on Computer Vision and Pattern Recognition, pp. \n1125–1134 (2017)\n 48. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normali-\nzation for generative adversarial networks. In: International Confer-\nence on Learning Representations (2018)\n 49. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Improved texture networks: \nmaximizing quality and diversity in feed-forward stylization and tex-\nture synthesis. In: Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, pp. 6924–6932 (2017)\n 50. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv pre-\nprint arXiv: 1607. 06450 (2016)\n 51. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Usz-\nkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, \nS., et al.: An image is worth 16x16 words: transformers for image \nrecognition at scale (2021)\n 52. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhut-\ndinov, R.R.: Improving neural networks by preventing co-adaptation \nof feature detectors. arXiv preprint arXiv: 1207. 0580 (2012)\n 53. Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., Tao, A., Kautz, J., Catanzaro, \nB.: High-resolution image synthesis and semantic manipulation \nwith conditional GANs. In: Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition, pp. 8798–8807 (2018)\n 54. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time \nstyle transfer and super-resolution. In: European Conference on \nComputer Vision, pp. 694–711 (2016). Springer\n 55. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using \nconvolutional neural networks. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, pp. 2414–2423 \n(2016)\n 56. Gatys, L., Ecker, A.S., Bethge, M.: Texture synthesis using convolu-\ntional neural networks. Adv. Neural. Inf. Process. Syst. 28, 262–270 \n(2015)\n 57. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., \nHuang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: ImageNet \nlarge scale visual recognition challenge. Int. J. Comput. Vis. 115(3), \n211–252 (2015)\n 58. Sajjadi, M.S., Scholkopf, B., Hirsch, M.: EnhanceNet: single image \nsuper-resolution through automated texture synthesis. In: Proceed-\nings of the IEEE International Conference on Computer Vision, pp. \n4491–4500 (2017)\n 59. Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard \nartifacts. Distillation 1(10), 3 (2016)\n W. Miao et al.\n21 Page 12 of 12\n 60. Rudin, L.I., Osher, S., Fatemi, E.: Nonlinear total variation based \nnoise removal algorithms. Physica D 60(1–4), 259–268 (1992)\n 61. Guo, Z., Chen, Z., Yu, T., Chen, J., Liu, S.: Progressive image \ninpainting with full-resolution residual network. In: Proceedings of \nthe 27th ACM International Conference on Multimedia, pp. 2496–\n2504 (2019)\n 62. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, \nG., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: PyTorch: \nan imperative style, high-performance deep learning library. Adv. \nNeural Inf. Process. Syst. 32 (2019)\n 63. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. \nICLR 9 (2015). arXiv preprint arXiv: 1412. 6980\n 64. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, \nD., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in \ncontext. In: European Conference on Computer Vision, pp. 740–\n755. Springer (2014)\n 65. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: \na 10 million image database for scene recognition. IEEE Trans. Pat-\ntern Anal. Mach. Intell. 40(6), 1452–1464 (2017)\n 66. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in \nthe wild. In: Proceedings of International Conference on Computer \nVision (ICCV) (2015)\n 67. Iskakov, K.: Semi-parametric image inpainting. arXiv preprint \narXiv: 1807. 02855 (2018)\n 68. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality \nassessment: from error visibility to structural similarity. IEEE Trans. \nImage Process. 13(4), 600–612 (2004)\n 69. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, \nS.: GANs trained by a two time-scale update rule converge to a local \nnash equilibrium. Adv. Neural Inf. Process. Syst. 30 (2017)\n 70. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethink-\ning the inception architecture for computer vision. In: Proceedings of \nthe IEEE Conference on Computer Vision and Pattern Recognition, \npp. 2818–2826 (2016)\n 71. Dolhansky, B., Ferrer, C.C.: Eye in-painting with exemplar genera-\ntive adversarial networks. In: Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition, pp. 7902–7911 (2018)\n 72. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention \ngenerative adversarial networks. In: International Conference on \nMachine Learning, pp. 7354–7363. PMLR (2019)\n 73. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The \nunreasonable effectiveness of deep features as a perceptual metric. \nIn: Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition, pp. 586–595 (2018)\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Inpainting",
  "concepts": [
    {
      "name": "Inpainting",
      "score": 0.8915252089500427
    },
    {
      "name": "Computer science",
      "score": 0.735590398311615
    },
    {
      "name": "Transformer",
      "score": 0.6603602170944214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6109656691551208
    },
    {
      "name": "Encoder",
      "score": 0.5815249681472778
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5271463394165039
    },
    {
      "name": "Hallucinating",
      "score": 0.49028804898262024
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42921558022499084
    },
    {
      "name": "Computer vision",
      "score": 0.4187266230583191
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24164456129074097
    },
    {
      "name": "Engineering",
      "score": 0.078499436378479
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210092944",
      "name": "Dalian University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 17
}