{
  "title": "Unsupervised contrastive learning based transformer for lung nodule detection",
  "url": "https://openalex.org/W4225426831",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2335120302",
      "name": "Chuang Niu",
      "affiliations": [
        "Imaging Center",
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2116246449",
      "name": "Ge Wang",
      "affiliations": [
        "Rensselaer Polytechnic Institute",
        "Imaging Center"
      ]
    },
    {
      "id": "https://openalex.org/A2335120302",
      "name": "Chuang Niu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116246449",
      "name": "Ge Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096038436",
    "https://openalex.org/W1986649315",
    "https://openalex.org/W6797127243",
    "https://openalex.org/W2752625590",
    "https://openalex.org/W2889646458",
    "https://openalex.org/W2268116731",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W6792919013",
    "https://openalex.org/W2524399695",
    "https://openalex.org/W2895992674",
    "https://openalex.org/W6785029211",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3054666633",
    "https://openalex.org/W6756341756",
    "https://openalex.org/W6718670374",
    "https://openalex.org/W2902159658",
    "https://openalex.org/W2561512519",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W2781660331",
    "https://openalex.org/W6801785775",
    "https://openalex.org/W1999529880",
    "https://openalex.org/W1992122542",
    "https://openalex.org/W6810563457",
    "https://openalex.org/W6801635846",
    "https://openalex.org/W6791370643",
    "https://openalex.org/W6840274575",
    "https://openalex.org/W6838856040",
    "https://openalex.org/W6774654717",
    "https://openalex.org/W2884672177",
    "https://openalex.org/W6844194202",
    "https://openalex.org/W6805070455",
    "https://openalex.org/W2755386144",
    "https://openalex.org/W2466312801",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W2061409412",
    "https://openalex.org/W2584017349",
    "https://openalex.org/W2394599079",
    "https://openalex.org/W1986439413",
    "https://openalex.org/W6777179611",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6777185910",
    "https://openalex.org/W3165191451",
    "https://openalex.org/W2736945546",
    "https://openalex.org/W4210358025",
    "https://openalex.org/W3202823735",
    "https://openalex.org/W3100175091",
    "https://openalex.org/W4226227406",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4282966576",
    "https://openalex.org/W3037156242",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3137513727",
    "https://openalex.org/W3201788907",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1938255733",
    "https://openalex.org/W4226103149",
    "https://openalex.org/W2964095005",
    "https://openalex.org/W3026063058",
    "https://openalex.org/W3106709020",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3103199493",
    "https://openalex.org/W3026092005",
    "https://openalex.org/W4286236352",
    "https://openalex.org/W3202941780"
  ],
  "abstract": "Abstract Objective. Early detection of lung nodules with computed tomography (CT) is critical for the longer survival of lung cancer patients and better quality of life. Computer-aided detection/diagnosis (CAD) is proven valuable as a second or concurrent reader in this context. However, accurate detection of lung nodules remains a challenge for such CAD systems and even radiologists due to not only the variability in size, location, and appearance of lung nodules but also the complexity of lung structures. This leads to a high false-positive rate with CAD, compromising its clinical efficacy. Approach. Motivated by recent computer vision techniques, here we present a self-supervised region-based 3D transformer model to identify lung nodules among a set of candidate regions. Specifically, a 3D vision transformer is developed that divides a CT volume into a sequence of non-overlap cubes, extracts embedding features from each cube with an embedding layer, and analyzes all embedding features with a self-attention mechanism for the prediction. To effectively train the transformer model on a relatively small dataset, the region-based contrastive learning method is used to boost the performance by pre-training the 3D transformer with public CT images. Results. Our experiments show that the proposed method can significantly improve the performance of lung nodule screening in comparison with the commonly used 3D convolutional neural networks. Significance. This study demonstrates a promising direction to improve the performance of current CAD systems for lung nodule detection.",
  "full_text": "Unsupervised Contrastive Learning based\nTransformer for Lung Nodule Detection\nChuang Niu1 and Ge Wang1\n1Biomedical Imaging Center, Department of Biomedical Engineering, Rensselaer\nPolytechnic Institute, Troy, New York, United States of America\nE-mail: wangg6@rpi.edu\nAbstract. Early detection of lung nodules with computed tomography (CT)\nis critical for the longer survival of lung cancer patients and better quality of\nlife. Computer-aided detection/diagnosis (CAD) is proven valuable as a second or\nconcurrent reader in this context. However, accurate detection of lung nodules remains\na challenge for such CAD systems and even radiologists due to not only the variability\nin size, location, and appearance of lung nodules but also the complexity of lung\nstructures. This leads to a high false-positive rate with CAD, compromising its\nclinical eﬃcacy. Motivated by recent computer vision techniques, here we present\na self-supervised region-based 3D transformer model to identify lung nodules among a\nset of candidate regions. Speciﬁcally, a 3D vision transformer (ViT) is developed that\ndivides a CT image volume into a sequence of non-overlap cubes, extracts embedding\nfeatures from each cube with an embedding layer, and analyzes all embedding features\nwith a self-attention mechanism for the prediction. To eﬀectively train the transformer\nmodel on a relatively small dataset, the region-based contrastive learning method is\nused to boost the performance by pre-training the 3D transformer with public CT\nimages. Our experiments show that the proposed method can signiﬁcantly improve\nthe performance of lung nodule screening in comparison with the commonly used 3D\nconvolutional neural networks.\n1. Introduction\nGlobal cancer statistics in 2018 indicates that Lung cancer is the most popular, i.e.,\n11.6% of the total cases, and the leading cause of cancer death, up to 18.4% of the total\ncancer deaths (Bray et al.; 2018). Various studies have shown that early detection and\ntimely treatment of lung nodules can improve the 5-year survival rate (Blandin Knight\net al.; 2017). Therefore, major eﬀorts have been made on early and accurate detection of\nlung nodules in diﬀerent aspects, such as imaging technologies ( NLST; 2017; Niu et al.;\n2022), diagnosis workﬂows (MacMahon et al.; 2005), and computer aided detection\nand computer aided diagnostic systems (Messay et al.; 2010). Particularly, recent\nresults indicate that computer aided detection/diagnosis (CAD) systems empowered\nby artiﬁcial intelligence (AI) algorithms as the second or concurrent reader can improve\narXiv:2205.00122v1  [cs.CV]  30 Apr 2022\nTransformer-based Lung Nodule Detection 2\nthe performance of lung nodule detection on chest radiographs (Yoo et al.; 2021) and\nCT images (Roos et al.; 2010; Prakashini et al.; 2016).\nLung cancer CAD systems usually involve lung region segmentation, nodule\ncandidate generation, nodule detection, benign and malignant nodule recognition, and\ndiﬀerent types of lung cancer classiﬁcation. In recent years, deep learning methods\nwere developed for CAD systems, continuously improving the performance of some\nor all the key components in a CAD system. For example, (Harrison et al.; 2017;\nHofmanninger et al.; 2020) showed that deep learning methods for CT lung segmentation\nsigniﬁcantly improved the performance through training the models with a variety of\ndatasets. Motivated by the progress in deep learning based objection detection (Ren\net al.; 2015; Lin et al.; 2017) in various domains (Jiang and Learned-Miller; 2017; Niu\net al.; 2018), the performance of lung nodule candidate generation and detection was\nsigniﬁcantly improved by adapting advanced object detection algorithms (Jaeger et al.;\n2020; Baumgartner et al.; 2021). Nevertheless, high false positive rate is still a main\nchallenge for accurate lung nodule detection Pinsky et al. (2018). Clearly, a key step\nfor accurate nodule detection is to eﬀectively reduce the false positive rate for nodule\ncandidates. Recent studies addressed this issue using various techniques, such as, 3D\nconvolutional neural network (Dou et al.; 2017), multi-scale prediction (Cheng et al.;\n2019; Gu et al.; 2018), relation learning Yang et al. (2020), multi-checkpoint ensemble\n(Jung et al.; 2018), multi-scale attention (Zhang et al.; 2022), etc. After identifying\nlung nodules, various methods were proposed to further analyze them, i.e., predicting\nthe malignancy (Shen et al.; 2017; Al-Shabi et al.; 2022) and sub-types (Liu et al.; 2018;\nYuan et al.; 2018) of lung nodules. It is exciting that adapting emerging techniques in\nmachine learning and computer vision based on the domain knowledge leads to great\nprogress in CAD systems with great potential for clinical translation.\nRecently, transformers Vaswani et al. (2017), originally developed for natural\nlanguage processing (NLP), have achieved great successes in various tasks of computer\nvision. The key component of the transformer is the attention mechanism that\nutilizes global dependencies between input and output. For the ﬁrst time, Vision\nTransformer (ViT) divides an image into a sequence of non-overlap patches, analyzes\nthem as a sequence of elements similar to words, and produces state-of-the-art results\ndemonstrating the eﬀectiveness and superiority in image classiﬁcation (Dosovitskiy\net al.; 2020). Since then, ViT has been successfully applied to various other vision\ntasks including medical imaging Pan et al. (2021) and medical image analysis (Lyu\net al.; 2021). However, the performance of the original ViT relies on a large labelled\nimage dataset including 300 millions images, and the conventional wisdom is that the\ntransformers do not generalize well if they are trained on insuﬃcient amounts of data.\nTherefore, directly adopting the transformers for CAD systems is not trivial when\nlabeled data are scarce.\nLack of labeled data is a common problem in the medical imaging and many other\nﬁelds. A most promising direction of deep learning is the so-called unsupervised or\nself-supervised learning Niu, Zhang, Wang and Liang (2020); Niu et al. (2021); Niu,\nTransformer-based Lung Nodule Detection 3\nFan, Wu, Li, Lyu and Wang (2020) that recently achieved remarkable results which\neven approach the performance of supervised counterparts. Particularly, unsupervised\nlearning works by pre-training a neural network on a large scale unlabeled dataset\nto beneﬁt downstream supervised tasks that only oﬀer a limited number of training\nsamples He et al. (2019). For unsupervised or self-supervised learning, the pretext task\nis the core to learn meaningful representation features. Recent progresses suggest that\ninstance contrastive learning (Chen et al.; 2020) and masked autoencoding (He et al.;\n2021) are two most eﬀective and scalable pretext tasks for unsupervised representation\nlearning. Speciﬁcally, instance contrastive learning maximizes the mutual information\nbetween two random transformations of the same instance (e.g., an object in a natural\nimage or a patient represented by a CT volume). This can be achieved by forcing the\nrepresentation features from diﬀerent transformations of the same instance to be similar\nwhile the features from diﬀerent instances to be dissimilar. On the other hand, masked\nautoencoding recovers masked parts from the rest visible data, which has been used\nfor pre-training in various tasks and recently produced encouraging results (He et al.;\n2021), using an asymmetric encoder-decoder architecture and a high proportion masking\nstrategy.\nBased on the above progresses, here we study how to eﬀectively adapt ViT and\nunsupervised pretraining for lung nodule detection, so that the false positive rate can\nbe reduced for lung nodules to be eﬀectively singled out of a set of candidates, in\ncomparison with the commonly used 3D CNNs. In our work, we adapt the original\ntransformer to a CT image volume with the fewest possible modiﬁcations. Advantages of\nkeeping the original transformer conﬁguration as much as possible include the scalability\nin the modeling capacity and the applicability across multiple modality datasets. With\nthis preference in mind, we simply divide an 3D CT image volume into non-overlap\ncubes and extract their linear embeddings as the input to the transformer. These cubes\nare equivalent to the tokens or words in NLP. However, without pretraining on large-\nscale datasets, the superiority of the transformer cannot be realized, especially for lung\nnodule analysis where labeled data are usually expensive and scarce, e.g., there are only\nover one thousand labels in public datasets. To overcome this diﬃculty, we perform\nunsupervised region-based contrastive learning on public CT images from the LIDC-\nIDRI dataset to eﬀectively train the adapted transformer. Our experimental results\nshow that while the adapted 3D transformer trained with a relatively small number of\nlabeled lung nodule data from scratch achieved worse results than the 3D CNN model,\nthe pretraining techniques enabled the adapted transformer to outperform the commonly\nused 3D CNN. Interestingly, we found that unsupervised pretraining is more eﬀective\nthan supervised pretraining with natural images in a transfer learning manner to boost\nthe performance of the adapted transformer.\nThe rest of this paper is organized as follows. In the next section, we describe our\ntransformer architecture and implementation details. In the third section, we report\nour experimental design and representative results in comparison to competing CNN\nnetworks. In the last section, we discuss relevant issues and conclude the paper.\nTransformer-based Lung Nodule Detection 4\nFigure 1. Transformer architecture for lung nodule detection.\n2. Methodology\n2.1. Vision Transformer for Lung Nodule Detection\nIn this section, we describe the architecture of our adapted Transformer for lung nodule\ndetection in a CT image volume. The whole architecture is depicted in Fig. 1, where\nthere are four parts. The details on each part are given as follows.\nInput and Linear Embedding: The input is a 3D tensor, x ∈RH×W×D, which\nis a local candidate volumetric region of interest in a whole CT volume. Similar to\nwhat ViT does, the image volume is divided into a sequence of non-overlap cubes,\nxc ∈RS×S×S, similar to words in NLP, where H,W,D are the input volume size, S is\nthe cube size, and cis the index for cubes. Then, the linear embedding layer maps these\ncubes to embedding features independently. In practice, the linear embedding layer is\nimplemented as a 3D convolutional layer, where both the kernel size and convolutional\nstride are S ×S ×S. Therefore, this embedding layer can directly take the original\n3D volume as input and outputs the embedding features of non-overlapped cubes, i.e.,\nzc = E(xc) ∈Rd, where c= 1,2,..., S×S×S, and d is the dimension of embedding.\nAs in ViT, the [ class] token of a learnable embedding is prepended to the sequence\nof embedded cubes, and the ﬁnal sequence of linear embedding features are denoted as\n[z0; z1; ··· ; zN ], where z0 denotes the learnable class embedding, and N = S×S×S+1\nis the total number of input embeddings.\nTransformer-based Lung Nodule Detection 5\nPosition Embedding: For the model to be aware of the relative position of each\ncube, position embeddings are coupled with the feature embeddings. In this study, we\nextend the sin-cosine position encoding (Dosovitskiy et al.; 2020) into the 3D space.\nSpeciﬁcally, sine and cosine functions of diﬀerent frequencies are used to encode 3D\nposition information as\nPE(x,y,z ) = [PEsin(x),PEcos(x),PEsin(y),PEcos(y),PEsin(z),PEcos(z)],\nPEsin(p) = sin(p/10000i/dpos),i = 0,1,··· ,dpos −1,\nPEcos(p) = cos(p/10000i/dpos),i = 0,1,··· ,dpos −1,\n(1)\nwhere (x,y,z ) is the relative position of a cube andPE(x,y,z ) ∈Rd is the corresponding\nposition embedding, here the position embedding of the class token is a zero vector. The\nposition embedding consists of six parts and the dimension of each part is d/6. To be\nconsistent with the notations of feature embeddings, we use PEc denotes the position\nembedding of a speciﬁc cube. Finally, the point-wise summations of position and feature\nembeddings are input to the transformer encoder.\nTransformer Encoder: The transformer encoder consists of L stacked identical blocks,\nwhere each block has two layers, i.e., a multi-head self-attention layer and a simple\npositionwise fully-connected layer. As shown in Fig. 1, the residual connection and layer\nnormalization are applied in these two sub-layers. More speciﬁcally, given a sequence of\ninput embeddings, z0 = [z0\n0 + PE0; z0\n1 + PE1; ··· ,z0\nS3 + PES3 ] ∈RN×d, the output of\nthe lth multi-head self-attention layer is computed as\n[qlm,klm,vlm] = LN(zl−1)Ulm\nqkv ,\nAlm = softmax(qlmklmT\n),\nzlm = Almvlm,m = 1,2,··· ,M,\nzl\natt = [zl1,zl2,··· ,zlM ]Ul\nmsa + zl−1,l = 1,2,··· ,L,\n(2)\nwhere the ﬁrst three equations describe the operation of a speciﬁc self-attention head\nand the last equation represents the integration of multiple heads. Speciﬁcally, LN(·)\ndenotes the layer norm function, Ulm\nqkv ∈Rd×3dm represents a linear layer that maps\neach input embedding vector zl−1 into three vectors, qlm,klm,vlm ∈RN×dm, which are\nknown as the query, key, and value vectors respectively,Alm ∈RN×N is the self-attention\nweight matrix computed as the inner product between query and key vectors followed by\na softmax function. Then, the output, zlm ∈RN×dm, of each self-attention head is the\nweighed sum over all input embeddings to realize a global attention. There are M self-\nattention heads running in parallel, with m being the head index, which jointly attend\nto information from diﬀerent representation subspaces at diﬀerent positions (Vaswani\net al.; 2017). To avoid increasing the number of parameters, the vector dimension in\neach self-attention head is split to dm = d/M. The output zl\natt of the multi-head self-\nattention layer is the concatenation of all self-attention outputs transformed by a linear\nlayer Ul\nmsa ∈Rd×D and increased by the signal from the residual connection. Then, this\nTransformer-based Lung Nodule Detection 6\nFigure 2. Region-based contrastive learning framework for lung nodule detection.\noutput is forwarded to the MLP layer for the ﬁnal output of the lth block:\nzl = MLP(LN(zl\natt)) + zl\natt. (3)\nThus, the ﬁnal output of the transformer encoder is zL ∈RN×d, which has the same\ndimension as the input embeddings.\nClassiﬁcation Head: The classiﬁcation head is a linear layer that projects extracted\nfeatures by the transformer encoder to classiﬁcation scores. The classiﬁcation head only\ntakes the feature vector at the position of the [ class] token and outputs a classiﬁcation\nscore as\ny= zL\n0 Ucls, (4)\nwhere zL\n0 ∈R1×d, Ucls ∈Rd×C, and C is the number of classes.\n2.2. Region-based Contrastive Learning\nIt is well known that the transformer is extremely data-hungry but labeled lung nodule\ndata is relatively scarce. Hence, we propose a region-based contrastive learning method\nto pretrain the adapted transformer model by leveraging more unlabeled CT image\nvolumes. The popular constrastive learning framework is adopted in Fig. 2. It consists\nof two branches that take many pairs of similar samples and outputs the their features.\nGenerally, this framework enforces similar samples to be closer to each other while\ndissimilar samples to be more distinct in the representation feature space as measured\nby the InfoNCE loss.\nIn the unsupervised context, how to properly deﬁne similar and dissimilar samples\nis the key component (Tian et al.; 2020). Although great progresses were reported\nby introducing various random transformation techniques, it is still an open problem\non how to keep useful information and compress noise and artifacts in representation\nfeatures for downstream tasks. Actually, knowledge on speciﬁc downstream tasks plays\nan important role in deﬁning appropriate similar samples (Tian et al.; 2020).\nIn our application, similar and dissimilar samples can be deﬁned as follows. First,\nas we focus on classifying sub-volumes of a CT volume as lung nodule or not, we divide\nthe whole CT volume into a set of non-overlap cubes and regard each as an unique\ninstance. This assumes that every 3D sub-region in a patient is diﬀerent from the\nothers. Second, two sub-regions with a large intersection should be similar to each\nTransformer-based Lung Nodule Detection 7\nother. Third, although diﬀerent organs/tissues are usually inspected under diﬀerent\nHU windows, the same region under slightly diﬀerent HU windows should be similar to\neach other. Fourth, two sub-regions diﬀerent by a random rotation should be similar to\neach other as the angular information is not critical in detecting lung nodules.\nBased on the above assumptions, we ﬁrst divide a CT volume into a set of non-\noverlap S1 ×S1 ×S1 cubes from all patient CT scans to build the whole training\ndataset, {xi}I\ni=1, where I is the total number of cubes. During training, two sub-\ncubes of S2 ×S2 ×S2 (S2 < S1) voxels are randomly cropped from a given cube and\nrandomly rotated, and their HU values are randomly clipped, as shown in Fig. 2. In\neach training iteration, a set of B cubes are randomly selected, and then each cube\nis randomly transformed to two sub-cubes x′\ni,x′′\ni . Finally, the network parameters are\noptimized with the InfoNCE loss as follows:\nL= 1\n2B\nB∑\ni=1\n(L(x′\ni,x′′\ni ) + L(x′′\ni ,x′\ni)),\nL(x′\ni,x′′\ni ) = −log\n(\nexp(P(F(x′\ni; θF); θP)T P(F(x′′\ni ; θm\nF); θm\nP)/τ)∑I\nj=1,j̸=i exp(P(F(x′\ni; θF); θP)T P(F(x′′\nj ; θm\nF); θm\nP)/τ)\n)\n,\n(5)\nwhere F and P represent the feature encoder and projection head functions with\nparameters θFand θPto be optimized, and θ′\nFand θ′\nPare the moving averaging versions\nof θF and θP. Note that the feature encoder is exactly the transformer model without\nthe classiﬁcation head, and the projection head is the same as in Moco v3. The loss\nterm L(x′\ni,x′′\ni ) maximizes the feature similarity between two random transformations\nfrom the same cube while minimizing the feature similarity from diﬀerent cubes. The\nﬁnal loss Lis the average over a batch of B samples.\n2.3. Implementation Details\nIn our adapted transformer, the size of a candidate region was set to H = W = D= 72,\nthe size of each non-overlap cube S = 8, the embedding dimension d= 384, the number\nof blocks L = 11, and the number of attention heads M = 12. In our region-based\ncontrastive learning, the size of non-overlap sub-regions was set to S1 = 96, and the\nsize of each input cube S2 = 72, the low and high HU values of the clip window were\nrandomly sampled from [−1200,−1000] and [600,800] respectively. During unsupervised\npre-training, the batch size was set toB = 1024, Adamw was used to optimize the model,\nthe learning rate was 0 .0001 with cosine annealing. At the ﬁne-tuning stage, only the\npretrained linear embedding layer and transformer endoer were kept, the projection\nhead were removed, and a randomly initialized classiﬁcation head was added, the batch\nsize was set to 64, and all other hyper parameters for training were kept the same as\nthose in Moco v3. To address the imbalance issue, we randomly sampled each training\nbatch according to the pre-deﬁned positive sampling ratio meaning so that each batch\napproximately had a ﬁxed ratio of positive to negative samples. By default, the positive\nsampling ratio was set to 0.2.\nTransformer-based Lung Nodule Detection 8\nFigure 3.Samples of our lung nodule dataset. The ﬁrst and second rows show positive\nand negative samples respectively.\n3. Experimental Design and Results\n3.1. Dataset and Preprocessing\nIn this study, 684 lung nodules of larger than 6mm in size were selected from 436 patients\nin the LIDC-IDRI (Armato III et al.; 2011) dataset. As negative samples, 100×436 (100\nfrom each patient) regions not overlapping with positive regions were randomly selected\nfrom the LUNA16 Setio et al. (2017) dataset. The constructed dataset was divided to\na training dataset of 349 patients and a test dataset of 87 patients respectively. All the\nCT volumes were interpolated along the longitudinal direction so that the longitudinal\nresolution is the same as the axial resolution. For region-based contrastive learning,\n84,875 non-overlap regions were collected from 801 CT volumes in the LINA16 dataset\nwithout any region in the testing dataset. The region size was set to 96×96×96 for both\nsupervised and unsupervised learning. Some positive and negative samples are shown\nin Fig. 3, where it can be seen that the appearance of positive nodules may be very\ndiﬀerent, and similar structures in the negative regions present strong interferences. To\ntest the generalizability of diﬀerent methods, the test set of LUNGx (Kirby et al.; 2016)\nwas used to evaluate the performance of diﬀerent models trained on the LIDC-IDRI\ndataset. The LUNGx test set consists of 73 CT scans, each of them contains 1 or 2\npositive nodules and 200 negative nodules per scan, and the prepossessing procedure is\nthe same as that for LIDC-IDRI dataset. Also, these datasets are extremely unbalanced\n(#positive:#negative ≈1:100 and 1:200), making this task challenging.\n3.2. Evaluation Metric\nDue to imbalance of positive and negative samples in the evaluation dataset, the common\nFree Response Receiver Operating Characteristic (FROC) curve and Competition\nPerformance Metric (CPM) were used to evaluate the model performance. Speciﬁcally,\nTransformer-based Lung Nodule Detection 9\nFigure 4. FROC curves of the selected competing methods.\nthe true positive rate (TPR) and false positive rate (FPR) are deﬁned as\nTPR = TP\nTP + FN ,\nFPR = FP\nTN + FP ,\n(6)\nwhere TP, FN, TN, FP are the number of true positive, false negative, true negative,\nand false positive, respectively. Then, the average number of false positives per scan,\nFPS, is deﬁned as\nFPS = FPR ×TN\nNS , (7)\nwhere NS is the number of CT scans. The FROC curve is plotted as TPR v.s. FPS,\nwhich is a variant of the ROC curve, i.e., TPR v.s. FPR. The CPM score is deﬁned as\nthe average TPR (also called sensitivity) at the predeﬁned FPS points: 0.125, 0.25, 0.5,\n1, 2, 4, and 8 respectively.\n3.3. Comparative Analysis\nIn this sub-section, we evaluated the eﬀectiveness of the proposed method relative\nto the following three baselines. First, we modiﬁed ResNet He et al. (2016) to the\n3D version, named ResNet3D, as a strong baseline method. Second, the transformer\nmodel was trained from scratch, named ScratchTrans. To make sure that ScratchTrans\nbe suﬃciently trained, we doubled the number of training epochs and observed\nthe essentially same results. Third, we initialized the transformer model with the\nweights of the pretrained DeiT on the labeled ImageNet dataset in a transfer learning\nTransformer-based Lung Nodule Detection 10\nTable 1. Quantitative results. The sensitivities at diﬀerent FPS points and CPM\nscores were computed, with the best result highlighted in bold.\nMethods 0.125 0.25 0.5 1 2 4 8 CPM\nResNet3D 0.773 0.879 0.924 0.947 0.955 0.977 0.985 0.920\nScratchTrans 0.561 0.659 0.712 0.765 0.788 0.841 0.909 0.748\nDeiTTrans 0.803 0.879 0.902 0.932 0.962 0.977 0.985 0.920\nURCTrans 0.902 0.917 0.955 0.962 0.962 0.977 0.977 0.950\nmanner, named the resultant network DeiTTrans. Finally, our proposed transformer\nmodel pretrained via unsupervised region-based contrastive learning is referred to as\nURCTrans.\nThe comparison results in terms of the sensitivity and CPM scores are summarized\nin Table 1, and the FROC curves are plotted in 4. These results show that the ResNet3D\nmodel is a very strong baseline with a 0.920 CPM score. The ScratchTrans achieved the\nworst results among these methods, which is consistent to the results in other domains\nshowing that the transformer is extremely data-hungry and cannot perform well without\na large-scale dataset. Through transfer learning, DeiTTrans signiﬁcantly improved the\nperformance of the transformer model and produced results similar to that obtained\nwith ResNet3D. In contrast, our pretraining method without leveraging any labeled\ndata oﬀered the best performance among all comparison methods (0.950 CPM score,\n3% higher than DeiTTrans and the commonly used ResNet3D). Further inspecting the\nsensitivities at diﬀerent FPS points, it can be seen that the URCTrans model performed\nsigniﬁcantly better than the others when the average number of false positive nodules\nper scan is small ( ≤1). That is the most desired result to eﬀectively avoid falsely\nreported nodules. Clearly, our experimental results demonstrate that the transformer\npretrained with more CT data through contrastive learning promises a performance\nsuperior to the commonly used 3D CNN models.\nTable 2. Generalizability performance results on LUNGx. The sensitivities at\ndiﬀerent FPS points and CPM scores were computed, with the best result highlighted\nin bold.\nMethods 0.125 0.25 0.5 1 2 4 8 CPM\nResNet3D 0.712 0.767 0.808 0.890 0.918 0.959 0.973 0.861\nScratchTrans 0.630 0.685 0.740 0.863 0.863 0.904 0.932 0.802\nDeiTTrans 0.781 0.822 0.877 0.890 0.904 0.904 0.904 0.869\nURCTrans 0.822 0.849 0.918 0.945 0.959 0.959 0.959 0.916\nTransformer-based Lung Nodule Detection 11\nThe generalizability performance results on LUNGx are reported in Table 2, where\nthe models trained on LIDC-IDRI were directly evaluated. Although the relative\nperformance of diﬀerent methods is the same as above, the performance improvement\nof URCTrans is signiﬁcantly increased imcomparison with ResNet3D and DeiTTrans\ncounterparts especially for the lower false positive number ( ≤1). These results further\ndemonstrated the superiority of the presented method in terms of the generalizability.\n3.4. Eﬀects of the Input Size\nTable 3.Quantitative results obtained by URCTrans with diﬀerent input sizes. The\nbest result is highlighted in bold.\nInput size 0.125 0.25 0.5 1 2 4 8 CPM\n64 0.826 0.879 0.917 0.947 0.962 0.977 0.985 0.926\n72 0.902 0.917 0.955 0.962 0.962 0.977 0.977 0.950\n80 0.916 0.916 0.932 0.939 0.962 0.977 0.977 0.946\nAs mentioned in (Cheng et al.; 2019; Gu et al.; 2018), diﬀerent sizes of an input\ntensor allow various levels of contextual information, leading to diﬀerent performance\nmetrics. Combining the results from multi-scale inputs would boost the performance\nfurther. In this sub-section, we investigate the eﬀect of the input size on the performance\nof the transformer model for lung nodule detection. The results are in Table 3, showing\nthat the medium input size of 72 achieved the best result. It seems heuristic that there\nis a trade-oﬀ between the input size and the model performance, since a too large input\nmay bring more interfering structures while a too small input may not contain enough\ncontextual information to identify lung nodules. Nevertheless, these relatively similar\nresults indicate that the transformer model is robust to the input size.\n3.5. Eﬀects of the Positive Sampling Ratio\nIn Sub-section 2.3, we applied a strategy that each batch of training samples was\nrandomly sampled according to a pre-deﬁned positive sampling ratio. Here we evaluated\nthe eﬀects of positive sampling ratios on the lung nodule detection performance of the\ntransformer model. The results in Table 4 show that when the positive sampling ratio\nwas set to 0.2, the result is the best. Actually, the larger positive sampling ratio the more\npositive nodules the model tends to predict, as demonstrated in Table 4. Nevertheless,\nit can be seen that the model is quit robust to this hyper-parameter as there is no big\ndiﬀerence in performance.\nTransformer-based Lung Nodule Detection 12\nTable 4.Quantitative results obtained by URCTrans using diﬀerent positive sampling\nratios. The best result is highlighted in bold. The numbers of positive nodules\npredicted by the transformer model trained with diﬀerent positive sampling ratios,\nwhere the input region is regarded as positive if the prediction score ≥0.5.\nPositive ratio 0.125 0.25 0.5 1 2 4 8 CPM #Predicted\n0.1 0.795 0.841 0.894 0.924 0.933 0.947 0.969 0.900 158\n0.2 0.826 0.879 0.917 0.947 0.962 0.977 0.985 0.926 217\n0.3 0.765 0.856 0.917 0.932 0.969 0.977 0.985 0.915 247\n0.4 0.788 0.841 0.879 0.917 0.962 0.962 0.962 0.902 369\n0.5 0.795 0.841 0.909 0.947 0.947 0.970 0.970 0.911 409\n4. Discussions and Conclusion\nIn this study, we have adapted the ViT model and unsupervised contrastive learning for\nlung nodule detection from a CT image volume. Using neither multi-scale inputs nor\nassembling techniques, our presented transformer model pretrained in an unsupervised\nmanner has outperformed the state-of-the-art 3D CNN models. Importantly, we have\nfound that unsupervised representation learning or pretraining on a large-scale dataset\ncan signiﬁcantly beneﬁt the transformer model, which is scalable, and highly desirable\nespecially when labeled data are scarce.\nOur pilot results suggest that for the medical analysis tasks where labeled data\nare expensive and limited, it is very promising to build a large-scale model, pre-trains\nit on a related big dataset via domain-knowledge driven self-supervised, and transfers\nthe learned large-scale prior to beneﬁt down-stream tasks. Although this study was\nonly focused on CT image representation learning, combining speciﬁc imaging modality\ndata with other modalities, such as diagnostic text reports, clinical data, other imaging\napproaches, etc, has potential to unleash strong power of AI for diagnosis and treatment.\nIn conclusion, we have presented an adapted 3D ViT model pretrained via region-\nbased contrastive learning for lung nodule detection. Speciﬁcally, we have introduced\nhow to adapt the generic transformer model for lung nodule detection. To make the\ntransformer model work well on a relatively small labeled dataset, we have introduced\na self-learning method leveraging public CT data. The comparative results have\ndemonstrated the superiority of the presented approach over the state of the art 3D\nCNN baselines. These ﬁndings suggest a promising direction to improve the CAD\nsystems via deep learning.\nREFERENCES 13\n5. Acknowledgement\nThis work was supported in part by NIH/NCI under Award numbers R01CA233888,\nR01CA237267, R21CA264772, and NIH/NIBIB under Award numbers R01EB026646,\nR01HL151561, R01EB031102.\nReferences\nAl-Shabi, M., Shak, K. and Tan, M. (2022). Procan: Progressive growing channel\nattentive non-local network for lung nodule classiﬁcation, Pattern Recognition\n122: 108309.\nArmato III, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves,\nA. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoﬀman, E. A. et al. (2011). The\nlung image database consortium (lidc) and image database resource initiative (idri): a\ncompleted reference database of lung nodules on ct scans, Medical physics 38(2): 915–\n931.\nBaumgartner, M., J¨ ager, P. F., Isensee, F. and Maier-Hein, K. H. (2021). nndetection:\nA self-conﬁguring method for medical object detection, International Conference on\nMedical Image Computing and Computer-Assisted Intervention , Springer, pp. 530–\n539.\nBlandin Knight, S., Crosbie, P. A., Balata, H., Chudziak, J., Hussell, T. and Dive,\nC. (2017). Progress and prospects of early detection in lung cancer, Open biology\n7(9): 170070.\nBray, F., Ferlay, J., Soerjomataram, I., Siegel, R. L., Torre, L. A. and Jemal, A.\n(2018). Global cancer statistics 2018: Globocan estimates of incidence and mortality\nworldwide for 36 cancers in 185 countries, CA: A Cancer Journal for Clinicians\n68(6): 394–424.\nChen, T., Kornblith, S., Norouzi, M. and Hinton, G. (2020). A simple framework for\ncontrastive learning of visual representations, International conference on machine\nlearning, PMLR, pp. 1597–1607.\nCheng, G., Xie, W., Yang, H., Ji, H., He, L., Xia, H. and Zhou, Y. (2019). Deep\nconvolution neural networks for pulmonary nodule detection in ct imaging.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. et al. (2020). An image\nis worth 16x16 words: Transformers for image recognition at scale, arXiv preprint\narXiv:2010.11929 .\nDou, Q., Chen, H., Yu, L., Qin, J. and Heng, P.-A. (2017). Multilevel contextual 3-d\ncnns for false positive reduction in pulmonary nodule detection, IEEE Transactions\non Biomedical Engineering 64(7): 1558–1567.\nGu, Y., Lu, X., Yang, L., Zhang, B., Yu, D., Zhao, Y., Gao, L., Wu, L. and Zhou,\nT. (2018). Automatic lung nodule detection using a 3d deep convolutional neural\nREFERENCES 14\nnetwork combined with a multi-scale prediction strategy in chest cts, Computers in\nBiology and Medicine 103: 220–231.\nHarrison, A. P., Xu, Z., George, K., Lu, L., Summers, R. M. and Mollura, D. J. (2017).\nProgressive and multi-path holistically nested neural networks for pathological lung\nsegmentation from ct images, International conference on medical image computing\nand computer-assisted intervention, Springer, pp. 621–629.\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ ar, P. and Girshick, R. (2021). Masked\nautoencoders are scalable vision learners, arXiv preprint arXiv:2111.06377 .\nHe, K., Fan, H., Wu, Y., Xie, S. and Girshick, R. (2019). Momentum contrast for\nunsupervised visual representation learning, arXiv preprint arXiv:1911.05722 .\nHe, K., Zhang, X., Ren, S. and Sun, J. (2016). Deep residual learning for image\nrecognition, Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR).\nHofmanninger, J., Prayer, F., Pan, J., R¨ ohrich, S., Prosch, H. and Langs, G. (2020).\nAutomatic lung segmentation in routine imaging is primarily a data diversity problem,\nnot a methodology problem, European Radiology Experimental 4(1): 1–13.\nJaeger, P. F., Kohl, S. A., Bickelhaupt, S., Isensee, F., Kuder, T. A., Schlemmer, H.-P.\nand Maier-Hein, K. H. (2020). Retina u-net: Embarrassingly simple exploitation of\nsegmentation supervision for medical object detection, Machine Learning for Health\nWorkshop, PMLR, pp. 171–183.\nJiang, H. and Learned-Miller, E. (2017). Face detection with the faster r-cnn, 2017 12th\nIEEE International Conference on Automatic Face Gesture Recognition (FG 2017) ,\npp. 650–657.\nJung, H., Kim, B., Lee, I., Lee, J. and Kang, J. (2018). Classiﬁcation of lung nodules in\nct scans using three-dimensional deep convolutional neural networks with a checkpoint\nensemble method, BMC medical imaging 18(1): 1–10.\nKirby, J. S., Armato, S. G., Drukker, K., Li, F., Hadjiiski, L., Tourassi, G. D., Clarke,\nL. P., Engelmann, R. M., Giger, M. L., Redmond, G. et al. (2016). Lungx challenge\nfor computerized lung nodule classiﬁcation, Journal of Medical Imaging 3(4): 044506.\nLin, T.-Y., Goyal, P., Girshick, R., He, K. and Doll´ ar, P. (2017). Focal loss for\ndense object detection, Proceedings of the IEEE international conference on computer\nvision, pp. 2980–2988.\nLiu, X., Hou, F., Qin, H. and Hao, A. (2018). Multi-view multi-scale cnns for lung\nnodule type classiﬁcation from ct images, Pattern Recognition 77: 262–275.\nLyu, Q., Namjoshi, S. V., McTyre, E., Topaloglu, U., Barcus, R., Chan, M. D., Cramer,\nC. K., Debinski, W., Gurcan, M. N., Lesser, G. J. et al. (2021). A transformer-based\ndeep learning approach for classifying brain metastases into primary organ sites using\nclinical whole brain mri images, arXiv preprint arXiv:2110.03588 .\nMacMahon, H., Austin, J. H., Gamsu, G., Herold, C. J., Jett, J. R., Naidich, D. P.,\nPatz Jr, E. F. and Swensen, S. J. (2005). Guidelines for management of small\nREFERENCES 15\npulmonary nodules detected on ct scans: a statement from the ﬂeischner society,\nRadiology 237(2): 395–400.\nMessay, T., Hardie, R. C. and Rogers, S. K. (2010). A new computationally eﬃcient\ncad system for pulmonary nodule detection in ct imagery, Medical image analysis\n14(3): 390–406.\nNiu, C., Dasegowda, G., Yan, P., Kalra, M. K. and Wang, G. (2022). X-ray\ndissectography improves lung nodule detection, arXiv preprint arXiv:2203.13118 .\nNiu, C., Fan, F., Wu, W., Li, M., Lyu, Q. and Wang, G. (2020). Suppression of\nindependent and correlated noise with similarity-based unsupervised deep learning,\narXiv preprint arXiv:2011.03384 .\nNiu, C., Shan, H. and Wang, G. (2021). Spice: Semantic pseudo-labeling for image\nclustering, arXiv preprint arXiv:2103.09382 .\nNiu, C., Zhang, J., Wang, G. and Liang, J. (2020). Gatcluster: Self-supervised gaussian-\nattention network for image clustering, European Conference on Computer Vision ,\nSpringer, pp. 735–751.\nNiu, C., Zhang, J., Wang, Q. and Liang, J. (2018). Weakly supervised semantic\nsegmentation for joint key local structure localization and classiﬁcation of aurora\nimage, IEEE Transactions on Geoscience and Remote Sensing 56(12): 7133–7146.\nNLST (2017). https://cdas.cancer.gov/nlst/. Accessed: 2022-04-17.\nPan, J., Zhang, H., Wu, W., Gao, Z. and Wu, W. (2021). Multi-domain integrative swin\ntransformer network for sparse-view tomographic reconstruction.\nPinsky, P. F., Bellinger, C. R. and Miller Jr, D. P. (2018). False-positive screens and\nlung cancer risk in the national lung screening trial: implications for shared decision-\nmaking, Journal of medical screening 25(2): 110–112.\nPrakashini, K., Babu, S., Rajgopal, K. and Kokila, K. R. (2016). Role of computer\naided diagnosis (cad) in the detection of pulmonary nodules on 64 row multi detector\ncomputed tomography,Lung India: Oﬃcial Organ of Indian Chest Society 33(4): 391.\nRen, S., He, K., Girshick, R. and Sun, J. (2015). Faster r-cnn: Towards real-time object\ndetection with region proposal networks, Advances in neural information processing\nsystems 28.\nRoos, J. E., Paik, D., Olsen, D., Liu, E. G., Chow, L. C., Leung, A. N., Mindelzun, R.,\nChoudhury, K. R., Naidich, D. P., Napel, S. et al. (2010). Computer-aided detection\n(cad) of lung nodules in ct scans: radiologist performance and reading time with\nincremental cad assistance, European radiology 20(3): 549–557.\nSetio, A. A. A., Traverso, A., De Bel, T., Berens, M. S., Van Den Bogaard, C.,\nCerello, P., Chen, H., Dou, Q., Fantacci, M. E., Geurts, B. et al. (2017). Validation,\ncomparison, and combination of algorithms for automatic detection of pulmonary\nnodules in computed tomography images: the luna16 challenge, Medical image\nanalysis 42: 1–13.\nREFERENCES 16\nShen, W., Zhou, M., Yang, F., Yu, D., Dong, D., Yang, C., Zang, Y. and Tian, J. (2017).\nMulti-crop convolutional neural networks for lung nodule malignancy suspiciousness\nclassiﬁcation, Pattern Recognition 61: 663–673.\nTian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C. and Isola, P. (2020). What makes\nfor good views for contrastive learning?, Advances in Neural Information Processing\nSystems 33: 6827–6839.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\nL. u. and Polosukhin, I. (2017). Attention is all you need, in I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R. Garnett (eds), Advances\nin Neural Information Processing Systems , Vol. 30, Curran Associates, Inc.\nYang, J., Deng, H., Huang, X., Ni, B. and Xu, Y. (2020). Relational learning between\nmultiple pulmonary nodules via deep set attention transformers, 2020 IEEE 17th\nInternational Symposium on Biomedical Imaging (ISBI) , IEEE, pp. 1875–1878.\nYoo, H., Lee, S. H., Arru, C. D., Doda Khera, R., Singh, R., Siebert, S., Kim, D., Lee, Y.,\nPark, J. H., Eom, H. J. et al. (2021). Ai-based improvement in lung cancer detection on\nchest radiographs: results of a multi-reader study in nlst dataset, European Radiology\n31(12): 9664–9674.\nYuan, J., Liu, X., Hou, F., Qin, H. and Hao, A. (2018). Hybrid-feature-guided lung\nnodule type classiﬁcation on ct images, Computers & Graphics 70: 288–299.\nZhang, H., Peng, Y. and Guo, Y. (2022). Pulmonary nodules detection based on multi-\nscale attention networks, Scientiﬁc Reports 12(1): 1–14.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6760998964309692
    },
    {
      "name": "CAD",
      "score": 0.6700739860534668
    },
    {
      "name": "Embedding",
      "score": 0.6315084099769592
    },
    {
      "name": "Transformer",
      "score": 0.6237654685974121
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5816820859909058
    },
    {
      "name": "Lung",
      "score": 0.5171986818313599
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48065435886383057
    },
    {
      "name": "Computer-aided diagnosis",
      "score": 0.46883267164230347
    },
    {
      "name": "Medicine",
      "score": 0.18471205234527588
    },
    {
      "name": "Voltage",
      "score": 0.09499895572662354
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Engineering drawing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210166946",
      "name": "Imaging Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I165799507",
      "name": "Rensselaer Polytechnic Institute",
      "country": "US"
    }
  ]
}