{
  "title": "TypeFormer: transformers for mobile keystroke biometrics",
  "url": "https://openalex.org/W4401108771",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4222605018",
      "name": "Stragapede, Giuseppe",
      "affiliations": [
        "Universidad Autónoma de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A4281802710",
      "name": "Delgado-Santos, Paula",
      "affiliations": [
        "Telefonica Research and Development"
      ]
    },
    {
      "id": null,
      "name": "Tolosana Moranchel, Rubén",
      "affiliations": [
        "Universidad Autónoma de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A4222605019",
      "name": "Vera-Rodriguez, Ruben",
      "affiliations": [
        "Universidad Autónoma de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2204466891",
      "name": "Guest Richard",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": null,
      "name": "Morales Moreno, Aythami",
      "affiliations": [
        "Universidad Autónoma de Madrid"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980714463",
    "https://openalex.org/W4210699789",
    "https://openalex.org/W2471556897",
    "https://openalex.org/W4220774629",
    "https://openalex.org/W3206472810",
    "https://openalex.org/W3173515334",
    "https://openalex.org/W3113333528",
    "https://openalex.org/W4298108777",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2979051136",
    "https://openalex.org/W4321020646",
    "https://openalex.org/W2550436552",
    "https://openalex.org/W3081559749",
    "https://openalex.org/W3113682125",
    "https://openalex.org/W4295679781",
    "https://openalex.org/W2304304210",
    "https://openalex.org/W2956092100",
    "https://openalex.org/W2567229823",
    "https://openalex.org/W2083358597",
    "https://openalex.org/W1983159352",
    "https://openalex.org/W2023372221",
    "https://openalex.org/W2546299758",
    "https://openalex.org/W2107589078",
    "https://openalex.org/W71453884",
    "https://openalex.org/W2084748102",
    "https://openalex.org/W1991868507",
    "https://openalex.org/W2109890574",
    "https://openalex.org/W2093598585",
    "https://openalex.org/W2039420041",
    "https://openalex.org/W2960844575",
    "https://openalex.org/W2013372444",
    "https://openalex.org/W1515219102",
    "https://openalex.org/W2076501458",
    "https://openalex.org/W2055925964",
    "https://openalex.org/W2561229552",
    "https://openalex.org/W2785686043",
    "https://openalex.org/W2054072338",
    "https://openalex.org/W2763543363",
    "https://openalex.org/W2787175445",
    "https://openalex.org/W2488083631",
    "https://openalex.org/W2056484502",
    "https://openalex.org/W2502611758",
    "https://openalex.org/W1977346231",
    "https://openalex.org/W2903604062",
    "https://openalex.org/W3027363955",
    "https://openalex.org/W2575313616",
    "https://openalex.org/W3044729815",
    "https://openalex.org/W3037560939",
    "https://openalex.org/W2795386701",
    "https://openalex.org/W4291890947",
    "https://openalex.org/W2334700837",
    "https://openalex.org/W2404010118",
    "https://openalex.org/W4210512452",
    "https://openalex.org/W3178729262",
    "https://openalex.org/W2783471871",
    "https://openalex.org/W3003490236",
    "https://openalex.org/W4387743988",
    "https://openalex.org/W3176590546",
    "https://openalex.org/W4382934573",
    "https://openalex.org/W6610928041",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2104328027",
    "https://openalex.org/W4213449268",
    "https://openalex.org/W4316925059",
    "https://openalex.org/W4392454913",
    "https://openalex.org/W2096733369",
    "https://openalex.org/W4392158419",
    "https://openalex.org/W3103303311"
  ],
  "abstract": "Abstract The broad usage of mobile devices nowadays, the sensitiveness of the information contained in them, and the shortcomings of current mobile user authentication methods are calling for novel, secure, and unobtrusive solutions to verify the users’ identity. In this article, we propose TypeFormer, a novel transformer architecture to model free-text keystroke dynamics performed on mobile devices for the purpose of user authentication. The proposed model consists in temporal and channel modules enclosing two long short-term memory recurrent layers, Gaussian range encoding, a multi-head self-attention mechanism, and a block-recurrent transformer layer. Experimenting on one of the largest public databases to date, the Aalto mobile keystroke database, TypeFormer outperforms current state-of-the-art systems achieving equal error rate values of 3.25% using only five enrolment sessions of 50 keystrokes each. In such way, we contribute to reducing the traditional performance gap of the challenging mobile free-text scenario with respect to its desktop and fixed-text counterparts. To highlight the design rationale, an analysis of the experimental results of the different modules implemented in the development of TypeFormer is carried out. Additionally, we analyse the behaviour of the model with different experimental configurations such as the length of the keystroke sequences and the amount of enrolment sessions, showing margin for improvement.",
  "full_text": "ORIGINAL ARTICLE\nTypeFormer: transformers for mobile keystroke biometrics\nGiuseppe Stragapede1 • Paula Delgado-Santos2 • Ruben Tolosana1 • Ruben Vera-Rodriguez1 •\nRichard Guest3 • Aythami Morales1\nReceived: 6 November 2023 / Accepted: 27 June 2024 / Published online: 30 July 2024\n/C211 The Author(s) 2024\nAbstract\nThe broad usage of mobile devices nowadays, the sensitiveness of the information contained in them, and the shortcomings\nof current mobile user authentication methods are calling for novel, secure, and unobtrusive solutions to verify the users’\nidentity. In this article, we propose TypeFormer, a novel transformer architecture to model free-text keystroke dynamics\nperformed on mobile devices for the purpose of user authentication. The proposed model consists in temporal and channel\nmodules enclosing two long short-term memory recurrent layers, Gaussian range encoding, a multi-head self-attention\nmechanism, and a block-recurrent transformer layer. Experimenting on one of the largest public databases to date, the\nAalto mobile keystroke database, TypeFormer outperforms current state-of-the-art systems achieving equal error rate\nvalues of 3.25% using only ﬁve enrolment sessions of 50 keystrokes each. In such way, we contribute to reducing the\ntraditional performance gap of the challenging mobile free-text scenario with respect to its desktop and ﬁxed-text coun-\nterparts. To highlight the design rationale, an analysis of the experimental results of the different modules implemented in\nthe development of TypeFormer is carried out. Additionally, we analyse the behaviour of the model with different\nexperimental conﬁgurations such as the length of the keystroke sequences and the amount of enrolment sessions, showing\nmargin for improvement.\nKeywords Keystroke dynamics /C1 Transformers /C1 Biometrics /C1 Mobile devices /C1 HCI\n1 Introduction\nThe rapid digitalisation of the society, together with the\npervasiveness of mobile devices, is making room for\nunprecedented human–computer interaction (HCI) scenar-\nios. Most people are now constantly connected to the\ninternet through their mobile devices, accessing remotely\ntheir private data, and carrying out sensitive operations in\nsectors such as Banking, Financial Services and Insurance\n(BFSI), healthcare, e-commerce, and government, among\nmany others [ 1]. This trend has increased the amount of\ncybercrimes observed [ 2], evidencing the need for novel\nand reliable security methods that fulﬁl context-speciﬁc\nconstraints, such as: (i) continuous protection; (ii) user-\nfriendliness; (iii) limited processing load, compatible with\n& Giuseppe Stragapede\ngiuseppe.stragapede@estudiante.uam.es\nPaula Delgado-Santos\npaula.delgadodesantos@telefonica.com\nRuben Tolosana\nruben.tolosana@uam.es\nRuben Vera-Rodriguez\nruben.vera@uam.es\nRichard Guest\nr.m.guest@soton.ac.uk\nAythami Morales\naythami.morales@uam.es\n1 Biometrics and Data Pattern Analytics (BiDA) Lab,\nUniversidad Autonoma de Madrid, 28049 Madrid, Spain\n2 Telefonica Research, Barcelona, Spain\n3 School of Electronics and Computer Science, University of\nSouthampton, Southampton SO17 1BJ, United Kingdom\n123\nNeural Computing and Applications (2024) 36:18531–18545\nhttps://doi.org/10.1007/s00521-024-10140-2(0123456789().,-volV)(0123456789().,- volV)\nmobile environment speciﬁcations; and (iv) immunity to\nspooﬁng. To meet such requirements, recent studies have\nexplored the feasibility of the user’s behavioural1 biometric\ntraits as an authentication method to create an additional\ntransparent security layer on top of traditional approaches\n[3, 4]. In fact, such traits can be constantly veriﬁed in a\npassive way [5, 6], i.e. without having the user to carry out\nany speciﬁc entry-point authentication task, such as placing\ntheir ﬁngertip on the dedicated sensor, or typing a pass\ncode, thus addressing (i) and (ii). Such methods are also\nconvenient as mobile devices come equipped with several\nsensors that can be treated as sources of biometric\nmodalities [ 7, 8]. Mobile behavioural biometric traits are\nalso captured as low-dimensional time-domain signals, i.e.\nthe acquisition and processing is fast (iii). Additionally, it\nhas been argued that spooﬁng behavioural biometrics\nrequires more advanced technical skills compared to their\nphysiological counterparts (iv) [ 2]. Keystroke dynamics\nrepresents one of the most popular and high-performance\nauthentication methods among mobile behavioural bio-\nmetrics [9].\nIn the present work, we propose a novel transformer\narchitecture, TypeFormer, for mobile keystrokes dynamics\nfor the purpose of user authentication. Transformers are\nrecent deep learning (DL) networks, originally characterised\nby an encoder–decoder architecture [ 10]. Since their pro-\nposal, Transformers have been growing steadily due to their\nwide-ranging modelling abilities in several application ﬁelds\nsuch as computer vision, machine translation, reinforcement\nlearning, time-series analysis for classiﬁcation and predic-\ntion, etc. [11]. In particular, in the present study, we propose a\nTransformer network based on a two-branch (temporal and\nchannel modules) architecture with long short-term memory\n(LSTM) recurrent layers, Gaussian RANGE ENCODING\n(GRE), a multi-head self-attention mechanism, and a block-\nrecurrent transformer layer (Fig. 3). TypeFormer is able to\nmap slices of keystroke sequences into a feature embedding\nspace where representations of sequences belonging to the\nsame subject (intra-subject variability) are closer than those\nbelonging to different subjects (inter-subject variability).\nTypeFormer is trained with the triplet loss function, and the\nsimilarity of the feature embeddings is measured with\nEuclidean distance.\nIn this way, while subjects type freely on their devices,\nTypeFormer might verify their identities passively by\ncomparing and processing continuously acquired data\nsamples with previously acquired and processed enrolment\ndata (Fig. 1).\nIn brief, the main contributions of the current work are\nas follows:\n• We propose TypeFormer, a novel Transformer architecture\nfor biometrics keystroke free-text veriﬁcation (Fig.3).\n• We provide an analysis of the different modules that\ncompose the ﬁnal architecture, starting from the\noriginal Vanilla Transfomer, ﬁrst considering only the\ntemporal module (with and without the recurrent\nlayers), then the channel module only, to reach the\nﬁnal conﬁguration of TypeFormer;\n• We perform an in-depth comparison with recent state-\nof-the-art keystroke veriﬁcation systems based on\nLSTM recurrent neural networks (RNN) and Trans-\nformers. By replicating the experimental protocol and\nadopting the same dataset [12], we outperform previous\napproaches [13, 14] in terms of equal error rate (EER),\ni.e. 3.25% using only ﬁve enrolment sessions consisting\nin 50-keystroke sequences. As a result, we also reduce\nthe traditional performance gap existing between\nmobile free-text and desktop ﬁxed-text scenarios.\nFinally, we also analyse the behaviour of the model\nwith different experimental conﬁgurations such as the\nlength of the keystroke sequences and the amount of\nenrolment sessions.\n• We make our experimental framework available to the\nresearch community, aiming to contribute to advancing\nthe state of the art of keystroke biometrics\n2.\nThe remainder of the article is organised as follows: Sect. 2\ndescribes key aspects of keystroke and Transformers. Then,\nSect. 3 presents the architecture of TypeFormer. The main\ncharacteristics of the databases considered are reported in\nSect. 4. In Sect. 5, a detailed description of the experi-\nmental setup is reported. Section 6 contains the experi-\nmental results and the comparison with the state of the art.\nFinally, in Sect. 7, we sum up our contributions and expose\nfuture research lines.\n2 Related works\n2.1 Keystroke biometrics\nRaw keystroke data generally consist in the timestamps of\nthe actions of pressing and releasing a key, the key code\ntyped, and additional features depending on the speciﬁc\nacquisition device such as the pressure and the area size of\nthe ﬁnger. From the raw data, several features are com-\nmonly extracted:\n1 In contrast with physiological biometrics, which pertains to the\nbiological characteristics of an individual, such as face or ﬁngerprint,\nall means that enable or contribute to differentiating between\nindividuals throughout the way they perform activities are labelled\nas behavioural, i.e. gait, keystroke dynamics, handwritten signature,\netc.\n2 https://github.com/BiDAlab/TypeFormer.\n18532 Neural Computing and Applications (2024) 36:18531–18545\n123\n• Latencies, i.e. the time intervals of press-to-press, press-\nto-release (which is also known as the hold time ),\nrelease-to-release, and release-to-press (ﬂy time ) events.\n• Frequencies, such as the number of times per second a\nkey is pressed or released.\n• Error rates, related to the usage of backspaces or\ndeletion options.\n• Screen coordinates (x, y) and their displacement, angles,\nvelocity, acceleration, etc.\nMoreover, a typical classiﬁcation of the keystroke systems\nis based on the text format [ 15]: ﬁxed text (also known as\ntext-dependent), in which the sequences of the keys typed\nby the user are pre-determined, as in the case of login\ncredentials, and free text (text-independent), in which the\nsequences of keys typed are arbitrary, as in the case of\nmessages. The latter entails additional challenges in com-\nparison with the former, i.e. the unstructured and sparse\nnature of the information captured, more frequent typing\nerrors, and differences in between enrolment and veriﬁca-\ntion sessions, leading to a higher intra-subject variability.\nThe performance might also be affected if the same subject\nis able to speak different languages [ 16]. As a result, the\nperformance reachable in the free-text scenario is usually\nworse than in the case of the ﬁxed-text one [ 13].\nAlthough biometric recognition based on keystroke has\nbeen investigated for over a decade [ 17, 18], it can be still\nconsidered a biometric modality at the early stages, espe-\ncially for mobile devices. In fact, before their application to\nmobile touchscreens, keystroke dynamics has been studied\non the mechanical keyboards of desktop and laptop com-\nputers, for which, up to date, more in-depth evaluations\nhave been conducted, and commercial applications have\nbeen proposed [ 17]. In addition, mobile devices entail\nfurther challenges with respect to desktop ones, such as the\nunconstrained and non-stationary acquisition conditions,\npossibly due to the users’ activity, body position, emotional\nstate, etc. [ 19].\nWe describe next some of the key factors in the devel-\nopment and evaluation of a keystroke dynamics system:\n• Authentication performance, quantiﬁed through popular\nmetrics in the ﬁeld of biometrics, such as EER, false\nacceptance rate (FAR), false rejection rate (FRR), true\nacceptance rate (TAR), accuracy, area under the curve\n(AUC), etc.\n• Number of data subjects included in the database for\ndevelopment and evaluation of the technology.\n• Amount of data required for each subject, i.e. number\nand duration of enrolment and veriﬁcation sessions.\n• Text format: ﬁxed text, transcript, or fully free text.\n• Time interval between two acquisition sessions of the\nsame subject, which can be a major source of variability\ndue to biometric ageing, as observed in other beha-\nvioural biometric modalities [ 20].\n• Information acquired, such as the timestamps of the\nactions of pressing and releasing a key, the key code\ntyped, and additional features depending on the speciﬁc\nacquisition device such as the pressure.\n• Instructions given to the subject during data acquisition\nwhich can lead to a restricted acquisition environment.\n• Other parameters such as the memory required to store\nand deploy the model, prediction time, etc.\nA typical issue of the ﬁeld of keystroke biometrics is the\nheterogeneity of databases, experimental protocols, and\nmetrics. Therefore, a rigorous comparison between the\ndifferent performance values is a difﬁcult operation. To\nFig. 1 Graphical representation of the workﬂow of TypeFormer, the proposed biometric keystroke free-text veriﬁcation system\nNeural Computing and Applications (2024) 36:18531–18545 18533\n123\nalleviate this aspect, Morales et al. provided a common\nexperimental framework for the ﬁxed-text format by pre-\nsenting the Keystroke Biometrics Ongoing Competition\n(KBOC) for user authentication using keystroke biometrics\n[21].\n2.2 Biometric keystroke verification\nThis section provides an overview of the key aspects of\nprevious keystroke veriﬁcation systems presented in the\nliterature. The discussed studies are also reported in\nTable 1 in chronological order. We consider systems\ndeveloped in both desktop ( D) and mobile ( M) scenarios.\n2.2.1 Traditional approaches\nIn one of the earliest pioneering works on keystroke bio-\nmetrics [ 22], Monrose and Rubin proposed a free-text\nkeystroke algorithm by using the mean latency and stan-\ndard deviation of digraphs and computing the Euclidean\ndistance between each test sequence and the reference\nproﬁle. Gunetti and Picardi [23] then extended the previous\nalgorithm to n-graphs. More recently, due to their popu-\nlarity, similar methods were used in [ 35] (2015) to study\nthe effect of the data size on the performance of free-text\nkeystroke, in [41] (2017) to study how detecting the user’s\nposition before authentication can signiﬁcantly improve\nperformance, and in [ 43] (2017) for benchmarking the\nlarge-scale database published, the Clarkson II database.\nThe inclusion of time-related features such as rhythm and\ntempo was proposed in [ 28]. The random forest (RF)\nclassiﬁer was adopted in [ 53] to assess which are the most\nsigniﬁcant features of digraph-based algorithms (2020).\nA very popular method for keystroke biometrics is\nsupport vector machine (SVM). Following the previous\nﬁndings, in [ 30] and [ 38], combinations of the existing\ndigraphs method for feature extraction and a SVM classi-\nﬁer to authenticate users were proposed. SVM was also\nadopted in [ 29] and in [ 33] in conjunction with mobile\ndevice background sensor data. Regardless of the classiﬁer\nused, fusing keystroke dynamics with simultaneous\nmovement sensor data included in mobile devices has\nproved to be very beneﬁcial in terms of authentication\nresults [5, 9, 52]. In a broad study (2018), Cilia et al. [ 49]\nstudied how differentiating typing modes (one or two\nhands) and user activity (standing or moving) during the\ndevelopment of a keystroke veriﬁcation system based on\nSVM can improve the authentication performance\nsigniﬁcantly.\nAmong other classiﬁers, we mention Hidden Markov\nModels (HMM), used in [ 24] to exploit typing rhythms in\nkeystroke dynamics, and then extended by Monaco et al.\n[44] into Partially Observable Hidden Markov Models\n(POHMM). With k-Nearest Neighbour ( k-NN) [ 25] and\nfuzzy logic [27], promising results have also been achieved\nin the early days of mobile keystroke biometrics. In the\nsame epoch (2009), Killourhy and Maxion collected one of\nthe ﬁrst public databases of the ﬁeld, the CMU keystroke\ndynamics database, and they carried out a benchmark\nevaluation with 14 different algorithms including Man-\nhattan, Euclidean, and Mahalanobis distances, k-Nearest\nNeighbour, SVM (one-class), a neural network, fuzzy\nlogic, and k-means [ 26]. A similar benchmark study was\nconducted in [ 42] on several algorithms such as Gaussian\nand Parzen Window Density Estimation, one-class SVM,\nk-NN, and\nk-means.\n2.2.2 Deep learning approaches\nThe advent of DL-based systems has not spared the ﬁeld of\nkeystroke biometrics, improving signiﬁcantly the authen-\ntication performance, in particular in the more challenging\nfree-text scenario. In [ 31] (2013), it was shown that a deep\nneural network was capable of outperforming other algo-\nrithms on the CMU keystroke dynamics database [ 26].\nApproaches based on neural networks were also used for\ncomplementary tasks to improve the authentication per-\nformance, such as predicting the digraphs that are not\npresent among the enrolment sessions by analysing the\nrelation between the keystrokes [ 32]. In [ 39], a convolu-\ntional neural network (CNN) was introduced in combina-\ntion with a Gaussian data augmentation technique for the\nﬁxed-text scenario, while in [ 34], a neural network was\napplied to RGB histograms obtained from ﬁxed-text key-\nstroke data. Moreover, multi-layer perceptron (MLP)\narchitectures have also been explored [ 58]( M).\nIn [50], based on the observation that a RNN is a very\nsuitable structure to learn from time-series [ 60, 61], a\ncombination of a convolutional and a recurrent network\nwas proposed in order to extract higher level keystroke\nfeatures on the SUNY Buffalo database [ 51] (2019). The\nconvolution process is performed before feeding the\nsequence to the recurrent network to characterise the key-\nstroke sequence better. RNN variants are popular in key-\nstroke biometrics, such as in [ 55] (birectional RNN) or in\n[59]( M), in which keystroke sequences are arranged as an\nimage-like matrix and then processed by a CNN combined\nwith a gated recurrent unit (GRU) network. In 2021, Acien\net al. presented TypeNet [ 13], a Siamese LSTM RNN for\nfree-text keystroke biometrics. They considered the largest\npublic databases to date, collected by researchers from the\nAalto University, [54], and [12], with, respectively, around\n168,000 and 68,000 subjects of free-text keystroke data\ndivided into 15 acquisition sessions per subject. In their\nwide-ranging work, among other things, they achieved\nstate-of-the-art authentication results at large scale in terms\n18534 Neural Computing and Applications (2024) 36:18531–18545\n123\nTable 1 Summary of different approaches presented in the literature for keystroke dynamics veriﬁcation\nStudy Database (Public) Number\nof\nsubjects\nScenario Classiﬁer\n1 Performance [%] Text\nformat\nData\namount\nMonrose and\nRubin [22]\nSelf-collected (7)4 2 D Weighted Euclidean\ndist\n90.7 (Acc.) for Fixed Text\n23.0 (Acc.) for Free\nText\nFixed,\nfree\nFew\nsentences\nGunetti and\nPicardi [23]\nSelf-collected (7) 205 D Different distance\nmeasures\n\\ 0.005 (FAR), \\ 5\n(FRR)\nFree 700–900\ncharacters\nJiang et al.\n[24]\nSelf-collected (7)5 8 D HMM 2.54 (ERR) Fixed 20 strokes\non average\nSaevanee\net al. [ 25]\nSelf-collected (7)1 0 M k-NN 99.0 (Accuracy) Fixed 10-digit\nnumbers\nKillourhy and\nMaxion\n[26]\nCMU database ( 4)5 1 D Manhattan dist., k-NN,\nSVM, Mahalanobis,\nNN, Euclidean dist.,\nFL, k-means\n0.096 (EER) with\nManhattan dist.\nFixed 10\nkeystrokes\nZahid et al.\n[27]\nSelf-collected (7)2 5 M FL, PSO 2.07 (FAR), 1.73 (FRR) Fixed 250\nkeystrokes\nHwang et al.\n[28]\nSelf-collected (7)2 5 M FF-MLP, RBFN, NN 4 (EER) Fixed 4 digits\nGiot et al.\n[29]\nGREYC Web-based ( 4)\n[29]\n100 D SVM 15.28 (EER) Fixed 5 captures\nBalagani\net al. [ 30]\nSelf-collected (7)3 4 D SVM \\ 1 (Average Error Rate) Free\ntext\n500\nkeystrokes\nDeng and\nZhong [31]\nCMU database ( 4)[ 26]5 1 D GMM, NN 3.5 /C0 5.5 (EER) Fixed,\nfree\n1 sequence\nAhmed et al.\n[32]\nSelf-collected (4)5 3 D Neural network Controlled: 2.13 (EER, 0\nFAR, 5 FRR)\nUncontrolled: 2.46\n(EER, 0.01 FAR, 4.8\nFRR)\nFree 500 actions\nGascon et al.\n[33]\nSelf-collected (7) 300 M SVM 92 (TAR at 1% FAR) Free 160\nkeystrokes\nAlpar [34] Self-collected ( 7)1 0 D NN, RGB histograms 90 (Acc.) Fixed 15\ncharacters\nHuang et al.\n[35]\nClarkson I ( 4)[ 36]3 9 D Same as [ 23] /C24 1 (Impostor Pass Rate) Free 1 k–10 k\nkeystrokes\nMorales et al.\n[21]\nBiosecurID (4)[ 37] 300 D Manhattan 5.32 (EER) Fixed /C24 25\nkeystrokes\nC¸ eker and\nUpadhyaya\n[38]\nClarkson I ( 4)[ 36]3 4 D SVM /C24 0 (EER) Free 500\nkeystrokes\nC¸ eker and\nUpadhyaya\n[39]\nCMU database ( 4)[ 26],\nGREYC Keystroke ( 4)\n[40], GREYC Web-\nBased (4)[ 29]\n267 D CNN 2.02 (EER) Free Few\nkeystrokes\nCrawford\net al. [ 41]\nSelf-collected (7)3 6 M Decision Tree [ 93 (AUC) Free Few\nkeystrokes\nKim et al.\n[42]\nSelf-collected (7) 150 D GDE, PWDE, 1-SVM,\nk-NN, and k-means\n(EER: 0.44 for Korean,\n0.84 for English)\nFree 100–1000\nkeystrokes\nMurphy et al.\n[43]\nClarkson II ( 4)[ 43] 103 D Same as [ 23] 2.17 /C0 10.7 (EER) Free 1000\nkeystrokes\nMonaco et al.\n[44]\nCMU database ( 4)[ 26],\n(4)[ 45], (4)[ 46], (4)\n[\n47], (4)[ 48]\n/C24 50 D POHMM 0.6–9 (EER), 60.7 /C0 97.1\n(Accuracy)\nFixed,\nfree\n0.12/C0 55.18\nevents (on\naverage)\nCilia et al.\n[49]\nSelf-collected (4)2 4 M SVM 0.44 /C0 3.93 (EER) Fixed Sentence\nbased\nNeural Computing and Applications (2024) 36:18531–18545 18535\n123\nof EER (%) while attempting to minimise the amount of\ndata per subject required for enrolment. Following [ 13], in\n[14], in 2022, we presented a preliminary attempt to use a\nTransformer architecture for keystroke biometrics, out-\nperforming TypeNet in a speciﬁc experimental setup. We\nselected [ 13] as a reference study for several reasons:\n(i) They adopt the largest mobile free-text keystroke\ndatabases available, the Aalto mobile keystroke database\n[12], (ii) their experimental protocol is publicly available\non GitHub, allowing us to use the same sets of subjects and\nmetrics, for development and evaluation, and (iii) they\nachieved state-of-the-art results for free-text mobile key-\nstroke biometrics. Consequently, references [ 13] and [ 14]\nare particularly relevant to the current study as they use the\nsame development and evaluation databases, and experi-\nmental protocol, allowing a direct comparison of the pro-\nposed systems (Sect. 6). Recently, in [ 62], a novel\napproach called DoubleStrokeNet for recognising subjects\nusing bigram embeddings was proposed. DoubleStrokeNet\nconsiders a Transformer-based neural network that distin-\nguishes between different bigrams. Additionally, self-su-\npervised learning techniques were employed to compute\nembeddings for both bigrams and users. The authors\nexperimented with the Aalto databases, reaching very\ncompetitive results in terms of recognition performance.\nLeveraging the temporal features of speciﬁc bigrams is a\nroute of potential interest in modelling subjects’ typing\nbehaviour. It is difﬁcult to compare results across different\nstudies, which adopt different experimental settings, e.g.\ntraining and evaluation data.\n2.3 Introduction to transformers\nThe ﬁrst Transformer was proposed by Vaswani et al. as a\nnew encoder–decoder architecture [ 10]. Such model, later\nnicknamed the Vanilla Transformer, is based purely on\nattention mechanisms, abandoning the idea of using con-\nvolutions or recurrence. The Vanilla Transformer was\nTable 1 (continued)\nStudy Database (Public) Number\nof\nsubjects\nScenario Classiﬁer\n1 Performance [%] Text\nformat\nData\namount\nLu et al. [ 50] SUNY buffalo ( 4)[ 51],\nClarkson II ( 4)[ 43]\n75 D CNN ? RNN 2.67 (EER) Free 30\nkeystrokes\nKim et al.\n[52]\nSelf-collected (4)5 0 M KS stat \\ 0.05 (EER) Free /C24 200\nkeystrokes\nAyotte et al.\n[53]\nSUNY Buffalo ( 4)[ 51],\nClarkson II ( 4)[ 43]\n101, 148 D RF 7.8 (EER) Free 200\ndigraphs\nAcien et al.\n[13]\nAalto databases ( 4)\n[12, 54], SUNY Buffalo\n(4)[ 51], Clarkson II\n(4)[ 43]\n168 K D; M RNN 9.2 (EER) for M, 2.2 for\nD\nFree 30–150\nkeystrokes\nEl-Kenawy\net al. [ 55]\nRHU dataset [ 56], MEU-\nMobile KSD Dataset\n[57]\n101, 148 M Bi-RNN 99.02 (Acc.), 99.32 (Acc.) Fixed Few\nkeystrokes\nStylios et al.\n[58]\nSelf-collected (4)3 9 M MLP 97.18 (Acc.) Fixed /C24 2 min\nsessions\nLi et al. [ 59] SUNY buffalo ( 4)[ 51],\nClarkson II ( 4)[ 43]\n101, 148 D CNN ? RNN 97.68 (Acc.), 88.62 (Acc.) Free 50\nkeystrokes\nStragapede\net al. [ 14]\nAalto Database M (4)\n[12]\n60 K M Transformer 3.84 (EER) Free 50\nkeystrokes\nTypeFormer Aalto databases ( 4)\n[12, 54], SUNY Buffalo\n(4)[ 51], Clarkson II\n(4)[ 43]\n60 K D; M Transformer 3.25 (EER) Free 30–100\nkeystrokes\n1HMM = Hidden Markov Models, k-NN = k-Nearest Neighbours, SVM = Support Vector Machine, NN = Neural Network, FL = Fuzzy Logic,\nPSO = Particle Swarm, Optimisation, FF-MLP = Feed-Forward Multi-Layer Perceptron, RBFN = Radial Basis Function Network, GMM =\nGaussian Mixture Model, CNN = Convolutional NN, GDE = Gaussian, Density Estimator, PWDE = Parzen Window Density Estimator,\nPOHMM = Partially Observable HMM, RNN = Recurrent Neural Network, KS = Kolmogorov–Smirnov, RF = Random, Forest, Bi-RNN =\nBidirectional RNN, and MLP = Multi-Layer Perceptron\n18536 Neural Computing and Applications (2024) 36:18531–18545\n123\nproposed for the task of machine translation, achieving\nremarkable results in comparison with existing systems in\nterms of quality of text translation and time consumption.\nIn comparison with existing DL architectures such as\nCNNs or RNNs, the main advantages of the Transformer\ncan be summarised as follows: (i) All sequences are pro-\ncessed in parallel; (ii) a self-attention mechanism is intro-\nduced to deal with long sequences; (iii) the training is more\nefﬁcient, modelling the whole sequences at once; and (iv)\ninspection of the whole sequences at once, without the\nneed to summarise previous samples [ 10, 63, 64].\nLater, several variations of the original Transformer\narchitecture have been proposed to overcome some of its\ndrawbacks and to deploy it in other application ﬁelds. In\nfact, its quadratic computational complexity and its con-\nsiderable memory usage limited its application to longer\ntime-series signals. To alleviate these aspects, the Two-\nstream Convolution Augmented Human Activity Trans-\nformer (THAT) was proposed by Li et al. for the task of\nhuman activity recognition (HAR) [ 65]. Such architecture\nwas designed based on the assumption that, similarly to\nimages, time-series signals have information in two\ndimensions. Therefore, the model comprises two modules:\n(i) the temporal module (extracting time features from\nunchanged data) and (ii) the channel module (extracting\nchannel features from transposed data). Then, the features\nextracted by each of the modules are concatenated for the\nprediction task. Another example of an interesting Trans-\nformer architecture variation is given by the block-recur-\nrent transformer, that has been recently introduced by\nHutchins et al. for the task of auto-regressive language\nmodelling [64]. In this approach, thanks to the recurrent on\nseries-wise connexions, all previous temporal information\nis retained. Furthermore, two attention mechanisms are\napplied at the same time (full- and cross-attention).\nIn the light of these and other adaptations, the popularity\nof Transformers increased in the past years due to the\nremarkable results obtained in other ﬁelds such as com-\nputer vision, reinforcement learning, time-series analysis\nfor classiﬁcation and prediction, biometrics, etc. [ 11, 66].\nRecently, this lead to the emergence of large pre-trained\nTransformers, also referred to as foundation models (FMs),\nrenowned for their adaptability across diverse tasks. These\nexpansive pre-trained Transformers encompass varied\narchitectures tailored to speciﬁc tasks, including large\nlanguage models (LLMs) for natural language processing\n[67], vision Transformers (ViT) for visual tasks [ 68], and\nmultimodal Transformers for tasks involving multiple\nmodalities. Despite the widespread adoption of these\nsophisticated models, within the realm of behavioural\nbiometrics, a scarcity of data presents a signiﬁcant chal-\nlenge, thereby limiting the evaluation of these models in\nthis speciﬁc task domain within existing literature. A\nthorough discussion of Transformers in different domains\nis out of the scope of the current article. Nevertheless, we\nrecommend two excellent surveys about vision Trans-\nformers [68] and Transformers for time-series [ 69].\nA preliminary version of this work was published in [14]\nas the ﬁrst application of Transformers to keystroke bio-\nmetrics. This article signiﬁcantly improves [ 14] in the\nfollowing aspects: (i) We propose a new Transformer\narchitecture, TypeFormer, leading to an improvement of\nthe authentication performance; (ii) we provide a more\nextensive evaluation of the model, analysing the behaviour\nof the system with different experimental conditions such\nas the number of enrolment sessions and the length of the\nkeystroke sequences; and (iii) we provide an in-depth\nanalysis of state-of-the-art keystroke veriﬁcation systems,\nremarking key aspects such as the scenario (ﬁxed or free\ntext) and database considered, classiﬁer, and performance.\n3 Proposed system: TypeFormer\nThis section contains a detailed description of all aspects of\nthe proposed keystroke veriﬁcation system.\n3.1 Feature extraction\nThe raw keystroke information available consists essen-\ntially in the timestamp of the event of pressing (ﬁnger\ndown) and releasing (ﬁnger up) a key, together with the\nASCII code typed. Such data are processed to extract a set\nof ﬁve features per character typed:\n[hold latency, inter-key latency, press latency, release\nlatency, key pressed]\nThe above-mentioned features are shown in Fig. 2. Due\nto the fact that the length of the free-text sequences is not\nﬁxed, they are sliced or zero-padded to produce a ﬁxed-size\ninput, ( L ¼ 30; 50; 70; 100), depending on the speciﬁc\nexperiment (see Sect. 5). The ASCII code (key pressed) is\nnormalised in the range [0, 1].\n3.2 TypeFormer architecture\nFollowing the same idea presented in [ 65], TypeFormer\ncontains two modules, each of them in a speciﬁc branch, to\nwhich the pre-processed Transfomer input sequences X\n(Sect. 3.1) are fed (Fig. 3): a temporal module (temporal-\nover-channel features) and a channel module (channel-\nover-temporal features). In both channels, X is modelled\nusing a GRE to preserve the information position. The\noutput sequence is deﬁned by an L1 normalised vector\nrepresenting the probability density function (PDF) of the\nGaussian distributions G. Moreover, the ﬁnal GRE is cal-\nculated by a weighted multiplication over several ranges,\nNeural Computing and Applications (2024) 36:18531–18545 18537\n123\ncontaining the behaviour of each of the samples in a dif-\nferent scenario.\nThe temporal module contains three ordered sets of\nlayers. Each of the sets of layers is composed, respectively,\nby N, R, and M layers. The N and M layers are identical and\nmade of two sub-layers: a multi-head self-attention mech-\nanism and a multi-scale keystroke LSTM RNN layer. The\nmulti-head self-attention mechanism connects the samples\namong the whole sequence obtaining long-range depen-\ndencies. The mechanism applies a weighted sum of the\ndifferent values V over the different queries Q and the\nmatching keys K. The output of the self-attention sub-layer\nis the result of applying the attention mechanism to F in-\ndependent heads. Then, the multi-scale keystroke LSTM\nRNN layer is activated by ReLU functions. Each of the\nscales contains a unique kernel. Following each sub-layer,\na residual connexion and a layer normalisation are included\n(Add & Norm in Fig. 3).\nBetween the N and M layers, R recurrent layers are\nincluded (graphically represented in detail on the right side\nof Fig. 3). The structure of such layers is based on the\nblock-recurrent transformer architecture presented in [ 64].\nInitially, the input sequence is shaped by a positional\nencoding. Then, a recurrent form of attention is introduced\nin the vertical and horizontal directions, based on two sub-\nlayers in each of the directions: (i) a multi-head self-at-\ntention mechanism, which applies full-attention to the\nsequences to obtain the matching values V and keys K, and\ncross-attention to the current states (initialised to 0) to\nextract the queries Q (replicated in F independent heads);\nand (ii) a multi-scale keystroke CNN network, which\ncomprises a CNN with ReLU activations and unique ker-\nnels for each of the scales. Every sub-layer is preceded by a\nlayer normalisation and followed by a residual connexion\n(Add & Norm). While the multi-scale keystroke CNN\nnetwork remains unchanged, the multi-head self-attention\nmechanism applies cross-attention to the sequences to\nobtain the matching queries Q, and full-attention to the\ncurrent states to extract the keys K and the values V (such\nmechanism is replicated in F independent heads). Fur-\nthermore, the residual connexions are replaced by forget\ngates, altering the current states.\nThe channel module input sequence X is transposed and\nmodelled by the GRE. Then, H layers (analogous to the\nN and M layers of the Temporal Module) are included,\nfollowed by a residual connexion and a layer normalisation\n(Add & Norm).\nSubsequently, each of the modules is followed by a\nconvolutional layer, after which the similarity of the output\nfeatures is concatenated into an output vector P and fed\ninto a sigmoid layer. Finally, for the authentication task\nconsidered in the present study, the output feature\nembedding vectors are compared using the Euclidean\ndistance.\nThe architecture of TypeFormer is based on a prelimi-\nnary transformer version proposed in [ 14]. However, this\narchitecture has been modiﬁed leading to improved\nPress (ASCII) ReleaseRelease\nQQ A A\nHL IL\nPL\nRL\nTime\nPress (ASCII)\nFig. 2 Example of the keystroke features extracted from the Aalto\nmobile keystroke database [12]. HL hold latency; IL inter-key latency;\nPL press latency; RL release latency; and ASCII: key pressed\nFig. 3 Graphical representation of TypeFormer, based on a Trans-\nformer architecture for biometrics keystroke free-text veriﬁcation. T\ntransposition operation; GRE Gaussian range rncoding; N, R, M, H:\nNumber of layers of each of the modules; X Pre-processed input\nsequence; and P Output feature embedding vector\n18538 Neural Computing and Applications (2024) 36:18531–18545\n123\nbiometric recognition performance. In particular, the main\nchanges can be summarised as follows: (i) The convolu-\ntional layers in the temporal and channel modules have\nbeen changed to LSTM RNN layers, which show better\nability to model time-domain signals [ 61]; and (ii) in the\ntemporal module, a set of block-recurrent transformer\nlayers based on [64] has been included between two sets of\nidentical recurrent layers, following the idea of [ 66]. The\nblock-recurrent block introduces a recurrent form of\nattention, in alternative to using the dot-product or peri-\nodicity-based series mechanism, which ﬁx an attention\nwindow size, summarising the sequence that the model has\npreviously seen. As presented in Table 2, this leads to\nimproved recognition results.\nThe speciﬁc details of the hyperparameter implementa-\ntion for the proposed Transformer are described in\nSect. 5.1.\n4 Databases description\n4.1 The database\nThe Aalto mobile keystroke database is a large-scale\ndatabase for mobile keystroke biometrics involving around\n260,000 subjects [ 12]. In this work, we have selected all\nsubjects that completed at least 15 acquisition sessions,\nreducing the number of subjects to 62,454. The raw data\navailable in the Aalto mobile keystroke database consist in\nthe timestamps of the key press (ﬁnger down) and key\nrelease (ﬁnger up) gestures with a 1-ms resolution. The\ndata were captured through a mobile web application in an\nunsupervised way. Subjects were asked to read, memorise,\nand type in their smartphone English sentences that were\nrandomly selected from a set of 1525 sentences obtained\nfrom the Enron mobile mail [ 70] and the Gigaword\nNewswire corpora [71]. Therefore, the text format adopted\nis free text, with sentences containing at least three words\nor 70 characters. Moreover, the volunteers were asked to\ntype as fast and accurately as possible. Concerning the\nvolunteers, they were selected from 163 countries,\napproximately 68% of the subjects involved were English\nnative speakers, and around 31% of them took a typing\ncourse.\n5 Experimental protocol\n5.1 TypeFormer hyperparameters\nThe best conﬁguration found in terms of the hyperparam-\neters of the proposed Transformer is described below. To\nachieve this, several combinations of hyperparameter were\nadopted for different trainings. Then, the EER on the val-\nidation set was used to select the best model among all\ntrainings. The Gaussian range encodings contain G ¼ 20\nGaussian distributions. The temporal module comprises\nN ¼ 9, R ¼ 2, and M ¼ 1 layers with F ¼ 10 heads each,\nwhile the channel module H ¼ 1 layer with F ¼ 5 heads.\nIn both modules, the multi-scale keystroke LSTM contains\nthree recurrent layers with kernel sizes 1, 3, and 5,\nrespectively. Each of them comprises D units and ReLU\nactivation functions, followed by dropout layers with a rate\nof 0.1. The multi-scale keystroke CNN networks of the R\nrecurrent layers contain D units each (where D corresponds\nto the keystroke sequence length L), ReLU activation\nfunctions, and kernel sizes 1, 3, and 5, respectively, fol-\nlowed by dropout layers with a rate of 0.1. Subsequent to\nthe temporal and channel modules, two convolutional\nlayers are included with D units, ReLU activation func-\ntions, and kernel sizes 128 and 32, respectively. Each of the\nconvolutional layers is followed by dropout layers with a\nrate of 0.5. Finally, a max-pooling layer followed by a\nlinear layer with sigmoid activation function is included.\nThe ﬁnal output vector contains S ¼ 64 features.\n5.2 Model development\nIn order to perform a fair comparison across different DL\narchitectures, in the current work, we replicate the public\nexperimental protocol presented by Acien et al. in [ 13].\nSpeciﬁcally, data belonging to the same non-overlapping\n30,000 and 400 subjects have been used, respectively, for\nthe purpose of training and validation. Each subject data\nare organised into 15 acquisition sessions. The triplet loss\nfunction is employed for the training, and a margin of a ¼\n1:0 was set on top of the Euclidean distance for each of the\npair combinations in the triplet. Additionally, the Adam\noptimiser with a learning rate of 0.001 is used. The\nTransformer is trained for 1000 epochs, considering\nTable 2 Experimental results of\nthe different modules\nimplemented in the\ndevelopment of TypeFormer, in\ncomparison with the Vanilla\nTransformer [10]( E is the\nnumber of enrolment sessions)\nSystem E ¼ 1 E ¼ 2 E ¼ 5 E ¼ 7 E ¼ 10\nVanilla Transformer [ 10] 10.28 8.56 7.41 6.95 6.61\nTemporal Module w/o Rec. layer 8.15 6.43 5.12 4.73 4.29\nTemporal module w/ Rec. layer 7.12 5.49 3.94 3.63 3.15\nChannel module 17.29 15.50 13.54 13.07 12.55\nTypeFormer (Temp. ? Channel Module w/ Rec. Layer) 6.17 4.57 3.25 2.86 2.54\nNeural Computing and Applications (2024) 36:18531–18545 18539\n123\nroughly 30,000 triplets per epoch, arranged into 1024-se-\nquence-sized batches. The triplets are formed by sampling\nsubjects randomly and with uniform distribution across the\ntraining set. At the end of each training epoch, the model\nperformance is quantiﬁed in terms of EER, and according\nto such metric, the best model is selected to be tested on the\nﬁnal evaluation subset. TypeFormer is implemented in\nPyTorch.\n5.3 Model evaluation\nWe describe next the experiments considered in the present\nstudy to validate the proposed TypeFormer. In all of them,\ndifferent subjects are used for training and evaluating the\nkeystroke veriﬁcation model.\nThe ﬁrst experiment analyses the performance of\nTypeFormer over an evaluation set of U ¼ 1; 000 unseen\nsubjects obtained from the same database considered in\ntraining. At the end of each of the training epochs, the best\nmodel is selected using a separate validation subset. We\nfollow the same protocol as [ 13], considering E enrolment\nsessions per subject. The genuine and impostor score dis-\ntributions are subject-speciﬁc. For each subject, genuine\nscores are obtained comparing the enrolment sessions ( E)\nwith ﬁve veriﬁcation sessions. The Euclidean distances are\ncomputed for each of the veriﬁcation sessions with each of\nthe E enrolment sessions, and then, values are averaged\nover the enrolment sessions. Therefore, for each subject,\nthere are ﬁve genuine scores, one for each veriﬁcation\nsession. Concerning the impostor score distribution, for\nevery other subject in the evaluation set, the averaged\nEuclidean distance value is obtained considering one ver-\niﬁcation session and the above-mentioned ﬁve enrolment\nsessions. Consequently, for each subject, there are 999\nimpostor scores. Based on such distributions, the EER\nscore is calculated per subject, and all EER values are\naveraged across the entire evaluation set. The number of\nenrolment sessions is variable ( E ¼ 1; 2; 5; 7; 10) in order\nto assess the performance adaptation of the system to\nreduced availability of enrolment data. Additionally, also\nthe experiments are repeated changing the input sequence\nlength, L ¼ 30; 50; 70; 100, to evaluate the optimal key-\nstroke sequence length.\n6 Experimental results\nStarting from the initial vanilla transformer proposed in\n[10], to validate each part of ﬁnal proposed system, Table 2\npresents the experimental results of the different modules\nimplemented in the development of TypeFormer. The\nresults are obtained on the ﬁnal evaluation dataset of the\nAalto mobile database. This analysis is carried out by\nconsidering a variable number of enrolment sessions E ¼\n1; 2; 5; 7; 10 along the columns and sequence length\nTable 3 Intra-database\nevaluation: system performance\nresults in terms of EER for the\nﬁnal evaluation dataset of the\nAalto mobile database\nSequence length L System Number of enrolment sessions E\n12571 0\n30 Acien et al. [ 13] 14.20 12.50 11.30 10.90 10.50\nTypeFormer 9.48 7.48 5.78 5.40 4.94\n50 Acien et al. [ 13] 12.60 10.70 9.20 8.50 8.00\nPreliminary Transformer [ 14] 6.99 – 3.84 – 3.15\nTypeFormer 6.17 4.57 3.25 2.86 2.54\n70 Acien et al. [ 13] 11.30 9.50 7.80 7.20 6.80\nTypeFormer 6.44 5.08 3.72 3.30 2.96\n100 Acien et al. [ 13] 10.70 8.90 7.30 6.60 6.30\nTypeFormer 8.00 6.29 4.79 4.40 3.90\nFig. 4 DET curves comparing the performance of TypeFormer with\nTypeNet ([ 13]) for keystroke sequences of length L ¼ 50. E\ncorresponds to the number of enrolment sessions considered. The\nsolid black line y ¼ x corresponds to all possible EER points (for\nwhich FAR = FRR by deﬁnition), whereas the grey lines, respec-\ntively, represent the FRRs at 1% FAR (dotted) and FRRs at 10% FAR\n(dashed)\n18540 Neural Computing and Applications (2024) 36:18531–18545\n123\nL ¼ 50. Although the Vanilla Transformer is solely based\non attention mechanisms, it shows the effectiveness of the\nTransformer architecture in modelling keystroke sequen-\nces. First, this architecture is modiﬁed by including the\nGaussian range encoding (instead of the Positional\nEncoding originally used in the Vanilla Transformer).\nThen, the point-wise feed-forward networks of the Vanilla\nTransformer are changed with LSTM recurrent layers\n(Temporal w/o Rec. Layer). By doing so, we obtain an\nimprovement for all considered amounts of enrolment\nsessions, and the recognition performance in terms of EER\nis improved on average by a 28.70%. Following [ 64], a\nblock-recurrent transformer layer is introduced in the\ntemporal module in the case of the temporal with recurrent\nlayer conﬁguration. This further reduces the EER by a\n20.03% (Temporal w/ Rec. Layer). Finally, we considered\nthe combination of the temporal with recurrent layer and\nchannel module conﬁgurations, corresponding to the ﬁnal\nTypeFormer architecture.\nTable 3 shows the results achieved by TypeFormer\nconsidering different sequence lengths L. In addition, to\nprovide a better comparison of TypeFormer with recent\nstate-of-the-art keystroke biometric systems, we include\nthe results achieved by TypeNet in [ 13] and our prelimi-\nnary study [ 14] on the same dataset as shown in the pre-\nvious Table 2. In general, in Table 3, we can see that in all\ncases, TypeFormer outperforms previous approaches over\nthe same evaluation set of 1000 subjects. In particular, the\nperformance improvement of TypeFormer averaged over\nall cases in the table ( E ¼ 1; 2; 5; 7; 10 and\nL ¼ 30; 50; 70; 100) consists in 47.3% in relative terms\nwith respect to TypeNet [ 13], an LSTM RNN-based\nsystem.\nAdditionally, considering only the results of Table 3\nobtained by TypeFormer, it is possible to observe that in all\ncases, the EER values decrease as the number of enrolment\nsessions E increases. Such trend is predictable and con-\nsistent for all sequence lengths L. Also, the rate of\nimprovement is higher going from E ¼ 1t o E ¼ 5 sessions\n(relative improvement of almost 50% going from 6.17% to\n3.25% EER for L ¼ 50) than from E ¼ 5t o E ¼ 10 (rel-\native improvement of around 20% going from 3.25% to\n2.54% EER for L ¼ 50).\nSimilarly, by carrying out an analogous analysis along\nthe rows, it is noticeable that increasing the input sequence\nlength L from 30 to 50, there is a signiﬁcant improvement\n(42.64% in relative terms on average over all considered\nenrolment session amounts E) in terms of EER. Never-\ntheless, such trend is reversed when increasing the\nsequence length L to 70 or 100 (respectively, a perfor-\nmance degradation of 12.38% and 28.38% in relative terms\non average over all considered enrolment session amounts\nE), leading to the conclusion that the optimal sequence\nlength must be around 50. This could be due to the fact that\nthe zero-padding operation carried out to equalise the\nlength of different keystroke sequences is not beneﬁcial for\nthe Transformer-based architecture that relies on an\nattention mechanism, that can perhaps be optimised. In\ncase of the RNN-based reference system [ 13], the longer\nthe input sequences, the better the results, showing the\nbeneﬁcial effects of the masking layer included in their\nnetwork.\nTable 4 Global EER (%), FRR at 1% FAR (%), and FRR at 1% FAR\n(%) of TypeNet [ 13] and TypeFormer for different amounts of\nenrolment sessions E. Such values correspond to the intersection\npoints of the DET curves with the straight lines plotted in Fig. 4. The\nsequence length L ¼ 50\nEnrolment sessions System Global EER (%) FRR at 1% FAR (%) FRR at 10% FAR (%)\nE ¼ 1 Acien et al. [ 13] 18.20 38.99 23.19\nTypeFormer 9.72 27.97 9.53\nE ¼ 5 Acien et al. [ 13] 14.40 34.93 17.40\nTypeFormer 6.32 17.47 4.68\nE ¼ 10 Acien et al. [ 13] 13.16 32.42 14.91\nTypeFormer 5.52 12.81 3.85\nTable 5 Comparison of the performance achieved by the proposed\nTypeFormer with related systems that followed different experimental\nprotocols in the studies in which they were originally proposed ( E =\nnumber of enrolment sessions = 5 and L = number of enrolment\nsessions considered = 50)\nSystem EER (%)\nPOHMM [72] 40.40\nDigraphs [38] 29.20\nCNN?RNN [50] 12.20\nTypeNet [13] 9.20\nPreliminary Transformer [ 14] 3.84\nTypeFormer 3.25\nNeural Computing and Applications (2024) 36:18531–18545 18541\n123\nTo provide a graphical representation of the differences\nin the performance of the compared systems, Fig. 4 reports\nthe detection error trade-off (DET) curves computed for the\ndifferent number of enrolment sessions available ( L ¼ 50).\nThe graph shows that our proposed approach outperforms\nthe LSTM RNN of TypeNet in all cases, i.e. E ¼ 1\n(TypeFormer) enrolment session vs. E ¼ 10 (TypeNet).\nThis shows the ability of TypeFormer to model keystroke\ndynamics. The DET curves shown in Fig. 4 are plotted\nconsidering the entire global genuine and impostor score\ndistributions, i.e. by grouping all scores regardless of the\nspeciﬁc subject. The solid black line y ¼ x corresponds to\nall possible EER points (for which FAR ¼ FRR by deﬁ-\nnition), whereas the grey lines, respectively, represent the\nFRRs at 1% FAR (dotted) and FRRs at 10% FAR (dashed).\nFor E ¼ 5, TypeFormer achieves 6.32% of global EER,\nwhile, by shifting the system threshold to set a FAR of 1%\nand 10%, we obtain corresponding FRRs of, respectively,\n17.47% and 4.68%. Table 4 contains all the intersection\npoints obtained from Fig. 4. We observe that setting the\nsystem threshold to a high security level (corresponding to\nFAR ¼ 1%) affects the usability of the system, i.e. it\nincreases the amount of false rejections for the legitimate\nusers. The computation of such metrics is limited to the\nglobal scenario due to the higher amount of genuine scores.\nLastly, Table 5 presents a comparison of the proposed\nTypeFormer with other systems presented in the literature\nthat were not originally evaluated according to the protocol\nadopted in this work [ 12]: digraphs and SVM [ 38],\nPOHMMs [ 72], and a combination of RNNs and CNNs\n[50]. The evaluation of the different system takes place on\nthe same set of 1000 subjects considering E ¼ 5 and\nL ¼ 50. TypeFormer shows the best performance, with\nEER absolute improvements of 37.15% (POHMM [ 72]),\n32.45% (Diagraphs [ 38]), 8.95% (CNN ? RNN [ 50]),\n5.95% (TypeNet [ 13]), and 0.59% (our preliminary\nTransformer architecture [ 14]). Such results show the\npotential of TypeFormer and Transformer-based architec-\ntures in the challenging free-text mobile scenario. For\ncompleteness, we also report the inference time for a single\nfeature extraction instantiation. Speciﬁcally, we consider as\ninput a biometric sample in the form of the pre-processed\nﬁve features described in Sect. 3.1 and a keystroke\nsequence length L\n¼ 50. The inference time is 46.4 ms on\naverage, considering all embeddings computed on the\nevaluation set. 3 The experiments are carried out on a\nNVIDIA GeForce RTX 3070 Ti graphics card. In terms of\nnumber of parameters, TypeFormer has approximately\n1.8M, whereas the preliminary Transformer has 400K, and\nTypeNet has 200K.\n6.1 Analysis of the feature embeddings\nThe output feature embeddings extracted by TypeFormer\nlie in a 64-dimensional space, and their pairwise relative\npositioning is measured throughout the Euclidean distance.\nIn this scenario, mathematical methods like the popular\nt-SNE [73] are useful to visualise data points in such high-\ndimensional spaces. Figure 5 depicts the output feature\nembedding space reduced to two dimensions through\nt-SNE. For better visualisation, we include examples of 10\nrandom subjects of the database (15 acquisition sessions\nper subject). Apart from few outliers, most groups are\nclearly separated, while data points belonging to the same\nsubjects are closer together. This is an indicator of small\nintra-class variability and high inter-class variability.\n7 Conclusions and future work\nIn the current article, we have proposed a novel Trans-\nformer-based architecture, TypeFormer, for the task of\nfree-text mobile keystroke authentication. TypeFormer\nfeatures two branches (temporal and channel modules)\nwith long short-term memory (LSTM) layers, Gaussian\nrange encoding (GRE), a multi-head self-attention mech-\nanism, and a block-recurrent transformer layer, and it was\ntrained with triplet loss. Its output consists in feature\nembedding vectors representing points in the output hyper-\nspace. The distance between embedding vectors is\nFig. 5 Two-dimensional graphical visualisation of the latent space\nthrough t-SNE considering 15 sessions of 10 subjects [ 73]. Selected\nparameters: perplexity ¼ 14, init ¼ 0pca0, n iter ¼ 1000\n3 sklearn.manifold.TSNE -- scikit-learn 1.1.1\ndocumentation.\n18542 Neural Computing and Applications (2024) 36:18531–18545\n123\nmeasured through the Euclidean distance, and it is less for\ninstances of data belonging to the same subject than for\nones of different subjects. The development of the model is\nbased on the Aalto mobile keystroke database [ 12], the\nlargest public databases of mobile keystroke dynamics.\nFirst, we have performed an analysis to validate the dif-\nferent modules that are present in the ﬁnal presented\nTransformer architecture. Then, in order to compare\nTypeFormer with the highest-performing systems recently\nproposed in the literature, we have replicated the experi-\nmental protocol of two recent studies [ 13, 14], by varying\nthe number of enrolment sessions ( E ¼ 1; 2; 5; 7; 10), input\nkeystroke sequence lengths ( L ¼ 30; 50; 70; 100), and\nconsidering the same database repartition. In all cases,\nTypeFormer outperformed previous approaches, reaching\nas little as 3.25% EER considering E ¼ 5 and L ¼ 50. This\nwould be an absolute improvement of 5.95% EER with\nrespect to previous LSTM RNN-based model (the corre-\nsponding relative improvement is around 65%) [ 13]. To\nadvance the state of the art of free-text mobile keystroke\nbiometrics, we make our proposed approach and experi-\nmental framework public\n4.\nConcerning future work, the next directions of research\nwill go towards exploring the effectiveness of Transform-\ners in modelling other biometric traits [ 74], including data\ncaptured by mobile device sensors [ 9, 75] and synthetic\ndata [76]. To this end, we will consider the optimisation of\nthe Transformer architecture to improve the performance\nwith longer sequences. Additionally, more sophisticated\ntraining approaches will be investigated, in terms of the\nloss function, such as the implementation of hard triplet\nmining, in order to force the model to learn from harder\ncomparisons [ 77], and output feature embedding distance\nmetrics. Finally, it would also be interesting to shed light\non explainability and privacy aspects of mobile keystroke\nauthentication [ 78, 79], i.e. investigating the subject\ninformation contained in the feature embeddings, i.e.\ngender, age, etc., to assess whether keystroke data should\nbe treated as privacy-sensitive biometric data. For this, the\nAalto mobile keystroke database can be useful due to the\nthe subject metadata available.\nAcknowledgements This project has received funding from the\nEuropean Union’s Horizon 2020 research and innovation programme\nunder the Marie Skłodowska-Curie grant agreement No. 860315.\nMoreover, it has been supported by INTER-ACTION (PID2021-\n126521OB-I00 MICINN/FEDER) and Ca ´tedra ENIA UAM-VER-\nIDAS en IA Responsable (NextGenerationEU PRTR TSI-100927-\n2023-2).\nFunding Open Access funding provided thanks to the CRUE-CSIC\nagreement with Springer Nature.\nData availability The database used is freely available for download\nhttps://userinterfaces.aalto.ﬁ/typing37k/. The database is associated to\nthe publication [ 12]. In our GitHub repository https://github.com/\nBiDAlab/TypeFormer, we provide all the necessary information to\nreplicate the experimental protocol of the benchmark evaluation of\nTypeFormer.\nDeclarations\nConflict of interest All authors certify that they have no affiliations\nwith or involvement in any organisation or entity with any financial\ninterest or non-financial interest in the subject matter or materials\ndiscussed in this manuscript.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Thariq Ahmed HF, Ahmad H (2020) Device free human gesture\nrecognition using Wi-Fi CSI: a survey. Eng Appl Artif Intell\n87:103281\n2. Rathgeb C, Tolosana Vera-Rodriguez R, Busch C (2022) Hand-\nbook Of digital face manipulation and detection: from deepfakes\nto morphing attacks. Springer, Berlin\n3. ISO 9241-11:2018(en): Ergonomics of human-system interaction\n(2018) Part 11: usability: deﬁnitions and concepts\n4. Patel VM, Chellappa R, Chandra D, Barbello B (2016) Contin-\nuous user authentication on mobile devices: recent progress and\nremaining challenges. IEEE Signal Process Mag 33(4):49–61\n5. Stragapede G, Vera-Rodriguez R, Tolosana R, Morales A, Acien\nA, Le Lan G (2022) Mobile behavioral biometrics for passive\nauthentication. Pattern Recognit Lett 157:35–41\n6. Delgado-Santos P, Tolosana R, Guest R, Vera-Rodriguez R,\nDeravi F, Morales A (2022) GaitPrivacyON: privacy-preserving\nmobile gait biometrics using unsupervised learning. Pattern\nRecogn Lett 161:30–37\n7. Delgado-Santos P, Stragapede G, Tolosana R, Guest R, Deravi F,\nVera-Rodriguez R (2022) A survey of privacy vulnerabilities of\nmobile device sensors. ACM Comput Surv 54:1–30\n8. Porwik P, Doroz R (2021) Adaptation of the idea of concept drift\nto some behavioral biometrics: preliminary studies. Eng Appl\nArtif Intell 99:104135\n9. Stragapede G, Vera-Rodriguez R, Tolosana R, Morales A (2023)\nBehavePassDB: public database for mobile behavioral biometrics\nand benchmark evaluation. Pattern Recogn 134:109089\n10. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nA.N, Kaiser L, Polosukhin I (2017) Attention is all you need. In:\nProc. Adv Neural Inform Process Syst\n11. Tay Y, Dehghani M, Bahri D, Metzler D (2022) Efﬁcient\nTransformers: a survey. ACM Comput Surv\n4 https://github.com/BiDAlab/TypeFormer.\nNeural Computing and Applications (2024) 36:18531–18545 18543\n123\n12. Palin K, Feit A.M, Kim S, Kristensson P.O, Oulasvirta A (2019)\nHow do people type on mobile devices? observations from a\nstudy with 37,000 volunteers. In: proc. int. conf. on human-\ncomputer interaction with mobile\n13. Acien A, Morales A, Monaco JV, Vera-Rodriguez R, Fierrez J\n(2021) TypeNet: deep learning keystroke biometrics. behavior,\nand identity science. IEEE Trans Biomet 4(1):57–70\n14. Stragapede G, Delgado-Santos P, Tolosana R, Vera-Rodriguez R,\nGuest R, Morales A (2023) Mobile keystroke biometrics using\nTransformers. In: proc. int. conf. on automatic face and gesture\nrecognition\n15. Mondal S, Bours P (2017) A Study on Continuous Authentication\nUsing a Combination of Keystroke and Mouse Biometrics.\nNeurocomputing, 230: 1-22\n16. Abuhamad M, Abusnaina A, Nyang D, Mohaisen D (2021)\nSensor-based continuous authentication of smartphones’ users\nusing behavioral biometrics: a contemporary survey. IEEE\nInternet Things J 8(1):65–84\n17. Maiorana E, Kalita H, Campisi P (2021) Mobile keystroke\ndynamics for biometric recognition: an overview. IET Biom\n10(1):1–23\n18. Roy S, Pradhan J, Kumar A, Adhikary DRD, Roy U, Sinha D, Pal\nRK (2022) A systematic literature review on latest keystroke\ndynamics based models. IEEE Access 10:92192–92236\n19. Teh PS, Zhang N, Teoh ABJ, Chen K (2016) A survey on touch\ndynamics authentication in mobile devices. Comput Secur\n59:210–235\n20. Tolosana R, Vera-Rodriguez R, Fierrez J, Ortega-Garcia J (2019)\nReducing the template ageing effect in on-line signature bio-\nmetrics. IET Biom 8(6):422–430\n21. Morales , Fierrez J, Gomez-Barrero M, Ortega-Garcia J, Daza R,\nMonaco J.V, Montalva˜o J, Canuto J, George A (2016) KBOC:\nKeystroke biometrics ongoing competition. In: proc. int. conf. on\nbiometrics theory, applications and systems\n22. Monrose F, Rubin A (1997) Authentication via keystroke\ndynamics. In: proc. conf. on computer and communications\nsecurity\n23. Gunetti D, Picardi C (2005) Keystroke analysis of free text. ACM\nTrans Inform Syst Secur 8(3):312–347\n24. Jiang C.-H, Shieh S, Liu J.-C (2007) Keystroke statistical learn-\ning model for web authentication. In: proc. of the symp. on\ninformation, computer and communications security\n25. Saevanee H, Bhatarakosol P (2008) User authentication using\ncombination of behavioral biometrics over the touchpad acting\nlike touch screen of mobile device. In: proc. int. conf. on com-\nputer and electrical engineering\n26. Killourhy K.S, Maxion R.A (2009) Comparing Anomaly-detec-\ntion algorithms for keystroke dynamics. In: proc. int. conf. on\ndependable systems networks\n27. Zahid S, Shahzad M, Khayam S.A, Farooq M (2009) Keystroke-\nbased user identiﬁcation on smart phones. In: proc. int. workshop\non recent advances in intrusion detection\n28. Hwang S-S, Cho S, Park S (2009) Keystroke dynamics-based\nauthentication for mobile devices. Comput Secur 28(1–2):85–93\n29. Giot R, El-Abed M, Hemery B, Rosenberger C (2011) Uncon-\nstrained keystroke dynamics authentication with shared secret.\nComput secur 30(6–7):427–445\n30. Balagani KS, Phoha VV, Ray A, Phoha S (2011) On the dis-\ncriminability of keystroke feature vectors used in ﬁxed text\nkeystroke authentication. Pattern Recogn Lett 32(7):1070–1080\n31. Deng Y, Zhong Y (2013) keystroke dynamics user authentication\nbased on gaussian mixture model and deep belief nets. Interna-\ntional scholarly research notices\n32. Ahmed AA, Traore I (2013) Biometric recognition based on free-\ntext keystroke dynamics. IEEE Trans Cybern 44(4):458–472\n33. Gascon H, Uellenbeck S, Wolf C, Rieck K (2014) Continuous\nAuthentication on mobile devices by analysis of yyping motion\nbehavior. Sicherheit 2014–Sicherheit, Schutz und Zuverla¨ssigkeit\n34. Alpar O (2014) Keystroke recognition in user authentication\nusing ANN based RGB histogram technique. Eng Appl Artif\nIntell 32:213–217\n35. Huang J, Hou D, Schuckers S, Hou Z (2015) Effect of data size\non performance of free-text keystroke authentication. In: proc.\nint. conf. on identity, security and behavior analysis\n36. Vural E, Huang J, Hou D, Schuckers S (2014) Shared research\ndataset to support development of keystroke authentication. In:\nproc. int. joint conf. on biometrics\n37. Fierrez J, Galbally J, Ortega-Garcia J, Freire MR, Alonso-Fer-\nnandez F, Ramos D, Toledano DT, Gonzalez-Rodriguez J,\nSiguenza JA, Garrido-Salas J et al (2010) BiosecurID: a multi-\nmodal biometric database. Pattern Anal Appl 13(2):235–246\n38. C¸ eker H, Upadhyaya S (2016) User authentication with keystroke\ndynamics in long-text data. In: proc. int. conf. on biometrics\ntheory, applications and systems\n39. C¸ eker H, Upadhyaya S (2017) Sensitivity analysis in keystroke\ndynamics using Convolutional Neural Networks. In: proc.\nworkshop on information forensics and security\n40. Giot R, El-Abed M, Rosenberger C (2009) GREYC Keystroke: a\nbenchmark for keystroke dynamics biometric systems. In: proc.\nint. conf. on biometrics: theory, applications, and systems\n41. Crawford H, Ahmadzadeh E (2017) Authentication on the go:\nassessing the effect of movement on mobile device keystroke\ndynamics. In: proc. symp. on usable privacy and security\n42. Kim J, Kim H, Kang P (2018) Keystroke dynamics-based user\nauthentication using freely typed text based on user-adaptive\nfeature extraction and novelty detection. Appl Soft Comput\n62:1077–1087\n43. Murphy , Huang J, Hou D, Schuckers S (2017) Shared dataset on\nnatural human-computer interaction to support continuous\nauthentication research. In: proc. int. joint conf. on biometrics\n44. Monaco JV, Tappert CC (2018) The partially observable hidden\nmarkov model and its application to keystroke dynamics. Pattern\nRecogn 76:449–462\n45. Bakelman N, Monaco J.V, Cha S.-H, Tappert C.C (2013) Key-\nstroke biometric studies on password and numeric keypad input.\nIn: proc. European intelligence and security informatics conf\n46. Coakley M.J, Monaco J.V, Tappert C.C (2016) Keystroke bio-\nmetric studies with short numeric input on smartphones. In: proc.\nint. conf. on biometrics theory, applications and systems\n47. Monaco J.V, Bakelman N, Cha S.-H, Tappert C.C (2013) Recent\nadvances in the development of a long-text-input keystroke bio-\nmetric authentication system for arbitrary text input. In: proc.\neuropean intelligence and security informatics conf., pp. 60–66\n48. Villani M, Tappert C, Ngo G, Simone J, Fort H.S, Cha S.-H\n(2006) Keystroke biometric recognition studies on long-text input\nunder ideal and application-oriented conditions. In: proc. conf. on\ncomputer vision andpattern recognition workshop\n49. Cilia D, Inguanez F (2018) Multi-model authentication using\nkeystroke dynamics for smartphones. In: proc. int. conf. on\nconsumer electronics\n50. Lu X, Zhang S, Hui P, Lio P (2020) Continuous authentication by\nfree-text keystroke based on CNN and RNN. Comput Secur\n96:101861\n51. Sun Y, Ceker H, Upadhyaya S (2016) Shared keystroke dataset\nfor continuous authentication. In: proc. int. workshop on infor-\nmation forensics and security\n52. Kim J, Kang P (2020) Freely typed keystroke dynamics-based\nuser authentication for mobile devices based on heterogeneous\nfeatures. Pattern Recogn 108:107556\n18544 Neural Computing and Applications (2024) 36:18531–18545\n123\n53. Ayotte B, Banavar M, Hou D, Schuckers S (2020) Fast free-text\nauthentication via instance-based keystroke dynamics. IEEE\nTrans Biom Behav Identity Sci 2(4):377–387\n54. Dhakal V, Feit A.M, Kristensson P.O, Oulasvirta A (2018)\nObservations on typing from 136 million keystrokes. In: proc. chi\nconf. on human factors in computing systems\n55. El-Kenawy E-SM, Mirjalili S, Abdelhamid AA, Ibrahim A,\nKhodadadi N, Eid MM (2022) Meta-heuristic optimization and\nkeystroke dynamics for authentication of smartphone users.\nMathematics 10(16):2912\n56. El-Abed M, Dafer M, Khayat R.E (2014) RHU Keystroke: a\nmobile-based benchmark for keystroke dynamics systems. In:\nproc. int. carnahan conf. on security technology, pp. 1–4\n57. Al-Obaidi N.M, Al-Jarrah M.M (2016) Statistical median-based\nclassiﬁer model for keystroke dynamics on mobile devices. In:\nproc. int. conf. on digital information processing and communi-\ncations, pp. 186–191\n58. Stylios I, Skalkos A, Kokolakis S, Karyda M (2022) BioPrivacy:\nDevelopment of a keystroke dynamics continuous authentication\nsystem. In: proc. computer security. ESORICS 2021 Int.\nworkshops\n59. Li J, Chang H.-C, Stamp M (2022) Free-text keystroke dynamics\nfor user authentication. Artif Intell Cybersecur, 357–380\n60. Tolosana R, Vera-Rodriguez R, Fierrez J, Ortega-Garcia J (2018)\nExploring recurrent neural networks for on-line handwritten\nsignature biometrics. IEEE Access 6:5128–5138\n61. Tolosana R, Vera-Rodriguez R, Fierrez J, Ortega-Garcia J (2020)\nBioTouchPass2: touchscreen password biometrics using time-\naligned recurrent neural networks. IEEE Trans Inf Forensics\nSecur 5:2616–2628\n62. Neacsu T, Poncu T, Ruseti S, Dascalu M (2023) Doublestrokenet:\nbigram-level keystroke authentication. Electronics 12(20):4309\n63. Wu H, Xu J, Wang J, Long M (2021) Autoformer: Decomposi-\ntion Transformers with auto-correlation for long-term series\nforecasting. In: Proc. advances in neural information processing\nsystems\n64. Hutchins D, Schlag I, Wu Y, Dyer E, Neyshabur B (2022) Block-\nrecurrent Transformers. In: Proc. advances in neural information\nprocessing systems\n65. Li B, Cui W, Wang W, Zhang L, Chen Z, Wu M (2021) Two-\nstream convolution augmented transformer for human activity\nrecognition. In: Proc. AAAI conf. on artiﬁcial intelligence\n66. Delgado-Santos P, Tolosana R, Guest R, Deravi F, Vera-Rodri-\nguez R (2023) Exploring Transformers for behavioural biomet-\nrics: a case study in gait recognition. Pattern Recogn 143:109798\n67. Zhou C, Li Q, Li C, Yu J, Liu Y, Wang G, Zhang K, Ji C, Yan Q,\nHe L, Peng H, Li J, Wu J, Liu Z, Xie P, Xiong C, Pei J, Yu P.S,\nSun L (2023) A comprehensive survey on pretrained foundation\nmodels: a history from BERT to ChatGPT. arXiv:2302.09419\n68. Han K, Wang Y, Chen H, Chen X, Guo J, Liu Z, Tang Y, Xiao A,\nXu C, Xu Y, Yang Z, Zhang Y, Tao D (2023) A survey on vision\ntransformer. IEEE Trans Pattern Anal Mach Intell 45(1):87–110\n69. Wen Q, Zhou T, Zhang C, Chen W, Ma Z, Yan J, Sun L (2022)\nTransformers in time series: a survey. arxiv:2202.07125\n70. Vertanen K, Kristensson P.O (2011) A versatile dataset for text\nentry evaluations based on genuine mobile emails. In: Proc. Int.\nConf. on human computer interaction with mobile devices and\nservices\n71. Graff D, Cieri C (2003) English Gigaword LDC2003T05. Lin-\nguistic Data Consortium, Philadelphia\n72. Monaco JV, Tappert CC (2018) The partially observable hidden\nmarkov model and its application to keystroke dynamics. Pattern\nRecognit 76:449–462\n73. Maaten L, Hinton G (2008) Visualizing data using t-SNE. J Mach\nLearn Res 9:11\n74. Tolosana R, Vera-Rodriguez R et al (2022) SVC-onGoing: sig-\nnature veriﬁcation competition. Pattern Recogn 127:108609\n75. Stragapede G, Vera-Rodriguez R, Tolosana R, Morales A, Fierrez\nJ, Ortega-Garcia J, Rasnayaka S, Seneviratne S, Dissanayake V,\nLiebers J, Islam A, Belhaouari S.B, Ahmad S, Jabin S (2022)\nIJCB 2022 Mobile behavioral biometrics competition (Mobi-\nleB2C). In: Proc. Int. joint conf. on biometrics\n76. Melzi P, Tolosana R. Vera-Rodriguez, R, Kim M, Rathgeb C, Liu\nX, DeAndres-Tame I, Morales A, Fierrez J, Ortega-Garcia J, et al\n(2024) FRCSyn-onGoing: benchmarking and comprehensive\nevaluation of real and synthetic data to improve face recognition\nsystems. Inf Fusion 107:102322\n77. Schroff F, Kalenichenko D, Philbin J (2015) FaceNet: A uniﬁed\nembedding for face recognition and clustering. In: Proc. Conf. on\ncomputer vision and pattern recognition\n78. Deandres-Tame I, Tolosana R, Vera-Rodriguez R, Morales A,\nFierrez J, Ortega-Garcia J (2024) How good is chatgpt at face\nbiometrics? a ﬁrst look into recognition, soft biometrics, and\nexplainability. IEEE Access 12:34390–34401\n79. Melzi P, Rathgeb C, Tolosana R, Vera-Rodriguez R, Busch C\n(2022) An overview of privacy-enhancing technologies in bio-\nmetric recognition. arXiv:2206.10465\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nNeural Computing and Applications (2024) 36:18531–18545 18545\n123",
  "topic": "Keystroke dynamics",
  "concepts": [
    {
      "name": "Keystroke dynamics",
      "score": 0.8639034032821655
    },
    {
      "name": "Computer science",
      "score": 0.8585837483406067
    },
    {
      "name": "Keystroke logging",
      "score": 0.7700165510177612
    },
    {
      "name": "Biometrics",
      "score": 0.7092931270599365
    },
    {
      "name": "Mobile device",
      "score": 0.5738372206687927
    },
    {
      "name": "Transformer",
      "score": 0.5163732767105103
    },
    {
      "name": "Mixture model",
      "score": 0.4938231408596039
    },
    {
      "name": "Authentication (law)",
      "score": 0.4508983790874481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36890313029289246
    },
    {
      "name": "Human–computer interaction",
      "score": 0.325667142868042
    },
    {
      "name": "Password",
      "score": 0.2914944887161255
    },
    {
      "name": "Computer security",
      "score": 0.1344887912273407
    },
    {
      "name": "World Wide Web",
      "score": 0.09686911106109619
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "S/KEY",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}