{
  "title": "Patch Slimming for Efficient Vision Transformers",
  "url": "https://openalex.org/W3169793979",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5038232518",
      "name": "Yehui Tang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101784732",
      "name": "Kai Han",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100727358",
      "name": "Yunhe Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001529504",
      "name": "Chang Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5016185815",
      "name": "Jianyuan Guo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5022997052",
      "name": "Chao Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074103823",
      "name": "Dacheng Tao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2515385951",
    "https://openalex.org/W2928560789",
    "https://openalex.org/W3102982004",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W3138796575",
    "https://openalex.org/W2058580716",
    "https://openalex.org/W2895361760",
    "https://openalex.org/W3034742519",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2807912816",
    "https://openalex.org/W2786054724",
    "https://openalex.org/W3152698000",
    "https://openalex.org/W2041459929",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2319920447",
    "https://openalex.org/W2962851801",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W2513419314",
    "https://openalex.org/W566555209",
    "https://openalex.org/W3093994940"
  ],
  "abstract": "This paper studies the efficiency problem for visual transformers by excavating redundant calculation in given networks. The recent transformer architecture has demonstrated its effectiveness for achieving excellent performance on a series of computer vision tasks. However, similar to that of convolutional neural networks, the huge computational cost of vision transformers is still a severe issue. Considering that the attention mechanism aggregates different patches layer-by-layer, we present a novel patch slimming approach that discards useless patches in a top-down paradigm. We first identify the effective patches in the last layer and then use them to guide the patch selection process of previous layers. For each layer, the impact of a patch on the final output feature is approximated and patches with less impact will be removed. Experimental results on benchmark datasets demonstrate that the proposed method can significantly reduce the computational costs of vision transformers without affecting their performances. For example, over 45% FLOPs of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the ImageNet dataset.",
  "full_text": "Patch Slimming for Efﬁcient Vision Transformers\nYehui Tang1,2, Kai Han2, Yunhe Wang2*, Chang Xu3,\nJianyuan Guo2,3, Chao Xu1, Dacheng Tao4,\n1School of Artiﬁcial Intelligence, Peking University. 2Huawei Noah’s Ark Lab.\n3School of Computer Science, University of Sydney. 4JD Explore Academy, China.\nyhtang@pku.edu.cn, {kai.han, yunhe.wang}@huawei.com, dacheng.tao@gmail.com.\nAbstract\nThis paper studies the efﬁciency problem for visual\ntransformers by excavating redundant calculation in given\nnetworks. The recent transformer architecture has demon-\nstrated its effectiveness for achieving excellent performance\non a series of computer vision tasks. However, similar to\nthat of convolutional neural networks, the huge computa-\ntional cost of vision transformers is still a severe issue.\nConsidering that the attention mechanism aggregates dif-\nferent patches layer-by-layer, we present a novel patch slim-\nming approach that discards useless patches in a top-down\nparadigm. We ﬁrst identify the effective patches in the last\nlayer and then use them to guide the patch selection process\nof previous layers. For each layer, the impact of a patch on\nthe ﬁnal output feature is approximated and patches with\nless impact will be removed. Experimental results on bench-\nmark datasets demonstrate that the proposed method can\nsigniﬁcantly reduce the computational costs of vision trans-\nformers without affecting their performances. For example,\nover 45% FLOPs of the ViT-Ti model can be reduced with\nonly 0.2% top-1 accuracy drop on the ImageNet dataset.\n1. Introduction\nRecently, transformer models have been introduced into\nthe ﬁeld of computer vision and achieved high performance\nin many tasks such as object recognition [6], image pro-\ncess [2], and video analysis [21]. Compared with the con-\nvolutional neural networks (CNNs), the transformer archi-\ntecture introduces less inductive biases and hence has larger\npotential to absorb more training data and generalize well\non more diverse tasks [6, 11, 27, 35, 38, 43]. However, sim-\nilar to CNNs, vision transformers also suffer high compu-\ntational cost, which blocks their deployment on resource-\nlimited devices such as mobile phones and various IoT de-\nvices. To apply a deep neural network in such real scenar-\n*Corresponding author.\nios, massive model compression algorithms have been pro-\nposed to reduce the required computational cost [24, 42].\nFor example, quantization algorithms approximate weights\nand intermediate features maps in neural networks with\nlow-bit data [3, 32]. Knowledge distillation improves the\nperformance of a compact network by transferring knowl-\nedge from giant models [17, 22].\nIn addition, network pruning is widely explored and used\nto reduce the neural architecture by directly removing use-\nless components in the pre-deﬁned network [13, 14, 25, 28].\nStructured pruning discards whole contiguous components\nof a pre-trained model, which has attracted much attention\nin recent years, as it can realize acceleration without speciﬁc\nhardware design. In CNNs, removing a whole ﬁlter for im-\nproving the network efﬁciency is a representative paradigm,\nnamed channel pruning (or ﬁlter pruning) [16, 25]. For ex-\nample, Liu et al. [25] introduce scaling factors to control the\ninformation ﬂow in the neural network and ﬁlters with small\nfactors will be removed. Although the aforementioned net-\nwork compression methods have made tremendous efforts\nfor deploying compact convolutional neural networks, there\nare only few works discussing how to accelerate vision\ntransformers.\nDifferent from the paradigm in conventional CNNs,\nthe vision transformer splits the input image into multiple\npatches and calculates the features of all these patches in\nparallel. The attention mechanism will further aggregate\nall patch embeddings into visual features as the output. Ele-\nments in the attention map reﬂect the relationship or similar-\nity between any two patches, and the largest attention value\nfor constructing the feature of an arbitrary patch is usually\ncalculated from itself. Thus, we have to preserve this in-\nformation ﬂow in the pruned vision transformers for retain-\ning the model performance, which cannot be guaranteed in\nthe conventional CNN channel pruning methods. Moreover,\nnot all the manually divided patches are informative enough\nand deserve to be preserved in all layers, e.g., some patches\nare redundant with others. Hence we consider developing\na patch slimming approach that can effectively identify and\n1\narXiv:2106.02852v2  [cs.CV]  4 Apr 2022\nThe (l+1)-th LayerThe l-th LayerThe (l-1)-th Layer\nFigure 1. The diagram of patch slimming for vision transformers.\nremove redundant patches.\nIn this paper, we present a novel patch slimming algo-\nrithm for accelerating the vision transformers. In contrast\nto existing works focusing on the redundancy in the net-\nwork channel dimension, we aim to explore the computa-\ntional redundancy in the patches of a vision transformer (as\nshown in Figure 1. The proposed method removes redun-\ndant patches from the given transformer architecture in a\ntop-down framework, in order to ensure the retained high-\nlevel features of discriminative patches can be well calcu-\nlated. Speciﬁcally, the patch pruning will execute from the\nlast layer to the ﬁrst layer, wherein the useless patches are\nidentiﬁed by calculating their importance scores to the ﬁnal\nclassiﬁcation feature ( i.e., class token). To guarantee the\ninformation ﬂow, a patch will be preserved if the patches\nin the same spatial location are retained by deeper layers.\nFor other patches, the importance scores determine whether\nthey are preserved, and patches with lower scores will be\ndiscarded. The whole pruning scheme for vision transform-\ners is conducted under a careful control of the network er-\nror, so that the pruned transformer network can maintain\nthe original performance with signiﬁcantly lower computa-\ntional cost. Extensive experiments validate the effectiveness\nof the proposed method for deploying efﬁcient vision trans-\nformers. For example, our method can reduce more than\n45% FLOPs of the ViT-Ti model with only 0.2% top-1 ac-\ncuracy loss on the ImageNet dataset.\n2. Related work\nStructure pruning for CNNs. Channel pruning\ndiscards the entire convolution kernels [23] to acceler-\nate the inference process and reduce the required memory\ncost [15, 23, 25, 33, 34]. To identify the redundant ﬁlters,\nmassive methods have been proposed. Wenet al. [41] add a\ngroup-sparse regularization on the ﬁlters and remove ﬁlters\nwith small norm. Beyond imposing sparsity regularization\non the ﬁlters directly, Liu et al. [25] introduce extra scaling\nfactors to each channel and these scaling factors are trained\nto be sparse. Filters with small scaling factors has less im-\npact on the network output and will be removed for accel-\nerating inference. He et al. [15] rethink the criterion that\nﬁlters with small norm values are less important and pro-\npose to discard the ﬁlters having larger similarity to others.\nTo maximally excavate redundancy, Tang [37] set up a sci-\nentiﬁc control to alleviate the distribution of irrelevant fac-\ntors and remove ﬁlters with little relation to the given task.\nIn the conventional channel pruning for CNNs, channels in\ndifferent layers have no one-to-one relationship, and then\nthe choice of effective channels in a layer has little impact\non that in other channels.\nStructure pruning for transformers. In the trans-\nformer model for NLP tasks, a series of works focus on re-\nducing the heads in the multi-head attention (MSA) module.\nFor example, Michel et al. [29] observes that removing a\nlarge percentages of heads in the pre-trained BERT [5] mod-\nels has limited impact on its performance. V oita et al. [39]\nanalyze the role of each head in the transformer and evaluate\ntheir contribution to the model performance. Those heads\nwith less contributions will be reduced. Besides the MSA\nmodule, the neurons in the multilayer perceptron (MLP)\nmodule are also pruned in [1]. Designed for vision trans-\nformers, VTP [44] reduces the number of embedding di-\nmensions by introducing control coefﬁcients and removes\nneurons with small coefﬁcients. Different from them, the\nproposed patch slimming explores the redundancy from a\nnew perspective by considering the information integration\nof different patches in a vision transformer. Actually, re-\nducing patches can be also combined with pruning in other\ndimensions to realize higher acceleration.\n3. Patch Slimming for Vision Transformer\nIn this section, we introduce the scheme of pruning\npatches in vision transformers. We ﬁrst review the vision\ntransformer brieﬂy and then introduce the formulation of\npatch slimming.\nIn vision transformer, the input image is split into N\npatches and then fed into transformer model for representa-\ntion learning. For an L-layer vision transformer model, the\nmultihead self-attention (MSA) modules and multi-layer\nperceptron (MLP) modules are its main components occu-\npying most of the computational cost. Denoting Zl−1,Z′\nl ∈\nRN×d as the input and the intermediate features of the l-th\nlayer, the MSA and MLP modules can be formulated as:\nMSA(Zl−1)\n= Concat\n[\nsoftmax\n(\nQh\nl Kh\nl\n⊤\n√\nd\n)\nVh\nl\n]H\nh=1\nWo\nl ,\nMLP(Z′\nl) =φ(Z′\nlWa\nl )Wb\nl ,\n(1)\nwhere dis embedding dimension,His the number of heads,\nQh\nl = Zl−1Whq\nl , Kh\nl = Zl−1Whk\nl , and Vh\nl = Zl−1Whv\nl\n2\nare the query, key and value of the h-th head in the l-th\nlayer, respectively. Wa\nl , Wb\nl are the weights for linear\ntransformation and φ(·) is the non-linear activation func-\ntion (e.g., GeLU). Most of recent vision transformer mod-\nels are constructed by stacking MSA and MLP modules\nalternately and a block Bl(·) is deﬁned as Bl(Zl−1) =\nMLP(MSA(Zl−1) +Zl−1) +Z′\nl.\nAs discussed above, there is considerable redundant in-\nformation existing in the patch level of vision transformers.\nTo further verify this phenomenon, we calculate the average\ncosine similarity between patches within a layer, and show\nhow similarity vary w.r.t. layers in Figure 3. The similar-\nity between patches increase rapidly as layers increase, and\nthe average similarity even exceed 0.8 in deeper layers. The\nhigh similarity implies that patches are redundant especially\nin the deeper layers and removing them will not obviously\naffect the feature calculation.\nPatch slimming aims to recognize and discard redundant\npatches for accelerating the inference process (as shown in\nFigure 1). Here we use a binary vector ml ∈{0,1}N to in-\ndicate whether a patch is preserved or not, the pruned MSA\nand MLP modules can be formulated as follows:\nˆMSAl( ˆZl−1)\n= Concat\n[\ndiag(ml)softmax\n(\nQh\nl Kh\nl\n⊤\n√\nd\n)\nVh\nl\n]H\nh=1\nWo\nl ,\nˆMLPl( ˆZ′\nl) =diag(ml)φ( ˆZ′\nlWa\nl )Wb\nl ,\n(2)\nwhere diag(ml) is a diagonal matrix whose diagonal line\nis composed of elements in ml. Speciﬁcally, ml,i = 0\nindicates that the i-th patch in thel-th layer is pruned. ˆZl−1,\nˆZ′\nl are the input and the intermediate features of the l-th\nlayer in a pruned vision transformer. Then the pruned block\nis deﬁned as ˆBl( ˆZl−1) = ˆMLPl( ˆMSAl( ˆZl−1)+ ˆZl−1)+ ˆZ′\nl.\nIn practical implementation, only the effective patches\nof input feature ˆZl−1 are selected to calculate queries, and\nthen all the subsequent operations are only implemented on\nthese effective patches. Thus, the computation of the pruned\npatches can be avoided 1.\nComputation Efﬁciency. Compared with the original\nblock Bl(·), the pruned ˆBl(·) can save a large amount of\ncomputational cost. Given a block B(·) with N patches\nand d-dimension embedding, the computational costs of\nMLP (2-layers with hidden dimension d′) and MSA are\n(2Ndd′) and (2N2d+ 4Nd2), respectively. After prun-\ning η% patches, all the computational components in MLP\nare pruned, and then η% FLOPs in the MLP module are re-\nduced. For the MSA module, the cost of calculating query,\nattention map and output projection can be reduced, and\n1According to ml, only effective patches from the shortcut branch are\nadded to the output of pruned MSA, while the output of pruned MLP is\npadded with zeros before added to the shortcut.\nthen η%(2N2d+ 2Nd2) FLOPs is reduced.\n4. Excavating Redundancy via Inverse Pruning\nIn this section, we present the top-down framework to\nprune patches in the vision transformer, and provide an ef-\nfective importance score estimation of each patch.\nThe l-th layer\nAttention map\nThe (l+1)-th layer\n(a) CNN (b) ViT\nShortcutLearned weights\nFigure 2. The comparison between channel pruning in CNNs and\npatch pruning in vision transformers.\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.4\n0.5\n0.6\n0.7\n0.8Similarity\nFigure 3. The average similarity of different patches varies w.r.t.\nnetwork depth in the ViT-Base model.\n4.1. Top-Down Pruning\nFor patch slimming in vision transformer, we adopt a\ntop-down manner to prune patches layer-by-layer. It is a\nnatural choice with two reasons as described in the follow-\ning.\nFor a CNN model, pruning channels in different lay-\ners independently can achieves high performance [25, 37].\nHowever, this paradigm cannot work well in vision trans-\nformers. The main reason is that patches in different lay-\ners of a vision transformer are one-to-one corresponding.\nFigure 2 compares pruning channels in CNNs and pruning\npatches in vision transformers. As own in Figure 2(a), chan-\nnels in adjacent layers of a CNN model are fully connected\nby learnable weights, and each channel contains informa-\ntion from the entire image. However, in the vision trans-\nformer (Figure 2(b)), different patches communicate with\nothers by an attention map, which reﬂects the similarity be-\ntween different patches. If patch i and patch j are more\n3\nsimilar, the corresponding value Aij\nlh tends to have a larger\nvalue. The diagonal elements Aii\nlh usually plays a dominant\nrole, that is, a patch pays highest attention to the input at\nthe position of itself. Besides, the shortcut connection di-\nrectly copies the feature in the l-layer to the corresponding\npatches in the next layer. This one-to-one correspondence\ninspires us to preserve some important patches in the same\nspatial locations of different layers, which can guarantee the\ninformation propagation across layers.\nAnother characteristic of vision transformer is that\ndeeper layers tend to have more redundant patches. The\nattention mechanism in the MSA module aggregates dif-\nferent patches layer-by-layer, and a large number of simi-\nlar patches are produced in the process (as shown in Fig-\nure 3). It implies that more redundant patches can be safely\nremoved in deeper layers, and fewer in shallower layers.\nBased on the above analysis, we start the pruning proce-\ndure from the output layer, and then prune previous layers\nby transmitting the selected effective patches from top to\ndown. Specially, all the patches preserved in the (l+ 1)-th\nlayer will be also preserved in the l-th layer. Thus, this top-\ndown pruning procedure can guarantee that shallow layers\nmaintain more patches than the deep layers, which is con-\nsistent with the redundancy characteristic of vision trans-\nformer.\n4.2. Impact Estimation\nWith the patch pruning scheme described in the above\nsection, all that’s left is to recognize redundant patches in a\nvision transformer, i.e., ﬁnd the optimal mask ml in each\nlayer. Our goal is to prune patches as many as possible to\nrealize maximal acceleration, while maintaining the repre-\nsentation ability of the output feature. Actually, only a part\nof patch embeddings in the last layer are used to predict the\nlabels of input images for a speciﬁc task. For example, in\nthe image classiﬁcation task, only a patch related to classi-\nﬁcation (i.e., class token) is sent to the classiﬁer for predict-\ning labels. Other patches in the output layer can be removed\nsafely without affecting network output. Supposing the ﬁrst\npatch is the class token, we can get the mask in the last layer,\ni.e., mL,1 = 1, and mL,i = 0,∀i= 2,3,··· ,N. Then for\nthe other layers, the optimization object is formulated as\nfollows:\nmin\nm1,m2,···,mL−1\nL−1∑\nl=1\n∥ml∥0,\ns.t.ml ∈{0,1}N ,\nEL =\ndiag(mL)\n(\nˆZL −ZL\n)\n2\nF\n≤ϵ,\n(3)\nwhere ∥·∥ 0 is the ℓ0-norm of a vector, i.e., the number of\nnon-zero elements. ∥·∥F is the Frobenius norm of a matrix.\nϵis the tolerable error. ˆZL and ZL are output features of the\npruned and unpruned transformers. Eq. 3 is hard to opti-\nmize directly, as it involveℓ0 optimization under constraint,\nwhich is non-convex, NP hard and requires combinatorial\nsearch [25]. To solve Eq. 3, we ﬁrstly deﬁne a signiﬁcation\nscore by approximating the impact of a patch on the recon-\nstruction error EL and then develop a pruning procedure.\nThe attention mechanism aggregates information from\ndifferent patches to one patch, which is the main cause to\nproduce redundant patch features. To focus on the attention\nlayer for excavating redundant patches, we reformulate the\ndeﬁnition of a block B(·) in a simple formulation. Denoting\nPh\nl = softmax\n(\nQh\nl Kh\nl\n⊤\n/\n√\nd\n)\n, the MSA module in Eq. 1\ncan be formulated as:\nMSA(Zl) = Concat\n[\nPh\nl Vh\nl\n]H\nh=1\nWo\nl\n=\nH∑\nh=1\nPh\nl Vh\nl Who\nl =\nH∑\nh=1\nPh\nl Zl−1Whv\nl Who\nl ,\n(4)\nwhere Wo\nl = [ W1o\nl ; W2o\nl ; ··· ; WHo\nl ], and Who\nl ∈\nR\nd\nH ×d. Then the original block Bl(Zl−1) and pruned block\nˆBl(Zl−1) can be represented as:\nBl(Zl−1) =O(\nH∑\nh=1\nPh\nl Zl−1,{Wl}),\nˆBl(Zl−1,ml) =O(\nH∑\nh=1\ndiag(ml)Ph\nl Zl−1,{Wl})\n(5)\nwhere O(·,Wl) is composed of multiple linear projection\nmatrices {Wl}in the MSA and MLP module, as well as\nnon-linear activation functions (e.g., GeLU).\nBased on the simpliﬁed formulation of a block (Eq. 5),\nwe here explore how a patch in the t-th layer affects the\nerror EL (Eq 3) of effective patches in the last layer. We\nreverse the transformer and prune it from the last to the ﬁrst\nlayer sequentially. Thus when it comes to the t-th layer,\nall the deeper layers have been pruned. To approximate the\nsigniﬁcance of each token, we have the following theorem.\nTheorem 1. The impact of thet-th layer’s patch on the ﬁnal\nerror EL can be reﬂected by a signiﬁcance metric st ∈RN .\nFor the i-th patch in the t-th layer, we have\nst,i =\n∑\nh∈[H]L∼t+1\nAh\nt [:,i]Uh\nt [i,:]\n2\nF , (6)\nwhere Ah\nt = ∏L\nl=t+1 diag(ml)Ph\nl and Uh\nt = Ph\nt |Zt−1|.\nAh\nt [:,i] denotes the i-th column of Ah\nt and Uh\nt [i,:] is i-th\nrow of Uh\nt . [H]L∼t+1 denotes all the attention heads in the\n(t+ 1)-th to L-th layer.\nProof. We use ˆFl∼t(Zt−1,{mt}L\nt ) (l > t) to denote fea-\nture of the l-th layer in a vision transformer, whose lay-\ners behind t-th layer have been pruned, while the previ-\nous layers has not pruned yet, i.e., ˆFL∼t(Zt−1,{mt}L\nt ) =\n4\nˆBL ◦ ˆBL−1 ◦···◦ ˆBt(Zt−1). When pruning the patch in\nthe t-th layer, we compare effective patches of the last layer\nfrom two transformers to decide whether the t-th layer has\nbeen pruned. Then the error EL is calculated as:\nEL = ||diag(mL)[ ˆFL∼(t+1)( ˆBt(Zt−1))\n−ˆFL∼(t+1)(Bt(Zt−1))]||2\nF .\n(7)\nThe error EL in the last layer can be represented by the\npatches in the (L−1)-th layer, i.e.,\nEL = ||diag(mL)Ph\nL[O(\nH∑\nh=1\nˆF(L−1)∼(t+1)( ˆBt(Zt−1)))\n−O(\nH∑\nh=1\nˆF(L−1)∼(t+1)(Bt(Zt−1)))]||2\nF\n≤CL||\nH∑\nh=1\ndiag(mL)Ph\nL|ˆF(L−1)∼(t+1)( ˆBt(Zt−1))\n−ˆF(L−1)∼(t+1)(Bt(Zt−1))|||2\nF ,\n(8)\nwhere |·|is the element-wisely absolute value. The inequal-\nity above comes the Lipschitz continuity [7, 8] of function\nO(·) and CL is the Lipschitz constant. Recalling thatO(·) is\ncompose of multiple linear projections and non-linear acti-\nvation function, the condition of Lipschitz continuity is sat-\nisﬁed [8]. EL can be further transmitted to previous layers,\nand for the t-th layer we have\nEL ≤\nL∏\nl=t+1\nCl||\n∑\nh∈[H]L∼t\nL∏\nl=t+1\ndiag(ml)Ph\nl (9)\n|ˆBt(Zt−1) −Bt(Zt−1)|||2\nF (10)\n≤\nL∏\nl=t\nCl||\n∑\nh∈[H]L∼t+1\nL∏\nl=t+1\ndiag(ml) (11)\nPh\nl (IN −diag(ml)) Ph\nt |Zt−1|||2\nF (12)\n= C′\nt||\n∑\nh∈[H]L∼t+1\nAh\nt (IN −diag(mt))Uh\nt ||2\nF , (13)\nwhere Ah\nt = ∏L\nl=t+1 diag(ml)Ph\nl ∈ RN×N , Uh\nt =\nPh\nt |Zt−1|∈ RN×d, and C′\nt = ∏L\nl=t Cl. [H]l∼t denotes\nall the attention heads in thet-th to l-th layer. To investigate\nhow each patch in the t-th layer affect the ﬁnal error EL,\nwe expand Eq. 13 w.r.t. each element in the indicator ml.\nDenoting ml,i as the i-th element in ml, Ah\nt [:,i] is the i-th\ncolumn of Ah\nt and Ut\nt [i,:] is i-th row of Uh\nt , Eq. 13 can be\nwritten as:\nEL ≤C′\nt||\n∑\nh∈[H]L∼t+1\nN∑\ni=1\nAh\nt [:,i](1 −mt,i)Uh\nt [i,:]||2\nF\n(14)\n≤C′\nt\nN∑\ni=1\n(1 −mt,i)\n∑\nh∈[H]L∼t+1\nAh\nt [:,i]Uh\nt [i,:]\n2\nF .\n(15)\nThen we get the importance of each patch, i.e., st,i =∑\nh∈[H]L∼t+1\nAh\nt [:,i]Uh\nt [i,:]\n2\nF .\nFor the i-th patch in the t-th layer, st,i reﬂects its im-\npact on the effective output of the ﬁnal layer. A larger st,i\nimplies the corresponding patch has larger impact to the\nﬁnal error, which can reﬂect the importance of a patch to\nthe model performance. The calculation of st,i involves all\nthe attention maps in behind layers and the input feature of\nthe current layer. Before pruning the current layer, we ran-\ndomly sample a subset of training dataset to calculate the\nsigniﬁcance scores st and the average st over these data is\nadopted. The obtained st can be viewed as the real-number\nscore for binary mt.\nAlgorithm 1 Patch Slimming for Vision Transformers.\nInput: Training dataset D, vision transformer T with L\nlayers, patch masks {ml}L\nl=1, tolerant value ϵ, pre-\nserved patch’s numberrand search granularity r′.\n1: Initialize mL,0 as 1 and other elements as 0.\n2: for l= L−1,··· ,1 do\n3: Randomly sample a subset of training data to get the\nsigniﬁcance score sl in the l-th layer;\n4: Set ml = ml+1, El = +∞, r= 0;\n5: while El+1 >ϵ do\n6: Set relements in ml,i to 1 according to positions\nof the largest rscores sl,i.\n7: Fine-tune l-th layer Bl(Zl−1) for a few epochs.\n8: Calculate error El+1 in the (l+ 1)-th layer.\n9: r= r+ r′.\n10: end while\n11: end for\nOutput: The pruned vision transformer.\n4.3. Pruning Procedure\nHere we conclude the overall pipeline of the proposed\npatch slimming method.\nWe start from the output layer and prune the previous\nlayers layer-by-layer from top to down. Specially, all the\npatches preserved in the (l+ 1)-th layer will be also pre-\nserved in the l-th layer. The other patches are greedily se-\nlected according to their impact scoressl,i, wherein patches\n5\nTable 1. Comparison of the pruned vision transformers with different methods on ImageNet. ‘FLOPs ↓’ denotes the reduction ratio of\nFLOPs.\nModel Method Top-1 Top-5 FLOPs FLOPs Throughput Throughput\nAcc. (%) Acc. (%) (G) ↓(%) (image / s) ↑(%)\nViT (DeiT)-Ti\nBaseline 72.2 91.1 1.3 0 2536 0\nSCOP [37] 68.9 (-3.3) 89.0 (-2.1) 0.8 38.4 3372 33.0\nPoWER [10] 69.4 (-2.8) 89.2 (-1.9) 0.8 38.4 3304 30.3\nHVT [30] 69.7 (-2.5) 89.4 (-1.7) 0.7 46.2 3524 38.9\nPS-ViT (Ours) 72.0 (-0.2) 91.0 (-0.1) 0.7 46.2 3576 41.0\nDPS-ViT (Ours) 72.1 (-0.1) 91.1 (-0.0) 0.6 53.8 3639 43.5\nViT (DeiT)-S\nBaseline 79.8 95.0 4.6 0 940 0\nSCOP [37] 77.5 (-2.3) 93.5 (-1.5) 2.6 43.6 1310 39.4\nPoWER [10] 78.3 (-1.5) 94.0 (-1.0) 2.7 41.3 1295 37.8\nHVT [30] 78.0 (-1.8) 93.8 (-1.2) 2.4 47.8 1335 42.1\nPS-ViT (Ours) 79.4 (-0.4) 94.7 (-0.3) 2.6 43.6 1321 40.5\nDPS-ViT (Ours) 79.5 (-0.3) 94.8 (-0.2) 2.4 47.8 1342 42.8\nViT (DeiT)-B\nBaseline 81.8 95.6 17.6 0 292 0\nSCOP [37] 79.7 (-2.1) 94.5 (-1.1) 10.2 42.0 403 38.1\nPoWER [10] 80.1 (-1.7) 94.6 (-1.0) 10.4 39.2 397 35.8\nVTP [44] 80.7 (-1.1) 95.0 (-0.6) 10.0 43.2 412 41.0\nPS-ViT (Ours) 81.5 (-0.3) 95.4 (-0.2) 9.8 44.3 414 41.8\nDPS-ViT (Ours) 81.6 (-0.2) 95.4 (-0.2) 9.4 46.6 413 41.3\nT2T-ViT-14\nBaseline 81.5 95.4 5.2 0 764 0\nPoWER [10] 79.9 (-1.6) 94.4 (-1.0) 3.5 32.7 991 29.7\nPS-T2T (Ours) 81.1 (-0.4) 95.2 (-0.2) 3.1 40.4 1055 38.1\nDPS-T2T (Ours) 81.3 (-0.2) 95.3 (-0.1) 3.1 45.4 1078 41.1\nwith larger scores are preserved preferentially. Consider-\ning the reconstruction error El+1 in the (l+ 1)-th layer is\ndirectly affected by the patch selection in the l-th layer,\nwe use it to determine whether the l-th layer has already\nenough patches. In practice, we iteratively select r′impor-\ntant patches in each step and continue the selection process\nin the current layer until El+1 is less than the given toler-\nate value ϵ. To make El+1 well maintain the representation\nability of current preserved patches, we ﬁne-tune the cur-\nrent block ˆBl for a few epochs after each step of patch se-\nlection. Taking the original feature Zl−1 in the (l−1)-th\nlayer as input, and the reconstruction error El+1 as the ob-\njective, the parameters in the current blockˆBl are optimized.\nNote that the block ˆBl is a very small model with only one\nMSA and one MLP modules, the ﬁne-tune process is very\nfast. After pruning, the mask ml is ﬁxed, and weight pa-\nrameters in the vision transformer is further ﬁne-tuned to be\ncompatible with the efﬁcient architecture. The procedure of\npatching slimming for vision transformer is summarized in\nAlgorithm 1.\n4.4. A Dynamic Variant\nIn the above procedure, whether a patch will be pre-\nserved is determined by the statistics over the training\ndataset. It exploits the commonalities of redundant ﬁlters\nfrom different input adequately. Besides, dynamic pruning\nis the improved version of static pruning methods, which\nselects different patches for each input image. The dy-\nnamic strategy has been widely explored for reducing chan-\nnels of CNN models [9, 18, 36]. Similarly, the proposed\npatch slimming paradigm can be easily extended to the dy-\nnamic variant ( dubbed as DPS-ViT), and here we present\na simple implementation. Following [9], we insert a small\nmodule Gin each block to predict which patch is effective.\nThe module Gcomposes of a downsampling layer, linear\nlayer and activation functions, which takes the input feature\nZl−1 ∈RN×d as input and outputs the approximate signif-\nicance score ˆsl ∈RN . Recalling that the score of a patch\nsl (Eq. 6) actually depends on the input images, the mod-\nule Gis trained to ﬁt sl calculated for each input instance\nin the training phase. At inference, only patches with large\nscore ˆsl,i are required to calculate. The dynamic strategy\nﬁnds redundant patches of vision transformers depending\non input data, which can excavate patch redundancy more\nadequately.\n5. Experiments\nIn this section, we empirically investigate the effective-\nness of the proposed patch slimming methods for efﬁcient\nvision transformers (PS-ViT). We evaluate our method on\nthe benchmark ImageNet (ILSVRC2012) [4] dataset, which\ncontains 1000-class natural images, including 1.2M training\nimages and 5k validation images. The proposed method is\ncompared with SOTA pruning methods and we also conduct\nextensive ablation studies to better understand our method.\n6\nTable 2. Comparisons with SOTA transformer models on Ima-\ngeNet.\nModel FLOPs (G) Top-1 Acc. (%)\nDeiT-S [38] 4.6 79.8\nDeiT-B [38] 17.5 81.8\nPVT-Small [40] 3.8 79.8\nPVT-Medium [40] 6.7 81.2\nPVT-Large [40] 9.8 81.7\nT2T-ViT-14 [43] 5.2 81.5\nT2T-ViT-19 [43] 8.9 81.9\nT2T-ViT-24 [43] 14.1 82.3\nTNT-S [12] 5.2 81.5\nTNT-B [12] 14.1 82.9\nSwin-T [26] 4.5 81.3\nSwin-S [26] 8.7 83.0\nSwin-B [26] 15.4 83.5\nLV-ViT-S [20] 6.6 83.3\nLV-ViT-M [20] 16.0 84.1\nPS-LV-ViT-S (Ours) 4.7 82.4\nDPS-LV-ViT-S (Ours) 4.5 82.9\nPS-LV-ViT-M (Ours) 8.6 83.5\nDPS-LV-ViT-M (Ours) 8.3 83.7\n5.1. Experiments on ImageNet\nWe conduct experiments on the standard ViT models [6]\n(DeiT [38]), an improved variant network T2T-ViT [43] and\nthe state-of-the art LV-ViT [20].\nImplementation details. For a fair comparison, we\nfollow the training and testing settings in the original pa-\npers [38, 43, 43], and the patch slimming is implemented\nbased on the ofﬁcial pre-trained models. The global tol-\nerant error is select from {0.01, 0.02}to get models with\ndifferent acceleration rates, and the search granularity r is\nset to 10. We ﬁne-tune the current block for 3 epochs af-\nter each iteration of patch selection. After determining the\nproper patches in each layer, the pruned transformers are\nﬁne-tuned following the training strategy in [38]. All the\nexperiments are conducted with PyTorch [31] and Mind-\nSpore [19] on NVIDIA V100 GPUs.\nCompeting methods. We compare our patch slimming\nwith several representative model pruning methods includ-\ning CNN channel pruning methods [37] and BERT prun-\ning methods [10]. SCOP [37] is a SOTA network prun-\ning method for reducing the channels of CNNs, and we re-\nimplement it to reduce the patches in vision transformers.\nPoWER [10] accelerates BERT inference by progressively\neliminating word-vector. HVT [30] directly designs efﬁ-\ncient vision transformer architectures by progressively re-\nducing the spatial dimensions through pooling operations.\nExperimental results. The experimental results are\nshown in Table 1, where ‘PS-’ and ‘DPS-’ denote the pro-\nposed patch pruning method and its dynamic variant, re-\nspectively. We evaluate on three versions of DeiT [38] with\ndifferent model sizes, i.e., DeiT-Ti, DeiT-S, and DeiT-B.\nTable 3. Learned patch pruning vs. uniform pruning.\nMethod Top-1 Top-5 FLOPs\nAcc. (%) Acc. (%) (G)\nBaseline 79.8 95.0 4.6\nUniform pruning 77.2 93.8 2.6\nOurs 79.4 94.7 2.6\nOur method achieve obviously higher performance com-\npared to the existing methods. The SCOP method [37] de-\nsigned for CNNs achieve poor performance when applied\nfor reducing patches in a vision transformer, implying sim-\nply migrating the channel pruning methods cannot work\nwell. PoWER [10] has a larger accuracy drop than our\nmethod, indicating the model compression method for NLP\nmodels is not optimal for CV models. Compared to the vi-\nsion transformer structure pruning method VTP [44], our\nmethod investigates a new prospective by pruning patches\nand achieve higher accuracy with similar FLOPs.\nAs for T2T-ViT model, our method can reduce the\nFLOPs by 40.4% and only have a small accuracy decrease\n(0.4%), which is much better than the compared PoWER\nmethod. This indicates that the patch-level redundancy ex-\nists in various vision transformer models and our method\ncan well excavate the redundancy.\nWe further conduct experiments on a SOTA transformer\nmodel, LV-ViT [20], and show the results in Table 2. The\nresults show that our patch pruning method also work well\non LV-ViT, e.g., the dynamic patch slimming reduces the\nFLOPs of LV-ViT-M from 16.0G to 8.3G, still achieving\n83.7% top-1 accuracy. Its performance is also superior to\nother SOTA models such as Swin transformer [26].\n5.2. Ablation Study\nWe conduct extensive ablation studies on ImageNet to\nverify the effectiveness of each component in our method.\nThe DeiT-S model on the ImageNet dataset is used as the\nbase model.\nThe effect of global tolerant error ϵ. The tolerant error\nϵaffects the balance between computational cost and accu-\nracy of the pruned model, which is empirically investigated\nin Figure 4. Increasing ϵ implies larger reconstructed er-\nror between features of the pruned DeiT and original DeiT,\nwhile more patches can be pruned to achieve higher accel-\neration rate. When the reduction of FLOPs is less than 45%,\nthere is almost no accuracy loss (less than 0.4%), which is\nbecause that a large number of patches are redundant.\nLearned patch pruning vs. uniform pruning. In our\nmethod, the number of patches required in a speciﬁc lay-\ners is determined automatically via the global tolerant value\nϵ. The architecture of the pruned DeiT model is shown in\nFigure 5. We can see that a pyramid-like architecture is ob-\ntained, where most of the patches in deep layers are pruned\nwhile more patches are preserved in shallow layers. To val-\n7\n0.005 0.01 0.02 0.03 0.04 0.05\n77.0\n77.5\n78.0\n78.5\n79.0\n79.5\n80.0Accuracy (%)\n20\n25\n30\n35\n40\n45\n50\n55\nReduction Rate(%)\nAccuracy\nReduced FLOPs\nFigure 4. The ImageNet accuracy and\nFLOPs of the pruned DeiT-S w.r.t. toler-\nant error ϵ.\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0\n25\n50\n75\n100\n125\n150\n175\n200Number of Patches\nOurs\nEven Pruning\nFigure 5. The architecture of pruned\nDeiT-S on ImageNet.\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n40\n50\n60\n70\n80Accuracy (%)\nPruning ratio = 0.2\nPruning ratio = 0.5\nPruning ratio = 0.8\nFigure 6. Pruning different layers of the\nDeiT-S model on ImageNet.\n0.1 0.3 0.5 0.7 0.9\nPruning rate\n72\n74\n76\n78Accuracy (%)\nOurs\nAttn\nRandom\n(a) The 3-th layer.\n0.1 0.3 0.5 0.7 0.9\nPruning rate\n77.5\n78.0\n78.5\n79.0\n79.5Accuracy (%)\nOurs\nAttn\nRandom (b) The 7-th layer.\n0.1 0.3 0.5 0.7 0.9\nPruning rate\n78.4\n78.6\n78.8\n79.0\n79.2\n79.4\n79.6\n79.8Accuracy (%)\nOurs\nAttn\nRandom (c) The 11-th layer.\nFigure 7. Accuracy w.r.t. the pruning rate of patches in a single layer.\nidate the superiority of the learned pyramid architecture, we\nalso implement a baseline that uniformly prunes all the lay-\ners with the similar pruning rate. We compare the results of\nthe proposed patch slimming method and uniform pruning\nin Table 3. The accuracy of uniform pruning is only 77.2%,\nwhich incurs a large accuracy drop (-2.6%).\nTo better understand the behavior of patch pruning in the\nvision transformer, we prune patches in a single layer to see\nhow the test accuracy change. The experiments are con-\nducted with DeiT-S model on ImageNet.\nRedundancy w.r.t. depth. We test the patch redun-\ndancy of different layers to verify the motivation of top-\ndown patch slimming procedure. We prune a single layer\nand keep the same pruning ratio for different layers. Fig-\nure 6 shows the accuracy of the pruned model after pruning\npatches of a certain layer, and each line denotes pruning\npatches with a given pruning rate. In deeper layers, more\npatches can be safely removed without large impact on the\nﬁnal performance. However, removing a patch in lower lay-\ners usually incurs obvious accuracy drop. The patch redun-\ndancy is extremely different across layers and deeper lay-\ners have more redundancy, which can be attributed to that\nthe attention mechanism aggregates features from different\npatches and the deeper patches have been fully communi-\ncated with each other. This phenomenon is different from\nthe channel pruning in CNNs, where lower layers are ob-\nserved to have more channel-level redundancy (Figure 4\nin [16]).\nEffectiveness of impact estimation. We deﬁne the\nscores sl in Eq. 6 to approximate signiﬁcance of a patch\nby propagating the reconstruction error of effective patches\nin output layer. To validate its effectiveness, we compare\nit with two baseline scores: ‘Random’ denotes removing\npatches in the layer randomly, and ‘Attn’ approximates the\nimportance of a patch only with the norm of its attention\nmap in the current layer. We compare the three scores by\nutilizing them to prune patches in different layer. The re-\nsults are presented in Figure 7, wherey-axis is the test accu-\nracy of the pruned models (without ﬁne-tuning). From the\nresults, our impact estimation manner suffers less accuracy\nloss than the others with the same pruning rate ( e.g., 50%).\nIt implies that our method can effectively identify patches\nthat really make contributions to the ﬁnal prediction.\n6. Conclusion\nWe propose to accelerate vision transformers by reduc-\ning the number of patches required to calculate. Con-\nsidering that the attention mechanism aggregates different\npatches layer-by-layer, a top-down framework is developed\nto excavate the redundant patches. The importance of each\npatch is also approximated according to its impact on the\neffective output features. After pruning, a compact vision\ntransformer with a pyramid-like architecture is obtained.\nExtensive experiments on benchmark datasets validate that\nthe proposed method can effectively reduce the computa-\ntional cost. In the future, we plan to combine the patch slim-\nming methods with more compression technologies ( e.g.,\nweight pruning, model quantization) to explore extremely\nefﬁcient vision transformers.\nAcknowledgment. This work is supported by Na-\ntional Natural Science Foundation of China under Grant\nNo.61876007, Australian Research Council under Project\nDP210101859 and the University of Sydney SOAR Prize.\n8\nReferences\n[1] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2\n[2] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 1\n[3] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. Binarized neural networks:\nTraining deep neural networks with weights and activations\nconstrained to+ 1 or-1. arXiv preprint arXiv:1602.02830 ,\n2016. 1\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 6\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 7\n[7] Paul Dupuis and Hitoshi Ishii. On lipschitz continuity of the\nsolution mapping to the skorokhod problem, with applica-\ntions. Stochastics: An International Journal of Probability\nand Stochastic Processes, 35(1):31–62, 1991. 5\n[8] Ken-ichi Funahashi and Yuichi Nakamura. Approximation\nof dynamical systems by continuous time recurrent neural\nnetworks. Neural networks, 6(6):801–806, 1993. 5\n[9] Xitong Gao, Yiren Zhao, Lukasz Dudziak, Robert Mullins,\nand Cheng zhong Xu. Dynamic channel pruning: Feature\nboosting and suppression. In International Conference on\nLearning Representations, 2019. 6\n[10] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje,\nVenkatesan Chakaravarthy, Yogish Sabharwal, and Ashish\nVerma. Power-bert: Accelerating bert inference via progres-\nsive word-vector elimination. In International Conference\non Machine Learning, pages 3690–3699. PMLR, 2020. 6, 7\n[11] Kai Han, Jianyuan Guo, Yehui Tang, and Yunhe Wang.\nPyramidtnt: Improved transformer-in-transformer baselines\nwith pyramid architecture. arXiv preprint arXiv:2201.00978,\n2022. 1\n[12] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer.arXiv preprint\narXiv:2103.00112, 2021. 7\n[13] Song Han, Huizi Mao, and William J Dally. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149, 2015. 1\n[14] Song Han, Jeff Pool, John Tran, and William J Dally. Learn-\ning both weights and connections for efﬁcient neural net-\nworks. arXiv preprint arXiv:1506.02626, 2015. 1\n[15] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi\nYang. Filter pruning via geometric median for deep con-\nvolutional neural networks acceleration. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4340–4349, 2019. 2\n[16] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning\nfor accelerating very deep neural networks. In Proceedings\nof the IEEE International Conference on Computer Vision ,\npages 1389–1397, 2017. 1, 8\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015. 1\n[18] Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang,\nand G Edward Suh. Channel gating neural networks. arXiv\npreprint arXiv:1805.12549, 2018. 6\n[19] Huawei. Mindspore. https://www.mindspore.cn/,\n2020. 7\n[20] Zihang Jiang, Qibin Hou, Li Yuan, Zhou Daquan, Yujun Shi,\nXiaojie Jin, Anran Wang, and Jiashi Feng. All tokens mat-\nter: Token labeling for training better vision transformers. In\nThirty-Fifth Conference on Neural Information Processing\nSystems, 2021. 7\n[21] Tae Hyun Kim, Mehdi SM Sajjadi, Michael Hirsch, and\nBernhard Scholkopf. Spatio-temporal transformer network\nfor video restoration. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 106–122, 2018.\n1\n[22] Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge\ndistillation by on-the-ﬂy native ensemble. arXiv preprint\narXiv:1806.04606, 2018. 1\n[23] Vadim Lebedev and Victor Lempitsky. Fast convnets using\ngroup-wise brain damage. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n2554–2564, 2016. 2\n[24] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and\nHans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv\npreprint arXiv:1608.08710, 2016. 1\n[25] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,\nShoumeng Yan, and Changshui Zhang. Learning efﬁcient\nconvolutional networks through network slimming. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 2736–2744, 2017. 1, 2, 3, 4\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 7\n[27] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma,\nand Wen Gao. Post-training quantization for vision trans-\nformer. Advances in Neural Information Processing Systems,\n34, 2021. 1\n[28] Zhenhua Liu, Jizheng Xu, Xiulian Peng, and Ruiqin Xiong.\nFrequency-domain dynamic pruning for convolutional neu-\nral networks. Advances in neural information processing\nsystems, 31, 2018. 1\n9\n[29] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? In Advances in Neural Infor-\nmation Processing Systems, volume 32, 2019. 2\n[30] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jian-\nfei Cai. Scalable vision transformers with hierarchical pool-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 377–386, 2021. 6, 7\n[31] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017. 7\n[32] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,\nand Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-\nnary convolutional neural networks. In European conference\non computer vision, pages 525–542. Springer, 2016. 1\n[33] Xiu Su, Shan You, Tao Huang, Fei Wang, Chen Qian, Chang-\nshui Zhang, and Chang Xu. Locally free weight sharing for\nnetwork width search. In ICLR. OpenReview.net, 2021. 2\n[34] Xiu Su, Shan You, Fei Wang, Chen Qian, Changshui Zhang,\nand Chang Xu. Bcnet: Searching for network width with bi-\nlaterally coupled network. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 2175–2184, 2021. 2\n[35] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng,\nChao Xu, and Yunhe Wang. Augmented shortcuts for vision\ntransformers. Advances in Neural Information Processing\nSystems, 34, 2021. 1\n[36] Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao\nXu, Dacheng Tao, and Chang Xu. Manifold regularized dy-\nnamic network pruning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5018–5028, 2021. 6\n[37] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chun-\njing Xu, Chao Xu, and Chang Xu. Scop: Scientiﬁc control\nfor reliable neural network pruning. Advances in Neural In-\nformation Processing Systems, 33:10936–10947, 2020. 2, 3,\n6, 7\n[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 7\n[39] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich,\nand Ivan Titov. Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can be pruned.\narXiv preprint arXiv:1905.09418, 2019. 2\n[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 7\n[41] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and\nHai Li. Learning structured sparsity in deep neural networks.\narXiv preprint arXiv:1608.03665, 2016. 2\n[42] Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Re-\nthinking the smaller-norm-less-informative assumption in\nchannel pruning of convolution layers. arXiv preprint\narXiv:1802.00124, 2018. 1\n[43] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 1, 7\n[44] Mingjian Zhu, Kai Han, Yehui Tang, and Yunhe Wang. Vi-\nsual transformer pruning. arXiv preprint arXiv:2104.08500,\n2021. 2, 6, 7\n10",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7585146427154541
    },
    {
      "name": "FLOPS",
      "score": 0.6944020390510559
    },
    {
      "name": "Computer science",
      "score": 0.6847465634346008
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5520591139793396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.505242645740509
    },
    {
      "name": "Architecture",
      "score": 0.4535563588142395
    },
    {
      "name": "Computer engineering",
      "score": 0.3701440095901489
    },
    {
      "name": "Engineering",
      "score": 0.18355944752693176
    },
    {
      "name": "Parallel computing",
      "score": 0.16171878576278687
    },
    {
      "name": "Electrical engineering",
      "score": 0.1428273618221283
    },
    {
      "name": "Voltage",
      "score": 0.12042874097824097
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}