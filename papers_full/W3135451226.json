{
  "title": "Convolution-Free Medical Image Segmentation Using Transformers",
  "url": "https://openalex.org/W3135451226",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2104215118",
      "name": "Davood Karimi",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A3214236667",
      "name": "Serge Didenko Vasylechko",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A1943132322",
      "name": "Ali Gholipour",
      "affiliations": [
        "Harvard University",
        "Boston Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2104215118",
      "name": "Davood Karimi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3214236667",
      "name": "Serge Didenko Vasylechko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1943132322",
      "name": "Ali Gholipour",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2751665805",
    "https://openalex.org/W2886943560",
    "https://openalex.org/W2805284222",
    "https://openalex.org/W2804047627",
    "https://openalex.org/W6604896550",
    "https://openalex.org/W2807122651",
    "https://openalex.org/W2036885187",
    "https://openalex.org/W2947263797",
    "https://openalex.org/W6603963165",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963717741",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2145889472",
    "https://openalex.org/W1597166651",
    "https://openalex.org/W2035964780",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W91070975",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2803522971",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2033358375",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2959828872",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2065875833",
    "https://openalex.org/W2900298334",
    "https://openalex.org/W2979999210",
    "https://openalex.org/W3035665735",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3019481315"
  ],
  "abstract": null,
  "full_text": "1\nConvolution-Free Medical Image Segmentation\nusing Transformers\nDavood Karimi, Serge Vasylechko , and Ali Gholipour\nDepartment of Radiology, Boston Children’s Hospital, Harvard Medical School, Boston, MA, USA\nAbstract\nLike other applications in computer vision, medical image segmentation has been most successfully addressed using deep\nlearning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties\nsuch as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks\n(CNNs) a strong and useful inductive bias for vision tasks. However, recent works have also highlighted the limitations of\nCNNs for segmenting ﬁne and complex structures in medical images. In this work we show that a different deep neural network\narchitecture, based entirely on self-attention between neighboring image patches and without any convolution operations, can\nachieve more accurate segmentations than CNNs. Given a 3D image block, our network divides it into n3 3D patches, where\nn = 3 or 5 and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the\nblock based on the self-attention between these patch embeddings. We show that the proposed model can achieve segmentation\naccuracies that are better than the state of the art CNNs on three datasets. We also propose methods for pre-training this model\non large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over\nCNNs can be signiﬁcant when labeled training data is small.\nIndex Terms: Medical image segmentation, deep learning, transformers, attention\nFig. 1. Proposed convolution-free network for 3D medical image segmentation.\nFig. 2. Example segmentations predicted by the proposed network and a state of the art CNN.\narXiv:2102.13645v2  [eess.IV]  3 Apr 2022\n2\nI. I NTRODUCTION\nImage segmentation is a central task in medical image\nanalysis. It is routinely used for quantifying the size and shape\nof the volume/organ of interest, population studies, disease\nquantiﬁcation, treatment planning, and computer-aided inter-\nvention. In most medical applications, manual segmentation by\nan expert is regarded as the gold standard. However, automatic\nmethods promise much faster, cheaper, and more reproducible\nsegmentations.\nClassical methods in medical image segmentation run the\ngamut from region growing [11] and deformable models [36]\nto atlas-based methods [32], Bayesian approaches [29], graph\ncuts [26], clustering methods [12], and more. In the past few\nyears, deep learning (DL) methods have emerged as a highly\ncompetitive alternative and they have achieved remarkable\nlevels of accuracy [3] [5] [18] [20] [38] [37] [19]. It appears\nthat DL methods have largely replaced the classical methods\nfor medical image segmentation.\nDue to their success, DL methods for medical image seg-\nmentation have attracted much attention from the research\ncommunity. Recent surveys of the published research on this\ntopic can be found in [14] [31]. Some of the main directions\nof these studies include improving network architectures, loss\nfunctions, and training strategies. Surprisingly, the one com-\nmon feature in all of these works is the use of the convolution\noperation as the main building block of the networks. The\nproposed network architectures differ in terms of the way\nthe convolutional operations are arranged, but they all build\nupon the same convolution operation. There have been several\nattempts to use recurrent networks [10] [2] and attention\nmechanisms [6] for medical image segmentation. However, all\nof those models still rely on the convolution operation. Some\nrecent studies have gone so far as to suggest that in practice\nmost of these architectures achieve largely similar results [17].\nSimilar to semantic segmentation, other computer vision\ntasks such as image classiﬁcation and object detection have\nalso been most successfully addressed with convolutional\nneural networks (CNNs) [30], [23]. These observations attest\nto the importance of the convolution operation to image under-\nstanding and modeling. The effectiveness of the convolution\noperation can be attributed to a number of key properties,\nincluding: 1) local (sparse) connections, 2) parameter (weight)\nsharing, and 3) translation equivariance [25] [24]. These prop-\nerties are in large part inspired by the neuroscience of vision\n[28], and give CNNs a very strong inductive bias. In fact,\na convolutional layer can be interpreted as a fully connected\nlayer with an “inﬁnitely strong prior” over its weights [13].\nAs a result, modern deep CNNs have been able to tackle a\nvariety of vision tasks with amazing success.\nIn other important applications, most prominently in natural\nlanguage processing, the dominant architectures were those\nbased on recurrent neural networks (RNNs) [7], [16]. The\ndominance of RNNs ended with the introduction of the\ntransformer model, which proposed to replace the recurrence\nwith an attention mechanism that could learn the relationship\nbetween different parts of a sequence as a whole [34].\nThe attention mechanism has had a profound impact on the\nﬁeld of natural language processing. In vision applications,\non the other hand, its impact has been much more limited.\nA recent survey of applications of transformer networks in\ncomputer vision can be found in [21]. The number of pixels\nin typical images is much larger than the number of units of\ndata (e.g., words) in natural language processing applications.\nThis makes it impossible to apply standard attention models to\nimages. As a result, despite numerous efforts (e.g., [35] [15]\n[9]) attention mechanism has not yet resulted in the type of\nsea change that has occurred in natural language processing.\nThe recently-proposed vision transformer (ViT) appears to be\na major step towards adapting transformer/attention models\nfor computer vision applications [8]. The main insight in\nthat work is to consider image patches, rather than pixels,\nto be the units of information in images. ViT embeds image\npatches into a shared space and learns the relation between\nthese embeddings using self-attention modules. Given massive\namounts of training data and computational resources, ViT\nwas shown to surpass CNNs in image classiﬁcation accu-\nracy. Subsequent work has shown that by using knowledge\ndistillation from a CNN teacher and using standard training\ndata and computational resources, transformer networks can\nachieve image classiﬁcation accuracy levels on par with CNNs\n[33].\nThe goal of this work is to explore the potential of self\nattention-based deep neural networks for 3D medical image\nsegmentation. We propose a network architecture that is based\non self-attention between linear embeddings of 3D image\npatches, without any convolution operations. We train the\nnetwork on three medical image segmentation datasets and\ncompare its performance with a state of the art CNN. The\ncontributions of this work are as follows:\n1) We propose the ﬁrst convolution-free deep neural network\nfor medical image segmentation.\n2) We show that our proposed network can achieve segmen-\ntation accuracies that are better than or at least on par\nwith a state of the art CNN on three different medical\nimage segmentation datasets. We show that, unlike recent\nworks on image classiﬁcation ([8] [33]), our network can\nbe effectively trained for 3D medical image segmentation\nwith datasets of ∼20 −200 labeled images.\n3) We propose pre-training methods that can improve our\nnetwork’s segmentation accuracy when large corpora of\nunlabeled training images are available. We show that\nwhen labeled training images are fewer in number, our\nnetwork performs better than a state of the art CNN with\npre-training.\nII. M ATERIALS AND METHODS\nA. Proposed network\nFigure 1 shows our proposed network for convolution-free\nmedical image segmentation. The input to the network is a 3D\nblock B ∈I RW×W×W×c, where W denotes the extent of the\nblock (in voxels) in each dimension and c is the number of\nimage channels. The block B is partitioned into n3 contiguous\nnon-overlapping 3D patches {pi ∈I Rw×w×w×c}N\ni=1, where\nw = W/n is the side length of each patch and N = n3 denotes\n3\nthe number of patches in the block. We choose n to be an odd\nnumber. In the experiments presented in this paper n is either\n3 or 5, corresponding to 27 or 125 patches, respectively. The\nnetwork uses the image information in all N patches in B\nto predict the segmentation for the center patch, as described\nbelow.\nEach of the N patches is ﬂattened into a vector of size\nI Rw3c and embedded into I RD using a trainable mapping\nE ∈I RD×w3c. Unlike [8], we do not use any additional tokens\nsince that did not improve the segmentation performance of\nour network in any way. The sequence of embedded patches\nX0 = [Ep1; ...; EpN ] + Epos constitutes the input to our\nnetwork. The matrix Epos ∈ I RD×N is intended to learn a\npositional encoding. Without such positional information, the\ntransformer ignores the ordering of the input sequence because\nthe attention mechanism is permutation-invariant. Therefore,\nin most NLP applications such embedding has been very im-\nportant. For image classiﬁcation, on the other hand, authors of\n[8] found that positional encoding resulted in relatively small\nimprovements in accuracy and that simple 1D raster encoding\nwas as good as more elaborate 2D positional encodings. For\nthe experiments presented in this paper, we left the positional\nencoding as a free parameter to be learned along with the\nnetwork parameters during training because we do not know\na priori what type of positional encoding would be useful. We\ndiscuss the results of experimental comparisons with different\npositional encodings below.\nThe encoder has K stages, each consisting of a multi-\nhead self-attention (MSA) and a subsequent two-layer fully\nconnected feed-forward network (FFN). Both the MSA and\nFFN modules include residual connections, ReLU activations,\nand layer normalization. More speciﬁcally, starting with the\nsequence of embedded and position-encoded patches, X0\ndescribed above, the kth stage of the encoder will perform\nthe following operations to map Xk to Xk+1:\n1) Xk goes through nh separate heads in MSA. The ith head:\na) Computes the query, key, and value sequences from the\ninput sequence:\nQk,i = Ek,i\nQ Xk, Kk,i = Ek,i\nK Xk, Vk,i = Ek,i\nV Xk\nwhere EQ, EK, Ev ∈I RDh×D\nb) Computes the self-attention matrix and then the trans-\nformed values:\nAk,i = Softmax(QT K)/\n√\nDh\nSAk,i = Ak,iV k,i\n2) Outputs of the nh heads are stacked and reprojected back\nonto I RD\nMSAk = Ek\nreproj[SAk,0; ...; SAk,nh]T\nwhere Ereproj ∈I RD×Dhnh\n3) The output of the MSA module is computed as:\nXk\nMSA = MSAk + Xk\n4) Xk\nMSA goes through a two-layer FFN to obtain the output\nof the kth encoder stage:\nXk+1 = Xk\nMSA + Ek\n2\n(\nReLU((Ek\n1 Xk\nMSA + bk\n1 )\n)\n+ bk\n2\nThe output of the last encoder stage, XK, will pass through\na ﬁnal FFN layer that projects it onto the I RNnclass and then\nreshaped into I Rn×n×n×nclass , where nclass is the number of\nclasses (for binary segmentation, nclass = 2):\nˆY = Softmax\n(\nEoutXK + bout)\n)\n.\nˆY is the predicted segmentation for the center patch of the\nblock.\nB. Implementation and training\nThe network was implemented in TensorFlow 1.16 and run\non an NVIDIA GeForce GTX 1080 GPU on a Linux machine\nwith 120 GB of memory and 16 CPU cores. We compare our\nmodel with 3D UNet++ [39], which is a state of the art CNN\nfor medical image segmentation.\nWe trained the network parameters to maximize the Dice\nsimilarity coefﬁcient, DSC [27], between ˆY and the ground-\ntruth segmentation of the center patch using Adam [22]. We\nused a batch size of 10 and a learning rate of 10−4, which\nwas reduced by half if after a training epoch the validation\nloss did not decrease.\nPre-training: Manual segmentation of complex structures\nsuch as the brain cortical plate can take several hours of a\nmedical expert’s time for a single image. Therefore, methods\nthat can achieve high accuracy with fewer labeled training\nimages are highly advantageous. To improve the network’s\nperformance when labeled training images are insufﬁcient, we\npropose to ﬁrst pre-train the network on unlabeled data for\ndenoising or inpainting tasks. For denoising pre-training, we\nadd Gaussian noise with SNR = 10 dB to the input block\nB, whereas for inpainting pre-training we set the center patch\nof the block to constant 0. In both cases, the target is the\nnoise-free center patch of the block. For pre-training, we use a\ndifferent output layer (without the softmax operation) and train\nthe network to minimize the ℓ2 norm between the prediction\nand the target. To ﬁne-tune the pre-trained network for the\nsegmentation task, we introduce a new output layer with\nsoftmax and train the network on the labeled data as explained\nin the above paragraph. We ﬁne-tune the entire network, rather\nthan only the output layer, on the labeled images because\nWe have found that ﬁne-tuning the entire network for the\nsegmentation task leads to better results.\nC. Data\nTable I shows the datasets used in this work. We used ∼\n1/5 of the training images for validation. The ﬁnal models\nwere evaluated on the test images. The images were split into\ntrain/validation/test at random.\n4\nTABLE I\nDATASETS USED FOR EXPERIMENTS IN THIS WORK .\ntarget organ image modality [ntrain, ntest]\nBrain cortical plate T2 MRI [18, 9]\nPancreas CT [231, 50]\nHippocampus MRI [220, 40]\nFig. 3. Example segmentations predicted by the proposed method and\nUNet++.\nIII. R ESULTS AND DISCUSSION\nTable II presents test segmentation accuracy of the propsoed\nmethod and UNet++ in terms of DSC, the 95 percentile\nof the Hausdorff Distance (HD95) and Average Symmetric\nSurface Distance (ASSD). In these experiment, we used these\nparameter settings for our network: K = 7, W = 24, n=\n3, D = 1024 , Dh = 256 , nh = 4 . We used the same\nparameter settings for all experiments reported in the rest\nof the paper, unless otherwise stated. For each dataset and\neach of the three criteria, we ran paired t-tests to see if\nthe differences were statistically signiﬁcant. As shown in the\ntable, segmentation accuracy for the proposed convolution-free\nmethod was signiﬁcantly better than, or at least on part with,\nUNet++. Figure 3 shows example slices from test images in\neach dataset and the segmentations predicted by the proposed\nmethod and UNet++. We have observed that the propsoed\nmethod is capable of accurately segmenting ﬁne and complex\nstructures such as the brain cortical plate. In terms of training\ntime, our network converged in approximately 24 hours of\nGPU time.\nTo assess the segmentation accuracy with reduced num-\nber of labeled training images, we trained our method and\nUNet++ with ntrain = 5, 10, and 15 labeled training images\nfrom cortical plate and pancreas datasets. For cortical plate\nsegmentation, we used 500 unlabeled images from the dHCP\ndataset [4] for pre-training. For pancreas segmentation, we\nused the remaining training images (i.e., 231 −ntrain) for pre-\ntraining. We pre-trained our model as explained in Section\nII-B. To pre-train UNet++, we used the method propsoed in\n[1]. Figure 4 shows the results of this experiment. With the\npropsoed pre-training, the convolution-free network achieves\nsigniﬁcantly more accurate segmentations with fewer labeled\ntraining images.\nFigure 5 shows example attention maps of the proposed\nnetwork for pancreas segmentation. As mentioned above, to\nprocess a test image of a desired size, we apply our network\nin a sliding window fashion. To generate the attention maps\nfor the entire image, at each location of the sliding window\nthe attention matrices (which are of size I RN×N ) are summed\nalong the column to determine the total attention paid to each\nof the N patches by the other patches in the block. Performing\nthis computation in a sliding window fashion and computing\nthe voxel-wise average gives us the attention maps shown in\nthese ﬁgures. They indicate how much attention is paid to\nevery part of the image.\nThe attention maps shown in Figure 5 show that, in general,\nthe early stages of the network have a wider attention scope.\nThey attend to other structures and anatomical features that\nsurround the organ of interest (here, the pancreas). The deeper\nstages of the network are more focused on the pancreas itself.\nA similar behaviour can be seen for cortical plate segmentation\nin Figure 6. In the earlier stage the network attends to the entire\nbrain, while in the deeper layers the network’s attention is\nmore focused to the regions around the cortical plate. Another\nobservation from these ﬁgures, especially Figure 5 is the\nvariability between the attention pattern in different heads of\nthe multi-head self-attention mechanism. In each stage, the\nfour separate heads of the MSA module adopt quite different\nattention patterns. This indicates that the multi-head design\nof the MSA gives the network more ﬂexibility to learn the\nattention patterns that help improve the segmentation accuracy.\nThe importance of multi-head design is well documented in\nnatural language processing applications [34] and our results\nshow it is important for 3D medical image segmentation as\nwell, as we show further below.\nTable III effects of some of the network design choices\non the segmentation accuracy on the pancreas dataset. In\nthis table, the baseline (ﬁrst row) corresponds to the settings\nthat we have used in the experiments reported above, i.e.,\nK = 7, W = 24, n= 3, D= 1024, Dh = 256, nh = 4.\nWe selected these settings based on preliminary experiments\nand we have found them to be good settings for different\ndatasets. Increasing the number/size of the patches or increas-\ning the network depth typically leads to slight improvements\n5\nTABLE II\nSEGMENTATION ACCURACY OF THE PROPOSED METHOD AND UNET++. B ETTER RESULTS FOR EACH DATASET /CRITERION ARE IN BOLD TYPE .\nASTERISKS DENOTE STATISTICALLY SIGNIFICANT DIFFERENCE (p <0.01 IN PAIRED T -TEST .)\nDataset Method DSC HD95 (mm) ASSD (mm)\nBrain cortical plate Proposed 0.879 ±0.026∗ 0.92 ± 0.04 0.24 ±0.03\nUNet++ 0.860 ± 0.024 0.91 ±0.04 0.25 ± 0.04\nPancreas Proposed 0.826 ±0.020∗ 5.72 ±1.61∗ 2.13 ±0.24∗\nUNet++ 0.808 ± 0.021 6 .67 ± 1.80 2 .45 ± 0.21\nHippocampus Proposed 0.881 ±0.017∗ 1.10 ±0.19∗ 0.40 ±0.04\nUNet++ 0.852 ± 0.022 1 .33 ± 0.26 0 .41 ± 0.07\nFig. 4. Segmentation accuracy (in DSC) for the proposed network and UNet++ with reduced labeled training data on the cortical plate (left) and pancreas\n(right) datasets.\nin accuracy. Furthermore, using a ﬁxed positional encoding or\nno positional encoding slightly reduces segmentation accuracy\ncompared with free-parameter/learnable positional encoding.\nFinally, using a single-head attention signiﬁcantly reduces the\nsegmentation accuracy, which indicates the importance of the\nmulti-head design to enable the network to learn a more\ncomplex relation between neighboring patches.\nIV. C ONCLUSIONS\nThe convolution operation has a strong basis in the structure\nof the mammalian primary visual cortex and it is well suited\nfor developing powerful techniques for image modeling and\nimage understanding. In recent years, CNNs have been shown\nto be highly effective in tackling various computer vision\nproblems. However, there is no reason to believe that no other\nmodel can outperform CNNs on a speciﬁc vision task. Medical\nimage analysis applications, in particular, pose speciﬁc chal-\nlenges such as 3D nature of the images and small number of\nlabeled images. In such applications, other models can be more\neffective than CNNs. In this work we presented a new model\nfor 3D medical image segmentation. Unlike all recent models\nthat use convolutions as their main building block, our model\nis based on self-attention between neighboring 3D patches.\nOur results show that the proposed network can outperform\na state of the art CNN on three medical image segmentation\ndatasets. With pre-training for denoising and in-painting tasks\non unlabeled images, our network also performed better than a\nCNN when only 5-15 labeled training images were available.\nWe expect that the network proposed in this paper should be\neffective for other tasks in medical image analysis such as\nanomaly detection and classiﬁcation.\nACKNOWLEDGMENT\nThis project was supported in part by the National Institute of\nBiomedical Imaging and Bioengineering and the National In-\nstitute of Neurological Disorders and Stroke of the National In-\nstitutes of Health (NIH) under award numbers R01EB031849,\nR01NS106030, and R01EB032366; in part by the Ofﬁce of\nthe Director of the NIH under award number S10OD0250111;\nin part by the National Science Foundation (NSF) under\naward 2123061; and in part by a Technological Innovations\nin Neuroscience Award from the McKnight Foundation. The\ncontent of this paper is solely the responsibility of the authors\nand does not necessarily represent the ofﬁcial views of the\nNIH, NSF, or the McKnight Foundation.\nThe DHCP dataset is provided by the developing Hu-\nman Connectome Project, KCL-Imperial-Oxford Consortium\nfunded by the European Research Council under the European\nUnion Seventh Framework Programme (FP/2007-2013) / ERC\nGrant Agreement no. [319456]. We are grateful to the families\nwho generously supported this trial.\n6\nTABLE III\nEFFECT OF SOME OF THE NETWORK HYPERPARAMETERS ON THE SEGMENTATION ACCURACY ON THE PANCREAS DATASET . THE BASELINE (FIRST ROW )\nCORRESPONDS TO THESE SETTINGS : K = 7, W= 24, n= 3, D= 1024, Dh = 256, nh = 4, WHICH ARE THE HYPERPARAMETER VALUES USED IN ALL\nEXPERIMENTS REPORTED IN THIS PAPER OTHER THAN IN THIS TABLE .\nParameter settings DSC HD95 (mm) ASSD (mm)\nbaseline 0.826 ± 0.020 5 .72 ± 1.61 2 .13 ± 0.24\nlarger blocks, n = 5 0 .828 ± 0.023 5 .68 ± 1.63 2 .01 ± 0.18\nno positional encoding 0.818 ± 0.026 5 .84 ± 1.74 2 .26 ± 0.27\nﬁxed positional encoding 0.823 ± 0.021 5 .81 ± 1.54 2 .16 ± 0.23\ndeeper network, K = 10 0 .827 ± 0.018 5 .50 ± 1.48 2 .10 ± 0.21\nshallower network, K = 4 0 .810 ± 0.023 6 .14 ± 1.80 2 .29 ± 0.38\nmore heads, nh = 8 0 .824 ± 0.017 5 .68 ± 1.56 2 .14 ± 0.21\nsingle head, nh = 1 0 .802 ± 0.026 6 .82 ± 1.40 2 .31 ± 0.35\nFig. 5. Example attention maps for two pancreas images.\nFig. 6. Example attention maps for a cortical plate image.\nREFERENCES\n[1] Bai, W., Oktay, O., Sinclair, M., Suzuki, H., Rajchl, M., Tarroni, G.,\nGlocker, B., King, A., Matthews, P.M., Rueckert, D.: Semi-supervised\nlearning for network-based cardiac mr image segmentation. In: Interna-\ntional Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 253–260. Springer (2017)\n[2] Bai, W., Suzuki, H., Qin, C., Tarroni, G., Oktay, O., Matthews, P.M.,\nRueckert, D.: Recurrent neural networks for aortic image sequence\nsegmentation with sparse annotations. In: International Conference on\nMedical Image Computing and Computer-Assisted Intervention. pp.\n586–594. Springer (2018)\n7\n[3] Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempﬂer, M., Crimi, A.,\nShinohara, R.T., Berger, C., Ha, S.M., Rozycki, M., et al.: Identifying\nthe best machine learning algorithms for brain tumor segmentation,\nprogression assessment, and overall survival prediction in the brats\nchallenge. arXiv preprint arXiv:1811.02629 (2018)\n[4] Bastiani, M., et al.: Automated processing pipeline for neonatal diffusion\nmri in the developing human connectome project. NeuroImage185, 750–\n763 (2019)\n[5] Bernard, O., Lalande, A., Zotti, C., Cervenansky, F., Yang, X., Heng,\nP.A., Cetin, I., Lekadir, K., Camara, O., Ballester, M.A.G., et al.:\nDeep learning techniques for automatic mri cardiac multi-structures\nsegmentation and diagnosis: is the problem solved? IEEE transactions\non medical imaging 37(11), 2514–2525 (2018)\n[6] Chen, J., Lu, Y ., Yu, Q., Luo, X., Adeli, E., Wang, Y ., Lu, L., Yuille,\nA.L., Zhou, Y .: Transunet: Transformers make strong encoders for\nmedical image segmentation. arXiv preprint arXiv:2102.04306 (2021)\n[7] Chung, J., Gulcehre, C., Cho, K., Bengio, Y .: Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint\narXiv:1412.3555 (2014)\n[8] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai,\nX., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,\nS., et al.: An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n[9] Dou, H., Karimi, D., Rollins, C.K., Ortinau, C.M., Vasung, L., Velasco-\nAnnis, C., Ouaalam, A., Yang, X., Ni, D., Gholipour, A.: A deep\nattentive convolutional neural network for automatic cortical plate seg-\nmentation in fetal mri. arXiv preprint arXiv:2004.12847 (2020)\n[10] Gao, Y ., Phillips, J.M., Zheng, Y ., Min, R., Fletcher, P.T., Gerig,\nG.: Fully convolutional structured lstm networks for joint 4d med-\nical image segmentation. In: 2018 IEEE 15th International Sympo-\nsium on Biomedical Imaging (ISBI 2018). pp. 1104–1108 (2018).\nhttps://doi.org/10.1109/ISBI.2018.8363764\n[11] Gibbs, P., Buckley, D.L., Blackband, S.J., Horsman, A.: Tumour volume\ndetermination from mr images by morphological segmentation. Physics\nin Medicine & Biology 41(11), 2437 (1996)\n[12] Goldszal, A.F., Davatzikos, C., Pham, D.L., Yan, M.X., Bryan, R.N.,\nResnick, S.M.: An image-processing system for qualitative and quanti-\ntative volumetric analysis of brain images. Journal of computer assisted\ntomography 22(5), 827–837 (1998)\n[13] Goodfellow, I., Bengio, Y ., Courville, A., Bengio, Y .: Deep learning,\nvol. 1. MIT press Cambridge (2016)\n[14] Hesamian, M.H., Jia, W., He, X., Kennedy, P.: Deep learning techniques\nfor medical image segmentation: achievements and challenges. Journal\nof digital imaging 32(4), 582–596 (2019)\n[15] Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial atten-\ntion in multidimensional transformers. arXiv preprint arXiv:1912.12180\n(2019)\n[16] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural com-\nputation 9(8), 1735–1780 (1997)\n[17] Isensee, F., Kickingereder, P., Wick, W., Bendszus, M., Maier-Hein,\nK.H.: No new-net. In: International MICCAI Brainlesion Workshop. pp.\n234–244. Springer (2018)\n[18] Kamnitsas, K., Ledig, C., Newcombe, V .F., Simpson, J.P., Kane, A.D.,\nMenon, D.K., Rueckert, D., Glocker, B.: Efﬁcient multi-scale 3d cnn\nwith fully connected crf for accurate brain lesion segmentation. Medical\nimage analysis 36, 61–78 (2017)\n[19] Karimi, D., Samei, G., Kesch, C., Nir, G., Salcudean, S.E.: Prostate\nsegmentation in mri using a convolutional neural network architecture\nand training strategy based on statistical shape models. International\nJournal of Computer Assisted Radiology and Surgery 13(8), 1211–1219\n(Aug 2018). https://doi.org/10.1007/s11548-018-1785-8, https://doi.org/\n10.1007/s11548-018-1785-8\n[20] Karimi, D., et al.: Accurate and robust deep learning-based\nsegmentation of the prostate clinical target volume in ultra-\nsound images. Medical Image Analysis 57, 186 – 196 (2019).\nhttps://doi.org/https://doi.org/10.1016/j.media.2019.07.005, http://www.\nsciencedirect.com/science/article/pii/S1361841519300623\n[21] Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.:\nTransformers in vision: A survey. arXiv preprint arXiv:2101.01169\n(2021)\n[22] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In:\nProceedings of the 3rd International Conference on Learning Represen-\ntations (ICLR) (2014)\n[23] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with\ndeep convolutional neural networks. In: Advances in neural information\nprocessing systems. pp. 1097–1105 (2012)\n[24] Le Cun, Y ., Boser, B., Denker, J.S., Henderson, D., Howard, R.E.,\nHubbard, W., Jackel, L.D.: Handwritten digit recognition with a back-\npropagation network. In: Proceedings of the 2nd International Confer-\nence on Neural Information Processing Systems. pp. 396–404 (1989)\n[25] LeCun, Y ., Bengio, Y ., Hinton, G.: Deep learning. nature 521(7553),\n436 (2015)\n[26] Mahapatra, D., Buhmann, J.M.: Prostate mri segmentation us-\ning learned semantic knowledge and graph cuts. IEEE Transac-\ntions on Biomedical Engineering 61(3), 756–764 (March 2014).\nhttps://doi.org/10.1109/TBME.2013.2289306\n[27] Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation. In: 3D Vision\n(3DV), 2016 Fourth International Conference on. pp. 565–571. IEEE\n(2016)\n[28] Olshausen, B.A., Field, D.J.: Emergence of simple-cell receptive ﬁeld\nproperties by learning a sparse code for natural images. Nature\n381(6583), 607–609 (1996)\n[29] Prince, J.L., Pham, D., Tan, Q.: Optimization of mr pulse sequences\nfor bayesian image segmentation. Medical Physics 22(10), 1651–1656\n(1995)\n[30] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time\nobject detection with region proposal networks. In: Advances in neural\ninformation processing systems. pp. 91–99 (2015)\n[31] Taghanaki, S.A., Abhishek, K., Cohen, J.P., Cohen-Adad, J., Hamarneh,\nG.: Deep semantic segmentation of natural and medical images: A\nreview. Artiﬁcial Intelligence Review pp. 1–42 (2020)\n[32] Thompson, P.M., Toga, A.W.: Detection, visualization and animation of\nabnormal anatomic structure with a deformable probabilistic brain atlas\nbased on random vector ﬁeld transformations. Medical image analysis\n1(4), 271–294 (1997)\n[33] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J ´egou,\nH.: Training data-efﬁcient image transformers & distillation through\nattention. arXiv preprint arXiv:2012.12877 (2020)\n[34] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, L., Polosukhin, I.: Attention is all you need. arXiv preprint\narXiv:1706.03762 (2017)\n[35] Wang, H., Zhu, Y ., Green, B., Adam, H., Yuille, A., Chen, L.C.:\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\nIn: European Conference on Computer Vision. pp. 108–126. Springer\n(2020)\n[36] Wang, Y ., Guo, Q., Zhu, Y .: Medical image segmentation based on\ndeformable models and its applications. In: Deformable Models, pp.\n209–260. Springer (2007)\n[37] Zeng, Q., Karimi, D., Pang, E.H., Mohammed, S., Schneider, C.,\nHonarvar, M., Salcudean, S.E.: Liver segmentation in magnetic reso-\nnance imaging via mean shape ﬁtting with fully convolutional neural\nnetworks. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. pp. 246–254. Springer (2019)\n[38] Zeng, Q., Samei, G., Karimi, D., Kesch, C., Mahdavi, S.S., Abolmae-\nsumi, P., Salcudean, S.E.: Prostate segmentation in transrectal ultrasound\nusing magnetic resonance imaging priors. International journal of com-\nputer assisted radiology and surgery 13(6), 749–757 (2018)\n[39] Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested\nu-net architecture for medical image segmentation. In: Deep learning in\nmedical image analysis and multimodal learning for clinical decision\nsupport, pp. 3–11. Springer (2018)",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7520641088485718
    },
    {
      "name": "Embedding",
      "score": 0.7060291171073914
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6933515071868896
    },
    {
      "name": "Segmentation",
      "score": 0.6825792789459229
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.6765564680099487
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.6650038361549377
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6192997097969055
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5025341510772705
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4837389290332794
    },
    {
      "name": "Transformer",
      "score": 0.4517563581466675
    },
    {
      "name": "Deep learning",
      "score": 0.43062570691108704
    },
    {
      "name": "Translation (biology)",
      "score": 0.4293139576911926
    },
    {
      "name": "Computer vision",
      "score": 0.3718734681606293
    },
    {
      "name": "Artificial neural network",
      "score": 0.28970497846603394
    },
    {
      "name": "Mathematics",
      "score": 0.15085095167160034
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I1288882113",
      "name": "Boston Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}