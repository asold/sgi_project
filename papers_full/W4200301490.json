{
  "title": "Action Transformer: A self-attention model for short-time pose-based human action recognition",
  "url": "https://openalex.org/W4200301490",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4202072685",
      "name": "Mazzia, Vittorio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284246773",
      "name": "Angarano, Simone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202072683",
      "name": "Salvetti, Francesco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4361290279",
      "name": "Angelini, Federico",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200859206",
      "name": "Chiaberge, Marcello",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3019142091",
    "https://openalex.org/W3046132566",
    "https://openalex.org/W6780950651",
    "https://openalex.org/W6787400974",
    "https://openalex.org/W3093309253",
    "https://openalex.org/W3204380095",
    "https://openalex.org/W6750401302",
    "https://openalex.org/W2922604276",
    "https://openalex.org/W2994857106",
    "https://openalex.org/W2235034809",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6778778669",
    "https://openalex.org/W3033673327",
    "https://openalex.org/W6754299077",
    "https://openalex.org/W6791867541",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W6748709139",
    "https://openalex.org/W3135266094",
    "https://openalex.org/W2971915722",
    "https://openalex.org/W2944006115",
    "https://openalex.org/W6704520437",
    "https://openalex.org/W6738587995",
    "https://openalex.org/W6747795875",
    "https://openalex.org/W6687109816",
    "https://openalex.org/W2977470591",
    "https://openalex.org/W2783323081",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W6773707430",
    "https://openalex.org/W6761327410",
    "https://openalex.org/W6763223085",
    "https://openalex.org/W6762621715",
    "https://openalex.org/W2921077640",
    "https://openalex.org/W3040842087",
    "https://openalex.org/W6775780680",
    "https://openalex.org/W6772404227",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W6762205418",
    "https://openalex.org/W2146634731",
    "https://openalex.org/W6675847638",
    "https://openalex.org/W2013076218",
    "https://openalex.org/W6658929250",
    "https://openalex.org/W6681199532",
    "https://openalex.org/W6697585492",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6794655914",
    "https://openalex.org/W6754826874",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3106678532",
    "https://openalex.org/W2730845691",
    "https://openalex.org/W3042727720",
    "https://openalex.org/W2962773068",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3111660312",
    "https://openalex.org/W2145546283",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3009946848",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W4302310419",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3004261437",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2296311849",
    "https://openalex.org/W4249279051",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W4365800037",
    "https://openalex.org/W2936468594",
    "https://openalex.org/W3198035615",
    "https://openalex.org/W2106233887",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3035225512",
    "https://openalex.org/W2948246283",
    "https://openalex.org/W2913668833",
    "https://openalex.org/W2186413054",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W4243657521",
    "https://openalex.org/W2593463961",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W4299296451",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W4293271853",
    "https://openalex.org/W2896457183"
  ],
  "abstract": null,
  "full_text": "Action Transformer: A Self-Attention Model for Short-Time Pose-Based\nHuman Action Recognition\nVittorio Mazzia1,2,3, Simone Angarano 1,2, Francesco Salvetti 1,2,3, Federico Angelini 4 and\nMarcello Chiaberge1,2\n1 Department of Electronics and Telecommunications, Politecnico di Torino, 10124 Turin, Italy\n2 PIC4SeR, Politecnico di Torino Interdepartmental Centre for Service Robotics, Turin, Italy\n3 SmartData@PoliTo, Big Data and Data Science Laboratory, Turin, Italy\n4 School of Computing, Newcastle University, 1 Science Square, Newcastle, NE45TG, UK\nAbstract— Deep neural networks based purely on attention\nhave been successful across several domains, relying on minimal\narchitectural priors from the designer. In Human Action\nRecognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers,\nimproving the overall generalization capability. In this work,\nwe introduce Action Transformer (AcT), a simple, fully self-\nattentional architecture that consistently outperforms more\nelaborated networks that mix convolutional, recurrent and\nattentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition re-\nsearch, the proposed approach exploits 2D pose representations\nover small temporal windows, providing a low latency solution\nfor accurate and effective real-time performance. Moreover,\nwe open-source MPOSE2021, a new large-scale dataset, as an\nattempt to build a formal training and evaluation benchmark\nfor real-time, short-time HAR. The proposed methodology was\nextensively tested on MPOSE2021 and compared to several\nstate-of-the-art architectures, proving the effectiveness of the\nAcT model and laying the foundations for future work on HAR.\nI. INTRODUCTION\nHuman Action Recognition (HAR) is a problem in com-\nputer vision and pattern recognition that aims to detect\nand classify human actions. The ability to recognize people\ninside a scene and predict their behavior is fundamental\nfor several applications, such as robotics [36], surveillance\nand security [46], [50], autonomous vehicles [33], [7], and\nautomatic video captioning [55], [45], [49]. Most previous\nworks dealing with HAR adopted datasets characterized by\nsamples with a long temporal duration [25], [47], [32]. Thus,\nHAR has been mainly treated as a post-processing operation,\nclassifying complex and long-lasting human actions by ex-\nploiting past and future information. Conversely, in this work,\nwe focus on short-time HAR, which aims at continuously\nclassifying actions within short past time steps (up to a\nsecond). This approach is fundamental to target real-time\napplications: in robotics, for example, HAR problem should\nbe solved promptly to react to sudden behavioral changes,\nonly relying on near past information.\nIn this paper, we propose a new model for HAR called the\nAction Transformer (AcT), schematized in Fig.1, inspired\nThis work has been developed with the contribution of the Politec-\nnico di Torino Interdepartmental Centre for Service Robotics PIC4SeR\n(https://pic4ser.polito.it) and SmartData@Polito (https://smartdata.polito.it).\nFig. 1. Overview of the Action Transformer architecture. Pose estimations\nare linearly projected to the dimension of the model, and together with\nthe class token, they form the input tokens of the transformer encoder.\nAs for Vision Transformer models [18], [44], [15], a learnable positional\nembedding is added to each input token. Then, only the output class token\nis passed through a multi-layer perceptron head to obtain the ﬁnal class\nprediction.\nby the simple and prior-free architecture of the Vision\nTransformer [18]. The Transformer architecture [48] has\nbeen one of the most important deep learning advances of the\nlast years in natural language processing (NLP). Moreover,\nmulti-head self-attention has proven to be effective for a\nwide range of tasks besides NLP, e.g. image classiﬁcation\n[18], [44], [15], image super-resolution [54], [56], and speech\nrecognition [17]. Furthermore, optimized versions of the\nTransformer have been developed for real-time and embed-\nded applications [8], proving that this architecture is also\nsuitable for Edge AI purposes. Recently, many models for\nHuman Action Recognition have proposed the integration\nof attention mechanisms with convolutional and recurrent\nblocks to improve the accuracy of models. However, solu-\ntions that rely exclusively on self-attention blocks have not\narXiv:2107.00606v6  [cs.CV]  10 Jan 2022\nbeen investigated for this task yet.\nWith AcT, we apply a pure Transformer encoder de-\nrived architecture to action recognition obtaining an accurate\nand low-latency model for real-time applications. We study\nthe model at different scales to investigate the impact of\nthe number of parameters and attention heads. We use\nthe new single-person, short-time action recognition dataset\nMPOSE2021 as a benchmark and exploit 2D human pose\nrepresentations provided by two existing detectors: OpenPose\n[9] and PoseNet [34]. Moreover, we compare AcT with\nother state-of-the-art baselines to highlight the advantages\nof the proposed approach. To highlight the effectiveness of\nself-attention, we conduct a model introspection providing\nvisual insights of the results and study how a reduction of\ninput temporal sequence length affects accuracy. We also\nconduct extensive experimentation on model latency on low-\npower devices to verify the suitability of AcT for real-time\napplications.\nThe main contributions of this work can be summarized\nas follows:\n• We study the application of the Transformer encoder\nto 2D pose-based HAR and propose the novel AcT\nmodel, proving that fully self-attentional architectures\ncan outperform existing convolutional and recurrent\nmodels for pose-based HAR.\n• We introduce MPOSE2021, a dataset for real-time\nshort-time HAR, suitable for both pose-based and RGB-\nbased methodologies. It includes 15429 sequences from\n100 actors and different scenarios, with limited frames\nper scene (between 20 and 30). In contrast to other\npublicly available datasets, the peculiarity of having a\nconstrained number of time steps stimulates the devel-\nopment of real-time methodologies that perform HAR\nwith low latency and high throughput.\nThe rest of the paper is organized as follows. Section\nII brieﬂy reports the relevant research on HAR and self-\nattention deep learning methodologies. Section III presents\nthe MPOSE2021 dataset, highlighting features and elements\nof novelty with respect to other existing datasets. Then,\nSection IV describes the architecture of the proposed AcT\nmodel, shortly recalling the main features of the Transformer\nencoder. Section V summarizes the experimentation con-\nducted to verify the effectiveness of the proposed approach,\ngives a visual insight into the model functioning, studies\nits behavior under temporal information reduction, and mea-\nsures its latency for real-time applications. Section VI draws\nsome conclusions for this work and indicates future research\ndirections.\nII. RELATED WORKS\nHuman Action Recognition algorithms aim at detecting\nand classifying human behaviors based on a different source\nof information. Works in this ﬁeld can be mainly subdi-\nvided into two broad categories: video-based and depth-\nbased methodologies[42]. In both, the input consists of a\nsequence of points representing body joints, extracted from\nthe corresponding RGB frame[28], in the case of video-based\nmethods, or a point cloud, in the case of depth-based ones.\nIn the latter, body joints are provided as 3D coordinates\n(skeletal data), such as those captured by Kinect sensors [38],\n[29], that can be possibly projected onto the 2D space [11],\n[53]. On the opposite, in this work, we focus on a video-\nbased analysis, where body joints are already provided as 2D\ncoordinates (pose data) by pose detection algorithms such as\nOpenPose [9] and PoseNet [34]. This characteristic makes\n2D HAR methodologies applicable in a great range of appli-\ncations using a simple RGB camera. Conversely, skeletal data\nrequire particular sensors to be acquired, such as Kinect or\nother stereo-cameras. That raises substantial limitations, such\nas availability, cost, limited working range (up to 5-6 meters\nin the case of Kinect [27]), and performance degradation in\noutdoor environments.\nAngelini et al. ﬁrst collected the MPOSE dataset for video-\nbased short-time HAR [2], obtaining 2D poses by process-\ning several popular HAR video datasets with OpenPose.\nMoreover, the authors proposed ActionXPose, a method-\nology that increases model robustness to occlusions. The\nsequences were classiﬁed using MLSTM-FCN [26], which\nexploits a combination of 1D convolutions, LSTM [23],\nand Squeeze-and-excitation attention [24]. The same authors\nsuccessively applied their approach to anomaly detection [4],\n[3] and expanded MPOSE with the novel ISLD and ISLD-\nAdditional-Sequences datasets. Conversely, Yan et al. [53]\nﬁrst applied OpenPose to extract 2D poses from the Kinetics-\n400 RGB dataset [11] and used graph convolutions to capture\nspatial and temporal information. Similarly, Shi et al. [39],\n[40] applied graph convolutions to pose information using\ntwo streams that extract information from both joints and\nbones to represent the skeleton. Hao et al.[21] introduced\nknowledge distillation and dense-connectivity to explore the\nspatiotemporal interactions between appearance and motion\nstreams along different hierarchies. At the same time, Si et\nbend\nbox\ncheck-watch\ncross-arms\nget-up\nhands-clap\njog\njump\nkick\npick-up\npjump\npoint\nrun\nscratch-head\nsit-down\nstanding\nturn\nwalk\nwave1\nwave2\n0\n500\n1000\n1500\n2000samples\nMPOSE2021 (20 actions, 100 actors, 15429 samples)\nKTH\nIXMAS\ni3DPost\nWeizmann\nISLD\nISLD-AS\nUTKinect\nUTD-MHAD\nFig. 2. The number of samples of MPOSE2021 divided by action.\nThe colors show the distribution of precursor datasets among classes,\nhighlighting the unbalanced nature of the data. The ﬁnal dataset contains\n15429 samples where each sample represents one of 100 actors performing\none of 20 actions.\nKTH\nIXMAS i3DPost\nWeizmann\nISLD\nISLD-AS UTKinectUTD-MHAD\ndataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0aver_conf\ndetector\nopenpose\nposenet\nFig. 3. Comparison between OpenPose and PoseNet average conﬁdence\n(aver conf) in the different MPOSE2021 sub-datasets. The detectors achieve\ndifferent average conﬁdence based on the considered precursor dataset.\nal. [41] put together a hierarchical spatial reasoning network\nand temporal stack learning network to extract discriminative\nspatial and temporal features. On the other hand, Liu et\nal. [30] proposed an ensemble of two independent bone-\nbased and joint-based models using a uniﬁed spatial-temporal\ngraph convolutional operator. Conversely, Cho et al. [14]\nﬁrst applied self-attention [48] to the skeletal-based HAR\nproblem. More recently, Plizzari et al. [35], inspired by Bello\net al. [6], employed self-attention to overcome the locality\nof the convolutions, again adopting a two-stream ensemble\nmethod, where self-attention is applied on both temporal and\nspatial information.\nUnlike previous methodologies, this paper presents an\narchitecture for HAR entirely based on the Transformer\nencoder, without any convolutional or recurrent layer. More-\nover, we focus on verifying the suitability of our method for\nlow-latency and real-time applications. For this reason, we\nintroduce a new 2D pose-based dataset speciﬁcally designed\nfor short-time HAR. Traditional datasets used by previous\nworks, such as 3D NTU RGB+D [38], [29], or Kinetics-\nSkeleton [11], [53], include long temporal sequences which\nmust be entirely scanned to make the correct classiﬁcation. In\ncontrast, the proposed MPOSE2021 dataset includes samples\nwith a temporal duration of at most 30 frames. That makes\nit a new and more suitable benchmark to test the short-time\nand low-latency performance of HAR models.\nIII. THE MPOSE2021 DATASET\nIn this section, MPOSE2021 is presented as an RGB-\nbased dataset designed for short-time, pose-based HAR. As\nin [2], [4], [3], video data have been previously collected\nfrom popular HAR datasets (precursors), i.e. Weizmann [20],\ni3DPost [19], IXMAS [51], KTH [37], UTKinetic-Action3D\n(RGB only) [52], UTD-MHAD (RGB only) [12], ISLD, and\nISLD-Additional-Sequences [2].\n20 22 24 26 28 30\nlength\n101\n102\n103\nsamples\nKTH\n20 22 24 26 28 30\nlength\n102\n103\nsamples\nIXMAS\n20 22 24 26 28 30\nlength\n101\n102\n103\nsamples\ni3DPost\n20 22 24 26 28 30\nlength\n100\n101\n102\nsamples\nWeizmann\n20 22 24 26 28 30\nlength\n101\n103\nsamples\nISLD\n20 22 24 26 28 30\nlength\n103\n104\nsamples\nISLD-AS\n20 22 24 26 28 30\nlength\n101\n102\nsamples\nUTKinect\n20 22 24 26 28 30\nlength\n102\nsamples\nUTD-MHAD\nFig. 4. MPOSE2021 sequence length distribution for the different sub-\ndatasets. It is possible to notice how most of them present a mode of 30\nframes per scene.\nDue to the heterogeneity of actions across different\ndatasets, labels are remapped to a list of 20 common classes.\nActions that cannot be remapped accordingly are discarded.\nTherefore, precursor videos are divided into non-overlapping\nsamples (clips) of 30 frames each whenever possible and\nretaining tail samples with more than 20 frames. A visual\nrepresentation of the number of frames per sample for\neach sub-dataset of MPOSE2021 is shown in Fig. 4. The\npeculiarity of a reduced number of time steps contrasts with\nother publicly available datasets and stimulates the develop-\nment of methodologies that require low latency to perform\na prediction. That would largely beneﬁt many real-world\napplications requiring real-time perception of the actions\nperformed by humans nearby.\nSubsequently, clips not containing a single action are\ndiscarded. Moreover, ambiguous clips are relabelled when-\never possible or discarded otherwise. This process leads to\n15429 samples, where each sample represents a single actor\nperforming a single action. The total number of distinct\nactors in MPOSE2021 is 100, and the number of samples\nfor each action is reported in Fig.2, which also shows the\ndistribution of precursor datasets.\nOpenPose [9] and PoseNet [34] are used to extract land-\nmarks from MPOSE2021 samples. The average conﬁdence\nis computed for each sample as the mean across landmarks\nand frames. It turns out that the two detectors achieve differ-\nent average conﬁdences based on the considered precursor\ndataset. The box plot of Fig.3 describes the comparison\nstatistics.\nDue to the signiﬁcant sample heterogeneity and the high\nnumber of actors, three different training/testing splits are\ndeﬁned for MPOSE2021, i.e. Split1, Split2, and Split3, by\nrandomly selecting 21 actors for testing and using the rest\nof them for training. This division makes the proposed\ndataset a challenging benchmark to effectively assess and\ncompare the accuracy and robustness of different methodolo-\ngies. Moreover, the suggested evaluation procedure requires\ntesting a target model on each split using ten validation\nfolds and averaging the obtained results. That makes it\npossible to produce statistics and reduces the possibility of\noverﬁtting the split testing set with an accurate choice of\nhyperparameters.\nWith MPOSE2021, we aim at providing an end-to-end and\neasy-to-use benchmark to robustly compare state-of-the-art\nmethodologies for the short-time human action recognition\ntask. We thus release a code repository 1 to access the\ndifferent levels of the dataset (video, RGB frames, 2D poses).\nMoreover, we open source a practical Python package to\naccess, visualize and preprocess the poses with standard\nfunctions. The Python package can be easily installed with\nthe command pip install mpose.\nIV. ACTION TRANSFORMER\nIn this section, we describe the architecture of the AcT\nnetwork (Fig.1) brieﬂy recalling some preliminary concepts\nassociated with the Transformer model[48].\nA. AcT Architecture\nA video input sequence with T frames of dimension H ×\nW and C channels Xrgb ∈RT ×H×W×C is pre-processed by a\nmulti-person 2D pose estimation network\nX2Dpose = F2Dpose (Xrgb) (1)\nthat extracts 2D poses of dimension N ×T ×P, where N is\nthe number of human subjects present in the frame and P\nis the number of keypoints predicted by the network. The\nTransformer architecture receives as input a 1D sequence\nof token embeddings, so each of the N sequences of pose\nmatrices X2Dpose ∈RT ×P is separately processed by the AcT\nnetwork. Nevertheless, at inference time, all detected poses\nin the video frame can be batch-processed by the AcT model,\nsimultaneously producing a prediction for all N subjects.\nFirstly, the T poses are mapped to a higher dimension Dmodel\nusing a linear projection map Wl0 ∈RP×Dmodel . As in BERT\n[16] and Vision Transformers [18], [44], [8], [10], a trainable\nvector of dimension Dmodel is added to the input T sequence.\nThis class token [CLS] forces the self-attention to aggregate\ninformation into a compact high-dimensional representa-\ntion that separates the different action classes. Moreover,\npositional information is provided to the sequence with a\nlearnable positional embedding matrix Xpos ∈R(T +1)×Dmodel\nadded to all tokens.\nThe linearly projected tokens and [CLS] are fed to a\nstandard Transformer encoder FEnc of L layers with a post-\nnorm layer normalization [48], [13], obtaining\nXL = FEnc (Xl0 ) =FEnc ([xl0\ncls;X2Dpose ]+ Xpos) (2)\n1https://github.com/PIC4SeRCentre/MPOSE2021\nFig. 5. Transformer encoder layer architecture (left) and schematic\noverview of a multi-head self-attention block (right). Input tokens go through\nL encoder layers and H self-attention heads.\nwhere XL ∈R(T +1)×Dmodel is the overall representation pro-\nduced by the Transformer encoder at its last layer. Finally,\nonly the [CLS] token xcls is fed into a linear classiﬁcation\nhead MLPhead that performs the ﬁnal class prediction\nˆz = MLPhead(xL\ncls) (3)\nwhere ˆz is the output logit vector of the model. At training\ntime, the supervision signal comes only from the [CLS]\ntoken, while all remaining T tokens are the only input of\nthe model. It is important to notice how the nature of the\nnetwork makes it possible to accept a reduced number of\nframes as input even if trained with a ﬁxed T . That gives an\nadditional degree of freedom at inference time, making AcT\nmore adaptive than other existing models.\nThe resulting network is a lightweight solution capable\nof predicting actions for multiple people in a video stream\nwith high accuracy. The advantage of building on 2D pose\nestimations enables effective real-time performance with low\nlatency and energy consumption.\nB. Transformer Architecture\nThe Transformer encoder [48] is made of L layers with\nalternating H multi-head self-attention and feed-forward\nblocks. Dropout [43], Layernorm [5], and residual connec-\ntions are applied after every block. The overall sequence of\nblocks of a Transformer encoder is summarized on the left\nof Fig.5.\nEach feed-forward block is a multi-layer perceptron with\ntwo layers and GeLu [22] non-linearity. The ﬁrst layer\nexpands the dimension from Dmodel to Dml p = 4 ·Dmodel and\napplies the activation function. On the other hand, the second\nlayer reduces the dimension back from Dml p to Dmodel .\nInstead, the multi-head QKV self-attention mechanism\n(MSA) is based on a trainable associative memory with key-\nvalue vector pairs. For the l-th layer of the Transformer\nencoder and the h-th head, queries ( Q), keys ( K) and values\nTABLE I\nACTION TRANSFORMER PARAMETERS FOR THE FOUR VERSION SIZES .\nWE FIX Dmodel /H = 64, LINEARLY INCREASING H, Dml p, AND L IN\nORDER TO OBTAIN DIFFERENT VERSIONS OF THE ACT NETWORK .\nModel H Dmodel Dml p L Parameters\nAcT-µ 1 64 256 4 227k\nAcT-S 2 128 256 5 1,040k\nAcT-M 3 192 256 6 2,740k\nAcT-L 4 256 512 6 4,902k\n(V) are computed as Q = XWQ, K = XWK and V = XWV\nrespectively, where WQ, WK and WV belong to RDmodel ×Dh\n, being Dh the dimension of the attention head. So, for each\nself-attention head (SA) and element in the input sequence\nX ∈RT ×Dmodel we compute a weighted sum over all values\nV. The attention weights Ai j are derived from the pairwise\nsimilarity between two elements of the sequence and their\nrespective query Qi and key Kj representations. Therefore, it\nis possible to compute the attention weights A ∈RT ×T for\nthe l-th layer as\nA = Softmax\n(QKT\n√Dh\n)\n(4)\nand the ﬁnal weighted sum SA as\nSA(X) =AV (5)\nWe perform this SA operation for all the H heads of the l-\nth layer. We concatenate the results and linearly project the\noutput tensor to the original dimension model as\nMSA(X) = [SA1(X);SA2(X);...;SAH(X)]WMSA (6)\nwhere WMSA ∈RH·Dh×Dmodel . All operations are schema-\ntized on the right side of Fig. 5. The input tensor X is ﬁrstly\nprojected to T ×H·Dh, and then a SA operation is performed\nto all H splits of the resulting projected tensor. Finally, all\nhead outputs are concatenated and linearly projected back to\nDmodel . So, the attention mechanism takes place in the time-\ndomain, allowing a global class embedding representation by\ncorrelating different time windows.\nIn order to reduce the number of hyperparameters and\nlinearly scale the dimension of AcT, we ﬁx Dmodel /H = 64,\nvarying H, Dml p, and L to obtain different versions of the\nnetwork. A simple grid search using train and validation\nsets is performed to determine lower and upper bounds for\nthe four parameters. In particular, in Table I, we summarize\nthe four AcT versions with their respective number of\nparameters. The four models (micro, small, medium, and\nlarge) differ for their increasing number of heads and layers,\nsubstantially impacting the number of trainable parameters.\nV. EXPERIMENTS\nThis section describes the main experiments conducted to\nstudy the advantages of using a fully self-attentional model\nfor 2D pose-based HAR. First, the four variants of AcT\ndescribed in Section IV are compared to existing state-of-the-\nart methodologies and baselines on MPOSE2021. Only for\nTABLE II\nHYPERPARAMETERS USED IN ACT EXPERIMENTS .\nTraining\nTraining epochs 350\nBatch size 512\nOptimizer AdamW\nWarmup epochs 40%\nStep epochs 80%\nRegularization\nWeight decay 1e-4\nLabel smoothing 0.1\nDropout 0.3\nRandom ﬂip 50%\nRandom noise σ 0.03\na speciﬁc comparison with ST-TR [35] and MS-G3D [30],\nwe use additional ensemble versions of our model named\nAcT-µ (x n), n being the number of ensembled instances.\nThen, we further analyze the behavior of the network in\norder to get a visual insight of the attention mechanism\nand study the performance under a reduction of temporal\ninformation. Finally, we study model latency for all the\ndesigned architectures with two different CPU types, proving\nthat AcT can easily be used for real-time applications.\nA. Experimental Settings\nIn the following experiments, we employ both the Open-\nPose and PoseNet versions of the MPOSE2021 dataset.\nEither set of data has T = 30 and P = 52 or 68 features,\nrespectively. In particular, for OpenPose we follow the same\npreprocessing of [2] obtaining 13 keypoints with four param-\neters each (position x,y and velocities vx,vy), while PoseNet\nsamples contain 17 keypoints with the same information.\nThe training set is composed of 9421 samples and 2867 for\ntesting. The remaining 3141 instances are used as validation\nto ﬁnd the most promising hyperparameters with a grid\nsearch analysis. All results, training, and testing code for\nthe AcT model are open source and publicly available 2.\nIn Table I are summarized all the hyperparameters for\nthe four versions of the AcT architecture and in Table II\nall settings related to the training procedure. The AdamW\noptimization algorithm [31] is employed for all training with\nthe same scheduling proposed in [48], but with a step drop\nof the learning rate λ to 1e-4 at a ﬁxed percentage (80%)\nof the total number of epochs. We employ the TensorFlow\n23 framework to train the proposed network on a PC with\n32-GB RAM, an Intel i7-9700K CPU, and an Nvidia 2080\nSuper GP-GPU. Following the previously deﬁned benchmark\nstrategy, the total training procedure for the four versions\ntakes approximately 32 hours over the three different splits.\nWe exploit publicly available code for what concerns other\nstate-of-the-art models and use the same hyperparameters\nand optimizer settings described by the authors in almost\nall the cases. The only exception is made for learning rate,\nepochs number, and batch size to adapt the methodologies\nto our dataset and obtain better learning curves.\nB. Action Recognition on MPOSE2021\nWe extensively experiment on MPOSE2021 considering\nsome baselines, common HAR architectures, and our pro-\n2https://github.com/PIC4SeRCentre/AcT\n3https://www.tensorﬂow.org\nTABLE III\nBENCHMARK OF DIFFERENT MODELS FOR SHORT -TIME HAR ON MPOSE2021 SPLITS USING OPEN POSE 2D SKELETAL REPRESENTATIONS .\nMPOSE2021 Split OpenPose 1 OpenPose 2 OpenPose 3\nModel Parameters Accuracy [%] Balanced [%] Accuracy [%] Balanced [%] Accuracy [%] Balanced [%]\nMLP 1,334k 82.66 ± 0.33 74.56 ± 0.56 84.41 ± 0.60 74.58 ± 1.00 83.48 ± 0.58 76.60 ± 0.77\nConv1D 4,037k 88.18 ± 0.64 81.97 ± 1.40 88.93 ± 0.43 80.49 ± 0.95 88.67 ± 0.38 83.93 ± 0.58\nREMNet [1] 4,211k 89.18 ± 0.51 84.20 ± 0.84 88.77 ± 0.35 80.29 ± 0.88 89.80 ± 0.59 86.18 ± 0.40\nActionXPose [2] 509k 87.60 ± 0.98 82.13 ± 1.50 88.42 ± 0.70 81.28 ± 1.40 89.96 ± 1.00 86.65 ± 1.60\nMLSTM-FCN [26] 368k 88.62 ± 0.74 83.55 ± 0.88 90.19 ± 0.68 83.84 ± 1.20 89.80 ± 0.94 87.33 ± 0.67\nT-TR [35] 3,036k 87.72 ± 0.87 81.99 ± 1.64 88.14 ± 0.53 80.23 ± 1.19 88.69 ± 0.95 85.03 ± 1.60\nMS-G3D (J) [30] 2,868k 89.90 ± 0.50 85.29 ± 0.98 90.16 ± 0.64 83.08 ± 1.10 90.39 ± 0.44 87.48 ± 1.20\nAcT-µ 227k 90.86 ± 0.36 86.86 ± 0.50 91.00 ± 0.24 85.01 ± 0.51 89.98 ± 0.47 87.63 ± 0.54\nAcT-S 1,040k 91.21 ± 0.48 87.48 ± 0.76 91.23 ± 0.19 85.66 ± 0.58 90.90 ± 0.87 88.61 ± 0.73\nAcT-M 2,740k 91.38 ± 0.32 87.70 ± 0.47 91.08 ± 0.48 85.18 ± 0.80 91.01 ± 0.57 88.63 ± 0.51\nAcT-L 4,902k 91.11 ± 0.32 87.27 ± 0.46 91.46 ± 0.42 85.92 ± 0.63 91.05 ± 0.80 89.00 ± 0.74\nST-TR [35] 6,072k 89.20 ± 0.71 83.95 ± 1.11 89.29 ± 0.81 81.53 ± 1.39 90.49 ± 0.53 87.06 ± 0.70\nMS-G3D (J+B) [30] 5,735k 91.13 ± 0.33 87.25 ± 0.50 91.28 ± 0.29 85.10 ± 0.50 91.42 ± 0.54 89.66 ± 0.55\nAcT-µ (x2) 454k 91.76 ± 0.29 88.27 ± 0.37 91.34 ± 0.40 86.88 ± 0.48 91.70 ± 0.57 88.87 ± 0.37\nAcT-µ (x5) 1,135k 92.43 ± 0.24 89.33 ± 0.31 91.55 ± 0.37 87.80 ± 0.39 92.63 ± 0.55 89.77 ± 0.35\nAcT-µ (x10) 2,271k 92.54 ± 0.21 89.79 ± 0.34 92.03 ± 0.33 88.02 ± 0.31 93.10 ± 0.53 90.22 ± 0.31\nFig. 6. Visual representation of the benchmark of different models for short-time HAR on MPOSE2021 splits using OpenPose 2D skeletal representations.\nFor brevity and clearness, the average balanced accuracy on the three splits of MPOSE2021 is reported. The lines connect models that use the same\nmethodology.\nposed AcT models. We report the mean and standard devia-\ntion of 10 models trained using different validation splits to\nobtain statistically relevant results. The validation splits are\nconstant for all the models and correspond to 10% of the train\nset, maintaining the same class distribution. The benchmark\nis executed for both OpenPose and PoseNet data and repeated\nfor all the three train/test splits provided by MPOSE2021.\nThe baselines chosen for the benchmark are a Multilayer\nPerceptron (MLP), a fully convolutional model (Conv1D),\nand REMNet, which is a more sophisticated convolutional\nnetwork with attention and residual blocks proposed in [1]\nfor time series feature extraction. In particular, the MLP is\ndesigned as a stack of three fully connected (FC) layers with\n512 neurons each, followed by a dropout layer and a ﬁnal\nFC layer with as many output nodes as classes. Instead, the\nConv1D model is built concatenating ﬁve 1D convolutional\nTABLE IV\nBENCHMARK OF DIFFERENT MODELS FOR SHORT -TIME HAR ON MPOSE2021 SPLITS USING POSE NET 2D SKELETAL REPRESENTATIONS .\nMPOSE2021 Split PoseNet 1 PoseNet 2 PoseNet 3\nModel Parameters Accuracy [%] Balanced [%] Accuracy [%] Balanced [%] Accuracy [%] Balanced [%]\nConv1D 4,062k 85.83 ± 0.71 79.96 ± 1.10 87.47 ± 0.35 78.51 ± 0.78 87.46 ± 0.67 81.31 ± 0.58\nREMNet [1] 4,269k 84.75 ± 0.65 77.23 ± 0.94 86.17 ± 0.68 75.79 ± 1.30 86.31 ± 0.60 79.20 ± 0.79\nActionXPose [2] 509k 75.98 ± 0.72 64.47 ± 1.10 79.94 ± 1.10 67.05 ± 1.40 77.34 ± 1.40 66.86 ± 1.40\nMLSTM-FCN [26] 368k 76.17 ± 0.84 64.75 ± 1.10 79.04 ± 0.72 65.62 ± 1.40 77.84 ± 1.30 67.05 ± 1.20\nAcT-µ 228k 86.66 ± 1.10 81.56 ± 1.60 87.21 ± 0.99 79.21 ± 1.60 87.75 ± 0.53 82.99 ± 0.87\nAcT-S 1,042k 87.63 ± 0.52 82.54 ± 0.87 88.48 ± 0.57 81.53 ± 0.68 88.49 ± 0.65 83.63 ± 0.99\nAcT-M 2,743k 87.23 ± 0.48 82.10 ± 0.66 88.50 ± 0.51 81.79 ± 0.44 88.70 ± 0.57 83.92 ± 0.96\nlayers with 512 ﬁlters alternated with batch normalization\nstages and followed by a global average pooling operator,\na dropout layer, and an FC output stage as in the MLP.\nFinally, the conﬁguration used for REMNet consists of two\nResidual Reduction Modules (RRM) with a ﬁlter number of\n512, followed by dropout and the same FC output layer as\nin the other baselines.\nRegarding state-of-the-art comparisons, four popular mod-\nels used for multivariate time series classiﬁcation, and in\nparticular HAR, are reproduced and tested. Among those,\nMLSTM-FCN [26] combines convolutions, spacial attention,\nand an LSTM block, and its improved version ActionXPose\n[2] uses additional preprocessing, leading the model to\nexploit more correlations in data and, hence, be more robust\nagainst noisy or missing pose detections. On the other hand,\nMS-G3D [30] uses spatial-temporal graph convolutions to\nmake the model aware of spatial relations between skeleton\nkeypoints, while ST-TR [35] joins graph convolutions with\nTransformer-based self-attention applied to both space and\ntime. As the last two solutions also propose a model ensem-\nble, these results are further compared to AcT ensembles\nmade of 2, 5, and 10 single-shot models. We also report\nthe achieved balanced accuracy for each model and use it\nas the primary evaluation metric to account for the uneven\ndistribution of classes.\nThe results of the experimentation for OpenPose are\nreported in Table III and, in synthesis, in Fig.6. The fully\nconvolutional baseline strongly outperforms the MLP, while\nREMNet proves that introducing attention and residual\nblocks further increases accuracy. As regards MLSTM-FCN\nand ActionXPose, it is evident that explicitly modeling both\nspatial and temporal correlations, made possible by the two\nseparate branches, slightly improves the understanding of\nactions with respect to models like REMNet. MS-G3D, in its\njoint-only (J) version, brings further accuracy improvement\nby exploiting graph convolutions and giving the network\ninformation on the spatial relationship between keypoints.\nOn the other hand, ST-TR shows performance comparable\nto all other single-shot models, despite, as MS-G3D, taking\nadvantage of graph information.\nThe proposed AcT model demonstrates the potential of\npure Transformer-based architectures, as all four versions\noutperform other methodologies while also showing smaller\nstandard deviations. Moreover, even the smallest AcT- µ\n(227k parameters) is able to extract general and robust fea-\ntures from temporal correlations in sequences. Increasing the\nnumber of parameters, a constant improvement in balanced\naccuracy can be observed for splits 3, while split 1 and 2\npresent oscillations. The difference between splits reﬂects\nhow much information is conveyed by training sets and how\nmuch the model can learn from that. So, it is evident that\nAcT scales best on split three because it presents complex\ncorrelations that a bigger model learns more easily. On the\ncontrary, it seems AcT- µ is able to extract almost all the\ninformation relevant for generalization from split 2, as the\naccuracy only slightly increases going towards more complex\nmodels.\nAs explained in Section II, ST-TR exploits an ensemble\nof two networks modeling spatial and temporal sequence\ncorrelations, respectively. Moreover, MS-G3D leverages fur-\nther information such as skeleton graph connections and\nthe position of bones in one of the ensembled networks.\nEnsembles are very effective in reducing model variance\nand enhancing performance by exploiting independent rep-\nresentations learned by each network, so it is unfair to\ncompare them with single architectures. For this reason,\nwe create three ensemble versions of AcT- µ to have an\neven confront, with 2, 5, and 10 instances, respectively. To\ncompute ensemble predictions, we average the output logits\nof the network instances before passing through a softmax\nfunction. The results reported at the bottom of Table III\nshow that AcT-µ (x2) outperforms MS-G3D (J+B) in all the\nbenchmarks except for balanced accuracy in split 3, despite\nhaving less than one-tenth of its parameters. Finally, the\nensembles AcT-µ (x5) and AcT- µ (x10), made of 5 and 10\ninstances respectively, achieve even higher accuracy on all\nthe splits with only around 1 to 2 million parameters. That\nproves how the balancing effect of the ensemble enhances\nmodel predictions even without feeding the network addi-\ntional information.\nAs PoseNet data is mainly dedicated to real-time and\nEdge AI applications, only the models designed for this\npurpose have been considered in the benchmark, excluding\nMS-G3D, ST-TR, and AcT-L. In general, the results give\nsimilar insights. The tested models are the same as the\nprevious case, with all the necessary modiﬁcations given\nFig. 7. Self-attention weights A of MPOSE2021 test samples. (l,t, p) represents the AcT-M l-th layer, the true label and the prediction respectively.\nThe three rightmost columns show three attention maps of a failed prediction and the other columns are from correct classiﬁcations. It is clear from all\nexamples how the model focuses on certain particular frames of the series in order to extract a global representation of the scene.\nby the different input formats. In the MLP case, however,\nperformance seriously degrades as networks strongly tend\nto overﬁt input data after a small number of epochs, so the\nresults are not included in Table IV. That is caused by the fact\nthat PoseNet is a lighter methodology developed for Edge\nAI, and hence noisy and even missing keypoint detections\nare more frequent. That results in less informative data and\nemphasizes the difference between sequences belonging to\ndifferent sub-datasets, confusing the model and inducing it\nto learn very speciﬁc but unusable features. The MLP is\ntoo simple and particularly prone to this kind of problem.\nNaturally, all the models are affected by the same problem,\nand the balanced accuracy on PoseNet is generally lower.\nThe same considerations made for OpenPose apply in this\ncase, where AcT outperforms all the other architectures\nand demonstrates its ability to give an accurate and robust\nrepresentation of temporal correlations. Also, it is interesting\nto notice that Conv1D performs better than REMNet, proving\nto be less prone to overﬁtting, and that standard deviations\nare more signiﬁcant than in the OpenPose case.\nC. Model Introspection\nIn order to have an insight into the frames of the se-\nquence the AcT model attends to, we extract the self-\nattention weights at different stages of the network. In Fig.7,\nMPOSE2021 test samples are propagated through the AcT-\nM model, and attention weights A of the three distinct heads\nare presented. It can be seen that the model pays attention\nto speciﬁc frames of the sequence when a speciﬁc gesture\ndeﬁnes the action. On the other hand, attention is much\nmore spread across the different frames for more distributed\nactions such as walking and running. Moreover, it is clear\nhow the three heads mostly focus on diverse frames of\nthe sequence. Finally, the rightmost columns show three\nattention maps of failed predictions. In these last cases,\nFig. 8. Self-attention of the [CLS] token, computed as the normalized sum\nof the last layer of the different heads. Scores give a direct insight into the\nframes exploited by AcT to produce the classiﬁcation output. The example\nclearly shows how bending positions are more insightful for the network to\npredict the jumping-in-place action. In the image, the attention score deﬁnes\nthe skeleton alpha channel.\nattention weights are less coherent, and the model cannot\nextract a valid global representation of the scene.\nInstead, in Fig.8, the last-layer self-attention scores of the\n[CLS] token are shown together with the RGB and skeleton\nrepresentations of the scene. The scores are computed as\nthe normalized sum of the three attention heads and give a\ndirect insight into the frames exploited by AcT to produce\nthe classiﬁcation output. It can be seen that bent poses are\nmuch more informative for the model to predict the jumping-\nin-place action.\nMoreover, we analyze the behavior of the network under\na progressive reduction of temporal information. That can\nbe easily done without retraining due to the intrinsic nature\nof AcT. In Fig.9, we present how the test-set balanced\naccuracy is affected by frame dropping. The two curves\nshow a reduction starting from the beginning and the end\nFig. 9. AcT-M balanced accuracy with an incremental reduction of temporal\ninformation. Due to the intrinsic nature of the network, it is possible to\nreduce the number of temporal steps without a retraining or any kind of\nexplicit adaptation.\nof the temporal sequence, respectively. It is interesting to\nnotice how the performance of AcT degrades with an almost\nlinear trend. That highlights the robustness of the proposed\nmethodology and demonstrates the possibility of adapting the\nmodel to applications with different temporal constraints.\nFinally, we also study the positional embeddings of the\nAcT-M model by analyzing their cosine similarity, as shown\nin Fig.10. Very nearby position embeddings demonstrate a\nhigh level of similarity, and distant ones are orthogonal or\nin the opposite direction. This pattern is constant for all T\nframes of the sequence, highlighting how actions are not\nparticularly localized and that relative positions are essential\nfor all the frames.\nD. Latency Measurements\nWe test the performance of all the considered models for\nreal-time applications. To do so, we use the TFLite Bench-\nmark4 tool, which allows running TensorFlow Lite models on\ndifferent computing systems and collecting statistical results\non latency and memory usage. In our case, two CPUs are\nemployed to measure model speed both on a PC and a mobile\nphone: an Intel i7-9700K for the former and the ARM-\nbased HiSilicon Kirin 970 for the latter. In both experiments,\nthe benchmark executes 10 warm-up runs followed by 100\nconsecutive forward passes, using 8 threads.\nThe results of both tests are reported in Fig. 11, where\nonly the MLP has been ignored because, despite being the\nfastest-running model, its accuracy results are much lower\nthan its competitors. The graph shows the great computa-\ntional efﬁciency of Transformer-based architectures, whereas\nconvolutional and recurrent networks result in heavier CPU\nusage. Indeed, in the Intel i7 case, REMNet achieves almost\nthe same speed as AcT-S, but its accuracy is 2% lower. More-\nover, AcT-µ is able to outperform REMNet, running at over\nfour times its speed. MLSTM-FCN and ActionXPose, being\nsmaller models, achieve lower latencies than the baselines:\n4https://www.tensorﬂow.org/lite/performance/measurement\n0 5 10 15 20 25\nFrames\n0\n5\n10\n15\n20\n25 Frames\n0.2\n0.0\n0.2\n0.4\n0.6\nCosine similarity\nFig. 10. Cosine similarities of the learned T position embeddings of AcT-M\nmodel.\nFig. 11. Study of the latency of different tested models on a high-\nperformance Intel CPU and on a mobile phone equipped with an ARM-\nbased CPU.\nthe former stays between AcT- µ and Act-S, while the latter\nperforms similarly to AcT-S. Those results are remarkable\nbut still outperformed by AcT-µ both on accuracy and speed.\nThe difference with the baselines is even more evident on\nthe ARM-based chip, as convolutional architectures seem to\nperform poorly on this kind of hardware. Indeed, REMNet\nand Conv1D run as fast as AcT-M with signiﬁcantly lower\naccuracies, and AcT-µ is ten times quicker. Nothing changes\nfor what concerns MLSTM-FCN and ActionXPose, less\naccurate and three times slower than AcT- µ.\nVI. CONCLUSION\nIn this paper, we explored the direct application of a purely\nTransformer-based network to human action recognition. We\nintroduced the AcT network, which signiﬁcantly outperforms\ncommonly adopted models for HAR with a simple and fully\nself-attentional architecture. In order to limit computational\nand power requests, building on previous HAR and pose\nestimation research, the proposed methodology exploits 2D\nskeletal representations of short time sequences, providing an\naccurate and low latency solution for real-time applications.\nMoreover, we introduced MPOSE2021, a large-scale open-\nsource dataset for short-time human action recognition, as an\nattempt to build a formal benchmark for future research on\nthe topic. Extensive experimentation with our methodology\nclearly demonstrates the effectiveness of AcT and poses the\nbasis for meaningful impact on many practical computer\nvision applications. In particular, the remarkable efﬁciency\nof AcT could be exploited for Edge AI, achieving good\nperformance even on computationally limited devices. Future\nwork may further investigate the AcT architecture with 3D\nskeletons and long sequence inputs. Moreover, relational\ninformation between points of the estimated pose are cur-\nrently scarcely exploited by our methodology. So, future\ninvestigations may attempt to include skeleton graphs as\nprior knowledge in the positional embedding of the network\nwithout impacting latency.\nREFERENCES\n[1] S. Angarano, V . Mazzia, F. Salvetti, G. Fantin, and M. Chiaberge.\nRobust ultra-wideband range error mitigation with deep learning at the\nedge. Engineering Applications of Artiﬁcial Intelligence , 102:104278,\n2021.\n[2] F. Angelini, Z. Fu, Y . Long, L. Shao, and S. M. Naqvi. 2d pose-based\nreal-time human action recognition with occlusion-handling. IEEE\nTransactions on Multimedia , 22(6):1433–1446, 2020.\n[3] F. Angelini and S. M. Naqvi. Joint rgb-pose based human action recog-\nnition for anomaly detection applications. In 2019 22th International\nConference on Information Fusion (FUSION), pages 1–7. IEEE, 2019.\n[4] F. Angelini, J. Yan, and S. M. Naqvi. Privacy-preserving online human\nbehaviour anomaly detection based on body movements and objects\npositions. In ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 8444–8448.\nIEEE, 2019.\n[5] L. J. Ba, J. R. Kiros, and J. E. Hinton. Layer normalization. CoRR,\nabs/1607.06450, 2016.\n[6] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le. Attention\naugmented convolutional networks. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 3286–3295,\n2019.\n[7] H. Ben-Younes, ´E. Zablocki, P. P´erez, and M. Cord. Driving behavior\nexplanation with multi-level fusion. Pattern Recognition, page 108421,\n2021.\n[8] A. Berg, M. O’Connor, and M. T. Cruz. Keyword Transformer: A Self-\nAttention Model for Keyword Spotting. In Proc. Interspeech 2021 ,\npages 4249–4253, 2021.\n[9] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y . Sheikh. Openpose:\nrealtime multi-person 2d pose estimation using part afﬁnity ﬁelds.\nIEEE transactions on pattern analysis and machine intelligence ,\n43(1):172–186, 2019.\n[10] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal, P. Bojanowski,\nand A. Joulin. Emerging properties in self-supervised vision trans-\nformers. CoRR, abs/2104.14294, 2021.\n[11] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new\nmodel and the kinetics dataset. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , pages 6299–\n6308, July 2017.\n[12] C. Chen, R. Jafari, and N. Kehtarnavaz. Utd-mhad: A multimodal\ndataset for human action recognition utilizing a depth camera and a\nwearable inertial sensor. In 2015 IEEE International conference on\nimage processing (ICIP) , pages 168–172. IEEE, 2015.\n[13] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. F.\nFoster, L. Jones, N. Parmar, M. Schuster, Z. Chen, Y . Wu, and\nM. Hughes. The best of both worlds: Combining recent advances\nin neural machine translation. CoRR, abs/1804.09849, 2018.\n[14] S. Cho, M. Maqbool, F. Liu, and H. Foroosh. Self-attention network\nfor skeleton-based human action recognition. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision ,\npages 635–644, 2020.\n[15] S. D’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos, G. Biroli, and\nL. Sagun. Convit: Improving vision transformers with soft convolu-\ntional inductive biases. In M. Meila and T. Zhang, editors, Proceedings\nof the 38th International Conference on Machine Learning , volume\n139 of Proceedings of Machine Learning Research, pages 2286–2296.\nPMLR, 18–24 Jul 2021.\n[16] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training\nof deep bidirectional transformers for language understanding. In\nJ. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the\n2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers) , pages 4171–4186. Association for Computational\nLinguistics, 2019.\n[17] L. Dong, S. Xu, and B. Xu. Speech-transformer: A no-recurrence\nsequence-to-sequence model for speech recognition. In 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5884–5888, 2018.\n[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In International Conference\non Learning Representations , 2021.\n[19] N. Gkalelis, H. Kim, A. Hilton, N. Nikolaidis, and I. Pitas. The\ni3dpost multi-view and 3d human action/interaction database. In 2009\nConference for Visual Media Production, pages 159–168. IEEE, 2009.\n[20] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri. Actions\nas space-time shapes. IEEE transactions on pattern analysis and\nmachine intelligence, 29(12):2247–2253, 2007.\n[21] W. Hao and Z. Zhang. Spatiotemporal distilled dense-connectivity\nnetwork for video action recognition. Pattern Recognition, 92:13–24,\n2019.\n[22] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 , 2016.\n[23] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735–1780, 1997.\n[24] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 7132–7141, 2018.\n[25] L. Huang, Y . Huang, W. Ouyang, and L. Wang. Part-aligned pose-\nguided recurrent network for action recognition. Pattern Recognition,\n92:165–176, 2019.\n[26] F. Karim, S. Majumdar, H. Darabi, and S. Harford. Multivariate lstm-\nfcns for time series classiﬁcation. Neural Networks , 116:237–245,\n2019.\n[27] B. Langmann, K. Hartmann, and O. Loffeld. Depth camera technology\ncomparison and performance evaluation. In ICPRAM (2), pages 438–\n444, 2012.\n[28] J. Li, X. Liu, M. Zhang, and D. Wang. Spatio-temporal deformable 3d\nconvnets with attention for action recognition. Pattern Recognition,\n98:107037, 2020.\n[29] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y . Duan, and A. C.\nKot. Ntu rgb+ d 120: A large-scale benchmark for 3d human activity\nunderstanding. IEEE transactions on pattern analysis and machine\nintelligence, 42(10):2684–2701, 2019.\n[30] Z. Liu, H. Zhang, Z. Chen, Z. Wang, and W. Ouyang. Disentangling\nand unifying graph convolutions for skeleton-based action recognition.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 143–152, 2020.\n[31] I. Loshchilov and F. Hutter. Decoupled weight decay regularization.\nIn International Conference on Learning Representations , 2019.\n[32] D. Luvizon, D. Picard, and H. Tabia. Multi-task deep learning for\nreal-time 3d human pose estimation and action recognition. IEEE\ntransactions on pattern analysis and machine intelligence , 2020.\n[33] M. Martin, A. Roitberg, M. Haurilet, M. Horne, S. Reiß, M. V oit, and\nR. Stiefelhagen. Drive&act: A multi-modal dataset for ﬁne-grained\ndriver behavior recognition in autonomous vehicles. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages\n2801–2810, 2019.\n[34] G. Papandreou, T. Zhu, L.-C. Chen, S. Gidaris, J. Tompson, and\nK. Murphy. Personlab: Person pose estimation and instance segmen-\ntation with a bottom-up, part-based, geometric embedding model. In\nProceedings of the European Conference on Computer Vision (ECCV),\npages 269–286, 2018.\n[35] C. Plizzari, M. Cannici, and M. Matteucci. Skeleton-based action\nrecognition via spatial and temporal transformer networks. Computer\nVision and Image Understanding , 208:103219, 2021.\n[36] I. Rodr ´ıguez-Moreno, J. M. Mart ´ınez-Otzeta, I. Goienetxea,\nI. Rodriguez-Rodriguez, and B. Sierra. Shedding light on people action\nrecognition in social robotics by means of common spatial patterns.\nSensors, 20(8):2436, 2020.\n[37] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions:\na local svm approach. In Proceedings of the 17th International\nConference on Pattern Recognition, 2004. ICPR 2004. , volume 3,\npages 32–36. IEEE, 2004.\n[38] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang. Ntu rgb+ d: A large\nscale dataset for 3d human activity analysis. In Proceedings of the\nIEEE conference on computer vision and pattern recognition , pages\n1010–1019, 2016.\n[39] L. Shi, Y . Zhang, J. Cheng, and H. Lu. Skeleton-based action\nrecognition with directed graph neural networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 7912–7921, 2019.\n[40] L. Shi, Y . Zhang, J. Cheng, and H. Lu. Two-stream adaptive graph\nconvolutional networks for skeleton-based action recognition. In\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12026–12035, 2019.\n[41] C. Si, Y . Jing, W. Wang, L. Wang, and T. Tan. Skeleton-based\naction recognition with hierarchical spatial reasoning and temporal\nstack learning network. Pattern Recognition, 107:107511, 2020.\n[42] L. Song, G. Yu, J. Yuan, and Z. Liu. Human pose estimation and\nits application to action recognition: A survey. Journal of Visual\nCommunication and Image Representation , page 103055, 2021.\n[43] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov. Dropout: a simple way to prevent neural networks from\noverﬁtting. The journal of machine learning research , 15(1):1929–\n1958, 2014.\n[44] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. Jegou. Training data-efﬁcient image transformers & distillation\nthrough attention. In M. Meila and T. Zhang, editors, Proceedings of\nthe 38th International Conference on Machine Learning , volume 139\nof Proceedings of Machine Learning Research , pages 10347–10357.\nPMLR, 18–24 Jul 2021.\n[45] Y . Tu, C. Zhou, J. Guo, S. Gao, and Z. Yu. Enhancing the alignment\nbetween target words and corresponding frames for video captioning.\nPattern Recognition, 111:107702, 2021.\n[46] G. Vallathan, A. John, C. Thirumalai, S. Mohan, G. Srivastava, and\nJ. C.-W. Lin. Suspicious activity detection using deep learning in se-\ncure assisted living iot environments. The Journal of Supercomputing,\n77(1573-0484):3242–3260, 2021.\n[47] G. Varol, I. Laptev, and C. Schmid. Long-term temporal convolutions\nfor action recognition. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 40(6):1510–1517, 2018.\n[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc., 2017.\n[49] B. Wan, W. Jiang, Y .-M. Fang, M. Zhu, Q. Li, and Y . Liu. Revisiting\nimage captioning via maximum discrepancy competition. Pattern\nRecognition, 122:108358, 2022.\n[50] X. Wang and G. Srivastava. The security of vulnerable senior\ncitizens through dynamically sensed signal acquisition. Transactions\non Emerging Telecommunications Technologies, page e4037, 2021.\n[51] D. Weinland, R. Ronfard, and E. Boyer. Free viewpoint action\nrecognition using motion history volumes. Computer vision and image\nunderstanding, 104(2-3):249–257, 2006.\n[52] L. Xia, C.-C. Chen, and J. K. Aggarwal. View invariant human action\nrecognition using histograms of 3d joints. In 2012 IEEE computer\nsociety conference on computer vision and pattern recognition work-\nshops, pages 20–27. IEEE, 2012.\n[53] S. Yan, Y . Xiong, and D. Lin. Spatial temporal graph convolutional\nnetworks for skeleton-based action recognition. In Proceedings of the\nAAAI conference on artiﬁcial intelligence , volume 32, pages 1113–\n1122, 2018.\n[54] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture\ntransformer network for image super-resolution. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 5791–5800, June 2020.\n[55] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong. End-to-end\ndense video captioning with masked transformer. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition ,\npages 8739–8748, 2018.\n[56] H. Zhu, H. Liu, C. Zhu, Z. Deng, and X. Sun. Learning spatial-\ntemporal deformable networks for unconstrained face alignment and\ntracking in videos. Pattern Recognition, 107:107354, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7968683838844299
    },
    {
      "name": "Artificial intelligence",
      "score": 0.602713942527771
    },
    {
      "name": "Transformer",
      "score": 0.6017850637435913
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5996529459953308
    },
    {
      "name": "Exploit",
      "score": 0.5972289443016052
    },
    {
      "name": "Action recognition",
      "score": 0.518699049949646
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5103155970573425
    },
    {
      "name": "Machine learning",
      "score": 0.4829258620738983
    },
    {
      "name": "Generalization",
      "score": 0.4479866325855255
    },
    {
      "name": "Action (physics)",
      "score": 0.4398888051509857
    },
    {
      "name": "Architecture",
      "score": 0.426917165517807
    },
    {
      "name": "Deep learning",
      "score": 0.4209652245044708
    },
    {
      "name": "Engineering",
      "score": 0.09355190396308899
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}