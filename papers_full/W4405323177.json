{
  "title": "Pecha, a language practice peer: Guiding language learning interactions through large language models",
  "url": "https://openalex.org/W4405323177",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3010696978",
      "name": "Ryan Lege",
      "affiliations": [
        "Kanda University of International Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2899111416",
      "name": "Euan Bonner",
      "affiliations": [
        "Kanda University of International Studies"
      ]
    },
    {
      "id": "https://openalex.org/A1976440137",
      "name": "Takako Aikawa",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3010696978",
      "name": "Ryan Lege",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2899111416",
      "name": "Euan Bonner",
      "affiliations": [
        "Kanda University of International Studies"
      ]
    },
    {
      "id": "https://openalex.org/A1976440137",
      "name": "Takako Aikawa",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293010198",
    "https://openalex.org/W3211732462",
    "https://openalex.org/W4319746602",
    "https://openalex.org/W2556343106",
    "https://openalex.org/W6629228227",
    "https://openalex.org/W198205688",
    "https://openalex.org/W4390273019",
    "https://openalex.org/W3012624518",
    "https://openalex.org/W4360615722",
    "https://openalex.org/W4378086088",
    "https://openalex.org/W6977634207",
    "https://openalex.org/W2804104977",
    "https://openalex.org/W4400762294",
    "https://openalex.org/W4288400169",
    "https://openalex.org/W3129733377",
    "https://openalex.org/W3135047948",
    "https://openalex.org/W4385069730",
    "https://openalex.org/W2918377494",
    "https://openalex.org/W4393221442",
    "https://openalex.org/W6604120125",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4399851955",
    "https://openalex.org/W4400344961",
    "https://openalex.org/W4379054888",
    "https://openalex.org/W4360602037"
  ],
  "abstract": "The interaction hypothesis of second language acquisition (Long, 1981) states that negotiated interaction is necessary for language development. In many language learning contexts, educators and stakeholders seek to provide opportunities for learners to engage in meaningful real-life interactions that help them build linguistic, semantic, and rhetorical competence. However, the opportunities provided for interaction can vary in their degree of effectiveness and may only sometimes lead to increased language ability. If these interactions are scaffolded correctly, they can be tuned to maximize their benefits (Loewen &amp; Sato, 2018). Unfortunately, this is not always practical from a temporal and economic perspective. Artificial intelligence (AI) could be the solution for providing learners with individualized, comprehensive assistance during their learning interactions. Accordingly, the authors developed a bespoke application that employs advanced natural language processing and large language model AI technologies to support learner interactions. The application was developed to support students in two different contexts: Kanda University of International Studies in Japan, where students study English, and Massachusetts Institute of Technology in the United States, where learners study Japanese. The rationale for creating the application and selecting its major features is discussed. This is followed by a discussion of how the application functions and how it will be used. The authors will then discuss their plans for implementation into both informal and formal learning contexts at the two universities. They conclude by discussing potential limitations and plans for improving the application.",
  "full_text": "Technology in Language Teaching & Learning\nISSN 2652-1687 \nhttps://www.castledown.com/journals/tltl/\nTechnology in Language Teaching & Learning, 6(3), 1716 (2024) \nhttps://doi.org/10.29140/tltl.v6n3.1716\nCopyright: © 2024 Ryan Lege, Euan Bonner and Takako Aikawa. This is an open access article distributed \nunder the terms of the  Creative  Commons Attribution Non-Commercial 4.0 International License, which permits \nunrestricted use,  distribution, and reproduction in any medium, provided the original author and source are credited.  \nData Availability Statement: All relevant data are within this paper.\nPecha, A Language Practice Peer:  \nGuiding Language Learning Interactions  \nThrough Large Language Models\nRYAN LEGE a \nEUAN BONNER b \nTAKAKO AIKAWA c \naKanda University of International Studies, Japan \nlege-r@kanda.kuis.ac.jp\nbKanda University of International Studies, Japan \nbonner-e@kanda.kuis.ac.jp\ncMassachusetts Institute of Technology, United States \ntaikawa@mit.edu\nAbstract\nThe interaction hypothesis of second language acquisition (Long, 1981) states that negotiated \ninteraction is necessary for language development. In many language learning contexts, educators and \nstakeholders seek to provide opportunities for learners to engage in meaningful real-life interactions \nthat help them build linguistic, semantic, and rhetorical competence. However, the opportunities \nprovided for interaction can vary in their degree of effectiveness and may only sometimes lead to \nincreased language ability. If these interactions are scaffolded correctly, they can be tuned to maximize \ntheir benefits (Loewen & Sato, 2018). Unfortunately, this is not always practical from a temporal \nand economic perspective. Artificial intelligence (AI) could be the solution for providing learners \nwith individualized, comprehensive assistance during their learning interactions. Accordingly, the \nauthors developed a bespoke application that employs advanced natural language processing and \nlarge language model AI technologies to support learner interactions. The application was developed \nto support students in two different contexts: Kanda University of International Studies in Japan, \nwhere students study English, and Massachusetts Institute of Technology in the United States, where \nlearners study Japanese. The rationale for creating the application and selecting its major features is \ndiscussed. This is followed by a discussion of how the application functions and how it will be used. \nPecha, A Language Practice Peer 2\nThe authors will then discuss their plans for implementation into both informal and formal learning \ncontexts at the two universities. They conclude by discussing potential limitations and plans for \nimproving the application.\nKeywords: CALL, artificial intelligence, AI, large language models, LLM, language learning, \ninteraction \nIntroduction\nLanguage learning contexts and rationales vary considerably worldwide. Language skill can be necessary \nfor career advancement, daily survival, or can be pursued out of intellectual curiosity (Gallagher-Brett, \n2004). In some contexts, language learning is a mandatory part of the educational system. Sometimes, \nwhen learning languages, access to speakers of the target language may be limited, so learning to \ncommunicate can present unique challenges. Though language exists to transmit information between \nparties, its study can often be divorced from this purpose if there are few opportunities to apply a target \nlanguage in authentic, meaningful contexts. \nThe significance of real language practice in second language acquisition (SLA) is a cornerstone of \nLong’s (1981) interaction hypothesis, one of the most influential theories in SLA literature (Tran, \n2009). Long (1981) observed that the conditions for language acquisition are best fostered through \nmodification and negotiation in authentic conversations with target language speakers. Namaziandost \nand Nasri (2019) note that while learners can possess both spoken fluency and accuracy, they may still \nhave difficulty participating in interactions, highlighting the important role of interaction as a mediator \nof communication. Ellis (1991) further expanded on this, positing that the effectiveness of input is \nenhanced through the negotiation of communication issues, which tunes the input to the appropriate \nlevel for language learners. This concept aligns with Krashen’s (1982) theory of comprehensible input, \nthough rather than ensuring an initially suitable input state, Long focuses on the interactive, adaptable \nnature of the input (Tran, 2009). While the interaction hypothesis is widely accepted as a leading \nexplanation of SLA, and the literature broadly supports its claims, scholars (Tran, 2009; Ebrahimi, \n2015) concur that it only partially elucidates the intricate language acquisition process. Nonetheless, it \nis widely acknowledged that interaction and negotiation of input are vital components of SLA. \nIn contexts with limited access to target language speakers, the challenge of providing opportunities \nfor authentic language practice is being addressed in many ways. Advances in computer-mediated \ncommunication (CMC) allow people to engage with others remotely, providing more opportunities to \npractice than were ever available before (Pouromid, 2019). Collaborative international online learning \n(COIL) has become a favored approach to connecting language learners with speakers of their target \nlanguages. Effective COIL, however, must navigate around some unique challenges. These challenges \ninclude scheduling, high-speed internet access, the cost of arranging exchanges, and the constraints \nof CMC (Kučerová, 2023; Minei et al., 2021). Even when there is access to synchronous video con -\nferencing, the field of view limits nonverbal cues (Koester, 2022), and audio channels tend to overlap, \nmaking some of the natural constituents of conversation, like backchanneling and interrupting difficult \n(Boland et al., 2022). Combined, these factors mean that while COIL is effective in many contexts, it \ncannot be universally applied in all contexts that need interactive language practice opportunities.\nRecently, the hegemony of generative AI has pushed it to the forefront of academic discourse in all \ndisciplines, including second language acquisition. This creates another potential avenue for addressing \nthe need for interaction and negotiation in a target language. Educators and researchers have begun to \n3 Technology in Language Teaching & Learning, 6(3)\nexplore how Large Language Models (LLMs), AI models trained on enormous amounts of language \ntexts that can generate human-like text by predicting the next word in a sequence, may be applied \nas practice partners for language learning (Belda-Medina & Calvo-Ferrer, 2022). However, there is \nstill a lack of consensus on whether these models can provide authentic communicative or interactive \nexperiences, with researchers noting that there are still many shortcomings (Zhai, 2023). Wood and \nMoss (2024) argue that human interaction is “irreplaceable” and crucial for “enhancing students’ \ncritical thinking abilities, fostering creativity and providing essential context and perspective” (p. 164). \nResnick (2024) proposes that AI tools should be used to support “creative learning experiences [that] \ninvolve people learning with and from one another” (p. 8). In accordance with Resnick’s proposal, and \ngiven the current state of LLMs, instead of pursuing AI as a replacement for human interaction, it may \nbe more beneficial to consider the potential of AI to augment and support human interaction. \nLanguage educators in many contexts still rely on creating face-to-face opportunities for interaction, \ndespite the current focus on the capabilities of LLMs. This is the case for the institutions where the \nauthors of this paper teach foreign languages. The authors currently teach in very different contexts \nand are responsible for teaching different languages, Japanese and English. The first context is teaching \nJapanese at a private research institution in the United States, Massachusetts Institute of Technology \n(MIT). The second context is teaching English to students at a private university, Kanda University of \nInternational Studies (KUIS) in Japan. Each context deals with the issue of providing opportunities for \nreal language practice in different ways. \nJapanese Practice at MIT\nStudents studying Japanese at MIT can participate in the Japanese Lunch Table , where students \ncan practice Japanese with native speakers while having their lunch. The sessions are not limited to \nMIT students; they are also available to exchange students, visiting scholars and their spouses. The \nJapanese Lunch Table is very informal and welcomes students of all proficiency levels, from beginners \nto advanced speakers. However, the lack of structure may result in participants having limited real \nconversations in Japanese. \nIn a third-year Japanese class at MIT Japanese speakers are invited to join the class via Zoom twice a \nsemester, allowing students to practice their Japanese in Zoom’s breakout rooms. During these sessions, \nstudents are encouraged to ask questions, with each breakout room assigned one native Japanese \nspeaker and two or three students. Each session lasts 45 minutes. The students enjoy this experience \nas it provides a rare opportunity to engage in in-depth conversations with Japanese speakers, beyond \njust greetings and self-introductions. However, the sessions occur only twice a semester, which is not \nfrequent enough, and providing real-time feedback during the sessions is challenging despite visits to \nall the breakout rooms.\nEnglish Practice at KUIS\nAt KUIS, all students study English in addition to other languages, global topics, and liberal arts \nsubjects. To provide opportunities to practice the target language, the university has prepared facilities \nand services for students’ benefit. They can meet with lecturers for formalized practice sessions or \njoin informal group discussions in the English lounge, an all-English area available to students during \nthe university’s operating hours. In addition, students can choose language practice partners from the \nbody of exchange students at the university to provide mutual practice opportunities for both parties. \nThe wide variety of formal and informal opportunities for students to use their language of study is \none of the strong points of the university, which has also been expanded to offer online, synchronous \nvideo sessions. However, the availability of self-access language practice facilities and services is not \nPecha, A Language Practice Peer 4\nwithout its issues; sometimes, students may find it challenging to fit practice sessions into their busy \nschedules and may come out of sessions unsure of how they benefited (see Mynard et al., 2020). This \nresults in some students not using the services offered or not using them as much as they need for \npractice.\nIssues with Language Practice Sessions\nIn the current language practice sessions offered in both contexts, learners have valuable opportunities \nto utilize their language skills. However, there are significant areas for improvement. When learners \nengage in real interaction and negotiation of meaning, it can be challenging to document and retain \nnew phrases, insights, and other information in real time. Even if students engage in a post-interaction \nreflective task, they may struggle to recall important conversation details. This can lead to a situation \nwhere the experiences are valuable yet have little long-term effect on language development. The \neffectiveness of informal language practice sessions can vary greatly depending on the session structure \nand individual learner variables. As Loewen and Sato (2018) stress, mere interaction is not enough; \neducators should focus on maximizing the impact of interaction for the benefit of language learners. \nAI, specifically LLMs, can provide solutions to these problems.\nAI as a Solution\nWhat Are LLMs?\nFirst proposed by Google researchers (Vaswani et al., 2017), deep learning transformer-based LLMs \nare AI systems trained on vast quantities of human-generated content and can produce human-like \ntext and other media based on their training data. As the technology has improved, they have become \nexpert-level text generators in many fields including language acquisition (O’Grady & Lee, 2023). \nSince the advent of these LLMs, most notably the LLM powering OpenAI’s ChatGPT , language \nteachers, researchers, and learners have begun exploring and applying them in both formalized and \ninformal instruction. \nMany of the current LLMs available for use, such as OpenAI’s GPT range, Google’ s Gemini, and \nAnthropic’ s Claude, are also available as application programming interfaces (APIs), allowing for \napplication developers to integrate them into their own applications. Applications that have begun \nto incorporate LLMs to implement such intelligent systems and personalization include Duolingo, \nSquirrel AI, Mathway, Grammarly, Coursera, VIPKid, and Khan Academy.\nHow Are LLMs Being Applied in Language Education?\nWhile the field of computer-assisted language learning (CALL) has been using natural language \nprocessing (NLP) models for years (Antoniadis et al., 2013), widespread access to LLMs that are \nsimple and easy to use has made NLP more accessible than ever before. Gao et al. (2023) demonstrated \nthrough empirical means that LLMs can exhibit high spoken language intelligence and impressive \naccuracy when prompted with specific domain knowledge and best prompting techniques. They note \nthat LLMs “hold considerable promise for improving conversational spoken language learning”  \n(p. 15). LLMs have even been shown to be an effective method for generating written feedback and can \nbe a positive and motivating tool to improve student writing (Meyer et al., 2024).\nAdditionally, LLMs are increasingly being used as tutors to help support learners and give them practice \nopportunities. Kwon (2023) designed a custom tutor, GPTutor, to generate personalized feedback to \nlearners for study at the appropriate language level for students. Liu et al. (2024) investigated using \n5 Technology in Language Teaching & Learning, 6(3)\nLLMs to enhance an intelligent tutoring system that used pictures as prompts for conversation with an \nLLM. They found that the LLM could offer various types of cognitively scaffolded feedback based on \nconstructivist learning theories to “align with personalized learning needs” (p. 6). The increasing use \nof LLM-powered chatbots in educational settings like this, both on and off-campus, is transforming \nstudent support and practice opportunities. These chatbots, with their advanced capabilities and \naccessibility, are fostering engaging and interactive learning environments that encourage independent \nlearning and boost student engagement (Kooli, 2023; Wang & Xue, 2024).\nMoving beyond tutoring systems, Yu and Guo (2023) discussed additional avenues for artificial intel-\nligence systems powered by LLMs to enhance educational outcomes. They suggest that intelligent \nsystems could be used for marking homework and additional speech interaction with the content. They \nalso discuss using LLMs to personalize assessment, learning content, teaching routes, and learning \nexperiences. \nHow Are the Authors Applying LLMs in Their Contexts?\nThe authors at KUIS have been utilizing this access to LLM APIs since 2021 in several projects aimed \nat investigating its usage in language acquisition. In 2021, a study was conducted on the potential \nfor LLMs to be utilized by teachers and students in the classroom (Bonner, Lege, & Frazier, 2023). \nIn 2022, the authors created and tested a student-customizable AI conversation partner, where stu -\ndents were able to create characters with individual appearances, personalities, and life histories for \nclassroom conversation practice (Loretzen & Bonner, 2023). Current projects include a classroom \nmaterial-trained AI teacher assistant web application for students to query during class to aid in their \nunderstanding and AI-generated live-action cartoons that students can adjust the content and language \nlevel of in real time to suit their learning goals.\nThe author at MIT has been adapting the use of LLMs since 2023 for her third-year Japanese \ncourse. She has explored whether AI could assist by providing instant feedback on student writing, \nthus freeing up time for instructors to focus more on designing in-class learning activities. The \ninitial prompt provided to the LLM used for this assignment is simple: “modify my Japanese.” \nStudents are then required to perform a writing assignment that consists of three elements: (i) their \noriginal sentence, (ii) their modified version by AI, and (iii) their reflection and/or analysis on the \ndifferences between (i) and (ii). Overall, the feedback from students regarding the use of AI for this \nassignment has been positive. They particularly appreciate being able to consult with AI anytime \nand anywhere and find that they can learn vocabulary and grammar beyond the scope of the text -\nbook. However, some students have expressed doubts about the efficacy of AI in improving their \nwriting skills. \nIn 2023, the authors at KUIS and MIT agreed to combine their LLM research experience by \ncollaborating on a project to investigate the role that LLMs could play as third-party participants \nin language conversation practice, customized to the student’s needs and able to provide post \nconversation feedback based on their specific needs and performance. Across society, much of the \nfocus is on fears that AI will replace human teachers and tutors in the current language teaching and \nlearning landscape. While the ability for AI to complete tasks with near human efficiency is possible, \nthe authors felt that instead of using AI as replacements, it would be more meaningful to consider AI’s \npotential to augment the human-to-human component of language learning. Rather than replacing the \npractice partners or tutors with AI versions, it would be better to focus on the ability of AI to provide \nscaffolding support and feedback. Therefore, the authors are developing an application, Pecha (named \nafter the Japanese phrase pecha kucha, which is an onomatopoeic expression that means chattering) , \nto allow learners to maximize their access to the affordances of face-to-face interaction. To do this, the \nPecha, A Language Practice Peer 6\nauthors have reconceptualized the role of AI not as a replacement conversation partner or teacher, but \nas a peer that supports learners before interactions, participates during them, and helps the learners \nreflect on their experience afterwards.\nApplication Description\nIn both the KUIS and MIT contexts, language learners often practice what they have learned by \nengaging in language practice sessions with another speaker of the target language. For this project, \nthe authors are developing Pecha to support learners in these interactions. A learner sits down with \na practice partner or instructor and engages in a 15-minute conversation on a topic of their choice in \nthe target language. The application is displayed on a monitor and sits to the side of the table between \nthe two speakers. The application harnesses a combination of LLM generative AI, speech-to-text, and \ntext-to-speech technologies, initially supporting both Japanese and English modes, with a modular \ndesign that allows for the inclusion of additional languages. This technical integration is facilitated \nthrough a modular framework utilizing cloud server requests to several different online services for all \ncore functionality. The backbone design of the application is platform agnostic, allowing for the testing \nand use of various interchangeable AI services depending on the circumstances. The flexible nature of \nthe application allows it to be adapted for different contexts where there may be varying pedagogical \nneeds, such as scaffolding for lower-level learners in their first language.\nFor AI, the application can switch between accessing LLM platforms such as those offered by OpenAI, \nAnthropic, Google, and even open-source fine-tuned language-specific LLMs, ensuring adaptability \nto the evolving landscape of LLM AI technologies. For example, in cases where a significant amount \nof text needs to be sent for analysis and summarization, LLMs with APIs supporting extremely long \ncontext windows such as Gemini (which currently supports up to approximately 2 million short words, \nor tokens, of content) can be accessed and used. \nSpeech-to-text is handled by individual speaker diarization via speech-to-text cloud services. \nDiarization enables each speaker to be transcribed separately, enabling both overall conversation \nand individual speaker analysis by isolating specific speakers during conversations. Services such as \nAmazon Web Services, Assembly.ai, and Deepgram offer varying levels of accuracy depending on the \naudio environment and language. This speaker role separation subsequently allows for the isolation \nof both the learner’s and interlocutor’s output which can then be forwarded individually to the LLM \nfor processing while still allowing the separate processing of the conversation as a whole. As the \nongoing diarized transcript is received, it is paired with specialized commands that instruct the AI at \nregular intervals to monitor and respond to issues with vocabulary misapplication, sentence structure, \nand conversation coherence. Additionally, it catalogs these issues along with an overall conversation \nsummary for personalized post-session feedback.\nPrivacy and data protection are paramount in the design of this system. V oice recordings are immediately \ndiscarded after they are converted to text. When submitting content for analysis, the application utilizes \nLLM platforms with strict policies against using user data for training or other purposes. Additionally, \nstandard secure online databases handle and store all incoming and outgoing information. Students are \nalso fully informed about data handling practices at the beginning of their sessions and are advised \nto avoid discussing personal or identifying information during their interactions. Finally, to ensure \nstudents only access their own content, the system generates a five-digit alphanumeric passcode for \nthem to use at the beginning of each session.\nText-to-speech is integrated to provide the option for auditory as well as text-based responses by \nthe AI and utilizes cloud services that provide realistic life-like voice synthesis such as Resemble.\n7 Technology in Language Teaching & Learning, 6(3)\nai, Google Cloud Voices, and OpenAI text-to-speech. The application also features an intuitive touch \ninterface paired with an animated avatar for enhanced interaction in language acquisition scenarios. \nBased on existing artwork used in a previous AI chatbot project (Lorentzen & Bonner, 2023), the user \ncan customize the AI’s appearance and voice (Figure 1). UI and UX appearance and flow are based on \ncurrent best practices. The combination of realistic voice synthesis, customizable avatars, and user-\nfriendly design elements works to create a more immersive and less intimidating learning experience, \npotentially boosting learner confidence and participation.\nNeeds Assessment\nFor Pecha to meet learner needs, it needs to know more about their background and learning goals. \nOn first use learners create an account, and then an intuitive onboarding system guides them through a \nseries of simple questions to elicit key information about their language learning needs and goals. For \nexample, if the learner is taking a language aptitude test, they can specify which areas of the test are the \nmost problematic and give their desired outcome/score. They are also prompted to add any language \nstructures or vocabulary they want to use during their session. If they wish to add any additional \nmaterials or texts, they can also do that here (Figure 1). \nAfter learners have used the application once, the pre-session process is simplified for subsequent uses, \nwith learners having the option to revise or supplement this information. If the learner has completed \na prior session, Pecha welcomes returning learners with a succinct overview of their previous session, \nbridging past and present learning while promoting progress through strategic repetition.\nInteraction Phase\nDuring the conversation practice session, the AI can be set to play the role of an active participant in \naddition to listening and taking notes for post-session feedback. When Pecha is in active mode, it listens \nFigure 1. Pre-interaction needs assessment prototype menu.\nPecha, A Language Practice Peer 8\nand requests permission to intervene via a non-intrusive visual/audio cue to offer real-time assistance \n(Figure 2). The AI can participate in the discussion, inject comments, ask probing questions, and subtly \ncorrect incorrect vocabulary by weaving the right words into its responses. Users can prompt the AI for \nsuggestions or examples and even set it to challenge their opinions, enhancing critical thinking through \nconversational role-play. As Pecha is not placed directly between the participants, they can choose to \nignore any of these cues.\nOn a technical level, this is achieved by having the application record audio in fifteen-second portions \nand send them sequentially to a speech-to-text transcription service, which diarizes and returns the \ntranscription to the application. The application then sends that transcription regularly to an LLM for \nsummarization and language learning analysis. \nThe analysis itself consists of monitoring the transcription to identify the topic of conversation, the \nopinions of the speakers, and potential areas for the application to chime in with its own opinions \nand suggestions. Additionally, on the language learning assistance side, the application also monitors \nthe type of vocabulary and sentence structures being used so that when it chimes in, it can attempt to \nutilize them in a contextually appropriate manner. This content is also provided to the learner at the end \nof the conversation for self-study.\nPecha determines when to chime in based on the contents of the conversation, what the students \nset as their learning goals, and the amount of time that has passed since it last contributed. This is \ndetermined by comparing the student’s stated goals with the content of the ongoing conversation and \nidentifying patterns of vocabulary misapplication, conversation topic expansion, areas for potential \nsentence structure improvement, and other goal-related issues. When the application determines that \nit is suitable for it to chime in, the screen provides a visual cue, and a light audio tone is played. The \nlearner can then decide to ignore the request or tap the screen to allow Pecha to begin speaking using \nFigure 2. During interaction prototype screen.\n9 Technology in Language Teaching & Learning, 6(3)\ntext-to-speech. Significant attention has been paid to making sure that the chime-in cues do not overly \ndistract the participants from their conversation and students are also free to press a button to initiate \na direct conversation with the AI to get specific feedback whenever desired. These features strike a \nbalance between learner autonomy and guided instruction, allowing users to choose when they receive \nfeedback while ensuring all insights are firmly grounded in the context of their interaction.\nFeedback and Reflective Analysis\nPost-interaction, Pecha presents the learner with a diarized interaction transcript. The application \nalso analyzes the transcript for the complexity of lexical items and applies formatting to highlight \nvocabulary used based on word lists and the target vocabulary identified by the student. The transcript \nis segmented by topic, with summaries of the interaction provided in a panel of the user interface. This \nallows the learners to recall and reflect on their performance during the interaction. Finally, Pecha \noffers suggestions about vocabulary and language to improve future interactions (Figure 3). \nAdditional information and features can also be easily provided, such as suggested learning plans. This \ndata is also fed back into the pre-interaction system to supplement the original data provided in the \nneeds analysis step for subsequent interactions. \nLimitations\nDiarization Accuracy\nOne of the most important elements of the system is its ability to discern individual speakers through \ndiarization. The ability to differentiate the learner from their  interlocutor in the transcription and \nFigure 3. Post-interaction dashboard prototype screen.\nPecha, A Language Practice Peer 10\nprovide advice relevant only to the learner is paramount. The technology has advanced significantly \nand continues to do so, but it is not infallible. While rare, instances where both speakers are at \ntimes incorrectly merged into a single line of dialogue attributed to only one speaker do occur. \nSuch inaccuracies can impede the application’s ability to provide precise, personalized feedback \neven though it does not affect the AI’s understanding of the overall conversation. Fortunately, the \ndiarization system is overall quite capable of differentiating speakers by voice, and the merger of \ntwo speakers into a single line of dialogue has so far not resulted in the rest of the transcription \nsubsequently mixing up roles as to who is Speaker A or Speaker B. However, one of the more \ncommon problems being faced is that the diarization system always assigns the first speaker heard \nto the role of Speaker A, necessitating an opening message being displayed on the screen that the \nlearner always commence the conversation for the application to provide assistance to the correct \nspeaker.\nTranscription Accuracy\nThe effectiveness of the application is also reliant on the clarity of the learner’s pronunciation and \ncorrect language usage. Speech-to-text technologies have long struggled with accuracy due to their \ntraining data, which results in less accurate transcripts when the user has an accent that differs from \nthe training data (Koenecke et al., 2020). Hirai and Kovalyova (2024) examined accuracy rates \nfor Japanese learners and found that the speech-to-text models they investigated struggled with \nincorrect syllabification and phonemic misapplication. This problem is particularly problematic \nfor Japanese speakers who may misapply the /r/ and /l/ sounds or syllabify words with consonant \nclusters. However, their study was conducted in 2020 using tools developed before today’s more \npowerful models. Modern speech-to-text APIs also benefit from the ability to create custom \nlexicons to aid the models’ accuracy. The researchers can also see the confidence levels for each \nword transcribed by the software. That said, in cases where students have pronounced accents or \nsignificantly misapply complex grammar structures or vocabulary, the accuracy and usefulness of \nthe system will be affected as it will struggle to transcribe dialogue lines accurately, leading to less \neffective feedback. This can be further complicated by extended periods of silence or minimal verbal \ninteraction from shy or less proficient learners. Therefore, this application will be predominantly \nhelpful for language learners at intermediate or advanced levels who wish to work on higher-level \nskills rather than acquire basic ones.\nAudio Environment Sensitivity\nTranscription performance is also highly dependent on the audio environment. While the system can \nhandle a certain level of indistinct background noise, nearby conversations or loud ambient sounds \ncan disrupt the transcription and diarization processes. For instance, a conversation taking place \nin a busy language practice space on campus might result in the AI capturing snippets of adjacent \ntables, potentially confusing these with the learner’s dialogue and thus distorting the feedback. To \naddress this, Pecha will support multiple microphone inputs in the future to allow for individual lapel \nmicrophones.\nReliability of LLM Models for Extended Conversations\nThe application’s dependency on LLMs introduces another layer of complexity when working with \nextended conversations. As dialogues grow in length and the amount of transcribed text increases, the AI \nis more likely to provide irrelevant or inaccurate feedback. This risk is exacerbated if the conversation \nsummary becomes overly long, which can lead to feedback errors. To mitigate this challenge, the \n11 Technology in Language Teaching & Learning, 6(3)\napplication attempts to truncate summaries after they reach a significant length by re-submitting them \nfor further summarization at regular intervals. However, some conversation details can be lost in this \nprocess, so the project has limited conversation sessions to fifteen minutes.\nAI Interjection Frequency\nThe design of the AI’s interjection system to engage and participate in the conversation also presents \nchallenges. The AI is programmed to interject when it feels appropriate within calculated interval \nwindows, such as a maximum of once every forty-five seconds or more if it already recently \nparticipated. Due to the nature of LLMs and their tendency to fall into self-determined patterns, this \ncan lead to a situation where the AI may incorrectly detect a pattern in its past participation behavior \nand start to participate only at self-determined set intervals, regardless of the content of the actual \nconversation. This can cause it to miss opportunities for more naturally timed contributions that \ncould better enhance learning. For example, if a learner is determined to misunderstand something, \nthe AI might wait too long to interject rather than provide more immediate help. To address this, \nthe application is designed to recall only the time since the most recent interjection and forget all \nprevious interjection timings.\nFuture Plans and Conclusion\nImplementation Plans\nThe authors plan to trial Pecha within the Japanese and English language practice areas of MIT and \nKUIS. The application will allow learners to maximize the benefits of those interactions at both \nuniversities, where robust systems already support students in practicing their target language in real \ncontextual situations with peers and teachers. It will focus on students’ goals and learning needs and \nprovide comprehensive post-interaction support, including advice, self-study tasks, transcripts, and \nconversation summaries. This reflective and iterative analysis offers instructive feedback to students, \nallowing them to see their progress over time in terms of proficiency. This holistic feedback helps \nstudents reflect deeply on their experiences. Given students’ busy schedules, Pecha increases the \neffectiveness of their limited interaction time. It can function seamlessly in both face-to-face and online \nsettings, thus expanding accessibility, but will probably be most advantageous during face-to-face \ninteractions, where the added affordances of a shared environment and fully visible non-verbal cues \nenrich the interaction. Pecha will be a supportive system that builds learners’ self-confidence, like an \nencouraging peer that assists learners on their journey towards improved communicative competence.\nThe focus of this ongoing project is on development of the prototype and testing its performance with \nJapanese and English learners across various learning contexts at both MIT and KUIS. Once sufficient \ndata has been gathered and issues addressed, there are several potential next steps for development \nand expansion, including expanding support for additional languages and investigating what other \nforms of feedback the system can provide students both during and after their conversation sessions. \nAdditionally, for language educators, the application could compile session summaries and offer \ncustomizable performance feedback that is also made available directly to teachers. This feedback \ncould be collated along with other students to provide teachers insight into the performance of whole \nclasses, providing insights into their progress and areas needing attention, allowing for targeted support \nand enhanced teaching efficacy.\nThere are few fields with such an obvious natural connection to LLMs, as the field of language learning. \nThough the purposes for language learning may vary, the concept of language as the medium for \nPecha, A Language Practice Peer 12\ntransmitting and negotiating information remains key. Just how LLMs fit into fulfilling this ultimate \npurpose must be explored and evaluated. Following the conclusion of the prototyping phase of this \nproject, the authors plan to conduct research evaluating the following questions: \n• To what degree can an LLM fulfill the role of communicative facilitator within the \ninformal learning space at MIT and the formal learning space at KUIS? \n• Will the support provided by the LLM assist students in improving their communica -\ntive competence and acquisition of language structures? \nMultiple roles of LLMs in language learning and teaching need to be explored in the field of language \neducation so that their roles can be defined based on empirical evidence. For this to occur, implementa-\ntions of LLMs must be created and applied in real situations with actual language learners. The authors \nhope that Pecha can provide a way to explore how LLMs can be used to augment and improve current \nparadigms of language practice. As more research is conducted into LLM use in language learning, it \nis hoped that research will unveil where this technology can really be best put to use in aiding learners \nand educators the most. \nReferences\nAntoniadis, G., Granger, S., Kraif, O., Ponton, C., & Zampa, V . (2013). NLP and CALL: integration is \nworking. arXiv preprint, http://arxiv.org/abs/1302.4814\nBelda-Medina, J., & Calvo-Ferrer, J. R. (2022). Using Chatbots as AI conversational partners in \nlanguage learning. Applied Sciences, 12(17), 8427. https://doi.org/10.3390/app12178427\nBoland, J. E., Fonseca, P., Mermelstein, I., & Williamson, M. (2022). Zoom disrupts the rhythm of \nconversation. Journal of Experimental Psychology: General, 151 (6), 1272–1282. https://doi.\norg/10.1037/xge0001150\nBonner, E., Lege, R., & Frazier, E. (2023). Large language model-based artificial intelligence in the \nlanguage classroom: practical ideas for teaching. Teaching English with Technology, 23 (1). \nhttps://dx.doi.org/10.56297/BKAM1691/WIEO1749\nEbrahimi, S. (2015). Interaction hypothesis: An insufficient explanation for second language acquisition. \nInternational Journal of Basic Sciences & Applied Research , 4(6), 350–353. http://isicenter.\norg/fulltext/paper-22052015123907.pdf\nEllis, R. (1991). The interaction hypothesis: A critical evaluation. Regional Language Centre Seminar, \nSingapore.\nGallagher‐Brett, A. (2004). Seven hundred reasons for studying languages. University of Southampton. \nhttps://eprints.soton.ac.uk/362669/1/700_reasons.pdf\nGao, Y ., Nuchged, B., Li, Y ., & Peng, L. (2023). An investigation of applying large language models \nto spoken language learning. Applied Sciences , 14(1), 224–239.  https://doi.org/10.3390/\napp14010224\nHirai, A., & Kovalyova, A. (2024). Speech-to-text applications’ accuracy in English language learners’ \nspeech transcription.  Language Learning & Technology, 28 (1), 1–21. https://hdl.handle.\nnet/10125/73555\nKoenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., ... & Goel, S. (2020). Racial \ndisparities in automated speech recognition. Proceedings of the national academy of sciences, \n117(14), 7684–7689. https://doi.org/10.1073/pnas.1915768117\nKoester, A. (2022). Why face-to-face communication matters: A comparison of face-to-face and \ncomputer-mediated communication. In COVID-19, Communication and Culture  (pp. 115–\n134). Routledge.\n13 Technology in Language Teaching & Learning, 6(3)\nKooli, C. (2023). Chatbots in education and research: a critical examination of ethical implications and \nsolutions. Sustainability, 15(7), 5614. https://doi.org/10.3390/su15075614\nKrashen, S. (1982). Principles and practices in second language acquisition. Pergamon.\nKučerová, K. (2023). Benefits and challenges of conducting a Collaborative Online International \nLearning Class (COIL). International Journal on Studies in Education (IJonSE), 5 (2), 192–\n212. http://dx.doi.org/10.46328/ijonse.110\nKwon, T. (2023). Interfaces for personalized language learning with generative language models \n[Masters Thesis, Columbia University]. http://doi.org/10.7916/26qy-3n10\nLiu, Z., Yin, S. X., Lee, C., & Chen, N. F. (2024). Scaffolding language learning via multi-\nmodal tutoring systems with pedagogical instructions. arXiv preprints . http://arxiv.org/\nabs/2404.03429\nLoewen, S., & Sato, M. (2018). Interaction and instructed second language acquisition. Language \nTeaching, 51(3), 285–329. https://doi.org/10.1017/S0261444818000125\nLong, M. H. (1981). Input, interaction, and second-language acquisition. Annals of the  \nNew York Academy of Sciences, 379 (1), 259–278. https://doi.org/10.1111/j.1749-6632.1981.\ntb42014.x\nLorentzen, A., & Bonner, E. (2023). Customizable ChatGPT AI chatbots for conversation practice. The \nFLTMAG. https://www.doi.org/10.69732/JLOQ2431\nMeyer, J., Jansen, T., Schiller, R., Liebenow, L. W., Steinbach, M., Horbach, A., & Fleckenstein, \nJ. (2024). Using LLMs to bring evidence-based feedback into the classroom: AI-generated \nfeedback increases secondary students’ text revision, motivation, and positive emotions. \nComputers and Education: Artificial Intelligence , 6, 100199.  https://doi.org/10.1016/j.\ncaeai.2023.100199\nMinei, E., Razuvaeva, T., & Dyshko, D. (2021). Modern day digital pen pals: A semester-long \nCollaborative Online International Learning (COIL) project. Communication Teacher, 35(4), \n336–344. https://doi.org/10.1080/17404622.2021.1887906\nMynard, J., Burke, M., Hooper, D., Kushida, B., Lyon, P., Sampson, R., & Taw, P. (2020).  Dynamics of \na social language learning community: Beliefs, membership and identity. Multilingual Matters. \nhttps://doi.org/10.21832/9781788928915\nNamaziandost, E., & Nasri, M. (2019). A meticulous look at Long’s (1981) Interaction Hypothesis: \n‎Does it have any effect on speaking skill?‎ Journal of Applied Linguistics and Language \nResearch, 6 (2), 218–230. https://www.jallr.com/index.php/JALLR/article/view/1023/\npdf1023\nO’Grady, W., & Lee, M. (2023). Natural syntax, artificial intelligence and language acquisition. \nInformation, 14(7), 418. https://doi.org/10.3390/info14070418\nPouromid, S. (2019). Towards multimodal interactions in the multilingual EFL classroom: Lessons from \na COIL experience. Indonesian Journal of Applied Linguistics, 8(3). https://doi.org/10.17509/\nijal.v8i3.15262\nResnick, M. (2024). Generative AI and creative learning: Concerns, opportunities, and choices. An \nMIT Exploration of Generative AI. https://doi.org/10.21428/e4baedd9.cf3e35e5\nTran, T.H. (2009). The interaction hypothesis: A literature review. Education Resources Information \nCenter. https://files.eric.ed.gov/fulltext/ED507194.pdf\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. \n(2017). Attention is all you need. Advances in Neural Information Processing Systems , 30. \nhttps://research.google/pubs/attention-is-all-you-need/\nWang, Y ., & Xue, L. (2024). Using AI-driven chatbots to foster Chinese EFL students’ academic \nengagement: An intervention study. Computers in Human Behavior, 108353–108353. https://\ndoi.org/10.1016/j.chb.2024.108353\nPecha, A Language Practice Peer 14\nWood, D., & Moss, S. H. (2024). Evaluating the impact of students’ generative AI use in educational \ncontexts. Journal of Research in Innovative Teaching & Learning, 17(2), 152–167. https://doi.\norg/10.1108/JRIT-06-2024-0151\nYu, H., & Guo, Y . (2023). Generative artificial intelligence empowers educational reform: Current \nstatus, issues, and prospects. Generative Artificial Intelligence Empowers Educational Reform: \nCurrent Status, Issues, and Prospects, 8. https://doi.org/10.3389/feduc.2023.1183162\nZhai, C. (2023). A systematic review on artificial intelligence dialogue systems for enhancing English \nas foreign language students’ interactional competence in the university. Computers and \nEducation: Artificial Intelligence, 4, 100134. https://doi.org/10.1016/j.caeai.2023.100134",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6023626923561096
    },
    {
      "name": "Comprehension approach",
      "score": 0.4453575611114502
    },
    {
      "name": "Language acquisition",
      "score": 0.42878293991088867
    },
    {
      "name": "Linguistics",
      "score": 0.4248400330543518
    },
    {
      "name": "Psychology",
      "score": 0.3068884015083313
    },
    {
      "name": "Natural language processing",
      "score": 0.27091288566589355
    },
    {
      "name": "Natural language",
      "score": 0.22265475988388062
    },
    {
      "name": "Mathematics education",
      "score": 0.18816834688186646
    },
    {
      "name": "Philosophy",
      "score": 0.052917271852493286
    }
  ]
}