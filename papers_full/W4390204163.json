{
  "title": "B2-ViT Net: Broad Vision Transformer Network With Broad Attention for Seizure Prediction",
  "url": "https://openalex.org/W4390204163",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5024982688",
      "name": "Shuiling Shi",
      "affiliations": [
        "Kunming University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5042460312",
      "name": "Wenqi Liu",
      "affiliations": [
        "Kunming University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017455791",
    "https://openalex.org/W2089677793",
    "https://openalex.org/W2888598199",
    "https://openalex.org/W2142181855",
    "https://openalex.org/W2806458000",
    "https://openalex.org/W4292264206",
    "https://openalex.org/W2154997432",
    "https://openalex.org/W4285165236",
    "https://openalex.org/W2799610518",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2976267777",
    "https://openalex.org/W1983364832",
    "https://openalex.org/W2962984603",
    "https://openalex.org/W2133288538",
    "https://openalex.org/W4220882290",
    "https://openalex.org/W2913846632",
    "https://openalex.org/W2970007912",
    "https://openalex.org/W2166158727",
    "https://openalex.org/W2286948618",
    "https://openalex.org/W4367663369",
    "https://openalex.org/W3135148659",
    "https://openalex.org/W2738226240",
    "https://openalex.org/W6633205339",
    "https://openalex.org/W2317674142",
    "https://openalex.org/W4360770828",
    "https://openalex.org/W4200198364",
    "https://openalex.org/W4385452890",
    "https://openalex.org/W2965277555",
    "https://openalex.org/W2169812774",
    "https://openalex.org/W2097715699",
    "https://openalex.org/W2063615912",
    "https://openalex.org/W1981211771",
    "https://openalex.org/W2152282628",
    "https://openalex.org/W2534582782",
    "https://openalex.org/W2780723646",
    "https://openalex.org/W2804824909",
    "https://openalex.org/W3192474593",
    "https://openalex.org/W4293652293",
    "https://openalex.org/W2008056655",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W4296203798",
    "https://openalex.org/W2918703174",
    "https://openalex.org/W2916882487",
    "https://openalex.org/W1556131344",
    "https://openalex.org/W4200286258"
  ],
  "abstract": "Seizure prediction are necessary for epileptic patients. The global spatial interactions among channels, and long-range temporal dependencies play a crucial role in seizure onset prediction. In addition, it is necessary to search for seizure prediction features in a vast space to learn new generalized feature representations. Many previous deep learning algorithms have achieved some results in automatic seizure prediction. However, most of them do not consider global spatial interactions among channels and long-range temporal dependencies together, and only learn the feature representation in the deep space. To tackle these issues, in this study, an novel bi-level programming seizure prediction model, B2-ViT Net, is proposed for learning the new generalized spatio-temporal long-range correlation features, which can characterize the global interactions among channels in spatial, and long-range dependencies in temporal required for seizure prediction. In addition, the proposed model can comprehensively learn generalized seizure prediction features in a vast space due to its strong deep and broad feature search capabilities. Sufficient experiments are conducted on two public datasets, CHB-MIT and Kaggle datasets. Compared with other existing methods, our proposed model has shown promising results in automatic seizure prediction tasks, and provides a certain degree of interpretability.",
  "full_text": "IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. XX, NO. XX, XXXX 2023 1\nB2-ViT Net: Broad Vision Transformer Network\nwith Broad Attention for Seizure Prediction\nShuiling Shi˙ID , and Wenqi Liu˙ID\nAbstract— Seizure prediction are necessary for epileptic\npatients. The global spatial interactions among channels,\nand long-range temporal dependencies play a crucial role\nin seizure onset prediction. In addition, it is necessary to\nsearch for seizure prediction features in a vast space to\nlearn new generalized feature representations. Many previ-\nous deep learning algorithms have achieved some results\nin automatic seizure prediction. However, most of them do\nnot consider global spatial interactions among channels\nand long-range temporal dependencies together, and only\nlearn the feature representation in the deep space. To\ntackle these issues, in this study, an novel bi-level program-\nming seizure prediction model, B2-ViT Net, is proposed for\nlearning the new generalized spatio-temporal long-range\ncorrelation features, which can characterize the global in-\nteractions among channels in spatial, and long-range de-\npendencies in temporal required for seizure prediction. In\naddition, the proposed model can comprehensively learn\ngeneralized seizure prediction features in a vast space\ndue to its strong deep and broad feature search capabil-\nities. Sufﬁcient experiments are conducted on two public\ndatasets, CHB-MIT and Kaggle datasets. Compared with\nother existing methods, our proposed model has shown\npromising results in automatic seizure prediction tasks,\nand provides a certain degree of interpretability.\nIndex Terms— Automatic seizure prediction, electroen-\ncephalogram (EEG), vision transformer (ViT), multi-head\nself-attention, broad attention, broad learning system\n(BLS).\nI. INTRODUCTION\nEPILEPSY is a chronic non-infectious disease caused\nby paroxysmal abnormal super-synchronous discharge\nactivity of brain neurons. It is one of the most common\nneurological diseases worldwide and covers all age groups,\naround 50 million epileptic patients worldwide [1]. Epilepsy\nis associated with adverse outcomes, including serious comor-\nbidities, injury and death [2]. The central problem of epilepsy\nis the unpredictability of seizures, which can have a persistent\nnegative impact on patients’ life.\nIf seizures can be predicted a few minutes before onset,\npatients will be able to take precautions against injury and\nopen the door to new and timely treatment for the prevention or\nS. Shi is with the Data Science Research Center, Kunming University\nof Science and Technology, Kunming 650500, China, and also with\nthe Faculty of Science, Kunming University of Science and Technology,\nKunming 650500, China (e-mail: shishuiling0409@sina.com).\nW. Liu is with the Data Science Research Center, Kunming University\nof Science and Technology, Kunming 650500, China, and also with\nthe Faculty of Science, Kunming University of Science and Technology,\nKunming 650500, China (e-mail: liuwenq2215@sina.com)\nManuscript received; revised. Corresponding author: Wenqi Liu.\ncontrol of impending seizures [3]. In addition, doctors usually\nprovide treatment plans for patients with epilepsy based on\nthe type and number of seizure onset recorded by patients.\nBut, epilepsy data recorded by patients and their caregivers\nare often unreliable. It takes a lot of time and energy for\ndoctors to detect seizures from long-term electroencephalo-\ngram (EEG) records. To make effective treatment plans, it\nis necessary to use seizure prediction algorithms to identify\nseizure events automatically. Therefore, an automatic seizure\nprediction algorithm is vitally important for patients with\nepilepsy. EEG is generated by synchronous activity of a large\nnumber of neurons in the brain, which is consistent with the\nsuper-synchronous discharge mechanism of epilepsy, so EEG\nis an indispensable source of data for predicting seizures.\nThese seizure prediction algorithms usually have two main\nfunctions: (1) They can be integrated into wearable technology\nand combined with an online alarm system to start therapeutic\ninterventions [4], [5]. (2) It can assist medical workers in\nreviewing ofﬂine long-term EEG records to detect seizures\nautomatically [6].\nA complete seizure often includes interictal, preictal, ictal\nand postictal [7], [8]. The seizure prediction tasks can be\nsimpliﬁed as a classiﬁcation of interictal and preictal. When\na certain amount of preictal data is predicted, it can provide\nearly warning for the impending seizure onset.\nIn recent years, deep learning algorithms have attracted\nextensive attention in various ﬁelds because of their great\ngeneralization ability and more automatic feature extraction\nability, encouraging their application in the ﬁeld of seizure\nprediction. Truonget al.[9] used short-time fourier transform\n(STFT) to extract EEG features from the original EEG signals\nand used convolutional neural network (CNN) [10] to classify\nthe interictal and preictal. Ozcanet al. [11] extracted spectral\nband power, statistical moment and hjorth parameters to reveal\nthe frequency and time domain features of the EEG signals.\nThe features are given as input to a 3D CNN [12]. Daoud\net al. [13] used deep convolutional neural network (DCNN)\nand concatenated with a bidirectional long short-term memory\n(Bi-LSTM) network as the back-end of model to classify.\nMany studies have shown that seizures involve not only the\nseizure onset zone and its surroundings, but also the brain\nareas far away from seizure onset zone [3], [14]. Abnormal\ninteractions among different brain areas may lead to seizure\nonset. To characterize interactions among different brain areas\nwithin a whole-brain range, recent studies generally construct\nbrain functional connectivity networks based on scalp EEG\nusing channels as nodes [15], [16], [17]. According to the\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. XX, NO. XX, XXXX 2023\ninternational standard electrode positions, in multi-channel\nEEG data, different channels correspond to different brain\nregions, so abnormal interactions among different brain re-\ngions can be reﬂected by the interactions among different\nchannels. Furthermore, seizures do not occur randomly and\nhave been shown to have long-range temporal dependencies\n[3], [18], [19]. In summary, the global channel interactions in\nspatial, and long-range temporal dependencies are crucial to\nseizure prediction algorithms. However, most of the previous\ntraditional deep learning algorithms, such as CNN, they can\nonly capture local channel interactions in spatial and short-\nrange temporal dependencies due to the regular and local\nreceptive ﬁeld of convolution operators, without considering\nglobal channel interactions and long-range temporal depen-\ndencies together, resulting in the lack of interpretability of the\nmodel and the common results.\nIn fact, vision transformer (ViT) [20] algorithm based on\nglobal attention mechanism can achieve the global channel\ninteraction in spatial, and obtain long-range temporal depen-\ndence features required for seizure prediction. But ViT only\nconsiders the deep features of the last transformer modules,\ntransformer modules with different depths may contain com-\nplementary features related to seizure prediction tasks [21].\nThe complementary features can be obtained through the broad\nconnection of shallow and deep transformer modules. But\nthese complementary features are redundant and complicated.\nBy applying attention mechanisms to these complementary\nfeatures, we can further extract critical spatio-temporal long-\nrange correlation complementary features that are beneﬁcial\nto seizure prediction. However, the broad connection above\nis only used for the attention mechanism part to extract\nconnected attention information from different transformer\nmodules, instead of mapping all features together into a new\nvast space to learn new generalized features. It is necessary to\nsearch for seizure prediction features in a vast space, so as to\nlearn new generalized spatio-temporal long-range correlation\nfeatures that help predict seizures [22].\nTherefore, according to the neuroscience mechanism of\nseizure, a novel bi-level programming seizure prediction\nmodel, broad vision transformer network with broad atten-\ntion, called B2-ViT Net, is proposed for learning the new\ngeneralized spatio-temporal long-range correlation features,\nwhich can characterize the global channel interaction features\nin spatial and long-range dependence features in temporal,\ncaptures generalized features that are beneﬁcial to seizure pre-\ndiction, thus improving the prediction performance. Compared\nwith other black box deep learning models, our model can\nquantify the interaction weights among channels, and evaluate\nthe importance of each channel at any time, thus providing a\ncertain degree of interpretability.\nSpeciﬁcally, the contributions of our proposed method can\nbe summarized in the following aspects.\n1) Based on the neuroscience mechanism of seizure\nonset, we proposed a novel bi-level programming seizure\nprediction model B2-ViT Net, which considers the global\nspatial interactions among channels and long-range tem-\nporal dependencies together through the global attention\nmechanism, called spatio-temporal long-range correla-\ntions. The global attention mechanism here can innova-\ntively quantify the interaction weights among channels,\nand evaluate the importance of each channel at any time.\n2) Both deep and broad features are crucial for seizure\nprediction tasks. Previous seizure prediction algorithms\nonly focused on deep features while ignoring the gener-\nalized features that combine deep and broad. Generalized\nfeatures are characterized through linear and nonlinear\nrandom mappings in our model. Our proposed model can\ncomprehensively learn generalized spatio-temporal long-\nrange correlation features that are conducive to automatic\nseizure prediction in a vast space, improve the prediction\nperformance.\n3) Sufﬁcient experiments are conducted on two pub-\nlic datasets, CHB-MIT and Kaggle datasets. Compared\nwith other existing methods, our proposed method has\nachieved promising results in automatic seizure prediction\ntasks, obtains the highest AUC and the lowest FPR. On\nCHB-MIT dataset, B2-ViT obtains 0.923, 93.3%, and\n0.057/h on AUC, sensitivity and FPR, respectively. On\nthe Kaggle dataset, the proposed model reached 0.816,\n85.2%, and 0.013/h on AUC, sensitivity and FPR, re-\nspectively.\nII. PRELIMINARY KNOWLEDGE\nThis section introduces the preliminary knowledge of ViT\nand BLS, which helps to build B2-ViT Net.\nA. ViT: Vision Transformer\nTransformer is a deep neural network mainly based on\nself-attention mechanism, which is initially applied in natural\nlanguage processing. Inspired by its powerful global presenta-\ntion ability, researchers extend transformer to computer vision\ntasks, which is called ViT [20]. Compared with other networks\n(such as CNN), the model shows competitive performance on\nvarious benchmarks. The model follows the following steps:\n(1) Converting image data to sequences form as transformer\ninput; (2) applying linear projection to the sequences; (3)\nadding extra learnable classiﬁcation token, adding positional\nembedding;(4) a transformer encoder is applied to the pro-\ncessed data, which mainly includes multi-head self-attention\nmechanism (MHSA) block and multi-layer perceptron (MLP)\nblock.\nB. BLS: Broad Learning System\nBLS [23] has a strong ability to search broad features. It\nconsists mainly of feature nodes and enhancement nodes. The\nfeature nodes are obtained by a random mapping, and then\nthe feature nodes are mapped to a possible high-dimensional\nvector space to obtain enhancement nodes, so that the model\ncan automatically search features related to speciﬁc tasks in a\nvast vector space. Both two features yield the ﬁnal output.\nIII. D ATASETS AND METHODOLOGY\nThis section thoroughly introduces the datasets, data prepro-\ncessing, the modeling method of B2-ViT Net, and postprocess-\ning. In addition, the model frame diagram and algorithm table\nare also provided. The structure of B2-ViT is shown in Fig.2,\nthe detailed implementation steps of B2-ViT are summarized\nin Algorithm 1.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHI et al.: B2-VIT NET: BROAD VISION TRANSFORMER NETWORK WITH BROAD ATTENTION FOR SEIZURE PREDICTION 3\n,QWHULFWDO 3UHLFWDO ,FWDO63+ ,QWHULFWDO623\n6,+ 6,+Exclude Å 4h\f Exclude Å 4hç\n2QVHW\n$ODUP\nFig. 1: Deﬁnition of interictal, preictal, SIH, SPH, SOP and seizure period (from the ﬁlechb01 03.edf).\nTABLE I: SUMMARY OF CHB-MIT DATASET\nPatient\nID AgeGender No. of\nseizures\nInterical\nhours\nPreical\nhours\nPatient\nID AgeGender No. of\nseizures\nInterical\nhours\nPreical\nhours\nPat 1 11 F 7 17 3.5 Pat 14 9 F 5 5 2.5\nPat 2 11 M 3 23 1.5 Pat 18 18 F 6 24 3.0\nPat 3 14 F 6 22 3.0 Pat 19 19 F 3 25 1.5\nPat 5 7 F 5 14 2.5 Pat 20 6 F 5 20 2.5\nPat 9 10 F 4 46.3 2.0 Pat 21 13 F 4 22 2.0\nPat 10 3 M 6 26 3.0 Pat 23 6 F 5 12.9 2.5\nPat 13 3 F 5 14 2.5 Total - - 64 271.2 32\nTABLE II: SUMMARY OF KAGGLE DATASET\nPatient\nID AgeGender No. of\nseizures\nInterical\nhours\nPreical\nhours\nPatient\nID AgeGender No. of\nseizures\nInterical\nhours\nPreical\nhours\nDog 1 - - 4 80.0 2.0 Dog 4 - - 14 134 8.5\nDog 2 - - 7 83.3 3.5 Dog 5 - - 5 75 2.5\nDog 3 - - 12 240 6.0 Total - - 42 612.3 22.5\nA. Datasets\n1) CHB-MIT dataset: The CHB-MIT seizure EEG dataset\n[24] is obtained from Boston Children’s Hospital and included\nin the EEG database of the Massachusetts Institute of Technol-\nogy. It contains 23 records from 22 subjects (chb21 is recorded\nagain of chb01 subjects after 1.5 years). Each subject has\n9-24 recordings lasting for 1 hour (some of which are long\nrecords of 2-4 hours), and the dataset includes 884 hours of\ncontinuous scalp EEG recordings and 163 seizures. All EEG\ndata are collected using 10-20 international standard electrode\npositions, EEG is recorded using 18/23 lead, and the sampling\nfrequency is 256 Hz.\n2) Kaggle dataset: The American Epilepsy Society Seizure\nPrediction Challenge of Kaggle dataset [25] has iEEG data\nfrom 5 dogs and 2 patients, with 48 seizures and 627.7 hours\ninterictal records, which is simply denoted as Kaggle dataset.\nIntracranial EEG (iEEG) data of 5 dogs are recorded from\n16 implanted electrodes, and the sampling rate is 400 Hz.\nRecorded iEEG data of 2 patients from 15 deep electrodes\n(Patient 1) and 24 subdural electrodes (Patient 2), and the\nsampling rate is 5 kHz. The calculation is difﬁcult due to the\npatients’ high sampling rate of the Kaggle dataset, so the two\npatients’ iEEG data are not considered, which is consistent\nwith [9], [26], [27]. These two datasets are used in most\nseizure prediction tasks [8], [9], [26], [28].\nB. Preprocessing\nAs shown in Fig.1, a complete seizure can be divided into\npreictal, interictal, seizure interictal horizon (SIH), seizure pre-\ndiction horizon (SPH), and seizure occurrence period (SOP).\nSPH is the prediction period before the seizure, during which\nappropriate measures can be used to prevent or control the\nimpending seizure in advance. SOP is the interval where the\nseizure is expected to occur. SIH is deﬁned as EEG signals\nabout 4 hours before and 4 hours after the seizure [29], which\ncan reduce the interference caused by the near seizure state. To\npredict correctly, seizures must be after SPH and within SOP.\nThis paper follows the deﬁnitions of SOP and SPH proposed\nby [30]. In this work, SPH is set to 5 minutes and SOP is set\nto 30 minutes, which is consistent with most studies. CHB-\nMIT dataset has many seizures in a short time. For seizures\nless than 30 minutes from the previous seizure, we assume\nthat there are only the leading seizure exists. In addition, this\nwork only considers patients with seizures less than 10 times\na day, because it is not very necessary to perform this task for\npatients who have seizures every 2 hours on average. Based\non the above deﬁnition and consideration, this work evaluated\n64 seizures in the CHB-MIT dataset and 42 seizures of 5 dogs\nin the Kaggle dataset. These two datasets’ available data are\nsummarized in TableI and TableII.\nClassiﬁcation tasks often face the problem of class im-\nbalance, automatic seizure prediction tasks are no exception,\ninterictal data is far more than preictal data. To solve this\nproblem, the sliding overlap technique with step sizes is used\nto obtain more preictal data. the number of extra preictal data\nN after oversampling is computed as:\nN = (P \u0000 w)\ns +1 (1)\nwhere w is the sliding window length,P is the total length of\npreictal data, I is the total length of interictal data,R is the\nratio of the total length of preictal data to the total length of\ninterictal data ands = w ⇥ R.\nIn this paper, STFT [31] is used to preprocess the raw\nEEG data to extract time-frequency features, which converts\nthe original EEG signal into a time-frequency matrix. The\nwindow length of STFT is 30s. STFT is chosen because it\ncan capture the dynamic changes of the frequency charac-\nteristics of EEG signals of epileptic patients, and compared\nwith wavelet transform (WT) [32] and other signal analysis\nmethods, it has a shorter processing time of time series, which\nis helpful for real-time seizure prediction. Besides, it is widely\nused in EEG processing, retains most of the information in the\noriginal signal, and many studies have shown its advantages\nin EEG [9], [33]. The datasets used are contaminated with 60\nHz power line noise, so components in the 57-63 Hz and 117-\n123 Hz frequency ranges are excluded to eliminate power line\ninterference, and the DC component (0 Hz) is also removed.\nC. Proposed Method\nB2-ViT Net is a novel bi-level programming problem for\nseizure prediction. It considers the spatio-temporal long-range\ncorrelation features required for seizure prediction. In addition,\nit has strong global deep and broad feature search capabilities,\nwhich can comprehensively learn generalized spatio-temporal\nlong-range correlation features that are conducive to automatic\nseizure prediction in a vast space, thus improving the predic-\ntion performance.\nFor a given preprocessed imageI 2 RL⇥C1⇥W , L is the\nlength of sequence, C1 and W are the number of channels\nand width of image patches, which can be processed directly\nby the standard transformer. To get the inputx1 2 RL⇥C⇥D\nof the ﬁrst transformer layer, linear projection is adopted for\nsatisfying the required dimension D of transformer, C1 is\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. XX, NO. XX, XXXX 2023\n!\"#$%&!'\n!!\n\"!\n#!\n!\"#$()!%$!\n%&'()*+,\"-\"#\n. /\"0$\n()!\n123456\"7%89%7&89&\n)!\n&'(*!+,&)*+,-.)/()01*2()3!\n!\n!\n!\"#$%*-. / 0 1 2\"##$-3%\n! :\n;\n4).*50677(+78.+3&\"\n'\"\n(\"\n&#\n'#\n(#\n$%\n$'\n<\n$++4%5/ 6/ 7'\n%&'()*+<:#\n. ;\n4#..1 89+)*+,-\n:;4.( <=//>/012/?>3/@3 /A=//>/012/?B3/@3/B4/@4 >5CD5\n5EF5 >5 55\nGH 2H>/01-I J K: ; 4.)*+L67/\n8\n. D6/M=/06/>/01 EF/ >/015\n5\n>/01->/012//N 8 O + : I &\n=%! !!=,!\n!\"#>?\n@ABC-.D/! 8E/!9:; 0< 0=\n!F% !! F0!\n!*99(50-(*7:)(0+.5(,0P>\n;+'*+<(/(+70+.5(,0Q?\nG H1D2\" 8E2\"\n@:; 0< 0A\n!!\n$\n!\"#$%&'(()*(+#*\nR9+BCCD\nR9+)*+,-\n!\"#$\"\n!\n&)*+,-.)/()\"*2()0=\n&)*+,-.)/()\"*2()0>\n\"8+(*)#).?(<78.+\n&)*+,-.)/()\"*2()0@!\n&)*+,-.)/()\"*2()0!\n!\"#$%&'()*$+,-./$\n%I\n%JKLMJ\n!0#$1.0234.$.524062),7\n*&&!\"##$%&\n'()(\n! \"\n#S\nFig. 2: The architecture of B2-ViT Net. (a) The feature pre-extraction part using STFT. (b) The architecture of bi-level programming B2-ViT model.\nadditionally added with classiﬁcation token, which is recorded\nas C. After processing the input data, the model is ﬁrst divided\ninto two parts, one is ViT backbone to obtain deep features\nOutDeep, and the other is broad attention to obtain local broad\nfeatures OutBroad. The transformer layer includes two blocks:\nMHSA and MLP. In addition, residual connections are used\nin MHSA and MLP blocks, and LayerNorm (LN) is applied\nbefore each block. Next, the calculation process of MHSA,\nMLP and broad attention is introduced in detail.\nMulti-Head Self-Attention: Given the inputxi 2 RL⇥C⇥D\nof i-th layer. Then query qi 2 RL⇥C⇥(h⇥dq),k e yki 2\nRL⇥C⇥(h⇥dk) and value vi 2 RL⇥C⇥(h⇥dv) are obtained by\nchunking xi into three tensors and rearranging them,h is the\nnumber of head, wheredq, dk and dv are the dimension of\nqi, ki and vi, respectively,i 2 [1,l ], where l is the number of\ntransform layers. Then inner product, softmax and the second\nlinear projection are performed. The output of MHSA can be\nobtained by the following:\nMHSA(xi) = softmax(qi,k i,v i)wo\n= softmax(qikT\nip\ndq\n)viwo (2)\nwhere\nqi =[ [q1\ni ], [q2\ni ],. . . ,[qh\ni ]],q j\ni 2 RL⇥C⇥dq\nki =[ [k1\ni ], [k2\ni ],. . . ,[kh\ni ]],k j\ni 2 RL⇥C⇥dk\nvi =[ [v1\ni ], [v2\ni ],. . . ,[vh\ni ]],v j\ni 2 RL⇥C⇥dv\nwhere qj\ni , kj\ni , vj\ni are the corresponding value of j-th head\nin i-th layer of qi, ki, vi, wo is the weight matrix of the\nsecond linear projection. Because of the residual connection\nbetween the layers, the hidden layer’s outputˆyi in i-th layer\nis formulated by\nˆyi = xi + MHSA(xi) (3)\nMulti Layer Perceptron: MLP has two fully connected\nlayers and an activation function layer, the activation function\nused in this paper is GELU. The output of MLP can be denoted\nas\nMLP(ˆyi) = GELU(ˆyiw1l + b1l)w2l + b2l (4)\nwhere w1l, b1l, w2l, and b2l are the weights and bias of the\ncorresponding linear layers. The output yi in i-th layer is\nformulated as\nyi =ˆyi + MLP(ˆyi) (5)\nThe outputyi of i-th layer is the inputxi+1 of (i+1)-th layer,\nso the deep featureOutDeep is the output of last layer:\nOutDeep = yl (6)\nBroad Attention:Queries, keys and values of different layers\nare concatenated respectively as below:\nQ =[ q1,q 2,. . . ,ql],Q 2 RL⇥h⇥C⇥(l⇥dq)\nK =[ k1,k 2,. . . ,kl],K 2 RL⇥h⇥C⇥(l⇥dk)\nV =[ v1,v 2,. . . ,vl],V 2 RL⇥h⇥C⇥(l⇥dv)\nSelf-attention is performed on the concatenated query Q,\nkey K and value V to get Attention(Q, K, V). In this\npaper, 1D adaptive average pooling is introduced to solve the\nproblem of dimension inconsistency between OutDeep and\nAttention(Q, K, V). The output featuresOutBroad of broad\nattention can be denoted as:\nMHSA(xi) = AdaptivePool(Attention(Q, K, V))\n= AdaptivePool(softmax(QKT\np\nd\n)V ) (7)\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHI et al.: B2-VIT NET: BROAD VISION TRANSFORMER NETWORK WITH BROAD ATTENTION FOR SEIZURE PREDICTION 5\nAlgorithm 1 Implementation Process of the B2-ViT Algorithm\nRequire: Input seizure EEG data X and labelYT;\nEnsure: Prediction matrix YP for for seizure detection;\nParameters: Wp1, bp1, Wp2, bp2: Linear projection parameters;\nWc, Wp: the class token and positional embedding matrices;\nWi: multi-head attention parameters for layeri; \u00001\ni , \u00001\ni ,\u00002\ni , \u00002\ni :\ntwo sets of layer-norm parameters for layer i; d: the dim of\none head; Wi\n1l, bi\n1l, Wi\n2l, bi\n2l: MLP parameters for layeri; l:\nthe depth of transformer block; \u0000: the coefﬁcient factor; the\nregularization coefﬁcient of BLS\u00001.\n1: Fed X into the STFT to obtain the initial featuresXs;\n2: Xp  Wp2 GELU(Wp1Xs+bp11T )+bp21T\n3: x1  cat(Wc, Xp)+ Wp\n4: for i =1 , 2,. . . ,ldo\n5: MHSA(xi), qi, ki, vi  MHSAttention(xi|Wi)\n6: ˆyi  xi + MHSA(xi)\n7: for t 2 [i]: ˆyi[:, :,t ]  layer norm(ˆyi[:, :,t ]|\u00001\ni , \u00001\ni )\n8: yi  ˆyi + Wi\n2lGELU(Wi\n1lxi + bi\n1l1T )+ bi\n2l1T\n9: for t 2 [i]: yi[:, :,t ]  layer norm(ˆyi[:, :,t ]|\u00002\ni , \u00002\ni )\n10: xi+1 = yi\n11: end for\n12: OutDeep  yl\n13: Q  q1 [ q2 [ ... [ ql\n14: K  k1 [ k2 [ ... [ kl\n15: V  v1 [ v2 [ ... [ vl\n16: Attend(Q, K, V)  Softmax(QKT /\np\nd)V\n17: Re  Rearrange(Attention(Q, K, V) |bhnd ! bn(hd))\n18: OutBroad  AdaptivePool(Re)\n19: OutDB = OutDeep + \u0000 ⇥ OutBroad\n20: for i =1 , 2,. . . ,ndo\n21: Random Wzi , \u0000zi ;\n22: Caculate Zi = \u0000i(OutDBWzi + \u0000zi )\n23: end for\n24: Stack all the mapping feature nodes as\nZn =[ Z1, Z2,. . . ,Zn]\n25: for j =1 , 2,. . . ,mdo\n26: Random Whj , \u0000hj ;\n27: Caculate Hj = ⇠j(ZnWhj + \u0000hj )\n28: end for\n29: Stack all the enhancement nodes as\nHm =[ H1, H2,. . . ,Hm]\n30: Calculate the weight connected the hidden layer and output layer\nw2 by: w2 =( AT A + \u00001I)\u00001AT YT\n31: Get the prediction matrix for seizure detection\nYP =[ Zn|Hm]w2\nwhere d is the hidden dimension of transformer layer.\nCombining the deep featureOutDeep and local broad fea-\nture OutBroad, the ﬁnal output feature OutDB of BViT is\ncomputed as:\nOutDB = OutDeep + \u0000 ⇥ OutBroad (8)\nwhere \u0000 can be used to adjust the weights of two types of\nfeatures. Finally, the probability of categories is calculated\nby a softmax function. So far, we have obtained the spatio-\ntemporal long-range correlation features required for seizure\nprediction. The feature data and its labels are denoted as\u0000\n(OutDB,Y T )|OutDB 2 RL⇥(C⇥D),Y T 2 RL⇥M  \nfrom M\nclasses.\nIt is necessary to search for seizure prediction features in a\nvast space to learn new generalized spatio-temporal long-range\ncorrelation features that help predict seizures. Therefore, the\nabove algorithm is extended to a vast space through BLS to\nlearn generalized features, so as to improve the performance\nand representation ability of seizure prediction tasks. Firstly,\nOutDB are randomly extended to a vast space via a linear\nrandom mapping, that is:\nZi\n\u0000\n= \u0000i(OutDBWzi + \u0000zi ),i =1 ,. . . ,n (10)\nwhere Wzi and \u0000zi are generated by a random mapping\u0000i.\nThen the set ofn groups of feature nodes can be deﬁned as\nZn \u0000\n=[ Z1,Z 2,. . . ,Zn].\nSecondly, the j-th group of enhancement nodes can be\nconstructed by\nHj\n\u0000\n= ⇠j(ZnWhj + \u0000hj ),j =1 ,. . . ,m (11)\nsimilarly, both Whj and \u0000hj are generated by the nonlinear\nrandom mapping ⇠j. The set of m groups of enhancement\nnodes can be deﬁned as Hm \u0000\n=[ H1,H 2,. . . ,Hm], ⇠j is the\ntansig function here, tansig is a hyperbolic tangent s-type\nnonlinear function, which is deﬁned as:\ntansig(x)= 2\n1+ e\u00002x \u0000 1 (12)\nTherefore, the output YP of the improved algorithm with\nBLS can be constructed by the following formula:\nYP =[ Zn|Hm]w2\n= Aw2 (13)\nw2 can be obtained by solving the ridge regression problem:\nw2 =( \u00002I + AAT )\u00001AT YT (14)\nwhere \u00002 is the regularization coefﬁcient.\nOur proposed model B2-ViT is a novel bilevel programming\nproblem, the goal of the model is shown in Eq. (9), wherex1\nis the input data,YT is the true label,w1,r is the corresponding\nweight of the frontr layer of our proposed model,Wz and \u0000z\nare the corresponding weight and offset of the feature nodes,\nWh and \u0000h are the corresponding weight and offset of the\nenhancement nodes, \u0000 and ⇠ are random mappings used to\ngenerate feature nodes and enhancement nodes,L is the length\nof x1, f(x1; w1,r) is a BViT function of inputx1, which is\nparameterized by a weight vectorw1,r, l is the loss function\nof BViT,OutDB can be denoted asf(x1,w 1,r\u00001), \u00001kw1,rk2\n2\nis the regularization term that penalizes the complexity of\nweights, softmax is a classiﬁcation function.\nD. Postprocessing\nIn this work, thek-of-n method is used to predict seizure\nas in [9], [11], an alarm is set only when at leastk of the n\npredictions are positive, we setk to 4 andn to 5. In addition,\nto avoid the increase of False Prediction Rate (FPR) caused by\nmultiple alarms in a short time, we set the refractory period\nto 30 min, that is, the reoccurring alarm within 30 min after\nthe alarm occurs will be ignored.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. XX, NO. XX, XXXX 2023\nminw2\nk[\u0000(x1,w 1,r\u00001; Wz,\u0000 z),⇠ (x1,w 1,r\u00001; Wz,\u0000 z,W h,\u0000 h)]w2 \u0000 YT k2\n2 + \u00002kw2k2\n2\n(\nw1,r = arg minw1,r\nPL\ni=1 l(YT,i,f (x1,i,w 1,r)) +\u00001kw1,rk2\n2\nw1,r =[ w1,r\u00001, softmax]\n(9)\nTABLE III: OVERALL PERFORMANCE COMPARISON ON CHB-MIT DATASET\nPatient CNN [9] DCNN+Bi-LSTM [13] Vision Transformer [20] AdderNet [8] Multi-scale ProtoPNet [26] B2-ViT\nAUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value\nPat 1 0.919 85.7 0.240 <0.001 0.944 100.0 0.116 0.005 0.997 85.7 0.000 <0.001 1.000 100.0 0.000 <0.001 0.965 100.0 0.000 <0.001 0.999 100.0 0.000 <0.001\nPat 2 0.335 33.3 0.000 <0.001 0.709 66.7 0.254 0.376 0.745 33.3 0.000 <0.001 0.815 66.7 0.000 <0.001 0.771 100.0 0.000 <0.001 0.856 100.0 0.043 <0.001\nPat 3 0.968 100.0 0.180 <0.001 0.738 66.7 0.277 0.042 0.975 66.7 0.091 <0.001 0.922 83.3 0.040 <0.001 0.890 100.0 0.000 <0.001 0.979 100.0 0.000 <0.001\nPat 5 0.871 80.0 0.190 0.010 0.846 80.0 0.157 0.013 0.885 60.0 0.000 <0.001 0.981 100.0 0.000 <0.001 0.728 100.0 0.101 <0.001 0.939 80.0 0.000 <0.001\nPat 9 0.742 50.0 0.120 0.067 0.717 75.0 0.424 0.234 0.753 50.0 0.043 0.003 0.596 25.0 0.043 0.082 0.636 50.0 0.000 <0.001 0.807 50.0 0.022 <0.001\nPat 10 0.556 33.3 0.000 0.025 0.866 66.7 0.478 0.023 0.692 66.7 0.115 <0.001 0.916 85.7 0.000 <0.001 0.765 100.0 0.294 <0.001 0.817 83.3 0.038 <0.001\nPat 13 0.968 80.0 0.140 <0.001 0.786 85.7 0.219 0.001 0.920 100.0 0.143 <0.001 0.996 100.0 0.143 <0.001 0.899 100.0 0.044 <0.001 0.969 100.0 0.143 <0.001\nPat 14 0.662 80.0 0.400 0.004 0.710 100.0 0.313 0.031 0.640 80.0 0.400 0.005 0.764 71.4 0.400 0.003 0.705 87.5 0.159 <0.001 0.731 100.0 0.400 <0.001\nPat 18 0.935 100.0 0.280 <0.001 0.955 50.0 0.208 0.036 0.825 100.0 0.083 <0.001 1.000 100.0 0.000 <0.001 0.811 75.0 0.000 <0.001 0.961 100.0 0.042 <0.001\nPat 19 0.999 100.0 0.000 <0.001 0.990 0.0 0.038 0.302 0.989 100.0 0.040 <0.001 0.993 100.0 0.040 <0.001 0.990 100.0 0.000 0.001 0.997 100.0 0.000 <0.001\nPat 20 0.984 100.0 0.250 <0.001 0.960 100.0 0.184 <0.001 0.982 100.0 0.100 <0.001 0.979 83.3 0.050 <0.001 0.996 100.0 0.000 <0.001 0.997 100.0 0.050 <0.001\nPat 21 0.903 100.0 0.230 <0.001 0.648 100.0 0.521 0.101 0.864 100.0 0.091 <0.001 0.969 100.0 0.125 <0.001 0.867 100.0 0.000 <0.001 0.947 100.0 0.000 <0.001\nPat 23 0.999 100.0 0.330 <0.001 0.839 85.7 0.095 <0.001 0.930 100.0 0.000 <0.001 0.992 100.0 0.078 <0.001 0.932 100.0 0.172 <0.001 0.998 100.0 0.000 <0.001\nAverage0.834 80.18 0.182 9/13 0.824 75.12 0.253 2/13 0.861 80.2 0.085 11/13 0.917 85.8 0.071 11/13 0.843 93.27 0.059 12/13 0.923 93.33 0.057 13/13\nTABLE IV: OVERALL PERFORMANCE COMPARISON ON KAGGLE DATASET\nParticipant CNN [9] DCNN+Bi-LSTM [13] Vision Transformer [20] AdderNet [8] Multi-scale ProtoPNet [26] B2-ViT\nAUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value AUC Sn(%) FPR/hp-value\nDog 1 0.498 50.0 0.19 0.053 0.566 50.0 0.352 0.59 0.577 25.0 0.000 <0.001 0.603 75.0 0.161 0.010 0.568 50.0 0.164 <0.001 0.608 50.0 0.000 <0.001\nDog 2 0.941 100.0 0.04 <0.001 0.953 100.0 0.048 <0.001 0.838 57.1 0.012 <0.001 0.917 85.7 0.024 <0.001 0.870 100.0 0.048 <0.001 0.898 100.0 0.024 <0.001\nDog 3 0.824 58.3 0.14 <0.001 0.831 91.7 0.189 <0.001 0.800 41.7 0.013 <0.001 0.830 75.0 0.050 <0.001 0.811 100.0 0.180 <0.001 0.887 83.3 0.004 <0.001\nDog 4 0.775 78.6 0.48 <0.001 0.754 85.7 0.191 <0.001 0.693 50.0 0.052 <0.001 0.726 92.9 0.313 <0.001 0.700 92.9 0.244 <0.001 0.773 92.9 0.037 <0.001\nDog 5 0.922 80.0 0.08 <0.001 0.928 80.0 0.134 <0.001 0.885 60.0 0.013 <0.001 0.892 80.0 0.040 <0.001 0.872 100.0 0.094 <0.001 0.913 100.0 0.000 <0.001\nAverage 0.792 73.38 0.19 4/5 0.806 81.5 0.183 4/5 0.759 47.8 0.02 5/5 0.794 81.7 0.118 4/5 0.764 88.6 0.146 5/5 0.816 85.2 0.013 5/5\nTABLE V: PERFORMANCE COMPARISON OF VIT, B-VIT AND B2-V IT ON CHB-MIT DATASETS\nPatient ViT BViT B2-ViT\nAUC Sn(%) FPR/h p-value AUC Sn(%) FPR/h p-value AUC Sn(%) FPR/h p-value\nPat 1 0.997 85.7 0.000 <0.001 0.998 100.0 0.059 <0.001 0.999 100.0 0.000 <0.001\nPat 2 0.745 33.3 0.000 <0.001 0.763 66.7 0.000 <0.001 0.856 100.0 0.043 <0.001\nPat 3 0.975 66.7 0.091 <0.001 0.977 83.3 0.182 <0.001 0.979 100.0 0.000 <0.001\nPat 5 0.885 60.0 0.000 <0.001 0.934 60.0 0.000 <0.001 0.939 80.0 0.000 <0.001\nPat 9 0.753 50.0 0.043 0.003 0.754 50.0 0.022 <0.001 0.807 50.0 0.022 <0.001\nPat 10 0.692 66.7 0.115 <0.001 0.534 66.7 0.115 <0.001 0.817 83.3 0.038 <0.001\nPat 13 0.920 100.0 0.143 <0.001 0.949 100.0 0.143 <0.001 0.969 100.0 0.143 <0.001\nPat 14 0.640 80.0 0.400 0.005 0.662 80.0 0.400 0.005 0.731 100.0 0.400 <0.001\nPat 18 0.825 100.0 0.083 <0.001 0.897 100.0 0.042 <0.001 0.961 100.0 0.042 <0.001\nPat 19 0.989 100.0 0.040 <0.001 0.991 100.0 0.000 <0.001 0.997 100.0 0.000 <0.001\nPat 20 0.982 100.0 0.100 <0.001 0.987 100.0 0.050 <0.001 0.997 100.0 0.050 <0.001\nPat 21 0.864 100.0 0.091 <0.001 0.875 100.0 0.046 <0.001 0.947 100.0 0.000 <0.001\nPat 23 0.930 100.0 0.000 <0.001 0.998 100.0 0.000 <0.001 0.998 100.0 0.000 <0.001\nAverage 0.861 80.2 0.085 11/13 0.871 85.1 0.081 12/13 0.923 93.3 0.057 13/13\nTABLE VI: PERFORMANCE COMPARISON OF VIT, B-VIT AND B2-V IT ON KAGGLE DATASETS\nParticipant ViT BViT B2-ViT\nAUC Sn(%) FPR/h p-value AUC Sn(%) FPR/h p-value AUC Sn(%) FPR/h p-value\nDog 1 0.577 25.0 0.000 <0.001 0.584 25.0 0.000 <0.001 0.608 50.0 0.000 <0.001\nDog 2 0.838 57.1 0.012 <0.001 0.865 71.4 0.012 <0.001 0.898 100.0 0.024 <0.001\nDog 3 0.800 41.7 0.013 <0.001 0.836 58.3 0.013 <0.001 0.887 83.3 0.004 <0.001\nDog 4 0.693 50.0 0.052 <0.001 0.716 78.6 0.045 <0.001 0.773 92.9 0.037 <0.001\nDog 5 0.885 60.0 0.013 <0.001 0.886 80.0 0.000 <0.001 0.913 100.0 0.000 <0.001\nAverage 0.759 47.8 0.02 5/5 0.777 62.7 0.014 5/5 0.816 85.2 0.013 5/5\nTABLE VII: COMPUTATIONAL COMPLEXITY EVA L UAT I O NMETRICS OF VIT, B-VIT AND B2-V IT ON CHB-MIT AND KAGGLE DATASETS\nModel CHB-MIT Kaggle\nParams (M) Training time (s) Params (M) Training time (s)\nViT 19.08 1798.64 19.07 3717.13\nBViT 19.08 1860.67 19.07 3825.45\nB2-ViT 19.08 1862.61 19.07 3826.62\nIV. EXPERIMENTAL RESULTS\nA. Experimental Settings and Evaluation Metrics\nIn this work, Area Under Curve (AUC), Sensitivity (Sn),\nFPR, p-value are choosen as the evaluation metrics of the\nproposed method. AUC is a performance metric to measure the\nquality of the classiﬁer. The closer to 1, the better the effect.\nSn is the ratio of correctly predicted seizures to all seizures.\nFPR is the number of mispredictions per hour.p-value is the\nprobability of predicting at leastm of M seizures, which can\nbe obtained by the following formula [9], [11]:\np =\nX\ni>m\n✓ i\nM\n◆\nPi(1 \u0000 P)M\u0000i (15)\nwhere P ⇡ 1 \u0000 e\u0000FPR⇥SOP, SOP is the seizure occurrence\nperiod, is set to 30 min. Ifp< 0.001, it can be considered\nthat our model is superior to random prediction at the 0.001\nsigniﬁcance level.\nTo make the results more reliable, the leave-one-out cross-\nvalidation (LOOCV) method is used for each subject. If the\nsubject has M seizures, M \u0000 1 seizures will be used for\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHI et al.: B2-VIT NET: BROAD VISION TRANSFORMER NETWORK WITH BROAD ATTENTION FOR SEIZURE PREDICTION 7\nTABLE VIII: EXPERIMENTAL SETUP AND PERFORMANCE RESULTS OF EXISTING METHODS ON CHB-MIT AND KAGGLE DATASETS\nYears Methods Datasets No. of patients-\nseizures\nValidationscheme\nIntericataldistance-Preictallength (min)\nNo. of patients\nover chance\nAverage AUC-Sn(%)\n-FDR/h\n2013 Zero-croassing intervals+\nBayesian Gaussian mixture [34] CHB-MIT 3-18 no CV 40-40 3/3 NR-83.8-0.165\n2017 EMD, PLV + SVM [35] CHB-MIT 21-65 10-fold CV 30-5 NR NR-82.4-NR\n2018 STFT + CNN [9] CHB-MITKaggle2014 13-647-48 LOOCV 240-3010080-30 12/135/7 0.834-80.2-0.182NR-75.0-0.210\n2023 Multi-scale ProtoPNet [26] CHB-MITKaggle2014 13-715-42 LOOCV 60-30240-60 13/135/5 0.843-93.27-0.0590.764-88.6-0.146\n2018 Wavelet Transform + CNN [36] CHB-MIT 15-18 10-fold CV 10-10 NR 0.866-87.8-0.147\n2019 Spectral power, statistical\nmoments, Hjorth + 3D CNN [11] CHB-MIT 16-77 LOOCV 60-60120-60240-60\n13/1614/1615/16\nNR-86.8-0.292NR-87.0-0.186NR-85.7-0.096\n2020\nCommon spatial pattern\nstatistics, Butterworth\nband-pass ﬁlter + CNNs [37]\nCHB-MIT 23-156 LOOCV 30-30 NR 0.90-92.0-0.120\n2018\nTemporal, frequency,\nchannels correlation,\ngraph theoretic\nfeatures + LSTM [38]\nCHB-MIT 24-185 10-fold CV\nNR-15NR-30NR-60NR-120\nNRNRNRNR\nNR-99.3-0.110NR-99.4-0.060NR-99.6-0.030NR-99.8-0.020\n2021 STFT + STCNN [27] Kaggle2014 7-64 LOOCV NR NR 0.746-82.0-0.380\n2021 STFT + ResidualNetwork-Self Attention [39] CHB-MIT 13-64 LOOCV 240-30 NR 0.913-89.3-NR\n2022 Raw data + AdderNet [8] CHB-MITKaggle2014 13-685-42 LOOCV 240-6010080-60 NR 0.917-85.8-0.0710.794-81.7-0.118\n2023 Temporal feature +\nPCA-SVM [40] CHB-MIT 23-173 NR NR NR 0.900-91.8-NR\n2023 Tangent space features +\nGSFDA [28]\nCHB-MITKaggle2014 15-895-42 LOOCV 240-6010080-60 NR 0.683-70.5-0.3810.622-61.9-0.422\n2023 SVM [41] +STFT CHB-MITKaggle2014 13-645-42 LOOCV 240-3010080-60 9/133/5 0.663-60.4-0.3470.654-69.4-0.208\nThis work STFT + B2-ViT Net CHB-MITKaggle2014 13-645-42 LOOCV 240-3010080-60 13/135/5 0.923-93.3-0.0570.816-85.2-0.013\ntraining, and the rest seizure will be used for testing. Each\nseizure will be taken as the testing set in turn. In addition,\nto monitor whether the model is overﬁtting in real-time and\nadjust the parameters of the model,M \u0000 1 seizures data for\ntraining are divided into training set and validation set, the\nproportion of validation set is set to 25% as in most seizure\nprediction studies. The number of transformer blocksl is 6,\nthe number of self-attention headsh is 8, the dimension of\none self-attention head is 64, the hidden layer size is 512.\nOur experiments are based on PyTorch 1.11.0, which is\nimplemented using Python 3.8 and Cuda 11.3.0. The loss is\noptimized by the Stochastic Gradient Descent (SGD) optimizer\n(learning rate = 0.001, momentum = 0.9, weight decay =\n5e\u00005), the cosine scheduler is used to optimize the learning\nrate, the epoch of training is set to 100, the loss function is\nCross Entropy Loss. The early stopping method with patience\n10 is used to obtain better generalization performance and\navoid over-ﬁtting. Two GeForce RTX 3090 Ti are used that\napproximately need 64 GB GPU memory in total.\nB. Overall Performance Comparison\nThe overall performance of B2-ViT Net is evaluated in\nthe following methods. CNN [9] is a forward neural network\nwith deep structure and convolution calculation, which is one\nof the representative algorithms of deep learning and has\nachieved good results in computer vision and natural language\nprocessing in recent years. It is one of the most popular in\ndeep learning methods currently designed for EEG decoding\n[42]. DCNN+Bi-LSTM [13] used DCNN to extract spatial\nfeatures, and Bi-LSTM was used as a classiﬁer to improve\nclassiﬁcation accuracy, which is typically designed to predict\nEEG seizures. Vision Transformer [20] is the application\nof transformer in the ﬁeld of computer vision, achieving\nperformance beyond CNN in most visual tasks. AdderNet [8]\nproposed a simple and effective end-to-end adder network\nand supervised contrastive learning, used addition instead\nof multiplication signiﬁcantly reduces computational costs.\nMulti-scale ProtoPNet [26] proposed a deep learning model for\npatient-speciﬁc seizure prediction, it attempted to measure the\nsimilarity between the inputs and prototypes (learned during\ntraining) as evidence to make ﬁnal predictions.\nFrom Table III, Table IV and Fig 3, the proposed B2-ViT\nscheme yields an average AUC of 0.923 while other methods\nonly achieve an average AUC of 0.834, 0.824, 0.861, 0.917,\n0.843 on the CHB-MIT datasets and an average AUC of\n0.816 while other baseline methods only achieve an average\nAUC of 0.792, 0.806, 0.759, 0.794, 0.764 on the Kaggle\ndatasets, which shows that our proposed method has good\nclassiﬁcation ability. In particular, patients 1, 19, 20 and 23\nof CHB-MIT reach an AUC greater than 0.99, which proves\nthe effectiveness of our method in distinguishing preictal EEG\nsignal from interictal EEG signal. In addition, our seizure\nprediction method is superior to other compared methods by\nsuccessfully warning 60 seizures out of 64 on the CHB-\nMIT dataset, 37 seizures out of 42 on the Kaggle dataset.\nMeanwhile, our method achieves a remarkably low FPR.\nAs a result, the bi-level programming model B2-ViT Net\nobtains the promising AUC,Sn and FPR, which indicates the\neffectiveness of our proposed method in automatic seizure\nprediction. In addition, for all subjects in CHB-MIT and\nKaggle datasets, thep-value is less than 0.001, this shows that\nour seizure predictor is signiﬁcantly better than the random\npredictor under 99.9% conﬁdence interval (0.001 signiﬁcance\nlevel), which is statistically signiﬁcant, providing signiﬁcantly\nexcellent performance in automatic seizure prediction of our\nproposed B2-ViT framework.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. XX, NO. XX, XXXX 2023\n濄\n濄\n!\"#$%&\"$'()*(+,%+-%.$(/#\"$%%%%%%%%%012.$%&\"$'()*(+,%+-%.$(/#\"$\n!\"#$%&#'()\n!\"#\n!$#\nFig. 3: The AUC for each seizure prediction on the (a) CHB-MIT and (b)\nKaggle datasets. Each bar represents one seizure. Correct and incorrect\npredictions of seizure are given withN and 4, respectively.!\"# !$#\n!\"#$%&' !\"#$%&'\n()**+, ()**+,\nFig. 4: (a) AUC comparison (left column) and (b)Sn comparison (right\ncolumn) of ViT, B-ViT and B2-ViT on CHB-MIT and Kaggle Datasets.\nTABLE IX: PERFORMANCE COMPARISON OF DIFFERENT WINDOW\nLENGTHS ON THE CHB-MIT DATASET\nModel Window AUC Sn(%) FDR/h p-value\nViT\n10s 0.843 77.6 0.090 1/13\n20s 0.859 78.6 0.090 1/13\n30s 0.861 80.2 0.085 1/13\n40s 0.860 79.5 0.095 2/13\nFig. 5: The impact of feature nodes and enhancement nodes on the\nB2-ViT Net (chb01).\n,QWHULFWDO\n!\"#$$%&\n!\"#$$%&\n!\"#$$%&\n3UHLFWDO\nFig. 6: Left: attention weights among channels. Channel 15 show strong\nabnormal connections with other channels, so channel 15 may be located in\nthe seizure zone, where 0 represents channel 1. The heatmap below\nrepresents the sum of attention for each channel. Right: channels’ attention\nweights of preictal and interictal. In preictal, channel 12, 13, 14, 16, 20 are\nassigned lower attention weights and other channels are assigned higher\nattention weights, where 0 represents the classﬁcation token, 1 represents\nchannel 1 (This ﬁgure originates from chb01).\nV. DISCUSSIONS\nA. Ablation Studies\nTo verify the effectiveness of our proposed B2-ViT model,\nwe conducted further ablation experiments, and the results are\nshown in TableV, VI and Fig. 4. It can be seen that on the\nCHB-MIT dataset, all the evaluation metrics of BViT model\nare higher than ViT, AUC is increased by 1%,Sn is increased\nby 4.9%, FPR is decreased by 0.004, and thep-value under the\nsigniﬁcance level of 0.001 is increased from 11/13 to 12/13.\nThe evaluation metrics of B2-ViT are signiﬁcantly higher than\nthose of BViT, with AUC increased by 4.2%,Sn increased\nby 8.2%, FPR decreased by 0.024, and the p-value under\nthe signiﬁcant level of 0.001 increased from 12/13 to 13/13.\nSimilar results can be obtained on the Kaggle dataset. As can\nbe observed, the results prove the effectiveness of B2-ViT\nmodel, B2-ViT model consistently outperforms ViT and BViT\nmodels. Moreover, TableVII shows the params and training\ntime of different algorithms, which indicates that our method\nachieves high performance improvement in a small increment\nof training time without increasing trainable parameters.\nB. Effects of Different Window Lengths of EEG Signals\nAn appropriate window length is expected to achieve better\nperformance. We evaluate the effect of different window\nlengths on the experimental results using the baseline method\nViT, and ﬁnd that the window length of 30s is more appro-\npriate. The results are shown in tableIX. Within 30s, with\nthe increase of window length, ViT contains more and more\ndistinctive feature information, and its performance is getting\nbetter and better. When the window length exceeds 30s, all\nevaluation metrics decline, and the classiﬁcation performance\nreaches the bottleneck. This shows that the window length of\n30s contains enough feature information for classiﬁcation, so\nthe window length of 30s is chosen for seizure prediction.\nC. Effects of Parameter Settings in BLS\nRelevant parameter settings in BLS may affect the experi-\nmental results of our proposed model, the number of feature\nnodes and enhancement nodes can be adjusted according to\ndifferent experimental scenarios. To verify the robustness of\nour proposed model, the inﬂuence of important experimental\nparameters of BLS on AUC is analyzed. Fig. 5 shows the\ncorresponding AUC under different mapping feature nodes and\nenhancement nodes. The range of the feature nodes’ groups is\nset to 10-15, and the number of enhancement nodes is set to 1,\n100, 500, 1000, 5000. It can be seen that the best experimental\nresults can be obtained when the mapping feature nodes and\nenhancement feature nodes are 165 and 100 respectively on\npatient 1 of the CHB-MIT dataset. The AUC of B2-ViT is\nrelatively stable, and good experimental results are obtained.\nTherefore, the automatic seizure prediction performance of\nB2-ViT does not ﬂuctuate obviously due to the change of\nparameters of BLS, which shows that our proposed method\nhas good robustness in BLS module.\nD. Performance Comparison of the Existing State-of-the\nArt Methods\nTable VIII shows the experimental settings and performance\nresults of the existing state-of-the-art methods on CHB-MIT\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nSHI et al.: B2-VIT NET: BROAD VISION TRANSFORMER NETWORK WITH BROAD ATTENTION FOR SEIZURE PREDICTION 9\nand Kaggle datasets, where NR is not reported values. It\nis necessary to point out that it is difﬁcult to compare our\nmethod directly with the existing methods due to the different\nexperimental settings, such as the Interictal distance-Preictal\nlength and validation scheme. Compared with our LOOCV\nstrategy, the no-cv and k-fold cv in [34], [35], [36] and [38] are\nmuch less challenging and stable, and the intrapatient variation\nof seizures is ignored. In addition, although statistical signif-\nicance research has always been emphasized, only [34], [9],\n[11], [26] and [43] have statistical evaluation. Moreover, deep\nlearning method is usually a black box, and the interpretability\nof the model is an important research direction at present, but\nfew studies give interpretability in speciﬁc scenarios.\nAs a result, compared with other methods, our bi-level\nprogramming model B2-ViT Net yields a competitive AUC,\nSn, FPR andp-value. The AUC, FPR andp-value has reached\nSOTA, only theSn lower than [38]. Although [38] achieved\nvery high sensitivity on the CHB-MIT dataset, they adopted\na complex time-consuming feature extraction method and\n10-fold CV instead of LOOCV . Because each seizure is\nindependent in LOOCV , it is more realistic and useful in\nclinical application. What’s more, our model uses the attention\nmechanism to explain the global spatial interactions among\nchannels and long-range temporal dependencies required for\nseizure prediction, so that the model has a certain degree of\ninterpretability.\nE. Limitations and Future Directions\nAlthough our proposed seizure prediction algorithm\nachieves strong prediction performance, some limitations still\nremain in the current work. On the one hand, due to the\nlack of detailed information on the patient’s epileptogenic\nzone and corresponding biomarkers, the results were not\nvalidated through neuroscience experiments. For example,\nchannels located in the epileptogenic zone may show strong\nabnormal connections with other channels, channels located in\nthe epileptogenic zone are assigned attention weights higher\nthan other channels. Besides, the neural links between brain\nregions assigned high attention weights were not captured. Fig.\n6 shows some of our conjectures.\nOn the other hand, our method is based on patient-\ndependent, meaning that both the training and test sets come\nfrom the same patient. It cannot be directly used for patient-\nindependent seizure early warning tasks, i.e., the model trained\nby one patient cannot be applied to another patient. This is\nmainly because our method lacks the ability to handle the\ndifferent distribution between the training and test sets. There-\nfore, transfer learning strategies [44], [45] will be considered\nto improve the performance of patients-independent seizure\nprediction tasks in our future work. In addition, we will try\nto cooperate with medical institutions, further explore the\nbiomarkers of the epileptogenic zone, the neural links between\nthe brain regions assigned high attention weights, and apply\nour proposed method to the realistic seizure prediction tasks\nin the future.\nVI. CONCLUSION\nBased on neuroscience mechanisms, we consider the global\nchannel interactions in spatial, long-range dependencies in\ntemporal together, and explore the generalized spatio-temporal\nlong-range correlation features required for seizure predic-\ntion in a vast space. A novel bilevel programming model\nB2-ViT Net is proposed for extracting generalized spatio-\ntemporal long-range correlation features for automatic seizure\nprediction. The proposed model has strong generalized feature\nsearch capability, which can comprehensively learn general-\nized spatio-temporal long-range correlation features that are\nconducive to automatic seizure prediction in a vast space, im-\nproving feature representation ability. In addition, the attention\nmechanism of our proposed model can calculate the interaction\nweights among channels, and evaluate the importance of each\nchannel at any time. We evaluated the performance of B2-\nViT model on the CHB-MIT and Kaggle datasets, the model\nyields promising results in terms of AUC, Sn, FPR andp-\nvalue, where the AUC, FPR andp-value have reached SOTA.\nExperimental results illustrate that our proposed method can\npredict seizures efﬁciently, help patients prevent or control the\nimpending seizure, and improve their quality of life.\nACKNOWLEDGMENT\nThis work was supported by the National Natural Science\nFoundation of China (No. 12371460).\nREFERENCES\n[1] P. Kwan, S. C. Schachter, and M. J. Brodie, “Drug-resistant epilepsy,”\nN. Engl. J. Med., vol. 365, no. 10, pp. 919–926, 2011.\n[2] L. Ridsdale, J. Charlton, M. Ashworth, M. P. Richardson, and M. C.\nGulliford, “Epilepsy mortality and risk factors for death in epilepsy: a\npopulation-based study,”Br. J. Gen. Pract., vol. 61, no. 586, pp. e271–\ne278, 2011.\n[3] L. Kuhlmann, K. Lehnertz, M. P. Richardson, B. Schelter, and H. P.\nZaveri, “Seizure prediction—ready for a new era,” Nat. Rev. Neurol.,\nvol. 14, no. 10, pp. 618–630, 2018.\n[4] S. L. Mosh´e, E. Perucca, P. Ryvlin, and T. Tomson, “Epilepsy: new\nadvances,” The Lancet, vol. 385, no. 9971, pp. 884–898, 2015.\n[5] C. Baumgartner and J. P. Koren, “Seizure detection using scalp-eeg,”\nEpilepsia, vol. 59, pp. 14–22, 2018.\n[6] Y . Sun, W. Jin, X. Si, X. Zhang, J. Cao, L. Wang, S. Yin, and D. Ming,\n“Continuous seizure detection based on transformer and long-term ieeg,”\nIEEE J. Biomed. Health Inform., vol. 26, no. 11, pp. 5418–5427, 2022.\n[7] E. Van Dellen, L. Douw, J. C. Baayen, J. J. Heimans, S. C. Ponten,\nW. P. Vandertop, D. N. Velis, C. J. Stam, and J. C. Reijneveld, “Long-\nterm effects of temporal lobe epilepsy on local neural networks: a\ngraph theoretical analysis of corticography recordings,”PloS One, vol. 4,\nno. 11, p. e8081, 2009.\n[8] Y . Zhao, C. Li, X. Liu, R. Qian, R. Song, and X. Chen, “Patient-\nspeciﬁc seizure prediction via adder network and supervised contrastive\nlearning,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 30, pp. 1536–\n1547, 2022.\n[9] N. D. Truong, A. D. Nguyen, L. Kuhlmann, M. R. Bonyadi, J. Yang,\nS. Ippolito, and O. Kavehei, “Convolutional neural networks for seizure\nprediction using intracranial and scalp electroencephalogram,”Neural\nNetw., vol. 105, pp. 104–111, 2018.\n[10] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,”Proc. IEEE, vol. 86, no. 11, pp. 2278–\n2324, 1998.\n[11] A. R. Ozcan and S. Erturk, “Seizure prediction in scalp eeg using 3d\nconvolutional neural networks with an image-based approach,”IEEE\nTrans. Neural Syst. Rehabil. Eng., vol. 27, no. 11, pp. 2284–2293, 2019.\n[12] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural networks\nfor human action recognition,”IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 35, no. 1, pp. 221–231, 2012.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. XX, NO. XX, XXXX 2023\n[13] H. Daoud and M. A. Bayoumi, “Efﬁcient epileptic seizure prediction\nbased on deep learning,”IEEE Trans. Biomed. Eng., vol. 13, no. 5, pp.\n804–813, 2019.\n[14] D. J. Englot, L. B. Hinkley, N. S. Kort, B. S. Imber, D. Mizuiri, S. M.\nHonma, A. M. Findlay, C. Garrett, P. L. Cheung, M. Mantleet al.,\n“Global and regional functional connectivity maps of neural oscillations\nin focal epilepsy,”Brain, vol. 138, no. 8, pp. 2249–2262, 2015.\n[15] Y . Chen, D. Yao, J. Yan, M. Jiang, T. Zhang, Z. Zhao, W. Zhao,\nJ. Zheng, R. Zhang, K. M. Kendrick, and X. Jiang, “Adversarial learning\nbased node-edge graph attention networks for autism spectrum disorder\nidentiﬁcation,” IEEE Trans. Neural Netw. Learn. Syst., pp. 1–12, 2022.\n[16] P. Li, D. Yao, H. Liu, Y . Si, C. Li, F. Li, X. Zhu, X. Huang, Y . Zeng,\nY . Zhang, and P. Xu, “Eeg based emotion recognition by combining\nfunctional connectivity network and local activations,”IEEE Trans. Bio.\nEng., vol. 66, no. 10, pp. 2869–2881, 2019.\n[17] T. Zhang, X. Wang, X. Xu, and C. P. Chen, “Gcb-net: Graph convolu-\ntional broad network and its application in emotion recognition,”IEEE\nTrans. Affect. Comput., vol. 13, no. 1, pp. 379–388, 2019.\n[18] M. J. Cook, A. Varsavsky, D. Himes, K. Leyde, S. F. Berkovic,\nT. O’Brien, and I. Mareels, “The dynamics of the epileptic brain reveal\nlong-memory processes,”Front. Neurol., vol. 5, p. 217, 2014.\n[19] E. J. Ngamga, S. Bialonski, N. Marwan, J. Kurths, C. Geier, and\nK. Lehnertz, “Evaluation of selected recurrence measures in discrim-\ninating pre-ictal and inter-ictal periods from epileptic eeg data,”Phys.\nLett. A, vol. 380, no. 16, pp. 1419–1425, 2016.\n[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” inInt. Conf. Learn. Represent.,\n2021.\n[21] N. Li, Y . Chen, W. Li, Z. Ding, D. Zhao, and S. Nie, “Bvit: Broad\nattention-based vision transformer,”IEEE Trans. Neural Netw. Learn.\nSyst., pp. 1–12, 2023.\n[22] T. Zhang, X. Gong, and C. L. P. Chen, “Bmt-net: Broad multitask\ntransformer network for sentiment analysis,” IEEE Trans. Cybern. ,\nvol. 52, no. 7, pp. 6232–6243, 2022.\n[23] C. P. Chen and Z. Liu, “Broad learning system: An effective and efﬁcient\nincremental learning system without the need for deep architecture,”\nIEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 1, pp. 10–24, 2017.\n[24] A. H. Shoeb, “Application of machine learning to epileptic seizure onset\ndetection and treatment,” Ph.D. dissertation, Massachusetts Institute of\nTechnology, 2009.\n[25] B. H. Brinkmann, J. Wagenaar, D. Abbot, P. Adkins, S. C. Bosshard,\nM. Chen, Q. M. Tieng, J. He, F. Mu˜noz-Almaraz, P. Botella-Rocamora\net al., “Crowdsourcing reproducible seizure forecasting in human and\ncanine epilepsy,”Brain, vol. 139, no. 6, pp. 1713–1722, 2016.\n[26] Y . Gao, A. Liu, L. Wang, R. Qian, and X. Chen, “A self-interpretable\ndeep learning model for seizure prediction using a multi-scale prototyp-\nical part network,”IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp.\n1847–1856, 2023.\n[27] R. Chen and K. K. Parhi, “Seizure prediction using convolutional neural\nnetworks and sequence transformer networks,” inProc. Annu. Int. Conf.\nIEEE Eng. Med. Biol. Soc., 2021.\n[28] Y . Zhao, S. Feng, C. Li, R. Song, D. Liang, and X. Chen, “Source-\nfree domain adaptation for privacy-preserving seizure prediction,”IEEE\nTrans. Ind. Inform., pp. 1–12, 2023.\n[29] Y . Zhang, Y . Guo, P. Yang, W. Chen, and B. Lo, “Epilepsy seizure\nprediction on eeg using common spatial pattern and convolutional neural\nnetwork,” IEEE J. Biomed. Health Inform., vol. 24, no. 2, pp. 465–474,\n2019.\n[30] T. Maiwald, M. Winterhalder, R. Aschenbrenner-Scheibe, H. U. V oss,\nA. Schulze-Bonhage, and J. Timmer, “Comparison of three nonlinear\nseizure prediction methods by means of the seizure prediction charac-\nteristic,” Phys. D Nonlinear Phenom., vol. 194, no. 3-4, pp. 357–368,\n2004.\n[31] L. Durak and O. Arikan, “Short-time fourier transform: two fundamental\nproperties and an optimal implementation,”IEEE Trans. Signal Process.,\nvol. 51, no. 5, pp. 1231–1242, 2003.\n[32] M. Farge, “Wavelet transforms and their applications to turbulence,”\nAnnu. Rev. Fluid Mech., vol. 24, no. 1, pp. 395–458, 1992.\n[33] K. Samiee, P. Kov´acs, and M. Gabbouj, “Epileptic seizure classiﬁcation\nof eeg time-series using rational discrete short-time fourier transform,”\nIEEE Trans. Biomed. Eng., vol. 62, no. 2, pp. 541–552, 2015.\n[34] A. Shahidi Zandi, R. Tafreshi, M. Javidan, and G. A. Dumont, “Pre-\ndicting epileptic seizures in scalp eeg based on a variational bayesian\ngaussian mixture model of zero-crossing intervals,”IEEE Trans. Biomed.\nEng., vol. 60, no. 5, pp. 1401–1413, 2013.\n[35] D. Cho, B. Min, J. Kim, and B. Lee, “Eeg-based prediction of epilep-\ntic seizures using phase synchronization elicited from noise-assisted\nmultivariate empirical mode decomposition,”IEEE Trans. Neural Syst.\nRehabil. Eng., vol. 25, no. 8, pp. 1309–1318, 2017.\n[36] H. Khan, L. Marcuse, M. Fields, K. Swann, and B. Yener, “Focal onset\nseizure prediction using convolutional networks,”IEEE Trans. Biomed.\nEng., vol. 65, no. 9, pp. 2109–2118, 2018.\n[37] Y . Zhang, Y . Guo, P. Yang, W. Chen, and B. Lo, “Epilepsy seizure\nprediction on eeg using common spatial pattern and convolutional neural\nnetwork,” IEEE J. Biomed. Health Inform., vol. 24, no. 2, pp. 465–474,\n2020.\n[38] K. M. Tsiouris, V . C. Pezoulas, M. Zervakis, S. Konitsiotis, D. D.\nKoutsouris, and D. I. Fotiadis, “A long short-term memory deep learning\nnetwork for the prediction of epileptic seizures using eeg signals,”\nComput. Biol. Med., vol. 99, pp. 24–37, 2018.\n[39] X. Yang, J. Zhao, Q. Sun, J. Lu, and X. Ma, “An effective dual self-\nattention residual network for seizure prediction,”IEEE Trans. Neural\nSyst. Rehabil. Eng., vol. 29, pp. 1604–1613, 2021.\n[40] L. Jiang, J. He, H. Pan, D. Wu, T. Jiang, and J. Liu, “Seizure detection\nalgorithm based on improved functional brain network structure feature\nextraction,” Biomed. Signal Process. Control, vol. 79, p. 104053, 2023.\n[41] M. Hearst, S. Dumais, E. Osuna, J. Platt, and B. Scholkopf, “Support\nvector machines,” IEEE Intell. Syst. Appl., vol. 13, no. 4, pp. 18–28,\n1998.\n[42] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,\nK. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,\n“Deep learning with convolutional neural networks for eeg decoding and\nvisualization,”Hum. Brain Mapp., vol. 38, no. 11, pp. 5391–5420, 2017.\n[43] C. Li, X. Huang, R. Song, R. Qian, X. Liu, and X. Chen, “Eeg-based\nseizure prediction via transformer guided cnn,”Measurement, vol. 203,\np. 111948, 2022.\n[44] T. Zhang, G. Su, C. Qing, X. Xu, B. Cai, and X. Xing, “Hierarchical\nlifelong learning by sharing representations and integrating hypothesis,”\nIEEE Trans. Syst., Man, and Cybern.: Syst., vol. 51, no. 2, pp. 1004–\n1014, 2019.\n[45] X. Li, R. La, Y . Wang, J. Niu, S. Zeng, S. Sun, and J. Zhu, “Eeg-based\nmild depression recognition using convolutional neural network,”Med.\nBiol. Eng. Comput., vol. 57, pp. 1341–1352, 2019.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3346955\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8418517112731934
    },
    {
      "name": "Computer science",
      "score": 0.6645207405090332
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6003745794296265
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5512955784797668
    },
    {
      "name": "Machine learning",
      "score": 0.5495903491973877
    },
    {
      "name": "Deep learning",
      "score": 0.5450066924095154
    },
    {
      "name": "Correlation",
      "score": 0.49937009811401367
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.49690869450569153
    },
    {
      "name": "Feature learning",
      "score": 0.4699523448944092
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41028666496276855
    },
    {
      "name": "Data mining",
      "score": 0.3393205404281616
    },
    {
      "name": "Mathematics",
      "score": 0.12005707621574402
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ]
}