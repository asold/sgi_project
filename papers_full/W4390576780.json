{
  "title": "SSNet: A Novel Transformer and CNN Hybrid Network for Remote Sensing Semantic Segmentation",
  "url": "https://openalex.org/W4390576780",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2120884319",
      "name": "Min Yao",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2512245037",
      "name": "Yao-Zu Zhang",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2115836675",
      "name": "Guofeng Liu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2613887392",
      "name": "Dongdong Pang",
      "affiliations": [
        "Tianshui Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6842806116",
    "https://openalex.org/W4205365435",
    "https://openalex.org/W3121898985",
    "https://openalex.org/W4207032598",
    "https://openalex.org/W3158574787",
    "https://openalex.org/W2908320224",
    "https://openalex.org/W3129918528",
    "https://openalex.org/W2893202425",
    "https://openalex.org/W3145627966",
    "https://openalex.org/W3135018180",
    "https://openalex.org/W3034427230",
    "https://openalex.org/W3113164744",
    "https://openalex.org/W3168495321",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4283450732",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W6798442695",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W6640295612",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6739696289",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W3035339581",
    "https://openalex.org/W3159637683",
    "https://openalex.org/W2963727650",
    "https://openalex.org/W2943351376",
    "https://openalex.org/W2965391153",
    "https://openalex.org/W2991471181",
    "https://openalex.org/W2899149423",
    "https://openalex.org/W2899319425",
    "https://openalex.org/W2964333009",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6792695861",
    "https://openalex.org/W6796617330",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W6760210069",
    "https://openalex.org/W4385346076",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W3034502973",
    "https://openalex.org/W2807974043",
    "https://openalex.org/W2883105896",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3044776120",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3181925591",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2922228302",
    "https://openalex.org/W3139049060"
  ],
  "abstract": "There are still various challenges in remote sensing semantic segmentation due to objects diversity and complexity. Transformer-based models have significant advantages in capturing global feature dependencies for segmentation. However, it unfortunately ignores local feature details. On the other hand, convolutional neural network (CNN), with a different interaction mechanism from transformer-based models, captures more small-scale local features instead of global features. In this article, a new semantic segmentation net framework named SSNet is proposed, which incorporates an encoder&#x2013;decoder structure, optimizing the advantages of both local and global features. In addition, we build feature fuse module and feature inject module to largely fuse these two-style features. The former module captures the dependencies between different positions and channels to extract multiscale features, which promotes the segmentation precision on similar objects. The latter module condenses the global information in transformer and injects it into CNN to obtain a broad global field of view, in which the depthwise strip convolution improves the segmentation accuracy on tiny objects. A CNN-based decoder progressively recovers the feature map size, and a block called atrous spatial pyramid pooling is adopted in decoder to obtain a multiscale context. The skip connection is established between the decoder and the encoder, which retains important feature information of the shallow layer network and is conducive to achieving flow of multiscale features. To evaluate our model, we compare it with current state-of-the-art models on WHDLD and Potsdam datasets. The experimental results indicate that our proposed model achieves more precise semantic segmentation.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nSSNet: A Novel Transformer and CNN Hybrid\nNetwork For Remote Sensing Semantic\nSegmentation\nMin Yao,Yaozu Zhang,Guofeng Liu,Dongdong Pang\nAbstract—There are still various challenges in remote sensing\nsemantic segmentation due to objects diversity and complexity.\nTransformer-based models have achieved encouraging results\nin semantic segmentation, which has significant advantages in\ncapturing global feature dependencies. However it unfortunately\nignores local feature details. On the other hand, Convolutional\nNeural Network (CNN), with a different interaction mechanism\nfrom Transformer-based models, captures more small-scale local\nfeatures, but experiences a difficulty to capture global features. In\nthis paper, a new semantic segmentation net framework named\nSSNet is proposed, which incorporates an encoder–decoder\nstructure, optimizing the advantages of both local and global\nfeatures. In addition, we build Feature Fuse Module(FFM)\nand Feature Inject Module(FIM) to largely fuse these two-\nstyle features. The former module captures the dependencies\nbetween different positions and channels to extract multi-scale\nfeatures, which promotes the segmentation precision on similar\nobjects. The latter module condenses the global information\nin Transformer and injects it into CNN to obtain a broad\nglobal field of view, in which the depth-wise strip convolution\nimproves the segmentation accuracy on tiny objects. A CNN-\nbased decoder progressively recovers the feature map size, and\na block called atrous spatial pyramid pooling (ASPP) is adopted\nin decoder to obtain a multi-scale context. The skip connection is\nestablished between the decoder and the encoder, which retains\nimportant feature information of the shallow layer network and\nis conducive to achieving flow of multi-scale features. To evaluate\nour model, we compares it with current state-of-the-art models\non WHDLD and Potsdam datasets. The experimental results\nindicate that our proposed model achieves more precise semantic\nsegmentation. The code of this work can be downloaded at\nhttps://github.com/stu-yzZ/SSNet.\nIndex Terms—Multi-scale features, fusion features, remote\nsensing, semantic segmentation\nI. I NTRODUCTION\nS\nEMANTIC segmentation is one of the most basic and\nimportant topics research in the fields of image process-\ning, which is widely applied to various segmentation tasks,\nsuch as remote sensing image segmentation [2], [3], medical\nimage segmentation [3], [4], etc. For remote sensing, seman-\ntic segmentation is also named ground-objects identification,\nreferring to a dense prediction task. In other words, it needs to\n(Corresponding author: Min Yao.)\nMin Yao,Yaozu Zhang and Guofeng Liu are with College of Information\nEngineering, Shanghai Maritime University,Shanghai 200120,China (e-mail:\nminyao@shmtu.edu.cn; z here@qq.com; 937413907@qq.com )\nDongdong Pang is with College of Resources and Environmental\nEngineering,Tianshui Normal University,Tianshui 741000, China (e-mail:\npangdongdong1994@163.com)\ncategorize each pixel in an image and then corresponds it to\nground-objects of different categories. With the fast develop-\nment of Computer Vision technology, semantic segmentation\nof remote sensing image becomes a current research hotspot\nand a useful tool in wide ranges of applications, such as\nbuilding extraction [6], land cover mapping [7], urban planning\n[8], environmental change monitoring [9], and agricultural pro-\nduction [10]. Remote sensing images contain various objects,\nsuch as airplanes, cars, roads, buildings, trees, etc. Due to the\ndiverse and complex nature of ground objects, remote sensing\nimage segmentation still faces great challenges, including high\nbackground complexity [11], high similarity objects [12], tiny\nobjects in high resolution images [13], as shown in Fig.1.\nRecently, as an important theory to extract image features,\nConvolutional Neural Network (CNN) has attracted wide\nattention to remote sensing image semantic segmentation.\nMany works based on CNN theory have already achieved\nencouraging results. Take Fully Convolutional Network (FCN)\n[6] for example, it is a seminal study in semantic segmenta-\ntion using CNN, which performs pixel-level categorization of\nimages. Since then, FCN has inspired many following works\n[24]-[26] and encoder-decoder structure has become a main\nchoice for semantic segmentation. Furthermore, researchers\nhave focused on enhancing such structure in various aspects.\nSpecifically, [24], [26], [28]-[30] explored expanding the re-\nceptive field. Contextual information has been acknowledged\nas a crucial feature, with [31]-[35] delving into this domain.\nAdditionally, there are works on auxiliary networks, which\naimed at extracting boundary information to assist in pixel\nclassification, such as [36]. Recently, SegNext[1], based on\nthe traditional convolutional improvement, has been proposed,\nwhich employs a similar pyramid structure to SegFormer[18]\nand uses multi-scale convolutional features to evoke spatial\nattention mechanism.\nOn the other hand, the outstanding modeling capabilities\nof self-attention have brought new achievements in computer\nvision. Computer vision models, such as [40],[41], and [42],\nemployed self-attention to capture dependencies between fea-\ntures in spatial and channel dimensions. Recently, Vision\nTransformer (ViT) [15] is a typical representation for image\nclassification proposed by Dosovitskiy et al. which was in-\nspired by Transformer scaling successes of natural language\nprocessing (NLP) [14]. To improve ViT, various works have\nexplored a variety of theories for modifying ViT and presented\n0000–0000/00$00.00 © 2021 IEEE Personal use is permitted, but republication/redistribution requires IEEE permission.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nFig. 1. Remote Sensing image segmentation example. The black box frames\nout the complex backgrounds of the given image while the purple box and\nthe green box show high similarity objects and tiny objects in high resolution\nimages. The cars, buildings and clutter in the purple box are highly similar\nin both shape and size, and the green boxes show some tiny objects.\nsome excellent performance, such as [18] and [16],[19],[43]-\n[46]. Both the decoder and encoder of DETR [43] use the\nTransformer structure , and Deformable DETR [44] improves\non this structure. A pure Transformer backbone network in\nsemantic segmentation, was introduced in SETR [45], and\nCNN-based decoders were proposed. In addition, related PE\nproblems were explored in CPVT [46], and a dynamic po-\nsitional embeddings (PE) theory was proposed. Meanwhile,\nmulti-scale feature maps are crucial in visual tasks, so PVT1\n[19] proposed a Transformer backbone with multi-scale, which\nis a significant improvement. SegFormer[18] improved ViT\nby introducing a hierarchical network, while using Mix-FFN\ninstead of PE, which is a remarkable achievement.\nFollowing ViT, Vision Transformer models usually divide an\nimage into multiple linearly embedded patches and input them\ninto a standard Transformer with positional embeddings (PE),\nresulting in encouraging performance. Transformer models\nare powerful at modeling global context [20], and require\nglobal features reasoning by computing self-attentions among\nall the tokens [21], but unfortunately deteriorate local feature\ndetails [22]. Due to this characteristic, great problems may\nexist, especially in semantic segmentation of remote sensing\nimages with complex backgrounds, or with small and highly\nsimilar objects. Here are several reasons. Firstly, complex\nbackgrounds can fool small objects, that is, self-attentional\nlearning of pixels on small objects absorbs complex back-\ngrounds noise, resulting in poor segmentation results [13].\nSecondly, it is difficult to classify highly similar objects based\non their shape, color and pattern. Last but not least, the image\nlocal continuity can be easily lost [18].\nWhile both CNN and Transformer models excel in the field\nof semantic segmentation, there are differences between them.\nOne major difference is the feature interaction mechanism.\nIn CNN, convolutional kernels are locally connected to the\ninput feature maps, where features only interact with their\nlocal neighbors [21]. In other words, the modeling range of\nCNN is limited by the receptive field of convolution kernels.\nAlthough the limited receptive fields of convolutions inevitably\nresult in the neglect of global image features, they are more\nsensitive to local information, hence more sensitive to tiny\nobjects. In addition, it is mentioned in [27] that CNNs are\n‘texture biased’ and make predictions mainly from texture\nin an image. Texture information plays a more important\nrole, when distinguishing between highly similar objects and\nbetween objects with complex backgrounds. Therefore, the\nfeature interaction mechanism of CNN has the benifit of\nremedying such fine details in the image that are deteriorated\nor neglected in Transformer. Various works [17], [20], [47]-\n[49] explored the theory of fusing CNN and Transformer.\nIn this paper, to compensate the limitations of Transformer\nin local modeling, a novel semantic segmentation network for\nremote sensing (RS) images called SSNet was constructed,\nwhich obtains feature maps from two complementary mecha-\nnism, SegFormer and SegNext, thus selectively promote the\nconvergence and flow of both. Additionally, Feature Fuse\nModule (FFM) and Feature Inject Module (FIM) are designed\nin our network. The purpose of FFM is to enhance the local\ndetails in Transformer by fusing the features of CNN and\nTransformer, then inject the fused features into Transformer.\nFIM selectively acquires multi-scale feature information to\ninject into CNN as to enhance the flow between global\ninformation and local information in each stage. Furthermore,\na CNN-based decoder is constructed to restore the feature\nmap size and acquire the semantic segmentation results. The\nprimary contributions of this work are as follows:\n1) We present a novel network designed specifically for re-\nmote sensing semantic segmentation, which retains local\nand global information in both branches. It maximizes the\ninjection of complementary information from CNNs into\nself-attention to obtain excellent segmentation results.\n2) To enhance the detail information of Transformer, four\nbranches are designed in FFM to handle two-style fea-\ntures. A combination of attention mechanisms and pool-\ning layers is used to wake up the module’s ability to\norchestrate global and local information.\n3) We apply strip convolution and Squeeze-and-\nExcitation(SE) module [40] to compensate for CNN’s\nneglect of global information during downsampling in\nFIM. SE module allows selective focus on channels,\nwhile depth-wise strip convolution increases the ability\nto capture tiny objects.\n4) We propose a CNN-based decoder that combines ASPP\nmodule to preserve more details. With skip connections\nwithin encoder and decoder, it improves deep features,\nrecover feature maps to the original image resolution, and\nachieves competitive results.\nII. R ELATED WORKS\nIn this section, we make a compendium of semantic segmen-\ntation models from three different perspectives. Section II-A\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nintroduces the use of CNN models in semantic segmentation.\nSection II-B is an introduction to the model of Self-Attention\nand Transformer. Section II-C describes the model combining\nboth Self-Attention and CNN.\nA. CNN-based methods for semantic segmentation\nWith the prevalence of computer vision in remote sensing,\nsemantic segmentation for remote sensing image based on\nCNN has garnered significant attention. Semantic segmenta-\ntion is systematically studied in a seminal work FCN [6]. FCN\nhas an encoder-decoder architecture, which has inspired many\nworks adopting this architecture in semantic segmentation. The\nencoder plays a crucial role in feature representation learning\nand like most other CNNs designed for computer vision,\nit comprises stacked convolutional layers. Considering the\ncomputational cost, a strategy of gradually reducing the feature\nmaps resolution is applied, allowing the encoder to learn more\nsemantic information by gradually increasing the receptive\nfield. Although FCN has high efficiency and low complexity,\nthe semantic segmentation results are not satisfying enough,\nbecause this network ignores the relationship between pixels\nand does not consider spatial consistency, leading to misclas-\nsification of object categories. After FCN, researchers focused\non improving it in different ways.\nPSPnet [26] and Deeplab series [24], [28]-[30] are examples\nthat improve FCN, in terms of expanding the receptive Field.\nPSPnet proposed the pyramid pooling module which incorpo-\nrates a (1 × 1) convolution operation for global pooling and\nupsampling and concat operations, incorporating contextual\ninformation at different scales and increasing the perceptual\nfield. PSPnet reduces the probability of mis-segmenting image\ncategories in the FCN network.\nThe Google team has proposed a series of semantic seg-\nmentation algorithms, among which [24], [28]-[30] are very\nwidely used. DeepLabV1 [24] is based on the improvement\nof CNN, which solves the problem of repeated pooling and\ndownsampling leading to resolution degradation. This makes\nthe image location information unrecoverable, and the use of\nConditional Random Field (CRF) improves the segmentation\nof fine details. DeepLabV2 [28] improves the network archi-\ntecture based on DeepLabV1, mainly proposing the ASPP to\naddress the problem of the existence of multi-scale objects in\nimages. DeepLabV3 [29] builds on DeepLabV2 by deleting\nthe fully-connected CRFs and improves the ASPP module\nby parallelizing the dilated convolution with different dilate\nrates. DeepLabV3+ [30] proposes a depth-wise separable\nconvolution, which has been highly influential in the field,\nand the model is obtained by adding decoder on top of\nDeepLabV3, significantly improving network performance.\nDepth-wise separable convolution can greatly reduce com-\nputation while maintaining performance and allowing better\nrecovery of object edge information.\nContextual information is also important for semantic seg-\nmentation performance, and many methods [31]-[35] explore\ncontext dependencies to obtain better segmentation results.\nEncNet [33] presents a new Context Encoding Module and en-\nhances model effectiveness by incorporating global contextual\ninformation. The main process is to do semantic segmentation\nby first predicting the category information present in the\nimage and then executing contextual encoding with feature\nattention mechanism. Authors of APCNet [35] argue that\nGlobalguided Local Affinity (GLA) is vital for construction of\nsemantic features, which was neglected in previous researches.\nConsidering this, the authors proposed a novel solution, the\nAdaptive Pyramid Context Network (APCNet), dedicated to\nadvancing semantic segmentation, which uses multiple Adap-\ntive Context Modules (ACMs) to adaptively construct multi-\nscale contextual representations. In particular, each ACM uses\nthe global image as a guide to every subregion, then uses\nthese affinities to compute the context vector. This favors\nthe further construction of adaptive and multi-scale contextual\nrepresentations.\nThe prediction of object boundary is likewise an aspect\nworth exploring. In GSCNN [36], the authors argue that color,\nshape, and texture contain various kinds of information that\nare critical to understanding an image, so it may not be ideal\nto process them together in CNN. Hence, GSCNN proposes a\nnovel dual-stream CNN architecture with a shape stream and\na classical stream processing information in parallel, where\nthe shape stream shape information apart. This architecture\nintroduces a novel type of gate to establish connections\nbetween the intermediate layers of the two streams. This new\ntype of gate is the key component in the architecture. Also,\nthanks to the sharper boundary prediction, GSCNN greatly\nimproves the segmentation performance on thin and smaller\nobjects.\nIn conclusion, FCN and corresponding variants were widely\nused in remote sensing semantic segmentation works [37]-\n[39]. The exploration of semantic segmentation models in\ndifferent aspects has also contributed to the progress of remote\nsensing images. CNN has demonstrated significant potential in\nthe realm of remote sensing semantic segmentation, owing to\nits modeling effectiveness and extensiveness.\nB. Self-Attention and Transformer in Vision\nRecently, self-attention mechanisms have been prevalent in\ncomputer vision tasks. Inductive bias of CNN may impose\nlimitations on the model’s ability to extract long-range spatial\ndependencies, thereby degrading its performance. To address\nthis issue, researchers have explored the incorporation of self-\nattention to aid CNN in feature extraction.\nSENet [40] leverages a global average pooling layer to\nestablish connections between channels and completes the\nrecalibrating of original features of the channel dimension\ndynamically and adaptively, which pays attention to the de-\npendence at the channel level of the model for the first\ntime. CBAM [41] uses channel-level and spatial-level attention\nmodules to refine adaptive features. The channel attention\nmodule emphasizes the relationship of feature maps between\ndifferent channels, and the spatial attention generates a spatial\nattention map, which allows the model to focus on important\nfeatures. DANet [42] proposes a Dual Attention Network,\nincluding Position Attention Module (PAM) and Channel\nAttention Module (CAM). These two modules are employed\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nFig. 2. The proposed framework consists of three main modules: A hierarchical hybrid encoder of Transformer and CNN to extract coarse and fine features;\nFFM and FIM ensure the flow of information between CNN and Transformer; a CNN-Based decoder to recover the size of the feature map.\nto capture the feature dependencies in spatial dimension and\nchannel dimension, respectively.\n[14] is one of the dominant architectures in NLP that utilizes\nmulti-head attention to establish long-range dependencies.\nInspired by the Transformer scaling successes in NLP, ViT\n[15] is a typical representative of image classification task\nand has achieved outstanding performance. But for dense\nprediction tasks, it requires a higher training cost and only\noutputs lower solution features.\nTo accommodate dense prediction tasks, some researchers\nmodify ViT architecture and achieved state-of-the-art perfor-\nmance. For example, DETR [43] leverages the Transformer\ndecoder to frame object detection as an end-to-end dictionary\nlookup problem, where learnable queries are used, effectively\neliminating the requirement for handcrafted processes like\nNon-Maximum Suppression (NMS). Building upon DETR,\ndeformable DETR [44] incorporates a deformable attention\nlayer to emphasize a sparse set of contextual elements, leading\nto faster convergence and improved performance. SETR [45] is\nthe pioneering network that employs a pure Transformer struc-\nture as backbone in semantic segmentation, which constructs\nthree different CNN-based decoders for backbone to obtain\ndense predicted results. By combining the Transformer-based\nencoder with a simple decoder, a powerful segmentation model\ncan be achieved. CPVT [46] advocated a novel Positional En-\ncoding (PE) scheme, named Conditional Positional Encoding\n(CPE). Different from PE used in previous works, such as ViT,\nwhich are predefined and input-agnostic, CPE is generated in\na dynamic way and is conditional on the local neighborhood\nof an input token. As a result, PE varies depending on input\nsize and ensures the desired property of translation-invariance.\nPVT1 [19] generates multi-scale feature maps by introduc-\ning pyramid modules to the Transformer framework, based\non which a pure Transformer backbone is proposed for dense\nprediction tasks. Although PVT1 reduces the computational\ncost to some extent, its complexity remains quadratic with the\nimage size. SegFormer proposes a hierarchical Transformer\nstructure without PE and significantly reduce computational\ncomplexity. It avoids PE interpolation, as it can adversely\naffect the model’s performance, under the condition where the\ntest resolution differs from the training one. In semantic seg-\nmentation, SegFormer demonstrates impressive performance\nas a dedicated Transformer.\nC. Self-Attention and CNN\nLocal feature details are tended to be neglected by the\nself-attention mechanism in Transformer [47], [22]. In con-\ntrast, CNN possesses distinct advantages in local modeling\nand translation-invariance. Since TransUNet [20] generates a\nnew encoder for improved semantic segmentation by sequen-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\ntially concatenating the two, this would allow it to benefit\nfrom Transformer self-attention while retaining the effective\nencoder-decoder structure of U-Net. TransFuse [48] performs\nparallel concatenation of them and fuses the relevant features\nby a BiFusion module which incorporates both self-attention\nand multi-modal fusion mechanisms. Furthermore, TransFuse\nemploys a simple progressive upsampling method to recover\nthe spatial resolution. UNetFormer [17] selects CNN as the\nencoder and also proposes an efficient global-local attention\nmechanism to model global and local information in the\ntransformer-based decoder. ST-UNet [49] uses Swin Trans-\nformer [16] to assist UNet [25], where CNN is used as the\nprimary encoder and Swin Transformer encoder is used as\nan auxiliary encoder. DS-Net [47] applies self-attention and\nconvolution as dual-resolution processing paths, in which the\nself-attention path is designed to capture local fine-grained\ndetails, while the convolution path aims to explore features\nfrom a global perspective.\nDifferent from existing excellent works, we design a novel\nnetwork that preserves both CNN and Transformer features. In\nthis design, both local and global information can be extracted\nat each stage, and the advantages of the two were fully utilized\nto obtain better semantic segmentation results.\nIII. M ETHODOLOGY\nIn this section, we provide a comprehensive overview of the\narchitecture of SSNet. In Section III-A, we generally introduce\nthe design of our pipeline. Subsequently, in Section III-B\nand C, we provide an in-depth introduction to the general\narchitecture schemes for SegFormer and SegNext. Then, in\nSection III-D and E, we introduce FIM and FFM, respectively,\nin terms of both mathematical principles and workflows.\nLastly, in Section III-F, we illustrate the structure of the CNN-\nbased decoder.\nA. Overall Architecture\nFig. 2 illustrates the proposed network, which follows the\nencoder–decoder paradigm, connecting both modules through\nskip connection layers. As depicted in Fig. 2, this framework\nmainly contains three modules: 1) a hybrid encoder of the\nSegFormer and SegNext. 2) Feature Fuse Module (FFM) used\nto fuse the feature from Transformer and CNN; Feature Inject\nModule (FIM) utilized to inject multi-scale information into\nthe CNN feature map from the Transformer branch. 3) a CNN-\nbased decoder for progressive recovery of feature map size and\nprediction of segmentation results.\nUnlike Transformer such as ViT and STER where back-\nbones only generate single-resolution feature maps when given\nan input remote sensing image, our backbone is to generate\nmulti-scale features in 4 stages. These different resolution\nfeature maps can enhance the property of semantic information\nextraction. In detail, for a given remote sensing (RS) image\n(3 ×H ×W), a hierarchical feature map Fn with a resolution\nof (Cn × H/2n+1 × W/2n+1) are obtained.\nFor FFM, it fuses two-style features in each stage, then\npasses the output features into the SegFormer to increase the\nTransformer abilities. And FIM injects the multi-scale features\nFig. 3. Structure of the SegFormer encoder. The main component of Seg-\nFormer Encoder is the Transformer block, which mainly comprises Efficient\nSelf-Attention and Mix-FFN.\nFig. 4. The structure of the core component MSCA in SegNext consisting\nof traditional convolution.\ninto the SegNext to enhance the CNN global perception\ncapability. CNN-based decoder is used to restore the feature\nmap size and inject the shallower informative features from the\nshallower layers of the encoder into the decoder by skipping\nconnections to obtain a more detailed feature map.\nB. Transformer-Based Encoder\nUnlike the generic Transformer, the SegFormer is more\nflexible and efficient, so we adopt SegFormer to extract global\nfeatures, which uses 4 stages to extract the feature map. Each\nstage generates feature maps at different resolution ratios than\nthe original image, including 1/4, 1/8, 1/16, 1/32. For an RS\nimage (3 × H × W), it was firstly split into patches of size\n(4 × 4). We use such small patches instead of size (16 × 16)\nemployed in ViT, since smaller patches perform better for\nintensive prediction tasks. These patches serve as inputs to the\nhierarchical Transformer encoder, resulting in the generation\nof multi-level feature maps.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nFig. 5. The structure of FFM. FFM takes full advantage of local features and global information by two different streams.\nAs shown in Fig. 3, the main component of SegFormer\nEncoder is the Transformer block, which mainly comprises\nefficient Self-Attention and Mix-FFN. During the original\nprocess, the self-attention is estimated as:\nAttention(Q, K, V) = Softmax(QKT )√dhead\n)V. (1)\nIn (1), the vectors query(Q), key(K), and value(V ) are fun-\ndamental components of the attention mechanism, commonly\nused as building blocks. Each of these vectors Q, K, and V\nhas dimensions of (C×H×W), where (H×W) represents the\nsequence length. [56] proposes the sequence reduction process\nto improve original multi-head self-attention, which brings the\ncomputational complexity down from O(N2) to O(N2/R),\nwhere R is a reduction ratio. In our experiments, we use the\nprocess and set the parameter R to [64, 16, 8, 1]. Moreover,\nfor semantic segmentation, positional encodings (PE) are not\nrequired. They introduce Mix-FFN which combines a (3 × 3)\nconvolution and Multilayer Perceptron (MLP) into each Feed-\nForward Network (FFN). Mix-FFN can be represented as\nfollows:\nXout = MLP(GELU(Conv3×3(MLP(Xin))))) +Xin, (2)\nwhere Xin and Xout represent the input and output feature\nmaps, so the feature map with approximate PE can be obtained\nby Mix-FFN. By choosing diverse hyperparameters and vary-\ning the number of Transformer blocks in each stage, we can\nobtain five SegFormer backbones with various complexities.\nWe choose mit-B5 as our Transformer backbone, which has a\ndeeper network layer and the best performance.\nC. CNN-Based Encoder\nThanks to the improvement of traditional convolutional\nblocks and the use of multi-scale feature convolution following\n[50], SegNext achieves a simple and efficient performance.\nAs shown in Fig. 4, the SegNext encoder is composed\nof three key components. 1) a depth-wise convolution to\nconsolidate local information. 2) multi-branch depth-wise strip\nconvolutions, which enable the capture of multi-scale feature\nmaps. Strip convolutions are a form of one-dimensional con-\nvolution operation, distinct from the common two-dimensional\nconvolution kernels, typically taking the shape of (n × 1)\nor (1 × n). The specific description can be found in the\nFIM presentation and Fig. 8. They are commonly applied\nfor horizontal or vertical processing of feature maps. 3) a\n(1 × 1) convolution facilitating the modeling of relationships\nin different channels. It can be written as:\nOut = Conv1×1(\n2X\ni=0\n(Scalei(DW−Conv(F)))) ⊗ F, (3)\nwhere DW−Conv is depth-wise convolution (5 × 5), and\nScalei denotes the convolution of three different scales respec-\ntively. Like some common semantic segmentation networks,\nSegNext adopts 4 stages and each produces feature maps\nat different resolutions. We choose SegNext-B as the CNN\nencoder because it has the optimal performance with lower\ncomputational requirements and can generate feature maps of\nthe same size as SegFormer.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nD. Feature Fuse Module\nHighly similar tiny foreground and confusing objects are\nprevalent in remote sensing images, which may significantly\nimpact the semantic segmentation quality. For such images,\nan adequate combination of local and global information,\nsuch as shape, color and texture, is needed to achieve better\nsegmentation effects. However, SegFormer approach weakens\nthe local feature detail to a certain extent, even if it uses\nsmaller patches, unlike ViT.\nTo improve the quality of segmentation, by taking full ad-\nvantages of local and global information, we propose Feature\nFuse Module (FFM). The FFM fuses features from both the\nCNN and Transformer branches and injects them back into\nthe Transformer branch, enriching the local details of the\nTransformer branch. We have designed two distinct streams\nto handle features from the CNN and Transformer branches\nseparately.\nAs shown in Fig. 5, the features from Transformer go\nthrough the pooling layer and Position Attention Module\n(PAM) [42] to obtain more local information, where PAM cap-\ntures the spatial relationships among different positions within\nthe feature map. Max pooling can retain more texture features,\nand avg pooling can better retain the overall features and\nhighlight the background information. On the second stream,\nthe Channel Attention Module (CAM) and strip convolution\nreceive the features from CNN, where CAM explicitly models\ninterdependencies between channels, and the depth-wise strip\nconvolution can obtain fine feature representations. Finally, we\nmultiply the features computed by the two streams and change\nthe dimensions by an (1 × 1) convolution as the final output.\nThis can be represented as follows:\nOut1 = (MaxPool(ft) + Avgpool(ft)) · CAM(fc), (4)\nOut2 = (\nX\nstr−Conv(fc)) · PAM(ft), (5)\nOutput = Conv1×1(Out1 · Out2), (6)\nwhere ft and fc represent the feature maps from Transformer\nand CNN, respectively. str−Conv is depth-wise strip convo-\nlution of different scales. The final results of the two streams\nare Out1 and Out2 respectively.\nFig. 6. The structure of PAM.\nPAM, illustrated in Fig. 6, firstly takes the input feature\nmap A and passes it through a convolution layer, resulting in\nthree different feature maps, namely B, C, and D, where B, C,\nand D are all of size (C × H × W), subsequently reshaping\nthem from (C × H × W) to (C × N) , where N = H ×\nW. Secondly, the transposed features BT ∈ R(N×C) of B ∈\nR(C×N) are multiplied with C ∈ R(C×N) and the weights\nS ∈ R(N×N) are obtained by softmax. Its calculation process\ncan be described as follows:\nSij = exp(Bi · Cj)\nNP\ni=1\n(exp(Bi · Cj)\n, (7)\nwhere Sij measures the ith position’s impact on jth position.\nThen, the feature map D ∈ R(C×N) and the transpose of\nthe weight S ∈ R(N×N) are multiplied together with a scale\nparameter α. After reshaping the result from (C ×N) to (C ×\nH × W), we perform an element-wise sum operation with\nthe original input feature map A to obtain the final output\nE ∈ R(C×H×W), as depicted below:\nEj = α\nNX\ni=1\n(SijDi) + Aj, (8)\nwhere α is initialized as 0 and gradually learns to assign more\nweighs by (8). It can be inferred that the resulting feature E\nfor each location is a weighted sum of all location features\nand the original features [42].\nFig. 7. The structure of CAM.\nThe structure of CAM is depicted in Fig. 7. We first reshape\nthe input feature map A from (C × H × W) to (C × N),\nwhere N = H ×W. Then, we perform a matrix multiplication\nbetween A ∈ R(C×N) and the transpose of AT ∈ R(N×C).\nBy applying the softmax layer on the result, we obtain the\nchannel attention map X ∈ R(C×C), which can be represented\nas follows:\nXij = exp(Ai · Aj)\nCP\ni=1\n(exp(Ai · Aj)\n, (9)\nwhere Xij measures the ith channel’s impact on the jth\nchannel.\nIn addition, we perform a multiplication between X ∈\nR(C×C) and A ∈ R(C×N), and then reshape the result\nto obtain R ∈ R(C×H×W). Afterward, we multiply the\nR ∈ R(C×H×W) by β which is a scale parameter and perform\nan element-wise sum operation with A to compute the final\noutput E ∈ R(C×H×W). This process can be represented as\nfollows:\nEj = β\nCX\ni=1\n(XijAi) + Aj, (10)\nwhere β learns a weight from 0. (10) implies that the final\nfeatures of each channel are computed as the weighted sum\nof the features from all channels and the original features [42].\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nFig. 8. The structure of FIM.\nE. Feature Inject Module\nWith the aim of leveraging the complementary nature of the\ntwo-style features, our work consecutively establishes FIM to\nintegrate multi-scale information from the Transformer branch\ninto the feature maps of CNN. This approach strengthens\nthe global perception capability of the CNN branch and\nfacilitates the efficient flow of feature maps between CNN\nand Transformer, as depicted in Fig.8.\nAs done in [51], [1], for every FIM branch, we employ three\ndepth-wise strip convolutions to extract information, which can\napproximate the standard depth-wise convolution of a large\nkernel and thus remains lightweight. That is to say we only\nneed a pair of (7×1) and (1×7) convolutions to approximate\nthe effect of a convolution with (7 × 7). This is one reason\nfor the usage of depth-wise strip convolution. Additionally,\nthere are many strip-like objects in the segmentation scenes.\nTherefore, strip convolution can be used as a complement to\nthe mesh convolution and helps extract strip-like features [51],\n[52]. In addition, we introduce the SE module [40]. For SE, the\nmain focus is to calculate the channel-wise weights for each\nfeature map that enters. With the addition of SE, by learning\nthe correlation between channels, the network is supposed to\nfocus on those channels that need more attention. FIM can be\nwritten as follows:\nOut = (\nX\nstr-Conv(f)) · SE(f), (11)\nwhere str-Conv means depth-wise strip convolution of differ-\nent sizes.\nThe SE module includes two main processes which\nare Squeeze and Excitation operations. The feature f ∈\nR(C×H×W) is squeezed to produce a tensor of p ∈ R(C×1×1).\nTherefore, each element in the vector encodes the global\ninformation of its corresponding channel, and the cth element\nof p is calculated using the following formula:\npc = Fgp(fc) = 1\nH × W\nHX\ni=1\nWX\nj=1\nfc(i, j), (12)\nwhere i and j refer to the position coordinates of the elements\nin feature f, and Fgp means global pooling layer.\nFig. 9. The structure of ASPP.\nNext, we change the dimensionality of the features through\ntwo different fully connected layers. Then the weights of each\nchannel in the input feature layer are obtained through the\nsigmoid function. In the end, the final output is obtained by\nmultiplying the weights with the original input feature f, and\nits detailed structure can be expressed as:\nV = Sigmoid(F C2(ReLu(F C1(p)))) ⊗ f, (13)\nwhere F Cmeans fully connected layer. The differences be-\ntween F C2 and F C1 are that numbers of neurons are set to\n(C/r) and C, respectively. In our SE modules, the reduction\nratio r is set to 16.\nF . CNN-Based Decoder\nClassical semantic segmentation U-shaped architecture is\nadopted in our work, and we employ a CNN-based decoder\nto progressively recover the feature map size and predict\nthe segmentation result. But encoders and decoders play\ndifferent roles in feature extraction, with encoders supporting\nthe extraction of shallow features such as color, texture and\nedges, while decoders are better at deep information, that\nis, semantic information. To keep the transmission of details\nand to promote the interaction of multi-scale characteristics,\nthis study uses skip connections to fuse shallow features with\ndeep features between CNN-based encoder and decoder. By\nemploying skip connections, we keep the transmission of\ndetails and improve the communication of multi-scale features,\nachieving the fusion of shallow and deep features between the\nCNN-based encoder and decoder. We aggregate feature maps\nfrom 4 stages with different resolutions.\nIn addition, ASPP is an excellent module in semantic\nsegmentation which samples the given inputs in parallel with\ndilation convolutions of different dilation rates. In our work,\nto obtain multi-scale contextual information, ASPP is added\nafter the encoder. ASPP is shown in Fig. 9, which includes\nglobal pooling operation, (1 × 1) convolution, upsampling\nand concat operations. In detail, the feature maps passed to\nASPP are firstly passed through four convolutional layers\nwith different dilation rates and a pooling layer. Then the\nobtained feature maps are concat to obtain feature maps\ncontaining multi-scale contextual information for subsequent\nsegmentation prediction.\nThe structure of the CNN-based decoder is depicted in\nFig. 2. First, it keeps the resolution of the feature maps in\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nTABLE I\nABLATION EXPERIMENT OF THE PROPOSED MODULES ON WHDLD AND POTSDAM VALIDATION SET\nNetwork\nModules IoU%(WHDLD) Evaluation Metrics\nFFM FIM Build Road Pavement Vegetation Bare Soil Water WHDLD Potsdam\nMIoU% Acc% MIoU% Acc%\nSegFormer+SegNext 54.82 57.71 41.25 81.33 36.76 93.34 60.86 87.36 76.33 84.69\nSegFormer+SegNext+FFM ! 55.74 59.28 42.33 81.52 37.02 93.31 61.53 88.38 77.17 85.68\nSegFormer+SegNext+FIM ! 55.26 58.88 41.91 81.42 36.98 93.30 61.29 87.53 76.68 85.35\nSegFormer+SegNext\n+FFM+FIM ! ! 56.00 59.76 42.64 82.03 37.28 93.90 61.93 88.62 77.67 85.92\nTABLE II\nABLATION EXPERIMENT OF SKIP CONNECTION ON WHDLD\nVALIDATION SET\nNetwork skip\nconnection\nEvaluation Metrics\nWHDLD Potsdam\nMIoU% Acc% MIoU% Acc%\nSegFormer\n+SegNext\n+FFM+FIM\nNo Skip 60.85 85.10 76.32 82.51\ns4+f1 61.02 86.44 76.39 83.76\ns3+f2 61.62 88.31 77.28 85.62\ns2+f3 61.28 86.77 76.65 84.03\nThree Skips 61.93 88.62 77.67 85.92\nCNN encoder stages 2-4, which are (128, H/8, W/8), (320,\nH/16, W/16), (512, H/32, W/32) respectively. The output\nfeature size of the ASPP module is (1024, H/32, W/32). To\nchange the output feature channel dimension of ASPP from\n1024 to 512, a (3 × 3) convolution is utilized, while keeping\nthe resolution. Let’s denote the obtained feature as f1, then\nf1 ∈ R(512,H/32,W/32). At this point, f1 and s4 which are the\nfeature maps output by CNN encoder stage 4, have the same\nsize. Then, we merge f1 with s4 in a way of elementwise\naddition, and the result size is (512, H/32, W/32). To gradually\nrecover to the original image size and fuse the feature maps of\ns3, we first change the channel dimension of result from 512\nto 320 by (1×1) convolution, and subsequently upsample that\nto (H/16, W/16) using bilinear interpolation and the result can\nbe named as f2 whose size is (320, H/16, W/16). Similarly,\nthen we merge f2 with s3, and the result size is (320, H/16,\nW/16). Next, the size is altered to (128, H/8, W/8) by an\n(1 × 1) convolution and bilinear interpolation, denoted as\nf3. Similarly, we fuse the features of f3 and s2 by way\nof elementwise addition to obtain a feature map (128, H/8,\nW/8), and we then restored the feature map by convolution\nand bilinear interpolation and output the final result.\nIV. EXPERIMENTAL SETUP\nA. Datasets\nThe proposed model has been evaluated on datasets of\nWHDLD and Potsdam respectively.\n1) WHDLD: WHDLD, released by Wuhan University in\n2018, is a dense labeling dataset suitable for multi-label tasks\nlike remote sensing image retrieval (RSIR) and classification,\nas well as pixel-based tasks such as semantic segmentation\n[53], [54]. Each image in the dataset is labeled with 6 class\nlabels, including building, road, pavement, vegetation, bare\nsoil, and water. The dataset comprises a total of 4940 images,\neach sized at 256 × 256 pixels. To ensure proper training,\nvalidation and testing, we split the dataset into a training set\n(3000 images), a validation set (1000 images), and a test set\n(940 images).\n2) Potsdam: Potsdam, situated in northeastern Germany,\nis characterized by large buildings, narrow streets, and dense\ntraffic. The Potsdam dataset has a ground sampling distance of\n5 cm and consists of 38 patches, each measuring 6000 × 6000\npixels. Within this dataset, six classes of objects are labeled,\nnamely building (blue), car (yellow), low vegetation (cyan),\nimpervious surface (white), tree (green), and background (red).\nFollowing [49] and [57], we utilized 24 images from the\ndataset for training, which were cropped into 13,824 images\nof size 256 × 256 pixels. The remaining 14 images were used\nfor verification and were likewise cropped into 8,064 images,\neach measuring 256 × 256 pixels.\nFig. 10. Comparison of segmentation results before and after using FFM in the\nproposed network. The fisrt colum are the original remote sensing images and\nthe second colum are ground truth segmentation results. (a) Results without\nFFM. (b) Results with FFM.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE III\nABLATION EXPERIMENT OF ENCODER ON WHDLD AND\nPOTSDAM VALIDATION SET\nEncoder Decoder WHDLD Potsdam\nMIoU% Acc% MIoU% Acc%\nSegNext MLP 59.83 85.71 73.50 81.89\nSegFormer(Mit-B0)\nMLP\n44.30 65.93 56.54 64.91\nSegFormer(Mit-B1) 49.98 74.46 62.87 72.18\nSegFormer(Mit-B2) 55.07 82.62 69.45 80.21\nSegFormer(Mit-B3) 58.51 84.29 73.33 81.70\nSegFormer(Mit-B4) 59.58 86.15 74.61 83.52\nSegFormer(Mit-B5) 60.41 86.52 75.56 83.68\nSegFormer(Mit-B5)\n+SegNext MLP 60.63 87.16 76.04 84.50\nB. Evaluation Metrics\nFor each method, we evaluate model performance using\nthe Mean Intersection over Union (MIoU) and Accuracy\n(Acc) metrics. These evaluation indicators are computed using\nthe accumulated confusion matrix, which are calculated as\nfollows:\nMIoU = 1\nN\nNX\nk=1\nT Pk\nT Pk + F Pk + F Nk\n, (14)\nAcc = T P+ T N\nT P+ F N+ F P+ F N, (15)\nwhere T P, F P, TN,and F Nindicate the true positive, false\npositive, true negative, and false negatives, respectively.\nC. Implementation Details\nFor our trials in present research, all experiments were\nperformed using PyTorch on our lab device which is equipped\nwith an Intel i9-10900 processor, an RTX3090 graphics pro-\ncessor, and 64G of RAM. All models are our own PyTorch-\nbased implementations, and we did not pre-train them. The\nAdamW algorithm was selected as the gradient decent op-\ntimizer algorithm for setting model parameters. The initial\nlearning rate is 0.0006, and weight decay is 0.001 with loss\nfunction of soft crossentropy.\nV. EXPERIMENTS AND RESULTS\nWe would like to emphasize that we trained all the networks\non our lab devices and did not use any pre-trained weights.\nThe ablation experiments were conducted using the WHDLD\nand Potsdam datasets with 300 epochs.\nA. Effect of Encoder Structure\nWe first analyzed the influence of various encoders, includ-\ning SegNext and various shallower SegFormer. The results are\npresented in Table III which demonstrates the performance\nof various types of encoders. As the Table III show, we can\ninitially observe that the best-performing architecture is the\nfusion of SegFormer (MiT-B5) and SegNext, while the shal-\nlowest SegFormer(MiT-B0) exhibits the lowest performance.\nAdditionally, the standalone SegNext outperforms SegFormer\n(MiT-B4)slightly. In terms of MIoU and ACC, the fused\narchitecture, which is SegFormer (MiT-B5) and SegNext,\nhave MIoU and ACC scores of 60.63 % and 87.16 % on\nWHDLD, and 76.04 % and 84.50% on Potsdam, respectively.\nOn WHDLD the fused architecture outperforms the standalone\nSegNext by 0.8% and 1.45%, and it also surpasses the deepest\nSegFormer by 0.22 % and 0.64 %. Similarly, on Potsdam, it\noutperforms the other two standalone architectures by 2.54 %\nand 2.61 %, and 0.48 % and 0.82 %, respectively. It turns out\nthat the encoder combining CNN and Transformer can capture\nmore information favorable for semantic segmentation.\nB. Effect of FFM\nTo explore the impact of FFM in our model, we conducted\na comparison of the semantic segmentation results with and\nwithout the incorporation of FFM. Table I shows these data,\non WHDLD and Potsdam, where MIoU and ACC improve by\n0.67% and 1.02 %, 0.84 % and 0.99 %, respectively. From the\npoint of view of details, on WHDLD we find that FFM better\nfacilitates the segmentation of Road, Pavement and Build, with\n1.57% and 1.08% and 0.92% improvement in MIoU for Road\nand Pavement and Build, respectively. From the visualization\nin Fig. 10, it can be found that Road and Pavement are\nhighly similar and indistinguishable in some ways, and after\nintegrating FFM, the segmentation performance of confusing\nobjects is significantly improved.\nC. Effect of FIM\nWe conducted an analysis of the effect of FIM, as presented\nin Table I. Firstly, when using the FIM independently, the\nTABLE IV\nABLATION EXPERIMENT OF ASPP ON WHDLD VALIDATION SET\nNetwork\nModules IoU%(WHDLD) Evaluation Metrics\nASPP Build Road Pavement Vegetation Bare Soil Water WHDLD Potsdam\nMIoU% Acc% MIoU% Acc%\nSegFormer+SegNext\n+FFM+FIM+Decoder\n# 55.79 58.76 41.97 81.51 37.76 93.54 61.56 88.43 77.20 85.73\n! 56.00 59.76 42.64 82.03 37.28 93.90 61.93 88.62 77.67 85.92\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nTABLE V\nRESULTS ON WHDLD VALIDATION SET\nMethod IoU% Evaluation Metrics\nBuild Road Pavement Vegetation Bare Soil Water MIoU% Acc%\nFCN [6] 53.52 56.15 39.27 80.10 38.37 92.66 60.01 87.09\nDeepLabV3 [29] 54.29 56.20 39.04 80.11 37.52 92.67 59.97 87.12\nUNet [25] 54.56 56.01 39.98 80.60 38.30 93.28 60.46 87.39\nDANet [42] 48.44 54.52 36.30 77.97 36.61 91.11 57.49 85.70\nTransUnet [20] 51.62 52.39 37.64 79.61 37.69 92.56 58.58 86.56\nOur Method 56.00 59.76 42.64 82.03 37.28 93.90 61.93 88.62\nsegmentation results on WHDLD show an improvement of\n0.43% on MIoU and 0.17 % on Acc. On Potsdam, MIoU\nand ACC were 76.68 % and 84.65 %, an increase of 0.35 %\nand 0.66% respectively. Moreover, the segmentation results of\nRoad and Pavement on WHDLD are significantly improved\nafter integrating FIM; MIoU achieves growth of 1.17 % and\n0.66% for Road and Pavement respectively, thanks to the\ndepth-wise strip convolutions in FIM, which are more sensitive\nto the strip-like objects. Although the results are better than the\noriginal model after adding FIM, they are still slightly inferior\nto the result using FFM. This is because FFM incorporates\nboth global and local information, while FIM is more focused\non strip-like objects.\nIn addition, we can observe the joint effect in Table I.\nAdding both FFM and FIM results in an increase in the\nMIOU and Acc indicators of the two datasets respectively,\nwhich exceeds the performance when adding FFM or FIM\nalone. Specifically, on WHDLD, MIoU and Acc increase by\n1.07% and 1.26% respectively, and on Potsdam, they increase\nby 1.43 % and 1.23 % respectively. These results indicate\nthat incorporating both FFM and FIM enhances the flow of\ndifferent information between CNN and Transformer, leading\nto improved semantic segmentation performance of the model.\nFig. 11. Comparison between the proposed network and other methods on WHDLD dataset. Examples are selected randomly from the validation set of the\nWHDLD dataset. (a) FCN. (b) DeepLabV3. (c) UNet. (d) DANet. (e) TransUnet. (f) Our network.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nTABLE VI\nRESULTS ON THE P OTSDAM VALIDATION SET\nMethod IoU% Evaluation Metrics\nImpervious Surface Building Low Vegetation Tree Car MIoU% Acc%\nFCN [6] 78.92 85.71 68.27 70.41 76.29 75.92 85.33\nDeepLabV3 [29] 80.10 87.53 69.17 71.76 76.34 76.68 85.91\nUNet [25] 79.33 87.01 69.54 72.56 77.02 77.09 85.90\nDANet [42] 79.23 86.57 68.42 70.24 74.22 75.74 85.42\nTransUnet [20] 77.03 82.41 68.32 70.46 75.52 74.74 84.27\nOur Method 80.35 88.09 69.58 72.81 77.54 77.67 85.92\nD. Effect of The Skip Connection\nIn the present part, we evaluate the effectiveness of the skip\nconnections in our model. As shown in Fig. 2, we introduce\na total of three skip connections in the decoder, s2, s3, s4,\nwhich are combined with f3, f2, f1 in turn. To avoid additional\ninterference, we keep the layers of the decoder unchanged\nand only disable the integration of skip connections. We\nconducted five experiments. The first one did not use any skip\nconnections, and then we added (s4+f1), (s3+f2), and (s2+f3)\nseparately, and finally, we added three skips to the model.\nIn a word, the addition of any skip connection leads to\nan improvement in the evaluation indicators of the model.\nSpecifically, the proposed model with 3 skip connections\nachieves the highest performance on WHDLD and Potsdam.\nFirst, the highest MIoU and ACC on WHDLD are 61.93 %\nand 88.62%, respectively, which are 1.08 % and 3.52% higher\nthan the model without any skip connections. And on Potsdam,\nour model achieves the highest MIoU and ACC of 77.67% and\n85.92%, respectively. Similarly, this result is 1.35% and 3.41%\nhigher than the lowest scores in MIoU and ACC respectively.\nAs shown in Table II, for WHDLD, we find that between not\nusing any skip connection and using (s4+f1), the model is not\nsignificantly improved. The addition of (s3+f2) improves the\nMIoU and ACC of the model the most, by 0.77 % and 3.21%,\nrespectively. After adding (s2+f3), the model behaves better\nFig. 12. Comparison between the proposed network and other methods on Potsdam dataset. Examples are selected randomly from the validation set of the\nPotsdam dataset. (a) FCN. (b) DeepLabV3. (c) UNet. (d) DANet. (e) TransUnet. (f) Our network.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nthan adding (s4+f1), but worse than adding (s3+f2), mean-\nwhile, MIoU and ACC are increased by 0.43 % and 1.67 %,\nrespectively, compared with adding (s4+f1). For Potsdam, we\ncan clearly observe that the experimental results have a similar\ntrend to the results on WHDLD. (s3+f2) has strong support\nfor model performance; it is 0.96 % and 3.11 % higher than\nno skip in MIoU and ACC. (s4+f1) and (s2+f3) contribute\nslightly more to the model than no skip. According to the\nexperimental results, there are significant differences in the\ninformation transmitted by different connections. Therefore,\nthe connection scheme should be optimized to effectively\nimprove the performance of the model.\nE. Effect of ASPP\nThe ASPP block effectively captures multi-scale features\nthat are crucial for semantic segmentation results by using\ndilation convolutions with various dilation rates. To investigate\nthe impact of ASPP in our model, we compared the semantic\nsegmentation results when the ASPP block was added and\nwhen it was not included.\nAs shown in Table IV , it can be seen that the metric\nincreases when including ASPP blocks in our network. On\nWHDLD and Potsdam, MIoU and ACC increase by 0.37 %\nand 0.19%, 0.47% and 0.19% respectively. This demonstrates\nthe positive effect of the ASPP block on improving the\nsegmentation performance of our model. In terms of specific\nclasses, ASPP is more significant for Road and Pavement,\nwith IoU increasing 1 % and 0.67 %, respectively. However,\nfor classes like Build and Water, the improvement was not as\nprominent. In general, the overall impact of the ASPP block\non the entire network was positive, so ASPP is incorporated\ninto our network.\nF . Comparison With Other Methods\nTo demonstrate the effectiveness of the proposed network\nSSNet, we compared it with a bunch of present methods,\nincluding FCN [6], DeeplabV3 [29], UNet [25], DANet [42],\nTransUnet [20], all of which we used Resnet101 [55] for\nthe backbone of the network except TransUnet and Unet. All\nmodels are based on CNN except TransUnet, which is a hybrid\nnetwork of Unet and ViT.\nTo be fair, we trained these models, using the same hyper-\nparameters and devices. It’s important to note that we did not\nuse any pre-trained weights for these models.\n1) Results on WHDLD Dataset: Table V presents the\nquantitative results on the WHDLD dataset, which further\nvalidates the effectiveness of our proposed model. From the\nquantitative point of view, our model reaches 61.93% in MIoU\nand 88.62 % in ACC; it is evident that the proposed model\nis superior to other methods in both MIoU and ACC and\noutperforms other models in terms of segmentation results for\neach category, except for the category of Bare Soil.\nFig. 11 illustrates the segmentation results of all models\nfrom a visual perspective. Thanks to the addition of FFM\nand FIM, our network not only improves the segmentation\nperformance of strip-like objects, but also better preserves the\ndetailed contours of some complex and irregular objects.\nTABLE VII\nCOMPARISON RESULTS OF FLOP S AND PARAMS AND MI OU ON\nWHDLD VALIDATION SET\nMethod FLOPs(G) Params(M) MIoU %\nFCN 54.22 51.94 60.01\nDeepLabV3 60.53 58.62 59.97\nUnet 40.21 17.26 60.46\nDANet 19.18 66.42 57.49\nTransUnet 25.00 66.81 58.58\nSSNet 10.73 54.00 61.93\n2) Results on Potsdam Dataset:We conducted experiments\non the Potsdam dataset to further evaluate the effectiveness\nof our method. Table VI presents the quantitative results on\nthe Potsdam dataset. Our model achieves MIoU of 77.67 %\nand ACC of 85.92%, which significantly outperforms previous\nmethods. Compared to other models, our network achieves\nthe highest enhancement of 2.93 % and 1.62 % in MIoU and\nACC, respectively, and it demonstrates an increase in the\nsegmentation performance of each category.\nTo visually showcase the segmentation performance of our\nmodel, we present the segmentation results in Fig. 12. In the\nfirst and third rows, our model is more likely to identify the\nImpervious Surface. The second and fourth rows are similar\nand mainly show the results of our model’s partitioning of the\nTree, and the fifth row mainly shows our model’s segmentation\nresults for the Background or Clutter.\nFig. 13. Performance VS Model efficiency of different models. The vertical\naxis represents the MIoU. The horizontal axis indicates the floating point\noperations (FLOPs). The diameter of the circle indicates the number of model\nparameter.\n3) The results for FLOPs, Params and MIoU on WHDLD\nDataset: When evaluating a deep learning model, two crucial\nfactors to consider are the number of floating-point operations\n(FLOPs) and the number of learnable parameters in the model.\nFLOPs offer an estimate of the model’s computational com-\nplexity, while params represent the number of parameters that\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\nrequire learning during model training, reflecting the model’s\nspace complexity.\nTable VII shows data of the various models on WHDLD.\nFirst and foremost, it is clear that SSNet has the best segmenta-\ntion performance of 61.93% and the lowest model complexity,\nalthough the number of parameters is in the middle of the pack.\nFor FLOPs, SSNet only needs 10.73G FLOPs, which is about\n2 - 4 times less than UNet, DANet and TransUnet, and more\nthan 5 times less than DeepLabV3 and FCN. Compared to\nstandard models like FCN, the SSNet model delivers state-of-\nthe-art efficiency while maintaining high accuracy and strong\nperformance for semantic segmentation.\nSince Transformer performs semantic computation based\non self-attention, SSNet does not have the least number of\nparameters, but it strikes an optimal balance between pa-\nrameters number and efficiency as well as property. SSNet\nachieves this level of performance with up to about 6 times\nfewer FLOPs than other models with similar parameter counts.\nFor instance, DeepLabV3 has 58.62M parameters but requires\n60.53G FLOPs compared to SSNet’s 10.73G FLOPs, while\nSSNet’s MIoU is 2 % higher than DeepLabV3. And with\n54M parameters, SSNet retains sufficient capacity for handling\ncomplex segmentation tasks, unlike extremely lightweight\nmodels like Unet (17.26M parameters). Benefiting from the\nadvantages of the CNN-transformer hybrid architecture, SSNet\ndemonstrates state-of-the-art MIoU performance while main-\ntaining relatively lower FLOPs and Params, showcasing its\nfeasibility and potential in remote sensing applications.\nFinally, for accuracy as measured by MIoU, SSNet achieves\n61.93% MIoU. This edges out prior top models by up to\n2%, representing a substantial accuracy gain. Fig. 13 visu-\nalizes the results of different models in the three performance\ndimensions-efficiency, computation cost, and precision.\nIn a word, the model effectively balances trade-offs between\nefficiency, size, and accuracy for superior overall performance\ncompared to prior methods.\nVI. CONCLUSION\nIn this research, we propose an innovative network for\nsemantic segmentation of remote sensing images. Our focus is\non effectively integrating the benefits of local and global infor-\nmation to enhance the feature discrimination of ground objects.\nThe proposed model follows the classical encoder-decoder\nmode, with the encoder combining CNN and Transformer. It\nconsists of four stages, producing feature maps with different\nresolutions, while the decoder gradually restores feature maps\nto the original map resolution size and predicts the semantic\nsegmentation results. Between the decoder and the encoder,\nwe use skip connections to keep the shallow and deep features\nfused with each other, enhancing the communication of multi-\nscale features. Moreover, we proposed FFM to improve the\nquality of segmentation which takes full advantages of local\nfeatures and global information. FIM managed to help extract\nstrip-like features as much as possible and learn the correlation\nbetween channels. Although our proposed model achieves en-\ncouraging performance on the Potsdam and WHDLD datasets,\nwe remain to be concerned with the parameters number and\nspeed, and it is unclear whether it can work well in small\nmobile devices. Also, we do not have a separate design for\nboundary detection, which we will verify in our future work.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349657\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7712864875793457
    },
    {
      "name": "Segmentation",
      "score": 0.5962752103805542
    },
    {
      "name": "Transformer",
      "score": 0.5187757015228271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5055356025695801
    },
    {
      "name": "Image segmentation",
      "score": 0.4356667995452881
    },
    {
      "name": "Computer vision",
      "score": 0.37729373574256897
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3586199879646301
    },
    {
      "name": "Natural language processing",
      "score": 0.3250791132450104
    },
    {
      "name": "Electrical engineering",
      "score": 0.10830095410346985
    },
    {
      "name": "Engineering",
      "score": 0.07851973176002502
    },
    {
      "name": "Voltage",
      "score": 0.07254120707511902
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96733725",
      "name": "Shanghai Maritime University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210163834",
      "name": "Tianshui Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 40
}