{
  "title": "Exploring Software Naturalness through Neural Language Models",
  "url": "https://openalex.org/W3035882142",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286992011",
      "name": "Buratti, Luca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286992012",
      "name": "Pujar, Saurabh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226622293",
      "name": "Bornea, Mihaela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226622296",
      "name": "McCarley, Scott",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347485315",
      "name": "Zheng, Yunhui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202201247",
      "name": "Rossiello, Gaetano",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748020125",
      "name": "Morari, Alessandro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225998510",
      "name": "Laredo, Jim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222197601",
      "name": "Thost, Veronika",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222445896",
      "name": "Zhuang, Yufan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287991514",
      "name": "Domeniconi, Giacomo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3021813414",
    "https://openalex.org/W2795753518",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W2981613521",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W2915009611",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3102782538",
    "https://openalex.org/W10589072",
    "https://openalex.org/W2954950681",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2969229669",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963762601",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W2970862273",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1992114977",
    "https://openalex.org/W2156981320",
    "https://openalex.org/W2995333547",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2078393527",
    "https://openalex.org/W2955426500",
    "https://openalex.org/W2993555399",
    "https://openalex.org/W2899384793",
    "https://openalex.org/W2944085062",
    "https://openalex.org/W2949737566",
    "https://openalex.org/W1771830246"
  ],
  "abstract": "The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree (AST) while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language models understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.",
  "full_text": "arXiv:2006.12641v2  [cs.CL]  24 Jun 2020\nExploring Software Naturalness through\nNeural Language Models\nLuca Buratti ∗\nIBM Research\nluca.buratti1@ibm.com\nSaurabh Pujar ∗\nIBM Research\nsaurabh.pujar@ibm.com\nMihaela Bornea ∗\nIBM Research\nmabornea@us.ibm.com\nScott McCarley ∗\nIBM Research\njsmc@us.ibm.com\nY unhui Zheng\nIBM Research\nzhengyu@us.ibm.com\nGaetano Rossiello\nIBM Research\ngaetano.rossiello@ibm.com\nAlessandro Morari\nIBM Research\namorari@us.ibm.com\nJim Laredo\nIBM Research\nlaredoj@us.ibm.com\nV eronika Thost\nIBM Research\nveronika.thost@ibm.com\nY ufan Zhuang\nIBM Research\nyufan.zhuang@ibm.com\nGiacomo Domeniconi\nIBM Research\ngiacomo.domeniconi1@ibm.com\nAbstract\nThe Software Naturalness hypothesis argues that programmi ng languages can be\nunderstood through the same techniques used in natural lang uage processing. W e\nexplore this hypothesis through the use of a pre-trained tra nsformer-based lan-\nguage model to perform code analysis tasks. Present approac hes to code analysis\ndepend heavily on features derived from the Abstract Syntax Tree (AST) while\nour transformer-based language models work on raw source co de. This work is\nthe ﬁrst to investigate whether such language models can dis cover AST features\nautomatically. T o achieve this, we introduce a sequence lab eling task that directly\nprobes the language model’s understanding of AST . Our resul ts show that trans-\nformer based language models achieve high accuracy in the AS T tagging task.\nFurthermore, we evaluate our model on a software vulnerabil ity identiﬁcation task.\nImportantly, we show that our approach obtains vulnerabili ty identiﬁcation results\ncomparable to graph based approaches that rely heavily on co mpilers for feature\nextraction.\n1 Introduction\nThe ﬁelds of Programming Languages (PL) and Natural Languag e Processing (NLP) have long re-\nlied on separate communities, approaches and techniques. R esearchers in the Software Engineering\ncommunity have proposed the Software Naturalness hypothesis [12] which argues that programming\nlanguages can be understood and manipulated with the same ap proaches as natural languages.\nThe idea of transferring representations, models and techn iques from natural languages to program-\nming languages has inspired interesting research. However , it raises the question of whether lan-\nguage model approaches based purely on source code can compe nsate for the lack of structure and\n∗ Equal Contribution. In no particular order.\nPreprint. Under review .\nsemantics available to graph-based approaches which incor porate compiler-produced features. The\napplication of language models to represent the source code has shown convincing results on code\ncompletion and bug detection [11, 18]. The use of structured information, such as the Abstract Syn-\ntax Tree (AST), the Control Flow Graph (CFG) and the Data Flow Graph (DFG), has proven to be\nbeneﬁcial for vulnerability identiﬁcation in source code [ 45]. However, the extraction of structured\ninformation can be very costly, and requires the complete pr oject. In the case of the C and C++ lan-\nguages, this can only be done through complete pre-processi ng and compilation that includes all the\nlibraries and source ﬁles. Because of these requirements, a pproaches based on structural informa-\ntion are not only computationally expensive but also inappl icable to incomplete code, for instance a\npull request.\nThis work explores the software naturalness hypothesis, em ploying the pre-training/ﬁne-tuning\nparadigm widely used with transformer-based [37] language models (LMs) [5, 38] to address tasks\ninvolving the analysis of both syntax and semantics of the so urce code in the C language. T o in-\nvestigate syntax, we ﬁrst introduce a novel sequence labeli ng task that directly probes the language\nmodel’s understanding of AST , as produced by Clang [48]. Fur thermore, we investigate the capa-\nbilities of LMs in handling complex semantics in the source c ode through the task of vulnerability\nidentiﬁcation (VI). All of our experiments involved LMs pre -trained from scratch on the C language\nsource code of 100 open source repositories. The use of langu age models with source code rather\nthan natural language leads to multiple challenges. Data sp arsity is a major problem when building\nlanguage models, leading to out-of-vocabulary (OOV) terms and poor representations for rare words.\nThese issues are particularly severe for PL because variabl e and function names can be of almost\narbitrary length and complexity.\nThere is a tradeoff between the granularity of tokenization and availability of long-range context for\nthe LM. Fine-grained tokenizations break identiﬁers into m any small common tokens, alleviating\nissues with rare tokens, but at the risk of spreading importa nt context across too many tokens. W e ad-\ndress the OOV and rare words problems by investigating three choices for tokenization, which span\nthe context/vocabulary size tradeoff from the extreme of ch aracter-based tokenization to subword\ntokenization styles familiar to the NLP community. W e indic ate that the choice of the pre-training\nobjective is closely connected to the choice of the vocabula ry. In particular, with character based to-\nkenization the pre-training task seems too easy. W e introdu ce a more difﬁcult, whole word masking\n(WWM) pre-training objective.\nIn our experiments, we show that our language model is able to effectively learn how to extract\nAST features from source code. Moreover, we obtain compelli ng results compared to graph-based\nmethods in the vulnerability identiﬁcation task. While cur rent approaches to code analysis depend\nheavily on features derived from the AST [45, 43, 43], our app roach works using raw source code\nwithout leveraging any kind of external features. This a maj or advantage, since it avoids a full\ncompilation phase to avoid the extraction of structural inf ormation. Indeed, our model can identify\nvulnerabilities during the software development stage or i n artifacts with incomplete code, which is\na valuable feature to increase productivity. W e show the mer its of simple approaches to tokenization,\nobtaining the best results using the character based tokeni zation with WWM pre-training.\nThe contributions of this work are summarized as follows: 1) we investigate the application of\ntransformer-based language models for complex source code analysis tasks; 2) we demonstrate that\ncharacter-based tokenization and pre-training with WWM el iminate the OOV and rare words; 3) this\nwork is the ﬁrst to investigate whether such language models can discover AST features automat-\nically; 4) our language model outperforms graph-based meth ods that use the compiler to generate\nfeatures.\n2 Model: C-BERT\nW e investigate the software naturalness hypothesis by pre- training from scratch a transformer-based\nLM, based on BER T [5], on a collection of repositories writte n in C language. Then, we ﬁne-tune it\non the AST node tagging (Section 3) and vulnerability identi ﬁcation (Section 4) tasks.\nHowever, the application of BER T on source code requires re- thinking the tokenization strategies.\nIndeed, tokenizers based on language grammar are unsuitabl e for use with transformer language\nmodels because of the nearly unlimited resulting vocabular y size. (There are more than 4 million\nunique C tokens in our pre-training corpus.) This greatly in creases the training time of the language\n2\nT able 1: A verage number of tokens per ﬁle in pre-training dat aset. For comparison there were 3,272\nC tokens per ﬁle in the pre-training data. Column 4 refers to t he VI task (Section 4)\nV ocab Size Pre-training data VI data\nT okenizer (tokens) (tokens/ﬁle) (tokens/ﬁle)\nChar 103 15,594 1789\nKeyChar 135 15,011 1469\nSP E 5,000 5,500 558\nmodel, and introduces unacceptably many OOV items in held-o ut data if truncated to typical vocab-\nulary sizes. Encouraged by recent work on subword tokenizat ion for source code [18] we explore\nthree subword tokenizer choices that reduce the vocabulary size and the impact of OOV and rare\nwords.\n2.1 T okenizers\nCharacter ( Char) W e investigate the extreme of subword tokenization: an ASCI I vocabulary, an\nalternative dismissed by [18]. Since our datasets are almos t entirely ASCII characters, we are able to\nreduce the total vocabulary size to 103 (including [CLS], [S EP], [MASK] and [UNK].) This choice\nminimizes vocabulary size, at the cost of limiting the size o f context window available to the model.\nCharacter + Keyword ( KeyChar ) Most programming languages have a relatively small number\nof keywords. By augmenting the Char vocabulary with 32 C language keywords [19], we are able\nto treat each keyword as a token, rather than breaking it up in to individual characters or subwords.\nThis reduces the number of tokens per document (and increase s the available context window), as\ncan be seen in T able 1.\nSentencepiece ( SP E) Neural NLP models almost always use a subword tokenizer such as Senten-\ncepiece [20] or Byte-Pair Encoding (BPE) [35]. W e follow oth er transformer based models and use\na sentencepiece tokenizer, with vocabulary size chosen to b e 5000. This includes all C keywords in\nthe vocabulary, along with many common library functions an d variable names. W e ﬁnd that this\nleaves an almost-negligible number of tokens ( < 0.001%) out-of-vocabulary in held-out data. T able\n1 shows that SP Etokenization reduces the number of tokens per document by ab out a factor of 3\ncompared to Char and KeyChar .\n2.2 T ransformer Based Language Models.\nOur model architecture is a multi-layer bidirectional tran sformer [37] based on BER T [5], which\nwe refer to as C-BERT. As in [5], the language model component of our model is ﬁrst p re-trained\non a large unlabeled corpus. T ask-speciﬁc training (\"ﬁne-t uning\") is then continued with different\ntask-speciﬁc objective functions on additional training d ata labeled for the tasks. In this section, we\nbrieﬂy review the four objective functions and associated c lassiﬁer layers involved in training our\nmodels. In all cases, our model is trained on ﬁxed-width wind ows of tokens, to which we prepend\na [CLS ] token to indicate the beginning of the sequence and appennd [SEP ] token to indicate the\nend of the sequence.\nFor every input sequence X = [x1 = CLS, x2, . . . , xT −1, xT = SEP ], the language model outputs\na sequence of contextualized token representations H = [h1 = hCLS , h2, . . . , hT = hSEP ] ∈\nRT ×768 where ht ∈ R768.\nMasked Language Model (MLM) Pre-training Objective A small percentage of tokens are se-\nlected for \"masking\", as described in [5]. A linear layer WLM ∈ R768×|V | (V denotes the C-BERT\nvocabulary associated with a tokenization) followed by softmax is added on top of C-BERT represen-\ntation to compute the probability distribution over V for each masked token. The objective function\nis maximum likelihood of the true labels as computed from\npLM = softmax(HWLM) ∈ R768×|V | (1)\nand evaluated over only the masked tokens. The model trained with this objective is used to initialize\nthe task-speciﬁc models.\n3\nWhole W ord Masked (WWM) Pre-training Objective The masked language model task used\nto pretrain C-BERT depends heavily on the tokenization. Particularly with Char tokenization, many\nmasked tokens are ASCII characters within a variable name th at is repeated elsewhere in the context.\nW e suspect in this case that the MLM task is too easy to adequat ely pretrain C-BERT, and conjecture\nthat making the pre-training task more difﬁcult by masking l onger spans of source code could result\nin stronger models on downstream tasks. Indeed, there is evi dence that such techniques are beneﬁcial\nin NLP . [9, 16]\nAs an alternative, we choose types of strings (for example, t ext strings that are legal variable or\nfunction names) to mask from the pre-training dataset, and w rite regular expressions to extract them.\nThese matches are stored in a trie to create a dictionary. Dur ing pre-training, when a token is selected\nfor masking, we identify which dictionary strings contain t he token and then add the entire span of\ntokens from that string to the set of tokens masked in the MLM o bjective. W e change the masking\nrate from 15% in MLM, to 3% for Char and KeyChar tokenizers and 9% for SP E. The new\nprobabilities ensure that the average number of masked toke ns remains roughly the same as MLM.\nAST Fine-tuning Objective. In this sequence labeling task, we add a linear layer WA ∈\nR768×|VAST | followed by softmax. Here VAST represents the set of AST labels. W e ﬁne tune the\nmodel using the cross entropy between the gold AST labels and\npAST = softmax(HWAST) (2)\nacross all tokens.\nVI Fine-tuning Objective. The VI is a binary classiﬁcation task and depends only on the hCLS\nembedding. W e add a single linear layer w ∈ R768. W e ﬁne-tune using the cross entropy between\nthe true labels and\npV I = sigmoid(h⊤\nCLS w) (3)\nevaluated across all context windows.\n2.3 Pre-training Corpus\nFor the pre-training dataset we chose the top-100 starred (a t least 5500 stars) GitHub C language\nrepositories. Forks and clones were excluded to avoid dupli cation. These include well-maintained,\nwidely-used repositories such as linux, git, postgres, and openssl. The total size of the pre-training\ncorpus is about 5.8 GB, much less than the corpus used to train BER T , or source code versions of\nBER T such as [17] and [7]. Comments were removed to keep only t okens related to code. W e\nimplemented the de-duplication strategy in [1] and found on ly about 40 ﬁles out of 60,000+ to be\nduplicates.\n2.4 Pre-training Details\nAll of our models are built using the Huggingface Pytorch Tra nsformers library [41]. Regardless\nof the type of tokenizer used, our model consists of 12 layers of transformers, 768-dimensional\nembeddings, 12 attention heads/layers, the same architect ural size of BERT BA S E [5]. W e divide\nthe training data into train, dev and test sets. Training is s topped based on dev set accuracy and\nmodels are selected for the VI task based on test set accuracy . W e train on 8 nodes, with 32 GPUs,\n512 maximum sequence length and batch size of 8, per GPU, for a total batch size of 256. Learning\nrate and number of epochs vary based on the tokenizer. W e foun d that Char and KeyChar models\nlearn faster compared to the SP Emodels. In total we selected 6 pretrained models, based on a\ncombination of 3 tokenizers and 2 masking strategies. W e exp eriment with different learning rates\n(LR). Char models with both MLM and WWM were trained with LR 10−4 and reach peak accuracy\nrelatively quickly compared to the respective KeyChar and SP Emodels. For KeyChar we used\nLR of 2 × 10−5 for MLM and 10−4 for WWM. For SP Ewe used LR of 2 × 10−5 for MLM and\n2 × 10−6 for WWM.\n3 AST Node T agging T ask\nThe Abstract Syntax Tree (AST) is an important structure pro duced by compilers and it has been\nused in prior works to improve the performance of code analys is tasks, such as vulnerability de-\ntection [45]. As a step toward full language model understan ding of the AST , in this section, we\n4\nT able 2: Example with ∗ used as multiplication (on the left); Example with ∗ used as pointer deref-\nerence (on the right)\ntoken cursor_kind token_kind token cursor_kind token_kind\nif IF_STMT KEYWORD sizeof CXX_UNAR Y_EXPR KEYWORD\n( IF_STMT PUNCTUA TION ( P AREN_EXPR PUNCTUA TION\nlensum DECL_REF_EXPR IDENTIFIER ∗ UNAR Y_OPERA TOR PUNCTUA TION\n∗ BINAR Y_OPERA TOR PUNCTUA TION ∗ UNAR Y_OPERA TOR PUNCTUA TION\n9 INTEGER_LITERAL LITERAL s DECL_REF_EXPR IDENTIFIER\n/ BINAR Y_OPERA TOR PUNCTUA TION -> MEMBER_REF_EXPR PUNCTUA TION\n10 INTEGER_LITERAL LITERAL coarse MEMBER_REF_EXPR IDENTIFIER\n> BINAR Y_OPERA TOR PUNCTUA TION ) P AREN_EXPR PUNCTUA TION\nmaxpos DECL_REF_EXPR IDENTIFIER\ndescribe a sequence labeling problem where the goal is to cap ture the syntactic role of each compo-\nnent of a linearized AST . Part-Of-Speech (POS) tagging is an analogous problem in NLP [25].\nIn detail, we propose two tasks to show that our models can dis cover the AST structure. For each task\nwe represent the source code as a sequence of programming tok ens, where each token is labeled with\nattributes token_kind and a cursor_kind . Gold labels for token_kind are produced by the compiler’s\ntokenizer component, while gold labels for the cursor_kind are produced by the compiler’s parser\ncomponent which maps each token to its cursor node in the AST . The role of the language model\nis to predict the correct token_kind and cursor_kind for every programming token by examining the\nsource code only.\nT o generate the gold data for our AST node tagging task we used Clang [48], an open-source C/C++\ncompiler with a modular design. Clang 2 has 5 token_kind labels and 209 cursor_kind labels. The\ncompiler handles the tokenization of the input source ﬁle an d assigns the attribute token_kind to each\nsource code token.\nThe AST is represented as a directed acyclic graph of cursor n odes. The cursor_kind is an attribute\nof the cursor to indicate the type of the node in the AST . Durin g parsing, every token in the source\ncode is mapped to the corresponding cursor node in the AST acc ording the rules of the C language\ngrammar.\nThe BER T -based language models and the baseline model solve d the token_kind task with ease.\nBoth accuracy and F 1 were greater than 99% across all three tokenizations on our data sets. On\nthis basis we concluded that small differences in performan ce on token_kind tasks were unlikely to\nprovide useful information about the strengths and weaknes s of our model. Further discussion will\nfocus entirely on the cursor_kind task.\nPredicting Clang’s cursor_kind label is similar to the task of word sense disambiguation in N LP . Just\nas the English word ’bank’ may refer to a ﬁnancial institutio n, or to the edge of a river, many tokens\nproduced by Clang have ambiguous meanings. For example the * operator is used as a binary\noperator, for multiplication, and as a unary operator, for d ereferencing a pointer variable (among\nother meanings.) W e show examples of Clang output for two C ex pressions containing * in T able 2.\nIn\nif ( lensum * 9 / 10 > maxpos ) (4)\nthe * is used for multiplication, and is labeled BINAR Y_OPERA TOR by Clang. In\nsizeof(**s->coarse) (5)\nboth instances of the * are used for pointer dereference, and are labeled UNAR Y _OPE RA TOR. The\nparentheses also have different labels in the two examples, and two meaning of the character > are\nimplicitly distinguished by its incorporation in the -> token.\nDatasets W e created two annotated datasets using the code from the FFm peg [46] and QEMU [47]\nGitHub repositories. FFmpeg is a collection of libraries an d tools to process multimedia content\nsuch as audio, video, subtitles and related metadata. It con tains 1,751 ﬁles with over 6 million\nClang tokens. QEMU is a generic machine and user space emulat or and virtualizer containing\n2 W e use libclang python bindings atop Clang 11.0\n5\n2,152 source ﬁles and over 7 million Clang tokens. Both projects ar e open source. W e split each\ndataset by randomly assigning the ﬁles in a 70/10/20 ratio between train, dev and test sets.\nW e use the libclang python bindings atop Clang 11.0 to parse t he C/C++ source ﬁles and traverse\nthe AST . In order to get the precise context information (e.g . cursor_kind for tokens), we make\nsure the source ﬁles can be correctly compiled by intercepti ng the build process to obtain necessary\ncompiler options and headers. W e apply the LM tokenizer to th e Clang output on each dataset and\nretain a unique, deterministic alignment between the Clang tokens and the LM’s tokens to produce\npredictions using C-BERT, as explained in Section 2.2.\nFine-tuning Details W e initialize from pre-trained C-BERT models for the three different tok-\nenization strategies described in Section 2 after validati ng that these models yielded acceptable\nperformance on the VI task. The sequence classiﬁer head in Eq uation 2 was randomly initialized.\nSelected experiments were repeated with 5 random number gen erator seeds in order to gauge ﬂuc-\ntuations. Since BER T is conﬁgured to handle a ﬁnite context w indow , the training ﬁle was divided\ninto non-overlapping windows of maximum 250 tokens. For the dev and test data a sliding window\napproach is used to create chunks of 250 tokens with 32 tokens overlap. When a token appears in\nmultiple segments, its prediction is determined through vo ting. Models were trained using a batch\nsize of 16, and for a maximum of 10 epochs or 24 hours, with ﬁnal models selected on the basis of\ndev-set F 1. W e investigated learning rates from the set of {5 × 10−5,2 × 10−5,10−4} For most\nexperiments the learning rate of 2 × 10−5 was used - we selected 10−4 if it yielded notably better\naccuracy during the learning rate exploration. Most experi ments were run on NV idia K-80s, with\nlearning rate exploration on NV idia V -100s.\nExperimental Results T est set results for the cursor_kind tasks are shown in T able 3. W e compare\nour transformer LMs with a BiLSTM baseline that uses the same tokenization as C-BERT. Both\nsystems produce cursor_kind predictions for every LM token in the input. W e use the alignm ent\nbetween C tokens and LM tokens to report C token level F1 and ac curacy for each dataset.\nT wo trends are immediately apparent. First, BER T -based lan guage models out-performed BiLSTM-\nbased language models across all three tokenizations, and o n both the FFmpeg and QEMU data sets.\nSecond, performance on the FFmpeg set was considerably high er than for the QEMU dataset, for\nboth the BER T and the BiLSTM-based models.\nDifferences in tokenization style led to only small changes in performance for C-BERT. These dif-\nferences were not statistically signiﬁcant - the absolute d ifferences between the min and max F 1\nacross ﬁve initialization seeds ranged from 1% to 3% for the different tokenizations of QEMU, and\nranged from 0.2% to 0.4% for the different tokenizations of FFmpeg. It appears that n eural language\nmodels are able to infer enough higher-level structures of p rogramming languages to perform these\ntasks without additional assistance from the tokenizer. Fo r the BiLSTM results the tokenization\nstyles show no impact on the FFmpeg dataset where both the acc uracy and the F1 score are high.\nThe tokenization has an impact on the QEMU dataset where the SP Etokenizer performs the best.\nW e analyzed the confusion matrix for both FFmpeg and QEMU dev sets. In both, the top 2 errors\nwere system predictions of COMPOUND_ST A TEMENT or DECL_REF _EXPR when the reference\nindicated O (for outside of a cursor_kind ). Clang typically indicates O for #include statements,\nmacro deﬁnitions, and architecture-dependent conditiona l compilations that are skipped. In particu-\nlar, we note that both repositories contain long macro deﬁni tions which superﬁcially resemble active\ncode, but which require fairly long context to identify as a m acro deﬁnition. The most common error\nnot involving ’O’ was predicting DECL_REF_EXPR for COMPOUN D_STMT .\nW e also experimented with BER T models pre-trained with the w hole-word masking objective. These\nmodels hurt performance here, whereas (we will see in Sectio n 4) they improve VI performance.\nFor example, on the QEMU dev-set, F 1 dropped from 93.3% to 82.4% for Char tokenization,\nfrom 93.3% to 88.2% for SP E, and from 94.1% to 90.8% for KeyChar tokenization. Further\ninvestigation is needed to determine whether this differen ce is due to the syntax/semantics focus of\nthe two tasks or whether the WWM is simply more beneﬁcial to a t ask such as VI with a lower\nbaseline performance.\n6\nT able 3: T est set accuracy and F1 for the AST cursor_kind tasks\nFFmpeg QEMU\nModel T okenizer Acc F1 Acc F1\nChar 94.96 95.71 71.68 80.53\nBiLSTM SP E 94.69 95.52 74.12 81.58\nKeyChar 95.68 96.58 66.20 76.19\nChar 97.10 97.72 81.06 87.43\nC-BERT SP E 97.72 98.29 81.11 87.79\nKeyChar 97.73 98.31 80.78 87.49\n4 V ulnerability Identiﬁcation T ask (VI)\nWhile VI has been investigated previously (e.g. Draper[23] , Juliet[28], Devign[45]) there is no\nsatisfactory comparable baseline. The Draper dataset [23] has many false positives, and the Juliet\ndataset [28] consists of synthetic data. Devign [45] releas ed appropriate natural data, but there are\nlimitations which make it impossible to compare with their r esults. Only 2 out of 4 projects from\nthe dataset were released, and the training/test split was n ot speciﬁed. Also, unspeciﬁed data was\nomitted from their published results because of computatio nal complexity and limitations of Joern\n[42] preprocessing, a key step in their pipeline. Furthermo re, lack of details about a key feature of\ntheir GGNN [21] network, the pooling layer, prevent exact re plication of the \"Devign composite\"\nmodel.\nData Description Our dataset consists of functions from the FFmpeg and QEMU pr ojects with\nnon-vulnerable/vulnerable labels as released by Devign, w ith the duplicates removed. W e also use\nthese two datasets to create a combined dataset. The origina l FFmpeg dataset contains 9,683 exam-\nples and QEMU contains 17,515 examples. W e call these three d atasets full FFmpeg, full QEMU\nand full combined datasets. In order to implement the GGNN baseline, we use Joern to compute\nmultiple graph features, including AST , just like Devign. W e skip some examples that yield a com-\npilation error with Joern. With the exclusion of problemati c samples, we get reduced versions of the\nfull datasets, and we call the resulting datasets FFmpeg reduced, QEMU reduced and combined\nreduced. The reduced dataset contains 6169 FFmpeg and 14,896 QEMU ex amples, which is a total\nreduction of 6133 relatively large functions.\nFine-tuning Details W e initialize with each of six pre-trained models described in Section 2.4.\nThe Huggingface implementation of BER T truncates function tokens beyond the count of 512. W e\ncall this default approach T runcate. W e ﬁne-tune according to Eq. 3. As can be seen in T able 1,\ncolumn 4, using Char and KeyChar tokenizers often pushes the context token count beyond 512.\nWith Char tokenizer, about 80% of FFmpeg functions have more than 510 t okens. W e aggregate\nsuch functions, similar to [29], by ﬁrst breaking the input e xample into N different segments of\nmaximum size 510 and then using a BiLSTM layer to combine the [ CLS] output. This output is\npassed to the linear layer for classiﬁcation. W e train for 10 Epochs, with learning rate of 2 × 10−5\nfor KeyChar , SP Eand 8 × 10−6 for Char, max sequence length of 512 and batch size of 4.\nBaselines The Naive baseline shows the accuracy if all instances are la beled vulnerable. Just like\nDevign, we use BiLSTM and CNN as baselines. Our strongest bas eline, GGNN, is implemented as\ndescribed in the Devign paper (GGRN composite) using the sam e graph features. Our implementa-\ntion of the Devign composite model, which is GGNN with poolin g, did not improve upon GGNN\nbecause we lacked adequate information on how to implement t he pooling layer. W e did not include\nDevign composite into our results. W e train BiLSTM and CNN ba selines on all the six datasets.\nW e train GGNN only on the reduced datasets due to Joern compil ation errors. Both BiLSTM and\nGGNN baselines are initialized with W ord2V ec [27] embeddin gs trained on source code from the\nFFmpeg and QEMU projects.\nExperiment Results W e report results with accuracy as the evaluation metric rat her than F1 score\nbecause our datasets are well-balanced, with 40%-55% label ed as vulnerable. The experiment re-\nsults are in T able 4. C-BERT models, with aggregation outperform the strongest GGNN bas eline by\n7\nT able 4: T est set accuracy for the VI task on the full and reduced (\"red\") datasets; C-BERT model id\nindicates tokenization (C|K|S), LM masking objective (M|W ) and aggregation(T|B); Aggr indicates\nthe aggregation method\nFFmpeg QEMU Combined\nModel LM + T okenizer Masking Aggr. full red full red full red\nNaive 51.1 46.5 42.4 41.1 45.5 42.7\nBiLSTM 59.5 58.3 61.6 64.0 57.6 61.5\nCNN 57.3 58.7 60.5 63.3 56.9 59.9\nGGNN NA 61.1 NA 65.8 NA 63.2\nCMT C-BERT Char MLM Truncate 52.7 54.7 57.3 58.7 55.5 57.8\nKMT C-BERT KeyChar MLM Truncate 55.5 57.0 57.5 59.5 56.2 58.1\nSMT C-BERT SP E MLM Truncate 57.7 54.8 59.3 60.5 57.4 58.0\nCWB C-BERT Char WWM BiLSTM 62.2 65.5 65.8 68.1 63.5 66.2\nKWB C-BERT KeyChar WWM BiLSTM 58.0 61.9 64.1 67.7 61.5 64.3\nSMB C-BERT SP E MLM BiLSTM 60.7 62.8 66.1 66.4 63.6 65.4\na reasonable margin of 3-4 points across all datasets. They a lso perform better than BiLSTM and\nCNN baselines on both the full and reduced datasets.\nThe three lines CMT , KMT and SMT show the effect of varying the tokenization strategy when\ntrained with the MLM objective. KMT performs better than CMT across all datasets and SMT\nhas the best overall performance. As expected, tokenizatio ns which enable the model to see longer\ncontext windows ( KeyChar and then SP E) achieve generally better results. All of our C-BERT\nMLM models improve signiﬁcantly upon the naive baseline.\nCWB, KWB and SMB are the best results for each of the three toke nizers. The best model is CWB,\nshowing the highest accuracy on most datasets. On the QEMU fu ll and combined full, the CWB\naccuracy is comparable to SMB. BiLSTM aggregation techniqu e improves results across all datasets,\nfor all models. WWM improves the performance of Char and KeyChar tokenizer, as expected.\nIt does not improve the SP Etokenization. W e found that WWM pre-training improves Char the\nmost, and mostly eliminates systematic differences in toke nization. Indeed, a Char model, CWB,\nhas the best results on 4 of the 6 datasets, while an SP Emodel with MLM pre-training, SMB, is\nbest on the other two.\n5 Related W ork\nIn recent years, research at the intersection of NLP , Machin e Learning and Software Engineering\nhas signiﬁcantly increased. T opics in this ﬁeld include cod e completion [33], [11], [13], program\nrepair [4], [34], [36], bug detection [31] and type inferenc e [32], [10]. [24], [30]. A common factor\namong all these techniques is the use of an n-gram or RNN based LM with the tokenization deﬁned\nby the programming language. Thus they are severely affecte d by the OOV and rare words problems.\nSub-word tokenization (e.g. BPE) was ﬁrst proposed by [18] a s a suitable alternative for source code\nand shows compelling results for code completion and bug det ection. In our work, we show that ﬁner\ngrained tokenization techniques are beneﬁcial for source c ode LMs compared to subword tokenizers.\nCuBert [17] is a recently introduced LM for modeling code. In contrast to our approach, this work\ndoes not consider character based tokenization and uses a su bword tokenizer. A Github corpus in\nPython is used for pre-training a BER T -like model which is ﬁn e-tuned for tasks like variable misuse\nclassiﬁcation, wrong binary operator detection, swapped o perands, function-docstring mismatch,\nand prediction of exception type. CodeBert [7] is another tr ansformer LM specialized for code\ngeneration from textual descriptions. Their specialized p re-training uses bimodal examples of source\ncode paired with natural language documentation from Githu b repositories.\nPrior to the use of machine learning techniques, static code analysis was used to understand code\nstructures and apply rules handcrafted by programmers to id entify potential vulnerabilities. T ools\nlike RA TS [14], Flawﬁnder [40], and Infer [6] are of this type . However, they produce many false\npositives, a problem identiﬁed early on by [2, 8, 15], making these tools difﬁcult to use effectively\nas part of the developer tool chain.\n8\nCurrent machine learning approaches to code analysis depen d heavily on features derived from\nthe compiler, such as the AST , or other derived structures th at require the compilation of source\ncode [45]. These features are paired with complex GGNN [26] m odels. In addition to the VI task\n[45], combining the AST and GGNN have shown good results in ty pe inference [39], code clone\ndetection [3, 44] and bug detection [22]. This evidence moti vated our AST tagging task.\n6 Conclusions\nThis work explores the software naturalness hypothesis by u sing language models on multiple source\ncode tasks. Unlike graph based approaches that use structur al features like the AST , CFG and\nDFG, our model is built on raw source code. Furthermore, we sh ow that the AST token_kind and\ncursor_kind can be learned with the LM. LMs can work even better than graph based approaches for\nVI because they avoid the computational cost and requiremen ts of full compilation. This makes our\napproach suitable for a wider range of scenarios, such as pul l requests. W e propose two character\nbased tokenization approaches that solve the OOV problem wh ile having very small vocabularies.\nW e suggest ways to improve them with aggregation and WWM. The se approaches work just as well\nand sometimes even better than a subword tokenizer like sent encepiece that has been previously\nexplored for source code. In future work we propose joint lea rning of the AST and VI tasks on top of\nLM to further improve code analysis, without using the compi ler to extract structured information.\nBroader Impact\nOur research supports the software naturalness hypothesis . This means that it may be possible to\ntransfer many recent advances in natural language understa nding into the software domain and im-\nprove source code understanding. The LM created by pre-trai ning on source code can thus be used\nfor different tasks such as VI, code completion [33], code re pair [36], as well as multi-modal tasks\ninvolving both natural language and source code [7]. These t asks have applications in the ﬁelds of\nsoftware security, developer tools and automation of softw are development and maintenance. Here\nwe focus on software security through the task of VI. Develop ment and improvement of end-to-\nend systems that can identify software vulnerabilities wil l make it easier to do the same in open\nsource software. As these tools become more available to eve ryone, it becomes imperative that\ndevelopers protect their code from malicious agents by inco rporating these tools in Continuous Inte-\ngration/Continuous Delivery pipelines to identify vulner abilities before code is exposed to others.\nReferences\n[1] A L L A M A N IS , M. The adverse effects of code duplication in machine learn ing models of\ncode. In Proceedings of the 2019 ACM SIGPLAN International Symposiu m on New Ideas,\nNew P aradigms, and Reﬂections on Programming and Software (2019), pp. 143–153.\n[2] C H E IRDA RI , F., A N D KA RA BAT IS , G. Analyzing false positive source code vulnerabilities\nusing static analysis tools. In 2018 IEEE International Conference on Big Data (Big Data)\n(2018).\n[3] C H E N , L., Y E , W., A N D ZH A N G, S. Capturing source code semantics via tree-based con-\nvolution over api-enhanced ast. In Proceedings of the 16th ACM International Conference\non Computing Frontiers (New Y ork, NY , USA, 2019), CF ’19, Association for Computing\nMachinery, p. 174–182.\n[4] C H E N , Z., K O M M RU S CH , S. J., T U FA N O, M., P O U CH E T , L., P O S H Y V A N Y K, D., A N D\nMO N P E RRU S , M. Sequencer: Sequence-to-sequence learning for end-to- end program repair.\nIEEE T ransactions on Software Engineering (2019).\n[5] D E V L IN , J., C H A N G, M.-W., L E E , K., A N D TO U TA N OV A, K. BER T: Pre-training of deep\nbidirectional transformers for language understanding. I n Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computa tional Linguistics: Human\nLanguage T echnologies, V olume 1 (Long and Short P apers) (Minneapolis, Minnesota, June\n2019), Association for Computational Linguistics, pp. 417 1–4186.\n[6] F ACE BO O K . Infer. https://fbinfer.com/, 2016.\n9\n[7] F E N G , Z., G U O, D., T A N G, D., D UA N , N., F E N G , X., G O N G, M., S H O U, L., Q IN , B., L IU ,\nT., J IA N G , D., E T A L . Codebert: A pre-trained model for programming and natural languages.\narXiv preprint arXiv:2002.08155 (2020).\n[8] G A D E L H A , M. R., S T E FFIN L O N G O , E., C O RD E IRO , L. C., F IS CH E R , B., A N D NICO L E ,\nD. A. Smt-based refutation of spurious bug reports in the cla ng static analyzer. In Proceed-\nings of the 41st International Conference on Software Engin eering: Companion Proceedings\n(2019), ICSE ’19, p. 11–14.\n[9] G O O G L E. Bert. github.com/google-research/bert/blob/master/README.md, 2018.\n[10] H E L L E N D O O RN , V . J., B IRD , C., B A RR , E. T., A N D AL L A M A N IS , M. Deep learning type\ninference. In Proceedings of the 2018 26th ACM Joint Meeting on European So ftware Engi-\nneering Conference and Symposium on the F oundations of Soft ware Engineering (New Y ork,\nNY , USA, 2018), ESEC/FSE 2018, Association for Computing Ma chinery, p. 152–162.\n[11] H E L L E N D O O RN , V . J., A N D DE V A N BU, P . Are deep neural networks the best choice for\nmodeling source code? In Proceedings of the 2017 11th Joint Meeting on F oundations of Soft-\nware Engineering (New Y ork, NY , USA, 2017), ESEC/FSE 2017, Association for Co mputing\nMachinery, p. 763–773.\n[12] H IN D L E , A., B A RR , E. T., S U, Z., G A BE L , M., A N D DE V A N BU, P . On the naturalness\nof software. In Proceedings of the 34th International Conference on Softwa re Engineering\n(2012), ICSE ’12, IEEE Press, p. 837–847.\n[13] H U S S A IN , Y ., H UA N G , Z., Z H O U, Y ., A N D WA N G, S. Deep transfer learning for source code\nmodeling. arXiv preprint arXiv:1910.05493 (2019).\n[14] I N C., S. S. Rough Audit T ool for Security. https://github.com/stgnet/rats, 2013.\n[15] J O H N S O N, B., S O N G, Y ., M U RP H Y-H IL L , E., A N D BOW D ID G E , R. Why don’t software\ndevelopers use static analysis tools to ﬁnd bugs? In Proceedings of the 2013 International\nConference on Software Engineering (2013), ICSE ’13, p. 672–681.\n[16] J O S H I, M., C H E N , D., L IU , Y ., W E L D , D. S., Z E T T L E M OY E R , L., A N D LE V Y, O. Spanbert:\nImproving pre-training by representing and predicting spa ns. T ransactions of the Association\nfor Computational Linguistics 8 (2020), 64–77.\n[17] K A NA D E , A., M A N IAT IS , P ., B A L A K RIS H NA N , G., A N D SH I, K. Pre-trained contextual\nembedding of source code. arXiv preprint arXiv:2001.00059 (2019).\n[18] K A RA M PAT S IS , R.-M., B A BII , H., R O BBE S , R., S U T TO N , C., A N D JA N E S, A. Big code !=\nbig vocabulary: Open-vocabulary models for source code. arXiv preprint arXiv:2003.07914\n(2020).\n[19] K E RN IG H A N , B. W., A N D RIT CH IE , D. M. The C Programming Language , 2nd ed. Prentice\nHall Professional T echnical Reference, 1988.\n[20] K U D O, T., A N D RICH A RD S O N , J. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In \"Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Processing: S ystem Demonstrations\" (\"Brus-\nsels, Belgium\", nov 2018), \"Association for Computational Linguistics\", pp. \"66–71\".\n[21] L I, Y ., T A RL OW, D., B RO CK S CH M ID T , M., A N D ZE M E L , R. Gated graph sequence neural\nnetworks. ICLR (2016).\n[22] L IA N G , H., S U N, L., W A N G, M., A N D X IN G YA N G, Y . Deep learning with customized\nabstract syntax tree for bug localization. IEEE Access 7 (2019), 116309–116320.\n[23] L O U IS KIM , R E BE CCA RU S S E L L. Draper VDISC Dataset - V ulnerability Detection in Source\nCode. https://osf.io/d45bw/, 2020.\n[24] M A L IK , R. S., P AT RA , J., A N D PRA D E L , M. Nl2type: Inferring javascript function types\nfrom natural language information. In 2019 IEEE/ACM 41st International Conference on\nSoftware Engineering (ICSE) (2019), pp. 304–315.\n[25] M A N N IN G , C. D., A N D SCH Ü T Z E , H. F oundations of Statistical Natural Language Process-\ning. MIT Press, Cambridge, MA, USA, 1999.\n[26] M ICRO S O F T . github.com/microsoft/gated-graph-neural-network-sam ples, 2015.\n10\n[27] M IKO L OV, T., S U T S K E V E R, I., C H E N , K., C O RRA D O , G. S., A N D DE A N , J. Distributed\nrepresentations of words and phrases and their composition ality. In Advances in Neural Infor-\nmation Processing Systems 26 , C. J. C. Burges, L. Bottou, M. W elling, Z. Ghahramani, and\nK. Q. W einberger, Eds. Curran Associates, Inc., 2013, pp. 31 11–3119.\n[28] NIST. Juliet test suite v1.3. https://samate.nist.gov/SRD/testsuite.php, 2017.\n[29] P A P PAG A RI , R., ˙ZE L A S KO , P ., V IL L A L BA , J., C A RM IE L , Y ., A N D DE H A K, N. Hierarchical\ntransformers for long document classiﬁcation. arXiv preprint arXiv:1910.10781 (2019).\n[30] P RA D E L , M., G O U S IO S , G., L IU , J., A N D CH A N D RA , S. T ypewriter: Neural type prediction\nwith search-based validation. arXiv preprint arXiv:1912.03768 (2019).\n[31] R AY, B., H E L L E N D O O RN , V ., G O D H A N E, S., T U, Z., B ACCH E L L I , A., A N D DE V A N BU, P .\nOn the \"naturalness\" of buggy code. In 2016 IEEE/ACM 38th International Conference on\nSoftware Engineering (ICSE) (2016), pp. 428–439.\n[32] R AY CH E V, V ., V E CH E V, M., A N D KRAU S E , A. Predicting program properties from “big\ncode”. In Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposiu m on Principles\nof Programming Languages (New Y ork, NY , USA, 2015), POPL ’15, Association for Comput-\ning Machinery, p. 111–124.\n[33] R AY CH E V, V ., V E CH E V, M., A N D YA H A V, E. Code completion with statistical language\nmodels. In In Proceedings of the 35th ACM SIGPLAN Conference on Program ming Language\nDesign and Implementation (2014), pp. 419–428.\n[34] S A N TO S , E. A., C A M P BE L L , J. C., P AT E L, D., H IN D L E , A., A N D AM A RA L , J. N. Syntax\nand sensibility: Using language models to detect and correc t syntax errors. In 2018 IEEE 25th\nInternational Conference on Software Analysis, Evolution and Reengineering (SANER) (2018),\npp. 311–322.\n[35] \"S E N N RICH , R., H A D D OW, B., A N D BIRCH , A. \"neural machine translation of rare words\nwith subword units\". In \"Proceedings of the 54th Annual Meeting of the Association f or Com-\nputational Linguistics (V olume 1: Long P apers)\" (\"Berlin, Germany\", aug 2016), \"Association\nfor Computational Linguistics\", pp. \"1715–1725\".\n[36] V A S IC , M., K A NA D E , A., M A N IAT IS , P ., B IE BE R , D., A N D SIN G H , R. Neural program\nrepair by jointly learning to localize and repair. arXiv preprint arXiv:1904.01720 (2019).\n[37] V A S WA N I, A., S H A Z E E R , N., P A RM A R , N., U S Z KO RE IT , J., J O N E S, L., G O M E Z , A. N.,\nKA IS E R , L. U., A N D PO L O S U K H IN , I. Attention is all you need. In Advances in Neural\nInformation Processing Systems 30 , I. Guyon, U. V . Luxburg, S. Bengio, H. W allach, R. Fergus,\nS. V ishwanathan, and R. Garnett, Eds. Curran Associates, In c., 2017, pp. 5998–6008.\n[38] W A N G, A., P RU K S ACH AT K U N , Y ., N A N G IA , N., S IN G H , A., M ICH A E L , J., H IL L , F., L E V Y,\nO., A N D BOW M A N , S. R. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. In NeurIPS (2019), pp. 3261–3275.\n[39] W E I, J., G OY A L, M., D U RRE T T , G., A N D DIL L IG , I. Lambdanet: Probabilistic type infer-\nence using graph neural networks. In International Conference on Learning Representations\n(2020).\n[40] W H E E L E R , D.A. Flawﬁnder. http://www.dwheeler.com/flawfinder, 2018.\n[41] W O L F, T., D E BU T, L., S A N H, V ., C H AU M O N D, J., D E L A N G U E , C., M O I, A., C IS TAC , P .,\nRAU LT, T., L O U F, R., F U N TOW ICZ , M., A N D BRE W, J. Huggingface’s transformers: State-\nof-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n[42] Y A M AG U CH I , F., G O L D E , N., A RP, D., A N D RIE CK , K. Modeling and discovering vulnera-\nbilities with code property graphs. In Proc. of IEEE Symposium on Security and Privacy (S&P)\n(2014).\n[43] Z H A N G, J., W A N G, X., Z H A N G, H., S U N, H., W A N G, K., A N D LIU , X. A novel neural\nsource code representation based on abstract syntax tree. I n Proceedings of the 41st Interna-\ntional Conference on Software Engineering (2019), ICSE ’19, IEEE Press, p. 783–794.\n[44] Z H A N G, J., W A N G, X., Z H A N G, H., S U N, H., W A N G, K., A N D LIU , X. A novel neural\nsource code representation based on abstract syntax tree. I n 2019 IEEE/ACM 41st International\nConference on Software Engineering (ICSE) (2019), pp. 783–794.\n11\n[45] Z H O U, Y ., L IU , S., S IOW, J., D U, X., A N D LIU , Y . Devign: Effective vulnerability identiﬁca-\ntion by learning comprehensive program semantics via graph neural networks. In Advances in\nNeural Information Processing Systems 32 (2019), Curran Associates, Inc., pp. 10197–10207.\n[46] FFMPeg. https://github.com/FFmpeg/FFmpeg, 2020.\n[47] QEMU. https://github.com/quemu/qemu, 2020.\n[48] Clang Static Analyzer. https://clang-analyzer.llvm.org, 2020.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8396017551422119
    },
    {
      "name": "Naturalness",
      "score": 0.6654345393180847
    },
    {
      "name": "Abstract syntax",
      "score": 0.615476131439209
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.596947193145752
    },
    {
      "name": "Natural language processing",
      "score": 0.5761500597000122
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5547347664833069
    },
    {
      "name": "Transformer",
      "score": 0.5454415082931519
    },
    {
      "name": "Source code",
      "score": 0.49377116560935974
    },
    {
      "name": "Language model",
      "score": 0.4668099880218506
    },
    {
      "name": "Natural language",
      "score": 0.4538199007511139
    },
    {
      "name": "Compiler",
      "score": 0.446443110704422
    },
    {
      "name": "Software",
      "score": 0.4425990879535675
    },
    {
      "name": "Programming language",
      "score": 0.41434794664382935
    },
    {
      "name": "Syntax",
      "score": 0.39161041378974915
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}