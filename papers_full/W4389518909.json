{
  "title": "Allies: Prompting Large Language Model with Beam Search",
  "url": "https://openalex.org/W4389518909",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2070768212",
      "name": "Sun Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097346119",
      "name": "Xiao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104159869",
      "name": "Yeyun Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009526818",
      "name": "Yan Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123654898",
      "name": "Daxin Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097638213",
      "name": "Linjun Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966337374",
      "name": "Nan Duan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4311731003",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4226157795",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4286903249",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4389523807",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4287185415",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385570481"
  ],
  "abstract": "With the advance of large language models (LLMs), the research field of LLM applications becomes more and more popular and the idea of constructing pipelines to accomplish complex tasks by stacking LLM API calls come true. However, this kind of methods face two limitations: narrow information coverage and low fault tolerance. In this work, we propose a novel method called ALLIES. Given an input query, ALLIES leverages LLMs to iteratively generate new queries related to the original query, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the original query, ALLIES captures and utilizes hidden knowledge that may not be directly obtainable through retrieval. We take zero-shot open-domain question answering (ODQA) as an application scene and evaluate ALLIES on the widely-used benchmarks, such as NQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES significantly outperforms other zero-shot baselines, indicating its effectiveness in tackling those challenges. Our code is available in https://github.com/microsoft/SimXNS/tree/main/ALLIES.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3794‚Äì3805\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nALLIES : Prompting Large Language Model with Beam Search\nHao Sun1‚àó, Xiao Liu2‚Ä†, Yeyun Gong2, Yan Zhang1, Daxin Jiang3, Linjun Yang3, Nan Duan2\n1 Peking University, 2 Microsoft Research Asia, 3 Microsoft\nsunhao@stu.pku.edu.cn, zhyzhy001@pku.edu.cn,\n{xiaoliu2,yegong,nanduan}@microsoft.com\nAbstract\nWith the advance of large language models\n(LLMs), the research field of LLM applications\nbecomes more and more popular and the idea\nof constructing pipelines to accomplish com-\nplex tasks by stacking LLM API calls come\ntrue. However, this kind of methods face two\nlimitations: narrow information coverage and\nlow fault tolerance. In this work, we propose a\nnovel method called ALLIES . Given an input\nquery, ALLIES leverages LLMs to iteratively\ngenerate new queries related to the original\nquery, enabling an iterative reasoning process.\nBy iteratively refining and expanding the scope\nof the original query, ALLIES captures and uti-\nlizes hidden knowledge that may not be directly\nobtainable through retrieval. We take zero-\nshot open-domain question answering (ODQA)\nas an application scene and evaluate ALLIES\non the widely-used benchmarks, such as NQ,\nWebQ and TriviaQA. The experimental results\ndemonstrate that ALLIES significantly outper-\nforms other zero-shot baselines, indicating its\neffectiveness in tackling those challenges. Our\ncode is available in https://github.com/\nmicrosoft/SimXNS/tree/main/ALLIES.\n1 Introduction\nWith the emergence of large language models\n(LLMs) [OpenAI, 2023, Scao et al., 2022, Taylor\net al., 2022, Chowdhery et al., 2022], researchers\nhave explored their potential to generate responses,\nincluding answering queries with the in-context\nlearning method [Brown et al., 2020]. In that\nmethod, the models are prompted with demon-\nstrations such as human-selected query-response\npairs [Shoeybi et al., 2019, Rae et al., 2021, Du\net al., 2022]. In this field, open-domain question\nanswering [Chen et al., 2017, Izacard and Grave,\n2021, 2020, Lazaridou et al., 2022] is an impor-\ntant and representative task that usually requires\n‚àóThis work was done during internship at MSRA.\n‚Ä†Xiao Liu is the corresponding author.\naccess to external corpora [Petroni et al., 2021]\nand utilizes a retriever component for knowledge\naugmentation [Ram et al., 2023, Shi et al., 2023,\nRashkin et al., 2021, Gao et al., 2022, Bohnet et al.,\n2022, Menick et al., 2022] to improve their ability\nto provide comprehensive and accurate answers.\nHowever, despite the advancements, these\nmethods still face two main limitations. (1)\nFirstly, narrow information coverage. When\nincorporating relevant information, the majority\nof these approaches only employ the query itself to\nfind or retrieve additional contextual information.\nNonetheless, there are instances where responding\nto the query necessitates implicit knowledge that\nis related to the query but cannot be easily found\nsolely using the given query. Consequently, the\nLLM may fail to acquire crucial information\nrequired for accurately responding to the query.\n(2) Secondly, low fault tolerance. Most of these\nmethods follow the pipeline style, consisting\nof unique steps calling LLM APIs to generate\nresponses to fulfill different needs in a single turn.\nIt means that the model is expected to provide the\ncorrect response in a single attempt. If an internal\nstep fails, either the whole pipeline will face the\nrisk of exception or the error will be propagated to\ndownstream steps. Consequently, if the model fails\nto find the necessary information or misinterprets\nthe question, it may produce an incorrect response.\nTo address the aforementioned limitations, we\npropose a novel approach called ALLIES that ap-\nplies a beam search strategy to generate responses.\nTo better elaborate the method, we take open-\ndomain question answering as the application scene\nand show an example of how ALLIES works in\nFigure 1. We adopt an interactive and iterative\nprocess. Initially, we generate additional queries\nby asking the LLM what other information they\nrequire, based on the existing query-evidence pair.\nThese generated queries serve as prompts for re-\ntrieving relevant evidence from external sources.\n3794\n‚Ä¶‚Ä¶Therefore, it can be inferred that the first driver‚Äôs license was not required until the introduction of the Motor Car Act in 1903.‚Ä¶‚Ä¶\nwhen was the first driver's license required?Original Question\n{‚Ä¶, In which country was the first driver‚Äôs license required?}\nComplement\nFind Evidence 1903, 0.8Answer & Score< Threshold,Continue!\n{‚Ä¶, When did the United Kingdom implement mandatory licensing for drivers and what was the minimum qualifying age?}\nThe first driver¬¥s license requirement was mandated on January 1, 1904, in the United Kingdom after the Motor Car Act 1903 received royal assent.‚Ä¶‚Ä¶\nJanuary 1, 1904, 0.9> Threshold, Terminate!\nBeam ùíä\nAnswer & ScoreFind Evidence\nBeamùíä+ùüè Answer & ScoreFind Evidence{‚Ä¶, Which state was the first to require registration of automobiles and in what year did this happen?}\nBased on the provided document, the first state to require registration of automobiles was New York in 1901. However.‚Ä¶‚Ä¶\n1901, 0.7< Threshold,Continue!\nComplementComplement\n‚Ä¶ ‚Ä¶\n‚Ä¶ ‚Ä¶ ‚Ä¶Beamùíä+ùüè\nFigure 1: The example of answering a question ‚Äúwhen was the first driver‚Äôs license required?‚Äù using ALLIES . The\ncorrect answer is ‚ÄúJanuary 1, 1904‚Äù.\nThe retrieved evidence is then added to the existing\nquery-evidence pair. Next, we employ the LLM to\nrespond to the initial query based on the augmented\nquery-evidence pairs. Subsequently, we solicit the\nLLM to score the response, taking into account the\nquery and the augmented query-evidence pair. This\nscoring process provides a measure of confidence\nin the generated response. The iterations continue\nuntil the score surpasses a predefined threshold,\nindicating a sufficiently confident answer or the\nmaximum depth of the tree traversal is reached.\nOnce either of these conditions is fulfilled, the pro-\ncess terminates, and the answer is outputted as the\nfinal result. Responding to the query using ALLIES\ncan be conceptualized as a tree traversal process,\nstarting from the root node and progressing towards\nthe leaf nodes, where each internal node in the tree\nrepresents a generated query.\nThe main advantages of our method are two\nfolds: (1) Firstly, we employ an extension strat-\negy that extends the original question to multiple\nrelevant questions, broadening the information cov-\nerage. This approach enables the LLM to gain a\ndeeper understanding of the complex question by\nfocusing on its constituent parts. By providing the\nLLM with more specific and targeted queries, we\nenhance their ability to comprehend and process\nthe question effectively. (2) Secondly, during the\niterative process, we employ a dynamic pruning\ntechnique that retains only the top B answers at\neach step. This increases the fault tolerance and\nrobustness of our model by allowing the LLM to\nmake mistakes during the reasoning process. Any\nerroneous answers can be replaced by alternative\nanswers, leading to more accurate and reliable re-\nsponses. This flexibility and adaptability contribute\nto the improved performance of our approach.\nWith the idea ofALLIES , we take zero-shot open-\ndomain question answering (ODQA) as an applica-\ntion scene and evaluate ALLIES in several popular\nbenchmarks. We conduct experiments on the NQ,\nTriviaQA and WebQ datasets. The results demon-\nstrate that ALLIES significantly outperforms sev-\neral representative baselines while maintaining an\nacceptable cost. The case study further confirms\nthe aforementioned advantages of our method.\nIn summary, our main contributions can be sum-\nmarized as follows:\n1. We propose ALLIES , which leverages a\nbeam search strategy for response generation.\nWithin this framework, we adopt an interac-\ntive and iterative process to enhance the accu-\nracy and robustness of the responses.\n2. By extending the original question into multi-\nple relevant questions and employing a dy-\nnamic pruning technique, we improve the\nunderstanding of complex questions and in-\ncrease the model‚Äôs robustness. This allows for\nmistakes and alternative answers, resulting in\nmore accurate and robust responses.\n3. By taking zero-shot ODQA as an application\nscene, results on the NQ, TriviaQA and WebQ\n3795\ndatasets demonstrate the significant outperfor-\nmance of our method compared to baseline\napproaches. The case study further validates\nthe advantages of our approach.\n2 Related Work\n2.1 Open-Domain Question Answering\nOpen-domain question answering is a task that\naims to provide answers to questions without re-\nlying on specific context. This task can be cate-\ngorized into two settings: the open-book setting\nand the closed-book setting. In the open-book set-\nting, models [Chen et al., 2017, Izacard and Grave,\n2021, 2020] typically consist of a retriever and a\nreader component. The retriever‚Äôs role is to re-\ntrieve relevant information from a corpus such as\nWikipedia [Chen et al., 2017, Izacard and Grave,\n2021] or web pages [Lazaridou et al., 2022, Nakano\net al., 2021], while the reader focuses on answering\nthe question based on the retrieved information.\nIn the closed-book setting, models have no ac-\ncess to external corpus and have to rely on its\nmodel parameters to store all the information. Re-\ncent works find that large-scale language mod-\nels like T5 [Raffel et al., 2020] can already an-\nswer questions without access to the external cor-\npus. However, small-scale language models like\nRoBERTa [Liu et al., 2019] or GPT-2 [Radford\net al., 2019] still face challenges in accurately an-\nswering questions in this setting.\n2.2 Large Language Model Enhanced\nQuestion Answering\nIn recent times, there has been a shift towards uti-\nlizing large language models (LLMs) for question\nanswering [Chowdhery et al., 2022, Du et al., 2022,\nLiu et al., 2021]. This research can be broadly cat-\negorized into two lines of work. The first line of\nwork focuses on preprocess methods [Borgeaud\net al., 2022, Ram et al., 2023, Shi et al., 2023],\nwhich involve obtaining relevant documents and\nthen utilizing LLMs to generate answers. Within\nthis line of work, there are two main approaches.\nRetrieve-then-read methods [Ram et al., 2023, Shi\net al., 2023] employ a retrieval model to retrieve rel-\nevant documents, while generate-then-read meth-\nods [Yu et al., 2022, Sun et al., 2022] fully leverage\nthe capabilities of LLMs. Furthermore, researchers\nhave demonstrated that combining generation and\nretrieval can lead to further gains [Yu et al., 2022].\nThe second line focuses on posthoc methods\nOriginal Query\nBeam \nInitialization\nBeam \nExpansion\nBeam Pruning\nBeam \nTermination\nBeam 1\nBeam ùëñ\nBeam ùëñ +1\nBeam ùëñ +1\nBeam ùê∑\nThe final response with the highest score\nIf score meets \nthe requirements\nFigure 2: The abstract process of A LLIES .\n(like works on QA with attribution) [Rashkin et al.,\n2021, Gao et al., 2022, Bohnet et al., 2022, Menick\net al., 2022], which involve generating an answer\nusing an LLM and then refining it with the help of\na verifier and a retriever. The retrieved documents\nin the second stage serve as explanations for the\ngenerated answer.\n3 Main Idea\nThe main idea of ALLIES is an interactive and it-\nerative process based on the widely-used search\nalgorithm, beam search1. We use a tuple with five\nslots to represent a state, which is the element of\na beam. Each state ‚ü®q, Q, E, r, s‚ü©consists of the\noriginal query q, the set of historical query comple-\ntions Q, the set of historical external evidences E,\nthe current response r, and the estimated score s\naccording to the current state. Assume the maxi-\nmum search depth is D, as illustrated in Figure 2,\nthere are four main stages of ALLIES .\n3.1 Beam Initialization\nIn the beginning, we initialize the beam by asking\nthe LLM to answer the query directly and by an-\nswering the query based on retrieved evidence. The\nretrieved evidence is obtained by first retrieving re-\nlated documents using the original query and then\nsummarizing the documents. The generated tuples\nwill be added to the beam.\n1https://archive.org/details/DTIC_ADA049288\n3796\nAlgorithm 1The process of generating the response to a given query using ALLIES .\nHyperparameters: The maximum number K of generated queries, the maximum depth D of extension, the number N of\ndocuments from retrieval, the score threshold S, and the beam size B.\nInput: A query q.\nOutput: The answer ÀÜa.\n1: Clear the initial beam S0 = ‚àÖ\n2: Answer the query q with the model knowledge a0 = Answer(q, ‚àÖ, ‚àÖ).\n3: Score the initial answer s0 = Score(q, ‚àÖ, ‚àÖ, a0).\n4: Add the current tuple to the initial beam S0 = S0 ‚à™{‚ü®q, ‚àÖ, ‚àÖ, a0, s0‚ü©}. ‚ñ∑ The first seed.\n5: Retrieve a evidence e1 = Retrieve(qori, q, N).\n6: Answer the query q with the model knowledge a1 = Answer(q, {q}, {e1}).\n7: Score the initial answer s1 = Score(q, {q}, {e1}, a1).\n8: Add the current tuple to the initial beam S0 = S0 ‚à™{‚ü®q, {q}, {e1}, a1, s1‚ü©}. ‚ñ∑ The second seed.\n9: for extension depth d in 1 ‚ÜíD do ‚ñ∑ Extending within the depth.\n10: Clear the beam for the current depth Sd = ‚àÖ.\n11: for each tuple in the previous beam ‚ü®q, Q, E, a, s‚ü©‚ààS d‚àí1 do ‚ñ∑ Iterate the previous tuples.\n12: Find the extended queries Q‚Ä≤= Ask(q, Q, E, K).\n13: for each extended query q‚Ä≤‚ààQ‚Ä≤do ‚ñ∑ Try each possible extension.\n14: Retrieve a evidence e‚Ä≤= Retrieve(qori, q‚Ä≤, N).\n15: Try to answer with all the evidences a‚Ä≤= Answer(q, Q‚à™{q‚Ä≤}, E‚à™{ e‚Ä≤}).\n16: Score the answer s‚Ä≤= Score(q, Q‚à™{q‚Ä≤}, E‚à™{ e‚Ä≤}, a‚Ä≤).\n17: Add the current extended tuple to the beam Sd = Sd ‚à™{‚ü®q, Q‚à™{q‚Ä≤}, E‚à™{ e‚Ä≤}, a‚Ä≤, s‚Ä≤‚ü©}.\n18: end for\n19: end for\n20: Trim the beam Sd by keeping only B tuples with largerest scores. ‚ñ∑ Prune the beam.\n21: if a tuple ‚ü®q, Q, E, a, s‚ü©‚ààS d meets s ‚â•S then ‚ñ∑ Examine the exit.\n22: SD = Sd.\n23: Exit the loop.\n24: end if\n25: end for\n26: Find the tuple ‚ü®q, Q, E, ÀÜa, smax‚ü©‚ààS D with the largest score smax and ÀÜa is the final answer.\n3.2 Beam Expansion\nDuring the beam search process, we iteratively pop\nout one element from the front of the beam. For\neach element, we generate queries using the Ask\nFunction. Then, for each generated query, we re-\ntrieve relevant evidence and ask the LLM to answer\nthe query based on both the retrieved evidence and\nthe reasoning history. The LLM scores the gener-\nated answers based on the reasoning history, and\nthe newly formatted tuples are added to the end of\nthe beam.\n3.3 Beam Pruning\nAt the end of each search depth, we rank the newly\ngenerated answers and keep only top B answers.\n3.4 Beam Termination\nIf the highest-ranking answer in the beam has\na score exceeding the predefined threshold, the\nsearch process terminates, and the answer is out-\nputted. Otherwise, the process continues. If none\nof the elements in the beam reaches the thresh-\nold, we output the highest-scoring answer when\nthe search reaches the maximum depth.\n4 Detailed Approach for ODQA\nIn this section, we present the application of AL-\nLIES in ODQA, whose algorithm is illustrated in\nAlgorithm 1. There are four key functions used in\nALLIES , each serving a specific purpose. The cor-\nresponding prompts are illustrated in Appendix C.\n4.1 Answering Function Answer(q, Q, E)\nThis function takes as input the original query q,\npreviously generated queries Q, and corresponding\nretrieval evidence E. It constructs a reasoning his-\ntory {‚ü®q1, e1‚ü©, ‚ü®q2, e2‚ü©, ...}by extracting qi ‚ààQ\nand ei ‚ààE. The function then asks the LLM to\nreason over the reasoning history and provide an\nanswer to the original query.\n4.2 Asking Function Ask(q, Q, E, K)\nGiven the query q, previously generated queries\nQ, corresponding retrieval evidence E, and the\nmaximum number of queries to be generated\nK, this function constructs a reasoning history\n{‚ü®q1, e1‚ü©, ‚ü®q2, e2‚ü©, ...}by extracting qi ‚ààQ and\nei ‚ààE. The LLM is then asked to reason over the\nreasoning history and determine what additional\ninformation it requires to answer the question. The\nfunction outputs the generated queries.\n3797\n4.3 Retrieval Function Retrieve(qori, q, N)\nGiven the original queryqori, query q, and the max-\nimum number of documents to be retrieved N, this\nfunction uses a dense retriever to retrieve the top-N\nmost similar documents. The LLM is then asked\nto extract the most useful information from the\ndocuments and summarize them, providing a con-\ncise version of the retrieved information. We can\nalso use LLM to directly generate a background\ndocument like GENREAD [Yu et al., 2022] as an al-\nternative and we call this function Retrieve\n‚Ä≤\n(qori).\n4.4 Scoring Function Score(q, Q, E, a)\nGiven the original query q, previously gen-\nerated queries Q, corresponding retrieval evi-\ndence E, and the generated answer a from the\nLLM, this function constructs a reasoning history\n{‚ü®q1, e1‚ü©, ‚ü®q2, e2‚ü©, ...}by extracting qi ‚ààQ and\nei ‚ààE. The LLM is then asked to consider the rea-\nsoning history and assess the probability that the\ncandidate answer is the true answer. The function\noutputs a score representing the confidence in the\ngenerated answer.\n5 Experiment\n5.1 Experimental Setting\nIn this section, we conduct experiments on three\nopen-domain question-answering (QA) datasets:\nNQ [Kwiatkowski et al., 2019], TriviaQA [Joshi\net al., 2017], and WebQ [Berant et al., 2013]. Since\nwe focus on zero-shot ODQA, we utilize only the\ncomplete test sets of NQ and WebQ. To reduce\ncosts, we randomly selected 1000 samples from the\nTriviaQA test set for evaluation purposes. Original\ndetailed statistics regarding these three datasets can\nbe found in Appendix A.\nWe evaluate the performance using two met-\nrics: the exact match (EM) score and the F1 score.\nSpecifically, a predicted answer is considered cor-\nrect only if its normalized form matches any of the\nnormalized versions of the answers provided in the\nanswer list. The F1 score measures the word over-\nlap between the normalized version of the predicted\nanswer and the answers in the provided answer list.\n5.2 Implementation\nWe employ GPT-3.5-Turbo hosted by Azure Ope-\nnAI services as our large language model (LLM).\nAs for the retriever component, we conduct sepa-\nrate finetuning for the NQ, TriviaQA, and WebQ\ndatasets using their respective training sets. The\narchitecture and performance of the dense retrieval\ncomponent can be found in Appendix D. For the\nretrieval corpus, we use the Wikipedia dump from\nDec. 20, 2018 as our retrieval corpus, encompass-\ning a collection of 21,015,324 documents.\n5.3 Baselines\nWe compare our method with three groups of zero-\nshot QA baselines.\nThe first group comprises baselines that utilize\na retriever in their approach. This includes models\nsuch as BM25 + InstructGPT, Contriever + Instruct-\nGPT, Google + InstructGPT, and DPR + Instruct-\nGPT. These models employ a retriever to retrieve\nrelevant information, which is then used by Instruct-\nGPT for answer generation. We obtained the re-\nported performance numbers for these baselines\nfrom GENREAD [Yu et al., 2022].\nThe second group consists of baselines that do\nnot utilize a retriever in their approach. This group\nincludes models such as GPT-3 [Brown et al.,\n2020], InstructGPT [Yu et al., 2022], FLAN [Wei\net al., 2021], GLaM [Du et al., 2022], and GEN-\nREAD [Yu et al., 2022]. The reported performance\nnumbers for these baselines are obtained from their\nrespective original papers.\nThe third group consists of models that we im-\nplemented ourselves, including directly answer,\nretrieve-then-answer, GENREAD [Yu et al., 2022],\nself-Ask [Press et al., 2022], and MCR [Yoran et al.,\n2023]. Directly answer refers to the utilization of\nthe LLM to directly answer the question. Retrieve-\nthen-answer involves retrieval before answering,\nwhere we experimented with different numbers of\nretrieved documents and reported their correspond-\ning performance, which is the simplified version\nof ALLIES without beam search. We implemented\nGENREAD, self-Ask, and MCR based on their\nopen-source code. However, we evaluate MCR\nonly on the NQ dataset due to its high API cost.\nTo ensure fairness among the baselines, we set the\nretrievers and LLM configurations to be the same.\n5.4 Main Results\nWe present the main results of our zero-shot exper-\niments in Table 1. Based on these results, several\nobservations can be made:\n(1) Among the methods that utilize a retriever,\nthe choice of the retriever has a significant impact\non the model‚Äôs performance. This indicates that the\nquality of the retrieved documents plays a crucial\nrole in determining the overall system performance.\n3798\nMethod NQ TriviaQA WebQ\nEM F1 EM F1 EM F1\n*Method w/ retriever, reported by [Yu et al., 2022].\nBM25 + InstructGPT 19.7 - 52.2 - 15.8 -\nContriever + InstructGPT 18.0 - 51.3 - 16.6 -\nGoogle + InstructGPT 28.8 - 58.8 - 20.4 -\nDPR + InstructGPT 29.1 - 53.8 - 20.2 -\n*Method w/o retriever.\nGPT-3 [Brown et al., 2020] 14.6 - - - 14.4 -\nInstructGPT [Yu et al., 2022] 20.9 - 57.5 - 18.6 -\nFLAN [Wei et al., 2021] 18.6 - 55.0 - - -\nGLaM [Du et al., 2022] 24.7 - - - 19.0 -\n*Reimplmentation.\nDirectly Answer 20.8 32.5 49.2 60.8 20.8 37.5\nRetrieve-Then-Answer (Top-1) 27.6 37.1 49.1 57.9 19.9 33.8\nRetrieve-Then-Answer (Top-5) 29.4 40.7 52.7 62.0 18.5 34.8\nRetrieve-Then-Answer (Top-10) 28.2 39.5 52.4 61.6 17.4 32.9\nGENREAD [Yu et al., 2022] 31.1 44.8 59.3 70.7 19.1 36.9\nSelf-Ask [Press et al., 2022] 26.4 36.5 59.4 68.5 15.1 29.5\nMCR [Yoran et al., 2023] 27.1 35.7 - - - -\nALLIES 38.0 47.8 61.4 70.8 28.2 45.6\nTable 1: Zero-shot open-domain QA performance.\nMethod NQ WebQ\nEM F1 EM F1\nw/o Evidence 22.44 34.54 19.78 36.54\nRetrieve&Summary 38.00 47.82 27.26 43.13\nGENREAD 37.98 49.47 28.20 45.49\nTable 2: Ablation study results on NQ and WebQ.\n(2) Among the methods that do not use a re-\ntriever, GENREAD achieves the highest perfor-\nmance. This demonstrates the effectiveness of the\ngenerate-then-read pipeline, where the model gen-\nerates background documents based on its own\nknowledge without relying on external corpus.\n(3) Our implemented baselines, such as MCR\nand self-Ask, may not perform as well as expected.\nThis is mainly because these methods heavily rely\non result parsing, which limits their generalizability\nto other applications.\n(4) Our proposed method, ALLIES , outperforms\nall existing baselines and achieves the highest per-\nformance on all datasets. This confirms the effec-\ntiveness of our model and demonstrates its superior-\nity in open-domain question answering tasks. Ad-\nditionally, our method relies less on result parsing,\nmaking it more generalizable to other applications.\n5.5 Ablation Study\nIn ALLIES , we utilize LLMs to ask questions and\nretrieve evidence based on those questions. To in-\nvestigate the effects of the evidence, we conduct\nMethod NQ TriviaQA WebQ\nRetrieve-Then-Answer 59.2% 64.6% 70.0%\nALLIES 69.6% 72.9% 82.0%\nTable 3: Query complementation analysis.\nablations by removing the evidence and using dif-\nferent types of evidence, as shown in Table 2.\nBased on the results, we draw several conclu-\nsions: (1) When the evidence is removed, we only\nprovide the LLM with related queries without any\nbackground information. In this case, the model‚Äôs\nperformance drops significantly, which confirms\nthat incorporating evidence into the model can\ngreatly improve its understanding of the query. (2)\nWhen using the LLM-generated background docu-\nment (GENREAD), we observe that our model\nachieves slightly better results compared to re-\ntrieval & summary. This finding aligns with the\nobservations made in GENREAD [Yu et al., 2022].\nThe improved performance can be attributed to the\nfact that LLMs have seen these related documents\nduring pretraining, and the generated documents\nare more specific and refined.\n5.6 Query Complementation Analysis\nBy iteratively generating new queries to comple-\nment the original query, our ALLIES is capable of\nexpanding the information coverage of the original\nquery and capturing hidden knowledge that may\nnot be directly obtainable through retrieval with\n3799\nMethod Retrieval Times API Times Tokens Per API Tokens Per Query\nDirectly Answer 0 1 54 1 √ó54 = 54\nGENREAD [Yu et al., 2022] 0 1 342 1 √ó342 = 342\nSelf-Ask [Press et al., 2022] 0 1 490 1 √ó490 = 490\nRetrieve-Then-Answer (Top-5) 1 1 744 1 √ó744 = 744\nALLIES (GENREAD) 0 19 290 19 √ó290 = 5510\nALLIES (Retrieval&Summary) 5 19 352 19 √ó352 = 6688\nMCR [Yoran et al., 2023] 12 12 3029 12 √ó3029 = 36348\nTable 4: The effectiveness analysis of ALLIES .\nthe original query. To verify this, we conduct a\nquery complementation analysis that compares the\nretrieval results of retrieve-then-answer and AL-\nLIES . Specifically, we record the percentage of\nretrieval results containing the ground truth answer\nand present the findings in Table 3.\nFrom the result, we can find that the retrieval\nresults of ALLIES outperform those of retrieve-\nthen-answer across all datasets, which verifies the\neffectiveness of ALLIES . By iteratively generating\nnew queries, we can expand the knowledge scope\nof the retrieval results, leading to a more compre-\nhensive understanding of the original query and\nnaturally producing better answers.\n5.7 Effectiveness Analysis\nIn ALLIES , the use of multiple iterations of retrieval\nand generation may introduce additional costs. To\nanalyze its effectiveness, we utilize the complete\nset of questions from the NQ dataset to conduct the\neffectiveness analysis, which systematically com-\npares the effectiveness of several methods.\nAs shown in Table 4, we can have the following\nconclusions: (1) Multi-turn QA methods, including\nALLIES and MCR, incur higher model inference\ncosts compared to single-turn QA methods such\nas Directly Answer, GENREAD, Self-Ask, and\nRetrieve-Then-Answer. This increase in cost is\nprimarily due to the multiple API calls involved.\n(2) Among the multi-turn QA methods, although\nALLIES requires more API calls, the token con-\nsumption per API is significantly lower than that\nof MCR, resulting in 1/6 inference cost of MCR.\nThe higher token consumption per API in MCR\ncan be attributed to the demonstration, which con-\nsumes a substantial number of tokens. (3) Gen-\nerally, single-turn QA methods have lower token\ncosts but exhibit lower model performance. In con-\ntrast, ALLIES achieves significantly better model\nperformance while maintaining an acceptable to-\nken cost compared to MCR, thus demonstrating the\neffectiveness of our method.\n1 2 3 4\nBeam Size\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500Value\nF1-Score\nEM\n1 2 3 4\nBeam Depth\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500Value\nF1-Score\nEM\nFigure 3: Performance comparison w.r.t. hyper-\nparameters on NQ dataset.\n5.8 Human Evaluation\nIn this section, we conducted a human evaluation\nto assess the accuracy of the scores generated by\nLLMs in our scoring function. We randomly se-\nlected 100 samples for score calculation and manu-\nally verified the generated scores.\nOur findings indicate that 93 percent of the gen-\nerated scores align with the requirements for score\ncalculation. This validation confirms the rationale\nbehind using LLMs to calculate the scores. How-\never, we also observed some rare cases where two\nanswers could both potentially address the ques-\ntion, but one of them was more accurate. In these\ncases, the LLMs assigned the same score to both\nanswers, potentially leading to the selection of the\nless accurate answer. This issue can be attributed\nto the coarse nature of the prompt used for scor-\ning, which can only assess the general relevance\nscore. To address this issue, one possible solution\nfor future work is to calculate the scores using an\nensemble-and-vote approach. This would involve\nasking LLMs to rank all possible answers instead\nof scoring them individually, which would poten-\ntially achieve more accurate and reliable scores.\n5.9 Hyper-parameter Study\nBeam size B and beam depth D are two important\nhyper-parameters in our method. We study their\neffects by changing one parameter while fixing\n3800\nQuestion: Who led the soldiers in ending the raid on the harper‚Äôs ferry arsenal?\nAnswer: [Brevet Colonel Robert E. Lee,First Lieutenant Israel Greene]\nGenerated Query:\n- What was the name of the leader who led the soldiers in ending the raid on the Harper‚Äôs Ferry arsenal?\n- Who was the overall commander of the soldiers who led the operation to retake the arsenal at Harpers Ferry?\nRetrieved Evidence:\n- The soldiers who led the operation to retake the arsenal at Harpers Ferry were under the overall command of\nColonel Robert E. Lee.\n- Colonel Robert E. Lee was in overall command of the operation to retake the arsenal. It is possible that he may\nhave played a role in leading the soldiers to end the raid.\nGenerated Answer with Score:\n- Answer 1: Colonel Robert E. Lee. Score: 0.8 ‚úî - Answer 2: First Lieutenant Israel Greene. Score: 0.7\nModel Predictions:\nDirectly Answer: John Brown Retrieve-Then-Answer: John Brown ALLIES: Colonel Robert E. Lee ‚úî\nQuestion: When was the first driver‚Äôs license required?\nAnswer: 1 January 1904\nGenerated Query:\n- In which country was the first driver‚Äôs license required?\n- When did the UK implement mandatory licensing for drivers and what was the minimum qualifying age?\nRetrieved Evidence:\n- The first driver¬¥s license requirement was mandated on January 1, 1904, in the United Kingdom after the Motor\nCar Act 1903 received royal assent. The minimum qualifying age was set at 17, and every car owner...\n- The first formal driving test in the UK was introduced with the Road Traffic Act 1934, which made compulsory\ntesting for all new drivers. Prior to this, UK driving licenses were introduced by the Motor Car Act 1903...\nGenerated Answer with Score:\n- Answer 1: January 1, 1904. Score: 0.9 ‚úî - Answer 2: 1903. Score: 0.8\nModel Predictions:\nDirectly Answer: 1903 Retrieve-Then-Answer: July 1913 ALLIES: 1 January 1904 ‚úî\nTable 5: Case studies of the process of ALLIES .\nother parameters and observing the performance\ntrends, which are shown in Figure 3.\nStudy on Beam SizeB. Beam size refers to the\nnumber of questions we keep at each layer during\nanswer searching. From the results, we observe\nthat the performance reaches its peak when the\nbeam size (B) is set to 2. Values smaller or larger\nthan this threshold lead to performance degradation.\nThis is primarily because a larger beam size pro-\nvides the model with more opportunities to make\nmistakes. However, when the beam size is too large,\nthe model struggles to effectively rank the multiple\ncandidates and select the best answer. Addition-\nally, an increase in beam size also incurs additional\ncomputational costs.\nStudy on Beam DepthD. Beam depth refers\nto the maximum depth our model can reach dur-\ning answer searching. From the results, we find\nthat the performance change during beam depth\ntuning is relatively small. This is mainly due to\nthe early stop mechanism we implemented, where\nthe answer searching can terminate before reach-\ning the maximum search depth if the answer score\nsurpasses the threshold. However, we also observe\nthat when the beam depth is too large (e.g., 4),\nthe model‚Äôs performance starts to decline. We be-\nlieve this is mainly because, in most cases, a beam\ndepth of 2 provides the model with sufficient back-\nground information. Increasing the beam depth\nbeyond that only introduces more noisy informa-\ntion, which may complicate the generation of the\ncorrect answer for the LLM.\n5.10 Case Study\nIn this section, we provide examples that illus-\ntrate the reasoning process of our ALLIES method,\nwhich is shown in Table 5. From these examples,\nwe draw the following conclusions:\n(1) The generated queries in our method are\nmore specific and focused compared to the origi-\nnal query. This specificity improves the accuracy\nof the retrieval process, resulting in more accurate\nand relevant retrieved evidence. Consequently, the\ngenerated answers are of higher quality.\n(2) During the answer generation process, there\nmight be instances where wrong answers are ini-\ntially predicted. However, our scoring function\neffectively assigns lower scores to these wrong an-\nswers based on the reasoning history. As a result,\nthe final output is the correct answer. This demon-\nstrates the robustness of our method in handling\npotential mistakes and effectively filtering out in-\ncorrect answers.\n3801\n6 Conclusion\nIn this paper, we introduceALLIES , a novel method\nthat addresses the limitations of using large lan-\nguage models (LLMs) for complex tasks. By lever-\naging LLMs to generate related queries iteratively,\nALLIES enables iterative reasoning and expands the\noriginal query‚Äôs scope to capture hidden knowledge.\nWe evaluate ALLIES in zero-shot open-domain\nquestion answering and demonstrate its superiority\nover other baselines on benchmarks. As for future\nwork, we plan to apply ALLIES in other complex\ntasks such as mathematical reasoning and so on.\nLimitations\nIn this work, we propose an effective response gen-\neration method ALLIES . The limitations of the\nproposed method are as follows:\n(1) The computational cost of ALLIES is rela-\ntively high due to the need for multiple API calls\nand document retrieval. This can limit its practical-\nity in resource-intensive scenarios or systems with\nlimited computational resources.\n(2) The operation of the model is based on the de-\nsigned prompt. When applied to a new application\nscenario, crafting effective prompts may require\nadditional time and effort from users.\nReferences\nOpenAI. Gpt-4 technical report. OpenAI, 2023.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon,\nMatthias Gall√©, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Beno√Æt Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Lauren√ßon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. BLOOM: A 176b-\nparameter open-access multilingual language model.\nCoRR, abs/2211.05100, 2022.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. Galac-\ntica: A large language model for science. CoRR,\nabs/2211.09085, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. Palm: Scaling language model-\ning with pathways. arXiv preprint arXiv:2204.02311,\n2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems,\n33:1877‚Äì1901, 2020.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter\nlanguage models using model parallelism. arXiv\npreprint arXiv:1909.08053, 2019.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. Scaling language models: Meth-\nods, analysis & insights from training gopher. arXiv\npreprint arXiv:2112.11446, 2021.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam:\nEfficient scaling of language models with mixture-\nof-experts. In International Conference on Machine\nLearning, pages 5547‚Äì5569. PMLR, 2022.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. Reading wikipedia to answer open-domain\nquestions. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1870‚Äì1879, 2017.\nGautier Izacard and Edouard Grave. Leveraging pas-\nsage retrieval with generative models for open do-\nmain question answering. In EACL 2021, pages 874‚Äì\n880, 2021.\nGautier Izacard and Edouard Grave. Distilling knowl-\nedge from reader to retriever for question answering.\nIn International Conference on Learning Representa-\ntions, 2020.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115, 2022.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. Kilt: a benchmark for knowledge intensive lan-\nguage tasks. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2523‚Äì2544, 2021.\n3802\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. In-context retrieval-augmented language\nmodels. arXiv preprint arXiv:2302.00083, 2023.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652, 2023.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. Measuring attribution in natural language\ngeneration models. arXiv preprint arXiv:2112.12870,\n2021.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\ncent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\net al. Rarr: Researching and revising what language\nmodels say, using language models. arXiv preprint\narXiv:2210.08726, 2022.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\nAttributed question answering: Evaluation and mod-\neling for attributed large language models. arXiv\npreprint arXiv:2212.08037, 2022.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. Teaching lan-\nguage models to support answers with verified quotes.\narXiv preprint arXiv:2203.11147, 2022.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, et al. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. Exploring the limits of trans-\nfer learning with a unified text-to-text transformer. J.\nMach. Learn. Res., 21(140):1‚Äì67, 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners. OpenAI blog,\n1(8):9, 2019.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck,\nPeter West, Ronan Le Bras, Yejin Choi, and Han-\nnaneh Hajishirzi. Generated knowledge prompt-\ning for commonsense reasoning. arXiv preprint\narXiv:2110.08387, 2021.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. Im-\nproving language models by retrieving from trillions\nof tokens. In International conference on machine\nlearning, pages 2206‚Äì2240. PMLR, 2022.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. Generate rather than\nretrieve: Large language models are strong context\ngenerators. arXiv preprint arXiv:2209.10063, 2022.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. Recitation-augmented language mod-\nels. arXiv preprint arXiv:2210.01296, 2022.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. Natural questions: A benchmark for\nquestion answering research. TACL 2019, pages 452‚Äì\n466, 2019.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. Triviaqa: A large scale distantly super-\nvised challenge dataset for reading comprehension.\nIn ACL 2017, pages 1601‚Äì1611, 2017.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. Semantic parsing on freebase from question-\nanswer pairs. In EMNLP 2013, pages 1533‚Äì1544,\n2013.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are\nzero-shot learners. In International Conference on\nLearning Representations, 2021.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. Measuring and nar-\nrowing the compositionality gap in language models.\narXiv preprint arXiv:2210.03350, 2022.\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel\nDeutch, and Jonathan Berant. Answering questions\nby meta-reasoning over multiple chains of thought.\narXiv preprint arXiv:2304.13007, 2023.\nJeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-\nscale similarity search with gpus. IEEE Trans-\naction‚Äôs on Big Data , 7(3):535‚Äì547, 2021. doi:\n10.1109/TBDATA.2019.2921572. URL https://\ndoi.org/10.1109/TBDATA.2019.2921572.\n3803\nA Data Statistics\nThe statistics of used datasets are shown in Table 8.\nB Hyper-parameters\nThe detailed hyper-parameters are shown in Ta-\nble 6.\nC Detailed Prompts of the Functions\nC.1 Answering Function Answer(q, Q, E)\nGiven the following query-evidence pair:\n{query-evidence pair}\nPlease refer to the query-evidence pair above, an-\nswer the following question with just one entity.\nQuestion: {query}\nThe answer is:\nC.2 Asking Function Ask(q, Q, E, K)\nGiven the question:\n{query}\nand following query-evidence pair:\n{query-evidence pair}.\nPlease generate some questions that can help an-\nswer the given question with the following con-\nstraints:\n1.You should output no more than k questions.\n2.You should directly output the ranked sub-\nquestions based on their importance.\n3.The generated questions should be diverse and\nfocus on different aspects of the given question.\n4.You should output in the following format:\nRanked Questions:\n1. [Question 1] . . .\nC.3 Retrieval Function Retrieve(qori, q, N)\nGiven the original question:\n{query}\nand the provided document:\n{doc}\noutput the factual information from the evidence\nthat is relevant to the question:\nC.4 Retrieval Function Retrieve\n‚Ä≤\n(qori)\nGenerate a short background document from\nWikipedia to answer the given question: {query}\nC.5 Scoring Function Score(q, Q, E, a)\nGiven the question:\n{query}\nand the candidate answer:\n{answer}\nand the Query-evidence pair:\n{query-evidence pair}\nrefer to the query-evidence pair below and utilize\nyour own reasoning ability to assess the probability\nthat the candidate answer is the true answer.\nPlease provide a number between 0 and 1 as the\noutput, following the guidelines below:\nIf the probability is between 0 and 0.3, it signifies\nthat the model has substantial evidence to suggest\nit is an incorrect answer.\nIf the probability is between 0.3 and 0.5, it sug-\ngests that the model leans towards considering it\nan incorrect answer, but lacks concrete evidence.\nIf the probability is between 0.5 and 0.7, it indicates\nthat the model leans towards considering it a correct\nanswer, but lacks concrete evidence.\nIf the probability is greater than 0.7, it signifies that\nthe model has substantial evidence to suggest it is\nthe correct answer.\nIf the candidate answer doesn‚Äôt provide clear so-\nlution to the question, the probability should be\n0.\nThe score is:\nD Dense Retriever\nDual Encoder. The predominant architecture cur-\nrently utilized for dense retrieval is known as the\ndual encoder. This architecture employs dense vec-\ntor representations, denoted as q and d, to encode\nqueries and documents, respectively. The similarity\nscores are then computed using the inner product\nas follows:\ns(q, d) =EQ(q)T ¬∑ED(d) (1)\nwhere EQ(¬∑) and ED(¬∑) refer to the query encoder\nand document encoder, respectively. To leverage\nthe embeddings, existing solutions typically em-\nploy approximate nearest neighbor (ANN) search\nalgorithms such as FAISS [Johnson et al., 2021].\nPerformance of Dual Encoder.The pre-trained\nlanguage model (PLM) used in the training of re-\ntrievers is COCONDENSER 2. The performances of\nDEs on different datasets can be found in Table 7.\n2Luyu/co-condenser-marco in huggingface.\n3804\nParameter NQ TriviaQA WebQ\nThreshold 0.8 0.8 0.8\nBeam Size 2 3 3\nBeam Depth 2 1 2\nRetrieval Number 2 - -\nExpand Question Number 2 2 3\nEvidence Type Retrieval GENREAD GENREAD\nLLM API GPT-3.5-Turbo GPT-3.5-Turbo GPT-3.5-Turbo\nTable 6: Hyper-parameters for ALLIES .\nDataset R@1 R@5 R@20 R@50 R@100 R@1k MRR@10 MAP@1k\nNQ 46.43 68.86 80.28 84.40 86.86 92.06 56.03 21.96\nTriviaQA 58.34 73.44 80.71 84.04 85.95 89.55 64.71 25.03\nWebQ 52.31 72.10 80.41 83.76 85.63 90.80 60.72 21.50\nTable 7: The results of the dual encoders on different datasets.\nDatasets Train Valid Test\nNQ [Kwiatkowski et al., 2019] 79,168 8,757 3,610\nTriviaQA [Joshi et al., 2017] 78,785 8,837 11,313\nWebQ [Berant et al., 2013] 3,478 300 2,032\nTable 8: Datasets splits and statistics.\n3805",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.838982105255127
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5146878361701965
    },
    {
      "name": "Process (computing)",
      "score": 0.4903562068939209
    },
    {
      "name": "Scope (computer science)",
      "score": 0.46949294209480286
    },
    {
      "name": "Information retrieval",
      "score": 0.4597568213939667
    },
    {
      "name": "Tree (set theory)",
      "score": 0.44948774576187134
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.4440486431121826
    },
    {
      "name": "Pipeline (software)",
      "score": 0.4363834857940674
    },
    {
      "name": "Intersection (aeronautics)",
      "score": 0.4170669913291931
    },
    {
      "name": "Programming language",
      "score": 0.2629329562187195
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}