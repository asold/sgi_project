{
  "title": "Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical texts.",
  "url": "https://openalex.org/W3212869160",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2338036277",
      "name": "Sophie I. Arnoult",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2521613504",
      "name": "Lodewijk Petram",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153862313",
      "name": "Piek Vossen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2251463950",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2251160521",
    "https://openalex.org/W3016005396",
    "https://openalex.org/W3031359449",
    "https://openalex.org/W68566686",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2889470921",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035390927"
  ],
  "abstract": "Pretrained language models like BERT have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These models are well known for their crosslingual performance, but have also shown competitive in-language performance on some tasks. We consider monolingual and multilingual models from the perspective of historical texts, and in particular for texts enriched with editorial notes: how do language models deal with the historical and editorial content in these texts? We present a new Named Entity Recognition dataset for Dutch based on 17th and 18th century United East India Company (VOC) reports extended with modern editorial notes. Our experiments with multilingual and Dutch pretrained language models confirm the crosslingual abilities of multilingual models while showing that all language models can leverage mixed-variant data. In particular, language models successfully incorporate notes for the prediction of entities in historical texts. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand: multilingual models lose their advantage when confronted with more semantical tasks.",
  "full_text": "Proceedings of LaTeCH-CLfL 2021, pages 21–30\nPunta Cana, Dominican Republic (Online), November 11, 2021.\n21\nBatavia asked for advice. Pretrained language models for Named Entity\nRecognition in historical texts.\nSophie Arnoult\nVrije Universiteit Amsterdam\ns.i.arnoult@\nvu.nl\nLodewijk Petram\nHuygens ING\nlodewijk.petram@\nhuygens.knaw.nl\nPiek Vossen\nVrije Universiteit Amsterdam\np.t.j.m.vossen@\nvu.nl\nAbstract\nPretrained language models like BERT have\nadvanced the state of the art for many NLP\ntasks. For resource-rich languages, one has the\nchoice between a number of language-speciﬁc\nmodels, while multilingual models are also\nworth considering. These models are well\nknown for their crosslingual performance, but\nhave also shown competitive in-language per-\nformance on some tasks. We consider mono-\nlingual and multilingual models from the per-\nspective of historical texts, and in particular for\ntexts enriched with editorial notes: how do lan-\nguage models deal with the historical and edi-\ntorial content in these texts? We present a new\nNamed Entity Recognition dataset for Dutch\nbased on 17th and 18th century United East\nIndia Company (VOC) reports extended with\nmodern editorial notes. Our experiments with\nmultilingual and Dutch pretrained language\nmodels conﬁrm the crosslingual abilities of\nmultilingual models while showing that all lan-\nguage models can leverage mixed-variant data.\nIn particular, language models successfully in-\ncorporate notes for the prediction of entities\nin historical texts. We also ﬁnd that multilin-\ngual models outperform monolingual models\non our data, but that this superiority is linked\nto the task at hand: multilingual models lose\ntheir advantage when confronted with more se-\nmantical tasks.\n1 Introduction\nPretrained language models (Peters et al., 2018;\nRadford et al., 2018; Devlin et al., 2019) have re-\ncently advanced the state-of-the-art in many NLP\ntasks, providing deep contextual language repre-\nsentations and ease of deployment. BERT (De-\nvlin et al., 2019) has proven particularly successful,\ncombining the attention-based contextual represen-\ntations of Transformers (Vaswani et al., 2017) with\na simple ﬁne-tuning procedure, allowing to deploy\nquickly to any sequence or document classiﬁcation\ntask.\nBERT has given birth to a myriad of variants,\ndiffering by their training procedure, model size\nor language. Resource-rich languages in particu-\nlar are spoilt for choice. Dutch for instance has\ntwo main monolingual models: the BERT-based\nBERTje (de Vries et al., 2019) and RobBERT (De-\nlobelle et al., 2020), itself based on RoBERTa (Liu\net al., 2019), a revision of BERT’s training proce-\ndure. But multilingual models like mBERT (De-\nvlin, 2018) and XLM-R (Conneau et al., 2020) are\nalso applicable. These models, which are trained on\n104, respectively 100 languages at once, perform\nwell on crosslingual transfer (Pires et al., 2019; Wu\nand Dredze, 2019; Conneau et al., 2020). Pires et al.\n(2019) notably show that mBERT learns generic\nrepresentations over different input languages and\nscripts. This makes multilingual models partic-\nularly interesting for historical texts, not only be-\ncause these contain noncontemporary language, but\nalso because they exhibit language variation due\nto language change over long periods of time or\nunstandardized spelling. But what when historical\ntexts are accompanied by modern notes? How do\nmonolingual and multilingual models compare on\nthe historical and editorial parts of such texts, and\non their combination?\nIf we only consider modern notes, it is not given\nthat monolingual models outperform multilingual\nmodels on their training language. Notwithstand-\ning possible differences in genre, the nature of\nthe linguistic task at hand is an important factor\nin determining whether a monolingual or mul-\ntilingual model is more appropriate. de Vries\net al. (2019) and Delobelle et al. (2020) show for\nDutch that mBERT is competitive with monolin-\ngual models on POS tagging (on Universal Depen-\ndencies POS), while Dutch models perform bet-\nter at semantic tasks like Semantic Role Labelling\nor language-speciﬁc tasks like agreement resolu-\ntion. For Named Entity Recognition (NER), re-\nsults appear mixed, as both BERTje and RobBERT\n22\nperform in-between mBERT and a version opti-\nmized by Wu and Dredze (2019). How semantic or\nlanguage-speciﬁc does a task need to be for mono-\nlingual models to perform better than multilingual\nmodels in historical texts with editorial notes?\nIn this article, we consider these questions in the\ncontext of NER in 17th and 18th century Dutch\ntexts. We introduce a new NER dataset based on\nthe General Letters of the United East India Com-\npany (VOC)1. These ofﬁcial letters were written\nto the board of the VOC between 1610 and 1795.\nThey report on the activities of the VOC (trade, con-\nﬂicts, intelligence) in and around Indonesia and up\nto Japan and South Africa. Between 1960 and 2017,\na selection of this corpus, spanning the time period\n1610-1767, was transcribed and edited by the Huy-\ngens Institute for the History of the Netherlands\n(Huygens ING) and its predecessors. Annotating\nentities in these letters is part of a larger effort to\nfacilitate historical research on these texts.\nOur dataset2 consists of 24.5k entities, with a\nrepartition of 40%/60% between historical text and\neditorial notes, where the historical text covers the\nperiod 1621 to 1760. We introduce a semantic com-\nponent in this dataset by distinguishing metonymi-\ncal uses of locations (which represent almost half\nof all entities). This allows us to compare two NER\nvariants: a standard one, where locations form a\nsingle entity type; and a more semantic one, where\nmodels additionally must distinguish locations in\nagent-like roles, as in for instance, Batavia asked\nfor advice.\nWe compare the performance of monolingual\nand multilingual models on both NER variants\nthrough BERTje (de Vries et al., 2019), Rob-\nBERT (Delobelle et al., 2020), mBERT (Devlin,\n2018) and XLM-R (Conneau et al., 2020). On\nstandard NER, we ﬁnd that multilingual models\noutperform monolingual models on both histori-\ncal text and modern notes. In crosstext ﬁnetuning\nand evaluation, we conﬁrm the strong crosslingual\ntransfer ability of multilingual models in the con-\ntext of historical language variation. We also show\nthat notes and historical text are complementary\nfor NER, as all pretrained models, multilingualand\nmonolingual, beneﬁt from their combination. On\nthe more semantic-oriented NER task, we ﬁnd that\nmonolingual and multilingual models perform com-\n1http://resources.huygens.knaw.nl/\nvocgeneralemissiven/index_html_en\n2Dataset and code are available at https://github.\ncom/cltl/voc-missives\npetitively, with RobBERT and mBERT performing\nbest. This conﬁrms the importance of the semantic\nnature of a task as a factor in choosing between\nmonolingual and multilingual models.\n2 Annotations\nThe labelset consists of ﬁve base labels—LOC,\nORG, PER, REL and SHP—for the entity types lo-\ncations, organisations, persons, religions and ships.\nFollowing (Benikova et al., 2014), this labelset is\nextended with secondary labels of the form Xderiv\nor Xpart for expressions derived from entity names\nby grammatical derivation (as with for instance, the\nlocation Banda and the derived Bandanezen3) or\ncomposition (as with Java and Javakofﬁe4). The\nlabelset is extended with a GPE label (geopolitical\nentities) to distinguish metonymical use of location\nnames.\n2.1 Data selection\nThe data selected for annotations consist of letters\nspread out through the General Letters corpus5.\nIn editing these letters, Huygens ING transcribed\nparts of the letters, and summarized other parts,\nwhich appear as in-text paragraph notes in the dig-\nital version of the corpus 6. Editorial comments\nappear as footnotes.\nWe use the post-OCR version of the corpus 7\nmade available in Text-Fabric (Roorda, 2016,\n2019). This is a clean text overall, with some er-\nrors in character recognition (e.g., Ib instead of lb,\nS* Malo instead of St Malo), page-to-text format-\nting (Chris -toffel for Christoffel) or tokenization\n(Schippers , instead of Schippers,).\nWe selected 25 letters for annotation, keeping\nthe historical text and editorial notes of these let-\nters apart to facilitate their separate use. Our ﬁnal\ndataset consists of 43 documents: 22 historical text\ndocuments and 21 editorial notes documents8.\n3Bandanese\n4Java coffee\n5The original corpus contains fourteen volumes, the ﬁrst\nthirteen of which were available for annotations.\n6http://resources.huygens.knaw.nl/\nretroboeken/generalemissiven/#page=0&\naccessor=toc&view=homePane\n7https://github.com/annotation/\napp-missieven, version 0.8.1\n8A number of documents were unfortunately lost because\nof server errors. From the 25 letters, 18 are annoted for both\ntext and notes, 4 for text only, and 3 for notes only.\n23\ntext notes total\ntokens 233k 201k 434k\nentities 9.57k 14.9k 24.5k\ndensity (%) 4.1 7.4 5.6\nentity types\nLOC 3927 6525 10.5k\nLOCderiv 2227 1414 3641\nLOCpart 14 21 35\nGPE 43 854 897\nORG 994 995 1989\nORGpart 8 41 49\nPER 1665 3337 5002\nREL 10 4 14\nRELderiv 159 73 232\nRELpart 7 10 17\nSHP 520 1652 2172\nTable 1: Annotation counts\n2.2 Annotation counts\nSummary statistics are provided in Table 1. Com-\npared to the historical text, the editorial notes show\na higher density of entities per token, and they refer\nless to peoples (through derived forms of locations\nand religions), and more to locations, persons and\nships. This agrees with notes commenting on pri-\nmary named entities. In contrast, the skewed dis-\ntribution of geopolitical entities between text and\nnotes appears to be a stylistic artefact: metonymi-\ncal use of locations is in fact concentrated in three\nnotes documents (from volumes 11 and 12), which\ncontain 88.0% of allgeopolitical entities (for 14.8%\nof all entities and 11.3% of all tokens). While we\nconsider here the editorial notes as being linguis-\ntically homogeneous, their writing spanned more\nthan ﬁfty years and they are at least stylistically\nvaried.\n2.3 Guidelines\nOur annotations focus on named entities and their\nderived forms, aiming at annotations that are both\nconsistent for NER and adaptable for extensions.\nWe present here the main considerations in anno-\ntating different types of entities.\nLocations Only the named entity is marked as\nan entity in compositional location names (rif van\n[Luang]LOC ), except when the full expression is\ntreated as a given name. This is in particular the\ncase with coastal areas, which were often the only\nknown part of an island for the VOC:\n(1) [Java’s Oostkust]LOC , [Sumatra’s\nWestkust]LOC , [Malabar]LOC en\n[Ceylon]LOC 9\nThe Coast then refers to a speciﬁc coastal area, like\nin this note:\n(2) de [Custe]LOC is hier en elders de [Kust van\nCoromandel]LOC 10\nPractically, we annotate compositional location\nnames as entities when their constituents are capi-\ntalized:\n(3) expeditie ter [Oostkust van Java]LOC 11\n(4) nabij de noordkust van [Java]LOC 12\nDerived (adjectival) forms of locations are an-\nnotated with LOCderiv and composed names with\nLOCpart:\n(5) de [Bandanezen]LOCderiv\n(6) Sijn [Portugeesche]LOCderiv Majesteyt\n(7) [Javakofﬁe]LOCpart\nLocation names with a semantic role of agent,\ntheme, experiencer, benefactive, or of trading or\npolitical actor are marked with a distinct GPE label\n(we use this label for our semantic-oriented NER\nexperiments; GPE entities are relabelled as LOC\nfor standard NER experiments).\n(8) [Batavia]GPE heeft advies gevraagd13\n(9) er is aan [Coromandel]GPE opgedragen14\n(10) de inkomsten van [Malakka]GPE 15\n(11) de vrede met [Frankrijk]GPE 16\n9Java’s Eastcoast, Sumatra’s Westcoast, [. . . ]\n10The Coast refers here and elsewhere to the Coast of Coro-\nmandel\n11expedition to the Eastcoast of Java\n12near the north coast of Java\n13Batavia has asked for advice\n14Coromandel has been ordered to\n15the revenues of Malakka\n16peace with France\n24\nOrganisations Compositional organisation\nnames are annotated as a whole in principle (Rade\nvan India, College van Schepenen). An exception\nis made for Kamer and Compagnie which are used\nproductively in the context of the VOC:\n(12) de [kamers]ORG [Delft]LOC ,\n[Rotterdam]LOC en [Hoorn]LOC\n(13) de respective [Comp.en]ORG van\n[Engelant]LOC ende [Nederlant]LOC\n(14) wegen d’[Engelse]LOCderiv en de\n[Nederlantse]LOCderiv [Comp.en]ORG\n(15) de [France]LOCderiv [Comp.ie]ORG van [S*\nMalo]LOC\nCommon nouns denoting organisations are not an-\nnotated17:\n(16) het [Siamse]LOCderiv hoff18\nPersons Person names are annotated without\nqualiﬁers or titles19:\n(17) ingenieur [Albert Legrand]PER\n(18) Radja [Simorang]PER\nReligions Religion names and derived forms are\na small but relevant group of entities in the context\nof the VOC:\n(19) concurrentie van [Engelsen]LOCderiv en\n[Moren]RELderiv 20\nWe systematize the annotation of religious groups\nby including non-capitalized names:\n(20) De [heidense]RELderiv bewoners [. . . ] de\n[Moslimse]RELderiv kustbewoners21\nShips Ships form a considerable part of the an-\nnotations. They are annotated without determiners\nor qualiﬁers:\n(21) de [Loenderveen]SHP\n17These are historically relevant while falling out of a lin-\nguistic deﬁnition of named entities. We might extend organi-\nsations to terms like hof, gouvernement or comptoir in a future\nversion of the dataset.\n18the Siamese court\n19Following (Benikova et al., 2014). The time and space\ncovered by the General Letters corpus entails however a great\nvariety of personal titles, that can be hard to tell from person\nnames. We might extend our labelset with titles in the future.\n20competition from Englishmen and Moors\n21the pagan inhabitants [. . . ] the Moslim coastal inhabi-\ntants\n2.4 Annotation process\nAnnotations were performed with Inception (Klie\net al., 2018) by two annotators, following guide-\nlines inspired from (Benikova et al., 2014). To\ndisambiguate difﬁcult cases, the annotators could\nrely on the indices of persons, locations and ships\naccompanying each volume of the corpus, as well\nas on a glossarium22. Annotations were performed\nfor the most part on raw text23 with the help of a\ngazetteer compiled from the indices; a few docu-\nments were preannotated with either string match-\ning from the gazetteer or with a preliminary NER\nsystem.\nAgreement was measured halfway through the\nannotation process, on three documents (two with\nhistorical text and one with editorial notes). We\nmeasure inter-annotator agreement with F score:\nlike Brandsen et al. (2020) and Deleger et al.\n(2012), we question the use of Cohen’s kappa for\nNER, as it is unclear how chance agreement should\nbe deﬁned for NER.\nTable 2 provides F scores for the text and notes\nand details cases of agreement and disagreement\nbetween both annotators. To this end, we ﬁrst pair\nup annotations with a same span to isolate cases\nof agreement and cases of label disagreement. We\nthen attempt to pair remaining annotations, and\ncount annotations that overlap with annotations\nof the other annotator; those that do are cases of\nspan disagreement, while the remaining cases are\nmentions that only one of the annotators identiﬁes\nas entities.\nThe analysis of disagreements presented next\nrevealed some inherent difﬁculties with annotating\nhistorical texts, errors in the annotations, but also\nunclarities in the guidelines. We used this analysis\nto streamline guidelines and correct annotations24.\nDisagreement analysis Most cases of label dis-\nagreement result from the confusion between per-\nson names and location or ship names, or between\nlocation names and derived forms thereof. Person\nnames cannot always be distinguished from loca-\ntion or ship names by the linguistic context alone,\nespecially with earlier language variants. In this\nexample for instance:\n22http://resources.huygens.knaw.nl/\nvocglossarium/VocGlossarium/zoekvoc\n23Extracted from TEI post-OCR ﬁles. Annotations on this\ntext were later ported to the Text-Fabric release of the corpus.\n24Corrections were performed by one of the authors, after\nannotations were gathered.\n25\ntext notes total\nF1 88.5 92.1 90.8\nentities 492/484 910/877 1402/1361\nagreeing 432 823 1255\ndisagreeing\n- label 10 24 34\n- span 39/39 24/25 63/64\n- entity 11/3 39/5 50/8\nTable 2: Inter-annotator agreement. Pairs of counts cor-\nrespond to the respective annotation counts of each an-\nnotator.\n(22) vroeg de sengadji van Lamakera\nMauwadasje25\nthe indices tell us thatLamakera refers to a location\nand Mauwadasje to a person (and the glossarium\nthat sengadji denotes the head of a village or dis-\ntrict). Linguistic context alone is not enough to\ndistinguish person from location, and the form of\nthe apposition of Mauwadasje is also unusual for\ncontemporary Dutch.\nMost cases of span disagreement26 concern in-\nﬁx abbreviations like Comp.e for Compagnie and\nqualiﬁers like Edele (noble) for Compagnie, person\ntitles like Khan or Radja and location qualiﬁers as\nin engte van Pambenaar27 or Noord-Celebes.\nMost cases of entity disagreement are due to\nomissions. Other cases concern whether or not to\nannotate: political actors in general like gouverne-\nment; derived forms of location names not denoting\npeoples, as in het Engels schip; compositional lo-\ncation names like Oostkust; metonymical uses of\nlocations28.\n3 Experimental Setup\n3.1 Models\nBERTje (de Vries et al., 2019) is a Dutch model\ntrained on 12GB of data from mixed, mainly\ncontemporary sources. The model is structurally\n25asked the sengadji of Lamakera Mauwadasje\n26Annotation counts differ for the notes because one entity\n(Castor en Pollux) was marked as two entities by the second\nannotator.\n27Pambenaar’s strait\n28These were ﬁrst marked by double LOC/ORG labels. In\nthese cases, the annotators would disagree on adding an ORG\nlabel.\nequivalent to BERT base, with 12 Transformer\nlayers of size H = 768 and 12 attention heads.\nThe tokenizer is based on Sentence-Piece (Kudo\nand Richardson, 2018), and has a vocabulary\nsize of 30k. Unlike BERT, BERTje is trained on\na Sentence-Order Prediction objective next to\nMasked Language Modelling (MLM).\nRobBERT (Delobelle et al., 2020) is a Dutch\nmodel trained on 39GB of data from the OS-\nCAR corpus (Ortiz Suárez et al., 2019). The\nmodel is structurally equivalent to BERT base\nwhile following the training procedure of\nRoBERTa (Liu et al., 2019), with a dynamic MLM\nobjective, and a tokenizer based on byte-level\nBPE following (Radford et al., 2019), with a\nvocabulary size of 40k (we consider RobBERT v2).\nmBERT (Devlin, 2018) is a multilingual model\ntrained on Wikipedia dumps of the 104 languages\nbest represented on Wikipedia (we consider the\ncased version); the model is trained without\nknowledge of which languages are input, while\ndata are sampled to compensate for less repre-\nsented languages. The vocabulary is shared across\nlanguages and built with Word-Piece (Wu et al.,\n2016) and has a size of 110k. The model is\nstructurally equivalent to BERTbase, and is trained\nwith an MLM and Next Sentence Prediction\nobjective.\nXLM-R (Conneau et al., 2020) is a multilingual\nmodel trained on a CommonCrawl corpus of 2.5TB,\nwith a vocabulary of 250k tokens built with the\nSentence-Piece tokenizer. Like RoBERTa (and\nRobBERT), XLM-R is trained with an MLM ob-\njective, taking sequences of tokens of ﬁxed length\nas input instead of sentences. We use XLM-Rbase,\nwhich is structurally equivalent to BERTbase.\n3.2 Data\nThe data are split in train/dev/test sets so as to\nobtain comparable splits for the notes and historical\ntext while keeping the notes and text of a same\nletter together. Besides, we selected the test set\nfor the historical text from the earliest part of the\ncorpus, so as to make it more challenging for a\nmodel trained and validated on later text; as the\nearlier letters contain little notes, the test set for\nthe notes is complemented with a letter for which\nonly the notes were annotated. Data selection for\n26\ntraining 1658-1730, 1743-44, 1759-60\nvalidation 1731-40, 1741 t, 1750n\ntest 1621-43, 1647 n, 1752n\nTable 3: Year ranges of selected letters for each data\nsubset. t: text only; n: notes only\ntext notes all\ntraining\ntokens 170k 160k 330k\nentities 7192 11.5k 18.7k\navg. length 56.6 89.4 68.9\nvalidation\ntokens 30.3k 24.4k 54.7k\nentities 981 1991 2972\navg. length 106.8 71.9 87.8\ntest\ntokens 32.3k 16.7k 49.0k\nentities 1401 1436 2837\navg. length 142.9 32.5 66.3\nTable 4: Data split. Average sequence lengths are in\ntokens, before subword tokenization.\nthe notes also accommodates for GPE labels being\nconcentrated in three letters (from 1744, 1750 and\n1752). Data selection is summarized in Table 3.\nThe data are tokenized with the IXA-pipe tok-\nenizer (Agerri et al., 2014). We do not segment\nsentences with the tokenizer, but take paragraphs\nand separate notes as text units, splitting sequences\nlonger than 256 subword units for each language\nmodel. This is motivated practically by the to-\nkenizer being to greedy (the letters abound with\nabbreviations), but we also believe that working\nat the paragraph level may be beneﬁcial as it pro-\nvides more context for NER. Summary statistics\nare provided in Table 4.\nFor both standard and semantic-oriented experi-\nments, rare labels are mapped to lexically-related\nlabels: REL and REL part to RELderiv, LOCpart\nto LOCderiv, ORGpart to ORG. Additionally, GPE\nlabels are remapped to LOC for standard NER.\n3.3 Hyperparameter settings\nAll models are ﬁne-tuned with the HuggingFace\nTransformers library (Wolf et al., 2020). Adam\nparameters are: β1 = 0.9; β2 = 0.999, ϵ= 10−8.\nWe used training batch sizes of 16, a learning rate\nof 5 · 10−5 and no weight decay. We did not per-\nform hyperparameter search, but we ﬁne-tuned all\nmodels with three different seeds (1, 10 and 100),\nand we report average values with standard devia-\ntion across the three runs. For each run, we selected\nthe best model within 4 epochs, taking loss on the\nvalidation set as a criterion and keeping checkpoint\nmodels every 100 steps when ﬁne-tuning on the\ntext or notes only, and every 200 steps when ﬁne-\ntuning on all the data29. All experiments were run\non a single Tesla P100 GPU. Fine-tuning on all\nthe data takes around 15 minutes per run for all\nmodels.\n4 Results\nWe ﬁrst present results on standard NER, compar-\ning the in-text (text or notes) and crosstext per-\nformance of monolingual and multilingual models,\nand assessing the reciprocal contribution of text and\nnotes. We end with results on semantic-oriented\nNER.\n4.1 In-text performance\nTable 5 presents results with ﬁne-tuning and evalu-\nation on the same part of the dataset (text, notes or\nboth). Multilingual models perform better than\nmonolingual models on average, albeit with a\nsmaller margin for the notes and the entire dataset.\nResults are higher for all models on the notes than\non the text; this may have to do with a more ho-\nmogeneous language in the notes, but also with\ndifferences in sequence lengths, which are long\non average in the text’s testset and short in the\nnotes30. The length of text sequences coupled with\nour working at the paragraph level may also ex-\nplain the higher unstability of results of BERTje\nand mBERT, as these were trained on the sentence\nlevel.\n4.2 Crosstext performance\nCrosstext results are presented in Table 6. These\nconﬁrm the superiority of multilingual models in\ncrosslingual transfer: while all models ﬁne-tuned\non text loose a few points when predicting on notes,\n29Fine-tuning takes about 550 steps for the notes, 850 for\nthe text and 1400 for all the data.\n30This resulted by chance from our data split. While it\nmight be interesting to try another split resulting in more even\nsequence lengths, the other constraints we followed, combined\nwith the limited number of letters, limit possibilities in that\ndirection.\n27\ntext notes all\nBERTje 84.4 (2.5) 91.8 (0.4) 90.4 (0.5)\nRobBERT 86.1 (0.8) 92.7 (0.5) 91.1 (0.9)\nmBERT 88.7 (3.2) 93.4 (0.7) 92.2 (0.6)\nXLM-Rbase 88.2 (0.6) 93.4 (0.4) 92.3 (0.3)\nTable 5: In-text ﬁne-tuning and evaluation (F1 scores\nand standard deviation)\ntext notes\nBERTje text 84.4 (2.5) 82.8 (2.9)\nnotes 68.7 (3.3) 91.8 (0.4)\nRobBERT text 86.1 (0.8) 84.1 (1.5)\nnotes 71.9 (1.8) 92.7 (0.5)\nmBERT text 88.7 (3.2) 86.8 (3.6)\nnotes 79.5 (1.2) 93.4 (0.7)\nXLM-Rbase\ntext 88.2 (0.6) 84.7 (0.6)\nnotes 77.9 (2.9) 93.4 (0.4)\nTable 6: Crosstext ﬁne-tuning and evaluation. Models\nﬁne-tuned on the text or notes (rows) and evaluated on\ntext and notes (columns).\nmultilingual models ﬁne-tuned on notes are more\nrobust when predicting on text, their performance\ndropping then by about 15 points compared to 20\npoints or more for monolingual models.\n4.3 Reciprocal contribution of notes and text\nTable 7 shows the effect of adding out-of-text data\nfor ﬁne-tuning. We see that notes are informative\nfor NER in historical text, as performance on the\ntext increases by 2 to 3 points when ﬁne-tuning on\nall data. The reverse does not hold: performance\non the notes also increases when ﬁne-tuning on\nall data, but only minimally (by 0.2 point for all\nmodels except BERTje, for which scores improve\nby 1.1 point). These trends hold for all models,\nmonolingual or multilingual, pointing to the gen-\neral contextual value of notes for named entities\ncombined with a general ability of pretrained lan-\nguage models to exploit context—while monolin-\ngual models are trained on a single language, they\nretain the ability to adapt to a variety of language\nvariants in ﬁne-tuning.\nTable 8 details the contribution of notes and text\nper label for XLM-R. Notes are informative for all\nin-text all\nBERTje text 84.4 (2.5) 87.8 (0.8)\nnotes 91.8 (0.4) 92.9 (0.6)\nRobBERT text 86.1 (0.8) 89.3 (1.1)\nnotes 92.7 (0.5) 92.9 (0.7)\nmBERT text 88.7 (3.2) 90.8 (0.7)\nnotes 93.4 (0.7) 93.6 (0.6)\nXLM-Rbase\ntext 88.2 (0.6) 91.0 (0.5)\nnotes 93.4 (0.4) 93.6 (0.1)\nTable 7: Reciprocal contribution of notes and text: ﬁne-\ntuning on in-text data (text or notes) or on all data, and\npredicting on text or notes (rows).\ntext notes\nin-text all in-text all\nLOC 93.0 (0.6) 94.4 (0.4) 96.7 (0.3) 96.7 (0.1)\nLOCd 92.0 (0.3) 93.1 (0.1) 89.6 (0.4) 92.5 (1.0)\nORG 87.5 (1.9) 90.2 (1.6) 89.8 (1.3) 90.9 (2.2)\nPER 74.1 (4.2) 79.3 (2.2) 89.4 (1.2) 88.4 (1.1)\nRELd 92.3 (1.9) 92.1 (2.3) 39.1 (36.7) 90.0 (5.8)\nSHP 60.0 (5.0) 76.2 (1.0) 86.8 (0.4) 86.4 (1.5)\nall 88.2 (0.6) 91.0 (0.5) 93.4 (0.4) 93.6 (0.1)\nTable 8: Detailed contribution of notes and text ﬁne-\ntuning data to text and notes predictions, for XLM-\nRbase.\nentities except religions and derived forms, which\nare poorly represented in the notes (results for the\nRELderiv label are also very unstable in the notes,\nas witnessed by the high standard deviation). The\nhighest gains are obtained for persons and ships,\nwhich are also much better represented in the notes\nthan the text. But notes are also informative for en-\ntities that are well represented in the text (locations,\nderived forms thereof and organisations). Recip-\nrocally, the text is informative for entities that are\ncomparatively better represented: derived forms of\nlocations, organisations and religions.\n4.4 Locations as political actors\nWe end with a semantic oriented NER experi-\nment, where models additionally must distinguish\nbetween metonymical (GPE) and standard uses\n(LOC) of locations. Results are presented in Ta-\nble 9. Adding this semantic orientation makes it\n28\n-GPE +GPE\nBERTje 90.4 (0.5) 88.4 (0.5)\nRobBERT 91.1 (0.9) 89.0 (0.5)\nmBERT 92.2 (0.6) 89.1 (1.2)\nXLM-Rbase 92.3 (0.3) 88.4 (2.2)\nTable 9: Distinguishing metonymical uses of locations.\n-GPE: no distinction; +GPE: distinction between LOC\nand GPE labels. Models ﬁne-tuned and evaluated on\nall the data. Boldface marks the two best performing\nmodels per case.\nmBERT RobBERT\n-GPE +GPE -GPE +GPE\nGPE - 82.0 (0.3) - 84.0 (1.5)\nLOC 95.7 (0.6) 93.0 (0.8) 94.8 (0.5) 92.2 (0.7)\nLOCd 92.7 (0.4) 91.0 (1.3) 92.2 (0.4) 91.1 (1.1)\nORG 90.6 (3.4) 90.5 (1.5) 88.6 (1.7) 88.7 (1.8)\nPER 84.7 (0.6) 84.0 (1.6) 83.8 (0.4) 82.7 (1.4)\nRELd 93.4 (3.9) 78.1 (23.8) 91.3 (4.1) 92.8 (1.9)\nSHP 82.1 (1.9) 77.0 (1.6) 79.6 (4.4) 80.6 (0.3)\nall 92.2 (0.6) 89.1 (1.2) 91.1 (0.9) 89.0 (0.5)\nTable 10: Distinguishing metonymical uses of loca-\ntions: detailed label scores.\na harder task for all models. Monolingual models\nhowever perform relatively better, with RobBERT\nalmost equalling mBERT as best performing model.\nMultilingual models suffer a larger drop in per-\nformance than monolingual models, coupled with\nmore instable results.\nWe compare detailed label scores for RobBERT\nand mBERT in Table 10. GPE-label distinction\nnegatively affects prediction of all entity types for\nmBERT. In contrast, RobBERT performs better\nthan mBERT on GPE prediction, and beneﬁts from\nthe GPE/LOC distinction for the prediction of a few\nentity types (derived forms of religions and ships).\nWe observe similar trends with XLM-R base and\nBERTje, respectively (for BERTje, scores improve\nfor RELderiv and LOCderiv by 1 point).\n5 Conclusion\nWe have introduced a new Dutch dataset for\nNamed Entity Recognition, consisting of Early\nModern Dutch VOC letters and modern edito-\nrial notes. Comparing monolingual and multilin-\ngual pretrained language models, we conﬁrm the\nstronger crosslingual abilities of multilingual mod-\nels, while showing that both monolingual and mul-\ntilingual models can leverage on mixed language\nvariants at ﬁne-tuning. For Named Entity Recogni-\ntion, pretrained language models are notably able\nto leverage on notes to improve text predictions.\nWe have further shown that multilingual pretrained\nlanguage models not only perform better on the\nhistorical part of this dataset, but also on modern\nDutch notes. However, the superiority of multilin-\ngual models on modern Dutch is very much linked\nto the task at hand, as orienting the data to more\nsemantic distinctions can turn the tables for mono-\nlingual models.\nAcknowledgements\nThis work is part of aClariah Work Package 6: Text\nproject, and supported by NWO project number CP-\nWP6-19-005. We thank the members of the VOC\nuse case group for interesting discussions and the\nanonymous reviewers for their pertinent comments.\nReferences\nRodrigo Agerri, Josu Bermudez, and German Rigau.\n2014. IXA pipeline: Efﬁcient and Ready to Use\nMultilingual NLP tools. In Proceedings of the Ninth\nInternational Conference on Language Resources\nand Evaluation (LREC’14), Reykjavik, Iceland. Eu-\nropean Language Resources Association (ELRA).\nDarina Benikova, Chris Biemann, and Marc Reznicek.\n2014. NoSta-D Named Entity Annotation for Ger-\nman: Guidelines and Dataset. In Proceedings of\nthe Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 2524–\n2531, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nAlex Brandsen, Suzan Verberne, Milco Wansleeben,\nand Karsten Lambers. 2020. Creating a Dataset\nfor Named Entity Recognition in the Archaeology\nDomain. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference , pages 4573–\n4577, Marseille, France. European Language Re-\nsources Association.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\n29\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. ArXiv: 1912.09582.\nLouise Deleger, Qi Li, Todd Lingren, Megan Kaiser,\nKatalin Molnar, Laura Stoutenborough, Michal\nKouril, Keith Marsolo, and Imre Solti. 2012. Build-\ning Gold Standard Corpora for Medical Natural Lan-\nguage Processing Tasks. AMIA Annual Symposium\nProceedings, 2012:144–153.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. RobBERT: a Dutch RoBERTa-based Lan-\nguage Model. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n3255–3265, Online. Association for Computational\nLinguistics.\nJacob Devlin. 2018. Multilingual BERT release.\nOriginal-date: 2018-10-25T22:57:34Z.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJan-Christoph Klie, Michael Bugert, Beto Boullosa,\nRichard Eckart de Castilho, and Iryna Gurevych.\n2018. The INCEpTION Platform: Machine-\nAssisted and Knowledge-Oriented Interactive Anno-\ntation. In Proceedings of the 27th International\nConference on Computational Linguistics: System\nDemonstrations, pages 5–9. Association for Compu-\ntational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv: 1907.11692.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous Pipeline for Process-\ning Huge Corpora on Medium to Low Resource In-\nfrastructures. In 7th Workshop on the Challenges\nin the Management of Large Corpora (CMLC-\n7), Cardiff, United Kingdom. Leibniz-Institut für\nDeutsche Sprache.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow Multilingual is Multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving Language Under-\nstanding by Generative Pre-Training. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Tech-\nnical report, OpenAI.\nDirk Roorda. 2016. Text-Fabric. Software. Version\n8.3.4.\nDirk Roorda. 2019. Text-fabric: handling Biblical data\nwith IKEA logistics. HIPHIL Novum , 5(2):126–\n135.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. ArXiv: 1706.03762.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-Art Natural Language Process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Be-\ncas: The Surprising Cross-Lingual Effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\n30\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s Neural Machine\nTranslation System: Bridging the Gap between Hu-\nman and Machine Translation. ArXiv: 1609.08144.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8535410165786743
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7451990842819214
    },
    {
      "name": "Language model",
      "score": 0.6879873275756836
    },
    {
      "name": "Natural language processing",
      "score": 0.6787405014038086
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6262935996055603
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.47777998447418213
    },
    {
      "name": "Task (project management)",
      "score": 0.47636935114860535
    },
    {
      "name": "Linguistics",
      "score": 0.3535783886909485
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I865915315",
      "name": "Vrije Universiteit Amsterdam",
      "country": "NL"
    }
  ]
}