{
    "title": "MTRT: Motion Trajectory Reconstruction Transformer for EEG-Based BCI Decoding",
    "url": "https://openalex.org/W4376224644",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5056298877",
            "name": "Pengpai Wang",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A5086917578",
            "name": "Zhongnian Li",
            "affiliations": [
                "China University of Mining and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5082357985",
            "name": "Peiliang Gong",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A5089137375",
            "name": "Yueying Zhou",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A5033847087",
            "name": "Fang Chen",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A5018821033",
            "name": "Daoqiang Zhang",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2766672259",
        "https://openalex.org/W2805710794",
        "https://openalex.org/W4220904310",
        "https://openalex.org/W4210839512",
        "https://openalex.org/W4210931189",
        "https://openalex.org/W2293112680",
        "https://openalex.org/W2972417789",
        "https://openalex.org/W2801522104",
        "https://openalex.org/W3137165525",
        "https://openalex.org/W4205230192",
        "https://openalex.org/W4310332333",
        "https://openalex.org/W2793499859",
        "https://openalex.org/W3196520170",
        "https://openalex.org/W3122597721",
        "https://openalex.org/W3102003537",
        "https://openalex.org/W3205898195",
        "https://openalex.org/W3093125198",
        "https://openalex.org/W2991004743",
        "https://openalex.org/W3095200866",
        "https://openalex.org/W3205618423",
        "https://openalex.org/W3208327004",
        "https://openalex.org/W4200271642",
        "https://openalex.org/W3003523273",
        "https://openalex.org/W2979708657",
        "https://openalex.org/W3167491448",
        "https://openalex.org/W3206384369",
        "https://openalex.org/W3136337655",
        "https://openalex.org/W2900802277",
        "https://openalex.org/W3129583232",
        "https://openalex.org/W3167195439",
        "https://openalex.org/W3121810080",
        "https://openalex.org/W2940585064",
        "https://openalex.org/W3198315730",
        "https://openalex.org/W2986586463",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3043677335",
        "https://openalex.org/W3205895886",
        "https://openalex.org/W3207343673",
        "https://openalex.org/W6791303936",
        "https://openalex.org/W2909474135",
        "https://openalex.org/W2170208697",
        "https://openalex.org/W3118598657",
        "https://openalex.org/W3037047196",
        "https://openalex.org/W2944275137",
        "https://openalex.org/W3177342940",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W3131920792",
        "https://openalex.org/W3209926072",
        "https://openalex.org/W3131283877",
        "https://openalex.org/W3138524970",
        "https://openalex.org/W3130628483",
        "https://openalex.org/W3137300096",
        "https://openalex.org/W3188727494",
        "https://openalex.org/W2944943102",
        "https://openalex.org/W2584452226",
        "https://openalex.org/W3012521722",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4205988666",
        "https://openalex.org/W3101017148",
        "https://openalex.org/W4226194808",
        "https://openalex.org/W3207212285"
    ],
    "abstract": "Brain computer interface (BCI) is a system that directly uses brain neural activities to communicate with the outside world. Recently, the decoding of the human upper limb based on electroencephalogram (EEG) signals has become an important research branch of BCI. Even though existing research models are capable of decoding upper limb trajectories, the performance needs to be improved to make them more practical for real-world applications. This study is attempt to reconstruct the continuous and nonlinear multi-directional upper limb trajectory based on Chinese sign language. Here, to reconstruct the upper limb motion trajectory effectively, we propose a novel Motion Trajectory Reconstruction Transformer (MTRT) neural network that utilizes the geometric information of human joint points and EEG neural activity signals to decode the upper limb trajectory. Specifically, we use human upper limb bone geometry properties as reconstruction constraints to obtain more accurate trajectory information of the human upper limbs. Furthermore, we propose a MTRT neural network based on this constraint, which uses the shoulder, elbow, and wrist joint point information and EEG signals of brain neural activity during upper limb movement to train its parameters. To validate the model, we collected the synchronization information of EEG signals and upper limb motion joint points of 20 subjects. The experimental results show that the reconstruction model can accurately reconstruct the motion trajectory of the shoulder, elbow, and wrist of the upper limb, achieving superior performance than the compared methods. This research is very meaningful to decode the limb motion parameters for BCI, and it is inspiring for the motion decoding of other limbs and other joints.",
    "full_text": "IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023 2349\nMTRT: Motion Trajectory Reconstruction\nTransformer for EEG-Based BCI Decoding\nPengpai Wang\n , Zhongnian Li, Peiliang Gong\n , Yueying Zhou\n , Fang Chen\n ,\nand Daoqiang Zhang\n , Senior Member, IEEE\nAbstract— Brain computer interface (BCI) is a system\nthat directly uses brain neural activities to communicate\nwith the outside world. Recently, the decoding of the human\nupper limb based on electroencephalogram (EEG) signals\nhas become an important research branch of BCI. Even\nthough existing research models are capable of decod-\ning upper limb trajectories, the performance needs to be\nimproved to make them more practical for real-world appli-\ncations. This study is attempt to reconstruct the continu-\nous and nonlinear multi-directional upper limb trajectory\nbased on Chinese sign language. Here, to reconstruct the\nupper limb motion trajectory effectively, we propose a novel\nMotion Trajectory Reconstruction Transformer (MTRT) neu-\nral network that utilizes the geometric information of human\njoint points and EEG neural activity signals to decode the\nupper limb trajectory. Specifically, we use human upper\nlimb bone geometry properties as reconstruction con-\nstraints to obtain more accurate trajectory information of\nthe human upper limbs. Furthermore, we propose a MTRT\nneural network based on this constraint, which uses the\nshoulder, elbow, and wrist joint point information and EEG\nsignals of brain neural activity during upper limb movement\nto train its parameters. To validate the model, we collected\nthe synchronization information of EEG signals and upper\nlimb motion joint points of 20 subjects. The experimental\nresults show that the reconstruction model can accurately\nreconstruct the motion trajectory of the shoulder, elbow,\nand wrist of the upper limb, achieving superior performance\nthan the compared methods. This research is very mean-\ningful to decode the limb motion parameters for BCI, and\nit is inspiring for the motion decoding of other limbs and\nother joints.\nManuscript received 15 September 2022; revised 14 April 2023;\naccepted 8 May 2023. Date of publication 11 May 2023; date of\ncurrent version 22 May 2023. This work was supported in part\nby the National Natural Science Foundation of China under Grant\n62136004, Grant 61876082, and Grant 61732006; and in part by the\nNational Key Research and Development Program of China under\nGrant 2018YFC2001600 and Grant 2018YFC2001602. (Corresponding\nauthor: Daoqiang Zhang.)\nThis work involved human subjects or animals in its research. Approval\nof all ethical and experimental procedures and protocols was granted by\nthe Nanjing University of Aeronautics and Astronautics, and performed\nin line with the Declaration of Helsinki.\nPengpai Wang, Peiliang Gong, Yueying Zhou, Fang Chen, and\nDaoqiang Zhang are with the Key Laboratory of Brain-Machine\nIntelligence Technology, Ministry of Education, MIIT Key Labora-\ntory of Pattern Analysis and Machine Intelligence, Nanjing Univer-\nsity of Aeronautics and Astronautics, Nanjing 211106, China (e-mail:\ndqzhang@nuaa.edu.cn).\nZhongnian Li is with the School of Computer Science, China University\nof Mining Technology, Xuzhou 221116, China.\nDigital Object Identifier 10.1109/TNSRE.2023.3275172\nIndex Terms — Brain computer interface, EEG, limb\nmotion decoding, trajectory reconstruction, transformer,\nmotor execution.\nI. I NTRODUCTION\nB\nRAIN computer interface (BCI) system can directly\ncommunicate with external equipment without relying on\nmuscles and nerves tissue. BCI can be applied in many fields,\nsuch as helping patients with spinal cord injury, amyotrophic\nlateral sclerosis, atresia syndrome and other patients with\nbrain-control equipment [1], [2], [3], [4], improving their\nability to take care of themselves and communicate with peo-\nple [5], [6], [7], and helping stroke patients with rehabilitation\ntraining [8], [9], [10]. It also has great potential in the game\nfield [11], [12], [13]. There are information communication\nmedia involved in BCI, such as, magnetic resonance images,\nnear-infrared, electroencephalogram (EEG) [14], [15], [16].\nAmong them, noninvasive EEG has the advantages of high\ntime resolution, low price, high practicability, and convenient\nacquisition, and is widely used in the field of brain computer\ninterface [17], [18], [19].\nRecently, researches show that EEG signals contain various\nmotion parameters of limb movement [20], [21], [22]. Decod-\ning motion parameters directly from EEG signals can provide\nintuitive and natural control [23]. Therefore, we can decode\nvarious motion parameters from EEG signals, such as velocity,\nacceleration, displacement, position, angular velocity, etc. [24],\n[25], [26]. The reconstruction of upper limb kinematic param-\neters by EEG signals can not only promote the rehabilitation\nof patients with stroke or spinal cord injury, but also control\nthe exoskeleton to enhance the strength and endurance of the\nordinary human body.\nEEG-based upper limb motion trajectory decoding is an\nimportant part of limb motion parameter decoding, which\nhas been explored by researchers. Some researchers simply\nclassify the two-dimensional plane movements extending from\nthe center to the outside, and can only identify the limited and\nspecific movement directions of the upper limbs. For example,\nÚbeda et al. [27] used multiple linear regression based on\nEEG to classify eight center-out movements of the arm. Zeng\net al. [28] used EEG to reconstruct the four-direction move-\nment of the hand centered and extended in two-dimensional\n(2D). Recent studies have decoded direction-specific motion\ntrajectories of human joints. For example, Jeong et al. [29]\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2350 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nproposed a deep learning framework for the classification of\nupper limb movements for six-direction arm reaching tasks\nin three-dimensional (3D) space. Shakibaee et al. [30] used\na nonlinear autoregressive network based on EEG to decode\nthe angular change trajectory of the knee joint in extension\nand flexion of the right knee in the sitting position. Pancholi\net al. [31] proposed a deep learning model based on EEG to\npredict the motion trajectory of one-handed hand grasping and\ntrial lifting. To sum up, these works only use EEG signals for\nsimple direction-specific classification or reconstruction.\nHowever, these literature are far from meeting the require-\nments of decoding when the upper limbs perform continuous\nand multidirectional nonlinear motions in 3D space. Therefore,\nwe propose a Motion Trajectory Reconstruction Transformer\n(MTRT) model based on the geometric constraints of human\njoints, i.e., we keep the spatial distances between the shoulder\nand elbow, elbow and wrist joint points at a fixed length during\nthe movement of the upper limbs as reconstruction constraints\nto obtain more accurate trajectory information of the human\nupper limbs. Consequently, we try to decode the continuous\nnonlinear upper limb joint point motion by studying and\nsolving the unique geometric characteristics of upper limb\njoints. In summary, the main contributions of this paper are in\nthree aspects:\n(1) To our knowledge, we are the first to decode the Chinese\nsign language motion trajectory with continuous multidirec-\ntional nonlinear upper limb movements in 3D space using EEG\nsignals.\n(2) We introduce a MTRT model by using constraints on\nthe geometric features of the joints to reconstruct the motion\ntrajectories of the joint points of the upper limbs.\n(3) The MTRT model has achieved good accuracy and\nprecision in decoding the spatial trajectory of human upper\nlimb skeleton points.\nII. R ELATED WORKS\nExtracting motion trajectory features from 3D nonlinear\nmotion EEG signals is often complicated, and the linear\ntrajectory reconstruction model can hardly meet this task.\nIn this paper, inspired by the success of the Transformer model\nin the fields of natural language processing and sequence\nsignals, we will investigate introducing the Transformer model\nto the task of motion trajectory reconstruction.\nCompared to motion direction classification, motion tra-\njectory reconstruction is more challenging and raises high\ndemands on reconstruction model. To solve this task, Little\net al. [32] explored a neural network based on a regulariza-\ntion algorithm to predict the trajectory of the elbow flexion\nangle to create an upper limb motion prediction model. Kim\net al. [33] studied the prediction of 3D hand trajectories by\nmultiple linear regression (MLR). The average correlation\ncoefficient between the predicted trajectory and the actual\ntrajectory is 0.684. Robinson et al. [20] reconstructed the\nposition of 2D hand motion trajectories. Their task involved\nright-hand movements from the center outward in a random\nsequence in four different directions. Using a Kalman filter\nto estimate motion trajectories, they obtained a correlation\nof 0.60 between recorded and estimated data. Sosnik and\nZheng [25] used a MLR model to predict the trajectories of the\nhands, elbows, and shoulders of seven subjects in a time-series\n3D space. The mean Pearson correlation coefficients between\nthe predicted and actual trajectories for the hand, elbow,\nand shoulder ranged to the highest of 0.49, 0.48, and 0.40,\nrespectively. Mondini et al. [34] reconstructed hand trajectories\nfrom low-frequency EEG signals. Motion parameters (2D\npositions) were regressed from the EEG using a regression\nmethod combining partial least squares (PLS) and Kalman\nfiltering. An overall significant online correlation between\nhand motion trajectories and decoded trajectories was obtained\nwith an average of 0.32. However, we found that during the\ntrajectory decoding process, normal human motion is nonlinear\nand directionally random, which brings challenges to motion\ntrajectory decoding. Therefore, the focus of this work is to\nuse the EEG signal information during 3D nonlinear motion\nto capture the position of joint points in space using deep\nneural networks to decode the motion trajectories of human\nupper limbs.\nThe Transformer model [35] was proposed in 2017, and\nit was an encoder-decoder architecture that generates global\ndependencies between input and output based on a multi-head\nattention mechanism. Compared with general deep learn-\ning [36], [37], the advantages of the Transformer model lie\nin the feature learning representation and attention mech-\nanism [38]. Furthermore, the masking mechanism in the\nTransformer model prevents the Transformer from shadow\nlearning [39]. When this mechanism is used in conjunction\nwith the attention mechanism, it can produce better feature\nrepresentations in the spatial dimension and learn the tiny\nfeatures of the dataset when generalizing it. The architectures\nof models such as classic Long Short-Term Memory (LSTM)\nrequire more time to train when dealing with sequential data\nand lack parallel processing capabilities [40]. The design of the\narchitecture of models such as Convolutional Neural Network\n(CNN) is not suitable for computing the temporal features\nof time series data [41]. So we introduce Transformer model\nto fully utilize the computing power, allow parallelization\nwith attention and feature representation learning, and reduce\ntraining time.\nIII. D ATA COLLECTION AND PRE-PROCESSING\nTwenty subjects (25×14-year-old) were recruited according\nto the experimental setup, including 9 females and 11 males\n[42]. All subjects were required to be in good health and full\nof energy without brain surgery or brain-related diseases. The\nsubjects were informed of all experimental procedures and\nrelevant precautions, signed a written informed consent form\nafter expressing their consent, and were given corresponding\ncash rewards according to the duration of the experiment. The\nacquisition equipment required in this paper includes an EEG\nacquisition instrument, Kinect V2, and a computer. To reduce\nthe influence of motion on the EEG acquisition process,\na portable wireless EEG acquisition device (NeuSen.W64,\nNeuracle) was used. The device has 64 channels, includ-\ning 59 EEG channels, 4 electrooculography, and 1 electro-\ncardiograph channels. Fifty-nine EEG channels were arranged\naccording to the international 10-20 standard.\nWANG et al.: MTRT FOR EEG-BASED BCI DECODING 2351\nA. Experimental Setup\nOne week before the experiment, the subjects were asked to\nlearn relevant Chinese sign language movements. During the\nexperiment, except for the movements of sign language, the\nlower limbs and trunk were kept still and movements such as\neye movements and swallowing were reduced.\nThe experimental process includes two runs, and the time\ninterval of each run is 15 minutes. Each run includes 22 execu-\ntions of 30 sign language sentences. The signed sentences are\nselected from the common sign language database. When the\nsubjects excuted the selected sign language, the upper limbs\nwere greatly expanded, which was convenient for the Kinect\nto collect movement data. The course of each trial consisted\nof 2 seconds of preparation time, 3 seconds of execution\ntime, and 3 seconds of rest time. The subjects performed\nsign language according to the prompt tone and the computer\nscreen cross prompt. At the same time as EEG acquisition,\nKinect collects the motion trajectories of the subjects’ upper\nlimb joint points, and the sampling rate is 30Hz. We reduce\nthe EEG acquisition device with a sampling rate of 1000Hz to\n900Hz, and the impedance of the electrodes is less than 5K .\nThe subjects were relaxed throughout the experiment and were\nnot allowed to open their mouth, swallow or chew. Except for\nthe sign language movement of upper limbs, the movement of\nother body parts is not allowed. The above measures are used\nto avoid electromyography (EMG) artifact in EEG.\nTo facilitate subsequent analysis and reconstruction tasks,\nwe selected 59 channels with EEG signals from 64 channels.\nThe EEG signals were frequency filtered and band-pass filtered\nat 0.1-100 Hz. Then, we remove the 50 Hz power frequency\nand electrooculography (EOG) interference signals in the\nEEG. Next we performed a whole-brain re-reference to the\nEEG data. The sentence process of the subjects executing\nthe sign language lasted 3s, and most of them were simple\nsentences with three or four sign language words. To facilitate\nthe training of the joint point motion trajectory reconstruction\nmodel, each EEG sequence is cut into trials with a length of\n4s (-0.5s∼3.5s).\nB. Joint Point Data Pre-Process\nWe use the Kinect instrument to collect the spatial position\nof the human upper limb joint points. We found that the\ncollected data is offset. In the process of arm movement, the\nlength of upper arm and lower arm will change due to the\nlimitation of environmental conditions such as clothes or light.\nHowever, in practice the length of the arm don’t change in\nmovement. Therefore, we need to correct the obtained joint\nspatial position data to further decode the joint point position\nmore accurately using EEG data later.\nTo correct the joint points position data, we use the Euler\nangle and inverse kinematics equation to correct the collected\ndata [43], [44], [45]. Specifically, we first extract the spatial\nposition data of joint points in the preparation stage of the\nexperiment, to calculate the length of the upper arm and lower\narm of the subject. Then the origin of the 3D coordinate system\nis positioned at the shoulder joint point, and the position data\nof the elbow and wrist joints are updated. In the process of arm\nFig. 1. The 3D coordinates of elbow and wrist joints are updated\nthrough the inverse motion equation. The position of the whole joint\npoints collected by Kinect is displayed in the 2D plane. The blue asterisk\nrefers to all human joint points collected. The red, blue and green dots\nare shoulder, elbow, and wrist nodes, respectively. Taking the right arm\nas an example, the length of the upper arm collected by the device\nis the distance of OE and the length of the lower arm is the distance\nof EW. Calculate the angle of OE with X, Y, and Z axes in the new\ncoordinate system (3D coordinate system with O as the origin) through\nEuler angle. The 3D coordinates of E′ are solved by using the inverse\nmotion equation through the angle and the actual arm length. The\ndistance of OE′ is the real upper arm length L1. Similarly, the distance\nof O′W′ is the real lower arm length L2.\nmovement, different angles in the process of movement are\ncalculated through the coordinates of the arm. Then the spatial\nposition of the new joint point is determined according to the\nangle and the fixed arm length. Through the same method,\nthe spatial position coordinates of elbow and wrist joint are\ncalculated in turn, as shown in Fig. 1.\nC. Trajectory Reconstruction Data\nIn our experimant, each subject executed 30 sign\nlanguage sentences, each sign language sentence included\n44 samples, and all subjects had a total of 20 ×30×44\n= 26400 samples. To remove residual noise in EEG\nelectrodes, the standard deviation (SD) of EEG amplitude\nwas calculated for each electrode in all samples, and if\nthe amplitude of a channel exceeded 6 SD, the electrode\nwas marked as a noisy channel. Finally, 9 channels are\nmarked as noise channels. The corresponding electrode\nnames are Fp1, AF7, AF8, F1, F8, T7, T8, TP8, and O2.\nNext, we performed a residual noise test on all samples and\ndeleted the trial if the EEG amplitude of any one of the\nresidual electrodes in the trial exceeded 6 SD [25], [46].\nA total of 4440 samples were removed, and the remaining\n21960 samples participated in the training and testing\nexperiments of the MTRT model. The pre-processed EEG data\nformat is samples ×timesteps×channels=21960×3600×50,\nand the pre-processed joint points data format is\nsamples×timesteps×joints=21960×120×18.\nIV. M OTION TRAJECTORY RECONSTRUCTION\nTRANSFORMER\nIn this section, we introduce our proposed Motion Trajec-\ntory Reconstruction Transformer (MTRT) model to reconstruct\njoint points trajectory of upper limb. The general framework\nof our model is shown in Fig. 2. First, we performed the\nacquisition of EEG data and Kinect joint point data. Then, two\n2352 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 2. The architecture of joint point trajectory reconstruction based on Chinese sign language. During the training phase, when the subjects\nperformed sign language follow the experimental paradigm, EEG acquisition equipment and Kinect were used to collect joint point trajectory data\nof EEG and upper limb movement at the same time. Then we pre-process the EEG data, including channel selection, down sampling, re-reference,\nand frequency band filtering. We select the data of 6 joint points of shoulder, elbow, and wrist in the left and right arms from the trajectory data of\n25 joint points. Further, the Euler angle and inverse motion equation are used to correct the data. Finally, the pre-processed EEG and corrected\njoint point data are input into the MTRT for model training. During the testing phase, the pre-processed EEG data is reconstructed to the motion\ntrajectory of joint points through the trained MTRT.\nkinds of data were pre-processed, and the MTRT model was\ntrained with the pre-processed data. Finally, the trajectory of\nthe joint points is reconstructed by EEG based on the trained\nmodel.\nIn this section, we introduce the EEG encoder and trajectory\ndecoder used to reconstruct sign language motion trajectories.\nThe model framework used in this paper consists of an\nEEG encoder and a joint point spatial location decoder. The\nEEG encoder learns the depth representation of the EEG in\na self-attention manner, and the joint point spatial location\ndecoder uses the participating EEG representations to generate\ncontinuous joint point 3D spatial coordinates.\nThe MTRT model architecture is shown in Fig. 3. Next\nwe describe the core modules of the MTRT in the following\nsections. The architecture of the MTRT model encodes the\nEEG features and decodes the sequence of joint point spatial\npositions. The model is divided into two parts, including the\nEEG encoder and the trajectory decoder.\nA. EEG Encoder\nThe model uses 6 encoder layers and a multi-head\nself-attention module to obtain attention weights for the\npre-processed input EEG features. Since the Transformer\nencoder and decoder are permutation-invariant, we add a fixed\nsinusoidal spatial positional encoding and object query at the\ninput as the learned positional embedding. The first sub-layer\nis a complex attention layer, and the second sub-layer is a\ncomplex-valued feed-forward network. Both sub-layers have\nresidual connections and normalization layers [47], [48]. Each\nlayer consists of two sub-layers: a multi-head self-attention\nand a fully connected feed-forward network. Each sub-layer\nstarts with layer normalization to mitigate internal covari-\nate shifts. There is a residual connection around each sub-\nlayer, which preserves the information of the input features\nand enhances the stability of the model. The output of\nthe Transformer layer is passed to a feed-forward network\n(FFN) module, which consists of a three-layer perceptron\nwith rectified linear unit (ReLU) activation, and then the final\ndetection predictions. Layer normalization is performed on the\nremaining connections in the encoder, which is called Norm &\nAdd in the encoder.\nTo obtain information from different representation\nsubspaces of different modalities at different locations,\nwe combine multiple attention functions to achieve multi-\nhead attention. Multi-head attention is the core module of\nTransformer, which allows the model to jointly focus on\ninformation in different representation sub-spaces at different\nlocations [49]. Multi-Head Attention calculates h (= 8) Scaled\nDot-Product Attention, and each Scaled Dot-Product Attention\ncan calculate the corresponding head. Scaled Dot-Product\nAttention is a mechanism for learning action dependencies\nin sign language EEG and capturing the internal structure\nWANG et al.: MTRT FOR EEG-BASED BCI DECODING 2353\nFig. 3. MTRT model structure for the reconstruction of joint points space location. The model is divided into two parts: EEG encoder and trajectory\ndecoder. The MTRT model has a total of six encoder layers and six decoder layers. Specifically, in the EEG encoder, the preprocessed EEG data\nand the positional encoding are input to the encoder. It is converted into a representation through six encoder layers, each encoder layer includes a\nmulti-head self-attention layer, a feed-forward neural network layer and a summation and normalization layer (Add & Norm layer). In the trajectory\ndecoder, the output of the EEG encoder is input to the decoder in the form of key and value. During the model training process, the joint point\nposition information and position encoding data are processed through multi-head attention and the Add & Norm layer. The result is a form of a\nquery and the encoder output is fed into the next multi-head attention and Add & Norm layer. Then the next data is predicted through the feedforward\nneural network and the Add & Norm layer to form an encoder layer.\nof EEG. The attention function maps a query and a set of\nkey-value pairs to an output, where the output is computed\nas a weighted sum of values. The weight assigned to each\nvalue is calculated by the query and the corresponding key.\nThe attention function maps a query and a set of key-value\npairs to an output, where the output is computed as a weighted\nsum of the values [50]. The weight assigned to each value is\ncalculated from the query and the corresponding key. We adopt\nscaled dot product attention because the scaling factor dv\navoids extremely small gradients after softmax. The query for\nall keys takes the dot product and divides by √dk [35]. Then\napply the softmax function to get the weights for these values.\nThe formula for calculating attention can be written as :\nAttention (Q, K, V ) = softmax\n(QK T\n√dk\n)\nV (1)\nwhere query Q ∈ Rt×dq , key K ∈ Rt×dk , value V ∈ Rt×dv and\noutput O ∈ Rt×dmodel . They are all matrices, t is the sequence\nlength, dk is the dimension of the query key, dv is the value\ndimension, and dmodel is the output dimension of the encoder\nwhich value is set to 256.\nThe query, key, and value are projected h times onto the\ndq , dk, and dv dimensions using the learned linearity, where\nh represents the number of heads. The attention function is\nthen executed in parallel on the projected query, key, and\nvalue, computing the dv dimensional output. The outputs of all\nheads are connected and linearly projected to deliver thedmodel\ndimensional results to the next feed-forward sub-layer. Each\nhead is then concatenated and fed to another linear projection\nto obtain the final output of multi-head attention. The formula\nfor multi-head attention is as follows :\nMultiHead(Q, K, V ) = Concat\n(\nOh1 , . . . ,Ohh\n)\nW o (2)\nwhere Ohi = Attention\n(\nQW Q\ni , K W K\ni , V W V\ni\n)\n, and learn-\nable projection matrices W Q\ni ∈ Rdmodel ×dk , W K\ni ∈ Rdmodel ×dk ,\nand W V\ni ∈ Rdmodel ×dv .\nThe input features are processed in the Transformer network\nand their order / position is not preserved, so positional\nencoding is employed to preserve the temporal order of the\nfeatures [51]. Therefore, the EEG signal data is encoded with\nsequence information of sine and cosine functions according\nto the equation. The input vector and the position encoding\nvectorby the element-wise addition input into the encoder\nlayer.\nThe output vector of the embedding layer needs to be\ncomputed through six encoder layers. There are two sub-layers\nin each coding layer. These are self-attention and a fully\nconnected feed-forward layer. In self-attention, the attention\nfunction is used for a set of queries, keys, and values,\nrespectively. The calculation of the output matrix is done using\nequations. Also, normalization is added at the end of each\nsub-layer. Generate an r-dimensional vector at the end of the\nencoder and pass it to the decoder.\nB. Trajectory Decoder\nThe structure of the decoder consists of six identical decoder\nlayers and an output layer. The output of the encoder and\nthe decoder content serve as the input for the training of\nthe decoder. Each decoder layer has three sub-layers, namely\nattention layers, complex FFN and another attention layer. The\nfirst attention layer is masked by additional diagonal lines\nto prevent attention to subsequent positions, to ensure that\nprevious joint point data does not depend on later joint space\nposition data points used for prediction. The second attention\nwill be performed on the encoding representation Xenc and\nthe decoder input Xdec .\n2354 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nInput a y-dimensional zero vector into the input layer of\nthe decoder. The y is a hyperparameter and its value is set\nto 1024. The decoder adopts the same multi-head attention\nas the encoder. Finally, by mapping the output of the last\ndecoder layer to a two-dimensional vector, a vector containing\nthe sequence of spatial coordinates of the three joints shoulder,\nelbow, and wrist of the human body is obtained. A positional\noffset is used between the input of the decoder and the target\noutput. We use LSTM as the timing processing module to\nperform timing processing and looping on the encoder input.\nSet the value of the parameter dmodel to 256. The dmodel\nparameter determines the output dimension of the sub-layer\noutput and the embedding layer [35]. Considering the large\ndifference of independent individuals in EEG, each subject is\ntrained separately in this paper, and the input format of the\nencoder is samples ×timesteps×channels. The output format\nof the decoder is samples ×timesteps×joints, where joints\nrepresents the sequence representation in the 3D space of the\nsix joints of the left and right shoulders, elbows and wrists\n(18=2×3). The embedding layers in the encoder and decoder\nshare the same set of weights. The value of parameter h in the\ncalculation of self-attention and multi-head attention is 8. The\nparameter h refers to the number of parallel attention layers\nor attention heads. The number of hidden units value in the\nfully connected sub-layer in the decoder layer is set to 1024.\nSet the parameters dk (queries, key vector dimension) and dv\n(value vector dimension) to 8. To implement the positional\nencoding block, the method is the same as in [35], using\nsine and cosine to implement the positional encoding. The\nkey and value vectors of the final encoding layer are input to\nthe third multi-head attention sub-layer of the decoder layer.\nThe multi-head attention layer in the decoder takes the query\nvector value from the layer below it.\nC. Geometric Information Constraints\nTo train the MTRT model, we apply loss functions on top\nof the trajectory decoder outputs, and minimize the errors\nbetween reconstruction and groundtruth joint points position.\nThe geometric constraints of skeleton points are only used\nas loss functions to train the model, not as data. When the\ntraining of the model is completed, the model can be directly\nused to reconstruct the motion trajectory of EEG data.\nIt is generally known that the lengths of the upper and\nlower arms are fixed when subjects perform sign language\nactions, the lengths of the left upper arm and the right upper\narm of each healthy subject are equal, and the length of the\nleft lower arm and the right lower arm are equal. Therefore,\nto reconstruct accurate joint point position data, we set the\nupper and lower arm lengths to be fixed and the left and right\narms to be equal in length as geometric constraints for the\nMTRT model. The calculation of the geometric features of\nthe upper limbs of the human body is also very simple and\nconvenient. The spatial distances between the shoulder joint\npoint and the elbow joint point, the elbow joint point and the\nwrist joint point are set as constants L1 and L2 in the whole\nmovement process. The lengths of the left and right arms\nare equal to obtain, i.e. L1lef t = L1right , L2lef t = L2right .\nThe geometric characteristics of human joints are used as the\nconstraints of the reconstructed model to obtain more accurate\ntrajectory information of human upper limbs. Therefore, the\njoint geometry constraint limb distance chanless loss ( L DC L)\nand left-right distance equal loss ( L R DE) set as :\nLDCL = 1\nm\nm∑\ni=1\n(\nlen′\ni − leni\n)2 (3)\nLRDE = 1\nm\nm∑\ni=1\n((\nL1lef ti − L1right i\n)2\n+\n(\nL2lef ti − L2right i\n)2)\n(4)\nwhere len ′\ni and len i are the predicted and actual length values\nof arm of sample i. L1lef ti and L1right i are the left and right\nupper limb length of sample i, and L2lef ti and L2right i are the\nleft and right lower limb length of sample i in reconstruction\njoint points position.\nWe use the mean squared error (MSE) and the distance\nloss between the shoulder, elbow, and wrist joint point (the\nlength of the forearm or rear arm is always equal during the\nmovement) to train the MTRT model. The MSE Loss is:\nMSE = 1\nn\nn∑\ni=1\n(\ny′\ni − yi\n)2 (5)\nwhere yi and y′\ni are the actual and predicted values of sample\ni, respectively. Overall, the loss of the MTRT model, named\nas trajectory reconstruction loss (TRL), consists of three parts:\nLDCL, LRDE and Mean Squared Error (MSE), SRL can be\nwritten as:\nTRL = 1\n2σ2\n1\nLDCL + 1\n2σ2\n2\nLRDE + 1\n2σ2\n3\nMSE + log σ1σ2σ3\n(6)\nwhere σ1, σ2, and σ3 are the trainable weights of the regression\nmodel. It is randomly initialized and iteratively optimized\nduring training.\nAfter the decoder is divided into blocks, the dense layer\nis used for mapping transformation, and the output of the\ndense layer is the spatial position sequence of the six joint\npoints. The model was trained for 50 epochs using the Adam\noptimizer [52] and the batch size was set to 8. The model\npredicts the 3D position of a joint point in space every\n30 timesteps.\nV. R ESULTS AND DISCUSSIONS\nTo achieve the goal of sign language motion trajectory,\nwe proposed the MTRT model to reconstruct the joint space\ntrajectory of EEG signals to obtain effective feature acquisition\nand high reconstruction accuracy. We will describe the perfor-\nmance evaluation of joint motion trajectories reconstruction\nbased on Chinese Sign Language EEG data. Furthermore,\nwe compare the performance of our method and comparative\nmethods. All scripts are written using the deep learning frame-\nwork of PyTorch 1.10 and CUDA 11.3. The model runs on a\nDell precision workstation devices configured with NVIDIA\nGeForce RTX 2080Ti GPU, 16 GB RAM and Intel i7-9700\nCPU@3-GHz, without any special hardware optimization.\nWANG et al.: MTRT FOR EEG-BASED BCI DECODING 2355\nFig. 4. Groundtruth trajectories and reconstructed trajectories of the left and right shoulders, elbows, and wrists of the 28th handed sentence from\nthe third subject. Its trajectory contains a total of 120 trajectory points and has a duration of four seconds. The red circles are the trajectory points\nof the groundtruth, and the blue circles represent the trajectory points of the reconstructed joint points.\nFig. 5. Training and validation loss changes for 50 epochs of MTRT\nmodel training.\nAfter 50 times of training, the MTRT model tends to converge,\nand the loss decreases steadily with the increase of training\ntimes, and finally stabilizes (Fig. 5). Next, we will summarize\nthe experiment data based on Chinese Sign Language and\ndiscuss the results of the proposed method.\nA. Comparison of Reconstruction Performance of\nDifferent Models\nIn trajectory decoding, a commonly used similarity measure\nbetween upper limb motion trajectories and decoded trajecto-\nries is the Pearson correlation coefficient ρ [29], [31], [34].\nThe Pearson correlation coefficient between true trajectories\nand decoded trajectories is defined as the quotient of their\ncovariance and standard deviation, and its formula can be\nwritten as:\nρ(G,R) = cov(G, R)\nσG σR\n=\n∑n\ni=1\n(\nGi − ¯G\n)(\nRi − ¯R\n)\n√∑n\ni=1\n(\nGi − ¯G\n)2\n√∑n\ni=1\n(\nRi − ¯R\n)2\n(7)\nwhere ρ(G,R) represents the Pearson correlation coefficient\nbetween the reconstructed joint point value G and the real\nvalue R, ¯G and ¯R represents the sample average of G and R,\nrespectively.\nIn addition, we used the Normalized Root Mean Square\nError (NRMSE) model performance measurement method as\nfollows [53]. We measured the decoding performance of the\n3D axis using NRMSE, and the results were the average values\nof the X-axis, Y-axis, and Z-axis. The total average NRMSE\nof the 3D axis for the six joint points in sign language using\nthe proposed method is 0.159.\nTo measure the performance of our trained MTRT model,\na Pearson correlation coefficient was calculated for the\ncomparative model and our model. Comparing models in\nour nonlinear multi-directional motion trajectory detection\ntask include MLR [25], LSTM [54] and CNN-LSTM [55],\nsequence-to-sequence (Seq2seq) [56], and Transformer mod-\nels. The Transformer model is our MTRT model that only uses\nMSE loss function.\nWe randomize the data before starting training, excute a\nperformance comparison experiment on the test data set of\n20 subjects, and calculate the average value of ρ between the\nreconstructed trajectory and the real trajectory. Comparative\nexperiments were carried out using the same hardware and\nsoftware environment. The three dimensions X, Y , and Z of\n2356 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nTABLE I\nPEARSON CORRELATION COEFFICIENTS BETWEEN THE RECONSTRUCTION OF THE MOVEMENT TRAJECTORIES OF THE SHOULDER ,\nELBOW, AND WRIST DURING THE EXECUTION OF SIGN LANGUAGE AND THE TRUTH SIGN LANGUAGE\nMOVEMENT TRAJECTORIES FOR THE REGRESSION MODELS\nTABLE II\nNRMSE OF RECONSTRUCTION PERFORMANCE COMPARISON W ITH SIX METHODS\nthe six joints of the shoulder, elbow, and wrist in the left and\nright directions are compared with ten-fold cross-validation.\nAs shown in Table I, the Pearson correlation coefficient\nbetween the spatial position of each joint point reconstructed\nby the MTRT model and the real value is consistently higher\nthan those of the comparison models, achieving a mean value\nof 0.94.\nB. Comparison of Initial and Reconstructed Spatial\nCoordinates\nTo visualize the reconstruction performance of our proposed\nmodel for the left and right joint motion trajectories of the\nshoulder, elbow, and wrist, we selected the data of the third\nsubject for visualization. Input the EEG data in the test set into\nthe trained MTRT model to get the 3D position coordinates\nof the six joints. Six joint points trajectories of groundtruth\nand reconstruction as shown in Fig. 4. The groundtruth and\nreconstructed joint point trajectories distributions are similar,\nindicating the superiority of the performance of our MTRT\nreconstruction model. It shows a comparison diagram of the\nreconstructed 3D coordinates of the right shoulder, right elbow,\nand right wrist and the real coordinates. It can be seen from\nthe visualized experimental results that the 3D coordinates of\nthe real joints are basically similar to the spatial distribution\nof the reconstructed 3D coordinates of the joints. There are\nsome points whose distribution is far from the true value, such\nas the right wrist and left elbow both predict isolated points\nthat are quite different from the true value. Fig. 6 shows the\nreconstruction results and real value distribution of the 6 joint\npoints in the three dimensions of X, Y , and Z. The distribution\ncore densities in the figure are very similar, indicating that the\nreconstruction model is effective.\nC. Activated Brain Regions for 3D Motion\nDuring the whole process of sign language execution,\nwe intercepted the 4 s segment, and the time range is from\n-0.5 s to 3.5 s. In this subsection, we selected the sentence of\nthe 28th sign language, and averaged all the sentence samples\nFig. 6. Reconstruction results and true value distributions in three\ndimensions for all relevant nodes.\nto obtain the EEG map showing the area of electrode activation\nas shown in Fig. 7. A total of nine pictures are obtained, and\nthe interval between each picture is 400 ms. It can be seen\nfrom the figure that the main brain area activated by joint\nmovements of the upper limbs during sign language is the\nparietal lobe area, and the main electrodes are FC2, FC3, C3,\nC4, CP3, CP4, P3, and P4. This provides reference value for\nlater decoding trajectories with fewer channels.\nD. Limitation and Future Directions\nIn this paper, the MTRT model is used to reconstruct the\n3D space position of the joints of the upper limbs of the\nhuman body and achieve good reconstruction results. The\nPearson correlation coefficient with the true value is also high,\nindicating a strong correlation between them. Due to the use\nof Kinect equipment to collect motion trajectories of limb\njoint points and adding interference factors such as light and\nclothing, the collected joints motion trajectories data and the\nreal joint motion trajectory data have a certain deviation. In the\nWANG et al.: MTRT FOR EEG-BASED BCI DECODING 2357\nFig. 7. Mean EEG electrode activity mapping at 400ms intervals during\nthe execution of the third handed sentence by the tenth subject.\nfuture, we will fix the spatial sensors at the joints of the limbs\nto obtain more accurate spatial motion trajectory data and\nmake the reconstructed data more accurate. It is also possible\nto reconstruct the motion trajectories of finer joints, such as\nknuckles. Furthermore, the MTRT model has relatively high\nrequirements on the amount of data, and more data samples\ncan train a more accurate reconstruction model. In the next\nstep, we plan to collect more joint motion data into the model\nfor training, and obtain a model with better robustness and\naccuracy by training a large number of data samples.\nVI. C ONCLUSION\nBCI can directly use brain neural activity to exchange infor-\nmation with external devices. It can help people with physical\ndisabilities to interact with the outside world. The field of BCI\nresearch includes brain-controlled devices, speech, and text\noutput, among which the direct acquisition of human body\nmovement information based on EEG signals is an important\ndirection. Existing studies have only explored the decoding of\nsimple linear limb movements through EEG signals, and the\ndecoding accuracy is not high. In this paper, the MTRT model\nis trained by the 3D spatial information of the joint points\nand the EEG signal, to decode the motion trajectory of the\njoint points of the upper limbs. We constrain the reconstructed\nmodel according to the geometric characteristics of the relative\ndistance of human joint points to obtain more accurate joint\npoint motion trajectories. To verify the performance of our\nMTRT model, we collected EEG signals and joint motion\ninformation of 20 subjects. The experimental results show that\nour proposed model can decode the complex multi-directional\nnonlinear upper limb motion trajectory based on Chinese\nSign Language. Our research is meaningful to decode human\nmotion information based on EEG signals, and provides a\nreference for decoding other joint points of the body. Our\nmethod can be used in the future for precise manipulation\nof external devices, such as robotic arms. In addition, it can\nalso be used for remote control of special equipment.\nREFERENCES\n[1] P. Ofner, A. Schwarz, J. Pereira, D. Wyss, R. Wildburger, and\nG. R. Müller-Putz, “Attempted arm and hand movements can be\ndecoded from low-frequency EEG from persons with spinal cord injury,”\nSci. Rep., vol. 9, no. 1, pp. 1–15, May 2019.\n[2] V . Chamola, A. Vineet, A. Nayyar, and E. Hossain, “Brain-computer\ninterface-based humanoid control: A review,” Sensors, vol. 20, no. 13,\np. 3620, Jun. 2020.\n[3] X. Gu et al., “EEG-based brain-computer interfaces (BCIs): A survey of\nrecent studies on signal sensing technologies and computational intel-\nligence approaches and their applications,” IEEE/ACM Trans. Comput.\nBiol. Bioinf., vol. 18, no. 5, pp. 1645–1666, Sep. 2021.\n[4] X. Gao, Y . Wang, X. Chen, and S. Gao, “Interface, interaction, and\nintelligence in generalized brain-computer interfaces,” Trends Cognit.\nSci., vol. 25, no. 8, pp. 671–684, Aug. 2021.\n[5] O. B. Guney, M. Oblokulov, and H. Ozkan, “A deep neural network for\nSSVEP-based brain-computer interfaces,” IEEE Trans. Biomed. Eng.,\nvol. 69, no. 2, pp. 932–944, Feb. 2022.\n[6] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis\nfrom neural decoding of spoken sentences,” Nature, vol. 568, no. 7753,\npp. 493–498, Apr. 2019.\n[7] R. Abiri, S. Borhani, E. W. Sellers, Y . Jiang, and X. Zhao, “A com-\nprehensive review of EEG-based brain-computer interface paradigms,”\nJ. Neural Eng., vol. 16, no. 1, Feb. 2019, Art. no. 011001.\n[8] L. Zhou, X. Tao, F. He, P. Zhou, and H. Qi, “Reducing false triggering\ncaused by irrelevant mental activities in brain-computer interface based\non motor imagery,” IEEE J. Biomed. Health Informat., vol. 25, no. 9,\npp. 3638–3648, Sep. 2021.\n[9] E. Mihelj, M. Bächinger, S. Kikkert, K. Ruddy, and N. Wenderoth,\n“Mental individuation of imagined finger movements can be achieved\nusing TMS-based neurofeedback,” NeuroImage, vol. 242, Nov. 2021,\nArt. no. 118463.\n[10] Z. Yuan et al., “Effect of BCI-controlled pedaling training system\nwith multiple modalities of feedback on motor and cognitive function\nrehabilitation of early subacute stroke patients,” IEEE Trans. Neural\nSyst. Rehabil. Eng., vol. 29, pp. 2569–2577, 2021.\n[11] T. Mondéjar, R. Hervás, E. Johnson, C. Gutiérrez-López-Franca, and\nJ. M. Latorre, “Analyzing EEG waves to support the design of serious\ngames for cognitive training,” J. Ambient Intell. Humanized Comput.,\nvol. 10, no. 6, pp. 2161–2174, Jun. 2019.\n[12] A. E. Alchalabi, S. Shirmohammadi, A. N. Eddin, and M. Elsharnouby,\n“FOCUS: Detecting ADHD patients by an EEG-based serious game,”\nIEEE Trans. Instrum. Meas., vol. 67, no. 7, pp. 1512–1520, Jul. 2018.\n[13] B. Kerous, F. Skola, and F. Liarokapis, “EEG-based BCI and video\ngames: A progress report,” Virtual Reality, vol. 22, no. 2, pp. 119–135,\nJun. 2018.\n[14] B. Du, X. Cheng, Y . Duan, and H. Ning, “FMRI brain decoding and its\napplications in brain-computer interface: A survey,” Brain Sci., vol. 12,\nno. 2, p. 228, Feb. 2022.\n[15] Y . Kwak, W.-J. Song, and S.-E. Kim, “FGANet: FNIRS-guided attention\nnetwork for hybrid EEG-fNIRS brain-computer interfaces,” IEEE Trans.\nNeural Syst. Rehabil. Eng., vol. 30, pp. 329–339, 2022.\n[16] A. Kline, N. D. Forkert, B. Felfeliyan, D. Pittman, B. Goodyear, and\nJ. Ronsky, “FMRI-informed EEG for brain mapping of imagined lower\nlimb movement: Feasibility of a brain computer interface,” J. Neurosci.\nMethods, vol. 363, Nov. 2021, Art. no. 109339.\n[17] Y . Miao and V . J. Koomson, “A CMOS-based bidirectional brain\nmachine interface system with integrated fdNIRS and tDCS for closed-\nloop brain stimulation,” IEEE Trans. Biomed. Circuits Syst., vol. 12,\nno. 3, pp. 554–563, Jun. 2018.\n[18] X. Zhang, L. Yao, X. Wang, J. Monaghan, D. McAlpine, and Y . Zhang,\n“A survey on deep learning-based non-invasive brain signals: Recent\nadvances and new frontiers,” J. Neural Eng., vol. 18, no. 3, Jun. 2021,\nArt. no. 031002.\n[19] P. D. E. Baniqued et al., “Brain-computer interface robotics for hand\nrehabilitation after stroke: A systematic review,” J. Neuroeng. Rehabil.,\nvol. 18, no. 1, pp. 1–25, 2021.\n[20] N. Robinson, W. J. Chester, and S. Kg, “Use of mobile EEG in decoding\nhand movement speed and position,” IEEE Trans. Hum.-Mach. Syst.,\nvol. 51, no. 2, pp. 120–129, Apr. 2021.\n2358 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\n[21] C. Ieracitano, F. C. Morabito, A. Hussain, and N. Mammone, “A hybrid-\ndomain deep learning-based BCI for discriminating hand motion plan-\nning from EEG sources,” Int. J. Neural Syst., vol. 31, no. 9, Sep. 2021,\nArt. no. 2150038.\n[22] L. Mercado et al., “Decoding the torque of lower limb joints from EEG\nrecordings of pre-gait movements using a machine learning scheme,”\nNeurocomputing, vol. 446, pp. 118–129, Jul. 2021.\n[23] A. Stroh and J. Desai, “Hand gesture-based artificial neural network\ntrained hybrid human–machine interface system to navigate a powered\nwheelchair,” J. Bionic Eng., vol. 18, no. 5, pp. 1045–1058, Sep. 2021.\n[24] T. Blom, S. Bode, and H. Hogendoorn, “The time-course of prediction\nformation and revision in human visual motion processing,” Cortex,\nvol. 138, pp. 191–202, May 2021.\n[25] R. Sosnik and L. Zheng, “Reconstruction of hand, elbow and shoul-\nder actual and imagined trajectories in 3D space using EEG cur-\nrent source dipoles,” J. Neural Eng. , vol. 18, no. 5, Oct. 2021,\nArt. no. 056011.\n[26] A. Buerkle, W. Eaton, N. Lohse, T. Bamber, and P. Ferreira, “EEG\nbased arm movement intention recognition towards enhanced safety in\nsymbiotic human–robot collaboration,” Robot. Comput.-Integr. Manuf.,\nvol. 70, Aug. 2021, Art. no. 102137.\n[27] A. Úbeda, J. M. Azorín, R. Chavarriaga, and J. D. R. Millán, “Classifi-\ncation of upper limb center-out reaching tasks by means of EEG-based\ncontinuous decoding techniques,” J. NeuroEng. Rehabil., vol. 14, no. 1,\npp. 1–14, Dec. 2017.\n[28] H. Zeng et al., “The advantage of low-delta electroencephalogram phase\nfeature for reconstructing the center-out reaching hand movements,”\nFrontiers Neurosci., vol. 13, p. 480, May 2019.\n[29] J. Jeong, K. Shim, D. Kim, and S. Lee, “Brain-controlled robotic\narm system based on multi-directional CNN-BiLSTM network using\nEEG signals,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 28, no. 5,\npp. 1226–1238, May 2020.\n[30] F. Shakibaee, E. Mottaghi, H. R. Kobravi, and M. Ghoshuni, “Decoding\nknee angle trajectory from electroencephalogram signal using NARX\nneural network and a new channel selection algorithm,” Biomed. Phys.\nEng. Exp., vol. 5, no. 2, Jan. 2019, Art. no. 025024.\n[31] S. Pancholi, A. Giri, A. Jain, L. Kumar, and S. Roy, “Source aware\ndeep learning framework for hand kinematic reconstruction using EEG\nsignal,” 2021, arXiv:2103.13862.\n[32] K. Little, B. K. Pappachan, S. Yang, B. Noronha, D. Campolo, and\nD. Accoto, “Elbow motion trajectory prediction using a multi-modal\nwearable system: A comparative analysis of machine learning tech-\nniques,” Sensors, vol. 21, no. 2, p. 498, Jan. 2021.\n[33] Y . J. Kim et al., “A study on a robot arm driven by three-dimensional\ntrajectories predicted from non-invasive neural signals,” Biomed. Eng.\nOnLine, vol. 14, no. 1, pp. 1–19, Dec. 2015.\n[34] V . Mondini, R. J. Kobler, A. I. Sburlea, and G. R. Müller-Putz,\n“Continuous low-frequency EEG decoding of arm movement for closed-\nloop, natural control of a robotic arm,” J. Neural Eng., vol. 17, no. 4,\nAug. 2020, Art. no. 046031.\n[35] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017, pp. 1–11.\n[36] X. Deng, J. Zhu, and S. Yang, “SFE-Net: EEG-based emotion\nrecognition with symmetrical spatial feature extraction,”\nin Proc. 29th ACM Int. Conf. Multimedia , Oct. 2021,\npp. 2391–2400.\n[37] R. Li, Y . Wang, and B.-L. Lu, “A multi-domain adaptive\ngraph convolutional network for EEG-based emotion recogni-\ntion,” in Proc. 29th ACM Int. Conf. Multimedia , Oct. 2021,\npp. 5565–5573.\n[38] C. Raffel et al., “Exploring the limits of transfer learning with a\nunified text-to-text transformer,” J. Mach. Learn. Res., vol. 21, no. 140,\npp. 1–67, 2020.\n[39] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “BENDR: Using trans-\nformers and a contrastive self-supervised learning task to learn from\nmassive amounts of EEG data,” Frontiers Hum. Neurosci., vol. 15,\nJun. 2021, Art. no. 653659.\n[40] P. Nagabushanam, S. Thomas George, and S. Radha, “EEG signal\nclassification using LSTM and improved neural network algorithms,”\nSoft Comput., vol. 24, no. 13, pp. 9981–10003, Jul. 2020.\n[41] N. Mammone, C. Ieracitano, and F. C. Morabito, “A deep CNN approach\nto decode motor preparation of upper limbs from time-frequency maps\nof EEG signals at source level,” Neural Netw., vol. 124, pp. 357–372,\nApr. 2020.\n[42] P. Wang, Y . Zhou, Z. Li, S. Huang, and D. Zhang, “Neural decoding of\nChinese sign language with machine learning for brain-computer inter-\nfaces,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29, pp. 2721–2732,\n2021.\n[43] J. Li, C. Xu, Z. Chen, S. Bian, L. Yang, and C. Lu, “HybrIK: A hybrid\nanalytical-neural inverse kinematics solution for 3D human pose and\nshape estimation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2021, pp. 3383–3393.\n[44] D. Pavllo, C. Feichtenhofer, M. Auli, and D. Grangier, “Modeling human\nmotion with quaternion-based neural networks,” Int. J. Comput. Vis.,\nvol. 128, no. 4, pp. 855–872, Apr. 2020.\n[45] Z. Wang, J. Chang, B. Li, C. Wang, and C. Liu, “Kinematics solution\nof snake-like manipulator based on improved backbone mode method,”\nin Proc. IEEE Int. Conf. Mechatronics Autom. (ICMA), Oct. 2020,\npp. 1774–1779.\n[46] R. Sosnik and O. B. Zur, “Reconstruction of hand, elbow and shoulder\nactual and imagined trajectories in 3D space using EEG slow cortical\npotentials,” J. Neural Eng., vol. 17, no. 1, Feb. 2020, Art. no. 016065.\n[47] A. Bhattacharya, T. Baweja, and S. P. K. Karri, “Epileptic seizure\nprediction using deep transformer model,” Int. J. Neural Syst., vol. 32,\nno. 2, Feb. 2022, Art. no. 2150058.\n[48] J. Zhao, Y . Zhao, and J. Li, “M3TR: Multi-modal multi-label recognition\nwith transformer,” in Proc. 29th ACM Int. Conf. Multimedia, Oct. 2021,\npp. 469–477.\n[49] Y . Huang, H. Xue, B. Liu, and Y . Lu, “Unifying multimodal transformer\nfor bi-directional image and text generation,” in Proc. 29th ACM Int.\nConf. Multimedia, Oct. 2021, pp. 1138–1147.\n[50] Z. Jia, Y . Lin, X. Cai, H. Chen, H. Gou, and J. Wang, “SST-\nEmotionNet: Spatial–spectral–temporal based attention 3D dense net-\nwork for EEG emotion recognition,” in Proc. 28th ACM Int. Conf.\nMultimedia, Oct. 2020, pp. 2909–2917.\n[51] Y . Zhang, B. Wu, W. Li, L. Duan, and C. Gan, “STST: Spatial–temporal\nspecialized transformer for skeleton-based action recognition,” in Proc.\n29th ACM Int. Conf. Multimedia, Oct. 2021, pp. 3229–3237.\n[52] Q. Tong, G. Liang, and J. Bi, “Calibrating the adaptive learning\nrate to improve convergence of Adam,” Neurocomputing, vol. 481,\npp. 333–356, Apr. 2022.\n[53] M. Spuler, A. Sarasola-Sanz, N. Birbaumer, W. Rosenstiel, and\nA. Ramos-Murguialday, “Comparing metrics to evaluate performance\nof regression methods for decoding of neural signals,” in Proc. 37th\nAnnu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC) , Aug. 2015,\npp. 1083–1086.\n[54] Y .-F. Chen et al., “Continuous bimanual trajectory decoding of coordi-\nnated movement from EEG signals,” IEEE J. Biomed. Health Informat.,\nvol. 26, no. 12, pp. 6012–6023, Dec. 2022.\n[55] S. Pancholi, A. Giri, A. Jain, L. Kumar, and S. Roy, “Source aware\ndeep learning framework for hand kinematic reconstruction using\nEEG signal,” IEEE Trans. Cybern., early access, May 9, 2022, doi:\n10.1109/TCYB.2022.3166604.\n[56] Y .-E. Lee and S.-H. Lee, “EEG-transformer: Self-attention from trans-\nformer architecture for decoding EEG of imagined speech,” inProc. 10th\nInt. Winter Conf. Brain-Comput. Interface (BCI), Feb. 2022, pp. 1–4."
}