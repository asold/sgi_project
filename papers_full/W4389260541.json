{
    "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
    "url": "https://openalex.org/W4389260541",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2972775501",
            "name": "Lujia Shen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2808051680",
            "name": "Yuwen Pu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2168495572",
            "name": "Shouling Ji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2113213391",
            "name": "Changjiang Li",
            "affiliations": [
                "Zhejiang University",
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2097984909",
            "name": "Xuhong Zhang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2135715740",
            "name": "Chunpeng Ge -",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A306707642",
            "name": "Ting Wang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2787708942",
        "https://openalex.org/W3126451824",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2913180865",
        "https://openalex.org/W3213831029",
        "https://openalex.org/W2184135559",
        "https://openalex.org/W4225691603",
        "https://openalex.org/W2911634294",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2787733970",
        "https://openalex.org/W3201396039",
        "https://openalex.org/W3026228083",
        "https://openalex.org/W2783097478",
        "https://openalex.org/W3092557781",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W2942091739",
        "https://openalex.org/W2798812533",
        "https://openalex.org/W4206857759",
        "https://openalex.org/W2970449623",
        "https://openalex.org/W4292794158",
        "https://openalex.org/W2971970905",
        "https://openalex.org/W3033863583",
        "https://openalex.org/W3152956381",
        "https://openalex.org/W3202495522",
        "https://openalex.org/W2997056298",
        "https://openalex.org/W4287752693",
        "https://openalex.org/W3024608270",
        "https://openalex.org/W2905526464",
        "https://openalex.org/W3083878034",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4285232587",
        "https://openalex.org/W3015189805",
        "https://openalex.org/W2781957615",
        "https://openalex.org/W2344365922",
        "https://openalex.org/W3208278397",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2950787360",
        "https://openalex.org/W2997532515",
        "https://openalex.org/W2949128310",
        "https://openalex.org/W3210951978",
        "https://openalex.org/W3175865316",
        "https://openalex.org/W2995368830",
        "https://openalex.org/W3168406300",
        "https://openalex.org/W3134094038",
        "https://openalex.org/W3150919882",
        "https://openalex.org/W2892090366",
        "https://openalex.org/W3204492454",
        "https://openalex.org/W3203135439",
        "https://openalex.org/W4286582257",
        "https://openalex.org/W3022779610",
        "https://openalex.org/W2902560707",
        "https://openalex.org/W3049021013",
        "https://openalex.org/W3177184533",
        "https://openalex.org/W3154971029",
        "https://openalex.org/W3197741062",
        "https://openalex.org/W2787146684",
        "https://openalex.org/W4281609047",
        "https://openalex.org/W3173968138",
        "https://openalex.org/W4285254085",
        "https://openalex.org/W2940009958",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3012942774",
        "https://openalex.org/W3111507638",
        "https://openalex.org/W2981878122",
        "https://openalex.org/W3006647218",
        "https://openalex.org/W4285296417",
        "https://openalex.org/W4287689183",
        "https://openalex.org/W3042135962",
        "https://openalex.org/W3213164726",
        "https://openalex.org/W2963859254",
        "https://openalex.org/W3007685714",
        "https://openalex.org/W2963143631",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W3191453585",
        "https://openalex.org/W3175402282",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4205758343",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4287670326",
        "https://openalex.org/W4289285778",
        "https://openalex.org/W3006622081",
        "https://openalex.org/W3082426877",
        "https://openalex.org/W4289422878",
        "https://openalex.org/W4287813862",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4288415846",
        "https://openalex.org/W3155936402",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W2963539306",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2982054702",
        "https://openalex.org/W3100646226",
        "https://openalex.org/W4287763432",
        "https://openalex.org/W2160536005",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W3204647170",
        "https://openalex.org/W4287184629",
        "https://openalex.org/W2950048339",
        "https://openalex.org/W2962818281",
        "https://openalex.org/W2963834268"
    ],
    "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in\\nnatural language processing (NLP) due to their exceptional performance.\\nHowever, recent studies show their vulnerability to textual adversarial attacks\\nwhere the model's output can be misled by intentionally manipulating the text\\ninputs. Despite various methods that have been proposed to enhance the model's\\nrobustness and mitigate this vulnerability, many require heavy consumption\\nresources (e.g., adversarial training) or only provide limited protection\\n(e.g., defensive dropout). In this paper, we propose a novel method called\\ndynamic attention, tailored for the transformer architecture, to enhance the\\ninherent robustness of the model itself against various adversarial attacks.\\nOur method requires no downstream task knowledge and does not incur additional\\ncosts. The proposed dynamic attention consists of two modules: (I) attention\\nrectification, which masks or weakens the attention value of the chosen tokens,\\nand (ii) dynamic modeling, which dynamically builds the set of candidate\\ntokens. Extensive experiments demonstrate that dynamic attention significantly\\nmitigates the impact of adversarial attacks, improving up to 33\\\\% better\\nperformance than previous methods against widely-used adversarial attacks. The\\nmodel-level design of dynamic attention enables it to be easily combined with\\nother defense methods (e.g., adversarial training) to further enhance the\\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\\npreserves the state-of-the-art robustness space of the original model compared\\nto other dynamic modeling methods.\\n",
    "full_text": null
}