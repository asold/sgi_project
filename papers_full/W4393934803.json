{
  "title": "Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner's Students",
  "url": "https://openalex.org/W4393934803",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5012757472",
      "name": "Merlin Carl",
      "affiliations": [
        "Europa-Universität Flensburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4322616316",
    "https://openalex.org/W4229693731",
    "https://openalex.org/W3033903890",
    "https://openalex.org/W6800827905",
    "https://openalex.org/W2994190437",
    "https://openalex.org/W1575002400",
    "https://openalex.org/W4282045675",
    "https://openalex.org/W3098556021",
    "https://openalex.org/W3100586651",
    "https://openalex.org/W2184432576",
    "https://openalex.org/W1980510330",
    "https://openalex.org/W3189373506",
    "https://openalex.org/W1514829103"
  ],
  "abstract": "We describe two systems currently being developed that use large language models for the automatized correction of (i) exercises in translating back and forth between natural language and the languages of propositional logic and first-order predicate logic and (ii) exercises in writing simple arguments in natural language in non-mathematical scenarios.",
  "full_text": "J. Narboux, W. Neuper and P. Quaresma (Eds.): Workshop on\nTheorem proving components for Educational software 2023 (ThEdu’23)\nEPTCS 400, 2024, pp. 28–43, doi:10.4204/EPTCS.400.3\n© M. Carl\nThis work is licensed under the\nCreative Commons Attribution License.\nUsing Large Language Models for (De-)Formalization and\nNatural Argumentation Exercises for Beginner’s Students\nMerlin Carl\nEUF\nFlensburg, Germany\nInstitut für Mathematik\nEuropa-Universität Flensburg\nFlensburg, Germany\nmerlin.carl@uni-flensburg.de\nWe describe two systems currently being developed that use large language models for the autom-\natized correction of (i) exercises in translating back and forth between natural language and the\nlanguages of propositional logic and first-order predicate logic and (ii) exercises in writing simple\narguments in natural language in non-mathematical scenarios.\n1 Using Large Language Models for Autoformalization\nAutoformalization, i.e., the automated translation from natural language to formal logic, is a natural\nlanguage processing task that has been adressed in a number of ways; natural language proof check-\ning systems such as Naproche (see, e.g., Cramer, [9]) and SAD (Verchinine et al., [15]) make use of\ngrammar-based approaches, with natural-language parsers producing intermediate formats between nat-\nural language and formal logic. Recently, the interest in machine learning approaches has increased,\nincluding the use of neural networks (Azerbayev et al., [2], Wang et al., [14]) and large language models\n(Wu et al., [16]); autoformalization for mathematics at the undergraduate level was explored in Azer-\nbayev et al. [2]. Two strong large language model that are publically available are OpenAI’s GPT-3.5,\nwith the associated text completion model text-davinci-003 1, and the more recent GPT-4-Turbo. 2 One\nadvantage of pretrained models over training neural networks is that frequently, a few examples are suf-\nficient to obtain a stable and usable model. It has been observed in [2] that GPT can successfully be used\nfor autoformalization. Not surprisingly, the performance improves drastically when one merely demands\nautoformalization of expressions in a controlled natural language (CNL) rather than arbitrary sentences\nin natural (mathematical) language, which can be used, e.g., for automated proof checking in teaching\ncontexts, see [7].\nFor the applications we describe in this article, however, we are interested not in translating mathe-\nmatical sentences into formal logic, but rather statements in natural language on everyday matters. To\nthis end, we wrote prompts for text-davinci-003 consisting of different numbers of examples, depending\non the complexity of the task:\n• 30 examples for the translation of natural language sentences into formulas in propositional logic\nwith a given notation.\n1See https://platform.openai.com/docs/models/gpt-3-5.\n2See https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo .\nM. Carl 29\n• 44 examples for the translation of natural language sentences into formulas in propositional logic,\ncombined with a classification determining whether that sentences formulates a claim or an as-\nsumption.\n• 56 examples for the translation of natural language sentences into formulas in first-order predicate\nlogic.\nTypical example prompts for the three tasks looked like this (original German; translated to English\nfor the convenience of the reader):\n• notation:{S:Fritz takes a boat;F:Fritz takes a plane;A:Fritz arrives in America;K:Fritz tries to\nswim}Fritz arrives in America if and only if he takes a boat or a plane, but not if he tries to\nswim.3#((S ∨F) ↔ A)∧(K → ¬A)§\n• notation:{W:This is supposed to be a joke;L:This is supposed to be funny;N:This is new}If this is\nsupposed to be a joke, it is neither funny nor new.♯[claim4,[W,→,[neg,[L,or,N]]]]§\n• notation:{B(x,y):x is the brother of y;S(x,y):x is the sister of y}The sister of someone’s brother is\nthat someone’s sister.♯∀x : ∀z : (∃y : (B(x,y)∧S(y,z)) → S(x,z))§\nAs one can see, these examples are structured as follows: the first part of the form “notation:{...}”\nintroduces a number of propositional letters, predicate letters and constant symbols, along with their\nintended semantics; this is followed by a sentence in natural language, a sharp serving as a separation\nsymbol, a formalization of the natural language sentence in the given notation and a stop symbol. In each\ncase, a few examples were added where either the natual language sentence was not a sentence at all, but\nrather some nonsense string, or could not be expressed in the given notation.\nRequests to the model prompted in this way can then be made by expanding the prompt by a string\nof the form “notation:{...}φ♯”, where φ is a natural language sentence. Experiments showed a satisfying\nperformance on the intended kind of (simple) example sentences. An impressive feature was that formal-\nizations worked well even when the precise formulation did not use the expressions given in the prompt.\nThus, in the notation “R: It rains, S: There is a storm; P-the party will be cancelled”, the sentence “If it\npours or there is a strong wind, there will be no feast” was (correctly) formalized as (R ∨S) → P.\nThe main drawbacks were the following:\n• The model showed a tendency to report sentences with strange, wrong or absurd content, such as\n“If the moon is made of green cheese, then there is a giraffe on the moon” are as erroneous, even\nthough they could easily be formalized within the given notation. Adding an explanation of the\nspecific meaning of “error” in this context and several further examples did not solve this issue.\nConsequently, such examples, which are somewhat typical for logic classes, should at the moment\nbe avoided when using the model.\n• The model showed a certain tendency to use all pieces of the given notation in its formalization, so\nthat, e.g., “Barking dogs don’t bark”5 was formalized as ∀x((D(x)∧B(x)) → S(x)) in the notation\nwhere D(x) stood for “x is a dog”, B(x) for “x barks” and S(x) for “x bites”. This was considerably\nimproved by adding several examples with superfluous notation.\n3Note that the system is to be used by students in Germany.\n4Translated for the convenience of the reader; in the actual prompt, we used “beh” for “Behauptung”, which represents\nclaims in the internal Diproche format, see [5]. For assumptions, it would have been “vss” for “V oraussetzung”.\n5Note that this is not a typo. In order to evaluate how true the model formalizes the given sentence, rather than some more\ncommon substitute for it, we deliberately used this (absurd) sentence rather than the well-known proverb that barking dogs\ndon’t bite.\n30 Deformalization Exercises with Large Language Models\n• The model showed a tendency to “project” expressions onto the given notation; for example, in the\nnotation from the last bullet point, “Barking cats don’t bite” was formalized as∀x((D(x)∧B(x)) →\nS(x)), even though no predicate letter for “cat” was contained in the notation (“meowing cats don’t\nbite”, however, led to an error message). Whether or not this presents a serious issue for the\nintended application remains to be seen.\n• For sentences of high logical complexity (e.g., containing several quantifier alternations or junc-\ntors), the formalizations were frequently wrong. This, however, does not present much of an issue\nfor the intended application.\n• The disadvantages of using cloud-based LLMs include (i) their lack of stability (continued training\ncan lead to a much worse performance, rendering working applications unusable), (ii) their lack\nof reliable availability (models may stop being available altogether, and temporarily become un-\navailable due to server capacity issues) and (iii) pricing. For these reasons, we expect our system\nto remain in an experimental state until workable local alternatives become available.\nBy now, text-davinci-003, which was state of the art when our system was developed early in 2023,\nis considered “legacy” by OpenAI and its use deprecated. Thus, additionally, we tried the same task with\nan AI-“assistant” based on the more recent model GPT-4-Turbo. Here, we achieved a surprisingly good\nperformance using merely a prompt explaining the notation to be used (i.e., without offering any initial\nexample cases); in contrast to the above examples, however, the notation – i.e., the abbreviations for\nthe atomic propositions, predicate and constant symbols to be used in the formalization – was fixed and\nthe same for all the examples. The precise prompts, along with the example sentences and the obtained\noutput for 50 instances of first-order-formalization and 57 instances of formalization in propositional\nlogic, can be found in the appendix. As can be seen from the examples (54)-(57) for propositional logic,\n“nonsense” sentences concerning strongly counterfactual scenarious did no longer pose an issue for GPT-\n4-Turbo. Concerning logically absurd sentences, such as (43), (44), (47), (48), one of these (44) was still\n(wrongly) labeled as “not expressable”,6 while the other two were processed correctly. In formalization\nin quantifier logic, all “absurd” examples, namely (24), (26), (33), were processed correctly. While a\ncertain flexibility of expression was retained – for example, the sentence “In terms of size, Fritz surpasses\nhimself” was formalized correctly given a notation that contained a two-place predicate for “larger than”\n– the issue of replacing non-expressible terms by terms in the vocabulary did no longer show up: Thus,\nthe propositional examples (20), (21), (27), (57) were correctly identified as “not expressable”.\nFor the task of formalization in propositional logic, we also evaluated several large language models\nthat are available locally, i.e., they can be run on the user’s local machine rather than remotely. The\nperformance of general local LLMs and also of LLMs focusing on mathematics – such as WizardMath-\n70B7 – turned out to be poor; the only reasonable results were obtained with models trained on the task\nof code-writing, in particular WizardCoder-34B.8\nFor propositional logic, GPT-4-Turbo formalized 55 of the 57 sample sentences correctly, or about\n96.5 percent. The best performance of a local large language model for the same set of sentences was\nsuccessful for 40 sentences, corresponding to a success rate of about 69 percent. As can be seen in lines\n(54)-(57), highly counterfactual content – which was a serious issue for davinci-003 – was consistently\n6When asked for the reason why (44) would not be expressable, the model replied that “the statement is self-contradictory\nand cannot be expressed meaningfully using the given notation or any standard logical operators because it violates the law of\nnon-contradiction. Thus, the proper response is “not expressable”.”, thus explicitly defending the (wrong) claim that logically\ninconsistent statements cannot be expressed in the formalism of propositional logic.\n7See https://huggingface.co/TheBloke/WizardMath-70B-V1.0-GGML .\n8See https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0 .\nM. Carl 31\nprocessed correctly by both models. Logically absurd statements, such as (43), (44), (48) were falsely\nlabeled as “not expressible” once by both models; here, GPT-4 generated a lengthy explanation claiming\nthat contradictory statements cannot be expressed in formal logic. Tautologous statements, such as (47)\nwere unproblematic, as were factually wrong statements for both models. Both models could success-\nfully handle a certain freedom of expression, as is demonstrated by the sentences (17), (18), (19), (31),\n(32), (34). Both models still had an issue with intricate logical structure, such as triple negation (38);\nas such examples are likely to arise rarely in the training data, this is to be expected. While GPT-4-\nTurbo clearly outperforms the local model, both models show usable results for the intended application,\nnamely simple exercises of limited logical complexity.\nFor the task of autoformalizing natural language sentences in first-order logic, no local LLM showed\na satisfying performance. The best results were obtained with GPT-4-Turbo, where 46 out of 50 provided\nexample sentences were formalized correctly, i.e., 92 percent. The unexpressable sentences (18), (32),\n(36) were correctly identified. In cases where the content was counterfactual or absurd, the model stated\nthis fact in a text comment accompanying the formalization, but still provided a correct formalization\n(see (24), (26), (27), (33)). For some of those sentences that were not formalized as expected, the\nmodel provided explanations: In the case of (13) (“If Hector barks, he is not a real dog”), the model\nexplained that the notation did not allow for expressing “is not a real dog”; given common ways of using\n“real XYZ”, this is arguably correct; a similar point can be made for (46). Without these examples, the\nsuccess rate would be about 96 percent. An actual mistake is (34), which was not expressible, as no\n2-place-predicate “x bites y” was given in the notation; in (35), the formulation “baying canine” was\napparently too “off” in order to be identified as “barking dog”. However, such formulations are not to\nbe expected from someone seriously attempting to solve a deformalization exercise, so that the practical\nrelevance of this mistake for the intended application appear limited.\nWe point out that the system is currently in an experimental stage. In particular, several practical\nchallenges in terms of pricing and stability of the underlying LLMs will need to be overcome before the\nsystem can be actually employed (and tested) in teaching.\n2 Automatized Correction of (De-)Formalization Exercises\nBeginner students frequently have difficulties both with expressing statements in the language of logic\nand with interpreting statements written in the language of formal logic. Even after becoming acquainted\nto the meaning of the logical symbols, extracting the meaning remains a challenge: Frequently, a sentence\nsuch as ∃x : child(x) ∧swims(x) will be read out as “There is x such that x is a child and x swims”,\nremaining close to the surface structure of the formula, rather than something like “Some child swims”.\nIndeed, the relation between “natural” ways of thinking of something and the way it is expressed in\nformal logic is quite intricate; one notorious example of this being the way of expressing dynamics via\nquantifier changes.9\nFormalization and deformalization exercises are meant to practice this crucial skill in university\neducation in mathematics. In a formalization exercise – also known as “math dictation”, see, e.g., [4] 10\n– a sentence in natural language is given, together with some formal vocabulary, and the student’s task is\nto produce a logical formula expressing this sentence. Thus, a typical formalization exercise could look\n9For example, in the definition of continuity, the idea of “moving one point closer to another” is a dynamical conception\nexpressed via quantifier changes; for a detailed discussion, see the considerations of the concept of continuity in Lakoff and\nNunez [12], p. 309-315.\n10We originally learned this term from Michael Junk.\n32 Deformalization Exercises with Large Language Models\nlike this:\n• Let S stand for the statement “The sun shines”,W for the statement “I go out for a walk”. Formalize\nthe statement “I won’t go for a walk unless the sun shines” in the language of propositional logic.\n• Let C(x) stand for “ x is a child”, S(x) for “x swims”. Formalize the statement “Some children\nswim” in the language of first-order predicate logic.\nIt is not too hard to provide automated feedback for such formalization exercises: One can, for\nexample, store the exercise in the form of a pair (η,φ) consisting of the natural language statement η to\nbe displayed to the user and a correct formalization φ of it, take the user’s input, determine whether it is\na well-formed formula at all (and, if not, provide according feedback), and, in case it is, pass the tasks\nφ → η and η → φ to an automated theorem prover, such as the PyProver, which can be imported into\nPython code as a package. Systems that work roughly in this way include the “mathematical logic tutor”\nof Moreno et al. [13] or the formalization exercises in Edukera [1].\nWe remark here11 that the concept of formalization is rather intricate. Indeed, a formalization in\nthe sense above is a special case of a translation, and so the difficult question about the adequacy of\ntranslations also applies to formalizations: When is a formula φ an adequate formalization of a natural\nlanguage sentence S? A naive approach might be to say that φ and S must be in some sense “provably\nequivalent”; this, however, leads to several difficulties: First, one would have to make precise the infor-\nmal notion of proof required to make sense of this criterion. Second, such a notion would necessarily\nbe relative to the set of accepted background assumptions, which are usually left implicit and may be\nhighly context-dependent. For example, when asked to formalize “If a, b are the lengths of the catheti\nof a right triangle and c is the length of its hypotenuse, then c3 > a3 + b3”, it should be acceptable – if\ntheory-laden – to respond with∀a,b,c ∈ R+((c > a+b∧a2 +b2 = c2) → c3 > a3 +b3), thus presuppos-\ning Pythagoras’ theorem; but if the sentence would be to formalize “If a, b are the lengths of the catheti\nof a right triangle and c is the length of its hypotenuse, then a2 +b2 = c2”, it would be most inadequate\nto formalize this in the same way, thus leading to a tautology. (One might even ask whether it can be for-\nmalized adequately in the language of real closed fields at all without circularity.) Third, mere provable\nequivalence is a poor criterion for the adequacy of a formalization; for example, formalizing either of\nthe preceding statements as 1 = 1 should certainly not be counted as correct.12 Other great examples of\nformalization, such as Gödel’s arithmetization of the concept of first-order provability require a creative\nmastering of some background theory. This kind of formalizing is a task resembling programming more\nthan translation, and it is not what the system sketched in this paper is meant to teach. The difference\nbetween “translation-like formalization” and “programming-like formalization”, although didactically\nquite obvious, is not easy to pin down logically. However, by considering everyday rather than mathe-\nmatical contexts, this issue is mostly avoided: There is not much background theory that could be used\nin expressing a sentence such as “Barking dogs don’t bite” into a first-order language with a vocabulary\nfor “barks”, “bites” and “is a dog”, let alone for interpreting the respective first-order formula in natu-\nral language. We will thus accept a formula φ as a correct formalization of a sentence S with a fixed\ngiven formalization ψ if and only if the equivalence φ ↔ ψ is a tautology in the respective logic (that\nis, propositional logic or first-order logic), i.e., using the empty background theory. This seems to yield\na didactically acceptable notion of adequacy, as long as the sentences under consideration are neither\ntautological nor contradictory – although “weird” formalizations accepted by a system working with this\npremise are still possible (e.g. by writing something like A ∨(A ∧A) or ¬¬¬¬A rather than A, or by\n11We thank one of our referees for pointing out that this point warrants a discussion.\n12Note that the former statement is correct, as a3 +b3 < a2c +b2c = c2 ·c = c3.\nM. Carl 33\nforming a conjunction with a number of unrelated tautologies), they are unlikely to actually be proposed\nby students, except by those who have already mastered the subject at hand anyway.\nDeformalization exercises, on the other hand, are a far more delicate matter. In a deformalization\nexercise, a student is given a formal vocabulary together with a logical formula using that vocabulary,\nand is asked to express it in natural language in the simplest possible terms. Thus, typical deformalization\nexercises look as follows:\n• Let S stand for the statement “The sun shines”, W for the statement “I go out for a walk”. Express\nthe statement W → S in natural language.\n• Let C(x) stand for “ x is a child”, S(x) for “x swims”. Express the statement ∃x : C(x) ∧S(x) in\nnatural language.\nThe automatized correction of such exercises is challenging in at least to ways: First, one needs to\nautomatically translate the user’s natural language input into the appropriate formal language, using the\nspecified vocabulary. Second, one needs to grade the “naturalness” of the user’s input, so that expressions\nsuch as “There is x such that x is a child and x swims” mentioned above receive a different feedback than\n“Some children swim”.\nIn our system, which is currently in an experimental stage but will eventually be integrated into the\nDiproche system ([8], [5], [7], [4], [6]), exercises are stored as triples (n,η,φ), where n is the available\nvocabulary, η is a natural language sentence and φ is the formalization thereof, which should be “opti-\nmal” in terms of naturalness. This kind of representation has the advantage that the same exercise can\nboth be used as a formalization exercise (displayingn and η, then asking for φ) and as a deformalization\nexercise (displaying n and φ, then asking for η).\n2.1 Checking Deformalizations\nIn this section, we will explain the architecture of the checking routine for deformalization exercises.\nAs explained above, users are given a set of notations and a formula using these notations. They can\nthen enter an arbitrary string in a text window in a web interface. This input string is passed on to\nthe formalization routine, which uses the input and the training examples to generate a request for a\nlarge language model (such as text-davinci-003) and passes it on to this LLM; if this yields the output\n“error”, an error message is displayed to the user and no further processing takes place; otherwise, the\nformalization of the users input and the formula given in the problem statement are passed on to the\nPyProver, which determines whether the implications between them are provable (i.e., propositional or\nfirst-order tautologies). More precisely, if ψ is the formal expression fixed in the problem statement\nand φ is the result of autoformalizing the user’s input, the PyProver is asked to prove both φ → ψ and\nψ → φ. The result of this is then passed on to the feedback creation, which will report which of these\nimplications could be verified.\nHowever, a merely logically correct answer does not imply that the input string is a good defor-\nmalization of the given formula, or that the user has understood this formula. Typically, beginners will\ntranslate a formula such as ∀x((D(x) ∧B(x)) → ¬S(x)) – in the vocabulary given above – “word for\nword” into something like “For all x, if x is a dog and x barks, then x does not bite”. Thus, in order\nto provide a meaningful assessment of the input, the mere logical correctness needs to be supplemented\nwith an evaluation of the “naturalness” or “simplicity” of the input string. This is the task of the “Grader”\nmodule.\nOur attempts to train language models to provide such an assessment were utterly unsuccessful so\nfar: Even with a considerable number of examples, the model appeared to be unable to distinguish\n34 Deformalization Exercises with Large Language Models\nunnaturally formulated sentences from perfectly naturally formulated sentences with an unusual content.\nWe thus resorted to a rather simple-minded solution, which, however, turned out to work quite well for\nour purpose: We measure the complexity of the input by relating its length to the length of the template\nsolution entered as part of the problem statement (although not displayed to the user) and normalize the\nresult. A “word for word translation” of logical syntax into natural language will usually be considerably\nlonger than the shortest formulation in natural language, so that this can be expected to approximate the\ndegree of naturalness satisfyingly well. More precisely, if|s| denotes the length of a string,η denotes the\ntemplate solution in the problem statement and φ is the user’s input, the degree of simplicity is given by\n10σ(10(|η|\n|φ| −0.7))\nwhere σ is the usual sigmoid function, given by σ(x) := 1\n1+e−x . This yields a measure of the input’s\nsimplicity on a scale from 0 (not natural at all) to 10 (very natural); feedback is provided depending\non whether this value is less than or equal to 5, strictly between 5 and 8 or equal to or above 8. Thus,\na good solution, which will have |η| ≈ |φ|, will have σ(10(|η|\n|φ| −0.7)) close to (but below) 1, while a\novercomplicated solution with |φ| much larger than η will lead to a value close to 0. A score of at least\n5 is awarded if the template solution has a length that is 70 percent (or less) of the length of the input\nstring.\nFinally, the feedback creation either reports that the input is logically incorrect (not equivalent to\nthe formula to be expressed) and, in this case, whether it is necessary, but not sufficient, sufficient,\nbut not necessary, or neither; in this case, no evaluation of the simplicity of the input is given. If, on\nthe other hand, the input was logically correct, then the user is provided with feedback reporting this,\naccompanied by the system’s evaluation of its simplicity, asking the user, if necessary, to try her hand at\nfurther simplifications of her expression.\nThere are at least two sources of potential mistakes in this approach: First, the LLM may provide a\nformalization not faithful to the user’s input, either by an actual formalization mistake or by interpreting\nthe natural language semantics in a subtly different way, and second, the ATP may fail to verify the\nequivalence even if the input is correct. Due to the decidability of propositional logic (and the fact that the\nexpressions arising in the given contexts will use less than 10 propositional variables and be of surveyable\nlength, so that no resource issues can occur), the latter kind of mistake can only occur for first-order logic.\nSince the formulas coming up in this context will be logically rather simple – typically restricted to a\nsingle quantifier change – this is not likely to be a frequent issue; however, a substantial evaluation of\nthis point will have to wait until the system can be actually employed with student users. 13 Potentially,\nit cannot lead to wrong solutions being reported as correct, but it could lead to correct solutions being\nreported as incorrect. In order not to confuse users, the feedback should thus be formulated as a “failure\nto verify” rather than as a claim that the solution is wrong. The user may then attempt to reformulate her\nsolution to make it easier processable. Another option would be to switch to an ATP that can generate\ncountermodels to non-verifable statements and report these back to the user. The former type of mistake\ncan lead both to correct solutions being reported as incorrect and to incorrect solutions being reported as\ncorrect. The latter issue may in particular come up for sentences that deviate from, but strongly resemble,\nvery common natural language sentences that are likely to occur in the training data. Also, issues are\n13One way to exclude such difficulties altogether would be to restrict exercises to decidable fragments of first-order logic,\nsuch as monadic first-order logic (see, e.g., [3]), formulas with at most two variables (see, e.g., [11]) etc.; these classes already\nlead to a rich supply of exercises. If this path is taken, one needs to ensure that the user’s input will also fall into the relevant\nclass.\nM. Carl 35\nInterface\nFormalize\ntext-davinci-003\nProverGrader\nFeedback Creation\nFigure 1: Flowchart for the correction routine for deformalization exercises. (Generated with the help of\nGPT-3.5.)\nlikely to occur when students write ungrammatical inputs. Again, an evaluation of the seriousness of\nsuch issues will have to wait until the system can actually be employed. To counter such difficulties, the\nformalization obtained by the system could be reported back to the user to check whether it faithfully\ncaptures what she intended to express. However, since the system is intended to teach basic skills in\ntranslating back and forth between formal logic and natural language, it is not clear that the intended\nusers will be able to determine whether the formalization was done correctly.\n3 Natural Language Argumentation\nBesides formalization, learning how to prove is another challenge for beginner students. The Diproche\nsystem ([8], [5]) is designed to provide automated feedback on the several aspects – including logical\ncorrectness – for solutions to simple beginner proving exercises written in a controlled natural language\n(CNL). Recently, large language models have been integrated in the Diproche architecture to provide a\nmore “liberal” CNL than the original parser-based one (see [7]).\nDiproche is focused on mathematical argumentation in areas such as propositional logic, Boolean set\ntheory or elementary number theory. As a preliminary exercise, it may also be helpful to practice logical\nargumentation in non-mathematical contexts, putting aside the difficulties students may have with the\nmathematical content and symbolism. Thus, a natural argumentation exercise might look like this:\n• Suppose that the following is true: If the sun shines, Hans goes for a walk. When Hans goes for a\nwalk, he takes his dog with him. When Hans takes his dog for a walk, the dog barks at the cat on\nthe neighbour’s roof. When the dog barks at the cat on the roof, the cat runs away. However, the\ncat still sits on the roof. Show that the sun does not shine.\n36 Deformalization Exercises with Large Language Models\nUsing the same prompts discussed in the last section, such statements can automatically be translated\ninto propositional logic14; the resulting formal representation can then be passed on to the checking com-\nponents of the Diproche system, which will provide feedback on the logical correctness of the argument.\nThus, an accepted solution to the above exercise could look like this:\n• The cat still sits on the roof. Hence the dog did not bark. Consequently, Hans did not take his dog\nfor a walk. So Hans did not go for a walk. Thus the sun does not shine.\nFor mathematical exercises, the Diproche system gives feedback on linguistical correctness, logical\ncorrectness, type mistakes (using variables without introducing them before, or in a wrong way, such\nas adding two propositions), success in achieving proof obligations and supposed logical or algebraic\nfallacies. For the “natural language argumentation” exercises, the feedback on type mistakes will be\nsuppressed, since this should not be an issue for the intended kind of argumentation.\n4 Further Work\nClearly, a system for didactical uses should be employed in teaching and evaluated in terms of usability\nand effectiveness. We plan to do so in the near future. On the technical side, the autoformalization\nwith text-davinci-003 or GPT-4-Turbo, although impressive in many respects, still leaves some things to\nbe desired.15 We plan to gather a substantial amount of training data and use it to fine-tune pretrained\nlanguage models. Whether this will lead to improved performance remains to be seen. Concerning\nthe formalization exercises (“math dictations”), the LLM technology could be used for automatically\ndeformalizing the user’s formal input and reporting it back to her in the case the formalization is not\ncorrect, thus making it clearer where the mistake lies; here, more traditional NLP techniques, such as the\n“pretty printer” described in [10], may also be useful.16\nMeanwhile, we believe that good use could be made of the “natural argumentation” framework in\nother subjects, in particular in philosophy: Here, it is a basic type of exercise to reconstruct plain text\narguments in a semi-formal style where assumptions and consequences are explicitly labeled, and the\napproach discussed here for the verification of natural language argumentation could then be used to\nverify whether an argument written in this way is in fact logically cogent.\n5 Acknowledgements\nWe thank our three anonymous referees for several comments that helped in improving the presentation\nof the paper, along with constructive criticism concerning its content.\nReferences\n[1] Edukera Homepage. https://www.edukera.com/.\n[2] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir R. Radev & Jeremy\nAvigad (2023): ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics. ArXiv,\ndoi:10.48550/arXiv.2302.12433. arXiv:arXiv:2302.12433v1.\n14The same can, of course, easily be done with first-order logic.\n15Recall in particular the issues with relying on remote services rather than local models.\n16We thank one of our anonymous referees for pointing out this reference to us.\nM. Carl 37\n[3] Heinrich Behmann (1922): Beiträge zur Algebra der Logik, insbesondere zum Entscheidungsproblem. Math-\nematische Annalen 86, pp. 163–229, doi:10.1007/BF01457985.\n[4] Merlin Carl (2020): Automatized Evaluatoin of Formalization Exercises in Mathematics ,\ndoi:10.48550/arXiv.2303.17513. ArXiv:2006.01800v2.\n[5] Merlin Carl (2020): Number Theory and Axiomatic Geometry in the Diproche System. Electronic Proceedings\nin Theoretical Computer Science 328, pp. 56–78, doi:10.4204/EPTCS.328.4.\n[6] Merlin Carl (2020): Using Automated Theorem Provers for Mistake Diagnosis in the Didactics of Mathemat-\nics, doi:10.48550/arXiv.2002.05083. ArXiv:2002.05083v1.\n[7] Merlin Carl (2023): Improving the Diproche CNL through Autoformalization via Large Language Models .\narXiv:arXiv:2303.17513.\n[8] Merlin Carl & Regula Krapf (2020): Diproche - ein automatisierter Tutor für den Einstieg ins Beweisen. In:\nDigitale Kompetenzen und Curriculare Konsequenzen, pp. 43–56.\n[9] Marcos Cramer (2013): Proof-checking mathematical texts in controlled natural language . Ph.D. thesis,\nRheinische Friedrich-Wilhelms-Universität Bonn.\n[10] Salwa Tabet Gonzalez, Stéphane Graham-Lengrand, Julien Narboux & Natarajan Shankar (2021): Se-\nmantic parsing of geometry statements using supervised machine learning on synthetic data . In Jasmin\nBlanchette, James H. Davenport, Peter Koepke, Michael Kohlhase, Andrea Kohlhase, Adam Naumowicz,\nDennis Müller, Yasmine Sharoda & Claudio Sacerdoti Coen, editors: Joint Proceedings of the FMM, FVPS,\nMathUI,NatFoM, and OpenMath Workshops, Doctoral Program, and Work in Progress at the Conference on\nIntelligent Computer Mathematics 2021 co-located with the 14th Conference on Intelligent Computer Math-\nematics (CICM 2021), Virtual Event, Timisoara, Romania, July 26 - 31, 2021, CEUR Workshop Proceedings\n3377, CEUR-WS.org. Available at https://ceur-ws.org/Vol-3377/natfom5.pdf.\n[11] Leon Henkin (1967): Logical Systems Containing Only a Finite Number of Symbols. Presses de l’Universite\nde Montreal, Montreal,.\n[12] George Lakoff & Rafel E. Núñez (2001): Where mathematics comes from : how the embodied mind brings\nmathematics into being. Basic Books.\n[13] A. Moreno & N. Budesca (2000): Mathematical Logic Tutor-Propositional Calculus. In: First International\nCongress on Tools for Teaching Logic, pp. 99–106.\n[14] Wang Qingxiang, Chad Brown, Cezary Kaliszyk & Josef Urban (2019): Exploration of Neural Ma-\nchine Translation in Autoformalization of Mathematics in Mizar . In: CPP 2020: Proceedings\nof the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs , pp. 85–98,\ndoi:10.1145/3372885.3373827.\n[15] Konstantin Verchinine, Alexander V . Lyaletski & Andrei Paskevich (2007):System for Automated Deduction\n(SAD): A Tool for Proof Verification. In: Automated Deduction – CADE-21. CADE 2007. , 4603, Springer,\nBerlin, Heidelberg., doi:10.1007/978-3-540-73595-3_29. Available at https://api.semanticscholar.\norg/CorpusID:6915907.\n[16] Yuhuai Wu, Albert Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik & Christian Szegedy\n(2022): Autoformalization with Large Language Models . In: 36th Conference on Neural Information Pro-\ncessing Systems (NeurIPS), doi:10.48550/arXiv.2205.12615.\n38 Deformalization Exercises with Large Language Models\n6 Appendix\nIn this appendix, we give the detailed results of testing the performance of GPT-4-Turbo and WizardCoder-\n34B in autoformalization of natural language sentence in propositional logic and of GPT-4-Turbo in\nfirst-order logic. Although text-davinci-003 was used in the version of the system described above, it is\nnow considered legacy and its use is deprecated; moreover, its performance in autoformalization tasks\nappears to have considerably dropped since spring 2023; therefore, no results for text-davinci-003 are\nlisted. We first give the prompt given to the LLM, followed by a table listing the input sentences, the\nmodel’s output and an evaluation of correctness. It should be noted that the sentences were tested in the\norder in which they are listed in the table and that other orders may lead to different results.\n6.1 Propositional Logic\nFor propositional logic, we used the following prompt; in WizardCoder-34B, temperature was set to 0,\nrepitition penalty to 1 (the minimum). Below, ⊻ is used for exclusive disjunction. In the case of missing\nbrackets, formulas were read according to the usual priority rules. In (31) and (56), a missing closing\nbracket was added in the WizardCoder output, which was still counted as correct, as this can easily be\ndone automatically.\nExpress the sentence as a formula in propositional logic, using the given notation.\nNotation:\n• S:“The sun shines”\n• R:“It rains”\n• P:“There will be a party”\n• M:“It is Monday”\n• A:“Andreas will attend the party”\n• B:“Barbara will attend the party”\n• C:“The moon is made of green cheese”\n• G:“There is a giraffe on the moon”\nFor the GPT-4-Turbo assistant, the following was added:\nIf the given sentence cannot be expressed with the given notation, return “not expressable”.\nNr Input GPT-4 +/- WizardCoder-34B +/-\n1 It does not rain and the\nsun shines.\n¬R ∧S + example\n2 It’s neither sunny nor\nrainy.\n¬S ∧¬R + ¬(S ∨R) +\n3 If there is a party on\nMonday, Andreas will\nattend it.\n(P ∧M) → A + M → A -\nM. Carl 39\n4 If it rains on Mon-\nday, Andreas will stay\naway from the party.\n(R ∧M) → ¬A + (M ∧R) → ¬A) +\n5 Andreas and Barbara\nwill only attend the\nparty together.\n(A ↔ B) + (A ↔ B) +\n6 There will be a party\nif the sun shines on\nMonday.\n(S ∧M) → P + (S → P) -\n7 It is not true that\nAndreas will abstain\nfrom the party when it\nrains.\n¬(R → ¬A) + ¬((R → ¬A)∧(M ∧R)) -\n8 On rainy Mondays,\nBarbara does not go to\nparties.\n(R ∧M) → ¬B + ((M ∧R) → ¬B) +\n9 It is a sunny, rainy\nMonday.\nS ∧R ∧M + (S ∧R ∧M) +\n10 On sunny Mondays, it\nrains.\n(S ∧M) → R + (M ∧S) → R) +\n11 Andreas and Barbara\nattend the party.\nA ∧B + (A ∧B) +\n12 At least one of An-\ndreas and Barbara at-\ntends the party.\nA ∨B + (A ∨B) +\n13 Exactly one of An-\ndreas and Barbara at-\ntends the party.\n(A ∧¬B)∨(¬A ∧B) + (A ⊻B) +\n14 Andreas parties pro-\nvided Barbara doesn’t.\n¬B → A + (→ ¬A) → P -\n15 Andreas goes to the\nparty provided Bar-\nbara doesn’t.\n¬B → A + (B → ¬A) +\n16 Barbara and Andreas\nonly visit Monday par-\nties.\n(B ∨A) → M + (M → (A ∨B))∧(¬A → M)∧(¬B → M) -\n17 On Monday, there is\ngood or bad weather.\nM → (S ∨R) + (M → (S ∨R)) +\n18 When it is wet, An-\ndreas does not go to\nthe party, but Barbara\ndoes.\nR → (¬A ∧B) + ((R → ¬B)∧B) -\n19 It is a bright Monday,\nand Andreas and Bar-\nbara jointly visit the\ncelebration.\n(S ∧M)∧(A ∧B) + (M ∧S ∧(A ∨B)) -\n20 On Tuesday, there is a\nthunderstorm.\nnot expressable + not expressable +\n21 Christa exercises on\nTuesday.\nnot expressable + not expressable +\n40 Deformalization Exercises with Large Language Models\n22 It’s a bright Monday. S ∧M + (M ∧S) +\n23 It rains if and only if\nthe sun shines.\nR ↔ S + (S → R)∧(R → S) +\n24 Sunshine and rain do\nnot occur together\n¬(S ∧R) + not expressible -\n25 Monday is no day for\nparties.\nM → ¬P + ¬M -\n26 Andreas is not a party\nperson.\n¬A / not expressable + ¬A +\n27 Andreas is not a\nTyrannosaurus.\nnot expressable + not expressable +\n28 Andreas stays away\nfrom the party.\n¬A + ¬A +\n29 Andreas does not stay\naway from the party.\nA + A +\n30 If Andreas stays away\nfrom the party, then so\ndoes Barbara.\n¬A → ¬B + (¬A → ¬B) +\n31 On sunny days, Bar-\nbara and Andreas cel-\nebrate.\nS → (A ∧B) + (S → (A ∧B) +\n32 Barbara feasts in the\nrain, but Andreas\ndoesn’t.\nR → (B ∧¬A) + (R → (B ∧¬A)) +\n33 It’s not Monday, but\nBarbara still parties.\n¬M ∧B + ¬M -\n34 In spite of the down-\npour, Andreas cele-\nbrates.\nR ∧A + (R → A) -\n35 It’s sun or rain, but not\nboth.\n(S ∨R)∧¬(S ∧R) + (S ⊻R) +\n36 On sunny and also on\nrainy days, no parties\ntake place.\n(S → ¬P)∧(R → ¬P) + not expressible -\n37 On sunny and also on\nrainy days, there are\nno parties.\n(S → ¬P)∧(R → ¬P) + ((S ⊻R) → ¬P) -\n38 It is wrong that the\nparty has not been\ncanceled.\n¬¬P / P - ¬¬P -\n39 Mondays are rainy. M → R + M ∧R -\n40 Rainy days are An-\ndreas’ party days.\nR → A + (R → A) +\n41 It is a lie that Andreas\nand Barbara attended\nthe party together.\n¬(A ∧B) + ¬(A ∧B) +\n42 Barbara parties on\nrainy Mondays.\n(R ∧M) → B + (R ∧M) → B +\n43 It’s Monday and it’s\nnot.\nM ∧¬M + (M ∧¬M) +\nM. Carl 41\n44 Andreas goes to the\nparty if he doesn’t.\nnot expressable - (¬A → A) +\n45 Barbara goes to the\nparty if and only\nif Andreas stays at\nhome.\nB ↔ ¬A + (A → B) -\n46 On party Mondays,\nthe sun shines and it\nrains.\n(P ∧M) → (S ∧R) + (M → (S ∧R)) -\n47 It’s Monday if and\nonly if it’s Monday.\nM ↔ M + (M ↔ M) +\n48 It is neither Monday\nnor not Monday.\n¬M ∧¬¬M / ¬M ∧M + not expressible -\n49 There is no party if\nAndreas isn’t there.\n¬A → ¬P + (¬A → ¬P) +\n50 Neither Andreas nor\nBarbara attend the\nparty.\n¬A ∧¬B + (¬A ∨¬B) +\n51 One of Andreas and\nBarbara does not at-\ntend the party.\n(¬A ∨¬B) + (¬A ∨¬B) +\n52 If it rains, then An-\ndreas does not attend\nMonday parties.\nR → (M → ¬A) + (R → (M ∧¬A)) +\n53 Given that it rains, the\nsun shines.\nR → S + (R → S) +\n54 If the moon is made\nof green cheese, then\nthere is a giraffe on it.\nC → G + (C → G) +\n55 There are no moon gi-\nraffes.\n¬G + ¬G +\n56 On Mondays, the\nmoon is made of\ngreen cheddar and\npopulated by giraffes.\nM → (C ∧G) + (M → (C ∧G)) +\n57 There is an elephant\non the moon.\nnot expressable + not expressible +\nmistakes (total) 2 17\n6.2 First-Order Logic\nFor first-order logic, WizardCoder-34B showed no satisfying performance; for GPT-4-Turbo, we used\nthe following prompt:\n42 Deformalization Exercises with Large Language Models\nExpress the given sentence as a formula in first-order logic, using the following notation:\n• D(x):x is a dog\n• B(b):b barks\n• S(a):a bites\n• L(a,b):a is larger than b\n• fr:Fritz\n• he:Hector\nIf the given sentence cannot be expressed with the given notation, return “not expressible”. No\ncomment or explanation; only return the formula.\nNr Input Output GPT-4 +/-?\n1 Fritz is a dog. D( f r) +\n2 Hector is taller than Fritz. L(he, f r) +\n3 If Hector barks, then he bites. B(he) → S(he) +\n4 There is a dog. ∃xD(x) +\n5 Every dog barks. ∀x(D(x) → B(x)) +\n6 Some dogs bite. ∃x(D(x)∧S(x)) +\n7 Hector does not bite. ¬S(he) +\n8 Barking dogs bite. ∀x((D(x)∧B(x)) → S(x)) +\n9 Barking dogs don’t bite. ∀x((D(x)∧B(x)) → ¬S(x)) +\n10 All dogs bite. ∀x(D(x) → S(x)) +\n11 Everything bites. ∀xS(x) +\n12 Barking dogs don’t bark. ∀x((D(x)∧B(x)) → ¬B(x)) +\n13 If Hector barks, he is not a real dog. not expressible -\n14 If Hector barks, he is not a dog. B(he) → ¬D(he) +\n15 Fritz and Hector are barking dogs. (D( f r)∧B( f r))∧(D(he)∧B(he)) +\n16 If a dog barks, then Fritz barks. ∀x((D(x)∧B(x)) → B( f r)) +\n17 No dog is bigger than Fritz. ∀x(D(x) → ¬L(x, f r)) +\n18 There is a god. not expressible. +\n19 Fritz is the tallest dog. D( f r)∧¬∃x(D(x)∧L(x, f r)) +\n20 No dog that is larger than Fritz will\nbite.\n∀x(D(x)∧L(x, f r) → ¬S(x)) +\n21 Fritz is smaller than Hector. L(he, f r) +\n22 One of Hector and Fritz barks. B(he)∨B( f r) +\n23 Some dogs are larger than both Fritz\nand Hector.\n∃x(D(x)∧L(x, f r)∧L(x,he)) +\n24 In terms of size, Fritz surpasses him-\nself.\nL( f r, f r) +\n25 Some dogs are larger than others. ∃x∃y(D(x)∧D(y)∧L(x,y)) +\n26 Each dog is larger than each dog. ∀x∀y(D(x)∧D(y) → L(x,y)) +\n27 Fritz is a dog and he isn’t. D( f r)∧¬D( f r) +\n28 Fritz is the only dog. D( f r)∧∀x(D(x) → x = f r) +\n29 Every dog is larger than every other\ndog.\n∀x∀y((D(x)∧D(y)∧x ̸= y) → L(x,y)) +\nM. Carl 43\n30 Exactly one of Hector and Fritz\nbarks.\nB(he)∧¬B( f r)∨¬B(he)∧B( f r) +\n31 Hector is larger than any other dog,\nexcept Fritz.\n∀x((D(x)∧x ̸= f r∧x ̸= he) → L(he,x)) +\n32 Meowing cats don’t scratch. not expressible +\n33 Biting dogs are not dogs and they\ndon’t bite.\n∀x((D(x)∧S(x)) → (¬D(x)∧¬S(x))) +\n34 Hector bites Fritz. S(he) -\n35 Hector is a baying canine. not expressible -\n36 Hugo is a biting dog. not expressible +\n37 If it does not bark, it’s not a dog. ∀x(¬B(x) → ¬D(x)) +\n38 It’s not a dog if it does not bark. ∀x(¬B(x) → ¬D(x) +\n39 There is more than one dog. ∃x∃y(D(x)∧D(y)∧x ̸= y) +\n40 Hector barks, so Fritz does the same. B(he) → B( f r) +\n41 Fritz barks, and so does Hector. B( f r)∧B(he) +\n42 Barking dogs and also biting dogs\nare smaller than Fritz.\n∀x((D(x)∧(B(x)∨S(x))) → L( f r,x)) +\n43 Of each two dogs, one is larger. ∀x∀y((D(x)∧D(y)∧x ̸= y) → (L(x,y)∨L(y,x))) +\n44 No two dogs are the same. ∀x∀y((D(x)∧D(y)∧x ̸= y) → ¬(x = y)) +\n45 All dogs are the same: they bite! ∀x(D(x) → S(x)) +\n46 All dogs are the same. not expressible -\n47 Hector is a dog that neither barks nor\nbites.\nD(he)∧¬B(he)∧¬S(he) +\n48 Hector and Fritz both bark, but only\none of them bites.\n(B(he)∧B( f r))∧((S(he)∧¬S( f r))∨(¬S(he)∧S( f r))) +\n49 All dogs bite, but only Fritz and\nHector bark.\n∀x(D(x) → S(x))∧∀x(D(x)∧B(x) → (x = f r∨x = he)) +\n50 Hector is not the only barker. ∃x(B(x)∧x ̸= he) +",
  "topic": "Argumentation theory",
  "concepts": [
    {
      "name": "Argumentation theory",
      "score": 0.898064374923706
    },
    {
      "name": "Computer science",
      "score": 0.608197808265686
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.5694811344146729
    },
    {
      "name": "Mathematics education",
      "score": 0.44034892320632935
    },
    {
      "name": "Linguistics",
      "score": 0.41719844937324524
    },
    {
      "name": "Natural language",
      "score": 0.41104137897491455
    },
    {
      "name": "Natural language processing",
      "score": 0.3742697238922119
    },
    {
      "name": "Psychology",
      "score": 0.3025745749473572
    },
    {
      "name": "Philosophy",
      "score": 0.11568495631217957
    },
    {
      "name": "Geography",
      "score": 0.07456359267234802
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}