{
  "title": "Training Language Models Using Target-Propagation",
  "url": "https://openalex.org/W2591470672",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5073227709",
      "name": "Sam Wiseman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5078511139",
      "name": "Sumit Chopra",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111465122",
      "name": "Marc’Aurelio Ranzato",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5053145694",
      "name": "Arthur Szlam",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028963874",
      "name": "Ruoyu Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002078510",
      "name": "Soumith Chintala",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017292577",
      "name": "Nicolas Vasilache",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2057624533",
    "https://openalex.org/W2164278908",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2963959597",
    "https://openalex.org/W17525587",
    "https://openalex.org/W2045079045",
    "https://openalex.org/W2949966795",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W1855112655",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2469894155",
    "https://openalex.org/W2117243393",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2016859302",
    "https://openalex.org/W1605920984",
    "https://openalex.org/W2606748186",
    "https://openalex.org/W2963055445",
    "https://openalex.org/W1606458877",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W179875071"
  ],
  "abstract": "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.",
  "full_text": "Training Language Models Using Target-Propagation\nSam Wiseman\nHarvard SEAS\nswiseman@seas.harvard.edu\nSumit Chopra\nFacebook AI Research\nspchopra@fb.com\nMarc’Aurelio Ranzato\nFacebook AI Research\nranzato@fb.com\nArthur Szlam\nFacebook AI Research\naszlam@fb.com\nRuoyu Sun\nFacebook AI Research\nruoyu@fb.com\nSoumith Chintala\nFacebook AI Research\nsoumith@fb.com\nNicolas Vasilache\nFacebook AI Research\nntv@fb.com\nAbstract\nWhile Truncated Back-Propagation\nthrough Time (BPTT) is the most popular\napproach to training Recurrent Neural\nNetworks (RNNs), it suffers from being\ninherently sequential (making paralleliza-\ntion difﬁcult) and from truncating gradient\nﬂow between distant time-steps. We\ninvestigate whether Target Propagation\n(TPROP) style approaches can address\nthese shortcomings. Unfortunately, ex-\ntensive experiments suggest that TPROP\ngenerally underperforms BPTT, and we\nend with an analysis of this phenomenon,\nand suggestions for future work.\n1 Introduction\nModern RNNs are trained almost exclusively us-\ning truncated Back-Propagation Through Time\n(BPTT) (Elman, 1990; Werbos, 1990; Mikolov\net al., 2010). Despite its popularity, BPTT training\nhas two major drawbacks, namely, that it is inher-\nently sequential (and thus difﬁcult to parallelize),\nand that it truncates the number of time-steps over\nwhich gradient information can propagate.\nInspired by the recent reports of success of\nTarget-Propagation (TPROP) in training non-\nrecurrent deep networks (Carreira-Perpi ˜n´an and\nWang, 2014; Lee et al., 2015; Taylor et al., 2016),\nwe explore training RNNs with TPROP, an idea\nthat has been suggested repeatedly in the litera-\nture (LeCun, 1986, 1988; Mirowski and LeCun,\n2009). TPROP can be understood as a generaliza-\ntion of backpropagation, where neural networks\nare trained by providing local targets for each hid-\nden unit. Such approaches have been motivated by\nappealing to biological plausibility, numerical sta-\nbility, computational parallelizability, and its con-\nduciveness to constrained training (LeCun, 1986;\nLe Cun, 1987; LeCun, 1988; Krogh et al., 1989;\nBengio, 2014; Carreira-Perpi˜n´an and Wang, 2014;\nLee et al., 2015).\nConcretely, we treat the hidden states of an\nRNN as free variables, which are optimized inde-\npendently, but encouraged to be predictable from\nprevious hidden states. Formulating the model in\nthis way offers an approach to avoiding the se-\nquential nature of BPTT training, by allowing for\noptimization over the parameters and all hidden\nstates simultaneously for the entire data set, which\nis easily parallelized.\nWe extensively evaluate applying TPROP to\ntrain language models, and we ﬁnd: (1) that batch\nTPROP is as effective as Batch BPTT in minimiz-\ning training loss, but they both fail to generalize\nwell; (2) that mini-batch TPROP achieves compa-\nrable generalization performance to BPTT only in\nthe limit when TPROP reduces to mere BPTT; (3)\nthat the relatively unconstrained nature of the hid-\nden state optimization appears to be responsible\nfor the performance degradation.\n1.1 Review: Truncated BPTT\nLetting xt represent the input at time t, an RNN\nproduces hidden states ht and predictions ˆyt at\neach time-step using the following recurrence\nht = g(xt,ht−1), (1)\nˆyt = f(ht), (2)\nwhere gand fare non-linear, parametric functions\nof their inputs. For instance, an Elman RNN for\nlanguage modeling might be speciﬁed by deﬁning\ng(xt,ht−1) = σ(Wxxt + Whht−1), and f(ht) =\nsoftmax(Wyht), where σdenotes the logistic sig-\nmoid function.\nIt is typical to apply RNNs to sequence-\nprediction problems that have a well-deﬁned loss\nℓ( ˆyt,yt) at every time-step, whereyt is the desired\narXiv:1702.04770v1  [cs.CL]  15 Feb 2017\nh 0 h 1 h 2 h t \u0000 1 h t\nx 1 x 2 x t\nˆy 1 ˆy 2 ˆy t\nx 2 x 3 x t +1\nf ( . ; W y )\ng ( ., . ; W x ,W h )g ( ., . ; W x ,W h )g ( ., . ; W x ,W h )\nf ( . ; W y )f ( . ; W y )\nlll\nFigure 1: An RNN unrolled for ttime-steps during training.\nBlack arrows indicate the forward direction of the unrolled\nrecurrence, and the red arrows the direction of the gradients.\noutput at the t’th step. In the case of language\nmodeling, ℓ is the cross-entropy loss, where the\nnumber of classes is equal to the number of words\nin the vocabulary, and yt = xt+1.\nGiven ℓ, as well as a dataset consisting of an\ninput sequence x1,...,x T and desired output se-\nquence y1,...,y T, it is possible to obtain a loss\nfor the entire dataset in terms of any parameters\nθ(and an initial hidden state h0) by unrolling the\nRNN in time, as depicted in Figure 1. In particu-\nlar, we have\nL(θ) =\nT∑\nt=1\nℓ(f(ht),xt+1)\n= ℓ(f(g(x1,h0)),x2) (3)\n+ℓ(f(g(x2,g(x1,h0)),x3) + ...\nTypically the loss in Equation 3 is not optimized\nin batch. Rather, the sequence is unrolled for a\nwindow of K steps, and a gradient step is taken\nto minimize the loss on those K steps. After up-\ndating parameters, the minimization continues by\nunrolling the next (non-overlapping) window ofK\nsteps, initializing h0 to the value of the K’th hid-\nden state in the previous window, minimizing the\nloss on this new window, and continuing through\nthe dataset in this way. Crucially, each window is\nconsidered to be independent of all previous ones,\nand so no gradients are calculated with respect to\ntime-steps in previous windows. Going forward,\nwe will refer to the loss induced by this window-\ning and truncation as the “Truncated BPTT Loss.”\nIssues with Truncated BPTT The Truncated\nBPTT Loss and associated minimization proce-\ndure suffers from two problems. First, the training\nalgorithm is inherently sequential: one cannot pro-\ncess two consecutive spans of size K in parallel,\nsince the processing of a K-window depends on\nhaving processed the previous K-window, which\nmakes parallelization impossible. Second, the\nhard truncation of the gradients beyond K steps\nx t x t +1 h t +1h t\nh t \u0000 1\nx t +1 x t +2ˆy t ˆy t +1\nˆh t\nC ( ., . ) C ( ., . )\nˆh t +1\nˆh t +2\nx t +2\nˆy t +2 x t +3\nf ( . ; W y )f ( . ; W y )f ( . ; W y )\ng ( ., . ; W x ,W h )g ( ., . ; W x ,W h )g ( ., . ; W x ,W h )\nl l l\nFigure 2: Schematic of TPROP training. The diamonds de-\npict the constraint functions, which take ˆht and ht as input.\nHollow circles indicate free variables, and solid circles indi-\ncate ﬁxed, observed variables. The free variableshts receives\ngradients from both the constraint and the recurrent unit.\nmakes it difﬁcult for the network to capture long-\nterm dependencies in the data.\n2 Target Propagation\nIn order to address the issues associated with the\ntruncated BPTT algorithm, we propose to train\nRNNs with a slightly modiﬁed loss. In particular,\nrather than unrolling the losses ℓ over time-steps\n(as in Equation 3), which has the effect of instanti-\nating the ht only implicitly, we instead treat theht\nas explicit variables to be optimized over. In order\nto maintain the recurrent property of the model,\nhowever, we add additional constraint terms to the\nloss, which encourage adjacent hidden states to\napproximately obey the parametric recurrence.\nMore concretely, we deﬁne ˆht to be the pre-\ndicted hidden state at time t, as follows:\nˆht = g(xt,ht−1) (4)\nˆyt = f(ˆht). (5)\nNote that Equation (4) uses the independent vari-\nable ht−1 on the right-hand-side; otherwise it is\nequivalent to Equation (1). Using Hto refer to the\nset of all ht, we then modify the loss to be:\nL(H,θ) =\nT∑\nt=1\nℓ(f( ˆht),xt+1) +λC( ˆht,ht), (6)\nwhere C is a penalty or constraint function (e.g.,\nan L2 penalty) introduced to force the predicted\nhidden state ˆht at time step t to be close to the\nfree hidden state ht, and λis a coefﬁcient whose\nvalue governs how strictly we wish to enforce the\nconstraint. This approach is referred to as Target-\nPropagation because the ht and the ˆht serve as tar-\ngets for each other during optimization. See Fig-\nure 2 for reference.\n2.1 The Blocked Target-Propagation\nAlgorithm (BTPROP)\nWe also consider a generalization of the TPROP\nloss, where instead of making each ht a free vari-\nable, we only have a free variable every B time\nsteps. The remaining hidden states are deﬁned im-\nplicitly through the recurrence (1), which further\nconstrains the model during training. We refer to\nthe sub-sequence of length B consisting of time-\nsteps beginning with a free variable and ending be-\nfore the next free variable as a “block.” Note that\nwith block size B = 1 we recover the TPROP for-\nmulation in the previous subsection.\nBeneﬁts of BTPROP We emphasize that the\nBTPROP loss offers an approach to addressing the\nissues with the Truncated BPTT Loss identiﬁed\nin Section 1.1. First, the independence of non-\nboundary time steps in different blocks suggests\nthat training may be efﬁciently parallelized by dis-\ntributing a large number of (contiguous) blocks to\neach machine in a cluster, which would necessitate\ninter-machine communication only for the small\nnumber of ht that border blocks on a different ma-\nchine. We note, however, that since parameters\nθ are shared across time-steps (and must there-\nfore be synchronized between machines), for such\na scheme to offer a speed-up it must also be the\ncase that Hand θcan be optimized independently,\nand that the H-optimization results in faster con-\nvergence of the θ-optimization.\nTo address the second issue with BPTT identi-\nﬁed in Section 1.1, we note that the penalty terms\nCencourage the ﬁnal hidden state of a block to be\nclose to the initial hidden state of the subsequent\nblock, which should allow for the capturing of de-\npendencies between multiple blocks during train-\ning. Also note that by restricting the block size we\ncan restrict the temporal dependence of the loss on\nany given ht, thereby mitigating the vanishing or\nexploding gradient problem.\nFinally, we note that we expect BTPROP to of-\nfer an advantage over TPROP (withB= 1), since it\nleads to a more constrained optimization problem\n(with fewer variables), and because it allows for\nintra-block BPTT, which has become a relatively\nmature technology.\n3 Training\nThe loss in Equation 6 can be seen as deriving\nfrom an equality-constrained formulation of the\ncross-entropy loss. In particular, if we introduce\nequality constraints between each ht and ˆht, as\nwell as a dual variable ut for each constraint, the\nLagrangian can be written (up to a constant) as\nLaug(H,θ, U) =\nT∑\nt=1\nℓ(f( ˆht),xi+1) + λ\n2 ||ht −ˆht + ut||2, (7)\nwhich is in the form of Equation 6 with Cdeﬁned\nas C( ˆht,ht) = 1\n2 ||ht −ˆht + ut||2.\nIt is natural to minimize the now-unconstrained\nloss (7) using an Alternating Direction Method\nof Multipliers (ADMM) style approach (Glowin-\nski and Marroco, 1975; Gabay and Mercier, 1976;\nBoyd et al., 2011), which results in the following\nmeta-algorithm:\n1. Minimize Laug with respect to H\n2. Minimize Laug with respect to θ\n3. Update duals: ut ←ut + αu∇ut Laug.\nWe note that for many choices of RNN architec-\nture it will be impossible to perform either the θ-\nminimization or the H-minimization analytically.1\nAs such, we simply use gradient-based algorithms\nfor a ﬁxed number of steps, and we ﬁnd that it\nis not necessary (and generally not advisable) to\nminimize until convergence.\nWe also consider two alternatives to the above\nalgorithm. The ﬁrst, which we refer to as the\nPenalty Method (PM) (Courant et al., 1943; No-\ncedal and Wright, 2006), is identical to ADMM,\nexcept that it avoids the use of dual variables en-\ntirely, and so skips step 3. The second, the Aug-\nmented Langrangian Method (ALM) (Hestenes,\n1969; Powell, 1967; Nocedal and Wright, 2006),\nminimizes jointly over H,θ before updating the\ndual variables, effectively merging steps 1. and 2.\nWe found that ADMM outperforms PM, and very\nminimally outperforms ALM as well, and so we\nreport ADMM results in what follows.\n4 Experiments\nWe run word-level language modeling experi-\nments on the Penn Tree Bank (PTB) (Marcus et al.,\n1Exceptions to this include various forms of multiplica-\ntive RNNs (Wu et al., 2016) with no non-linearity. In this\ncase, alternating minimizations can be carried out with least\nsquares. We experimented along these lines, but found such\nmethods to underperform gradient based approaches, which\nare also much more ﬂexible.\nBatch PTB PPL Minibatch PTB PPL Minibatch Text8 PPL\nB = 5 B = 10 B = 20 B = 5 B = 10 B = 20 B = 5 B = 10 B = 20\nH-steps = 1 192.00 201.08 180.08 137.27 130.93 127.43 242.02 220.11 205.67\nH-steps = 2 204.14 189.70 179.42 137.31 131.04 127.91 242.23 216.54 206.54\nH-steps = 5 226.16 184.60 188.99 137.37 132.95 128.56 241.47 216.87 204.60\nBPTT (K=B) 188.31 182.58 183.91 135.73 129.50 128.16 229.22 209.87 201.07\nTable 1: Validation perplexities for Batch PTB (left), Minibatch PTB (middle), and Minibatch Text8 (right), varying the number\nof H-optimization steps and B. We compare with BPTT performance when window-size K = B.\n1993) and Text8 datasets. 2 For all experiments\nwe use single-layer Gated Recurrent Unit (GRU)\nRNNs (Cho et al., 2014) (without Dropout (Sri-\nvastava et al., 2014)), and we use Adagrad (Duchi\net al., 2011) for optimization.\nWe report the perplexity obtained on the vali-\ndation dataset after freezing the ﬁnal parameters θ\nobtained during the alternating optimization pro-\ncess; no optimization is done at test time. (Pre-\nliminary experiments suggested that slightly bet-\nter results can be obtained by optimizing overpast\nht variables at test-time, though we did not pursue\nthis direction due to its inefﬁciency.)\n4.1 Results\nWe report our main results in Table 1, where we\ncompare BTPROP validation perplexity perfor-\nmance for various block-sizes and various num-\nbers of Hsteps with BPTT. We report only vali-\ndation numbers because training performance be-\ntween BTPROP and BPTT was generally compa-\nrable.\nStarting from the left portion of Table 1, we see\nthat BTPROP is roughly comparable to BPTT for\nbigger B. Importantly, however, BTPROP perfor-\nmance does not tend to improve with additional\nH-steps. This is disappointing because it can be\nshown that BTPROP with a single H-step is es-\nsentially equivalent to BPTT; see the Supplemen-\ntal Material for a more rigorous formulation and\nproof. Furthermore, it is clear from the table that\nboth Batch BTPROP and Batch BPTT are signif-\nicantly outperformed by their minibatch counter-\nparts, presumably due to the regularization effect\nof updating parameters after seeing only a subset\nof the data (see Keskar et al. (2016) for a discus-\nsion of this phenomenon). This suggests that even\nif Batch BTPROP were more easily parallelized, it\nwould not be worth the decreased generalization.\n2http://mattmahoney.net/dc/textdata\n∆ PPL\nH-steps = 0 -0.40\nH-steps = 0 & λ= 0 +2.36\nTable 2: Changes in PPL, usingH-steps = 1, B = 20 on Mini-\nbatch PTB as a baseline. Setting H-steps = 0 improves PPL\nas long as λ> 0.\nWe now consider the right two portions of Ta-\nble 1, which involve Minibatch BTPROP. We\nnote that applying BTPROP in a minibatch set-\nting is not straightforward, since the penalty term\nC( ˆht,ht) will now involve hidden states ht that\nhave not been updated since the previous epoch,\nand which therefore may not provide reasonable\ntargets. We address this by initializing the hid-\nden states ht in the current minibatch to the ˆht\ngiven by the current parameters before starting the\nH-optimization. We also emphasize that in the\nminibatch setting there is little hope for a paral-\nlelization gain; it is possible, however, that the\nBTPROP loss will still allow the model to bene-\nﬁt from training with longer-range dependencies.\nUnfortunately, the results again suggest that BT-\nPROP gains little over BPTT, and that moreover\nadditional H-steps are not in general beneﬁcial.\n4.2 BTPROP as L2 Regularizer\nAdditional experiments summarized in Table 2\nsuggest that the small BTPROP performance gains\nin the Minibatch setting (see Table 1) are at-\ntributable to the regularization effect of the L2\npenalty in (7), rather than training with longer de-\npendencies. There, we compare the Validation\nPPL obtained on Minibatch PTB with H-steps = 1\nand B= 20 with doing no H-optimization (which\nimproves (i.e., lowers)) perplexity, and with doing\nno H-optimization but also setting λ = 0 (which\nsigniﬁcantly hurts (i.e., increases) perplexity).\nFigure 3: Validation perplexities obtained from BTPROP\n(B = 10) and BPTT ( K = 10) as the dimension of the\nhidden states increases. BPTT’s improvement increases with\nthe dimensionality.\n4.3 A Possible Explanation and Future Work\nOne explanation for the H-optimization hurting\nperformance is that its relatively unconstrained\nnature may allow for ﬁnding hidden states that\ndecrease training loss without leading to param-\neters that generalize. Indeed, many of the re-\nported successes of TPROP-style training have in-\nvolved very constrained problems, such as those\nwith binary hidden-state constraints (Carreira-\nPerpi˜n´an and Raziperchikolaei, 2015) or sparsity\nconstraints (Kavukcuoglu et al., 2009). To test\nthis hypothesis, in Figure 3 we plot the perplex-\nity obtained from BTPROP with B= 10 (for both\n2 and 5 H-optimization steps) as well as the per-\nplexity obtained from BPTT with K = 10, as the\ndimensionality of the hidden state increases. We\nsee that BPTT actually underperforms BTPROP\nfor small hidden states, but improves relatively as\nthe hidden state size increases, lending support to\nour hypothesis. A similar pattern emerges when\ncomparing BTPROP with B = 5 and BPTT with\nK = 5. This suggests that getting BTPROP to\nwork with larger hidden states may involve ﬁnd-\ning additional approaches to constraining the H-\noptimization, such as by, for instance, penaliz-\ning the KL divergence between the distributions\nsoftmax(Wyˆht) and softmax(Wyht), which we\nleave to future work.\nReferences\nYoshua Bengio. 2014. How auto-encoders could pro-\nvide credit assignment in deep networks via target\npropagation. arXiv preprint arXiv:1407.7906 .\nStephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato,\nand Jonathan Eckstein. 2011. Distributed optimiza-\ntion and statistical learning via the alternating direc-\ntion method of multipliers. Foundations and Trends\nin Machine Learning 3(1):1–122.\nMiguel ´A. Carreira-Perpi˜n´an and Ramin Raziperchiko-\nlaei. 2015. Hashing with binary autoencoders. In\nCVPR. pages 557–566.\nMiguel ´A. Carreira-Perpi ˜n´an and Weiran Wang. 2014.\nDistributed optimization of deeply nested systems.\nIn Proceedings of AISTATS. pages 10–19.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078 .\nRichard Courant et al. 1943. Variational methods for\nthe solution of problems of equilibrium and vibra-\ntions. Bull. Amer. Math. Soc 49(1):1–23.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research 12(Jul):2121–2159.\nJeffrey L. Elman. 1990. Finding structure in time.\nCognitive Science 14(2):179–211.\nDaniel Gabay and Bertrand Mercier. 1976. A dual\nalgorithm for the solution of nonlinear variational\nproblems via ﬁnite element approximation. Com-\nputers & Mathematics with Applications 2(1):17–\n40.\nRoland Glowinski and A Marroco. 1975. Sur\nl’approximation, par ´el´ements ﬁnis d’ordre un, et\nla r ´esolution, par p ´enalisation-dualit´e d’une classe\nde probl `emes de dirichlet non lin ´eaires. Revue\nfranc ¸aise d’automatique, informatique, recherche\nop´erationnelle. Analyse num´erique 9(2):41–76.\nMagnus R Hestenes. 1969. Multiplier and gradient\nmethods. Journal of optimization theory and appli-\ncations 4(5):303–320.\nKoray Kavukcuoglu, Marc’Aurelio Ranzato, Rob Fer-\ngus, and Yann LeCun. 2009. Learning invariant fea-\ntures through topographic ﬁlter maps. In CVPR.\nIEEE.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-\ncedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang. 2016. On large-batch training for deep\nlearning: Generalization gap and sharp minima.\narXiv:1609.04836 .\nAnders Krogh, CI Thorbergsson, and John A Hertz.\n1989. A cost function for internal representations.\nIn NIPS. pages 733–740.\nYann Le Cun. 1987. Mod`eles connexionnistes de\nl’apprentissage. Ph.D. thesis, Paris 6.\nYann LeCun. 1986. Learning processes in an asymmet-\nric threshold network. Springer-Verlag, pages 233–\n240.\nYann LeCun. 1988. Theoretical framework for back-\npropagation. In Proceedings of the 1988 Connec-\ntionist Models Summer School . Morgan Kaufmann,\nCMU, Pittsburgh, Pa, pages 21–28.\nDong-Hyun Lee, Saizheng Zhang, Asja Fischer, and\nYoshua Bengio. 2015. Difference target propaga-\ntion. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases .\nSpringer, pages 498–515.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional linguistics 19(2):313–330.\nT. Mikolov, M. Karaﬁt, L. Burget, J. Cernock, and\nS. Khudanpur. 2010. Recurrent neural network\nbased language model. In INTERSPEECH.\nPiotr Mirowski and Yann LeCun. 2009. Dynamic fac-\ntor graphs for time series modeling. In Machine\nLearning and Knowledge Discovery in Databases\n(ECML/PKDD). Springer, ISBN:978-3-642-04173-\n0, volume 5782, pages 128–143.\nJorge Nocedal and Stephen J Wright. 2006. Numerical\noptimization, second edition. Numerical optimiza-\ntion pages 497–528.\nMichael JD Powell. 1967. ” A method for non-linear\nconstraints in minimization problems”.. UKAEA.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch 15(1):1929–1958.\nGavin Taylor, Ryan Burmeister, Zheng Xu, Bharat\nSingh, Ankit Patel, and Tom Goldstein. 2016. Train-\ning neural networks without gradients: A scalable\nADMM approach. In Proceedings of ICML. pages\n2722–2731.\nPaul J Werbos. 1990. Backpropagation through time:\nwhat it does and how to do it. Proceedings of the\nIEEE 78(10):1550–1560.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua\nBengio, and Ruslan Salakhutdinov. 2016. On multi-\nplicative integration with recurrent neural networks.\nIn NIPS. pages 2856–2864.\nA Supplemental Material\nA.1 Connection between BPTT and TPROP\nHere we show that when using the Penalty Method\nloss (or, equivalently, the ADMM loss while keep-\ning the dual variables ut set to 0), doing BTPROP\nwith H-steps = 1 and θ-steps = 1 results in gradi-\nents with respect to θ that are equal to the gradi-\nents with respect to θobtained from BPTT (up to\na constant factor), if: (1) the Hare initialized such\nthat ht = gθ(xt,ht−1); (2) the ht are updated with\nvanilla gradient descent.\nMore formally, let ℓ(gθ(xt,ht−1),yt) be a per-\ntime-step loss, where θ are the parameters of g.\nAlso deﬁne\nℓpm = ℓ(ht,yt) + λ\n2 ||gθ(xt,ht−1) −ht||2. (8)\nWe show that if we initialize ht = gθ(xt,ht−1)\nthen we have\n∂ℓpm(˜ht,yt)\n∂θ = ηλ∂ℓ(gθ(xt,ht−1),yt)\n∂θ , (9)\nwhere we have deﬁned\n˜ht = ht −η∂ℓpm(ht,yt)\n∂ht\n. (10)\nFrom the deﬁnition of ℓpm, we have\n∂ℓpm(ht,yt)\n∂ht\n= ∂ℓ(ht,yt)\n∂ht\n+ λ(ht −gθ(xt,ht−1))\n= ∂l(ht,yt)\n∂ht\n,\nwhere the last line follows from the fact that ht =\ngθ(xt,ht−1). Substituting into (10) then gives\n˜ht = ht −η∂ℓ(ht,yt)\n∂ht\n.\nNow, from the deﬁnition ofℓpm and the chain rule,\nwe can write the left hand side of (9) as\n∂ℓpm(˜ht,yt)\n∂θ\n= ∂gθ(xt,ht−1)\n∂θ λ\n(\ngθ(xt,ht−1) −˜ht\n)\n= ∂gθ(xt,ht−1)\n∂θ λ\n(\ngθ(xt,ht−1)−\nht + η∂ℓ(ht,yt)\n∂ht\n)\n= ∂gθ(xt,ht−1)\n∂θ λ\n(\nη∂ℓ(ht,yt)\n∂ht\n)\n, (11)\nλ {1, 0.1, 0.01}\nut learning rate {1, 0.1, 0.01}\nH-optimization learning rate {0.1, 0.01, 0.001}\nθ-optimization learning rate {0.1, 0.01, 0.001}\nθ-optimization steps {1}\nH-optimization steps {1, 2, 5}\nTable 3: Grid of hyperparameters used in experiments.\nwhere the last line again follows from the fact that\nht = gθ(xt,ht−1). Since we can also write the\nright hand side of (9) as\n∂ℓ(gθ(xt,ht−1),yt)\n∂θ = ∂gθ(xt,ht−1)\n∂θ\n∂ℓ(ht,yt)\n∂gθ(xt,ht−1)\n= ∂gθ(xt,ht−1)\n∂θ\n∂ℓ(ht,yt)\n∂ht\n,\nwe obtain the equality as desired.\nA.2 Hyperparameter Grid Used in\nExperiments\nSee Table 3.\nA.3 Increasing Mini-Batch Size with BPTT\nAn obvious approach to speed up training con-\nsists of increasing the mini-batch size of SGD.\nUnfortunately, the results in table 4 suggest that\nperformance deteriorates on both training and test\nsets whenever the minibatch size is big enough\nto leverage parallel computation (mini-batch size\nof size 4096 and above). These ﬁndings are in\nline with the ﬁnding in Section 4.1 that both batch\nBPTT and batch BTPROP are signiﬁcantly infe-\nrior to their mini-batch counterparts.\nWIKI 64 256 1024 4096 16000\ntraining 29.7 33 40.7 81 1070\ntest 47.7 49.1 52 78.7 938\nTEXT8 64 256 1024 4096 16000\ntraining 1.35 1.37 1.36 1.38 1.46\ntest 1.39 1.41 1.42 1.45 1.47\nTable 4: Varying the mini-batch size (columns) when train-\ning with BPTT using the wiki-large 103 dataset at the word\nlevel, and the text8 dataset at the character level. In the wiki-\nlarge 103, we used a two layer LSTM with 512 hidden units\nunrolled for 30 steps. In the text8 dataset, we used a one layer\nLSTM with 1024 hidden units unrolled for 75 time steps. In\nboth cases, increasing the mini-batch size increases perplex-\nity on both training and test sets.\nA.4 Code\nCode implementing our experiments can\nbe found at https://github.com/\nfacebookresearch/TPRNN",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7880876064300537
    },
    {
      "name": "Phenomenon",
      "score": 0.639240026473999
    },
    {
      "name": "Training (meteorology)",
      "score": 0.44916966557502747
    },
    {
      "name": "Artificial intelligence",
      "score": 0.447086364030838
    },
    {
      "name": "Work (physics)",
      "score": 0.4320386052131653
    },
    {
      "name": "Machine learning",
      "score": 0.42459022998809814
    },
    {
      "name": "Backpropagation",
      "score": 0.4222026467323303
    },
    {
      "name": "Artificial neural network",
      "score": 0.39296022057533264
    },
    {
      "name": "Epistemology",
      "score": 0.08985918760299683
    },
    {
      "name": "Engineering",
      "score": 0.06630104780197144
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 8
}