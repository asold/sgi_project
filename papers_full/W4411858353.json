{
  "title": "Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study",
  "url": "https://openalex.org/W4411858353",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5041792021",
      "name": "Ritesh Maurya",
      "affiliations": [
        "Madan Mohan Malaviya University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5027200234",
      "name": "Nikhil Kumar Rajput",
      "affiliations": [
        "Amity University"
      ]
    },
    {
      "id": "https://openalex.org/A5118744886",
      "name": "M G Diviit",
      "affiliations": [
        "Amity University"
      ]
    },
    {
      "id": "https://openalex.org/A5083798172",
      "name": "Satyajit Mahapatra",
      "affiliations": [
        "Manipal Academy of Higher Education"
      ]
    },
    {
      "id": "https://openalex.org/A5071351902",
      "name": "Manish Kumar Ojha",
      "affiliations": [
        "Amity University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995213289",
    "https://openalex.org/W3187751556",
    "https://openalex.org/W3033626851",
    "https://openalex.org/W4312779354",
    "https://openalex.org/W4389201797",
    "https://openalex.org/W4389202037",
    "https://openalex.org/W2977128309",
    "https://openalex.org/W4377084726",
    "https://openalex.org/W4280539423",
    "https://openalex.org/W4388725043",
    "https://openalex.org/W6840334356",
    "https://openalex.org/W4390166236",
    "https://openalex.org/W3105892552",
    "https://openalex.org/W4393397034",
    "https://openalex.org/W4390102952",
    "https://openalex.org/W6607532246",
    "https://openalex.org/W6609741587",
    "https://openalex.org/W4385970234",
    "https://openalex.org/W4389325518",
    "https://openalex.org/W4380551079",
    "https://openalex.org/W4311627281"
  ],
  "abstract": null,
  "full_text": "Exploring the potential of \nlightweight large language \nmodels for AI-based mental \nhealth counselling task: a novel \ncomparative study\nRitesh Maurya1, Nikhil Rajput2, M. G. Diviit2, Satyajit Mahapatra3 & Manish Kumar Ojha2\nIn recent years, Transformer-based large language models (LLMs) have significantly improved upon \ntheir text generation capability. Mental health is a serious concern that can be addressed using LLM-\nbased automated mental health counselors. These systems can provide empathetic responses to \nindividuals in need while considering the negative beliefs, stigma, and taboos associated with mental \nhealth issues. Considering the large size of these LLMs makes it difficult to deploy these automated \ncounselors on low cost/resource devices such as edge devices. Therefore, the motivation of the \npresent study to analyze the effectiveness of lightweight LLMs in the development of automated \nmental health counseling systems. In this study, lightweight open source LLMs such as Google’s T5s \n(small variant), BARTB (base variant), FLAN-T5s (small variant), and Microsoft’s GODELB (base variant) \nhave been fine-tuned for automated mental health counseling task utilizing a diverse set of datasets \npublicly available online. The experimental results reveal that BART’s base variant outperformed the \nother models across all key metrics such as ROUGE-1, ROUGE-2, ROUGE-L, and BLEU with scores of \n0.4727, 0.2665, 0.3554, and 25.3993 respectively. In comparison to other models, BART-base model \ngenerated empathetic, and emotionally supportive responses. These findings highlight the potential \nof lightweight LLMs (small size LLMs), in advancing the field of LLM-based mental health counseling \nsolutions and underscore the need for exploration of lightweight LLMs for this mental health \ncounseling use case. The code for this work is available at the following link:  h t t p s :  / / g i t h  u b . c o m  / d i v i i  t m \ng 0 3  / C o m p a  r a t i v e  - a n a l y  s i s - o f - L L M s - . g i t.\nKeywords Large language model, Mental health, Counseling, Artificial intelligence, Fine-tuning\nMental health crisis is a serious global challenge, according to the WHO report one out of eight individual \nis suffering from mental health related issues 1. As per the ICMR report(2017), 14.3% of the total population \nis suffering from mental disorders 2. India faces a severe shortage of mental health professionals, with the \navailability of 0.75 psychiatrists per 100,000 people, far below the WHO’s recommended ratio of 3 per 100,0003.\nIn recent years, large language models(LLMs) have been used for a a wide variety of natural language \nprocessing-related applications, including question-answering, text generation, language summarization, \netc4. Considering the advancement of LLMs in different fields, the applications of LLMs in the field of mental \nhealth counseling are still unexplored. The large size of these LLM models poses significant challenges in \ntheir deployment on low resource computing devices such as edge devices. Therefore, present study is aimed \nat exploring the potential of open source lightweight LLMs in the development of automated mental health \ncounseling systems.\nMost of the automated chat bots developed earlier for mental health counseling were either rule-based \nsystems or relying on conventional machine learning models for their development 5. The chat bots developed \nusing such methods were designed for domain specific issues such as depression, anxiety, suicide prevention \n1Department of Computer Science and Engineering, Madan Mohan Malaviya University of Technology, Gorakhpur \n273010, India. 2Department of Artificial Intelligence, Amity University, Noida 201303, India. 3Department of \nInformation and Communication Technology, Manipal Institute of Technology, Manipal Academy of Higher \nEducation, Manipal 576104, India. email: satyajit.mahapatra@manipal.edu\nOPEN\nScientific Reports |        (2025) 15:22463 1| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports\n\nor stress management. The response generated by the conventional AI-based conversational agents was based \non pre-defined rules without considering the need of an individual seeking mental counseling help. The major \nlimitation of these types of systems were lack of generalisation to wide variety of mental health issues.\nWith the advancement of Transformer-based LLM models in recent years, it has been made possible to \nunderstand, analyze and respond in a meaningful manner to different mental health issues using LLM-based \nautomated chatbots6. These LLM-based chatbots solves the limitations of rule-based chatbots earlier used for \nsolving diverse mental health issues. LLM model once fine-tuned on large dataset can be deployed for wide variety \nof mental health counseling related issues. However, considering the large size( billion number of parameters) \nof these LLMs prevents their deployment on edge devices thereby, limits their scalability. Considering the \nproblem of limited scalability of these LLM-based systems due to their large size, the proposed work analyses \nthe effectiveness of lightweight LLMs(parameter count in few millions) in the development of automated chat \nbots for metal health counseling related tasks.\nThe primary hypothesis of this study is: How do different lightweight language models perform in terms \nof accuracy and contextual relevance when generating responses for mental health counseling tasks under \ndiverse scenarios? To investigate this hypothesis, different lightweight LLMs, including T5s\n7, BARTB\n8, FLAN-\nT5s\n9, and GODELB\n10 have been fine-tuned on mental health conversation dataset. The performance and the \ncontextual relevance of responses generated by these models were analyzed. The dataset used for fine-tuning \nthese models comprised publicly available mental health counseling related conversations between patients and \nexperts, structured in form of questions and corresponding responses, curated from different online platforms. \nA comprehensive evaluation of these models was performed to check their efficacy in generating contextually \nrelevant and accurate responses within different mental counseling scenarios . Key performance metrics such \nas ROUGE (Recall-Oriented Understudy for Gisting Evaluation), BLEU(Bilingual Evaluation Understudy) and \nperplexity were used to benchmark the performance of these models. This detailed analysis provided valuable \ninsights into the suitability and effectiveness of these lightweight LLMs for mental health counseling applications.\nRelated works\nSome of the works related to the present study have been discussed in this section.\nThe recent advancement of LLMs in the field of natural language processing has opened new possibilities in \nhealthcare, particularly in addressing the mental health crisis. Several studies have been conducted to explore \nthe potential of artificial intelligence in mental health. In the work 11, researchers used rule-based systems in \ndeveloping chatbots to address mental health-related issues. These systems generate synthetic responses in \ncontrast to therapeutically relevant responses which are necessary for mental health counseling related task. In \nother work, researchers reviewed the use of AI in addressing diverse challenges in the field of mental health such \nas disease prevention, diagnosis and treatment interventions12. The researchers also proposed a method that uses \ndata from various digital footprints of an individual, such as their social media posts and data related to their \nuse of smartphones, to analyze the pattern and predict the mental health status of an individual13. In other work \nresearchers analyzed the successful integration of AI in the field of mental healthcare concerning issues need to \nbe addressed like privacy, bias and diagnostic accuracy 14. Researchers also analyzed the ethical issues and the \nrisk factors involved in the development of such AI-enabled chat bots such as over-reliance on the generated \nresponse and lack of emotional intelligence in these systems limits their adaptability in real life use case15. S chyff \net al.16 proposed a conservational AI-based metal health support chatbot named Leora which provide support \nto its users in case of mild symptoms of anxiety and depression. The response generated by these AI-based \nconversational agents were generalized in nature and did not provide personalized assistance. Rathnayaka et al.17 \nin their work proposed an AI and behavioral activation-based personalized assistance for recurrent emotional \nsupport catering to the need of individual seeking therapeutic response to mental health related queries.\nRecent advancements in the field of natural language processing with the development of Transformer-based \nLLMs models have significantly transformed healthcare. Peng et al.18 developed a model by fine-tuning the GPT-\n3 architecture with up to 20 billion parameters for healthcare text generation and biomedical natural language \nprocessing task. Singhal et al. 19 have developed Med-PALM 2 LLM model for medical question answering \ntask. This model achieved score of 67.2% on the MedQA dataset. Y ang et al. 20 proposed an ensemble of LLMs \nfor medical question answering. He et al. 21 fine tuned BERT model on knowledge of different disease such as \ntheir signs, symptoms, diagnosis and treatment. Despite the significant advancement of LLMs in the field of \nhealthcare their adoption in the field of mental healthcare is in nascent stage or still unexplored systematically. \nChallenges like data scarcity, low user engagement and high drop out rates prevent the deployment of these \nLLMs in the field of mental healthcar 22,23. Y adav et al.24 compared the state-of-the-art LLM models with and \nwithout fine-tuning and found that the fine-tuned LLMs performed better with improved generalization for \nautomatic generation of diagnostic summaries for mental state screening. Zheng et al.25 generated dataset named \nExTES (ExTensible Emotional Support dialogue dataset) and fine-tuned LLaMA model for mental healthcare \nwith emotional support.\nLLMs like Med-PaLM 2 have been successfully deployed in mental health diagnosis task with 92.5% accuracy \nin correctly diagnosing the depressive disorder26 Similarly, PaLM 2 model when fine-tuned on medical domain \ndata generates more comprehensive list of psychiatric diagnosis in comparison to the medical experts 27. Some \ngroup of researchers have proposed a framework to examining the issues such as bias, stereotyping, privacy \nviolation and exacerbating inequalities in LLMs 28 In other work researchers have proposed sociocultural-\ntechnical approach to address the challenges like technical costs, literacy gaps, biases, and inequalities29.\nConsidering the large size (billion number of parameters) of these LLMs which makes it impractical to deploy \nthese models on low resource edge device. Therefore, in this work open source lightweight(parameters in few \nmillions) LLMs have been explored for mental health counseling related task with special focus on generating \nempathetic, coherent, and contextually relevant response.\nScientific Reports |        (2025) 15:22463 2| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nMethodology\nThe proposed method evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, \nFLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation \ndatasets. The proposed method involves steps like data preprocessing, tokenization, model fine-tuning, and \nevaluation using different metrics (ROUGE, BLEU, Perplexity), along with preliminary human feedback. The \ngeneral flow of the proposed methodology is shown in Fig.  1. Detailed pictorial representation of the proposed \nmethodology has been shown in Fig.  3. The following subsections provide a detailed explanation of the sub-\nprocesses involved in the proposed methodology.\nData collection\nThe datasets used in this work were curated from diverse online sources such as HuggingFace and GitHub. The \ncollected data was organised in standardised format in form of question-response pairs, suitable for training \nof lightweight LLMs used in this study for the comparison purpose. The following datasets were used in this \nstudy for fine-tuning the LLMs: Aditya Mental Health Counselling Dataset30, Mental Health Counselling Chat31, \nCounsel Chat Dataset32 and Amod-Mental Health Counselling Conversations33. Diagramatic representation of \nthe distribution of data among these datasets has been shown in Fig. 2.\nTable 1 provides detailed information such as source, size and several other details about all four datasets \nused in this study for fine-tuning lightweight LLMs. The size of the combined datasets obtained after combining \nthe instances of conversation from all four datasets into a single dataset was 20,500. Each instance consists \nof question and corresponding answer in form of question-answer pair in English language. The combined \ndataset consists of mental health related conversation from wide variety of topics such as depression, anxiety, \nself-esteem, coping mechanism, stress management and relationship issues.\nFig. 1. Overall methodology framework used in this study for fine-tuning and evaluating lightweight LLMs in \nmental health counseling.\n \nScientific Reports |        (2025) 15:22463 3| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nEach dataset provides unique scenarios, enabling the fine-tuned language models to capture the details \nof various mental health concerns and respond with contextually appropriate and empathetic dialogue. The \ndiversity in topics and conversational styles within the datasets supports a comprehensive training, promoting \nthe development of language models capable of addressing complex mental health issues.\nData cleaning\nThe first step in data cleaning process involves deletion of useless columns and duplicate entries to avoid \nrepetition of data. Text fields in the dataset were cleaned by removal of HTML tags, extra strings and system \ngenerated prompts. The dataset was further organized into dialogue pairs in form of question and response. \nAll four datasets cleaned in such a manner were combined further to utilise them for fine-tuning different \nlightweight LLMs.\nData pre-processing\nThe pre-processing pipeline starting from data preparation to tokenisation involved the following key steps:\nInput preparation and Tokenization: Each question-answer pair in a the dataset was formatted as proper \ninput form suitable for model’s input. In case of T5S and FLAN-T5S models,each question in the dataset was \npre-fixed with “ < question >′′ to contextualize the input. Contextualized input was then tokenised using \n’T5Tokenizer’ from the Hugging Face Transformers library.It is based on SentencePiece tokenisation algorithm. \nMaximum sequence length was set to 512 to ensure that long conversations are truncated and padded as and \nwhen required to maintain the consistence in the length of input sequence.\nTarget Encoding: Transformers like T5 and BART expect separate tokenisation of responses corresponding \nto each question to generate the target sequences. Tokenisation on responses were applied using the same \ntokeniser as used in case of input questions. Maximum sequence length was set to 512 in case of target response \ngeneration. The encoded responses were assigned “labels” for supervised learning so that generated response and \nground truth response can be compared.\nDataset Mapping: Dataset mapping was applied to both training as well as validation sets to ensures batch-\nwise tokenisation of question-answer pairs for efficent processing. This mapping ensures proper structuring of \nall input sequences for further processing by the LLMs.\nSelection and fine-tuning lightweight-language models\nDifferent lightweight Sequence-to-sequence LLMs having a few million of parameters were selected and fine-\ntuned for mental health counseling conversation task. A detailed description of the selected lightweight-LLMs \nused in this study for the comparison purpose has been presented as follows:\nT5(Text-to-Text Transfer Transformer): A prominent Seq2Seq model developed by Google with just only \n60 million parameters makes it a suitable to be assessed for the present task of mental health counseling. It is \nbased on text-to-text framework, takes text as an input and generates text thereon which makes in suitable for \nthe present counseling conversation task. This model is efficient for wide variety of text generation tasks such as \nDataset bame Source Task\nSize \n(K) Description\nAditya Mental Health Counseling30 Hugging Face, GitHub Q&A 13.5 Contains Q&A pairs focused on mental health concerns like anxiety and stress.\nMental Health Counseling Chat31 Hugging Face Dialogue generation 2.1 Conversations on depression, anxiety, and emotional challenges for therapy \nmodels.\nCounsel Chat Dataset32 Counsel Chat, Hugging \nFace Question & Answering 1.4 Licensed counselors respond to user-submitted questions on mental health.\nAmod-Mental Health Counseling33 Hugging Face Conversation modeling 3.5 Features dialogues on depression, mood swings, and self-care.\nTable 1. Summary of four mental health datasets used in our experiment.\n \nFig. 2. Distribution of Mental Health Datasets.\n \nScientific Reports |        (2025) 15:22463 4| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\ndocument summarization, question-answering and many more.In this work the ’small’ variant of T5 model has \nbeen used for the fine-tuning purpose7.\nBART(Bidirectional and Auto-Regressive Transformers):  BART model developed by Facebook takes \ninspiration from bidirectional text understanding capabilities of BERT (Bidirectional Encoder Representations \nfrom Transformers)-based encoder model and auto-regressive capabilities of GPT(Generative pre-trained \ntransformer)-based decoder model. It is pre-trained on text corrupted through some noise function and it is \ngood at text generation tasks like summarization, translation, question answering and text classification. In this \nwork, the ’base’ variant of the BART model has been chosen considering its seq2seq framework with limited \nnumber (139 million) of parameters which makes it suitable for fine-tuning it on mental health counseling \nconversation task34.\nFLAN-T5: FLAN is a family of improved transformers, in contrast to T5, FLAT-T5 has been instruction-\ntuned and improves the performance over T5 for diverse range of text generation tasks such as summarization, \ntext generation, reasoning and question answering. FLAT-T5 is found to be more effective in zero-shot, few-shot \nand chain-of-thought reasoning.In this work FLAN-T5’s small size variant with 80 million parameters is used \nfor fine-tuning on mental health counseling dataset9.\nGODEL(Grounded Open Dialogue Language Model):  Developed by Microsoft Research, it is an open-\nsource pre-trained language model which is a successor of Microsoft’s DialoGPT model and it was primarily \ndeveloped for goal-oriented dialogue. This model is based on encoder-decoder architecture and it is well trained \nfor generating response conditioned on some external text10. The reason behind choosing this model is based on \nits suitability for dialogue generation task with only 220 million parameters.\nThe detailed description about each of these models has been provided in Table  2. These models were fine-\ntuned on the pre-processed combined dataset using the hyperparameters presented in experimental results \nsection. The detailed description of these hyper-parameters has been provided in the next section.\nResponse generation and evaluation of fine-tuned LLMs\nThe combined dataset was split into train and test set: 70% of the combined dataset was used for training whereas \n30% of it was used for testing purpose. The questions in the test set were given as an input to these fine-tuned \nlightweight LLMs and the response generated by these LLMs were recorded and compared with the actual \nresponses present in the dataset. Several quantitative metrics such as ROUGE35, BLEU36, and perplexity37, have \nbeen used to evaluate the effectiveness of the response generated by these fine-tuned models. These scores helps \nin evaluating the quality, relevance, and reliability of the response generated by these models.\nROUGE score evaluates the overlap between generated text and targeted response by evaluating recall across \nn-grams. The evaluation of ROUGE score ensures that the response generated by these models are aligned with \ntargeted response.\nBLEU (Bilingual Evaluation Understudy) score measures the precision of n-gram overlap between the \ngenerated response and the target response. High BLEU scores can contribute in generating more fluent \nand relevant responses, which may help in the improving the user engagement in mental health counseling \nconversations.\nPerplexity score quantifies the amount of uncertainty or model’s confidence in generated response. Lower \nperplexity score denotes higher confidence in the prediction made by the model and vice versa for the higher \nperplexity score. A low perplexity score ensures the consistent and coherent interactions during mental health \ncounseling conversation.\nUsing the ROUGE and BLEU score, informativeness and lexical similarity of the generated responses have \nbeen measured, whereas the lower perplexity score indicates a more coherent and a well-trained fine-tuned \nmodel. Altogether, these metrics provide a comprehensive evaluation framework, assessing the text similarity \nand the predictability of the fine-tuned LLMs; however, empathy and contextual relevance of the generated \nresponses have also been assessed by the human evaluator.\nThe proposed methodology has been outlined in Fig.  3, starting from data collection, pre-processing , \nsplitting, fine-tuning and evaluation of response generated by lightweight LLMs for mental health counseling \nconversation. 70% of the combined dataset was used for training whereas 30% of the dataset was used for testing \npurpose. The responses generated by these models were compared against the target response and different \nperformance metrics were evaluated. The generated responses were evaluated against ROUGE, BLEU and \nPerplexity scores, in addition to that the generated responses were also evaluated against contextual coherence \nand emphatic reply generated by these fine-tuned LLMs under different counseling scenarios using human \nevaluator.\nModel Size (in million) Open source Architecture type\nBARTB8 139 Ye s Transformer (Bidirectional Encoder-Autoregressive Deoder)\nFLAN-T5S9 80 Ye s Transformer (Encoder-Decoder)\nT5S7 60 Ye s Transformer (Encoder-Decoder)\nGODELB10 220 Ye s Transformer (Pretrained Language Model)\nTable 2. Comparison of different language models used in the study.\n \nScientific Reports |        (2025) 15:22463 5| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nExperimental results and discussions\nExperiment setup and performance metrics\nAll experiments were performed with a NVIDIA T4 GPU with 16GB of RAM. Test dataset consists of \napproximately 4,000 mental health counseling dialogues, used for testing the trained models. The evaluation \nwas performed by comparing the model-generated responses against the reference responses.\nHyperparameters used\nThe hyperparameters used in this work for fine-tuning the LLMs are summarized in Table  3. Batch size was \nset to 8 and the learning rate was set to 5 × 10−5. The fine-tuned models were trained for 50 epochs to ensure \nproper convergence without any sign of overfitting. Other hyperparamters like weight decay was set to 0.01 for \nregularization and mixed precision (fp16) was used for better efficiency and storage complexity.\nTokenisers\nThe details of the tokenisers and tokenisation approach used with the different fine-tuned LLM models used in \nthis work are presented in Table 4\nFig. 3. Flow graph for the propsoed methodology.\n \nScientific Reports |        (2025) 15:22463 6| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nPerformance comparison\nDifferent performance metrics used to compare the response generated by the fine-tuned models with the \ntarget response have been presented in Table  5. Different performance metrics such as ROUGE-1, ROUGE-2, \nROUGE-L, and BLEU scores were used to compare the performance of the models. The comparison of the fine-\ntuned models on these parameters ensure that the responses generated by these models are contextually relevant \nand coherent.\nAmong all the models, BARTB model achieved highest score across all the metrics, achieving a ROUGE-1 \nscore of 0.4727, ROUGE-2 of 0.2665, ROUGE-L of 0.3554 and BLEU score of 25.3993. These metrics ensures \nthat the responses generated by the BARTB model are contextually relevant as well as coherent. GODELB model \nachieved moderate performance with a BLEU score of 6.6183 and reasonable ROUGE scores. The response \ngenerated by GODEL B model as presented in Table  6 is found to be emphatic though it lacks specificity as \nobserved in the response generate by the BART B model. The performance of FLAN-T5 S and T5 S model was \nnot found satisfactory as FLAN-T5 S model achieved a ROUGE-1 score of 0.2632 and BLEU score of 3.0431, \nwhereas in case of T5S model ROUGE-1 score of 0.2585 and BLEU score of 3.0649 was achieved which suggest \nModel Generated text (Summary) Perplexity Analysis\nGODELB “ Anxiety can be a difficult thing to deal with... Let’s work together to identify the triggers... ” 3.40 Empathetic response generation \nor human-like anxiety support.\nBARTB\n“This question is a great one! Anxiety is a treatable condition... Talk to a therapist... Practice deep breathing \nexercises... ” 3.75 Structured actionable suggestions \nor comprehensive practical advice.\nFlan-T5s “ Anxiety can be a difficult emotion to manage... We can work together to develop coping strategies... ” 6.20 Concise coping strategies or less \ndetailed, higher perplexity.\nT5s\n“Can you help me with understanding how to deal with anxiety? Can you help me with understanding how \nto deal with anxiety?” 1.78 Repetitive and nonsensical \nresponses.\nTable 6. Responses generated by different models and a comparison of their perplexity scores for the common \nquestion, “Can you help me understand how to deal with anxiety?” .\n \nModel ROUGE-1 ROUGE-2 ROUGE-L BLEU\nT5S 0.2585 0.0877 0.1914 3.0649\nFlan-T5S 0.2632 0.0954 0.1990 3.0431\nBARTB 0.4727 0.2665 0.3554 25.3993\nGODELB 0.3350 0.1324 0.2328 6.6183\nTable 5. Performance metrics comparison.\n \nModel Tokenizer used Tokenization approach\nT5S T5Tokenizer SentencePiece\nFLAN-T5S T5Tokenizer SentencePiece\nBARTS BartTokenizer Byte-Pair Encoding (BPE)\nGODELS T5Tokenizer SentencePiece\nTable 4. Tokenisers and tokenisation approach.\n \nHyperparameter Value\nBatch size 8\nEpochs 50\nLearning rate 5 × 10−5\nWeight decay 0.01\nfp16 True\nMax length 512\nPadding ’max-length’\nTruncation ’true’\nGradient accumulation steps 2\nTable 3. Hyperparameters used for evaluating the language models in this study.\n \nScientific Reports |        (2025) 15:22463 7| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nthe limitations of T5 S and FLAN-T5 S model in capturing the . linguistic subtleties and psychological nuances \nnecessary for the generation of comprehensive counseling responses.\nTo analyse the linguistic coherence and psychological relevance of the response generated by these fine-tuned \nmodels, a comparison of the perplexity score obtained by these models when a common question is posed “Can \nyou help me with understanding how to deal with anxiety?” to these models as presented in Table 6 .\nT5B model achieved lowest perplexity score of 1.78, representing the fluency and coherence of the generated \ntext. However, the response generated by T5 B model against the common question was found to be repetitive \nand it also lack practical utility, undermining of the use of T5 B model in real life mental health counseling \napplications.\nGODELB achieved a perplexity score of 3.40 while the response generated by the model was found to be in \nempathetic tone, supporting anxiety but lack actionable detail. FLAN-T5-small, with a higher perplexity score \nof 6.20, generated concise but less fluent and psychologically irrelevant response.\nThough the BARTB model achieved a perplexity score of 3.75, however, considering the fluency, physiological \ndepth, actionable advice and well-framed structure of the generated response, BARTB model among other models \nprovide most coherent response making it a suitable model for the implementation of LLM-based mental health \ncounseling. In contrast, considering the low perplexity score of T5B model suggest that it prioritizes fluency over \nthe depth of a generated response whereas in case of GODELB and FLAN-T5S model generated response focuses \nmore on empathy than providing actionable advice which is necessary for therapeutic counseling use case.\nAnalysis of real-time response generation\nTo analyse the effectiveness of the real-time response generation capabilities of the fine-tuned models used \nin this study, the response generated by these models were assessed for their contextual coherence, relevance, \nempathy in addressing the mental health related queries. For this purpose, two contextually different questions \nwere posed to each of these fine-tuned models and the responses generated by these models were presented in \nTables 7 and 8.\nResponse to question 1: crying during therapy\nAnalysis of the response generated by these fine-tuned models as presented in Table 7, for the question “Is it \nnormal for people to cry during therapy, or is it just me?” has been made in this subsection. The response generated \nby the BARTB model was found to be coherent and empathetic, addressing emotional support and providing \nreassurance in a compassionate tone. The response generated by the FLAT-T5 s model was also empathetic \nand relevant; however, occasional repetitiveness in the generated response diminishes its fluency. In contrast, \nthe responses generated by the GODEL B and T5S models suffer from redundancy and contradictions, thereby \ndiminishing the reliability of these models in generating contextually coherent and empathetic responses.\nResponse to question 2: solutions for panic attacks\nAnalysis of the response generated by these fine-tuned models as presented in Table 8, for the question “Give me \nsolution for my panic attack. ” has been made in this subsection. The response generated by the BARTB model was \nfound to be structured and practical, suggesting suitable coping mechanism and actionable advice useful in case \nof therapeutic context. FLAN-T5 S provides response to the question in a supportive tone but it lacks detailed \nactionable advice, therefore, the response generated by the FLAN-T5S small was found to be less effective than \nQuestion: Give me solution for my panic attack.\nBARTB: Panic attacks can be scary and overwhelming, but there are a few things you can do to help yourself cope with them. One thing I would suggest is to find a therapist who \nspecializes in panic attacks. They can help you learn healthy coping mechanisms to manage your panic attacks, such as deep breathing exercises, mindfulness techniques, and \ncognitive restructuring. Y ou can also contact me via my website:  h t    t p : / / p  s y  c  h c e  n t r  a l . c   o m /  l i  b  / p  a n  i c  - a  n x   i e  t y - d i s o r d e r / b a s i c s / w h a t - i s - t h e - b e s t - c a s e - r i g h t - n o w / ? p a g e = 2  \nGODELB: Panic attacks can be scary and overwhelming. Let’s explore what might be causing your panic attacks and see if we can come up with a solution that works for you.\nFLAN-T5s: Panic attacks can be very distressing, but it’s important to remember that you are not alone. Let’s work together to identify the triggers for your panic attacks and \ndevelop coping strategies to manage them. We can also explore any underlying issues that may be contributing to your panic attacks.\nT5s: I’m sorry to hear about your panic attacks. I’m sorry to hear about your panic attacks. I’m sorry to hear about your panic attacks.\nTable 8. Responses from different models to the question: “Give me solution for my panic attack. ” .\n \nQuestion: Is it normal for people to cry during therapy, or is it just me?\nBARTB: Y es, definitely crying is normal during therapy. Anyone in the midst of new awareness of how they’ve been hurt, or feel despair or great sadness from burdensome life \nsituations, naturally will feel tears come up. Therapists are familiar with people crying during a session and will know how to be present for you while you sort through your painful \nemotions.\nGODELB: Y es, it is absolutely normal to cry in therapy. Therapy is a place where you can be vulnerable and you can be vulnerable. It is a place where you can be vulnerable and you \ncan be vulnerable. (Repeated multiple times).\nFLAN-T5s:Y es, it’s normal to cry during therapy. Therapy is a place where you are able to express your feelings in a safe and supportive environment. It’s also a place where you can \nexpress your feelings in a safe and supportive environment.\nT5s: It’s normal to cry during therapy, but it’s not normal to cry during therapy. It’s not normal to cry during therapy, but it’s not normal to cry during therapy.\nTable 7. Responses from different models to the question: “Is it normal for people to cry during therapy, or is \nit just me?” .\n \nScientific Reports |        (2025) 15:22463 8| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nthe response generated by the BART B model. GODEL model provides a brief and generic response, without \nany actionable advice. The response generated by the T5 S model is found to be contextually irrelevant with \ncontradictory statements.\nComparing the responses generated by fine-tuned lightweight LLMs for Question 1 and Question 2 as \npresented in Tables 7 and 8 reveals that the BARTB model generated detailed, actionable and emphatic response \naligning with the principle of mental health counseling. FLAT-T5 S model generated response with emphatic \ntone but lacks in depth required to be utilised in practical use cases. The responses generated by the GODEL and \nT5S model lacks detail and suffered with redundancy and contradiction.\nTable 9 presents a comparative analysis of language models used in this study on the basis of their memory \nrequirement and inference speed. Despite the significant difference between the memory requirement of these \nlanguage models, these models can easily be deployed on the mobile devices considering their small memory \nrequirement and fast inference speed.\nAll the models evaluated in this study are lightweight enough to be deployed on low cost GPUs or even on \nmobile System-on-Chip (SoC) devices. The low memory footprint and fast inference make these models well \nsuited in real world scenarios. Although in previous studies26,27 LLMs like GPT-3 or LLaMA have shown decent \nperformance on mental health related conversation tasks. Considering the high computational demands of these \nLLMs in terms of memory and compute, their deployment at end devices is still not plausible.\nThese findings highlights the potential of BARTB model for its application in the implementation of mental \nhealthcare support system.\nLimitations and future work\nThough the present study provides a valuable insight into the application of lightweight LLMs in the \nimplementation of automated mental health question-answering task, the following limitations of the proposed \nstudy should also be acknowledge: The datasets utilised in this study though collected from diverse online \nresources the verifiability of collected data limits its use in diverse mental healthcare contexts. Considering the \nfocus of the present study on small LLMs with limited parameters, the large or medium size LLMs with more \ntraining data may improve the performance in real-life use cases of mental healthcare.\nWhile automatic metrics such as ROUGE, BLEU and Perplexity scores have been used to evaluate the \nresponse generated by the language models used in this study, a preliminary human assessment by single non-\nexpert evaluator have also been made to assess the coherence and empathy of generated responses. However, \nwe acknowledge the limitations of this informal evaluation. In future work, we plan to involve mental health \ncounselors and users to develop human-in-loop-based framework providing domain-specific feedback and \nalso providing more rigorous assessment of the emotional and therapeutic effectiveness of the model-generated \nresponses.\nEthical considerations\nDeployment of language models for mental health counseling raises several ethical concerns. Some of the \nissues are the generation of inappropriate or misleading responses and hallucinations, which could be harmful \nif interpreted as professional advice. Therefore, users of AI-based conversational systems should be informed \nthat all the advices are AI generated, and the responses generated by the language models should not be taken \nseriously without the advice of an expert physiologist.\nAnother ethical consideration is related with the data privacy related to the sensitive personal information. \nMoreover, inherent biases in training data might lead to biased responses requiring future efforts in bias detection \nand mitigation.\nResponsible deployment of these systems should follow a human-in-the-loop framework and must involve \nmental health professionals to assure safety and fairness,in practical use.\nConclusion\nThis study investigated the potential of lightweight Large Language Models (LLMs) for AI-driven mental \nhealth counseling, focusing on four models–Microsoft-Godel, BART-base, T5-small, and FLAN-T5-small–\nfine-tuned on a curated dataset derived from diverse, high-quality mental health counseling resources. Among \nthese models, BART-base consistently achieved the highest ROUGE-1, ROUGE-2, ROUGE-L, and BLEU \nscores, demonstrating its superior capability to generate coherent, contextually relevant linguistically accurate \nresponses. The responses generated by fine-tuned lightweight LLMs were also analyzed based on empathy and \nemotional supportiveness. These results underscore the viability of lightweight LLMs like BART-base for mental \nhealth applications, even in scenarios requiring nuanced and emotionally supportive communication.\nModel Parameters (in Millions) Size (MB) Inference speed (ms/input)\nT5B 60M ∼140 ∼45\nFLAN-T5S 80M ∼160 ∼50\nBARTB 139M ∼480 ∼70\nGODELB 220M ∼500 ∼75\nTable 9. Comparison of lightweight LLMs in terms of parameter count, model size, and inference speed.\n \nScientific Reports |        (2025) 15:22463 9| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\nFurthermore, the study highlights the critical role of a curated and diverse dataset in fine-tuning LLMs for \nspecialized tasks such as mental health counseling. The dataset not only enhanced model performance but also \nserved as a valuable foundation for fine-tuning LLMs to this sensitive domain. With further improvements in \ndata quality and validation, it could enable the development of robust AI-powered mental health counseling \ntools.\nOverall, this research demonstrates the potential of lightweight, accessible LLMs to contribute to mental \nhealth interventions, offering scalable and cost-effective solutions. Future research should explore refining these \nmodels for broader generalizability, integrating ethical frameworks to ensure safe deployment, and addressing \nthe challenges of real-world implementation to advance the field of AI-driven mental health counseling support \nsystem. Future research should also focus on curating and validating a more diversified and high-quality dataset \nto enhance the robustness and effectiveness of these models.\nData availability\nThe datasets used in this study have been collected from the following sources.Mental Health Counselling Da -\ntaset on Hugging Face, Chat Data on GitHub, dataset for counseling dialogues and Mental Health Counseling \nConversations on Hugging Face.\nReceived: 24 February 2025; Accepted: 30 May 2025\nReferences\n 1. Who mental health report.  h t t p s :  / / w w w .  w h o . i n  t / t e a m  s / m e n  t a l - h e  a l t h - a  n d - s u b  s t a n c  e - u s e /  w o r l d -  m e n t a l  - h e a l t h - r e p o r t.\n 2. Collaborators, I.S.-L.D.B.I.M.D. The burden of mental disorders across the states of India: The global burden of disease study \n1990–2017. Lancet Psychiatry 7, 148–161. https://doi.org/10.1016/S2215-0366(19)30475-4 (2020).\n 3. National mental health survey.  h t t p s :  / / s c i e  n c e . t h  e w i r e .  i n / h e  a l t h / t  h e - c a s  e - t o - e  x p a n d  - p s y c h  i a t r i c  - e d u c a  t i o n - f o r - m b b s - s t u d e n t s /.\n 4. Bubeck, S. et al. Sparks of artificial general intelligence: Early experiments with gpt-4. Preprint at arXiv (2023). arXiv:2303.12712.\n 5. The rise of ai in mental health care.  h t t p s :  / / t r e n  d s r e s e  a r c h . o  r g / i n  s i g h t /  s m a r t -  t h e r a p  y - s o l  u t i o n s  - t h e - r  i s e - o f  - a i - i n - m e n t a l - h e a l t h - c a \nr e / # : ~ : t e x t = I n % 2 0 t h e % 2 0 r e a l m % 2 0 o f % 2 0 m e n t a l , e n h a n c i n g % 2 0 t r e a t m e n t % 2 0 a c c e s s i b i l i t y % 2 0 a n d % 2 0 e ff   e c t i v e n e s s .\n 6. Denecke, K., Abd-Alrazaq, A. & Househ, M. Artificial intelligence for chatbots in mental health: Opportunities and challenges.  \nhttps://doi.org/10.1007/978-3-030-67303-1_10 (2021).\n 7. Google. T5 small. https://huggingface.co/google-t5/t5-small (2023).\n 8. Facebook. Bart base. https://huggingface.co/facebook/bart-base (2023).\n 9. Google. Flan-t5 small. https://huggingface.co/google/flan-t5-small (2023).\n 10. Microsoft. Godel v1.1 large seq2seq.  h t t p s :  / / h u g g  i n g f a c  e . c o / m  i c r o s  o ft   / G O  D E L - v 1  _ 1 - l a r  g e - s e q 2 s e q (2023).\n 11. D’ Alfonso, S. Ai in mental health. Curr. Opin. Psychol. 36, 112–117. https://doi.org/10.1016/j.copsyc.2020.04.005 (2020).\n 12. Feng, X., Hu, M. & Guo, W . Application of artificial intelligence in mental health and mental illnesses. In Proceedings of the \n3rd International Symposium on Artificial Intelligence for Medicine Sciences , 506–511, https://doi.org/10.1145/3570773.3570834 \n(Association for Computing Machinery, New Y ork, NY , USA, 2022).\n 13. Darzi, P . Could artificial intelligence be a therapeutic for mental issues?. Sci. Insights 43, 1111–1113.  h t t p s : / / d o i . o r g / 1 0 . 1 5 3 5 4 / s i . 2 3 \n. c o 1 3 2     (2023).\n 14. Shimada, K. The role of artificial intelligence in mental health: A review. Sci. Insights 43, 1119–1127.  h t t p s : / / d o i . o r g / 1 0 . 1 5 3 5 4 / s i . 2 \n3 . r e 8 2 0     (2023).\n 15. Abd-Alrazaq, A. A. et al. An overview of the features of chatbots in mental health: A scoping review. Int. J. Med. Inform. 132, \n103978. https://doi.org/10.1016/j.ijmedinf.2019.103978 (2019).\n 16. van der Schyff, E., Ridout, B., Amon, K., Forsyth, R. & Campbell, A. Providing self-led mental health support through an artificial \nintelligence-powered chat bot (leora) to meet the demand of mental health care. J. Med. Internet Res. 25, e46448.  h t t p s : / / d o i . o r g / 1 \n0 . 2 1 9 6 / 4 6 4 4 8     (2023).\n 17. Rathnayaka, P . et al. A mental health chatbot with cognitive skills for personalised behavioural activation and remote health \nmonitoring. Sensors https://doi.org/10.3390/s22103653 (2022).\n 18. Peng, C. et al. A study of generative large language model for medical research and healthcare. npj Digital Med. 6, 210.  h t t p s : / / d o i . \no r g / 1 0 . 1 0 3 8 / s 4 1 7 4 6 - 0 2 3 - 0 0 9 5 8 - w     (2023).\n 19. Singhal, K. et al. Towards expert-level medical question answering with large language models (2023). arXiv:2305.09617.\n 20. Y ang, H. et al. One llm is not enough: Harnessing the power of ensemble learning for medical question answering.  h t t p s : / / d o i . o r g / 1 0 . \n1 1 0 1 / 2 0 2 3 . 1 2 . 2 1 . 2 3 3 0 0 3 8 0     (2023).\n 21. He, Y ., Zhu, Z., Zhang, Y ., Chen, Q. & Caverlee, J. Infusing disease knowledge into bert for health question answering, medical \ninference and disease name recognition (2020). arXiv:2010.03746.\n 22. Stade, E. C. et al. Large language models could change the future of behavioral healthcare: a proposal for responsible development and \nevaluation https://doi.org/10.1038/s44184-024-00056-z (2024).\n 23. Lai, T. et al. Supporting the demand on mental health services with ai-based conversational large language models (llms). \nBioMedInformatics 4, 8–33. https://doi.org/10.3390/biomedinformatics4010002 (2024).\n 24. Y adav, M., Sahu, N. K., Chaturvedi, M., Gupta, S. & Lone, H. R. Fine-tuning large language models for automated diagnostic \nscreening summaries (2024). arXiv:2403.20145.\n 25. Zheng, Z., Liao, L., Deng, Y . & Nie, L. Building emotional support chatbots in the era of llms (2023). arXiv:2308.11584.\n 26. Galatzer-Levy, I. R., McDuff, D., Natarajan, V ., Karthikesalingam, A. & Malgaroli, M. The capability of large language models to \nmeasure psychiatric functioning. arXiv preprint arXiv:2308.01834, https://doi.org/10.48550/arXiv.2308.01834 (2023).\n 27. McDuff, D., Schaekermann, M., Tu, T. & et al. Towards accurate differential diagnosis with large language models. arXiv preprint \narXiv:2312.00164, https://doi.org/10.48550/arXiv.2312.00164 (2023).\n 28. Solaiman, I. et al. Evaluating the social impact of generative ai systems in systems and society. arXiv preprint arXiv:2306.05949, \nhttps://doi.org/10.48550/arXiv.2306.05949 (2023).\n 29. Malgaroli, M. et al. Large language models for the mental health community: Framework for translating code to care. Lancet Dig. \nHealth 7, e282–e285. https://doi.org/10.1016/S2589-7500(24)00034-6 (2024).\n 30. Aditya. Mental health counselling dataset. https://hugg ingface.co/d atasets/Adit ya149/Menta l_Health_Counselling_Dataset \n(2023).\n 31. Bertagnolli, N. Counselchat dataset.  h t t p s :  / / g i t h  u b . c o m  / n b e r t  a g n o l  l i / c o u  n s e l - c  h a t / b l  o b / m a  s t e r / d  a t a / c o  u n s e l c  h a t - d a t a . c s v (2020).\n 32. Bertagnolli, N. Counsel chat: A dataset for counseling dialogues.  h t t p s :  / / h u g g  i n g f a c  e . c o / d  a t a s e  t s / n b e  r t a g n o  l l i / c o  u n s e l - c h a t \n(2023).\nScientific Reports |        (2025) 15:22463 10| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/\n 33. Amod. Mental health counseling conversations. https://hugg ingface.co/d atasets/Amod /mental_hea lth_counseling_conversations \n(2023).\n 34. Bart model overview. https://hugg ingface.co/d ocs/transfor mers/en/mod el_doc/bart.\n 35. Face, H. Rouge metric (2023).\n 36. GeeksforGeeks. Nlp - bleu score for evaluating neural machine translation in python.  h t t p s :  / / w w w .  g e e k s f  o r g e e k  s . o r g  / n l p - b  l e u - s c  \no r e - f o  r - e v a  l u a t i n  g - n e u r  a l - m a c  h i n e - t r a n s l a t i o n - p y t h o n / (2023).\n 37. Face, H. Perplexity in transformers. https://hugg ingface.co/d ocs/transfor mers/en/per plexity (2023).\nAuthor contributions\nRitesh Maurya conceptualised and designed the study and prepared the manuscript. Nikhil Rajput and M. G. \nDiviit contributed to the implementation of the proposed framework. Satyajit Mahapatra contributed to data \ncollection and critically reviewed the manuscript. Manish Kumar Ojha provided valuable insights into the test-\ning of the proposed framework.\nFunding\nOpen access funding provided by Manipal Academy of Higher Education, Manipal\nDeclarations\nCompeting interest\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to S.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:22463 11| https://doi.org/10.1038/s41598-025-05012-1\nwww.nature.com/scientificreports/",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.65339195728302
    },
    {
      "name": "Computer science",
      "score": 0.5806397795677185
    },
    {
      "name": "Task (project management)",
      "score": 0.4362277388572693
    },
    {
      "name": "Medicine",
      "score": 0.3433920741081238
    },
    {
      "name": "Psychiatry",
      "score": 0.30416542291641235
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}