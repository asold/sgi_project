{
  "title": "Efficient Self-supervised Vision Transformers for Representation Learning",
  "url": "https://openalex.org/W3175466730",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2225388594",
      "name": "Li ChunYuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1658219663",
      "name": "Yang Jianwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2370394005",
      "name": "Zhang, Pengchuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2007162959",
      "name": "Gao Mei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1585946068",
      "name": "Xiao Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222155691",
      "name": "Dai, Xiyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101029194",
      "name": "Yuan Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao Jian-feng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3105236818",
    "https://openalex.org/W2970241862",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W2342877626",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2743200750",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2948433173",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W2308529009",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2951326654",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2558661413",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3036982689",
    "https://openalex.org/W2964074409",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3022061250",
    "https://openalex.org/W3007700590",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3121480429",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3106428938",
    "https://openalex.org/W3168822201",
    "https://openalex.org/W343636949",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W3034576826",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3113997557",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W3135715136",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2951873722",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2971155163",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W3161438416",
    "https://openalex.org/W2337374958",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2144796873",
    "https://openalex.org/W2990500698",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3135367836"
  ],
  "abstract": "This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and models are publicly available: https://github.com/microsoft/esvit",
  "full_text": "Published as a conference paper at ICLR 2022\nEFFICIENT SELF -SUPERVISED VISION\nTRANSFORMERS FOR REPRESENTATION LEARNING\nChunyuan Li1 Jianwei Yang1 Pengchuan Zhang1 Mei Gao2 Bin Xiao2 Xiyang Dai2\nLu Yuan2 Jianfeng Gao1\n1Microsoft Research at Redmond, 2Microsoft Cloud + AI\n{chunyl,jianwyan,penzhan,xuga,bixi,xidai,luyuan,jfgao}@microsoft.com\nABSTRACT\nThis paper investigates two techniques for developing efﬁcient self-supervised\nvision transformers (EsViT) for visual representation learning. First, we show\nthrough a comprehensive empirical study that multi-stage architectures with sparse\nself-attentions can signiﬁcantly reduce modeling complexity but with a cost of\nlosing the ability to capture ﬁne-grained correspondences between image regions.\nSecond, we propose a new pre-training task of region matching which allows the\nmodel to capture ﬁne-grained region dependencies and as a result signiﬁcantly\nimproves the quality of the learned vision representations. Our results show\nthat combining the two techniques, EsViT achieves 81.3% top-1 accuracy on\nthe ImageNet linear probe evaluation, outperforming prior arts with around an\norder magnitude of higher throughput. When transferring to downstream linear\nclassiﬁcation tasks, EsViT outperforms its supervised counterpart on 17 out of 18\ndatasets. The code and pre-trained models are released at: https://github.\ncom/microsoft/esvit\n1 I NTRODUCTION\nSelf-supervised learning (SSL) with Transformers (Vaswani et al., 2017) has become a de facto\nstandard of model choice in natural language processing (NLP). The dominant approaches such as\nGPT (Radford et al., 2018) and BERT (Devlin et al., 2019) are pre-training on a large text corpus\nand then ﬁne-tuning to various smaller task-speciﬁc datasets, showing superior performance. Larger\nTransformers pre-trained with larger-scale language datasets often lead to a stronger generalization\nability, demonstrated by improved performance in downsteam tasks (with no sign of performance\nsaturation yet), as exempliﬁed in GPT-3 (Brown et al., 2020).\nIn computer vision (CV), however, self-supervised visual representation learning is still dominated\nby convolutional neural networks (CNNs). Sharing a similar goal/spirit with NLP, SSL in CV aims\nto learn general-purpose image features from raw pixels without relying on manual supervisions,\nand the learned networks are expected to serve as the backbone of various downstream tasks such as\nclassiﬁcation, detection and segmentation. Recently, impressive performance have been achieved by\nCNN-based SSL, outperforming state-of-the-art (SoTA) fully-supervised pre-training methods (He\net al., 2020; Caron et al., 2020) on tasks with a limited number of labels. The key to success is\nview-level learning: maximizing agreement of learned representations between differently augmented\nviews of the same example. Recent works, including SimCLR-v2 (Chen et al., 2020d), BYOL (Grill\net al., 2020) and SwA V (Caron et al., 2020), have scaled up the CNN-based models to hundreds of\nmillions of parameters. However, SSL has not enjoyed the same scaling success in CV as that in NLP.\nSeveral attempts have been made to close the gap by combining SSL with Transformer and self-\nattention architectures. Early works include Selﬁe (Trinh et al., 2019), which generalizes the concept\nof masked language modeling of BERT for images. The idea has been recently revisited in Vision\nTransformer (ViT) (Dosovitskiy et al., 2021) via pre-training on a much larger scale dataset,e.g., JFT-\n300M. ImageGPT (iGPT) (Chen et al., 2020b) generalizes the concept of auto-regressive language\nmodeling of GPT for images, showing encouraging ImageNet recognition accuracy with a large\nmodel size. Contrastive learning with ViT has also been studied very recently in DINO (Caron\net al., 2021) and MoCo-v3 (Chen et al., 2021), where new SoTA result by linear probe evaluation on\n1\narXiv:2106.09785v2  [cs.CV]  6 Jul 2022\nPublished as a conference paper at ICLR 2022\n20 45 100 200 500 1000\nThroughput (#Images/s) in log space\n78\n80Linear probing accuracy (%)\nSwAV (ResNet50-w5)\nBYOL (ResNet200-w2)\nSimCLR-v2 (ResNet152-w3+SK)\nMoCo-v3 (ViT-L-BN/7)\nMoCo-v3 (ViT-H-BN/14)\nDINO (DeiT-S/8)\nDINO (ViT-B/8)\nDINO (ViT-B/16)\nEsViT (Swin-T)\nEsViT (Swin-S)\nEsViT (Swin-B)\nEsViT (Swin-S, W=14)\nEsViT (Swin-B, W=14)\n0 50 100 200 500\n# Parameters (Millions)\n74\n76\n78\n80Linear probing accuracy (%)\nSwAV (ResNet)\nBYOL (ResNet)\nSimCLR-v2 (ResNet)\nMoCo-v3 (ViT)\nDINO (ViT, P=16)\nMoBY\nEsViT (V)\nEsViT (V+R)\nFigure 1: Efﬁciency vs accuracy comparison under the linear classiﬁcation protocol on ImageNet.\nLeft: Throughput of all SoTA SSL vision systems, circle sizes indicates model parameter counts;\nRight: performance over varied parameter counts for models with moderate (throughout/#parameters)\nratio. EsViT pre-trained with and without the region-matching task are shown before and after the\narrows, respectively. Please refer Section 4.1 for details.\nImageNet-1K is achieved, by exhaustively consuming computation resource on full self-attention\noperators with long sequences of split image patches.\nAiming to improve the efﬁciency of Transformer-based SSL, this paper presents Efﬁcient self-\nsuperivsed Vision Transformers (EsViT), by using a multi-stage architecture and a region-based\npre-training task for self-supervised representation learning. Our main ﬁndings and contributions can\nbe summarized as follows:\n(1) An intriguing property of self-supervised monolithic Transformers is ﬁrstly reported in our paper:\nautomatic discovery of semantic correspondence between local regions.\n(2) We present the ﬁrst comprehensive empirical study to show the pros and cons of multi-stage\nvision Transformer architectures for SSL. Though greatly reducing compute complexity, we ﬁnd\nthat the multi-stage architecture causes the loss of the property in (1).\n(3) A region matching pre-train task is proposed to alleviate the issue in (2), and further improve the\nlearned representations and attentions.\n(4) We validate the new EsViT, which combines the two techniques, on a range of tasks. It sig-\nniﬁcantly reduces the cost in building SoTA SSL vision systems, as summarized in Figure 1,\nand shows better scaling performance on accuracy vs. throughput and model size. Under the\nlinear evaluation protocol, EsViT achieves 81.3% top-1 accuracy, showing the best performance\ncompared with all systems, and is 3.5×parameter-efﬁcient and has at least 10×higher through-\nput than previous SoTA (81.0%, MoCo-v3 with ViT-BN-L/7 (Chen et al., 2021)). Compared\nwith its supervised counterpart Swin Transformers (Liu et al., 2021), EsViT shows superior\nperformance on 17 out 18 datasets, when transferring the learned representations to downstream\nlinear classiﬁcation tasks.\n2 M ETHODS\nTransformer-based SSL methods emerge very recently to lead the state-of-the-art performance on\nthe ImageNet linear probe task (Chen et al., 2021; Caron et al., 2021). It inherits the successes\nfrom (1) monolithic Transformer architectures that dominate in NLP (Devlin et al., 2019; Radford\net al., 2018), and (2) instance-level contrastive learning objectives that demonstrate arguably the\nbest SSL performance in computer vision (Chen et al., 2020c). Though simple and effective, the\nexisting Transformer-based SSL methods require a large amount of compute resources (e.g., >1.7\nTPU years of training) to reach SoTA performance. We believe that the SSL system efﬁciency is\nhighly related to two ingredients: the network architecture and the pre-train task. To strike for a better\ntradeoff between accuracy and efﬁciency, we present EsViT, showing better synergy of networks (a\nmulti-stage Transformer architecture) and pre-train tasks (a non-contrastive region-matching task).\n2.1 N ETWORK ARCHITECTURES : F ROM MONOLITHIC TO MULTI -STAGE VIT BACKBONE\nMulti-stage ViT. This paper presents the ﬁrst empirical study of multi-stage Transformer architec-\ntures (Vaswani et al., 2021; Wang et al., 2021; Liu et al., 2021; Zhang et al., 2021; Wu et al., 2021)\n2\nPublished as a conference paper at ICLR 2022\nfor SSL. Each stage consists of a patch merging/embedding module, and a Transformer with sparse\nself-attention module. (i) The patch merging module plays a slightly different roles in different\nstages. In the ﬁrst stage, it splits an input RGB image into non-overlapping patches. Each patch is\ntreated as a “token”, constructed as a concatenation of the raw pixel RGB values, which is further\nprojected into a C-dimension feature. In the later stage, the patch merging module concatenates the\nfeatures of each group of 2 ×2 neighboring patches, and applies a linear layer on the 4C-dimensional\nconcatenated features. This reduces the number of tokens by a multiple of 2 ×2 = 4, and the output\ndimension is set to 2C. (ii) A Transformer with sparse self-attention module are then employed to\nenable interactions among the merged features. The two modules above are repeated for multiple\ntimes, typically 4 times, resulting in a multi-stage ViT. As a result, a hierarchical representation is\ngenerated: the number of tokens is reduced and the feature dimension (and the number of heads in\nself-attentions) of each token is increased, as the network gets deeper. An overview comparison of the\nmonolithic and multi-stage Transformer architectures for SSL is illustrated in Figure 7 in Appendix.\nAn intriguing property of self-supervised monolithic ViT.Though straightforward in implemen-\ntation, changing from monolithic to multi-stage architecture without careful treatments may lose\nsome desirable properties of self-supervised Transformers In out study, we ﬁrst empirically note an\nintriguing property of self-supervised monolithic ViT(Caron et al., 2021): the pre-trained model\nexhibits a very strong ability to automatically discovers correspondences, even without a region-level\nmatching objective speciﬁed in training.\nWe quantitatively evaluate the correspondence learning to illustrate this property, as discussed in\nthe following process. (i) Simulated benchmark. Based on 50K images in the ImageNet validation\ndataset, we create a simple evaluation benchmark with mild augmentations: For a center-crop image,\nwe apply HorizontalFlip, then ColorJitter and RandomGrayscale to create a new\naugmented view. In this way, ground-truth correspondences are created. (ii) Evaluation process.\nGiven two views of the same image, we use the pre-trained backbone to extract the top-layer features,\nand ﬁnd the feature vector in one view that best matches the other in terms of highest cosine similarity.\nThe accuracy is measured as the averaged percentage of correctly identifying the region-to-region\ncorrespondences. Please see details in Section C.7 in Appendix. (iii) Results. We quantitatively\nshow that a self-supervised monolithic ViT yields 95% accuracy. However, simply replacing the\nnetwork with a multi-stage Transformer yields only 66% accuracy. This signiﬁcant degradation\n(absolute 29% accuracy drop) reveals the loss of the correspondence learning property. We ﬁrst raise\nthis critical problem, and believe that it has a large impact on the pre-trained model’s performance in\nvarious downstream tasks.\n2.2 P RE-TRAINING TASKS : D ELVING INTO VIEWS WITH REGIONS\nWe employ a non-contrastive learning framework to build our SSL method. Speciﬁcally, Self-\ndistillation with no labels (DINO) (Caron et al., 2021) is considered. It leverages the knowledge\ndistillation learning paradigm where a student network gθs is trained to match the output of a given\nteacher network gθt , parameterized by θs and θt respectively. The neural network gis composed of\na backbone f (e.g., Transformers or ConvNets), and of a projection head h: g= h◦f. The features\nused in downstream tasks are the output of backbone f. In SSL, different augmented views ˜xof an\nimage xare fed into backbone network to obtain feature maps z= f(˜x). Two MLP heads followed\nby softmax per network further convert the feature vectorsz∈zinto probability vectors p= h(z);\none head for view-level and the other head for region-level, respectively.\nMore precisely, from a given image, we generate a set Vof different views1 following (Caron et al.,\n2021). The resulting feature map at the top layer for each view is z= [z1,...,z T ], where T is the\nsequence length, and zi is a region-level representation for the local patch at position i. Average\npooling is applied to obtain the view-level representation ¯z= avg-pool(z).\nView-level task Given the augmented view set for student Vand teacher V∗, a set of pairs P=\n{(s,t)|˜xs ∈V,˜xt ∈V∗and s ̸= t}is constructed to perform cross-view prediction tasks. We\n1This set often contains views of two different resolutions V= [Vg, Vl], where Vg = {˜xgi |i = 1, 2}is a\nglobal-view set of higher resolution, and Vl = {˜xli |i = 1, . . . ,8}is a local-view set of lower resolution. All\nviews Vare passed through the student while only the global views Vg are passed through the teacher.\n3\nPublished as a conference paper at ICLR 2022\nconsider the pre-training task at the view level proposed by (Caron et al., 2021):\nLV = 1\n|P|\n∑\n(s,t)∈P\nMV (s,t), with MV (s,t) = −ps log pt, (1)\nwhere ps = h(¯zs) and pt = h(¯zt) are the probability output of an MLP head hover the view-level\nrepresentations ¯zs and ¯zt, learned by student and teacher, respectively. In DINO, ViT/DeiT are\nconsidered, hence the view-level representation is the feature of the [CLS] token.\nRegion-level task In (Caron et al., 2021), the LV encourages “local-to-global” correspondences\nonly at a coarse level: the large crop and the small crop are matched in the view level, leaving\nregion-to-region correspondence unspeciﬁed. In monolithic Transformers, the drop paths and skip\nconnections from low-level features to high-level features help the the latter to remain discriminative,\nthus maintain good region-matching performance. However, such a property gets diluted due to\nthe merging operators in multi-stage Transformers. As shown in our experiments later, training a\nmulti-stage network with LV only indeed results in sub-optimal representations.\nFurther, it could be a waste of computation not to leverage region-level features zthat are computed\nin the process of extracting view-level feature. Inspired by the success of masked language modeling\ntask in BERT, we argue that it is important to have region-level pre-training task for computer vision,\nso that the model can (1) amortize the computation and fully leverage the extracted region-level\nfeatures, and (2) take into account the co-occurrences/structures between local features. Unfortunately,\ndirectly performing masked patch prediction (MPP) for the multi-stage Transformer architecture is\ninfeasible, as the one-to-one correspondences between the input visual tokens and output features get\ndiluted due to the merging operation. Even for monolithic architectures, MPP has not been proved\neffective in computer vision, as empirically shown in (Dosovitskiy et al., 2021).\nTo address this problem, we propose a non-contrastive, region-matching method that directly works\nat the level of local features by taking into account their correspondences:\nLR = 1\n|P|\n∑\n(s,t)∈P\nMR(s,t), with MR(s,t) = −1\nT\nT∑\ni=1\npj∗ log pi, j∗= arg max\nj\nzT\ni zj\n∥zi∥∥zj∥, (2)\nwhere pi = h′(zi) and pj = h′(zj) are the probability outputs of a new MLP head h′over the local\nfeatures of student zi ∈zs and teacher zj ∈zt, respectively. j∗is the index of the feature in zt that\nbest matches the i-th feature in zs, in the sense of highest cosine similarity. Note that zi and zj∗ are\ncontextualized features of two best matched regions from different augmentated views, minimizing\nLR encourages different contexts ( i.e., surrounding regions) to learn invariant features, and thus\ncaptures the region-dependency.\nGlobalTokenLocalTokens(Top-layerfeaturemaps)\nMLPHead MLPHeadView-levelPrediction Region-levelPrediction\n<latexit sha1_base64=\"0fqNNX3hjodXGn3XogT/1+ePt9I=\">AAAB/3icbVDLSsNAFJ3UV62vqODGTbAIrkoioi6Lbly4qGIf0IQwmU7boZNJmLkRS8zCX3HjQhG3/oY7/8ZJm4W2Hhg4nHMv98wJYs4U2Pa3UVpYXFpeKa9W1tY3NrfM7Z2WihJJaJNEPJKdACvKmaBNYMBpJ5YUhwGn7WB0mfvteyoVi8QdjGPqhXggWJ8RDFryzT03xDAkmKfXmZ+6QB8gvc0y36zaNXsCa544BamiAg3f/HJ7EUlCKoBwrFTXsWPwUiyBEU6zipsoGmMywgPa1VTgkCovneTPrEOt9Kx+JPUTYE3U3xspDpUah4GezNOqWS8X//O6CfTPvZSJOAEqyPRQP+EWRFZehtVjkhLgY00wkUxntcgQS0xAV1bRJTizX54nreOac1pzbk6q9YuijjLaRwfoCDnoDNXRFWqgJiLoET2jV/RmPBkvxrvxMR0tGcXOLvoD4/MHEaqWyQ==</latexit>\nL R\n<latexit sha1_base64=\"YinnPXKRCuCqMAFZiNeZ7Wuy52w=\">AAAB/3icbVDLSsNAFJ34rPUVFdy4CRbBVUlE1GXRjQsXFewDmhAm00k7dDIJMzdiiVn4K25cKOLW33Dn3zhps9DWAwOHc+7lnjlBwpkC2/42FhaXlldWK2vV9Y3NrW1zZ7et4lQS2iIxj2U3wIpyJmgLGHDaTSTFUcBpJxhdFX7nnkrFYnEH44R6ER4IFjKCQUu+ue9GGIYE8+wm9zMX6ANk7Tz3zZpdtyew5olTkhoq0fTNL7cfkzSiAgjHSvUcOwEvwxIY4TSvuqmiCSYjPKA9TQWOqPKySf7cOtJK3wpjqZ8Aa6L+3shwpNQ4CvRkkVbNeoX4n9dLIbzwMiaSFKgg00Nhyi2IraIMq88kJcDHmmAimc5qkSGWmICurKpLcGa/PE/aJ3XnrO7cntYal2UdFXSADtExctA5aqBr1EQtRNAjekav6M14Ml6Md+NjOrpglDt76A+Mzx8XwpbN</latexit>\nL V\nView2\nView1\nFigure 2: Pre-training objectives, including view-\nlevel (left) and region-level (right) prediction.\nThe overall pre-training objective of EsViT is\nL = LR + LV , we learn to match the fea-\nture distributions at both the view and region\nlevels by minimizing the cross-entropy loss\nw.r.t. the parameters of the student network\ngθs . A visual illustration is in Figure 2, and\nthe full algorithm is in Appendix. We updates\nteacher/student network alternatively: (i) Given\na ﬁxed teacher network, the student network is\nupdated by minimizing the full cross-entropy\nloss: θs ← arg minθs L(s,t; θs). (ii) The\nteacher model is updated as an exponential mov-\ning average (EMA) of the student weightsθt ←λθt + (1−λ)θs, with λfollowing a cosine schedule\nfrom 0.996 to 1 during training. By default, the full objective Lis used from the beginning. One can\nalso load a checkpoint trained by LV only, and add LR for continual pre-training, which is shown\neffective in boosting performance in our experiments.\nComputational overhead Note that applying LR on the traditional monolithic Transformer archi-\ntecture can be prohibitively computationally expensive, as it requires O(T2) to compute LR. For a\ntypical image of resolution 224×224, the feature map length of ViT/DeiT (with patch size 16) at\n4\nPublished as a conference paper at ICLR 2022\nthe top layer is T = 196, while the multi-stage architecture yields T = 49, which requires 3 times\nless compute in computing LR. To empirically illustrate this, we show in Appendix Section C.2 that\nLR adds acceptable extra memory and computational cost (around 1.2 and 1.05 ×, respectively) for\nmulti-stage Transformers, while it will quickly go out-of-memory for monolithic Transformers when\nthe batch size is increased.\n3 R ELATED WORKS\nRelation to mask prediction tasksWe can consider the proposedLR as a proxy to mimick masked\nlanguage modeling in BERT, where the “ground-truth” local token is a soft label provided by the\nteacher network, while the student network makes predictions to match that target, based on the\ncontext of regions in a different augmented view. Importantly, our LR considers softmax with\ncross-entropy in the objective, rather than MSE as in MPP. A very sharp teacher distribution is used\nby choosing small temperatures. This encourages the model to focus on the salient dimensions,\nrather than waste modeling capability on training short-range dependencies and high-frequency\ndetails (Ramesh et al., 2021).\nRelation to DenseCL The proposed LR mostly related to DenseCL (Wang et al., 2020b) in that\nthe region correspondences in both methods are determined as the two most similar grid features.\nOne critical difference is that DenseCL is a contrastive region-matching task, while our LR is a\nnon-contrastive region-matching task, where no negative samples/queue is needed. This technical\ndifference has a signiﬁcant impact on the downstream task performance. We ﬁnd that LR is particu-\nlarly effective in serving our goal to improve image classiﬁcation performance and build efﬁcient &\naffordable SoTA SSL system; In contrast, DenseCL degrades the classiﬁcation performance.\nRelation to other region-level tasksThe ideas of leveraging local region-level pre-training tasks\nfor visual representation learning have been explored for ConvNets (Misra & Maaten, 2020; Xiong\net al., 2020; Wang et al., 2020b; Xie et al., 2021a; Yang et al., 2021; Xie et al., 2021c). We summarize\nthe differences in three aspects: (i) Motivation. Our region-matching task LR aims to recover the lost\nproperty of automatic correspondence learning in self-supervised monolithic Transformers, while\nmost existing region-level tasks aim to improve dense visual prediction tasks. (ii) Technical differ-\nence. Our LR is a non-contrastive region-matching task, while others are contrastive learning. (iii)\nEmpirical performance. Most region-level tasks improve dense visual prediction tasks but sacriﬁce\ntheir image classiﬁcation performance, while LR consistently improves classiﬁcation performance.\nAmong them, EsViT training method achieves the best ImageNet linear probe performance with\nminimum computational overhead. For detailed comparisons, please refer to Table 7 in Appendix.\nSelf-supervised vision Transformers. The research on Transformer-based self-supervised repre-\nsentation learning just scratches the tip of the iceberg, and only a few attempts are made on this\ntopic. ImageGPT (Chen et al., 2020b) and MoCo-v3 (Chen et al., 2021) dedicate huge compute\nresource with large models to exploring the frontier. DINO (Caron et al., 2021) achieves comparable\nperformance of large self-supervised ConvNets using small/medium-size Transformers. The proposed\nEsViT further pursues efﬁcient and affordable solutions to self-supervised vision Transformers. For\nmore general related works on Transformers for vision tasks and self-supervised ConvNets, please\nrefer to Section B in Appendix.\n4 E XPERIMENTAL RESULTS\nWe describe the experimental settings in Appendix Section C.3, and evaluate the proposed EsViT to\nanswer three questions: Q1: How does EsViT perform on standard ImageNet benchmark compared\nto SoTA methods? Q2: How effective EsViT is when transferring to downstream tasks? Q3: What\nare the design choices and empirical contributions of LR? Q4: When does the intriguing property of\nself-supervised Transformers exist, including learned correspondence and attentions?\n4.1 C OMPARISONS WITH PRIOR ART ON IMAGE NET\nWe report top-1 linear probe and k-NN accuracy on the ImageNet validation set. Table 1 presents\ncomparisons with SoTA SSL systems across various architectures. Please refer to Figure 1 for\ncomparisons over scaling parameter counts and throughput. Our ﬁndings are summarized below.\nComparisons with self-supervised Transformers.The DINO- and MoCo-based ViT has higher\naccuracy and smaller models than iGPT, under the same linear probing protocol and training data.\n5\nPublished as a conference paper at ICLR 2022\nMethod #Parameters ↓ Throughput↑ Linear↑ k-NN↑\nSoTA SSL methods with Big ConvNets\nSwA V , RN50w5 (Caron et al., 2020) 586 76 78.5 67.1\nBYOL, RN200w2 (Grill et al., 2020) 250 123 79.6 73.9\nSimCLR-v2, RN152w3+SK (Chen et al., 2020d) 794 46 79.8 73.1\nSkyline methods with excessively long sequences for self-attentions\nDINO, DeiT-S/8 (Caron et al., 2021) 21 180 79.7 78.3\nDINO, ViT-B/8 (Caron et al., 2021) 85 63 80.1 77.4\nMoCo-v3, ViT-B-BN/7 (Chen et al., 2021) 85 ∼63 79.5 -\nMoCo-v3, ViT-L-BN/7 (Chen et al., 2021) 304 ∼17 81.0 -\niGPT, iGPT-XL (Chen et al., 2020b) 6801 - 72.0 -\nEsViT, Swin-T/W=14 28 660 78.7(77.9) 77.0(75.5)\nEsViT, Swin-S/W=14 49 383 80.8(79.4) 79.1(77.3)\nEsViT, Swin-B/W=14 87 254 81.3(80.5) 79.3(78.3)\nTransformer-based SSL, with moderate sequence length for self-attentions\nMasked Patch Pred., ViT-B/16 (Dosovitskiy et al., 2021) 85 312 79.9 † -\nDINO, DeiT-S/16 (Caron et al., 2021) 21 1007 77.0 74.5\nDINO, ViT-B/16 (Caron et al., 2021) 85 312 78.2 76.1\nMoCo-v3, ViT-B/16 (Chen et al., 2021) 85 312 76.7 -\nMoCo-v3, ViT-H-BN/16 (Chen et al., 2021) 632 ∼32 79.1 -\nMoBY , Swin-T (Xie et al., 2021b) 28 808 75.1 -\nEsViT, Swin-T 28 808 78.1(77.0) 75.7(74.2)\nEsViT, Swin-S 49 467 79.5(79.2) 77.7(76.8)\nEsViT, Swin-B 87 297 80.4(79.6) 78.9(77.7)\nTable 1: Comparison with SoTA across different architectures on ImageNet linear probing. EsViT\nwith LL +LR is reported, while EsViT with onlyLR is shown in parentheses. W = 14 is the window\nsize, otherwise the default W = 7. ViT-BN is ViT that has BatchNorm (Frankle et al., 2020), and\n“/P” denotes a patch size of P×P. “∼” indicates through-puts estimated by comparing different\npapers, detailed in Appendix. †The mask patch prediction in (Dosovitskiy et al., 2021) is pre-trained\non JFT-300M and end-to-end ﬁne-tuned in ImageNet, which we append as a reference.\nAt the similar level of model size and compute complexity, the proposed EsViT improve SoTA\nmethods DINO/MoCo-v3 by a large margin: EsViT (Swin-B) outperforms DINO (ViT-B/16) by\n2.2% linear probe accuracy and 2.8% k-NN accuracy in absolute values. EsViT (Swin-B) even\nperforms slightly better than DINO (ViT-B/8) (0.3% higher linear probe accuracy and 1.5% higher\nk-NN accuracy), with 4×higher throughput. MoBY (Xie et al., 2021b) is a con-current work that\ninvestigates multi-stage ViT in SSL. With the same architecture Swin-T, our EsViT pre-training\ntasks signiﬁcantly outperform MoBY , showing 3% higher accuracy. In EsViT, longer sequences in\nself-attention is implemented by increasing the window size. We experiment this by considering a\nwindow size of W=14. Overall, the proposed EsViT (Swin-B/W=14) shows the best performance\n(top-1 accuracy 81.3%, top-5 accuracy 95.5%, k-NN accuracy 79.3%), compared with all systems,\nand is 3.5×parameter-efﬁcient and has at least 10×higher throughput than previous SoTA MoCo-v3.\nComparisons with big ConvNets.We compare with the SoTA big ResNets reported by SimCLR-\nv2 (Chen et al., 2020d), BYOL (Grill et al., 2020) and SwA V (Caron et al., 2020). Among them, the\nbest accuracy 79.8% under the linear probing protocol is reported by SimCLR-v2 with SK-ResNet,\nwhere Selective Kernel (SK) (Li et al., 2019c) is a form of attention to enhance CNNs. It is clear\nin Figure 1 (b) that all ConvNets-based SSL methods show an envelope in the regime of scaling up\nmodel sizes after passing 500M. EsViT achieves better accuracy than their highest envelope, with\n16×less model parameters and 8×higher throughput.\n4.2 T RANSFER LEARNING\nWe also conduct transfer learning in downstream tasks to evaluate the quality of learned representa-\ntions. Two sets of tasks are considered:\n• Classiﬁcation on a suite of 18 small datasets. As exempliﬁed in (Radford et al., 2021), it is a\ncommon and clean approach to evaluate a learned representation by ﬁtting a linear classiﬁer\non the representation and measuring its performance across multiple datasets. We study 18\ndatasets used in (Radford et al., 2021). Automatic hyper-parameter tuning is considered to\nensure fairness of comparison. Besides averaged scores, we report # winsas the number of\ndatasets on which the model outperforms its supervised counterpart. Detailed dataset description\nand settings are in Appendix.\n6\nPublished as a conference paper at ICLR 2022\nCaltech101\nCIFAR10CIFAR100\nDTD\nFER2013\nFGVCAircraftFlowers102\nFood101 GTSRB\nHatefulMemes\nMNIST\nPatchCamelyon\nOxfordPetsStanfordCars\nSTL10 SUN397 UCF101VOC2007\n50\n60\n70\n80\n90\n100Score (%)\n+1.05 +0.96\n+2.89\n+4.18\n+2.47 +7.61\n+5.27\n+1.98 +10.2\n+4.59\n+0.20\n-1.6\n+0.11\n+4.95\n+0.63\n+4.06\n+2.04\n+2.29\nSupervised: 77.86 EsViT (V): 79.92 EsViT (V+R): 80.86\nFigure 3: Linear probing on 18 downstream datasets. Averaged scores are reported for each method.\nEsViT outperforms its supervised counterpart on 17 out of 18 datasets.\nAPbb APbb\n50 APbb\n75\nSup. 46.0 68.1 50.3\nEsViT 46.2(46.2) 68.0(67.9) 50.6(50.5)\nAPmb APmb\n50 APmb\n75\nSup. 41.6 65.1 44.9\nEsViT 41.6(41.7) 64.9(64.8) 44.8(45.1)\nTable 2: COCO Detection & Segmentation.\nPre-train Data ImageNet-1K 18 Datasets\nLinear k-NN Scores # Wins\nSupervised - - 77.29 -\nImageNet-1K 78.0(77.1) 75.7(73.7) 80.66 16\nWebVision-v1 75.9(75.4) 71.2(69.4) 80.00 14\nOpenImages-v470.6(69.6) 62.0(60.3) 77.97 10\nImageNet-22K 75.0(73.5) 67.9(66.1) 81.03 17\nTable 3: Impact of the pre-train datasets.\n• Detection and segmentation on COCO. Different from previous monolithic self-supervised ViT,\nthe multi-stage architecture in EsViT can be readily used for dense visual tasks that require\nhierarchical feature representations.\nComparison with supervised counterparts. We compare with the supervised-learning Swin,\nwhose checkpoints are downloaded from the ofﬁcial codebase2. Figure 3 shows the classiﬁcation\nresults of Swin-S, EsViT consistently outperforms its supervised variant, often by a large margin.\nSimilar conclusions are drawn for other model sizes. On COCO detection and segmentation task,\nhowever, EsViT shows comparable results with the variant withLV only (shown in parentheses) and\nthe supervised counterpart (Swin-T trained with 3×schedule), as shown in Table 2. We hypothsize\nthis is related to the non-constrastive nature of EsViT, as explained later.\nEffects of larger, less-curated pre-train datasets.The performance of Transformer-based SSL\nresearch has thus far been limited to highly curated pre-train data such as ImageNet-1K. To push the\nfrontier in leveraging large amounts of unlabeled data, we explore the effects of pre-training from\nlarger, less-curated image datasets: WebVision-v1 (Li et al., 2017), OpenImages-v4 (Kuznetsova\net al., 2020) and ImageNet-22K (Deng et al., 2009), described in Appendix. The pre-train epochs\non different datasets are adjusted so that all models see a similar number of augmented views. We\nsummarize the results in Table 3 and would like to emphasize the following ﬁndings. First, LR\nimproves LV (shown in parentheses) on all datasets. Second, all EsViT pre-trained checkpoints\noutperform supervised checkpoint in downstream classiﬁcation tasks, but performance varies a lot,\nwith ImageNet-22K checkpoint showing the best transfer ability. Third, ImageNet-1K pre-trained\nmodel shows the best ImageNet-1K linear probe performance. We hypothesize that it is not only the\nsize of pre-train dataset matters, but also the distribution of image classes matters: more diverse and\nwell-balanced distribution results in a stronger generalization ability.\n4.3 D ISCUSSION ON THE NON-CONTRASTIVE REGION -MATCHING TASK\nCompatibility with various network architectures.We investigate ResNet-50 and different efﬁ-\ncient sparse Transformers in Table 4. DeiT is shown as a baseline reference. Batch size = 1024 in\nthis experiment. To ensure fair comparison, we modify all into a 4-stage architecture with the number\nof Transformer layers in each stage as 2-2-6-2. We see that LR improves all network architectures,\nincluding ResNet-50, Swin (Liu et al., 2021), ViL (Zhang et al., 2021), CvT (Wu et al., 2021)\nand PvT (Wang et al., 2021). Though directly adding LR to monolithic ViT is computationally\ninfeasible, we uniformly sampled top-layer grid features of DeiT and then add LR, but did not\nobserve performance improvement. This is partly because the monolithic ViT itself already has a\ngood corresponding ability, an extra region-matching task does not provide new learning signals. As\n2https://github.com/microsoft/Swin-Transformer\n7\nPublished as a conference paper at ICLR 2022\nMethod #Param. Im./sPre-train tasksLineark-NN\nDeiT 21 1007 LV 75.9 73.2\nR-50 24 1237 LV 75.3† 67.5†\nLV 75.0 69.3LV+LR 75.7 71.2\nSwin 28 808 LV 77.1 73.7LV+LR 77.6 75.4\nViL 28 386 LV 77.3 73.9LV+LR 77.5 74.5\nCvT 29 848 LV 77.6 74.8LV+LR 78.5 76.7\nPvT 24 851 LV 75.4 72.0LV+LR 76.3 72.9\nTable 4: Different architectures with and without\nLR. DeiT and ResNet-50 are shown as references.\n†Numbers reported in (Caron et al., 2021).\n0 50 100 200# Epoch\n70\n74\n76\n78k-NN (%)\nLV (Tiny)\nLV + LR (Tiny)\nLV (Small)\nLV + LR (Small)\nLV (Base)\nLV + LR (Base)\nFigure 4: Learning curves of different pre-\ntraining tasks. For Base model, LR is added\nfrom the 200th epoch.\nPre-trainingResNet50in different settings ImageNet-1K COCO\nTypes Methods #Epochs #Views Linear k-NN APbb APmb\nSupervised - - 38.2 33.3\nContrastive MoCo-v2 200 2 67.5 55.6 38.7 33.9\nDenseCL 200 2 63.6(-3.9) 48.6(-7.0) 39.1(+0.4) 34.2(+0.3)\nNon-ContrastiveLV 200 2 69.2 59.9 37.8 33.1\nLV +LR 200 2 69.9(+0.7) 61.7(+1.8) 38.0(+0.2) 33.2(+0.1)\nTable 5: Comparison between contrastive and non-contrastive region-matching tasks.\ncompared in Appendix Table 12 with the ResNet-50 backbone, EsViT learning method shows the\nhighest accuracy, compared with existing SSL methods.\nModel scaling withLR. We compare the pre-training objective with and without LR in Table 1.\nAcross different model scales and window sizes, the proposed region level LR can consistently\nimprove the performance. The gains can be clearly seen by k-NN accuracy (around 1-2%), where\nno additional tuning is needed as in linear probe. Figure 4 demonstrates that LR helps model\nconvergence, and can be used as a drop-in to improve models trained with the view level task.\nContrastive vs Non-contrastive region-matching tasks. The proposed LR adds a non-contrastive\nregion-matching task to the non-contrastive view-level taskLV ; On the contrary, DenseCL adds a\ncontrastive region-matching task to the contrastive view-level task MoCo-v2. In Table 5, we compare\nfour methods in the same setting with ResNet-50. DenseCL improves dense visual prediction perfor-\nmance, but hurts classiﬁcation performance. LR improves both tasks, especially the classiﬁcation\nperformance. One limitation is that the non-contrastive methods show lower performance in dense\nprediction tasks, this is consistent with the observations for BYOL in (Wang et al., 2020b). The\nsimple LR shows the best ImageNet accuracy compared with all sophisticated region-level tasks in\nthis 200-epoch setting in Appendix Table 7, and the best overall accuracy in Table 12. It indicates\nthat LR well serves our goal in building efﬁcient SoTA SSL systems.\nDesign choices ofLR. We ablate a couple of choices in constructing LR in Eq. (2). (i) Softmax\nvs MSE. One alternative way to measure the distance between two projected vectors is MSE, as\nemployed in the popular non-contrastive SSL algorithm BYOL (Grill et al., 2020). When adding\nregion-matching tasks to BYOL and pre-training 50 epochs,Softmax and MSE yield k-NN accuracy\nof 37.2% and 34.9%, while the baseline BYOL yields 33.1%. We also replace the region-matching\nmetric in EsViT as MSE, yielding k-NN accuracy 72.6%, which lower than the view-level task only\n(74.2%). These results show that Softmax is essential in LR. (ii) Optimal Transport (OT) vs\nSimple Argmax. To avoid heavy computational overhead, a simple feature-level argmax solution is\nconsidered in Eq. (2) to pair two local regions. To study the impact of high region-matching quality,\nwe consider OT. Empirically, we observe OT yields slightly higherk-NN accuracy at the early stage,\nbut the gain is diminished in the end. Considering the extra computational cost of solving OT with an\ninner loop in sinkhorn algorithm (Cuturi, 2013), we opt for simple argmax in our experiments.\n8\nPublished as a conference paper at ICLR 2022\n(a) DINO: DeiT-S (b) EsViT: LV (c) EsViT: LV +LR\nFigure 5: The learned correspondences. Yellowlines are the top-10 correspondences between two\nviews, where the numbers indicates the rankings of similarity scores, yellow dots with the same\nnumber are paired.\n(a) DINO: DeiT-S (b) EsViT: LV (c) EsViT: LV +LR\nFigure 6: Visualization of the the learned attention map for different heads in the last layer. The query\nis the blue dot in the center of the images. We visualize masks (as red) obtained by thresholding\nthe self-attention maps to keep 60% of the probability mass. Note that all 6 heads are visualized\nfor DINO with DeiT-S, and 6 out of 24 heads in EsViT are chosen to visualize (ranked by entropy\nvalues). Please see enlarged pictures with all heads in Appendix.\n4.4 Q UALITATIVE STUDIES\nVisualization of correspondences. Given two views of the same image, we use the pre-trained\nbackbone to extract the top-layer featuresz1 and z2. For each feature vector in z1, we ﬁnd the feature\nvector in z2 that best matches it in terms of highest cosine similarity, as deﬁned in Equation (2). In\nFigure 5, we show the top-10 correspondences between two views for three methods. In Figure 5\n(b), EsViT with LV tends to identify pairs in the background as the most matched ones (and in a\nwrong way in this example). This could be a valid solution to LV , as the invariance in the level of\naggregated global features does not necessarily induce invariances in the local region level. This is\nsigniﬁcantly alleviated with LR (shown in Figure 5 (c)), a task that implicitly requires local matching.\nSurprisingly, DINO is able to learn good correspondences even without the region-level matching task.\nTo the best of our knowledge, this is a previously unreported intriguing property of self-supervised\nTransformers with monolithic architectures: good semantic correspondences are automatically\nlearned. We hypothesize that features at lower layers (image patch itself in the extreme case)\ncan directly pass to higher layers, and the former regularizes the latter to remain discriminative.\nNevertheless, the proposed LR can dramatically reduce the issue, and is good remedy to rescue the\nloss of semantic correspondence for the multi-stage architecture. In Appendix, we quantitatively\nmeasures the correspondence learning ability of these SSL methods on ImageNet validation dataset,\nthe observations are consistent: LR improves the matching accuracy from 66% to 91%.\nVisualization of attention maps. We look at the self-attention in the different heads of the last\nlayer in Figure 6. A local region on the edge of the main object is employed as query, and the\nattended regions are highlighted in red for those the query’s top 60% mass are assigned. In Appendix,\nwe visualize more examples with different query positions. DINO tends to automatically learn\nclass-speciﬁc attention maps leading to foreground object segmentation, regardless of its query\nlocated in foreground or background. This is probably because main objects remain as the major\ninvariance factor in different augmented views. This property is lost when a multi-stage architecture\nis employed, as shown in EsViT with LV . These patterns are consistent for different heads. After\nintroducing LR for EsViT, we note that the attention maps become more diverse in different heads,\ni.e., entropy values of attentions get more skewed, and attended regions are more different. This is\nperhaps because LR requires each region to consider many matching tasks to regions in different\naugmented views, each head automatically learns to distribute the tasks and complete a few of them.\n9\nPublished as a conference paper at ICLR 2022\n5 C ONCLUSIONS\nIn this paper, we ﬁrst discover the automatic correspondence learning property of self-supervised\nmonolithic Transformers. Inspired by this, we present efﬁcient self-supervised vision Transformers\n(EsViT) to with two major insights: a multi-stage Transformer architecture with sparse self-attentions,\nand a non-contrastive region-matching pre-training task. The synergy of both helps EsViT reach the\nSoTA performance of SSL vision systems with signiﬁcantly less compute and smaller model size.\nOur study also reveals that exploration of effective solutions to learn from larger and less curated\npre-training data in the wild is a key but less studied factor in paving the way toward the scaling\nsuccess of SSL vision systems.\nETHICS STATEMENT\nThough self-supervised learning (SSL) has great potentials to learn powerful representation with-\nout human annotation, the existing techniques to build SoTA SSL vision systems tend to be Red\nAI (Schwartz et al., 2020): it could be environmentally unfriendly and the computational cost is\nextensively high. The required training resource is typically not accessible for a lab environment\n(thus raising barriers to participation in AI research). For example, the prior art MoCo-v3 has greatly\npushes the performance limit of SSL system (Chen et al., 2021). The authors kindly reported that\n“it (MoCo-v3, ViT-H) takes 9.8 hours per 100 epochs using 512 TPUs. This is a gigantic scale of\ntraining: for the 300-epoch ViT-H, this amounts to ∼625 TPU days, or ∼1.7 TPU years of training.”\nThe SoTA model MoCo-v3 with ViT-BN-L/7 should have a higher cost than this. Even for a smaller\nmodel ViT-B, “it takes 24 hours in 128 GPUs (vs. 2.1 hours in 256 TPUs)”. Hence, improving the\nefﬁciency of building SoTA SSL systems is of high value for the community and society to achieve\nGreen AI(Schwartz et al., 2020).\nTo this end, we propose EsViT to provide more affordable and efﬁcient solutions for the community\nto experiment and explore the directions of SoTA SSL in computer vision. Our EsViT model shows\nthe best ImageNet linear probe performance compared with all existing SSL vision systems, and is\n3.5×parameter-efﬁcient and has 10×higher throughput than previous SoTA. This efﬁciency gain can\nsigniﬁcantly decrease its carbon footprint and increase its inclusivity, encouraging more researchers\nto participate the study of the SSL topic.\nREPRODUCIBILITY STATEMENT\nOur paper provides comprehensive empirical studies on the EsViT algorithm. We provide PyTorch-\nstyle pseudo-code in Appendix. We also include an example code with instruction as supplementary\nmaterial to ensure the reproducibility. For empirical results on both various network architecture and\nlarge-scale datasets, we provide detailed hyper-parameter speciﬁcations. We release the pre-trained\ncheckpoints and codebase for the research community for reproducible research.\nREFERENCES\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuper-\nvised learning of visual features. In ECCV, 2018.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. arXiv preprint\narXiv:2006.09882, 2020.\n10\nPublished as a conference paper at ICLR 2022\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294, 2021.\nHanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chun-\njing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint\narXiv:2012.00364, 2020a.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and\nIlya Sutskever. Generative pretraining from pixels. In ICML, 2020b.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. ICML, 2020c.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big\nself-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029,\n2020d.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020e.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised visual\ntransformers. arXiv preprint arXiv:2104.02057, 2021.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. Uniter: Learning universal image-text representations. arXiv preprint\narXiv:1909.11740, 2019.\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural\ninformation processing systems, 2013.\nZhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UP-DETR: Unsupervised pre-training for\nobject detection with transformers. arXiv preprint arXiv:2011.09094, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. NAACL, 2019.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In ICCV, 2015.\nJeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In NeurIPS,\n2019.\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas\nBrox. Discriminative unsupervised feature learning with exemplar convolutional neural networks.\nT-PAMI, 2015.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\nJonathan Frankle, David J Schwab, and Ari S Morcos. Training Batchnorm and only Batchnorm: On\nthe expressive power of random features in CNNs. arXiv preprint arXiv:2003.00152, 2020.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\npredicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\nPriya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training\nImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n11\nPublished as a conference paper at ICLR 2022\nPriya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat\nSingh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual\nfeatures in the wild. arXiv preprint arXiv:2103.01988, 2021.\nJean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\nand maximization. arXiv preprint arXiv:1808.06670, 2018.\nXu Ji, Jo ˜ao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised\nimage classiﬁcation and segmentation. In ICCV, 2019.\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab\nKamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4.\nInternational Journal of Computer Vision, 2020.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic\ncolorization. In ECCV, 2016.\nChunyuan Li, Xiujun Li, Lei Zhang, Baolin Peng, Mingyuan Zhou, and Jianfeng Gao. Self-supervised\npre-training with hard examples improves visual representations. arXiv preprint arXiv:2012.13493,\n2020a.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-VL: A universal encoder\nfor vision and language by cross-modal pre-training. arXiv preprint arXiv:1908.06066, 2019a.\nJunnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive\nlearning of unsupervised representations. arXiv preprint arXiv:2005.04966, 2020b.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019b.\nWen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual\nlearning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.\nXiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In CVPR, 2019c.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks. In ECCV, 2020c.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in Adam. arXiv preprint\narXiv:1706.02677, 2018.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. VilBERT: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. NeurIPS, 2019.\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations.\nIn CVPR, pp. 6707–6717, 2020.\n12\nPublished as a conference paper at ICLR 2022\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\npuzzles. In ECCV, 2016.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In International Conference on Machine Learning, 2018.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In CVPR, 2016.\nYunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence\nCarin. Variational autoencoder for deep learning of images, labels and captions. NIPS, 2016.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. OpenAI Blog, 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green AI. Communications of the\nACM, 2020.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training\nof generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.\nHao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from\ntransformers. EMNLP, 2019.\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What\nmakes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. MLP-mixer: An\nall-MLP architecture for vision. arXiv preprint arXiv:2105.01601, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e\nJ´egou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nTrieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selﬁe: Self-supervised pretraining for image\nembedding. arXiv preprint arXiv:1906.02940, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nAshish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efﬁcient visual backbones. CVPR, 2021.\nHuiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab:\nEnd-to-end panoptic segmentation with mask transformers. arXiv preprint arXiv:2012.00759,\n2020a.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\n13\nPublished as a conference paper at ICLR 2022\nXinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for\nself-supervised visual pre-training. arXiv preprint arXiv:2011.09157, 2020b.\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia\nXia. End-to-end video instance segmentation with transformers. arXiv preprint arXiv:2011.14503,\n2020c.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\nEnze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Zhenguo Li, and Ping Luo. Detco:\nUnsupervised contrastive learning for object detection. arXiv preprint arXiv:2102.04803, 2021a.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.\nIn ICML, 2016.\nZhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-supervised\nlearning with swin transformers. arXiv preprint arXiv:2105.04553, 2021b.\nZhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself:\nExploring pixel-level consistency for unsupervised visual representation learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021c.\nYuwen Xiong, Mengye Ren, and Raquel Urtasun. Loco: Local contrastive representation learning.\narXiv preprint arXiv:2008.01342, 2020.\nCeyuan Yang, Zhirong Wu, Bolei Zhou, and Stephen Lin. Instance localization for self-supervised\ndetection pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021.\nFuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer\nnetwork for image super-resolution. In CVPR, 2020.\nJianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and\nimage clusters. In CVPR, 2016.\nTian Yonglong, Olivier J. Henaff, and Aaron van den Oord. Divide and contrast: Self-supervised\nlearning from uncurated data. arXiv preprint arXiv:2105.08054, 2021.\nXiaohang Zhan, Jiahao Xie, Ziwei Liu, Yew-Soon Ong, and Chen Change Loy. Online deep clustering\nfor unsupervised representation learning. In CVPR, pp. 6688–6697, 2020.\nPengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding.\narXiv preprint arXiv:2103.15358, 2021.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning\nby cross-channel prediction. In CVPR, 2017.\nMinghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object\ndetection with adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Uniﬁed\nvision-language pre-training for image captioning and VQA. AAAI, 2020.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\nChengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of\nvisual embeddings. In CVPR, 2019.\n14\nPublished as a conference paper at ICLR 2022\nA M ETHODS\nA.1 A LGORITHMS\nWe summarize the training algorithm procedure of EsViT withLV +LR in Algorithm 1. To clearly\noutline the main idea of the algorithm, we show the algorithm for two augmented views. For the full\nalgorithm to deal with multi-crop, please refer to our codebase. In Algorithm 1, for a mini-batch of\nsize n, the teacher/student network consists of three output variables: (1)p ∈Rn×K is the probability\nvector for the view-level representation, output by an MLP head. (2) z ∈Rn×T×P is the feature\nmap, containing T region-level features of dimension P. (3) pz ∈Rn×T×K are probability vectors\nof z, output by a different MLP head.\nAlgorithm 1:EsViT with LV +LR, pseudocode with 2-crop.\n# gs, gt: student and teacher networks\n# Cv, Cr: view and region center (K)\n# tmp s, tmp t: student and teacher temperatures\n# a, b: network and center momentum rates.\n# n: batch size, K: MLP-head-projected probability vector length,\nT: last layer feature map length, P: last layer feature vector\nlength\n1 gt.params = gs.params\n# The main training loop\n2 for x in loader:\n3 x1, x2 = augment(x), augment(x) # two random views\n4\n# student output, p:n×K, pz:n×T×K, z:n×T×P\n5 p s1, pz s1, z s1 = gs(x1)\n6 p s2, pz s2, z s2 = gs(x2)\n# teacher output, p:n×K, pz:n×T×K, z:n×T×P\n7 p t1, pz t1, z t1 = gt(x1)\n8 p t2, pz t2, z t2 = gt(x2)\n9\n# view-level loss\n10 loss v = Hv(p s1, p t2)/2 +Hv(p s2, p t1)/2\n# region-level loss\n11 loss r = Hr(pz s1, pz t2, z s1, z t2)/2 +Hr(pz s2, pz t1, z s2, z t1)/2\n12 loss = loss v/2 +loss r/2\n13 loss.backward() # back-propagate\n14\n# update student, teacher and centers\n15 update(gs) # AdamW for student\n16 gt.params = a ∗gt.params + (1−a) ∗gs.params # EMA for teacher\n17 Cv = b ∗Cv + (1−b) ∗cat([p t1,p t2].mean(0)) # EMA for view center\n18 Cr = b ∗Cr + (1−b) ∗cat([pz t1,pz t2].mean(0)) # EMA for region center\n19\n# The view-level loss function\n20 def Hv(s, t):\n21 t = t.detach() # stop gradient\n22 s = softmax(s / tmp s, dim=-1)\n23 t = softmax((t - Cv) / tmp t, dim=-1)\n24 return - (t * log(s)).sum(dim=-1).mean()\n# The region-level loss function\n25 def Hr(ps, pt, zs, zt):\n26 pt = pt.detach() # stop gradient\n27 ps = softmax(ps / tmp s, dim=-1) # n ×T×K\n28 pt = softmax((pt - Cr) / tmp t, dim=-1) # n ×T×K\n29 sim matrix = torch.matmul(zs , zt.permute(0, 2, 1)) # n×T×T\n30 sim idx = sim matrix.max(dim=-1)[1].unsqueeze(2) # n ×T×1\n31 pt idxed = torch.gather(pt, 1, sim idx.expand(-1, -1, pt.size(2)))\n32 return - (pt idxed * log(ps)).sum(dim=-1).mean()\n15\nPublished as a conference paper at ICLR 2022\nA.2 N ETWORK ARCHITECTURE CONFIGURATIONS AND IMPLEMENTATION DETAILS\nInspired by great successes of the multi-stage ConvNet architecture such as VGG (Simonyan &\nZisserman, 2014)/ResNets (He et al., 2016) for computer vision, the multi-stage Transformer-based\nnetworks have been explored very recently in the supervised learning setting (Vaswani et al., 2021;\nWang et al., 2021; Liu et al., 2021; Zhang et al., 2021; Wu et al., 2021). In multi-stage vision\nTransformers, since a larger number of patches is often produced at the early stages, an efﬁcient\nTransformer with sparse self-attentions is considered to reduce the computational complexity. The\nbasic idea is to split the feature maps into non-overlapping local windows (with size W×W), and\nself-attention is performed within each local window. This however has one drawback that features\nin different local windows cannot interact. Various methods have been proposed to best approximate\nfull-attention, with different trade-off between accuracy and efﬁciency.\nWe brieﬂy describe three schemes as follows, and benchmark them in the experiments. (i) Swin\nTransformer (Liu et al., 2021): A shifted window partitioning approach is proposed, which alternates\nbetween two partitioning conﬁgurations in consecutive Transformer blocks, so that each local feature\nis grouped into different windows in self-attentions. (ii) Vision Longformer (ViL)(Zhang et al., 2021):\nFeatures in each local window are further allowed to attend all features in the 8-neighboring windows.\n(iii) Convolution vision Transformer (CvT) (Wu et al., 2021): Features in neighboring windows are\nconsidered in the convolutional projection in self-attentions.\nThe window size is set to W = 7 by default. The query dimension of each head in self-attentions is\nd= 32, and the hidden layer width of each MLP is 4×of its input’s width, for all experiments. The\narchitecture conﬁgurations of model variants employed in the experiments are summarized in Table 6.\nSome notable implementation detailed are described as follows:\n• The three conﬁgurations Swin-T, Swin-S and Swin-B indicate Tiny, Small, and Base models,\nrespectively, which are almost identical to the original implementation (Liu et al., 2021), except\nthat we add special treatments to deal with input augmented views of different resolutions, when\nthe resolution (feature map size more speciﬁcally) is not divisible by the window size ( i.e.,\nresolution 96 and window size=7 or 14).\n• Swin-T and Swin-S with window size W=14 are customized by us to allow full self-attention\nin stage 3 (where the majority of model capacity is allocated to) and stage 4, to study the impact\nof longer sequences in EsViT.\n• In the original ViL (Zhang et al., 2021) and CvT (Wu et al., 2021) papers, different positional\nembedding strategies and multi-stage network conﬁgurations were employed. We modify them\nby only utilizing relative position bias and their proposed sparse self-attention mechanisms, and\ncreate a similar 4-stage architecture with Swin-T for fair comparison.\nRelative Position Bias. To facilitate SSL, we consider relative position bias (Liu et al., 2021) to\ncharacterize the spatial information between features for the three efﬁcient Transformers aforemen-\ntioned, and do not use absolute position embeddings. This is because augmented views of varied\nresolutions can be cropped from anywhere in an image in SSL, maintaining the relative positions is\neasy in implementation, and is largely sufﬁcient for invariance learning among these views.\nB R ELATED WORK\nSelf-supervised ConvNets. ConvNets-based SSL has been extensively studied in the literature.\nBased on the pre-training tasks, they can be broadly categorized into three classes: Handcrafted\npretext tasks (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016; Gidaris et al., 2018;\nZhang et al., 2016; Larsson et al., 2016; Zhang et al., 2017; Pu et al., 2016; Donahue & Simonyan,\n2019), contrastive learning (Dosovitskiy et al., 2015; Zhuang et al., 2019; Oord et al., 2018; Hjelm\net al., 2018; Bachman et al., 2019; He et al., 2020; Chen et al., 2020c; Grill et al., 2020) and prototype\nlearning (Caron et al., 2018; 2020; Li et al., 2020b; Xie et al., 2016; Yang et al., 2016; Ji et al., 2019;\nZhan et al., 2020). It is also known that data augmentations play a crucial role in SSL pipeline (Chen\net al., 2020e; Caron et al., 2020; Tian et al., 2020; Li et al., 2020a). The impact of pre-training dataset\nsize/quality is explored for ConvNets in SSL (Goyal et al., 2021; Yonglong et al., 2021). To date,\nthe search of best pre-taining tasks/datasets and augmentations are based on CNNs. Among them,\n16\nPublished as a conference paper at ICLR 2022\nTransformer\nTransformer\n[CLS]MLPHeadCross-ViewPrediction\nRepeatedTop-layerfeaturemaps\nInputaugmentedviews\nView2View1\nTransformer(SparseS.A.)\n…PatchMerging\n…\nTransformer(SparseS.A.)PatchMerging\nPoolMLPHead\nInputaugmentedviews\nRepeated\nCross-ViewPrediction\nTop-layerfeaturemaps\nView2View1\n(a) Baseline monolithic architecture (b) Multi-stage architecture\nFigure 7: Architecture comparison. (a) The monolithic transformer. For all layers, the transformer\nblocks share the same network conﬁgurations and input token sequence sizes are the same. (b) The\nmulti-stage Transformer organizes an input image into a long sequence of smaller patches, sparse\nself-attentions (S.A.) are utilized at early stages to maintain model expressiveness while reducing\ncomputational complexity; The neighboring tokens at an intermediate layer are gradually merged,\nconstituting a short sequence to ease the compute burden of self-attention at late stages.\nStage 1 Stage 2 Stage 3 Stage 4\nMerging Rate 4× 8× 16× 32×Feature Map 56×56 28×28 14×14 7×7\nSwin-T,W=7 concat4×4,96-d, LN concat2×2,192-d, LN concat2×2,384-d, LN concat2×2,768-d, LN[ window size: 7×796-d(3heads)\n]\n×2\n[ window size: 7×7192-d(6heads)\n]\n×2\n[ window size: 7×7384-d(12heads)\n]\n×6\n[ window size: 7×7768-d(24heads)\n]\n×2\nSwin-S,W=7 concat4×4,96-d, LN concat2×2,192-d, LN concat2×2,384-d, LN concat2×2,768-d, LN[ window size: 7×796-d(3heads)\n]\n×2\n[ window size: 7×7192-d(6heads)\n]\n×2\n[ window size: 7×7384-d(12heads)\n]\n×18\n[ window size: 7×7768-d(24heads)\n]\n×2\nSwin-B,W=7 concat4×4,128-d, LN concat2×2,256-d, LN concat2×2,512-d, LN concat2×2,1024-d, LN[ window size: 7×7128-d(4heads)\n]\n×2\n[ window size: 7×7256-d(8heads)\n]\n×2\n[ window size: 7×7512-d(16heads)\n]\n×18\n[ window size: 7×71024-d(32heads)\n]\n×2\nSwin-T,W=14 concat4×4,96-d, LN concat2×2,192-d, LN concat2×2,384-d, LN concat2×2,768-d, LN[ window size: 14×1496-d(3heads)\n]\n×2\n[ window size: 14×14192-d(6heads)\n]\n×2\n[ window size: 14×14384-d(12heads)\n]\n×6\n[ window size: 7×7768-d(24heads)\n]\n×2\nSwin-S,W=14 concat4×4,96-d, LN concat2×2,192-d, LN concat2×2,384-d, LN concat2×2,768-d, LN[ window size: 14×1496-d(3heads)\n]\n×2\n[ window size: 14×14192-d(6heads)\n]\n×2\n[ window size: 14×14384-d(12heads)\n]\n×18\n[ window size: 7×7768-d(24heads)\n]\n×2\nViL-T,W=7 concat4×4,96-d, LN concat2×2,192-d, LN concat2×2,384-d, LN concat2×2,768-d, LN[ window size: 7×796-d(3heads)\n]\n×2\n[ window size: 7×7192-d(3heads)\n]\n×2\n[ window size: 7×7384-d(6heads)\n]\n×6\n[ window size: 7×7768-d(12heads)\n]\n×2\nCvT-T,W=7 concat4×4,64-d, LN concat2×2,192-d, LN concat2×2,384-d, LN concat2×2,768-d, LN[ window size: 7×764-d(1head)\n]\n×2\n[ window size: 7×7192-d(3heads)\n]\n×2\n[ window size: 7×7384-d(6heads)\n]\n×6\n[ window size: 7×7768-d(12heads)\n]\n×2\nTable 6: Model conﬁgurations considered in our experiments.\nSimCLR-v2 (Chen et al., 2020d), BYOL (Grill et al., 2020) and SwA V (Caron et al., 2020) achieve\nthe highest ImageNet linear probe performance with large ConvNet architectures. The performance\ntends to saturate with an increasingly growing model size, raising a question if ConvNets reach a\nlimit in SSL.\nTransformers for vision. Vision Transformers (ViT) (Dosovitskiy et al., 2021) shows the great\npotentials of generalizing Transformers for computer vision, by achieving compelling accuracy in\nsupervised learning, especially with large-scale data and high capacity models. DeiT (Touvron et al.,\n2020) further provides an effective ViT training strategy to ease the adaption of Transformers for\npractitioners. Transformers have also been applied to other vision tasks, ranging from low-level\ntasks such as image generation (Parmar et al., 2018; Chen et al., 2020b) and enhancement (Chen\net al., 2020a; Yang et al., 2020), to high-level tasks such as object detection (Carion et al., 2020;\nZhu et al., 2020; Zheng et al., 2020; Dai et al., 2020) and segmentation (Wang et al., 2020a;c), and\nto vision-language tasks (Lu et al., 2019; Tan & Bansal, 2019; Chen et al., 2019; Su et al., 2019;\n17\nPublished as a conference paper at ICLR 2022\nName Framework Pre-train Task Description Major Motivation Major Downstream Task\nPerformance\nImageNet\nPIRL Contrastive Jigsaw pretext task in a way\nthat encourages the image\nrepresentations to be invari-\nant to the image patch pertur-\nbation\nGeneral-purpose visual\nbackbone learning\nMostly for image\nclassiﬁcation.\nImage classiﬁcation 63.6%\nDenseCL Contrastive A pairwise contrastive\n(dis)similarity loss at the\npatch level between two\nviews. A queue of negative\nsample features from other\nimages is maintained\nImproving dense visual\nprediction task perfor-\nmance\nImproving object detection\nperformance.\n63.6%\nDetCo Contrastive Multi-level features with\nthree contrastive tasks be-\ntween global images and\nlocal patches are considered:\nglobal-global, global-local,\nlocal-local.\nImproving dense visual\nprediction task perfor-\nmance\nImproving object detec-\ntion performance. DetCo\nachieves the best perfor-\nmance trade-off on both\nclassiﬁcation and detection.\n68.6%\nPixPro Contrastive Features from the two views\nare encouraged to be con-\nsistent between a regular\npatch representation and a\nsmoothed patch representa-\ntion within the same image.\nImproving dense visual\nprediction task perfor-\nmance\nMostly focusing on the im-\nproved performance on de-\ntection tasks.\n66.3%\nInstLoc Contrastive image instances are pasted\nat various locations and\nscales onto background im-\nages. The pretext task is\nto predict the instance cat-\negory given the composited\nimages and the foreground\nbounding boxes\nImproving dense visual\nprediction task perfor-\nmance\nMostly focusing on the im-\nproved performance on de-\ntection tasks.\n61.7%\nEsViT (ours) Non-\ncontrastive\nA pairwise cross-entropy\nloss at the patch level be-\ntween two positive views.\nNo need/interaction with\nother images in the batch\n(eg, no negative samples)\nRecovering the auto-\nmatic correspondence\nlearning property of\nself-supervised mono-\nlithic transformers, and\nthus improving learning\nefﬁciency.\nConsistently improving\nimage classiﬁcation tasks.\nIt creates new SoTA 81.3%\non ImageNet linear probe\naccuracy, showing 3.5x\nparameter-efﬁcient and has\n10x higher throughput than\nprevious SoTA MoCo-v3.\nReporting 75.7% ImageNet\nlinear probe performance\nfor ResNet-50.\n69.9%\nTable 7: Discussion of related works on various region-level tasks. The last columns reports the\nImageNet linear probe performance for ResNet-50 trained with 2 augmented views for 200 epochs.\nLi et al., 2019b;a; Zhou et al., 2020; Li et al., 2020c). Marrying Transformers with multi-stage\narchitectures (Vaswani et al., 2021; Wang et al., 2021; Liu et al., 2021; Zhang et al., 2021; Wu\net al., 2021) show higher classiﬁcation accuracy in supervised learning, and enables applicability of\nTransformers for a broader range of vision tasks. Given these properties, we believe multi-stage ViT\nis a must-study baseline for SSL in computer vision.\nDiscussion with other region-level tasks.In Table 7, we compare LR against the existing region-\nlevel tasks, including PIRL (Misra & Maaten, 2020), DenseCL (Wang et al., 2020b), DetCo (Xie\net al., 2021a), InstLoc (Yang et al., 2021), PixPro (Xie et al., 2021c). Most of these region-level\ntasks improve object detection tasks, but hurt the ImageNet classiﬁcation accuracy. DetCo achieves\nthe best trade-off: improving the performance of both tasks, but with a sophisticated multi-level,\nglobal-local interaction algorithm. With the same number of pre-training epochs and augmented\nviews, EsViT achieve the best ImageNet linear probe accuracy among all region-level tasks, with as\nminimum computational overhead as possible. This well serves our goal of building efﬁcient SSL\nSoTA image classiﬁcation system.\n18\nPublished as a conference paper at ICLR 2022\nData source Table 2 in\nDINO (Caron\net al., 2021)\nTable 3 in MLP-Mixer (Tol-\nstikhin et al., 2021)\nTable 1 in Swin (Liu\net al., 2021)\nOur runs\nDeiT-S /P=16 1007 940.4\nDeiT-B /P=16 312 292.3\nDeiT-S /P=8 180\nDeiT-B /P=8 63\nViT-B /P=16 312 861\nViT-S /P=16 102 280\nViT-H /P=14 32 87\nViT-L /P=7 17 47†\nSwin-T /W=7 808 755.2 726.13\nSwin-S /W=7 467 436.9\nSwin-B /W=7 297 278.1\nSwin-T /W=14 660 593.24\nSwin-S /W=14 383 344.20\nSwin-B /W=14 254 228.36\nViL-T /W=7 386 346.72\nCvT-T /W=7 848 761.89\nTable 8: Throughput estimate and standardization. All numbers in orange are estimated/converted,\nwhile numbers in blue are collected from the papers, and numbers in green are runs on our machines.\nAll papers report the throughput of ViT-B or DeiT-B, which are essentially the same model. We\nuse this fact to align the throughput reported in different papers. †This number is estimated via the\nstatement in (Chen et al., 2021) that “reducing the patch size to7 ×7 keeps the model size unchanged,\nbut increases FLOPs to ∼6×”. All numbers are standardized into throughput reported by (Caron\net al., 2021).\nC E XPERIMENTS\nC.1 T HROUGHPUT ESTIMATE AND CONVERSION\nSince different papers report throughput on different hardwares, it is not ready to compare the numbers\ndirectly. Noting that all papers report the throughput for ViT-B/DeiT-B, we use this number to align\nand convert the throughput. In Table 8, we describe our process and results of standardizing the\nthroughput.\nC.2 T HE COMPUTATION AND MEMORY OVERHEAD OF THE PROPOSED LR\nWe emphasize that adding LR to the multi-stage transformer architectures yields acceptable extra\ncomputational cost, while adding LR directly to the monolithic transformer architectures has a\nhuge computational overhead. To demonstrate this, we report the cost comparisons in Table 9. For\neach setting, we report [Memory Usage (MB) / Running time per iteration (second/iteration)]. In\nTable Table 9 (a), when the batch size is gradually increased ( e.g., , batch-size=12), the memory\ncost increases nearly 4 times for monolithic architectures, while increases 1.6 times for multi-stage\narchitectures. Similar trends are shown for training cost per iteration increase ratio (1.47 vs 1.15).\nThis indicates LR can more naturally ﬁt multi-stage architectures.\nSimilarly, we compare computational cost comparisons [Memory Usage (MB) / Running time per\niteration (second/iteration)] in Table 9 (b), for other network architecture conﬁgurations . From the\nincreased cost ratio, we see that LR adds acceptable cost in terms of both memory and training time,\ncompared with the baseline.\nC.3 E XPERIMENTAL SETTINGS OF PRE -TRAINING AND EVALUATION ON IMAGE NET\nWe study unsupervised pre-training performed in ImageNet-1K dataset (Deng et al., 2009) without\nlabels. The default training details are described as follows, mostly following (Caron et al., 2021).\nWe train with the Adamw optimizer (Loshchilov & Hutter, 2018), a batch size of 512, and total\nepochs 300. Linear warmup of the learning rate is used during the ﬁrst 10 epochs, with its base value\n19\nPublished as a conference paper at ICLR 2022\nBatch-Size=1 Batch-Size=8 Batch-Size=12\nMonolithic ViT-S LV 1391 / 0.209036 2735 / 0.234993 3533 / 0.238160\nViT-S LV +LR 2641 / 0.233150 10358 / 0.321155 14128 / 0.352339\nIncreased Cost Ratio 1.8986 / 1.1153 3.7872 / 1.3666 3.9988 / 1.4794\nMulti-stage Swin-T LV 1704 / 0.285451 4229 / 0.323884 5611 / 0.366367\nSwin-T LV +LR 2346 / 0.301672 6232 / 0.374634 8917 / 0.421135\nIncreased Cost Ratio 1.3767 / 1.0568 1.4736 / 1.2323 1.5889 / 1.1494\n(a) Comparisons for increased batch sizes.\nEsViT LV LV +LR Increased Cost Ratio\nTiny (W=7) 1704 / 0.285451 2346 / 0.301672 1.3767 / 1.0568\nSmall (W=7) 2685 / 0.501876 3132 / 0.535203 1.1664 / 1.0664\nBase (W=7) 3726 / 0.516058 4374 / 0.550617 1.1739 / 1.0669\nTiny (W=14) 2159 / 0.288118 2801 / 0.310108 1.0890 / 1.0763\nSmall (W=14) 3518 / 0.496823 4153 / 0.521739 1.1805 / 1.0501\nBase (W=14) 5032 / 0.511701 5681 / 0.537826 1.1289 / 1.0510\n(b) Comparisons for various network architecture conﬁgurations.\nTable 9: Computational cost comparisons in the format of [Memory Usage (MB) / Running time per\niteration (second/iteration)].\nDataset Classes Train size Test size Evaluation metric Source link\nFood-101 102 75,750 25,250 Accuracy Tensorﬂow\nCIFAR-10 10 50,000 10,000 Accuracy TensorFlow\nCIFAR-100 100 50,000 10,000 Accuracy TensorFlow\nSUN397 397 19,850 19,850 Accuracy Tensorﬂow\nStanford Cars 196 8,144 8,041 Accuracy Stanfold Cars\nFGVC Aircraft (variants) 100 6,667 3,333 Mean-per-class FGVC website\nVOC2007 classiﬁcation 20 5,011 4,952 11-point mAP voc2007\nDescribable Textures 47 3,760 1,880 Accuracy TensorFlow\nOxford-IIIT Pets 37 3,680 3,669 Mean-per-class Oxford-IIIT Pet\nCaltech-101 102 3,060 6084 Mean-per-class TensorFlow\nOxford Flowers 102 102 2,040 6,149 Mean-per-class TensorFlow\nMNIST 10 60,000 10,000 Accuracy TensorFlow\nFacial Emotion Recog. 2013∗ 8 32,298 3,589 Accuracy Kaggle fer2013\nSTL10 10 5,000 8,000 Accuracy TensorFlow\nGTSRB∗ 43 26,728 12,630 Accuracy GTSRB website\nPatchCamelyon 2 294,912 32,768 Accuracy TensorFlow\nUCF101∗ 101 9,537 3783 Accuracy TensorFlow\nHateful Memes 2 8,500 500 ROC-AUC FaceBook\nTable 10: A suite of 18 datasets used in linear probe. ∗indicates dataset whose train/test size we\nobtained is slightly different from Table 9 in (Radford et al., 2021).\ndetermined with the linear scaling rule (Goyal et al., 2017): lr = 0.0005 ∗batchsize/256. After\nthis warmup, the learning rate is decayed with a cosine schedule. We build our systems based on\nSwin Transformers (Liu et al., 2021) in our experiments. Swin-B has a model size and computation\ncomplexity similar to ViT-B/DeiT-B (patch size 16). We also considered Swin-T and Swin-S, which\nhave the complexity that are similar to those of ResNet-50 (DeiT-S) and ResNet-101, respectively.\nThe default window size is W=7.\nOne major common protocol to evaluate SSL is linear probe on ImageNet-1K, where features are\nextracted from a frozen backbone, and a supervised linear classiﬁer is trained. For all Transformer\nmodels, we use the concatenation of view-level features ¯zin the last 4 layers (the results are similar\nto the use of last 3 or 5 layers in our initial experiments).\n20\nPublished as a conference paper at ICLR 2022\nMethods CLIP Supervised Supervised ‡ Supervised EsViT\nResNet-50 ResNet-50 ResNet-50 Swin-T Swin-T\nFood-101 86.4 71.3 71.3 77.4 80.0\nCIFAR-10 88.7 91.8 91.8 94.0 95.3\nCIFAR-100 70.3 74.5 74.5 77.5 82.2\nSUN397 73.3 60.5 60.3 64.3 67.6\nStanford Cars 78.3 49.9 50.1 55.3 66.4\nFGVC Aircraft (variants) 49.1 48.5 48.4 51.5 61.1\nVOC2007 classiﬁcation 87.1 83.8 83.6 84.2 85.5\nDescribable Textures 76.4 72.3 72.6 73.1 78.1\nOxford-IIIT Pets 88.2 92.4 92.1 93.3 92.8\nCaltech-101 89.6 90.8 90.4 90.8 93.0\nOxford Flowers 102 96.1 90.8 91.1 91.5 97.4\nMNIST 98.3 98.3 98.3 98.3 98.3\nFacial Emotion Recog. 2013 64.2 54.9 55.9 55.1 59.3\nSTL10 97.2 96.4 97.0 97.9 98.9\nGTSRB 82.4 70.6 75.7 72.9 84.3\nPatchCamelyon 82.7 82.5 82.6 84.0 84.6\nUCF101 81.6 71.2 72.1 79.0 81.1\nHateful Memes 65.7 56.5 49.9 51.2 52.0\nAverage 80.86 75.39 75.43 77.29 80.99\nTable 11: The linear probe results on 18 datasets at the scale of ResNet-50/Swin-T.‡indicates the\nresults reproduced by us, which veriﬁes that our implementation pipeline is consistent with (Radford\net al., 2021).\nMethod View-level Region-level Top-1 Accuracy (%)\nPerformance comparison of ResNet-50 with 200 epochs and 2 augmented views\nMoCo-v2 Contrastive - 67.5\nDenseCL Contrastive Contrastive 63.6\nDetCo Contrastive Contrastive 68.6\nDINO Non-Contrastive - 69.2\nEsViT Non-Contrastive Non-Contrastive 69.9\nSoTA performance comparison of ResNet-50 with numbers and settings reported in each paper\nMoCo-v2 (800 epochs) Contrastive - 72.2\nSwA V (800 epochs, w/ multi-crop)Contrastive - 75.3\nBarlow Twins (1000 epochs) - - 73.2\nVICReg (1000 epochs) - - 73.2\nSimSiam (800 epochs, 2 views) Non-Contrastive - 71.3\nBYOL (1000 epochs, w/ multi-crop)Non-Contrastive - 74.3\nDINO (300 epochs, w/ multi-crop) Non-Contrastive - 75.0\nEsViT (300 epochs, w/ multi-crop) Non-Contrastive Non-Contrastive 75.7\nTable 12: Linear probe performance of a ResNet-50 network with different SSL methods.\nC.4 C OMPARISON WITH A RESNET-50 BACKBONE\nTo compare our EsViT learning method with other SSL algorithms, we conduct experiments with a\nResNet-50 backbone, and show the results in Table 12.\nC.5 L INEAR PROBE ON A SUITE OF SMALL DATASETS\nDatasets. Table 10 shows details and source of all datasets used for linear probe, including the\nnumber of classes, the size of training set and testing set, metrics used in evaluation, as well as a public\nsource of the dataset. Note that original UCF101 dataset is a video dataset. Here the middle frame of\neach video is extracted to form a classiﬁcation dataset. There are 3 train/val splits in Tensorﬂow, we\nuse the ﬁrst one.\n21\nPublished as a conference paper at ICLR 2022\nAutomatic hyper-parameter tuning. We rigorously follow (Radford et al., 2021) to conduct\ntraining and evaluation for linear probe on the downstream datasets. We train a logistic regression\nclassiﬁer using scikit-learn’s L-BFGS implementation, with maximum1,000 iterations, and report\nthe corresponding metric for each dataset. We determine the L2 regularization strength λusing\na hyperparameter sweep on the validation sets over the range between 10−6 and 106 , with 96\nlogarithmically spaced steps. To save compute required for the sweeps, we perform a parametric\nbinary search that starts with λ = [10 −6,10−4,10−2,1,102,104,106] and iteratively halves the\ninterval around the peak until it reaches a resolution of 8 steps per decade. The hyperparameter\nsweeps are performed on a validation split of each dataset. For the datasets that contain a validation\nsplit in addition to a test split, we use the provided validation set to perform the hyperparameter\nsearch, and for the datasets that do not provide a validation split or have not published labels for the\ntest data, we split the training dataset to perform the hyperparameter search. For the ﬁnal result, we\ncombine the validation split back with the training split and report the performance on the unused\nsplit.\nDetailed results. Only the last layer feature is considered for all models for simplicity, though\nadding features from more layers may potentially improve the results. Table 11 shows the results\nfor architectures at a similar scale of ResNet-50 or Swin-T. The ﬁrst two columns are numbers\nfrom (Radford et al., 2021). CLIP with ResNet-50 is pre-trained on 400 million image-text pairs.\nSupervised ResNet-50 and Swin-T are pre-trained on ImageNet-1K, on which EsViT with Swin-T is\npre-trained as well (Batch Size=512). EsViT outperforms its supervised counterpart, and is on par\nwith the performance of CLIP in a similar image encoder architecture scale.\nC.6 P RE-TRAINING DATASETS\nWe describe the statistics and training schedule on larger and less curated datasets in Table 13. The\npre-training epochs are chosen so that the model is trained with a similar number of augmented views.\nName Description Size (#Images) Epochs Warmup\nImageNet-1K (Deng et al., 2009) Images evenly distributed in 1K object concepts1.2million 300 10WebVision-v1 (Li et al., 2017)Web images with 1K concept queries from ImageNet-1K2.4million 150 5OpenImages-v4 (Kuznetsova et al., 2020)Diverse/complex scenes with several objects for detection7.5million 50 2ImageNet-22K (Deng et al., 2009) Images distributed in 22K object concepts in a hierarchy14.2million 30 1\nTable 13: Pre-train dataset statistics and training schedule.\nC.7 R ESULTS ON CORRESPONDENCE LEARNING\nWe ﬁrst quantitatively evaluate the correspondence learning results with 50K images in the ImageNet\nvalidation dataset. We create a simple evaluation dataset with mild augmentations. For a center-crop\nimage, we apply HorizontalFlip, then ColorJitter and RandomGrayscale to create\na new augmented view. In this way, ground-truth correspondences are created. Please see the 1st\nrow of Figure 9 for one such example. The top-10 correspondences are used for evaluation. Two\nmetrics are considered: (1) Accuracy measures the percentage of correctly matched region pairs,\n(2) distance error indicates the averaged ℓ2 distance between the predicted matched region and\nground-truth region (the value is 0 for perfect matching). The results are reported in Figure 8. DINO\nwith monolithic Transformers shows surprisingly good performance on correspondence learning.\nThe use of multi-stage Transformer architecture reduces this ability, shows a lack of good region\ncorrespondence. With LR, the region matching ability is signiﬁcantly recovered.\nIn Figure 9, we visualize the correspondences for more images. Overall, DINO with monolithic\nTransformers is able to discover most salient correspondences of semantic meaning in the mild\naugmentation conditions, even without an implicit region matching loss in training. We believe this\npreviously underestimated property is whole-noting, and has potentials to enable more applications.\nHowever, this desired property gets dilated when changing from monolithic to multi-stage Transformer\narchitecture (from column 1 to column 2), then the proposed region level task can alleviate this issue\n(from column 2 to column 3).\nTo more speciﬁcally analyze the correspondences, we note the following results. The ﬁrst row shows\na simple case, where only images of left-to-right ﬂipped views are presented. The ground-truth\n22\nPublished as a conference paper at ICLR 2022\n(a) Accuracy (b) Distance error\nFigure 8: Quantitative evaluation on correspondence learning on ImageNet validation set. LR can\nsigniﬁcantly improve correspondence learning quality for multi-stage architectures. As a reference,\nDINO (LV with monolithic Transformer architecture) achieves 0.95 accuracy and 2.49 distance error,\nwhich we believe is a strong evidence to identify the intriguing property of automatic correspondence\nlearning.\ncorrespondences should be horizontal lines that link the two ﬂipped regions. It reveals that the\nview-level pre-train task alone is insufﬁcient to learn good correspondences for the multi-stage\nTransformer architecture, while region matching task can alleviate this issue signiﬁcantly. Similar\nobservations are shown in row 3 and row 4.\nWe further study more cases that requires real-world correspondences in row 2, row 5 and row\n6. These views are not generated with data augmentation (as in model pre-training), but are often\npresented in more practical scenarios: one-to-many mappings, cartoon-to-toy, seasonal changing of\nthe scene, respectively. The proposed region matching task can work particularly well in those cases.\nC.8 M ORE VISUALIZATION RESULTS OF ATTENTION MAPS\nWe visualize attention maps at the top layer in Figure 10, 11, 12. With a monolithic Transformer\narchitecture, DINO can automatically identify the main foreground objects. Unfortunately, changing\nfrom monolithic to the multi-stage Transformer architecture (From left column to middle column), this\nproperty gets lost. There are more heads in the multi-stage architecture than monolithic architecture\n(24 heads vs 6 heads in this case) in the last year. A fair number of heads in EsViT shows redundant\npatterns, this issue can be reduced when the region-level matching task is added (From middle column\nto right column).\nWe observed that DINO with monolithic Transformer architecture only learns to attend the fore-\nground objects, even when the query is a background region (see Figure 12). This is perhaps because\nDINO models are trained to learn view-level invariance, the main objects in the pre-train dataset\nImageNet tend to be the principle factor that remains invariant across different augmented views.\nHence, all backgrounds are ignored, regardless of the query positions. This is improved in EsViT\nwith the region-level pre-train task, as the model is trained to match individual regions.\nDINO shows high entropy values in all of 6 heads (perhaps a required condition to cover all regions\nof the main object). In EsViT, LR plays an interesting role in modulating the entropy distributions\namong heads: it increases those with larger entropy values, while decreasing those with lower entropy\nvalues. In another word, it makes the attention patterns in different heads more diverse.\n23\nPublished as a conference paper at ICLR 2022\n(a) DINO: DeiT-S (b) EsViT: LV (c) EsViT: LV +LR\nFigure 9: The learned correspondences. Yellowlines are the top-10 correspondences between two\nviews, where the numbers indicates the rankings of similarity scores, yellow dots with the same\nnumber are paired. The blue dot and red triangle indicates the most similar local regions that\ncorrespond to the global feature of the view itself and the other view, respectively. Please zoom in for\ndetailed correspondence mappings.\n24\nPublished as a conference paper at ICLR 2022\n(a) DINO: DeiT-S (b) EsViT: LV (c) EsViT: LV +LR\nFigure 10: The learned attention maps for all heads at the top layer, ranked by the entropy of softmax\nprobability. Query is the blue dot in the top-left of the image. Top: Entropy of each heads. Middle:\ntop 60% probability mass. Bottom: full attention maps. LR shows more attention patterns than LV\nonly.\n25\nPublished as a conference paper at ICLR 2022\n(a) DINO: DeiT-S (b) EsViT: LV (c) EsViT: LV +LR\nFigure 11: The learned attention maps for all heads at the top layer, ranked by the entropy of softmax\nprobability. Query is the blue dot in the center of the image. Top: Entropy of each heads. Middle: top\n60% probability mass. Bottom: full attention maps. LR shows more attention patterns than LV only.\n26\nPublished as a conference paper at ICLR 2022\n(a) DINO: DeiT-S (b) EsViT: LV (c) EsViT: LV +LR\nFigure 12: The learned attention maps for all heads at the top layer, ranked by the entropy of softmax\nprobability. Query is the blue dot in the top-left of the image. Top: Entropy of each heads. Middle:\ntop 60% probability mass. Bottom: full attention maps. DINO mainly attends the main object even\nwhen the query is a background region.\n27",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8294257521629333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6217360496520996
    },
    {
      "name": "Transformer",
      "score": 0.6131983399391174
    },
    {
      "name": "Machine learning",
      "score": 0.5592494010925293
    },
    {
      "name": "Feature learning",
      "score": 0.4890769124031067
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4741073250770569
    },
    {
      "name": "Representation (politics)",
      "score": 0.46352559328079224
    },
    {
      "name": "Code (set theory)",
      "score": 0.4390139579772949
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3253118395805359
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}