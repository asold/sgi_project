{
  "title": "Entity-aware and Motion-aware Transformers for Language-driven Action Localization",
  "url": "https://openalex.org/W4224993577",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101633714",
      "name": "Shuo Yang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104466700",
      "name": "Xinxiao Wu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6757188556",
    "https://openalex.org/W3194532355",
    "https://openalex.org/W2611788449",
    "https://openalex.org/W2931886155",
    "https://openalex.org/W2939519298",
    "https://openalex.org/W2742343242",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W3176201273",
    "https://openalex.org/W3047516682",
    "https://openalex.org/W3138878737",
    "https://openalex.org/W6864487941",
    "https://openalex.org/W2970401629",
    "https://openalex.org/W3174490084",
    "https://openalex.org/W3124671614",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3046947115",
    "https://openalex.org/W2111078031",
    "https://openalex.org/W1942126453",
    "https://openalex.org/W2337252826",
    "https://openalex.org/W3088744711",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W3176763654",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W2998712570",
    "https://openalex.org/W3139032910",
    "https://openalex.org/W2797724514",
    "https://openalex.org/W2970898753",
    "https://openalex.org/W3016237555",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W2991773160",
    "https://openalex.org/W6755811877",
    "https://openalex.org/W3178087530",
    "https://openalex.org/W3144612699",
    "https://openalex.org/W3175402857",
    "https://openalex.org/W3175082063",
    "https://openalex.org/W2964089981",
    "https://openalex.org/W2952686080",
    "https://openalex.org/W4214931087",
    "https://openalex.org/W3092739351",
    "https://openalex.org/W3174364033",
    "https://openalex.org/W3180476551",
    "https://openalex.org/W2963017553",
    "https://openalex.org/W2904824998",
    "https://openalex.org/W3093174808",
    "https://openalex.org/W2798354744",
    "https://openalex.org/W3135773387",
    "https://openalex.org/W3035640828",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3129089995",
    "https://openalex.org/W2963393391",
    "https://openalex.org/W3175817778",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3132753749",
    "https://openalex.org/W2997429269"
  ],
  "abstract": "Language-driven action localization in videos is a challenging task that involves not only visual-linguistic matching but also action boundary prediction. Recent progress has been achieved through aligning language queries to video segments, but estimating precise boundaries is still under-explored. In this paper, we propose entity-aware and motion-aware Transformers that progressively localize actions in videos by first coarsely locating clips with entity queries and then finely predicting exact boundaries in a shrunken temporal region with motion queries. The entity-aware Transformer incorporates the textual entities into visual representation learning via cross-modal and cross-frame attentions to facilitate attending action-related video clips. The motion-aware Transformer captures fine-grained motion changes at multiple temporal scales via integrating long short-term memory into the self-attention module to further improve the precision of action boundary prediction. Extensive experiments on the Charades-STA and TACoS datasets demonstrate that our method achieves better performance than existing methods.",
  "full_text": "Entity-aware and Motion-aware Transformers for Language-driven Action\nLocalization\nShuo Yang, Xinxiao Wu‚àó\nBeijing Laboratory of Intelligent Information Technology,\nSchool of Computer Science, Beijing Institute of Technology\n{shuoyang,wuxinxiao}@bit.edu.cn\nAbstract\nLanguage-driven action localization in videos is\na challenging task that involves not only visual-\nlinguistic matching but also action boundary pre-\ndiction. Recent progress has been achieved through\naligning language query to video segments, but es-\ntimating precise boundaries is still under-explored.\nIn this paper, we propose entity-aware and motion-\naware Transformers that progressively localizes ac-\ntions in videos by first coarsely locating clips\nwith entity queries and then finely predicting ex-\nact boundaries in a shrunken temporal region with\nmotion queries. The entity-aware Transformer in-\ncorporates the textual entities into visual represen-\ntation learning via cross-modal and cross-frame at-\ntentions to facilitate attending action-related video\nclips. The motion-aware Transformer captures\nfine-grained motion changes at multiple temporal\nscales via integrating long short-term memory into\nthe self-attention module to further improve the\nprecision of action boundary prediction. Exten-\nsive experiments on the Charades-STA and TACoS\ndatasets demonstrate that our method achieves bet-\nter performance than existing methods.\n1 Introduction\nLanguage-driven action localization, also called temporal\nvideo grounding or video moment retrieval, aims to localize\nthe start and end frames of an action relevant to the language\nquery. It has attracted growing attention for its wide appli-\ncations, such as robotic navigation and video understanding.\nThis task is challenging since it requires not only aligning\nthe language query to video segments but also estimating the\ntemporal boundaries of the desired action.\nTremendous effects have been devoted to the alignment\nbetween language query and video segments. Several early\nstudies [Hendricks et al., 2017 ] resort to learning a com-\nmon visual-textual embedding space by pushing dissimi-\nlar or pulling similar visual features and linguistic features.\nLater, in order to explore more detailed semantics for visual-\ntextual alignment, some methods [Chen and Jiang, 2019] ex-\n‚àócorresponding author\nLanguage query: a person is watching the mirror\nEntities:  a person is watching the mirror\nMotion: a person is watching the mirror \nGround Truth \nFigure 1: Illustration of coarse-to-fine action localization using en-\ntity and motion queries.\ntract semantic concepts of actions or objects to enrich the\nholistic features of both video and language. In more re-\ncent years, various attention operations have been proposed\nto learn elaborate cross-modal relations, such as self-modal\nor cross-modal graph attention [Liu et al., 2020], context-\nquery attention [Zhang et al., 2021a] and local-global inter-\naction [Mun et al., 2020]. All these methods mainly focus on\nlearning and aligning the visual and linguistic representations\nfor language-driven action localization without considering\nthe explicit modeling of finer action boundaries for precise\nlocalization.\nThis paper investigates a coarse-to-fine strategy to progres-\nsively estimate the action boundaries in untrimmed videos\nwith high precision. 1 With this in mind, we propose entity-\naware and motion-aware Transformers that first coarsely lo-\ncate video clips from the entire video with textual entities\nand then finely predict exact boundaries in a shrunken tem-\nporal region with motion queries. For example, as illustrated\nin Figure 1, the query sentence of ‚Äúa person is watching the\nmirror‚Äù can be divided into two types of information: the en-\ntities of ‚Äúperson & mirror‚Äù and the motion of ‚Äúis watching‚Äù.\nOur method first finds the frames in which the ‚Äúperson & mir-\nror‚Äù appear, and then localizes the start and end boundaries\nbetween which the ‚Äúis watching‚Äù happens.\nTo be more specific, the entity-aware Transformer incorpo-\nrates textual entities of language query into visual represen-\ntation learning via cross-modal attention. The learned visual\nfeatures are capable of attending to the salient action-related\nobjects so as to facilitate selecting action-related video clips.\nMoreover, cross-frame attention is employed to leverage con-\n1Code is available at https://github.com/shuoyang129/eamat\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1552\n‚Äúa person is watching \nthe mirror‚Äù\n Transformer\nmotion query\n‚Äú a person is watching the mirror‚Äù\n3D-CNN\n LSTM\nTransformer\nCross-frame\nTransformer\nCross-modal\nFusion\nCross-modal\nFusion\nLSTM\nTransformer\nCross-frame\nTransformer\n Prediction\nPrediction\nœÑùë†   œÑùëí\n‚Äúa person is watching \nthe mirror‚Äù\nAction boundaries\nEntity-aware Transformer\nMotion-aware Transformer\nEntities-relevant Score\nGloVe\n PoS Tagging\n‚Äúa person is watching \nthe mirror‚Äù\nTransformer\n‚Äúa person is watching \nthe mirror‚Äù\nGloVe\n PoS Tagging\n‚Äúa person is watching the mirror‚Äù\nentity query\nFigure 2: Overview of our entity-aware and motion-aware Transformers for language-driven action Localization.\ntextual information from adjacent frames to learn more ro-\nbust entity features. Starting with more accessible entities, the\nentity-aware Transformer narrows the searching space for ac-\ntion localization by coarsely locating temporal regions where\nthe desired action is more likely to happen.\nAn action consists of sequential motions and large mo-\ntion changes usually lie on the action boundaries. To im-\nprove the action localization precision, it is significantly es-\nsential to capture the fine-grained motion changes in videos.\nSo we propose a motion-aware Transformer that integrates\na long short-term memory cell into the self-attention mod-\nule in Transformer. Intuitively, the long short-term memory\ncell is a natural way to capture the consecutive local mo-\ntion changes, and we apply it at multiple temporal scales to\ndeal with various durations of the same action. Transformer\nis capable of modeling the long-range dependency and has\nbeen proved its effectiveness in many visual and linguistic\ntasks [Vaswani et al., 2017], and it is reasonable to use it\nfor modeling the global motion interactions. Therefore, our\nmotion-aware Transformer can capture fine-grained motion\nchanges at multiple time granularities, which benefits a lot to\nlocalizing the exact boundaries of desired actions.\nThe main contributions of this paper are summarized as\nfollows: (1) We propose a coarse-to-fine framework for\nlanguage-driven action localization, which extracts detailed\nentity and motion queries to progressively estimate the ac-\ntion boundaries with high precision. (2) We propose entity-\naware and motion-aware Transformers as an effective imple-\nment of the coarse-to-fine localization, where the newly de-\nsigned motion-aware Transformer models fine-grained mo-\ntion changes at multiple temporal scales by integrating long\nshort-term memory into self-attention. (3) Extensive exper-\niments on popular benchmarks, Charades-STA and TACoS,\ndemonstrate that the proposed method performs favorably\nagainst existing methods.\n2 Related Work\nThe language-driven action localization task is firstly pro-\nposed in [Gao et al., 2017; Hendricks et al., 2017]. It is\ntackled by first generating proposals with manually designed\ntemporal bounding boxes and then ranking the proposals by\nthe given language query. To enhance the visual and linguistic\nrepresentations, ACRN [Liu et al., 2018] proposes a mem-\nory attention mechanism to emphasize the language-related\nvisual features with context information. SCDM [Yuan et\nal., 2020] modulates the temporal convolution operations for\nbetter correlating and composing the sentence related video\ncontents. 2D-TAN [Zhang et al., 2020] uses a 2D temporal\nadjacent network to learn contextual and structural informa-\ntion between adjacent moment candidates. MAST [Zhang et\nal., 2021c] aggregates multi-stage features to represent mo-\nment proposals using a BERT-variant Transformer backbone.\nThese proposal-based methods are relatively inefficient since\na large number of proposals causes redundant computation.\nMoreover, the boundaries of proposals are fixed, leading to\ninflexible estimations.\nTo mitigate the defects of manually designed proposals,\nproposal-free methods [Zeng et al., 2020; Yuanet al., 2019;\nHahn et al., 2019; Lu et al., 2019; Wu et al., 2020; Li et\nal., 2021; Zhao et al., 2021] are proposed to directly pre-\ndict the action boundaries through visual and linguistic rep-\nresentation alignment. ExCL [Ghosh et al., 2019] and Se-\nqPAN [Zhang et al., 2021b] predict the start and end time\nby leveraging the cross-modal interaction between the text\nand video; LGI [Mun et al., 2020], CSMGAN [Liu et al.,\n2020], FIAN [Qu et al., 2020], CBLN [Liu et al., 2021],\nSMIN [Wang et al., 2021], I2N [Ning et al., 2021] explore\nthe local and global context information for accurate local-\nization.\nRather than mainly focusing on aligning the visual and lin-\nguistic representations in the aforementioned methods, we at-\ntempt to achieve high localization precision by designing a\nprogressive strategy that first narrows the target regions and\nthen localizes the finer boundaries. The most related work\nto our method is VSLNet [Zhang et al., 2021a] that searches\nfor the target action within a highlighted region, which ex-\ntends the target action segment by a simple hyper-parameter\nin a span-based question answering framework. In contrast,\nour method coarsely locates the temporal region first by the\napparent action-related entities, and then finely predicts the\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1553\naction boundaries by explicitly modeling fine-grained motion\nchanges at both short and long times, achieving high preci-\nsion, good generalization and interpretability.\n3 Our Method\nGiven an untrimmed video V = {vt}T\nt=1 and a language\nquery S = {wi}N\ni=1 where vt represents the t-th video frame,\nwi represents the i-th word, and T and N represent the num-\nber of video frames and text words, respectively, our task\naims to localize the target action boundaries (œÑs, œÑe) where\nœÑs and œÑe represent the start and end frames of the action\ncorresponding to the query, respectively. As shown in Fig-\nure 2, our method has two main components: an entity-aware\nTransformer and a motion-aware Transformer. The former\nincorporates the entity terms, i.e., subjects and objects, of the\nlanguage query into the visual representation learning to filter\nout the video clips that have no action-relevant entities. The\nlatter captures fine-grained motion changes by integrating a\nlong short-term memory cell into the self-attention module\nguided by the motion terms, i.e., verbs, of the language query\nto refine the start and end frames.\n3.1 Entity and Motion Query Extraction\nWe encode the input language query S into entity query fea-\ntures Fe\nq and motion query features Fm\nq . The words in S are\nclassified into three classes: entity, motion, and others, by us-\ning the part of speech tags 2 of the words. The classification\nprobabilities of the word i are denoted by pi = [pe\ni ; pm\ni ; po\ni ] ‚àà\n{0, 1}3. For the word i, if its part of speech tag is related to\nentity (i.e., noun, adjective), then pi = [1, 0, 0]; if its part\nof speech tag is related to motion (i.e.,verb, adverb), then\npi = [0, 1, 0]; otherwise, pi = [0, 0, 1].\nThe word features Q = [w1, w2, ¬∑¬∑¬∑ , wN ]‚ä§ ‚àà RN√ódw of\nS are first initialized using the GloVe embedding [Penning-\nton et al., 2014], where wi denotes the i-th word feature with\ndimension dw and N denotes the word number in the lan-\nguage query. And then a Transformer block is used to learn\nthe relationships between the words, given by\nFq = Transformer q(FC1(Q)) (1)\nwhere Fq = [ fq,1, fq,2, ¬∑¬∑¬∑ , fq,N ]‚ä§ ‚àà RN√ód are the\nlearned linguistic query features; FC1(¬∑) is a fully connected\nlayer that projects the word feature from dimension dw to\nd; Transformer q(¬∑) is a standard Transformer block, as\nshown in Figure 3(a), which consists of multi-head self-\nattention, residual connection, layer normalization and feed-\nforward network. Finally, the entity query features Fe\nq =\n[fe\nq,1, fe\nq,2, ¬∑¬∑¬∑ , fe\nq,N ]‚ä§ ‚àà RN√ód and motion query features\nFm\nq = [fm\nq,1, fm\nq,2, ¬∑¬∑¬∑ , fm\nq,N ]‚ä§ ‚àà RN√ód of the language query\nare calculated by\nfe\nq,i = fq,i ¬∑ (pe\ni + po\ni ), fm\nq,i = fq,i ¬∑ (pm\ni + po\ni ). (2)\n3.2 Entity-aware Transformer\nAs the objects in video are more easily accessible and pro-\nvide rich indication information for actions, it is natural to\n2https://www.nltk.org/\nLinear\nLinear\nLinear\nLinear\nT\nSoftmax\nFFN\nLN\nLN\nVin\nVout\nT\nSoftmax\nFFN\nLN\nLN\nVin\nVout\nK\nQ\nV\n(a) Transformer with frame-wise linear projection for Q,K,V function \n(b) Transformer with cross-frame multi-scale LSTM for Q,K,V function \nL1\nL2\nLs\n‚Ä¶\nK\nQ\nV\nFigure 3: Standard Transformer (a) and our LSTM Transformer (b).\nnarrow down the searching space from all the frames to the\nactual relevance using the entities in the language query. So\nwe propose an entity-aware Transformer to coarsely select the\nvideo clips that are related to the input entity queries. As il-\nlustrated in Figure 2, the entity-aware Transformer first learns\nrelationships between video frames via cross-frame attention\nto provide more contextual information, then fuses the entity\nquery features into each video frame via cross-modal atten-\ntion, next, attends complementary information across differ-\nent frames via cross-frame attention, and finally predicts an\naction-relevant score for each frame to indicate whether the\nframe is action-relevant or not. According to the predicted\naction-relevant scores, we select action-relevant video clips\nwhere the desired action may happen.\nCross-frame Transformer.For each video V , we extract its\nvisual features Fv = [fv,1, fv,2, ¬∑¬∑¬∑ , fv,T ]‚ä§ ‚àà RT√ódv by a\npre-trained 3D ConvNet, where fv,i denotes the i-th visual\nfeature with dimension dv that is computed on a short video\nclip and T denotes the number of features. A standard Trans-\nformer block is then used to attend contextual information\nacross different frames:\nFe\nv = Transformer e(FC2(Fv)) (3)\nwhere Fe\nv = [fe\nv,1, fe\nv,2, ¬∑¬∑¬∑ , fe\nv,T ]‚ä§ ‚àà RT√ód are the updated\nvisual features that pay more attention to the entities; FC2(¬∑)\nis a fully connected layer that projects the visual feature from\ndimension dv to d; Transformer e(¬∑) represents a standard\nTransformer block, as shown in Figure 3(a). We concentrate\non the appearance information of frames without considering\nthe temporal information between them, so no position em-\nbedding is input to the Transformer.\nCross-modal Fusion.We introduce the context-query atten-\ntion (CQA) [Zhang et al., 2021a] to integrate the entity query\nfeatures into visual features of each frame. Given the visual\nfeatures Fe\nv and the entity query features Fe\nq, CQA first com-\nputes the their similarity S = Fe\nv ¬∑Fe\nq\n‚ä§ ‚àà RT√óN , followed by\na row-wise and column-wise softmax normalization to obtain\ntwo similarity matricesSr and Sc. Then two attention weights\nare derived by AV Q= Sr ¬∑ Fe\nq and AQV = Sr ¬∑ S‚ä§\nc ¬∑ Fe\nv. The\nentity-aware visual features Fve are computed by\nFve = FC3([Fe\nv; AV Q; Fe\nv ‚äô AV Q; Fe\nv ‚äô AQV ]) (4)\nwhere Fve = [ fve\n1 , fve\n2 , ¬∑¬∑¬∑ , fve\nT ]‚ä§ ‚àà RT√ód; ‚äô denotes\nelement-wise multiplication; [¬∑ ] is concatenation; FC3(¬∑) is\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1554\na fully connected layer that projects the concatenated feature\nfrom dimension 4d to d.\nPrediction. We calculate the action-relevant score Pe =\n[pe,1, pe,2, ¬∑¬∑¬∑ , pe,T ]‚ä§ ‚àà RT of video frames using two fully\nconnected layers for action location prediction:\nPe = sigmoid(FC5(ReLU(FC4(Fve)))) (5)\nwhere the output feature dimensions of FC4(¬∑) and FC5(¬∑)\nare d\n2 and 1, respectively. The higher the action-relevant score\nis, the higher the probability that the corresponding frame is\nselected as action regions.\n3.3 Motion-aware Transformer\nGiven the coarsely located video clips by the entity-aware\nTransformer, we propose a motion-aware Transformer to re-\nfine the action boundaries by capturing both fine-grained lo-\ncal and global motion changes. As shown in Figure 2, the\nmotion-aware Transformer first learns contextual motion in-\nformation by a novel LSTM Transformer, then attends the\naction-relevant parts by the action-relevant score of entity-\naware Transformer and fuses motion query features into them\nvia cross-modal attention, next, captures the fine-grained mo-\ntion changes and global motion changes via the LSTM Trans-\nformer, and finally predicts the action boundaries.\nLSTM Transformer.The standard Transformer can capture\nglobal motion changes due to its capability of modeling long-\nrange dependency where the self-attention module plays a vi-\ntal role. The self-attention module first conducts linear pro-\njections on each input unit to obtain query, key, and value fea-\ntures, and then uses the similarity of query-key feature to ag-\ngregate the value features, as shown in Figure 3(a). However,\nthe linear projections cannot capture local motion changes in\nsuccessive frames. Thus we replace the linear projection with\na LSTM cell, as shown in Figure 3(b), which learns sequen-\ntial local motion changes in videos. In order to deal with\nthe duration variations of the same action in different videos,\nwe apply the long short-term memory at multiple temporal\nscales.\nSpecifically, given an input sequence Vin = {vin\ni }T\ni=1,\nthe multi-head self-attention module of our LSTM Trans-\nformer is given by MSA( fQ, fK, fV ) = [ h1, h2, ¬∑¬∑¬∑ , hn]\nwhere a single head is calculated as hi = SAi(fQ, fK, fV ) =\nsoftmax(fQf‚ä§\nK/\n‚àö\nd)fV , where d is the dimension of inter-\nmediate features and fŒΩ = LSTM S\nŒΩ (Vin), ŒΩ‚àà [Q, K, V].\nThe LSTM S is a multi-scale version of the LSTM, denoted\nas LSTM S(Vin) = [ L1(Vin); L2(Vin); ¬∑¬∑¬∑ ; LS(Vin)],\nwhere [¬∑] is concatenation. The s-th scale LSTM is calculated\nby\nLs(Vin) =LSTM s(¬∑¬∑¬∑ , Vin\ni‚àí2s, Vin\ni‚àís, Vin\ni , Vin\ni+s, Vin\ni+2s, ¬∑¬∑¬∑ ).\n(6)\nOne-time running of Ls can update input sequence every s\nframes, and sliding Ls in the input sequence one frame for s\ntimes can update all input sequence.\nFor each video V and its visual features Fv =\n[fv,1, fv,2, ¬∑¬∑¬∑ , fv,T ]‚ä§ ‚àà RT√ódv , the LSTM Transformer\nis used to learn both fine-grained local and global motion\nchanges:\nFm\nv = Transformer m(FC2(Fv)) (7)\nwhere Fm\nv = [fm\nv1, fm\nv2, ¬∑¬∑¬∑ , fm\nvN ] ‚àà RT√ód are the updated mo-\ntion features that pay more attention to the motion changes;\nFC2(¬∑) is the fully connected layer used in Equation (3)\nthat projects the visual feature from dimension dv to d;\nTransformer m(¬∑) represents the LSTM Transformer.\nCross-modal Fusion.Before cross-modal fusion, we first at-\ntend the action-relevant video frames by the action-relevant\nscore Pe in a soft manner: Fm\nv = Pe ‚äô Fm\nv , where ‚äô is an\nelement-wise multiplication. Then motion query features Fm\nq\n(calculated in Section 3.1) are fused into the visual motion\nrepresentation Fm\nv via CQA (described in Section 3.2) to ob-\ntain the motion-aware visual features:\nFvm = CQA(Fm\nq , Fm\nv ) (8)\nwhere Fvm = [fvm\n1 , fvm\n2 , ¬∑¬∑¬∑ , fvm\nT ]‚ä§ ‚àà RT√ód.\nPrediction. The start scores Ss ‚àà RT and the end scores\nSe ‚àà RT for target action segment are predicted by a two-\nbranch network consisting of two fully connected layers:\nSs = FC7(ReLU(FC6(Fvm)))\nSe = FC9(ReLU(FC8(Fvm))) (9)\nwhere the output feature dimensions of FCi(¬∑), i‚àà {6, 8}\nand FCj(¬∑), j‚àà {7,9} are d\n2 and 1, respectively. Then the\nprobability distributions of action start and end boundaries\nare computed by Pb\ns = softmax(Ss), Pb\ne = softmax(Se) ‚àà\nRT . Finally, the predicted start and end boundaries of target\naction segment are derived by maximizing the joint probabil-\nity:\n( ÀÜœÑs, ÀÜœÑe) = arg maxts,te Pb\ns(ts) √ó Pb\ne(te),\npb\nse = Pb\ns( ÀÜœÑs) √ó Pb\ne( ÀÜœÑe)\n(10)\nwhere pb\nse is the optimized score of the predicted boundaries\n( ÀÜœÑs, ÀÜœÑe). We also apply another branch of two fully connected\nlayers network to predict a inner probability for each frame\nas a auxiliary task only for training [Wang et al., 2021]. Let\nPin = [pin\n1 , pin\n2 , ¬∑¬∑¬∑ , pin\nT ]‚ä§ ‚àà RT denote the probability of\nbeing action frames, calculated by\nPin = sigmoid(FCb(ReLU(FCa(Fvm)))) (11)\nwhere the output feature dimensions of FCa(¬∑) and FCb(¬∑)\nare d\n2 and 1, respectively.\n3.4 Training Objective\nGiven the predicted probability distribution of action bound-\naries Pb\ns and Pb\ne , the training objective for action boundary\nprediction is formulated by\nLboundary = fXE (Pb\ns, œÑs) +fXE (Pb\ne, œÑe) (12)\nwhere fXE (¬∑) is a cross-entropy function, and (œÑs, œÑe) are the\nground-truth boundaries. Given the inner probability Pin, the\ntraining objective for action frame prediction is formulated\nby\nLinner = fBXE (Pin, Yin) (13)\nwhere fBXE (¬∑) is a binary cross-entropy function and Yin =\n{yin\ni }T\ni=1 ‚àà {0, 1}, when œÑs ‚â§ i ‚â§ œÑe, yin\ni = 1, otherwise\nyin\ni = 0. The overall objective is given by\nL = Œª1Lboundary + Œª2Linner (14)\nwhere Œª1 and Œª2 are hyper-parameters.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1555\nMethods R@1; IoU = ¬µ mIoU0.3 0.5 0.7\nVSL [Zhang et al., 2021a] 70.46 54.19 35.22 50.02\nLGI [Mun et al., 2020] 72.96 59.46 35.48 51.38\nDeNet [Zhou et al., 2021] - 59.7 38.52 -\nSS [Ding et al., 2021] - 60.75 36.19 -\nCPNet [Li et al., 2021] 71.94 60.27 38.74 52.00\nACRM [Tang et al., 2022] 73.47 57.53 38.33 -\nICG [Nan et al., 2021] 67.63 50.24 32.88 48.02\nCPN [Zhao et al., 2021] 68.48 51.07 31.54 48.08\nSPA [Zhang et al., 2021b] 73.84 60.86 41.34 53.92\nCBLN [Liu et al., 2021] - 61.13 38.22 -\nOurs 74.19 61.69 41.96 54.45\nTable 1: Comparison with the state-of-the-art methods on the\nCharades-STA dataset.\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nWe evaluate our method on two datasets: Charades-\nSTA [Gao et al., 2017 ] and TACoS [Regneri et al.,\n2013]. The Charades-STA dataset is built on the Charades\ndataset [Sigurdsson et al., 2016] and contains 16,128 annota-\ntions, including 12,408 for training and 3,720 for test. The\nTACoS dataset is built on the MPII Cooking Compositive\ndataset [Rohrbach et al., 2012] and contains 18,818 annota-\ntions, including 10146 for training, 4589 for validation, and\n4083 for test.\nWe use the metrics of R@n; IoU = ¬µ and mIoU for\nevaluation. R@n; IoU = ¬µ denotes the percentage of test\nsamples that have at least one result whose IoU with ground-\ntruth is larger than ¬µ in top-n predictions and mIoU denotes\nthe average IoU over all test samples. We set n = 1 and\n¬µ ‚àà [0.3, 0.5, 0.7].\n4.2 Implementation Details\nFollowing the previous methods, 3D convolutional features\n(C3D for TACoS, and I3D for Charades-STA) are extracted\nto encode videos. We adopt Adam [Kingma and Ba, 2014 ]\nfor optimization with an initial learning rate of 5e-4 and a lin-\near decay schedule. The loss weights Œª1 and Œª2 in Equation\n(14) are set to 1 and 10, respectively. The number of Trans-\nformer Blocks is set to 1 and 3 for early and late Transformers\nin entity-aware and motion-aware Transformers. The feature\ndimension of all intermediate layers is set to 512, the head\nnumber of multi-head self-attention is set to 8, the layer num-\nber and scale number of long short-term memory are set to 1\nand 3, respectively.\n4.3 Comparison Results\nWe compare our method with the latest state-of-the-art meth-\nods on the Charades-STA and TACoS datasets in Table 1 and\nTable 2, respectively. From the results, it is interesting to ob-\nserve that our method achieves the best performance in terms\nof all evaluation metrics on both two datasets, clearly validat-\ning the superiority of the proposed entity-aware and motion-\naware Transformers on improving the localization precision\nvia a coarse-to-fine strategy.\nMethods R@1; IoU = ¬µ mIoU0.3 0.5 0.7\nBPNet [Xiao et al., 2021] 25.96 20.96 14.08 19.53\nVSL [Zhang et al., 2021a] 29.61 24.27 20.03 24.11\nI2N [Ning et al., 2021] 31.80 28.69 - -\nSS [Ding et al., 2021] 41.33 29.56 - -\nCPNet [Li et al., 2021] 42.61 28.29 - 28.69\nCBLN [Liu et al., 2021] 38.89 27.65 - -\nICG [Nan et al., 2021] 38.84 29.07 19.05 28.26\nSMIN [Wang et al., 2021] 48.01 35.24 - -\nCPN [Zhao et al., 2021] 48.29 36.58 21.25 34.63\nOurs 50.11 38.16 26.82 36.43\nTable 2: Comparison with the state-of-the-art methods on the\nTACoS dataset.\nMethods R@1; IoU = ¬µ mIoU0.3 0.5 0.7\nOurs w/o EA Trans 71.15 58.25 38.79 51.87\nFC Trans 67.18 48.14 27.69 46.24\nT-Conv Trans 73.60 55.86 37.39 53.20\nT-Conv 61.64 37.34 21.91 42.50\nLSTM 72.34 59.14 40.53 53.12\nOurs 74.19 61.69 41.96 54.45\nTable 3: Ablation studies on the Charades-STA dataset.\n4.4 Ablation Studies\nWe perform in-depth ablation studies to evaluate each compo-\nnent of our method on the Charades-STA dataset. The results\nare shown in Table 3.\nEffect of Entity-aware Transformer.To evaluate the entity-\naware Transformer, we design a baseline model called ‚ÄúOurs\nw/o EA Trans‚Äù that uses only the motion-aware Transformer\nwith the input word feature of the language query. As shown\nin Table 3, our method outperforms ‚ÄúOurs w/o EA Trans‚Äù\nwith gains of 3% on all evaluation metrics, clearly demon-\nstrating the effectiveness of the entity-aware Transformer.\nAnalysis of Motion-aware Transformer.To evaluate the\nMotion-aware Transformer, we design several variants of\nour method for comparison, denoted as ‚ÄúFC Trans‚Äù, ‚ÄúT-\nConv Trans‚Äù, ‚ÄúT-Conv‚Äù and ‚ÄúLSTM‚Äù: (i) ‚ÄúFC Trans‚Äù and\n‚ÄúT-Conv Trans‚Äù replace the LSTM cell by fully connected\nlayers and temporal convolutional layers, respectively. So\n‚ÄúFC Trans‚Äù degrades into a standard Transformer; (ii) ‚ÄúT-\nConv‚Äù and ‚ÄúLSTM‚Äù replace the LSTM Transformer by tem-\nporal convolutional and LSTM layers, respectively. For a\nfair comparison, ‚ÄúT-Conv Trans‚Äù and ‚ÄúT-Conv‚Äù have multi-\nple kernel sizes of 3, 5, and 7, and ‚ÄúLSTM‚Äù has the same\nmulti-scale LSTM as LSTM Transformer. From the result\nin Table 3, we have the following observations. (1) Com-\npared with ‚ÄúT-Conv Trans‚Äù, our method achieves better re-\nsults with gains of 5.83% on R@1; IoU = 0.5 and 4.57% on\nR@1; IoU = 0.7. Moreover, ‚ÄúT-Conv Trans‚Äù outperforms\n‚ÄúFC Trans‚Äù by 7.72% on R@1; IoU = 0.5 and 9.70% on\nR@1; IoU = 0.7. These validate that carefully modeling of\nlocal motion changes significantly improves the localization\naccuracy. (2) Compared with ‚ÄúLSTM‚Äù, our method achieves\nbetter results. Moreover, ‚ÄúT-Conv Trans‚Äù outperforms ‚ÄúT-\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1556\nGT\nOurs\nQ:  person eats a sandwich from the table\n13.8s8.1s\n7.6s 13.6s\nPe\nGT 17.8s13.6s\nOurs 11.4s 18.8s\nPe\nQ:  the person puts the cup down\nGT 15.7s9.8s\nOurs 11.2s\n20.5s\nPe\nQ:  lastly the person takes a drink from a cup\nGT14.0s0.0s\nOurs0.0s 14.4s\nPe\nQ: a person is cooking on a stove\nFigure 4: Examples of action localization visualization on the Charades-STA dataset. The action-relevant score Pe is predicted by the entity-\naware Transformer and brighter colors indicate higher values.\ni i 4 2 . 0 --- - -------------- - ‚Ä¢---------------\nll) 61. s 1 - - ------ - -----r---------- --- - ----1 \nr---- 41 . s\nÔøΩ 61.0 \nl l i ÔøΩ 41.6 \n::J 60. 5 i : 1 ::J 41. 4 : .. t ¬∑t\n0 : : : : : ÔøΩ 60. 0 -+---------------------------- ----------------+------------------- --‚Ä¢-- ÔøΩ 41. 2 --J-----------------------------------------------+--------------------+--\n‚Ä¢ \" I I ‚Ä¢ II\"\\ I I \n,....-4 : : \nÔøΩ 41 O : : 5 9 5 : i ,--, ‚Ä¢ _\"_____ ---- - --- , -- ----- - --- r - ----- -------- -- ‚Ä¢---\n@) . i i @)408 --) ÔøΩ 5 9 0 -+----------- ---------------------------------+----------------------- --- ÔøΩ ‚Ä¢ : . i i i 40 6 j \n5 8 . 5 --: ----------------------- ------------------------ ------------------------ -- . ! \n62 \nÔøΩ 61 \nÔøΩ 60 \n0 \n1--1 \nÔøΩ\" 59 \n1 2 3 4 1 2 3 \nScale ofLSTM Scale ofLSTM \n62 \n11,2=10 \ntn 61 \n0 \nÔøΩ60 \n0 \n>--i \n,:.._,\" 59 \n4 \nÔøΩ 58 ÔøΩ 58 ---- Al = l \n57 57 1 2 4 6 8 10 1 2 4 6 8 10 \nAl u\nFigure 5: Analysis of the effect of scale number in LSTM Trans-\nformer on the Charades-STA dataset.\nConv‚Äù by more than 10% on all evaluation metrics. These\nshow that the global changes captured by Transformer are\nbeneficial to action localization.\n4.5 Parameter Analysis\nTemporal Scale Number in LSTM Transformer.The per-\nformances of different temporal scales in LSTM Transformer\non the Charades-STA dataset are shown in Figure 5. We ob-\nserve that when the scale number increases, the performance\nfirst increases and then gradually decreases, which demon-\nstrates that modeling local motion changes at more temporal\nscales can improve the location precision, but also may bring\nredundant information.\nLoss Weights. To analyze the effect of the loss weights Œª1\nand Œª2 in Equation (14), we vary the value ofŒª1 in [1,10] and\nthe value of Œª2 in [1,10]. The results are shown in Figure 6.\nIt is interesting to observe that when Œª1 increases, the perfor-\nmance drops dramatically. In contrast, the performance im-\nproves along with the increasing Œª2, which shows that larger\nŒª2 boosts the per-frame inner prediction, and thus helps the\nboundary localization.\n4.6 Qualitative Analysis.\nWe show several examples of action localization results on\nthe Charades-STA dataset in Figure 4 by visualizing the cor-\nresponding action-relevant scores predicted by the entity-\naware Transformer where bright colors indicate higher val-\nues. From the first three cases, we see that the action-\nrelevant scores make it easier to accurately localize the action\nboundaries by paying more attention to the target action in\nFigure 6: Analysis of the effect of loss weights on the Charades-STA\ndataset.\na shrunken temporal. However, in the last case, the entities\n(‚Äúperson & cup‚Äù) remain unchanged that the action-relevant\nscores of different frames are similar (the margin between\nmaximum and minimum is less than 0.1), thus contributing\nless to the final boundary localization. Moreover, the entity\nword ‚Äúdrink‚Äù is wrongly classified to a motion word, so that\nthe predicted boundaries wrongly fall in the boundaries of\n‚Äúdrink‚Äù.\n5 Conclusion\nWe have presented a novel coarse-to-fine model called entity-\naware and motion-aware Transformers for language-driven\naction localization. It can progressively predict the action\nboundaries with high precision by first attending the action-\nrelevant clips via the entity-aware Transformer and then re-\nfining the start and end frames via the motion-aware Trans-\nformer. By integrating multi-scale long short-term mem-\nory cells into the self-attention module, the motion-aware\nTransformer succeeds in capturing the fine-grained motion\nchanges, thus achieving promising results. Extensive exper-\niments on two public datasets have demonstrated that our\nmethod outperforms the state-of-the-art methods. In the fu-\nture work, we are going to apply the entity-aware and motion-\naware Transformers to weakly-supervised language-driven\naction localization.\nAcknowledgments\nThis work was supported in part by the Natural Science Foun-\ndation of China (NSFC) under Grant No 62072041.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1557\nReferences\n[Chen and Jiang, 2019] Shaoxiang Chen and Yu-Gang Jiang.\nSemantic proposal for activity localization in videos via\nsentence query. In Proc. of AAAI, 2019.\n[Ding et al., 2021] Xinpeng Ding, Nannan Wang, et al.\nSupport-set based cross-supervision for video grounding.\nIn Proc. of ICCV, 2021.\n[Gao et al., 2017] Jiyang Gao, Chen Sun, et al. Tall: Tem-\nporal activity localization via language query. In Proc. of\nICCV, 2017.\n[Ghosh et al., 2019] Soham Ghosh, Anuva Agarwal, et al.\nExcl: Extractive clip localization using natural language\ndescriptions. In Proc. of ACL, 2019.\n[Hahn et al., 2019] Meera Hahn, Asim Kadav, et al. Tripping\nthrough time: Efficient localization of activities in videos.\nIn BMVC, 2019.\n[Hendricks et al., 2017] Lisa Anne Hendricks, Oliver Wang,\net al. Localizing moments in video with natural language.\nIn Proc. of ICCV, 2017.\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[Li et al., 2021] Kun Li, Dan Guo, and Meng Wang.\nProposal-free video grounding with contextual pyramid\nnetwork. In Proc. of AAAI, 2021.\n[Liu et al., 2018] Meng Liu, Xiang Wang, et al. Attentive\nmoment retrieval in videos. In Proc. of ACM SIGIR, 2018.\n[Liu et al., 2020] Daizong Liu, Xiaoye Qu, et al. Jointly\ncross- and self-modal graph attention network for query-\nbased moment localization. In Proc. of ACM MM, 2020.\n[Liu et al., 2021] Daizong Liu, Xiaoye Qu, et al. Context-\naware biaffine localizing network for temporal sentence\ngrounding. In Proc. of CVPR, 2021.\n[Lu et al., 2019] Chujie Lu, Long Chen, et al. Debug: A\ndense bottom-up grounding approach for natural language\nvideo localization. In Proc. of EMNLP, 2019.\n[Mun et al., 2020] Jonghwan Mun, Minsu Cho, and Bo-\nhyung Han. Local-global video-text interactions for tem-\nporal grounding. In Proc. of CVPR, 2020.\n[Nan et al., 2021] Guoshun Nan, Rui Qiao, Yao Xiao, et al.\nInterventional video grounding with dual contrastive\nlearning. In Proc. of CVPR, 2021.\n[Ning et al., 2021] Ke Ning, Lingxi Xie, et al. Interaction-\nintegrated network for natural language moment localiza-\ntion. TIP, 2021.\n[Pennington et al., 2014] Jeffrey Pennington, Richard\nSocher, et al. Glove: Global vectors for word representa-\ntion. In Proc. of EMNLP, 2014.\n[Qu et al., 2020] Xiaoye Qu, Pengwei Tang, et al. Fine-\ngrained iterative attention network for temporal language\nlocalization in videos. In Proc. of ACM MM, 2020.\n[Regneri et al., 2013] Michaela Regneri, Marcus Rohrbach,\net al. Grounding action descriptions in videos. TACL,\n2013.\n[Rohrbach et al., 2012] Marcus Rohrbach, Michaela Reg-\nneri, et al. Script data for attribute-based recognition of\ncomposite activities. In ECCV, 2012.\n[Sigurdsson et al., 2016] Gunnar A Sigurdsson, G ¬®ul Varol,\net al. Hollywood in homes: Crowdsourcing data collec-\ntion for activity understanding. In Proc. of ECCV, 2016.\n[Tang et al., 2022] Haoyu Tang, Jihua Zhu, et al. Frame-wise\ncross-modal matching for video moment retrieval. IEEE\nTransactions on Multimedia, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, et al.\nAttention is all you need. In NeurIPS, 2017.\n[Wang et al., 2021] Hao Wang, Zheng-Jun Zha, et al. Struc-\ntured multi-level interaction network for video moment lo-\ncalization via language query. In Proc. of CVPR, 2021.\n[Wu et al., 2020] Jie Wu, Guanbin Li, et al. Tree-structured\npolicy based progressive reinforcement learning for tem-\nporally language grounding in video. In Proc. of AAAI,\n2020.\n[Xiao et al., 2021] Shaoning Xiao, Long Chen, et al. Bound-\nary proposal network for two-stage natural language video\nlocalization. In Proc. of AAAI, 2021.\n[Yuan et al., 2019] Yitian Yuan, Tao Mei, and Wenwu Zhu.\nTo find where you talk: Temporal sentence localization in\nvideo with attention based location regression. In Proc. of\nAAAI, 2019.\n[Yuan et al., 2020] Yitian Yuan, Lin Ma, et al. Semantic\nconditioned dynamic modulation for temporal sentence\ngrounding in videos. TPAMI, 2020.\n[Zeng et al., 2020] Runhao Zeng, Haoming Xu, et al. Dense\nregression network for video grounding. InProc. of CVPR,\n2020.\n[Zhang et al., 2020] Songyang Zhang, Houwen Peng, and\nJiebo Luo. Learning 2d temporal adjacent networks for\nmoment localization with natural language. In Proc. of\nAAAI, 2020.\n[Zhang et al., 2021a] Hao Zhang, Aixin Sun, et al. Natural\nlanguage video localization: A revisit in span-based ques-\ntion answering framework. TPAMI, 2021.\n[Zhang et al., 2021b] Hao Zhang, Aixin Sun, Wei Jing, et al.\nParallel attention network with sequence matching for\nvideo grounding. In Proc. of ACL, 2021.\n[Zhang et al., 2021c] Mingxing Zhang, Yang Yang, et al.\nMulti-stage aggregated transformer network for temporal\nlanguage localization in videos. In Proc. of CVPR, 2021.\n[Zhao et al., 2021] Yang Zhao, Zhou Zhao, et al. Cascaded\nprediction network via segment tree for temporal video\ngrounding. In Proc. of CVPR, 2021.\n[Zhou et al., 2021] Hao Zhou, Chongyang Zhang, et al. Em-\nbracing uncertainty: Decoupling and de-bias for robust\ntemporal grounding. In Proc. of CVPR, 2021.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n1558",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8211772441864014
    },
    {
      "name": "Transformer",
      "score": 0.7509956359863281
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5791865587234497
    },
    {
      "name": "CLIPS",
      "score": 0.5195396542549133
    },
    {
      "name": "Action recognition",
      "score": 0.5147852897644043
    },
    {
      "name": "Computer vision",
      "score": 0.4480189383029938
    },
    {
      "name": "Motion (physics)",
      "score": 0.4263351261615753
    },
    {
      "name": "Natural language processing",
      "score": 0.3573830723762512
    },
    {
      "name": "Voltage",
      "score": 0.09881648421287537
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}