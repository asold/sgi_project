{
  "title": "The unreasonable effectiveness of large language models in zero-shot semantic annotation of legal texts",
  "url": "https://openalex.org/W4388764805",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2187915899",
      "name": "Jaromír Šavelka",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A718266907",
      "name": "Kevin D. Ashley",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2187915899",
      "name": "Jaromír Šavelka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A718266907",
      "name": "Kevin D. Ashley",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6770321754",
    "https://openalex.org/W2063112000",
    "https://openalex.org/W4386504863",
    "https://openalex.org/W6693591013",
    "https://openalex.org/W4315606663",
    "https://openalex.org/W6782067991",
    "https://openalex.org/W2963594477",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2779545291",
    "https://openalex.org/W6784117431",
    "https://openalex.org/W4287391247",
    "https://openalex.org/W2121882970",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1528607527",
    "https://openalex.org/W6856225661",
    "https://openalex.org/W4316829821",
    "https://openalex.org/W6791145691",
    "https://openalex.org/W6640462745",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3094446431",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4366197581",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6786636820",
    "https://openalex.org/W6788622633",
    "https://openalex.org/W2103035252",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W3214559854",
    "https://openalex.org/W4375958664",
    "https://openalex.org/W6758551548",
    "https://openalex.org/W3180395890",
    "https://openalex.org/W6712146269",
    "https://openalex.org/W6768249719",
    "https://openalex.org/W3173327529",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2776168059",
    "https://openalex.org/W6764787635",
    "https://openalex.org/W6855919623",
    "https://openalex.org/W6773280126",
    "https://openalex.org/W6786180127",
    "https://openalex.org/W4200001896",
    "https://openalex.org/W6693899176",
    "https://openalex.org/W6712854417",
    "https://openalex.org/W3107372745",
    "https://openalex.org/W6804875173",
    "https://openalex.org/W6798707981",
    "https://openalex.org/W3186492090",
    "https://openalex.org/W2964051087",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2985250120",
    "https://openalex.org/W4226160860",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4311253308",
    "https://openalex.org/W4230329770",
    "https://openalex.org/W4392773006",
    "https://openalex.org/W4392773043",
    "https://openalex.org/W3135190223",
    "https://openalex.org/W2973832122",
    "https://openalex.org/W3081663193",
    "https://openalex.org/W40716904",
    "https://openalex.org/W2527596457",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W3177813494"
  ],
  "abstract": "The emergence of ChatGPT has sensitized the general public, including the legal profession, to large language models' (LLMs) potential uses (e.g., document drafting, question answering, and summarization). Although recent studies have shown how well the technology performs in diverse semantic annotation tasks focused on legal texts, an influx of newer, more capable (GPT-4) or cost-effective (GPT-3.5-turbo) models requires another analysis. This paper addresses recent developments in the ability of LLMs to semantically annotate legal texts in zero-shot learning settings. Given the transition to mature generative AI systems, we examine the performance of GPT-4 and GPT-3.5-turbo(-16k), comparing it to the previous generation of GPT models, on three legal text annotation tasks involving diverse documents such as adjudicatory opinions, contractual clauses, or statutory provisions. We also compare the models' performance and cost to better understand the trade-offs. We found that the GPT-4 model clearly outperforms the GPT-3.5 models on two of the three tasks. The cost-effective GPT-3.5-turbo matches the performance of the 20× more expensive text-davinci-003 model. While one can annotate multiple data points within a single prompt, the performance degrades as the size of the batch increases. This work provides valuable information relevant for many practical applications (e.g., in contract review) and research projects (e.g., in empirical legal studies). Legal scholars and practicing lawyers alike can leverage these findings to guide their decisions in integrating LLMs in a wide range of workflows involving semantic annotation of legal texts.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/seven.tnum November /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nOPEN ACCESS\nEDITED BY\nMohammad Akbari,\nAmirkabir University of Technology, Iran\nREVIEWED BY\nStefano Silvestri,\nNational Research Council (CNR), Italy\nGijs Van Dijck,\nMaastricht University, Netherlands\n*CORRESPONDENCE\nJaromir Savelka\njsavelka@cs.cmu.edu\nRECEIVED /one.tnum/eight.tnum August /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /one.tnum/six.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/seven.tnum November /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nSavelka J and Ashley KD (/two.tnum/zero.tnum/two.tnum/three.tnum) The\nunreasonable eﬀectiveness of large language\nmodels in zero-shot semantic annotation of\nlegal texts. Front. Artif. Intell./six.tnum:/one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Savelka and Ashley. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nThe unreasonable eﬀectiveness of\nlarge language models in\nzero-shot semantic annotation of\nlegal texts\nJaromir Savelka/one.tnum* and Kevin D. Ashley /two.tnum\n/one.tnumSchool of Computer Science, Carnegie Mellon University, Pittsbur gh, PA, United States, /two.tnumSchool of Law,\nUniversity of Pittsburgh, Pittsburgh, PA, United States\nThe emergence of ChatGPT has sensitized the general public, incl uding the\nlegal profession, to large language models’ (LLMs) potenti al uses (e.g., document\ndrafting, question answering, and summarization). Althoug h recent studies have\nshown how well the technology performs in diverse semantic annot ation tasks\nfocused on legal texts, an inﬂux of newer, more capable (GPT-/four.tnum) orcost-eﬀective\n(GPT-/three.tnum./five.tnum-turbo) models requires another analysis. This paper addresses recent\ndevelopments in the ability of LLMs to semantically annotate le gal texts in zero-\nshot learning settings. Given the transition to mature gene rative AI systems, we\nexamine the performance of GPT-/four.tnum and GPT-/three.tnum./five.tnum-turbo(-/one.tnum/six.tnumk), comparing it to\nthe previous generation of GPT models, on three legal text ann otation tasks\ninvolving diverse documents such as adjudicatory opinions, con tractual clauses,\nor statutory provisions. We also compare the models’ perform ance and cost\nto better understand the trade-oﬀs. We found that the GPT-/four.tnum model clearly\noutperforms the GPT-/three.tnum./five.tnum models on two of the three tasks. The cost-eﬀective\nGPT-/three.tnum./five.tnum-turbo matches the performance of the /two.tnum/zero.tnum× more expensive text-davinci-\n/zero.tnum/zero.tnum/three.tnum model. While one can annotate multiple data points within asingle prompt,\nthe performance degrades as the size of the batch increases. This work provides\nvaluable information relevant for many practical applications ( e.g., in contract\nreview) and research projects (e.g., in empirical legal studies ). Legal scholars and\npracticing lawyers alike can leverage these ﬁndings to guide the ir decisions in\nintegrating LLMs in a wide range of workﬂows involving semantic annotation of\nlegal texts.\nKEYWORDS\nlegal text analytics, large language models (LLM), zero-shot cl assiﬁcation, semantic\nannotation, text annotation\n/one.tnum Introduction\nThis paper analyzes the capabilities of the newest state-of-the-art generative pre-trained\ntransformers, i.e., GPT-3.5-turbo(-16k) and GPT-4, on semantic annotation of various types\nof legal texts in zero-shot learning settings. The aim of this paper is to react to the releases\nand updates of the newest generations of the OpenAI’s GPT models and assess if and to\nwhat extent the ﬁndings presented by similar past studies performed with the text-davinci-\n003 model still hold. Hence, the focus is not only on the performance of the newest models\nbut also on the comparison of their performance to that of the earlier GPT model. To that\nend we use selected parts of three legal document corpora that various research groups\nassembled and manually annotated in the past. We aim to compare the eﬀectiveness of these\nmassively large language models (LLM) in semantically annotating the legal documents. The\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\ndata sets were carefully selected to represent a wide variety\nof legal tasks and documents involving semantic annotation.\nSpeciﬁcally, we focus on shorter text snippets, usually one or several\nsentences long, in decisions of the U.S. Board of Veterans’ Appeals,\ncommercial contracts, and state statutory provisions dealing with\npublic health emergencies. Hence, we evaluate the eﬀectiveness of\nthe models in tasks that could be part of a typical contract review,\nstatutory/regulatory provisions investigation, or case-law analysis.\nWe also compare the performance of these general (i.e., not ﬁne-\ntuned\n/one.tnum) GPT models in annotating small batches of short text\nsnippets coming from such legal documents based on compact, one\nsentence long, semantic type deﬁnitions, against the performance\nof a traditional statistical machine learning (ML) model (random\nforest) and ﬁne-tuned BERT model (RoBERTa).\nThe release of ChatGPT\n/two.tnumhas made the general public aware\nof the capabilities of GPT models to write ﬂuent text and answer\nquestions. Although, the underlying GPT-3 technology has been\navailable since 2020 (\nBrown et al., 2020 ), the ready availability of\nthis easier-to-use version of the tool is spurring legal practitioners,\neducators and scholars alike to anticipate how the legal profession\nwill change in the near future or be forced to. Researchers have\nalready applied the technology to tasks traditionally reserved for\nlegal experts.\nPerlman (2022) claim that ChatGPT requires re-\nimagining how to obtain legal services and prepare lawyers for\ntheir careers. He does so in a human-authored abstract to a\nscholarly article automatically generated with ChatGPT.\nKatz et al.\n(2023) tested GPT-4 on the Uniform Bar Examination (UBE),\nobserving the system comfortably passing the exam. These use cases\nsuggest future applications for the newly emerging technology in\nproviding legal services and increasing access to justice, namely\nlegal document drafting, legal question answering and/or advising,\nas well as explaining complex legal matters in plain language and\nsummarizing legal documents (\nXu and Ashley, 2023 ).\nSemantic annotation of legal texts involves labeling sentences\nor other portions of legal documents in terms of conceptual types\nsuch as rhetorical roles the sentences play in legal arguments, types\nof contracts clauses, and types or purposes of statutory provisions.\nWhile semantic annotation of legal texts may appear less dramatic\nthan these future use cases, we argue that it may prove to be the\nmost promising and valuable application of the massive LLMs to\nthe domain of law. For a variety of reasons (e.g., the danger of\nmisleading advice, professional ethics, limits on who can provide\nlegal services) legal professionals cannot delegate to machines\nthe task of understanding legal documents. Semantic annotation\nenables conceptually indexing large corpora of legal documents in\nterms that legal professionals understand and can use to ﬁnd and\nreview for themselves relevant legal arguments, contractual clauses,\nor statutory provisions.\nSemantic annotation lies at the heart of many high-volume\nworkﬂows for contract review or case analysis that are currently\nprohibitively expensive for all but the largest legal teams\n/one.tnum Transformer language models are pre-trained on large text corpora.Fine-\ntuning the language model involves training it on new data to improv e its\nperformance on a speciﬁc task.\n/two.tnum OpenAI: ChatGPT. Available online at:\nhttps://chat.openai.com/\n(accessed February /one.tnum/one.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\nand projects because they require the time of expensive legal\nprofessionals to manually annotate training sets of examples of legal\ntypes. Supervised machine learning employs such labeled data in\ntraining sets of examples as a step toward learning automatically\nto annotate previously unseen instances of the conceptual types.\nGPT models oﬀer the potential for automatic annotation of new\ninstances without any training examples, that is, zero-shot learning.\nSpeciﬁcally, the need for manual annotation may be replaced with\none or two sentence deﬁnitions of the types. The workﬂow process\nwill become more incremental. Instead of requiring a lot of manual\nannotation to get started, automation will produce annotations\nearlier, and legal professionals can focus their time instead on\nensuring that the annotations are meaningful and useful. Thus,\nGPT and similar technology have the potential to (i) dramatically\nlower the cost of applications such as e-discovery or contract\nreview; (ii) unlock diverse novel applications that would otherwise\nnot be economically feasible; and, perhaps most importantly,\n(iii) democratize access to these sophisticated workﬂows that are\ncurrently accessible only to a small group of elite legal operatives\nand their clients.\nFor example, our work may enable legal practitioners and\nresearchers to undertake eﬃcient exploratory annotation of a\ndataset of legal texts that does not involve the cost of ﬁne-tuning\na large language model. Fine-tuning depends on the availability of\nlabeled data. Often, the process may not require a great deal of\nlabeled data, perhaps only several tens of documents. If one wishes\nto perform a task on a data set of legal texts that have not yet\nbeen labeled, however, the cost can be signiﬁcant, especially if one\nis not yet fully certain about which types to include in the type\nsystem. The zero-shot approach enables one to sketch and improve\nthe type system incrementally. One lists the types with sentence-\nlong descriptions for each, applies zero-shot annotation with the\nGPT model, improves the descriptions based on the results, and\nreapplies the automatic annotation. Depending on the complexity\nof the task and the intended use of the annotations, the labels\nproduced in this way may often be suﬃcient. In other cases, they\nmay be a much needed proof of the feasibility of the intended task,\nmaking it possible to spend additional resources on human-labeling\nof a dataset for ﬁne-tuning.\nTo investigate the capabilities of various state-of-the-art GPT\nmodels on semantic annotation of short text snippets from\nvarious types of legal documents, we analyzed the following\nresearch questions:\n1. Using brief type deﬁnitions from a non-hierarchical type\nsystem describing short snippets of text, how well can\nGPT-3.5-turbo(-16k) and GPT-4 models classify such texts in\nterms of the type system’s categories as compared to the text-\ndavinci-003 model?\n2. What are the eﬀects of performing the annotation on batches\nof data points compared to annotating each data point\nindividually.\nOur work contributes to AI & Law research in the following\nways. To our knowledge, this is the ﬁrst comprehensive study that:\n1. Compares the capabilities of text-davinci-003,\nGPT-3.5-turbo(-16k) and GPT-4 models on semantic\nannotation of short text snippets from adjudicatory opinions,\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\ncontractual clauses, and statutory and regulatory provisions in\nzero-shot settings.\n2. Investigates the performance and cost trade-oﬀs of the OpenAI’s\nstate-of-the-art GPT models in the context of legal text\nannotation tasks.\nWe release the prompts used in our experiments as well as\nsettings for the models in an accompanying GitHub repository.\n/three.tnum\n/two.tnum Related work\nThe zero-shot learning capabilities of the GPT models have\nbeen recognized by various AI & Law research groups. Yu\net al. (2022) explored these in the context of the COLIEE\nentailment task based on the Japanese Bar exam, improving\nsigniﬁcantly on the then existing state-of-the-art. Similarly,\nKatz\net al. (2023) successfully applied GPT-4 to the Uniform Bar\nExamination, and Bommarito et al. (2023) to the Uniform CPA\nExamination developed by the American Institute of Certiﬁed\nPublic Accountants.\nSarkar et al. (2021) investigated the potential\nof various techniques, including LLMs (BERT), in zero/few-shot\nclassiﬁcation of legal texts.\nSavelka et al. (2023) employed GPT-\n4 to evaluate the explanatory value of case sentences that refer\nto a statutory term of interest. Other studies were focused on\nthe capabilities of the GPT models to conduct legal reasoning\n(\nBlair-Stanek et al., 2023 ; Nguyen et al., 2023 ), to model U.S.\nsupreme court cases ( Hamilton, 2023 ), to give legal information\nto laypeople ( Tan et al., 2023 ), and to support online dispute\nresolution (Westermann et al., 2023 ).\nAnalyzing small textual snippets such as sentences ( Savelka\net al., 2017 ) in adjudicatory opinions in terms of their function\nor role is an important task in legal text processing. Prior\nresearch utilizing supervised machine learning (ML) or expert\ncrafted rules can roughly be distinguished into two categories.\nFirst, the task could be labeling smaller textual units, often\nsentences, according to some predeﬁned type system (e.g.,\nrhetorical roles, such as evidence, reasoning, conclusion). Examples\nfrom several domains and countries include court (\nSavelka and\nAshley, 2017 ) and administrative decisions from the U.S. ( Walker\net al., 2019 ; Zhong et al., 2019 ), multi-domain court decisions\nfrom India ( Bhattacharya et al., 2019 ) or Canada ( Xu et al.,\n2021a,b), international court ( Poudyal et al., 2020 ) or arbitration\ndecisions ( Branting et al., 2019 ), or judicial decisions from\nmultiple countries and legal domains ( Savelka et al., 2020 ).\nResearchers have also focused on identifying sections that report\ncase outcomes (\nPetrova et al., 2020 ; Xu et al., 2020 ). A second\ntask involves identifying a few contiguous functional parts that\ncomprise multiple paragraphs, as has been done in U.S. (\nSavelka\nand Ashley, 2018 ), Canadian ( Farzindar and Lapalme, 2004 ),\nFrench ( Boniol et al., 2020 ), Czech ( Harašta et al., 2019 ), or even\nmulti-jurisdictional (Savelka et al., 2021 ) legal domains.\nClassifying legal norms in terms of their semantic types has\nbeen a topic of persistent interest in AI & Law. Researchers have\ntrained traditional statistical supervised ML models on manually\nannotated texts to identify deﬁnitions, prohibitions, or obligations\n/three.tnumhttps://github.com/jsavelka/unreasonable_eﬀectiveness.\nin statutory texts ( Biagioli et al., 2005 ; Francesconi et al., 2010 ) or\nto classify sentences in statutes as deﬁnitional, publication, or scope\nof change provisions (\nde Maat et al., 2010 ). Other work focuses on\nﬁner-grained semantic analysis of statutes to identify obligations,\npermissions, antecedents, subject agents, or themes (\nWyner and\nPeters, 2011 ), concepts, and deﬁnitions ( Winkels and Hoekstra,\n2012). A long tradition of analyzing European Union legislative\ndocuments also employed manual text annotation ( Pouliquen\net al., 2006 ; Boella et al., 2012 ). ML models trained on sentences\nfrom cases that have been manually annotated as better or worse\nexplanations of statutory terms have also learned to select higher\nquality explanations in new cases (\nSavelka and Ashley, 2022 ).\nClassiﬁcation of contractual clauses in terms of various\nsemantic types has also received much interest from the AI & Law\ncommunity.\nChalkidis et al. (2017) employed a combination of\nstatistical ML and hand-crafted rules to analyze the clauses in terms\nof types such as termination clause, governing law or jurisdiction.\nLater they utilized various deep learning methods such as CNN,\nLSTM or BERT (\nChalkidis et al., 2021 ). Leivaditi et al. (2020) made\na benchmark data set available comprising 179 lease agreement\ndocuments focusing on recognition of entities and red ﬂags, and\nWang et al. (2023) released a Merger Agreement Understanding\nDataset (MAUD). In our work, we have focused on 12 semantic\ntypes from the Contract Understanding Atticus Dataset (CUAD)\n(\nHendrycks et al., 2021 ).\n/three.tnum Data\nAs in Savelka (2023), we used three existing manually annotated\ndata sets in our experiments. Each data set supports various tasks\ninvolving diﬀerent types of legal documents. All of them include\nannotations attached by experts to (usually) short pieces of text. We\nﬁltered and processed the data sets to make them suitable for this\nwork’s experiments.\n/three.tnum./one.tnum BVA decisions of veterans claims\nThe U.S. Board of Veterans’ Appeals\n/four.tnum(BV A) is an\nadministrative body within the U.S. Department of Veterans\nAﬀairs (V A) responsible for hearing appeals from veterans who\nare dissatisﬁed with decisions made by V A regional oﬃces. The\nBV A reviews a wide range of issues, including claims for disability\ncompensation, survivor beneﬁts, and other compensation and\npension claims.\nWalker et al. (2019) analyzed 50 BV A decisions\nissued between 2013 and 2017. All of the decisions were arbitrarily\nselected cases dealing with claims by veterans for service-related\npost-traumatic stress disorder (PTSD). For each decision, the\nresearchers extracted sentences addressing the factual issues. The\nsentences were then manually annotated with rhetorical roles they\nplay in the respective decisions (\nWalker et al., 2017 ):\n/four.tnum U.S. Department of Veterans’ Appeals: Board of Veteran’s App eals.\nAvailable online at: https://www.bva.va.gov/ (accessed February /nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nFIGURE /one.tnum\nSemantic types distribution. The ﬁgure shows the distribution i n terms of number of examples of the semantic types across the t hree data sets. In the\nBVA data set (left), although Evidence is the dominant category, the four remaining types are reasonably represented. The CUAD data set (center)\ndoes not have a clearly dominant type. Several types are represe nted by a relatively small number of examples ( ∼ /one.tnum/zero.tnum/zero.tnum). The PHASYS data set(right) is\nheavily skewed toward the Response label (/six.tnum/two.tnum./four.tnum%).\nTABLE /one.tnumBVA dataset properties.\nCount Min. len. Mean len. Max. len.\nFinding 485 5 24.4 72\nReasoning 702 3 26.7 92\nEvidence 2,416 4 24.8 204\nLegal Rule 935 5 34.8 115\nCitation 1,112 1 13.5 163\nOverall 5,650 1 24.4 204\nThe ﬁgure shows basic properties of the documents, i.e., sentences from a djudicatory\ndecisions, for each of the semantic types. The lengths (min, mean, ma x) are measured\nin words.\n• Finding—States an authoritative ﬁnding, conclusion or\ndetermination of the trier of fact—a decision made “as a\nmatter of fact” instead of “as a matter of law.”\n• Reasoning—Reports the trier of fact’s reasoning based on the\nevidence, or evaluation of the probative value of the evidence,\nin making the ﬁndings of fact.\n• Evidence—States the content of the testimony of a witness,\nor the content of documents introduced into evidence, or\ndescribes other evidence.\n• Legal Rule —States one or more legal rules in the abstract,\nwithout stating whether the conditions of the rule(s) are\nsatisﬁed in the case being decided.\n• Citation—References legal authorities or other materials,\nand usually contains standard notation that encodes useful\ninformation about the cited source.\nThe original PTSD data set\n/five.tnumfrom Walker et al. (2017) contains\n478 sentences that are simply annotated as Sentence. This is\npresumably a catch-all category reserved for sentences that do\nnot ﬁt any of the above deﬁnitions. Given the nature of our\n/five.tnum GitHub: VetClaims–JSON. Available online at: https://github.com/\nLLTLab/VetClaims-JSON (accessed February /nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\nexperiments, and since there is no compact deﬁnition of this type of\nsentences provided by the researchers, we exclude sentences of this\ntype from the data set.\nFigure 1 (left) shows the distribution of the\nlabels. While the Evidence sentences are clearly the majority label,\nthere is reasonable representation of the other four sentence types.\nBasic properties of the documents (i.e., sentences from adjudicatory\ndecisions) are provided in\nTable 1. The sentences are 24.4 words\nlong on average, with the longest sentence having 204 words. We\ndid not perform any other modiﬁcations on this data set. Hence,\nthe results reported here can be, with some caveats, compared to\nthe results presented in earlier studies by various groups (\nWalker\net al., 2019 ; Savelka et al., 2020 ; Westermann et al., 2021 ).\nSentences in this dataset were classiﬁed manually by teams\nof two trained law students, and they were curated by a law\nprofessor with expertise in legal reasoning. The dataset has been\nreleased publicly and, hence, open to scrutiny (\nWalker et al., 2019 ).\nWhile there are detailed publicly-available annotation guidelines\nrelated to this data set,\n/six.tnumwe work with the informal (compact) type\ndeﬁnitions provided in the Readmeﬁle/seven.tnumof the data set repository\nin constructing the prompt for the GPT models as in Savelka\n(2023). These deﬁnitions are very close to those provided above.\n/three.tnum./two.tnum Contract Understanding Atticus\nDataset\nThe Contract Understanding Atticus Dataset (CUAD) is a\ncorpus of 510 commercial legal contracts that have been manually\nlabeled under the supervision of professional lawyers. This eﬀort\nresulted in more than 13,000 annotations.\n/eight.tnumThe data set, released\n/six.tnum Available online at:https://github.com/LLTLab/VetClaims-JSON/tree/\nmaster/LLT%/two.tnum/zero.tnumAnnotation%/two.tnum/zero.tnumProtocols(accessed February /nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\n/seven.tnumhttps://github.com/LLTLab/VetClaims-JSON/blob/master/README.md\n/eight.tnum The Atticus Project: Contract Understanding Atticus Dataset(CUAD).\nAvailable online at: https://www.atticusprojectai.org/cuad (accessed\nFebruary /nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nTABLE /two.tnumCUAD dataset properties.\nCount Min.\nlen.\nMean\nlen.\nMax.\nlen.\nAnti-assignment 549 5 46.1 703\nAudit rights 540 8 46.6 219\nCovenant not to Sue 136 12 65.8 340\nGoverning law 440 7 33.8 141\nIP ownership assignment 252 6 61.5 217\nInsurance 446 5 44.1 240\nMinimum commitment 300 2 51.4 306\nPost-termination services 336 14 70.2 275\nRevenue-proﬁt sharing 311 10 49.7 210\nTermination for convenience 195 4 38.4 333\nVolume restriction 143 12 43.6 164\nWarranty duration 135 10 48.3 145\nOverall 3,783 2 48.7 703\nThe ﬁgure shows basic properties of the documents, i.e., contractual cla uses, for each of the\nsemantic types. The lengths (min, mean, max) are measured in word s.\nby Hendrycks et al. (2021), identiﬁes 41 types of legal clauses that\nare typically considered important in contract review in connection\nwith corporate transactions. The original 41 categories are a mix\nof clause-level and sub-sentence-level annotations (e.g., an eﬀective\ndate, names of parties). In this study we work with clause-level types\nonly (typically consisting of one or several sentences). Speciﬁcally,\nwe decided to work with the 12 most common clause-level types\npresent in the corpus:\n• Anti-assignment—Is consent or notice required of a party if\nthe contract is assigned to a third party?\n• Audit Rights —The right to audit the books, records, or\nphysical locations of the counterparty to ensure compliance.\n• Covenant not to Sue —Is a party restricted from bringing a\nclaim against the counterparty?\n• Governing Law —Which state/country’s law governs the\ninterpretation of the contract?\n• IP Ownership Assignment —Does IP created by one party\nbecome the property of the counterparty?\n• Insurance—A requirement for insurance that must be\nmaintained by one party for the beneﬁt of the counterparty.\n• Minimum Commitment —A minimum amount or units per-\ntime period that one party must buy.\n• Post-termination Services —Is a party subject to obligations\nafter the termination or expiration of a contract?\n• Revenue-Proﬁt Sharing —Is one party required to share\nrevenue or proﬁt with the counterparty?\n• Termination for Convenience —Can a party terminate this\ncontract without cause?\n• Volume Restriction —A fee increase or consent requirement if\none party’s use of the product/services exceeds threshold.\n• Warranty Duration —What is the duration of any warranty\nagainst defects or errors?\nFigure 1 (center) shows the distribution of the clause types. No\nsingle type dominates the distribution. The more common types\nsuch as Anti-assignment or Audit Rights each appear more than 500\ntimes, whereas the least represented types such as Covenant not to\nSue or Warranty Duration still have more than 100 examples. Basic\nproperties of the documents (i.e., contractual clauses) are provided\nin\nTable 2. The clauses are 48.7 words long on average, with the\nlongest sentence having 703 words. Besides the above described\nﬁltering, we did not perform any other transformations on this data\nset. Hence, we focus on a subset of tasks described in\nHendrycks\net al. (2021).\nHendrycks et al. (2021) states that the contracts were labeled\nby law students and checked by experienced lawyers. The law\nstudents went through 70–100 hours of training for labeling that\nwas designed by experienced lawyers. The process was supported\nby extensive documentation on how to identify each label category\nin a contract that takes up more than one hundred pages. The\ndata set includes brief, one sentence long, type deﬁnitions that are\npublicly available.\n/nine.tnumThese deﬁnitions roughly correspond to those\nprovided above. We utilize these deﬁnitions in the GPT models’\nprompt construction (see Section 4.2 for details).\n/three.tnum./three.tnum PHASYS statutes and regulations\nanalysis\nAt the University of Pittsburgh’s Graduate School of Public\nHealth, researchers have manually coded state and local laws\nand regulations related to emergency preparedness and response\nof the public health system (PHS). They use the codes to\ngenerate, analyze and compare network diagrams representing\nvarious functional features of states’ regulatory frameworks for\npublic health emergency preparedness. As described more fully\nin\nSweeney et al. (2013), they retrieved candidate sets of relevant\nstatutes and regulations from a full-text legal information service\nand identiﬁed relevant spans of text. They then coded the relevant\nspans as per instructions in the codebook,\n/one.tnum/zero.tnumrepresenting relevant\nfeatures of those spans as sequences of numerical codes. In this\nwork we focus on one speciﬁc dimension of that code, namely\nthe purpose of the legal provision in terms of the following\nthree categories:\n• Emergency Preparedness —An eﬀort to plan for a\ndisaster/emergency before it occurs (also “emergency\nreadiness”).\n• Emergency Response —An eﬀort to lessen the impact of a\ndisaster/emergency after it occurs.\n• Emergency Recovery —An eﬀort to respond to the impact of a\ndisaster/emergency after it has ended in an attempt to return\nto the state of normalcy.\n/nine.tnum The Atticus Project: Atticus Commercial Contract Labels. Ava ilable\nonline at: https://www.atticusprojectai.org/atticus-labels (accessed February\n/nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\n/one.tnum/zero.tnum PHASYS ARM /two.tnum—LEIP Code Book. Available online at:\nhttps://\nwww.phasys.pitt.edu/pdf/Code_Book_Numerical_Deﬁntions.pdf (accessed\nFebruary /nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nTABLE /three.tnumPHASYS dataset properties.\nCount Min.\nlen.\nMean\nlen.\nMax.\nlen.\nEmergency preparedness 450 22 110.9 1,062\nEmergency response 1,231 22 145.7 6,390\nEmergency recovery 291 29 164.1 3,459\nOverall 1,972 22 140.4 6,390\nThe ﬁgure shows basic properties of the documents, i.e., statutory provis ions, for each of the\nsemantic types. The lengths (min, mean, max) are measured in word s.\nCategorizing states’ emergency-related statutory provisions by\npurpose facilitated cross-state comparisons.\nFollowing the approach described in Savelka et al. (2014), the\nstatutory and regulatory texts were automatically divided into text\nunits which are often non-contiguous spans of text which may be\nreferenced with citations. A citation is understood as a unique path\nthrough a tree representing a structure of a document. Each text\nunit contains pieces of texts that can be found at each node of\nsuch path.\nFigure 1 (right) shows the distribution of the provision types\naccording to their purpose. The Response category is clearly\ndominating the distribution with 1,231 occurrences (62.4%).\nThis problematic class imbalance creates interesting issues and\nquestions that warrant further investigations (Section 6). Basic\nproperties of the documents (i.e., statutory provisions) are\nprovided in\nTable 3. The provisions are 140.4 words long on\naverage, with the longest sentence having 6,390 words. The\ncodebook mentioned above provides short (i.e., one sentence\nlong) deﬁnitions for each of the three types. These deﬁnitions\nroughly correspond to those provided above. We worked with\nthese deﬁnitions in constructing the prompt for the GPT models\n(Section 4.2).\n/four.tnum Experiments\nWe used several systems and experimental setups as\nbaselines to the GPT models applied in the zero-shot\nsettings. We describe the models used as baselines, the\nevaluated GPT models, and the experimental settings in the\nsubsections below.\n/four.tnum./one.tnum Models\n/four.tnum./one.tnum./one.tnum Random forest\nA random forest (\nHo, 1995 ) is an ensemble classiﬁer that\nﬁts a number of decision trees on sub-samples of the data set.\nIt can be understood as a team of experts (the decision trees)\neach examining diﬀerent pieces of the data (sub-samples). After\nall experts have analyzed their pieces, they come together and\nmake a ﬁnal decision through averaging. This approach helps\nto not only improve the predictive accuracy but also to prevent\noverﬁtting—a common pitfall where a model performs well on\nthe training data but fails to generalize to unseen data. As\nan implementation of random forest we used the scikit-learn’s\nRandom Forest Classiﬁer module.\n/one.tnum/one.tnum\nIncluding random forest in our experiments serves to compare\nthe GPT models to a well-regarded traditional ML technique. Note\nthat the random forest model still does not capture semantics as\nmore advanced models do. Also note, that random forest as a\nsupervised ML model requires training data which is in contrast\nto the evaluated GPT models that do not require any task speciﬁc\ntraining data in the zero-shot settings.\n/four.tnum./one.tnum./two.tnum RoBERTa\nBERT (bidirectional encoder representation from\ntransformers) (\nDevlin et al., 2018 ), based on the transformer\narchitecture from Vaswani et al. (2017), has gained immense\npopularity. A large number of models using similar architectures\nhave been proposed, e.g., RoBERTa (\nLiu et al., 2019 ), ALBERT ( Lan\net al., 2019 ), or T5 ( Raﬀel et al., 2019 ). The core capability of these\nmodels is their ﬁne-tuning on a downstream task. The original\nmodel is typically trained on large corpora of general language\nresources, such as Wikipedia or book corpora, to perform weakly\nsupervised tasks such as masked token prediction or the next\nsentence prediction. For a downstream task one typically adds to\nthe core model a small layer that handles, e.g., the classiﬁcation into\nspeciﬁc classes, such as in this work. Using a task speciﬁc data set,\nthe augmented model is then further trained (ﬁne-tuned) starting\nfrom the parameters optimized during the pre-training phase.\nIn this work, we use RoBERTa (a robustly optimized\nBERT pre-training approach) described in\nLiu et al. (2019)./one.tnum/two.tnum\nOut of the available models, we chose to work with the\nsmaller roberta-basemodel that has 125 million parameters.\nRoBERTa is using the same architecture as BERT. However, the\nauthors of\nLiu et al. (2019) conducted a replication study of BERT\npre-training and found that BERT was signiﬁcantly undertrained.\nThey used the insights thus gained to propose a better pre-training\nprocedure. Their modiﬁcations include longer training with bigger\nbatches and more data, removal of the next sentence prediction\nobjective, training on longer sequences on average (still limited\nto 512 tokens), and dynamic changing of the masking pattern\napplied to the training data (\nLiu et al., 2019 ). Note that there\nare other models that would presumably perform better than the\nroberta-basemodel used in this work. For example, the larger\nmodels would likely achieve better performance. Additionally,\nthere are models that have been pre-trained on legal texts such as\nthose presented in\nChalkidis et al. (2020) or Zheng et al. (2021).\nHowever, this paper is not about matching or out-performing the\nstate-of-the-art. This paper is about showing that the modern GPT\nmodels can perform reasonably well in zero-shot settings. Hence,\nthe widely used ﬁne-tuned roberta-baseis used as an upper-\nbound baseline. Note that the ﬁne-tuning step requires task speciﬁc\nannotated data.\n/one.tnum/one.tnum Scikit learn: Random Forest Classiﬁer. Available online at:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.\nRandomForestClassiﬁer.html (accessed February /five.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum)\n/one.tnum/two.tnumgithub.com/pytorch/fairseq/tree/master/examples/roberta\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\n/four.tnum./one.tnum./three.tnum Generative pre-trained transformers (GPT)\nThe original GPT model ( Radford et al., 2018 ) is a 12-\nlayer decoder-only transformer with masked self-attention heads.\nIts core capability is likewise the ﬁne-tuning on a downstream\ntask. The GPT-2 model (\nRadford et al., 2019 ) largely follows the\ndetails of the original GPT model with a few modiﬁcations, such\nas layer normalization moved to the input of each sub-block,\nadditional layer-normalization after the ﬁrst self-attention block,\nand a modiﬁed initialization. Compared to the original model it\ndisplays remarkable multi-task learning capabilities (\nRadford et al.,\n2019). The next generation of GPT models ( Brown et al., 2020 )\nuses almost the same architecture as GPT-2. The only diﬀerence\nis that it uses alternating dense and locally banded sparse attention\npatterns in the layers of the transformer. The main focus of\nBrown\net al. (2020) was to study the dependence of performance on model\nsize where eight diﬀerently sized models were trained (from 125\nmillion to 175 billion parameters). The largest of the models is\ncommonly referred to as GPT-3. The interesting property of these\nmodels is that they appear to be very strong zero- and few-shot\nlearners. This ability appears to improve with the increasing size\nof the model (\nBrown et al., 2020 ). The technical details about the\nrecently released GPT-4 model have not been disclosed due to\nconcerns about potential misuses of the technology as well as a\nhighly competitive market for generative AI (\nOpenAI, 2023).\nIn our experiments, we used gpt-4(GPT-4),\ngpt-3.5-turbo(-16k)and text-davinci-003\n(both GPT-3.5). As of this writing, GPT-4 is by far the\nmost advanced model released by OpenAI. The GPT-4 and\ngpt-3.5-turbo(-16k)models are focused on dialog between\na user and a system. On the other hand, text-davinci-003\nis a more general model focused on text completion. It builds\non the previous text-davinci-002(\nOuyang et al., 2022 ).\nThat, in turn, is based on code-davinci-002. It is focused\non code-completion tasks and is sometimes referred to as codex\n(\nChen et al., 2021 )./one.tnum/three.tnum\n/four.tnum./two.tnum Experimental design\n/four.tnum./two.tnum./one.tnum Baselines\nFor the random forest classiﬁer, we split each of the three data\nsets into 10 similarly sized folds. The split was performed at the level\nof documents in which the evaluated text snippets were contained.\nAs a result all the text snippets from a particular document were\nassigned to the same fold (e.g., all the sentences from a single BV A\ndecision). This was important to safe-guard against pieces of text\nfrom a single document being included in the training set as well as\nthe test set. Within each iteration of the cross-validation, we utilized\ngrid search.\n/one.tnum/four.tnumto select the best set of hyperparameters (5-fold\ninternal cross-validation on the training set). The hyperparameter\n/one.tnum/three.tnum OpenAI: Model index for researchers. Available online at:https://\nbeta.openai.com/docs/model-index-for-researchers/instruc tgpt-models\n(accessed January /one.tnum/five.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\n/one.tnum/four.tnum Scikit learn: GridSearchCV. Available online at:\nhttps://scikit-learn.\norg/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n(accessed February /five.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum). Grid search is an automated way to search\nspace that was being considered was deﬁned over the type of n-\ngrams to be used (1-grams, {1 2}-grams, or {1 2,3}-grams), number\nof estimators (10, 100) and maximum tree depth (8, unlimited).\nWe ﬁne-tuned the base RoBERTa model for 10 epochs on\nthe training set within each of the cross-validation folds.\n/one.tnum/five.tnumWe\nemployed the same splits as in evaluating the random forest’s\nperformance. We set the sequence length to 512 and the batch size\nto 16. As optimizer we employed the Adam algorithm (\nKingma\nand Ba, 2014 ) with initial learning rate set to 4 e− 5. We stored a\nmodel’s checkpoint after the end of each training epoch. The best\nperforming checkpoint evaluated on the training set was used to\nmake the predictions on the corresponding test fold.\n/four.tnum./two.tnum./two.tnum GPT\nIn evaluating the performance of the general (not ﬁne-tuned)\nGPT models, we applied them to a batch of text snippets using\nthe openaiPython library\n/one.tnum/six.tnumwhich is a wrapper for the OpenAI’s\nREST API./one.tnum/seven.tnumIn an attempt to minimize costs, we made the batches\nas large as possible. Their size is limited by the size of the evaluated\ntext snippets that can ﬁt into the prompt (8k tokens for GPT-4,\n4k tokens for gpt-3.5-turboand text-davinci-003, and\n16k for gpt-3.5-turbo-16k), while still leaving enough room\nfor the completed predictions. For text-davinci-003we re-\nuse the baseline experiments performed in\nSavelka (2023) where\nthe batch sizes were set to 50 for the BV A decision sentences, 20 for\nthe CUAD contractual clauses, and 10 for the PHASYS statutory\nand regulatory provisions. In this work we use dynamically sized\nbatches for the GPT-4 and gpt-3.5-turbo(-16k)ﬁtting as\nmany data points into a batch as the above described prompt\nlimitations allow.\nTo generate the automatic predictions, we embed each\nbatch in the prompt templates shown in\nFigures 2–4.\nIn these ﬁgures, the {{document_type}}tokens are\nreplaced with “adjudicatory decisions” (BV A), “contracts”\n(CUAD), or “statutes and regulations” (PHASYS). We\nreplace the {{category_n_name}}tokens with the\nnames of the semantic types from the type systems and\nthe {{category_n_definition}}tokens with their\ncorresponding deﬁnitions. Finally, the tokens marked as\n{{text_snippet_n}}are replaced with the analyzed\ntext snippets. The models return the list of predicted labels\nas the response (prompt completion). We emphasize that the\nconstruction of the prompts is focused on maximizing the cost\neﬀectiveness, therefore accessibility, of the proposed approach\nwhich may somewhat limit the performance of the evaluated GPT\nmodels. This important issue is further discussed in Section 6.\nWe set the GPT parameters as follows. Temperature\ncontrols the creativeness of the output: the higher the\nfor optimal values of various parameters, known as hyperparamete rs, that\ndeﬁne the ML model architecture.\n/one.tnum/five.tnum In each epoch, the model cycles through all the training data onetime.\n/one.tnum/six.tnum GitHub: OpenAI Python Library. Available online at:\nhttps://github.com/\nopenai/openai-python (accessed February /nine.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\n/one.tnum/seven.tnum A wrapper encloses a software application to make data compatible or\nabstract away complexity.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nFIGURE /two.tnum\nThe system prompt template for GPT-/four.tnum and gpt-/three.tnum./five.tnum-turbo(-/one.tnum/six.tnumk) models. The preamble (/one.tnum) primes the model to generate semantic typepredictions.\nThe document_type token (/two.tnum) is replaced with the document type according to the data set (e.g., “adjudicatory decisions” for BVA) . The\ncategory_n_name tokens (/three.tnum) are substituted with the names of the semantic types and the category_n_deﬁnition tokens (/four.tnum) with the corresponding\ndeﬁnitions.\nFIGURE /three.tnum\nThe user prompt template for the GPT-/four.tnum and gpt-/three.tnum./five.tnum-turbo(-/one.tnum/six.tnumk) models. The text_snippet_n tokens (#) are replaced with the analy zed text snippets.\ntemperaturethe more creative the output but it can also\nbe less factual. We set it to 0.0, which corresponds to no\nrandomness. The top_pparameter is related to temperature\nand also inﬂuences creativeness of the output. We set top_pto\n1, as is recommended when temperatureis set to 0.0. The\nmax_tokensparameter controls the maximum length of the\noutput. We set it to 500 (a token roughly corresponds to a word).\nThe frequency_penaltyparameter aﬀects repetitiveness.\nWe set it to 0, which allows repetition by ensuring no penalty\nis applied to repetitions. Finally, we set the related parameter,\npresence_penalty, to 0, ensuring no penalty is applied to\ntokens appearing multiple times in the output, which happens\nfrequently in our use case.\n/four.tnum./two.tnum./three.tnum Evaluation measures\nWe use precision ( P), recall ( R), and F 1-measure (F 1), i.e., the\ntraditional information retrieval measures, to evaluate performance\nof the various systems.\n/one.tnum/eight.tnumThe performance is evaluated at the level\n/one.tnum/eight.tnum Precision is the ratio of the number of positive predictions that are\ncorrect over the total number of positive predictions. Recall is t he ratio\nof the individual text snippets for each semantic type. Therefore, Pj,\nRj, and Fj\n1 for a semantic type Tj are computed as follows:\nPj =\n|S|∑\ni= 1\nth(si) = Tj ∧ ta(si) = Tj\nta(si) = Tj\nRj =\n|S|∑\ni= 1\nth(si) = Tj ∧ ta(si) = Tj\nth(si) = Tj\nFj\n1 = 2PjRj\nPj + Rj\nS stands for the set of all text snippets in a data set (e.g., all\nsentences from BV A decisions); Tj represent a speciﬁc type (e.g.,\nFinding); th(si) stands for a human expert annotation of sentence\nsi; and ta(si) is an annotation of sentence si generated automatically.\nThe overall P, R, and F1 measures for each data set are computed at\nthe micro level.\nof positive predictions that are correct over the number of insta nces that\nwere positive. F/one.tnum is the harmonic mean of precision and recall where both\nmeasures are treated as equally important.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nFIGURE /four.tnum\nThe prompt template for text-davinci-/zero.tnum/zero.tnum/three.tnum Model. The preamble (/one.tnum) primes the model to generate semantic type predictions. The document_type\ntoken (/two.tnum) is replaced with the document type according to the data set (e.g., “adjudicatory decisions” for BVA). The category_n_na me tokens (/three.tnum) are\nsubstituted with the names of the semantic types and the cate gory_n_deﬁnition tokens (/four.tnum) with the corresponding deﬁnitions. The text_snippet_n\ntokens (/five.tnum) are replaced with the analyzed text snippets.\n/five.tnum Results\nThe experimental results of applying the GPT and the baseline\nmodels to the three tasks involving adjudicatory opinions (BV A),\ncontract clauses (CUAD) and statutory and regulatory provisions\n(PHASYS) are shown in\nTable 4. It appears that the zero-shot\nGPT models perform reasonably well. This is especially apparent\nwhen one considers that they operate solely on the basis of the\nlists of the types with one sentence long descriptions. The zero-\nshot models are even quite competitive with the supervised ML\nalgorithms trained on the in-domain data. The supervised models\nhave been trained on large portions of the available data (i.e.,\nseveral thousand examples). The GPT models have not been\ntrained on any in-domain annotated examples. The GPT-4 model\neven matches the performance of the random forest model on\nthe BV A and CUAD tasks. It does not match the performance\nof the ﬁne-tuned RoBERTa model, which is to be expected.\nIt is important to appreciate that the RoBERTa model utilized\nthousands of task-speciﬁc annotated data to reach the reported\nperformance whereas the GPT-4 model did not have access to\nany such data. The performance of the gpt-3.5-turbo(-16k)\nmodels is somewhat lower as compared to the GPT-4 model.\nThis is not surprising as GPT-4 is a much more powerful as\nwell as more expensive model. Interestingly, the cost-eﬀective\ngpt-3.5-turbomodel performs comparably to the much more\nexpensive text-davinci-003. The lower performance of the\ngpt-3.5-turbo-16kis most likely due to the large size of the\nprompt where too many data points might have been included in a\nsingle batch.\nTable 5’s three confusion matrices provide more detailed\ninformation about the performance of the GPT-4 model under\nthe zero-shot condition. Regarding the CUAD contractual clauses,\nthe system appears to have problems distinguishing only a\nsmall number of classes, such as Minimum Commitment , Proﬁt\nSharing, or Volume Restrictions . As for the BV A adjudicatory\ndecisions, the Reasoning class appears to be the most problematic.\nThe system misclassiﬁes a large number of Evidence sentences\n(654) as Reasoning. The PHASYS statutory and regulatory\nprovisions seem to present the greatest challenge. The system\nmislabels a large number of Emergency Response provisions as\nEmergency Preparedness.\n/six.tnum Discussion\nThe performance of the GPT models in the zero-shot\nsettings suggests the approach’s promising potential in semantically\nannotating short text snippets coming from legal documents.\nThe results indicate the feasibility of the zero-shot approach in\nmany existing workﬂows that rely on semantic annotation (e.g.,\nin contract review or case-law analysis). Various applications of\nthe approach (e.g., standalone, human-in-the-loop) are likely to\ndramatically lower the cost of experimenting with and performing\nsuch workﬂows. The technical barriers for operating the GPT\nmodels are relatively low—none in case of some variants such as\nChatGPT. The economic barriers are also (almost) non-existent\nwhen it comes to experimenting and relatively low when it comes\nto running the workﬂows on document sets of typical sizes\n(e.g., hundreds or lower thousands of contracts) when the batch\nprediction (as employed here) is utilized.\nIn this study, we consider the cost of the proposed approach\nas an important factor. This is because the most valuable beneﬁt\nof zero-shot LLMs could be the democratization of access to the\nsophisticated workﬂows that involve semantic annotation of legal\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nTABLE /four.tnumExperimental results.\nRandF BERT td-/zero.tnum/zero.tnum/three.tnumgpt-/three.tnum./five.tnum gpt-/one.tnum/six.tnumk gpt-/four.tnum\nP R F 1 P R F 1 P R F 1 P R F 1 P R F 1 P R F 1\nBV A 0.84 0.84 0.83 0.92 0.92 0.92 0.81 0.70 0.73 0.80 0.68 0.71 0.73 0.53 0.57 0.84 0.81 0.82\nCitation 0.99 0.98 0.99 1.0 1.0 1.0 0.98 0.94 0.96 0.95 0.90 0.92 0.86 0.40 0.55 0.97 0.88 0.92\nEvidence 0.79 0.98 0.87 0.94 0.95 0.94 0.93 0.65 0.77 0.92 0.64 0.75 0.92 0.50 0.64 0.92 0.84 0.88\nFinding 0.82 0.66 0.74 0.85 0.88 0.87 0.50 0.56 0.53 0.56 0.59 0.57 0.61 0.39 0.47 0.66 0.73 0.70\nLegal rule 0.92 0.85 0.89 0.94 0.96 0.95 0.86 0.61 0.72 0.82 0.52 0.64 0.53 0.64 0.58 0.85 0.84 0.84\nReasoning 0.70 0.27 0.40 0.74 0.70 0.72 0.30 0.72 0.43 0.29 0.76 0.42 0.24 0.82 0.37 0.44 0.63 0.52\nCUAD 0.89 0.89 0.89 0.95 0.95 0.95 0.84 0.84 0.83 0.87 0.86 0.86 0.81 0.80 0.80 0.90 0.90 0.90\nAnti-assign0. 0.91 0.97 0.94 0.99 0.98 0.99 0.81 0.94 0.87 0.93 0.93 0.93 0.92 0.88 0.90 0.92 0.95 0.93\nAudit rights 0.86 0.96 0.91 0.96 0.97 0.97 0.95 0.91 0.93 0.96 0.89 0.93 0.89 0.82 0.86 0.94 0.96 0.95\nC0. not to sue 0.97 0.81 0.88 0.97 0.94 0.96 0.73 0.71 0.72 0.77 0.83 0.80 0.65 0.81 0.72 0.94 0.91 0.93\nGoverning law 1.0 1.0 1.0 0.99 1.0 1.0 0.99 1.0 0.99 1.0 1.0 1.0 0.99 0.94 0.96 0.98 0.98 0.98\nIP assignment 0.90 0.86 0.88 0.94 0.93 0.93 0.75 0.96 0.84 0.71 0.96 0.81 0.63 0.89 0.74 0.90 0.91 0.91\nInsurance 0.94 0.97 0.95 0.97 0.97 0.97 0.97 0.95 0.96 0.98 0.95 0.97 0.96 0.87 0.92 0.96 0.98 0.97\nMin. commit. 0.82 0.79 0.80 0.92 0.93 0.92 0.68 0.67 0.67 0.82 0.66 0.73 0.71 0.60 0.65 0.82 0.79 0.80\nPost-term. S. 0.78 0.76 0.77 0.85 0.85 0.85 0.80 0.42 0.55 0.64 0.78 0.70 0.55 0.70 0.62 0.81 0.79 0.80\nProﬁt sharing 0.82 0.92 0.87 0.94 0.94 0.94 0.76 0.81 0.78 0.88 0.87 0.87 0.77 0.81 0.79 0.91 0.89 0.90\nTermination C. 0.90 0.88 0.89 0.95 0.96 0.96 0.83 0.96 0.89 0.85 0.93 0.89 0.80 0.84 0.82 0.86 0.97 0.91\nVolume rest. 0.86 0.50 0.63 0.90 0.90 0.90 0.47 0.45 0.46 0.47 0.29 0.36 0.49 0.27 0.35 0.64 0.48 0.55\nWarranty dur. 0.95 0.79 0.86 0.95 0.93 0.94 0.82 0.81 0.81 0.91 0.74 0.82 0.80 0.70 0.75 0.92 0.89 0.91\nPHASYS 0.69 0.69 0.64 0.74 0.75 0.74 0.64 0.54 0.54 0.68 0.51 0.53 0.68 0.31 0.24 0.67 0.53 0.54\nResponse 0.69 0.95 0.80 0.80 0.84 0.82 0.72 0.57 0.63 0.79 0.44 0.56 0.83 0.11 0.20 0.78 0.43 0.55\nPreparedness 0.63 0.18 0.28 0.64 0.56 0.60 0.33 0.68 0.45 0.32 0.83 0.46 0.25 0.97 0.39 0.33 0.82 0.47\nRecovery 0.82 0.40 0.53 0.65 0.63 0.64 0.77 0.17 0.28 0.77 0.36 0.49 0.71 0.09 0.16 0.76 0.49 0.60\nThe performance is reported in terms of F 1 scores. The micro-P , R, and F 1 are used for the overall data set statistics (BV A, CUAD, PHASYS rows). RandF means random forest and BERT means\nbase RoBERTa. The td-003 section reports the performance of the text- davinci-003 model. The gpt-3.5 and gpt-16k refer to the performance of t he gpt-3.5-turbo(-16k) models, respectively. The\ngpt-4 column reports the performance of the most powerful GPT-4 model. The bold v alues describe the overall performance of the models on the datasets.\ndocuments. Such workﬂows are currently accessible to only a\nsmall group of elite legal operations due to their requirements\non technical expertise and workload scale. Performing semantic\nannotation via a zero-shot LLM will be far less expensive than\nmanual annotation or the use of existing enterprise solutions based\non the supervised ML paradigm. Nevertheless, it may still incur\nnon-negligible cost.\nFor the sake of the analysis here, let us consider the current\npricing scheme of text-davinci-003[cheaper than GPT-\n4 but more expensive than gpt-3.5-turbo(-16k)models]\nbased on the number of tokens submitted to and generated by\nthe model. Currently, the cost is set to $0.02 per 1,000 tokens\nin the prompt, including the completion.\n/one.tnum/nine.tnumAssuming we would\nlike to maximize the model performance, disregarding the cost,\nwe could analyze each data point (i.e., text snippet) individually\nutilizing the available prompt to its full potential. This could entail\nproviding more verbose deﬁnitions of the semantic types, as well\n/one.tnum/nine.tnum OpenAI: Pricing. Available online at:https://openai.com/api/pricing/\n(accessed February /one.tnum/one.tnum, /two.tnum/zero.tnum/two.tnum/three.tnum).\nas supplying larger number of examples. While this could lead to a\nbetter performance in the task of analyzing the text snippets, the\nnumber of exchanged tokens would rise dramatically across the\nthree tasks presented in this study.\nThe task of analyzing 50 adjudicatory decisions (such as in the\nBV A corpus) is fairly realistic despite the fact that, in practice, much\nlarger number of documents may be involved. Using the batch\napproach employed in this study, the cost of such analysis would\namount to no more than $9.26.\n/two.tnum/zero.tnumMaximizing the performance and\nanalyzing one data point per prompt, on the other hand, could\ncost $462.96. For CUAD the discrepancy would be $15.50 (batch)\ncompared to $309.98 (one data point) and for PHASYS it would\nbe $16.16 (batch) vs. $161.59 (one data point). Hence, assuming\ncomparable numbers of documents to those present in the data\nsets studied in this work, the batch approach incurs costs in the\nballpark of several to at most the lower tens of dollars. The approach\nanalyzing one text snippet at a time could amount to the cost of\n/two.tnum/zero.tnum The number of tokens is computed as the count of analyzed text\nsnippets times the size of the prompt (/four.tnum,/zero.tnum/nine.tnum/seven.tnum), divided by the batch size (/five.tnum/zero.tnum).\nFrontiers in\nArtiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nTABLE /five.tnumConfusion matrices of the GPT-/four.tnum model under the zero-shot learning condition.\nCUAD\nAA AR CS GL IP IN MC PT PS TC VR WD\nAnti-assignment 519 6 1 1 6 1 4 7 4 1 15 2\nAudit rights 3 519 0 2 0 3 1 20 0 0 2 3\nCvnt. not to sue 1 0 124 0 0 0 0 3 0 1 3 0\nGoverning law 1 1 0 431 1 0 0 2 3 0 0 0\nIP assignment 6 0 8 1 230 1 0 6 4 0 0 0\nInsurance 3 1 0 1 1 435 0 9 3 0 0 0\nMin. commitment 2 1 0 2 0 0 237 1 9 2 34 2\nPost-term. services 2 11 1 1 14 5 6 267 4 1 13 4\nProﬁt sharing 0 1 0 0 0 1 9 11 278 1 5 0\nTermination Cnv. 8 0 2 0 0 0 12 6 1 189 0 2\nVolume restriction 4 0 0 1 0 0 26 1 5 0 69 2\nWarranty duration 0 0 0 0 0 0 5 3 0 0 2 120\nBVA PHASYS\nCIT EVD FND LR RSN RES PRP REC\nCitation 982 1 1 25 6 Response 519 70 77\nEvidence 31 2020 26 4 107 Preparedness 666 369 72\nFinding 3 59 354 5 112 Recovery 38 6 142\nLegal rule 86 5 12 782 36\nreasoning 10 331 92 119 441\nCUAD\nAA AR CS GL IP IN MC PT PS TC VR WD\nAnti-assignment 519 6 1 1 6 1 4 7 4 1 15 2\nAudit rights 3 519 0 2 0 3 1 20 0 0 2 3\nCvnt. not to sue 1 0 124 0 0 0 0 3 0 1 3 0\nGoverning law 1 1 0 431 1 0 0 2 3 0 0 0\nIP assignment 6 0 8 1 230 1 0 6 4 0 0 0\nInsurance 3 1 0 1 1 435 0 9 3 0 0 0\nMin. commitment 2 1 0 2 0 0 237 1 9 2 34 2\nPost-term. services 2 11 1 1 14 5 6 267 4 1 13 4\nProﬁt sharing 0 1 0 0 0 1 9 11 278 1 5 0\nTermination Cnv. 8 0 2 0 0 0 12 6 1 189 0 2\nVolume restriction 4 0 0 1 0 0 26 1 5 0 69 2\nWarranty duration 0 0 0 0 0 0 5 3 0 0 2 120\nBVA PHASYS\nCIT EVD FND LR RSN RES PRP REC\nCitation 982 1 1 25 6 Response 519 70 77\nEvidence 31 2020 26 4 107 Preparedness 666 369 72\nFinding 3 59 354 5 112 Recovery 38 6 142\nLegal rule 86 5 12 782 36\nreasoning 10 331 92 119 441\nThe columns show the true labels as assigned by human experts, while t he rows report the predictions of the system. The detailed view into t he performance of the model on the CUAD data set\n(left) reveals that most of the types are handled rather successfully. Th e confusion matrix for the BV A data set (top-right) reveals that the Reasoning type is challenging and it is often confused\nwith other types ( Evidence, Legal Rule ). The performance on the PHASYS data set (bottom-left) suggests that a la rge number of Recovery provisions are predicted as the two other semantic\ntypes. At the same time, a large portion of the Response provisions are mislabeled as Preparedness.\nTABLE /six.tnumThe eﬀects of prediction batching.\nP R F1 Tokens Cost\nBatches of 50 data points 0.85 0.78 0.80 71,600 $1.43\nOne per prompt 0.87 0.79 0.81 897,751 $17.96\nThe table shows performance of the text-davinci-003 model in terms of Precision (P), Recall\n(R) and micro F 1 measure (F 1). The Tokens column reports the number of tokens each\napproach exchanged with the model. The Cost column estimates the cost of such exchange\ngiven the OpenAI’s pricing scheme for the text-davinci-003 model as of the publication of\nthis paper.\nseveral hundred dollars. While such a cost may still be considered\nacceptable in many use cases, it presents a signiﬁcant barrier in\nmany others.\nAfter exploring the potential cost diﬀerences between the\napproaches, we would also like to understand the diﬀerences in\nperformance. To that end we conducted a limited experiment on\nthe BV A data set that benchmarks (i) the batch approach used in\nthis study to (ii) the approach where the prompt is kept exactly\nthe same except only one example at a time being submitted (as\nopposed to a batch of 50). The limited experiment was performed\non 10 randomly sampled BV A decisions (1,175 sentences overall).\nTable 6 reports the results of the above experiment. First of\nall, it appears clear that focusing the model on predicting just one\nsingle label as opposed to a batch of 50 labels somewhat improves\nthe performance. It is an open question whether the increase of\nF1 from 0.80 to 0.81 justiﬁes 12.6 times higher cost. The answer\nlikely depends on particular circumstances. While the absolute\nmonetary ﬁgures listed in\nTable 6 might not appear prohibitive,\na realistic analysis would likely involve a greater number of\nopinions than the 10 included in this limited experiment. There,\nthe diﬀerence in cost may determine whether the analysis is\neconomically viable or prohibitively expensive. Note that including\nan excessive number of examples in a single prompt may seriously\ndegrade the performance as evidenced by the performance of the\ngpt-3-5-turbo-16k(\nTable 4).\nPerhaps the most exciting application of the zero-shot semantic\nanalysis could be the enabling of unique workﬂows that would have\ntraditionally been performed manually or deemed not feasible. It\nis often the case that the cost of manual annotation performed\nby a human with rare and valuable expertise is not economically\nviable. Authoring of precise and verbose annotation guidelines as\nwell as training of other individuals to perform the task according\nto the guidelines might be equally expensive. Hence, an approach\nis appealing that requires no more than specifying compact (one\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nsentence long) semantic type deﬁnitions. The eﬀectiveness of an\nLLM utilizing such deﬁnitions could be evaluated, at least in\na limited way, by (i) applying the model to no more than a\nfew hundred data points, and (ii) an expert manually inspecting\nthe predictions. The appeal of this approach extends equally\nto legal scholarship, especially empirical legal studies (see, e.g.,\nGray et al., 2023 ). We (optimistically) anticipate that large scale\nexpensive projects of the past may become routine endeavors in the\nnear future.\n/seven.tnum Limitations\nWhile the results of our experiments are promising, limitations\nclearly exist. First, the performance of the models is far from perfect\nand there is still a gap between the performance of the zero-shot\nLLM compared to the performance of the ﬁne-tuned LLM systems\ntrained on thousands of example data points. Hence, in workﬂows\nwith low tolerance for inaccuracies in semantic annotation, the\nzero-shot LLM predictions may require keeping a human-expert in\nthe loop. The outcome of such human-computer interaction may\nbe a high quality data set of the size that enables ﬁne-tuning of a\npowerful domain-adapted LLM.\nThe performance of the zero-shot approach diﬀers across the\nthree data sets. While the performance on the CUAD data set\nseems very reasonable, the performance on the BV A data set suﬀers\nfrom some clear limitations. Speciﬁcally, the model struggles with\nthe Reasoning type. It mislabels many sentences of other types as\nReasoning and fails to recognize many Reasoning sentences as such\n(\nTable 5). This is consistent with the performance of the supervised\nML models. While the ﬁne-tuned base RoBERTa is clearly more\nsuccessful in handling this semantic type compared to the zero-\nshot GPT models, it still struggles (F 1 = 0.71). The random forest\nmodel under-performs the zero-shot models. Hence, the correct\nrecognition of this type may require extremely nuanced notions\nthat may be diﬃcult to acquire through a compact one-sentence\ndeﬁnition (zero-shot GPT models) or word occurrence features\n(random forest). For such situations, the proposed approach might\nnot (yet) be powerful enough and the only viable solution could be\nﬁne-tuning an LLM.\nThe performance of the zero-shot GPT models on the PHASYS\ndata set is not very satisfactory and warrants further investigation.\nFor the purpose of this study, we identify several challenges this\ndata set poses that make it diﬃcult even for the supervised ML\nmodels (\nTable 4). First of all, the data set is clearly imbalanced with\nthe Response semantic type constituting 62.4% of the available data\npoints. Second, the deﬁnitions of the semantic types appear to be\nsomewhat less clear and of lower quality than for the other two data\nsets (Section 3.3). The PHASYS annotation guidelines often list just\nthe names of the types and do not even include deﬁnitions. Hence,\nwe hypothesize that the manual annotation of this data set heavily\nrelied on the informal expertise of the human annotators, which\nwas not adequately captured in the annotation guidelines. Finally,\nthere might be the same issue as with the Reasoning type from the\nBV A data set. The ﬁne-grained distinctions between what counts as\nemergency Response as opposed to Preparedness may simply be too\nnuanced to be captured in a compact deﬁnition.\nA related limitation stems from our focus on annotation tasks\ninvolving relatively brief type deﬁnitions from non-hierarchical\ntype systems describing relatively short snippets of text. Various\nlegal domain tasks need to be performed on longer snippets of\ntext that involve more complex deﬁnitions or require drawing\nﬁner distinctions. Examples may include comparing strengths and\nweaknesses in legal arguments about speciﬁc fact situations or\nﬂagging risks inherent in certain contractual language. A more\ncomplex approach may be necessary for using GPT-based models\nwhere longer more complex documents are involved.\nThe fact that OpenAI’s GPT models are constantly changing\npresents another limitation of this work. The models we used may\nnot be available in the future. This well-known limitation aﬀects\nresearch with GPT models generally.\n/eight.tnum Conclusions\nWe evaluated several OpenAI’s GPT models on three semantic\nannotation tasks using three corpora with diverse types of\nlegal documents—adjudicatory opinions, contractual clauses, and\nstatutory and regulatory provisions. We utilized the models in the\nzero-shot settings. The models were provided with a list of compact,\none sentence long, deﬁnitions of the semantic types. The task was\nto assign a batch of short text snippets one of the deﬁned categories.\nThe results of the experiment are very promising, where the most\nsuccessful GPT-4 model achieved (micro) F 1 = 0.82 for the\nrhetorical roles of sentences from adjudicatory decisions, 0.90 for\nthe types of contractual clauses, and 0.54 for the purpose of public-\nhealth system’s emergency response and preparedness statutory and\nregulatory provisions.\nWe compared the approach of batch annotation to annotating\none data point at a time in terms of the accuracy of the predictions\nas well as their cost. While analyzing one data point (i.e., text\nsnippet) at a time yields slightly improved performance, the\nimprovements are oﬀset by much higher cost of performing the\nanalysis (more than 10 × ). Our ﬁndings are important for legal\nprofessionals, educators and scholars who intend to leverage the\ncapabilities of state-of-the-art LLMs to lower the cost of existing\nhigh-volume workloads, involving semantic annotation of legal\ndocuments, or to unlock novel workﬂows that would have not been\neconomically feasible if performed manually or using supervised\nML. The zero-shot capabilities of these LLMs appear to have\npotential to democratize access to the sophisticated work that\ntraditionally has been reserved for only a small group of elite legal\noperations and their clients, at least for the kinds of legal tasks\naddressed here.\n/nine.tnum Future work\nWhile our study of LLMs’ performance on semantic annotation\nof short text snippets coming from diverse types of legal documents\nyielded valuable insights, it is subject to limitations (Section 7) and\nleaves much room for improvement. Hence, we suggest several\ndirections for future work:\n• Augmenting the semantic type deﬁnitions with examples\nshould result in improved performance. This warrants further\ninvestigation.\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\n• When employing batch prediction, that is, analyzing multiple\ntext snippets in a single prompt, the ordering of the snippets\nmay be of importance. In our study, we use random ordering.\nUnderstanding the eﬀects of utilizing a particular kind of\nordering, for example, the one in which the snippets appear\nin a document, could yield interesting insights.\n• Label imbalance and/or nuanced deﬁnitions of semantic types,\nsuch as those encountered in the PHASYS data set, seem\nto present a formidable challenge for the zero-shot LLMs to\nsemantically annotate legal documents.\n• We also envision that the approach could be successfully\ncombined with high-speed similarity annotation\nframeworks (\nWestermann et al., 2019 , 2020) to enable\nhighly cost eﬃcient annotation in situations where resources\nare scarce.\nData availability statement\nThe BV A dataset is available online at https://github.com/\nLLTLab/VetClaims-JSON (accessed February 9, 2023). The CUAD\ndataset is available online at https://www.atticusprojectai.org/cuad\n(accessed February 9, 2023). For information concerning the\navailability of the PHASYS dataset, contact information may\nbe found at\nhttps://www.phasys.pitt.edu/contact.html. The data\ngenerated in this study are predicted labels for the data points.\nRequests to access the datasets should be directed to KA,\nashley@pitt.edu, or JS, jsavelka@cs.cmu.edu.\nAuthor contributions\nJS: Writing—original draft, Writing—review & editing. KA:\nWriting—review & editing.\nFunding\nThe author(s) declare that no ﬁnancial support was\nreceived for the research, authorship, and/or publication of this\narticle.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nBhattacharya, P., Paul, S., Ghosh, K., Ghosh, S., and Wyner, A . (2019).\n“Identiﬁcation of rhetorical roles of sentences in Indian leg al judgments, ” in JURIX\n2019, Vol. 322 (Amsterdam: IOS Press), 3.\nBiagioli, C., Francesconi, E., Passerini, A., Montemagni, S ., and Soria, C. (2005).\n“Automatic semantics extraction in law documents, ” in Proceedings of the 10th\nInternational Conference on Artiﬁcial Intelligence and Law (Bologna), 133–140.\nBlair-Stanek, A., Holzenberger, N., and Van Durme, B. (2023). Can gpt-\n3 perform statutory reasoning? arXiv [preprint]. doi: 10.1145/3594536.359\n5163\nBoella, G., Di Caro, L., Lesmo, L., and Rispoli, D. (2012). “Multi-la bel classiﬁcation\nof legislative text into eurovoc, ” in Legal Knowledge and Information Systems: JURIX\n2012: the Twenty-ﬁfth Annual Conference, Vol. 250 (Amsterdam: IOS Press), 21.\nBommarito, J., Bommarito, M., Katz, D. M., and Katz, J. (2023 ). Gpt as\nknowledge worker: a zero-shot evaluation of (ai)cpa capabilitie s. arXiv [preprint].\ndoi: 10.2139/ssrn.4322372\nBoniol, P., Panagopoulos, G., Xypolopoulos, C., El Hamdani, R., Amar iles, D. R.,\nand Vazirgiannis, M. (2020). “Performance in the courtroom : automated processing\nand visualization of appeal court decisions in France, ” in Proceedings of the Natural\nLegal Language Processing Workshop .\nBranting, K., Weiss, B., Brown, B., Pfeifer, C., Chakrabort y, A., Ferro, L., et al. (2019).\n“Semi-supervised methods for explainable legal prediction, ” in ICAIL (Montreal, QC),\n22–31.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhar iwal, P., et al. (2020).\nLanguage models are few-shot learners. Adv. Neural Inf. Process. Syst . 33, 1877–1901.\ndoi: 10.5555/3495724.3495883\nChalkidis, I., Androutsopoulos, I., and Michos, A. (2017). “Ex tracting contract\nelements, ” inProceedings of the 16th edition of the International Conference on Artici al\nIntelligence and Law (London), 19–28.\nChalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N. , and Androutsopoulos,\nI. (2020). “Legal-bert: the muppets straight out of law school, ” i n Findings of the\nAssociation for Computational Linguistics: EMNLP , 2898–2904.\nChalkidis, I., Fergadiotis, M., Malakasiotis, P., and Androu tsopoulos, I. (2021).\nNeural contract element extraction revisited: letters from s esame street. arXiv\n[preprint]. doi: 10.48550/arXiv.2101.04355\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. O., Kaplan, J., et al. (2021).\nEvaluating Large Language Models Trained on Code .\nde Maat, E., Krabben, K., and Winkels, R. (2010). “Machine lear ning versus\nknowledge based classiﬁcation of legal texts, ” in Legal Knowledge and Information\nSystems (Amsterdam: IOS Press), 87–96.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). B ert: pre-training\nof deep bidirectional transformers for language understand ing. arXiv [preprint].\ndoi: 10.48550/arXiv.1810.04805\nFarzindar, A., and Lapalme, G. (2004). “Letsum, an automatic te xt summarization\nsystem in law ﬁeld, ” in JURIX (Amsterdam).\nFrancesconi, E., Montemagni, S., Peters, W., and Tiscornia , D. (2010). Integrating\na Bottom-Up and Top-Down Methodology for Building Semantic Resource s for the\nMultilingual Legal Domain . Berlin: Springer.\nGray, M., Savelka, J., Wesley, O., and Ashley, K. D. (2023). “Auto matic identiﬁcation\nand empirical analysis of legally relevant factors, ” in ICAIL (Braga), 101–110.\nHamilton, S. (2023). Blind judgement: agent-based supreme cou rt modelling with\ngpt. arXiv [preprint]. doi: 10.48550/arXiv.2301.05327\nHarasta, J., Savelka, J., Kasl, F., and Míšek, J. (2019). Automatic Segmentation of\nCzech Court Decisions Into Multi-paragraph Parts . Bern: Jusletter IT.\nHendrycks, D., Burns, C., Chen, A., and Ball, S. (2021). “Cuad: an expert-annotated\nnlp dataset for legal contract review, ” in 35th Conference on Neural Information\nProcessing Systems (NeurIPS 2021) Track on Datasets and Benchmarks .\nHo, T. K. (1995). “Random decision forests, ” in Proceedings of 3rd International\nConference on Document Analysis and Recognition, Vol. 1 (Montreal, QC: IEEE),\n278–282.\nKatz, D. M., Bommarito, M. J., Gao, S., and Arredondo, P. (202 3). Gpt-4 Passes the\nBar Exam .\nFrontiers in Artiﬁcial Intelligence /one.tnum/three.tnum frontiersin.org\nSavelka and Ashley /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/seven.tnum/nine.tnum/seven.tnum/nine.tnum/four.tnum\nKingma, D. P., and Ba, J. (2014). Adam: a method for stochasti c optimization. arXiv\n[preprint]. doi: 10.48550/arXiv.1412.6980\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricu t, R. (2019).\nAlbert: a lite bert for self-supervised learning of language repres entations. arXiv\n[preprint]. doi: 10.48550/arXiv.1909.11942\nLeivaditi, S., Rossi, J., and Kanoulas, E. (2020). A benchmar k for lease contract\nreview. arXiv [preprint]. doi: 10.48550/arXiv.2010.10386\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (20 19).\nRoberta: a robustly optimized bert pretraining approach. arXiv [preprint].\ndoi: 10.48550/arXiv.1907.11692\nNguyen, H.-T., Goebel, R., Toni, F., Stathis, K., and Satoh, K. (2023). How\nwell do sota legal reasoning models support abductive reasoning? arXiv [preprint].\ndoi: 10.48550/arXiv.2304.06912\nOpenAI (2023). Gpt-4 Technical Report .\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mi shkin, P., et al. (2022).\n“Training language models to follow instructions with human fee dback, ” inAdvances\nin Neural Information Processing Systems (Cambridge, MA: MIT Press).\nPerlman, A. M. (2022). The Implications of Openai’s Assistant for Legal Services and\nSociety.\nPetrova, A., Armour, J., and Lukasiewicz, T. (2020). “Extra cting outcomes from\nappellate decisions in US state courts, ” in JURIX (Amsterdam), 133.\nPoudyal, P., Savelka, J., Ieven, A., Moens, M. F., Goncalves, T., and Quaresma, P.\n(2020). “Echr: legal corpus for argument mining, ” in Proceedings of the 7th Workshop\non Argument Mining (Barcelona), 67–75.\nPouliquen, B., Steinberger, R., and Ignat, C. (2006). Automat ic annotation\nof multilingual text collections with a conceptual thesaurus. arXiv [preprint].\ndoi: 10.48550/arXiv.cs/0609059\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving\nLanguage Understanding by Generative Pre-training .\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutsk ever, I. (2019).\nLanguage Models are Unsupervised Multitask Learners .\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Mate na, M., et al. (2019).\nExploring the limits of transfer learning with a uniﬁed text-to- text transformer. arXiv\n[preprint]. doi: 10.5555/3455716.3455856\nSarkar, R., Ojha, A. K., Megaro, J., Mariano, J., Herard, V., and McCrae, J. P. (2021).\n“Few-shot and zero-shot approaches to legal text classiﬁcation : a case study in the\nﬁnancial sector, ” inProceedings of the Natural Legal Language Processing Workshop 2021\n(Punta Cana: Association for Computational Linguistics), 1 02–106.\nSavelka, J. (2023). Unlocking practical applications in legal doma in: evaluation\nof gpt for zero-shot semantic annotation of legal texts. arXiv [preprint].\ndoi: 10.1145/3594536.3595161\nSavelka, J., and Ashley, K. D. (2017). “Using conditional rand om ﬁelds to detect\ndiﬀerent functional types of content in decisions of United S tates courts with example\napplication to sentence boundary detection, ” in Workshop on Automated Semantic\nAnalysis of Information in Legal Texts , 10.\nSavelka, J., and Ashley, K. D. (2018). “Segmenting us court dec isions into functional\nand issue speciﬁc parts, ” in JURIX (Amsterdam), 111–120.\nSavelka, J., and Ashley, K. D. (2022). Legal information retri eval for understanding\nstatutory terms. Artif. Intell. Law 30, 1–45. doi: 10.1007/s10506-021-09293-5\nSavelka, J., Ashley, K. D., Gray, M., Westermann, H., and Xu, H. ( 2023). Can gpt-4\nsupport analysis of textual data in tasks requiring highly speciali zed domain expertise?\nAutomat. Semant. Anal. Inf. Legal Text 3441, 1–12. Available online at: https://ceur-ws.\norg/Vol-3441/paper1.pdf\nSavelka, J., Grabmair, M., and Ashley, K. D. (2014). “Mining inf ormation from\nstatutory texts in multi-jurisdictional settings, ” in Legal Knowledge and Information\nSystems (Amsterdam: IOS Press), 133–142.\nSavelka, J., Walker, V. R., Grabmair, M., and Ashley, K. D. (2017) . Sentence\nboundary detection in adjudicatory decisions in the united states. Traitement Automat.\nLang. 58, 21.\nSavelka, J., Westermann, H., and Benyekhlef, K. (2020). Cross-Domain\nGeneralization and Knowledge Transfer in Transformers Trained on Leg al Data .\nCEUR WS.\nSavelka, J., Westermann, H., Benyekhlef, K., Alexander, C. S., Grant, J. C.,\nAmariles, D. R., et al. (2021). “Lex rosetta: transfer of predic tive models across\nlanguages, jurisdictions, and legal domains, ” in Proceedings of the Eighteenth\nInternational Conference on Artiﬁcial Intelligence and Law (Montreal, QC),\n129–138.\nSweeney, P. M., Bjerke, E., Potter, M., Guclu, H., Keane, C., As hley, K., et al. (2013).\n“Network analysis of manually-encoded state laws and prospects fo r automatio, ” in\nNetwork Analysis in Law , eds R. Winkels, N. Lettieri, and S. Faro, 53–77.\nTan, J., Westermann, H., and Benyekhlef, K. (2023). “Chatgpt a s an artiﬁcial\nlawyer?, ” inArtiﬁcial Intelligence for Access to Justice (AI4AJ 2023) , ed L. Karl Branting\n(Braga: CEUR WS).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inf. Process. Syst . 30, 5998–6008.\nWalker, V. R., Han, J. H., Ni, X., and Yoseda, K. (2017). “Seman tic types\nfor computational legal reasoning: propositional connectives and sentence\nroles in the veterans’ claims dataset, ” in Proceedings of the 16th Edition\nof the International Conference on Articial Intelligence and Law (London),\n217–226.\nWalker, V. R., Pillaipakkamnatt, K., Davidson, A. M., Linares, M ., and Pesce,\nD. J. (2019). “Automatic classiﬁcation of rhetorical roles fo r sentences: comparing\nrule-based scripts with machine learning, ” in ASAIL@ ICAIL 2385 (Montreal, QC).\nWang, S. H., Scardigli, A., Tang, L., Chen, W., Levkin, D., Che n, A., et al. (2023).\nMaud: an expert-annotated legal nlp dataset for merger agreeme nt understanding.\narXiv [preprint]. Available online at: https://arxiv.org/abs/2301.00876\nWestermann, H., Savelka, J., and Benyekhlef, K. (2023). “Llmed iator: Gpt-4 assisted\nonline dispute resolution, ” inArtiﬁcial Intelligence for Access to Justice (AI4AJ 2023 ), ed\nL. Karl Branting (Braga: CEUR WS).\nWestermann, H., Savelka, J., Walker, V. R., Ashley, K. D., and Be nyekhlef, K. (2019).\n“Computer-assisted creation of boolean search rules for text c lassiﬁcation in the legal\ndomain, ” inJURIX, Vol. 322 (Amsterdam: IOS Press), 123.\nWestermann, H., Savelka, J., Walker, V. R., Ashley, K. D., and Be nyekhlef, K. (2020).\n“Sentence embeddings and high-speed similarity search for fa st computer assisted\nannotation of legal documents, ” in JURIX, Vol. 334 (Amsterdam: IOS Press), 164.\nWestermann, H., Savelka, J., Walker, V. R., Ashley, K. D., and Be nyekhlef, K. (2021).\n“Data-centric machine learning: Improving model performance and understanding\nthrough dataset analysis, ” in Legal Knowledge and Information Systems (Amsterdam:\nIOS Press), 54–57.\nWinkels, R., and Hoekstra, R. (2012). “Automatic extraction of legal concepts and\ndeﬁnitions, ” inLegal Knowledge and Information Systems: JURIX 2012: the Twenty-Fi fth\nAnnual Conference, Vol. 250 (Amsterdam: IOS Press), 157.\nWyner, A., and Peters, W. (2011). “On rule extraction from reg ulations, ” in Legal\nKnowledge and Information Systems (Amsterdam: IOS Press), 113–122.\nXu, H., and Ashley, K. D. (2023). Argumentative segmentation enhancement for\nlegal summarization. Automat. Semant. Anal. Inf. Legal Text 3441, 141–150. Available\nonline at: https://ceur-ws.org/Vol-3441/paper15.pdf\nXu, H., Savelka, J., and Ashley, K. D. (2020). “Using argument m ining for legal text\nsummarization, ” inJURIX, Vol. 334 (Amsterdam: IOS Press).\nXu, H., Savelka, J., and Ashley, K. D. (2021a). “Accounting for sentence position\nand legal domain sentence embedding in learning to classify cas e sentences, ” in Legal\nKnowledge and Information Systems (Amsterdam: IOS Press), 33–42.\nXu, H., Savelka, J., and Ashley, K. D. (2021b). “Toward summari zing case\ndecisions via extracting argument issues, reasons, and con clusions, ” in Proceedings\nof the 18th International Conference on Artiﬁcial Intelligence and Law (São Paulo),\n250–254.\nYu, F., Quartey, L., and Schilder, F. (2022). Legal Prompting: Teaching a Language\nModel to Think Like a Lawyer . ACL.\nZheng, L., Guha, N., Anderson, B. R., Henderson, P., and Ho, D. E. (2021). “When\ndoes pretraining help? assessing self-supervised learning for law and the casehold\ndataset of 53,000+ legal holdings, ” in Proceedings of the Eighteenth International\nConference on Artiﬁcial Intelligence and Law (São Paulo), 159–168.\nZhong, L., Zhong, Z., Zhao, Z., Wang, S., Ashley, K. D., and Grab mair, M. (2019).\n“Automatic summarization of legal decisions using iterativ e masking of predictive\nsentences, ” inICAIL (Montreal, QC), 163–172.\nFrontiers in Artiﬁcial Intelligence /one.tnum/four.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6863577365875244
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6831819415092468
    },
    {
      "name": "Automatic summarization",
      "score": 0.6296936273574829
    },
    {
      "name": "Annotation",
      "score": 0.49321264028549194
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4470415711402893
    },
    {
      "name": "Natural language processing",
      "score": 0.41487985849380493
    },
    {
      "name": "Information retrieval",
      "score": 0.3839198350906372
    }
  ]
}