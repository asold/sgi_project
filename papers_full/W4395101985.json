{
  "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
  "url": "https://openalex.org/W4395101985",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3165879730",
      "name": "Pau Rodriguez Inserte",
      "affiliations": [
        "Lingua et Machina (France)"
      ]
    },
    {
      "id": "https://openalex.org/A3098456035",
      "name": "Mariam Nakhlé",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223204103",
      "name": "Raheel Qader",
      "affiliations": [
        "Lingua et Machina (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2771679541",
      "name": "Gaëtan Caillaut",
      "affiliations": [
        "Lingua et Machina (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2298405651",
      "name": "Jingshu Liu",
      "affiliations": [
        "Lingua et Machina (France)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3203486519",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4380352301",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2787423662",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.",
  "full_text": "Proceedings of the Sixth Workshop on Financial Technology and Natural Language Processing, pages 1–10\nNovember 1, 2023. ©2023 Association for Computational Linguistics\n1\nLarge Language Model Adaptation for Financial Sentiment Analysis\nPau Rodriguez Inserte1, Mariam Nakhlé1,2, Raheel Qader1∗\n, Gaëtan Caillaut1, Jingshu Liu1\n1Lingua Custodia, Paris, France\nfirstname.lastname@linguacustodia.com\n2Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\nAbstract\nNatural language processing (NLP) has re-\ncently gained relevance within financial institu-\ntions by providing highly valuable insights into\ncompanies and markets’ financial documents.\nHowever, the landscape of the financial domain\npresents extra challenges for NLP, due to the\ncomplexity of the texts and the use of specific\nterminology. Generalist language models tend\nto fall short in tasks specifically tailored for\nfinance, even when using large language mod-\nels (LLMs) with great natural language under-\nstanding and generative capabilities. This paper\npresents a study on LLM adaptation methods\ntargeted at the financial domain and with high\nemphasis on financial sentiment analysis. To\nthis purpose, two foundation models with less\nthan 1.5B parameters have been adapted using a\nwide range of strategies. We show that through\ncareful fine-tuning on both financial documents\nand instructions, these foundation models can\nbe adapted to the target domain. Moreover, we\nobserve that small LLMs have comparable per-\nformance to larger scale models, while being\nmore efficient in terms of parameters and data.\nIn addition to the models, we show how to gen-\nerate artificial instructions through LLMs to\naugment the number of samples of the instruc-\ntion dataset.\n1 Introduction\nNatural Language Processing (NLP) has become\nan increasingly important field in the financial in-\ndustry, with applications ranging from sentiment\nanalysis and named entity recognition to question\nanswering. Information retrieved using machine\nlearning from financial reports, news or posts in so-\ncial media can be used as indicators of companies’\nperformance or as insights of a market. Many in-\ndustry actors are interested in extracting this infor-\nmation to use it as a resource that can provide them\n∗Corresponding author.\nwith a competitive advantage, such as firms fore-\ncasting internal future benefits and losses, investors\nextracting differential information for trading pur-\nposes or any practitioner interested in tracking fi-\nnancial assets. Nevertheless, some characteristics\nof financial text make these tasks especially chal-\nlenging for models that have been trained on gen-\neral domain data. The use of specific terminology\nalong with the high complexity of the documents,\nleads these generalist language models to underper-\nform on financial tasks, which suggests that domain\nadaptation might be required to improve accuracy\nof interpretation and analysis.\nFurthermore, the rapid evolution of large lan-\nguage models (LLMs) and their proven capabilities\nfor NLP tasks has made them stand out and become\nan interesting option to study. Due to the fact that\neven the best general language models fall short\nfor some financial tasks, some proposals have been\nrecently presented for a financial domain adapta-\ntion of LLMs. These models tailored for finance,\nsuch as BloombergGPT (Wu et al., 2023), have\nbeen introduced as multitasking generative mod-\nels specifically designed for financial text under-\nstanding and generation. However, these fine-tuned\nmodels still show room for improvement, both in\nperformance and in the efficiency of the proposed\ntraining strategies.\nThis paper tackles various aspects of adapting\nLLMs to the financial domain. In particular, we\nexplore diverse strategies of domain adaptation and\nfine-tuning of LLMs for financial sentiment analy-\nsis, and conduct a series of experiments over two\ndifferent foundation models. The study focuses par-\nticularly on smaller manageable models, up to 1.5B\nparameters, in order to explore the possibilities of\nmodels that can be accessible with relatively low\nhardware requirements. Although the adapted mod-\nels are smaller than the current state-of-the-art ones,\nresults show that they achieve similar or higher per-\nformance. In addition, a curated data collection\n2\nwith two main datasets is also presented. One con-\nstructed with financial documents and reports, and\nthe other a set of instructions for financial tasks.\nWe show, step by step, the process of creating these\ndatasets and particularly focus on the use of more\npowerful LLMs to generate synthetic instructions\nto fine-tune smaller LLMs. Finally, apart from the\nmain focus of the study which is on financial senti-\nment analysis, other tasks have also been evaluated\nto analyze the multitasking capabilities of our mod-\nels.\n2 Related work\nSentiment analysis is one of the most common use\ncases of NLP. In this task, a model classifies a\ntext according to the sentiment detected, usually\nbetween positive, negative and neutral. However,\nwhile any sentiment analysis model would be ca-\npable of undertaking financial sentiment analysis,\nan adaptation is required. In this section, the evo-\nlution of sentiment analysis in the financial do-\nmain is studied using models based on Transform-\ners (Vaswani et al., 2023).\nFinBERT1 (Araci, 2019) is based on the idea\nof training a BERT (Devlin et al., 2018) model in\ntwo steps to adapt it to the financial domain and\nthe sentiment analysis task. The first step consists\nof further pre-training the model on financial doc-\numents, as this strategy has already be proven to\nbe effective (Howard and Ruder, 2018) for domain\nadaptation. This step aims at helping the model\nto understand financial terminologies better than\nthe base model. Authors used a subset of Reuters’\nTRC24 dataset2, a collection of news articles pub-\nlished by Reuters that was filtered with keywords\nrelated to finance, to fine-tune the model. In the sec-\nond step, the model is prepared for the sentiment\nanalysis task by adding a dense layer to the last\nhidden state of the classification token CLS of the\nencoder-based architecture, a recommended prac-\ntice for classification with BERT. This task is fine-\ntuned using the Financial PhraseBank (FPB) (Malo\net al., 2014), a financial sentiment analysis dataset.\nFinBERT presents remarkable results on financial\nsentiment analysis, outperforming the state-of-the-\nart. Nevertheless, the model is strongly limited to\nsentiment analysis and underperforms greatly on\nother tasks.\n1Several models under the name of FinBERT exist, how-\never in this work we only discuss the first model.\n2https://trec.nist.gov/data/reuters/reuters.\nhtml\n2.1 Base large language models\nRecent advances in the field of large language\nmodels (LLMs) have shown that these models can\nachieve remarkable capabilities in understanding\ncomplex natural language. They are also capa-\nble of performing zero-shot and few-shot learning,\nin which they can generate accurate responses for\ntasks that they have not seen during training (Rad-\nford et al., 2019). This makes LLMs a great choice\nin multitask settings where one model is expected\nto perform several tasks. Most of today’s LLMs\nare based on Transformer models (Vaswani et al.,\n2023), typically set in decoder-only architectures.\nTraining of LLMs is typically split in two stages.\nThe first part of the training is the most computa-\ntionally expensive since the model is trained using\nlarge amounts of text. For this reason, conducting\nthe training of a LLM from scratch requires high\ncomputational resources. Nevertheless, many re-\nsearch groups and companies are releasing these\nmodels to the public to be used as base or founda-\ntion for other models, to enable research to move\nforward. Using these pre-trained models is highly\nbeneficial for researchers with fewer data or hard-\nware resources, as they can be used as a starting\npoint for fine-tuning on specific tasks, such as chat-\nting, following instructions or giving outputs in a\nspecific style or format. Although most LLMs are\ntrained on general domain data, there have been a\nfew works recently to adapt LLMs to the financial\ndomain. In the next subsections two such work are\nreviewed.\n2.2 Financial large language models\nBloombergGPT. One of the first decoder-\nonly LLMs trained specifically for finance is\nBloombergGPT (Wu et al., 2023), a model of 50B\nparameters based on BLOOM’s architecture (Scao\net al., 2023). The corpus collected for the training\nof this LLM consisted in the combination of 363\nbillion tokens from financial documents with 345\nbillion tokens from general purpose datasets. The\nmodel was trained from scratch, without using any\nfoundation model as a base, with the objective of\npredicting the next token of the documents, and\nwithout fine-tuning on instructions. However, the\nresults presented by BloombergGPT are far from\nthe ones achieved by other models, some of them\nof a much smaller scale. In addition, the results\nreported did not outperform other generalist LLMs,\nas we will show later in this paper.\n3\nFinMA. The open model FinMA from PIXIU’s\nframework (Xie et al., 2023) introduced by Chance-\nFocus reported better scores on several financial\ntasks than larger generalist LLMs, such as GPT-\n4 (OpenAI, 2023), and BloombergGPT. They used\nLLaMA (Touvron et al., 2023a) as the pre-trained\nmodel and fine-tuned it with instructions tailored\nfor financial multitasking. The instruction dataset\nconsists of texts formed by an instruction, an in-\nput and an answer. The dataset includes a data\naugmentation strategy in which the inputs of those\ntasks with few samples were used with 10 differ-\nent instructions. This augmentation strategy, while\nincreasing the number of samples in the dataset,\ndid not increase its diversity as the same set of 10\ninstructions were always repeated.\nIn the same paper in which FinMA was\npresented, the PIXIU framework also included\nFLARE, a financial evaluation benchmark. This\nbenchmark has been used to evaluate the experi-\nments carried out in this project.\n2.3 Financial benchmark\nFor the evaluation of large language models, the\nFLARE benchmark3 from PIXIU framework has\nbeen used. The tasks of this benchmark which are\nrelevant to our work are presented below.\nFinancial Sentiment Analysis. Financial senti-\nment analysis task over two different benchmarks,\nthe Financial Phrase Bank (FPB) (Malo et al., 2014)\nand FIQA-SA (Maia et al., 2018).\nNews Headline Classification. Headlines task\ncontains 9 different subtasks, each one associated\nwith 9 different gold questions, in which the ex-\npected answers are “yes” or “no”. The inputs ana-\nlyzed are gold news from the Gold dataset (Sinha\nand Khandait, 2020).\nNamed Entity Recognition. NER task is based\non detecting financial named-entities in U.S. public\nagreements in the (Salinas Alvarado et al., 2015)\ndataset. The tagged entities correspond to people,\norganizations and locations.\n3 Methodology\nIn this section, we describe the methods designed\nto conduct the experiments. First, we list the foun-\ndation models that are used as a part of this project.\nThen, two new dataset collections are introduced,\none with data based on documents and the second\nwith instructions. We also give details of designing\n3https://github.com/chancefocus/PIXIU\na data augmentation strategy for the instructions\nas well as the description of the training process\ncarried out to fine-tune the foundation models.\n3.1 Foundation models\nAs stated earlier, the focus in this work is on smaller\nsized models that can be adapted to achieve perfor-\nmance of larger models. The two models that we\nuse are listed below:\nOPT. Meta AI’s large language models suite\nOPT (Open Pre-trained Transformers) (Zhang et al.,\n2022) were presented as a collection of 9 models\nranging from 125M to 175B parameters, being one\nof the first publicly available LLMs.\nPythia. EleutherAI presented Pythia (Biderman\net al., 2023), a suite of decoder-only language mod-\nels with sizes ranging from 70M to 12B parameters.\nThese models are trained on the Pile dataset (Gao\net al., 2020), a curated collection of English texts\nfrom a wide variety of sources.\n3.2 Datasets\nIn order to train LLMs, two main different ap-\nproaches can be taken with respect to data. When\na model is trained from scratch, the data used are\ncollections of documents, for which the model has\nthe objective of predicting the next token. This is\nusually the training carried out to obtain founda-\ntion LLMs. However, these models can be further\npre-trained for domain adaptation, in the same way\nthat FinBERT was trained. This approach is based\non the idea of continuing the training of the model\nwith financial documents to shift from a general\nto a financial language model. Moreover, it has\nbeen proven that large language models can im-\nprove their performances, especially on unseen (or\nzero-shot) tasks by fine-tuning them to follow in-\nstructions. For this fine-tuning method, the train-\ning objective is the same, predicting the next to-\nken of the text, with the only difference being that\nthe format of this data relies on an “instruction”,\n“input”, “answer” format. For this project, one\ndataset was collected for each of these two training\nstrategies. In addition, the instruction-based dataset\nwas augmented artificially with samples generated\nfrom another LLM (LLaMA 2 13B (Touvron et al.,\n2023b)).\nDocument dataset. The collection of docu-\nments used to further pre-train the base LLMs is\na combination of general and financial documents\nfrom different sources. The purpose of this mixture\nis to add diversity to the training data, with finance\n4\nbeing the most represented domain. Having general\ndata in the training set prevents the model to com-\npletely drift the domain and result in a model that\nis unable to understand general language. The data\nsources of these documents are described below:\n• EDGAR Files (Financial). EDGAR is the\nElectronic Data Gathering, Analysis, and Re-\ntrieval system online platform operated by the\nSEC4 (United States Securities and Exchange\nCommission). It is used by companies to elec-\ntronically file registration statements, periodic\nreports, and other forms required by the SEC.\nThe database of these documents is open to ev-\neryone, allowing the retrieval of high-quality\nfinancial text.\n• Reuters News (Financial). Reuters is a news\nagency specializing in business and finance\nthat released Reuters Corpora, a collection of\nfinancial news made available for use in NLP\nresearch. The collection used in this dataset is\nTRC2 (Thomson Reuters Text Research Col-\nlection), that contains more than 1.8 million\nnews.\n• In-house Dataset (Financial). As a part of\nthis project, a diverse collection of in-house\nfinancial text has been obtained. The text in\nthis dataset is mostly at sentence level, as they\nwere originally used for machine translation.\nThis is the only private set used for the project.\n• The Pile (General). The Pile (Gao et al.,\n2020), from EleutherAI, is a dataset that com-\nprises 22 diverse high-quality subsets, several\nof which originate from academic or profes-\nsional sources. The idea behind this dataset’s\nconstruction is that diversity enhances gen-\neral cross-domain knowledge and downstream\ngeneralization capability of large language\nmodels. It includes data from general news\nto scientific articles, code, etc. . . The propor-\ntions of these subsets are kept as-is in the sub-\nsample used for this project.\nThe lengths of the documents of this dataset had\nto be adapted to the models’ context length, which\ncorresponds to the longest sequence of tokens that\nthe model can support. In this project the context\nwas limited to 2048 tokens. The pre-processing of\n4https://www.sec.gov/edgar/searchedgar/\ncompanysearch\nthe dataset consisted in concatenation of all docu-\nments from the same source, using a special token\n(<endoftext>) to separate them. The long concate-\nnated text is then sliced in blocks of 2048 tokens,\nwhich are mixed and shuffled with all the other\nblocks of the dataset. Since some datasets used\nin this project are extremely large, we decided to\ntake a smaller proportion from each one. The sum-\nmary of the ratio used for each partition is shown\nin Table 1.\nSubset Domain # Tokens %\nEDGAR Finance 100k 25.7\nReuters Finance 36k 9.3\nIn-house Finance 38k 9.7\nThe Pile General 215k 55.3\nTotal 389k 100\nTable 1: Proportion and absolute number of tokens taken\nfrom each dataset.\nInstruction-based dataset. Instruction fine-\ntuning is a strategy used to improve LLMs’ perfor-\nmance for specific tasks by teaching them to follow\nspecific format of questions and answers. LLMs\nlearn by being trained on this specific format of\ntext, while keeping the same training objective, pre-\ndicting the next sequence of tokens. Fine-tuning on\ninstructions is the most common technique to adapt\nfoundation models to specific use-cases, mainly be-\ncause this method not only improves performance\non the trained tasks, but also augments zero-shot\nand few-shot capabilities. Models trained on in-\nstructions are usually consistent in the format in\nwhich data is presented. In Table 2 the format used\nfor our dataset is displayed.\nTemplate\n### Instruction: Description of the task\n### Input: Input to analyze\n### Answer: Answer to predict\nTable 2: Template for instructions.\nTo create the instruction dataset, we used the in-\nstructions dataset published by PIXIU that targeted\nthe FLARE benchmark. However, this dataset has\npoorly curated prompts and includes a suboptimal\ndata augmentation strategy. For instance, certain\nparts of the dataset have been up-sampled by using\nten different instructions over the same input. De-\nspite having more samples, the up-sampled version\n5\nSubset Instr PIXIU Augm\nFPB 4,838 48,380 6,633\nFiQA-SA 1,173 11,730 2,825\nNER 609 6,090 2,609\nHeadline 102,708 102,708 102,708\nFinQA 8,242 8,242 8,242\nConvFinQA 3,892 3,892 3,892\nBigData22 0 7,164 0\nACL18 0 27,053 0\nCIKM18 0 4,967 0\nTOTAL 121,462 220,226 126,909\nTable 3: Comparison of the instructions used for each\ntask in the original instructions dataset (Instr) with one\ninput-answer by instruction, the up-sampled version\n(PIXIU), and the dataset augmented by LLM inference\n(Augm).\nof dataset lacks diversity which may lead to poor\nperformance as discussed by Zhou et al. (2023).\nFor this reason, the dataset proposed in this project\nhas been designed from scratch, only reusing the\nunique input - answerpairs from a down-sampled\nversion of PIXIU’s dataset that includes a single\ninstruction for each input.\nInstruction data augmentation. The main idea\nbehind instruction data augmentation is to bring\nnew inputs to the dataset, so the model has more\ndiverse examples to learn from. Two different meth-\nods have been defined for generating these instruc-\ntions dependent on the target task. For sentiment\nanalysis, the model has to generate an input for a\ngiven sentiment given an example with that label.\nThis strategy has been used to augment both FPB\nand FIQA-SA subsets. In Table 7 of Appendix A\nthe template used for this task is presented.\nFor NER, since it is not a sequence classification\ntask, the inference method is different. The first\nsolution proposed was based on letting the model\ngenerates both the new sentence and its NER tags at\nthe same time, only guiding the model by including\na few examples in the prompt. However, the variety\nof the sentences generated by the model was too\nshort and the tags were incorrect, indicating that\nthe task was too hard for the model. Our solution\nwas to use existing unlabeled sentences, which re-\nduced the generative task to a tagging process. The\nsentences used for this augmentation were in-house\nfinancial sentences. Moreover, in this case the ex-\nample given to the model is fixed in order to make\nsure all types of entities are present in the prompt.\nThe format of the tagging was chosen using prompt\nengineering. In Table 8 of Appendix A, the tem-\nplate used for NER data augmentation is shown.\nThe Headline task was not augmented since it had\nenough samples, even when considering that there\nare 9 subtasks in the benchmark.\nUsing the above-mentioned two templates, new\ninputs are inferred to be added to the instructions\ndataset. The model used for the generation of these\nnew samples is LLaMA-2-13B, quantized in 4-bits\nto reduce the GPU memory required. Table 3 shows\na comparison of the number of samples targeting\neach task before and after the augmentation. The\ndecision on the number of synthetic samples gen-\nerated was taken considering the number of orig-\ninal samples. The reason behind not generating\neven a larger number of instructions is that despite\ntheir high-quality, artificial samples could intro-\nduce some noise to the dataset by generating sen-\ntences too different from the original distribution,\nintroducing erratic input-answer pairs or NER tags\nor to duplicate some inputs after several iterations.\nIn Table 3 there is a comparison between PIXIU’s\ndataset before and after down-sampling as well as\nafter the data augmentation.\n3.3 Training method\nAs stated earlier, in this work, we use two pre-\ntrained foundation models, namely Pythia-1.4B\nand OPT-1.3B, and fine-tune them in two stages as\ndetailed below:\n• Further pre-training. The models are fine-\ntuned to predict the next token of the text\nin the document-based dataset, following the\nsame idea as in FinBERT and without being\nfine-tuned on a specific task. The idea here\nis to tilt the models to become more famil-\niar with the financial domain. Both models\nare trained on a total of 389, 000 tokens in-\ntroduced in context blocks of 2, 048 tokens.\nThe models are trained for two epochs, sav-\ning 4 checkpoints at every epoch. The best\ncheckpoint is selected for each model.\n• Instruction fine-tuning. The model is in-\nstructed to perform financial tasks using the\ninstructions dataset. Since the length of these\ninstructions is generally shorter than on the\ndocument-based dataset, the context length\nis reduced to 1, 000 tokens to speed up the\ntraining. Sequences shorter than this length\nare padded with a padding token. Instructions\n6\nlonger than 1, 000 tokens are cut off. For in-\nstruction fine-tuning, models are trained for 1\nepoch.\nFor both set-ups, training is performed with\nAdamW optimizer (Loshchilov and Hutter, 2019),\na batch size of 32, and applying gradient accu-\nmulation of 4 for training efficiency. The initial\nlearning rate is set to 1e-4, while the weight decay\nis adjusted to 0.1. These values remained the same\nfor all the conducted experiments. The training of\nthese models was carried out on a H100 GPU.\n4 Results\n4.1 Classical algorithms versus LLMs for\nfinancial sentiment analysis\nPrior to conducting an evaluation of our fine-tuned\nLLMs for financial sentiment analysis, we study the\nperformance of the current state-of-the-art models\nand classical machine learning algorithms. Com-\npared to LLMs, classical algorithms do not require\na lot of computation, they could be easily trained\nand tested. For the sake of simplicity, the evaluation\nhas been carried out only on the FPB. Based on the\nresults in Table 4, the lowest score, unsurprisingly,\nis obtained by the lexicon approach. Classical ma-\nchine learning algorithms on the other hand are\nable to obtain results considerably higher than lexi-\ncon, and even match or pass LLM scores in some\ncases. Overall conclusions that can be depicted\nfrom the results can be summarized as follows:\n• The domain adaptation and training of Fin-\nBERT on this specific task, gives the model\nan advantage over general models. Compar-\ning FinMA-30B with GPT-4, it can be seen\nthat a smaller model fine-tuned for finance has\nbetter performance than a generalist one.\n• BloombergGPT was a good starting point for\nfinancial LLMs. However, its performance\non tasks like sentiment analysis is poor. One\nlikely reason is that this model has not been\nfine-tuned on instructions.\n• FinMA-30B proves the relevance of fine-\ntuning on instructions to improve performance\non financial tasks. Nevertheless, as mentioned\nbefore, the train dataset might be not suffi-\nciently diverse, which may impact the model’s\ncapability in real-world scenario.\nAlgorithm and features Accuracy\nLexicon approach\nLoughran-McDonald dictionary 0.59\nClassical ML algorithms\nSVM 0.77\nNaive Bayes 0.73\nXGB 0.80\nTransformers approach\nFinBERT 0.85\nGPT-4 0.71\nBloombergGPT -\nFinMA-30B 0.87\nTable 4: Performance of financial sentiment analysis. A\ncomparison between traditional approaches and modern\ntransformer based models.\n4.2 Financial domain adaptation\nIn this section we show the impacts of the two stage\nfine-tuning as well as improvements brought by the\nartificially augmented instruction dataset. Models\nare evaluated using a subset of the tasks proposed\nin the FLARE benchmark: FPB, FIQA-SA, Head-\nlines and NER. For the classification tasks (FPB,\nFIQA-SA and Headlines), the predictions are ob-\ntained by forcing the model to generate one of the\nexpected class label. For example, in FPB, this\nmeans to choose the next token only amongst the\nones needed to generate the labels (positive, nega-\ntive or neutral), and sticking with the most probable\nones (the highest logits). When evaluating on NER,\nthe generation is not constrained.\nThe first experiment that we carry out is to see\nthe effects of fine-tuning on documents versus in-\nstructions. Based on the results of Table 5, it is\nclear that performance of both Pythia and OPT\nmodels show similar behaviors and that fine-tuning\nbrings significant improvements over the base mod-\nels. Particularly, instruction fine-tuning improve-\nment is much higher than just further pre-training\non documents. This conclusion seems to be aligned\nwith what we observe in the literature of instruction\ntuning of other domains.\nNext, in order to evaluate the effect of augment-\ning the number of instructions using the strategy\ndesigned for this dataset, the models are compared\nafter fine-tuning with the base instructions dataset\nand with the augmented instructions dataset. The\nresults of using data augmentation are not straight-\nforward. In Table 5, it can be seen that the per-\nformance of the models augmented instructions\nis improved for the sentiment analysis tasks, but\nthe scores goes down for the other two. In the\n7\nF1 scores\nFine-tuning data FPB FIQA-SA Headlines NER\nPythia-1.4B (base) 0.20 0.29 0.16 0\nDocs 0.41 0.16 0.57 0.30\nInstr 0.82 0.73 0.93 0.59\nAugmented instr 0.82 0.79 0.90 0.56\nDocs + Augmented instr 0.84 0.83 0.97 0.69\nOPT-1.3B (base) 0.19 0.48 0.29 0\nDocs 0.13 0.58 0.39 0\nInstr 0.84 0.77 0.93 0.53\nAugmented instr 0.86 0.79 0.97 0.29\nDocs + Augmented instr 0.86 0.81 0.96 0.34\nTable 5: Comparison of Pythia-1.4B and OPT-1.3B fine-tuned with different strategies. The results reported\ncorrespond to the base models without fine-tuning ( base), models with document further pre-training ( Docs),\nmodels fine-tuned on instructions (Instr), models fine-tuned on augmented instructions dataset (Augmented instr),\nfine-tuning first with documents and then with augmented instructions (Docs + Augmented instr).\ncase of Headlines, this effect can be caused by\nthe fact that this task is the most represented in\nthe dataset and, by introducing new samples, the\nmodel is less focused on this task. For NER, the\nissue can be explained by the difference between\nthe text of the synthetic samples and the original\ntest set. As explained in previous sections, NER is\naugmented using in-house data, and even though\nthe chosen sentences were also in the financial do-\nmain, the sources are different and that might have\nintroduced errors in the predictions.\nFinally, we can test the implications of instruc-\ntion fine-tuning after further pre-training the model\nwith the financial documents. This simply means\nthat the model is fine-tuned two times. Since the\naugmented instructions proved to be better than the\noriginal instructions, this experiment is conducted\non the earlier instruction dataset. As shown in the\nlast row of Table 5, this approach seems to lead to\na higher score in every task. Therefore, the domain\nadaptation method inspired by FinBERT’s training\nstrategy, proves to be effective for decoder-only\nLLMs and not only for financial sentiment analysis,\nbut for multiple financial NLP tasks.\n4.3 Comparison with other Financial LLMs\nIn this section, the results of the best models ob-\ntained through the previous experiments (Docs +\nAugmented instr) are compared against the state-\nof-the-art LLMs for finance. As can be seen in\nTable 6, both fine-tuned Pythia-1.4B and OPT-1.3B\nover perform GPT-4 in classification tasks, which\nincludes financial sentiment analysis. This is made\npossible because of the domain adaptation con-\nducted for these two base models. For NER, which\nis a generative task, GPT-4 is still the LLM with the\nhighest score. When the models of these projects\nare compared to BloombergGPT, the biggest cur-\nrent LLM tailored for finance, it can be observed\nthat the scores obtained are much higher for clas-\nsification tasks, specially for sentiment analysis,\nand that Pythia also obtains better score for NER.\nIn terms of efficiency, these results are achieved\nwith models that have approximately 97% fewer\ntraining parameters than BloombergGPT5. In the\ncomparative with the collection of FinMA mod-\nels, the PIXIU LLMs still outperform the models\nfine-tuned with our domain adaptation strategy in\nsome tasks, specially when compared to FinMA-\n30B. However, when FinMA-7B, the model with\nthe closest size to the models presented in this\nproject, is evaluated in financial sentiment analysis\nand Headlines, it can be observed that the scores\nare almost equivalent to the fine-tuned Pythia-1.4B\nand OPT-1.3B. In this case, however, the biggest\nimprovement with respect to FinMA-7B is in terms\nof efficiency. Pythia-1.4B and OPT-1.3B have ap-\nproximately 78% fewer training parameters than\nFinMA-7B, and the number of instructions used\ngoes from 220, 226 down to 126, 909, which is\nonly a 57% of the number of samples used for\nPIXIU models.\nTherefore, from the general comparison it can\nbe seen that the models fine-tuned in this project\nover perform most LLMs in financial tasks, with\nthe only exception of FinMA models. In addition,\nthe size of the models and the training strategy\n5BloombergGPT has 50B trainable parameters. Pythia-\n1.4B and OPT-1.3B have approximately 1.5B parameters. The\namount of data used is not comparable since BloombergGPT\nwas trained from scratch.\n8\nF1 scores\nModels FPB FIQA-SA Headlines NER\nBloombergGPT 0.51 0.75 0.82 0.61\nGPT-4 0.78 - 0.86 0.83\nFinMA-7B 0.86 0.84 0.98 0.75\nFinMA-30B 0.88 0.87 0.97 0.62\nPythia-1.4B 0.84 0.83 0.97 0.69\nOPT-1.3B 0.86 0.81 0.96 0.34\nTable 6: Comparison of state-of-the-art with Pythia-1.4B and OPT-1.3B fine-tuned on documents and the augmented\ndataset. Performance of models retrieved from BloombergGPT and PIXIU papers. BloombergGPT is not a publicly\navailable model, so it is not possible to evaluate it under the same conditions as the other models. Thus, ChatGPT,\nGPT-4 and FinMA-30B are evaluated on zero-shot, BloombergGPT is only reported in a five-shot setting and its\naccuracy was not published.\nhave been proven to be more efficient than the ones\nproposed for other models.\n5 Conclusion\nThis project has covered a wide range of aspects of\nfinancial LLMs. Through a series of experiments,\nusing Pythia-1.4B and OPT-1.3B as base models,\nwe studied the adaptation of relatively small LLMs\nfor finance. The experiments we conducted first\nshow that LLMs adapted to the financial domain\nthrough further pre-training followed by instruc-\ntion fine-tuning perform better than some of the\nbest current generalist LLMs (such as GPT-4) on\nfinancial tasks. Second, it validates our training\nstrategy since our LLMs obtain higher or similar\nscores than other financial LLMs that were trained\nwith much more parameters and larger datasets.\nLowering the requirements to fine-tune LLMs\nfor this specific industry can be key for the future\nof several companies, since it can enable smaller\norganizations to host their own LLMs or, at least,\nto make them more accessible. Furthermore, it\nis worth mentioning that the models used for this\nproject as well as most datasets, except for the in-\nhouse subset (only 9.7% of the documents dataset),\nare open and publicly available. In addition to the\nfindings related to domain adaptation of LLMs for\nfinancial tasks and the models presented, a strategy\nfor the generation of samples for the instructions\ndataset is introduced. Moreover, the two datasets\nused for the project are described with enough de-\ntails to be reproduced by other researchers. Finally,\nthe paper also presented a comprehensive study\nthat delves into the state-of-the-art and the evolu-\ntion of approaches for financial sentiment analysis,\nranging from traditional dictionary-based methods\nto the more recent advancements in LLMs.\nDespite the fact that the results showed great\nperformance of the small-sized models, in further\nresearch these fine-tuning strategies could be ap-\nplied to larger models and study their impact on\ndifferent scales and domains. An interesting op-\ntion to study in the future are Low-Rank Adapters\nor LoRA (Hu et al., 2021), a method that reduces\nthe number of trainable parameters by freezing the\nfoundation model weights and injecting trainable\nrank decomposition matrices into each layer of the\nLLM.\nLimitations\nThe limitations of this work can be summarized as\nfollowing:\n• Generative capabilities: The final fine-tuned\nmodel seems to perform very well on classifi-\ncation tasks such as sentiment analysis, while\nstill lagging behind in generative ones.\n• Unseen tasks: our work concentrates on cer-\ntain tasks that have been studied in previous\nsimilar work, but for a full understanding of\nits limitations, one needs to test it on unseen\ntasks.\n• Large models: we believe that testing the\nsame strategy of multiple fine-tuning stages\nwould yield even better results with larger\nmodels such as LLaMA-2-7B or even larger\nmodels.\nReferences\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models.\n9\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization.\nMacedo Maia, Siegfried Handschuh, André Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www’18 open challenge:\nfinancial opinion mining and question answering. In\nCompanion proceedings of the the web conference\n2018, pages 1941–1942.\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\nlenius, and Pyry Takala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. Journal of the Association for Information\nScience and Technology, 65(4):782–796.\nOpenAI. 2023. Gpt-4 technical report.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Tim-\nothy Baldwin. 2015. Domain adaption of named\nentity recognition to support credit risk assessment.\nIn Proceedings of the Australasian Language Tech-\nnology Association Workshop 2015, pages 84–90,\nParramatta, Australia.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, and et al. 2023. Bloom: A 176b-\nparameter open-access multilingual language model.\nAnkur Sinha and Tanmay Khandait. 2020. Impact of\nnews on the commodity market: Dataset and results.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2023. Attention is all\nyou need.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\nBloomberggpt: A large language model for finance.\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao\nLai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023. Pixiu: A large language model, in-\nstruction data and evaluation benchmark for finance.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023. Lima: Less\nis more for alignment.\nA Instruction data augmentation\nexamples\n10\nTemplate\nWrite a sentence with a{yi} financial sentiment.\nUse the format <stc> sentence </stc>. Reuse\nterms from the example. Example: ’<stc> {xi}\n</stc>’\nExample\nWrite a sentence with a positive financial\nsentiment. Use the format <stc> sentence </stc>.\nReuse terms from the example. Example: ’<stc>\nShares of Standard Chartered ( STAN ) rose 1.2\n% in the FTSE 100 , while Royal Bank of Scotland\n( RBS ) shares rose 2 % and Barclays shares (\nBARC ) ( BCS ) were up 1.7 % . </stc>’\nTable 7: Template for sentiment analysis input gener-\nation with financial sentiment fixed and dynamic shot.\nExample given for positive input inference. {xi, yi}\nis an input-answer pair sampled from one of the two\nsubsets.\nTemplate\nIdentify the named entities that represent a\nperson (’PER’), an organization (’ORG’), or\na location (’LOC’) in a financial context.\nUse the format ’Entities: entity name, entity\ntype’.\nSentence: ’The Bank gave money to the Borrower\nto open a business in New York.’; Entities:\n’Bank, ORG | Borrower, PER | New York, LOC’\nDo the same with this sentence, identifying\n’PER’, ’ORG’, ’LOC’ entities.\nSentence: {xi}; Entities:\nExample\nIdentify the named entities that represent a\nperson (’PER’), an organization (’ORG’), or\na location (’LOC’) in a financial context.\nUse the format ’Entities: entity name, entity\ntype’.\nSentence: ’The Bank gave money to the Borrower\nto open a business in New York.’; Entities:\n’Bank, ORG | Borrower, PER | New York, LOC’\nDo the same with this sentence, identifying\n’PER’, ’ORG’, ’LOC’ entities.\nSentence: ‘350 , Wellesley , Massachusetts\n02481 doing business as \" Silicon Valley East\n\" and AKAMAI TECHNOLOGIES , INC . (\" Borrower\n\"), whose address is 201 Broadway , 4th Floor\n, Cambridge , Massachusetts 02139 provides the\nterms on which Bank will lend to Borrower and\nBorrower will repay Bank’; Entities:\nTable 8: Template for NER tags generation given a\nsentence of the financial domain.",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.6766858100891113
    },
    {
      "name": "Computer science",
      "score": 0.6746309995651245
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5270676612854004
    },
    {
      "name": "Language model",
      "score": 0.4393686354160309
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33524036407470703
    },
    {
      "name": "Psychology",
      "score": 0.06360378861427307
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}