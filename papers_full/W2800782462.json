{
  "title": "Captioning Transformer with Stacked Attention Modules",
  "url": "https://openalex.org/W2800782462",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2113012120",
      "name": "Xinxin Zhu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2122072007",
      "name": "Lixiang Li",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1479773632",
      "name": "Jing Liu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2171906865",
      "name": "Haipeng Peng",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2005698038",
      "name": "Xinxin Niu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2113012120",
      "name": "Xinxin Zhu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2122072007",
      "name": "Lixiang Li",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1479773632",
      "name": "Jing Liu",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2171906865",
      "name": "Haipeng Peng",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2005698038",
      "name": "Xinxin Niu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2463955103",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2220981600",
    "https://openalex.org/W2964018924",
    "https://openalex.org/W2552161745",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6680111315",
    "https://openalex.org/W2527569769",
    "https://openalex.org/W6687567705",
    "https://openalex.org/W2949376505",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W3103022576",
    "https://openalex.org/W1514535095"
  ],
  "abstract": "Image captioning is a challenging task. Meanwhile, it is important for the machine to understand the meaning of an image better. In recent years, the image captioning usually use the long-short-term-memory (LSTM) as the decoder to generate the sentence, and these models show excellent performance. Although the LSTM can memorize dependencies, the LSTM structure has complicated and inherently sequential across time problems. To address these issues, recent works have shown benefits of the Transformer for machine translation. Inspired by their success, we develop a Captioning Transformer (CT) model with stacked attention modules. We attempt to introduce the Transformer to the image captioning task. The CT model contains only attention modules without the dependencies of the time. It not only can memorize dependencies between the sequence but also can be trained in parallel. Moreover, we propose the multi-level supervision to make the Transformer achieve better performance. Extensive experiments are carried out on the challenging MSCOCO dataset and the proposed Captioning Transformer achieves competitive performance compared with some state-of-the-art methods.",
  "full_text": null,
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9898074269294739
    },
    {
      "name": "Transformer",
      "score": 0.8247561454772949
    },
    {
      "name": "Computer science",
      "score": 0.805675745010376
    },
    {
      "name": "Memorization",
      "score": 0.7311078906059265
    },
    {
      "name": "Sentence",
      "score": 0.5772021412849426
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5030049681663513
    },
    {
      "name": "Speech recognition",
      "score": 0.48849618434906006
    },
    {
      "name": "Natural language processing",
      "score": 0.463786244392395
    },
    {
      "name": "Machine translation",
      "score": 0.4355085492134094
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3345505893230438
    },
    {
      "name": "Voltage",
      "score": 0.13028529286384583
    },
    {
      "name": "Linguistics",
      "score": 0.1105504035949707
    },
    {
      "name": "Engineering",
      "score": 0.1054130494594574
    },
    {
      "name": "Electrical engineering",
      "score": 0.09467968344688416
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    }
  ]
}