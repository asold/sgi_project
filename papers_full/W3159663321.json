{
    "title": "ConTNet: Why not use convolution and transformer at the same time?",
    "url": "https://openalex.org/W3159663321",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2350669065",
            "name": "Yan, Haotian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1776089507",
            "name": "Li Zhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2465658467",
            "name": "Li Weijian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2376739287",
            "name": "Wang Changhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2014358805",
            "name": "Wu Ming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2035719904",
            "name": "Zhang, Chuang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2612445135",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W2963984455",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2601564443",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W2401231614",
        "https://openalex.org/W2804047946",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W2963495494",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2963855133",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2982220924",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W3109635183",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W1569098853",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W2982770724",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W2963843116",
        "https://openalex.org/W2963351448"
    ],
    "abstract": "Although convolutional networks (ConvNets) have enjoyed great success in computer vision (CV), it suffers from capturing global information crucial to dense prediction tasks such as object detection and segmentation. In this work, we innovatively propose ConTNet (ConvolutionTransformer Network), combining transformer with ConvNet architectures to provide large receptive fields. Unlike the recently-proposed transformer-based models (e.g., ViT, DeiT) that are sensitive to hyper-parameters and extremely dependent on a pile of data augmentations when trained from scratch on a midsize dataset (e.g., ImageNet1k), ConTNet can be optimized like normal ConvNets (e.g., ResNet) and preserve an outstanding robustness. It is also worth pointing that, given identical strong data augmentations, the performance improvement of ConTNet is more remarkable than that of ResNet. We present its superiority and effectiveness on image classification and downstream tasks. For example, our ConTNet achieves 81.8% top-1 accuracy on ImageNet which is the same as DeiT-B with less than 40% computational complexity. ConTNet-M also outperforms ResNet50 as the backbone of both Faster-RCNN (by 2.6%) and Mask-RCNN (by 3.2%) on COCO2017 dataset. We hope that ConTNet could serve as a useful backbone for CV tasks and bring new ideas for model design",
    "full_text": "ConTNet: Why not use convolution and transformer at the same time?\nHaotian Yan1,2* Zhe Li2* Weijian Li2 Changhu Wang2 Ming Wu1 Chuang Zhang1\n1School of AI, Beijing University of Posts and Telecommunications.\n{yanhaotian, wuming, zhangchuang}@bupt.edu.cn\n2ByteDance AI Lab, Beijing.\n{lizhe.axel, liweijian, wangchanghu}@bytedance.com\nAbstract\nAlthough convolutional networks (ConvNets) have en-\njoyed great success in computer vision (CV), it suffers\nfrom capturing global information crucial to dense predic-\ntion tasks such as object detection and segmentation. In\nthis work, we innovatively propose ConTNet (Convolution-\nTransformer Network), combining transformer with Con-\nvNet architectures to provide large receptive Ô¨Åelds. Unlike\nthe recently-proposed transformer-based models (e.g., ViT,\nDeiT) that are sensitive to hyper-parameters and extremely\ndependent on a pile of data augmentations when trained\nfrom scratch on a midsize dataset (e.g., ImageNet1k), Con-\nTNet [50] can be optimized like normal ConvNets (e.g.,\nResNet) and preserve an outstanding robustness. It is also\nworth pointing that, given identical strong data augmen-\ntations, the performance improvement of ConTNet is more\nremarkable than that of ResNet. We present its superiority\nand effectiveness on image classiÔ¨Åcation and downstream\ntasks. For example, our ConTNet achieves 81.8% top-1 ac-\ncuracy on ImageNet which is the same as DeiT-B with less\nthan 40% computational complexity. ConTNet-M also out-\nperforms ResNet50 as the backbone of both Faster-RCNN\n(by 2.6%) and Mask-RCNN (by 3.2%) on COCO2017\ndataset. We hope that ConTNet could serve as a useful\nbackbone for CV tasks and bring new ideas for model de-\nsign. The code will be released at https://github.com/yan-\nhao-tian/ConTNet.\n1. Introduction\nConvolutional networks (ConvNets) are widely used in\ncomputer vision and become a dominating method in al-\nmost all intelligent vision systems [26, 31, 43, 13]. Since\nmost landmark ConvNets mainly use 3x3 convolutions, the\nreceptive Ô¨Åeld is limited within a local neighbourhood to\ncapture local representation. However, the large receptive\n*contributed equally\nTransformer EncoderSpatial Convolution 3√ó3\nSpatial Convolution 7√ó7\n+\n√óùüê\nFullyConnected Global Pooling\nMax Pooling\nFigure 1. Illustration of the proposed ConTNet framework. Con-\nTNet contains multiple ConT blocks, which are composed of two\ntransformer encoder layers and a convolution layer.\nÔ¨Åled is of great importance to construct a contextual visual\nunderstanding especially in some downstream tasks such as\nobject detection and semantic segmentation. In order to en-\nlarge the receptive Ô¨Åeld of the ConvNet, stacking multiple\nconvolutional layers (conv layer) seems to be the consen-\nsus and induce the Ô¨Çourishing of convolutional backbones\nuseful for various downstream tasks [38, 16, 15, 22].\nIn Natural Language Processing (NLP), the very major\nissue is how to model long-range dependencies in long se-\nquences. Self-attention mechanism represented by trans-\nformer has become the foremost method and achieved state-\nof-the-art results on many NLP tasks. The success of trans-\nformer [45] strongly motivates researchers to utilize trans-\n1\narXiv:2104.13497v3  [cs.CV]  10 May 2021\nPE\nW\nH PP2\nMulti-Head AttentionFeedForwardAdd &Norm Add &Norm\nP2\nreshape reshapeinput output\nWH\nFigure 2. A Patch-wise Standard Transformer Encoder (STE) in ConTNet. PE denotes positional encoding. H and W is the height and\nwidth of the input and output image separately. P is the size of patch. P2 is the length of the input and output sequence of STE.\nformer in CV tasks [4, 11, 44]. However, these vision trans-\nformer are highly sensitive to training settings such as learn-\ning rate, number of training epochs, optimizer, data aug-\nmentation, etc.\nWe mainly concentrate on the following challenges in\nCV: (1) ConvNets is deÔ¨Åcient in large receptive Ô¨Åelds due to\nthe locality of convolution, leading to a performance degra-\ndation on downstream tasks. (2) The transformer-based vi-\nsion model requires special training settings or hundreds\nof millions of images as the pretraining dataset, which is\na practical constraint hampering the widespread adoption.\nTo overcome these challenges, we propose a novel\nConvolution-Transformer Network (ConTNet) for CV\ntasks. ConTNet is implemented by stacking multiple ConT\nblocks as shown in Figure 1. The ConT block treated the\nstandard transformer encoder (STE) as an independent com-\nponent the same as a conv layer. SpeciÔ¨Åcally, as shown in\nFigure 2 a feature-map is split into several patches of the\nsame size and each patch Ô¨Çattened to a (super) pixel se-\nquence is next sent to STE. Finally the patch embeddings\nare reshaped back to feature-maps and they are fed into the\nnext conv layer or STE.\nIn the proposed ConTNet, the STE plays a leading role in\ncapturing more contextual features, while conv layers efÔ¨Å-\nciently extract local visual information. Besides, we also\nÔ¨Ånd that embedding STE into ConvNet architectures can\nmake the network more robust. Or in other words, ConTNet\ncan be trained easily just like the most popular ResNet [15].\nWe demonstrate that ConTNet is superior to DeiT\n(transformer-based network) and ResNet (convolution-\nbased network) on ImageNet classiÔ¨Åcation. In accordance\nwith our empirical results, ConTNet can be optimized\nstraightforward like normal ConvNets, and therefore do not\nrequires as many tricks as DeiT [44] or the tremendous\namount of pretraining data used by ViT [11]. Another inter-\nesting Ô¨Ånding is that ConTNet gains more performance im-\nprovement from strong data augmentation and other train-\ning tricks than that of ResNet [15], which can be attributed\nto the overÔ¨Åtting risk of transformer architecture. Some key\nresults are listed below: Our ConT-M achieves a1.6% top-1\naccuracy over ResNet50 on ImageNet [9] dataset with al-\nmost 25% relative save of computational cost. Likewise,\nwe demonstrate that ConTNet signiÔ¨Åcantly improves dense\nprediction tasks, especially object detection and segmen-\ntation, against the most popular backbone ResNet [15].\nOur ConT-M yields around 3% improvement compared to\nResNet-50 [15] based on FCOS [42] and Mask-RCNN [14].\nIn a nutshell, this work‚Äôs contributions are threefold.\n1) To our knowledge, our proposed ConTNet is the Ô¨Årst\nexploration to build a neural network with both of the\nstandard transformer encoder (STE) and spatial convo-\nlution.\n2) In contrast to the recently trendy visual transformer,\nConTNet is much easier to optimize. In addition to\nrobustness, ConTNet performs excellently on image\nrecognition.\n3) The empirical results present that a good transfer learn-\ning performance is promised. These results suggest\nthat ConTNet provides a new conv-transformer-based\npattern to enlarge model‚Äôs receptive Ô¨Åeld.\n2. Related Work\n2.1. ConvNets\nDeep Convolutional Neural Networks (ConvNets) en-\nables the computer vision (CV) model to perform at a higher\nlevel. This decade, we have witnessed that many novel\nideas make ConvNets unprecedented powerful, generaliz-\nable, and computationally economical [15, 22, 46, 18, 25,\n26, 5, 17, 38, 23]. A succession of researchers improves the\nperformance by deepening the network and adopt the multi-\nbranch architecture that lays the foundation for modern net-\nwork design [15, 22, 41, 49, 16, 51, 40]. An alternative\nline of practitioners turns to reformulate convolution or ba-\nsic block as versatile image features [8, 56, 21, 47, 12]. One\nof these functional attempts is to incorporate self-attention\nmechanism into ConvNets, which promotes global features\nutilization of ConvNets [47, 20, 3, 33, 48, 21]. SENet [21]\nH\nW\n77 14\n14\nH\nW\nH33\nW\nSTE STE CONV\n‚ÅÑùëä2\nH /2\nFigure 3. Information Ô¨Çow in a ConT block. For the area covered by orange shadow, the blue arrow indicates that output value of each\nspatial position is computed from the entire covered area through a patch-wise STE. The red arrow is a conv layer with a kernel size of 3\nand a stride of 2, computing only the output value of center position from the entire covered area.\nmodels the interdependencies between channels according\nto the global context of feature-map. GENet [20] is a gen-\neralization of SENet, still Ô¨Årst gathering global context as a\nsignal to recalibrate the feature channel. Like CBAM [48]\nand BAM [33], multiple papers rescale both of different\nspatial positions and channels by aggregating all contex-\ntual information. Non-Local Net [47] implements a pair-\nwise pixel interaction to augment the long-range depen-\ndencies across all temporal frames and spatial positions,\nand Non-local block can introduce the self-attention mech-\nanism. However, the usage of self-attention by Non-Local\nNet has weaknesses, for instance, the Non-Local Block\nis hard-weight and insensitive to position of pixels. An-\nother series of works seek to adopt self-attention mecha-\nnism along the entire network or even replacing all spatial\nconvolution with self-attention to construct an efÔ¨Åcient fully\nattentional network [35, 19, 2, 53]. These efforts also aim\nto enhance the long-range dependencies of ConvNets.\nNevertheless, many of these attentional networks are de-\nsigned elaborately and have not shown strong applicabil-\nity for downstream tasks. Based on these observations, we\ninnovatively develop ConTNet combining standard trans-\nformer encoder (STE) together with convolution layer (conv\nlayer). The network design follows the principle of STE for\nglobal features and conv layer for local features. Such an\narchitecture provides a reasonable formula for elevating the\nnetwork‚Äôs ability to model long-range dependencies.\n2.2. Transformer\nTransformer [45] is an encoder-decoder neural network\nfor sequence-to-sequence tasks, which has achieved many\nstate-of-the-art results and further revolutionized NLP with\nthe success of BERT [10]. The recently trendy visual\ntransformer has shown that an end-to-end standard trans-\nformer can implement image classiÔ¨Åcation and other vi-\nsion tasks [4, 30, 24, 55, 57]. ViT [11] cuts the images\ninto some non-overlapping patches and encodes the patches\nset as a token sequence, whose head is attached to a learn-\nable classiÔ¨Åcation token. The performance of ViT depends\non large-scale pretrain datasets like ImageNet-21k dataset\nor JFT-300M dataset, which trumps the convolutional in-\nductive bias. DeiT [44] explores distillation to extend ViT\nto a data-efÔ¨Åcient vision transformer straightly trained on\nImageNet, but the training course of DeiT is complex and\nunstable. Transformer has also been extended to solving\ndense prediction problems or low-level tasks. For exam-\nple, DETR [4] is the Ô¨Årst work to using transformer for ob-\nject detection. DETR uses ConvNets to extract features and\nuses transformer to model the object detection as an end-\nto-end dictionary lookup problem. However, DETR is still\nvery sensitive to the hyper-parameters setting and requires\na longer training period.\nIn our practice, embedding STEs into ConvNet, which\nseems to alternately employ transformer and convolution,\ncan make transformer architecture as robust as convolution\n(see Figure 2). We combine transformer and convolution as\na ConT block, and ConTNet is formed by stacking ConT\nblocks as shown in Figure 1. ConTNet can be trained from\nscratch on ImageNet dataset and generalized to dense pre-\ndiction tasks without unusual settings as expected.\n3. ConTNet\nIn this section, we describe the proposed Convolution-\nTransformer Network (ConTNet) in details.\n3.1. Network architecture\nWe propose the novel convolution-transformer-based\nnetwork ConTNet. The structure of ConTNet is shown in\nFigure 1. ConTNet is composed of standard transformer\nencoders (STEs) and spatial convolutions that are stacked\nseemingly alternately. More precisely, our very Ô¨Årst step to-\nwards ConTNet is setting up a relatively shallow ConvNet,\nwhich has four stages processing feature-maps with differ-\nent sizes and channels. This ConvNet is then extended to\nConTNet through inserting STE between two neighboring\nconvolutional (conv) layers. Such an extension is made to\nStage Input size ConT-Ti ConT-S ConT-M ConT-B\nStage 0 224 √ó224 7 √ó7, 64, stride=2, padding=3\nStage 1 56 √ó56\nÔ£Æ\nÔ£∞\nD= 48,\nDffn = 192,\nH = 1\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 64,\nDffn = 256,\nH = 1\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 64,\nDffn = 256,\nH = 1\nÔ£π\nÔ£ª√ó2\nÔ£Æ\nÔ£∞\nD= 64,\nDffn = 256,\nH = 1\nÔ£π\nÔ£ª√ó3\nStage 2 28 √ó28\nÔ£Æ\nÔ£∞\nD= 96,\nDffn = 384,\nH = 2\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 128,\nDffn = 512,\nH = 2\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 128,\nDffn = 512,\nH = 2\nÔ£π\nÔ£ª√ó2\nÔ£Æ\nÔ£∞\nD= 128,\nDffn = 512,\nH = 2\nÔ£π\nÔ£ª√ó4\nStage 3 14 √ó14\nÔ£Æ\nÔ£∞\nD= 192,\nDffn = 768,\nH = 4\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 256,\nDffn = 1024,\nH = 4\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 256,\nDffn = 1024,\nH = 4\nÔ£π\nÔ£ª√ó2\nÔ£Æ\nÔ£∞\nD= 256,\nDffn = 1024,\nH = 4\nÔ£π\nÔ£ª√ó6\nStage 4 7 √ó7\nÔ£Æ\nÔ£∞\nD= 384,\nDffn = 768,\nH = 8\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 512,\nDffn = 1024,\nH = 8\nÔ£π\nÔ£ª√ó1\nÔ£Æ\nÔ£∞\nD= 512,\nDffn = 1024,\nH = 8\nÔ£π\nÔ£ª√ó2\nÔ£Æ\nÔ£∞\nD= 512,\nDffn = 1024,\nH = 8\nÔ£π\nÔ£ª√ó3\n7 √ó7 global average pooling, 1000-d fc, softmax\nTable 1. Detailed settings of ConTNet series. Inside the brackets, we list the hyper-parameter of each ConTBlock. D is the embedding\ndimension of MHSA, Dffn is the dimension of FFN, and H is the head number of MHSA. Outside the brackets, the number of stacked\nblocks on the stage is presented. In each stage, the last conv layer performs downsampling and increases dimension.\ncapture global features supplementary to local representa-\ntions learned by conv layers.\nTo systematically embedding STEs into ConvNet, we\ndesign a block make STE fully integrated with conv lay-\ners by grouping them in pairs. A ConT block cascades a\npair of STEs and a conv layer as shown in Figure 1. Each\nConT block comprises two STEs and one conv layer with a\nkernel size of 3 √ó3. In our implementation, the spatial size\nof split patches is set to 7 and 14 sequentially. Inspired by\nResNet, we construct a shortcut connection to implement\nthe residual learning y= f(x)+ x, which is widely utilized\nto improve the performance of ConvNets. ConTNet is still\na 4-stage-style hierarchical network because we desire it to\nbe suitable for downstream tasks, especially object detec-\ntion and segmentation. In each stage, the last conv layer of\nthe stage has a stride of 2 to conduct downsampling and in-\ncreasing channels. When trained on the ImageNet dataset,\nthe size of each stage‚Äôs feature-map is[56,28,14,7], and the\nchannel of each stage is determined by scaling the popular\nsetting [64,128,256,512] used by most ConvNets [15]. The\nhead number of multi-head self attention of STE in each\nstage is set to [1,2,4,8] to keep the channel of single-head\nattention 64. Note that all conv layers in ConTNet have a\nkernel of 3√ó3 except for the top and the bottom conv layer.\nA conv layer with a kernel of 7 √ó7 and a MaxPooling are\nplaced at the top of the network, which follows the practice\nof ResNet. And the last conv layer of the last stage has a\nkernel of 1 √ó1 to save parameters.\nTo produce architectural variants of ConTNet, we scale\nthe depth and width of ConTNet to reach different computa-\ntional budgets. Table 1 presents four architectures: ConT-Ti\n(Tiny), ConT-S (Small), ConT-M (Medium), ConT-B (Big).\nFrom ConT-Ti to ConT-B, the parameters and FLOPs are\ngradually increased, and the depth or width grows progres-\nsively.\n3.2. Patch-wise Standard Transformer Encoder\nIn this subsection, we revisit the Standard Transformer\nEncoder (STE) and present a procedure for the execution of\nits capturing long-range dependency in ConTNet.\nIn the raw application of standard Transformer [45], a\nsequence of words is received as input and Ô¨Ånally translated\ninto a new sentence. When applied to Computer Vision\n(CV) tasks, Transformer encoder has to take a 2D image\nas input. On the principle of retaining authentic STE, the\ninput 2D image x2d ‚àà RH√óW √óC is Ô¨Çattened into a se-\nquence of pixels denoted by x1d ‚ààR(H√óW )√óC , where H\nand W are height and width of the input image separately,\nand Cis the image channel. Towards a simple but powerful\nutilization of STE, we develop a method that roughly simi-\nlar to convolutional Ô¨Ålter (see Figure 2). Given a 2D-image\nx2d ‚ààRH√óW √óCin , a convolutional Ô¨Ålter aggregates spatial\ninformation across a local neighbourhood. For example, a\nkernel of shape 3 √ó3 √óCin slides over every position of\nimage to do a inner product of local window and the ker-\nnel weights. Assuming that the input x2d and the output\ny2d ‚ààRH√óW √óCout has the same spatial size, for a spatial\nposition xij;i‚àà[0,H),j‚àà[0,W), the corresponding output value\nyij;i‚àà[0,H),j‚àà[0,W) is calculated as follow:\nyij = Conv (xij ) , (1)\nConv (xij ) =\nk‚àë\na,b=‚àík\nWa,bxi+a,j+b, (2)\nwhere W ‚àà R(2k+1)√ó(2k+1)√óCin √óCout is the weight of\nconvlutional Ô¨Ålter, and a,b ‚àà[‚àík,k]. With above analy-\nsis, a pixel-to-pixel mapping is supported explicitly in con-\nvolutional Ô¨Ålter. By contrast, the STE learns a mapping\nfrom sequence to sequence, as shown in Figure 2. There-\nfore, a potential way to exploit the STE in a conv layer\nfashion is considering a sequence as a pixel. Prior to per-\nforming on an image, STE Ô¨Årst splits it into several patches\nof the same size P √óP, and treats each patch as a se-\nquence of pixels. The split images can be denoted by a\nnew tensor xp\n2d ‚ààRHp √óWp √óP2 √óC , where Hp is set to H/\npand Wp is set to W/p. Instead of that each spatial posi-\ntion of x2d is a pixel, each spatial position of xp\n2d is a se-\nquence xp\nmn;m‚àà[0,Hp),n‚àà[0,Wp) ‚ààRP2 √óC . Assuming that\nthe input xp\n2d has the same spatial size and channel as the\noutput yp\n2d, for a spatial position (m,n), the output value\nyp\nmn;m‚àà[0,Hp),n‚àà[0,Wp) ‚ààRP2 √óC is calculated as follow:\nyp\nmn = STE (xp\nmn ) . (3)\nThe operation of STE is enumerated exactly according to\n[45], which can be formulated:\nSTE (xp\nmn ) = FFN (MHSA (xp\nmn + PE)) , (4)\nwhere MHSA ()is a Multi-Head Self Attention (MHSA)\nmechanism and FFN ()is a 2-layer Feed Forward Network\n(FFN), and PE ‚ààRP2 √óC is positional encoding.\nFinally, we descirbe a principled execution of MHSA\nand FFN. Take a sequence x1d ‚ààRN√óC as input, a single-\nhead attention value is computed using:\nA= softmax\n((Wqx1d)(Wkx1d)T\n‚àöDh\n)\n(Wvx1d), (5)\nwhere Wq, Wk and Wv ‚ààRC√óDh are the learned linear\ntransformations, and Dh is typically set to D/h. D is the\nembedding dimension and his the number of head. Multi-\nhead attention value is obtained by projecting concatenated\nsingle attention values:\nAmh = [A1 ; A2 ; ...; Ah ] Wmhsa, (6)\nMHSA (x1d) = LN (Amh + x1d) , (7)\nwhere Wmhsa ‚ààRD√óC is the learned weights that aggre-\ngates multiple attention values, andLN ()is Layernorm [1],\napplied after a residual connection. The output of MHSA is\nfed into FFN:\nFFN (x1d) = LN (W2W1x1d + x1d) , (8)\nwhere W1 ‚ààRC√óDÔ¨Än and W2 ‚ààRDÔ¨Än √óC are both learned\nlinear transformations.\n3.3. Analysis and More Details\nWe have revealed that(1) the STE is adopted on each se-\nquence xp\nmn;m‚àà[0,Hp),n‚àà[0,Wp) ‚ààRP2 √óC which is Ô¨Çattened\nfrom the uniformly split patch by sliding the window on the\nseparate input image xp\n2d ‚ààRHp √óWp √óP2 √óC and (2) Con-\nTNet captures both of global and local features by Ô¨Åltering\nfeature-maps with conv layers and STEs alternately. More\ndetails about these two implementations are reported in this\nsubsection.\nUsing STE like a kernel: In ConTNet, an STE slides over\nthe image and translates each split patch into a new patch,\nwhich performs like a Ô¨Ålter with kernel size and stride both\nequal to the patch size. We Ô¨Ånd such an kernel-like op-\neration has two favourable properties. Concretely speak-\ning, the patch-wise STE is weight-shared, which has trans-\nlation equivariance and computational efÔ¨Åciency. Instead\nof a pixel-wise translation equivariance, a relatively rough\npatch-wise translation equivariance is obtained through our\noperation. For each patch, some work endeavors to model a\npixel-wise translation equivariance when performing a self-\nattention mechanism on a 2D-shape image. To alleviate\nthis issue, we choose a simple but effective method: A\nparameter-shared learned PE is added to each split patch,\nrecording each pixel‚Äôs coordinate in a patch and avoid per-\nmutation equivariance problem [45]. In contrast with a\nconv layer with a kernel of 3 √ó3 that has 9C2 parame-\nters and a computational complexity of9C2HW (assuming\nthat the input and the output are of the same channel) , an\nSTE has 2DmhsaDffn + 4D2\nmhsa + P2Dmhsa parameters\nand a computational complexity of 2DmhsaDffn HW +\n4D2\nmhsaHW+(HW) /P2. When the dimension of MHSA\nDmhsa and the input‚Äôs channel Cin are equal and the di-\nmension of FFN Dffn is equal to four times Dmhsa (prac-\ntice of [45]), the increase of parameters is 3C2 + P2C\nand the increase of computational compexity is 3C2HW +\n(HW) /P2.\nAlternately capturing features with STE and conv layer:\nConvNets are built upon multiple conv layers with an ad-\nvantage of locality as well as a lack of global features.\nTo take full advantage of Transformer to enlarge receptive\nÔ¨Åelds, we build ConT block, the basic block of ConTNet,\ncontaining two STEs and one conv layer. In each ConT\nblock, STEs Ô¨Årst capture more global features, and then a\nconv layer with a small kernel captures more local features.\nHence, ConTNet interweaves the features alternately cap-\ntured by STEs and conv layers via stacking ConT blocks.\nOwing to the patch-wise operation on the STE, one pixel\nhas interactions with all pixels in its patch, and we hypoth-\nesize the size of the split patch can be adjusted Ô¨Çexibly to\nmodel kernels of different receptive Ô¨Åelds. We introduce\na dynamic setting of patch size rather than a Ô¨Åxed version\nalong the entire network. The default patch size of the Ô¨Årst\nand the second STE is set to 7 and 14 separately, and the\nkernel size of conv layer is set to 3 (see Figure 3). An iden-\ntity shortcut is connected between the input and output of\nthe ConT block by doing an element-wise addition. To con-\nduct downsampling and dimension changing, the conv layer\nof the last ConT block in each stage has a stride of 2 and in-\ncreases dimension. In this special case, a projection shortcut\nreplaces the identity shortcut, using a 1 √ó1 conv layer with\na stride of 2 to match the increased dimension and smaller\nsize for element-wise addition.\n4. Experiments\nIn this section, we conduct extensive experiments on im-\nage classiÔ¨Åcation and downstream tasks to evaluate the ef-\nfectiveness of our ConTNet.\n4.1. Image ClassiÔ¨Åcation\nWe compare ConTNet to advanced ConvNets\n(ResNet [15]) and transformer-based backbone (DeiT [44])\nseparately to assess the image classiÔ¨Åcation performance\non ImageNet dataset [9]. ImageNet dataset contains a\ntrain set of 1.28 million images and a validation set of\n50000 images. We train all models on the train set and test\nperformance on the validation set. ResNet is the chosen\ncompetitor against ConTNet since it is the most popular\nConvNet in CV applications. On the other hand, we\nconsider DeiT as representative of the visual transformer,\nwhich can be directly trained from scratch on ImageNet\ndataset.\nTowards a fair competition, we trained ConTNet and\nNetwork FLOPs(G) #Param(M) Top-1(%)\nRes-18 1.8 11.7 71.5\nConT-S 1.5 10.1 74.9\nRes-50 4.0 25.6 77.1\nConT-M 3.1 19.2 77.6\nRes-101 7.6 44.5 78.2\nConT-B 6.4 39.6 77.9\nTable 2. Top1 accuracy (%) on ImageNet. ResNet and ConTNet\nare trained with the same setting.\nNetwork FLOPs(G) #Param(M) Top-1(%)\nRes-18* 1.8 11.7 73.2\nConT-S* 1.5 10.1 76.5\nRes-50* 4.0 25.6 78.6\nConT-M* 3.1 19.2 80.2\nRes-101* 7.6 44.5 80.0\nConT-B* 6.4 39.6 81.8\nTable 3. Top1 accuracy (%) on ImageNet. ‚àóindicates that the\nmodel is trained with strong data augmentation containing mixup\nand auto-augmentation. AdamW is used to optimize ConTNet\nhere instead of SGD. The fairness of comparsion is enabled by\nthe fact that SGD is fairly good for ResNet.\nNetwork FLOPs(G) #Param(M) Top-1(%)\nDeiT-Ti 1.3 5.7 72.2\nConT-Ti* 0.8 5.8 74.9\nDeiT-S 4.6 22.1 79.8\nConT-M* 3.1 19.2 80.2\nDeiT-B 17.6 86.6 81.8\nConT-B* 6.4 39.6 81.8\nTable 4. Top1 accuracy(%) on ImageNet. Results of DeiT is from\nthe original paper. ‚àóindicates that model is trained with mixup\nand auto-augmentation with AdamW optimizer.\nResNet with identical training settings. We use SGD [39]\nto train both of ConTNet and ResNet with a batch-size of\n512 for 200 epochs on 8 Nvidia V100 GPUs. Cosine decay\nschedule is used to adjust learning rate. The initial learning\nrate is set to 0.2 for ResNet and conv layers in ConTNet,\nand the initial learning rate of STEs in ConTNet is uniquely\nset to 0.005. We use a weight decay of 0.00005 and a mo-\nmentum of 0.9. We follow a simple data-augmentation in\nResNet and perform regularization by using label smooth\nwith œµ of 0.1. Table 2 shows the results of comparison.\nSpeciÔ¨Åcally, ConT-S ourperforms ResNet-18 (by 3.4%),\nand ConT-M performs sligtly better than ResNet-50 (by\n0.5%), while saving nearly 20% parameters as well as 25%\ncomputational costs. However, ConT-B achieves a slightly\nMethod Backbone Param(M) FLOPs(G) AP AP50 AP75 APs APm APl\nRetinaNet Res-50 32.0 235.6 36.5 55.4 39.1 20.4 40.3 48.1\nConT-m 27.0 217.2 37.9( +1.4) 58.1( +2.7) 40.2( +1.1) 23.0( +2.6) 40.6( +0.3) 50.4( +2.3)\nFCOS Res-50 32.2 242.9 38.7 57.4 41.8 22.9 42.5 50.1\nConT-m 27.2 228.4 40.8( +2.1) 60.5( +3.1) 44.7( +2.9) 25.1( +2.2) 44.6( +2.1) 53.0( +2.9)\nFaster RCNN Res-50 41.5 241.0 37.4 58.1 40.4 21.2 41.0 48.1\nConT-m 36.6 225.6 40.0( +2.6) 62.4( +4.3) 43.0( +2.6) 25.4( +4.2) 43.0( +1.9) 52.0( +3.9)\nTable 5. Object detection mAP (%) on the COCO validation set. We evaluate the performance of detection model by replacing backbone\nResNet50 in ReTinaNet, FCOS and Faster-RCNN with ConTNet.\nlower accuracy than ResNet-101 (by ‚àí0.3%), as the com-\nputational complexity is lower. We hypothesize that STE\nin ConTNet has a high risk of overÔ¨Åtting. To eliminate\nthe risk, we train ConTNet and ResNet both with strong\ndata augmentations including auto-augmentation [7] and\nmixup [52]. Additionally, we use AdamW [32] optimizer\nin stead of SGD to train ConTNet with an initial learning\nrate of 0.0005 and a weight decay of 0.05. Table 3 shows\nthe results with strong augmentations. In the case of us-\ning the same strong data augmentation methods, ConTNet\nearns more beneÔ¨Åts than ResNet, and all ConTNets outper-\nform ResNets on ImageNet dataset across different com-\nputational budgets. For example, ConT-B achieves 81.8%,\nwhich is 1.8% better than Res-101.\nThe results of DeiT presented in Table 4 is sourced from\nthe original paper, which is trained with a pipeline of strong\ndata augmentations and many tricks with AdamW opti-\nmizer [44]. With the same optimizer, fewer data augmen-\ntations and other tricks, ConTNet presents a better perfor-\nmance against DeiT. For example, ConT-B achieves81.8%,\nthe same as DeiT-B, while our FLOPs is 60% fewer (6.4 G\nvs.17.6 G).\nOne can also Ô¨Ånd that the performance of DeiT is ex-\ntremely sensitive to the data augmentations and training\ntricks [44]. For instance, without either stochastic depth or\nrandom erasing, the convergence of DeiT is seemingly dis-\nabled. If traind with a SGD optimizer instead of AdamW,\nDeiT achieves a much lower accuracy by 7.3. By con-\ntrast, our experiments have shown that ConTNet can be op-\ntimized stably even in the same setting as ResNet and only\na few data augmentation tricks incur a better accuracy ex-\nceeding DeiT, which veriÔ¨Åes that ConTNet has an advan-\ntage of robustness over DeiT.\n4.2. Downstream tasks\nDownstream tasks are more sensitive to the size of the\nreceptive Ô¨Åeld, and models capable of capturing long-range\ndependencies would achieve better results. To demonstrate\nConTNet has a stronger ability to capture global informa-\ntion, we make use of ConT-M as the backbone to imple-\nment some downstream tasks, including object detection,\ninstance segmentation and semantic segmentation. Our em-\npirical results show that ConT-M obtains a better perfor-\nmance with lower computational complexity in contrast to\nusing resnet50 as backbone.\nObject Detection: We examine the object detection task on\nthe COCO2017 [29] dataset containing a train set of 118k\nimages and a validation set of 5k images. We adopt Faster-\nRCNN [36] (two-stage), FCOS [42] (one-stage) and Reti-\nnaNet [28] (one-stage) as the detection model with ResNet-\n50 backbone. Both ConT-M and ResNet-50 are pretrained\non ImageNet dataset with the same training setting. Then all\nmodels are trained through mmdetection following the same\ntraining setting. We modify the input size slightly from\n(1333,800) to (1344,784) and employ marginal padding\nbetween stage 2 and 3 to match the patch size of STE. Be-\ncause the ConTNet conducts downsampling with the last\nconv layer of each stage, we tweak the stride from 2 to 1\nand add Average-Pooling layer for downsampling in order\nto take full use of FPN [27].\nTable 5 records the results of detection experiments. The\nConTNet-based model outperforms baseline in all object\ndetection methods. Take a close look at these results,APl is\nimproved most largely, and APs has a higher performance\nimprovement than Am. This observation proves that Con-\nTNet has a relatively large receptive Ô¨Åeld to capture more\nglobal features without compromising locality. We con-\nclude that using STE and conv layer alternately can learn\nmore contextual representation than ConvNet while main-\ntaining local features. Most notably, such improvements\nare attained without any bells and whistles, thus providing\nan evidence that ConTNet is applicable and stable for the\ndownstream task.\nInstance Segmentation: We use COCO2017 instance\ndataset to investigate the performance of ConTNet on in-\nstance segmentation. We employ Mask-RCNN [14] with\nResNet50 as baseline, following the training settings in\nobject detection experiments. Table 6 shows that ConT-\nNet improves the bboxmap by 2.3% and the segmmap by\nBackbone APbb APbb\ns APbb\nm APbb\nl APmk APmk\ns APmk\nm APmk\nl\nRes-50 38.2 21.9 40.9 49.5 34.7 18.3 37.4 47.2\nConT-M 40.5( +2.3) 25.1( +3.2) 44.4( +3.5) 52.7( +3.2) 38.1( +3.4) 20.9( +2.6) 41.0( +3.6) 50.3( +3.1)\nTable 6. Instance segmentation mAP (%) on the COCO validation set. We replace the ResNet50 of Mask-RCNN with ConT-M.\nModel mIOU(%)\nPSP-Res50‚ãÜ 77.30\nPSP-Res50 77.12\nPSP-ConTM 78.28\nTable 7. mIOU on cityscapes validation set. ‚ãÜ indicates the results\nof original implementation.\n3.4%. The experiments on instance segmentation also sup-\nport the conclusion proposed in the object detection sub-\nsection. When ConTNet serves as the backbone, ConTNet\nimproves the model‚Äôs performance and captures global fea-\ntures better.\nSemantic Segmentation: We conduct semantic segmenta-\ntion experiments on the Cityscapes dataset [6]. We use PSP-\nNet [54] for this implementation. We replace the ResNet50\nin PSPNet with ConT-M and follow training and validation\nprotocols in PSPNet except for input size and optimizer.\nThe original input size is(713,713) and we use a input size\nof (672,672) instead to match split operation of STE. We\nuse AdamW optimizer to train our model on training set\nand evaluate the performance on validation set. Table 7\nshows that our model achieves a higher mIOU than base-\nline (1.16%).\n4.3. Ablation study\nPositional Encoding: Positional encoding (PE) is essen-\ntial to Transformer in NLP tasks. The order of words de-\nÔ¨Ånes the grammar for a sentence, and the order of pixels\ndetermines the semantics of an image. In ConTNet, a stan-\ndard transformer encoder (STE) infuses a shared learnable\n2D position encoding into each split patch. Table 8 shows\nthe results of different position encoding schemes. Expect-\nedly, the infusion of PE increases performance of a position-\nunware ConTNet signiÔ¨Åcantly. Compared to adopting 2D\nposition encoding [34], the performance degrades slightly\nusing 1D position encoding. We next replace self-attention\nin STE with relative attention from [37] to implement a rela-\ntive position encoding, which yields an additional improve-\nment.\nAs mentioned above, ConTNet adopts a patch-wise po-\nsitional infusion by adding a shared PE to each split patch.\nWe are curious aboout the effect of adding PE to entire\nPosition Encoding Placement Top-1(%)\nNone None 78.9\n1D learnable patch-wise 80.1\n2D learnable patch-wise 80.2\n2D learnable image-wise 79.3\nRelative patch-wise 80.4\nTable 8. Results of different position encoding schemes on Ima-\ngeNet classiÔ¨Åcation. In the case of 2D learnable postion encoding,\nresults of different placements are listed.\nPatch size Top-1(%)\n[ 7, 7], [ 7, 7], ..., [ 7, 7], [ 7, 7] 79.7\n[14,14], [14,14], ..., [14,14], [14,14] 78.8\n[7,7], [14,14], ..., [7,7], [14,14] 80.0\n[7,14], [7,14], ..., [7,14], [7,14] 80.2\nTable 9. Results of different combinations of patch size on Ima-\ngeNet classÔ¨Åcation. Using multiple patch sizes is a better setting.\nimage before splitting it into patch. Table 8 shows that\npatch-wise position encoding outperforms image-wise posi-\ntion encoding (by 0.9%). The result suggests that PE should\nbe used in combination with patch-wise STE more than in\nisolation.\nImpact of different patch sizes: Patch size is an important\nhyper-parameter in ConTNet. In order to explore the effect\nof different patch sizes and Ô¨Ånd a suitable patch size, we test\nfour different arrangements of patch size by tuning ConT-\nM: 1) All patch size is set to 7, 2) all patch size is to 14, 3)\nthe patch size of the Ô¨Årst block in each stage is set to 7 and\nthat of the second block is set to 14,4) the patch size of Ô¨Årst\nSTE in each block is set to 7 and that of second STE is set\nto 14. Table 9 shows that except setting 2), all patches are\n14 √ó14, the results of other arrangements of patch sizes are\nrelatively close. So we choose the patch size of 7 and 14\nalternately as the default setting.\nRange of learning rate: We would also like to know the\nimpacts of different learning rates on our model. A wide\nrange of learning rate choices means that it can be applied\nto various downstream tasks and transferred stably. We\ntrain ConT-M with SGD optimizer and adjust the learn-\nNetwork Conv LR STE LR Top-1(%)\nConT-M 0.2 0.01 77.4\nConT-M 0.2 0.005 77.6\nConT-M 0.1 0.005 77.6\nConT-M 0.1 0.001 77.1\nTable 10. Results of different pairs of learning rates on ImageNet\nvalidation. The network is trained with SGD optimizer.\nGroup number FLOPs(G) #Param(M) Top-1(%)\n1 3.1 19.2 80.2\n4 2.6 15.0 78.8\n8 2.5 14.3 79.3\n16 2.4 14.0 79.1\ndepthwise 2.5 15.3 79.6\nTable 11. Results of replacing convolution in ConT Block with\ngroup convolution with different number of groups. ‚Äùdepthwise‚Äù\nmeans that the convolution is depth-wise separable.\ning rate of conv layers and STE separately. In our prac-\ntice, the initial learning rate of STE is denoted by lrste ‚àà\n{0.01,0.005,0.001}and the initial learning rate of conv\nlayer is denoted by lrconv ‚àà{0.2,0.1}. Table 10 shows the\nresults of ConT-M trained with each group of learning rates\nand we Ô¨Ånd that the performance of ConT-M is not suscepti-\nble to learning rate. Therefore, the performance of applying\nConTNet as the backbone of computer vision models can\nnot be limited by the speciÔ¨Åc learning rate setting adopted\nin other tasks, which suggests that ConTNet is of great ro-\nbustness.\nGroup convolution: ConTNet is equipped with multiple\nconv layers that aggregate local features supplementary to\nglobal information captured by STE. Group convolution is a\nsuccessful variants of convolution to reduces computational\ncost. In authentic convolution, the weight of kernel is de-\nnoted by Wk ‚ààRk√ók√óCin √óCout , which is grouped into a\nset of kernels W1,W2,..., Wg ‚ààRk√ók√óCin /g√óCout /g in a\ngroup convolution. The grouped convolutional kernel dis-\nentangles the channel number and therefore achieve a lower\ncomputational and memory costs than authentic convolu-\ntion. Table 11 shows the impact of different group settings\non performance of ConT-M, which reveals that grouped\nconvolution induces a non-negligible performance degrada-\ntion for ConTNet. An alternative solution to increase efÔ¨Å-\nciency of our model is to exploit depth-wise separable con-\nvolution. As can be seen from Table 11, depth-wise sepa-\nrable convolution make the best trade-off between the com-\nputational cost and inference performance.\n4.4. Visualization\nVisualization of segmentation: We list some visualized\nresults of semantic segmentation to compare performance\nqualitatively as shown in Figure 4. In the Ô¨Årst column, the\ntrue class of the area inside the yellow box is labeled as bus\nboth in ground truth and the fourth row. However, in the\nthird row, these pixels are labeled as truck. In the second\ncolumn, inside the left yellow box is a truck as shown in\nthe second and fourth tow, but in the third row, some pixels\nof the truck are labeled as car. As for the right yellow box\nin the second column, the results of the fourth row labeled\nmore pixels than that of the third row. In the third column, a\nlarge area of side walk is left unlabeled due to the occlusion\nof shadow as shown in the yellow box. In contrast, as can be\nseen from the fourth row, almost all of the sidewalk pixels\nthat were missed in the third row are labeled.\nThe segmentation results indicated that ConTNet aggre-\ngates more global information than ConvNets and the pix-\nels of a large object enjoy a better inner consistency. Such\na promotion of segmentation network is stemmed from the\nability of modeling long-range dependencies by exploiting\nSTE in ConTNet.\n5. Concolusion\nIn this work, we propose an innovative convolution-\ntransformer network (ConTNet). ConTNet combines trans-\nformer and convolution by stacking standard transformer\nencoder (STE) and spatial convolution alternately. We in-\ntroduce a weight-shared patch-wise STE to model a large\nkernel. A series of experiments demonstrate that ConT-\nNet outperforms ResNet and achieves comparable accu-\nracy of trendy visual transformer with much lower com-\nputational complexity. We verify that the effectiveness of\nimplementing downstream tasks with ConTNet backbone\nis primarily derived from aggregating more contextual in-\nformation. Moreover, we demonstrate that ConTNet is as\nrobust as ConNets for example ResNet. In other words,\nConTNet‚Äôs performance does not rely on strong data aug-\nmentations and fancy training tricks, and is not sensitive to\nhyper-parameters as well. Finally, in ConTNet we use the\nvanilla convolution and transformer, if replaced with the im-\nproved versions of convolution and transformer recently, a\nfurther performance is promised. We hope that our work\ncould bring some new ideas for model design.\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016. 5\n[2] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le. At-\ntention augmented convolutional networks. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3286‚Äì3295, 2019. 3\nFigure 4. Visualized comparison of segmentation results for PSPNet on Cityscapes validation set. Each column is a group of sample from\nvalidation set and the top two rows are the raw image with its corresponding groud truth. The bottom two rows represents the results of\nPSPNet with ResNet50 backbone and ConT-M backbone separately. The yellow bounding box marks the region where the segmentation\nresults presents a considerable variance.\n[3] Y . Cao, J. Xu, S. Lin, F. Wei, and H. Hu. Gcnet: Non-local\nnetworks meet squeeze-excitation networks and beyond. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision Workshops, pages 0‚Äì0, 2019. 2\n[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nand S. Zagoruyko. End-to-end object detection with trans-\nformers. In European Conference on Computer Vision, pages\n213‚Äì229. Springer, 2020. 2, 3\n[5] F. Chollet. Xception: Deep learning with depthwise separa-\nble convolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1251‚Äì1258,\n2017. 2\n[6] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele. The\ncityscapes dataset for semantic urban scene understanding.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3213‚Äì3223, 2016. 8\n[7] E. D. Cubuk, B. Zoph, D. Mane, V . Vasudevan, and Q. V . Le.\nAutoaugment: Learning augmentation policies from data.\narXiv preprint arXiv:1805.09501, 2018. 7\n[8] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei.\nDeformable convolutional networks. In Proceedings of the\nIEEE international conference on computer vision , pages\n764‚Äì773, 2017. 2\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248‚Äì255. Ieee, 2009. 2, 6\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805, 2018. 3\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[12] S. Gao, M.-M. Cheng, K. Zhao, X.-Y . Zhang, M.-H. Yang,\nand P. H. Torr. Res2net: A new multi-scale backbone archi-\ntecture. IEEE transactions on pattern analysis and machine\nintelligence, 2019. 2\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-\nbased convolutional networks for accurate object detection\nand segmentation. IEEE transactions on pattern analysis\nand machine intelligence, 38(1):142‚Äì158, 2015. 1\n[14] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-\ncnn. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), Oct 2017. 2, 7\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n770‚Äì778, 2016. 1, 2, 4, 6\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\ndeep residual networks. In European conference on com-\nputer vision, pages 630‚Äì645. Springer, 2016. 1, 2\n[17] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li. Bag\nof tricks for image classiÔ¨Åcation with convolutional neural\nnetworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 558‚Äì567,\n2019. 2\n[18] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: EfÔ¨Å-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv preprint arXiv:1704.04861, 2017. 2\n[19] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks\nfor image recognition. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision , pages 3464‚Äì\n3473, 2019. 3\n[20] J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi. Gather-\nexcite: Exploiting feature context in convolutional neural\nnetworks. arXiv preprint arXiv:1810.12348, 2018. 2, 3\n[21] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132‚Äì7141, 2018. 2\n[22] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-\nberger. Densely connected convolutional networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 4700‚Äì4708, 2017. 1, 2\n[23] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nInternational conference on machine learning , pages 448‚Äì\n456. PMLR, 2015. 2\n[24] Y . Jiang, S. Chang, and Z. Wang. Transgan: Two\ntransformers can make one strong gan. arXiv preprint\narXiv:2102.07074, 2021. 3\n[25] S. Kornblith, J. Shlens, and Q. V . Le. Do better imagenet\nmodels transfer better? In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 2661‚Äì2671, 2019. 2\n[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiÔ¨Åcation with deep convolutional neural networks. Ad-\nvances in neural information processing systems , 25:1097‚Äì\n1105, 2012. 1, 2\n[27] T.-Y . Lin, P. Doll¬¥ar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie. Feature pyramid networks for object detection.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2117‚Äì2125, 2017. 7\n[28] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll¬¥ar. Focal\nloss for dense object detection. In Proceedings of the IEEE\ninternational conference on computer vision , pages 2980‚Äì\n2988, 2017. 7\n[29] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll¬¥ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. InEuropean conference on computer\nvision, pages 740‚Äì755. Springer, 2014. 7\n[30] Z. Liu, S. Luo, W. Li, J. Lu, Y . Wu, C. Li, and L. Yang.\nConvtransformer: A convolutional transformer network for\nvideo frame synthesis. arXiv preprint arXiv:2011.10185 ,\n2020. 3\n[31] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 3431‚Äì3440, 2015. 1\n[32] I. Loshchilov and F. Hutter. Fixing weight decay regulariza-\ntion in adam. 2018. 7\n[33] J. Park, S. Woo, J.-Y . Lee, and I. S. Kweon. Bam: Bottleneck\nattention module. arXiv preprint arXiv:1807.06514, 2018. 2,\n3\n[34] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,\nA. Ku, and D. Tran. Image transformer. In International\nConference on Machine Learning, pages 4055‚Äì4064. PMLR,\n2018. 8\n[35] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Lev-\nskaya, and J. Shlens. Stand-alone self-attention in vision\nmodels. arXiv preprint arXiv:1906.05909, 2019. 3\n[36] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nwards real-time object detection with region proposal net-\nworks. arXiv preprint arXiv:1506.01497, 2015. 7\n[37] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention\nwith relative position representations. arXiv preprint\narXiv:1803.02155, 2018. 8\n[38] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 1, 2\n[39] S. Sra, S. Nowozin, and S. J. Wright. Optimization for ma-\nchine learning. Mit Press, 2012. 6\n[40] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi. Inception-\nv4, inception-resnet and the impact of residual connections\non learning. In Proceedings of the AAAI Conference on Ar-\ntiÔ¨Åcial Intelligence, volume 31, 2017. 2\n[41] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818‚Äì2826, 2016. 2\n[42] Z. Tian, C. Shen, H. Chen, and T. He. Fcos: Fully convo-\nlutional one-stage object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 9627‚Äì9636, 2019. 2, 7\n[43] A. Toshev and C. Szegedy. Human pose estimation via deep\nneural networks‚Äô. CVPR.(Columbus, Ohio, 2014) , pages\n1653‚Äì1660. 1\n[44] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-\nrolles, and H. J ¬¥egou. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 2, 3, 6, 7\n[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. arXiv preprint arXiv:1706.03762, 2017. 1, 3, 4, 5,\n6\n[46] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,\nX. Wang, and X. Tang. Residual attention network for im-\nage classiÔ¨Åcation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3156‚Äì3164,\n2017. 2\n[47] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neu-\nral networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7794‚Äì7803,\n2018. 2, 3\n[48] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon. Cbam: Convo-\nlutional block attention module. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 3‚Äì19,\n2018. 2, 3\n[49] S. Xie, R. Girshick, P. Doll ¬¥ar, Z. Tu, and K. He. Aggre-\ngated residual transformations for deep neural networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1492‚Äì1500, 2017. 2\n[50] H. Yan, Z. Li, W. Li, C. Wang, M. Wu, and C. Zhang. Con-\ntnet: Why not use convolution and transformer at the same\ntime? arXiv preprint arXiv:2104.13497, 2021. 1\n[51] S. Zagoruyko and N. Komodakis. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016. 2\n[52] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz.\nmixup: Beyond empirical risk minimization. arXiv preprint\narXiv:1710.09412, 2017. 7\n[53] H. Zhao, J. Jia, and V . Koltun. Exploring self-attention for\nimage recognition. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10076‚Äì10085, 2020. 3\n[54] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene\nparsing network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2881‚Äì2890,\n2017. 8\n[55] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu,\nJ. Feng, T. Xiang, P. H. Torr, et al. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. arXiv preprint arXiv:2012.15840, 2020. 3\n[56] X. Zhu, H. Hu, S. Lin, and J. Dai. Deformable convnets\nv2: More deformable, better results. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9308‚Äì9316, 2019. 2\n[57] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. De-\nformable detr: Deformable transformers for end-to-end ob-\nject detection. arXiv preprint arXiv:2010.04159, 2020. 3"
}