{
  "title": "Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models",
  "url": "https://openalex.org/W4386907276",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222212885",
      "name": "Ko, Hyung-Kwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222212882",
      "name": "Jeon, Hyeon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4306800557",
      "name": "Park, Gwanmo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116236559",
      "name": "Kim Dae Hyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2524833921",
      "name": "Kim， Nam Wook",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2406051915",
      "name": "Kim, Juho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381208356",
      "name": "Seo， Jinwook",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4206636317",
    "https://openalex.org/W2083928261",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3172214016",
    "https://openalex.org/W3081277912",
    "https://openalex.org/W3153965221",
    "https://openalex.org/W4385570934",
    "https://openalex.org/W3021218348",
    "https://openalex.org/W2151715145",
    "https://openalex.org/W3202563689",
    "https://openalex.org/W2888660171",
    "https://openalex.org/W2295423240",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4386148461",
    "https://openalex.org/W3093933617",
    "https://openalex.org/W1516293359",
    "https://openalex.org/W4389989038",
    "https://openalex.org/W2132881639",
    "https://openalex.org/W4249415694",
    "https://openalex.org/W3090149233",
    "https://openalex.org/W2152922709",
    "https://openalex.org/W4302275696",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W3198767185",
    "https://openalex.org/W4237375617",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W2965663461",
    "https://openalex.org/W4385570745",
    "https://openalex.org/W2054901814",
    "https://openalex.org/W2522351341",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4224995224",
    "https://openalex.org/W3080485811",
    "https://openalex.org/W4313122483",
    "https://openalex.org/W4404783301",
    "https://openalex.org/W3021265955",
    "https://openalex.org/W2725765016",
    "https://openalex.org/W1961845056",
    "https://openalex.org/W2965346190",
    "https://openalex.org/W3163379691",
    "https://openalex.org/W4285292339",
    "https://openalex.org/W4300466035",
    "https://openalex.org/W1599853232",
    "https://openalex.org/W2601243251",
    "https://openalex.org/W4220661167",
    "https://openalex.org/W2888611489",
    "https://openalex.org/W4379533921",
    "https://openalex.org/W3092587559",
    "https://openalex.org/W3213578841",
    "https://openalex.org/W1896226795",
    "https://openalex.org/W4307475337",
    "https://openalex.org/W2730072090",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4297094736",
    "https://openalex.org/W2516678343",
    "https://openalex.org/W4388286232",
    "https://openalex.org/W4280596375",
    "https://openalex.org/W2752843814",
    "https://openalex.org/W2160382748",
    "https://openalex.org/W4389520783",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W4366549916",
    "https://openalex.org/W2428096022",
    "https://openalex.org/W2795226127",
    "https://openalex.org/W3116465435",
    "https://openalex.org/W4385565485",
    "https://openalex.org/W2798413829",
    "https://openalex.org/W2964101465",
    "https://openalex.org/W3174906424",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W4226149412",
    "https://openalex.org/W2753472863",
    "https://openalex.org/W2601982504",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4385565383",
    "https://openalex.org/W2897132999",
    "https://openalex.org/W3028907449",
    "https://openalex.org/W4366549000",
    "https://openalex.org/W3134547365",
    "https://openalex.org/W4387835442"
  ],
  "abstract": "We introduce VL2NL, a Large Language Model (LLM) framework that generates rich and diverse NL datasets using only Vega-Lite specifications as input, thereby streamlining the development of Natural Language Interfaces (NLIs) for data visualization. To synthesize relevant chart semantics accurately and enhance syntactic diversity in each NL dataset, we leverage 1) a guided discovery incorporated into prompting so that LLMs can steer themselves to create faithful NL datasets in a self-directed manner; 2) a score-based paraphrasing to augment NL syntax along with four language axes. We also present a new collection of 1,981 real-world Vega-Lite specifications that have increased diversity and complexity than existing chart collections. When tested on our chart collection, VL2NL extracted chart semantics and generated L1/L2 captions with 89.4% and 76.0% accuracy, respectively. It also demonstrated generating and paraphrasing utterances and questions with greater diversity compared to the benchmarks. Last, we discuss how our NL datasets and framework can be utilized in real-world scenarios. The codes and chart collection are available at https://github.com/hyungkwonko/chart-llm.",
  "full_text": "Natural Language Dataset Generation Framework for\nVisualizations Powered by Large Language Models\nHyung-Kwon Ko\nhyungkwonko@gmail.com\nKAIST\nRepublic of Korea\nHyeon Jeon\nhj@hcil.snu.ac.kr\nSeoul National University\nRepublic of Korea\nGwanmo Park\ngmpark@hcil.snu.ac.kr\nSeoul National University\nRepublic of Korea\nDae Hyun Kim\ndhkim16@cs.stanford.edu\nKAIST\nRepublic of Korea\nNam Wook Kim\nnam.wook.kim@bc.edu\nBoston College\nUSA\nJuho Kim\njuhokim@kaist.ac.kr\nKAIST\nRepublic of Korea\nJinwook Seo∗\njseo@snu.ac.kr\nSeoul National University\nRepublic of Korea\nABSTRACT\nWe introduce VL2NL, a Large Language Model (LLM) framework\nthat generates rich and diverse NL datasets using Vega-Lite specifi-\ncations as input, thereby streamlining the development of Natural\nLanguage Interfaces (NLIs) for data visualization. To synthesize rel-\nevant chart semantics accurately and enhance syntactic diversity in\neach NL dataset, we leverage 1) a guided discovery incorporated into\nprompting so that LLMs can steer themselves to create faithful NL\ndatasets in a self-directed manner; 2) a score-based paraphrasing to\naugment NL syntax along with four language axes. We also present\na new collection of 1,981 real-world Vega-Lite specifications that\nhave increased diversity and complexity than existing chart collec-\ntions. When tested on our chart collection, VL2NL extracted chart\nsemantics and generated L1/L2 captions with 89.4% and 76.0% accu-\nracy, respectively. It also demonstrated generating and paraphrasing\nutterances and questions with greater diversity compared to the\nbenchmarks. Last, we discuss how our NL datasets and framework\ncan be utilized in real-world scenarios. The codes and chart collec-\ntion are available at https://github.com/hyungkwonko/chart-llm.\nCCS CONCEPTS\n• Human-centered computing →Visualization; Natural lan-\nguage interfaces.\n∗corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0330-0/24/05. . . $15.00\nhttps://doi.org/10.1145/3613904.3642943\nKEYWORDS\nVega-Lite, natural language datasets, large language models, frame-\nwork, natural language interfaces, data visualization\nACM Reference Format:\nHyung-Kwon Ko, Hyeon Jeon, Gwanmo Park, Dae Hyun Kim, Nam Wook\nKim, Juho Kim, and Jinwook Seo. 2024. Natural Language Dataset Genera-\ntion Framework for Visualizations Powered by Large Language Models. In\nProceedings of the CHI Conference on Human Factors in Computing Systems\n(CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA,\n22 pages. https://doi.org/10.1145/3613904.3642943\n1 INTRODUCTION\nRecent advancements in Natural Language Processing (NLP) tech-\nniques empowered individuals with limited data analysis and vi-\nsualization expertise to engage in text-based interaction and ex-\necute data visualization tasks [ 75, 83]. Many studies have incor-\nporated Natural Language Interfaces (NLIs) into their systems to\naugment more natural and user-friendly interactions [73]. For ex-\nample, Voder [78] enables querying key data insights within charts\nusing NL sentences, significantly decreasing the reliance on manual\nprogramming for data retrieval. Furthermore, users can provide text\nto receive automatic recommendations for the most appropriate\nchart types [17, 60], rather than selecting effective representations\nmanually based on graphical language criteria.\nWhile the presence of suitable datasets modeling human behav-\niors is crucial in developing effective NLIs or tools for visualizations,\nprior work has repeatedly pointed to the scarcity of sizable pairs\nof high-quality datasets (chart, NL) [11, 14, 27, 47, 74, 79]. In detail,\nexisting chart collections are occasionally synthetic [47, 98], limited\nin diversity (e.g., chart type) [11, 51], or are limited to simpler charts\n(e.g., basic bar charts, univariate line charts) [18]. Making things\nworse, only a fraction of these collections (17 out of 56) is publicly\naccessible [11]. Furthermore, prior work builds the NL datasets that\ngoes with the visualizations through crowdsourcing [79]. However,\nthe process can be costly and time-consuming as it requires re-\ncruiting specific sets of target users of the system, some of whom\narXiv:2309.10245v4  [cs.HC]  22 Jan 2024\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nmust meet notably stringent qualification criteria. Moreover, it is\nchallenging to capture the language variations that arise from a\ndiverse spectrum of user expertise, usage scenarios, and personal\npreferences, although this is essential for addressing the syntactic\nvariations among the target users of the systems in the real-world\n[20, 73, 97]. What exacerbates the situation is there are multiple\ntypes of NL tasks (e.g., captioning, chart generation & modification,\nand chart question-answering) where each one necessitates a new\ndataset tailored to the specific task or transfer knowledge.\nWe present a new collection of 1,981 Vega-Lite specifications (Fig-\nure 2). This is the largest set of human-generated charts obtained\nfrom GitHub to date. It covers varying levels of complexity from a\nsimple line chart without any interaction (i.e., simple charts) to a\nchart with four plots where data points are linked with selection\ninteractions (i.e., extra complex charts) (see the charts highlighted\nwith red stroke in Figure 2). As we focus on amassing a richer set\nof charts in terms of complexity, more than 86% of them are in com-\nplex and extra complex levels. Compared to the benchmarks, our\ndataset shows the highest average pairwise edit distance between\nspecifications, which proves that the charts are highly diverse from\none another. Moreover, it contains the largest number of charts\nwith composite views, interactions (e.g., tooltips, panning & zoom-\ning, and linking), and diverse chart types (e.g., map, grid & matrix,\ndiagram, etc.) (Table 2).\nWe also introduce VL2NL, a 3-stage NL generation framework\nthat can be generalized to various NL tasks on visualizations (Fig-\nure 3). First, the framework preprocesses the underlying datasets\nand minifies Vega-Lite specification for efficient and effective usage\nby an LLM. Next, the framework leverages guided discovery [7] so\nthat LLMs can steer themselves to create varying NL datasets in a\nself-directed manner. Here, it analyzes and integrates chart seman-\ntics (e.g., mark, encoding) with our scaffolding in accordance with\nthe characteristics of each NL dataset. Also, by answering on key\nquestions, it autonomously concentrates on the chart’s key features\nor propose high-level decisions. Finally, the framework applies a\nscore-based paraphrasing (Table 5) with an LLM to simulate and\ninclude syntactic variations of human language in NL datasets.\nTo test VL2NL, we generated L1 captions that simply describe\nhow the chart encodes data, L2 captions that describe the statistical\nproperties of the data in a chart [45], utterances for chart generation\n[79], and questions for chart question answering [24, 32]. Our exper-\niments showed that the accuracy of the analyzed chart semantics\nand generated L1/L2 captions is 89.4% and 76.0%, respectively. More-\nover, the generated and paraphrased NL datasets showed greater\nsyntactic diversity in terms of 4.75 out of 6 within-distribution\nmetrics on average. Last, we demonstrate the application of our NL\ndatasets in finetuning experiments, and the use of VL2NL in both\nfully-automatic and mixed-initiative modes within an interactive\nsystem for real-world scenarios.\nThe main contributions of our work are summarized as follows:\n•We collect 1,981 real-world Vega-Lite specifications that are\ndiverse and go beyond simple charts;\n•We present 3-stage NL dataset generation framework for\nvisualizations powered by LLMs that employs guided discov-\nery and score-based paraphrasing;\n•We perform quantitative and qualitative analysis on the NL\ndatasets generated by our framework.\n2 BACKGROUND AND RELATED WORK\nIn this section, we explain Vega-Lite specification and existing chart\ncollections. Next, we present the types of NL datasets that are of\nparticular interest in the context of this work. Last, we explain the\nuse of LLMs in synthesizing NL datasets.\n2.1 Chart Datasets\nAccording to Chen et al. ’s recent survey [11], chart datasets are\ntypically collected in three formats: bitmap graphics (e.g., .png),\nvector graphics (e.g., .svg), and programs (e.g., Vega-Lite specifi-\ncations [68]). Among the surveyed datasets, the majority (48 out\nof 56) consisted of bitmap graphics, followed by vector graphics\n(10 out of 56), while programs were less prevalent, comprising only\nfive instances (some works included multiple formats).\nAmong many program formats, we are especially interested in\nVega-Lite (Figure 1), which is an abstract specification that enables\nthe creation of interactive visualizations using a high-level grammar.\nIt is represented as a nested JSON object, consisting of numerous\nkey-value pairs, which can be also seen as a tree structure [47, 98].\nEach key defined in the specification is referred to as a property\n[89], serving a distinct role in generating charts. For example,mark\nproperty is used to map data to graphical elements (e.g., points,\nlines).\nVega-Lite provides additional advantages beyond those offered\nby SVG formats, since it is easy to modify and reuse for creat-\ning diverse chart variations [ 22]. It provides interactive features\nlike zooming, panning, and brushing, as well as concatenating or\nfaceting multiple plots/views. Furthermore, it support data-driven\nmanipulation, allowing users to dynamically update the data and\nreflect changes in real time. It can be seamlessly converted to other\nformats like bitmaps and SVG [69], while converting from those\nformats to program specifications typically requires manual effort\nor complex external algorithms [63].\nThere are two types of Vega-Lite benchmarks: synthetic and\nreal-world datasets. A critical limitation of synthetic datasets lies\nis their reliance on pre-defined templates and rules, which leads\nto a high degree of repetition and a limited range of chart types\nand functionalities (see Table 1). On the other hand, the real-world\ndataset reveals significant variation from one spec to another, en-\nsuring a high level of diversity in realistic scenarios. However, they\nare generally much smaller in size compared to synthetic datasets\n[11].\nWe found three synthetic Vega-Lite datasets. In detail, Poco et\nal. generated 4,318 Vega specifications [ 71] using the Compass\nrecommendation engine [89]. They randomly selected values for\na few variables (e.g., fonts, font size, legend positions, etc.) from a\ncurated set of options. These specifications were later converted to\nVega-Lite specifications in Data2Vis [18]. Zhao et al. [98] followed\na similar approach to generate the Chartseer dataset, consisting of\n9,925 specifications based on Data2Vis, although it is specifically\ndesigned for training a deep learning model and may not readily\nrender into charts, making it less suitable for broader research\nadoption. The nvBench dataset [47] presented 7,274 specifications,\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n{ \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\"height\": 400, \"width\": 400, \"autosize\": { \"type\": \"fit\", \"contains\": \"padding\"}, \"data\": { \"url\": https://data_url.csv},\"layer\": [{ \"mark\": { \"type\": \"line\"}, \"encoding\": { \"x\": { \"field\": \"Year\", \"type\": \"temporal\", \"axis\": { \"grid\": false} },y\": { \"field\": \"Value1\", \"type\": \"quantitative\", \"axis\": { \"grid\": false} }, \"color\": { ... } } },{\"mark\": { \"type\": \"rule\", \"color\": \"#e54753\", \"size\": 2}, \"encoding\": { \"x\": { \"field\": \"x\", \"type\": \"quantitative\"} }, \"data\": { ...}}]}\n{\"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\"autosize\": { \"contains\": \"padding”,\"type\": \"fit”},\"data\": { \"url\": https://data_url.csv},\"height\": 400,\"layer\": [{\"encoding\": {\"color\": { ... }\"x\": { \"axis\": { \"grid\": false}        \"field\": \"Year\", \"type\": \"temporal\", },\"y\": { \"axis\": { \"grid\": false} \"field\": \"Value1\", \"type\": \"quantitative\", },},\"mark\": { \"type\": \"line\"},},{\"data\": { ...}\"encoding\": { \"x\": { \"field\": \"x\", \"type\": \"quantitative\"} },\"mark\": {\"color\": \"#e54753\", \"size\": 2,\"type\": \"rule”},}],\"width\": 400,}(a) Raw Vega-Lite Code Data(b) After sorting keys and setting indentation\n(c) Generated Vega-Lite chart\nVega-Liteinterpreter root\nschemaautosizedataheightlayerwidth\ncontainstypeurl\nencodingmark\nencodingmark\ndata\n...\n(d) Tree Structure\nJSON Parser\nFigure 1: Example of Vega-Lite Specification. As previously noted in several works [ 47, 98], Vega-Lite specification can be\nregarded to follow a tree structure, with its keys (i.e., properties) connected in a nested structure.\nrepresenting SQL queries as tree structures and mapping them into\nVega-Lite specifications.\nThere are two real-world datasets that consist of human-generated\nspecifications. For instance, Kim et al. [32] curated 52 charts from\nvarious web sources, encompassing two chart types (bar chart and\nline chart). Additionally, the Vega-Lite gallery example dataset [70],\nthe largest publicly available human-generated collection of Vega-\nLite data, provides 716 high-quality examples with diverse chart\ntypes and interactions. However, due to the challenges associated\nwith data collection, these datasets have a limited quantity of speci-\nfications compared to synthesized datasets. As a result, researchers\noften face difficulties in finding a comprehensive set of specifica-\ntions for their own research purposes.\n2.2 NLIs for Data Visualization\nNLIs for data visualization have garnered significant attention due\nto their user-friendly nature [ 74, 80, 86]. These interfaces allow\nusers to focus on their tasks rather than learning how to interact\nwith systems [13]. A recent survey paper [74] suggested six high-\nlevel topics (e.g., visualization recommendation) to cluster tasks.\nThey also presented a pipeline with seven stages by extending the\nclassical information visualization pipeline [8].\nTo address diverse NLI tasks, we considered three types of NL\ndatasets: captions, utterances, and questions. This choice was made\nbased on the analysis of each topic, the number of representative\nworks, and the relevance of NL datasets to their respective tasks.\nThe first NL dataset is chart caption. The captions can help\npeople communicate and grasp insights in the charts easily, also\nimproving the accessibility for readers of the blind and low vision\npeople [45]. A lot of research delved into this problem leveraging\nfrom templates [56] to deep learning models [61, 64, 77].\nLundgard and Satyanarayan [45] proposed a four-level classifi-\ncation of captions where each level contains different semantic con-\ntent of the same chart: L1 provides elemental and encoded attributes,\nincluding chart type and encoding channel; L2 encompasses statis-\ntical and relational attributes such as descriptive statistics and cor-\nrelation; L3 addresses perceptual and cognitive attributes, covering\ncomplex trends and patterns; L4 contains contextual and domain-\nspecific knowledge. Recently, VisText [ 84] generated L2/L3 cap-\ntions by training ByT5 transformer model [93] with crowdsourced\ndataset. Our work shares similarities with VisText in generating\ncaptions with varying levels. However, it differs in that we do not\nrely on crowdsourcing NL datasets or training machine learning\nmodels. Instead, our approach solely depends on Vega-Lite speci-\nfication input and vanilla LLMs. It is worth noting that previous\nstudies in caption generation have predominantly focused on basic\nchart types, as highlighted in [74]. In contrast, our work offers a\ngeneralizable solution capable of generating captions for complex\nand diverse charts.\nThe second NL dataset is utterance for chart generation. For\nmany decades, automatically representing graphical information\nhas been one of the important topics in information visualization\n[49]. Many NLIs were introduced and adopted to solve multiple\nstages that are entangled one another for the automatic represen-\ntation. The most relevant stages are 1) utterance interpretation\n[19, 25, 35, 42, 48, 60, 81, 97] and 2) mapping utterances to visual\nelements [26, 30, 46, 50, 57, 82, 85, 88, 90], and 3) human interaction\nfor clarifying ambiguity or suggesting commands [20, 25, 59, 60, 73].\nSrinivasan et al. [79] analyzed the characteristics and semantics\nof NL utterances employed in chart generation. According to their\nresearch, NL utterances can be classified into three types based on\ntheir structures: commands, which are instructions or systematic\nrequests; queries, which are concise lists of keywords similar to\nweb search queries; and questions, which are data-driven inquiries\nthat users wish to visualize. In our work, we generate all three types\nof utterances, incorporating heightened syntactic diversity for a\ncomprehensive evaluation.\nThe last NL dataset is question. Chart Question Answering is a\npopular task in both machine learning [10, 43, 44, 52] and human-\ncomputer interaction [32] communities. This popularity stems from\nits effectiveness in eliciting insights and aiding in decision-making\nprocesses [24].\nKim et al. [32] investigated the semantics used in the questions\nby collecting 629 crowd-sourced questions and provided two or-\nthogonal dimensions. First axis is lookup or compositional, which\nis whether to retrieve a single value or using multiple mathematical\noperations. Second axis is visual or non-visual, which is whether\nto reference visual chart features or not. These question types are\nall focusing on retrieving factual short answers. In our work, we\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nTable 1: Summary of the Vega-Lite dataset construction pro-\ncess. First we collect all possible cases of URLs including\nVega-Lite specifications (a). Next, we have filtered unique\nURLs that are allowed to redistribute for academic purpose\n(b, c). Finally we iteratively inspect each specification manu-\nally to check whether it is valid and unique, since we want\nto collect charts with a high level of diversity (d).\n# of URLs / Vega-Lite specs\n(a) URLs crawled 67,789\n(b) URLs w/o duplicate 18,420\n(c) URLs w/ license 7,408\n(d) Specs after manual inspection 1,981\ntarget five different types of questions, including the aforemen-\ntioned types as well as the open-ended question type [24], which\nencourages deeper reflection on the underlying reasons or causes\nbehind specific events or patterns.\n2.3 LLMs and NL Datasets\nMany past research typically have used crowdsourcing to collect\nvarying types of NL datasets (e.g., captions, utterances, questions,\netc.) by asking crowd workers to come up with generation queries\nusing available chart datasets [32, 47, 79]. However, this approach\nis frequently time-consuming and costly [ 16, 91], which can ad-\nversely affect the scalability of datasets. It is prone to issues such\nas participant laziness and the collection of subpar queries [5]. To\nensure a consistent performance among workers, it is essential to\nsimplify the tasks and making them easy to follow, thereby pre-\nventing workers from feeling overwhelmed or fatigued during the\nstudy, as recommended by Kittur et al. [36]. With all these efforts,\nsuch crowdsourced NL datasets are often fragmented, posing chal-\nlenges for researchers seeking to apply them to their own tasks.\nThe characteristics of NL queries designed for each task can vary\nsignificantly, making a single NL dataset unsuitable for other tasks.\nThis motivates the need for a unified and adaptable framework\nthat can generate NL datasets tailored to any specific NLIs for data\nvisualization research.\nAs LLMs are known to simulate human behavior [62] and have\nbecome more prevalent due to their powerful performance, re-\nsearchers are increasingly using generated NL datasets to train\nsmaller-sized language models for specific tasks [55, 72, 95]. This\ntraining strategy is known as ‘teaching via data’ [41]. Here, LLMs,\nacting as teacher models, generate synthetic datasets which are\nthen used to train smaller-sized models, referred to as students,\ndesigned for specific tasks. This method is adopted to increase the\nperformance of different tasks like knowledge-based question an-\nswering [41], symbolic language generation (e.g., SQL query) [96],\nand semantic parsing [67]. Our work aligns with this trend, aiming\nto assist researchers in developing NLIs for data visualization by\ngenerating the necessary NL datasets using LLMs.\n3 VEGA-LITE DATASET\nWe have collected a new set of 1,981 real-world Vega-Lite specifica-\ntions. In this section, we present the details of our data collection\nprocess.\n3.1 Dataset Construction\n3.1.1 Search Queries. We utilize the GitHub API 1 to create our\nVega-Lite dataset. Due to the API’s limitation of providing up to\n1,000 results per search query, we employ various techniques, as\nwe elaborate below, to crawl Vega-Lite specifications in a mutually\nexclusive and exhaustive manner to the best of our abilities.\nWhen building search queries, we use the keyword https://\nvega.github.io/schema/vega-lite/[version] to indicate\nthe version of the specification that Vega-Lite uses for rendering\npurposes. We collect versions from v2 to v5: there are no v1 data to\nbe found. To partition the query into a more fine-grained manner,\nwe use keywords such as .csvand .jsonto gather specifications\nwith external links. Similarly, we employ keywords like values\nand datasetsto identify ones with internally embedded data. We\nalso leverage additional keywords using the main properties defined\nin the version 5 Vega-Lite specification2. These properties encom-\npass essential elements for creating a single plot, including data,\ntransform, mark, and encoding, while there are properties like\nlayer, facet, concat, and repeat, which are specifically rele-\nvant to visualizing composite views [68] (e.g., layered plots, trellis\nplots, or multiple views). A comprehensive list of the properties we\nuse can be found on the official documentation page3.\n3.1.2 Inclusion and Exclusion Criteria. We target files with exten-\nsion .json, vg.json, .vl.json, .vl, and .vg which denotes\nVega-Lite specifications. We also examine HTML and JavaScript\nfiles containing Vega-Lite specifications manually to get additional\nspecifications. Throughout the process, we exclude forked reposi-\ntories to prevent redundancy. We also filter out any data from the\nbenchmark datasets, such as Vega-Lite gallery [70].\n3.1.3 Post-processing. To obtain a large number of unique sets of\nVega-Lite specifications, we follow a step-by-step approach. During\nthe initial stage, a total of 67,789 URLs are collected. Despite efforts\nto ensure a mutually exclusive and comprehensive set of specifi-\ncations, duplicate URLs are identified and removed, resulting in\n18,420 unique URLs. Each URL is scrutinized to verify the license of\nthe corresponding repository, ensuring compliance with copyright\nregulations for academic redistribution. This process yields 7,408\nURLs. Lastly, we verify their validity using the Vega-Lite editor [69].\nThis involves identifying the URLs of the datasets used by each\nspecification and making necessary modifications, ranging from\nminor adjustments such as closing unclosed brackets to more sig-\nnificant ones like debugging the entire code, in order to achieve\nsuccessful rendering. An overview of the post-processing and the\nnumber of URLs and specifications obtained at each stage can be\nfound in Table 1. Our chart collection is publicly accessible via the\nfollowing link: https://hyungkwonko.info/chart-llm-data.\n1https://docs.github.com/en/rest\n2https://github.com/vega/schema\n3https://vega.github.io/vega-lite/docs\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nSimple (2)\nMedium (5)\nComplex (18)\nExtr a \nComplex (23)\nFigure 2: Vega-Lite dataset divided by their complexity levels: simple, medium, complex, extra complex. These 48 charts were\nselected via stratified sampling and used in our evaluation (Section 5). The level is divided based on the number of keys each\nspecification contains. The number of keys, which are the criteria for dividing the levels, are set based on the quartiles (Q1, Q2,\nQ3) of Vega-Lite example gallery dataset [70].\n3.2 Quantitative Analysis\n3.2.1 Benchmarks. We compare three synthetic and two real-world\nVega-Lite datasets [18, 32, 47, 70, 98] described in Section 2. To en-\nsure a fair comparison, we implement a process to remove exact\ncode duplication within each benchmark. In detail, each specifi-\ncation is sorted in alphabetical order by the keys and edited to\nmaintain consistent indentation. Next, we convert each file into a\nhash where files with identical hashes are subsequently removed\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nTable 2: Summary statistics of our dataset and benchmark datasets that are publicly available. Two types of datasets are\npresented: synthetic and real-world datasets. The best statistics within each type are highlighted in bold, while the best statistics\nacross all datasets are also underscored.\nType Evaluation Metric / Criteria\nSynthetic data (machine-generated) Real-world data (human-generated)\nData2Vis [18] Chartseer [98] nvBench [47] Kim et al. [32] Gallery [70] Ours\nQuantity # of specs 4,318 9,897 6,680 52 709 1,981\nComplexity\nTotal # of keys across specs 101,881 147,676 98,074 769 26,469 107,802\nAverage # of keys in a spec 24 15 15 15 37 54\nSimple (key ≤16) 0 6,164 6,354 41 186 73\nMedium (key ≤24) 4,318 3,733 326 10 170 199\nComplex (key ≤41) 0 0 0 1 179 733\nExtra complex (key> 41) 0 0 0 0 174 976\nAverage depth of JSON 4.00 3.00 3.48 3.13 5.01 5.19\nAverage branching factor 1.22 1.44 1.18 1.17 1.41 1.38\nDiversity\nTotal # of unique keys 24 12 18 31 275 362\nAverage pairwise edit distance 122.62 75.90 48.18 129.51 1,096.11 1,549.48\nComposite views 0 0 0 0 136 746\nInteraction (e.g., zoom, pan) 0 0 0 0 188 1,010\n# of chart types 6 6 4 2 10 10\nfrom the dataset. Following this procedure, the number of specifi-\ncations in Chartseer dataset decrease from 9,917 to 9,897, nvBench\ndecrease from 7,241 to 6,680, and the Vega-Lite gallery example\ndataset decrease from 716 to 709.\n3.2.2 Quality Metrics. To comprehensively assess the Vega-Lite\ndatasets, we consider three different aspects: quantity, complexity,\nand diversity. Initially, we count the number of collected specifica-\ntions to determine the overall quantity of Vega-Lite specifications,\nas previously done by Luo et al. [47]. However, we argue that addi-\ntional metrics are necessary to gauge the quality of the Vega-Lite\ndataset. This is because some specifications include only manda-\ntory properties to construct a single plot without any interaction\n(e.g., data, encoding, mark for a simple bar chart), while oth-\ners contain multiple plots or views linked by varying interactions.\nTherefore, the number of keys in a specification can highly differ\ndepending on whether it includes properties for data pre-processing\n(e.g., aggregate, calculate, etc.), interactivity (e.g., bind, se-\nlect, etc.), or composite views (e.g., concat, repeat, etc.). We\ncan expect the Vega-Lite specification becomes more complex as\nthe number of defined properties increases. Therefore, we propose\na new standard to understand the complexity of a Vega-Lite dataset\nby counting the total number of keys present across all specifica-\ntions and the average number of keys in a singe specification. To\nensure a fair comparison, we only consider keys defined in the ver-\nsion 5 specification. We also ignore keys associated with internally\nembedded datasets, such as values and datasets, along with\ntheir corresponding keys. In addition to this, we also measure the\naverage depth and branching factor of the JSON structure as they\nare commonly adopted to evaluate the complexity of a JSON file.\nWe found no metrics to quantify the diversity of chart dataset\n[11]. Therefore, we also propose metrics for gaining insights into the\ndiversity of dataset in terms of both the range of properties within\nthe entire dataset and the variance between individual specifica-\ntions. Specifically, we count the number of unique keys employed\nacross the entire dataset and calculate the average pairwise edit\ndistance among all possible pairs of specifications. The number of\nunique keys indicates how many distinct properties that can be de-\nfined in a Vega-Lite specification are used across the specifications.\nFor example, if a handful of unique keys are used within the dataset,\nthis indicates a restricted recurrence of only a few properties. In\nturn, it likely signifies a low level of diversity. The average pairwise\nedit distance provides an overview of the dissimilarity between\neach pair at the code level. To perform this analysis, we sort the\nkeys alphabetically, replace their corresponding values with empty\nvalues, and exclude keys associated with embedded datasets, as\nmentioned earlier.\n3.2.3 Complexity Levels. We observe that the existing criteria used\nto establish the complexity levels of charts are somewhat subjective\nand may not possess broad applicability [ 31, 45, 47]. Instead, we\nsuggest using the number of keys as a criterion for categorizing the\ncomplexity levels of charts, particularly in the context of Vega-Lite\nspecifications. This is because, as explained above, the number of\nproperties increases proportionately to the number of keys in a\nspecification. To establish the standard number of keys, we refer to\nthe Vega-Lite example gallery dataset [70] and calculate the quar-\ntiles (Q1, Q2, Q3) based on the distribution of the number of keys.\nThese quartiles, specifically 16, 24, and 41, are utilized as reference\npoints to divide the specifications’ level of complexity. For instance,\na specification with a total number of keys less than or equal to 16\nis classified as ‘simple’ complexity. Likewise, a specification with a\ntotal number of keys greater than 16 and less than or equal to 24 is\nclassified as ‘medium’ complexity (Figure 2).\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n3.2.4 Composite View, Interactivity, and Chart Type Distribution.\nWe choose three additional factors by referring to previous works\n[6, 40] to further assess the quality of the datasets. First, we examine\nthe presence of composite views, which offer diverse perspectives\non the same data simultaneously [12]. Secondly, considering the\nbenefits of collecting Vega-Lite specifications over static bitmap\nimages, we count the number of charts that incorporate interactive\ntechniques such as tooltips, zooming, and brushing. Lastly, we eval-\nuate the number of charts types based on the taxonomy proposed\nby Borkin et al. [6].\n3.2.5 Results. We present the results in terms of quantity, com-\nplexity, and diversity, highlighting the superiority of our dataset\ncompared to the benchmarks. Regarding quantity, all three syn-\nthetic datasets demonstrate a higher number of specifications com-\npared to the other three real-world datasets. Among all datasets,\nChartseer shows the highest number of specifications (i.e., 9,897),\nwhile our dataset has 1,981 specifications which outnumbers the\nother real-world datasets in terms of quantity.\nIn terms of complexity, our dataset ranks the first in average\nnumber of keys in a single specification (i.e., 54) and the second in\ntotal number of keys across specifications (i.e., 107,802), which is 1.4\nand 4.0 times larger than the largest previous real-world Vega-Lite\ndataset, respectively. Chartseer presents the highest total number\nof keys across specifications (i.e., 147,676) with the smallest average\nnumber of keys per specification (i.e., 15) among all datasets. Our\ndataset includes the highest number of specifications classified as\ncomplex (i.e., 733) and extra complex (i.e., 976), while all synthetic\ndatasets do not contain any specifications in the complex and extra\ncomplex level. Data2Vis and nvBench demonstrate the largest num-\nber of specifications classified as medium (i.e., 4,318) and easy (i.e.,\n6,354), respectively. Our dataset also exhibits the highest average\ndepth of JSON structure (i.e., 5.19), while Chartseer showcases the\nhighest average branching factor (i.e., 1.44).\nLastly, with respect to diversity, our dataset demonstrates the\nlargest total number of unique keys and the highest average pair-\nwise edit distance among all datasets. Furthermore, our dataset\nincludes the largest number of specifications featuring composite\nviews (i.e., 1,010) and interactions (i.e., 746), exceeding the Vega-\nLite gallery dataset by 1.8 and 5.3 times, respectively. None of\nthe synthetic datasets or Kim et al. ’s dataset include specifications\nwith composite views and interactions. Both our dataset and the\nVega-Lite gallery dataset cover the widest variety of chart types,\nencompassing ten types: Area, Bar, Circle, Diagram, Distribution,\nGrid & Matrix, Line, Map, Point, and Trees & Networks. Please refer\nto Table 2 for detailed results.\n4 VL2NL: NL GENERATION FRAMEWORK\nThe goal of our framework is to generate high-quality NL datasets\nusing Vega-Lite specifications and prompt engineering. VL2NL\nconsists of three stages (Figure 3). First it preprocesses underlying\ndatasets (e.g., csv) and minifies the Vega-Lite specifications. Next,\nit identifies relevant and accurate information through guided-\ndiscovery. Last, it increases syntactic diversity using score-based\nparaphrasing. To generate each type of NL dataset, we design each\nprompt to be maximally helpful by selecting the most appropriate\nstrategies (Table 3).\nTable 3: Prompting techniques to generate each NL dataset.\nEach prompt is designed by choosing the most appropriate\ntechniques considering their different characteristics.\nTarget Technique L1 caption L2 caption Utterance Question\nSemantic(S) Scaffolding O - O -\n(K) Keyquestion - O O O\nSyntactic Paraphrasing - - O O\n4.1 Pre-processing Vega-Lite Specifications\nUsing raw Vega-Lite specifications is not appropriate for prompt-\ning, because some of them include the dataset they use within the\nspecification, resulting in excessively file length. Therefore, we save\nthe data as an external files with the most suitable data formats\n(e.g., .csv, .json). Subsequently, the location of the saved files\nis overwritten with their URLs, rather than being embedded in\nthe specification. In our current implementation of the framework,\nwe only support .csvdata format. Therefore, we have converted\n.jsonfiles into .csvfiles. Last, we minify the Vega-Lite specifi-\ncations by removing all line breaks and indentations to reduce the\nnumber of tokens sent through API usage.\n4.2 Ensuring Accuracy and Relevance\nOur framework leverages the concept of guided discovery [7] based\non Chain-of-Thought prompting [87] to harness the maximum rea-\nsoning capability of LLMs. We employ two strategies of guided\ndiscovery: providing scaffolds [23]. and posing key questions [15].\nTo analyze and integrate the chart semantics necessary for gener-\nating a specific NL dataset, we assist LLMs by offering scaffolds.\nAdditionally, we furnish LLMs with key questions to guide their\nself-directed progress. This maximizes the use of LLMs’ reasoning\nabilities, allowing them to make decisions on which aspects to fo-\ncus on and delve into when creating a particular data. Below, we\ndenote each step where we italicize the relevant phrases, and utilize\nsymbols for (S) scaffolding and (K) key question to make them\neasily identifiable.\nTo demonstrate how we can ensure relevance and accuracy in\ngenerating different types of NL datasets, we have selected three\ndatasets commonly used in NLIs for data visualization research.\nWe made these selections based on their significance in conjunc-\ntion with related tasks, as indicated in a recent survey paper [74]:\ncaptions (L1, L2), utterance (command, query, question), question\n(visual-lookup, visual-compositional, nonvisual-lookup, nonvisual-\ncompositional, open-ended). The detailed generation process for\neach NL dataset can differ from one another. Here, we design each\nstep in prompting to be merged or separated when generating\ndifferent NL datasets so they can best capture each of their charac-\nteristics. We only explain the high-level descriptions of each, and\nthe detailed and full prompting used for generating each NL dataset\nis presented in Appendix A.\n4.2.1 L1 Caption. Considering real-world Vega-Lite specifications,\nwe first understand whether the given chart is (S) a composite\nview (e.g., layered, trellis, and multiple views), to enable top-down\nanalysis of each chart one by one. The prompt follows a template\nwith three questions to answer: Is it a composite view?; If it is,\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nV ega-Lite specification\n{\n  \"$schema\": ”vega-lite/v5.json\",\n  \n  \"mark\": \"bar\",\n  “encoding”: {\n  ....\n}\n“data”: {\n    “Region”: [ ... ],\n    “Profit”: [ ... ],\n  },\nPr epr ocessing\n Guided disco v er y (b y LLM)\nGener ated NL datasets\n P ar aphr ased NL datasets\nInput Stage 1 Stage 2 Stage 3\nOutput 1 Output 2\na b c\ne f\ndata.csv\nMinified\nVega-Lite\nspec\nScor e-based par aphr asing\nRewrite the following sentence as if it \nwere spoken by a person with a given \nscore for language usage\n\nScore: 1-5\n\nLanguage Axis: subjectivity, formality,\nclarity, and expertise\nL1 Caption:  T his is a bar char t that uses data fr o m  a superst or e .  T he char t encodes the 'R egion '  fie l d on \nthe x -a x is and the su m  of ' Pr ofit '  on the y-a x is .  T he ' Ship Status '  fie l d is r epr esented b y diff er ent co l ors .\n\nL2 Caption:  T he visua l i z ation pr esents the t ota l  pr ofit for each r egion ,  w ith the W est r egion ha ving the \nhighest pr ofit of appr o x i m ate l y 67860.56  and the South r egion ha ving the l o w est pr ofit of appr o x i m ate l y \n26551.72.\n\nUtter ance:  C r eate a bar char t sho w ing the su m  of pr ofit fr o m  diff er ent r egions .\n\nQuestion:  W hich r egion has the l ongest bar r epr esenting the highest t ota l  pr ofit ?\nUtter ance: C ou l d y ou possib l y put t ogether a visua l,  \nm a ybe a bar char t ,  that giv es us an idea of ho w  pr ofits \nar e distributed acr oss v arious r egions ?\n\nQuestion: W hich ar ea is depicted w ith the m ost \ne x tended bar sy m bo l i z ing the m a x i m u m  o v er a ll  gain ?\n Scaffoldin g\ne.g., composite view, field s\nchart semantics\nK ey Q uestion s\ne.g., W hat is the most prominent \nand meaningful feature in the \ngiven chart ?\nd\nFigure 3: LLM Framework to Generate NL Datasets for Visualizations. We start by (b) preprocessing underlying datasets and\nminifying Vega-Lite specifications. Subsequently, (c) we employ scaffolding and key questions, (e) to generate NL datasets\nlike L1/L2 captions, utterances, and questions. (d) This is followed by score-based paraphrasing, (f) allowing us to produce\nsyntactically paraphrased NL datasets.\nTable 4: Results of automatic qualitative coding [ 21]. From previous NL datasets of captions, utterances, and questions, we\nidentified four language axes of syntactic diversity: subjectivity, formality, clarity, and expertise. The top five most frequently\noccurring codes within each axis are presented along with their respective frequencies in parentheses.\nAxes Formality Clarity Expertise Subjectivity\nDirections Colloquial/Standard Implicit/Explicit Non-technical/Technical Subjective/Objective\nExample\ncodes\ninterrogative form (540) specificity (221) economic (159) descriptive (611)\nformal (384) specific (91) geographical context (125) negative connotation (58)\npassive voice (211) ambiguity (68) financial (106) subjectivity (22)\nanalytical (195) conciseness (64) business-oriented (81) negative (15)\ncommand-oriented (166) abstract (63) business terminology (65) third person perspective (11)\nidentify its type among layered, trellis, and multiple views; and\ndetermine the number of plots in the chart. Next, it analyzes each\nchart individually based on the provided scaffold of (S) chart se-\nmantics: Data, Transform, Mark, Chart-Type, Encoding, Style, and\nInteraction, using the information about composite view. Here, we\naccess the underlying dataset to provide(S) fields that are presented\nin the Vega-Lite specification, along with their synonyms (i.e., (S)\ntitles also found in the same Vega-Lite specification) and their (S)\nunique values if they are categorical variables. After analyzing all\nof these semantics, the LLM finally generates the L1 caption by\ncombining them.\n4.2.2 L2 Caption. L2 captions, unlike L1 captions that provide an\noverall description of the chart, offer the flexibility to selectively\nfocus on specific features that capture the viewer’s interest. To\ncraft informative and insightful captions, we follow a structured\napproach centered around a key question: (K) What is the most\nprominent and meaningful feature in the given chart? Once we\nidentify this feature, we delve deeper by exploring the mathematical\noperations required to analyze it: (K) What is the mathematical\noperation(s) required to describe the feature? Subsequently, based\non these operations, we generate (K) a series of questions to an-\nalyze the feature (e.g., for the simple line chart with a red border\nin Figure 2, the following questions are generated: What was the\nhighest stock price of Google?; What was the lowest stock price\nof Google?; What is the difference between the highest and lowest\nstock prices of Google?). This process allows us to create captions\nthat provide valuable insights into the chart’s content. When an-\nswering questions, we utilize backing datasets and LangChain [9] to\nperform required calculation that bolster the factual integrity of the\ngenerated captions. This step is crucial, as large language models\n(LLMs) have been known to produce hallucinations in response\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nTable 5: Example of score-based paraphrasing with two axes. We used a sample LLM-generated utterance to create the chart\nshown in Figure 3. We perform linear interpolation to manipulate formality and expertise scores from 1 to 5, one at a time to\ngenerate 25 paraphrased sentences. While the content (i.e., semantics) of the utterance remains consistent, the tone and voice\n(i.e., syntax) change linearly in accordance with the provided scores.\nSample Utterance: Create a bar chart showing the sum of profit from different regions.\nExpertise (Non-technical: 1, Technical: 5)\n1 2 3 4 5\n1\nHey, can you whip up\na bar graph showing\nhow much dough we’ve\nmade from different\nplaces?\nHey, can you make a bar\ngraph showing the total\nprofit from different re-\ngions?\nCan you put together a\nbar chart showing the\naggregate profit from\nvarious geographical ar-\neas?\nCan you construct a bar\nchart illustrating the cu-\nmulative profit derived\nfrom distinct regions?\nCan you generate a\nbar chart delineating\nthe summation of fiscal\ngain from disparate ge-\nographical sectors?\nFormality (Colloquial: 1, Standard: 5)\n2\nCould you create a bar\nchart that shows how\nmuch money we’ve\nmade from different\nplaces?\nCould you create a bar\nchart that shows the to-\ntal profit from different\nregions?\nCould you create a bar\nchart that illustrates the\naggregate profit from\nvarious geographical ar-\neas?\nCould you create a bar\nchart that delineates the\ncumulative profit de-\nrived from distinct re-\ngions?\nCould you create a bar\nchart that represents\nthe summation of fiscal\ngain from disparate ge-\nographical sectors?\n3\nPlease create a bar\nchart showing how\nmuch money we’ve\nmade from different\nplaces.\nPlease create a bar chart\nshowing the total profit\nfrom different regions.\nPlease create a bar chart\nillustrating the aggre-\ngate profit from various\ngeographical areas.\nPlease create a bar chart\ndelineating the cumula-\ntive profit derived from\ndistinct regions.\nPlease create a bar\nchart representing the\nsummation of fiscal\ngain from disparate\ngeographical sectors.\n4\nIt is requested that you\ncreate a bar chart show-\ning the money made\nfrom different places.\nIt is requested that you\ncreate a bar chart show-\ning the total profit from\ndifferent regions.\nIt is requested that you\ncreate a bar chart il-\nlustrating the aggregate\nprofit from various geo-\ngraphical areas.\nIt is requested that you\ncreate a bar chart delin-\neating the cumulative\nprofit derived from dis-\ntinct regions.\nIt is requested that you\ncreate a bar chart repre-\nsenting the summation\nof fiscal gain from dis-\nparate geographical sec-\ntors.\n5\nYou are required to\nconstruct a bar chart\ndemonstrating the mon-\netary gain from various\nlocations.\nYou are required to\nconstruct a bar chart\ndemonstrating the total\nprofit from different re-\ngions.\nYou are required to con-\nstruct a bar chart il-\nlustrating the aggregate\nprofit from various geo-\ngraphical areas.\nYou are required to con-\nstruct a bar chart delin-\neating the cumulative\nprofit derived from dis-\ntinct regions.\nYou are required to con-\nstruct a bar chart repre-\nsenting the summation\nof fiscal gain from dis-\nparate geographical sec-\ntors.\nto mathematical problems [28]. Once each question is answered,\nthe collected information is subsequently incorporated into the\nfinal prompting stage for generating L2 captions. It’s important to\nnote that, unlike previous work [84], we do not use any of the L1\ncaptions when generating L2 captions. Instead, this is performed\nas an independent process.\n4.2.3 Utterance. Similar to L1 captions, we begin by analyzing\nwhether the chart is a (S) composite view. We then proceed to\ngenerate (S) instructions for each plot independently. This process\nentails creating a comprehensive set of step-by-step instructions for\nconstructing each plot. To enhance readability and user-friendliness,\nwe ensure that each instruction focuses on a single specific action,\naligning with the same semantics used when generating L1 captions.\nFor example, in the case of the simple line chart with a red border\nshown in Figure 2, the following instructions are generated:\n•Data: Use Google’s stock price data;\n•Chart-Type: Create a line chart;\n•Mark: Use a line mark;\n•Encoding: Encode the x-axis with the date field, using a\ntemporal type and a time unit of year, month, date, hours,\nand minutes, and scale it using UTC;\n•Encoding: Encode the y-axis with the price field, using a\nquantitative type.\nHowever, it is important to note that the generated instructions\nmay sometimes feature overly technical variable names from the\nchart, which might not align with users’ NL usage patterns. In\nsuch cases, we leverage information from synonyms found in the\nunderlying dataset. Specifically, we use (S) title of the Vega-Lite\nspecification and (S) values from the fields to replace the technical\nterms, resulting in more user-friendly instructions.\nNext, we ask a key question to (K) identify primary and sec-\nondary information. In this context, we anticipate that LLMs are\nable to automatically prioritize crucial semantics to paint a compre-\nhensive picture of the chart, such as chart type or encoding, over\nadditional instructions like style or interaction. This thought is\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nbased on Wang et al. ’s observations [86], who noted that the typical\nworkflow for creating visualizations often starts with this informa-\ntion (e.g., ’show me the price over time as a line chart’). Once we\nhave all these components ready, we proceed to generate each type\nof utterance one by one, adhering to(S) specific rules for each type.\nFor commands, we employ the imperative voice. For queries, we\nuse only variables, fields, attributes, mathematical formulas, abbre-\nviations, and prepositions, while avoiding verbs and articles. For\nquestions, we formulate inquiries in the form of questions. Across\nall types, we maintain the following rules: express each utterance\nin a single sentence, utilize only primary information, and keep the\nlanguage concise and straightforward.\n4.2.4 Question. In general, we conduct chart question answering\nto facilitate decision-making [32]. Thus, the process involves an-\nalyzing charts through a question-answering, which ultimately\nleads to a conclusion and informs the decision-making. To gener-\nate questions, we employ a reverse thought process. This entails\nfirst identifying the decisions that can be derived from the charts\n(i.e., (K) What higher-level decision can be made by analyzing this\nchart?), followed by formulating a possible conclusion that leads to\nsuch a decision (i.e., (K) What is a possible conclusion that can be\nreached from this decision?). Finally, we determine what needs to\nbe analyzed (i.e., (K) What specific value can be retrieved to reach\nthis conclusion? What are the mathematical operations to reach the\nconclusion?). We generate non-visual lookup and compositional\nquestions using the provided values and mathematical operations.\nTo transform these into visual questions, we identify the necessary\nvisual attributes and incorporate them into the generated questions\n(i.e., (K) What visual attributes are required to paraphrase this\nquestion?). Finally, we formulate an open-ended question designed\nto lead to the same conclusion obtained in the previous step.\n4.3 Increasing Syntactic Diversity\n4.3.1 Automatic Qualitative Coding. Before increasing the syntac-\ntic diversity of NL datasets, we need to analyze which meaningful\naxes of diversity to address. To this end, we collected sample NL sen-\ntences from existing sources, which consist of 2,147 captions, 893\nutterances, and 629 questions [32, 45, 79]. Next, following the auto-\nmatic coding process that Hämäläinen et al. have proposed [21], we\nutilized these sample sentences to conduct a thematic analysis using\nLLMs, generating five different codes for each caption, utterance,\nand question (see the prompt in Appendix B). We manually checked\nthe generated codes to eliminate irrelevant and erroneous ones,\nresulting in 15,271 valid codes out of 18,345. Then, we retained 2,759\nunique codes and vectorized them using Sentence-Bert [65]. After-\nwards, we applied dimensionality reduction technique to project\nthem into a lower dimensional space using UMAP [54], reducing\nthe 100-dimensional vectors to 5-dimensional vectors. Next, we\nemployed HDBSCAN [53] to cluster them into a few classes for\ndetailed investigation. We aggregated clusters into a higher-level\ncluster, except for the codes that are not clustered through HDB-\nSCAN, to derive the final themes.\nWe identified a total of six themes, but selected four meaningful\naxes related to NL syntax–clarity, expertise, formality, subjectivity\n(Table 4). Two themes were removed–1) measurement, and 2) chart\nand data analytics–as they are not directly related to the syntax of\nNL datasets but rather to the semantic properties of charts. Clarity\nrepresents a language axis with two opposite meanings—implicit\nand explicit. Implicit language relies on context, shared knowledge,\nand non-verbal cues to convey meaning, while explicit language is\nclear and direct, leaving little room for interpretation or misunder-\nstanding. The expertise axis also has two opposite meanings—non-\ntechnical and technical. Technical language includes specialized\nterminology and jargon, whereas non-technical language is more\naccessible to a general audience and avoids the use of complex\nterms. Formality, the third language axis, ranges from colloquial,\nwhich is informal and used in everyday conversation, to standard,\nwhich follows established rules and conventions. Finally, the sub-\njectivity axis encompasses subjective language, which expresses\npersonal opinions, feelings, or judgments, and objective language,\nwhich presents facts or information without bias or personal inter-\npretation.\n4.3.2 Score-based Paraphrasing. Our paraphrasing technique is\ninspired by a linear interpolation in the latent space for image gen-\neration and manipulation as demonstrated in many system and\napplication papers [1, 3, 38, 58]. This technique enables a smooth\ntransition from one expression to another by focusing on creating\ncontrollable and meaningful syntactic variations of a single sen-\ntence. The key idea is that we assign language axesand employ\na five-point Likert-scale to each. Here, we focus on altering only\nthe sentence’s syntax, while maintaining its meaning. In detail,\nwe provide LLMs with a sentence we want to paraphrase, and an\nexplanation about one of the defined axes and its two directions.\nWe assign a specific value on a Likert scale ranging from one to five,\nto paraphrase the sentence as if it were spoken by a person using a\nlanguage with a certain degree indicated by the score. This tech-\nnique can be extended to involve multiple axes and scores (refer to\nan example result with two axes in Table 5). The detailed prompts\nwe used are presented in Appendix C.\n5 EXPERIMENTS\nIn this section, we introduce quantitative analysis of our generated\nNL datasets, lexical analysis on generated utterances, and types of\nlow-level tasks in generated questions.\n5.1 Experimental Setup\nOur experiment aims to investigate the effectiveness of our frame-\nwork in generating diverse NL datasets from Vega-Lite specifi-\ncations, with a focus on accuracy and diversity. To achieve this,\nwe apply tailored metrics to each NL dataset, taking into account\ntheir different characteristics. L1/L2 captions are independent of\nthe perception of humans or machines because they focus on con-\nveying objective information [ 45]. Thus, we measured accuracy\nto determine how precisely each caption level contained relevant\ninformation. We assess the diversity of utterances and questions, as\nit is important to reflect inclusive language usage among individu-\nals with different background. The results are presented in Table 6\nand Table 7 where each type of NL dataset is classified with capital\nEnglish letter (A-G).\n5.1.1 Benchmarks. For utterances and questions, we utilized crowd-\nsourced NL datasets gathered in prior studies [32, 79] (F-BM and\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nTable 6: Accuracy of the generated chart semantics and L1/L2 captions for 48 sample charts (Figure 2). Although 41 out of the 48\nsample charts used in our experiment are complex and extra complex, LLMs were able to capture chart semantics and generate\nL1/L2 captions successfully in general.\nMetadata Accuracy\nNL Type (#) Source Chart/NL # w/ Strict criteria w/ Lenient criteria\nA. Chart Semantics (9) LLM 48/432 89.4% 96.9%\nB. L1 Caption (1) LLM 48/48 76.0% 95.8%\nC. L2 Caption (1) LLM 48/48 76.0% 87.5%\nTable 7: Quantitative comparison of benchmarks and LLM-generated utterances and questions. Two type of metrics were adopted,\ncross-distribution, which is to compare the two distributions to get the similarity and difference, and within-distribution,\nwhich is to compare the diversity within a single distribution. Each NL dataset has come from 4 sources, gold standard or\nbenchmarks, LLM, LLM.P (paraphrased), LLM.P2 (paraphrased with 2 axes). The best metric from all sources are bold, while the\nbest metric in ours (LLM, LLM.P, LLM.P2) are underlines.\nMetadata Cross-Distribution Within-Distribution\nNL Type (#) Source Chart/NL # FD (↓) Precision ( ↑) Recall (↑) RC (↑) Chamfer ( ↑) MST ( ↑) Span ( ↑) Sparsness (↑) Entropy (↑)\nD. Utterance (3)\nGold 48/144 · · · 3.15 0.19 47.59 3.26 2.34 2.60\nLLM\n48/144\n0.58 0.81 0.31 3.31 0.19 49.81 3.41 2.48 2.54\nLLM.P 0.45±0.01 0.81±0.01 0.66±0.01 3.65±0.36 0.16±0.01 54.58±3.63 3.52±0.21 2.70±0.22 2.09±0.51\nLLM.P2 0.46±0.00 0.80 ±0.02 0.67 ±0.03 3.43±0.38 0.16 ±0.01 52.91 ±3.11 3.50 ±0.22 2.49 ±0.20 2.27 ±0.35\nE. Question (5)\nGold 48/240 · · · 3.47 0.17 70.28 3.45 2.64 2.44\nLLM\n48/240\n0.35 0.84 0.56 6.20 0.09 105.16 5.92 4.40 1.68\nLLM.P 0.35±0.00 0.76±0.02 0.64 ±0.03 4.17±0.20 0.13 ±0.01 70.00 ±3.47 4.28 ±0.30 3.13 ±0.11 2.51±0.10\nLLM.P2 0.36±0.00 0.74 ±0.02 0.64 ±0.03 4.29±0.34 0.14 ±0.01 77.36±5.91 4.44 ±0.27 3.12 ±0.19 2.21 ±0.34\nF. Utterance (3)\nBM [79] 30/804 · · · 10.42 0.07 177.63 10.56 8.41 2.42\nLLM 30/90 · · · · · · · · -\nLLM.P 30/804 1.11±0.27 0.63 ±0.03 0.51±0.05 12.36±0.18 0.06±0.00 209.59±7.07 11.74 ±0.45 9.66 ±0.38 2.40 ±0.06\nLLM.P2 30/804 0.87±0.56 0.58±0.04 0.43 ±0.08 12.24±0.18 0.06 ±0.00 227.70±15.66 11.86±0.37 9.79±0.19 2.45±0.07\nG. Question (4)\nBM [32] 52/629 · · · 8.66 0.07 202.83 11.36 6.12 1.96\nLLM 52/208 · · · · · · · · -\nLLM.P 52/619 0.33±0.00 0.52±0.01 0.13±0.01 11.95±0.27 0.05±0.00 247.16±11.72 12.46±0.49 9.07±0.32 2.41±0.14\nLLM.P2 52/629 0.33±0.00 0.50 ±0.01 0.18 ±0.01 11.61±0.18 0.06 ±0.00 222.39±8.21 12.69±0.24 8.73±0.25 2.39 ±0.06\nG-BM). In case of utterance dataset, we only used the singleton case,\nso it was 804 sentences instead of 893. However, when it comes\nto captions, we could not find suitable benchmarks for comparing\nwith different caption levels. Previous research employed bitmap\nimages of charts [45, 84], whereas our approach leverages Vega-Lite\nspecifications. This difference in data format prevented us from\nmaking an exact comparison.\n5.1.2 Gold Standard Datasets. Given that benchmarks mostly fo-\ncus on simple and medium level complexity with confined diversity,\nwe decided to make a gold standard dataset to test the generalizable\nperformance of our framework over diverse and complex charts.\nWe referred to previous works [ 33, 39] that have demonstrated\nhow to create gold standard datasets. We selected 48 Vega-Lite\nspecifications (Figure 2) by stratified sampling, taking into account\ntheir complexity level and whether they included interaction or\ncomposite views. Subsequently, three visualization experts (first\nthree authors) collaborated to develop three guidelines for gen-\nerating utterances and questions. These guidelines were crafted\nby referring to relevant suggestions and guidelines from prior re-\nsearch [32, 34, 79, 86]. We began by creating sample utterances and\nquestions for the same chart using the initial drafts, and jointly\nrevised each guideline by reviewing the generated NL datasets.\nAfter making consensus about the final guidelines Appendix D,\nwe divided the charts into thirds, with each person tasked with\ngenerating NL datasets for their assigned charts. This resulted in 48\nutterances (comprising 16 commands, 16 queries, and 16 questions)\nand 80 questions (including 16 non-visual lookup, 16 non-visual\ncompositional, 16 visual lookup, 16 visual compositional, and 16\nopen-ended questions) per each expert. After one expert created NL\ndatasets for the assigned charts, the other two individuals conducted\nverification to find any issues or errors within these generated NL\ndatasets. In cases where issues or errors were detected, all three\nexperts convened to discuss and reach a consensus on how to ad-\ndress them. This collaborative effort resulted in the generation of\n144 utterances with three different phrasings and 240 questions\ncategorized into five types (D-Gold and E-Gold).\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nTable 8: Types of low-level tasks in questions Top four ranked\nin both datasets were identical, while LLM-generated dataset\nhas more questions assigned to these ranks.\nLow-level Analytical Task Gold # (%) LLM # (%)\nRetrieve Value 94 (39.2%) 103 (42.9%)\nFind Extremum 35 (14.6%) 65 (27.1%)\nCorrelate 31 (12.9%) 41 (17.1%)\nCompute Derived Value 31 (12.9%) 17 (7.1%)\nFilter 23 (9.6%) 3 (1.3%)\nFind Anomalies 14 (5.8%) 0\nCharacterize Distribution 5 (2.1%) 4 (1.7%)\nSort 3 (1.3%) 0\nCluster 1 (0.4%) 0\nDetermine Range 1 (0.4%) 5 (2.1%)\nETC 2 (0.8%) 2 (0.8%)\nSum 240 (100%) 240 (100%)\n5.1.3 LLM-generated Datasets. To generate our datasets, we used\nan official API of GPT44 with the gpt-4-0613model. We set the\ntemperature to 0.0, to solely observe the influence of our paraphras-\ning technique on diversity. We used different prompt for generating\neach dataset and paraphrasing the generated NL datasets (see Ap-\npendix A and Appendix C). Here, we generated all types of chart\nsemantics, captions, utterances, and questions for the 48 sample\ncharts, as well as all types of utterances for 30 charts from the\nbenchmark. This resulted in a total of 432 chart semantics (A-LLM),\n48 L1 captions (B-LLM), 48 L2 captions (C-LLM), 144 utterances\n(D-LLM), and 240 questions (E-LLM) for the 48 sampled charts, and\n90 utterances for the 30 benchmark charts (F-LLM). Since the bench-\nmark [32] did not include open-ended questions, we generated only\nfour types of questions. This led to a total of 208 questions for the\n52 charts (G-LLM).\nWe augmented our NL datasets for utterances and questions\nusing the generated NL datasets (*-LLM) and the score-based para-\nphrasing technique, resulting in augmented paraphrased NL datasets\n(*-LLM.P and *-LLM.P2). With four language axes and five Likert-\nscale values (1-5), it is possible to generate 20 different versions (4*5)\nof paraphrased sentences for each original sentence (i.e., LLM.P).\nLikewise, in case of two axes, there are six combinations chosen\nfrom the four axes. Since there are five Likert-scale options for\neach axis, this leads to the generation of 150 (6*5*5) different para-\nphrased sentence versions per original sentence (i.e., LLM.P2). We\nmeticulously generated all possible paraphrases and selected five\ndistinct sets of NL datasets to mitigate any sampling bias. Thus\nwe calculated metrics and their averages and standard deviations\nacross these five sample sets.\nWhen sampling the paraphrased sentences, our goal is to com-\npare the syntactic diversity of different NL datasets while aligning\nthe semantic diversity of the two datasets being compared to en-\nsure a fair comparison. To this end, we adjust the frequency of\neach chart-NL pair in both datasets. This is necessary because the\nbenchmark data exhibit biases in NL sentence distribution for each\n4https://platform.openai.com/docs/models/gpt-4\nchart. For instance, one chart has 30 associated questions, while an-\nother chart has only one question. We count the frequency of each\nchart-NL pair and reflect the same frequency when augmenting\nthe datasets. This became an issue when creating G-LLM.P, since\none chart has 30 questions, which exceeds the maximum number\nof paraphrases possible (limited to 20) through our single-axis para-\nphrasing method. As a result, our overall number of NL datasets\nreaches 619.\nLast, we included open-ended questions in E-LLM.P and E-LLM.P2,\nas these questions were available in E-Gold. However, we did not\ninclude them in G-LLM.P and G-LLM.P2 in Table 7 to preserve the\nsemantic diversity of the datasets.\n5.1.4 Procedure. We manually grade chart semantics and L1/L2\ncaptions to compute their accuracy. To enhance the reliability of our\nscoring, two experts (the first and second authors) independently\nscored them and calculated the average score. Specifically, the chart\nsemantics include whether they contain composite views, the type\nof composite view, the number of plots, chart type, mark, trans-\nform, encoding, style, and interaction. We scored whether each of\nthem is correct or not. However, during our evaluation of style, we\nencountered many cases where multiple width or height values\nwere defined within the Vega-Lite specification. In such cases, we\nchose to exclude the width and height information from our style\nevaluation. Moreover, we encountered many cases that were hard\nto definitively categorize as either correct or incorrect. For instance,\nsituations where nine lines were drawn on the same chart but di-\nvided into separate layers, resulting in a count of nine plots instead\nof one. As a result, we adopted two different scoring approaches,\nconsisting of strict and lenient criteria. Strict criteria only considers\nthose that were 100% accurate. For instance, if a stacked bar chart\nwas categorized as a bar chart, it was deemed incorrect. Conversely,\nwith lenient criteria, we adopted a more flexible approach, con-\nsidering the aforementioned cases as correct. We extended these\ncriteria to the evaluation of L1/L2 captions as well as their formal\ndefinitions [45]. As they contain objective information, we applied\nthe same two criteria and reasoning to assess their accuracy.\nTo assess the quality of utterances and questions in compar-\nison to both the benchmark and the gold standard dataset, we\nemploy two types of statistical metrics: within-distribution and\ncross-distribution metrics. The within-distribution metrics are de-\nsigned to calculate the similarity and divergence between a given\ndataset and another dataset by means of comparison. Examples of\nsuch metrics include Frechet distance (FD), precision, and recall.\nBy utilizing these metrics, we can evaluate how closely a given\ndistribution aligns with the benchmark distribution. These metrics\nhave already been applied in the comparison of human-generated\nand LLM-generated datasets [21]. To this end, we vectorize the gold\nstandard, benchmarks, and LLM-generated as well as paraphrased\ndatasets, transforming them into sets of vectors for quantitative\ncomparison.\nHowever, we recognize that the aforementioned metrics may not\nprovide a comprehensive measure of the quality of LLM-generated\nand -paraphrased NL datasets. These metrics mainly focus on the\ncoverage of distribution rather than emphasizing diversity. It is\ncrucial to delve deeper into a single distribution, as duplicate or\nhighly similar data points may be present within it [ 37, 76]. To\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\naddress this, we incorporate cross-distribution metrics [ 66] that\nallow us to quantify the diversity within a single distribution. These\nmetrics include remote-clique (average of mean pairwise distances),\nChamfer distance (average of minimum pairwise distances), MST\ndispersion (sum of edge weights of MST), span (Pth percentile\ndistance to centroid), sparseness (mean distance to medoid), and\nentropy (Shannon-Wiener index for points in a grid partition).\n5.2 Quantitative Results\nWe first report the accuracy of chart semantics and L1/L2 captions.\nUnder the strict criteria, the accuracy rates for chart semantics, L1\ncaptions, and L2 captions were 89.4%, 76.0%, and 76.0%, respectively.\nIn detail, accuracy under strict criteria reveals that ‘chart-type’\nachieved the lowest accuracy at 75%, while ‘mark’ and ‘interac-\ntion’ showed the highest accuracy at 96.9%. Under lenient criteria,\nthe accuracy rates for chart semantics, L1 captions, and L2 cap-\ntions significantly improved to 96.9%, 95.8%, and 87.5%, respectively.\nSpecifically, the lowest accuracy for chart semantics was observed\nin the ‘number of plots’ (88.5%), while ‘mark’ and ‘interaction’ main-\ntained the highest accuracy at 100%. Additionally, the accuracy of\nchart type substantially improved to 97.9%. A summary of these\nresults is provided in Table 6.\nWe next report the diversity of utterance and question. In terms\nof cross-distribution metrics, LLM.P exhibited the highest quality in\nterms of precision (D), precision and recall (F), and precision and FD\n(G). In case of datasets containing five question types (E), the metric\nresults were not consistent. Specifically, LLM.P performed the best\nin FD, LLM was the best for precision, and LLM.P2 achieved the\nhighest recall. When considering within-distribution metrics, LLM-\ngenerated and paraphrased datasets demonstrated greater diversity\ncompared to the gold standard and benchmark datasets. On average,\nhigher diversity was observed in 4.75 out of six metrics. For both\nquestion and utterance datasets (E, F), paraphrased datasets with\ntwo axes demonstrated greater diversity than paraphrased datasets\nwith one axis in four out of six metrics. Conversely, in the other two\ndatasets (D, G), paraphrased datasets with one axis exhibited higher\ndiversity in four out of six metrics. In the utterance dataset (D),\nparaphrasing increased diversity in four out of six metrics, whereas\nin the question dataset (E), paraphrasing reduced diversity in four\nmetrics. A summary of the results is presented in Table 7.\n5.3 Lexical Analysis in Utterances\nTo gain a deeper understanding of the syntactic diversity in LLM-\ngenerated datasets, we conducted a lexical analysis on three NL\ndatasets (F-BM, F-LLM.P, F-LLM.P2) to investigate the types of\nwords used within each dataset. Our pre-processing steps encom-\npassed sentence tokenization, converting all text to lowercase, re-\nmoval of stopwords, and lemmatization. As evidenced by the quan-\ntitative outcome presented in the previous section, the LLM.P exhib-\nited a notable richness in its lexical diversity. It contained a total of\n555 unique words, surpassing the benchmark dataset’s count of 349\nunique words. Also, the total word count in the LLM.P, amounting\nto 7,132 words, exceeded that of the benchmark dataset, which\nconsisted of 4,480 words. In case of LLM.P2, it demonstrated an\neven greater number of unique words, totaling 608, surpassing both\nthe benchmark and LLM.P datasets in this regard. However, the\noverall word count in LLM.P2 was lower at 6,645 words compared\nto the LLM.P dataset (7,132 words).\nThere were some additional patterns in the use of specific words\nemployed within the LLM-generated datasets. First, the paraphrased\ndataset introduced a multitude of new action verbs. For instance,\nwhen issuing commands, terms such as construct, fabricate, or-\nganize, and arrange were employed to create charts (e.g., ‘Fabri-\ncate a line diagram’). In previous work [79], there was a tendency\namong crowd workers to adhere to specific terminology, thus re-\nsearchers have to be careful when providing instructions for col-\nlecting datasets. Our paraphrasing technique effectively addresses\nthis issue by promoting diverse syntax through the use of various\naction verbs automatically. Second, the datasets incorporate words\nthat may be adopted by people of specific groups or domains, but\nnot used often by ordinary people, such as domain-specific jargon\n(e.g., provenance, bifurcated, barometric, pecuniary). Last, certain\nwords have been adopted to introduce diverse tones and voices\nof the speaker. These encompass terms of a more personal and\ninformal nature, as well as expressions that convey uncertainty\nand speculation (e.g., maybe, seems, might, quite, sure), as well as\nwords that have been included to enhance conversational aspects\n(e.g., possibly, would, could).\n5.4 Types of Low-level Tasks in Questions\nBased on a taxonomy [2] comprising ten low-level analytical tasks,\nwe conducted an analysis of the question types present in the gold\nstandard and LLM-generated questions (E-Gold and E-LLM). This\nanalysis aimed to assess the dissimilarities or similarities between\nthese questions. To this end, we associated each low-level analytical\ntask with individual questions within both datasets.\nBoth datasets exhibited a congruent pattern, with identical rank-\nings for the top four elements. The task with the highest frequency\nin both datasets is retrieve value, which is unsurprising, as it con-\nsists of 40% of lookup questions in the dataset. Notably, in the LLM-\ngenerated dataset, the second most prevalent task is find extremum\nat 27.1%. This percentage closely aligns with Kim et al. ’s observation\n[32], where they reported a similar prevalence of questions related\nto extrema at 26.7%. Furthermore, it is worth highlighting that, akin\nto their research, there is a clear bias towards certain task types,\nincluding retrieve value, find extremum, correlate, and compute\nderived value Table 8.\n6 APPLICATION\n6.1 Finetuning LLMs for Data Visualization\nWe demonstrate that the NL datasets generated by our framework\ncan be used to augment the performance of ML models. It is im-\nportant to note, however, that the effectiveness of our datasets is\ncontingent upon the availability of a sufficient number of human-\ngenerated datasets that exhibit similar distributions to the test\ndatasets. We believe our approach serves as a cost-effective and effi-\ncient way to be used in conjunction with the conventional method\nof crowdsourcing human-generated NL datasets. In essence, our\nframework’s output can be strategically employed as supplemen-\ntary datasets for finetuning LLMs.\nTo be specific, to replicate the benchmark dataset’s experiment\n[79], we performed an experiment to classify ten chart types (e.g.,\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nTable 9: The result of finetuning experiment. The LLMs\ntrained with NL datasets generated by our framework ei-\nther matched or surpassed the performance of LLMs (C, D,\nE) compared to when using only the benchmark dataset (A).\nSource Train # Test # Accuracy (#)\nA. BM [79] 723 81 76.3% (61.8)\nB. LLM.P 723 81 58.8% (47.6)\nC. BM + LLM.P 723 81 76.8% (62.2)\nD. BM + LLM.P 1446 81 83.2% (67.4)\nE. BM + LLM.P + LLM.P2 2169 81 85.4% (69.2)\ncolored scatterplots, stacked & grouped bar charts, multiseries line\ncharts, etc.) using utterances. This classification task is important as\nit can be further used for building visualization systems like chart\ntype recommendation. We prepared five datasets for finetuning: A.\nthe benchmark dataset (723 utterances), B. the utterances generated\nand paraphrased with one axis by our framework (723 utterances),\nC. half of A and half of B (362 from A + 361 from B), D. A and B (723\nfrom A + 723 from B), E. D as well as the utterances generated and\nparaphrased with two axes by our framework (1446 from D + 723\nadditional utterances). Only 90% of the benchmark dataset is used\nfor finetuning and the rest 10% were used for the test. Similarly,\nwe used only 90% of our datasets to maintain an equal number of\nutterances as in the benchmark dataset. Following common ML\npractices, we selected OpenAI’s babbage-002model for training\nsmaller models on downstream tasks, setting hyperparameters to\ndefault configurations (i.e., number of epochs as 3, learning rate\nmultiplier as 2, and the batch size were 1, 1, 1, 2, 4 for each case).\nEach experiment was repeated five times to calculate the average\naccuracy to mitigate the stochastic behavior of LLMs.\nAs denoted in Table 9, we observed an increase in performance\nwhen using LLM-generated NL datasets alongside the benchmark\ndataset for finetuning the models. Using only the benchmark datasets\nresulted in an accuracy of 76.3% (61.8 accurate prediction on av-\nerage out of 81, Table 9-A). When we combined the benchmark\ndatasets with our dataset, the accuracy slightly improved to 76.8%\n(62.2 out of 81, see Table 9-C), indicating that the addition of a\nnon-human-generated datasets did not negatively impact accuracy.\nMoreover, the performance increased to 83.2% when we leveraged\nadditional NL datasets generated by LLMs (67.4 out of 81, Table 9-\nD). The accuracy was the highest when we used more NL datasets\nparaphrased with two language axes by our framework, which is\n85.4% (69.2 out of 81, Table 9-E). Last, using only LLM-generated\nNL datasets showed decreased accuracy, which is 58.8% (47.6 out of\n81, Table 9-B).\nThe results suggest that using the NL datasets, generated and\nparaphrased by our framework, can enhance the performance of\nML models in downstream tasks. We believe a key factor in this\nimproved performance is the increased syntactic diversity of the\ngenerated utterances, which also accurately mimic semantic char-\nacteristics. Our results align with a previous finding that utilizing\nAI-generated datasets can become a more cost-effective strategy\nfor training scalable ML models with significantly fewer human\nlabels [4]. This suggests that the synergistic use of both human\nefforts and our automated framework can substantially enhance\nthe quality of training data and the performance of the models.\n6.2 Leveraging Fully-automatic and\nMixed-initiative Modes in VL2NL\nTo further explore how visualization researchers can use our frame-\nwork, we performed a case study with two experts: E1, a professor,\nand E2, a postdoctoral researcher. Both have earned their Ph.D.\nin visualization and have conducted research for 10 and 7 years,\nrespectively.\nFor the case study, we implemented a system with two modes:\nfully-automatic and mixed-initiative (Figure 4). In the fully-automatic\nmode, the scaffolding is set by us and key questions are generated\nautomatically by the LLMs. Therefore, users had no control, but\ncould click the button to generate NL datasets for their chosen\ncharts. In the mixed-initiative mode, users can select which scaf-\nfolding to consider and provide additional information as answers\nto key questions. They can actively contribute by specifying di-\nrections to steer its focus accordingly. For example, in case of L1\ncaption, they can choose which chart semantics to consider or add\nmore when generating it. Similarly, for question, users can make a\nhigh-level decision themselves, specifying where or what to focus\non when analyzing the charts.\nTo clarify our study protocol, we first provided the experts with\nan overview of our framework’s concept. Following this, they were\ngiven a task to create NL utterances for 10 line charts, which de-\npicted stock prices of various technology companies [92]. This was\nconducted using two modes: fully-automatic and mixed-initiative.\nWe emphasized to the participants that the utterances they gener-\nated would be instrumental in training an ML model to translate\nthese utterances back into the corresponding line charts. We also\nhighlighted the significance of utterance diversity in enhancing\nthe performance of ML models, based on our discussion in (Sec-\ntion 6.1). Finally, the experts provided feedback on their anticipated\nuse of both modes for generating utterances. Each expert spent\napproximately 45 minutes for the study.\nBoth experts agreed on using both modes to generate utterances\nmore effectively for training ML models. Specifically, E1 suggested\nthe following scenario: initially, researchers generate a large num-\nber of utterances automatically to observe their distribution. Next,\nthey identify areas lacking in diversity, which then become the focus\nfor generating additional utterances subsequently. By repeatedly\ntesting and generating utterances in these sparse areas, particu-\nlarly using the mixed-initiative mode, they can achieve a more\ndiverse and evenly distributed utterances. This process, iterated\nover multiple times, could improve the performance of the ML mod-\nels. Similarly, E2 also advocated starting with the fully-automatic\nmode before using the mixed-initiative mode. E2 said this approach\nallows experts to better understand the model’s behavior and the\nnature of the utterances it generates. This step is crucial to avoid\n‘option paralysis, ’ a state of cognitive overload that may occur when\nfaced with a lot of choices without a clear strategy for improve-\nment. With a deeper understanding of the model’s behavior, they\ncan proceed more effectively.\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nA\nD\nB C\nE\nFigure 4: A system with two modes (fully-automatic and mixed-initiative) to generate NL datasets using VL2NL. The mixed-\ninitiative mode encompasses several features. First, users can select the types of NL datasets they want to generate (C). They\ncan inspect each chart (A) and subsequently choose the specific ones they wish to use for generating NL datasets (E). Users can\nchange or provide information that the system utilizes (B). Once these are completed, the system returns the generated NL\ndatasets (D). In contrast, the fully-automatic mode does not include (B). As a result, dataset generation in this mode strictly\nfollows the scaffolding defined by researchers, along with key questions and answers generated by LLMs.\n7 DISCUSSION\n7.1 Strengths and Weaknesses of VL2NL\n7.1.1 VL2NL Can Guide Itself via Key Questions. We observed sev-\neral interesting key questions discovered by LLMs that play a vi-\ntal role in guiding themselves. They were formulated through a\nmeticulous analysis of chart contents. Various decision-making pro-\ncesses were identified, spanning diverse domains such as financial\ndecision-making (e.g., assessing whether to invest in a company’s\nstock), public policy planning (e.g., formulating policies based on\nemployment trends across different age groups and countries), and\nlocation-based business strategies (e.g., selecting optimal sites for\na new shoe factory relative to the distribution of existing facili-\nties). These key questions served as the foundation for eliciting\nsubsequent conclusions, retrieving specific values, and deciding\nmathematical operations for generating interesting questions.\n7.1.2 VL2NL Works Robustly on Different Chart Complexity. Con-\nsidering that the 48 sampled charts mostly belong to the categories\nof complex and extra-complex charts, our observations indicate that\nthe reported accuracy (Table 6) pertaining to chart semantics and\nL1/L2 captions does not exhibit a dependence on the complexity\nlevels of the Vega-Lite specifications. This finding underscores the\nrobustness of the system. For instance, it successfully generated\nan accurate L1 caption for a chart comprising two views intercon-\nnected through selection interactions (see Figure 5-a). Similarly, it\neffectively generated an L2 caption for a chart containing multiple\nplots, allowing the selection of a data range in the main bar plot\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\nL1 Caption: This visualization consists of two \nviews. The first view is a geoshape char t that \nshows the geogr aphical distribution of FR V \ndistricts. The districts ar e color ed based on the \nper centage of r esponses abo v e the 7.7-minute \nstandar d. The user can select a quar ter and a \ndistrict t o view detailed information. The \nsecond view is a line char t that r anks the \nstations based on the per centage of r esponses \nabo v e the standar d. The stations ar e \nr epr esented b y lines and cir cles, and the user \ncan select a station t o highlight it. The char t \nalso includes text marks that label the stations \nand their r ankings.\nL1 caption: This is a Stack ed Ar ea Char t that \nvisualiz es the GDP of G7 countries fr om 1952 \nt o 2017. The x-axis r epr esents the y ear , and the \ny-axis r epr esents the t otal GDP . Each countr y is \nr epr esented b y a diff er ent color . The ar eas ar e \nsemi-tr anspar ent, but become opaque when \nselected fr om the legend. A t ooltip pr o vides \nadditional information when an ar ea is ho v er ed \no v er .\nL1 caption: This la y er ed scatter plot visualiz es \ndata about hate crimes in the UK, specifically \nfocusing on women. The scatter plot uses \npoints t o r epr esent the 'v alue ' of each r ecor d, \nwith diff er ent shapes indicating diff er ent \n' models'. The points ar e color ed and filled \nbased on the ' gr oup ' field. A rule mark is used \nt o indicate the r ange between ' min ' and ' max' \nfor each ' gr oupaction '. The color scheme used \nis ' dark2'.\nL2 Caption: The visualization shows the t otal number of v oters in each \ndepar tment of Colombia, with Bogota D .C. ha ving the highest number of \nv oters at 5,702,805 and V aupes ha ving the least at 21,537. The a v er age \nabstention per centage acr oss all depar tments is appr o ximately 50.21%, \nindicating that about half of the t otal v oters did not v ote. The diff er ence \nbetween the t otal number of v oters and the actual v oters v aries \nsignificantly acr oss depar tments, with Bogota D .C. ha ving the highest \ndiff er ence of 2,130,107 and V aupes ha ving the least diff er ence of 13,482.\na\nb\nc d\nFigure 5: Four examples of generated L1/L2 captions with corresponding charts. We found that VL2NL can successfully generate\ncaptions even on complex charts with varying interactions and multiple views.\nto trigger the highlighting of related data points in other plots (see\nFigure 5-d).\n7.1.3 VL2NL Depends Highly on Vega-Lite Specifications. We ob-\nserved that the framework is highly dependent on the Vega-Lite\nspecifications in generating NL datasets. In many cases, this de-\npendency is advantageous as it enables a focus on intricate func-\ntionalities such as interactions. For a particular chart, determining\nthe presence of interactions was challenging because the selection\ninteraction was indicated solely by the color label of the chart.\nNevertheless, the framework successfully captured this (see Fig-\nure 5-b). Similarly, charts lacking titles or descriptions can pose\na challenge in comprehending the content of charts. However, it\nappears that the framework was able to extract additional informa-\ntion, even utilizing the URL of the data included in the specification\n(e.g., how-did-levels-of-uk-hate-crime-change-during-\nand-after-covid-19/data/f5.csv), enabling the generation\nof informative and coherent captions (see Figure 5-c).\nHowever, we have identified certain cases where relying solely\non Vega-Lite specifications proves disadvantageous. First, in some\ninstances, the generated NL datasets include information that was\nnot visually represented in the chart but was present in the Vega-\nLite specifications. For instance, additional categories or values\nthat exceed the specified axis range were presented in the NL\ndataset. Second, if certain information is not explicitly stated within\nthe Vega-Lite specification, it cannot be incorporated into the NL\ndataset. For example, when generating trellis plots, the number of\nplots is determined using the unique count of elements. However,\nsince the number is not explicitly provided in the specification, our\nframework is unable to predict the exact plot number accurately.\nLast, any errors present in the Vega-Lite specifications are faithfully\nrepresented in the generated NL datasets. For instance, a specifi-\ncation contained a typo that divided facets into 3 parts but was\nmistakenly denoted as 4, our framework predicted the number of\nplots as 4 instead of the correct 3. Similarly, the Vega-Lite specifica-\ntion included code that is non-functional, which is reflected in the\ngenerated captions, resulting in inaccuracies.\n7.1.4 VL2NL Predicts Only Common Chart Types. Our categoriza-\ntion relied on the chart type taxonomy presented by Borkin et\nal. [6], which led to different categorizations even when the same\nmark was used. For instance, although the same point mark was\nemployed, it could be interpreted as either a distribution chart (e.g.,\ndot array) or a scatter plot. Furthermore, we conducted an analysis\nof the charts by considering their detailed sub-types rather than\ngrouping them into larger categories. For instance, a chart featuring\na stacked area chart was considered incorrect according to strict\ncriteria if it was predicted as an area chart. However, we observed\nthat in most cases, the LLM framework tended to assign the charts\nto the most prevalent and common chart types such as scatterplots,\narea charts, and bar charts, rather than classifying them as distribu-\ntion charts, stacked area charts, or stacked bar charts (Figure 5-c).\nWith this reasons, the chart type exhibited the highest accuracy\ngap between strict and lenient criteria.\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n7.2 Limitations and Future Work\n7.2.1 Enriching Capabilities of VL2NL through External Resources.\nWhile Vega-Lite specifications serve as powerful inputs for gen-\nerating various types of NL datasets, it is inherently challenging\nto extract information that does not exist within these specifica-\ntions. Although our framework can operate in both fully-automatic\nand mixed-initiative manner, it does not rely on external resources.\nThis limitation can potentially impact the performance of NL gen-\neration, as it aligns with observations in guided discovery, where\ninsufficient prior knowledge can hinder learners from formulating\nhypotheses, interpreting data effectively, and engaging in system-\natic experimentation [15]. To enhance the capabilities of VL2NL,\nwe suggest accessing external information to guide the process dur-\ning NL dataset generation. For instance, generating L3/L4 captions\noften necessitates access to common or domain-specific knowledge\n[45]. In this regard, employing tools like ReAct [94] becomes advan-\ntageous, as it facilitates reasoning to assist the model in deducing,\ntracking, and updating action plans while also handling exceptions.\nThis enables us to proactively retrieve information from the web\nwhen required.\n7.2.2 Augmenting Vega-Lite Specifications. While we have pre-\nsented the largest amount of Vega-Lite specifications and acknowl-\nedge their ability as input for generating diverse NL datasets, it is\nnoteworthy that the quantity of Vega-Lite specifications is signifi-\ncantly smaller compared to bitmap images. This is mainly because\ncollecting Vega-Lite specifications is more challenging when com-\npared to other formats. This limitation hinders the effective training\nor fine-tuning of machine learning models to achieve robust perfor-\nmance. Consequently, we posit the need for methods to augment\nVega-Lite specifications. Various augmentation techniques have\nbeen introduced and adopted for bitmap images of charts to in-\ncrease both their quantity [29] and diversity [98]. However, to the\nbest of our knowledge, we have not found any pertinent research\nthat addresses the augmentation of Vega-Lite specifications. As part\nof our future work, we aim to tackle this gap by developing a re-\nverse engineering technique [63] specifically designed for Vega-Lite\nspecifications.\n7.2.3 Covering Additional NL datasets. Our framework exhibits\npotential for generalization across multiple NL datasets. However,\nwe recognize that it covers only limited number of types, which we\naim to expand in our future research. Specifically, we plan to create\nconversational NL datasets to facilitate interactive communication\nwith NLIs, given the growing significance of conversational agents.\nWe believe a dataset based on deeper analysis of users’ conversa-\ntional characteristics will be immensely beneficial for researchers.\nWe also plan to address reference datasets linking charts with text\nto help make interactive documents. We believe this will make the\nconnection between them clearer, and the reading experience more\nenjoyable and engaging.\n8 CONCLUSION\nWe introduce VL2NL designed to generate diverse NL datasets\naimed at enhancing NLIs for data visualization research. Our frame-\nwork takes a Vega-Lite specification as input and employs guided\ndiscovery to accurately generate various NL datasets, including\ncaptions, utterances, and questions. We also propose a score-based\nparaphrasing approach to enhance the syntactic diversity of the\ngenerated NL datasets. We also present a dataset comprising 1,981\nVega-Lite specifications. This dataset surpasses the baselines in\nterms of complexity and diversity. Our experimental results sub-\nstantiate that the framework excels in accurately generating both\nL1 and L2 captions, while achieving higher diversity in the genera-\ntion of utterances and questions compared to the baselines. Last, we\nintroduce real-world scenarios of using LLM-generated NL datasets\nand our framework. We hope our framework and chart collection\ncan advance research in developing NLIs for data visualization.\nACKNOWLEDGMENTS\nThis work was supported by the National Research Foundation of\nKorea (NRF) grant funded by the Korea government (MSIT) (No.\n2023R1A2C200520911). This work was also supported by Institute of\nInformation & communications Technology Planning & Evaluation\n(IITP) grant funded by the Korea government (MSIT) (No.2019-0-\n00075, Artificial Intelligence Graduate School Program (KAIST)).\nREFERENCES\n[1] Rinat Abdrashitov, Fanny Chevalier, and Karan Singh. 2020. Interactive Explo-\nration and Refinement of Facial Expression Using Manifold Learning. In Proceed-\nings of the 33rd Annual ACM Symposium on User Interface Software and Technology\n(Virtual Event, USA)(UIST ’20). Association for Computing Machinery, New York,\nNY, USA, 778–790. https://doi.org/10.1145/3379337.3415877\n[2] Robert Amar, James Eagan, and John Stasko. 2005. Low-level components of\nanalytic activity in information visualization. In IEEE Symposium on Information\nVisualization, 2005. INFOVIS 2005. IEEE, 111–117.\n[3] Toshiki Aoki, Rintaro Chujo, Katsufumi Matsui, Saemi Choi, and Ari Hautasaari.\n2022. EmoBalloon-Conveying Emotional Arousal in Text Chats with Speech\nBalloons. In CHI Conference on Human Factors in Computing Systems . 1–16.\n[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\net al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073 (2022).\n[5] Michael S Bernstein, Greg Little, Robert C Miller, Björn Hartmann, Mark S\nAckerman, David R Karger, David Crowell, and Katrina Panovich. 2010. Soylent:\na word processor with a crowd inside. In Proceedings of the 23nd annual ACM\nsymposium on User interface software and technology . 313–322.\n[6] Michelle A Borkin, Azalea A Vo, Zoya Bylinskii, Phillip Isola, Shashank Sunkavalli,\nAude Oliva, and Hanspeter Pfister. 2013. What makes a visualization memorable?\nIEEE transactions on visualization and computer graphics 19, 12 (2013), 2306–2315.\n[7] Ann L Brown and Joseph C Campione. 1994. Guided discovery in a community of\nlearners. The MIT Press.\n[8] Stuart K Card, Jock Mackinlay, and Ben Shneiderman. 1999. Readings in informa-\ntion visualization: using vision to think . Morgan Kaufmann.\n[9] Harrison Chase. 2022. LangChain. https://github.com/hwchase17/langchain\n[10] Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann\nBansal, and Ajay Joshi. 2020. Leaf-qa: Locate, encode & attend for figure question\nanswering. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision . 3512–3521.\n[11] Chen Chen and Zhicheng Liu. 2023. The State of the Art in Creating Visualization\nCorpora for Automated Chart Analysis. Computer Graphics Forum (2023). https:\n//doi.org/10.1111/cgf.14855\n[12] Xi Chen, Wei Zeng, Yanna Lin, Hayder Mahdi Ai-Maneea, Jonathan Roberts, and\nRemco Chang. 2020. Composition and configuration patterns in multiple-view\nvisualizations. IEEE Transactions on Visualization and Computer Graphics 27, 2\n(2020), 1514–1524.\n[13] Kenneth Cox, Rebecca E Grinter, Stacie L Hibino, Lalita Jategaonkar Jagadeesan,\nand David Mantilla. 2001. A multi-modal natural language interface to an infor-\nmation visualization environment. International Journal of Speech Technology 4\n(2001), 297–314.\n[14] Kenny Davila, Srirangaraj Setlur, David Doermann, Bhargava Urala Kota, and\nVenu Govindaraju. 2020. Chart mining: A survey of methods for automated chart\nanalysis. IEEE transactions on pattern analysis and machine intelligence 43, 11\n(2020), 3799–3819.\n[15] Ton De Jong and Wouter R Van Joolingen. 1998. Scientific discovery learning\nwith computer simulations of conceptual domains. Review of educational research\n68, 2 (1998), 179–201.\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\n[16] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan,\nYang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A mobile app dataset\nfor building data-driven design applications. In Proceedings of the 30th annual\nACM symposium on user interface software and technology . 845–854.\n[17] Victor Dibia. 2023. LIDA: A Tool for Automatic Generation of Grammar-Agnostic\nVisualizations and Infographics using Large Language Models. arXiv preprint\narXiv:2303.02927 (2023).\n[18] Victor Dibia and Çağatay Demiralp. 2019. Data2vis: Automatic generation of\ndata visualizations using sequence-to-sequence recurrent neural networks. IEEE\ncomputer graphics and applications 39, 5 (2019), 33–46.\n[19] Siwei Fu, Kai Xiong, Xiaodong Ge, Siliang Tang, Wei Chen, and Yingcai Wu.\n2020. Quda: natural language queries for visual data analytics. arXiv preprint\narXiv:2005.03257 (2020).\n[20] Tong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu, and Karrie G Karahalios.\n2015. Datatone: Managing ambiguity in natural language interfaces for data\nvisualization. In Proceedings of the 28th annual acm symposium on user interface\nsoftware & technology . 489–500.\n[21] Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large\nlanguage models in generating synthetic hci research data: a case study. In\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems .\n1–19.\n[22] Jonathan Harper and Maneesh Agrawala. 2017. Converting basic D3 charts into\nreusable style templates. IEEE transactions on visualization and computer graphics\n24, 3 (2017), 1274–1286.\n[23] Cindy E Hmelo-Silver, Ravit Golan Duncan, and Clark A Chinn. 2007. Scaffolding\nand achievement in problem-based and inquiry learning: a response to Kirschner,\nSweller, and. Educational psychologist 42, 2 (2007), 99–107.\n[24] Enamul Hoque, Parsa Kavehzadeh, and Ahmed Masry. 2022. Chart question\nanswering: State of the art and future directions. In Computer Graphics Forum ,\nVol. 41. Wiley Online Library, 555–572.\n[25] Enamul Hoque, Vidya Setlur, Melanie Tory, and Isaac Dykeman. 2017. Applying\npragmatics principles for interaction with visual analytics. IEEE transactions on\nvisualization and computer graphics 24, 1 (2017), 309–318.\n[26] Kevin Hu, Diana Orghian, and César Hidalgo. 2018. DIVE: A mixed-initiative\nsystem supporting integrated data exploration workflows. In Proceedings of the\nworkshop on human-in-the-loop data analytics . 1–7.\n[27] Maeve Hutchinson, Aidan Slingsby, Radu Jianu, and Pranava Madhyastha. 2023.\nTowards Visualisation Specifications from Multilingual Natural Language Queries\nusing Large Language Models. In EuroVis 2023 - Posters , Christina Gillmann,\nMichael Krone, and Simone Lenti (Eds.). The Eurographics Association. https:\n//doi.org/10.2312/evp.20231072\n[28] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in\nnatural language generation. Comput. Surveys 55, 12 (2023), 1–38.\n[29] Daekyoung Jung, Wonjae Kim, Hyunjoo Song, Jeong-in Hwang, Bongshin Lee,\nBohyoung Kim, and Jinwook Seo. 2017. Chartsense: Interactive data extraction\nfrom chart images. In Proceedings of the 2017 chi conference on human factors in\ncomputing systems . 6706–6717.\n[30] Sean Kandel, Ravi Parikh, Andreas Paepcke, Joseph M Hellerstein, and Jeffrey\nHeer. 2012. Profiler: Integrated statistical analysis and visualization for data\nquality assessment. In Proceedings of the International Working Conference on\nAdvanced Visual Interfaces . 547–554.\n[31] Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh\nThakkar, Enamul Hoque, and Shafiq Joty. 2022. Chart-to-text: A large-scale\nbenchmark for chart summarization. arXiv preprint arXiv:2203.06486 (2022).\n[32] Dae Hyun Kim, Enamul Hoque, and Maneesh Agrawala. 2020. Answering ques-\ntions about charts and generating visual explanations. In Proceedings of the 2020\nCHI conference on human factors in computing systems . 1–13.\n[33] Dae Hyun Kim, Enamul Hoque, Juho Kim, and Maneesh Agrawala. 2018. Facil-\nitating document reading by linking text and tables. In Proceedings of the 31st\nAnnual ACM Symposium on User Interface Software and Technology . 423–434.\n[34] Dae Hyun Kim, Vidya Setlur, and Maneesh Agrawala. 2021. Towards understand-\ning how readers integrate charts and captions: A case study with line charts. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems .\n1–11.\n[35] Robert Kincaid and Graham Pollock. 2017. Nicky: Toward a virtual assistant for\ntest and measurement instrument recommendations. In 2017 IEEE 11th Interna-\ntional Conference on Semantic Computing (ICSC) . IEEE, 196–203.\n[36] Aniket Kittur, Boris Smus, Susheel Khamkar, and Robert E Kraut. 2011. Crowd-\nforge: Crowdsourcing complex work. In Proceedings of the 24th annual ACM\nsymposium on User interface software and technology . 43–52.\n[37] Mark Klein and Ana Cristina Bicharra Garcia. 2015. High-speed idea filtering\nwith the bag of lemons. Decision Support Systems 78 (2015), 39–50.\n[38] Hyung-Kwon Ko, Subin An, Gwanmo Park, Seung Kwon Kim, Daesik Kim, Bo-\nhyoung Kim, Jaemin Jo, and Jinwook Seo. 2022. We-toon: A Communication\nSupport System between Writers and Artists in Collaborative Webtoon Sketch\nRevision. In The 35th Annual ACM Symposium on User Interface Software and\nTechnology. 1–14.\n[39] Nicholas Kong, Marti A Hearst, and Maneesh Agrawala. 2014. Extracting refer-\nences between text and charts via crowdsourcing. In Proceedings of the SIGCHI\nconference on Human Factors in Computing Systems . 31–40.\n[40] Yixuan Li, Yusheng Qi, Yang Shi, Qing Chen, Nan Cao, and Siming Chen. 2022.\nDiverse interaction recommendation for public users exploring multi-view visu-\nalization using deep learning. IEEE Transactions on Visualization and Computer\nGraphics 29, 1 (2022), 95–105.\n[41] Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu,\nand Jianyong Wang. 2023. FlexKBQA: A Flexible LLM-Powered Framework for\nFew-Shot Knowledge Base Question Answering. arXiv preprint arXiv:2308.12060\n(2023).\n[42] Can Liu, Yun Han, Ruike Jiang, and Xiaoru Yuan. 2021. Advisor: Automatic\nvisualization answer for natural-language question on tabular data. In 2021 IEEE\n14th Pacific Visualization Symposium (PacificVis) . IEEE, 11–20.\n[43] Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene,\nChenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin\nAltun. 2022. DePlot: One-shot visual language reasoning by plot-to-table transla-\ntion. arXiv preprint arXiv:2212.10505 (2022).\n[44] Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,\nMandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin Eisenschlos. 2022.\nMatcha: Enhancing visual language pretraining with math reasoning and chart\nderendering. arXiv preprint arXiv:2212.09662 (2022).\n[45] Alan Lundgard and Arvind Satyanarayan. 2021. Accessible visualization via\nnatural language descriptions: A four-level model of semantic content. IEEE\ntransactions on visualization and computer graphics 28, 1 (2021), 1073–1083.\n[46] Yuyu Luo, Xuedi Qin, Nan Tang, and Guoliang Li. 2018. Deepeye: Towards\nautomatic data visualization. In 2018 IEEE 34th international conference on data\nengineering (ICDE) . IEEE, 101–112.\n[47] Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin.\n2021. Synthesizing natural language to visualization (NL2VIS) benchmarks\nfrom NL2SQL benchmarks. In Proceedings of the 2021 International Conference on\nManagement of Data . 1235–1247.\n[48] Yuyu Luo, Nan Tang, Guoliang Li, Jiawei Tang, Chengliang Chai, and Xuedi Qin.\n2021. Natural language to visualization by neural machine translation. IEEE\nTransactions on Visualization and Computer Graphics 28, 1 (2021), 217–226.\n[49] Jock Mackinlay. 1986. Automating the design of graphical presentations of\nrelational information. Acm Transactions On Graphics (Tog) 5, 2 (1986), 110–141.\n[50] Jock Mackinlay, Pat Hanrahan, and Chris Stolte. 2007. Show me: Automatic\npresentation for visual analysis. IEEE transactions on visualization and computer\ngraphics 13, 6 (2007), 1137–1144.\n[51] Anita Mahinpei, Zona Kostic, and Chris Tanner. 2022. LineCap: Line Charts\nfor Data Visualization Captioning Models. In 2022 IEEE Visualization and Visual\nAnalytics (VIS). IEEE, 35–39.\n[52] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque.\n2022. ChartQA: A benchmark for question answering about charts with visual\nand logical reasoning. arXiv preprint arXiv:2203.10244 (2022).\n[53] Leland McInnes, John Healy, and Steve Astels. 2017. hdbscan: Hierarchical density\nbased clustering. J. Open Source Softw. 2, 11 (2017), 205.\n[54] Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform man-\nifold approximation and projection for dimension reduction. arXiv preprint\narXiv:1802.03426 (2018).\n[55] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data\nwith language models: Towards zero-shot language understanding. Advances in\nNeural Information Processing Systems 35 (2022), 462–477.\n[56] Valerie S Morash, Yue-Ting Siu, Joshua A Miele, Lucia Hasty, and Steven Landau.\n2015. Guiding novice web workers in making image descriptions using templates.\nACM Transactions on Accessible Computing (TACCESS) 7, 4 (2015), 1–21.\n[57] Dominik Moritz, Chenglong Wang, Greg L Nelson, Halden Lin, Adam M Smith,\nBill Howe, and Jeffrey Heer. 2018. Formalizing visualization design knowledge\nas constraints: Actionable and extensible models in draco. IEEE transactions on\nvisualization and computer graphics 25, 1 (2018), 438–448.\n[58] Mohammad Amin Mozaffari, Xinyuan Zhang, Jinghui Cheng, and Jin LC Guo.\n2022. GANSpiration: Balancing Targeted and Serendipitous Inspiration in User\nInterface Design with Style-Based Generative Adversarial Network. In CHI Con-\nference on Human Factors in Computing Systems . 1–15.\n[59] Arpit Narechania, Adam Fourney, Bongshin Lee, and Gonzalo Ramos. 2021. DIY:\nAssessing the correctness of natural language to sql systems. In26th International\nConference on Intelligent User Interfaces . 597–607.\n[60] Arpit Narechania, Arjun Srinivasan, and John Stasko. 2020. NL4DV: A toolkit for\ngenerating analytic specifications for data visualization from natural language\nqueries. IEEE Transactions on Visualization and Computer Graphics 27, 2 (2020),\n369–379.\n[61] Jason Obeid and Enamul Hoque. 2020. Chart-to-text: Generating natural lan-\nguage descriptions for charts by adapting the transformer model. arXiv preprint\narXiv:2010.09142 (2020).\n[62] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra\nof human behavior. arXiv preprint arXiv:2304.03442 (2023).\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n[63] Jorge Poco and Jeffrey Heer. 2017. Reverse-engineering visualizations: Recovering\nvisual encodings from chart images. In Computer graphics forum , Vol. 36. Wiley\nOnline Library, 353–363.\n[64] Xin Qian, Eunyee Koh, Fan Du, Sungchul Kim, Joel Chan, Ryan A Rossi, Sana\nMalik, and Tak Yeon Lee. 2021. Generating accurate caption units for figure\ncaptioning. In Proceedings of the Web Conference 2021 . 2792–2804.\n[65] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing . Association for Computational\nLinguistics. https://arxiv.org/abs/1908.10084\n[66] Samuel Rhys Cox, Yunlong Wang, Ashraf Abdul, Christian Von Der Weth, and\nBrian Y. Lim. 2021. Directed diversity: Leveraging language embedding dis-\ntances for collective creativity in crowd ideation. In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems . 1–35.\n[67] Andy Rosenbaum, Saleh Soltan, Wael Hamza, Amir Saffari, Macro Damonte,\nand Isabel Groves. 2022. CLASP: Few-shot cross-lingual data augmentation for\nsemantic parsing. arXiv preprint arXiv:2210.07074 (2022).\n[68] Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer.\n2017. Vega-Lite: A Grammar of Interactive Graphics. IEEE Transactions on\nVisualization & Computer Graphics (Proc. InfoVis) (2017). https://doi.org/10.1109/\ntvcg.2016.2599030\n[69] Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer.\n2023. Vega Editor.\n[70] Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer.\n2023. Vega-Lite gallery.\n[71] Arvind Satyanarayan, Ryan Russell, Jane Hoffswell, and Jeffrey Heer. 2015. Re-\nactive vega: A streaming dataflow architecture for declarative interactive visu-\nalization. IEEE transactions on visualization and computer graphics 22, 1 (2015),\n659–668.\n[72] Timo Schick and Hinrich Schütze. 2021. Generating datasets with pretrained\nlanguage models. arXiv preprint arXiv:2104.07540 (2021).\n[73] Vidya Setlur, Sarah E Battersby, Melanie Tory, Rich Gossweiler, and Angel X\nChang. 2016. Eviza: A natural language interface for visual analysis. InProceedings\nof the 29th annual symposium on user interface software and technology . 365–377.\n[74] Leixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang, Xuming Hu, Xiongshuai\nZhang, Zhiwei Tai, and Jianmin Wang. 2022. Towards natural language interfaces\nfor data visualization: A survey. IEEE transactions on visualization and computer\ngraphics (2022).\n[75] Leixian Shen, Yizhi Zhang, Haidong Zhang, and Yun Wang. 2023. Data player:\nAutomatic generation of data videos with narration-animation interplay. arXiv\npreprint arXiv:2308.04703 (2023).\n[76] Pao Siangliulue, Joel Chan, Steven P Dow, and Krzysztof Z Gajos. 2016. Idea-\nHound: improving large-scale collaborative ideation with crowd-powered real-\ntime semantic modeling. In Proceedings of the 29th Annual Symposium on User\nInterface Software and Technology . 609–624.\n[77] Andrea Spreafico and Giuseppe Carenini. 2020. Neural data-driven captioning of\ntime-series line charts. In Proceedings of the International Conference on Advanced\nVisual Interfaces . 1–5.\n[78] Arjun Srinivasan, Steven M Drucker, Alex Endert, and John Stasko. 2018. Aug-\nmenting visualizations with interactive data facts to facilitate interpretation and\ncommunication. IEEE transactions on visualization and computer graphics 25, 1\n(2018), 672–681.\n[79] Arjun Srinivasan, Nikhila Nyapathy, Bongshin Lee, Steven M Drucker, and John\nStasko. 2021. Collecting and characterizing natural language utterances for\nspecifying data visualizations. InProceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems . 1–10.\n[80] Arjun Srinivasan and John Stasko. 2017. Natural language interfaces for data\nanalysis with visualization: Considering what has and could be asked. In Pro-\nceedings of the Eurographics/IEEE VGTC conference on visualization: Short papers .\n55–59.\n[81] Arjun Srinivasan and John Stasko. 2017. Orko: Facilitating multimodal interaction\nfor visual exploration and analysis of networks.IEEE transactions on visualization\nand computer graphics 24, 1 (2017), 511–521.\n[82] Chris Stolte, Diane Tang, and Pat Hanrahan. 2002. Polaris: A system for query,\nanalysis, and visualization of multidimensional relational databases. IEEE Trans-\nactions on Visualization and Computer Graphics 8, 1 (2002), 52–65.\n[83] Nicole Sultanum and Arjun Srinivasan. 2023. DataTales: Investigating the use\nof Large Language Models for Authoring Data-Driven Articles. arXiv preprint\narXiv:2308.04076 (2023).\n[84] Benny J. Tang, Angie Boggust, and Arvind Satyanarayan. 2023. VisText: A\nBenchmark for Semantically Rich Chart Captioning. In The Annual Meeting of\nthe Association for Computational Linguistics (ACL) . http://vis.csail.mit.edu/pubs/\nvistext\n[85] Manasi Vartak, Sajjadur Rahman, Samuel Madden, Aditya Parameswaran, and\nNeoklis Polyzotis. 2015. Seedb: Efficient data-driven visualization recommenda-\ntions to support visual analytics. In Proceedings of the VLDB Endowment Interna-\ntional Conference on Very Large Data Bases , Vol. 8. NIH Public Access, 2182.\n[86] Yun Wang, Zhitao Hou, Leixian Shen, Tongshuang Wu, Jiaqi Wang, He Huang,\nHaidong Zhang, and Dongmei Zhang. 2022. Towards Natural Language-Based Vi-\nsualization Authoring. IEEE Transactions on Visualization and Computer Graphics\n29, 1 (2022), 1222–1232.\n[87] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824–24837.\n[88] Kanit Wongsuphasawat, Dominik Moritz, Anushka Anand, Jock Mackinlay, Bill\nHowe, and Jeffrey Heer. 2015. Voyager: Exploratory analysis via faceted browsing\nof visualization recommendations.IEEE transactions on visualization and computer\ngraphics 22, 1 (2015), 649–658.\n[89] Kanit Wongsuphasawat, Dominik Moritz, Anushka Anand, Jock Mackinlay, Bill\nHowe, and Jeffrey Heer. 2016. Towards a general-purpose query language for\nvisualization recommendation. In Proceedings of the Workshop on Human-In-the-\nLoop Data Analytics . 1–6.\n[90] Kanit Wongsuphasawat, Zening Qu, Dominik Moritz, Riley Chang, Felix Ouk,\nAnushka Anand, Jock Mackinlay, Bill Howe, and Jeffrey Heer. 2017. Voyager 2:\nAugmenting visual analysis with partial view specifications. In Proceedings of the\n2017 chi conference on human factors in computing systems . 2648–2659.\n[91] Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey P\nBigham. 2023. WebUI: A Dataset for Enhancing Visual UI Understanding with\nWeb Semantics. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1–14.\n[92] Yumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets and\nhistorical prices. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . 1970–1979.\n[93] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mi-\nhir Kale, Adam Roberts, and Colin Raffel. 2022. Byt5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transactions of the Association for\nComputational Linguistics 10 (2022), 291–306.\n[94] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\nModels. In International Conference on Learning Representations (ICLR) .\n[95] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao\nYu, and Lingpeng Kong. 2022. Zerogen: Efficient zero-shot learning via dataset\ngeneration. arXiv preprint arXiv:2202.07922 (2022).\n[96] Jiacheng Ye, Chengzu Li, Lingpeng Kong, and Tao Yu. 2023. Generating Data for\nSymbolic Language with Large Language Models. arXiv preprint arXiv:2305.13917\n(2023).\n[97] Bowen Yu and Cláudio T Silva. 2019. FlowSense: A natural language inter-\nface for visual data exploration within a dataflow system. IEEE transactions on\nvisualization and computer graphics 26, 1 (2019), 1–11.\n[98] Jian Zhao, Mingming Fan, and Mi Feng. 2020. Chartseer: Interactive steering\nexploratory visual analysis with machine intelligence. IEEE Transactions on\nVisualization and Computer Graphics 28, 3 (2020), 1500–1513.\nA PROMPTS FOR NL GENERATION\nIn the prompt, certain variables are enclosed within curly brackets.\nWe colored them blue for easy recognition. Here, we provide a de-\ntailed explanation of each variable and specify its usage in different\nNL generation prompts:\n•vl[all]: Minified Vega-Lite specification;\n•ftt_str[L1/L2 caption, utterance]: Information about fields,\ntitles, types, and values;\n•prompt [L2 caption]: Questions derived from the guided\ndiscovery process;\n•info [L2 caption]: Answers for the questions computed\nthrough LangChain library [9];\n•info_first_concat[utterance]: A list of primary instruc-\ntions by analyzing the chart semantics.\nA.1 L1 Caption\n1 {vl}\n2\n3 Let's generate a level 1 NL description step by step.\n4\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\n5 Step 1. Determine if the visualization contains\ncomposite views, such as layered plots, trellis plots,\nor other types of multiple view displays, and provide\na count of the number of plots if any are present.\n6 Step 2. Analyze the semantics of each chart\nindividually, including [Data], [Transform], [Mark],\n[Chart-Type], [Encoding], [Style], and [Interaction].\nRefer to this:\n7 {ftt_str}\n8 Step 3. Generate a level 1 NL description using the\nsemantics. It contains elemental and encoded\nproperties of the visualization (i.e., the visual\ncomponents that comprise a graphical representation's\ndesign and construction).\n9\n10 ##\n11 Step 1. Composite Views:\n12 - True/False:\n13 - (If True) Type: (layered, trellis, multiple views)\n14 - Number of plots:\n15 Step 2. Chart Semantics:\n16 - Data:\n17 - Field (Value):\n18 - Transform:\n19 - Mark:\n20 - Chart-Type:\n21 - Encoding:\n22 - Style:\n23 - Interaction (e.g., tooltip):\n24 Step 3. Level 1 NL Description:\nA.2 L2 Caption\n1 {vl}\n2\n3 Let's generate question(s) step by step.\n4\n5 Step 1. What is the most prominent and meaningful\nfeature in the given chart?\n6 Step 2. What is the mathematical operation(s) (e.g.,\nmax, min, sum, difference, and average) required to\ndescribe the feature?\n7 Step 3. Generate question(s) using the mathematical\noperation(s) required to describe the feature. If\nthere are multiple questions, separate them with\nsemicolon(;).\n8\n9 ##\n10 Step 1. Features:\n11 Step 2. Operations:\n12 Step 3. Questions:\n1 Refer to this: {ftt_str}\n2 Do not draw any charts to answer the question.\n3\n4 Question: {prompt}\n1 Information: {info}\n2\n3 {ftt_str}\n4\n5 Generate a concise level 2 NL description of a\nvisualization, with 1 or 2 sentences. It contains\nstatistical and relational properties of the\nvisualization (e.g., descriptive statistics, extrema,\noutliers, correlations, point-wise comparisons).\n6\n7 ##\n8 Level 2 NL Description:\nA.3 Utterance\n1 {vl}\n2\n3 Step 1. Determine if the visualization contains\ncomposite views, such as layered plots, trellis plots,\nor other types of multiple view displays, and provide\na count of the number of plots if any are present.\n4 Step 2. Provide a list of instructions to create the\nchart using natural language.\n5 - Write instructions for each view and separate with\n<%>\n6 - Separate each instruction by a semicolon (;)\n7 - Divide each instruction to contain only one specific\naction\n8 - Use the following chart semantics to specify\ninstructions: [Data], [Chart-Type], [Mark],\n[Encoding], [Transform], [Style], [Interaction]\n9 Step 3. Given the information about the fields and\ntheir synonyms, please replace the field names with\ntheir corresponding synonyms.\n10 {ftt_str}\n11\n12 ##\n13 Step 1. Composite Views:\n14 - True/False:\n15 - (If True) Type: (layered, trellis, multiple views)\n16 - Number of plots:\n17 Step 2. Instructions:\n18 [View #]; [<Chart Semantic>]: <Instruction>; [<Chart\nSemantic>]: <Instruction>; ... <%>\n19 Step 3. Instructions:\n20 [View #]; [<Chart Semantic>]: <Instruction>; [<Chart\nSemantic>]: <Instruction>; ... <%>\n1 {inst_first_concat}\n2 The above are instructions to generate a chart. Let's\ngenerate combined instructions ([Command], [Query],\n[Question]) for each view step by step.\n3\n4 Step 1. Identify the primary information in each view.\n5 Step 2. Identify the secondary information in each\nview.\n6 Step 3. Generate a [Command] for each view using only\nthe primary info. Please follow these rules:\n7 - Use imperative voice\n8 - Write in a single sentence\n9 - Use only the primary info\n10 - Make it concise and simple\n11 Step 4. Generate a [Query] for each view using only\nthe primary info. Please follow these rules:\n12 - Refrain from using verbs and articles (e.g., a, the)\nNL Dataset Generation Framework for Visualizations Powered by LLMs CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n13 - Use only variables, fields, attributes, mathematical\nformulas (e.g., sum, avg, mix, max, count, order),\nabbreviations (e.g., vs), and prepositions (e.g., of,\nby, for, with, over, from, to)\n14 - Write in a single sentence\n15 - Use only the primary info\n16 - Make it concise and simple\n17 Step 5. Generate a [Question] for each view using only\nthe primary info. Please follow these rules:\n18 - Ask an inquiry as a question\n19 - Write in a single sentence\n20 - Use only the primary info\n21 - Make it concise and simple\n22\n23 ##\n24 View #<Number>:\n25 Step 1. Primary Information:\n26 Step 2. Secondary Information:\n27 Step 3. Command:\n28 Step 4. Query:\n29 Step 5. Question:\nA.4 Question\n1 {vl}\n2\n3 Let's generate a lookup question, a compositional\nquestion, and an open-ended question for a given\nVega-Lite spec step by step. The lookup question\nrequires a single value retrieval. The compositional\nquestion requires multiple operations.\n4\n5 Step 1. What higher-level decision can be made by\nanalyzing this chart?\n6 Step 2. What is a possible conclusion that can be\nreached from this decision?\n7 Step 3. What specific value can be retrieved to reach\nthis conclusion?\n8 Step 4. Generate a lookup question using this value,\nwithout including any visual attributes such as color,\nlength, size, or position.\n9 Step 5. What visual attributes are required to\nparaphrase this question?\n10 Step 6. Paraphrase the generated question using the\nchart's visual attributes.\n11 Step 7. What are the mathematical operations (e.g.,\nmax, min, sum, difference, and average) to reach the\nconclusion in Step 2?\n12 Step 8. Generate a compositional question using these\noperations, without including any visual attributes\nsuch as color, length, size, or position.\n13 Step 9. What visual attributes are required to\nparaphrase this question?\n14 Step 10. Paraphrase the generated question using the\nchart's visual attributes.\n15 Step 11. Generate an open-ended question to reach the\nconclusion in Step 2.\n16\n17 ##\n18 Step 1. Decision:\n19 Step 2. Conclusion:\n20 Step 3. Specific Value:\n21 Step 4. Lookup Question:\n22 Step 5. Visual Attributes:\n23 Step 6. Paraphrased Question:\n24 Step 7. Operations:\n25 Step 8. Compositional Question:\n26 Step 9. Visual Attributes:\n27 Step 10. Paraphrased Question:\n28 Step 11. Open-ended Question:\nB PROMPT FOR AUTOMATIC QUALITATIVE\nCODING\nWhen extracting codes, we omitted the words ‘language’ and ‘use\nof’ since they were frequently added to the code. We believe that\nthese additions do not contribute any additional meaning to the\nthematic analysis.\n1 Let's perform a thematic analysis in the field of\nhuman-computer interaction. Generate characteristics\nof languages leveraged in the given sentence. The\ntotal number is five and each of them is separated by\nsemicolons. Do not add numbering or any explanations.\n2\n3 Sentence: {sent}\n4\n5 ##\n6 ; ; ; ;\nC PROMPTS FOR SCORE-BASED\nPARAPHRASING\nWe explain the variables used in our prompts:\n•Example Sentence: A sentence we want to paraphrase;\n•Axis: An explanation about each of the defined language\naxes;\n•Direction: A set of two opposite directions of the given\nlanguage axis;\n•Score: A specific value on a Likert-scale ranging from one\nto five assigned to each of the language axis.\nC.1 Paraphrasing with one axis\n1 {Axis}\n2\n3 Score of 1 indicates a higher tendency to use\n{Direction-1} language and a Score of 5 indicates a\nhigher tendency to use {Direction-2} language. Rewrite\nthe following sentence as if it were spoken by a\nperson with a given score for language usage.\n4\n5 Sentence: {Example Sentence}\n6 Score: {Score}\nC.2 Paraphrasing with two axes\n1 {Axis-1}\n2 {Axis-2}\n3\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ko et al.\n4 Score-A of 1 indicates a higher tendency to use\n{Direction-1-1} language and a Score-A of 5 indicates\na higher tendency to use {Direction-1-2} language.\n5 Score-B of 1 indicates a higher tendency to use\n{Direction-2-1} language and a Score-B of 5 indicates\na higher tendency to use {Direction-2-2} language.\n6 Rewrite the following sentence as if it were spoken by\na person with a given score for language usage.\n7\n8 Sentence: {Example Sentence}\n9 Score-A: {Score-A}, Score-B: {Score-B}\nD GOLD REFERENCE GUIDELINES\nD.1 Utterance\n•Imagine writing utterances to display a visualization using\na system like Excel, Tableau, or Microsoft Power BI;\n•Refer to both the dataset and the chart to better understand\nthe context in which the data has been used for the visual-\nization and formulate more naturalistic utterances.\n•Avoid referring to specific instructions to prevent acclimati-\nzation to the words or phrases in the instruction [79];\n•Write utterances as singletons, which are basic types of ut-\nterances, but can be more than one sentence if necessary due\nto complexity, forming a sequential utterance that provides\nall necessary information;\n•Write utterance for each view. If the chart is has layered\nplots, and they have different chart types, write utterance\nwith according to the number of different chart types;\n•Focus on primary information such as chart type and en-\ncoding rather than secondary information such as style and\ninteraction [86].\nD.2 Question\n•Ask one question in one complete sentence;\n•Keep questions clear and concise, avoiding overly broad or\nvague questions by focusing on specific aspects of the chart;\n•Formulate questions that can elicit useful insights from the\nvisualization to facilitate visual data analysis and decision-\nmaking [32].",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8213810324668884
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7928811311721802
    },
    {
      "name": "Chart",
      "score": 0.7442933320999146
    },
    {
      "name": "Syntax",
      "score": 0.6451714038848877
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5848916172981262
    },
    {
      "name": "Natural language processing",
      "score": 0.5385947823524475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5237122178077698
    },
    {
      "name": "Visualization",
      "score": 0.5007660388946533
    },
    {
      "name": "Programming language",
      "score": 0.4819304943084717
    },
    {
      "name": "Information retrieval",
      "score": 0.3207625150680542
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}