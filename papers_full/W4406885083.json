{
    "title": "Anchoring Bias in Large Language Models: An Experimental Study",
    "url": "https://openalex.org/W4406885083",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3215587542",
            "name": "Jiaxu Lou",
            "affiliations": [
                "National History Day"
            ]
        },
        {
            "id": "https://openalex.org/A2116522593",
            "name": "Yifan Sun",
            "affiliations": [
                "National History Day"
            ]
        },
        {
            "id": "https://openalex.org/A3215587542",
            "name": "Jiaxu Lou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116522593",
            "name": "Yifan Sun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4392602768",
        "https://openalex.org/W4399528455",
        "https://openalex.org/W4402671800",
        "https://openalex.org/W4401857176",
        "https://openalex.org/W4206358945",
        "https://openalex.org/W4235928593",
        "https://openalex.org/W2042651776",
        "https://openalex.org/W4403787503",
        "https://openalex.org/W4404780679",
        "https://openalex.org/W4399356353",
        "https://openalex.org/W4402775298",
        "https://openalex.org/W2147091291",
        "https://openalex.org/W4387560733",
        "https://openalex.org/W3125537118"
    ],
    "abstract": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly advanced artificial intelligence by enabling machines to generate and comprehend human-like text. Despite their impressive capabilities, LLMs are not free of limitations. They have shown various biases. While much research has explored demographic biases, the cognitive biases in LLMs have not been equally studied. This study delves into anchoring bias, a cognitive bias where initial information disproportionately influences judgment. Utilizing an experimental dataset, we examine how anchoring bias manifests in LLMs and verify the effectiveness of various mitigation strategies. Our findings highlight the sensitivity of LLM responses to biased hints. At the same time, our experiments show that, to mitigate anchoring bias, one needs to collect hints from comprehensive angles to prevent the LLMs from being anchored to individual pieces of information, while simple algorithms such as Chain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection are not sufficient.",
    "full_text": null
}