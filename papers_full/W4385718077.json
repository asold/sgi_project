{
  "title": "Limits for learning with language models",
  "url": "https://openalex.org/W4385718077",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1766628584",
      "name": "Nicholas Asher",
      "affiliations": [
        "Institut de Recherche en Informatique de Toulouse"
      ]
    },
    {
      "id": "https://openalex.org/A5067158676",
      "name": "Swarnadeep Bhar",
      "affiliations": [
        "Université de Toulouse"
      ]
    },
    {
      "id": "https://openalex.org/A2104340292",
      "name": "Akshay Chaturvedi",
      "affiliations": [
        "Institut de Recherche en Informatique de Toulouse"
      ]
    },
    {
      "id": "https://openalex.org/A2120479283",
      "name": "Julie Hunter",
      "affiliations": [
        "Université de Toulouse"
      ]
    },
    {
      "id": "https://openalex.org/A2152799710",
      "name": "Soumya Paul",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6629338076",
    "https://openalex.org/W6604788710",
    "https://openalex.org/W2015495764",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4220974656",
    "https://openalex.org/W2125960523",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6631024605",
    "https://openalex.org/W2137831583",
    "https://openalex.org/W2137983211",
    "https://openalex.org/W3099843385",
    "https://openalex.org/W3161374759",
    "https://openalex.org/W4296413749",
    "https://openalex.org/W3022842238",
    "https://openalex.org/W6632206580",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W6757755882",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6633906613",
    "https://openalex.org/W2805083708",
    "https://openalex.org/W6740483536",
    "https://openalex.org/W6636146035",
    "https://openalex.org/W2888296173",
    "https://openalex.org/W1523016082",
    "https://openalex.org/W27434444",
    "https://openalex.org/W1496937667",
    "https://openalex.org/W6667329600",
    "https://openalex.org/W2983068647",
    "https://openalex.org/W2313916554",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W6678763245",
    "https://openalex.org/W6630094177",
    "https://openalex.org/W1542886316",
    "https://openalex.org/W2732724430",
    "https://openalex.org/W1565236324",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W2072160811",
    "https://openalex.org/W1495022714",
    "https://openalex.org/W4312107638",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W3183889129",
    "https://openalex.org/W2333018025",
    "https://openalex.org/W1539382740",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W4211142623",
    "https://openalex.org/W3175606037",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2963059228",
    "https://openalex.org/W2772097715",
    "https://openalex.org/W2124927118",
    "https://openalex.org/W4250738323",
    "https://openalex.org/W4310348270",
    "https://openalex.org/W1608748195",
    "https://openalex.org/W2972780553",
    "https://openalex.org/W1503705371",
    "https://openalex.org/W2575135073",
    "https://openalex.org/W4303648003",
    "https://openalex.org/W114519366",
    "https://openalex.org/W2085574295"
  ],
  "abstract": "International audience",
  "full_text": "Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 236–248\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nLimits for learning with Language Models\nNicholas Asher\nCNRS, IRIT\nasher@irit.fr\nSwarnadeep Bhar\nUniversité de Toulouse 3\nIRIT\nswarnadeep.bhar@irit.fr\nAkshay Chaturvedi\nUniversité de Toulouse 3\nIRIT\nakshay91.isi@gmail.com\nJulie Hunter\nLINAGORA\njhunter@linagora.com\nSoumya Paul\nTELINDUS\nsoumya.paul@gmail.com\nAbstract\nWith the advent of large language models\n(LLMs), the trend in NLP has been to train\nLLMs on vast amounts of data to solve diverse\nlanguage understanding and generation tasks.\nThe list of LLM successes is long and varied.\nNevertheless, several recent papers provide em-\npirical evidence that LLMs fail to capture im-\nportant aspects of linguistic meaning. Focusing\non universal quantification, we provide a theo-\nretical foundation for these empirical findings\nby proving that LLMs cannot learn certain fun-\ndamental semantic properties including seman-\ntic entailment and consistency as they are de-\nfined in formal semantics. More generally, we\nshow that LLMs are unable to learn concepts\nbeyond the first level of the Borel Hierarchy,\nwhich imposes severe limits on the ability of\nLMs, both large and small, to capture many\naspects of linguistic meaning. This means that\nLLMs will continue to operate without formal\nguarantees on tasks that require entailments and\ndeep linguistic understanding.\n1 Introduction\nThe success of large language models (LLMs) has\nled researchers in NLP to harness LLMs trained on\nvast amounts of data to solve a variety of language\nunderstanding and generation tasks, and some have\nclaimed that LLMs can solve any task that can be\nspecified via prompting (Brown et al., 2020). While\nthe list of LLM successes is long, there have been\nseveral recent papers that provide empirical evi-\ndence that LLMs at least sometimes fail to capture\nimportant aspects of linguistic meaning (Kuhnle\nand Copestake, 2019; Sinha et al., 2020; Yuksek-\ngonul et al., 2022; Chaturvedi et al., 2022; Kalouli\net al., 2022). Those who have dabbled in “BERTol-\nogy” with respect to linguistic meaning often have\nthe feeling that fixing one LLM deficiency just\nleads to the discovery of new ones.\nThis paper provides a theoretical explanation\nof certain of these observed failings of LLMs. In\nparticular, we prove that LLMs cannot learn the\nnotions of semantic entailment or consistency as\ndefined in formal semantics (Dowty et al., 1981)\nbecause they are incapable of mastering universal\nquantification. Our work builds on Siegelmann\nand Sontag (1992); Siegelmann (2012); Weiss et al.\n(2018), concerning the expressive power of neu-\nral networks, but we focus on the learnability of\nsemantic concepts and use novel tools.\nOur argument has widespread implications: not\nonly does a general capacity to recognize seman-\ntic entailment and consistency underlie everyday\nconversational interactions, but the meanings of a\ngreat many common linguistic expressions depend\non universal quantification. This set includes—but\nis certainly not limited to—a long list of quanti-\nfiers (every, some, many, most,... every other, ...),\ntemporal adverbs (always, never, eventually) that\nare essential to planning (Lamport, 1980), modal\noperators (possibly, necessarily,...), and certain dis-\ncourse connectives and adverbs (therefore, if / then,\nexcept, because, ...).\nWe begin in Section 2 by contextualizing our\nclaims in terms of expectations about the linguistic\ncapacities and applications of LLMs. In Section 3,\nwe introduce the framework of continuation seman-\ntics, which will allow us to adapt certain notions\ncentral to truth-conditional semantics to the case of\nLLMs. Section 4 lays out the core of our theoretical\nargument, focusing first on what is needed to learn\nuniversal quantification and then generalizing our\nargument to a wide range of linguistic expressions.\nOur theoretical argument suggests that we should\nexpect certain empirical failures from LLMs, and in\nSection 5, we provide evidence that our predictions\nare borne out. Section 6 concludes.\n2 Context\nOur results are particularly relevant to downstream\ntasks that require an agent to not only create fluent,\ncreative and contextually relevant speech but also\n236\nto act precisely based on the meaning of linguistic\nexpressions and reliably recognize semantic incon-\nsistency. For a robot that has been instructed (via\nconversation) to tighten every screw of a door, to\nnever walk on an airplane wing, or to stop drilling\nimmediately if certain conditions hold, acting ap-\npropriately requires being able to infer what do to\nbased on the linguistic meaning of the words every,\nnever, stop, immediately and if —and in these cases,\ngetting things mostly right won’t do, especially if\nlives or substantial economic loss are at risk.\nAn important corollary of our argument is that\nwhile it might be tempting to separate reasoning\nand linguistic competence (Mahowald et al., 2023),\nthe former is in fact inextricably tied to our ability\nto draw inferences based onlinguistic content—not\njust on, say, mathematical or real-world facts. This\nin turn suggests that approaches which attempt to\npatch up knowledge deficiencies for LLMs by giv-\ning them access to external models (Mialon et al.,\n2023) will fall short in developing reliable models\nof linguistic understanding because LLMs fail to\ngrasp the notions that underlie the very way that\nsentences (and actions) are woven together in con-\nversation.\nEmpirical studies like Chaturvedi et al. (2022)\nshow that LLM failures to respect semantic en-\ntailment in question answering tasks follow from\nfundamental features of LLM training; thus while\nextensive training and large data sets may improve\nLLM results, performance will inevitably remain\nunstable and we should continue to expect hallu-\ncinations and reasoning errors in NLP tasks like\nquestion-answering and natural language inference.\n3 Language models and formal semantics\nwith continuations\n3.1 LLMs and strings\nWe consider LLMs trained on transformer architec-\ntures over very large corpora using classic language\nmodeling tasks, namely masked language modeling\nor next sentence prediction. The former involves\nmasking certain words in a given corpus and train-\ning the model to guess the missing words, while\nin the latter, a context (a sentence typically) is pro-\nvided to the model, which is trained to predict the\nsentence that follows. This unsupervised training\nallows language models to build rich internal rep-\nresentations that have been shown through probing\nto contain at least implicitly a large amount of lin-\nguistic information (Devlin et al., 2019; Liu et al.,\n2019; Tenney et al., 2018).\nFormally, LLMs learn a function f : C×X →\n[0,1] that assigns a probability to a word (or string\nor discourse move) x∈Xgiven a context (or finite\nstring) C. More abstractly, let V be a countable set\ncalled the vocabulary. For i> 0, let Vi denote the\nset of all length istrings in the vocabulary V and\nV≤i denote the set of all strings V whose length\nis at most i. V∗denotes the set of all finite strings\nand Vω the set of countably infinite strings in V.\nWe can then rewrite f as f: V≤n →µ, where µ\nis a probability measure (which is often called its\nprediction) over Vn+m for m≥1. Typically, the\nprediction function is used on strings of length m\nwhere mis smaller than n.\nBy exploiting f, an LLM can extend µ to a\ndistribution on the set of strings V∗. The most\nstraightforward way is to follow autoregressive\nmodels that calculate the probability of strings\nvia conditionalization. For a new sentence s′ =\n(w1,w2,...,w m+1), and an input string sof length\nn provided as context, we have:\nµn+m+1(s′|s) = µn+1(w1|s)×µn+2(w2 |s,w1)×\n(1)\n...×µn+m(wn |s,wm−1,...,w 1)\nFor any s′∈V∗, µ(s′) represents the confidence\nwith which an LLM predicts s′, after training on\nstrings in V≤n.\n3.2 Linguistic meaning\nIn what follows, we are in interested strings that\nhave a well formed meaning and are evaluable as\ntrue or false. Linguists use truth conditional se-\nmantics to define the meanings of strings or well\nformed sentences in terms of the conditions under\nwhich they are true. Thanks to the work of Tarski\n(1944, 1956), we can formalize the notion of truth\nconditions using the set-theoretic notion of a model\nthat defines denotations or truth conditions for sen-\ntences recursively from denotations for sentential\nconstituents (Dowty et al., 1981).\nThe notion of a model not only serves to define\ntruth conditions; it also captures entailments. We\ndefine the notion of semantic consequence using\nthe notion of a model or structure A as follows\n(Chang and Keisler, 1973):\nDefinition 1. ϕis a semantic consequence of Γ (in\nsymbols, Γ |= ϕ) if and only if in every structureA\nin which Γ is satisfied (A |= Γ), A also makes true\nor satisfies ϕ(A |= ϕ). That is: ∀A, A |= Γ ⇒\nA |= ϕ\n237\nThe notion of semantic consequence integrates\nentailment with truth conditional meaning; two\nstrings have exactly the same entailments just in\ncase they are true in the same models. Accordingly\nwe can capture the truth conditional meaning of a\nstring in terms of the strings that it entails.Socrates\nis a man, for example, entails Socrates is human,\nSocrates is mortal, Socrates is an adultbut also that\nsomeone is a man, human, mortal and so on. What\nit means for Socrates to be a man (and, indirectly,\nthe meaning of man) can be captured by the full set\nof these entailments.\nOur idea is to apply truth conditional semantics\nto LLMs by representing models themselves as\nstrings. Semanticists have used strings and con-\ntinuation semantics (Reynolds, 1974) —in which\nthe meaning of a string s is defined in terms of\nits possible continuations, the set of longer strings\nS that contain s—to investigate the meaning and\nstrategic consequences of conversational moves\n(Asher et al., 2017), temporal expressions (Fer-\nnando, 2004), generalized quantifiers (Graf, 2019),\nand the “dynamic” formal semantics of (Kamp and\nReyle, 1993; Asher, 1993)(De Groote, 2006; Asher\nand Pogodalla, 2011). In our case, we will use\nstrings to define models As. We will use this trick\nto reformulate semantic consequence: where ∥ϕ∥\nis the set of strings describing models that satisfy a\ntruth evaluable string ϕ:Γ |= ϕiff ∥Γ∥⊆∥ ϕ∥.\nLLMs naturally find their place in such a frame-\nwork (Fernando, 2022): given their training regime,\nthe meaning of any natural language expression for\nan LLM is a function from input contexts to sets\nof larger strings or continuations. LLMs provide a\nprobability distribution over possible continuations\nand can predict possible continuations of a given\ntext or discourse.\n4 Learning limits for semantic concepts\nSemantic consequence defines linguistic entail-\nments and importantly provides the fundamental\nconnection between meaning and inference that en-\nsures linguistic understanding (Montague, 1974).\nCrucial to |= is the use of universal quantification\nover all possible structures—an infinite space of\npossible circumstances of evaluation or set of pos-\nsibilities. A true grasp of semantic consequence\nthus requires an understanding of universal quan-\ntification at least over countably infinite domains.\nIn Section 4.1, we show that an LLM’s training\nregime makes it fundamentally unable to learn the\nconcept of universal quantification. In Section 4.2,\nwe generalize our argument to show that LLMs are\nincapable of learning a wide variety of everyday\nsemantic concepts.\n4.1 Learning the full meaning of every\nTo see if the set of strings that define the concept\nevery is learnable for an LLM, consider (1).\n(1) Every object is blue.\nWe will use strings of atomic formulas and their\nnegations to define models (or more precisely their\natomic diagrams) that we will use to test whether an\nLLM M can learn the concept of universal quantifi-\ncation through inductive reasoning from a series of\nindividual trials over finite subsequences of strings\nrepresenting countably infinite domains. In partic-\nular, we will ask whether it is possible to train an\nLLM M to judge, for a string sof arbitrary length\nn, whether sis consistent with (1), or equivalently,\ngiven that sdefines a model As, whether given As,\n(1) is true. If Mcan reliably judge in which models\nAs (1) is true, we can conclude it has learned the\nmeaning of every.\nTo this end, consider a language Lcontaining\nnegation, the predicate is blue and a countably in-\nfinite number of constants ai enumerating objects\nof a countably infinite domain. Lformulas are of\nthe form ai is blue and ai is not blue. We use the\nformulas of Las “words” to define the set of finite\nstrings, V∗\nLand the set of countably infinite strings\nVω\nL. Each such string corresponds to a finite or\ncountably infinite model in which (1) is true or not.\nIn the course of training, M will be presented\nwith finite sequences that define structures of in-\ncreasing size. For each n and set of models of\nsize n, M will form a set of hypotheses Hn, where\nfor h ∈Hn,h : Vn\nL →{0,1}. Hn corresponds\nto the hypothesis space of the problem; each hn\nsays whether a presented sequence of length nis\nconsistent with (1). As each hn ∈Hn is a charac-\nteristic function of a subset of Vn\nL, we can identify\nhypotheses with sets of strings. So for instance,\nhn\n∀ is the set of strings in Vn\nL that are consistent\nwith (1) and that define models in which (1) is true.\nWe will additionally assume that hn\n∀ picks out a\nsuitable set for each Vn, and similarly for each hn\nk.\nHowever simply learning hn\n∀for some nwill not\nbe sufficient for M to learn the meaning of every.\nUniversal quantification is a concept that applies to\narbitrarily large domains. So the question, Can M\n238\ninductively learn the meaning of every? becomes\nCan M inductively learn hypothesis hω\n∀ ∈Hω?\nTo answer this question, we first have to spec-\nify what we mean by inductive learning. Recall\nthat an LLM M has learned from unsupervised\ntraining a function f: V≤n →µ(V≤n+m) with\nµ(V≤n+m) a probability distribution over comple-\ntions of length mof contexts of length n. An LLM\ncan use this distribution to compute probability\nvalues for arbitrarily long strings or continuations\nusing Equation 1.\nIn the case at hand, M needs to use this distribu-\ntion over Lstrings to compute the probability that a\nstring sis in h∀or the probability of sgiven h∀. To\nlearn inductively M must use its training data D≤n\nto update its prior for the distribution µn using a\nrationally justifiable form of inductive inference;\ne.g., for h∈Hn,µn(s|h) = µn(h|s)×µn(s)\nµn(h) .\nAdditionally, we consider two constraints on dis-\ntributions to define learning in terms of an induc-\ntively inferred change in the distribution from the\npriors. The first constraint, Max Ent, says that the\ndistribution µ prior to training should assign all\nhypotheses a weight based on maximum entropy or\na least informative distribution. This is usual with\nauto-regressive models and a common assumption\nin other models.\nThe second constraint is that distributions for\ninductive learning should be non-degenerate. We\nhave assumed that our LLM M has been trained\nover sequences of length n. Through Equation\n1, M can extend the distribution it has learned\nfor Vn to one over Vn+m for any string of fi-\nnite length n + m. Recall that we are looking\nat strings of Lthat define structures; the structures\ndefined by strings of length n+ m are indepen-\ndent of those defined in Vn and none is intuitively\nmore likely than another. So the prior distribu-\ntion over Vn+m should consider as equally likely\nall continuations s.a ∈ Vn+m, where s ∈ Vn,\na ∈Vm and . is concatenation. There are also\ncorrespondingly more hypotheses in Hn+m than\nin Hn, since there are V|m|more strings in Vn+m\nthan in Vn. Thus µn+m(s.a|hk) <µn(s.a|hk) for\ns.a∈Vn+m,s ∈Vn for each hk. Non-degenerate\ndistributions will reflect this and should make the\nmodel converge to the least general hypothesis sup-\nported by the evidence (Muggleton et al., 1992;\nPlotkin, 1972).\nDefinition 2. M’s distributions over sets of hy-\npotheses Hn, µn(Hn), after training over Vn are\nnon-degenerate if ∀h ∀δ (0 < δ≤1),∃m >0\nsuch that ∀a ∈ Vm ∀s ∈ Vn : µn(s.a|h) =\nmax{0,µn(s|h) −δ}, where s.a∈Vn+m.\nProposition 1. Models that calculate distributions\nover strings using Equation 1 have non-degenerate\ndistributions.\nAs continuations get longer the probability of the\ncontinuation will decrease monotonically.2\nBecause quantifiers like every and some are elim-\ninable in terms of Boolean functions when we con-\nsider finite structures definable with strings in V∗,\nwe must consider strings in Vω to define countably\ninfinite models that capture the full truth conditions\nof every. To extend a distribution overVn for finite\nnto a distribution over Vω, we lift the probability\nof a string to the set of its continuations. In Vω, the\nset of strings Acharacterizes the set A.Vω, where\nA.Vω is the set of all strings formed by concatenat-\ning a string from Awith a string from Vω. Using\nthis correspondence, the probabilities of sets of fi-\nnite strings in Vn can lifted to probabilities of sets\nof the form Vn.Vω. The laws of probability extend\nthe distribution to complements, intersections and\nunions of such sets.\nWe now propose a simple but general notion of\ninductive learning.\nDefinition 3. Suppose µ0 is M’s Max Ent prior\ndistribution and let h∈Hβ for some countable β.\nM effectively learns hiff after some finite amount\nof training using inductive inference, there is an α,\nsuch that: for any s∈Vβ, µβ(s|h) >α>µ 0(s|h)\niff s∈h.\nProposition 2. If M can effectively learn hn\n∀from\nsequences of Vn\nL for arbitrarily large n∈ω, then\nM can effectively learn hω\n∀\nAssume that M cannot effectively learn hω\n∀but it\ncan effectively learn hn\n∀for arbitrarily large n∈ω.\nThen it must admit some string s∈Vω\nL, such that\ns ̸∈hω\n∀. But then at some finite stage i, si must\nhave ¬blue(ai). By hypothesis M has learned hi\n∀.\nSo it has ruled out si and a fortiori s. 2\nWe now negatively answer our question, Can\nM inductively learn hypothesis hω\n∀?, under either\nof two conditions: (i) M has non-degenerate dis-\ntributions; (ii) M obeys Max Ent and inductive\ninference.\nProposition 3. Suppose M’s distributions are non-\ndegenerate. Then hω\n∀is not effectively learnable by\nM over Hω.\n239\nSuppose M trained on V≤n has effectively\nlearned hn\n∀. So ∀s ∈hn\n∀, µn(s|h∀) > αwhere\nαis as in Definition 3. Since M’s distributions are\nnon-degenerate, ∃m, such that for all s∈Vm,∃δ:\n0 < δ ≤ 1 where µm(s|h∀) −δ < α and a\ncontinuation of s, s.a, such that s.a ∈hm+n\n∀ but\nµm+n(s.a|h∀) = µm(s|h∀) −δ <α. 2\nBy Propositions 1 and 3, a basic auto-regressive\nmodel cannot learn h∀. We can generalize Proposi-\ntion 3 to other :\nProposition 4. Suppose M’s priors only obey Max\nEnt and M uses inductive inference. Then hω\n∀ is\nnot effectively learnable by M over Hω.\nSuppose M’s training data D≤n ⊆V≤n and M\nhas learned hn\n∀. To learn hn+m\n∀ , M must project\nits distribution of Hn onto Hn+m. But the dis-\ntributions in Hn and Hn+m are independent; for\none thing the cardinality of Hn, |Hn|, is such that\n|Hn| < |Hn+m| = |Hn|× 2m. Our assump-\ntions about inductive inference on D≤n make it\nno more likely that every will be associated with\nh∀than it is with any of the 2m h∈Hn+m, where\nstrings in hcontain the same nprefix as an s∈hn\n∀\nbut h∩hn+m\n∀ = ∅. In Hn+m these hypotheses\nh can be distinguished from h∀. Max Ent pri-\nors over Hn+m imply that for any s ∈ Vn+m,\nµn+m(s|h∀) = 1\n2m µn(s|h∀). 2\nCorollary 1. M cannot effectively learn hn\n∀from\nsequences in Vn\nL for arbitrarily large n∈N. There\nis some nsuch that hn\n∀is not effectively learnable.\nWhile LLMs can represent any Borel function\nto an arbitrary degree of precision (Hornik et al.,\n1989), Propositions 3 and 4 shows they cannot al-\nways learn such functions, given either the con-\nstraints of inductive epistemology or the way LLMs\ngenerate probabilities for strings. In particular,\ngiven our assumptions, no LLM can effectively\nlearn hω\n∀. In addition, each LLM is bounded by\nsome number nin the size of sequences for which\nit can learn hn\n∀. LLMs do not have the capacity to\nlearn the meaning of ‘every’ even over all finite\ndomains.1\nEven supposing that an LLM can effectively\nlearn hn\n∀for some n, this does not amount to under-\nstanding every. hn\n∀can be effectively represented\nwith quantifier free conjunctions of formulae, and\n1Unlike Hume’s problem of induction (Popper, 1963) and\n(Wolpert et al., 1995), we exploit particularities of LLMs and\nthe structure of a classification problem. The finite bound on\nlearning of hypotheses goes beyond standard Humean conclu-\nsions\nthese do not correctly approximate reasoning with\na sentence like (2) that applies to arbitrarily large\ndomains. Identifying ∀with a finite conjunction of\nlength nwill make ∀xFx consistent with ¬∀xFx\nin larger structures. In ωstructures, for example,\n¬∀xFx is consistent with every finite subset of the\nΠ0\n1 string blue(0), blue(1), blue(2), ..., in hω\n∀,\nmaking it inevitable that LLMs will reason incor-\nrectly with every in large enough structures.\nThe situation worsens with sampling: suppose\nthat when we present our model M a long string,\nM only samples some of the elements in the string;\nthe threat of inconsistency in such a situation can\nbecome high and we have no guarantees that such\ninconsistencies will not arise. 2 But this reason-\ning is not independent of the meaning of every; as\nthe semantics and rules of first order logic show,\nthis reasoning is an integral part of the meaning\nof every. As a result, LLMs unable to grasp se-\nmantic consequence defined in terms of universal\nquantification; and we thus cannot provide them\nguarantees that they follow semantic entailments\nwhen asked to do semantic tasks. This predicts\nphenomena like LLM hallucinations and observed\nelementary reasoning errors.\n4.2 Generalizing our answer to Q2\nUsing tools from statistical learning and the Borel\nHierarchy, we now generalize Propositions 3 and 4\nto other concepts beyond every.\nStatistical learning examines the application of\na learned function over a test domain and the ex-\npected loss over novel applications. The ability to\nbring the error over test to that over the training\nset is typically taken to indicate an ability to gen-\neralize (Neyshabur et al., 2017). Villa et al. (2013)\ndefine learnability in statistical learning theory via\nthe notion of uniform consistency. Let µbe a dis-\ntribution over Hand µn the update of µ after n\ntraining samples zi = (xi,yi). Let Azn be an algo-\nrithm for picking out a hypothesis from Hbased\non ntraining samples. inf His the hypothesis in\nHwith the lowest possible error (Shalev-Shwartz\net al., 2010; Kawaguchi et al., 2017).\nDefinition 4. An algorithm A on a hypothesis\nspace His uniformly consistent if and only if\n∀ϵ> 0 limn→∞supµ\nµn({zn : Eµ({Azn −inf HEµ >ϵ}) = 0\n2Approximation and approximation error can also affect\nlearnability of mathematical functions (Colbrook et al., 2022).\n240\nIn our case, the best hypothesis, inf H, for in-\nstance h∀, will yield 0 error. Our question is\nwhether there is an algorithm that converges to\nthat hypothesis given a certain Hand certain as-\nsumptions.\nDefinition 5. A class of hypotheses His unifor-\nmally learnable just in case there exists a uniformly\nconsistent algorithm for H.\nThis enables us to link learnability with a number\nof other features:\nTheorem 1. (Anthony et al., 1999) LetY = {0,1}.\nThen the following conditions are equivalent: (i)\nHis uniformly learnable; (ii) Empirical risk mini-\nmization on His uniformly consistent; (iii) His a\nuGC-class; (iv) the VC-dimension of His finite.\nThe Borel Hierarchy We now turn to generalize\nthe hypotheses we are investigating. Vω has a nat-\nural topology, the Cantor topology, which allows\nus to characterize linguistic concepts precisely. To\ndefine the topology, we first define the basic open\nsets to be sets of the form A.Vω, denoted as O(A),\nwhere A⊆V∗is a set of finite strings. Importantly,\nO(A) sets are both open and closed or clopen, be-\ncause if A⊂V∗is a countable set, then the com-\nplement of A.Vω, (V∗\\A).Vω, is also open. And\nthus, A.Vω is also closed. The ∆0\n1 class is at the\nintersection of the Σ0\n1 and Π0\n1 classes and consists\nof the clopen sets. Σ0\n1 sets include countable unions\nof ∆0\n1 sets, while Π0\n1 are complements of Σ0\n1 sets\nand so include countable intersections of ∆0\n1 sets.\nThese sets form the basis of the Borel hierar-\nchy of sets that includes the ∆0\n1, Σ0\n1, and Π0\n1 sets,\nand more generally includes Σ0\nα+1 as the countable\nunion of all Π0\nα and ∆0\nα sets, and Π0\nα+1 as the com-\nplement of Σ0\nα+1 sets, with ∆0\nα = Σ0\nα ∩Π0\nα. The\nhierarchy is strict and does not collapse (Kechris,\n1995). We will use this hierarchy to characterize\nlinguistic concepts. Below is a picture of some\nsimple Borel sets and their ⊆relations.\nΣ0\n1 Σ0\n2 Σ0\n3\nΠ0\n1 Π0\n2 Π0\n3\n∆0\n1 ∆0\n2 ∆0\n3\nAs an example, hω\n∀ ⊆Vω\nL of the previous section\nis a Π0\n1 Borel set; i.e., hω\n∀ = ⋂\ni∈ωBi where the Bi\nare ∆0\n1.\nWe are interested in the learnability of Borel\nsets B with respect to a hypothesis space. The\nhypothesis space Hn for V≤n and algorithem An\nthat an LLM can consider is typically fixed by the\nmaximal length strings it has been trained on. But\nwe will be looking at how an LLM extends its\ntraining generalizing to longer and longer strings.\nMore generally, we consider a countable collection\nof hypotheses—in the case of every and VL, the set\nconsists of h∀,ht hefirst 2n etc. We will assume a\ncountable hypothesis space Hω for the Borel sets in\nVω, with |V|>2 we want to learn in what follows.\nDefinition 6. An LLM M can effectively learn a\nBorel set S ⊂Vω out of a countable set of hypothe-\nses Hiff M has a uniformly consistent algorithm\nsuch that hS = inf H, as in Definition 4, and where\nhS : Vω →{0,1}defines S.\nClearly if h is inf H, and Ais uniformly con-\nsistent, then Definition 3 is satisfied; i.e., there is\nsome α>ϵ such that µ(s|h) >α iff s∈h.\nTheorem 2. An LLM with either (a) non degener-\nate distribution or (b) Max Ent priors and trained\non V<n for some finite n to learn h ⊂Vω via\ninductive inference (i) can effectively learn a ∆0\n1\nset O(S) ⊂Vω\nL, where Sis a finite subset of V≤n,\ngiven HO(V<n), a hypothesis space restricted to\n∆0\n1 sets; but (ii) it cannot effectively learn anyΠ0\n1\nBorel set B ⊂Vω\nL.\nWe first show (i). Let H= {O(A) : A ⊆V<n}.\nAny h∈H is determined by a finite set of prefixes\nP in V<n. There are only finitely many such sets in\nV≤n, and so M has an algorithm Athat eliminates\nat each finite stage of training some ∆0\n1 O(P) sets.\nThis enables it to converge uniformly toward 0\nexpected error for the set of finite prefixes that\ndetermines O(S) and so eventually M will have\neffectively learned O(S).\nNow for (ii). We first consider the case (ii.a)\nwhere our learned model has non-degenerate distri-\nbutions. Consider an arbitrary Π0\n1 complete set B.\nSo B = ⋂\nn∈ωO(Bn), with O(Bn+1) ⊂O(Bn),\nwhere the Bi ⊂V∗. To compute B, M needs a\nuniformly consistent algorithm Aover our count-\nable hypothesis space Hthat converges on hB, the\nhypothesis defining B. Now suppose M has been\ntrained on strings in V<n; its algorithms Aare thus\nrestricted to H<n.\nSuppose M trained on V≤n has effectively\nlearned hn\nBn. Let s∈hn\nBn. Since M’s distributions\nare non-degenerate, ∀α ≥0,∃m, δ : 0 < δ≤1\nwhere µm(s|hB) −δ < αand a continuation of\ns, s.a, such that s.a ∈hm+n\nBn but µm\nM(s.a|hB) =\nµm\nM(s|hB) −δ <α. So there is no convergence at\nany finite stage nof Atowards hB. Non uniform\nlearnability of Hthen follows.\n241\n(ii.b) Let’s now assume that M only has Max\nEnt priors and learns by inductive inference. Uni-\nform convergence of any algorithm obeying these\nconditions is not guaranteed as a similar argument\nas in Proposition 3 applies. 2\nCorollary 2. The hypothesis space HB is not uni-\nformly learnable. Hence the the VC-dimensions of\nHB are not finite, and empirical risk minimization\non HB are not uniformly consistent.\nCorollary 3. M cannot effectively learn Σ0\n1 com-\nplete Borel sets.\nAssume M can effectively learn a Σ0\n1 complete\nset. Then it can effectively learn a Π0\n1 set that is its\ncomplement, which is impossible by Theorem 2.2\nProposition 5. An LLM M cannot effectively\nlearn Borel sets Bof higher complexity than ∆0\n1.\nProposition 2 and Corollary 2 show that M cannot\neffectively learn Π0\n1 or Σ0\n1 sets. But any Π0\nn or\nΣ0\nn complete Borel set B for n > 1 is at least a\ncountable intersection or countable union of such\nsets. So Bis not effectively learnable.2\nAsher et al. (2017); Asher and Paul (2018) exam-\nine concepts of discourse consistency and textual\nand conversational coherence, which true, human-\nlike conversational capacity requires. Using con-\ntinuations in a game-theoretic setting, they show\nthose concepts determine more complex Π0\n2 sets in\nthe Borel Hierarchy; and intuitive measures of con-\nversational success—like the fact that one player\nhas more successful unrefuted attacks on an op-\nponent’s position than vice versa—determine Π0\n3\nsets. Given Proposition 5, LLMs cannot learn these\nconcepts, which are needed for full conversational\nmastery.\nProposition 6. For any LLM M, there is a max-\nimally large and fixed number nsuch that Hn is\nuniformly learnable for M but Hn+k is not uni-\nformly learnable, for k> 0.\nSuppose that for M Hn is uniformly learnable for\nall n. Then, M can compute the countable intersec-\ntion of sets defined by the best hypotheses in Hn\nfor each n. So M can effectively learn a Π0\n1 set,\nwhich contradicts Theorem 2.2\nCorollary 4. M cannot effectively learn ∆0\n1 sets\nof the form O(A) if the length of Ais longer than\nthe maximal number nsuch that Hn is uniformly\nlearnable for M.\n4.3 The importance of order\nOrder is important for the most elementary rea-\nsoning about linguistic content in finite domains.\nLet us add another predicate Ato Lto form the\nlanguage L+. Now consider the strings in Vω\nL+.\nStrings consistent with (2) may include formu-\nlae like A(ai) or ¬A(ai), paired with a choice of\nblue(ai) or ¬blue(ai). Even to find effectively ini-\ntial segments of strings in hω\n∀L+ , M must learn\nsome sentence structure or word order. The nega-\ntion sign has to be paired with the predicateblue; if\nit’s appended to A(e.g., large, or some other inde-\npendent term), this should count as a string in hn\n∀.\nIf sis a finite string, Mdoes not effectively capture\nword order if it does not distinguish between sand\npermutations of elements in s.\nProposition 7. If M does not effectively capture\nword order, it cannot effectively learn basic sets of\nthe form O(A) for A⊂V∗.\nLet s∈Abe a string containing A(ai)∧¬blue(ai)\nbut Ahas no string containing ¬A(ai) ∧blue(ai).\nIf M does not capture word order, M cannot dis-\ntinguish between sand s’s permutation containing\n¬A(ai) ∧blue(ai). 2\nCorollary 5. If M does not effectively capture\nword order, it will not reason soundly in proposi-\ntional logic.\nThe example in Proposition 7 shows that M will\nnot be able to reason about logical structure if it\ndoes not effectively capture word order. 2\nYuksekgonul et al. (2022); Sinha et al. (2020)\nprovide evidence that small to moderate sized\nLLMs do not reliably capture word order. Our\nempirical examples show even GPT3.5 and Chat-\nGPT have difficulties with sentential word order,\nand, worryingly, with the order of arguments in a\nlogical operator; the example in Appendix B sug-\ngests that even ChatGPT can’t be trusted to always\ndo elementary inferences involving conditionals\ncorrectly. Thus, LLMs with their initial training do\nnot necessarily find basic ∆0\n1 sets of the form a.Vω\nwhere ais a single string but only setsA.Vω where\nAis a set of prefixes that are permutations on a.\nThis is surprising and poses extreme difficulties\nfor valid reasoning with operators that have order\ndependent arguments.\n242\n5 Empirical investigations of LLMs with\nevery\nWhile the theoretical argument laid out in Section\n4 does not hinge on empirical statistics of LLM\nfailures, it certainly suggests that we should expect\nsuch failures. In this section, we describe some of\nthe tests we have performed using continuations to\nquery LLMs directly about their mastery of univer-\nsal quantification.\nLet us return to our simple example from above,\nrepeated here as (2):\n(2) Everything is blue.\nWe used finite sequences of formulas as a context,\ne.g., a1 is blue, a2 is red, a3 is red,..., ai is blue\nto determine a model. We then asked an LLM M\nwhether (2) in this model, allowing us to gauge its\nbehavior with respect to finite domains.\nBERT-large and RoBERTa-large already failed\nto reliably distinguish very small models (contain-\ning 2 and 5 objects respectively) in which (2) is\ntrue from those in which it is not. To test these\nmodels, we fine-tuned BERT-large and RoBERTa-\nlarge on the CoQA dataset (Reddy et al., 2019). For\nfinetuning, the model had 4 output heads for yes,\nno, unknown, and span type questions. Since the\nCoQA dataset provides a rationale for each ques-\ntion, the models were jointly trained on question\nanswering and rationale tagging tasks to enhance\ntheir performance. We report scores on the fine-\ntuned models on CoQA for 1 epoch as we did not\nobserve significant improvement with an increased\nnumber of epochs.\nFor BERT-large, we provided strings like (3) and\nthen asked Is everything blue?\n(3) My car is blue. My house is blue\nWe generated a total of 5 examples in which (2)\nwas true and 5 examples in which (2) is false. All\nthe examples had only 2 objects. The inconsistent\nexamples were constructed by varying the position\nof the object which was inconsistent with the asked\nquestion and by trying out different combination of\ncolours and objects.\nThe consistent examples were of the form:\n1. The car is blue. The house is blue.\n2. The car is purple. The house is purple.\n3. The car is yellow. The house is yellow.\n3. The shirt is violet. The table is violet.\n4. The cup is black. The plate is black.\nObject Count Pass Fraction\n2 1/1\n3 2/3\n4 1/4\n5 0/5\n6 0/6\n7 0/7\n8 0/8\n9 0/9\n10 0/10\nTable 1: Pass fraction on inconsistent examples for\nRoBERTa-large\nInconsistent examples were of the form:\n1. The car is blue. The house is red.\n2. The car is green. The house is purple.\n3. The car is yellow. The house is brown.\n2. The shirt is violet. The table is brown.\n3. The cup is black. The plate is white.\nBERT-large was able to correctly identify the\nconsistent examples but failed for all the incon-\nsistent examples. As the model failed for all the\ninconsistent examples with 2 objects, we did not\nexperiment with models containing more than 2\nobjects.\nFor RoBERTa-large, we generated a total of 9\nconsistent examples and 53 inconsistent examples.\nWe constructed sequences ranging from 2 to 10\nobjects. For each number, the inconsistent exam-\nples were constructed by varying the position of\nthe object in the string (context) which is respon-\nsible for the inconsistency. The model was able to\ncorrectly identify all the consistent examples. For\nmodels of a given size (i.e., number of objects), we\ndefined the pass fraction as the ratio of the exam-\nples in which the model was able to report models\ninconsistent with (2) correctly to the total number\nof inconsistent examples. Table 1 reports the pass\nfraction on inconsistent examples.\nWhile BERT’s and RoBERTa’s behavior was\nstable on the strings tested, GPT3.5 davinci and\nChatGPT, while more robust, are unstable from\none day to the next, even when temperature is set\nto 0 (on GPT3.5). This made it difficult to pin down\nthe models’ abilities, though some generalizations\nemerged. Typically (though not always), these\nmodels can recognize which objects in a string\nhave a certain property, but they cannot necessarily\nexploit this information to answer questions about\nthe string as a whole (see the “hats” example in\nAppendix A). In addition both GPT3.5 and Chat-\n243\nGPT will sometimes (frequently in our most recent\ntests) over-generalize and say that all items in a\nlist are, say, blue if it is specified for all items but\none that they are blue and it is not specified one\nway or the other for the remaining item (see the\nfifteen hearts example from ChatGPT in Appendix\nA). Thus, even these sophisticated models still fail\non more complicated questions and longer strings.\nOur empirical observations on LLMs like BERT\nand RoBERTa and probing of ChatGPT strongly\nsupport our argument that LLMs are unable to mas-\nter quantification, complementing observed LLM\ndifficulties with negation (Naik et al., 2018; Kass-\nner and Schütze, 2019; Hossain et al., 2020; Hos-\nseini et al., 2021) and to some extent quantifiers\n(Kalouli et al., 2022).\n6 Conclusions\nWe have shown that LLMs’ demonstrably inade-\nquate grasp of the meanings of words like every\nand other linguistic constructions has a theoretical\nfoundation and explanation: for certain expressions\nS, S’s content should be defined via consistent sets\nof strings in Vω, and LLMs cannot effectively learn\ncertain sets in Vω. More generally, LLMs cannot\neffectively learn full meanings of first order quanti-\nfiers or any Borel sets beyond the basic open sets,\nwhich means that they fail to grasp the meaning of a\nlong list of mundane, frequently used expressions.\nMany of these expressions are syncategorematic\nterms and express what we might call precise con-\ncepts. Such concepts are needed for understand-\ning ordinary entailment across all expressions; in\naddition, correctly reasoning with these concepts\nand grasping their entailments is essential to un-\nderstanding them. Reasoning and entailment are\nintimately tied with meanings. For us and most\nformal semanticists (Montague, 1974), grasping\nmeaning and correctly reasoning with linguistically\nexpressed concepts go hand in hand; if you cannot\nexploit the meanings of words in correct reasoning,\nyou do not really know what they mean. The incor-\nrect reasoning of LLMs exemplifies their failure to\ngrasp semantic entailments and meaning.\nOur arguments go beyond those of Bender and\nKoller (2020), who argue that stochastic models\ncannot capture linguistic meaning because they\nconsider only form, not denotation. While we agree\nthat denotation plays a very important role in mean-\ning for many expressions, the meaning of most\nexpressions, and especially that of syncategoreg-\nmatic ones, requires us to capture their semantic\nentailments. We have shown that we can capture\nthese entailments within the semantic framework\nof LLMs using continuation semantics. But we\nhave also shown that LLMs nevertheless fail in this\ntask.\nLLMs can learn certain types of ∆0\n1 sets and\nfinite intersections and unions of learnable ∆0\n1\nsets. For many open class words—including many\nnouns, adjectives and verbs—whose characteristic\ndenotations can be determined given a finite sam-\nple, this probably suffices to capture their meaning\nor at least a very good approximation of it. In ad-\ndition, many NLP tasks may not involve logical\ninference but an independent form of string op-\ntimization; in text summarization or translation,\nwhere given a context s, M tries to find an optimal\ncontinuation s′. If the length of s.s′falls within the\nconstraints of Corollary 4, then we can expect an\nLLM to succeed at such a task.\nProposition 6 and Corollary 4 generalize Corol-\nlary 1 and they all point to a general limit on learn-\nability for LLMs. They establish that language\nmodels have strict bounds even on the ∆0\n1 sets\nthey can effectively learn. So we cannot count on\nLLMs having full linguistic competence even on fi-\nnite domains. Different models may have different\nlimits; smaller models generally with lower limits.\nThis motivates a comparative study of the limits\nof learnability for different LLMs, complementing\nColbrook et al. (2022).\nBecause we do not make assumptions about\nmemory but only about inductive processes and\nlearning, our results hold for arbitrarily large LLMs\nand for any task that relies on an LLM’s capacity\nof string prediction, even if strings are not directly\npredicted.\nOur research implies that full language mastery\nneeds a different approach from one in which one\nseeks to build ever larger LLMs with language\nmasking or autoregressive training. Following\nRaissi et al. (2017), we believe we need to inject\nknowledge about linguistic structure and content\ninto our models to further constrain learning and in\nparticular hypothesis sets and learning architectures\nas suggested in Steinert-Threlkeld and Szymanik\n(2019).\n244\nLimitations\nOur paper offers an in-depth look at the limits of\nstatistical learnability for LLMs in terms of Borel\nsets. Our results however rely on certain, rather\nmild hypotheses about learning. We do not know\nhow our claims fare when different assumptions\nabout learning are made.\nAnother limitation is that we have not looked at\npotential ways of restricting learning hypotheses\nabout the meanings of linguistic expressions. Di-\nrectly relevant are the constraints on generalized\nquantifiers of Barwise and Cooper (1981). Nor\nhave we said much about compositionality and\nlearning, although they are well known to inter-\nact. Ideally, we should assign a meaning to every\nwhich consists in a reduced hypothesis space, but\nthat has to combine with the meanings of, say, the\nother words in (2).\nA third limitation concerns the extent to which\nan LLM can leverage what it has been trained to\ndo in learning semantic concepts on other tasks. In\nmany experiments we see that an LLM can appear\nto learn X but then cannot exploit X in task Y . We\nleave this issue for future research, but we believe\nit may be key to understanding debates about LLM\nperformance and the role of forgetting in LLM\nlearning (Kirkpatrick et al., 2017).\nEthics Statement\nWe show that LLM reliability in gauging the ac-\ncuracy of universal quantifiers is limited. Given\nhow quantification is linked to concepts like seman-\ntic consequence, it means that we cannot assume\nLLMs to have anything like full semantic compe-\ntence. This suggests that we cannot fully trust them\nwith tasks where accuracy on reasoning is required.\nThus, our work has certain ethical implications and\nprovides a cautionary tale for those working on\nLLMs as general AI models.\nAcknowledgments\nFor financial support, we thank the National Inter-\ndisciplinary Artificial Intelligence Institute ANITI\n(Artificial and Natural Intelligence Toulouse In-\nstitute), funded by the French ‘Investing for the\nFuture– PIA3’ program under the Grant agree-\nment ANR-19-PI3A-000, and we also thank the\nprojects COCOBOTS (ANR-21-FAI2-0005) and\nDISCUTER (ANR-21-ASIA-0005). We also thank\nthe COCOPIL “Graine\" project of the Région Occ-\nitanie of France.\nReferences\nAnthony, M., Bartlett, P. L., Bartlett, P. L., et al. (1999).\nNeural network learning: Theoretical foundations ,\nvolume 9. cambridge university press Cambridge.\nAsher, N. (1993). Reference to Abstract Objects in\nDiscourse. Kluwer Academic Publishers.\nAsher, N. and Paul, S. (2018). Strategic conversation\nunder imperfect information: epistemic Message Ex-\nchange games. Logic, Language and Information ,\n27.4:343–385.\nAsher, N., Paul, S., and Venant, A. (2017). Message\nexchange games in strategic conversations. Journal\nof Philosophical Logic, 46.4:355–404.\nAsher, N. and Pogodalla, S. (2011). Sdrt and continu-\nation semantics. In Onada, T., Bekki, D., and Mc-\nCready, E., editors, New Frontiers in Artificial Intelli-\ngence: JSAI-isAI 2010 Workshops, LENLS, JURISIN,\nAMBN, ISS, Tokyo, Japan, November 18-19, 2010,\nRevised Selected Papers, pages 3–15. Springer Berlin\nHeidelberg, Berlin, Heidelberg.\nBarwise, J. and Cooper, R. (1981). Generalized quanti-\nfiers in natural language. Linguistics and Philosophy,\n4(1):159–219.\nBender, E. M. and Koller, A. (2020). Climbing towards\nNLU: On meaning, form, and understanding in the\nage of data. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5185–5198, Online. Association for Computa-\ntional Linguistics.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sas-\ntry, G., Askell, A., et al. (2020). Language models\nare few-shot learners. Advances in neural informa-\ntion processing systems, 33:1877–1901.\nChang, C. C. and Keisler, H. J. (1973). Model theory.\nnorth Holland.\nChaturvedi, A., Bhar, S., Saha, S., Garain, U., and\nAsher, N. (2022). Analyzing semantic faithfulness\nof language models via input intervention on conver-\nsational question answering. Computing Research\nRepository, arXiv:2212.10696.\nColbrook, M. J., Antun, V ., and Hansen, A. C. (2022).\nThe difficulty of computing stable and accurate neu-\nral networks: On the barriers of deep learning and\nsmale’s 18th problem. Proceedings of the National\nAcademy of Sciences, 119(12):e2107151119.\nDe Groote, P. (2006). Towards a montagovian account\nof dynamics. In Semantics and linguistic theory ,\nvolume 16, pages 1–16.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). Bert: Pre-training of deep bidirectional trans-\nformers for language understanding.\n245\nDowty, D. R., Wall, R., and Peters, S. (1981). Introduc-\ntion to Montague semantics . Dordrecht. Synthese\nLibrary vol. 11.\nFernando, T. (2004). A finite-state approach to events\nin natural language semantics. Journal of Logic and\nComputation, 14(1):79–92.\nFernando, T. (2022). Strings from neurons to language.\nIn Proceedings of the 3rd Natural Logic Meets Ma-\nchine Learning Workshop (NALOMA III), pages 1–\n10.\nGraf, T. (2019). A subregular bound on the complexity\nof lexical quantifiers. In Proceedings of the 22nd\nAmsterdam colloquium.\nHornik, K., Stinchcombe, M., and White, H. (1989).\nMultilayer feedforward networks are universal ap-\nproximators. Neural networks, 2(5):359–366.\nHossain, M. M., Kovatchev, V ., Dutta, P., Kao, T., Wei,\nE., and Blanco, E. (2020). An analysis of natural\nlanguage inference benchmarks through the lens of\nnegation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 9106–9118.\nHosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D.,\nSordoni, A., and Courville, A. (2021). Understand-\ning by understanding not: Modeling negation in lan-\nguage models. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1301–1312.\nKalouli, A.-L., Sevastjanova, R., Beck, C., and Romero,\nM. (2022). Negation, coordination, and quantifiers in\ncontextualized language models. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 3074–3085.\nKamp, H. and Reyle, U. (1993). From Discourse to\nLogic: Introduction to Modeltheoretic Semantics of\nNatural Language, Formal Logic and Discourse Rep-\nresentation Theory. Kluwer Academic Publishers.\nKassner, N. and Schütze, H. (2019). Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. arXiv preprint\narXiv:1911.03343.\nKawaguchi, K., Kaelbling, L. P., and Bengio, Y . (2017).\nGeneralization in deep learning. arXiv preprint\narXiv:1710.05468.\nKechris, A. (1995). Classical descriptive set theory .\nSpringer-Verlag, New York.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness,\nJ., Desjardins, G., Rusu, A. A., Milan, K., Quan, J.,\nRamalho, T., Grabska-Barwinska, A., et al. (2017).\nOvercoming catastrophic forgetting in neural net-\nworks. Proceedings of the national academy of sci-\nences, 114(13):3521–3526.\nKuhnle, A. and Copestake, A. (2019). The meaning of\n“most” for visual question answering models. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 46–55.\nLamport, L. (1980). Sometime is sometimes not never:\nOn the temporal logic of programs. In Proceedings\nof the 7th ACM SIGPLAN-SIGACT symposium on\nPrinciples of programming languages , pages 174–\n185. ACM.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,\nD., Levy, O., Lewis, M., Zettlemoyer, L., and Stoy-\nanov, V . (2019). Roberta: A robustly optimized bert\npretraining approach.\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher,\nN., Tenenbaum, J. B., and Fedorenko, E. (2023). Dis-\nsociating language and thought in large language\nmodels: a cognitive perspective. arXiv preprint\narXiv:2301.06627.\nMialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pa-\nsunuru, R., Raileanu, R., Rozière, B., Schick, T.,\nDwivedi-Yu, J., Celikyilmaz, A., et al. (2023). Aug-\nmented language models: a survey. arXiv preprint\narXiv:2302.07842.\nMontague, R. (1974). Formal Philosophy. Yale Univer-\nsity Press, New Haven.\nMuggleton, S., Feng, C., et al. (1992). Efficient induc-\ntion of logic programs. Inductive logic programming,\n38:281–298.\nNaik, A., Ravichander, A., Sadeh, N., Rose, C., and\nNeubig, G. (2018). Stress test evaluation for natural\nlanguage inference. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 2340–2353.\nNeyshabur, B., Bhojanapalli, S., McAllester, D., and\nSrebro, N. (2017). Exploring generalization in deep\nlearning. Advances in neural information processing\nsystems, 30.\nPlotkin, G. (1972). Automatic methods of inductive\ninference. PhD thesis, The University of Edinburgh.\nPopper, K. (1963). Conjectures and refutations: The\ngrowth of scientific knowledge. routledge.\nRaissi, M., Perdikaris, P., and Karniadakis, G. E. (2017).\nPhysics informed deep learning (part i): Data-driven\nsolutions of nonlinear partial differential equations.\narXiv preprint arXiv:1711.10561.\nReddy, S., Chen, D., and Manning, C. D. (2019). CoQA:\nA Conversational Question Answering Challenge.\nTransactions of the Association for Computational\nLinguistics, 7:249–266.\nReynolds, J. C. (1974). On the relation between direct\nand continuation semantics. In International Collo-\nquium on Automata, Languages and Programming.\n246\nShalev-Shwartz, S., Shamir, O., Srebro, N., and Sridha-\nran, K. (2010). Learnability, stability and uniform\nconvergence. The Journal of Machine Learning Re-\nsearch, 11:2635–2670.\nSiegelmann, H. T. (2012). Neural networks and ana-\nlog computation: beyond the Turing limit. Springer\nScience & Business Media.\nSiegelmann, H. T. and Sontag, E. D. (1992). On the\ncomputational power of neural nets. In Proceedings\nof the fifth annual workshop on Computational learn-\ning theory, pages 440–449.\nSinha, K., Parthasarathi, P., Pineau, J., and Williams,\nA. (2020). Unnatural language inference. arXiv\npreprint arXiv:2101.00010.\nSteinert-Threlkeld, S. and Szymanik, J. (2019). Learn-\nability and semantic universals. Semantics and Prag-\nmatics, 12(4).\nTarski, A. (1944). The semantic conception of truth:\nand the foundations of semantics. Philosophy and\nphenomenological research, 4(3):341–376.\nTarski, A. (1956). The concept of truth in formalized\nlanguages. In translated by J.H. Woodger, editor,\nLogic, Semantics and Metamathematics, pages 152–\n278. Oxford University Press, New York.\nTenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., Mc-\nCoy, R. T., Kim, N., Van Durme, B., Bowman, S. R.,\nDas, D., et al. (2018). What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations.\nVilla, S., Rosasco, L., and Poggio, T. (2013). On learn-\nability, complexity and stability. In Empirical Infer-\nence, pages 59–69. Springer.\nWeiss, G., Goldberg, Y ., and Yahav, E. (2018). On\nthe practical computational power of finite preci-\nsion rnns for language recognition. arXiv preprint\narXiv:1805.04908.\nWolpert, D. H., Macready, W. G., et al. (1995). No free\nlunch theorems for search. Technical report, Techni-\ncal Report SFI-TR-95-02-010, Santa Fe Institute.\nYuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D.,\nand Zou, J. (2022). When and why vision-language\nmodels behave like bag-of-words models, and what\nto do about it? arXiv preprint arXiv:2210.01936.\nA GPT 3.5 and ChatGPT on every\nBecause our access to GPT 3.5 and ChatGPT is lim-\nited and the responses that we receive seem to vary\ndepending on the day that we query the models, we\nqualify the results in these appendices as anecdotal.\nNevertheless, we emphasize that for each type of\nexample that we provide, we made numerous trials\nin order to make sure that the results were repeated\nand so seemed to follow from features of the model,\nnot just random quirks.\nIn the queries below, GPT3.5 on 18.01.2023,\nwith temperature set to 0, correctly classifies both\nblue hats as being blue when queried directly about\nthe color of the beret, but still fails to correctly an-\nswer Were all of the hats blue? (Results sometimes\nvaried with the form of the prompt; e.g., using com-\nmas rather than and can impact GPT’s answers).\nInput Text: One guy was wearing a blue beret and\nanother guy was wearing a pink bonnet and a third\nguy was wearing a blue baseball cap.\nQ: Were all of the hats blue?\nGPT: No, only one of the hats was blue.\nQ: Which hat was blue? GPT: The blue baseball\ncap.\nQ: Is a beret a hat? GPT: Yes, a beret is a type of\nhat.\nQ: What color is the beret? GPT: The beret is blue.\nHere is an input and query to ChatGPT on\n20.01.2023, showing that it ignores underspecifica-\ntion\nInput text: I have a shirt with a picture of ten blue\nbears, five blue snails, three blue guitars, fifteen\nhearts, 15 blue fluffy rabbits and 25 blue stars. Is\neverything on my shirt blue?\nChatGPT: Yes, everything on your shirt is blue.\nB ChatGPT and GPT3.5 on word order\nand argument order\nThe examples below illustrate how both GPT3.5\nand ChatGPT fail to recognize instructions about\nword order. When asked to write sentences that end\nwith a particular word, for example, the models\nmanage to produce sentences in which the target\nword appears, but in most cases, it is not in the\ncorrect position.\nInstruction: Write five sentences that end with\nthe word “house”.\nChatGPT:\n1. I built a house with my own two hands.\n2. The storm destroyed the old house.\n3. We found a beautiful house to rent for the\nsummer.\n4. The house was haunted, so we left in the middle\nof the night.\n247\n5. I always dreamed of living in a big white house.\nGPT 3.5:\n1. I can’t wait to get home and relax in my own\nhouse.\n2. The old house had been abandoned for years.\n3. The house was filled with the smell of freshly\nbaked cookies.\n4. The house was surrounded by a tall white picket\nfence.\n5. The house was the perfect place to spend a lazy\nsummer day.\nFor GPT3.5 we set temperature to 0. These ob-\nservations suggest both GPT 3.5 and ChatGPT fail\nto respect the last word constraint.\nFinally, we offer this example that shows that\nChatGPT can confuse the order of arguments in a\nlogical operator, thus leading to incorrect reason-\ning.\nPrompt: If p then q. q, therefore p.\nChatGPT:\nThis is a valid logical argument. It is known as\nmodus ponens and it is a valid rule of inference\nin propositional logic. The statement “If p then\nq” is known as the conditional statement and “q,\ntherefore p” is the conclusion that can be drawn\nfrom the conditional statement.\n248",
  "topic": "Meaning (existential)",
  "concepts": [
    {
      "name": "Meaning (existential)",
      "score": 0.6656075716018677
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.635964572429657
    },
    {
      "name": "Computer science",
      "score": 0.5999205708503723
    },
    {
      "name": "Hierarchy",
      "score": 0.5513607263565063
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.458809494972229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4439048171043396
    },
    {
      "name": "Logical consequence",
      "score": 0.4394860863685608
    },
    {
      "name": "Natural language processing",
      "score": 0.43567758798599243
    },
    {
      "name": "Confusion",
      "score": 0.41968652606010437
    },
    {
      "name": "Linguistics",
      "score": 0.41644132137298584
    },
    {
      "name": "Psychology",
      "score": 0.23263058066368103
    },
    {
      "name": "Political science",
      "score": 0.1574935019016266
    },
    {
      "name": "Programming language",
      "score": 0.1224982738494873
    },
    {
      "name": "Psychoanalysis",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210119061",
      "name": "Institut de Recherche en Informatique de Toulouse",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I17866349",
      "name": "Université Fédérale de Toulouse Midi-Pyrénées",
      "country": "FR"
    }
  ],
  "cited_by": 16
}