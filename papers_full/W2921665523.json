{
    "title": "Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks",
    "url": "https://openalex.org/W2921665523",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2359649633",
            "name": "Fang Kuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2747104507",
            "name": "Toshev, Alexander",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123643299",
            "name": "Fei Fei Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744670902",
            "name": "Savarese, Silvio",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2821377707",
        "https://openalex.org/W2567015638",
        "https://openalex.org/W2029143333",
        "https://openalex.org/W2772262724",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2804117224",
        "https://openalex.org/W2362143032",
        "https://openalex.org/W2805116782",
        "https://openalex.org/W2963088756",
        "https://openalex.org/W2171028727",
        "https://openalex.org/W2745868649",
        "https://openalex.org/W2002639639",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2013489252",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W1490092700",
        "https://openalex.org/W2963447367",
        "https://openalex.org/W2784853476",
        "https://openalex.org/W2168359464",
        "https://openalex.org/W2557465155",
        "https://openalex.org/W2808492412",
        "https://openalex.org/W2953127211",
        "https://openalex.org/W2137051796",
        "https://openalex.org/W2772545238",
        "https://openalex.org/W2019738489",
        "https://openalex.org/W2787214294",
        "https://openalex.org/W2798483995",
        "https://openalex.org/W2963948945",
        "https://openalex.org/W2964348070",
        "https://openalex.org/W2777629900",
        "https://openalex.org/W2770387316",
        "https://openalex.org/W2884565639",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2113243634",
        "https://openalex.org/W2103581399",
        "https://openalex.org/W2145339207",
        "https://openalex.org/W2754261200",
        "https://openalex.org/W2962887844",
        "https://openalex.org/W2963454359",
        "https://openalex.org/W3038029863",
        "https://openalex.org/W2150839555",
        "https://openalex.org/W3037651954",
        "https://openalex.org/W2951713345",
        "https://openalex.org/W2964043796",
        "https://openalex.org/W2963871073",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2530887700",
        "https://openalex.org/W2418628973",
        "https://openalex.org/W2593841437"
    ],
    "abstract": "Many robotic applications require the agent to perform long-horizon tasks in partially observable environments. In such applications, decision making at any step can depend on observations received far in the past. Hence, being able to properly memorize and utilize the long-term history is crucial. In this work, we propose a novel memory-based policy, named Scene Memory Transformer (SMT). The proposed policy embeds and adds each observation to a memory and uses the attention mechanism to exploit spatio-temporal dependencies. This model is generic and can be efficiently trained with reinforcement learning over long episodes. On a range of visual navigation tasks, SMT demonstrates superior performance to existing reactive and memory-based policies by a margin.",
    "full_text": "Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks\nKuan Fang1 Alexander Toshev2 Li Fei-Fei1 Silvio Savarese1\n1Stanford University 2Google Brain\nAbstract\nMany robotic applications require the agent to perform\nlong-horizon tasks in partially observable environments. In\nsuch applications, decision making at any step can depend\non observations received far in the past. Hence, being\nable to properly memorize and utilize the long-term his-\ntory is crucial. In this work, we propose a novel memory-\nbased policy, named Scene Memory Transformer (SMT).\nThe proposed policy embeds and adds each observation\nto a memory and uses the attention mechanism to exploit\nspatio-temporal dependencies. This model is generic and\ncan be efﬁciently trained with reinforcement learning over\nlong episodes. On a range of visual navigation tasks, SMT\ndemonstrates superior performance to existing reactive and\nmemory-based policies by a margin.\n1. Introduction\nAutonomous agents, controlled by neural network poli-\ncies and trained with reinforcement learning algorithms,\nhave been used in a wide range of robot navigation ap-\nplications [1, 2, 3, 23, 28, 32, 47, 50, 51]. In many of\nthese applications, the agent needs to perform tasks over\nlong time horizons in unseen environments. Consider a\nrobot patrolling or searching for an object in a large unex-\nplored building. Typically, completing such tasks requires\nthe robot to utilize the received observation at each step\nand to grow its knowledge of the environment,e.g. building\nstructures, object arrangements, explored area , etc. There-\nfore, it is crucial for the agent to maintain a detailed memory\nof past observations and actions over the task execution.\nThe most common way of endowing an agent’s policy\nwith memory is to use recurrent neural networks (RNNs),\nwith LSTM [5] as a popular choice. An RNN stores the\ninformation in a ﬁxed-size state vector by combining the\ninput observation with the state vector at each time step.\nThe policy outputs actions for the agent to take given the\nupdated state vector. Unfortunately, however, RNNs often\nfail to capture long-term dependencies [34].\nTo enhance agent’s ability to plan and reason, neural\nnetwork policies with external memories have been pro-\nScene Memory\nSMT Policy Tasks\nRoaming\nCoverage\nSearch\n1\n2\nt\nPolicy Network\nFigure 1. The Scene Memory Transformer (SMT) policy embeds\nand adds each observation to a memory. Given the current ob-\nservation, the attention mechanism is applied over the memory to\nproduce an action. SMT is demonstrated successfully in several\nvisual navigation tasks, all of which has long time horizons.\nposed [32, 49]. Such memory-based policies have been pri-\nmarily studied in the context of robot navigation in partially\nobservable environments, where the neural network learns\nto encode the received observations and write them into a\nmap-like memory [16, 17, 19, 33, 44]. Despite their supe-\nrior performance compared to reactive and RNN policies,\nexisting memory-based policies suffer from limited ﬂexibil-\nity and scalability. Speciﬁcally, strong domain-speciﬁc in-\nductive biases go into the design of such memories, e.g. 2D\nlayout of the environment, predeﬁned size of this layout,\ngeometry-based memory updates, etc. Meanwhile, RNNs\nare usually critical components for these memory-based\npolicies for exploiting spatio-temporal dependencies. Thus\nthey still suffer from the drawbacks of RNN models.\nIn this work, we present Scene Memory Transformer\n(SMT), a memory-based policy using attention mecha-\nnisms, for understanding partially observable environments\nin long-horizon robot tasks. This policy is inspired by the\nTransformer model [43], which has been successfully ap-\nplied to multiple natural language processing problems re-\ncently. As shown in Fig. 1, SMT consists of two modules: a\nscene memory which embeds and stores all encountered ob-\nservations and a policy network which uses attention mech-\nanism over the scene memory to produce an action.\nThe proposed SMT policy is different from existing\nmethods in terms of how to utilize observations received\nin the previous steps. Instead of combining past observa-\ntions into a single state vector, as commonly done by RNN\npolicies, SMT separately embeds the observations for each\ntime step in the scene memory. In contrast to most existing\n1\narXiv:1903.03878v1  [cs.LG]  9 Mar 2019\nmemory models, the scene memory is simply a set of all em-\nbedded observations and any decisions of aggregating the\nstored information are deferred to a later point. We argue\nthat this is a crucial property in long-horizon tasks where\ncomputation of action at a speciﬁc time step could depend\non any provided information in the past, which might not\nbe properly captured in a state vector or map-like memory.\nThe policy network in SMT adopts attention mechanisms\ninstead of RNNs to aggregate the visual and geometric in-\nformation from the scene memory. This network efﬁciently\nlearns to utilize the stored information and scales well with\nthe time horizon. As a result, SMT effectively exploits long-\nterm spatio-temporal dependencies without committing to a\nenvironment structure in the model design.\nAlthough the scene memory grows linearly with the\nlength of the episode, it stores only an embedding vector at\neach steps. Therefore, we can easily store hundreds of ob-\nservations without any burden in the device memory. This\noverhead is justiﬁed as it gives us higher performance com-\npared to established policies with more compact memories.\nFurther, as the computational complexity of the origi-\nnal model grows quadratically with the size of the scene\nmemory, we introduce a memory factorization procedure as\npart of SMT. This reduces the computational complexity to\nlinear. The procedure is applied when the number of the\nstored observations is high. In this way, we can leverage\na large memory capacity without the taxing computational\noverhead of the original model.\nThe advantages of the proposed SMT are empirically\nveriﬁed on three long-horizon visual navigation tasks:\nroaming, coverage and search. We train the SMT pol-\nicy using deep Q-learning [30] and thus demonstrate for\nthe ﬁrst time how attention mechanisms introduced in [43]\ncan boost the task performance in a reinforcement learning\nsetup. In these tasks, SMT considerably and consistently\noutperforms existing reactive and memory-based policies.\nVideos can found at https://sites.google.com/\nview/scene-memory-transformer\n2. Related Work\nMemory-based policy using RNN. Policies using\nRNNs have been extensively studied in reinforcement learn-\ning settings for robot navigation and other tasks. The most\ncommon architectural choice is LSTM [20]. For exam-\nple, Mirowski et al. [28] train an A3C [29] agent to nav-\nigate in synthesized mazes with an LSTM policy. Wu et\nal. [46] use a gated-LSTM policy with multi-modal inputs\ntrained for room navigation. Moursavian et al. [31] use an\nLSTM policy for target driven navigation. The drawbacks\nof RNNs are mainly two-fold. First, merging all past obser-\nvations into a single state vector of ﬁxed size can easily lose\nuseful information. Second, RNNs have optimization dif-\nﬁculties over long sequences [34, 42] in backpropagation\nthrough time (BPTT). In contrast, our model stores each\nobservations separately in the memory and only aggregate\nthe information when computing an action. And it extracts\nspatio-temporal dependencies using attention mechanisms,\nthereby it is not handicapped by the challenges of BPTT.\nExternal memory. Memory models have been exten-\nsively studied in natural language processing for variety\nof tasks such as translation [43], question answering [39],\nsummarization [27]. Such models are fairly generic, mostly\nbased on attention functions and designed to deal with input\ndata in the format of long sequences or large sets .\nWhen it comes to autonomous agents, most of the ap-\nproaches structure the memory as a 2D grid. They are ap-\nplied to visual navigation [16, 17, 33, 48], interactive ques-\ntion answering [14], and localization [7, 19, 21]. These\nmethods exhibit certain rigidity. For instance, the 2D layout\nis of ﬁxed size and same amount of memory capacity is allo-\ncated to each part of the environment. Henriques et al. [19]\ndesigns a differentiable mapping module with 2.5D repre-\nsentation of the spatial structure. Such a structured memory\nnecessiates write operations, which compress all observa-\ntions as the agent executes a task and potentially can lose in-\nformation which could be useful later in the task execution.\nOn the contrary, our SMT keeps all embedded observations\nand allows for the policy to attend to them as needed at any\nstep. Further, the memory operations in the above papers\nare based on current estimate of robot localization, where\nthe memory is being modiﬁed and how it is accessed. In\ncontrast, we keep all pose information in its original form,\nthus allow for potentially more ﬂexible use.\nA more generic view on memory for autonomous agents\nhas been less popular. Savinov et al. [36] construct a topol-\ngical map of the environment, and uses it for planning. Oh\nat al. [32] use a single-layer attention decoder for control\nproblems. However, the method relies on an LSTM as\na memory controller, which comes with the challenges of\nbackpropgation through time. Khan et al. [24] apply the\nvery general Differentiable Neural Computer [15] to con-\ntrol problems. While this approach is hard to optimize and\nis applied on very simple navigation tasks.\nVisual Navigation. We apply SMT on a set of visual\nnavigation tasks, which have a long history in computer vi-\nsion and robotics [6, 10, 40]. Our approach falls into vi-\nsual navigation, where the agent does not have any scene-\nspeciﬁc information about the environment [8, 9, 37, 41,\n45]. As in recent works on end-to-end training policies for\nnavigation tasks [2, 23, 28, 47, 50, 51], our model does not\nneed a map of the environment provided beforehand. While\n[23, 28] evaluates their models in 3D mazes, our model can\nhandle more structured environments as realistic cluttered\nindoor scenes composed of multiple rooms. In contrast to\n[28, 50, 51] which train the policy for one or several known\nscenes, our trained model can generalize to unseen houses.\n2\nInput Observation Scene Memory\nEmbeddingsObservations\nPolicy Network\nFC\nLayer\nAction\nat-1\npt\nIt\nEncoder Decoder\nEncoded Memory\no1\no2\not\nFigure 2. The Scene Memory Transformer (SMT)policy. At each time step t, the observation ot is embedded and added to the scene\nmemory. SMT has access to the full memory and produces an action according to the current observation.\n3. Method\nIn this section, we ﬁrst describe the problem setup. Then\nwe introduce the Scene Memory Transformer (SMT) and its\nvariations as shown in Fig. 2.\n3.1. Problem Setup\nWe are interested in a variety of tasks which require\nan embodied agent to navigate in unseen environments\nto achieve the task goal. These tasks can be formu-\nlated as the Partially Observable Markov Decision Pro-\ncess (POMDP) [22] (S,A,O,R(s,a),T(s′|s,a),P(o|s))\nwhere S, A, Oare state, action and observation spaces,\nR(s,a) is the reward function, T(s′|s,a) and P(o|s) are\ntransition and observation probabilities.\nThe observation is a tuple o = (I,p,a prev) ∈O com-\nposed of multiple modalities. I represents the visual data\nconsisting of an RGB image, a depth image and a semantic\nsegmentation mask obtained from a camera sensor mounted\non the robot. pis the agent pose w.r.t. the starting pose of\nthe episode, estimated or given by the environment. aprev is\nthe action taken at the previous time step.\nIn our setup, we adopt a discrete action space deﬁned as\nA= {go forward,turn left,turn right}, a common\nchoice for navigation problems operating on a ﬂat surface.\nNote that these actions are executed under noisy dynamics\nmodeled by P(s′|s,a), so the state space is continuous.\nWhile we share the same Oand Aacross tasks and en-\nvironments, each task is deﬁned by a different reward func-\ntion R(s,a) as described in Sec. 4.1. The policy for each\ntask is trained to maximize the expected return, deﬁned as\nthe cumulative reward Eτ[∑\ntR(st,at)] over trajectories\nτ = (st,at)H\nt=1 of time horizon H unrolled by the policy.\n3.2. Scene Memory Transformer\nThe SMT policy, as outlined in Fig. 2, consists of two\nmodules. The ﬁrst module is the scene memory M which\nstores all past observations in an embedded form. This\nmemory is updated at each time step. The second module,\ndenoted by π(a|o,M), is an attention-based policy network\nthat uses the updated scene memory to compute an distribu-\ntion over actions.\nIn a nutshell, the model and its interaction with the envi-\nronment at time tcan be summarized as:\not ∼ P(ot|st)\nMt = Update(Mt−1,ot)\nat ∼ π(at|ot,Mt)\nst+1 ∼ T(st+1|st,at)\nIn the following we deﬁne the above modules.\n3.2.1 Scene Memory\nThe scene memory M is intended to store all past observa-\ntions in an embedded form. It is our intent not to endow\nit with any geometric structure, but to keep it as generic as\npossible. Moreover, we would like to avoid any loss of in-\nformation when writing to M and provide the policy with\nall available information from the history. So we separately\nkeep observations of each step in the memory instead of\nmerging them into a single state vector as in an RNN.\nThe scene memory can be deﬁned recursively as fol-\nlows. Initially it is set to the empty set ∅. At the current\nstep, given an observation o = (I,p,a prev), as deﬁned in\nSec. 3.1, we ﬁrst embed all observation modalities, concate-\n3\n+\n+\nAtt\nUKV\nFC\n+\n+\nAtt\nUKV\nFC\n+\n+\nAtt\nKVU\nFC\n+\n+\nAtt\nUKV\nFC\nFPS\nM M̃ M C\nFigure 3. Encoder without memory factorization, encoder with\nmemory factorization, and decoder as in Sec. 3.2.2.\nnate them, and apply a fully-connected layer FC:\nψ(o) =FC({φI(I),φp(p),φa(aprev)}) (1)\nwhere φI, φp, φa are embedding networks for each modal-\nity as deﬁned in Sec. 3.4. To obtain the memory for the next\nstep, we update it by adding ψ(o) to the set:\nUpdate(M,o) =M ∪{ψ(o)} (2)\nThe above memory grows linearly with the episode\nlength. As each received observation is embedded into low-\ndimensional vectors in our design, one can easily store hun-\ndreds of time steps on the hardware devices. While RNNs\nare restricted to a ﬁxed-size state vector, which usually can\nonly capture short-term dependencies.\n3.2.2 Attention-based Policy Network\nThe policy network π(a|o,M) uses the current observation\nand the scene memory to compute a distribution over the\naction space. As shown in Fig. 2, we ﬁrst encode the mem-\nory by transforming each memory element in the context of\nall other elements. This step has the potential to capture the\nspatio-temporal dependencies in the environment. Then, we\ndecode an action according to the current observation, using\nthe encoded memory as the context.\nAttention Mechanism.Both encoding and decoding are\ndeﬁned using attention mechanisms, as detailed by [43]. In\nits general form, the attention function Att applies n1 atten-\ntion queries U ∈Rn1×dk over n2 values V ∈Rn2×dv with\nassociated keys K ∈Rn2×dk , where dk and dv are dimen-\nsions of keys and values. The output of Att hasn1 elements\nof dimension dv, deﬁned as a weighted sum of the values,\nwhere the weights are based on dot-product similarity be-\ntween the queries and the keys:\nAtt(U,K,V ) =softmax(UKT)V (3)\nAn attention block AttBlock is built upon the above func-\ntion and takes two inputs X ∈Rn1×dx and Y ∈Rn2×dy\nof dimension dx and dy respectively. It projects X to the\nqueries and Y to the key-value pairs. It consists of two\nresidual layers. The ﬁrst is applied to the above Att and\nthe second is applied to a fully-connected layer:\nAttBlock(X,Y ) =LN(FC(H) +H) (4)\nwhere H = LN(Att(XWU,YW K,YW V) +X)\nwhere WU ∈Rdx×dk , WK ∈Rdy×dk and WV ∈Rdy×dv\nare projection matrices and LN stands for layer normaliza-\ntion [4]. We choose dv = dx for the residual layer.\nEncoder. As in [43], our SMT model uses self-attention\nto encode the memory M. More speciﬁcally, we use M\nas both inputs of the attention block. As shown in Fig. 3,\nthis transforms each embedded observation by using its re-\nlations to other past observations:\nEncoder(M) =AttBlock(M,M ) (5)\nIn this way, the model extracts the spatio-temporal depen-\ndencies in the memory.\nDecoder. The decoder is supposed to produce actions\nbased on the current observation given the contextC, which\nin our model is the encoded memory. As shown in Fig. 3, it\napplies similar machinery as the encoder, with the notable\ndifference that the query in the attention layer is the embed-\nding of the current observation ψ(o):\nDecoder(o,C) =AttBlock(ψ(o),C) (6)\nThe ﬁnal SMT output is a probability distribution over\nthe action space A:\nπ(a|o,M) = Cat(softmax(Q)) (7)\nwhere Q = FC(FC(Decoder(o,Encoder(M))))\nwhere Cat denotes categorical distribution.\nThis gives us a stochastic policy from which we can sam-\nple actions. Empirically, this leads to more stable behaviors,\nwhich avoids getting stuck in suboptimal states.\nDiscussion. The above SMT model is based on the\nencoder-decoder structure introduced in the Transformer\nmodel, which has seen successes on natural language pro-\ncessing (NLP) problems such as machine translation, text\ngeneration and summarization. The design principles of the\nmodel, supported by strong empirical results, transfer well\nfrom the NLP domain to the robot navigation setup, which\nis the primary motivation for adopting it.\nFirst, an agent moving in a large environment has to\nwork with dynamically growing number of past observa-\ntions. The encoder-decoder structure has shown strong per-\nformance exactly in the regime of lengthy textual inputs.\nSecond, contrary to common RNNs or other structured ex-\nternal memories, we do not impose a predeﬁned order or\nstructure on the memory. Instead, we encode temporal and\nspatial information as part of the observation and let the pol-\nicy learn to interpret the task-relevant information through\nthe attention mechanism of the encoder-decoder structure.\n4\n3.2.3 Memory Factorization\nThe computational complexity of the SMT is dominated\nby the number of query-key pairs in the attention mecha-\nnisms. Speciﬁcally, the time complexity is O(|M|2) for the\nencoder due to the self-attention, and O(|M|) for the de-\ncoder. In long-horizon tasks, where the memory grows con-\nsiderably, quadratic complexity can be prohibitive. Inspired\nby [26], we replace the self-attention block from Eq. (4)\nwith a composition of two blocks of similar design but more\ntractable computation:\nAttFact(M, ˜M) =AttBlock(M,AttBlock( ˜M,M )) (8)\nwhere we use a “compressed” memory ˜Mobtained via ﬁnd-\ning representative centers from M. These centers need to\nbe dynamically updated to maintain a good coverage of the\nstored observations. In practice, we can use any cluster-\ning algorithm. For the sake of efﬁciency, we apply iterative\nfarthest point sampling (FPS) [35] to the embedded obser-\nvations in M, in order to choose a subset of elements which\nare distant from each other in the feature space. The run-\nning time of FPS is in O(|M||˜M|) and the ﬁnal complexity\nof AttFact is O(|M||˜M|). With a ﬁxed number of centers,\nthe overall time complexity becomes linear. The diagram of\nthe encoder with memory factorization is shown in Fig. 3.\n3.3. Training\nWe train all model variants and baselines using the stan-\ndard deep Q-learning algorithm [30]. We follow [30] in the\nuse of an experience replay buffer, which has a capacity\nof 1000 episodes. The replay buffer is initially ﬁlled with\nepisodes collected by a random policy and is updated ev-\nery 500 training iterations. The update replaces the oldest\nepisode in the buffer with a new episode collected by the\nupdated policy. At every training iteration, we construct\na batch of 64 episodes randomly sampled from the replay\nbuffer. The model is trained with Adam Optimizer [25] with\na learning rate of 5 ×10−4. All model parameters except\nfor the embedding networks are trained end-to-end. During\ntraining, we continuously evaluate the updated policy on the\nvalidation set (as in Sec. 4.1). We keep training each model\nuntil we observe no improvement on the validation set.\nThe embedding networks are pre-trained using the SMT\npolicy with the same training setup, with the only difference\nthat the memory size is set to be 1. This leads to a SMT with\nno attention layers, as attention of size 1 is an identity map-\nping. In this way, the optimization is made easier so that\nthe embedding networks can be trained end-to-end. After\nbeing trained to convergence, the parameters of the embed-\nding networks are frozen for other models.\nA major difference to RNN policies or other memory-\nbased policies is that SMT does not need backpropagation\nthrough time (BPTT). As a result, the optimization is more\nstable and less computationally heavy. This enables training\nthe model to exploit longer temporal dependencies.\n3.4. Implementation Details\nImage modalities are rendered as 640 ×480 and sub-\nsampled by 10. Each image modality is embedded into 64-\ndimensional vectors using a modiﬁed ResNet-18 [18]. We\nreduce the numbers of ﬁlters of all convolutional layers by a\nfactor of 4 and use stride of 1 for the ﬁrst two convolutional\nlayers. We remove the global pooling to better capture the\nspatial information and directly apply the fully-connected\nlayer at the end. Both pose and action vectors are embed-\nded using a single 16-dimensional fully-connected layer.\nAttention blocks in SMT use multi-head attention mech-\nanisms [43] with 8 heads. The keys and values are both128-\ndimensional. All the fully connected layers in the attention\nblocks are 128-dimensional and use ReLU non-linearity.\nA special caution is to be taken with the pose vector.\nFirst, at every time step all pose vectors in the memory are\ntransformed to be in the coordinate system deﬁned by the\ncurrent agent pose. This is consistent with an ego-centric\nrepresentation of the memory. Thus, the pose observations\nneed to be re-embedded at every time step, while all the\nother observations are embedded once. Second, a pose vec-\ntor p= (x,y,θ ) at time tis converted to a normalized ver-\nsion p = (x/λ,y/λ, cos θ,sin θ,e−t), embedding in addi-\ntion its temporal information t in a soft way in its last di-\nmension. This allows the model to differentiate between\nrecent and old observation, assuming that former could be\nmore important than latter. The scaling factor λ= 5is used\nto reduce the magnitude of the coordinates.\n4. Experiments\nWe design our experiments to investigate the following\ntopics: 1) How well does SMT perform on different long-\nhorizon robot tasks 2) How important is its design proper-\nties compared to related methods? 3) Qualitatively, what\nagent behaviors does SMT learn?\n4.1. Task Setup\nTo answer these questions, we consider three visual nav-\nigation tasks: roaming, coverage, and search. These tasks\nrequire the agent to summarize spatial and semantic infor-\nmation of the environment across long time horizons. All\ntasks share the same POMDP from Sec. 3.1 except that the\nreward functions are deﬁned differently in each task.\nRoaming: The agent attempts to move forward as much\nas possible without colliding. In this basic navigation task,\na memory should help the agent avoid cluttered areas and\noscillating behaviors. The reward is deﬁned as R(s,a) = 1\niff a= go forward and no collision occurs.\nCoverage: In many real-world application a robot needs\nto explore unknown environments and visit all areas of these\n5\nenvironments. This task clearly requires a detailed memory\nas the robot is supposed to remember all places it has vis-\nited. To deﬁne the coverage task, we overlay a grid of cell\nsize 0.5 over the ﬂoorplan of each environment. We would\nlike the agent to visit as many unoccupied cells as possible,\nexpressed by reward R(s,a) = 5iff robot entered unvisited\ncell after executing the action.\nSearch: To evaluate whether the policy can learn beyond\nknowledge about the geometry of the environment, we de-\nﬁne a semantic version of the coverage tasks. In particular,\nfor six target object classes 1, we want the robot to search\nfor as many as possible of them in the house. Each house\ncontains 1 to 6 target object classes, 4.9 classes in average.\nSpeciﬁcally, an object is marked as found if more than 4%\nof pixels in an image has the object label (as in [46]) and\nthe corresponding depth values are less than 2 meter. Thus,\nR(s,a) = 100iff after taking action awe ﬁnd one of the\nsix object classes which hasn’t been found yet.\nWe add a collision reward of −1 for each time the agent\ncollides. An episode will be terminated if the agent runs\ninto more than 50 collisions. To encourage exploration, we\nadd coverage reward to the search task with a weight of0.2.\nThe above tasks are listed in ascending order of complex-\nity. The coverage and search tasks are studied in robotics,\nhowever, primarily in explored environments and are con-\ncerned about optimal path planning [13].\nEnvironment. We use SUNCG [38], a set of synthetic\nbut visually realistic buildings. We use the same data split\nas chosen by [46] and remove the houses with artifacts,\nwhich gives us 195 training houses and 46 testing houses.\nWe hold out 20% of the training houses as a validation\nset for ablation experiments. We run 10 episodes in each\nhouse with a ﬁxed random seed during testing and vali-\ndation. The agent moves by a constant step size of 0.25\nmeters with go forward. It turns by 45◦degree in place\nwith turn left or turn right. Gaussian noise is added\nto simulate randomness in real-world dynamics.\nModel Variants. To investigate the effect of different\nmodel aspects, we conduct experiments with three variants:\nSMT, SMT + Factorization, and SM + Pooling. The sec-\nond model applies SMT with AttFact instead of AttBlock.\nInspired by [12], the last model directly applies a max pool-\ning over the elements in the scene memory instead of using\nthe encoder-decoder structure of SMT.\nBaselines. We use the following baselines for com-\nparison. A Random policy uniformly samples one of the\nthree actions. A Reactive policy is trained to directly com-\npute Q values using a purely feedforward net. It is two\nfully-connected layers on top of the embedded observation\nat every step. A LSTM policy [28] is the most common\nmemory-based policy. A model with arguably larger capac-\nity, called FRMQN [32], maintains embedded observations\n1We use television, refrigerator, bookshelf, table, sofa, and bed.\nMethod Reward Distance Collisions\nRandom 58.3 25.3 42.7\nReactive [28] 308.9 84.6 29.3\nLSTM [28] 379.7 97.9 11.4\nFRMQN [32] 384.2 99.5 13.8\nSM + Pooling 366.8 96.7 20.1\nSMT + Factorization 376.4 98.6 17.9\nSMT 394.7 102.1 13.6\nTable 1. Performance on Roaming.The average of cumulative\nreward, roaming distance and number of collisions are listed.\nMethod Reward Covered Cells\nRandom 94.2 27.4\nReactive [28] 416.2 86.9\nLSTM [28] 418.1 87.8\nFRMQN [32] 397.7 83.2\nSM + Pooling 443.9 91.5\nSMT + Factorization 450.1 99.3\nSMT 474.6 102.5\nTable 2. Performance on Coverage.The average of cumulative\nreward and number of covered cells are listed.\nMethod Reward Classes Ratio\nRandom 140.5 1.79 36.3%\nReactive [28] 358.2 3.14 61.9%\nLSTM [28] 339.4 3.07 62.6%\nFRMQN [32] 411.2 3.53 70.2%\nSM + Pooling 332.5 2.98 60.6%\nSMT + Factorization 432.6 3.69 75.0%\nSMT 428.4 3.65 74.2%\nTable 3.Performance on Search.The cumulative of total reward,\nnumber of found classes and ratio of found classes are listed.\nin a ﬁxed-sized memory, similarly as SMT. Instead of using\nthe encode-decoder structure to exploit the memory, it uses\nan LSTM, whose input is current observation and output is\nused to attend over the memory.\nFor all methods, we use the same pretrained embedding\nnetworks and two fully-connected layers to compute Q val-\nues. We also use the same batch size of 64 during training.\nTo train LSTM and FRMQN, we use truncated back propa-\ngation through time of 128 steps.\n4.2. Comparative Evaluation\nThe methods are compared across the three different\ntasks: roaming in Table 1, coverage in Table 2, and search\nin Table 3. For each task and method we show the attained\nreward and task speciﬁc metrics.\nEffect of memory designs.Across all tasks, SMT out-\nperforms all other memory-based models. The relative per-\nformance gain compared to other approaches is most sig-\nniﬁcant for coverage (14% improvements) and considerable\nfor search (5% improvements). This is consistent with the\n6\nFigure 4. Found classes by time steps.For the search task, we\nshow number of found target object classes across time steps.\nnotion that for coverage and search memorizing all past ob-\nservations is more vital. On roaming, larger memory ca-\npacity (in SMT case) helps, however, all memory-based ap-\nproaches perform in the same ballpark. This is reasonable in\nthe sense that maintaining a straight collision free trajectory\nis a relatively short-sight task.\nIn addition to memory capacity, memory access via at-\ntention brings improvements. For all tasks SMT outper-\nforms SM + Pooling. The gap is particularly large for object\nsearch (Table 3), where the task has an additional seman-\ntic complexity of ﬁnding objects. Similarly, having multi-\nheaded attention and residual layers brings an improvement\nover a basic attention, as employed by FRMQN, which is\ndemonstrated on both coverage and search.\nThe proposed memory factorization brings computa-\ntional beneﬁts, at no or limited performance loss. Even if\nit causes drop sometimes, the reward is better than SM +\nPooling and other baseline methods.\nImplications of memory for navigation.It is also im-\nportant to understand how memory aids us at solving navi-\ngation tasks. For this purpose, in addition to reward, we re-\nport number of covered cells (Table 2) and number of found\nobjects (Table 3). For both tasks, a reactive policy presents\na strong baseline, which we suspect learns general explo-\nration principles. Adding memory via SMT helps boost the\ncoverage by 18% over reactive, and 17% over LSTM poli-\ncies and 23% over simpler memory mechanism (FRMQN).\nWe also observe considerable boosts of number of found\nobjects by 5% in the search task.\nThe reported metrics above are for a ﬁxed time horizon\nof 500 steps. For varying time horizons, we show the per-\nformance on search in Fig. 4. We see that memory-based\npolicies with attention-based reads consistently ﬁnd more\nobject classes as they explore the environment, with SMT\nvariants being the best. This is true across the full execution\nwith performance gap increasing steadily up to 300 steps.\n4.3. Ablation Analysis\nHere, we analyze two aspects of SMT: (i) size of the\nscene memory, and (ii) importance of the different obser-\nvation modalities and componenets.\n(a) Effects of memory capacity. (b) Effects of each component.\nFigure 5. Ablation Experiments. (a) We sweep the memory ca-\npacity from 50 steps to 500 steps and evaluate the reward of tra-\njectories of 500 steps. (b) We leave out one component at a time\nin our full model and evaluate the averaged reward for each task.\nMemory capacity.While in the previous section we dis-\ncussed memory capacity across models, here we look at the\nimportance of memory size for SMT. Intuitively a memory-\nbased policy is supposed to beneﬁts more from larger mem-\nory over long time horizons. But in practice this depends\non the task and the network capacity, as shown in Fig. 5 (a).\nAll three tasks beneﬁt from using larger scene memory. The\nperformance of roaming grows for memory up to 300 el-\nements. For coverage and search, the performance keeps\nimproving constantly with larger memory capacities. This\nshows that SMT does leverage the provided memory.\nModalities and components. For the presented tasks,\nwe have image observations, pose, previous actions. To un-\nderstand their importance, we re-train SMT by leaving out\none modality at a time. We show the resulting reward in\nFig. 5 (b). Among the observation modalities, last action,\npose and the depth image play crucial roles across tasks.\nThis is probably because SMT uses relative pose and last\naction to reason about spatial relationships. Further, depth\nimage is the strongest clue related to collision avoidance,\nwhich is crucial for all tasks. Removing segmentation and\nRGB observations leads to little effect on coverage and drop\nof 10 for roaming since these tasks are deﬁned primarily by\nenvironment geometry. For search, however, where SMT\nneeds to work with semantics, the drop is 15 and 20.\nWe also show that the encoder structure brings perfor-\nmance boost to the tasks. Especially in the search task,\nwhich is most challenging in terms of reasoning and plan-\nning, the encoder boosts the task reward by 23.7.\n7\nSMT (Ours)LSTMReactive\nReward: 437, Found Classes 4 / 6 Reward: 591, Found Classes 5 / 6 Reward: 757, Found Classes 6 / 6\nReward: 312, Covered Cells: 63 (39.9%) Reward: 348, Covered Cells: 72 (45.6%) Reward: 439, Covered Cells: 96 (60.8%)\nReward: 336, Distance: 366, Collisions: 30 Reward: 383, Distance: 389, Collisions: 6 Reward: 413, Distance: 422, Collisions: 9\nThe Coverage Task\nThe Roaming Task\nThe Search Task\nFigure 6. Visualization of the agent behaviors.We visualize the trajectories from the top-down view as green curves. Starting point and\nending point of each trajectory are plot in white and black. Navigable area are masked in dark purple with red lines indicating the collision\nboundaries. For the coverage task, we mark the covered cells in pink. For the search task, we mark target objects with yellow masks.\n4.4. Qualitative Results\nTo better understand the learned behaviors of the agent,\nwe visualize the navigation trajectories in Fig. 6. We choose\nreactive and LSTM policies as representatives of memory-\nless and memory-based baselines to compare with SMT.\nIn the roaming task, our model demonstrates better\nstrategies to keep moving and avoid collisions. In many of\nthe cases, the agent ﬁrst ﬁnds a long clear path in the house,\nwhich lets it go straight forward without frequently making\nturns. Then the agent navigates back and forth along the\nsame route until the end of the episode. As a result, SMT\nusually leads to compact trajectories as shown in Fig. 6, top\nrow. In contrast, reactive policy and LSTM policy often\nwander around the scene with a less consistent pattern.\nIn the coverage task, our model explores the unseen\nspace more efﬁciently by memorizing regions that have\nbeen covered. As shown in Fig. 6, middle row, after most of\nthe cells inside a room being explored, the agent switches\nto the next unvisited room. Note that the cells are invisi-\nble to the agent, it needs to make this decision solely based\non its memory and observation of the environment. It also\nremembers better which rooms have been visited so that it\ndoes not enter a room twice.\nIn the search task, our model shows efﬁcient exploration\nas well as effective strategies to ﬁnd the target classes. The\nsearch task also requires the agent to explore rooms with\nthe difference that the exploration is driven by target object\nclasses. Therefore, after entering a new room, the agent\nquickly scans around the space instead of covering all the\nnavigable regions. In Fig. 6, if the agent ﬁnds the unseen\ntarget it goes straight towards it. Once it is done, it will\nleave the room directly. Comparing SMT with baselines,\nour trajectories are straight and direct between two targets,\nwhile baseline policies have more wandering patterns.\n5. Conclusion\nThis paper introduces Scene Memory Transformer, a\nmemory-based policy to aggregate observation history in\nrobotic tasks of long time horizons. We use attention mech-\nanism to exploit spatio-temporal dependencies across past\nobservations. The policy is trained on several visual navi-\ngation tasks using deep Q-learning. Evaluation shows that\nthe resulting policy achieves higher performance to other\nestablished methods.\nAcknowledgement: We thank Anelia Angelova, Ashish\nVaswani and Jakob Uszkoreit for constructive discussions.\nWe thank Marek Fiˇser for the software development of the\nsimulation environment, Oscar Ramirez and Ayzaan Wahid\nfor the support of the learning infrastructure.\n8\nReferences\n[1] P. Anderson, A. X. Chang, D. S. Chaplot, A. Doso-\nvitskiy, S. Gupta, V . Koltun, J. Kosecka, J. Ma-\nlik, R. Mottaghi, M. Savva, and A. R. Zamir. On\nevaluation of embodied navigation agents. CoRR,\nabs/1807.06757, 2018. 1\n[2] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. John-\nson, N. S¨underhauf, I. Reid, S. Gould, and A. van den\nHengel. Vision-and-Language Navigation: Interpret-\ning visually-grounded navigation instructions in real\nenvironments. In CVPR, 2018. 1, 2\n[3] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and\nA. A. Bharath. A brief survey of deep reinforcement\nlearning. CoRR, abs/1708.05866, 2017. 1\n[4] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450 , 2016.\n4\n[5] B. Bakker. Reinforcement learning with long short-\nterm memory. In Advances in neural information pro-\ncessing systems, pages 1475–1482, 2002. 1\n[6] F. Bonin-Font, A. Ortiz, and G. Oliver. Visual naviga-\ntion for mobile robots: A survey.Journal of Intelligent\nand Robotic Systems, 53:263–296, 2008. 2\n[7] D. S. Chaplot, E. Parisotto, and R. Salakhutdi-\nnov. Active neural localization. arXiv preprint\narXiv:1801.08214, 2018. 2\n[8] A. J. Davison. Real-time simultaneous localisation\nand mapping with a single camera. In ICCV, 2003.\n2\n[9] F. Dayoub, T. Morris, B. Upcroft, and P. Corke.\nVision-only autonomous navigation using topometric\nmaps. In Intelligent robots and systems (IROS), 2013\nIEEE/RSJ international conference on , pages 1923–\n1929. IEEE, 2013. 2\n[10] G. N. DeSouza and A. C. Kak. Vision for mobile\nrobot navigation: A survey. IEEE transactions on\npattern analysis and machine intelligence, 24(2):237–\n267, 2002. 2\n[11] D. Eigen and R. Fergus. Predicting depth, surface nor-\nmals and semantic labels with a common multi-scale\nconvolutional architecture. 2015 IEEE International\nConference on Computer Vision (ICCV), pages 2650–\n2658, 2015. 11\n[12] S. M. A. Eslami, D. J. Rezende, F. Besse, F. Viola,\nA. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu,\nI. Danihelka, K. Gregor, D. P. Reichert, L. Buesing,\nT. Weber, O. Vinyals, D. Rosenbaum, N. C. Rabi-\nnowitz, H. King, C. Hillier, M. M. Botvinick, D. Wier-\nstra, K. Kavukcuoglu, and D. Hassabis. Neural scene\nrepresentation and rendering. Science, 360:1204–\n1210, 2018. 6\n[13] E. Galceran and M. Carreras. A survey on coverage\npath planning for robotics. Robotics and Autonomous\nsystems, 61(12):1258–1276, 2013. 6\n[14] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon,\nD. Fox, and A. Farhadi. Iqa: Visual question an-\nswering in interactive environments. arXiv preprint\narXiv:1712.03316, 1, 2017. 2\n[15] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Dani-\nhelka, A. Grabska-Barwi ´nska, S. G. Colmenarejo,\nE. Grefenstette, T. Ramalho, J. Agapiou, et al. Hy-\nbrid computing using a neural network with dynamic\nexternal memory. Nature, 538(7626):471, 2016. 2\n[16] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and\nJ. Malik. Cognitive mapping and planning for visual\nnavigation. CVPR, pages 7272–7281, 2017. 1, 2\n[17] S. Gupta, D. F. Fouhey, S. Levine, and J. Malik. Unify-\ning map and landmark based representations for visual\nnavigation. CoRR, abs/1712.08125, 2017. 1, 2\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual\nlearning for image recognition. In CVPR, pages 770–\n778, 2016. 5\n[19] J. F. Henriques and A. Vedaldi. Mapnet : An allo-\ncentric spatial memory for mapping environments. In\nCVPR, 2018. 1, 2\n[20] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\n2\n[21] R. Jonschkowski, D. Rastogi, and O. Brock. Differ-\nentiable particle ﬁlters: End-to-end learning with al-\ngorithmic priors. arXiv preprint arXiv:1805.11122 ,\n2018. 2\n[22] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra.\nPlanning and acting in partially observable stochastic\ndomains. Artif. Intell., 101:99–134, 1998. 3\n[23] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and\nW. Ja´skowski. Vizdoom: A doom-based ai research\nplatform for visual reinforcement learning. In Com-\nputational Intelligence and Games (CIG), 2016 IEEE\nConference on, pages 1–8. IEEE, 2016. 1, 2\n[24] A. Khan, C. Zhang, N. Atanasov, K. Karydis, V . Ku-\nmar, and D. D. Lee. Memory augmented control net-\nworks. arXiv preprint arXiv:1709.05706, 2017. 2\n[25] D. P. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization. International Conference for Learn-\ning Representations, 2015. 5\n[26] J. Lee, Y . Lee, J. Kim, A. R. Kosiorek, S. Choi, and\nY . W. Teh. Set transformer. CoRR, abs/1810.00825,\n2018. 5, 11\n[27] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepa-\nssi, L. Kaiser, and N. Shazeer. Generating wikipedia\n9\nby summarizing long sequences. arXiv preprint\narXiv:1801.10198, 2018. 2\n[28] P. W. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J.\nBallard, A. Banino, M. Denil, R. Goroshin, L. Sifre,\nK. Kavukcuoglu, D. Kumaran, and R. Hadsell. Learn-\ning to navigate in complex environments. CoRR,\nabs/1611.03673, 2016. 1, 2, 6\n[29] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lilli-\ncrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asyn-\nchronous methods for deep reinforcement learning. In\nICML, 2016. 2\n[30] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu,\nJ. Veness, M. G. Bellemare, A. Graves, M. A.\nRiedmiller, A. Fidjeland, G. Ostrovski, S. Petersen,\nC. Beattie, A. Sadik, I. Antonoglou, H. King,\nD. Kumaran, D. Wierstra, S. Legg, and D. Hass-\nabis. Human-level control through deep reinforcement\nlearning. Nature, 518:529–533, 2015. 2, 5\n[31] A. Mousavian, A. Toshev, M. Fiser, J. Kosecka, and\nJ. Davidson. Visual representations for semantic target\ndriven navigation. arXiv preprint arXiv:1805.06066,\n2018. 2\n[32] J. Oh, V . Chockalingam, S. P. Singh, and H. Lee.\nControl of memory, active perception, and action in\nminecraft. In ICML, 2016. 1, 2, 6\n[33] E. Parisotto and R. Salakhutdinov. Neural map: Struc-\ntured memory for deep reinforcement learning.CoRR,\nabs/1702.08360, 2017. 1, 2\n[34] R. Pascanu, T. Mikolov, and Y . Bengio. On the difﬁ-\nculty of training recurrent neural networks. In ICML,\n2013. 1, 2\n[35] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++:\nDeep hierarchical feature learning on point sets in a\nmetric space. In Advances in Neural Information Pro-\ncessing Systems, pages 5099–5108, 2017. 5\n[36] N. Savinov, A. Dosovitskiy, and V . Koltun. Semi-\nparametric topological memory for navigation. In-\nternational Conference on Learning Representations ,\n2018. 2\n[37] R. Sim and J. J. Little. Autonomous vision-based\nexploration and mapping using hybrid maps and rao-\nblackwellised particle ﬁlters. 2006 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems ,\npages 2082–2089, 2006. 2\n[38] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and\nT. Funkhouser. Semantic scene completion from a sin-\ngle depth image. CVPR, 2017. 6\n[39] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end\nmemory networks. In Advances in neural information\nprocessing systems, pages 2440–2448, 2015. 2\n[40] S. Thrun. Simultaneous localization and mapping.\nIn Robotics and cognitive approaches to spatial map-\nping, pages 13–41. Springer, 2007. 2\n[41] M. Tomono. 3-d object map building using dense\nobject models with sift-based recognition features.\n2006 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems, pages 1885–1890, 2006. 2\n[42] T. H. Trinh, A. M. Dai, T. Luong, and Q. V . Le. Learn-\ning longer-term dependencies in rnns with auxiliary\nlosses. In ICML, 2018. 2\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural In-\nformation Processing Systems , 2017. 1, 2, 4, 5, 11,\n12\n[44] D. Wierstra, A. F ¨orster, J. Peters, and J. Schmidhuber.\nRecurrent policy gradients.Logic Journal of the IGPL,\n18:620–634, 2010. 1\n[45] D. Wooden. A guide to vision-based map building.\nIEEE Robotics & Automation Magazine , 13:94–98,\n2006. 2\n[46] Y . Wu, Y . Wu, G. Gkioxari, and Y . Tian. Building\ngeneralizable agents with a realistic and rich 3d envi-\nronment. CoRR, abs/1801.02209, 2018. 2, 6\n[47] F. Xia, A. R. Zamir, Z.-Y . He, A. Sax, J. Malik, and\nS. Savarese. Gibson env: Real-world perception for\nembodied agents. CoRR, abs/1808.10654, 2018. 1, 2\n[48] J. Zhang, L. Tai, J. Boedecker, W. Burgard,\nand M. Liu. Neural slam. arXiv preprint\narXiv:1706.09520, 2017. 2\n[49] M. Zhang, Z. McCarthy, C. Finn, S. Levine, and\nP. Abbeel. Learning deep neural network policies\nwith continuous memory states. In 2016 IEEE In-\nternational Conference on Robotics and Automation\n(ICRA), pages 520–527. IEEE, 2016. 1\n[50] Y . Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei,\nA. Gupta, R. Mottaghi, and A. Farhadi. Visual se-\nmantic planning using deep successor representations.\n2017 IEEE International Conference on Computer Vi-\nsion (ICCV), pages 483–492, 2017. 1, 2\n[51] Y . Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta,\nL. Fei-Fei, and A. Farhadi. Target-driven visual nav-\nigation in indoor scenes using deep reinforcement\nlearning. 2017 IEEE International Conference on\nRobotics and Automation (ICRA) , pages 3357–3364,\n2017. 1, 2\n10\nA. Environment Details\nIn all experiments, we simulate a mobile base of the\nFetch robot. The Fetch robot receives visual observations\nfrom a Primesense Carmine 1.09 short-range RGBD sen-\nsor mounted on its head. Accordingly, we render images\nof 640 ×480 resolution. To simulate the operation range\nof the depth sensor, we only render depth values for points\nthat are within 5 meters from the camera. We also provide\na binary mask indicating which pixels have valid depth val-\nues and concatenate the mask with the depth image as its\nsecond channel. We also add zero-mean Gaussian noise of\nwith a standard deviation of 0.05 meter to each pixel. The\nsegmentation mask uses the class labels from NYU40 [11]\nwith each pixel label encoded in the one-hot manner. We\nsubsample the rendered images by a factor of 10, providing\nus RGB images of 64×48×3, depth images of 64×48×2\nand segmentation masks of 64 ×48 ×40.\nThe environment dynamics is simulated for the Fetch\nrobot operating on a planar surface. The robot moves for-\nward and take turns by controlling the velocity of its two\nwheels, with a wheel radius of 0.065 meters and axis width\nof 0.375 meters. We add a zero-mean Gaussian with a stan-\ndard deviation of 0.5 rad/s to both wheels to simulate the\nnoisy dynamics. We check the collisions between the robot\nand the meshes of the environment. The robot will be reset\nto the previous pose when it collides by taking the action.\nB. Analysis of Memory Factorization\nIn memory factorization, it is crucial to choose represen-\ntative centers that have a good coverage of all past obser-\nvations. Therefore, the centers should be distant from each\nother in the feature space. Since the memory keeps grow-\ning across time, the centers are supposed to be dynamically\nupdated during the task execution instead of remaining as\nstatic vectors for all episodes.\nIn this section, we compare the farthest point sampling\n(FPS) used in SMT with two alternative types of represen-\ntative centers. We refer to Window as the baseline which\nuses the last |˜M|time steps in a ﬁxed time window as rep-\nresentative centers. In this way, the centers are dynamically\nupdated but only focus on the most recent history. We also\nimplemented the static inducing points in [26], which we\nrefer to as Static. The |˜M|inducing points are trained as\nneural network weights and remain static during test time.\nWe compare the performance of the three types of centers\non the validation set by setting |˜M|to be 100. As shown in\nTable. 4, FPS achieves comparable task performance with\nStatic in the roaming task. And it outperforms the two base-\nlines in coverage and search.\nCenter Type Roaming Coverage Search\nWindow 378.0 451.6 438.7\nStatic 383.9 457.96 445.9\nFPS 383.3 481.2 462.7\nTable 4. Performance of using different types of representative\ncenter in memory factorization. Average rewards are listed.\nFigure 7. Robustness to noisy dynamics. We compare three po-\nsitional embedding methods under noisy environment dynamics.\nThe standard deviation of the noise is swept from 0.0 to 1.0.\nC. Robustness to Noisy Dynamics\nIn this section, we evaluate the robustness of our model\nto noisy environment dynamics. Instead of retrieving the\nground truth poses pt from the environment, we estimate\nthe pose using the action at. at provides us translation and\nrotation of the agent w.r.t. the previous pose. Thus we can\nestimate the ˆpt+1 at each time step using at and the previ-\nous estimation ˆpt. When there is no noise, ˆpt is equivalent\nto pt. With the Gaussian noise added at each time step, the\nnoise added to ˆpt will be a Gaussian process. Therefore,\nwhen computing the observation embedding using the rela-\ntive poses, recent steps suffer less from the noisy dynamics.\nIn our design of SMT, we use a positional embedding of\nthe time step similar to [43], but with exponential functions\ninstead of sinusoidal functions. The positional embedding\nprovides temporal information of each time step for the pol-\nicy. Sinusoidal function is periodic and provides only rela-\ntive temporal information. In contrast, the exponential func-\n11\ntion is monotonic and represents how recent each time step\nis. In the long-horizon tasks we are interested in, we believe\nrelative temporal information is not sufﬁcient for the agent\nto understand long-term dependencies.\nTo validate this assumption, we compare the exponential\nembedding with the two baselines. No embedding does\nnot embed the positional embedding of the time step. Sinu-\nsoidal uses the same sinusoidal embedding function as in\n[43]. We sweep the standard deviation of the noise from\n0.0 to 1.0 and evaluates the average rewards on the val-\nidation set. In practice, we found the temporal informa-\ntion not only improves the performance given clean obser-\nvations, but also helps leverage the noisy environment dy-\nnamics across time. As shown in Fig. 7, the average rewards\ndecrease with more noises in dynamics. Sinusoidal and ex-\nponential embeddings both mitigate the performance drop.\nIn the roaming task, the two embedding methods have com-\nparable effects. While in coverage and search, exponential\nembedding has the superior performance.\nD. More Visualization\nWe present more visualization of the agent behaviors for\nroaming in Fig. 8, for coverage in Fig. 9 and for search in\nFig. 10. As in the main paper, we visualize the trajecto-\nries from the top-down view as green curves, with white\nand black dots indicating the starting and ending points.\nNavigable area are masked in dark purple with red lines as\nthe collision boundaries. In the coverage task (Fig. 9), we\nmark the covered cells in pink. In the search task (Fig. 10),\nwe mark target objects with yellow masks. These ﬁgures\ndemonstrate similar behaviors as analyzed in the main pa-\nper. The same reactive and LSTM baselines as in the main\npaper are used to compare with the proposed SMT policy.\n12\nReactive LSTM SMT (Ours)\nFigure 8. Visualization of the agent behaviors in the Roaming Task.\n13\nReactive LSTM SMT (Ours)\nFigure 9. Visualization of the agent behaviors in the Coverage Task.\n14\nReactive LSTM SMT (Ours)\nFigure 10. Visualization of the agent behaviors in the Search Task.\n15"
}