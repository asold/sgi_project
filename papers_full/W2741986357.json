{
    "title": "Cross-Lingual Word Embeddings for Low-Resource Language Modeling",
    "url": "https://openalex.org/W2741986357",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A2278151816",
            "name": "Oliver Adams",
            "affiliations": [
                "Carnegie Mellon University Australia"
            ]
        },
        {
            "id": null,
            "name": "Adam Makarucha",
            "affiliations": [
                "Carnegie Mellon University Australia"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University Australia"
            ]
        },
        {
            "id": "https://openalex.org/A2120683996",
            "name": "Steven Bird",
            "affiliations": [
                "Carnegie Mellon University Australia"
            ]
        },
        {
            "id": "https://openalex.org/A2188741563",
            "name": "Trevor Cohn",
            "affiliations": [
                "Carnegie Mellon University Australia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2126725946",
        "https://openalex.org/W2963174553",
        "https://openalex.org/W1523296404",
        "https://openalex.org/W2251855842",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2963514026",
        "https://openalex.org/W2295781714",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2118090838",
        "https://openalex.org/W2963915291",
        "https://openalex.org/W2085779738",
        "https://openalex.org/W2299662909",
        "https://openalex.org/W2963216505",
        "https://openalex.org/W168564468",
        "https://openalex.org/W2284175681",
        "https://openalex.org/W2080021477",
        "https://openalex.org/W2144945507",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963824800",
        "https://openalex.org/W4230813784",
        "https://openalex.org/W744643371",
        "https://openalex.org/W2963932686",
        "https://openalex.org/W2007249154",
        "https://openalex.org/W2067438047",
        "https://openalex.org/W2020073413",
        "https://openalex.org/W2254973503",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2171082019",
        "https://openalex.org/W2053921957",
        "https://openalex.org/W2158199200",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W1576954243",
        "https://openalex.org/W2123024445",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2486889634",
        "https://openalex.org/W342285082",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2963002901",
        "https://openalex.org/W2252046065",
        "https://openalex.org/W2270364989",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2160815625",
        "https://openalex.org/W2273474218",
        "https://openalex.org/W574924521",
        "https://openalex.org/W2987352513",
        "https://openalex.org/W2399730253",
        "https://openalex.org/W2251033195",
        "https://openalex.org/W2513016193",
        "https://openalex.org/W2574132677",
        "https://openalex.org/W2963088995"
    ],
    "abstract": "Oliver Adams, Adam Makarucha, Graham Neubig, Steven Bird, Trevor Cohn. Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 2017.",
    "full_text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947,\nValencia, Spain, April 3-7, 2017.c⃝2017 Association for Computational Linguistics\nCross-Lingual Word Embeddings for Low-Resource Language Modeling\nOliver Adams,♠♥ Adam Makarucha,♠ Graham Neubig,♣ Steven Bird,♥♦ Trevor Cohn♥\n♠IBM Research – Australia\n♥Computing and Information Systems, University of Melbourne\n♣School of Computer Science, Carnegie Mellon University\n♦International Computer Science Institute, University of California Berkeley\noliver.adams@gmail.com, adamjm@au1.ibm.com,\ngneubig@cs.cmu.edu, {t.cohn,steven.bird}@unimelb.edu.au\nAbstract\nMost languages have no established writ-\ning system and minimal written records.\nHowever, textual data is essential for nat-\nural language processing, and particularly\nimportant for training language models to\nsupport speech recognition. Even in cases\nwhere text data is missing, there are some\nlanguages for which bilingual lexicons are\navailable, since creating lexicons is a fun-\ndamental task of documentary linguistics.\nWe investigate the use of such lexicons\nto improve language models when tex-\ntual training data is limited to as few as a\nthousand sentences. The method involves\nlearning cross-lingual word embeddings\nas a preliminary step in training monolin-\ngual language models. Results across a\nnumber of languages show that language\nmodels are improved by this pre-training.\nApplication to Yongning Na, a threatened\nlanguage, highlights challenges in deploy-\ning the approach in real low-resource en-\nvironments.\n1 Introduction\nMost of the world’s languages are not actively\nwritten, even languages with an ofﬁcial writing\nsystem (Bird, 2011). This limits the available\ntextual data to small quantities of phonemic tran-\nscriptions prepared by linguists. Since phone-\nmic transcription is time-consuming, such data is\nscarce. This makes language modeling, which\nis a key tool for facilitating speech recognition\nof these languages, a difﬁcult challenge. One\nof the touted advantages of neural network lan-\nguage models (NNLMs) is their ability to model\nsparse data (Bengio et al., 2003; Gandhe et al.,\n2014). However, despite the success of NNLMs\non large datasets (Mikolov et al., 2010; Sutskever\net al., 2011; Graves, 2013), it remains unclear\nwhether their advantages transfer to scenarios with\nextremely limited amounts of data.\nAppropriate initialization of parameters in neu-\nral network frameworks has been shown to be ben-\neﬁcial across a wide variety of domains, includ-\ning speech recognition, where unsupervised pre-\ntraining of deep belief networks was instrumental\nin attaining breakthrough performance (Hinton et\nal., 2012). Neural network approaches to a range\nof NLP problems have also been aided by ini-\ntialization with word embeddings trained on large\namounts of unannotated text (Frome et al., 2013;\nZhang et al., 2014; Lau and Baldwin, 2016). How-\never, in the case of extremely low-resource lan-\nguages we do not have the luxury of this unanno-\ntated text.\nAs a remedy to this problem we focus on cross-\nlingual word embeddings (CLWEs), which learn\nword embeddings using information from multi-\nple languages. Recent advances in CLWEs have\nshown that high quality embeddings can be learnt\neven in the absence of bilingual corpora by har-\nnessing bilingual lexicons (Gouws and Søgaard,\n2015; Duong et al., 2016). This is useful as some\nthreatened and endangered languages have been\nsubject to signiﬁcant linguistic investigation, lead-\ning to the creation of high-quality lexicons, de-\nspite the dearth of transcriptions. For example, the\ntraining of a quality speech recognition system for\nYongning Na, a Sino-Tibetan language spoken by\napproximately 40k people, is hindered by this lack\nof data (Do et al., 2014) despite signiﬁcant linguis-\ntic investigation of the language (Michaud, 2008;\nMichaud, 2016).\nIn this paper we address two research questions.\nFirst, is the quality of CLWEs dependent on hav-\ning large amounts of data in multiple languages, or\ncan large amounts of data in a single source lan-\n937\nguage inform embeddings trained with littletarget\nlanguage data? Secondly, can such CLWEs im-\nprove language modeling in low-resource contexts\nby initializing the parameters of an NNLM?\nTo answer these questions, we scale down the\navailable monolingual data of the target language\nto as few as 1k sentences, while maintaining a\nlarge source language dataset. We assess intrin-\nsic embedding quality by considering correlation\nwith human judgment on the WordSim353 test\nset (Finkelstein et al., 2001). We then perform\nlanguage modeling experiments where we initial-\nize the parameters of a long short-term mem-\nory (LSTM) language model for low-resource lan-\nguage model training across a variety of language\npairs.\nResults indicate that CLWEs remain resilient\nwhen target language training data is drastically\nreduced in a simulated low-resource environment,\nand that initializing the embedding layer of an\nNNLM with these CLWEs consistently leads to\nbetter performance of the language model. In light\nof these results, we explore the method’s applica-\ntion to Na, an actual low-resource language with\nrealistic manually created lexicons and transcribed\ndata. We present a discussion of the negative re-\nsults found which highlights challenges and future\nopportunities.\n2 Related Work\nThis paper draws on work in three general areas,\nwhich we brieﬂy describe in this section.\nNeural network language models and word em-\nbeddings Bengio et al. (2003) and Goodman\n(2001) introduce word embeddings in the context\nof an investigation of neural language modeling.\nOne claimed advantage of such models is the abil-\nity to cope with sparse data by sharing information\namong words with similar characteristics. Neural\nlanguage modeling has since demonstrated pow-\nerful capabilities at the word level (Mikolov et al.,\n2010) and character level (Sutskever et al., 2011).\nNotably, LSTM models (Hochreiter and Schmid-\nhuber, 1997) for modeling long-ranging statisti-\ncal inﬂuences have been shown to be effective\n(Graves, 2013; Zaremba et al., 2014).\nWord embeddings have became more popular\nthrough the application of shallow neural network\narchitectures that allow for training on large quan-\ntities of data (Mnih et al., 2009; Bengio et al.,\n2009; Collobert and Weston, 2008; Mikolov et\nal., 2013a), leading to many further investiga-\ntions (Chen et al., 2013; Pennington et al., 2014;\nShazeer et al., 2016; Bhatia et al., 2016). A key\napplication of word embeddings has been in the\ninitializing of neural network architectures for a\nwide variety of NLP tasks with limited annotated\ndata (Frome et al., 2013; Zhang et al., 2014; Zoph\net al., 2016; Lau and Baldwin, 2016).\nLow-resource language modeling and language\nmodel adaptation Bellegarda (2004) review\nlanguage model adaptation, and argue that small\namounts of in-domain data are often more valu-\nable than large amounts of out-of-domain data, but\nthat adapting background models using in-domain\ndata can be even better. Kurimo et al. (2016)\npresent more recent work on improving large vo-\ncabulary continuous speech recognition using lan-\nguage model adaptation for low-resource Finno-\nUgric languages.\nCross-lingual language modeling has also been\nexplored with work on interpolation of a sparse\nlanguage model with one trained on a large\namount of translated data (Jensson et al., 2008),\nand integrated speech recognition and translation\n(Jensson et al., 2009; Xu and Fung, 2013).\nGandhe et al. (2014) investigate NNLMs for\nlow-resource languages, comparing NNLMs with\ncount-based language models, and ﬁnd that\nNNLMs interpolated with count-based methods\noutperform standard n-gram models even with\nsmall quantities of training data. In contrast, our\ncontribution is an investigation into harnessing\nCLWEs learnt using bilingual dictionaries in order\nto improve language modeling in a similar low-\nresource setting.\nCross-lingual word embeddings Cross-lingual\nword embeddings have also been the subject of\nsigniﬁcant investigation. Many methods require\nparallel corpora or comparable corpora to connect\nthe languages (Klementiev et al., 2012; Zou et al.,\n2013; Hermann and Blunsom, 2013; Chandar A\nP et al., 2014; Ko ˇcisk´y et al., 2014; Coulmance\net al., 2015; Wang et al., 2016), while others use\nbilingual dictionaries (Mikolov et al., 2013b; Xiao\nand Guo, 2014; Faruqui and Dyer, 2014; Gouws\nand Søgaard, 2015; Duong et al., 2016; Ammar et\nal., 2016), or neither (Miceli Barone, 2016).\nIn particular, we build on the work of Duong et\nal. (2016). Their method harnesses monolingual\ncorpora in two languages along with a bilingual\n938\nlexicon to connect the languages and represent the\nwords in a common vector space. The model\nbuilds on the continuous bag-of-words (CBOW)\nmodel (Mikolov et al., 2013a) which learns em-\nbeddings by predicting words given their contexts.\nThe key difference is that the word to be predicted\nis a target language translation of a source lan-\nguage word centered in a source language context.\nSince dictionaries tend to include a number of\ntranslations for words, the model uses an iterative\nexpectation-maximization style training algorithm\nin order to best select translations given the con-\ntext. This process thus allows for polysemy to be\naddressed which is desirable given the polysemous\nnature of bilingual dictionaries.\n3 Resilience of Cross-Lingual Word\nEmbeddings\nPrevious work using CLWEs typically assumes a\nsimilar amount of training data of each available\nlanguage, often in the form of parallel corpora.\nRecent work has shown that monolingual corpora\nof two different languages can be tied together\nwith bilingual dictionaries in order to learn em-\nbeddings for words in both languages in a common\nvector space (Gouws and Søgaard, 2015; Duong et\nal., 2016). In this section we relax the assumption\nof the availability of large monolingual corpora on\nthe source and target sides, and report an experi-\nment on the resilience of such CLWEs when data\nis scarce in the target language but plentiful in a\nsource language.\n3.1 Experimental Setup\nWord embedding quality is commonly assessed by\nevaluating the correlation of the cosine similar-\nity of the embeddings with human judgements of\nword similarity. Here we follow the same evalu-\nation procedure, except where we simulate a low-\nresource language by reducing the availability of\ntarget English monolingual text while preserving\na large quantity of source language text from other\nlanguages. This allows us to evaluate the CLWEs\nintrinsically using the WordSim353 task (Finkel-\nstein et al., 2001) before progressing to down-\nstream language modeling where we additionally\nconsider other target languages.\nWe trained a variety of embeddings on English\nWikipedia data of between 1k and 128k sentences\nfrom the training data of Al-Rfou et al. (2013).\nIn terms of transcribed speech data, this roughly\nequates to between 1 and 128 hours of speech. For\nthe training data, we randomly chose sentences\nthat include words in the WordSim353 task pro-\nportionally to their frequency in the set. As mono-\nlingual baselines, we use the skip-gram (SG) and\nCBOW methods of Mikolov et al. (2013a) as im-\nplemented in the Gensim package ( ˇReh˚uˇrek and\nSojka, 2010). We additionally used off-the-shelf\nCBOW Google News Corpus embeddings with\n300 dimensions, trained on 100 billion words.\nThe CLWEs were trained using the method of\nDuong et al. (2016) since their method addresses\npolysemy which is rampant in dictionaries. The\nsame 1k-128k sentence English Wikipedia data\nwas used but with an additional 5 million sen-\ntences of Wikipedia data in a source language. The\nsource languages include Japanese, German, Rus-\nsian, Finnish, and Spanish, which represent lan-\nguages of varying similarity with English, some\nwith great morphological and syntactic differ-\nences. To relate the languages, we used thePanLex\nlexicon (Kamholz et al., 2014). Following Duong\net al. (2016), we used the default window size of\n48 so that the whole sentence’s context is almost\nalways taken into account. This mitigates the ef-\nfect of word re-ordering between languages. We\ntrained with an embedding dimension of 200 for\nall data sizes as a larger dimension turned out to be\nhelpful in capturing information from the source\nside.1\n3.2 Results\nFigure 1 shows correlations with human judgment\nin the WordSim353 task. The x-axis represents the\nnumber of English training sentences. Coloured\nlines represent CLWEs trained on different lan-\nguages: Japanese, German, Spanish, Russian and\nFinnish.2\nWith around 128k sentences of training data,\nmost methods perform quite well, with German\nbeing the best performing. Interestingly the\nCLWE methods all outperform GNC which was\ntrained on a far larger corpus of 100 billion words.\nWith only 1k sentences of target training data, all\nthe CLWEs have a correlation around 0.5, with\nthe exception of Finnish. Interestingly, no consis-\n1Hyperparameters for both mono and cross-lingual word\nembeddings: iters=15, negative=25, size=200, window=48,\notherwise default. Smaller window sizes led to similar results\nfor monolingual methods.\n2We also tried Italian, Dutch, German and Serbian, yield-\ning similar results but omitted for presentation.\n939\n1,000 10,000 100,000\n0.0\n0.2\n0.4\n0.6\n0.8\nSentences\nSpearman’sρ\nGNC CBOW SG –ja\n–de –ru –ﬁ –es\nFigure 1: Performance of different embeddings on\nthe WordSim353 task with different amounts of\ntraining data. GNC is the Google News Corpus\nembeddings, which are constant. CBOW and SG\nare the monolingual word2vec embeddings. The\nother, colored, lines are all cross-lingual word em-\nbeddings harnessing the information of 5m sen-\ntences of various source languages.\ntent beneﬁt was gained by using source languages\nfor which translation with English is simpler. For\nexample, Spanish often under-performed Russian\nand Japanese as a source language, as well as the\nmorphologically-rich Finnish.\nNotably, all the CLWEs perform far better than\ntheir monolingual counterparts on small amounts\nof data. This resilience of the target English word\nembeddings suggests that CLWEs can serve as a\nmethod of transferring semantic information from\nresource-rich languages to the resource-poor, even\nwhen the languages are quite different. However,\nthe WordSim353 task is a constrained environ-\nment, so in the next section we turn to language\nmodeling, a natural language processing task of\nmuch practical importance for resource-poor lan-\nguages.\n4 Pre-training Language Models\nLanguage models are an important tool with\nparticular application to machine translation and\nspeech recognition. For resource-poor languages\nand unwritten languages, language models are\nalso a signiﬁcant bottleneck for such technologies\nas they rely on large quantities of data. In this sec-\ntion, we assess the performance of language mod-\nels on varying quantities of data, across a number\n1,000 10,000 100,000200\n400\n600\n800\n1,000\nSentences\nPerplexity\nMKN3 MKN4 MKN5 20\n50 100 200 300\nFigure 2: Perplexity of language models on the\nvalidation set. Numbers in the legend indicate\nLSTM language models with different hidden\nlayer sizes, as opposed to Modiﬁed Kneser-Ney\nlanguage models of order 3, 4 and 5.\nof different source–target language pairs. In par-\nticular, we use CLWEs to initialize the ﬁrst layer\nin an LSTM recurrent neural network language\nmodel and assess how this affects language model\nperformance. This is an interesting task not simply\nfor the practical advantage of having better lan-\nguage models for low-resource languages. Lan-\nguage modeling is a syntax-oriented task, yet syn-\ntax varies greatly between the languages we train\nthe CLWEs on. This experiment thus yields some\nadditional information about how effectively bilin-\ngual information can be used for the task of lan-\nguage modeling.\n4.1 Experimental Setup\nWe experiment with a similar data setup as in\nSection 3. However, target training sentences are\nnot constrained to include words observed in the\nWordSim353 set, and are random sentences from\nthe aforementioned 5 million sentence corpus. For\neach language, the validation and test sets consist\nof 3k randomly selected sentences. The large vo-\ncabulary of Wikipedia and the small amounts of\ntraining data used make this a particularly chal-\nlenging language modeling task.\nFor our NNLMs, we use the LSTM language\nmodel of Zaremba et al. (2014). As a count-\nbased baseline, we use Modiﬁed Kneser-Ney\n(MKN) (Kneser and Ney, 1995; Chen and Good-\nman, 1999) as implemented in KenLM (Heaﬁeld,\n940\n2011). Figure 2 presents some results of tuning\nthe dimensions of the hidden layer in the LSTM\nwith respect to perplexity on the validation set, 3\nas well as tuning the order of n-grams used by\nthe MKN language model. A dimension of 100\nyielded a good compromise between the smaller\nand larger training data sizes, while an order 5\nMKN model performed slightly better than its\nlower-order brethren.4\nInterestingly, MKN strongly outperforms the\nLSTM on low quantities of data, with the LSTM\nlanguage model not reaching parity until between\n16k and 32k sentences of data. This is consistent\nwith the results of Chen et al. (2015) and Neubig\nand Dyer (2016) that show that n-gram models are\ntypically better for rare words, and here our vo-\ncabulary is large but training data small since the\ndata are random Wikipedia sentences. However\nthese ﬁndings are inconsistent with the belief that\nNNLMs have the ability to cope well with sparse\ndata conditions because of the smooth distribu-\ntions that arise from using dense vector represen-\ntations of words (Bengio et al., 2003). Traditional\nsmoothing stands strong.\n4.2 English Results\nWith the parameters tuned on the English valida-\ntion set as above, we evaluated the LSTM lan-\nguage model when the embedding layer is initial-\nized with various monolingual and cross-lingual\nword embeddings. Figure 3 compares the perfor-\nmance of a number of language models on the test\nset. In every case where pre-trained embeddings\nwere used, the embedding layer was held ﬁxed\nduring training. However, we observed similar re-\nsults when allowing them to deviate from their ini-\ntial state. For the CLWEs, the same language set\nwas used as in Section 3. The curves for the source\nlanguages (Dutch, Greek, Finnish, and Japanese)\nare remarkably similar, as were those for the lan-\nguages omitted from the ﬁgure (German, Russian,\nSerbian, Italian, and Spanish). This suggests that\nthe English target embeddings are gleaning simi-\nlar information from each of the languages, infor-\nmation likely to be more semantic than syntactic,\ngiven the syntactic differences between the lan-\nguages.\n3We used 1 hidden layer but otherwise the same as the\nSmallConﬁg of models/rnn/ptb/ptb word lm.py available in\nTensorﬂow.\n4Note that all perplexities in this paper include out-of-\nvocabulary words, of which there are many.\n1,000 10,000 100,000200\n400\n600\n800\n1,000\nSentences\nPerplexity\nMKN LSTM GNC mono\n–nl –el –ﬁ –ja\nFigure 3: Perplexity of LSTMs when pre-trained\nwith cross-lingual word embeddings trained on the\nsame data. LSTM is a neural network language\nmodel with no pre-trained embeddings. mono is\npre-trained with monolingual word2vec embed-\ndings. GNC is pre-trained with Google News Cor-\npus embeddings of dimension 300. The rest are\npre-trained with CLWEs using information trans-\nfer from different source languages. MKN is an\norder 5 Modiﬁed Kneser-Ney baseline.\n1,000 10,000 100,000200\n300\n400\n500\n600\nSentences\nPerplexity\nMKN LSTM –nl\n–el –ﬁn –ja\nFigure 4: Perplexities when interpolating MKN\nwith LSTMs pre-trained with various cross-\nlingual word embeddings. LSTM interpolates\nMKN with a neural network language model with\nno pre-trained embeddings. The rest are inter-\npolations of MKN with LSTMs pre-trained with\nCLWEs using information transfer from different\nsource languages. MKN is an order 5 Modiﬁed\nKneser-Ney baseline without interpolation.\n941\nWe compare these language models pre-trained\nwith CLWEs with pre-training using other embed-\ndings. Pre-training with the Google News Cor-\npus embeddings of the method of Mikolov et al.\n(2013c) unsurprisingly performs the best due to\nthe large amount of English data not available\nto the other methods, making it a sort of oracle.\nMonolingual pre-training of word embeddings on\nthe same English data (mono) used by the CLWEs\nyields poorer performance.\nThe language models initialized with pre-\ntrained CLWEs are signiﬁcantly better than their\nun-pre-trained counterpart on small amounts of\ndata, reaching par performance with MKN at\nsomewhere just past 4k sentences of training\ndata. In contrast, it takes more than 16k sen-\ntences of training data before the plain LSTM lan-\nguage model began to outperform MKN. The out-\nperformance of LSTMs by MKN with the lowest\namounts of training data motivated interpolation\nof MKN probabilities with LSTM language model\nprobabilities, as shown in Figure 4. Such interpo-\nlation allows for consistent improvement beyond\nthe performance of MKN or CLWE-pre-trained\nLSTMs alone.\n4.3 Other Target Languages\nIn Table 1 we present results of language model\nexperiments run with other languages used as the\nlow-resource target. In this table English is used in\neach case as the large source language with which\nto help train the CLWEs. The observation that the\nCLWE-pre-trained language model tended to per-\nform best relative to alternatives at around 8k or\n16k sentences in the English case prompted us to\nchoose these slices of data when assessing other\nlanguages as targets.\nThe pre-trained LSTM language model outper-\nforms its non-pre-trained counterpart for all lan-\nguages. There is competition between MKN and\nthe CLWE-pre-trained models. The languages for\nwhich MKN tends to do better are typically those\nfurther from English or those with rich morphol-\nogy, making cross-lingual transfer of information\nmore challenging. There seems to be a degree of\nasymmetry here: while all languages helped En-\nglish language modeling similarly, English helps\nthe other languages to varying degrees. For all lan-\nguages, interpolating MKN with the CLWE ( In-\nterp.) yields the best performance, corroborating\nthe ﬁndings of Gandhe et al. (2014).\nNeural language modeling of sparse data can\nbe improved by initializing parameters with cross-\nlingual word embeddings. The consistent per-\nformance improvements gained by an LSTM us-\ning CLWE-initialization is a promising sign for\nCLWE-initialization of neural networks for other\ntasks given limited target language data.\n5 First Steps in an Under-Resourced\nLanguage\nHaving demonstrated the effectiveness of CLWE-\npre-training of language models using simulation\nin a variety of well-resourced written languages,\nwe proceed to a preliminary investigation of this\nmethod to a low-resource, unwritten language, Na.\nYongning Na is a Sino-Tibetan language spoken\nby approximately 40k people in an area in Yunnan,\nChina, near the border with Sichuan. It has no or-\nthography and is tonal with a rich morphophonol-\nogy. Given the small quantity of manually tran-\nscribed phonemic data available in the language,\nNa provides an ideal test bed for investigating the\npotential and difﬁculties this method faces in a re-\nalistic setting. In this section we report results in\nNa language modeling and discuss hurdles to be\novercome.\n5.1 Experimental Setup\nThe phonemically transcribed corpus 5 consists of\n3,039 phonemically transcribed sentences which\nare a subset of a larger spoken corpus. These\nsentences are segmented at the level of the word,\nmorpheme and phonological process, and have\nbeen translated into French, with smaller amounts\ntranslated into Chinese and English. The corpus\nalso includes word-level glosses in French and En-\nglish. The lexicon of Michaud (2016) contains ex-\nample sentences for entries, as well as translations\ninto French, English and Chinese.\nThe lexicon consists of around 2k Na entries,\nwith example sentences and translations into En-\nglish, French and Chinese. To choose an appro-\npriate segmentation of the corpus, we used a hier-\narchical segmentation method where words were\nqueried in the lexicon. If a given word was present\nthen it was kept as a token, otherwise the word was\nsplit into its constituent morphemes.\nWe took 2,039 sentences to be used as train-\ning data, with the remaining 1k sentences split\n5Available as part of the Pangloss collection at\nhttp://lacito.vjf.cnrs.fr/pangloss.\n942\n8k sentences 16k sentences\nLang MKN LSTM CLWE Interp. MKN LSTM CLWE Interp.\nGreek 827.3 920.3 780.4 650.6 749.8 687.9 634.4 549.5\nSerbian 492.8 586.3 521.3 408.0 468.8 485.3 447.8 365.7\nRussian 1656.8 2054.5 1920.4 1466.2 1609.5 1757.3 1648.3 1309.1\nItalian 777.0 794.9 688.3 592.2 686.2 627.7 559.7 493.4\nGerman 997.4 1026.0 1000.9 831.8 980.0 908.8 874.1 761.5\nFinnish 1896.4 2438.8 2165.5 1715.3 1963.3 2233.2 2109.9 1641.2\nDutch 492.1 491.3 456.2 381.4 447.9 412.8 378.0 330.1\nJapanese 1902.8 2662.4 2475.6 1866.7 1816.8 2462.8 2279.6 1696.9\nSpanish 496.3 481.8 445.6 387.7 445.9 412.9 369.6 331.2\nTable 1: Perplexity of language models trained on 8k and 16k sentences for different languages. MKN\nis an order 5 Modiﬁed Kneser-Ney language model. LSTM is a long short-term memory neural network\nlanguage model with no pre-training. CLWE is an LSTM language model pre-trained with cross-lingual\nword embeddings, using English as the source language.Interp. is an interpolation of MKN with CLWE.\nTypes Tokens\nTones 2,045 45,044\nNo tones 1,192 45,989\nTable 2: Counts of types and tokens across the\nwhole Na corpus, given our segmentation method.\nTones No tones\nMKN 59.4 38.0\nLSTM 74.8 46.0\nCLWE 76.6 46.2\nLem 76.8 44.7\nEn-split 76.4 47.0\nTable 3: Perplexities on the Na test set using En-\nglish as the source language. MKN is an order 5\nModiﬁed Kneser-Ney language model. LSTM is a\nneural network language model without pretrain-\ning. CLWE is the same LM with pre-trained Na–\nEnglish CLWEs. Lem is the same as CLWE except\nwith English lemmatization. En-split extends this\nby preprocessing the dictionary such that entries\nwith multiple English words are converted to mul-\ntiple entries of one English word.\nequally between validation and test sets. The\nphonemic transcriptions include tones, so we cre-\nated two preprocessed versions of the corpus: with\nand without tones. Table 2 exhibits type and to-\nken counts for these two variations. In addition\nto the CLWE approach used in Sections 3 and\n4, we additionally tried lemmatizing the English\nWikipedia corpus so that it each token was more\nlikely to be present in the Na–English lexicon.\n5.2 Results and Discussion\nTable 3 shows the Na language modeling results.\nPre-trained CLWEs do not signiﬁcantly outper-\nform that of the non-pre-trained, and MKN out-\nperforms both. Given the size of the training data,\nand the results of Section 4, it is no surprise that\nMKN outperforms the NNLM approaches. But the\nlack of beneﬁt in CLWE-pre-training the NNLMs\nrequires some reﬂection. We now proceed to dis-\ncuss the challenges of this data to explore why\nthe positive results of language model pre-training\nthat were seen in Section 4 were not seen in this\nexperiment.\nTones A key challenge arises because of Na’s\ntonal system. Na has rich tonal morphology. Syn-\ntactic relationships between words inﬂuence the\nsurface form tone a syllable takes. Thus, seman-\ntically identical words may take different surface\ntones than is present in the relevant lexical entry,\nresulting in mismatches with the lexicon.\nIf tones are left present, the percentage of Na\ntokens present in the lexicon is 62%. Remov-\ning tones yields a higher hit rate of 88% and al-\nlows tone mismatches between surface forms and\nlexical entries to be overcome. This beneﬁt is\ngained in exchange for higher polysemy, with an\naverage of 4.1 English translations per Na entry\nwhen tones are removed, as opposed to 1.9 when\ntones are present. Though this situation of poly-\nsemy is what the method of Duong et al. (2016)\nis designed to address, it means the language\nmodel fails to model tones and doesn’t signiﬁ-\ncantly help CLWE-pre-training in any case. Fu-\nture work should investigate morphophonological\n943\nprocessing for Na, since there is regularity behind\nthese tonal changes (Michaud, 2008) which could\nmitigate these issues if addressed.\nPolysemy We considered the polysemy of the\ntokens of other languages’ corpora in the Pan-\nLex dictionaries. Interestingly they were higher\nthan the Na lexicon with tones removed, ranging\nfrom 2.7 for Greek–English to 19.5 for German–\nEnglish. It seems the more important factor is the\namount of tokens in the English corpus that were\npresent in the lexicon. For the Na–English lexicon,\nthis was only 18% and 20% when lemmatized and\nunlemmatized, respectively. However it was 67%\nfor the PanLex lexicon. Low lexicon hit rates of\nboth the Na and English corpora must damage the\nCLWEs modeling capacity.\nLexicon word forms Not all the forms of many\nEnglish word groups are represented. For exam-\nple, only the inﬁnitive ‘to run’ is present, while\n‘running’, ‘ran’and ‘runs’are not. The limited\nscope of this lexicon motivates lemmatization on\nthe English side as a normalization step, which\nmay be of some beneﬁt (see Table 3). Further-\nmore, such lemmatization can be expected to re-\nduce the syntactic information present in embed-\ndings which does not transfer between languages\nas effectively as semantics.\nSome common words, such as ‘reading’ are\nnot present in the lexicon, but ‘to read aloud’\nis. Additionally, there are frequently entries such\nas ‘way over there’ and ‘masculine given name’\nthat are challenging to process. As an attempt\nto mitigate this issue, we segmented such English\nentries, creating multiple Na–English entries for\neach. However, results in Table 3 show that this\nfailed to show improvements. More sophisticated\nprocessing of the lexicon is required.\nLexicon size There are about 2,115 Na entries in\nthe lexicon and 2,947 Na–English entries, which\nmakes the lexicon especially small in comparison\nto the PanLex lexicon used in the previous experi-\nments. Duong et al. (2016) report large reductions\nin performance of CLWEs on some tasks when\nlexicon size is scaled down to 10k.\nTo better understand how limited lexicon size\ncould be affecting language model performance,\nwe performed an ablation experiment where ran-\ndom entries in the PanLex English–German lexi-\ncon were removed in order to restrict its size. Fig-\nure 5 shows the performance of English language\n1,000 10,000 100,000 1,000,000\n600\n650\n700\n750\n800\nEntries\nPerplexity\nLSTM full-dict sub-dict\nFigure 5: Perplexities of an English–German\nCLWE-pretrained language model trained on 2k\nEnglish sentences as the dictionary size available\nin CLWE training increases to its full size ( sub-\ndict). As points of comparison, LSTM is a long\nshort-term memory language model with no pre-\ntraining and full-dict is a CLWE-pretrained lan-\nguage model with the full dictionary available.\nmodeling when training data is restricted to 2k\nsentences (to emulate the Na case) and the size\nof the lexicon afforded to the CLWE training is\nadjusted. This can only serve as a rough compari-\nson, since PanLex is large and so a 1k entry subset\nmay contain many obscure terms and few useful\nones. Nevertheless, results suggest that a critical\npoint occurs somewhere in the order of 10k en-\ntries. However, since improvements are demon-\nstrated even with smaller dictionaries, this is fur-\nther evidence that more sophisticated preprocess-\ning of the Na lexicon is required.\nDomain Another difference that may contribute\nto the results is that the domain of the text is signif-\nicantly different. The Na corpus is a collection of\nspoken narratives transcribed, while the Wikipedia\narticles are encyclopaedic entries, which makes\nthe registers very different.\n5.3 Future Work on Na Language Modeling\nThough the technique doesn’t work out of the box,\nthis sets a difﬁcult and compelling challenge of\nharnessing the available Na data more effectively.\nThe lexicon is a rich source of other informa-\ntion, including part-of-speech tags, example sen-\ntences and multilingual translations. In addition to\nbetter preprocessing of the lexical information we\nhave already used, harnessing this additional in-\nformation is an important next step to improving\nNa language modeling. The corpus includes trans-\nlations into French, Chinese and English, as well\n944\nas glosses. Some CLWE methods can additionally\nutilize such parallel data (Coulmance et al., 2015;\nAmmar et al., 2016) and we leave to future work\nincorporation of this information as well.\nThe tonal system is well described (Michaud,\n2008), and so further Na-speciﬁc work should al-\nlow differences between surface form tones and\ntones in the lexicon to be bridged.\nOur work corroborates the observation that\nMKN performs well on rare words (Chen et al.,\n2015). Interpolation is an effective means to har-\nness this strength when training data is sparse.\nFurthermore, hybrid count-based and NNLMs\n(Neubig and Dyer, 2016) promise the best of both\nworlds for language modeling for low-resource\nlanguages.\n6 Conclusion\nIn this paper we have demonstrated that CLWEs\ncan remain resilient when training data in the tar-\nget language is scaled down drastically. Such\nCLWEs continue to perform well on the Word-\nSim353 task, as well as demonstrating down-\nstream efﬁcacy across a number of languages\nthrough initialization of NNLMs. This work sup-\nports CLWEs as a method of transfer of infor-\nmation to resource-poor languages by harnessing\ndistributional information in a large source lan-\nguage. We can expect parameter initialization with\nCLWEs trained on such asymmetric data condi-\ntions to aid in other NLP tasks too, though this\nshould be empirically assessed.\nAcknowledgements\nThis work was conducted during Oliver Adams’\ninternship at IBM Research Australia. We are\ngrateful for support from NSF Award 1464553\nand the DARPA/I2O, Contract Nos. HR0011-15-\nC-0114 and HR0011-15-C-0115.\nReferences\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena.\n2013. Polyglot: Distributed word representations\nfor multilingual NLP. In Proceedings of the Seven-\nteenth Conference on Computational Natural Lan-\nguage Learning, pages 183–192.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A. Smith.\n2016. Massively multilingual word embeddings.\narXiv:1602.01925.\nJerome R. Bellegarda. 2004. Statistical language\nmodel adaptation: Review and perspectives. Speech\nCommunication, 42(1):93–108.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. The Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nYoshua Bengio, J ´erˆome Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. Pro-\nceedings of the 26th annual international conference\non machine learning, pages 41–48.\nParminder Bhatia, Robert Guthrie, and Jacob Eisen-\nstein. 2016. Morphological priors for probabilis-\ntic neural word embeddings. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 490–500.\nSteven Bird. 2011. Bootstrapping the language\narchive: New prospects for natural language pro-\ncessing in preserving linguistic heritage. Linguistic\nIssues in Language Technology, 6:1–16.\nSarath Chandar A P, Stanislas Lauly, Hugo Larochelle,\nMm Mitesh Khapra, Balaraman Ravindran, Vikas C\nRaykar, and Amrita Saha. 2014. An autoencoder\napproach to learning bilingual word representations.\nIn Advances in Neural Information Processing Sys-\ntems 27, pages 1853–1861.\nStanley F Chen and Joshua Goodman. 1999. An\nempirical study of smoothing techniques for lan-\nguage modeling. Computer Speech & Language ,\n13(4):359–394.\nYanqing Chen, Bryan Perozzi, R Al-Rfou, and Steven\nSkiena. 2013. The expressive power of word em-\nbeddings. arXiv:1301.3226.\nWelin Chen, David Grangier, and Michael Auli. 2015.\nStrategies for training large vocabulary neural lan-\nguage models. arXiv:1512.04906.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. Proceed-\nings of the 25th international conference on Ma-\nchine learning, pages 160–167.\nJocelyn Coulmance, Jean-Marc Marty, Guillaume\nWenzek, and Amine Benhalloum. 2015. Trans-\ngram, fast cross-lingual word-embeddings. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1109–\n1113.\nThi-Ngoc-Diep Do, Alexis Michaud, and Eric Castelli.\n2014. Towards the automatic processing of Yongn-\ning Na (Sino-Tibetan): developing a ‘light’ acous-\ntic model of the target language and testing ‘heavy-\nweight’ models from ﬁve national languages. In 4th\nInternational Workshop on Spoken Language Tech-\nnologies for Under-resourced Languages , pages\n153–160.\n945\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven\nBird, and Trevor Cohn. 2016. Learning crosslingual\nword embeddings without bilingual corpora. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1285–\n1295.\nManaal Faruqui and Chris Dyer. 2014. Improving\nvector space word representations using multilingual\ncorrelation. In Proceedings of the 14th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 462–471.\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Eytan\nRuppin. 2001. Placing search in context: The con-\ncept revisited. In Proceedings of the 10th Interna-\ntional Conference on World Wide Web, WWW ’01,\npages 406–414.\nAndrea Frome, Greg S Corrado, Jon Shlens, Samy\nBengio, Jeff Dean, Marc’Aurelio Ranzato, and\nTomas Mikolov. 2013. Devise: A deep visual-\nsemantic embedding model. In Advances in Neu-\nral Information Processing Systems 26, pages 2121–\n2129.\nAnkur Gandhe, Florian Metze, and Ian Lane. 2014.\nNeural network language models for low resource\nlanguages. In INTERSPEECH-2014, pages 2615–\n2619.\nJoshua Goodman. 2001. A Bit of Progress in Lan-\nguage Modeling. Technical Report.\nStephan Gouws and Anders Søgaard. 2015. Simple\ntask-speciﬁc bilingual word embeddings. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1386–1390.\nAlex Graves. 2013. Generating sequences with recur-\nrent neural networks. arXiv:1308.0850.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187–197.\nKarl Moritz Hermann and Phil Blunsom. 2013. A sim-\nple model for learning multilingual compositional\nsemantics. arXiv:1312.6173.\nGeoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\nAbdel-rahman Mohamed, Navdeep Jaitly, Andrew\nSenior, Vincent Vanhoucke, Patrick Nguyen, Tara N\nSainath, and Others. 2012. Deep neural networks\nfor acoustic modeling in speech recognition: The\nshared views of four research groups. Signal Pro-\ncessing Magazine, IEEE, 29(6):82–97.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nArnar Jensson, Koji Iwano, and Sadaoki Furui. 2008.\nDevelopment of a speech recognition system for ice-\nlandic using machine translated text. In The ﬁrst\nInternational Workshop on Spoken Languages Tech-\nnologies for Under-resourced Languages, pages 18–\n21.\nArnar Jensson, Tasuku Oonishi, Koji Iwano, and\nSadaoki Furui. 2009. Development of a WFST\nbased speech recognition system for a resource deﬁ-\ncient language using machine translation. Proceed-\nings of Asia-Paciﬁc Signal and Information Process-\ning Association, pages 50–56.\nDavid Kamholz, Jonathan Pool, and Susan Colowick.\n2014. PanLex: Building a resource for panlingual\nlexical translation. In Proceedings of the Ninth In-\nternational Conference on Language Resources and\nEvaluation, pages 3145–3150.\nAlexandre Klementiev, Ivan Titov, and Binod Bhat-\ntarai. 2012. Inducing crosslingual distributed rep-\nresentations of words. In Proceedings of COLING\n2012, pages 1459–1474.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In1995\nInternational Conference on Acoustics, Speech, and\nSignal Processing, pages 181–184.\nTom´aˇs Koˇcisk´y, Karl Moritz Hermann, and Phil Blun-\nsom. 2014. Learning bilingual word representations\nby marginalizing alignments. In Proceedings of the\n52nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n224–229.\nMikko Kurimo, Seppo Enarvi, Ottokar Tilk, Matti Var-\njokallio, Andr´e Mansikkaniemi, and Tanel Alum ¨ae.\n2016. Modeling under-resourced languages for\nspeech recognition. Language Resources and Eval-\nuation, pages 1–27.\nJey Han Lau and Timothy Baldwin. 2016. An empiri-\ncal evaluation of doc2vec with practical insights into\ndocument embedding generation. In 1st Workshop\non Representation Learning for NLP, pages 78–86.\nAntonio Valerio Miceli Barone. 2016. Towards cross-\nlingual distributed representations without parallel\ntext trained with adversarial autoencoders. In Pro-\nceedings of the 1st Workshop on Representation\nLearning for NLP, pages 121–126.\nAlexis Michaud. 2008. Phonemic and tonal analysis of\nYongning Na*. Cahiers de Linguistique Asie Orien-\ntale, 37(2):159–196.\nAlexis Michaud. 2016. Online Na-English-\nChinese Dictionary. https://halshs.archives-\nouvertes.fr/halshs-01204638. This is version 1.1 of\nthe dictionary.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Re-\ncurrent neural network based language model. In\nINTERSPEECH-2010, pages 1045–1048.\n946\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efﬁcient estimation of word represen-\ntation in vector space. arXiv:1301.3781.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever.\n2013b. Exploiting similarities among languages for\nmachine translation. arXiv:1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013c. Distributed represen-\ntations of words and phrases and their composition-\nality. In Advances in neural information processing\nsystems, pages 3111–3119.\nAndriy Mnih, Zhang Yuecheng, and Geoffrey Hin-\nton. 2009. Improving a statistical language model\nthrough non-linear prediction. Neurocomputing,\n72(7-9):1414–1418.\nGraham Neubig and Chris Dyer. 2016. Generalizing\nand hybridizing count-based and neural language\nmodels. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1163–1172.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1532–1543.\nRadim ˇReh˚uˇrek and Petr Sojka. 2010. Software frame-\nwork for topic modelling with large corpora. InPro-\nceedings of the LREC 2010 Workshop on New Chal-\nlenges for NLP Frameworks, pages 45–50.\nNoam Shazeer, Ryan Doherty, Colin Evans, and Chris\nWaterson. 2016. Swivel: Improving embeddings by\nnoticing what’s missing.arXiv:1602.02215.\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011. Generating text with recurrent neu-\nral networks. In Proceedings of the 28th Inter-\nnational Conference on Machine Learning , pages\n1017–1024.\nRui Wang, Hai Zhao, Sabine Ploux, Bao-Liang Lu, and\nMasao Utiyama. 2016. A novel bilingual word em-\nbedding method for lexical translation using bilin-\ngual sense clique. arXiv:1607.08692.\nMin Xiao and Yuhong Guo. 2014. Distributed word\nrepresentation learning for cross-lingual dependency\nparsing. In Proceedings of the Eighteenth Confer-\nence on Computational Natural Language Learning,\npages 119–129.\nPing Xu and Pascale Fung. 2013. Cross-lingual lan-\nguage modeling for low-resource speech recogni-\ntion. IEEE Transactions on Audio, Speech and Lan-\nguage Processing, 21(6):1134–1144.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv:1409.2329.\nJiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and\nChengqing Zong. 2014. Bilingually-constrained\nphrase embeddings for machine translation. In Pro-\nceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 111–121.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. arXiv:1604.02201.\nWill Y . Zou, Richard Socher, Daniel Cer, and Christo-\npher D. Manning. 2013. Bilingual word embed-\ndings for phrase-based machine translation. In Pro-\nceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1393–\n1398.\n947"
}