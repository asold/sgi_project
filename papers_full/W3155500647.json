{
  "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving",
  "url": "https://openalex.org/W3155500647",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2102321989",
      "name": "Aditya Prakash",
      "affiliations": [
        "Max Planck Institute for Intelligent Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2804111224",
      "name": "Kashyap Chitta",
      "affiliations": [
        "Max Planck Institute for Intelligent Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2119899351",
      "name": "Andreas Geiger",
      "affiliations": [
        "Max Planck Institute for Intelligent Systems"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963120444",
    "https://openalex.org/W6774797786",
    "https://openalex.org/W2951517617",
    "https://openalex.org/W6781890385",
    "https://openalex.org/W2963400571",
    "https://openalex.org/W6765361892",
    "https://openalex.org/W6761555420",
    "https://openalex.org/W3114753236",
    "https://openalex.org/W3034782805",
    "https://openalex.org/W6754918364",
    "https://openalex.org/W2986701260",
    "https://openalex.org/W6750749703",
    "https://openalex.org/W6764514022",
    "https://openalex.org/W2991653934",
    "https://openalex.org/W3035360167",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6771829063",
    "https://openalex.org/W2945577597",
    "https://openalex.org/W2985871763",
    "https://openalex.org/W6769571200",
    "https://openalex.org/W2905173465",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6782015440",
    "https://openalex.org/W6745935785",
    "https://openalex.org/W3131387676",
    "https://openalex.org/W6779371956",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W6712616374",
    "https://openalex.org/W6745669638",
    "https://openalex.org/W6761988744",
    "https://openalex.org/W2559767995",
    "https://openalex.org/W6763392207",
    "https://openalex.org/W6784915420",
    "https://openalex.org/W6784685041",
    "https://openalex.org/W3003639745",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3034445502",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6679847170",
    "https://openalex.org/W6734086268",
    "https://openalex.org/W2555618208",
    "https://openalex.org/W3004129452",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2982267872",
    "https://openalex.org/W3009593063",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6779332932",
    "https://openalex.org/W6776934765",
    "https://openalex.org/W6755864109",
    "https://openalex.org/W3090789587",
    "https://openalex.org/W6772033386",
    "https://openalex.org/W2962687116",
    "https://openalex.org/W6783158279",
    "https://openalex.org/W3129347733",
    "https://openalex.org/W6755103542",
    "https://openalex.org/W2982745079",
    "https://openalex.org/W2431874326",
    "https://openalex.org/W6722836162",
    "https://openalex.org/W6779135888",
    "https://openalex.org/W3034652687",
    "https://openalex.org/W2963539305",
    "https://openalex.org/W6773248631",
    "https://openalex.org/W2949907962",
    "https://openalex.org/W3083300948",
    "https://openalex.org/W2953139137",
    "https://openalex.org/W2963858432",
    "https://openalex.org/W2996166203",
    "https://openalex.org/W3037211062",
    "https://openalex.org/W2798395692",
    "https://openalex.org/W3015571647",
    "https://openalex.org/W2970219816",
    "https://openalex.org/W2769282630",
    "https://openalex.org/W3037583715",
    "https://openalex.org/W2937814560",
    "https://openalex.org/W2799352588",
    "https://openalex.org/W3048875849",
    "https://openalex.org/W2945057173",
    "https://openalex.org/W2898900571",
    "https://openalex.org/W2943516367",
    "https://openalex.org/W3034552332",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W2896583015",
    "https://openalex.org/W2952787450",
    "https://openalex.org/W2937051932",
    "https://openalex.org/W2798930779",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2894705404"
  ],
  "abstract": "How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion.",
  "full_text": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving\nAditya Prakash*1 Kashyap Chitta*1,2 Andreas Geiger1,2\n1Max Planck Institute for Intelligent Systems, T¬®ubingen 2University of T¬®ubingen\n{firstname.lastname}@tue.mpg.de\nAbstract\nHow should representations from complementary sen-\nsors be integrated for autonomous driving? Geometry-\nbased sensor fusion has shown great promise for percep-\ntion tasks such as object detection and motion forecasting.\nHowever, for the actual driving task, the global context of\nthe 3D scene is key, e.g. a change in trafÔ¨Åc light state can\naffect the behavior of a vehicle geometrically distant from\nthat trafÔ¨Åc light. Geometry alone may therefore be insuf-\nÔ¨Åcient for effectively fusing representations in end-to-end\ndriving models. In this work, we demonstrate that imitation\nlearning policies based on existing sensor fusion methods\nunder-perform in the presence of a high density of dynamic\nagents and complex scenarios, which require global con-\ntextual reasoning, such as handling trafÔ¨Åc oncoming from\nmultiple directions at uncontrolled intersections. There-\nfore, we propose TransFuser, a novel Multi-Modal Fusion\nTransformer, to integrate image and LiDAR representations\nusing attention. We experimentally validate the efÔ¨Åcacy of\nour approach in urban settings involving complex scenarios\nusing the CARLA urban driving simulator. Our approach\nachieves state-of-the-art driving performance while reduc-\ning collisions by 76% compared to geometry-based fusion.\n1. Introduction\nImage-only [16, 8, 41, 3, 42, 64, 53] and LiDAR-\nonly [46, 23] methods have recently shown impressive re-\nsults for end-to-end driving. However, these studies focus\nprimarily on settings with limited dynamic agents and as-\nsume near-ideal behavior from other agents in the scene.\nWith the introduction of adversarial scenarios in the re-\ncent CARLA [21] versions, e.g. vehicles running red lights,\nuncontrolled 4-way intersections, or pedestrians emerg-\ning from occluded regions to cross the road at random\nlocations, image-only approaches perform unsatisfactory\n(Tab. 1) since they lack the 3D information of the scene re-\n*indicates equal contribution\nCamera View\nLiDAR View\nEgo-Vehicle\nTrafÔ¨Åc\nTrafÔ¨Åc\nTrafÔ¨Åc Lights\nTrafÔ¨Åc Lights\nSituation\nFigure 1: Illustration. Consider an intersection with on-\ncoming trafÔ¨Åc from the left. To safely navigate the intersec-\ntion, the ego-vehicle (green) must capture the global con-\ntext of the scene involving the interaction between the traf-\nÔ¨Åc light (yellow) and the vehicles (red). However, the trafÔ¨Åc\nlight state is not visible in the LiDAR point cloud and the\nvehicles are not visible in the camera view. Our TransFuser\nmodel integrates both modalities via global attention mech-\nanisms to capture the 3D context and navigate safely.\nquired in these scenarios. While LiDAR consists of 3D in-\nformation, LiDAR measurements are typically very sparse\n(in particular at distance), and additional sensors are re-\nquired to capture information missing in LiDAR scans, e.g.\ntrafÔ¨Åc light states.\nWhile most existing methods for end-to-end driving fo-\ncus on a single input modality, autonomous driving sys-\ntems typically come equipped with both cameras and Li-\nDAR sensors [21, 47, 25, 59, 17, 26, 48, 1, 62]. This\nraises important questions: Can we integrate representa-\ntions from these two modalities to exploit their comple-\nmentary advantages for autonomous driving? To what ex-\ntent should we process the different modalities indepen-\ndently and what kind of fusion mechanism should we em-\nploy for maximum performance gain? Prior works in the\narXiv:2104.09224v1  [cs.CV]  19 Apr 2021\nÔ¨Åeld of sensor fusion have mostly focused on the per-\nception aspect of driving, e.g. 2D and 3D object detec-\ntion [22, 12, 66, 9, 44, 31, 34, 61, 33, 37], motion fore-\ncasting [22, 36, 5, 35, 63, 6, 19, 38, 32, 9], and depth es-\ntimation [24, 60, 61, 33]. These methods focus on learn-\ning a state representation that captures the geometric and\nsemantic information of the 3D scene. They operate pri-\nmarily based on geometric feature projections between the\nimage space and different LiDAR projection spaces, e.g.\nBird‚Äôs Eye View (BEV) [22, 12, 66, 9, 44, 31, 34, 61, 33]\nand Range View (RV) [39, 37, 22, 38, 9, 51]. Information\nis typically aggregated from a local neighborhood around\neach feature in the projected 2D or 3D space.\nWhile these approaches fare better than image-only\nmethods, we observe that the locality assumption in their ar-\nchitecture design hampers their performance in complex ur-\nban scenarios (Tab. 1a). For example, when handling trafÔ¨Åc\nat intersections, the ego-vehicle needs to account for inter-\nactions between multiple dynamic agents and trafÔ¨Åc lights\n(Fig. 1). While deep convolutional networks can be used to\ncapture global context within a single modality, it is non-\ntrivial to extend them to multiple modalities or model in-\nteractions between pairs of features. To overcome these\nlimitations, we use the attention mechanism of transform-\ners [54] to integrate global contextual reasoning about the\n3D scene directly into the feature extraction layers of dif-\nferent modalities. We consider single-view image and Li-\nDAR inputs since they are complementary to each other and\nour focus is on integrating representations from different\ntypes of modalities. We call the resulting modelTransFuser\nand integrate it into an auto-regressive waypoint prediction\nframework (Fig. 2) designed for end-to-end driving.\nContributions: (1) We demonstrate that imitation learning\npolicies based on existing sensor fusion approaches are un-\nable to handle adversarial scenarios in urban driving, e.g.,\nunprotected turnings at intersections or pedestrians emerg-\ning from occluded regions. (2) We propose a novel Multi-\nModal Fusion Transformer (TransFuser) to incorporate the\nglobal context of the 3D scene into the feature extraction\nlayers of different modalities. (3) We experimentally vali-\ndate our approach in complex urban settings involving ad-\nversarial scenarios in CARLA and achieve state-of-the-art\nperformance. Our code and trained models are available at\nhttps://github.com/autonomousvision/transfuser.\n2. Related Work\nMulti-Modal Autonomous Driving:Recent multi-modal\nmethods for end-to-end driving [58, 65, 51, 3] have shown\nthat complementing RGB images with depth and semantics\nhas the potential to improve driving performance. Xiao et\nal. [58] explore RGBD input from the perspective of early,\nmid and late fusion of camera and depth modalities and ob-\nserve signiÔ¨Åcant gains. Behl et al. [3] and Zhou et al. [65]\ndemonstrate the effectiveness of semantics and depth as ex-\nplicit intermediate representations for driving. In this work,\nwe focus on image and LiDAR inputs since they are com-\nplementary to each other in terms of representing the scene\nand are readily available in autonomous driving systems.\nIn this respect, Sobh et al. [51] exploit a late fusion archi-\ntecture for LiDAR and image modalities where each input\nis encoded in a separate stream and then concatenated to-\ngether. However, we observe that this fusion mechanism\nsuffers from high infraction rates in complex urban scenar-\nios (Tab. 1b) due to its inability to account for the behav-\nior of multiple dynamic agents. Therefore, we propose a\nnovel Multi-Modal Fusion Transformer that is effective in\nintegrating information from different modalities at multi-\nple stages during feature encoding and hence improves upon\nthe limitations of the late fusion approach.\nSensor Fusion Methods for Object Detection and Mo-\ntion Forecasting:The majority of the sensor fusion works\nconsider perception tasks, e.g. object detection [22, 12, 66,\n7, 44, 31, 34, 61, 33, 37] and motion forecasting [36, 5,\n35, 63, 6, 19, 38]. They operate on multi-view LiDAR,\ne.g. Bird‚Äôs Eye View (BEV) and Range View (RV), or com-\nplement the camera input with depth information from Li-\nDAR by projecting LiDAR features into the image space or\nprojecting image features into the BEV or RV space. The\nclosest approach to ours is ContFuse [34] which performs\nmulti-scale dense feature fusion between image and LiDAR\nBEV features. For each pixel in the LiDAR BEV represen-\ntation, it computes the nearest neighbors in a local neigh-\nborhood in 3D space, projects these neighboring points into\nthe image space to obtain the corresponding image features,\naggregates these features using continuous convolutions,\nand combines them with the LiDAR BEV features. Other\nprojection-based fusion methods follow a similar trend and\naggregate information from a local neighborhood in 2D or\n3D space. However, the state representation learned by\nthese methods is insufÔ¨Åcient since they do not capture the\nglobal context of the 3D scene which is important for safe\nmaneuvers in adversarial scenarios. To demonstrate this,\nwe implement a multi-scale geometry-based fusion mecha-\nnism, inspired by [34, 33], involving both image-to-LiDAR\nand LiDAR-to-image feature fusion for end-to-end driving\nin CARLA and observe high infraction rates in the com-\nplex urban setting (Tab. 1b). To overcome this limitation,\nwe propose an attention-based Multi-Modal Fusion Trans-\nformer that incorporates global contextual reasoning and\nachieves superior driving performance.\nAttention for Autonomous Driving:Attention has been\nexplored in the context of driving for lane changing [13],\nobject detection [11, 32] and motion forecasting [32, 50,\n49, 28, 15, 30, 29, 56]. Chen et al. [11] employ a recur-\nrent attention mechanism over a learned semantic map for\npredicting vehicle controls. Li et al. [32] utilize attention\nto capture temporal and spatial dependencies between ac-\ntors by incorporating a transformer module into a recurrent\nneural network. SA-NMP [56] is a concurrent work that\nlearns an attention mask over features extracted from a 2D\nCNN, operating on LiDAR BEV projections and HD maps,\nto focus on dynamic agents for safe motion planning. Chen\net al. [13] utilize attention in a hierarchical deep reinforce-\nment learning framework to focus on the surrounding vehi-\ncles for lane changing in the TORCS racing simulator. They\nincorporate a spatial attention module to detect the most rel-\nevant regions in the image and a temporal attention module\nto weight different time-step image inputs, which leads to\nsmoother lane changes. However, none of these approaches\nconsiders multiple modalities or encodes the global context\nof the 3D scene which is necessary for safely navigating\nadversarial scenarios. In contrast, we demonstrate the ef-\nfectiveness of attention for feature fusion between different\nmodalities on challenging urban driving scenarios.\n3. Method\nIn this work, we propose an architecture for end-to-end\ndriving (Fig. 2) with two main components: (1) a Multi-\nModal Fusion Transformer for integrating information from\nmultiple modalities (single-view image and LiDAR), and\n(2) an auto-regressive waypoint prediction network. The\nfollowing sections detail our problem setting, input and out-\nput parameterizations, and each component of the model.\n3.1. Problem Setting\nWe consider the task of point-to-point navigation in an\nurban setting [23, 45, 46, 8, 16] where the goal is to com-\nplete a given route while safely reacting to other dynamic\nagents and following trafÔ¨Åc rules.\nImitation Learning (IL):The goal of IL is to learn a pol-\nicy œÄ that imitates the behavior of an expert œÄ‚àó. In our\nsetup, a policy is a mapping from inputs to waypoints that\nare provided to a separate low-level controller to output ac-\ntions. We consider the Behavior Cloning (BC) approach of\nIL which is a supervised learning method. An expert policy\nis Ô¨Årst rolled out in the environment to collect a dataset,\nD = {(Xi,Wi)}Z\ni=1 of size Z, which consists of high-\ndimensional observations of the environment, X, and the\ncorresponding expert trajectory, deÔ¨Åned by a set of 2D way-\npoints in BEV space, i.e., W= {wt = (xt,yt)}T\nt=1. This\nBEV space uses the coordinate frame of the ego-vehicle.\nThe policy, œÄ, is trained in a supervised manner using the\ncollected data, D, with the loss function, L.\nargmin\nœÄ\nE(X,W)‚àºD[L(W,œÄ(X))] (1)\nThe high-dimensional observation,X, includes a front cam-\nera image input and a LiDAR point cloud from a single\ntime-step. We use a single time-step input since prior\nworks on IL for autonomous driving have shown that us-\ning observation histories may not lead to performance\ngain [40, 55, 2, 57]. We use the L1 distance between the\npredicted trajectory, œÄ(X), and the expert trajectory, W, as\nthe loss function. We assume access to an inverse dynamics\nmodel [4], implemented as a PID Controller I, which per-\nforms the low-level control, i.e., steer, throttle, and brake,\nprovided the future trajectory W. The actions are deter-\nmined as a = I(W).\nGlobal Planner: We follow the standard protocol of\nCARLA 0.9.10 and assume that high-level goal locations\nGare provided as GPS coordinates. Note that these goal\nlocations are sparse and can be hundreds of meters apart as\nopposed to the local waypoints predicted by the policy œÄ.\n3.2. Input and Output Parameterization\nInput Representation:Following [45, 23], we convert the\nLiDAR point cloud into a 2-bin histogram over a 2D BEV\ngrid with a Ô¨Åxed resolution. We consider the points within\n32m in front of the ego-vehicle and 16m to each of the sides,\nthereby encompassing a BEV grid of 32m √ó32m. We di-\nvide the grid into blocks of 0.125m√ó0.125m which results\nin a resolution of 256 √ó256 pixels. For the histogram, we\ndiscretize the height dimension into 2 bins representing the\npoints on/below and above the ground plane. This results in\na two-channel pseudo-image of size 256 √ó256 pixels. For\nthe RGB input, we consider the front camera with a FOV\nof 100‚ó¶. We extract the front image at a resolution of 400\n√ó300 pixels which we crop to 256 √ó256 to remove radial\ndistortion at the edges.\nOutput Representation: We predict the future trajectory\nWof the ego-vehicle in BEV space, centered at the current\ncoordinate frame of the ego-vehicle. The trajectory is repre-\nsented by a sequence of 2D waypoints,{wt = (xt,yt)}T\nt=1.\nWe use T = 4, which is the default number of waypoints\nrequired by our inverse dynamics model.\n3.3. Multi-Modal Fusion Transformer\nOur key idea is to exploit the self-attention mechanism of\ntransformers [54] to incorporate the global context for im-\nage and LiDAR modalities given their complementary na-\nture. The transformer architecture takes as input a sequence\nconsisting of discrete tokens, each represented by a feature\nvector. The feature vector is supplemented by a positional\nencoding to incorporate positional inductive biases.\nFormally, we denote the input sequence as Fin ‚àà\nRN√óDf , where N is the number of tokens in the sequence\nand each token is represented by a feature vector of dimen-\nsionality Df. The transformer uses linear projections for\ncomputing a set of queries, keys and values (Q, K and V),\nQ = FinMq, K = FinMk, V = FinMv (2)\nTransFuser\nResNet18\nResNet34\nTransformer\n64 x 64 x 64 32 x 32 x 128 16 x 16 x 256 8 x 8 x 512\nTransformer Transformer Transformer\n64 x 64 x 64 32 x 32 x 128 16 x 16 x 256 8 x 8 x 512\nMLP GRU GRU GRU GRU\n(0, 0) w1 w2 w3\nGoal Location512\n64\nùõøw1 ùõøw2 ùõøw3 ùõøw4\nRGB \nImage\nLiDAR \nBEV\nSelf-Attention\nH H\nH H\nWW\nW W\nC\nC\nC\nC\n(2*H\n*W) x C\n(2*H*W) x C\nx L\nConv\n+\nPool\nAvgPool\n+\nFlatten\nConv\n+\nPool\nConv\n+\nPool\nConv\n+\nPool\nConv\n+\nPool\nConv\n+\nPool\nConv\n+\nPool\nConv\n+\nPool\nAvgPool\n+\nFlatten\nFigure 2: Architecture. We consider single-view RGB image and LiDAR BEV representations (Sec. 3.2) as inputs to our\nMulti-Modal Fusion Transformer (TransFuser) which uses several transformer modules for the fusion of intermediate feature\nmaps between both modalities. This fusion is applied at multiple resolutions (64√ó64, 32√ó32, 16√ó16 and 8√ó8) throughout\nthe feature extractor resulting in a 512-dimensional feature vector output from both the image and LiDAR BEV stream,\nwhich is combined via element-wise summation. This 512-dimensional feature vector constitutes a compact representation\nof the environment that encodes the global context of the 3D scene. It is then processed with an MLP before passing it to an\nauto-regressive waypoint prediction network. We use a single layer GRU followed by a linear layer which takes in the hidden\nstate and predicts the differential ego-vehicle waypoints{Œ¥wt}T\nt=1, represented in the ego-vehicle‚Äôs current coordinate frame.\nwhere Mq ‚àà RDf √óDq , Mk ‚àà RDf √óDk and Mv ‚àà\nRDf √óDv are weight matrices. It uses the scaled dot prod-\nucts between Q and K to compute the attention weights and\nthen aggregates the values for each query,\nA = softmax\n(QKT\n‚àöDk\n)\nV (3)\nFinally, the transformer uses a non-linear transformation to\ncalculate the output features, Fout which are of the same\nshape as the input features, Fin.\nFout = MLP(A) +Fin (4)\nThe transformer applies the attention mechanism multiple\ntimes throughout the architecture resulting in L attention\nlayers. Each layer in a standard transformer has multiple\nparallel attention ‚Äòheads‚Äô, which involve generating several\nQ, K and V values per Fin for Eq. (2) and concatenating\nthe resulting values of A from Eq. (3).\nUnlike the token input structures in NLP, we operate on\ngrid structured feature maps. Similar to prior works on the\napplication of transformers to images [52, 10, 43, 20], we\nconsider the intermediate feature maps of each modality to\nbe a set rather than a spatial grid and treat each element of\nthe set as a token. The convolutional feature extractors for\nthe image and LiDAR BEV inputs encode different aspects\nof the scene at different layers. Therefore, we fuse these\nfeatures at multiple scales (Fig. 2) throughout the encoder.\nLet the intermediate grid structured feature map of a sin-\ngle modality be a 3D tensor of dimension H√óW√óC. For\nSdifferent modalities, these features are stacked together to\nform a sequence of dimension (S‚àóH‚àóW) √óC. We add a\nlearnable positional embedding, which is a trainable param-\neter of dimension (S‚àóH‚àóW) √óC, so that the network can\ninfer spatial dependencies between different tokens at train\ntime. We also provide the current velocity as input by pro-\njecting the scalar value into a C dimensional vector using\na linear layer. The input sequence, positional embedding,\nand velocity embedding are combined using element-wise\nsummation to form a tensor of dimension(S‚àóH‚àóW)√óC.\nAs shown in Fig. 2, this tensor is fed as input to the trans-\nformer which produces an output of the same dimension.\nWe have omitted the positional embedding and velocity em-\nbedding inputs in Fig. 2 for clarity. The output is then re-\nshaped into Sfeature maps of dimension H√óW√óCeach\nand fed back into each of the individual modality branches\nusing an element-wise summation with the existing feature\nmaps. The mechanism described above constitutes feature\nfusion at a single scale. This fusion is applied multiple\ntimes throughout the ResNet feature extractors of the image\nand LiDAR BEV branches at different resolutions (Fig. 2).\nHowever, processing feature maps at high spatial resolu-\ntions is computationally expensive. Therefore, we down-\nsample higher resolution feature maps from the early en-\ncoder blocks using average pooling to a Ô¨Åxed resolution of\nH = W = 8 before passing them as inputs to the trans-\nformer and upsample the output to the original resolution\nusing bilinear interpolation before element-wise summation\nwith the existing feature maps.\nAfter carrying out dense feature fusion at multiple res-\nolutions (Fig. 2), we obtain a feature map of dimension\n8 √ó8 √ó512 from the feature extractors of each modality for\nan input of resolution 256 √ó256 pixels. These feature maps\nare reduced to a dimension of 1 √ó1 √ó512 by average pool-\ning and Ô¨Çattened to a 512-dimensional feature vector. The\nfeature vector of dimension 512 from both the image and\nthe LiDAR BEV streams are then combined via element-\nwise summation. This 512-dimensional feature vector con-\nstitutes a compact representation of the environment that en-\ncodes the global context of the 3D scene. This is then fed to\nthe waypoint prediction network which we describe next.\n3.4. Waypoint Prediction Network\nAs shown in Fig. 2, we pass the 512-dimensional fea-\nture vector through an MLP (comprising 2 hidden lay-\ners with 256 and 128 units) to reduce its dimensional-\nity to 64 for computational efÔ¨Åciency before passing it to\nthe auto-regressive waypoint network implemented using\nGRUs [14]. We initialize the hidden state of the GRU with\nthe 64-dimensional feature vector. The update gate of the\nGRU controls the Ô¨Çow of information encoded in the hid-\nden state to the output and the next time-step. It also takes\nin the current position and the goal location (Sec. 3.1) as\ninput, which allows the network to focus on the relevant\ncontext in the hidden state for predicting the next waypoint.\nWe provide the GPS coordinates of the goal location (reg-\nistered to the ego-vehicle coordinate frame) as input to the\nGRU rather than the encoder since it lies in the same BEV\nspace as the predicted waypoints and correlates better with\nthem compared to representing the goal location in the per-\nspective image domain [8]. Following [23], we use a single\nlayer GRU followed by a linear layer which takes in the hid-\nden state and predicts the differential ego-vehicle waypoints\n{Œ¥wt}T\nt=1 for T = 4 future time-steps in the ego-vehicle\ncurrent coordinate frame. Therefore, the predicted future\nwaypoints are given by {wt = wt‚àí1 + Œ¥wt}T\nt=1. The input\nto the Ô¨Årst GRU unit is given as (0,0) since the BEV space\nis centered at the ego-vehicle‚Äôs position.\nController: We use two PID controllers for lateral and lon-\ngitudinal control to obtain steer, throttle and brake values\nfrom the predicted waypoints, {wt}T\nt=1. The longitudinal\ncontroller takes in the magnitude of a weighted average of\nthe vectors between waypoints of consecutive time-steps\nwhereas the lateral controller takes in their orientation. For\nthe PID controllers, we use the same conÔ¨Åguration as in\nthe author-provided codebase of [8]. Implementation de-\ntails can be found in the supplementary.\n3.5. Loss Function\nFollowing [8], we train the network using an L1 loss be-\ntween the predicted waypoints and the ground truth way-\npoints (from the expert), registered to the current coordi-\nnate frame. Let wgt\nt represent the ground truth waypoint\nfor time-step t, then the loss function is given by:\nL=\nT‚àë\nt=1\n||wt ‚àíwgt\nt ||1 (5)\nNote that the ground truth waypoints{wgt\nt }which are avail-\nable only at training time are different from the sparse goal\nlocations Gprovided at both training and test time.\n4. Experiments\nIn this section, we describe our experimental setup, com-\npare the driving performanceof our approach against sev-\neral baselines, conduct an infraction analysisto study dif-\nferent failure cases, visualize the attention maps of Trans-\nFuser and present an ablation studyto highlight the impor-\ntance of different components of our model.\nTask: We consider the task of navigation along a set of\npredeÔ¨Åned routes in a variety of areas, e.g. freeways, urban\nareas, and residential districts. The routes are deÔ¨Åned by a\nsequence of sparse goal locations in GPS coordinates pro-\nvided by a global planner and the corresponding discrete\nnavigational commands, e.g. follow lane, turn left/right,\nchange lane. Our approach uses only the sparse GPS lo-\ncations to drive. Each route consists of several scenarios,\ninitialized at predeÔ¨Åned positions, which test the ability of\nthe agent to handle different kinds of adversarial situations,\ne.g. obstacle avoidance, unprotected turns at intersections,\nvehicles running red lights, and pedestrians emerging from\noccluded regions to cross the road at random locations. The\nagent needs to complete the route within a speciÔ¨Åed time\nlimit while following trafÔ¨Åc regulations and coping with\nhigh densities of dynamic agents.\nDataset: We use the CARLA [21] simulator for training\nand testing, speciÔ¨Åcally CARLA 0.9.10 which consists of\n8 publicly available towns. We use 7 towns for training\nand hold out Town05 for evaluation. For generating train-\ning data, we roll out an expert policy designed to drive us-\ning privileged information from the simulation and store\ndata at 2FPS. Please refer to the supplementary material\nfor additional details. We select Town05 for evaluation\ndue to the large diversity in drivable regions compared to\nother CARLA towns, e.g. multi-lane and single-lane roads,\nhighways and exits, bridges and underpasses. We consider\ntwo evaluation settings: (1) Town05 Short: 10 short routes\nof 100-500m comprising 3 intersections each, (2) Town05\nLong: 10 long routes of 1000-2000m comprising 10 inter-\nsections each. Each route consists of a high density of dy-\nnamic agents and adversarial scenarios which are spawned\nat predeÔ¨Åned positions along the route. Since we focus on\nhandling dynamic agents and adversarial scenarios, we de-\ncouple this aspect from generalization across weather con-\nditions and evaluate only on ClearNoon weather.\nMetrics: We report results on 3 metrics. (1) Route Com-\npletion (RC), percentage of route distance completed, (2)\nDriving Score (DS), which is route completion weighted\nby an infraction multiplier that accounts for collisions with\npedestrians, vehicles, and static elements, route deviations,\nlane infractions, running red lights, and running stop signs,\nand (3) Infraction Count. Additional details regarding the\nmetrics and infractions are provided in the supplementary.\nBaselines: We compare our TransFuser model to several\nbaselines. (1) CILRS [16] is a conditional imitation learn-\ning method in which the agent learns to predict vehicle con-\ntrols from a single front camera image while being condi-\ntioned on the navigational command. We closely follow the\nauthor-provided code and reimplement CILRS for CARLA\n0.9.10 to account for the additional navigational commands\ncompared to CARLA 0.8.4. (2) LBC [8] is a knowledge\ndistillation approach where a teacher model with access to\nground truth BEV semantic maps is Ô¨Årst trained using ex-\npert supervision to predict future waypoints followed by an\nimage-based student model which is trained using supervi-\nsion from the teacher. It is the current state-of-the-art ap-\nproach on CARLA 0.9.6. We use the latest author-provided\ncodebase for training on CARLA 0.9.10, which combines\n3 input camera views by stacking different viewpoints as\nchannels. (3) Auto-regressive IMage-based waypoint pre-\ndiction (AIM): We implement our auto-regressive waypoint\nprediction network with an image-based ResNet-34 encoder\nwhich takes just the front camera image as input. This base-\nline is equivalent to adapting the CILRS model to predict\nwaypoints conditioned on sparse goal locations rather than\nvehicle controls conditioned on navigational commands.\nThe image encoder used for this is the same as CILRS and\nour model. (4) Late Fusion: We implement a version of\nour architecture where the image and the LiDAR features\nare extracted independent of each other using the same en-\ncoders as TransFuser but without the transformers (similar\nto [51]), which are then fused through element-wise sum-\nmation and passed to the waypoint prediction network. (5)\nGeometric Fusion: We implement a multi-scale geometry-\nbased fusion method, inspired by [34, 33], involving both\nimage-to-LiDAR and LiDAR-to-image feature fusion. We\nunproject each 0.125m √ó0.125m block in our LiDAR BEV\nrepresentation into 3D space resulting in a 3D volume. We\nrandomly select 5 points from the LiDAR point cloud ly-\ning in this 3D volume and project them into the image\nspace. We aggregate the image features of these points via\nelement-wise summation before passing them to a 3-layer\nMLP. The output of the MLP is then combined with the Li-\nDAR BEV feature of the corresponding 0.125m √ó0.125m\nblock at multiple resolutions throughout the feature extrac-\ntor. Similarly, for each image pixel, we aggregate informa-\ntion from the LiDAR BEV features at multiple resolutions.\nThis baseline is equivalent to replacing the transformers in\nour architecture with projection-based feature fusion.\nWe also report results for the expert used for generating\nour training data, which deÔ¨Ånes an upper bound for the per-\nformance on each evaluation setting. We provide additional\ndetails regarding all the baselines in the supplementary.\nImplementation Details: We use 2 sensor modalities, the\nfront camera RGB image and LiDAR point cloud converted\nto BEV representation (Sec. 3.2), i.e., S = 2. The RGB im-\nage is encoded using a ResNet-34 [27] which is pre-trained\non ImageNet [18]. The LiDAR BEV representation is en-\ncoded using a ResNet-18 [27] which is trained from scratch.\nIn our default TransFuser conÔ¨Åguration, we use 1 trans-\nformer per resolution and 4 attention heads for each trans-\nformer. We select Dq,Dk,Dv from {64,128,256,512}for\nthe 4 transformers corresponding to the feature embedding\ndimension Df at each resolution. For each of our base-\nlines, we tested different perception backbone and chose the\nbest: ResNet-34 for CILRS and AIM, ResNet-50 for LBC,\nResNet-34 as the image encoder and ResNet-18 as the Li-\nDAR BEV encoder for each of the sensor fusion methods.\nAdditional details can be found in the supplementary.\n4.1. Results\nPerformance of CILRS and LBC:In our Ô¨Årst experiment,\nwe examine to what extent the current image-based meth-\nods on CARLA scale to the new 0.9.10 evaluation setting\ninvolving complex multi-lane intersections, adversarial sce-\nnarios, and heavy infraction penalties. From the results\nin Tab. 1a we observe that CILRS performs poorly on all\nevaluation settings. This is not surprising since CILRS is\nconditioned on discrete navigational commands whose data\ndistribution is imbalanced as shown in the supplementary.\nWhile the original LBC [8] architecture uses only the front\ncamera image as input, the authors recently released an up-\ndated version of their architecture with 2 major modiÔ¨Åca-\ntions, (1) multi-view camera inputs (front, 45‚ó¶left, and 45‚ó¶\nright), (2) target heatmap as input (instead of navigational\ncommand) which is formed by projecting the sparse goal lo-\ncation in the image space. We train their updated model on\nour data and observe that LBC performs signiÔ¨Åcantly better\nthan CILRS on the short routes (Tab. 1a), which is expected\nsince it is trained using supervision from the teacher model\nwhich uses ground truth BEV semantic labels. However,\nLBC‚Äôs performance drops drastically when evaluated on the\nlong routes where it achieves 32.09 RC but suffers multiple\ninfractions resulting in a low DS of 7.05. This is due to\nthe frequent red light infractions and collision with vehicles\n(Tab. 1b) resulting in large multiplicative penalties on the\nDS. These results show that CILRS and LBC are unable to\nhandle the complexities of urban driving.\nAIM is a strong baseline:Since the performance of CILRS\nand LBC drops signiÔ¨Åcantly on the long routes, we focus on\nMethod Town05 Short Town05 Long\nDS ‚Üë RC ‚Üë DS ‚Üë RC ‚Üë\nCILRS [16] 7.47 ¬±2.51 13.40 ¬±1.09 3.68 ¬±2.16 7.19 ¬±2.95\nLBC [8] 30.97 ¬±4.17 55.01 ¬±5.14 7.05 ¬±2.13 32.09 ¬±7.40\nAIM 49.00 ¬±6.83 81.07 ¬±15.59 26.50 ¬±4.82 60.66 ¬±7.66\nLate Fusion 51.56 ¬±5.24 83.66 ¬±11.04 31.30 ¬±5.53 68.05 ¬±5.39\nGeometric Fusion 54.32 ¬±4.85 86.91 ¬±10.85 25.30 ¬±4.08 69.17 ¬±11.07\nTransFuser (Ours) 54.52 ¬±4.29 78.41 ¬±3.75 33.15 ¬±4.04 56.36 ¬±7.14\nExpert 84.67 ¬±6.21 98.59 ¬±2.17 38.60 ¬±4.00 77.47 ¬±1.86\n(a) Driving Performance. We report the mean and standard deviation over 9\nruns of each method (3 training seeds, each seed evaluated 3 times) on 2 metrics:\nRoute Completion (RC) and Driving Score (DS), in Town05 Short and Town05\nLong settings comprising high densities of dynamic agents and scenarios.\n0\n4\n8\n12\nCollision \nPedestrians\nCollision \nVehicles\nCollision \nLayout\nRed Light \nViolation\nLBC AIM Late Fusion Geometric Fusion TransFuser\n(b) Infractions. We report the mean value of the\ntotal infractions incurred by each model over the\n9 evaluation runs in the Town05 Short setting.\nTable 1: Results. We compare our TransFuser model with CILRS, LBC, auto-regressive image-based waypoint predic-\ntion network (AIM), and sensor fusion methods (Late Fusion of image and LiDAR features, Geometric feature projections\nbetween image and LiDAR BEV space) in terms of driving performance (Tab. 1a) and infractions incurred (Tab. 1b).\ndesigning a strong image-based baseline next. Towards this\ngoal, we replace the learned controller of CILRS with our\nauto-regressive waypoint prediction network. We observe\nthat AIM signiÔ¨Åcantly outperforms CILRS on all evaluation\nsettings (Tab. 1a), achieving nearly 7 times better perfor-\nmance. This is likely because AIM uses our inverse dynam-\nics model (PID controller) for low-level control and repre-\nsents goal locations in the same BEV coordinate space in\nwhich waypoints are predicted. In contrast, LBC‚Äôs goal lo-\ncations are represented as heatmaps in image space. Fur-\nthermore, AIM uses an auto-regressive GRU-based way-\npoint prediction network which enables the processing of\nthese goal locations directly at the Ô¨Ånal stage of the net-\nwork. This provides a prior that simpliÔ¨Åes the learning of\nbehaviors that follow the path to the goal location which\ncould make the encoder prioritize information regarding\nhigh-level semantics of the scene, e.g. trafÔ¨Åc light state,\nrather than features relevant for low-level control. AIM out-\nperforms LBC by 58.21% on the short routes and 275.89%\non the long routes. The red light violations of LBC lead\nto a compounding of other infractions (e.g. collisions with\nvehicles), which rapidly drops its DS compared to AIM.\nSensor Fusion Methods: The goal of this experiment is\nto determine the impact of the LiDAR modality on the driv-\ning performance and compare different fusion methods. For\nthis, we compare our TransFuser to two baselines, Late\nFusion (LF) and Geometric Fusion (GF). We observe that\nLF outperforms AIM on all evaluation settings (Tab. 1a).\nThis is expected since LiDAR provides additional 3D con-\ntext which helps the agent to better navigate urban environ-\nments. Furthermore, we observe even better performance\non the short routes when replacing the independent feature\nextractors of image and LiDAR branches with multi-scale\ngeometry-based fusion encoder. However, both LF and GF\nsuffer from a sharp drop in DS compared to their RC. We\nhypothesize that this occurs because they do not incorporate\nglobal contextual reasoning which is necessary to safely\nnavigate the intersections, and focus primarily on naviga-\ntion to the goal at all costs while ignoring obstacles which\nleads to several infractions Tab. 1b. This has a compound-\ning effect on the long routes due to the exponential nature\nof the infraction penalty, resulting in a rapid drop in DS. In\ncontrast, our TransFuser model outperforms GF by 31.02%\non DS with an 18.52% lower RC on Town05 Long. It also\nachieves a 51.58% reduction compared to LF and 76.11%\nreduction compared to GF in collisions and 23.5% reduc-\ntion compared to LF and 21.93% reduction compared to GF\nin red light violations. This shows that our model drives\ncautiously and focuses on dynamic agents and trafÔ¨Åc lights.\nThis indicates that attention is effective in incorporating the\nglobal context of the 3D scene which allows for safe driv-\ning. We provide driving videos in the supplementary.\nLimitations: We observe that all fusion methods struggle\nwith red light violations (Tab. 1b). This is because detecting\nred lights is very challenging in Town05 since they are lo-\ncated on the opposite side of the intersection and are barely\nvisible in the input image. Unlike some existing meth-\nods [53], we do not use any semantic supervision for red\nlights which furthers exacerbates this issue since the learn-\ning signal for red light detection is very weak. We expect\nthe red light detection performance of the fusion approaches\nto improve when incorporating such additional supervision.\n4.2. Attention Map Visualizations\nThe transformer takes in 64 image feature tokens and\n64 LiDAR feature tokens as input where each token cor-\nresponds to a 32 √ó32 patch in the input modality. We con-\nsider 1000 frames from Town05 intersections and examine\nthe top-5 attention weights for the 24 tokens in the 2nd, 3rd\nand 4th rows of the image feature map and the 24 tokens in\nthe 4th, 5th and 6th rows of the LiDAR feature map. We\nselect these tokens since they correspond to the intersection\nFigure 3: Attention Maps. For the yellow query token,\nwe show the top-5 attended tokens in green and highlight\nthe presence of vehicles in the LiDAR point cloud in red.\nTransFuser attends to the vehicles and trafÔ¨Åc lights at inter-\nsections, albeit at a slightly different location.\nregion in the input modality and contain trafÔ¨Åc lights and\nvehicles. We observe that for 62.75% of the image tokens,\nall the top-5 attended tokens belong to the LiDAR and for\n88.87%, at least one token in the top-5 attended tokens be-\nlong to the LiDAR. Similarly, for 78.45% of the LiDAR to-\nkens, all the top-5 attended tokens belong to the image and\nfor 98.95%, at least one token in the top-5 attended tokens\nbelong to the image. This indicates that TransFuser is ef-\nfective in aggregating information from image and LiDAR.\nWe show four such frames in Fig. 3. We observe a common\ntrend in attention maps: TransFuser attends to the vehicles\nand trafÔ¨Åc lights at intersections, albeit at a slightly different\nlocation in the image and LiDAR feature maps. Additional\nvisualizations are provided in the supplementary.\n4.3. Ablation Study\nIn our default conÔ¨Åguration, we use 1 transformer per\nresolution, 8 attention layers and 4 attention heads for each\ntransformer and carry out fusion at 4 resolutions. In this ex-\nperiment, we present ablations on number of scales, atten-\ntion layers, shared or separate transformers and posi- tional\nembedding, in the Town05 Short evaluation setting.\nIs multi-scale fusion essential?We show results on scales\n1 to 4 where 1 indicates fusion at a resolution of8√ó8 in the\nlast ResNet layer, 2 indicates fusion at 8 √ó8 and 16 √ó16 in\nthe last and the penultimate ResNet layers respectively and\nsimilarly for scales 3 and 4. We observe an overall degra-\ndation in performance when reducing the number of scales\nfrom 4 to 1 (Tab. 2). This happens because different con-\nvolutional layers in ResNet learn different types of features\nregarding the input, therefore, multi-scale fusion is effective\nin integrating these features from different modalities.\nAre multiple transformers necessary?We test a version\nof our model which uses shared parameters for the trans-\nformers (Shared Transformer in Tab. 2) and observe a sig-\nniÔ¨Åcant drop in DS. This is intuitive since different convolu-\ntional layers in ResNet learn different types of features due\nParameter Value DS ‚Üë RC ‚Üë\nScale\n1 41.94 56.09\n2 52.82 74.70\n3 52.41 71.40\nShared Transformer - 55.36 77.54\nAttention layers 1 50.46 96.53\n4 51.38 79.35\nNo Pos. Embd - 52.45 93.64\nDefault ConÔ¨Åg - 59.99 74.86\nTable 2: Ablation Study. We report the DS on Town05\nShort setting for different TransFuser conÔ¨Ågurations.\nto which each transformer has to focus on fusing different\ntypes of features at each resolution.\nAre multiple attention layers required?We report results\nfor 1-layer and 4-layer variants of our TransFuser in Tab. 2.\nWe observe that while the 1-layer variant has a very high\nRC, its DS is signiÔ¨Åcantly lower. However, when we in-\ncrease the number of attention layers to 4, the model can\nsustain its DS even with an 18% lower RC. This indicates\nthat the model becomes more cautious with additional at-\ntention layers. As we further increase Lto 8 in the default\nconÔ¨Åguration, DS also increases. This shows that multiple\nattention layers lead to cautious driving agents.\nIs the positional embedding useful?Intuitively, we expect\nthe learnable positional embedding to help since modeling\nspatial dependencies between dynamic agents is crucial for\nsafe driving. This is indeed apparent in Tab. 2 where we\nobserve a signiÔ¨Åcant drop in DS in the absence of positional\nembedding even though RC increases by 25%.\n5. Conclusion\nIn this work, we demonstrate that IL policies based on\nexisting sensor fusion methods suffer from high infrac-\ntion rates in complex driving scenarios. To overcome this\nlimitation, we present a novel Multi-Modal Fusion Trans-\nformer (TransFuser) for integrating representations of dif-\nferent modalities. The TransFuser uses attention to capture\nthe global 3D scene context and focuses on dynamic agents\nand trafÔ¨Åc lights, resulting in state-of-the-art performance\non CARLA. Given that our method is Ô¨Çexible and generic,\nit would be interesting to explore it further with additional\nsensors, e.g. radar, or apply it to other embodied AI tasks.\nAcknowledgements: This work was supported by the\nBMWi in the project KI Delta Learning (project number:\n19A19013O) and the German Federal Ministry of Edu-\ncation and Research (BMBF): T ¬®ubingen AI Center, FKZ:\n01IS18039B. Andreas Geiger was supported by the ERC\nStarting Grant LEGO-3D (850533) and the DFG EXC num-\nber 2064/1 - project number 390727645. The authors thank\nthe International Max Planck Research School for Intelli-\ngent Systems (IMPRS-IS) for supporting Kashyap Chitta.\nReferences\n[1] Waymo open dataset: An autonomous driving dataset. https:\n//www.waymo.com/open, 2019.\n[2] Mayank Bansal, Alex Krizhevsky, and Abhijit S. Ogale.\nChauffeurnet: Learning to drive by imitating the best and\nsynthesizing the worst. In Proc. Robotics: Science and Sys-\ntems (RSS), 2019.\n[3] Aseem Behl, Kashyap Chitta, Aditya Prakash, Eshed Ohn-\nBar, and Andreas Geiger. Label efÔ¨Åcient visual abstractions\nfor autonomous driving. In Proc. IEEE International Conf.\non Intelligent Robots and Systems (IROS), 2020.\n[4] Richard Bellman. Adaptive Control Processes - A Guided\nTour, volume 2045. Princeton University Press, 2015.\n[5] Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urta-\nsun. Spagnn: Spatially-aware graph neural networks for re-\nlational behavior forecasting from sensor data. InProc. IEEE\nInternational Conf. on Robotics and Automation (ICRA) ,\n2020.\n[6] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:\nLearning to predict intention from raw sensor data. In Proc.\nConf. on Robot Learning (CoRL), 2018.\n[7] Can Chen, Luca Zanotti Fragonara, and Antonios Tsour-\ndos. Roifusion: 3d object detection from lidar and vision.\narXiv.org, 2009.04554, 2020.\n[8] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp\nKr¬®ahenb¬®uhl. Learning by cheating. In Proc. Conf. on Robot\nLearning (CoRL), 2019.\n[9] Ke Chen, Ryan Oldja, Nikolai Smolyanskiy, Stan Birch-\nÔ¨Åeld, Alexander Popov, David Wehr, Ibrahim Eden, and\nJoachim Pehserl. Mvlidarnet: Real-time multi-class scene\nunderstanding for autonomous driving using multiple views.\narXiv.org, 2006.05518, 2020.\n[10] Mark Chen, A. Radford, Jeff Wu, Heewoo Jun, Prafulla\nDhariwal, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In Proc. of the International Conf. on\nMachine learning (ICML), 2020.\n[11] Shi-tao Chen, Songyi Zhang, Jinghao Shang, Badong Chen,\nand Nanning Zheng. Brain inspired cognitive model with\nattention for self-driving cars. arXiv.org, 1702.05596, 2017.\n[12] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In Proc. IEEE Conf. on Computer Vision and Pat-\ntern Recognition (CVPR), 2017.\n[13] Yilun Chen, Chiyu Dong, Praveen Palanisamy, Priyan-\ntha Mudalige, Katharina Muelling, and John M. Dolan.\nAttention-based hierarchical deep reinforcement learning for\nlane change behaviors in autonomous driving. In Proc.\nIEEE International Conf. on Intelligent Robots and Systems\n(IROS), 2019.\n[14] Kyunghyun Cho, Bart van Merrienboer, C ¬∏ aglar G ¬®ulc ¬∏ehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using RNN\nencoder-decoder for statistical machine translation. In Proc.\nof the Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 2014.\n[15] Chiho Choi and Behzad Dariush. Looking to relations for\nfuture trajectory forecast. In Proc. of the IEEE International\nConf. on Computer Vision (ICCV), 2019.\n[16] Felipe Codevilla, Eder Santana, Antonio M. L ¬¥opez, and\nAdrien Gaidon. Exploring the limitations of behavior\ncloning for autonomous driving. In Proc. of the IEEE In-\nternational Conf. on Computer Vision (ICCV), 2019.\n[17] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In Proc.\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2016.\n[18] Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li\nFei-fei. Imagenet: A large-scale hierarchical image database.\nIn Proc. IEEE Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), 2009.\n[19] Nemanja Djuric, Henggang Cui, Zhaoen Su, Shangxuan\nWu, Huahua Wang, Fang-Chieh Chou, Luisa San Mar-\ntin, Song Feng, Rui Hu, Yang Xu, Alyssa Dayan, Sid-\nney Zhang, Brian C. Becker, Gregory P. Meyer, Carlos\nVallespi-Gonzalez, and Carl K. Wellington. Multixnet: Mul-\nticlass multistage multimodal motion prediction. arXiv.org,\n2006.02000, 2020.\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. arXiv.org, 2010.11929, 2020.\n[21] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Proc. Conf. on Robot Learning (CoRL), 2017.\n[22] Sudeep Fadadu, Shreyash Pandey, Darshan Hegde, Yi Shi,\nFang-Chieh Chou, Nemanja Djuric, and Carlos Vallespi-\nGonzalez. Multi-view fusion of sensor data for improved\nperception and prediction in autonomous driving. arXiv.org,\n2008.11901, 2020.\n[23] Angelos Filos, Panagiotis Tigas, Rowan McAllister,\nNicholas Rhinehart, Sergey Levine, and Yarin Gal. Can au-\ntonomous vehicles identify, recover from, and adapt to distri-\nbution shifts? In Proc. of the International Conf. on Machine\nlearning (ICML), 2020.\n[24] Chen Fu, Chiyu Dong, Christoph Mertz, and John M. Dolan.\nDepth completion via inductive fusion of planar LIDAR and\nmonocular camera. arXiv.org, 2009.01875, 2020.\n[25] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora\nVig. Virtual worlds as proxy for multi-object tracking anal-\nysis. In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2016.\n[26] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? The KITTI vision benchmark\nsuite. In Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2012.\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proc. IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR),\n2016.\n[28] Ying-Fan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and\nZhaoqi Wang. Stgat: Modeling spatial-temporal interactions\nfor human trajectory prediction. In Proc. of the IEEE Inter-\nnational Conf. on Computer Vision (ICCV), 2019.\n[29] Boris Ivanovic and Marco Pavone. The trajectron: Proba-\nbilistic multi-agent trajectory modeling with dynamic spa-\ntiotemporal graphs. In Proc. of the IEEE International Conf.\non Computer Vision (ICCV), 2019.\n[30] Vineet Kosaraju, Amir Sadeghian, Roberto Mart ¬¥ƒ±n-Mart¬¥ƒ±n,\nIan D. Reid, Hamid RezatoÔ¨Åghi, and Silvio Savarese. Social-\nbigat: Multimodal trajectory forecasting using bicycle-gan\nand graph attention networks. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2019.\n[31] Jason Ku, Melissa MoziÔ¨Åan, Jungwook Lee, Ali Harakeh,\nand Steven L. Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In Proc. IEEE In-\nternational Conf. on Intelligent Robots and Systems (IROS),\n2018.\n[32] Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng,\nMengye Ren, Sean Segal, and Raquel Urtasun. End-to-end\ncontextual perception and prediction with interaction trans-\nformer. In Proc. IEEE International Conf. on Intelligent\nRobots and Systems (IROS), 2020.\n[33] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urta-\nsun. Multi-task multi-sensor fusion for 3d object detection.\nIn Proc. IEEE Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), 2019.\n[34] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.\nDeep continuous fusion for multi-sensor 3d object detection.\nIn Proc. of the European Conf. on Computer Vision (ECCV),\n2018.\n[35] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,\nSergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-\nception and prediction with tracking in the loop. In Proc.\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2020.\n[36] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-\nous: Real time end-to-end 3d detection, tracking and motion\nforecasting with a single convolutional net. In Proc. IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR),\n2018.\n[37] Gregory P. Meyer, Jake Charland, Darshan Hegde, Ankit\nLaddha, and Carlos Vallespi-Gonzalez. Sensor fusion for\njoint 3d object detection and semantic segmentation. InProc.\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR) Workshops, 2019.\n[38] Gregory P. Meyer, Jake Charland, Shreyash Pandey, Ankit\nLaddha, Carlos Vallespi-Gonzalez, and Carl K. Wellington.\nLaserÔ¨Çow: EfÔ¨Åcient and probabilistic object detection and\nmotion forecasting. arXiv.org, 2003.05982, 2020.\n[39] Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-\nGonzalez, and Carl K. Wellington. Lasernet: An efÔ¨Åcient\nprobabilistic 3d object detector for autonomous driving. In\nProc. IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR), 2019.\n[40] Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, and Yann Le-\nCun. Off-road obstacle avoidance through end-to-end learn-\ning. In Advances in Neural Information Processing Systems\n(NeurIPS), 2005.\n[41] Eshed Ohn-Bar, Aditya Prakash, Aseem Behl, Kashyap\nChitta, and Andreas Geiger. Learning situational driving. In\nProc. IEEE Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR), 2020.\n[42] Aditya Prakash, Aseem Behl, Eshed Ohn-Bar, Kashyap\nChitta, and Andreas Geiger. Exploring data aggregation in\npolicy learning for vision-based urban autonomous driving.\nIn Proc. IEEE Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), 2020.\n[43] Di Qi, L. Su, Jia Song, E. Cui, Taroon Bharti, and Arun Sa-\ncheti. Imagebert: Cross-modal pre-training with large-scale\nweak-supervised image-text data. arXiv.org, 2001.07966,\n2020.\n[44] Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun.\nSemi-parametric image synthesis. In Proc. IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR), 2018.\n[45] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and\nSergey Levine. PRECOG: prediction conditioned on goals\nin visual multi-agent settings. In Proc. of the IEEE Interna-\ntional Conf. on Computer Vision (ICCV), 2019.\n[46] Nicholas Rhinehart, Rowan McAllister, and Sergey Levine.\nDeep imitative models for Ô¨Çexible inference, planning, and\ncontrol. In Proc. of the International Conf. on Learning Rep-\nresentations (ICLR), 2020.\n[47] Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen\nKoltun. Playing for data: Ground truth from computer\ngames. In Proc. of the European Conf. on Computer Vision\n(ECCV), 2016.\n[48] German Ros, Laura Sellart, Joanna Materzynska, David\nVazquez, and Antonio Lopez. The synthia dataset: A large\ncollection of synthetic images for semantic segmentation of\nurban scenes. In Proc. IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR), 2016.\n[49] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki\nHirose, Hamid RezatoÔ¨Åghi, and Silvio Savarese. Sophie: An\nattentive GAN for predicting paths compliant to social and\nphysical constraints. In Proc. IEEE Conf. on Computer Vi-\nsion and Pattern Recognition (CVPR), 2019.\n[50] Amir Sadeghian, Ferdinand Legros, Maxime V oisin, Ricky\nVesel, Alexandre Alahi, and Silvio Savarese. Car-net: Clair-\nvoyant attentive recurrent network. In Proc. of the European\nConf. on Computer Vision (ECCV), 2018.\n[51] Ibrahim Sobh, Loay Amin, Sherif Abdelkarim, Khaled El-\nmadawy, M. Saeed, Omar Abdeltawab, M. Gamal, and Ah-\nmad El Sallab. End-to-end multi-modal sensors fusion sys-\ntem for urban automated driving. In Advances in Neural In-\nformation Processing Systems (NeurIPS) Workshops, 2018.\n[52] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. In Proc. of the IEEE Inter-\nnational Conf. on Computer Vision (ICCV), 2019.\n[53] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.\nEnd-to-end model-free reinforcement learning for urban\ndriving using implicit affordances. In Proc. IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR), 2020.\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2017.\n[55] Dequan Wang, Coline Devin, Qi-Zhi Cai, Philipp\nKr¬®ahenb¬®uhl, and Trevor Darrell. Monocular plan view net-\nworks for autonomous driving. In Proc. IEEE International\nConf. on Intelligent Robots and Systems (IROS), 2019.\n[56] Bob Wei, Mengye Ren, Wenyuan Zeng, Ming Liang, Bin\nYang, and Raquel Urtasun. Perceive, attend, and drive:\nLearning spatial attention for safe self-driving. arXiv.org,\n2011.01153, 2020.\n[57] Chuan Wen, Jierui Lin, Trevor Darrell, Dinesh Jayaraman,\nand Yang Gao. Fighting copycat agents in behavioral cloning\nfrom observation histories. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2020.\n[58] Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu,\nand Antonio M. L¬¥opez. Multimodal end-to-end autonomous\ndriving. arXiv.org, 1906.03199, 2019.\n[59] Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End-\nto-end learning of driving models from large-scale video\ndatasets. In Proc. IEEE Conf. on Computer Vision and Pat-\ntern Recognition (CVPR), 2017.\n[60] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun\nBao, and Hongsheng Li. Depth completion from sparse lidar\ndata with depth-normal constraints. In Proc. of the IEEE\nInternational Conf. on Computer Vision (ICCV), 2019.\n[61] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Ge-\noff Pleiss, Bharath Hariharan, Mark E. Campbell, and Kil-\nian Q. Weinberger. Pseudo-lidar++: Accurate depth for 3d\nobject detection in autonomous driving. InProc. of the Inter-\nnational Conf. on Learning Representations (ICLR), 2020.\n[62] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike\nLiao, Vashisht Madhavan, and Trevor Darrell. BDD100K: A\nDiverse Driving Video Database with Scalable Annotation\nTooling. arXiv.org, 1805.04687, 2018.\n[63] Zhishuai Zhang, Jiyang Gao, Junhua Mao, Yukai Liu,\nDragomir Anguelov, and Congcong Li. Stinet: Spatio-\ntemporal-interactive network for pedestrian detection and\ntrajectory prediction. In Proc. IEEE Conf. on Computer Vi-\nsion and Pattern Recognition (CVPR), 2020.\n[64] Albert Zhao, Tong He, Yitao Liang, Haibin Huang, Guy Van\nden Broeck, and Stefano Soatto. Lates: Latent space distil-\nlation for teacher-student driving policy learning. arXiv.org,\n1912.02973, 2019.\n[65] Brady Zhou, Philipp Kr ¬®ahenb¬®uhl, and Vladlen Koltun. Does\ncomputer vision matter for action? Science Robotics, 4(30),\n2019.\n[66] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang\nGao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Va-\nsudevan. End-to-end multi-view fusion for 3d object detec-\ntion in lidar point clouds. In Proc. Conf. on Robot Learning\n(CoRL), 2019.",
  "topic": "End-to-end principle",
  "concepts": [
    {
      "name": "End-to-end principle",
      "score": 0.7097534537315369
    },
    {
      "name": "Computer science",
      "score": 0.6625020503997803
    },
    {
      "name": "Modal",
      "score": 0.5512161254882812
    },
    {
      "name": "Fusion",
      "score": 0.5065268278121948
    },
    {
      "name": "Sensor fusion",
      "score": 0.5033847689628601
    },
    {
      "name": "Computer vision",
      "score": 0.46814969182014465
    },
    {
      "name": "Transformer",
      "score": 0.454143762588501
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44661185145378113
    },
    {
      "name": "Lidar",
      "score": 0.4233604669570923
    },
    {
      "name": "Collision",
      "score": 0.42306554317474365
    },
    {
      "name": "Perception",
      "score": 0.41080567240715027
    },
    {
      "name": "Simulation",
      "score": 0.36966967582702637
    },
    {
      "name": "Real-time computing",
      "score": 0.3639669418334961
    },
    {
      "name": "Engineering",
      "score": 0.1841103732585907
    },
    {
      "name": "Voltage",
      "score": 0.10152339935302734
    },
    {
      "name": "Geography",
      "score": 0.09035584330558777
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Remote sensing",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}