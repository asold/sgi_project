{
    "title": "Towards Continual Knowledge Learning of Language Models",
    "url": "https://openalex.org/W3202099651",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221985156",
            "name": "Jang, Joel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225384518",
            "name": "Ye, Seonghyeon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225178403",
            "name": "Yang, Sohee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3179441533",
            "name": "Shin, Joongbo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2747038894",
            "name": "Han Jang-Hoon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3181925972",
            "name": "Kim, Gyeonghun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4294960858",
            "name": "Choi, Stanley Jungkyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223223007",
            "name": "Seo, Minjoon",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3047026265",
        "https://openalex.org/W3156476125",
        "https://openalex.org/W2950681488",
        "https://openalex.org/W3174266714",
        "https://openalex.org/W3120658156",
        "https://openalex.org/W2793353489",
        "https://openalex.org/W3107969673",
        "https://openalex.org/W3097784654",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W3132730484",
        "https://openalex.org/W2952862139",
        "https://openalex.org/W3039578880",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W3002800333",
        "https://openalex.org/W3177765786",
        "https://openalex.org/W3176793246",
        "https://openalex.org/W2963475460",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W3186138538",
        "https://openalex.org/W3104215796",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3098425262",
        "https://openalex.org/W2963961878",
        "https://openalex.org/W3082928416",
        "https://openalex.org/W3126553126",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3211777899",
        "https://openalex.org/W2963540014",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W2594284271",
        "https://openalex.org/W2971008823",
        "https://openalex.org/W3100283070",
        "https://openalex.org/W3213870215",
        "https://openalex.org/W3186804217",
        "https://openalex.org/W3158724005",
        "https://openalex.org/W2962724315",
        "https://openalex.org/W3144297471",
        "https://openalex.org/W2785611959",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W3213310943",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W2426267443",
        "https://openalex.org/W2612431505",
        "https://openalex.org/W3156891177",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3017701505",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W11298561",
        "https://openalex.org/W3207715300",
        "https://openalex.org/W2970172215",
        "https://openalex.org/W3169283738"
    ],
    "abstract": "Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https://github.com/joeljang/continual-knowledge-learning.",
    "full_text": "TOWARDS CONTINUAL KNOWLEDGE LEARNING OF\nLANGUAGE MODELS\nJoel Jang1 Seonghyeon Ye1 Sohee Yang1 Joongbo Shin2\nJanghoon Han2 Gyeonghun Kim2 Stanley Jungkyu Choi2 Minjoon Seo1\n1KAIST AI 2LG AI Research\n{joeljang,vano1205,sohee.yang,minjoon}@kaist.ac.kr\n{jb.shin,janghoon.han,ghkayne.kim,stanleyjk.choi}@lgresearch.ai\nABSTRACT\nLarge Language Models (LMs) are known to encode world knowledge in their pa-\nrameters as they pretrain on a vast amount of web corpus, which is often utilized\nfor performing knowledge-dependent downstream tasks such as question answer-\ning, fact-checking, and open dialogue. In real-world scenarios, the world knowl-\nedge stored in the LMs can quickly become outdated as the world changes, but\nit is non-trivial to avoid catastrophic forgetting and reliably acquire new knowl-\nedge while preserving invariant knowledge. To push the community towards bet-\nter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world knowl-\nedge, the update of outdated knowledge, and the acquisition of new knowledge.\nWe adopt applicable recent methods from literature to create several strong base-\nlines. Through extensive experiments, we ﬁnd that CKL exhibits unique chal-\nlenges that are not addressed in previous CL setups, where parameter expansion is\nnecessary to reliably retain and learn knowledge simultaneously. By highlighting\nthe critical causes of knowledge forgetting, we show that CKL is a challenging and\nimportant problem that helps us better understand and train ever-changing LMs.\nThe benchmark datasets, model checkpoints, and code to reproduce our results are\navailable at this https URL.\n1 I NTRODUCTION\nRecent works have shown that large Language Models (LM), such as T5 (Raffel et al., 2019) and\nGPT-3 (Brown et al., 2020), have the capability of storing a tremendous amount of world knowledge\nin their parameters when pretrained on a vast corpus of text (Petroni et al., 2019). These pretrained\nLMs have shown potential to serve as knowledge bases when probed for world knowledge without\nany ﬁnetuning through the LAnguage Model Analysis (LAMA) task (Petroni et al., 2019), which\nrequires probing LMs for world knowledge in a zero-shot manner through slot-ﬁlling, and promising\nresults utilizing the encoded world knowledge when ﬁnetuned on various Knowledge Intensive Lan-\nguage Tasks (KILT) (Petroni et al., 2021), e.g., question answering, knowledgeable open dialogues.\nWhile the world knowledge stored in LMs has diverse use cases, it can quickly become outdated\nas the world changes fast, and LMs need to frequently renew their internal world knowledge ac-\ncordingly. For example, it is impossible to probe for new information such as “ won the US\nElection 2020” from the original T5 (Raffel et al., 2019) which was pretrained on C4 web corpus\nfrom April 2019.1 Also, information that may have once been considered accurate may no longer\nbe valid because the information has been updated. For instance, the answer to “Which soccer team\ndoes Cristiano Ronaldo play for?” has changed from Juventus to Manchester United in September\n2021. Meanwhile, time-invariant information learned from the original corpus such as “ Barack\nObama was born in Honolulu, Hawaii” should not be altered within the LMs.\n1T5 was initially pretrained on the C4 dataset (about 750 GB), which is a cleansed dump of Common Crawl\nextracted from the web in April 2019.\n1\narXiv:2110.03215v4  [cs.CL]  24 May 2022\nLM Pretraining(continued)\nGPT-2\nT5\nRandom LMs\nLM Pretraining\nPretrained LMsContinually Pretrained LMs\nGPT-2\nT5\n GPT-2\nT5\n!! !\"\nINVARIANTLAMAUPDATEDLAMANEWLAMACKL Benchmark\nFigure 1: Overview of the CONTINUAL KNOWLEDGE LEARNING benchmark. I NVARIANT LAMA is used to\nmeasure the time-invariant world knowledge gained from D0. U PDATED LAMA is used to measure the update\nof world knowledge from D0 →D1. N EWLAMA is used to measure new world knowledge gained from D1.\nDespite its importance, the challenge of renewing the internal world knowledge stored in the pa-\nrameters of LMs is nontrivial and has only been explored in rather speciﬁc settings. For example,\nrecent works have proposed to modify speciﬁc target knowledge such as individual facts (De Cao\net al., 2021; Zhu et al., 2020; Dai et al., 2021). Dhingra et al. (2021) have addressed LMs as tempo-\nral knowledge bases by jointly modeling text with its timestamp. But the problem of renewing the\nworld knowledge of LMs in a more general and scalable way, such as through continual pretraining\non a corpus with new knowledge, has not been formally formulated or explored by previous works.\nMoreover, the community lacks a benchmark that can be used to systematically study how the inter-\nnal knowledge of LMs changes through the training on new information. Lastly, methodologies to\neffectively renew the knowledge of LMs at scale have yet to be thoroughly explored.\nIn this work, we propose a novel continual learning (CL) formulation named CONTINUAL KNOWL -\nEDGE LEARNING (CKL), where we attempt to renew the internal world knowledge of LMs through\ncontinual pretraining on new corpora. We systematically categorize world knowledge into three\nmain categories and make benchmark datasets to measure each of them during CKL: (1) I NVARI -\nANT LAMA for time-invariant world knowledge in LMs that should not be forgotten or altered,\n(2) UPDATED LAMA for outdated world knowledge that needs to be updated in the LMs, and (3)\nNEWLAMA for new world knowledge that should be injected into the LMs. We also propose a\nnovel metric named FUAR ( FORGOTTEN / ( UPDATED + ACQUIRED ) RATIO ) that can measure\nthe trade-off between forgetting, updating, and acquiring knowledge. Finally, while one might think\nof implementing contemporary CL methods for this benchmark, we show that CKL has nontriv-\nial differences to traditional CL formulations and require approaches speciﬁc to CKL. We ﬁnd and\ncompare model architectures and training methodologies (Chen et al., 2020; He et al., 2021; Hu\net al., 2021; Wang et al., 2021b) from the literature that have shown potential to mitigate forgetting\nof knowledge gained during pretraining, establishing them as baselines for the CKL benchmark.\nIn sum, while the challenge of renewing the internal world knowledge of LMs is essential in real-\nworld scenarios, it has yet to be formulated or extensively explored. Therefore, in this paper:\n• We propose a novel CL formulation called CONTINUAL KNOWLEDGE LEARNING (CKL)\nand construct a new benchmark to measure the amount of forgetting and amount of world\nknowledge gained by continued pretraining on a novel language modeling corpus that we\nconstruct, containing new knowledge.\n• We explore LM architectures and training methodologies that are natural baselines for CKL\nin literature, denoting them as CKL methods, and performing extensive experiments on\nour CKL benchmark. We categorize them into regularization, rehearsal, and parameter-\nexpansion methods, same as in traditional CL literature, and compare the effectiveness of\neach type of method using a novel metric named FUAR that we propose to measure the\ntrade-off between forgotten knowledge and updated or acquired knowledge.\n• Towards creating an ever-changing LM, we perform extensive analysis in the CKL bench-\nmark and highlight important challenges and ﬁndings: parameter-expansion methods have\nthe limitation of memory inefﬁciency despite performing the best in most of our experi-\nments and seeing the same data repeatedly during continued pretraining is a critical cause\nof forgetting. Also, we show interesting results that need further exploration: learning rate\ncan be varied to balance the forgetting and learning of new knowledge, CKL may help in\n2\nperforming previous-knowledge-intensive tasks after gaining new world knowledge, and\nCKL methods are transferable across LM architectures despite showing a different trend in\nperformance.\nAn overview of the proposed CKL benchmark is shown in Figure 1.\n2 R ELATED WORK\nLanguage Models (LMs) utilizing knowledge from external sources, such as Retrieval-Augmented\nGeneration (RAG) (Lewis et al., 2020a) and Blender Bot 2.0 (Xu et al., 2021; Komeili et al., 2021),\ncope with the changing world by updating the external sources during inference or searching the\ninternet for retrieving recent information. However, recent works have shown that these memory-\naugmented models suffer from hallucination, which means that they present false information as\nif it were correct, despite being given updated knowledge during inference (Zhang & Choi, 2021),\nwhich worsens as the size of the LM increases (Longpre et al., 2021), making it more so important\nfor implicit parameters to be renewed as well.\nIn order to renew the internal knowledge of LMs, one might consider pretraining LMs from scratch\nwith a newly updated text corpus of a scale similar to the one used during initial pretraining, such\nas a recent dump of the entire Wikipedia. However, this approach is computationally demanding\nand also environmentally harmful (Patterson et al., 2021). Another alternative approach is contin-\nuing the pretraining process on a much smaller corpus containing new world knowledge, but such\na methodology is known to suffer from catastrophic forgetting (McCloskey & Cohen, 1989; Kirk-\npatrick et al., 2017), where the models forget previously learned knowledge as they acquire new\nknowledge.\nLazaridou et al. (2021); Jin et al. (2021) suggests implementing prior Continual Learning (CL)\nmethods (Sun et al., 2020; d’Autume et al., 2019) to address this problem. However, it is impor-\ntant to note that there are nontrivial differences between traditional CL and the proposed Continual\nKnowledge Learning (CKL) formulation which make applying traditional CL methods inadequate.\nIn traditional CL, methods can be largely categorized intoregularization, rehearsal, and parameter-\nexpansion methods. (1) While regularization methods (Kirkpatrick et al., 2017) require identifying\nimportant parameters used for previous tasks, exactly how and where the knowledge is stored in\nthe parameters of an LM is currently extremely difﬁcult to identify and localize (Vig et al., 2020;\nDe Cao et al., 2021). (2) While prior rehearsal methods (Lopez-Paz & Ranzato, 2017) consider\nlearning all of the streams of tasks at once (multi-task learning) as the performance upper-bound\nand replicate such a setting with samples stored in the episodic memory, a few samples from the\npretraining corpus cannot represent the overall world knowledge from the corpus. Moreover, if LMs\nare pretrained on a shufﬂed concatenation of stream of corpora, there is no guarantee that the LMs\nwill acquire the correct, recent information from the recent corpora, especially in cases where the\nformer corpora are much bigger than the latter ones, which is shown by experiments in Section 5.1.\n(3) Lastly, prior parameter-expansion methods (Rusu et al., 2016; Yoon et al., 2018) focus onlearn-\ning a stream of different tasks via strong supervision, while in CKL, the focus isconstantly updating\nworld knowledge from a stream of corpora via self-supervision.\nBecause of these fundamental differences, instead of contemporary CL methods mentioned above,\nwe explore methodologies from the literature that are suitable for CKL (Chen et al., 2020; He et al.,\n2021; Hu et al., 2021; Wang et al., 2021b), modifying and adapting each method according to our\nneeds as CKL methods. Lastly, while it has been pointed out that some of the traditional CL formu-\nlations may have little practical importance in real-world scenarios by Prabhu et al. (2020), CKL is\nmuch closer to the initial motivation behind CL, which is that the “fundamental characteristic of nat-\nural intelligence is its ability to continually learn new knowledge while updating information about\nthe old ones” (Prabhu et al., 2020). Details of related works regarding the traditional CL methods\nand how CKL methods address the fundamental differences are provided in Appendix A.\n3 C ONTINUAL KNOWLEDGE LEARNING (CKL)\nIn this section, we explain the formulation of the task, the data construction process, and the pro-\nposed metric measuring the trade-off between forgetting previous world knowledge and updating\nand learning of new world knowledge.\n3\n3.1 T ASK FORMULATION\nWhen viewing the task of renewing the internal knowledge of LMs as one of CL formulations,\npretraining on the original corpus can be considered as a previous task, and continued pretraining\non new corpus can be considered as the current task, the main objective becoming retaining the\ntime-invariant world knowledge gained through initial pretraining while efﬁciently learning new\nand updated world knowledge through continued pretraining. Throughout the paper, we let D0\nrefer to the corpus used for initial pretraining and let D1 denote the new corpus used for continued\npretraining.\nNew Text Corpus for Language Modeling For LMs to renew their internal knowledge, they need\nto be continually pretrained on a new text corpus D1 which has the updated and new information.\nD1 should ideally be much smaller than D0, as a large D1 amounting to the size of D0 will result in\nmassive computational costs similar to pretraining the LMs from scratch. For constructing D1, we\ncrawl recently published news articles from the web making CC-R ECENT NEWS .2\nProbing LMs for World Knowledge The most widely used task for probing LMs for world\nknowledge is the LAnguage Model Analysis (LAMA) (Petroni et al., 2019) task, which consists\nof cloze sentences created from a set of knowledge sources using manually deﬁned templates. We\ndeﬁne that an LM knows a fact if it can successfully predict in a zero-shot manner the masked entity\nin the cloze sentence, such as “ Dante was born in ” as Florence. While there may be other\nalternatives for measuring the world knowledge encoded in LMs 3, we construct our main datasets\nas LAMA tasks, while also additionally providing the corresponding question pairs to the cloze\nsentences for those who want to test on CBQA as well.\nMeasuring Retention of Time-invariant World Knowledge We deﬁne time-invariant world\nknowledge as the information present in D0 that has no possibility of conﬂicting with informa-\ntion from D1. For example, if the information of the birthplace of Barack Obama is present in D0, it\nis unlikely that D1 contains information that contradicts that fact. Also, we classify instances where\nthe time-stamps are ﬁxed such as “Cristiano Ronaldo played for in 2010.” as time-invariant.\nThese time-invariant instances should not be changed as LMs are continually pretrained on D1. In\norder to measure how much time-invariant information is lost due to catastrophic forgetting dur-\ning continued pretraining, we create I NVARIANT LAMA, a subset of LAMA (Petroni et al., 2019),\nconsisting of only time-invariant cloze sentences detailed in Appendix B.1.\nMeasuring Update of Outdated World Knowledge In this work, we deﬁne outdated world\nknowledge as information that is conﬂicting between D0 and D1. For example, the President of\nthe US may be Barack Obama in D0 and Joe Biden in D1. In this case, the LM should update its\ninternal knowledge as Joe Biden as the US president. If an LM is pretrained on both D0 and D1\nsimultaneously, there is no guarantee that the LM will acquire the correct, recent information from\nD1, especially in cases where D0 is much bigger than D1, which is one of the biggest difference\nbetween the CKL and traditional CL setting. For measuring update of outdated information, we\nconstruct UPDATED LAMA which is made up of cloze statements for which answers can be found\nin both D0 and D1, but are conﬂicting.\nMeasuring Acquisition of New World Knowledge We deﬁne new world knowledge as the in-\nformation present in D1, but not in D0. To measure new knowledge acquired through continued\npretraining on D1, we construct NEWLAMA which is made up of detailed cloze statements requir-\ning new knowledge from D1 to correctly answer. We provide two datasets for measuring new world\nknowledge: N EWLAMA, for which each of the instances is veriﬁed that the answer does not exist\nin D0, but only in D1, and N EWLAMA-E ASY for which each of the instances does not perfectly\ncomply with our strict deﬁnition of new world knowledge due to its creation process, but is used to\ngenerally measure the new knowledge acquired from continued pretraining on D1 at a larger scale.\n2CC-R ECENT NEWS consists of 221,779 articles (∼168M tokens), which is estimated to be about 750 times\nsmaller than C4, a cleansed version of the April 2019 Common Crawl dataset (https://commoncrawl.org/) that\nwas used to initially pretrain the T5 LM (Raffel et al., 2019).\n3Closed-book question answering (CBQA) (Roberts et al., 2020) can also be considered as a task that mea-\nsures the world knowledge of LMs through ﬁnetuning, but it has been pointed out that much of its performance\nincreases are due to the test-train overlap (Lewis et al., 2020b; Wang et al., 2021a) in the datasets.\n4\nTable 1: Dataset statistics. Input and answer length are the corresponding average token lengths.\nDataset Size Input Length Answer Length Dataset Size Input Length Answer Length\nINVARIANT LAMA 17474 11.9 1.3 NEWLAMA 797 14.7 8.7\nUPDATED LAMA 924 13.7 9.4 NEWLAMA-E ASY 11177 44.4 6.1\nNEWLAMA-E ASY can be considered easier since each instance was constructed to be similar to\nthe data distribution seen during continued pretraining.\nDataset Construction The data for continual pretraining, CC-R ECENT NEWS , is constructed us-\ning news-please (Hamborg et al., 2017). INVARIANT LAMA is constructed by manually selecting 28\ntime-invariant relations from T-Rex (Elsahar et al., 2018). For UPDATED LAMA and N EWLAMA,\nwe use Amazon Mechanical Turk (mturk) 4 for crowd-sourcing Human Intelligent Tasks (HITs).\nThe process requires selecting answerable questions from a list of questions generated by the model\nintroduced in Lewis et al. (2021) and converting them into cloze sentences. We have also separately\nhired 11 experts to verify the correctness and search the C4 database to categorize each instance\nfollowing our deﬁnition of updated and new. N EWLAMA-E ASY is constructed at a larger scale\nthrough a two-phase mturk process where sentences selected from articles containing new informa-\ntion are decontextualized and paraphrased 5 before being masked, veriﬁed and converted to corre-\nsponding questions. The constructed dataset statistics are in Table 1. Important details about the\ndata construction pipeline, examples, and more ﬁne-grained statistics are provided in Appendix B.\n3.2 C OMBINED METRIC FOR CKL\nWe propose a novel metric, FUAR (FORGOTTEN / ( UPDATED + ACQUIRED ) RATIO ), that can\ncompare the efﬁciency of each CKL method using the trade-off between forgotten time-invariant\nknowledge and updated or newly acquired knowledge. FUAR represents relatively how many time-\ninvariant knowledge instances are forgotten in order to learnone new or updated knowledge instance.\nWe ﬁrst deﬁne FUAR for the general case where there can be multiple corpora used for training an\never-changing LM.\nLet T be an arbitrary task and (Di)n\ni=0 be a sequence of corpora used for LM pretraining, where D0\nis the initial pretraining corpus. We deﬁne Gap (T,Da,Db) =Score(T ) of LMa −Score(T ) of LMb,\nwhere LMa represents the LM after being pretrained on Da. Then, we denote TF = (T F\ni )n−1\ni=0 as a\nsequence of tasks from (Di)n−1\ni=0 measuring the forgetting of invariant-knowledge from each corre-\nsponding corpous. If there is no such task from corpus Di, the value of T F\ni is set to n.d., which\nmeans not deﬁned. Likewise, we denote TU\nn and T A\nn as tasks from Dn measuring the update and\nacquisition of new knowledge, respectively. We deﬁne FUAR as follows:\nFUAR(TF ,TU\nn ,T A\nn ) =\n\n\n\nn−1\n∑\ni=0\nmax(0,Gap(T F\ni ,Di,Dn))1 {T F\ni ̸=n.d.}\nn−1\n∑\ni=0\n{max(0,Gap(TUn ,Dn,Di))1 {T F\ni ̸=n.d.}+ max(0,Gap(T An ,Dn,Di))1 {T F\ni ̸=n.d.}}\n,\nif denominator > 0,\nno gain, otherwise.\n(1)\nThe choice of benchmark tasks TF , TU\nn , and T A\nn can differ according to each experimental setup.\nFUAR value of 1.0 represents an equal trade-off scenario where one time-invariant knowledge in-\nstance of TF is forgotten on average to gain one new or updated knowledge instance of TU\nn and\nT A\nn . The two terms in the denominators are summed because newly gained knowledge and updated\nknowledge are mutually exclusive by deﬁnition. When the value is smaller than 1, it means that the\nmodel obtains more new or updated knowledge than the amount of forgotten knowledge, so methods\n4https://www.mturk.com\n5Decontextualization model from Choi et al. (2021) and back-translation model from Tiedemann & Thot-\ntingal (2020) is used.\n5\nthat exhibit a low FUAR value can be considered suitable for CKL. If the value is zero, then it is a\ncase where no forgetting occurs at all and is the upper bound for performance. If the denominator is\n0, we denote the case as no gain and regard it as the worst possible case.6\n4 E XPERIMENTAL SETUP\nWe perform extensive experiments with an encoder-decoder model, T5 (Raffel et al., 2019), a\nlarge LM ( ∼737M params) initially pretrained on April 2019 dump of C4 and May 2020 dump\nof Wikipedia (thus D0 in our experiments) with salient span masking (SSM). The details of the pre-\ntraining, continual pretraining, and evaluation conﬁgurations are in Appendix C. We establish the\nfollowing methods as the baselines for the CKL benchmark and categorize them intoregularization,\nrehearsal, and parameter-expansion methods. The speciﬁc hyperparamters used for the implemen-\ntation of each method are detailed in Appendix D.\nInitial refers to the setting where we evaluate the LM before any continued pretraining. The perfor-\nmance of this model can be considered as theupper-bound for INVARIANT LAMA and lower-bound\non UPDATED LAMA and N EWLAMA.\nVanilla is a speciﬁc setting of further pretraining (Gururangan et al., 2020), where the domain is\nnew knowledge, and the LM is further pretrained without any training strategies.\nRecAdam (Chen et al., 2020) falls into the category of regularization methods. It places a stronger\nindependent assumption among the model parameters than the traditional regularization method\n(EWC (Kirkpatrick et al., 2017)) and does not access the initial pretraining corpus to regularize the\nmodel weights during continued pretraining. The optimizer is annealed so that less regularization is\napplied as the training progresses.\nMix-Review (He et al., 2021) falls into the category of rehearsal methods, which assumes access to\nthe initial pretraining corpus and mixes in random subsets of the initial pretraining data during con-\ntinued pretraining, depending on the mix-ratio at the current time step. As the training progresses,\nthe mix-ratio decays towards 0, decreasing the amount of the mixed original data at each iteration.\nLoRA (Hu et al., 2021) falls into the category of parameter-expansion methods. It freezes the\noriginal parameters of the LM and adds trainable rank-decomposition matrices into each layer that\nare updated during continued pretraining. Hu et al. (2021) has implemented this approach with\ndecoder-only models (GPT-2 (Radford et al., 2019) & GPT-3 (Brown et al., 2020)) while we apply\nit to an encoder-decoder model, denoting it as T5-LoRA.\nK-Adapter (Wang et al., 2021b) is another parameter-expansion method that freezes the original\nparameters of the LM while adding k number of new layers, namely adapters, that are updated\nduring continued pretraining. Wang et al. (2021b) have shown successful injection of factual and\nlinguistic knowledge for encoder-only models, BERT (Devlin et al., 2019) & RoBERTa (Liu et al.,\n2019), while we also apply it to an encoder-decoder model, T5, and decoder-only model, GPT-2.\nModular is a newly proposed parameter-expansion method speciﬁcally for encoder-decoder models\nwhich freezes the original, pretrained encoder while adding a new, randomly initialized encoder that\nis updated during continued pretraining. For the newly added encoder, we vary the size to T5-small\nwhile keeping the size of the original encoder and decoder to be T5-large.\n5 E XPERIMENTAL RESULTS\nIn this section, we ﬁrst show the main experimental results for the CKL Benchmark. Then, since\nmultiple steps of continual knowledge learning, i.e., CKL are needed for training a true, ever-\nchanging LM, we explore the effects of multiple CKL phases as well as how epochs, corpus size,\nand the total number of training steps affect CKL. We further explore how learning rates affect CKL\nin Appendix E, how continual pretraining on D1 affects the performance of KILT tasks which re-\n6Each of the last two sentences means that we do not measure positive backward transfer and negative for-\nward transfer, respectively. The latter in some cases actually do happen (shown in Appendix G). Explanations\nabout the backward and forward transfer are in Appendix A.1.\n6\nTable 2: Zero-shot probing performance on the CKL benchmark. The best results for each task and metric are\nshown in bold, and the second-best results are underlined.\nMethod # of Params\n(Trainable / Total)\nIL UL NL NLE FUAR\n((IL),UL,NL) ↓EM EM EM EM\nT5-Initial 0M / 737M 24.17 1.62 1.88 10.32 -\nT5-Vanilla 737M / 737M 12.89 10.17 3.77 17.75 1.08\nT5-RecAdam 737M / 737M 13.20 12.55 4.02 17.85 0.84\nT5-MixReview 737M / 737M 13.92 6.49 2.89 14.86 1.74\nT5-LoRA 403M / 738M 16.58 12.77 4.52 19.56 0.55\nT5-Kadapters (k=2) 427M / 762M 19.59 12.34 5.03 18.75 0.33\nT5-Kadapters (k=3) 440M / 775M 19.76 12.66 4.02 19.00 0.33\nT5-Modular 438M / 773M 20.29 12.66 4.65 19.24 0.28\nquire knowledge from D0 in Appendix F, how CKL methods transfer across LM architectures in\nAppendix G, and how the prediction outputs change during CKL in Appendix H.\n5.1 M AIN RESULTS\nTable 2 shows our main experimental result on the CKL benchmark. While only the exact match\n(EM) is reported in Table 2, we report the F1 score as well as the mean precision at k ( P@k,\nk=1,5,10,20,50,100) in Appendix J. The T5 models are originally pretrained on C4 (about 1 tril-\nlion token updates) and Wikipedia, which is considered as D0.7, and then continually pretrained\non CC-RecentNews (corpus D1) for 4 epochs (25k global training steps, about 673 million token\nupdates) using each of the CKL methods. Each of IL, UL, NL, NLE stands for INVARIANT LAMA,\nUPDATED LAMA, N EWLAMA, and N EWLAMA-E ASY , respectively. Detailed descriptions about\nthe setup for this experiment are included in the caption.\nWe ﬁrst ﬁnd that all of the CKL methods except for T5-MixReview are more effective at forgetting\nless time-invariant knowledge while updating and acquiring new knowledge than using the na¨ıve ap-\nproach of T5-Vanilla as shown by the FUAR. This result also highlights the main difference between\nCKL and CL; while rehearsal methods show strong performances in traditional CL settings (Prabhu\net al., 2020; Bang et al., 2021), in CKL, it shows the worst performance since the update of outdated\nknowledge and acquisition of new knowledge is severely deterred as shown in the performance of\nUL and NL while not showing competitive mitigation of forgetting as shown in the performance of\nIL compared to other CKL methods. Amongst the other CKL methods, we observe a rather con-\nsistent trend that the parameter-expansion methods achieve better results. The ﬁrst and second-best\nresults on all of UL, NL, and NLE are all from parameter-expansion methods. Meanwhile, although\nUL and NL are constructed following the same procedure, there is a huge difference between the\nEM scores of UL and NL. We analyze the source of this difference in Appendix I.\nFigure 9 visualizes how the EM scores of each task change as T5-Kadapters, the CKL method with\nthe most robust performance, and T5-Vanilla are continually pretrained on D1. In all of the tasks,\nthe performance of T5-Initial can be considered as the upper-bound for IL and lower-bound for UL,\nNL, NLE. Corresponding with our main observations, CKL allows considerable retention of time-\ninvariant world knowledge while improving updating and gaining new world knowledge compared\nto T5-Vanilla, mitigating the overall trade-off.\n5.2 E XPLORING MULTIPLE PHASES OF CKL\nIn order to show the potential for creating a truly ever-changing LM, we explore the effect of multiple\nCKL phases by creating CC-R ECENT NEWS -SMALL , denoted as S MALL , which is a small variant\nof CC-R ECENT NEWS that consists of randomly sampled 10% of the original corpus. We then split\n7In this work, we see C4 and Wikipedia together as D0, because we do not measure how the knowledge in\nLMs change in between training on those two corpora.\n7\n0\n5\n10\n15\n20\n25\nTitle\nT5-Initial T5-Vanilla T5-Kadapters (k=2)\n12\n16\n20\n24\n0 1 2 3 4\nEM\nEpochs\n(a) INVARIANT LAMA\n0\n4\n8\n12\n0 1 2 3 4\nEM\nEpochs (b) UPDATED LAMA\n1.5\n2.5\n3.5\n4.5\n5.5\n0 1 2 3 4\nEM\nEpochs (c) NEWLAMA\n10\n13\n16\n19\n0 1 2 3 4\nEM\nEpochs (d) NEWLAMA-E ASY\nFigure 2: Performance at each epoch during continued pretraining in the main experimental setting.\nTable 3: Zero-shot probing performance after T5 models are continually pretrained on different subsets of\nCC-R ECENT NEWS . NLE and IL stand for NewLAMA-Easy and InvariantLAMA, respectively. There are\nthree scenarios according to the corpus used for continual pretraining, explained in the text of Section 5.2. The\nFUAR of the three scenarios is calculated differently, and the corresponding tasks are shown in the table as the\nparameters of FUAR: TF , TUn , and T An . In this setting, TF consists of only a single task T F\n0 (IL) measuring the\ntime-invariant information lost from D0 only. For S MALL , we calculate the gap on NLE using the weighted\nsum of the gaps on NLEP1 and NLEP2 with uniform weights.\nCorpus Method # of Params\n(Trainable / Total)\nIL NLE P1 NLEP2\nEM EM EM\nFUAR\nT5-Initial 0M / 737M 24.17 8.69 9.45 ((IL),n.d.,NLE) ↓\nSMALL\n(SMALL -P1\n+ SMALL -P2)\nT5-Vanilla 737M / 737M 11.86 17.77 16.42 1.53\nT5-RecAdam 737M / 737M 11.85 16.46 13.93 2.01\nT5-MixReview 737M / 737M 14.36 14.18 13.93 1.97\nT5-LoRA 403M / 738M 14.26 20.60 19.90 0.87\nT5-Kadapters (k=2) 427M / 762M 18.16 18.34 16.42 0.72\nT5-Kadapters (k=3) 440M / 775M 17.12 20.98 20.39 0.61\nT5-Modular 438M / 773M 16.40 19.47 19.90 0.73\nFUAR\nT5-Initial 0M / 737M 24.17 8.69 9.45 ((IL),n.d.,NLEP1) ↓\nSMALL -P1\nT5-Vanilla 737M / 737M 9.68 20.60 11.44 1.22\nT5-RecAdam 737M / 737M 11.78 20.42 11.94 1.06\nT5-MixReview 737M / 737 M 16.13 15.88 11.94 1.12\nT5-LoRA 403M / 738M 14.75 20.79 13.93 0.78\nT5-Kadapters (k=2) 427M / 762M 19.11 20.60 10.95 0.42\nT5-Kadapters (k=3) 440M / 775M 19.08 18.15 10.94 0.54\nT5-Modular 438M / 773M 17.08 18.90 11.94 0.69\nFUAR\nT5-Initial 0M / 737M 24.17 8.69 9.45 ((IL,n.d.),n.d.,NLEP2) ↓\nSMALL -P1 →\nSMALL -P2\nT5-Vanilla 737 M / 737 M 9.40 14.37 23.38 1.06\nT5-RecAdam 737M / 737M 7.25 14.56 20.90 1.48\nT5-MixReview 737M / 737M 13.20 17.20 16.92 1.47\nT5-LoRA 404M / 740M 13.25 16.07 22.39 0.84\nT5-Kadapters (k=2) 427M / 788M 15.78 16.07 23.38 0.60\nT5-Kadapters (k=3) 440M / 813M 15.47 15.31 20.90 0.76\nT5-Modular 438M / 809M 14.66 15.31 20.40 0.87\nCC-R ECENT NEWS -SMALL into two different splits by the published date of each article to simulate\na setting where multiple CKL phases are needed, denoted as S MALL -P1 (05.2020 - 11.2020)) and\nSMALL -P2 (11.2020 - 04.2021). NLE 8 is also split into two different, smaller datasets, NLE P1\nand NLEP2, each comprising of instances constructed from articles in S MALL -P1 and S MALL -P2,\n8We use NEWLAMA-E ASY instead of NEWLAMA because the number of instances in NL corresponding\nto articles from SMALL is too small for robust evaluation.\n8\nSmall-P1\n0\n5\n10\n15\n20\n25\n0 2500 5000 10000 15000 20000 25000\nEM\nGlobal Training Steps\nT5-Initial\nMain\nSmall\nSmall-P1→Small-P2\n(a) T5-Vanilla\nSmall-P1\n0\n5\n10\n15\n20\n25\n0 2500 5000 10000 15000 20000 25000\nEM\nGlobal Training Steps\nT5-Initial\nMain\nSmall\nSmall-P1→Small-P2 (b) T5-Kadapters (k=2)\nFigure 3: Performance at each epoch on I NVARIANT LAMA during continued pretraining in M AIN , SMALL ,\nand SMALL -P1→SMALL -P2 scenarios. Each marker indicates the result at each continual pretraining epoch.\nrespectively. We compare how CKL methods for T5 perform on IL, NLE P1, and NLE P2 when\ncontinually pretrained entirely on SMALL for 5k steps (8 epochs), and when sequentially pretrained\non S MALL -P1 and then on S MALL -P2 for 2.5k steps (8 epochs) each. In the scenario S MALL -\nP1→SMALL -P2, there are two CKL phases where D0 is C4 and Wikipedia, D1 is SMALL -P1, and\nD2 is SMALL -P2. The rest of the conﬁgurations are set identical with the main experiments.\nComparing the performance on IL of the two scenarios, S MALL and SMALL -P1→SMALL -P2, re-\nsults show that LMs are prone to more forgetting as they go through multiple CKL phases, despite\nhaving the same number of training steps. One of the reasons may be due to the learning rate\nscheduling, which is initialized at the start of each phase.\nFurthermore, despite showing the best performance overall, the drawbacks of parameter-expansion\nmethods are also highlighted in the SMALL -P1→SMALL -P2 setting; they require new parameters to\nbe added at every phase of the update. For example, the number of total parameters of T5-Modular\nincreases by 36M in every round of the continual pretraining phase. Likewise, considering a large\nnumber of CKL phases introduces new problems that should be additionally studied. Taking into\naccount that LMs should be updated frequently with a small amount of data in real-world scenarios\nfor gaining up-to-date world knowledge about the ever-changing world in a computation-effective\nmanner, more research is needed to mitigate the amount of forgetting that follows the larger number\nof update phases.\nEffects of Epochs, Corpus Size, and Total Number of Training Steps in CKL on Forgetting\nFigure 3 shows the result of T5-Vanilla and T5-Kadapters during continued pretraining in different\nscenarios from Table 2 and 3, where each point in the graph represents the performance of IL after\nevery epoch. Comparing MAIN (4 epochs) and SMALL (8 epochs) in Figure 3 (a) T5-Vanilla, we can\nsee that more forgetting occurs in S MALL , even though trained for ﬁve times less number of global\ntraining steps. This phenomenon is further highlighted when comparing results from S MALL -P1 (8\nepochs) which shows the most amount of forgetting despite being trained for ten times less number\nof global training steps. While the overall drop is much mitigated in Figure 3 (b) T5-Kadapters, we\nobserve the same trend between each scenario which goes to show how critical observing the same\ndata repeatedly during continued pretraining is for causing forgetting.\nThe results are in line with ﬁndings from Lee et al. (2021) which suggest LMs should be pretrained\nwith just a few epochs on less duplicating data for efﬁciency. We add additional intuition to their\nﬁndings and conjecture that the inefﬁciency of pretraining from duplicate data could have been\ncaused by the forgetting of the rather long-tail knowledge in the pretraining corpus.\n6 C ONCLUSION\nIn this paper, we propose C ONTINUAL KNOWLEDGE LEARNING (CKL), where we establish\nbenchmark datasets and metrics, and explore methodologies towards continual knowledge learning\nof an ever-changing LM. We ﬁnd that parameter-expansion methods show the most robust perfor-\nmance throughout all of the experimental settings, which nevertheless has severe memory inefﬁ-\nciency and that seeing the same data often is a critical cause of forgetting. We also discuss several\nother interesting results of which we leave further exploration to future studies. To this end, we\nsuggest the community to explore CKL for the better design of an ever-changing LM.\n9\nACKNOWLEDGMENTS\nThe authors would like to thank Sang-Woo Lee, Jinheon Baek, Miyoung Ko, Hyunji Lee, and Eunbi\nChoi for helpful discussions. This work was supported by Institute of Information & communica-\ntions Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.\n2019-0-00075, Artiﬁcial Intelligence Graduate School Program (KAIST)).\nREFERENCES\nJihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory:\nContinual learning with a memory of diverse samples. In CVPR, 2021.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In NeurIPS, 2020.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and\nlearn: Fine-tuning deep pretrained language models with less forgetting. In EMNLP, 2020.\nEunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael\nCollins. Decontextualization: Making sentences stand-alone. TACL, 9:447–461, 2021.\nDamai Dai, Li Dong, Y . Hao, Zhifang Sui, and Furu Wei. Knowledge neurons in pretrained trans-\nformers. ArXiv, abs/2104.08696, 2021.\nCyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic\nmemory in lifelong language learning. In NeurIPS, 2019.\nNicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In\nEMNLP, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. Time-aware language models as temporal knowledge bases. arXiv preprint\narXiv:2106.15110, 2021.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard\nof wikipedia: Knowledge-powered conversational agents. In ICLR, 2019.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Elena Sim-\nperl, and Frederique Laforest. T-rex: A large scale alignment of natural language with knowledge\nbase triples. In LREC, 2018.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:\nLong form question answering. In ACL, 2019.\nZhaochen Guo and Denilson Barbosa. Robust named entity disambiguation with random walks.\nSemantic Web, 9(4):459–479, 2018.\nSuchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. Don’t stop pretraining: adapt language models to domains and tasks. InACL,\n2020.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. In ICML, 2020.\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic\nnews crawler and extractor. In 15th International Symposium of Information Science (ISI 2017) ,\npp. 218–223, 2017.\n10\nTianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and Fuchun Peng. An-\nalyzing the forgetting problem in pretrain-ﬁnetuning of open-domain dialogue response models.\nIn EACL, 2021.\nJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F ¨urstenau, Manfred Pinkal, Marc\nSpaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust disambiguation of named\nentities in text. In EMNLP, 2011.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nXisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and\nXiang Ren. Lifelong pretraining: Continually adapting language models to emerging corpora.\narXiv preprint arXiv:2110.08534, 2021.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In ACL, 2017.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-\ning catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,\n114(13):3521–3526, 2017.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. arXiv\npreprint arXiv:2107.07566, 2021.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. TACL, 7:453–466, 2019.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tay-\nfun Terzi, Mai Gimenez, Cyprien de Masson d’Autume, Sebastian Ruder, Dani Yogatama, et al.\nPitfalls of static language modelling. arXiv preprint arXiv:2102.01951, 2021.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-\nBurch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv\npreprint arXiv:2107.06499, 2021.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via\nreading comprehension. In CoNLL, 2017.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented gener-\nation for knowledge-intensive nlp tasks. In NeurIPS, 2020a.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel. Question and answer test-train overlap in\nopen-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020b.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich K¨uttler, Aleksandra Piktus,\nPontus Stenetorp, and Sebastian Riedel. Paq: 65 million probably-asked questions and what you\ncan do with them. In EACL, 2021.\nYanyang Li, Ye Lin, Tong Xiao, and Jingbo Zhu. An efﬁcient transformer decoder with compressed\nsub-layers. arXiv preprint arXiv:2101.00542, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.\nEntity-based knowledge conﬂicts in question answering. arXiv preprint arXiv:2109.05052, 2021.\n11\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In\nNeurIPS, 2017.\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. Psychology of learning and motivation, 24:109–165, 1989.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training.arXiv\npreprint arXiv:2104.10350, 2021.\nFabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? In EMNLP, 2019.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowl-\nedge intensive language tasks. In NAACL, 2021.\nNina Poerner, Ulli Waltinger, and Hinrich Sch¨utze. E-bert: Efﬁcient-yet-effective entity embeddings\nfor bert. In Findings of EMNLP, 2019.\nAmeya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions\nour progress in continual learning. In ECCV, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? In EMNLP, 2020.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint\narXiv:1606.04671, 2016.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. AutoPrompt:\nEliciting knowledge from language models with automatically generated prompts. In EMNLP,\n2020.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. Lamol: Language modeling for lifelong language\nlearning. In ICLR, 2020.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale\ndataset for fact extraction and veriﬁcation. In NAACL, 2018.\nJ¨org Tiedemann and Santhosh Thottingal. OPUS-MT — Building open translation services for the\nWorld. In EAMT, Lisbon, Portugal, 2020.\nPat Verga, Haitian Sun, Livio Baldini Soares, and William W Cohen. Facts as experts: Adaptable\nand interpretable neural memory over symbolic knowledge. In NAACL, 2021.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason\nHuang, Yaron Singer, and Stuart Shieber. Causal mediation analysis for interpreting neural nlp:\nThe case of gender bias. In NeurIPS, 2020.\nCunxiang Wang, Pai Liu, and Yue Zhang. Can generative pre-trained language models serve as\nknowledge bases for closed-book qa? In ACL, 2021a.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang,\nMing Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. In\nFindings of ACL, 2021b.\n12\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural\nlanguage processing. In EMNLP System Demonstrations, 2020.\nJing Xu, Arthur Szlam, and Jason Weston. Beyond goldﬁsh memory: Long-term open-domain\nconversation. arXiv preprint arXiv:2107.07567, 2021.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. In EMNLP, 2018.\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically\nexpandable networks. In ICLR, 2018.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. Defending against neural fake news. In NeurIPS, 2019.\nMichael J.Q. Zhang and Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA.\nEMNLP, 2021.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and\nSanjiv Kumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363,\n2020.\n13\nA E XTENSION OF RELATED WORKS\nAs mentioned in Section 2, there are fundamental differences between the traditional CL formu-\nlations and CKL which make the previous CL methods inadequate for the CKL setting. In this\nsection, we introduce the prior traditional continual learning methods in detail, explore the methods\nfrom the literature set as baselines for the CKL benchmark and how they address the identiﬁed lim-\nitations of CL methods, and provide descriptions about alternative methods making LMs cope with\nthe changing world.\nA.1 T RADITIONAL CONTINUAL LEARNING\nTraditional continual learning (CL) methods focus on addressing two aspects of transfer between\nsequentially incoming tasks: forward transfer and backward transfer (Lopez-Paz & Ranzato, 2017).\nForward transfer refers to how past tasks affect the performance of the current and future tasks.\nBackward transfer refers to how current or future tasks affect the performance of previous tasks.\nThe general pretrain-ﬁnetune approach can be seen as an instance ofpositive forward transferwhere\na model performs better on a target task after being pretrained on a more general source task. More-\nover, catastrophic forgetting can be seen as an instance ofnegative backward transferwhere previous\ntasks suffer performance due to continued training on different tasks. With respect to these two as-\npects, CL approaches can be categorized into three main approaches: regularization, rehearsal, and\nparameter-expansion methods.\nRegularization Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) is a method that\nregularizes important parameters of previous tasks while training for the current tasks, helping mit-\nigate the negative backward transfer of previous tasks. Important parameters are measured via a\nFisher information matrix computed by measuring the magnitude of the gradient update step of each\nparameter during training of previous tasks.\nRehearsal Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) is one of the ﬁrst\nrehearsal methods that utilize samples from each task stored in episodic memory and places an\ninequality constraint with respect to the losses of the samples in order to prevent negative backward\ntransfer as well as allow the positive backward transfer. Other methods such as Experience replay\nand local adaptation (d’Autume et al., 2019) replay samples stored in the memory of previous tasks\nduring training to mitigate forgetting.\nParameter-expansion Progressive Neural Networks (PNN) (Rusu et al., 2016) is one of the ear-\nliest parameter-expansion/sharing approaches that introduce new sets of parameters for each new\ntask where previous parameters are frozen and can be connected via lateral connections allowing for\npositive forward transfer. PNN not only prevents negative backward transfer but also surpassed the\nprevious pretrain-ﬁnetune approach in terms of positive forward transfer in some tasks.\nA.2 CKL M ETHODS FOR LANGUAGE MODELS\nAs mentioned in Section 2, we explore the methods from the literature that have addressed the\nlimitations of CL methods and thus are applicable to CKL. We also categorize these methods into\nthe three main categories of CL.\nRegularization Most CL methods that utilize regularization require computing important param-\neters of the previous task, which in this case is pretraining on the original text corpus. Determining\nthese parameters is oftentimes unrealistic since it requires large-scale pretraining which can hardly\nbe replicated by most. Also, exactly how and where the knowledge is stored in the parameters of an\nLM is currently extremely difﬁcult to identify and localize (Vig et al., 2020; De Cao et al., 2021).\nRecAdam (Chen et al., 2020) overcomes this limitation by following the same training objective\nas EWC (Kirkpatrick et al., 2017) with a stronger independent assumption and places a quadratic\npenalty, ridding the need to access the initial pretraining corpus.\n14\nRehearsal Large LMs are usually pretrained on a vast amount of raw text corpus such as Common\nCrawl9. When treating pretraining as a CL task, limitations exist when trying to apply previous\nrehearsal methods since a few samples from the pretraining corpus cannot represent the overall\nworld knowledge from the original pretraining corpus. Mix-Review (He et al., 2021) solves this issue\nby performing preliminary experiments in a smaller pretraining setting by assuming access to the\npretraining corpus during ﬁnetuning and mixing random subsets of pretraining corpus depending on\na mix-ratio that anneals towards the target task as training progresses. Mix-Review can be considered\na mild version of multi-task learning.\nParameter-expansion K-Adapter (Wang et al., 2021b) shares and freezes the original parame-\nters and adds new parameters through adapters for continued pretraining of factual and linguis-\ntic knowledge and improve performance on three different knowledge-driven downstream tasks.\nMore recently, LoRA (Hu et al., 2021) freezes the original parameters and injects trainable rank-\ndecomposition matrices into each layer of the Transformer architecture, greatly reducing the num-\nber of trainable parameters and the computational hardware requirement while performing on-par or\nbetter than training all of the parameters. Both methods hypothesize freezing the original parameters\nallows mitigation of catastrophic forgetting. We test out the hypothesis through implementation in\nour CKL benchmark.\nA.3 M ETHODS OF INTEGRATING WORLD KNOWLEDGE WITH LANGUAGE MODELS\nExplicit Methods Facts-as-Experts (Verga et al., 2021) store representations of entities in the form\nof key-value pairs into external memory that can be modiﬁed during inference time. RAG (Lewis\net al., 2020a) accesses a dense vector index of Wikipedia with a retriever and swaps indexes for\nupdating the behavior of the model as the world changes. Blender Bot 2.0 (Xu et al., 2021; Komeili\net al., 2021), is also one of the explicit methods that search the internet for recent knowledge and\nsaves recent conversations in external long-term memory. Explicit methods, such as swapping in-\ndexes, adding explicit entity-relation knowledge, or searching the internet are in need of manual\nintervention during inference or are bound to tasks that require retrieval. In this paper, we focus\nonly on implicit methods.\nImplicit Methods Zhu et al. (2020) proposed a new task of explicitly modifying speciﬁc facts\nwithout forgetting unmodiﬁed facts and provided several benchmark approaches without utilizing\nnon-parametric memory, including constrained layer-wise ﬁnetuning. Wang et al. (2021b) proposed\nK-Adapter, a method that adds adapters to frozen layers of pretrained LMs to inject factual and\nlinguistic knowledge and improve performance on downstream tasks. Chen et al. (2020) proposed a\nnew optimizer that simulates the pretraining optimization while ﬁnetuning on the target task without\nneeding access to the pretraining corpus, improving performance on the GLUE benchmark. De Cao\net al. (2021) propose using a hyper-network to edit factual knowledge.\nEven though these implicit methods are efﬁcient methods of injecting or modifying knowledge from\nthe implicit parameters of the LMs, they are all limited to injecting speciﬁc knowledge such as the\ncase of (Wang et al., 2021b) or modifying past knowledge such as the case of (Zhu et al., 2020;\nDe Cao et al., 2021). No work, to the best of our knowledge, has speciﬁcally addressed the catas-\ntrophic forgettingof world knowledge gained from the initial pretraining when continued pretraining\non new text corpus for the gain of new world knowledge.\nB D ATASET CONSTRUCTION\nIn this section, we describe the dataset construction process we undergo in creating the benchmark\ndatasets used in CKL. For the construction, we use Amazon Mechanical Turk (mturk)10 for crowd-\nsourcing Human Intelligent Tasks (HITs) and separately hire 11 experts for annotation that requires\nextensive searching of the C4 corpus. In addition, three more experts 11 who set up the data con-\nstruction process and prepared the annotation guideline to ensure the quality of the data through\npost-validation and giving feedback to the annotators in real-time. The interfaces used for mturk\nHITs are provided in Appendix B.2.\n9https://commoncrawl.org/\n10https://www.mturk.com\n11The ﬁrst three authors of the paper.\n15\nPAQ Question Generator\nIs the \nprediction correct in C4 ?\nSelect a question that requires  recent knowledge to answer AND answer can be found in the article\nquestion\nyes\nold_answer = prediction\nConvert to cloze sentence\nprobably-asked questions\ncloze sentence\nnew_answer = answer\nprediction\nPretrained T5 LM\nno\nevidence = evidence in C4 \nadd to UpdatedLAMA\nCan an alternative answer \nbe found in C4 ?\nno\nyes\nold_answer = alternative \nevidence = evidence in C4 \nadd to UpdatedLAMA\nadd to NewLAMA\nsentence\nDecontextualization Model\nDoes the news article \nhave new information ?\nyes\nno disregard\nanswer = selected_word\nBack-Translation Model\ndecontextualized & paraphrased sentence\nIs the answer still in \nparaphrased sentence ?\nno\ndisregard\nyes\nConvert to question and answer the question\nadd to NewLAMA-Easy \n(a) (b)\ndisregard\nprediction != \nnew_answer\nalternative_answer \n!= new_answer\nno\ndisregard\nno\nyes\nyes\nHIT\nHIT\nHIT\nIn-Lab Experts\nDo the \nanswers agree?\nno disregard\nrecent news article from CC-\nRecentNews\nrecent news article from  \nCC-RecentNews\nprobably-asked questions\nIs the prediction  \ncorrect in C4? \nCan an alternative \nanswer be found in C4?\nprediction != \nnew_answer\nalternative answer \n!= new_answer\nrecent news article from CC-\nRecentNews\nrecent news article from  \nCC-RecentNews\nDoes the news article \nhave new information?\nChoose a sentence that conveys the most recent information \nfrom the article. Then, select an entity from the sentence.\nIs the answer still in  \nparaphrased sentence ?\nDo the  \nanswers agree?\nFigure 4: Dataset construction pipeline for (a) UPDATED LAMA, N EWLAMA, and (b) N EWLAMA-E ASY\nCC-R ECENT NEWS We ﬁrst construct CC-R ECENT NEWS , a novel text corpus containing rel-\natively new knowledge as D1. We use news-please (Hamborg et al., 2017), similar to the CC-\nNEWS (Liu et al., 2019) and REALNEWS dataset (Zellers et al., 2019), to crawl 221,779 news\narticles published from May 2020 to April 2021. LMs initially pretrained on D0 constructed be-\nfore May 2020 can be continually pretrained on CC-R ECENT NEWS to gain relatively recent world\nknowledge.\nINVARIANT LAMA We create INVARIANT LAMA, a subset of the LAMA (Petroni et al., 2019)\ntask for measuring time-invariant knowledge which might be forgotten during CKL. Among the\n41 relations of the T-REx (Elsahar et al., 2018) subset of LAMA, we manually select 28 relation\ntypes that probe for time-invariant instances (a full list of time-invariant relations are provided in\nAppendix B.1). We also remove instances where the answer overlapped with the subject following\nPoerner et al. (2019) since the answers for these instances can be inferred from the cloze statement\nitself. Lastly, we remove instances where the answer was a non-entity to leave only the instances\nthat require world knowledge for prediction on their answers (Guu et al., 2020).\nUPDATED LAMA and N EWLAMA We construct UPDATED LAMA and N EWLAMA for mea-\nsuring the update of outdated knowledge and acquisition of new knowledge during CKL. The chal-\nlenge of constructing U PDATED LAMA is that a knowledge instance can be only considered as the\nknowledge that requires update only if it is present in both D0 and D1 with changed details, and\nthe challenge of constructing NEWLAMA is that the knowledge can be considered new only if it is\nin D1 but not in D0. Therefore we set up the data construction process carefully. The pipeline for\nthe creation of a single instance of U PDATED LAMA and N EWLAMA, is shown in Figure 4 (a).\nEach potential instance starts off from a single article from CC-R ECENT NEWS and goes through\nthe pipeline which will end up being (1) discarded (2) added to U PDATED LAMA or (3) added to\nNEWLAMA in the end. The procedure is as follows:\n(1) First, a list of Probably-Asked Questions (Lewis et al., 2021) are generated using the PAQ ques-\ntion generator on a single news article from CC-R ECENT NEWS . (2) The list of PAQs and the news\narticle is given to the crowd-sourced worker to select a question that asks for the mostrecent knowl-\nedge for which the answer (denoted asnew answer) can be found in the article. (3) The crowd-source\nworker is instructed to convert the question into a cloze sentence so that it can be given as input to\na pretrained T5 LM. The predictions of the T5 LM are stored along with the questions and cloze\nsentences. (4) The expert annotator ensures the quality of the questions and cloze sentences by cor-\n16\nrecting them whenever necessary and checks whether the model prediction is correct by searching\nthrough the C4 corpus as a representative of D012. If the prediction is correct and the prediction is\nnot the same with the new answer, the following instance must be present in both D0 and D1 with\ndetails changed, and thus is added to UPDATED LAMA along with the evidence document found in\nC4. If same, the instance is discarded because the instance is neither updated nor new. (5) Lastly,\nif the model prediction is wrong, the expert annotator is asked to ﬁnd an alternative answer for the\nquestion in C4. If not found, the instance is added to N EWLAMA since the answer to the question\ncould only be found in the article of CC-R ECENT NEWS (D1), but not in C4 (D0). Similarly, if the\nalternative answer is found in C4, we check whether it is the same as the new answer and add the\ninstance to UPDATED LAMA if not the same and disregard it otherwise.\nThroughout the whole process, a validator checks the sanity of the data and gives detailed real-time\nfeedback on the work of the annotator.\nNEWLAMA-E ASY Even though NEWLAMA corresponds to our exact deﬁnition of new knowl-\nedge that we deﬁne in the task formulation, scaling the size of the dataset was difﬁcult since each\ninstance required searching the whole C4 database for answers. Instead, we provide a much larger,\neasier variant NEWLAMA-E ASY where we test the general new knowledge acquired during con-\ntinued pretraining on CC-R ECENT NEWS . The pipeline for the creation of a single instance of\nNEWLAMA-E ASY is shown in Figure 4 (b) and follows the following procedures:\n(1) First, the crowd-sourced worker is instructed to classify whether the given article contains new\ninformation or not. (We deﬁne new as not likely to be known before May 2020). If the article con-\ntains new information, the worker is instructed to select a sentence from the article that contains the\nmost recent information and an entity among the possible answer candidates in the sentence and dis-\ncard the article if otherwise. We provide the possible entities through a Named-Entity Recognition\nModel. (2) We make the selected sentence stand-alone from the article through the decontextual-\nization model provided by Choi et al. (2021). (3) The decontextualized sentence is paraphrased by\na back-translation model (en →de→en) (Tiedemann & Thottingal, 2020) and checked whether the\nselected word is still in the paraphrased sentence; the sentence is discarded if not. (4) Next, we mask\nout the selected word from the sentence and ask two crowd-sourced workers to convert the cloze\nsentence into a question and answer the question. (5) If the answers agree among the workers as\nwell as correspond to the actual selected word, we add the instance to NEWLAMA-E ASY .\nThe speciﬁc interfaces used for the mturk HITs are provided in Appendix B.2. Statistics of the\nconstructed datasets are in Appendix B.3.\nB.1 T IME -INVARIANT RELATIONS OF LAMA\nTable 4 shows the list of 28 time-invariant relations of I NVARIANT LAMA. We manually ﬁlter the\n44 original LAMA relations to leave only the time-invariant relations. Templates such as “[X] works\nfor [Y] .” and “[X] is a member of [Y] .” are excluded because the answer may change for different\ntimestamps. In the template, [X] and [Y] refers to subject and object labels, respectively. Given a\ntemplate with only the subject included, the model has to predict the object label [Y] for knowledge\nprobing.\nB.2 I NTERFACES USED FOR THE CONSTRUCTION OF CKL BENCHMARK\nThe Mturk interface used during construction of UPDATED LAMA and N EWLAMA, N EWLAMA-\nEASY , and NEWLAMA-E ASY are shown in Figure 5, 6, and 7, respectively.\nB.3 D ATASET STATISTICS AND EXAMPLES\nWe report the data statistics for the CKL benchmark in Table 5. We measure the size, average input\ntoken length, average answer token length, and the answer types of each constructed dataset. One\nthing to consider is that LAMA (Petroni et al., 2019) from which we constructed INVARIANT LAMA\nis originally constructed for only single-token decoding (1.3 with the T5-tokenizer) because multi-\ntoken decoding entails additional, tunable parameters (beam size, n-gram repetition penalties, etc.).\n12The expert annotators are instructed to use https://c4-search.apps.allenai.org/ for searching through the C4\ncorpus.\n17\nTable 4: Relations of INVARIANT LAMA\nRelation Template ([X], [Y]) Example\nP19 [X] was born in [Y] . Taras Kuzio was born in Halifax .\nP20 [X] died in [Y] . Georgios Roilos died in Athens.\nP279 [X] is a subclass of [Y]. Hutterite German is a subclass of Bavarian .\nP37 The ofﬁcial language of [X] is [Y]. The ofﬁcial language of Azad Kashmir is English .\nP449 [X] was originally aired on [Y] . Microsoap was originally aired on BBC.\nP47 [X] shares border with [Y] . Illinois shares border with Kentucky .\nP138 [X] is named after [Y] . Logan International Airport is named after Boston .\nP364 The original language of [X] is [Y] . The original language of The Fatal Eggs is Russian .\nP527 [X] consists of [Y] . AIM alliance consists of Apple .\nP176 [X] is produced by [Y] . Alfa Romeo 155 is produced by Fiat .\nP27 [X] is [Y] citizen . Woodrow Lloyd is Canada citizen .\nP407 [X] was written in [Y] . France Culture was written in French .\nP30 [X] is located in [Y] . Lavoisier Island is located in Antarctica .\nP178 [X] is developed by [Y]. Tizen is developed by Intel .\nP1376 [X] is the capital of [Y], London is the capital of England .\nP131 [X] is located in [Y] . Pershing County is located in Nevada .\nP1412 [X] used to communicate in [Y]. Jacques Rivette used to communicate in French .\nP17 [X] is located in [Y] . Eibenstock is located in Germany .\nP276 [X] is located in [Y] . Delhi Technological University is located in India .\nP937 [X] used to work in [Y]. Pierre Trudeau used to work in Ottawa .\nP140 [X] is afﬁliated with the [Y] religion . Emirate of Granada is afﬁliated with the Islam religion .\nP103 The native language of [X] is [Y] . The native language of Anastasy V onsyatsky is Russian .\nP190 [X] and [Y] are twin cities . Beijing and Milan are twin cities .\nP1001 [X] is a legal term in [Y] . Surgeon General is a legal term in Canada .\nP495 [X] was created in [Y] . La Grande Vadrouille was created in France .\nP36 The capital of [X] is [Y] . The capital of Granville County is Oxford .\nP740 [X] was founded in [Y]. Grimaldi Group was founded in Naples .\nP361 [X] is part of [Y] . Sinqa is part of Andes .\nTable 5: CKL benchmark dataset statistics\nDataset Size Avg. Input Avg. Answer Answer TypesToken # Token #\nINVARIANT LAMA 17474 11.9 1.3 Geographical (54%), Language (14.9%), Nationalities (7.2%)\nPerson (6.3%), Location (5.7%), Organization (5.3%), etc. (6.6%)\nUPDATED LAMA 924 13.7 9.4 Person (61.47%), Organization (8.3%), Geographical (6.6%),\nNumerals (5.19%), Date (2.4%), etc. (16.04%)\nNEWLAMA 797 14.7 8.7 Person (59.7%), Organization (10.2%), Numerals (7.6%)\nDate (5.3%), Geographical (4.8%), etc. (12.4%)\nNEWLAMA-\nEASY 11177 44.4 6.1 Person (48.5%), Organization (13%), Geographical (9.8%)\nDate (5.5%), Nationalities (3.4%), Numerals (2.5%), etc. (17.3%)\nThe newly constructed datasets U PDATED LAMA, N EWLAMA, and N EWLAMA-E ASY require\nmulti-token decoding which adds a level of difﬁculty for the task compared to I NVARIANT LAMA.\nMoreover, NEWLAMA-E ASY has a different input distribution (longer input sequences) than the\nother datasets since the decontextualization and back-translation processes are applied to create each\ninstance, which makes the sentences longer. Lastly, some examples of the CKL benchmark datasets\nare provided in Table 6.\n18\nFigure 5: Mturk interface used for construction of UPDATED LAMA and N EWLAMA\nFigure 6: First mturk interface used for construction of NEWLAMA-E ASY\nFigure 7: Second mturk interface used for construction of NEWLAMA-E ASY\n19\nTable 6: Examples of INVARIANT LAMA, U PDATED LAMA, N EWLAMA, and N EWLAMA-E ASY\nTask Input Output\nINVARIANT LAMA\niPod Touch is produced by . Apple\nThe Sharon Cuneta Show was created in . Philippines\nThe native language of Lee Chang-dong is . Korean\nUPDATED LAMA\nis the prime minister of England. Theresa May→\nBoris Johnson\nhas the most passing yards in the NFL. Brady Quinn→\nJalen Guyton\nBale has champions league titles with 3→4Real Madrid.\nNEWLAMA\nAlicia Braga plays in the New Mutant. Cecilia Reyes\nowns the rights to the Falcon and the DisneyWinter Soldier.\nTesla invested in the digital currency bitcoin. 1.5 billion\nNEWLAMA-E ASY\nThe decision of the two volleyball stars Bria and Cimone\nHoward UniversityWoodard to withdraw from the Power 5 School to study\nat has become a national story.\nAllen Lazard is ofﬁcially listed as questionable with a sixnuclear injury after missing the last games.\nC E XPERIMENTAL CONFIGURATION\nPretraining Congifuration We utilize the T5 initially pretrained on C4 (April 2019) and contin-\nually pretrained with salient span masking (Guu et al., 2020) on Wikipedia (May 2020) as initial-\nization. We use the checkpoints from Wolf et al. (2020). We also perform the SSM objective during\nCKL because it was shown to help LMs “focus on problems that require world knowledge” (Guu\net al., 2020; Roberts et al., 2020).\nContinual Pretraining Conﬁgurations The input and output sequence length is ﬁxed to 350.\nWe use gradient accumulation for cases where the same number of training batches could not be\nloaded on the GPUs due to the varying memory consumption required for different methods and\nset the global batch size to 60. We use Adafactor optimizer with an initial learning rate of 1e-3.\nWe show the effects of learning rate variation regarding the trade-off between maintaining previous\nknowledge and acquiring new knowledge in Appendix E. We use learning rate warm-up for the ﬁrst\n10% of training and linearly decay the learning rate to half of the initial learning rate towards the end\nof training. For all of the experiments, we use 4 32GB V100 GPUs for training with each method\nexcept Mix-Review, where we use 16 32GB V100 GPUs. The details of the conﬁgurations used for\nevaluation on each individual CKL task are provided in Appendix C.\nEvaluation Conﬁgurations For T5 based models, all evaluation is done in a zero-shot manner and\nis processed with a single GPU. For I NVARIANT LAMA, the input and output length is ﬁxed as 25\nand 4 respectively. For UPDATED LAMA and N EWLAMA, the input and output length is 50 and 10\nrespectively. Lastly, the input and output length is 150 and 10 respectively for NEWLAMA-E ASY .\nThe rationale of this hyperparameter is based on average input and answer token in Table 5.\nUnlike T5 models, GPT-2 based models need additionallight-tuning for 1 epoch for evaluation. For\nINVARIANT LAMA, the input and output length is 50 and 3 respectively. The training batch size is\n32 and the learning rate is 1e-3. For evaluation on the acquisition of new knowledge, the input and\noutput length is 100 and 10 respectively. The training batch size is 8 due to memory constraints and\nthe learning rate is 1e-3. For both tuning processes, 4 V100 32GB GPUs are used. The detailed\nresult and discussion of GPT-2 based models are shown in Appendix G.\nD H YPERPARAMETERS FOR IMPLEMENTATION OF CKL M ETHODS\nRecAdam (Chen et al., 2020) We use the same hyperparameter setting for the optimizer as in Chen\net al. (2020): we set the coefﬁcient of the quadratic penalty γ to 5,000, and select the best t0 and k in\n100, 250, 500, 1,000 and 0.05, 0.1, 0.2, 0.5, 1 respectively for the annealing coefﬁcient λ(t).\n20\nTable 7: Result of T5-Vanilla and T5-Kadapters continually pretrained with various learning rates. The exper-\niments are done under the setting of S MALL scenario in Table 3, thus D0 are C4 (April 2019) and Wikipedia\n(May 2020), and D1 is CC-R ECENT NEWS -SMALL . Each of IL and NLE stands for I NVARIANT LAMA and\nNEWLAMA-E ASY . The parameters of FUAR are TF , TU\n1 , and T A\n1 , the tasks measuring the amount of time-\ninvariant knowledge from corpus D0, updated knowledge from D1, and newly acquired knowledge from D1,\nrespectively.\nMethod Learning Rate\nIL NLE FUAR\n((IL),n.d.,NLE) ↓EM EM\nT5-Initial - 24.17 8.9 -\nT5-Vanilla 1e-05 19.15 13.56 1.08\nT5-Vanilla 1e-04 17.45 15.21 1.06\nT5-Vanilla 5e-04 14.88 15.89 1.33\nT5-Vanilla 1e-03 11.19 18.77 1.32\nT5-Kadapters (k=2) 1e-04 19.93 14.93 0.70\nT5-Kadapters (k=2) 1e-03 16.46 19.59 0.72\nMix-Review (He et al., 2021) We use the English Wikipedia 13 to represent the original pretraining\ncorpus. The mix-decay and mix-ratio are set to 4 and 0.7, respectively, which is the best hyperpa-\nrameter setting in the paper.\nLoRA (Hu et al., 2021) We only freeze the encoder for the encoder-decoder LM and the entire\nmodel for the decoder-only LM. We use the optimal rank r of 4 and adapt both Wq and Wv in the\nself-attention module, which corresponds to the best performing hyperparameter setting in the paper.\nK-Adapter (Wang et al., 2021b) Similarly with T5-LoRA, we freeze the encoder for the encoder-\ndecoder LM and the entire model for GPT-2. We implement k = 2,3 for both T5 and GPT-2 to\nsee the effect of increasing # of parameters. Unlike in the original paper, we set the conﬁguration\nof the adapter identical to a single transformer layer from the original LM, ridding the need of an\nup-projection and down-projection layer.\nModular We use a projection layer before adding the hidden state outputs from both encoders to\nmatch the dimensions.\nWhy do we add parameters to only the encoder for T5? For parameter-expansion methods, we\nadd parameters to only the encoder because the encoder is applied to the input sequence and the\ndecoder is applied to the output sequence. Since most of the computational cost comes from the\ndecoder computing for the output sequence in an auto-regressive manner as highlighted in (Li et al.,\n2021), the newly added parameters in the encoder are roughly expected to have minimal additional\ncomputational cost.\nWhy do we freeze parameters of only the encoder for T5? K-Adapter and LoRA are initially\nproposed to freeze all of the parameters except for the newly added parameters. However, when\napplying this methodology to T5, it was empirically shown that unfreezing the parameters of the\ndecoder results in better performances when utilized together with parameter-expansion methods in\nterms of overall trade-off.\nE E XPLORING THE TRADE -OFF OF VARYING THE LEARNING RATE FOR\nCONTINUAL PRETRAINING\nTable 7 shows that lowering the learning rate for the continual pretraining leads to less forgetting of\nthe original knowledge, but also less learning of new knowledge. The experiments are done under\nthe setting of SMALL scenario in Table 3.\nBy comparing the FUAR among the T5-Vanilla models with different learning rates, it can be seen\nthat there is no rule of thumb for choosing the appropriate learning rate since FUAR is the lowest\n13https://huggingface.co/datasets/wikipedia\n21\nTable 8: Dev performance on KILT benchmark datasets after ﬁnetuning. Each model is ﬁnetuned on the train\nsets of KILT after continually trained on CC-R ECENT NEWS dataset for 4 epochs.\nMethod\nFact Checking Entity Linking Slot-ﬁlling Open Domain QA Dialogue\nFEVER AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW\nACC ACC ACC ACC ACC ACC EM EM EM Rouge F1\nT5-Initial 80.39 81.44 50.47 48.92 44.64 4.40 25.63 17.64 28.38 13.46 13.92\nT5-Vanilla 78.02 81.19 48.17 46.46 44.08 2.04 24.93 14.36 26.51 13.38 13.07\nT5-RecAdam 77.83 81.44 49.12 47.01 43.04 2.58 24.65 14.86 25.99 13.71 12.69\nT5-MixReview 77.17 80.77 49.38 46.22 44.08 2.47 25.07 14.57 26.36 13.57 12.73\nT5-LoRA 79.89 81.44 48.82 47.29 45.68 3.01 25.49 16.71 28.23 13.42 13.60\nT5-Kadapters (k=2) 80.35 80.94 48.91 46.65 45.52 3.33 26.20 16.57 26.89 13.15 12.94\nT5-Kadapters (k=3) 80.31 80.52 47.09 46.26 45.60 3.12 24.79 16.57 25.62 13.82 13.42\nT5-Modular 80.54 82.44 48.44 44.81 48.16 3.44 24.51 18.43 28.31 13.72 14.03\nTable 9: Hyperparameters and dataset details for all tasks of KILT.\nFact Checking Entity Linking Slot-ﬁlling Open Domain QA Dialogue\nFEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW\nEpoch 5 20 - - 9 30 45 12 50 6 8\nInput Seq 25 768 512 2,048 25 25 35 50 25 35 175\nOutput Seq 10 6 6 6 6 6 6 8 10 350 40\nLR 1e-4 1e-4 - - 1e-3 1e-4 1e-3 1e-4 1e-3 1e-3 1e-4\nBatch Size 128 16 128 48 512 256 256 256 128 32 64\nTrain Size 104,966 18,395 - - 2,284,168 147,909 87,372 88,869 61,844 272,634 63,734\nDev Size 10,444 4,784 3,396 5,599 5,000 3,724 2,837 5,600 5,359 1,507 3,054\nin learning rate of 1e-4 and increases for both lower and higher learning rates. We suppose that the\noptimal learning rate heavily depends on the corpus size of D1 and the model capacity of LM. We\nalso report the performance of T5-Kadapters, which is a CKL method that shows robust performance\nthroughout most experiments. Applying T5-Kadapters consistently mitigates the trade-off between\nforgetting and acquiring new knowledge as shown by the improvement in FUAR from the T5-Vanilla\nmodel with the same learning rates, although the level of effectiveness varies according to the value\nof the learning rate. We do not perform extensive experiments with each of the varying learning\nrates since searching for the optimal learning rate for each different continued pretraining setting\nmay be out-of-scope with this research.\nF E XPLORING HOW CONTINUALLY PRETRAINING ON D1 AFFECTS KILT\nTASKS WHICH REQUIRES KNOWLEDGE FROM D0\nIn addition to the CKL benchmark, we also show in Table 8 the performance on the dev set of\nKILT (Petroni et al., 2021) after ﬁnetuning each of the continually pretrained models of Table 2.\nSince KILT is made from Wikipedia, which corresponds to the old pretraining corpus D0, the per-\nformance on KILT measures how continual pretraining on new corpus D1 affects the performance\non the knowledge obtained from D0 if ﬁnetuning is done on behalf of the knowledge from D0.\nConﬁguration KILT (Petroni et al., 2021) consists of 5 different tasks and 11 datasets: Open-\nDomain Question Answering (Joshi et al., 2017; Kwiatkowski et al., 2019; Fan et al., 2019; Yang\net al., 2018), Fact Checking (Thorne et al., 2018), Entity Linking (Hoffart et al., 2011; Guo &\nBarbosa, 2018), Slot-ﬁlling (Levy et al., 2017), and Knowledgeable Open Dialogue (Dinan et al.,\n2019). Because each task requires a different training objective than the one used during pretraining,\nadditional ﬁnetuning is necessary. We search for the hyperparameters such as training epochs, batch\nsize, input size, output size, and learning rate of each individual KILT task to match the T5-base\ndev performance reported by Petroni et al. (2021). Using the identiﬁed conﬁgurations, we perform\nexperiments on all of the KILT tasks with the continually pretrained models for each method as the\ninitialization checkpoints. Evaluation metrics are different for each dataset: accuracy for discrete\n22\noutput (fact-checking, entity linking, slot-ﬁlling), Exact Match (EM) for question answering tasks\nwith short output, ROUGE-L for ELI5 (question answering task with long output), and F1-score for\nWizard of Wikipedia (dialogue). The data statistics and the hyperparameters used for ﬁnetuning on\neach KILT dataset is reported in Table 9.\nExperimental Result We ﬁrst focus on the performance on zero-shot Relation Extraction (zsRE),\nwhich is measured on the dev set of 12 relations that are ensured to have no overlap with the 84 rela-\ntions of the train set (Levy et al., 2017). Since the setting is similar to the zero-shot probing setting\nof IL, the trend of the result on the two datasets are similar. The performance of T5-Vanilla drops to\nhalf from that of T5-Initial as shown in IL, and the best performing method for both datasets is T5-\nModular. In addition, corresponding with results from the CKL benchmark, parameter-expansion\nmethods generally show stronger performance than the other methods.\nHowever, for the other datasets that cannot be performed in a zero-shot manner, the intermediate\nprocess of continually pretraining on corpus D1 does not seem to be that harmful on the ﬁnetuning\nfor the target tasks even though they are more related to the knowledge of D0. Even T5-Vanilla\nshows modest performance, sometimes with better results than some other CKL baselines. One\nhypothesis is that the models could have regained the original knowledge from corpus D0 through\nthe ﬁnetuning process. Also, some of the knowledge could have been recovered through the test-\ntrain overlap (Lewis et al., 2020b; Wang et al., 2021a).\nA more surprising ﬁnding is that the performance of some of the parameter-expansion methods are\neven higher than that of T5-Initial, which is considered to be the upper bound for KILT because T5-\nInitial is only trained on behalf of the knowledge from D0. For example, T5-Modular shows higher\nscores than T5-Initial on 6 out of 11 tasks. Since the parameter-expansion methods force the model\nto store the new knowledge in the newly added parameters during continual pretraining, one careful\nconjecture is these LMs have learned to combine and utilize in its internal representation of both old\nand new knowledge stored in separate parameters during ﬁnetuning to maximize the performance.\nG E XPLORING HOW CKL M ETHODS TRANSFER ACROSS LM\nARCHITECTURES\nWe perform experiments with GPT-2 Large (∼774M params) (Radford et al., 2019) initially pre-\ntrained on WebText and Wikipedia14 (D0) and continually trained on CC-R ECENT NEWS -SMALL ,\ni.e., SMALL (D1) for 8 epochs. For continued pretraining, we use the common teacher-forcing pre-\ntraining objective. The initial learning rate for the continued pretraining stage is empirically chosen\nas 1e-4 (results with learning rate as 1e-3 are shown in Appendix G.1). After continued pretraining,\nwe apply light-tuning, a process denoted for ﬁnetuning the model for only one epoch on a small\nportion of data similar to the evaluation set. Training on a single epoch constrains the model to\nbarely adapt to the input-output form of the data and not to learn the knowledge in tuning samples,\nmitigating the problem suggested by Lewis et al. (2020b).\nTo measure the time-invariant knowledge, we use InvariantLAMA (IL) because most of the slots to\nﬁll are at the end of the sentence. For light-tuning on behalf of IL, we use additional T-Rex data from\nShin et al. (2020) which has a similar distribution as instances from IL. Among them, 5,000 instances\nwith the same time-invariant relations as IL are randomly sampled for light-tuning. On the other\nhand, unlike IL where most of the slots to ﬁll are at the end of the sentences, the LAMA datasets for\nnew knowledge in our CKL benchmark mostly have the slots at the beginning of the sentences.\nTherefore, we use the corresponding CBQA dataset of N EWLAMA-E ASY , N EWQUESTIONS -\nEASY (NQE) to roughly measure the new knowledge. 15 For light-tuning on behalf of NQE, 5,000\ninstances are sampled from a set of QA pairs constructed from CC-R ECENT NEWS but not CC-\nRECENT NEWS -SMALL to remove the test-train overlap.\n14GPT-2 was initially pretrained on WebText (Dec 2019), which consists of 8 million documents with\nWikipedia pages excluded. In order to measure the performance on I NVARIANT LAMA constructed from\nWikipedia, we continually pretrain GPT-2 on a subset of Wikipedia (May 2020) for 14k global training steps\nbefore CKL.\n15The QA version of UL, NL and NLE will be also released with the main CKL benchmark.\n23\nTable 10: Performance of decoder-only models initially pretrained on Dec 2019 dump of Webtext and May\n2020 dump of Wikipedia (D0) continually pretrained on CC-R ECENT NEWS -SMALL (D1) for 8 epochs with a\nlearning rate of 1e-4. Each of IL and NQE stands for I NVARIANT LAMA and N EWQUESTIONS -EASY . The\nparameters of FUAR are TF , TU\n1 , and T A\n1 , the tasks measuring the amount of time-invariant knowledge from\ncorpus D0, updated knowledge from D1, and newly acquired knowledge from D1, respectively.\nMethod\nIL NQE FUAR\n((IL),n.d.,NQE) ↓EM EM\nGPT2-Initial 38.11 4.3 -\nGPT2-Vanilla 35.88 5.79 1.58\nGPT2-Recadam 35.50 5.79 1.84\nGPT2-Mixreview 38.93 5.57 0\nGPT2-Lora 37.99 6.23 0.06\nGPT2-Kadapters (k=2) 37.85 6.34 0.13\nGPT2-Kadapters (k=3) 38.03 5.79 0.06\nTable 10 shows the CKL benchmark performance of GPT-2 models. We report the results aver-\naged over 5 runs with different random seeds. As in Table 2, parameter-expansion methods show\nrobust performance on both IL and NQE, resulting in low FUAR. This shows that these methods\nare not only effective on the encoder-decoder model but also the decoder-only model as well. One\ninteresting result in Table 10 is that GPT2-MixReview performs the best on IL, with performance\neven higher than the initial model, which results in the best FUAR of 0 which means no forgetting\noccurred at all. We suppose that the training strategy of GPT2-MixReview, allowing access to sam-\nples of D0 during continued pretraining, would have allowed fast adaptation to knowledge from D0\nduring the light-tuning phase. Performance of GPT2-MixReview suggests that it makes it possible\nto regain the original knowledge for decoder-only models even with small tuning steps.\nWe want to highlight that the discrepancy of the performances among the CKL methods between\nencoder-decoder LM (T5) and decoder-only LM (GPT-2) may not solely be on the LM architecture,\nbut also on the learning rate and the evaluation method (light-tuning was used to evaluate GPT-\n2 while we evaluated T5 in a zero-shot manner). We leave further exploration of training ever-\nchanging decoder-only LMs such as GPT-2 as future work.\nG.1 F AILED GPT-2 EXPERIMENTS WITH LARGER LEARNING RATE\nTable 11 shows the CKL benchmark result of GPT-2 models continually pretrained on CC-\nRECENT NEWS -SMALL for 8 epochs with a learning rate of 1e-3. By comparing the results in this\ntable with those in Table 10, which is for models continually pretrained with a learning rate of 1e-4,\nthe results in Table 11 shows worse performance on both IL and NQE. Unlike in Appendix E, in-\ncreasing the learning rate does not result in better learning of new knowledge. Instead, NQE perfor-\nmance is even worse than GPT2-Initial for GPT2-Vanilla, GPT2-Recadam, and GPT2-MixReview.\nFUAR is no gain for these cases by the deﬁnition of the metric because the denominator has the\nvalue of zero. This shows that a large learning rate for continual pretraining may lead to failure:\nneither retaining old knowledge nor acquiring new knowledge effectively. For parameter-expansion\nmethods, because many parameters including the decoder are frozen during the continual training\nprocess, they seem to be less prone to the effect of a large learning rate.\nH E XPLORING THE PREDICTION CHANGE DURING CONTINUAL\nPRETRAINING\nTable 12 shows the prediction results of T5-Vanilla and T5-Modular on three knowledge probing\ntasks: I NVARIANT LAMA, U PDATED LAMA, and N EWLAMA. We show the prediction for every\ntraining epoch for each model. The instances are selected from the predictions that T5-Modular got\ncorrect but T5-Initial got wrong on the ﬁnal prediction, in order to see where the gap of the EM\ncomes from.\n24\nTable 11: Performance of decoder-only models initially pretrained on Dec 2019 dump of Webtext and May\n2020 dump of Wikipedia (D0) continually pretrained on CC-R ECENT NEWS -SMALL (D1) for 8 epochs with a\nlearning rate of 1e-3. These are the results failed due to a large learning rate. Each of IL and NQE stands for\nINVARIANT LAMA and N EWQUESTIONS -EASY .\nMethod\nIL NQE FUAR\n((IL),n.d.,NQE) ↓EM EM\nGPT2-Initial 38.11 4.37 -\nGPT2-Vanilla 23.03 1.64 no gain\nGPT2-Recadam 25.38 2.73 no gain\nGPT2-Mixreview 32.07 1.64 no gain\nGPT2-Lora 34.52 5.46 3.29\nGPT2-Kadapters (k=2) 33.67 6.01 2.71\nGPT2-Kadapters (k=3) 31.75 7.65 1.94\nTable 12: Change of Prediction Outputs During Continued Pretraininig\nCloze Sentence Model Epoch 1 Epoch 2 Epoch 3 Epoch 4 Answer\nIL\nThe native language of\nYvonne Monlaur is .\nV French French Khmer Malaya FrenchM French French French French\nSonic Drift 2 is developed by . V Sonic D Sonic the Sonic Found Sonic the SegaM Sonic R Sega Sega Sega\nWebKit is developed by . V Microsoft Google GitHub Google AppleM Apple Apple Apple Apple\nThe ofﬁcial language of Republic of\nIngushetia is .\nV Russian English Kazakh English RussianM Russian Russian Russian Russian\nThe capital of Roman Empire is . V Rome Rome Constantino Constantino RomeM Rome Rome Rome Rome\nUL\nThe biggest exporter of crude oil\nto china is .\nV Saudi Arabia Saudi Arabia Saudi Arabia Saudi Arabia Saudi Arabia →\nRussiaM Russia Saudi Arabia Russia Russia\nis the head of\nthe euro zone central bank\nV Mario Draghi Yves Le Maire Yves Dujarric Mario Draghi Mario Draghi →\nChristine LagardeM Mario Draghi Christine Lagarde Christine Lagarde Christine Lagarde\nis the manager of\nchelsea in the premier league\nV Mauricio Fernandez Steve Bruce Frank Lampard Mikel Arteta Luis Enrique →\nFrank LampardM Jose Mourinho Jose Mourinho Frank Lampard Frank Lampard\nis the price for a ﬂat in nottingham V What 999 £1.25m £1.25m 36,000 →\n40,000M This 30,000 pounds 40,000 pounds 40,000\nwas the governor of New York\nat the time this article was written\nV Andrew M. Cuomo Cuomo Andrew Cuomo Franklin D. Roosevelt Martin Van Buren →\nAndrew CuomoM Andrew Cuomo Andrew Cuomo Andrew M. Cuomo Andrew Cuomo\nNL\nis on the Bills all-pro team V Corey Williams Corey Connor WilliamsM Williams Williams Williams Williams\nis the founder of the popular\ncryptocurrency bitcoin\nV Satoshi Nakamoto Satoshi Nakamoto Yuri Xiaobo Satoshi NakamotoM Vitalik Buterin Satoshi Nakamoto Satoshi Nakamoto Satoshi Nakamoto\nThe bail for kyle rittenhouse is . V Rs. 1 crore a whopping $1 million $2 million $1 million $2 millionM $2 million $2 million $2 million $2 million\nThe las vegas raiders beat\nin the playoffs\nV the Las Vegas Raiders the New Orleans Saints the Las Vegas Raiders the sacramento the New Orleans SaintsM the New Orleans Saints the Kansas City Chiefs the Kansas City Chiefs the New Orleans Saints\nis the host of ellen de generes show V Yves samantha s Norma Mike Ellen DeGeneresM Elise Ellen DeGeneres Ellen deGenes Ellen DeGeneres\nI E XPLORING THE CAUSE OF THE EM G AP BETWEEN UPDATED LAMA AND\nNEWLAMA\nAs shown in the main experiment, Table 2, there is a considerable gap between the EM of U P-\nDATED LAMA (UL) and N EWLAMA (NL) over all the methods, despite undergoing the same data\nconstruction process. We attempt to analyze the causation by ﬁrst analyzing what answer types\nmake up the EM score of both UL and NL of T5-Vanilla, which are 10.17 and 3.77, respectively.\nAs shown in Figure 8a, the cloze sentences that take Person type as the ground truth makes up most\nof the EM of both tasks, despite Person type answers taking up a similar proportion out of the total\nanswer types (61.46% for UL and 59.7% for NL). Since UL consists of probes requiring an update\nof information from D0, one might conjecture that the EM gap is simply due to the difference of the\n25\nEM\n0\n3\n6\n9\n12\nUpdatedLAMA NewLAMA\nPerson Organization Numerals\nGeographical etc.\n(a) Composition of ground truth categories\nof the correctly predicted instances\nEM\n0\n6\n12\n18\n24\nPerson (Overlapping Answers)\nUpdatedLAMA NewLAMA\n(b) EM measured using the in-\nstances from UL and NL with\noverlapping Person type answers\nFigure 8: Analyzing the cause of the EM gap between UPDATED LAMA and N EWLAMA.\nTable 13: F1 Score of Main Results.\nMethod\nIL UL NL NLE FUAR\n((IL),UL,NL) ↓EM EM EM EM\nT5-Initial 24.88 2.62 3.19 14.49 -\nT5-Vanilla 13.11 11.89 5.84 22.53 0.68\nT5-RecAdam 13.39 14.33 6.15 22.68 0.57\nT5-MixReview 14.09 8.11 4.80 18.89 1.10\nT5-LoRA 17.04 14.50 7.45 24.59 0.36\nT5-Kadapters (k=2) 19.88 13.67 7.43 24.04 0.22\nT5-Kadapters (k=3) 19.91 14.31 6.55 23.33 0.21\nT5-Modular 21.35 12.78 6.94 24.42 0.17\nfrequency in each corpus of the entities that serve as the ground truths, e.g., those entities appear\nmore in corpus D0 than in D1. In order to get rid of the inﬂuence of frequency of entities when\nanalyzing the source of the EM gap, we ﬁnd overlapping Person type answers from UL and NL,\nand analyze only the 67 probing sentences for both datasets each paired to one of these entities. As\nshown in Figure 8b, the EM on UL is still much higher than that of NL. Manually analyzing these\ninstances, we ﬁnd that the probing sentences for NL ask for relatively more ﬁne-grained knowledge\ncompared to UL, since the instances of UL by deﬁnition are overlapped cloze sentences with differ-\nent answers in the corpus D0 and D1, that naturally make them be coarse-grained. For instance, the\nprobing sentences for entity “Tim Walz” in UL and NL are “ is the governor of Minnesota\nthis year.” and “ is the governor of Minnesota calling for the evacuation of St. Paul.”, respec-\ntively. We thus conjecture that the main causation of the EM gap to be UL consisting of instances\nrequiring coarse-grained knowledge, which is likely to have appeared more during inD1, while NL\nconsisting of instances requiring ﬁne-grained knowledge, which is expected to likely have appeared\nless in D1.\nJ A DDITIONAL ANALYSIS OF MAIN RESULTS\n26\nT5-Initial\n T5-Baseline\n T5-RecAdam\n T5-MixReview\nT5-LoRA\n T5-Kadapters (k=2) T5-Kadapters(k=3)\n T5-Modular\nmean P@k\n0\n10\n20\n30\n40\n50\n60\n70\n0 25 50 75 100\nk\n(a) INVARIANT LAMA\nmean P@k\n0\n8\n16\n24\n32\n40\n0 25 50 75 100\nk (b) UPDATED LAMA\nmean P@k\n0\n5\n10\n15\n20\n25\n30\n0 25 50 75 100\nk (c) NEWLAMA\nmean P@k\n0\n10\n20\n30\n40\n50\n0 25 50 75 100\nk (d) NEWLAMA-E ASY\nFigure 9: Mean P@k curve for CKL benchmark with varying k.\n27"
}