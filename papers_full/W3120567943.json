{
    "title": "ChiSquareX at TextGraphs 2020 Shared Task: Leveraging Pretrained Language Models for Explanation Regeneration",
    "url": "https://openalex.org/W3120567943",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3119900081",
            "name": "Aditya Girish Pawate",
            "affiliations": [
                "Indian Institute of Technology Kharagpur"
            ]
        },
        {
            "id": "https://openalex.org/A3120505275",
            "name": "Varun Madhavan",
            "affiliations": [
                "Indian Institute of Technology Kharagpur"
            ]
        },
        {
            "id": "https://openalex.org/A3120093865",
            "name": "Devansh Chandak",
            "affiliations": [
                "Indian Institute of Technology Bombay"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963339923",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2914397182",
        "https://openalex.org/W2990261547",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2982940459",
        "https://openalex.org/W2790767126",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W3119370638",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2983719617",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3031043369",
        "https://openalex.org/W2985848674",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W4252742993",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W2125444198"
    ],
    "abstract": "In this work, we describe the system developed by a group of undergraduates from the Indian Institutes of Technology for the Shared Task at TextGraphs-14 on Multi-Hop Inference Explanation Regeneration (Jansen and Ustalov, 2020). The shared task required participants to develop methods to reconstruct gold explanations for elementary science questions from the WorldTreeCorpus (Xie et al., 2020). Although our research was not funded by any organization and all the models were trained on freely available tools like Google Colab, which restricted our computational capabilities, we have managed to achieve noteworthy results, placing ourselves in 4th place with a MAPscore of 0.49021in the evaluation leaderboard and 0.5062 MAPscore on the post-evaluation-phase leaderboard using RoBERTa. We incorporated some of the methods proposed in the previous edition of Textgraphs-13 (Chia et al., 2019), which proved to be very effective, improved upon them, and built a model on top of it using powerful state-of-the-art pre-trained language models like RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), SciB-ERT (Beltagy et al., 2019) among others. Further optimization of our work can be done with the availability of better computational resources.",
    "full_text": "Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs), pages 103–108\nBarcelona, Spain (Online), December 13, 2020\n103\nChiSquareX at TextGraphs 2020 Shared Task: Leveraging Pre-trained\nLanguage Models for Explanation Regeneration∗\nAditya Girish Pawate\nIIT Kharagpur\nadityagirish\npawate@gmail.com\nDevansh Chandak\nIIT Bombay\ndchandak99@gmail.com\nVarun Madhavan\nIIT Kharagpur\nvarun.m.iitkgp\n@gmail.com\nAbstract\nIn this work, we describe the system developed by a group of undergraduates from the Indian\nInstitutes of Technology, for the Shared Task at TextGraphs-14 on Multi-Hop Inference Explana-\ntion Regeneration (Jansen and Ustalov, 2020). The shared task required participants to develop\nmethods to reconstruct gold explanations for elementary science questions from the WorldTree\nCorpus (Xie et al., 2020). Although our research was not funded by any organization and all\nthe models were trained on freely available tools like Google Colab which restricted our com-\nputational capabilities, we have managed to achieve noteworthy results placing ourselves in the\n4th place with a M AP score of 0.49021 in the evaluation leaderboard and 0.5062 M AP score on\nthe post-evaluation-phase leaderboard using RoBERTa. We incorporated some of the methods\nproposed in the previous edition of Textgraphs-13 (Chia et al., 2019), which proved to be very\neffective, improved upon them, and built a model on top of it using powerful state-of-the-art\npre-trained language models like RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), SciB-\nERT (Beltagy et al., 2019) among others. Further optimization of our work can be done with the\navailability of better computational resources.\n1 Introduction\nThe Shared Task is aimed at Multi-hop Inference for Explanation Regeneration. Participants are required\nto develop new and improve existing methods to reconstruct gold explanations for the WorldTree Corpus\n(Xie et al., 2020) of elementary science questions, their answers, and explanations.\nQuestion: Which of the following is an example of an organism taking in nutrients? (A) a dog\nburying a bone (B) a girl eating an apple (C) an insect crawling on a leaf (D) a boy planting tomatoes\nAnswer: (B) a girl eating an apple\nGold Explanation Facts:1) A girl means a human girl: Grounding 2) Humans are living organisms:\nGrounding 3) Eating is when an organism takes in nutrients in the form of food: Central 4) Fruits are\nkinds of foods: Grounding 5) An apple is a kind of fruit: Grounding\nIrrelevant Explanation Facts:1) Some ﬂowers become fruits. 2) Fruit contains seeds. 3) living\nthings live in their habitat. 4) Consumers eat other organisms\nTable 1: An Example for Explanation Regeneration\nThe example highlights an instance for this task, where systems need to perform multi-hop inference to\ncombine diverse information and identify relevant explanation sentences required to answer the speciﬁc\nquestion. The task provides a new and more challenging corpus of 9029 explanations and a set of gold\nexplanations for each question and correct answer pair.\n1Full, replicable code is available on Github for all methods described here, athttps://github.com/dchandak99/\nTextGraphs-2020\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n104\nTeam MAP on leaderboard\nBaidu PGL 0.6033\nalvysinger 0.5843\naisys 0.5233\nChiSquareX 0.4902\nRed Dragon 0.4793\nmler 0.3367\ndustalov (Baseline) 0.2344\nTable 2: Final Leaderboard\nAttributes TG 2019 TG 2020\nQuestions 1680 4367\nExplanations 4950 9029\nTables 62 81\nTable 3: Dataset Comparison\n2 Dataset\nThe dataset is the WorldTree Corpus V2.1(Xie et al., 2020) of Explanation Graphs and Inference Patterns\nsupporting Multi-hop Inference (Februrary 2020 snapshot). It is a newer version of the dataset used in\nthe TextGraphs-2019 (Jansen and Ustalov, 2019). The comparison between the two datasets is shown in\nTable 3.\n3 Problem Review\nThe problem statement requires participants to build a system that, given a question and its answer\nchoices, can identify the sentences that explain the answer given the question. This is a challenging\ntask due to the presence of other irrelevant sentences in the corpora for the given question, which have\nequally signiﬁcant lexical and semantic overlap as the correct ones (Fried et al., 2015). When a more\nclassical graph theory approach using the semantic overlap of explanations and questions is tried, it\nleads to the problem of semantic drift (Jansen, 2018). More classic graph methods were attempted in\n(Kwon et al., 2018), where the challenge of semantic drift in multi-hop inference was analyzed, and the\neffectiveness of information extraction methods was demonstrated. Also, approaching the question as\na language generation task is not effective and the current state-of-the-art models (Du ˇsek et al., 2020)\nare not capable of generating the exact explanations as required by this task. So this task can easily be\ntransformed into a sentence ranking problem in which we need to rank the relevant facts over all other\ngiven facts present in the corpus. The evaluation metric used for the task is the widely used and robust\nmean average precision (MAP) metric.\nWe have explained a few initial experiments that were undertaken in Section 4.1, followed by the\npre-processing methods we incorporated in Section 4.2. We have then discussed our models in Sections\n4.3 through 4.6. We have ﬁnally shown all our results and discussions in Section 5 followed by the\nconclusion and acknowledgments.\n4 Model\n4.1 Initial Experiments\nWe used the pure textual form of each explanation, problem and correct answer, rather than using a\nsemi-structured form given in the column-oriented ﬁles provided in the dataset. Initially, we just reduced\nthe original text of the questions that included all the answer choices. This was done by removing the\nincorrect answers, which thereby resulted in an improvement in performance. This is similar to what was\nseen in the previous edition of the task. Taking the TFIDF baseline with the basic pre-processing we got a\nMAP score of 0.3065 on the hidden test set. Taking this as the starting point, we built aSentenceBERT\nModel in which we converted all questions and explanations into contextual word embedding vectors and\nranked the explanations in descending order of cosine similarity of the embedded vectors. We observed\na drop in the model’s performance with the MAP score of 0.2427 on the test dataset, which is worse than\nthe simple TFIDF ranker. We realized that it was the semantic overlap between the question and the\nirrelevant explanations that caused such an unexpected performance drop on further inspection. So we\nnoted that we should not use contextual word embeddings, but instead, we must improve the simple but\n105\neffective information retrieval technique of TFIDF for the ranker. We then used the Sublinear TFIDF 2\nand Binary TFIDF. The optimized Sublinear TFIDF vectorizer gave a boost in the score: 0.3254 MAP.\n4.2 Preprocessing\nIt was seen that the TFIDF algorithm was very sensitive to keywords, so we applied the pre-processing\nand optimization techniques mentioned in (Chia et al., 2019). For each of these, we performed Penn-\nTreebank tokenization, followed by lemmatization using the lemmatization ﬁles provided with the\ndataset.3 We used NLTK for tokenization to reduce the vocabulary size needed by combining the differ-\nent forms of the same keyword. We also removed stopwords, which thereby removed noise in the texts.\nA simple TFIDF based ranker along with the above pre-processing returned a M AP score of 0.3850.\nSubstituting Sublinear TFIDF, we noticed that the score increased to 0.4080 M AP. With some experi-\nmentation, we were able to further improve the M AP score to 0.426 using Binary TFIDF. Finally, we\napplied Recursive TFIDF as proposed in this paper (Chia et al., 2019), in which the authors treated the\nTFIDF vector as a representation of the current chain of reasoning, each successive iteration built on\nthe representation to accumulate a sequence of explanations. We optimized all the other variables like\nnormalization, maxlen, hops, scale. We found the MAP to be completely independent of the normal-\nization used. For maxlen = {128, 125, 144}, we found maxlen = 128 to be most efﬁcient. For number\nof hops = {1,2,3}, we found 1 to be best. This may be because semantic drift creeps in as we explore\nthe nodes further away from the current node. A scaling factor was used in each successive explanation\nas it is added to the TFIDF vector. For the downscaling factors= {1.25, 1.3, 1.35}, we found 1.25 was\noptimum. We got a slight improvement in the score 0.4430 M AP when used along with Binary TFIDF.\nAll these steps were done as a part of the pre-processing step.\n4.3 Pure Language Model approach\nAfter doing all the pre-processing steps, we tried to apply a simple pure language model based approach\nthat has shown good performance in Text Classiﬁcation tasks. We took each processed question and con-\ncatenated each of the 9029 explanations to it one by one. Then for each of these question + explanation\npairs, we used a simple language model based BERT classiﬁer (BERTForSequenceClassification )\nto predict whether the explanation was one of the gold explanations for that question. The result for this\nwas 0.4116 M AP, which was lesser than we expected. We deduced that there are two major problems\nwith this simplistic approach.\n•Class imbalance: Most of the question-explanations would be labeled 0 (False), since out of the\n9029 total explanations only a few would actually be gold explanations for the question. This causes\nthe classiﬁer to output the 0 label almost all the time and prevents it from learning the true relations\nbetween the gold explanations and the question. It is possible that the class imbalance could be\nmitigated by searching for better hyperparameter values; however, we weren’t able to do that with\nthe available resources, so we applied a different technique to address this.\n•Non-scalability: This approach would require inferences equal to the number of explanations in\nthe corpus for every question. While it’s possible to do this for this relatively small corpus of\n9029 explanations, as the number of explanations becomes larger, this approach would no longer be\nfeasible; requiring too much time for training and, more importantly, for inference.\n4.4 Using TFIDF to retrieve relevant explanations\nTo address the above problems, we applied the optimal TFIDF vectorizer (TFIDFbinary + recursive) ob-\ntained in the pre-processing step to ﬁrst obtain the most relevant explanations for a given question based\non the lexical overlap between the question and the explanations. The number of explanations retrieved\nby this initial ranker ( top k) was a tuned parameter. This technique was very effective at retrieving the\ngold explanations for a question. We have shown the fraction of gold explanations retrieved when we\n2https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html\n3PTB tokenization and stopwords from the NLTK package\n106\nMethod MAP Score\nTFIDF + preproc 0.3850\nsublinear + preproc 0.4080\nbinary + preproc 0.4267\nrecursive 0.4429\nsublinear + recursive 0.4429\nbinary + recursive 0.4430\nTable 4: Inital scores using only pre-processing\nand TFIDF\ntop k Fraction Retrieved\n10 0.484\n30 0.676\n50 0.767\n80 0.837\n100 0.865\n300 0.965\n500 0.987\nTable 5: Value of parametertop k vs Fraction of\ngold explanations retrieved\nconsider the top k explanations in Table 5. We can see that almost 87% of the gold explanations are\nretrieved when we that top 100 explanations from TFIDF, and almost 99% of gold explanations are re-\ntrieved when we took the top 500 explanations. This saves our computation as we now need to only train\nthe model for at max 500 explanations per question instead of 9029 explanations. Now top k retrieved\nexplanations are concatenated to the questions as in the previous approach, and the classiﬁer model is\ntrained to classify whether a given explanation among thetop k explanations is the right explanation for\nthe question or not. The M AP score using the BERTForSequenceClassification model was 0.4365\nMAP. We inferred that the score was low because the model was predicting the 0 label for almost all\ninputs since there was still a signiﬁcant imbalance in the training dataset (though signiﬁcantly less than\nbefore).\n4.5 Addressing class imbalance\nTo address the class imbalance in question explanation pairs, we applied a simple approach of over-\nsampling of the minority class (Positive or ‘1’ label). We simply repeated the gold explanations during\ntraining such that for each question, the number of positive and negative labeled explanations would be\nequal (equal to top k/2). Hence the explanations for a given question were the top k/2 negatively la-\nbeled explanations plus the positively labeled explanations retrieved by TFIDF repeated top k/2 times.\nThis was only applied while training, not during inference in the validation and test datasets. Using this\nsimple technique, we were able to get a signiﬁcant boost in the performance for the baseline of BERT\nwith 0.4506 MAP score.\nFigure 1: The Overall Flow of the Model\nFigure 2: Inside the\nClassiﬁcation Model\n4.6 Pre-trained Language Models\nWe tried out all pre-trained language models available for sequence classiﬁcation. We optimized\nthe following hyperparameters: top k, num train epochs, batch size, learning rate, epsilon,\ngradient accumulation steps, max grad norm, weight decay. The batch size is dependent on\nthe GPU RAM available. The parameters top k and num train epochs are a function of training time.\nSince we needed to optimize the training time, we ﬁrst trained all models withtop k as 100 with 3 epochs\nto get a preliminary model performance. Then we took the best models and trained it for a higher top k\n(500 or 300 whichever was feasible) to get a boost in score. Our best performing model took close to 8\nhours to complete the training. Further details of the models are given in the supplementary.\n107\nParameter Value\nOptimizer AdamW\nLearning Rate 2e-5\nEpsilon 1e-8\nMax Grad Norm 1\nGradient Accumulation Steps 1\nTable 6: Hyper Parameter Values\n5 Results and discussion\nWe present the scores in the table given below. We got our best performance from RoBERTa. When\nwe observe the results, we see that there is only a slight variation in the ﬁnal scores of most pre-trained\nlanguage models. We observe that the models overﬁt the given data. We could not perform a grid search\nto optimize all parameters due to computational constraints and had to manually search for the best\nhyperparameters due to which the performance of any given model may not be optimal. Further, we have\ntrained only RoBERTa and BART for top k 500 explanations and not other models because they had a\nlong training time or a higher RAM requirement.\nModel Num Param Batch Size top k Train MAP Dev MAP Test MAP\nRoBERTa (optimized) 355M 256 500 0.7210 0.5184 0.5061\nRoBERTa 355M 256 500 0.6708 0.5062 0.4902\nRoBERTa 355M 256 100 0.6182 0.4800 0.4798\nBART 406M 128 300 0.7167 0.5036 0.4865\nBART 406M 32 100 0.6708 0.4670 0.4769\nSciBERT 110M 256 100 0.6544 0.4950 0.4855\nELECTRA 355M 128 100 0.6143 0.4943 0.4854\nALBERT 223M 32 100 0.6280 0.4813 0.4731\nDistilBERT 134M 256 100 0.6049 0.4793 0.4641\nBERT 110M 256 150 0.5776 0.4609 0.4506\nTable 7: Final Results\n6 Conclusion\nWe have given a system description of our team ChiSquareX which stood 4th place in the evaluation\nphase leaderboard with a M AP score of 0.4902. We have presented a system with optimized pre-\nprocessing of the dataset followed by an optimized TFIDF information retrieval scheme to obtain initial\nranks, and then further pre-trained language model based re-ranker to rank the ﬁnal explanations. Despite\nthe computational constraints, just by leveraging Google Colab and other open-source tools, we have\nmanaged to ﬁne-tune state-of-the-art pre-trained language models like RoBERTa, BART and ELECTRA\non the (Xie et al., 2020) dataset and achieve a reasonable MAP score.\nAcknowledgements\nWe would ﬁrstly like to thank the organizers Peter Jansen and Dmitry Ustalov for holding this shared task.\nIt was a great learning experience for us. We would also like to thank the participants of TextGraphs-\n2019; their work was a great source of inspiration for us as to how to proceed with the task. (Chia et\nal., 2019) in particular, was a source of a number of simple but effective text pre-processing techniques\nto greatly improve performance. Additionally, we would like to extend a big thanks to the makers and\nmaintainers of the excellent HuggingFace (Wolf et al., 2020) repository, without which most of our\nresearch would have been impossible.\n108\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientiﬁc text. In\nEMNLP/IJCNLP.\nYew Ken Chia, Sam Witteveen, and Martin Andrews. 2019. Red dragon AI at TextGraphs 2019 shared task:\nLanguage model assisted explanation generation. In Proceedings of the Thirteenth Workshop on Graph-Based\nMethods for Natural Language Processing (TextGraphs-13), pages 85–89, Hong Kong, November. Association\nfor Computational Linguistics.\nOndˇrej Du ˇsek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the State-of-the-Art of End-to-End\nNatural Language Generation: The E2E NLG Challenge. Computer Speech & Language, 59:123–156, January.\nDaniel Fried, Peter Jansen, Gustave Hahn-Powell, Mihai Surdeanu, and Peter Clark. 2015. Higher-order lexical\nsemantic models for non-factoid answer reranking. Transactions of the Association for Computational Linguis-\ntics, 3(0):197–210.\nPeter Jansen and Dmitry Ustalov. 2019. TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation\nRegeneration. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language\nProcessing (TextGraphs-13), pages 63–77, Hong Kong. Association for Computational Linguistics.\nPeter Jansen and Dmitry Ustalov. 2020. TextGraphs 2020 Shared Task on Multi-Hop Inference for Explanation\nRegeneration. In Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs).\nAssociation for Computational Linguistics.\nPeter Jansen. 2018. Multi-hop inference for sentence-level TextGraphs: How challenging is meaningfully com-\nbining information for science question answering? In Proceedings of the Twelfth Workshop on Graph-Based\nMethods for Natural Language Processing (TextGraphs-12), pages 12–17, New Orleans, Louisiana, USA, June.\nAssociation for Computational Linguistics.\nHeeyoung Kwon, Harsh Trivedi, Peter Jansen, Mihai Surdeanu, and Niranjan Balasubramanian. 2018. Control-\nling information aggregation for complex question answering. In Gabriella Pasi, Benjamin Piwowarski, Leif\nAzzopardi, and Allan Hanbury, editors, Advances in Information Retrieval, pages 750–757, Cham. Springer\nInternational Publishing.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\nStoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural lan-\nguage generation, translation, and comprehension. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7871–7880, Online, July. Association for Computational Linguistics.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R ´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine\nJernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexan-\nder M. Rush. 2020. Huggingface’s transformers: State-of-the-art natural language processing.\nZhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, and Peter Jansen.\n2020. WorldTree v2: A corpus of science-domain structured explanations and inference patterns supporting\nmulti-hop inference. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5456–\n5473, Marseille, France, May. European Language Resources Association."
}