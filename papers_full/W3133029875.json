{
  "title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks",
  "url": "https://openalex.org/W3133029875",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221795652",
      "name": "Nogueira, Rodrigo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1835403423",
      "name": "Jiang Zhi-ying",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672090663",
      "name": "Lin, Jimmy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1732222442",
    "https://openalex.org/W2951107864",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W3100879603",
    "https://openalex.org/W2970900584",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W3100778284",
    "https://openalex.org/W2981037730",
    "https://openalex.org/W3034811314",
    "https://openalex.org/W3106531402",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3021524072",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3024482470",
    "https://openalex.org/W2963005248",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3022766797",
    "https://openalex.org/W2995359496",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W3111739346",
    "https://openalex.org/W2984812384",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W2998209000",
    "https://openalex.org/W3083835029",
    "https://openalex.org/W2995971510",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2173051530",
    "https://openalex.org/W2971094176",
    "https://openalex.org/W2950645060",
    "https://openalex.org/W2970609357",
    "https://openalex.org/W1771459135",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W3092044512",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2887020936",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2970308008",
    "https://openalex.org/W3092689172",
    "https://openalex.org/W2951756287",
    "https://openalex.org/W3128590981",
    "https://openalex.org/W2548137223",
    "https://openalex.org/W3082274269"
  ],
  "abstract": "The ability to perform arithmetic tasks is a remarkable trait of human intelligence and might form a critical component of more complex reasoning tasks. In this work, we investigate if the surface form of a number has any influence on how sequence-to-sequence language models learn simple arithmetic tasks such as addition and subtraction across a wide range of values. We find that how a number is represented in its surface form has a strong influence on the model's accuracy. In particular, the model fails to learn addition of five-digit numbers when using subwords (e.g., \"32\"), and it struggles to learn with character-level representations (e.g., \"3 2\"). By introducing position tokens (e.g., \"3 10e1 2\"), the model learns to accurately add and subtract numbers up to 60 digits. We conclude that modern pretrained language models can easily learn arithmetic from very few examples, as long as we use the proper surface representation. This result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement. Moreover, we show that regardless of the number of parameters and training examples, models cannot learn addition rules that are independent of the length of the numbers seen during training. Code to reproduce our experiments is available at https://github.com/castorini/transformers-arithmetic",
  "full_text": "arXiv:2102.13019v3  [cs.CL]  12 Apr 2021\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nIN V E S T IG AT I N G T H E LI M I TAT I O N S O F TR A N S F O R M -\nE R S W I T H SI M P L E AR I T H M E T IC TA S K S\nRodrigo Nogueira, Zhiying Jiang & Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of W aterloo\nABSTRACT\nThe ability to perform arithmetic tasks is a remarkable trai t of human intelligence\nand might form a critical component of more complex reasonin g tasks. In this\nwork, we investigate if the surface form of a number has any in ﬂuence on how\nsequence-to-sequence language models learn simple arithm etic tasks such as addi-\ntion and subtraction across a wide range of values. W e ﬁnd tha t how a number\nis represented in its surface form has a strong inﬂuence on th e model’s accu-\nracy. In particular, the model fails to learn addition of ﬁve -digit numbers when\nusing subwords (e.g., “32”), and it struggles to learn with c haracter-level repre-\nsentations (e.g., “3 2”). By introducing position tokens (e .g., “3 10e1 2”), the\nmodel learns to accurately add and subtract numbers up to 60 d igits. W e conclude\nthat modern pretrained language models can easily learn ari thmetic from very\nfew examples, as long as we use the proper surface representa tion. This result\nbolsters evidence that subword tokenizers and positional e ncodings are compo-\nnents in current transformer designs that might need improv ement. Moreover, we\nshow that regardless of the number of parameters and trainin g examples, mod-\nels cannot seem to learn addition rules that are independent of the length of the\nnumbers seen during training. Code to reproduce our experim ents is available at\nhttps://github.com/castorini/transformers-arithmetic\n1 I NTRODUC TI ON\nAbstraction and composition are two important themes in the study of human languages, made\npossible by different linguistic representations. Althou gh treatments in different linguistic traditions\nvary, representations at the lexical, syntactic, and seman tic levels are a common feature in nearly\nall theoretical studies of human language, and until relati vely recently, these representations are\nexplicitly “materialized” in language processing pipelin es (for example, semantic role labeling takes\nas input a syntactic parse).\nHowever, with the advent of pretrained transformer models, these intermediate representations no\nlonger have any explicit “reality”: while various studies h ave found evidence of syntactic and seman-\ntic knowledge in these models (T enney et al., 2019), it is no l onger possible to isolate, for example,\na subject–verb relation in a speciﬁc part of the model. With t ransformers, the only input to the model\nis the surface form of text combined with supplemental embed dings (e.g., positional embeddings,\nand in the case of BER T , segment embeddings).\nWhat are the consequences of this exclusive focus on the surf ace form of text? Some might say,\nnothing, as bigger models, better pretraining objectives, etc. will lead us to models that are capable of\nreasoning (Brown et al., 2020). W e believe this to be an unten able position and present a case study\nin simple arithmetic tasks where having the right represent ation is the difference between a nearly-\nimpossible-to-learn task and an easy-to-learn task. Our wo rk shows that it is possible to “inject”\nrepresentations into transformer models by simple manipul ations of the input sequence (in our case,\nexplicitly enumerating the semantics of digit positions), and that doing so makes it possible for\noff-the-shelf models to easily perform simple arithmetic, whereas it is nearly impossible otherwise.\nWhile we present only a case study, our ﬁndings have broader i mplications for various language\nanalysis tasks: First, although end-to-end training enabl ed by neural networks is a powerful tool,\n1\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nhaving the right representation is crucial also. Second, we demonstrate a simple way in which rep-\nresentations can be “injected” into transformer models in a completely transparent manner, without\nany need to re-pretrain. This work points out a path that migh t allow us to combine the best of both\nworlds: leveraging the power of pretraining, with addition al guidance from our understanding of the\nproblem domain.\nHowever, we ﬁnd that even explicit semantic representation s have their limits. Despite our best\nefforts, we ﬁnd that models cannot extrapolate, i.e., they f ail to perform simple arithmetic when\nevaluated on inputs whose length distribution differs from the one seen during training. This appears\nto be a problem that neither larger models, more compute, nor more data can solve.\nThere are, of course, many previous papers that investigate the representation of numbers and various\nnumeric reasoning tasks in the literature. W e present relat ed work in Appendix A.\n2 M ETHODOLOGY\nOur tasks are the addition and subtraction of two numbers. W e cast them as sequence-to-sequence\ntasks in which both inputs to the models and target outputs ar e treated as sequences of tokens. For\nthe addition task, an example input is “What is 52 plus 148?” a nd the target output is “200”. For the\nsubtraction task, an example input is “What is 20 minus 185?” and the target output is “-165”.\nW e programmatically generate training, development, and t est sets of different sizes depending on\nthe experiment. The input template is always “What is [numbe r1] [operation] [number2]?”, where\n[number1] and [number2] are numbers randomly sampled and [o peration] is either “plus” or “mi-\nnus”. Below , we discuss different ways of representing [num ber1] and [number2] and their corre-\nsponding answer. W e use two different methods to sample numb ers for training, development, and\ntest sets, which are described below .\nBalanced sampling: T o generate training and development sets, we ﬁrst set the ma ximum number\nof digits D and then create each example as follows: W e ﬁrst sample d from [2, D] and then inde-\npendently sample [number1] and [number2] from [10d− 1,10d − 1]. W e then compute the answer\naccording to the operation (i.e., either addition or subtra ction). This method ensures that the set will\nhave a roughly equal proportion of d-digit numbers, where d ∈ [2, D].\nRandom sampling: T o generate test sets, we sample [number1] and [number2] ind ependently from\n[0,10D − 1]. This results in approximately 90% of the numbers having D-digits, 9% having (D− 1)-\ndigits, and so on. This unbalanced set aims at evaluating mod els on the largest numbers it was trained\non. W e study how different sampling methods inﬂuence model e ffectiveness in Appendix G.\nMetric: Our metric is accuracy. That is, the model receives a score of one if its output matches the\ntarget output exactly. Otherwise, it receives a score of zer o.\nOur experiments use T5 (Raffel et al., 2020), a pretrained se quence-to-sequence model where ev-\nery natural language processing task—for example, machine translation, question answering, and\nclassiﬁcation—is formulated as feeding the model some inpu t sequence and training it to generate\nsome output sequence. W e follow this same approach and feed t he addition or subtraction question\n(described above) as a sequence of tokens to the model and tra in it to generate the answer, token by\ntoken. W e use greedy decoding as beam search showed similar e ffectiveness but is slower.\nW e train the models using the AdamW optimizer (Loshchilov & H utter, 2018), batches of 128 ex-\namples, and a learning rate of 0.0003. W e experimented with all T5 model sizes except for T5-11B\ndue to its computational cost. W e refer to T5-small, T5-base , and T5-large as T5-60M, T5-220M,\nand T5-770M, respectively, to easily distinguish models by their numbers of parameters. W e also\nexperiment with “vanilla” (i.e., non-pretrained) transfo rmers (see Appendix B).\nPrevious studies have recognized that commonly used subwor d tokenization techniques today are\nnot ideal to represent numbers (W allace et al., 2019; Henigh an et al., 2020; Saxton et al., 2018;\nLample & Charton, 2019), although none of them studied the pr oblem in depth. Here, we inves-\ntigate how six different number representations, illustra ted in T able 1, impact model accuracy on the\narithmetic tasks. In our main results, we only experiment wi th the “standard” ordering of generating\ndigits (i.e., most to least signiﬁcant), but in Appendix C, w e also experimented with inverting the\norder.\n2\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nOrthography Example Notes\nD E CIM A L 832 default representation\nCH A RACT E R 8 3 2 ensures consistent tokenization\nFIX E D -CH A RACT E R 0 8 3 2 ensures consistent positions (e.g., max. 4 digits)\nU N D E RS CO RE 8_3_2 underscores provide hints on digit signiﬁcance\nWO RD S eight hundred thirty-two leverages pretraining\n10- BA S E D 8 100 3 10 2 easy to determine digit signiﬁcance\n10 E -BA S E D 8 10e2 3 10e1 2 10e0 more compact encoding of above\nT able 1: Different ways of representing numbers explored in this work.\n2 5 10 15 20 25 300\n0.2\n0.4\n0.6\n0.8\n1\n# of digits\nT est Accuracy\n10 E -BA S E D “3 10e1 2 10e0”\n10- BA S E D “3 10 2”\nWO R D S “thirty-two”\nU N D E R S C O R E “3_2”\nFI X E D -C H A R AC T E R “0 0 3 2”\nC H A R AC T E R “3 2”\nD E C I M A L “32”\nFigure 1: Accuracy of different number representations on t he addition task.\nD EC I M A L: Digits are represented in the Hindu–Arabic numeral form (al so called decimal form).\nC H A R AC TER: Digits are separated by a white space, thus allowing the mode l to work on embed-\ndings that always represent single digits.\nFI X ED-C H A R AC TER: In the character representation above, it is hard to determi ne the signiﬁcance\nof a digit by relative position embeddings because relative positions change on a per example basis.\nT o address this, we introduce the FIX E D -CH A RACT E R representation in which numbers have the\nsame maximum number of digits.\nU N D ER S C O R E: Digits are separated by an underscore token. A possible adva ntage of this repre-\nsentation is that the model can learn to ﬁnd the signiﬁcance o f a digit by counting the number of\nunderscores to the right until the least signiﬁcant digit.\nWO R D S: Numbers are converted to words using the num2words package.1 W e can anticipate two\nadvantages in this representation: (1) the T5 model was pret rained on large amounts of textual data,\nso it likely knows that “hundred” is larger than “ten” (Zhang et al., 2020); (2) digits are surrounded\nby tokens that describe their signiﬁcance (“hundred”, “tho usand”, etc.), thus making it easier to ﬁnd\nwhich two digits in the input sequence should be added (or sub tracted).\n10- BA S ED: Digits are separated by powers of 10, which we call position t okens. This representation\nallows the model to ﬁnd the signiﬁcance of a digit by simply in specting its left or right tokens.\n10 E-BA S ED: Digits are separated by powers of 10 represented using scien tiﬁc notation. This or-\nthography has a more compact representation for the positio n tokens of large numbers than the\n10- BA S E D orthography. For example, in the 10- BA S E D orthography, the position token of the most\nsigniﬁcant digit of a 60-digit number occupies 60 character s (i.e., “1” followed by 59 zeros). In the\n10 E -BA S E D orthography, this position token occupies only 5 character s (i.e., “10e59”).\n3\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\n3 R ESULTS\nW e present results in Figure 1. Each point in the graph repres ents the mean accuracy of a T5-220M\nmodel trained for 100 epochs with ﬁve different sets of 1,000 addition examples sampled using the\nbalanced method. A separate development set of 1,000 exampl es is used to select the best checkpoint\nof each run. Error bars correspond to 95% conﬁdence interval s. The values on the x-axis represent\nthe maximum number of digits used for training and testing. W e use a maximum of 30-digit numbers\nas some representations such as WO RD S would result in input sequences that have too many tokens\n(e.g., more than 512), and hence prohibitively long trainin g times.\nIn the D E CIM A L representation, the model barely learns addition of 2-digi t numbers, and it fails to\nlearn addition of larger numbers, i.e., it has an accuracy of zero for 5 digits or more. One explanation\nfor this failure is because numbers are not systematically t okenized into digits. For instance, “132”\nmight be tokenized as “1” and “32”, whereas “232” might be tok enized as “23” and “2”. Hence, the\nmodel would have to learn that sometimes the embedding of a to ken refers to a single digit, other\ntimes to two digits, etc. It might be hard to learn (i.e., need more examples) to map an embedding\nto a number when the number of digits it represents changes ir regularly (dependent on the training\ndata of the tokenizer).\nThe CH A RACT E R and U N D E RS CO RE representations have much higher accuracy than D E CIM A L ,\nthus showing that it is easier to learn when embeddings repre sent single digits. Both representations\nexhibit decreasing accuracy as we increase the number of dig its, until reaching an accuracy of zero\nwith 15-digit addition. One explanation for this failure is that, since digits with the same signiﬁcance\nhave different positions in each example, the model has to co unt the number of digits on the right\nside in order to ﬁnd its signiﬁcance. With larger numbers, co unting becomes harder.\nThe FIX E D -CH A RACT E R representation achieves higher accuracy than CH A RACT E R and U N D E R-\nS CO RE for numbers longer than 12 digits, thus showing that the mode l can learn to memorize digit\npositions to determine their signiﬁcance. However, with an accuracy of approximately 20% for 15-\ndigit numbers, the memorization strategy eventually break s down. It appears to be hard to learn\nrelative positional embeddings that precisely encode the d istance between two tokens for our task.\nThe WO RD S representation shows stable accuracy in the range of 40-60% from 5 to 15 digits. Our\nhypothesis for this stability is that the intrinsic positio n tokens present in this representation (e.g.,\n“hundred”, “thousand”) make it easier for the model to ﬁnd an d sum two digits that are far apart in\nthe input sequence. However, for 20 digits or more, the model s fail at the task. Pretraining might\nhave contributed to the high accuracy on 15 digits or less bec ause the model might have already\nseen these numbers in this representation in the pretrainin g corpus. On the other hand, it is very\nunlikely that the corpus contains numbers of 20 digits or mor e expressed in plain English. W e\nfurther investigate the impact of pretraining in Appendix E .\nWith up to 15 digits, the 10- BA S E D and 10 E -BA S E D representations achieve accuracy close to 100%.\nOur explanation for their success is the explicit position t okens added between each digit, which\nallows the model to inspect the left or right tokens of a digit to determine its signiﬁcance.\nIn the Appendices, we present a number of additional experim ental results that build on our main\nﬁndings here. In Appendix B, we study the impact of various po sition embeddings on the addi-\ntion task. In Appendix C, we investigate how models of differ ent sizes perform interpolation and\nextrapolation tasks. Although larger models perform bette r than smaller ones, we show that not\neven 3B-parameter models can learn simple arithmetic rules . In Appendix D, we show that all rep-\nresentations can reach accuracies of 97% or more when enough training data is provided. Results\nhere, however, show that representations do matter when tra ining data is scarce. In Appendices E\nand F, we study how pretraining can impact a model’s ability t o learn arithmetic. Finally, in Ap-\npendix G, we investigate how a mismatch between the length di stribution of training and test sets\ncan be problematic for the addition task.\n1 https://github.com/savoirfairelinux/num2words\n4\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\n4 C ONCLUSIO N\nRumelhart et al. (1985) wrote in their germinal “backpropag ation” paper that “unfortunately, this\n[addition] is the one problem we have found that reliably lea ds the system into local minima”. Al-\nmost four decades later, despite remarkable progress in neu ral networks, the ﬁeld is still exploring\nthis task. Our small contribution is to show that simple mani pulations of surface representations to\nrender semantics explicit can help neural models to learn si mple arithmetic tasks. It remains to be\nseen if this “trick” can be applied to other tasks, but our res ults provide evidence that improving\ntokenizers and positional encodings are promising directi ons for future exploration.\nACKNOWLE DG M EN TS\nThis research was supported in part by the Canada First Resea rch Excellence Fund and the Natural\nSciences and Engineering Research Council (NSERC) of Canad a. In addition, we would like to\nthank Google Cloud for credits to support this work.\nREFERENC ES\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Givin g BER T a calculator: Finding\noperations and arguments with reading comprehension. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9t h International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp. 5949–5954, 2019.\nT om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jare d D. Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda As kell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, T om Henighan, Rewon Child , Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sig ler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCand lish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In Advances in Neural Information\nProcessing Systems, pp. 1877–1901, 2020.\nJui Chu, Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. Learning to generate correct\nnumeric values in news headlines. In Companion Proceedings of the W eb Conference 2020, pp.\n17–18, 2020.\nMostafa Dehghani, Stephan Gouws, Oriol V inyals, Jakob Uszk oreit, and Lukasz Kaiser. Universal\ntransformers. In International Conference on Learning Representations, 2018.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T out anova. BER T: Pre-training of deep\nbidirectional transformers for language understanding. I n Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nT echnologies, V olume 1 (Long and Short P apers), pp. 4171–4186, 2019.\nDavid Ding, Felix Hill, Adam Santoro, and Matt Botvinick. Ob ject-based attention for spatio-\ntemporal reasoning: Outperforming neuro-symbolic models with ﬂexible distributed architec-\ntures. arXiv preprint arXiv:2012.08508, 2020.\nDheeru Dua, Y izhong W ang, Pradeep Dasigi, Gabriel Stanovsk y, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discret e reasoning over paragraphs. In\nProceedings of the 2019 Conference of the North American Cha pter of the Association for Com-\nputational Linguistics: Human Language T echnologies, V olume 1 (Long and Short P apers), pp.\n2368–2378, 2019.\nMor Geva, Ankit Gupta, and Jonathan Berant. Injecting numer ical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meeting of the Association fo r Computational Lin-\nguistics, pp. 946–958, July 2020.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and W eizhu Chen. D eBER T a: Decoding-enhanced\nBER T with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.\n5\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nT om Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christophe r Hesse, Jacob Jackson, Heewoo\nJun, T om B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hall acy, Benjamin Mann, Alec Radford,\nAditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman , Dario Amodei, and Sam McCan-\ndlish. Scaling laws for autoregressive generative modelin g. arXiv preprint arXiv:2010.14701 ,\n2020.\nDanny Hernandez, Jared Kaplan, T om Henighan, and Sam McCand lish. Scaling laws for transfer.\narXiv preprint arXiv:2102.01293, 2021.\nZhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better\nrelative position embeddings. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: Findings, pp. 3327–3335, 2020.\nChengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Y ing gong Zhao, Libin Shen, Haofen\nW ang, and Kewei Tu. Learning numeral embeddings. arXiv preprint arXiv:2001.00003, 2019.\nDevin Johnson, Denise Mak, Andrew Barker, and Lexi Loessber g-Zahl. Probing for multilingual\nnumerical understanding in transformer-based language mo dels. In Proceedings of the Third\nBlackboxNLP W orkshop on Analyzing and Interpreting Neural Networks for NLP , pp. 184–192,\n2020.\nArmand Joulin and T omas Mikolov. Inferring algorithmic pat terns with stack-augmented recurrent\nnets. Advances in Neural Information Processing Systems, 28:190–198, 2015.\nŁukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorit hms. arXiv preprint arXiv:1511.08228,\n2015.\nNal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint\narXiv:1507.01526 , 2015.\nGuolin Ke, Di He, and Tie-Y an Liu. Rethinking the positional encoding in language pre-training.\narXiv preprint arXiv:2006.15595, 2020.\nGuillaume Lample and François Charton. Deep learning for sy mbolic mathematics. In International\nConference on Learning Representations, 2019.\nJierui Li, Lei W ang, Jipeng Zhang, Y an W ang, Bing Tian Dai, an d Dongxiang Zhang. Modeling\nintra-relation in math word problems with different functi onal multi-head attentions. In Proceed-\nings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6162–6167,\n2019.\nBill Y uchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Bir ds have four legs?! NumerSense:\nProbing numerical commonsense knowledge of pre-trained la nguage models. arXiv preprint\narXiv:2005.00683 , 2020.\nQianying Liu, W enyv Guan, Sujian Li, and Daisuke Kawahara. T ree-structured decoding for solving\nmath word problems. In Proceedings of the 2019 Conference on Empirical Methods in N atural\nLanguage Processing and the 9th International Joint Confer ence on Natural Language Process-\ning (EMNLP-IJCNLP), pp. 2370–2379, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay re gularization. In International Confer-\nence on Learning Representations, 2018.\nSwaroop Mishra, Arindam Mitra, Neeraj V arshney, Bhavdeep S achdeva, and Chitta Baral. T owards\nquestion format independent numerical reasoning: A set of p rerequisite tasks. arXiv preprint\narXiv:2005.08516 , 2020.\nAakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and E duard Hovy. Exploring numeracy in\nword embeddings. In Proceedings of the 57th Annual Meeting of the Association fo r Computa-\ntional Linguistics, pp. 3374–3380, 2019.\n6\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nBenjamin Newman, John Hewitt, Percy Liang, and Christopher D. Manning. The EOS decision\nand length extrapolation. In Proceedings of the Third BlackboxNLP W orkshop on Analyzingand\nInterpreting Neural Networks for NLP, pp. 276–291, 2020.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Ch ristopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representatio ns. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association forComputational Linguistics: Human\nLanguage T echnologies, V olume 1 (Long P apers), pp. 2227–2237, 2018.\nStanislas Polu and Ilya Sutskever. Generative language mod eling for automated theorem proving.\narXiv preprint arXiv:2009.03393, 2020.\nEric Price, W ojciech Zaremba, and Ilya Sutskever. Extensio ns and limitations of the neural GPU.\narXiv preprint arXiv:1611.00736, 2016.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sh aran Narang, Michael Matena, Y anqi\nZhou, W ei Li, and Peter J. Liu. Exploring the limits of transf er learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\nQiu Ran, Y ankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. NumNe t: Machine reading comprehen-\nsion with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Join t Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2474–2484, 2019.\nAbhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and E duard Hovy. EQUA TE: A benchmark\nevaluation framework for quantitative reasoning in natura l language inference. In Proceedings\nof the 23rd Conference on Computational Natural Language Le arning (CoNLL) , pp. 349–361,\n2019.\nY uanhang Ren and Y e Du. Enhancing the numeracy of word embedd ings: A linear algebraic perspec-\ntive. In CCF International Conference on Natural Language Processing and Chinese Computing,\npp. 170–178. Springer, 2020.\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Willia ms. Learning internal representations\nby error propagation. T echnical report, Institute for Cogn itive Science, University of California,\nSan Diego, 1985.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmee t Kohli. Analysing mathematical rea-\nsoning abilities of neural models. In International Conference on Learning Representations ,\n2018.\nImanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa J ojic, Jürgen Schmidhuber, and Jian-\nfeng Gao. Enhancing the transformer with explicit relation al encoding for math problem solving.\narXiv preprint arXiv:1910.06611, 2019.\nHongjie Shi. A sequence-to-sequence approach for numerica l slot-ﬁlling dialog systems. In Pro-\nceedings of the 21th Annual Meeting of the Special Interest G roup on Discourse and Dialogue ,\npp. 272–277, 2020.\nAlon T almor, Y anai Elazar, Y oav Goldberg, and Jonathan Bera nt. oLMpics—on what language\nmodel pre-training captures. arXiv preprint arXiv:1912.13283, 2019.\nIan T enney, Dipanjan Das, and Ellie Pavlick. BER T rediscove rs the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Association fo r Computational Linguistics , pp.\n4593–4601, 2019.\nA vijit Thawani, Jay Pujara, Pedro A. Szekely, and Filip Ilie vski. Representing numbers in NLP: a\nsurvey and a vision. arXiv preprint arXiv:2103.13136, 2021.\nAndrew Trask, Felix Hill, Scott E. Reed, Jack Rae, Chris Dyer , and Phil Blunsom. Neural arithmetic\nlogic units. In Advances in Neural Information Processing Systems, pp. 8035–8044, 2018.\n7\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you ne ed. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nEric W allace, Y izhong W ang, Sujian Li, Sameer Singh, and Mat t Gardner. Do NLP models know\nnumbers? Probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 5310–5318, 2019.\nBenyou W ang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simon-\nsen. Encoding word order in complex embeddings. In International Conference on Learning\nRepresentations, 2019.\nXikun Zhang, Deepak Ramachandran, Ian T enney, Y anai Elazar , and Dan Roth. Do language em-\nbeddings capture scales? In Proceedings of the Third BlackboxNLP W orkshop on Analyzingand\nInterpreting Neural Networks for NLP, pp. 292–299, 2020.\nY anyan Zou and W ei Lu. Quantity tagger: A latent-variable se quence labeling approach to solving\naddition-subtraction word problems. In Proceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pp. 5246–5251, 2019a.\nY anyan Zou and W ei Lu. T ext2Math: End-to-end parsing text in to math expressions. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing ( EMNLP-IJCNLP), pp. 5330–5340,\n2019b.\n8\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nA R ELATED WORK\nRecent studies have explored the numerical capabilities le arned by neural networks trained on\nlarge amounts of texts (T almor et al., 2019; Jiang et al., 201 9; Naik et al., 2019; W allace et al., 2019;\nLin et al., 2020; Johnson et al., 2020; Mishra et al., 2020). S ee Thawani et al. (2021) for a detailed\nsurvey.\nA common ﬁnding is that the learned embeddings capture magnitude (e.g., 2 < 3), but many mod-\nels fail to capture numeracy (e.g., two=2) (Naik et al., 2019; W allace et al., 2019; Ren & D u, 2020;\nZhang et al., 2020). Character-level models such as ELMO (Pe ters et al., 2018) have stronger nu-\nmeracy than sub-word models such as BER T (Devlin et al., 2019 ), perhaps because two numbers\nthat are similar in value can have very different sub-word to kenizations (W allace et al., 2019). Our\nwork shows that characters are adequate representations fo r small to medium numbers, but they are\nnot sufﬁcient when dealing with large numbers, which requir e precise position representations for\neach digit.\nHowever, independently of the tokenization method, pretra ined word embeddings have trouble\nextrapolating to numbers unseen during training (W allace e t al., 2019). Some alternatives to im-\nprove the extrapolation capabilities of neural models incl ude augmenting pretraining corpora with\nnumerical texts (Geva et al., 2020; Chu et al., 2020) or using scientiﬁc notation to represent num-\nbers (Zhang et al., 2020). Similarly, better numerical skil ls can be achieved by augmenting input\ntexts with pre-computed numerical computations (Andor et a l., 2019) or by explicitly inferring math-\nematical equations from natural language text (Zou & Lu, 201 9a;b; Li et al., 2019; Liu et al., 2019;\nShi, 2020).\nSpecial architectures have also been proposed for arithmet ic tasks (Kaiser & Sutskever, 2015;\nKalchbrenner et al., 2015; Price et al., 2016; Trask et al., 2 018). Many of these models are capable\nof summing numbers larger than the ones seen during training . In contrast, more general-purpose\narchitectures fail to extrapolate on numerical tasks (Joul in & Mikolov, 2015; Dehghani et al., 2018;\nSchlag et al., 2019).\nOthers have proposed neural–symbolic hybrids, which are ty pically composed of a neural model to\nconvert inputs to contiguous vector representations and a s ymbolic component that applies rules over\nthese vectors (Ran et al., 2019). However, a body of evidence has shown that neural networks can\nperform reasoning tasks. For instance, a modern pretrained model with self-attention that uses the\nright level of input representation can outperform neural– symbolic hybrids on artiﬁcial reasoning\ntasks that require answering questions from videos (Ding et al., 2020). Deep learning models were\nalso successfully applied to symbolic integration, to solv e differential equations (Lample & Charton,\n2019), and automated theorem proving (Polu & Sutskever, 202 0).\nFurthermore, it is not clear how architectures specialized to some tasks can be adapted to simultane-\nously perform a range of tasks a human is capable of. Our work i nstead focuses on a general-purpose\narchitecture that can be applied to almost all natural langu age processing tasks.\nNovel ways of encoding positions of tokens in the transforme r architecture have been proposed,\nbut they were mostly evaluated on natural language processi ng tasks, showing small performance\ngains (Ke et al., 2020; He et al., 2020; W ang et al., 2019; Huan g et al., 2020). W e instead expose the\nlimitations of subword tokenizers and positional encoding s using simple arithmetic tasks.\nDatasets such as DROP (Dua et al., 2019), EQUA TE (Ravichande r et al., 2019), or Mathematics\nQuestions (Saxton et al., 2018) test numerical reasoning; t hey contain examples that require com-\nparing, sorting, and performing other complex mathematica l tasks. This work focuses on isolating\nthe failure cases of the transformer architecture by studyi ng how it performs simple arithmetic tasks.\nW e argue that this is a necessary skill to solve more complex r easoning tasks.\nB P OSITION EMBEDDINGS\nHere, we study the impact of various position embeddings on t he addition task. Since pretraining\nfrom scratch is a costly process, we experiment with only sma ll transformer models ﬁne-tuned\nwithout pretraining.\n9\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\n2 3 4 5 6 7 8 9\n0\n0.2\n0.4\n0.6\n0.8\n1\n# of digits\nT est Accuracy\n10 E, P OS -M ASKED , W ITH TGT\n10, P OS -M ASK ED , W ITH TGT\nCHAR , P OS -M ASKE D , W ITH TGT\n10 E, P OS -M ASKED , N O TGT\n10, P OS -M ASK ED , N O TGT\nCHAR , P OS -M ASKE D , N O TGT\n10 E, S INUSOIDAL\n10, S INUSOIDA L\nCHAR , S INUSOIDA L\nFigure 2: Addition accuracy of vanilla transformers with di fferent position encoding methods.\nThe architecture of the transformer follows V aswani et al. ( 2017) except we use 4 layers for the\nencoder and the decoder, respectively. W e look into the effe ct of representation and positional\nencoding on addition from 2 digits to 9 digits. Due to the cost of these experiments, we choose a\nsubset of the representations studied in Section 3: 10 E -BA S E D, 10- BA S E D, and CH A RACT E R .\nThe dataset is split into training and test sets with a ratio o f 9:1. For 3–9 digits addition, we randomly\ngenerate 10,000 samples for the whole dataset. For 2-digit a ddition, we use all of the combinations\nfor every addend a ∈ [10,99], which results in less than 10,000 samples. The models are tr ained for\n55 epochs with a learning rate of 10− 5.\nW e ﬁnd that the original positional encoding in V aswani et al . (2017) fails to learn addition effec-\ntively, as shown in Figure 2. This might be due to the correlat ion introduced by two heterogeneous\nsignals—embedding and absolute positional encoding (Ke et al., 2020). Therefore, we designed a\nposition-wise masked embedding for this task.\nMore speciﬁcally, for an n-digit number whose embedding is e with embedding size d, we will set\ne[u : v] = 1for i − th digit in the number, where u = int( d\nn ) ·(n − i) and v = int( d\nn ) ·(n − i + 1).\nW e set other position embedding values to 0. Note that i follows the “Big-Endian” style (e.g., i = 3\nfor “2” in the number “271”). However, during inference, dig it information is not provided for the\ntarget sequence as we don’t know the exact digit of the decode d number in advance. So, we face a\nformat discrepancy between training and inference. T o inve stigate how this discrepancy will affect\nthe result, we train the model in two different ways—trainin g with target position provided and\ntraining without target position provided (position encod ing for the target is the zero vector). Note\nthat position encoding is provided for the source sequence i n both cases for training and inference;\nposition encoding is not provided for the target sequence du ring inference in both cases. The results\nare shown in Figure 2, labeled as “WITH TGT” and “NO TGT”, resp ectively. W e label our position-\nwise masked embedding as “Pos-Masked”. The original repres entation is called “Sinusoidal”.\nConsistent with previous experiments, 10 E -BA S E D performs best given the same position encoding\nand training strategies. Comparing “WITH TGT” and “NO TGT”, we can see that training with tar-\nget position encoding creates ﬂuctuations among different digits. In general, it performs worse than\ntraining without target position encoding given the same en coding representation. Unsurprisingly,\nunder our experiment setting, whether the target position i s provided is not as important as having\nthe same format between training and inference.\nC E XPERIME NT S ON EXTRAPOLAT IO N\nOne advantage of working with arithmetic tasks is that the ru les to be learned are well deﬁned and\nrelatively simple. Thus, it is easy to verify if models learn ed such rules by evaluating them on\nnumbers that are larger than the ones they were trained on. If successful, such a model would have\nno problem correctly adding or subtracting arbitrarily lon g numbers.\nIn this section, we investigate how models of different size s perform interpolation and extrapolation\ntasks. W e train T5-60M, T5-220M, T5-770M, and T5-3B models o n numbers that are sampled using\nthe “balanced” method. Models are trained 100K iterations u sing batches of 128 examples and a\nlearning rate of 10− 3. W e save checkpoints every 2,000 iterations, and the best ch eckpoint is chosen\nusing a separate validation set of 10,000 examples. The mode ls are evaluated on a test set of 10,000\nexamples with numbers sampled using the “random” method.\n10\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nInterpolation Extrapolation\nOrder: Inverse Regular Inverse Regular\nOperation: Add Sub Add Sub Add Sub Add Sub\nT5-60M 1.000 0.934 0.998 0.830 0.000 0.000 0.004 0.000\nT5-220M 1.000 0.998 1.000 0.995 0.000 0.000 0.862 0.641\nT5-770M 1.000 0.947 0.999 0.982 0.003 0.000 0.442 0.373\nT5-3B 1.000 0.997 1.000 0.993 0.974 0.865 0.988 0.982\nT able 2: Interpolation and extrapolation accuracy. Interp olation refers to training and testing on\nup to 60-digit numbers. Extrapolation refers to training on up to 50-digit numbers and testing on\n60-digit numbers. W e highlight in bold accuracy above 97%.\nFor interpolation experiments, the models are trained and e valuated on up to 60-digit numbers. For\nextrapolation experiments, the models are trained on up to 5 0-digit numbers and evaluated on 60-\ndigit numbers. W e use that many digits for training because t he models could not extrapolate with\nfewer; see more below .\nRegular vs. inverse orders: Auto-regressive models such as the ones used in this work gen erate\nthe output sequence token by token. Thus, to produce the ﬁrst digit of the answer, which is the most\nsigniﬁcant one, the model has to perform all the carry operat ions. In the addition example “What\nis 52 plus 148?”, to produce the ﬁrst digit “2”, the model has t o perform the carry operation for the\nunit digits (2 and 8), and then the carry for the decimal digit s (5 and 4). Hence, the model has to\nperform the digit-wise addition (or subtraction) of all the digits in the question before generating the\nﬁrst digit of the answer. W e call this generation order “regu lar”.\nAnother way to produce an answer is by generating the least si gniﬁcant digits ﬁrst. This order is\nperhaps easier to learn than the “regular” order because to d ecode each digit, the model only needs\nto add (or subtract) single digits and check if the previous d igit-wise operation had a carry. W e call\nthis generation order “inverse”.\nThe results presented in T able 2 show that models of all sizes successfully perform interpolation\ntasks. T wo exceptions are T5-60M on the subtraction tasks, w hich achieve 0.934 and 0.830 accuracy\nfor inverse and regular orders, respectively. Nevertheles s, compared to the extrapolation results,\nthese numbers are high enough to consider them as successful runs.\nOn extrapolation tasks, T5-3B succeeds on almost all of them , whereas smaller models fail more\noften. Even on tasks where T5-220M achieves reasonable accu racy (0.862 and 0.641 on addition\nand subtraction using regular order, respectively), T5-3B outperforms T5-220M by large margins.\nThis result provides evidence that larger models might perf orm better on data whose distribution is\noutside its training data distribution. However, it remain s to be investigated if this trend holds for\nmore complex tasks, especially those involving natural lan guage.\nThe difference in accuracy is negligible between regular an d inverse orders on interpolation tasks.\nHowever, models trained and evaluated on the regular order s how higher extrapolation accuracy\nthan those that use the inverse order. For example, T5-220M f ails to extrapolate on both addition\nand subtraction tasks when using the inverse order (i.e., ac curacy is zero), but it performs better\nwhen using the regular order, with accuracy between 60–90%. This result is perhaps surprising\nsince one would expect that the inverse order would be easier to learn.\nSupported by recent work, we suspect that the problem is rela ted to the bias of selecting the termi-\nnation (i.e., end-of-sequence) token when the generated se quence becomes longer than those seen\nduring training (Newman et al., 2020). In the inverse order, the answer is generated from least to\nmost signiﬁcant digit, so the model might have a tendency to s elect the termination token right after\nit generates the most signiﬁcant digit seen during training . In the regular order, however, the model\nhas to predict the full length of the sequence before emittin g the ﬁrst and second tokens. For ex-\nample, the ﬁrst two tokens of the answer to the question 1060 + 1060 are “2” and “10e60”. This\nexplicit length prediction allows the model to better gener alize to longer sequences, but it appears to\nbe insufﬁcient to induce models to learn addition rules that are independent of the length of numbers\nseen during training (more below).\n11\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nW e observe high variance in accuracy for the extrapolation e xperiments. For example, during the\ntraining of a T5-770M model on up to 30-digit numbers, the acc uracy ranges from 20% to 50% when\nevaluated on 60-digit numbers. Extrapolation accuracy als o oscillates between 20–40 percentage\npoints when changing the seed for training data generation.\nExtrapolation is hardly achieved when trained on fewer than 50 digits, regardless of the model size.\nFor example, T5-220M, T5-770M, and T5-3B trained on 15 digit s show an accuracy of zero when\nevaluated on 20 digits.\nBeyond a critical amount, increasing the training data does not improve extrapolation accuracy. For\nexample, when trained on up to 30-digit and evaluated on 60-d igit numbers, a T5-770M showed\na similar accuracy range (20%–50%) when trained with either 100K, 1M, or 10M examples. As\ntraining progresses, interpolation accuracy always reach es 100%, but extrapolation accuracy starts\nto decrease after some number of training steps. The number o f training steps after which this drop\noccurs varies dramatically between runs that differ only in the seed used to generate the training\ndata. W e are unable to isolate the cause of this behavior.\nContrary to the hypothesis of Newman et al. (2020), we ﬁnd tha t the end-of-sequence token does\nnot seem to be the cause of extrapolation failures. For examp le, when a T5-770M model trained on\n30-digit numbers is evaluated on 60-digit numbers, it corre ctly generates the ﬁrst 23 position tokens\n(i.e., from “10e60” until “10e38”) but it suddenly skips to p osition token “10e27”, and continues\ngenerating the correct position tokens until the last one (“ 10e0”). Here we show one such sequence:\n1 10e60 0 10e59 1 10e58 2 10e57 3 10e56 0 10e55 2 10e54 7 10e53 0 10 e52\n1 10e51 0 10e50 3 10e49 9 10e48 0 10e47 5 10e46 3 10e45 1 10e44 5 10 e43 3\n10e42 6 10e41 3 10e40 6 10e39 0\n10e38 8 10e27 1 10e26 4 10e25 1 10e24 2 10e23\n6 10e22 6 10e21 9 10e20 5 10e19 3 10e18 4 10e17 8 10e16 3 10e15 8 10 e14 8\n10e13 9 10e12 5 10e11 3 10e10 5 10e9 0 10e8 6 10e7 4 10e6 3 10e5 5 10 e4 6\n10e3 7 10e2 2 10e1 2 10e0\nHence, although the model correctly emits the end-of-seque nce token after the “10e0” token, it\ndecides to shorten the sequence in the middle of the generati on, i.e., by skipping position tokens\n“10e37” until “10e28”. This skipping behavior is consisten t across model sizes, dataset sizes, and\nextrapolation ranges (e.g., training on 20 digits, evaluat ing on 30 digits, etc.). Investigating it further\nmight help us understand why neural models often fail on extr apolation tasks.\nD I MPACT OF DATA SIZE\nIn Section 3, we show that the choice of orthography has a larg e impact on the addition task when\ntraining data is scarce (i.e., 1,000 training examples). In this section, we investigate how these\nrepresentations perform with varying amounts of training d ata. W e train and evaluate T5-220M on\nthe addition task of up to 30-digit numbers using the regular order. Due to the high computational\ncost of training this model on millions of examples, we reduc e the number of epochs depending on\nthe dataset size, which is detailed in T able 3. W e select the b est checkpoint using a validation set of\n10,000 examples and evaluate the models on a test set of 10,00 0 examples.\nSize Epochs\n103 200\n104 100\n105 20\n106 10\n107 1\nT able 3: Number of training epochs for each dataset size pres ented in Figure 3.\nResults are shown in Figure 3. The 10 E -BA S E D representation presents the best results for training\nsizes of 1,000 and 10,000 examples, followed by 10- BA S E D, WO RD S , U N D E RS CO RE , CH A RACT E R ,\nand D E CIM A L . For larger datasets such as 10M examples, almost all repres entations achieve more\n12\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nthan 99.9% accuracy. The exception is the D E CIM A L representation, which still has a high error of\n2.1% even when trained with 10M examples.\nW e conclude that with enough training data, models can learn the addition task regardless of the\nrepresentation. The limitations of some representations a re exposed only when training data is\nsmall.\n1,000 10 ,000 100 ,000 1 ,000,000 10 ,000,0000\n0.2\n0.4\n0.6\n0.8\n1\n# Training examples\nT est Accuracy\n10 E -BA S E D “3 10e1 2 10e0”\n10- BA S E D “3 10 2”\nWO R D S “thirty-two”\nU N D E R S C O R E “3_2”\nC H A R AC T E R “3 2”\nD E C I M A L “32”\nFigure 3: Accuracy of different number representations whe n varying the amount of training exam-\nples. The task is addition of 30-digit numbers.\nE P RETRAI NE D VS . F ROM SCRATCH MODELS\nOne hypothesis for the high interpolation accuracy reporte d in Section 3 despite using a small num-\nber of training examples is that the model has already seen ad dition and subtraction examples during\npretraining. T o test this hypothesis, we compare pretraine d models with models trained from scratch\n(i.e., no pretraining on the masked language modeling task) on the addition task. In this experiment,\nthe models never see the same training example more than once . That is, they are not limited by\ntraining data.\nFigure 4 shows that both pretrained T5-220M and T5-3B need ap proximately ten times fewer train-\ning examples (and compute) than models trained from scratch to reach 100% accuracy on the addi-\ntion of 60-digit numbers.\n0.2 0 .4 1 2 4 80\n0.2\n0.4\n0.6\n0.8\n1\nMillions of examples seen during training (log scale)\nT est Accuracy\nT5-220M, P RE T RA IN E D\nT5-3B, P RE T RA IN E D\nT5-220M, F RO M S CRAT CH\nT5-3B, F RO M S CRAT CH\nFigure 4: Accuracy of pretrained models vs. from scratch mod els with respect to the number of\ntraining examples. Models are trained and evaluated on numb ers with up to 60 digits in length.\nF A CCURAC Y ON DIFFERENT BASES\nHere we propose another way to test how pretraining can impac t a model’s ability to learn arith-\nmetic. W e hypothesize that a model might have difﬁculty lear ning bases different than base 10 (i.e.,\n13\n1st Mathematical Reasoning in General Artiﬁcial Intelligence W orkshop, ICLR 2021.\nT est Accuracy\nBase From Scratch Pretrained\n2 0.000 ± 0.000 0.999 ± 0.001\n3 0.000 ± 0.000 0.999 ± 0.002\n10 0.000 ± 0.000 0.993 ± 0.003\n19 0.000 ± 0.000 0.976 ± 0.007\nT able 4: T est set accuracy of 15-digit addition on various ba ses. Numbers are represented with\n10 E -BA S E D orthography.\ndecimal) because examples rarely occur in the pretraining c orpus. T o test this hypothesis, we train\na T5-220M model on addition examples using binary, ternary, decimal, and base 19. While there\nmight be examples of binary addition in the pretraining corp us, our expectation is that it contains\nfew (if any?) examples of addition using base 19 numbers. W e u se the 10 E -BA S E D orthography and\ninverse order due to its slightly better accuracy (see T able 2). W e also evaluate models trained from\nscratch.\nW e report the mean accuracy and 95% conﬁdence intervals of a m odel trained with ﬁve different\nsets of 1,000 addition examples for 100 epochs. A separate de velopment set of 1,000 examples was\nused to select the best checkpoint of each run. W e trained and evaluated on numbers equivalent to\n15 decimal digits.\nFor these experiments, we use only 1,000 training examples s ince experiments in Appendix D show\nthat models can successfully learn with enough training dat a, thus too much data defeats the pur-\npose of measuring the impact of pretraining; see also Hernan dez et al. (2021). Results are shown\nin T able 4. The pretrained model has no problem learning bina ry, ternary, and decimal bases, but\nits accuracy degrades slightly on base 19. Since it is unlike ly that the pretrained model has encoun-\ntered substantial numbers of examples of addition in rare ba ses (i.e., ternary and 19), it seems that\npretraining helps on this task in other ways than simple memo rization.\nT o show that the task is not easy, we also report in the table th at models trained from scratch fail\nto learn the task regardless of the base. This result is expec ted since a large number of parameters\n(220M) need to be learned from scratch using just 1,000 examp les.\nG I MPACT OF DIFFERENT LENGTH DISTRIBUT IO NS\nHere we investigate to what extent a mismatch between the len gth distribution of training and test\nsets is problematic for the addition task. W e train T5-220M m odels on 100,000 examples, select\nthe best checkpoint using a development set of 10,000 exampl es, and evaluate on another 10,000\nexamples. Here we use the regular order. Training and test se ts are generated using either the\nbalanced or random sampling methods described in Section 2.\nResults are shown in T able 5. When trained on the balanced dis tribution, the model succeeds on\nboth random and balanced evaluation sets. When trained on th e random distribution, it succeeds on\nthe random evaluation set, but it fails on the balanced evalu ation set. In other words, when trained\non data where most numbers (i.e., 90%) have 60 digits, it does not learn to add numbers with fewer\ndigits. This shows that models have problems performing add ition of sequences shorter than the\nones seen during training. This is complementary to the resu lts presented in Appendix C, which\nshows that models cannot generate examples longer than the o nes seen during training.\nT est\nBalanced Random\nTrain Balanced 1.000 1.000\nRandom 0.014 1.000\nT able 5: Accuracy on 60-digit addition, with balanced and ra ndom sampling as described in Sec-\ntion 2.\n14",
  "topic": "Arithmetic",
  "concepts": [
    {
      "name": "Arithmetic",
      "score": 0.7356722354888916
    },
    {
      "name": "Transformer",
      "score": 0.5722526907920837
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5589940547943115
    },
    {
      "name": "Computer science",
      "score": 0.4636012613773346
    },
    {
      "name": "Mathematics",
      "score": 0.33481794595718384
    },
    {
      "name": "Electrical engineering",
      "score": 0.1439356505870819
    },
    {
      "name": "Engineering",
      "score": 0.1283450424671173
    },
    {
      "name": "Epistemology",
      "score": 0.10433852672576904
    },
    {
      "name": "Philosophy",
      "score": 0.08234763145446777
    },
    {
      "name": "Voltage",
      "score": 0.0755394697189331
    }
  ]
}