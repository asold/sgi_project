{
    "title": "Can large language models provide secondary reliable opinion on treatment options for dermatological diseases?",
    "url": "https://openalex.org/W4393994738",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2111317264",
            "name": "Usman Iqbal",
            "affiliations": [
                "Taipei Medical University",
                "University of Tasmania",
                "UNSW Sydney",
                "Government of Western Australia Department of Health"
            ]
        },
        {
            "id": "https://openalex.org/A5037904618",
            "name": "Leon Tsung‐Ju Lee",
            "affiliations": [
                "Taipei Medical University Hospital",
                "Taipei Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2572715367",
            "name": "Annisa Ristya Rahmanti",
            "affiliations": [
                "Taipei Medical University",
                "Universitas Gadjah Mada"
            ]
        },
        {
            "id": "https://openalex.org/A2116171563",
            "name": "Leo Anthony Celi",
            "affiliations": [
                "Beth Israel Deaconess Medical Center",
                "Harvard University",
                "Massachusetts Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2682987909",
            "name": "Yu-Chuan (Jack) Li",
            "affiliations": [
                "Taipei Medical University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4361298490",
        "https://openalex.org/W4319301633",
        "https://openalex.org/W4377010360",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4323350039",
        "https://openalex.org/W4367174612",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4319663047",
        "https://openalex.org/W4378214315",
        "https://openalex.org/W2045345984",
        "https://openalex.org/W4367175150",
        "https://openalex.org/W3161195140",
        "https://openalex.org/W6850507425",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4383346782",
        "https://openalex.org/W4367186868",
        "https://openalex.org/W4379231355",
        "https://openalex.org/W4372231834",
        "https://openalex.org/W4367175039",
        "https://openalex.org/W4366769280",
        "https://openalex.org/W4387829826",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W4321351832"
    ],
    "abstract": "Abstract Objective To investigate the consistency and reliability of medication recommendations provided by ChatGPT for common dermatological conditions, highlighting the potential for ChatGPT to offer second opinions in patient treatment while also delineating possible limitations. Materials and Methods In this mixed-methods study, we used survey questions in April 2023 for drug recommendations generated by ChatGPT with data from secondary databases, that is, Taiwan’s National Health Insurance Research Database and an US medical center database, and validated by dermatologists. The methodology included preprocessing queries, executing them multiple times, and evaluating ChatGPT responses against the databases and dermatologists. The ChatGPT-generated responses were analyzed statistically in a disease-drug matrix, considering disease-medication associations (Q-value) and expert evaluation. Results ChatGPT achieved a high 98.87% dermatologist approval rate for common dermatological medication recommendations. We evaluated its drug suggestions using the Q-value, showing that human expert validation agreement surpassed Q-value cutoff-based agreement. Varying cutoff values for disease-medication associations, a cutoff of 3 achieved 95.14% accurate prescriptions, 5 yielded 85.42%, and 10 resulted in 72.92%. While ChatGPT offered accurate drug advice, it occasionally included incorrect ATC codes, leading to issues like incorrect drug use and type, nonexistent codes, repeated errors, and incomplete medication codes. Conclusion ChatGPT provides medication recommendations as a second opinion in dermatology treatment, but its reliability and comprehensiveness need refinement for greater accuracy. In the future, integrating a medical domain-specific knowledge base for training and ongoing optimization will enhance the precision of ChatGPT’s results.",
    "full_text": null
}