{
  "title": "Accurate predictions on small data with a tabular foundation model",
  "url": "https://openalex.org/W4406170795",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4227125062",
      "name": "Hollmann, Noah",
      "affiliations": [
        "Berlin Institute of Health at Charité - Universitätsmedizin Berlin",
        "University of Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A2747286929",
      "name": "Muller Samuel",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A4383178143",
      "name": "Purucker, Lennart",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A4225868796",
      "name": "Krishnakumar, Arjun",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": null,
      "name": "Körfer, Max",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": null,
      "name": "Hoo, Shi Bin",
      "affiliations": [
        "University of Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A4209981128",
      "name": "Schirrmeister, Robin Tibor",
      "affiliations": [
        "University of Freiburg",
        "University Medical Center Freiburg"
      ]
    },
    {
      "id": "https://openalex.org/A3114371693",
      "name": "Hutter, Frank",
      "affiliations": [
        "University of Freiburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202428668",
    "https://openalex.org/W6801744293",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2766447205",
    "https://openalex.org/W6745620495",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4311726128",
    "https://openalex.org/W4387947637",
    "https://openalex.org/W6853165500",
    "https://openalex.org/W4247692122",
    "https://openalex.org/W2143891888",
    "https://openalex.org/W4390833695",
    "https://openalex.org/W1973137995",
    "https://openalex.org/W4233045210",
    "https://openalex.org/W6841055480",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W3011307848",
    "https://openalex.org/W28412257",
    "https://openalex.org/W1980264541",
    "https://openalex.org/W4384648089",
    "https://openalex.org/W6926258385",
    "https://openalex.org/W6963984227",
    "https://openalex.org/W4306175122",
    "https://openalex.org/W4392011633",
    "https://openalex.org/W6870265164",
    "https://openalex.org/W4386076484",
    "https://openalex.org/W1857789879",
    "https://openalex.org/W1992021819",
    "https://openalex.org/W4236137412",
    "https://openalex.org/W1676820704",
    "https://openalex.org/W2150446468",
    "https://openalex.org/W4234698323",
    "https://openalex.org/W3216660278",
    "https://openalex.org/W2132862423",
    "https://openalex.org/W1967320885"
  ],
  "abstract": "Abstract Tabular data, spreadsheets organized in rows and columns, are ubiquitous across scientific fields, from biomedicine to particle physics to economics and climate science 1,2 . The fundamental prediction task of filling in missing values of a label column based on the rest of the columns is essential for various applications as diverse as biomedical risk models, drug discovery and materials science. Although deep learning has revolutionized learning from raw data and led to numerous high-profile success stories 3–5 , gradient-boosted decision trees 6–9 have dominated tabular data for the past 20 years. Here we present the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model that outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time. In 2.8 s, TabPFN outperforms an ensemble of the strongest baselines tuned for 4 h in a classification setting. As a generative transformer-based foundation model, this model also allows fine-tuning, data generation, density estimation and learning reusable embeddings. TabPFN is a learning algorithm that is itself learned across millions of synthetic datasets, demonstrating the power of this approach for algorithm development. By improving modelling abilities across diverse fields, TabPFN has the potential to accelerate scientific discovery and enhance important decision-making in various domains.",
  "full_text": "Nature | Vol 637 | 9 January 2025 | 319\nArticle\nAccurate predictions on small data with a \ntabular foundation model\nNoah Hollmann1,2,3,7 ✉, Samuel Müller1,7 ✉, Lennart Purucker1, Arjun Krishnakumar1, \nMax Körfer1, Shi Bin Hoo1, Robin Tibor Schirrmeister4,5 & Frank Hutter1,3,6 ✉\nTabular data, spreadsheets organized in rows and columns, are ubiquitous across \nscientific fields, from biomedicine to particle physics to economics and climate \nscience1,2. The fundamental prediction task of filling in missing values of a label \ncolumn based on the rest of the columns is essential for various applications as \ndiverse as biomedical risk models, drug discovery and materials science. Although \ndeep learning has revolutionized learning from raw data and led to numerous \nhigh-profile success stories3–5, gradient-boosted decision trees6–9 have dominated \ntabular data for the past 20 years. Here we present the Tabular Prior-data Fitted \nNetwork (TabPFN), a tabular foundation model that outperforms all previous \nmethods on datasets with up to 10,000 samples by a wide margin, using substantially \nless training time. In 2.8 s, TabPFN outperforms an ensemble of the strongest \nbaselines tuned for 4 h in a classification setting. As a generative transformer-based \nfoundation model, this model also allows fine-tuning, data generation, density \nestimation and learning reusable embeddings. TabPFN is a learning algorithm that is \nitself learned across millions of synthetic datasets, demonstrating the power of this \napproach for algorithm development. By improving modelling abilities across diverse \nfields, TabPFN has the potential to accelerate scientific discovery and enhance \nimportant decision-making in various domains.\nThroughout the history of artificial intelligence, manually created \nalgorithmic components have been replaced with better-performing \nend-to-end learned ones. Hand-designed features in computer vision, \nsuch as SIFT (Scale Invariant Feature Transform)10 and HOG (Histogram \nof Oriented Gradients)11, have been replaced by learned convolutions; \ngrammar-based approaches in natural language processing have been \nreplaced by learned transformers12; and the design of customized open-\ning and end-game libraries in game playing has been superseded by \nend-to-end learned strategies 3,13. Here we extend this end-to-end  \nlearning to the ubiquitous domain of tabular data.\nThe diversity of tabular data sets them apart from unprocessed \nmodalities such as text and images. While in language modelling for \nexample the meaning of a word is consistent across documents, in \ntabular datasets the same value can mean fundamentally different \nthings. A drug discovery dataset, for example, might record chemical \nproperties, whereas another dataset in materials science might docu-\nment thermal and electric properties. This specialization leads to a \nproliferation of smaller, independent datasets and associated models. \nT o illustrate, on the popular tabular benchmarking website openml.org, \n76% of the datasets contain less than 10,000 rows at the time of writing.\nDeep learning methods have traditionally struggled with tabular \ndata, because of the heterogeneity between datasets and the heteroge-\nneity of the raw data itself: Tables contain columns, also called features, \nwith various scales and types (Boolean, categorical, ordinal, integer, \nfloating point), imbalanced or missing data, unimportant features, \noutliers and so on. This made non-deep-learning methods, such as \ntree-based models, the strongest contender so far14,15.\nHowever, these traditional machine learning models have sev -\neral drawbacks. Without substantial modifications, they yield poor \nout-of-distribution predictions and poor transfer of knowledge from \none dataset to another16. Finally, they are hard to combine with neural \nnetworks, as they do not propagate gradients.\nAs a remedy, we introduce TabPFN, a foundation model for small- \nto medium-sized tabular data. This new supervised tabular learning \nmethod can be applied to any small- to moderate-sized dataset and \nyields dominant performance for datasets with up to 10,000 samples \nand 500 features. In a single forward pass, TabPFN significantly out-\nperforms state-of-the-art baselines on our benchmarks, including \ngradient-boosted decision trees, even when these are allowed 4 h of \ntuning, a speedup of 5,140× (classification) and 3,000× (regression). \nFinally, we demonstrate various foundation model characteristics \nof TabPFN, including fine-tuning, generative abilities and density  \nestimation.\nPrincipled in-context learning\nTabPFN leverages in-context learning (ICL)17, the same mechanism \nthat led to the astounding performance of large language models, to \nhttps://doi.org/10.1038/s41586-024-08328-6\nReceived: 17 May 2024\nAccepted: 31 October 2024\nPublished online: 8 January 2025\nOpen access\n Check for updates\n1Machine Learning Lab, University of Freiburg, Freiburg, Germany. 2Computational Medicine, Berlin Institute of Health at Charité, Universitätsmedizin Berlin, Berlin, Germany. 3Prior Labs, \nFreiburg, Germany. 4Neuromedical AI Lab, Department of Neurosurgery, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany. 5Medical \nPhysics, Department of Diagnostic and Interventional Radiology, Medical Center - University of Freiburg, Faculty of Medicine, University of Freiburg, Freiburg, Germany. 6ELLIS Institute \nTübingen, Tübingen, Germany. 7These authors contributed equally: Noah Hollmann, Samuel Müller. ✉e-mail: noah@priorlabs.ai; samuelgabrielmuller@gmail.com; fh@cs.uni-freiburg.de\n320 | Nature | Vol 637 | 9 January 2025\nArticle\ngenerate a powerful tabular prediction algorithm that is fully learned. \nAlthough ICL was first observed in large language models, recent \nwork has shown that transformers can learn simple algorithms \nsuch as logistic regression through ICL 18–21. Prior-data Fitted Net -\nworks (PFNs) have shown that even complex algorithms, such as \nGaussian Processes and Bayesian Neural Networks, can be approxi-\nmated with ICL 22. ICL enables us to learn a wider space of possible \nalgorithms, including cases for which a closed-form solution does  \nnot exist.\nWe build on a preliminary version of TabPFN23, which demonstrated \nthe applicability of in-context-learning17 for tabular data in principle \nbut had many limitations that rendered it inapplicable in most cases. \nBased on a series of improvements, the new TabPFN scales to 50× larger \ndatasets; supports regression tasks, categorical data and missing  \nvalues; and is robust to unimportant features and outliers.\nThe key idea behind TabPFN is to generate a large corpus of synthetic \ntabular datasets and then train a transformer-based12 neural network \nto learn to solve these synthetic prediction tasks. Although traditional \napproaches require hand-engineered solutions for data challenges \nsuch as missing values, our method autonomously learns effective \nstrategies by solving synthetic tasks that include these challenges. This \napproach leverages ICL as a framework for exemplar-based declarative \nprogramming of algorithms. We design desired algorithmic behaviour \nby generating diverse synthetic datasets that demonstrate the desired \nbehaviour and then train a model to encode an algorithm that satisfies \nit. This shifts the algorithm design process from writing explicit instruc-\ntions to defining input–output examples, opening up possibilities for \ncreating algorithms in various domains. Here, we apply this approach \nto the high-impact field of tabular learning, generating a powerful \ntabular prediction algorithm.\nOur ICL approach differs fundamentally from standard super -\nvised deep learning. Usually, models are trained per dataset, upd-\nating model parameters on individual samples or batches according \nto hand-crafted weight-updating algorithms, such as Adam 24.  \nAt inference time, the learned model is applied to test samples. By \ncontrast, our approach is trained across datasets and is applied to \nentire datasets at inference time rather than individual samples. Before \nbeing applied to real-world datasets, the model is once pre-trained \non millions of synthetic datasets representing different prediction \ntasks. At inference time, the model receives an unseen dataset with \nboth labelled training and unlabelled test samples and performs \ntraining and prediction on this dataset in a single neural network  \nforward pass.\nFigures 1 and 2 outline our approach:\n1. Data generation: we define a generative process (referred to as our \nprior) to synthesize diverse tabular datasets with varying relation-\nships between features and targets, designed to capture a wide range \nof potential scenarios that our model might encounter. We sample \nmillions of datasets from the generative process. For each dataset, \na subset of samples has their target values masked, simulating a \nsupervised prediction problem. Further details of our prior design \nare shown in the section ‘Synthetic data based on causal models’ .\n2. Pre-training: we train a transformer model, our PFN, to predict the \nmasked targets of all synthetic datasets, given the input features \nand the unmasked samples as context. This step is done only once \nduring model development, learning a generic learning algorithm \nthat can be used to predict any dataset.\n3. Real-world prediction: the resulting trained model can now be  \napplied to arbitrary unseen real-world datasets. The training samples \nare provided as context to the model, which predicts the labels of \nthese unseen datasets through ICL.\nOur approach also has a theoretical foundation as described in \nref. 22. It can be viewed as approximating Bayesian prediction for a \nprior defined by the synthetic datasets. The trained PFN will approxi-\nmate the posterior predictive distribution p(, ,)test test train train̂ ∣yX Xy   and \nthus return a Bayesian prediction for the specified distribution over \nartificial datasets used during PFN pre-training.\na TabPFN is trained on synthetic data to take entire\ndatasets as inputs and predict in a forward pass\nTabPFN can now be applied to arbitrary\nunseen real-world datasets\nXtrain\nXtest\nytrain\n?\nTabPFN\nneural network\nparameterized by T\n–log qT (ytest |...)\nTraining loss to be optimized\nacross millions of datasets\n?\nTabPFN\nAn arbitrary real-world dataset\nb\n1.2\n8.9\n1.0\n33.3\nx1 x2\n6.1\n9.1\n2.9\n2.2\n3.0\n3.1\n6.7\n?\ny\nTraining rowsTest\nWe predict this entry\nInput dataset\nEach node represents one entry in the table\n1D feature attention 1D sample attention MLP\n2D TabPFN layer (12×)\n0 5 10\nPredicted y distribution\nDensity\nThe vector is transformed\nto a piece-wise constant\n(Riemann) distribution\nwith an MLP\nA synthetic dataset\nPrediction Prediction\nytest\nXtrain\nXtest\nytrain\nPredictions: ˆ testy\nFig. 1 | Overview of the proposed method. a, The high-level overview of TabPFN \npre-training and usage. b , The TabPFN architecture. We train a model to solve \nmore than 100 million synthetic tasks. Our architecture is an adaptation of the \nstandard transformer encoder that is adapted for the two-dimensional data \nencountered in tables.\nNature | Vol 637 | 9 January 2025 | 321\nAn architecture designed for tables\nThe transformer architecture is currently the favoured architecture for \nflexible deep learning and foundation models4,5. Transformer models \nwork on sequences and combine information between sequence items \nusing so-called attention mechanisms25, allowing them to effectively \ncapture long-range dependencies and learn complex relationships in \ndata. Although transformer-based models can be applied to tabular \ndata26,27, TabPFN addresses two key limitations inherent to them. First, \nas transformers are designed for sequences, they treat the input data \nas a single sequence, not using the tabular structure. Second, machine \nlearning models are often used in a fit-predict model, in which a model \nis fitted on the training set once and then reused for multiple test data-\nsets. Transformer-based ICL algorithms, however, receive train and \ntest data in a single pass and thus perform training and prediction at \nonce. Thus, when a fitted model is reused, it has to redo computations \nfor the training set.\nT o better use the tabular structure, we propose an architecture that \nassigns a separate representation to each cell in the table, inspired \nby refs. 22,28. Our architecture, visualized in Fig. 1b, uses a two-way \nattention mechanism, with each cell attending to the other features \nin its row (that is, its sample) and then attending to the same feature \nacross its column (that is, all other samples). This design enables the \narchitecture to be invariant to the order of both samples and features \nand enables more efficient training and extrapolation to larger tables \nthan those encountered during training, in terms of both the number \nof samples and features.\nT o mitigate repeating computations on the training set for each test \nsample in a fit-predict setting, our model can separate the inference \non the training and test samples. This allows us to perform ICL on the \ntraining set once, save the resulting state and reuse it for multiple test \nset inferences. On datasets with 10,000 training samples and 10 fea-\ntures, our optimized train-state caching results in inference speedups of \naround 300× on CPU (from 32 s to 0.1 s) and 6× on GPU. With 10× more \nfeatures (100), the speedups increase to 800× on CPU and 30× speedup \non GPU. These measurements focus solely on the core inference pro-\ncess, excluding pre-processing and ensembling steps detailed in the \nsection ‘Inference details’ . The lower speedups on GPUs are because of \nan underutilization of their massively parallel architecture.\nWe further optimize the memory and compute requirements of the \narchitecture by computing layer norms in half-precision, using flash \nattention29, activation checkpointing and sequential computation of \nthe state. Our optimizations reduce the memory requirements by a \nfactor of four, resulting in less than 1,000 bytes per cell. This enables \nthe prediction on datasets with up to 50 million cells (for example,  \n5 million rows × 10 features) on a single H100 GPU.\nFor regression tasks, we use a piece-wise constant output distribu-\ntion, following refs. 22,30, which allows our models to predict a prob-\nability distribution of target values instead of a single value, including, \nfor example, bimodal distributions.\nSynthetic data based on causal models\nThe performance of TabPFN relies on generating suitable synthetic \ntraining datasets that capture the characteristics and challenges of \nreal-world tabular data. T o generate such datasets, we developed an \napproach based on structural causal models (SCMs)31. SCMs provide a \nformal framework for representing causal relationships and generative \nprocesses underlying the data. By relying on synthetic data instead \nof large collections of public tabular data, we avoid common prob-\nlems of foundational models, such as privacy and copyright infringe-\nments, contaminating our training data with test data32 or limited data  \navailability.\nAs shown in Fig. 2, our generative pipeline first samples high-level \nhyperparameters, such as dataset size, number of features and diffi-\nculty level, to govern the overall properties of each synthetic dataset. \nb\nFor each generated sample,\npropagate initialization data\nthrough the graph\n1 Sample random feature (F)\nand target (T) node positions, and \nread off data at those positions\n2\n3\nPostprocessing,\nquantization and\nwarping\n4\na c\nSample number of data points\nSample underlying parameters Build computational graph and graph structure Final datasets\nSample number of features\nSample number of nodes\nSample graph complexity\nSample graph\nF\nDiscretization\nConnection types\nT\nF\nNeural network Tree\nFig. 2 | Overview of the TabPFN prior. a, For each dataset, we first sample \nhigh-level hyperparameters. b , Based on these hyperparameters, we construct \na structural causal model that encodes the computational function generating \nthe dataset. Each node holds a vector and each edge in the computational \ngraph implements a function according to one of the connection types. In step 1, \nusing random noise variables we generate initialization data, which is fed into \nthe root nodes of the graphs and propagated through the computational graph \nfor each to-be-generated sample. In step 2, we randomly sample feature and \ntarget node positions in the graph, labelled F and T, respectively. In step 3,  \nwe extract the intermediate data representations at the sampled feature and \ntarget node positions. In step 4, we post-process the extracted data. c,  We \nretrieve the final datasets. We plot interactions of feature pairs and the node \ncolour represents the class of the sample.\n322 | Nature | Vol 637 | 9 January 2025\nArticle\nGuided by these hyperparameters, we construct a directed acyclic \ngraph specifying the causal structure underlying the dataset.\nT o generate each sample within a dataset, we propagate randomly \ngenerated noise, called our initialization data, through the root nodes \nof the causal graph. This initialization data are generated by sampling \nfrom a random normal or uniform distribution with varying degrees \nof non-independence between samples, see section ‘Initialization \ndata sampling’ . As these data traverse the edges of the computational \ngraph, we apply a diverse set of computational mappings: small \nneural networks with linear or nonlinear activations (for example, \nsigmoid, ReLU (rectified linear unit), modulo, sine), discretization \nmechanisms for generating categorical features and decision tree \nstructures to encode local, rule-based dependencies. At each edge, \nwe add Gaussian noise, introducing uncertainty into the generated \ndata. We save the intermediate data representations at each node \nto be retrieved later. See section ‘Computational edge mappings’  \nfor details.\nAfter traversing the causal graph, we extract the intermediate repre-\nsentations at the sampled feature and target nodes, yielding a sample \nconsisting of feature values and an associated target value.\nBy incorporating various data challenges and complexities into the \nsynthetic datasets, we create a training ground that allows TabPFN to \ndevelop strategies for handling similar issues in real-world datasets. \nFor instance, consider the case of missing values, commonly present \nin tabular data. By exposing TabPFN to synthetic datasets with varying \npatterns and fractions of missing values in our synthetic data genera-\ntion process, the model learns effective ways of handling missing val-\nues that generalize to real-world datasets. We apply post-processing \ntechniques to further enhance the realism and challenge the robustness \nof the learned prediction algorithms. This includes warping with the \nKumaraswamy distribution33, introducing complex nonlinear distor-\ntions and quantization mimicking discretized features. See section \n‘Post-processing’ for details.\nThrough this generative process, we created a massive corpus of \naround 100 million synthetic datasets per model training, each with a \nunique causal structure, feature types and functional characteristics.\nQualitative analysis\nWe first analyse the behaviour of TabPFN on toy problems to build \nintuition and disentangle the impact of various dataset characteristics. \nAs regression problems are easier to visualize, we focus on these in our \nqualitative analysis. In Fig. 3a, we compare TabPFN with a diverse set of \nstandard predictors, with all methods using default settings.\nLinear (ridge) regression can naturally model only linear functions, \nleading to simple and interpretable predictions but catastrophic failure \non many of the toy functions. Multilayer perceptrons (MLPs)34 perform \nworse on datasets with highly non-smooth patterns14. This is especially \napparent for the step function. TabPFN, by contrast, models either \nfunction type, smooth or non-smooth, out of the box. This includes \na good approximation to step functions despite TabPFN being a neu-\nral network. CatBoost9, representative of tree-based methods, fits \nonly piece-wise constant functions. Although this leads to approxi-\nmation errors and unintuitive predictions, it avoids catastrophic  \nfailures.\nThe main advantage of TabPFN over all baselines is its inherent abil-\nity to model uncertainty at no extra cost. Whereas classical regression \nmethods output a single real-valued prediction, TabPFN returns a target \ndistribution, capturing the uncertainty of predictions. These uncer-\ntainty modelling abilities of TabPFN extend beyond simple distribu-\ntions and can handle complex, multi-modal distributions. Figure 3b \nshows this by modelling the density of light reaching a detector screen \nin a double-slit experiment35 for different slit distances and widths. In \nthis classic experiment, photons are sent through two slits creating a \nmulti-modal intensity pattern because of the wave-like interference \nbehaviour of light. TabPFN predicts these intricate patterns in just \na single forward pass, requiring only 1.2 s. By contrast, traditional \nmethods such as CatBoost require training multiple quantile models \nat different quantiles and reconstructing the distribution from these \npredictions. Even after tuning CatBoost specifically for this task, it \nproduced substantially worse predictions compared with TabPFN, \nsee Fig. 3b. With default settings, CatBoost requires 169.3 s and yields \nfurther deteriorated results. Qualitatively, we observe that TabPFN is \nTrue functionTabPFNCatBoostMLPLinear\n0.5\n0\n–0.5\n0.5\n0\n–0.5\n0.5\n0\n–0.5\n0.5\n0\n–0.5\n0.5\n0\n–0.5\n–0.5 0.50 –0.5 0.50 –0.5 0.50 –0.5 0.50 –0.5 0.50 –0.5 0.50\nsin(x) + x x2 |x|\nHomoscedastic\nnoise\nHeteroscedastic\nnoiseStep function True function TabPFNC atBoost (quantile)\nPosition on the wall (m)\n0.8\n0.6\n0.4\n0.2\n0\n–0.2\n–0.4\n–0.6\n–0.8\n0.5 1.0 0.5 1.0 0.5 1.0\nPosition on the wall (m)\n0.4\n0.2\n0\n–0.2\n–0.4\n24 24 24\nSlit width (mm)\nSlit separation (/uni03BCm) Slit separation (/uni03BCm) Slit separation (/uni03BCm)\nSlit width (mm) Slit width (mm)\na b\nFig. 3 | The behaviour of TabPFN and a set of baselines on simple functions. \nIn all plots, we use orange for the ground truth and blue for model predictions. \na, Each column represents a different toy function, each having a single feature \n(along the x-axis) and a target (along the y-axis). TabPFN can model a lot of \ndifferent functions, including noisy functions. b, TabPFN can model distributions \nover outputs out of the box, which is exemplified by predicting the light \nintensity pattern in a double-slit experiment after observing the positions of \n1,000 photons.\nNature | Vol 637 | 9 January 2025 | 323\nmore accurate in predicting very low densities and has fewer artefacts \ncompared with CatBoost.\nQuantitative analysis\nWe quantitatively evaluate TabPFN on two dataset collections: the \nAutoML Benchmark36 and OpenML-CTR2337. These benchmarks com-\nprise diverse real-world tabular datasets, curated for complexity, rel-\nevance and domain diversity. From these benchmarks, we use the 29 \nclassification datasets and 28 regression datasets that have up to 10,000 \nsamples, 500 features and 10 classes. We further evaluated additional \nbenchmark suites from refs. 14,15, as well as five Kaggle competitions \nfrom the Tabular Playground Series.\nWe compared TabPFN against state-of-the-art baselines, including \ntree-based methods (random forest38, XGBoost (XGB)7, CatBoost9, \nLightGBM8), linear models, support vector machines (SVMs) 39 and \nMLPs34.\nEvaluation metrics include ROC AUC (area under the receiver operat-\ning characteristic curve; One-vs-Rest) and accuracy for classification, \nand R2 (coefficient of determination) and negative RMSE (root mean \nsquared error) for regression. Scores were normalized per dataset, \nwith 1.0 representing the best and 0.0 the worst performance with \nrespect to all baselines.\nFor each dataset and method, we ran 10 repetitions with different ran-\ndom seeds and train–test splits (90% train, 10% test). We tuned hyper-\nparameters using random search with five-fold cross-validation, with \ntime budgets ranging from 30 s to 4 h. All methods were evaluated using \neight CPU cores, with TabPFN additionally using a consumer-grade GPU \n(RTX 2080 Ti; other methods did not benefit from this, see Extended \nData Fig. 2d). TabPFN was pre-trained once using eight NVIDIA RTX \n2080 GPUs over 2 weeks, allowing for ICL on all new datasets in a single \nforward pass. These modest computational requirements make similar \nresearch accessible to academic labs. For details, refer to the section \n‘Detailed evaluation protocol’ .\n \nComparison with state-of-the-art baselines\nFigure 4a demonstrates the strong out-of-the-box performance of \nTabPFN compared with tuned and default configurations of XGBoost, \nCatBoost and a random forest. For classification tasks, TabPFN sur-\npasses CatBoost, the strongest default baseline, by 0.187 (0.939 com-\npared with 0.752) in normalized ROC AUC in the default setting and by \n0.13 (0.952 compared with 0.822) in the tuned setting. For regression, \nTabPFN outperforms CatBoost in normalized RMSE by 0.051 (0.923 \ncompared with 0.872) in the default setting and by 0.093 (0.968 com-\npared with 0.875) in the tuned setting. In Fig. 4b, we show per-dataset \ncomparisons. Although for some datasets CatBoost outperforms \nTabPFN, TabPFN wins on most of the datasets.\nFigure 4c shows how the performance of TabPFN and the baselines \nimprove with more time spent on hyperparameter search. The default \nof TabPFN, taking 2.8 s on average for classification and 4.8 s for regres-\nsion, outperforms all baselines, even when tuning them for 4 h—a \nspeedup of 5,140× and 3,000×, respectively. We show comparisons \non a larger number of metrics in Extended Data Tables 1 and 2.\nAs shown in Extended Data Fig. 2, similar to our primary benchmarks, \nTabPFN substantially outperformed all baselines on the benchmarks of \nrefs. 14,15. The benchmark of ref. 14 is particularly noteworthy because \non this benchmark, tree-based methods were previously found to excel. \nMoreover, we show in Extended Data Table 6 that default TabPFN \noutperforms default CatBoost on all five Kaggle competitions with \nless than 10,000 training samples from the latest completed Tabular  \nPlayground Series.\nEvaluating diverse data attributes\nIn Fig. 5a,b, we show the robustness of TabPFN to dataset character-\nistics that are traditionally hard to handle for neural-network-based \napproaches14,23.\nFigure 5a provides an analysis of the performance of TabPFN across \nvarious dataset types. First, we add uninformative features (randomly \na cb\nNormalized ROC AUC\n0.9\n0.8\n0.7\n0.6\n3009 00 3,600 14,40 0603051\nAv erage /f_it + pre dict time (s)\nTa bPFN XGB CatBoost LightGBM RF\nAv erage /f_it + pre dict time (s)\nNormalized negative RMSE\n0.9\n1.0\n0.8\n0.7\n0.6\n0.5\n3009 00 3,600 14,40 0603051\nTa bPFN XGB CatBoost LightGBM RF\nPer dataset normalized ROC comparison\nof Catboost and Ta bPFN\nPer dataset normalized RMSE comparison\nof Catboost and Ta bPFN\n1.000.750.500.2501 .000.750.500.250 CatBoost (4h tuned)\nCatBoost (default)\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nCatBoost (default)\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nCatBoost (4 h tuned)\nT abPFN (default)\nWilcoxon P < 0.001W ilcoxon P < 0.001\nWilcoxon P = 0.0153 Wilcoxon P < 0.001\nCatboost\nstr onger \nT abPFN\nstro nger\nTa bPFN\nstr onger\nCatboost\nstr onger \nCatboost \nstr onger\nT abPFN\nstro nger\nTa bPFN\nstr onger\nCatboost\nstr onger\nTa bPFN (4 h tuned)\n1.000.750.500.2501 .000.750.500.250\nT abPFN (default) Ta bPFN (4 h tuned)\nMagni/f_ication Magni/f_ication\nMagni/f_ication Magni/f_ication\nNormalized\nROC AUC\nNormalized\naccuracy\nNormalized\nnegative RMSE\nNormalized\nR2\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nRegr ession\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nRegr ession\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1.0\n0.8\n0.9\n0.6\n1.0\n0.8\n0.9\n0.6\n1.0\n0.8\n0.9\n0.6\n1.0\n0.8\n0.9\n0.6\nDefault T uned (4 h)Default T uned (4 h)\nDefault T uned (4 h)Default T uned (4 h)\nLin\nRF\nMLP\nSVM\nLGBM\nCB\nXGB\nT abPFN\nRF\nLGBM\nCB\nXGB\nT abPFN\nRF\nLGBM\nCB\nXGB\nT abPFN\nRF\nLGBM\nCB\nXGB\nT abPFN\nRF\nLGBM\nCB\nXGB\nT abPFN\nLin\nRF\nMLP\nSVM\nLGBM\nCB\nXGB\nT abPFN\nLin\nRF\nMLP\nSVM\nLGBM\nCB\nXGB\nT abPFN\nLin\nRF\nMLP\nSVM\nLGBM\nCB\nXGB\nT abPFN\nClassi/f_ication\nClassi/f_ication\n0.7 0.7\n0.7 0.7\nFig. 4 | Comparison of TabPFN on our test benchmarks, containing datasets \nwith up to 10,000 samples and 500 features. Performance was normalized \nper dataset before aggregation using all baselines; intervals represent the 95% \nconfidence interval. Wilcoxon P refers to the two-sided Wilcoxon signed-rank \ntest P value54. a, Average performance of the default as well as the tuned versions \nof TabPFN and our baselines. All methods are tuned for ROC AUC or RMSE, \nrespectively, thus decreasing the representativeness of the secondary metrics. \nLGBM, LightGBM; MLP , multilayer perceptron; SVM, support vector machines; \nRF , random forest; CB, CatBoost; XGB, XGBoost; Lin, logistic regression for \nclassification and ridge regression for regression tasks. Plots on the right-hand \nside show a magnified analysis of the strongest baselines considered. b , A per- \ndataset comparison of TabPFN with its strongest baseline, CatBoost. Each dot \nis the average score on one dataset. c , The impact of hyperparameter tuning for \nthe considered methods. The x -axis shows the average time required to fit and \npredict with the algorithm.\n324 | Nature | Vol 637 | 9 January 2025\nArticle\nshuffled features from the original dataset) and outliers (multiply each \ncell with 2% probability with a random number between 0 and the out-\nlier factor). The results show that TabPFN is very robust to uninforma-\ntive features and outliers, something typically hard for neural networks, \nas can be seen with the MLP baseline. Second, although dropping either \nsamples or features hurts the performance of all methods, with half \nthe samples TabPFN still performs as well as the next best method \nusing all samples.\nIn Fig. 5b, we split our test datasets into subgroups and perform \nanalyses per subgroup. We create subgroups based on the presence of \ncategorical features, missing values, number of samples and number of \nfeatures in the datasets. The sample- and feature-number subgroups \nare split such that a third of the datasets fall into each group. We can \nsee that none of these characteristics strongly affect the performance \nof TabPFN relative to the other methods. However, we note that these \nresults should not be taken as evidence that TabPFN scales well beyond \nthe 10,000 samples and 500 features considered here. We show four \nfurther ablations in Extended Data Fig. 1.\nComparison with tuned ensemble methods\nWe compare the performance of TabPFN with AutoGluon 1.0 (ref. 40), \nwhich combines various machine learning models, including our base-\nlines, into a stacked ensemble41, tunes their hyperparameters and then \ngenerates the final predictions using post hoc ensembling (PHE)42,43. It \nthus represents a different class of methods compared with individual \nbaselines.\nT o assess whether TabPFN can also be improved by a tuned ensemble \napproach, we introduce TabPFN (PHE). TabPFN (PHE) automatically \ncombines only TabPFN models with PHE and tunes their hyperparam-\neters using a random portfolio from our search space. We detail this \napproach in the section ‘TabPFN (PHE)’ .\nFigure 5c–d compares the performance of TabPFN, TabPFN (PHE), \nAutoGluon and CatBoost. For TabPFN (PHE) and AutoGluon, we start \nwith a minimal budget of 300 s for tuning because AutoGluon oth -\nerwise does not reliably return results. In just 2.8 s, TabPFN (default) \noutperforms AutoGluon for classification tasks, even if AutoGluon is \nallowed up to 4 h, a 5.140× speedup. TabPFN (PHE) further improves \nperformance leading to an average normalized ROC AUC score of 0.971, \ncompared with 0.939 for TabPFN (default) and 0.914 for AutoGluon. \nFor regression tasks, tuning hyperparameters is more important. Here, \nTabPFN (PHE) outperforms AutoGluon (allowed 4 h) after its minimal \ntuning budget of 300 s, a 48× speedup.\nFoundation model with interpretability\nApart from its strong predictive performance, TabPFN exhibits key \nfoundation model abilities, such as data generation, density estima-\ntion, learning reusable embeddings and fine-tuning. We showcase \nthese abilities through proof-of-concept experiments on the Ger -\nman Credit Dataset44, which contains credit risk information and the \nmfeat-factors45 dataset classifying handwritten digits based on a tabular \nrepresentation.\nTabPFN can estimate the probability density function of numerical \nfeatures, as shown in Fig. 6a, and the probability mass function of cat-\negorical features. Computing the sample densities enables anomaly \ndetection to identify issues such as fraud, equipment failures, medical \nemergencies or low-quality data.\nTabPFN also allows synthesizing new tabular data samples that mimic \nreal-world dataset characteristics as shown in Fig. 6b. This enables appli-\ncations such as data augmentation or privacy-preserving data sharing46.\nThe architecture of TabPFN yields meaningful feature repre -\nsentations that can be reused for downstream tasks such as data \nNormalized average performance\n(ROC AUC and negative RMSE)\nNormalized ROC AUC\nNormalized negative RMSE\nDropping features Number of samples Number of features\nOutlier factorM issing values Categorical features\nCategorical features\nNumber of features\nUninformative features\nDropping samples\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nFraction (%)\nOutlier Fraction\n0\n90\nFraction kept (%)\n2550100\nFraction kept (%)\n50100\nTabPFN (default) \nCatBoost (default) \nMLP (default)\n \nLinear (default) TabPFN (default) \nTabPFN (PHE) \nTabPFN\nAutoGluon\nCatBoost\nTabPFN (PHE) \nTabPFN\nAutoGluon\nCatBoost\nCatBoost (default) \nMLP (default)\n \nLinear (default) TabPFN (default) \nCatBoost (default) \nMLP (default)\n \nLinear (default) TabPFN (default) \nCatBoost (default) \nMLP (default)\n \nLinear (default) \n0\n100\n10,000\n25\nMissing values?\nNumber of samples\nYes\nNo\n1–1,999\n2,000–3,999\n4,000–10,000\n1–19\n20–39\n40–500\nNo\nYes\nDataset win rate\non ROC AUC\nWilcoxon P = 0.0024\n0.8\n0.8\n0.975\n0.950\n0.925\n0.900\n0.875\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0.6\n0.4\n0.2\n0\nAutogluon\nTabPFN\n(PHE)\n1.00\n0.95\n0.90\n0.85\n0.80\n0.75\n53 0\nAverage /f_it + predict time (s)\n60 3009 00 3,600 14,400 53 0\nAverage /f_it + predict time (s)\n60 3009 00 3,600 14,400\nAutogluon\nTabPFN\n(PHE)\nDataset win rate\non RMSE\nWilcoxon P = 0.0101\na b\ndc\nFig. 5 | Robustness across datasets and performance comparison with \ntuned ensembles.  a, A comparison of modified datasets. We can see that \nTabPFN is not more vulnerable to the modifications compared with baselines. \nWe also see that TabPFN reproduces the accuracy of CatBoost (default) with \nonly half the training samples provided. Here we normalize scores per dataset \n(sharing one normalization across all modifications of one experiment) to \navoid negative outliers. b , We split the test datasets by data characteristics and \nanalyse the performance per subgroup. c, Classification performance. Left,  \nthe win rate of TabPFN (PHE) against AutoGluon (with one tie excluded); right, \nthe ROC AUC score over time for tuning each method, with the first marker \nrepresenting the default configuration for the non-ensembling methods.  \nd, Regression performance presented as in c but using the RMSE metric. \nIntervals represent the 95% confidence interval and Wilcoxon P refers to the \ntwo-sided Wilcoxon signed-rank test P value 54.\nNature | Vol 637 | 9 January 2025 | 325\nimputation and clustering. We extract and visualize learned embed-\ndings from the mfeat-factors dataset in Fig. 6c, showing improved \nclass separation compared with the raw data on the first two principal  \ncomponents.\nFurthermore, we demonstrate the ability of TabPFN to improve per-\nformance through fine-tuning on related datasets. Unlike tree-based \nmethods, the neural architecture of TabPFN enables fine-tuning on \nspecific dataset classes. We conduct proof-of-concept experiments \nusing sine curve datasets with varying offsets between fine-tuning and \ntest data. Figure 6d shows an example fine-tuning result. Our analysis \nacross 50 runs (Extended Data Fig. 4) shows that TabPFN successfully \ntransfers knowledge even when labels differ significantly between \nfine-tuning and test tasks, with performance improving as distributions \nbecome more similar. This could, for example, enable fine-tuning for a \nrange of datasets from medical studies to obtain an improved general \nmodel for medical diagnosis tasks. For details, refer to section ‘Founda-\ntion model abilities’ .\nFinally, we have developed a methodology to easily interpret the \npredictions of TabPFN. Interpretability is crucial for building trust \nand accountability when deploying models in high-stakes domains. \nWe support the computation of feature importance through SHAP47 \n(Shapley Additive Explanations), a game-theoretic approach to \nexplain predictions. SHAP values represent the contribution of each \nfeature to the output of the model. Extended Data Fig. 3 compares \nthe feature importance and impact for logistic regression, CatBoost \nand TabPFN. TabPFN achieves high accuracy while learning simple, \ninterpretable feature relationships. By contrast, logistic regression is \ninterpretable but less accurate, whereas CatBoost is accurate but quali-\ntatively less interpretable because of complex, non-smooth decision  \nboundaries.\nConclusion\nTabPFN represents a major change in tabular data modelling, lever-\naging ICL to autonomously discover a highly efficient algorithm that \noutperforms traditional human-designed approaches on datasets with \nup to 10,000 samples and 500 features. This shift towards foundation \nmodels trained on synthetic data opens up new possibilities for tabular \ndata analysis across various domains.\nPotential future directions include scaling to larger datasets 48, \nhandling data drift49, investigating fine-tuning abilities across related \ntabular tasks50 and understanding the theoretical foundations of our \napproach51. Future work could also explore creating specialized priors \nto handle data types such as time series 52 and multi-modal data, or \nspecialized modalities such as ECG, neuroimaging data53 and genetic \ndata. As the field of tabular data modelling continues to evolve, we \nbelieve that foundation models, such as TabPFN, will play a key part in \nempowering researchers. T o facilitate the widespread use of TabPFN, \nin the section ‘User guide’ we discuss how to use it effectively.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-024-08328-6.\n1. Borisov, V. et al. Deep neural networks and tabular data: a survey. IEEE Trans. Neural Netw. \nLearn. Syst. 35, 7499–7519 (2024).\n2. van Breugel, B. & van der Schaar, M. Position: why tabular foundation models should  \nbe a research priority. In Proc. 41st International Conference on Machine Learning  \n48976–48993 (PMLR, 2024).\n3. Silver, D. et al. Mastering the game of go with deep neural networks and tree search. \nNature 529, 484–489 (2016).\n4. Jumper, J. M. et al. Highly accurate protein structure prediction with AlphaFold. Nature \n596, 583 – 589 (2021).\n5. OpenAI. GPT-4 Technical Report. Preprint at https://arxiv.org/abs/2303.08774 (2023).\n6. Friedman, J. H. Greedy function approximation: a gradient boosting machine. Ann. Stat. \n1189–1232 (2001).\n7. Chen, T. & Guestrin, C. Xgboost: A scalable tree boosting system. In Proc. 22nd ACM \nSIGKDD International Conference on Knowledge Discovery and Data Mining (eds \nKrishnapuram, B. et al.) 785–794 (ACM Press, 2016).\n8. Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Proc. 30th \nInternational Conference on Advances in Neural Information Processing Systems   \n(eds Guyon, I. et al.) 3149–3157 (Curran Associates, 2017).\n9. Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. & Gulin, A. CatBoost: unbiased \nboosting with categorical features. In Proc. 30th International Conference on Advances  \nin Neural Information Processing Systems (eds Bengio, S. et al.) 6639–6649 (Curran \nAssociates, 2018).\n10. Lowe, D. G. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. \n60, 91–110 (2004).\n11. Dalal, N. & Triggs, B. Histograms of oriented gradients for human detection. In Proc. 2005 \nIEEE Computer Society Conference on Computer Vision and Pattern Recognition \n(CVPR’05) 886–893 (IEEE, 2005).\n12. Vaswani, A. et al. Attention is all you need. In Proc. 30th International Conference on \nAdvances in Neural Information Processing Systems (eds Guyon, I. et al.) 6000–6010 \n(Curran Associates, 2017).\n13. Silver, D. et al. Mastering the game of go without human knowledge. Nature 550, 354–359 \n(2017).\n14. Grinsztajn, L., Oyallon, E. & Varoquaux, G. Why do tree-based models still outperform \ndeep learning on typical tabular data? In Proc. 36th International Conference on Neural \nInformation Processing Systems Vol. 35, 507–520 (ACM, 2022).\n15. McElfresh, D. et al. When do neural nets outperform boosted trees on tabular data? In \nProc. 37th International Conference on Neural Information Processing System Vol. 36, \n76336–76369 (ACM, 2024).\n16. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning (MIT Press, 2016).\n17. Brown, T. et al. Language models are few-shot learners. In Proc. Advances in Neural \nInformation Processing Systems (eds Larochelle, H. et al.) Vol. 33, 1877–1901 (Curran \nAssociates, 2020).\n18. Garg, S., Tsipras, D., Liang, P. S. & Valiant, G. What can transformers learn in-context? A case \nstudy of simple function classes. In Proc. Advances in Neural Information Processing \nSystems Vol. 35, 30583–30598 (ACM, 2022).\nData density estimation\nHigh density\nMedium density (10th percentile)\nLow density (2nd percentile)\nSynthetic data generation\nActual samples\nGenerated samples\nEmbedded data + PCA\nPCA 1\nPCA 1\nPCA 2P CA 2\nOriginal data + PCA\nFine-tuning data\nDefault TabPFN predictions\nFinetuned TabPFN predictions\n80\n70\n60\n50\n40\n30\n20\n0 5,000 10,000 15,000\nCredit_amount\n0 5,000 10,000 15,000\nCredit_amount\nAge\n80\n70\n60\n50\n40\n30\n20\nAge\nPrediction Ground truthT raining sample\nYYY\nX\nab cd\nFig. 6 | Showcase of the application of TabPFN as tabular foundation model. \na,b, On the German Credit Dataset, we perform data density estimation ( a) and \ngeneration of new synthetic samples ( b). c, We show our learned embeddings \nare useful representations of each sample on the handwritten digits dataset \n(mfeat-factors) with different classes forming different clusters. d , We \ndemonstrate fine-tuning TabPFN for a specific set of tasks. Fine-tuned on a \ndataset containing various sine curves (top), we see the model makes more \naccurate predictions on another sine curve dataset.\n326 | Nature | Vol 637 | 9 January 2025\nArticle\n19. Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. & Zhou, D. What learning algorithm is \nin-context learning? Investigations with linear models. In Proc. The Eleventh International \nConference on Learning Representations (ICLR, 2023).\n20. Von Oswald, J. et al. Transformers learn in-context by gradient descent. In Proc. 40th \nInternational Conference on Machine Learning 35151–35174 (PMLR, 2023).\n21. Zhou, H. et al. What algorithms can transformers learn? A study in length generalization. \nIn Proc. The Twelfth International Conference on Learning Representations (ICLR, 2024).\n22. Müller, S., Hollmann, N., Pineda-Arango, S., Grabocka, J. & Hutter, F. Transformers  \ncan do Bayesian inference. In Proc. The Tenth International Conference on Learning \nRepresentations (ICLR, 2022).\n23. Hollmann, N., Müller, S., Eggensperger, K. & Hutter, F. TabPFN: a transformer that solves \nsmall tabular classification problems in a second. In Proc. The Eleventh International \nConference on Learning Representations (ICLR, 2023).\n24. Kingma, D. & Ba, J. Adam: A method for stochastic optimization. In Proc. International \nConference on Learning Representations (ICLR, 2015).\n25. Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly learning to align \nand translate. In Proc. 3rd International Conference on Learning Representations   \n(eds Bengio, Y. & LeCun, Y.) (ICLR, 2015).\n26. Gorishniy, Y., Rubachev, I., Khrulkov, V. & Babenko, A. Revisiting deep learning models  \nfor tabular data. In Proc. Advances in Neural Information Processing Systems 34  \n(eds Ranzato, M. et al.) 18932–18943 (NeurIPS, 2021).\n27. Zhu, B. et al. XTab: cross-table pretraining for tabular transformers. In Proc. 40th \nInternational Conference on Machine Learning (eds Krause, A. et al.) 43181–43204  \n(PMLR, 2023).\n28. Lorch, L., Sussex, S., Rothfuss, J., Krause, A. & Schölkopf, B. Amortized inference for \ncausal structure learning. In Proc. Advances in Neural Information Processing Systems \n(eds Koyejo, S. et al.) Vol. 35, 13104–13118 (ACM, 2022).\n29. Dao, T., Fu, D., Ermon, S., Rudra, A. & Ré, C. Flashattention: fast and memory-efficient \nexact attention with io-awareness. In Proc. Advances in Neural Information Processing \nSystems (eds Koyejo, S. et al.) Vol. 35, 16344–16359 (2022).\n30. Torgo, L. & Gama, J. Regression using classification algorithms. Intell. Data Anal. 1, 275–292 \n(1997).\n31. Pearl, J. Causality 2nd edn (Cambridge Univ. Press, 2009).\n32. Jiang, M. et al. Investigating Data Contamination for Pre-training Language Models. \nPreprint at https://arxiv.org/abs/2401.06059 (2024).\n33. Kumaraswamy, P. A generalized probability density function for double-bounded random \nprocesses. J. Hydrol. 46, 79–88 (1980).\n34. Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain \nMechanisms. Report No. 1196-0-8 (Cornell Aeronautical Lab, 1961).\n35. Young, T. I. The bakerian lecture. experiments and calculations relative to physical optics. \nPhilos. Trans. R. Soc. Lond. 94, 1–16 (1804).\n36. Gijsbers, P. et al. AMLB: an AutoML benchmark. J. Mach. Learn. Res. 25, 1–65 (2024).\n37. Fischer, S. F., Feurer, M. & Bischl, B. OpenML-CTR23 – a curated tabular regression \nbenchmarking suite. In Proc. AutoML Conference 2023 (Workshop) (AutoML, 2023).\n38. Breimann, L. Random forests. Mach. Learn. 45, 5–32 (2001).\n39. Cortes, C. & Vapnik, V. Support-vector networks. Mach. Learn. 20, 273–297 (1995).\n40. Erickson, N. et al. Autogluon-tabular: robust and accurate automl for structured data. \nPreprint at https://arxiv.org/abs/2003.06505 (2020).\n41. Wolpert, D. Stacked generalization. Neural Netw. 5, 241–259 (1992).\n42. Caruana, R., Niculescu-Mizil, A., Crew, G. & Ksikes, A. Ensemble selection from libraries  \nof models. In Proc. 21st International Conference on Machine Learning (ed. Greiner, R.) \n(Omnipress, 2004).\n43. Purucker, L. O. et al. Q(D)O-ES: Population-based quality (diversity) optimisation for post \nhoc ensemble selection in AutoML. In Proc. International Conference on Automated \nMachine Learning Vol. 224 (PMLR, 2023).\n44. Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository https://doi.\norg/10.24432/C5NC77 (1994).\n45. Duin, R. Multiple Features. UCI Machine Learning Repository https://doi.org/10.24432/\nC5HC70 (1998).\n46. Rajotte, J.-F. et al. Synthetic data as an enabler for machine learning applications in \nmedicine. iScience 25, 105331 (2022).\n47. Lundberg, S. M. & Lee, S.-I. A unified approach to interpreting model predictions. In Proc. \nAdvances in Neural Information Processing Systems (eds Guyon, I. et al.) Vol. 30, 4765–4774 \n(Curran Associates, 2017).\n48. Feuer, B. et al. TuneTables: context optimization for scalable prior-data fitted networks.  \nIn Proc. 38th Conference on Neural Information Processing Systems (NeurIPS, 2024).\n49. Helli, K., Schnurr, D., Hollmann, N., Müller, S. & Hutter, F. Drift-resilient tabPFN: In-context \nlearning temporal distribution shifts on tabular data. In Proc. 38th Conference on Neural \nInformation Processing Systems (NeurIPS, 2024).\n50. Thomas, V. et al. Retrieval & fine-tuning for in-context tabular models. In Proc. 1st \nWorkshop on In-Context Learning at the 41st International Conference on Machine \nLearning (ICML, 2024).\n51. Nagler, T. Statistical foundations of prior-data fitted networks. In Proc. 40th International \nConference on Machine Learning (eds Krause, A. et al.) Vol. 202, 25660–25676 (PMLR, \n2023).\n52. Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V. & White, C. ForecastPFN: synthetically- \ntrained zero-shot forecasting. In Proc. 37th Conference on Advances in Neural Information \nProcessing Systems (eds Oh, A. et al.) (NeurIPS, 2023).\n53. Czolbe, S. & Dalca, A. V. Neuralizer: General neuroimage analysis without re-training.  \nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 6217–6230 \n(IEEE, 2023).\n54. Wilcoxon, F. in Breakthroughs in Statistics: Methodology and Distribution (eds Kotz, S.  \n& Johnson, N. L.) 196–202 (Springer, 1992).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025\nMethods\nUser guide\nWhen to use TabPFN. TabPFN excels in handling small- to medium-sized \ndatasets with up to 10,000 samples and 500 features (Fig. 4 and Extended  \nData Table 1). For larger datasets and highly non-smooth regression \ndatasets, approaches such as CatBoost9, XGB7 or AutoGluon40 are likely \nto outperform TabPFN.\nAlthough TabPFN provides a powerful drop-in replacement for tra-\nditional tabular data models such as CatBoost, similar to these mod-\nels, it is intended to be only one component in the toolkit of a data \nscientist. Achieving top performance on real-world problems often \nrequires domain expertise and the ingenuity of data scientists. As for \nother modelling approaches, data scientists should continue to apply \ntheir skills and insights in feature engineering, data cleaning and prob-\nlem framing to get the most out of TabPFN. We hope that the train-\ning speed of TabPFN will facilitate faster iterations in the data science  \nworkflow.\nLimitations of TabPFN. The limitations of TabPFN are as follows:  \n(1) the inference speed of TabPFN may be slower than highly optimized \napproaches such as CatBoost; (2) the memory usage of TabPFN scales \nlinearly with dataset size, which can be prohibitive for very large data-\nsets; and (3) our evaluation focused on datasets with up to 10,000 \nsamples and 500 features; scalability to larger datasets requires further \nstudy.\nComputational and time requirements. TabPFN is computation -\nally efficient and can run on consumer hardware for most datasets. \nHowever, training on a new dataset is recommended to run on a (con-\nsumer) GPU as this speeds it up by one to three orders of magnitude. \nAlthough TabPFN is very fast to train, it is not optimized for real-time \ninference tasks. For a dataset with 10,000 rows and 10 columns, our \nmodel requires 0.2 s (0.6 s without GPU) to perform a prediction for \none sample, whereas CatBoost (default) can do the same in 0.0002 s. \nIn ref. 55, further optimizing TabPFN specifically for inference tasks \nhas already been explored, resulting in four times faster inference \nperformance compared with even XGBoost, but so far also reducing \npredictive quality. Refer to the section ‘Details on the neural archi -\ntecture’ for details on the memory usage and runtime complexity of  \nTabPFN.\nData preparation. TabPFN can handle raw data with minimal pre-  \nprocessing. If we simply provide the data in a tabular format (NumPy \nmatrix), TabPFN will automatically handle missing values, encode \ncategorical variables and normalize features. Although TabPFN \nworks well out of the box, we can further improve the performance \nusing dataset-specific pre-processing. This can also be partly done \nautomatically with our PHE technique or manually by modifying the  \ndefault settings. When manually pre-processing data, we should keep \nin mind that the neural network of TabPFN expects roughly normally \ndistributed features and targets after all pre-processing steps. If we, \nfor example, know that a feature follows a log distribution, it might \nhelp to exponentiate it before feeding it to TabPFN. As TabPFN does \nz-normalization of all inputs, scaling does not affect the predictions. \nAs for all algorithms, however, using domain knowledge to combine \nor remove features can increase performance.\nHyperparameter tuning. TabPFN provides strong performance out \nof the box without extensive hyperparameter tuning (see section \n‘Comparison with state-of-the-art baselines’). If we have additional \ncomputational resources, we can further optimize the performance \nof TabPFN using hyperparameter optimization (HPO) or the PHE tech-\nnique described in the section ‘TabPFN (PHE)’ . Our implementation \ndirectly provides HPO with random search and PHE.\nDetails on the neural architecture\nOur architecture is a variation of the original transformer encoder12 \nand the original PFN architecture22, but it treats each cell in the table \nas a separate time position, similar to that in ref. 28. Therefore, it can \ngeneralize to more training samples as well as features than seen dur-\ning training.\nFigure 1b details our new architecture. All features that go into our \narchitecture are first mapped to floating point values, that is, cate -\ngoricals are transformed to integers. These values are subjected to \nz-normalization using the mean and standard deviation for each fea-\nture separately across the whole training set. These values are now \nencoded with simple linear encoders. Each layer first has an attention \nover features, followed by an attention over samples, both of which \noperate separately on each column or row, respectively. These two \nsub-layers are followed by an MLP sublayer. Each sublayer is followed \nby a residual addition and a half-precision layer norm.\nWe found that encoding groups of features can be even more effec-\ntive compared with encoding one value per representation. For our \nhyperparameter search space, we selected six architectures for clas-\nsification and five for regression. In three of the six classification models \nand four of the five regression models, including the TabPFN default, a \ntransformer position encodes two features of one example; in others, \nit represents one value.\nAlthough the inter-feature attention is a classical fully connected \nattention, our inter-sample attention does not allow the test samples \nto attend to each other but only to the training data. Therefore, we \nmake sure that the test samples do not influence each other or the train-\ning set representations. T o allow our model to differentiate features \nmore easily that have the same statistics, for example, two features that \nhave the same entries just in different orders, we use random feature \nembeddings that we add to all embeddings before the first layer. We \ngenerate one embedding per feature by projecting a random vector of \none-fourth the size of our embeddings through a learned linear layer \nand add this to all embeddings representing an instance of that feature.\nAs the representations of training samples are not influenced by the \ntest set, we cache the keys and values of the training samples to allow \nsplitting training and inference. We use a special variant of multi-query \nattention for our inter-sample attention from test samples56 to save \nmemory when caching representations. In our variant, we use all keys \nand values for the attention between samples of the training set, but \nrepeatedly use the first key and value for attention from the test sam-\nples. This allows caching only one key or value vector pair per cell in \nthe training set that is fed into our inter-sample attention of new test \nsamples.\nThe compute requirements of this architecture scale quadratically \nwith the number of samples (n) and the number of features (m), that is \nO(n2 + m2), and the memory requirements scale linearly in the dataset \nsize, O(n ⋅ m).\nFinally, we found that pre-processing inputs can help performance, \nthus we can perform z-normalization of all inputs across the sample \ndimension and add an extra input for each cell that indicates whether \nthe input was missing; the input itself is set to 0 in these cases. All \ninputs are finally linearly encoded into the embedding dimension \nof TabPFN.\nDetails on the causal generative process\nAn SCM ≔G Zϵ(, ) consists of a collection Z ≔ (z1, …, zk) of structural \nassignments (called mechanisms): zf zϵ=( ,) ,i i iiPA ()G  where G iPA( ) is the \nset of parents of node i (its direct causes) in the underlying directed \nacyclic graph (DAG) G (the causal graph), fi is a (potentially nonlinear) \ndeterministic function and ϵi is a noise variable. Causal relationships \nin G are represented by edges pointing from causes to effects 31. As  \nour prior is a sampling procedure, we can make a lot of choices on,  \nfor example, the graph size or complexity. By defining a probability  \nArticle\ndistribution over these hyperparameters in the prior, the posterior \npredictive distribution approximated by TabPFN at inference time \nimplicitly represents a Bayesian ensemble, jointly integrating over a \nweighted hyperparameter space. The specific hyperparameter ranges \nand sampling strategies are chosen to cover a diverse set of scenarios \nthat we expect to encounter in real-world tabular data.\nGraph structure sampling. The structural causal models underlying \neach dataset are based on a DAG G. We sample these graphs using the \ngrowing network with redirection sampling method57, a preferential \nattachment process that generates random scale-free networks. We \neither sample a single connected component or merge multiple disjoint \nsubgraphs. Disjoint subgraphs lead to features that are marginally \nindependent of the target if they are not connected to the target node, \nreflecting real-world scenarios with uninformative predictors.\nT o control the complexity of the sampled DAGs, we use two hyper-\nparameters: the number of nodes N and the redirection probability P. \nN is sampled from a log-uniform distribution, Na blog ~( ,)U , where a \nand b are hyperparameters controlling the range of the graph size. The \nredirection probability P  is sampled from a gamma distribution, \nP ~ Γ(α, β), where α and β are shape and rate parameters, respectively. \nLarger values of N yield graphs with more nodes, whereas smaller val-\nues of P lead to denser graphs with more edges on average57.\nComputational edge mappings. In our implementation, each SCM \nnode and sample is represented as a vector in Rd . When propagating \ndata through the SCM, the deterministic functions fi at each edge map \nthe input vectors to an output vector using four types of computa -\ntional modules:\n1. Small neural networks: here we initialize weight matrices RW ∈ dd×  \nusing Xavier initialization58 and apply a linear transformation Wx + b \nto the input vectors Rx ∈ d , where b ∈ dR  is a bias vector. After the \nlinear projection, we apply element-wise nonlinear activation func-\ntions RRσ :→ dd , randomly sampled from a set, including identity, \nlogarithm, sigmoid, absolute value, sine, hyperbolic tangent, rank \noperation, squaring, power functions, smooth ReLU59, step function \nand modulo operation.\n2. Categorical feature discretization: to generate categorical features \nfrom the numerical vectors at each node, we map the vector to the \nindex of the nearest neighbour in a set of per node randomly sampled \nvectors {p1, …, pK} for a feature with K categories. This discrete index \nwill be observed in the feature set as a categorical feature. We sample \nthe number of categories K from a rounded gamma distribution with \nan offset of 2 to yield a minimum number of classes of 2. T o further \nuse these discrete class assignments in the computational graph, \nthey need to be embedded as continuous values. We sample a second \nset of embedding vectors pp{ ′, …, ′}K1  for each class and transform \nthe classes to these embeddings.\n3. Decision trees: to incorporate structured, rule-based dependencies, \nwe implement decision trees in the SCMs. At certain edges, we select \na subset of features and apply decision boundaries on their values \nto determine the output60. The decision tree parameters (feature \nsplits, thresholds) are randomly sampled per edge.\n4. Noise injection: at each edge, we add random normal noise from the \nnormal distribution N σI(0,) 2 .\nInitialization data sampling. For each to-be-generated sample, we \nrandomly generate initialization data ϵ that is inserted at the DAG root \nnodes and then propagated through the computational graph. The \nnoise variables ϵ are generated according to one of three sampling \nmechanisms:\n1. Normal: ϵ σ~( 0, )ϵ\n2N , where σϵ\n2 is a hyperparameter.\n2. Uniform: Uϵ aa~( −, ), where a is a hyperparameter.\n3. Mixed: for each root node, we randomly select either a normal or \nuniform distribution to sample the initialization noise ϵ from.\nFurthermore, we sample input data with varying degrees of non-  \nindependence for some datasets. Here we first sample a random frac-\ntion ρ of samples to serve as prototypes x x*,… , *M1 , where M = ρn and n \nis the dataset size. Then, for each input vector x i to be sampled, we \nassign weights αij to the prototypes and linearly mix the final input as\n∑xα x= *, (1)i\nj\nM\nij j\n=1\nwhere ∑jαij = 1. The weights αij are sampled from a multinomial distri-\nbution, αi ~ Multinomial(β), where β is a temperature hyperparameter \ncontrolling the degree of non-independence: larger β yields more uni-\nform weights, whereas smaller β concentrates the weights on fewer \nprototypes per sample.\nPost-processing. Each dataset is post-processed randomly with one \nor more of the following post-processings: (1) For some datasets, we \nuse the Kumaraswamy feature warping, introducing nonlinear distor-\ntions33 to features as done in ref. 61. (2) We quantize some continuous \nfeatures into buckets of randomly sampled cardinality K , mimick-\ning binned or discretized features commonly encountered in data-\nsets. We map a feature value x to the index of the bucket it falls into,  \ndetermined by K + 1 bin edges sampled from the set of values this fea-\nture takes. (3) T o introduce scenarios for dynamic imputation and \nhandling of incomplete datasets, a common challenge in data sci -\nence, we randomly designate a fraction ρmiss of the data as missing ac-\ncording to the missing completely at random strategy. Each value is \nmasked as missing with probability ρmiss, independently of the data  \nvalues.\nTarget generation. T o generate target labels for regression tasks, we \nselect a randomly chosen continuous feature without post-processing. \nFor classification labels, we select a random categorical feature \nthat contains up to 10 classes. Thus, natively our method is limited \nto predicting at most 10 classes. This number can be increased by \npre-training on datasets with a larger number of classes or by using \napproaches such as building a one-vs-one classifier, one-vs-rest clas-\nsifier or building on approaches such as error-correcting output codes  \n(ECOC)62.\nTraining details\nThe training loss of any PFN is the cross-entropy between the tar-\ngets of held-out samples of synthetic datasets and the model pre-\ndiction. For a test set (X test, ytest) = Dtest, the training loss is given by \nL qD=[ −l og (| ,) ]∪ Dp D θPFN( (, )) () test test traintest test train yXE Xy  . By minimizing \nthis loss, the PFN learns to approximate the true Bayesian posterior \npredictive distribution for a chosen prior over datasets (and potentially \ntheir latent variables) D, as shown in ref. 22.\nWe trained our final models for approximately 2,000,000 steps with \na batch size of 64 datasets. That means the models used for TabPFN \nare trained on around 130,000,000 synthetically generated datasets \neach. One training run requires around 2 weeks on one node with eight \nNvidia RTX 2080 Ti GPUs. We sample the number of training samples \nfor each dataset uniformly up to 2,048 and use a fixed validation set \nsize of 128. We sample the number of features using a beta distribution \n(k = 0.95, b = 8.0) that we linearly scale to the range 1–160. T o avoid \npeaks in memory usage, the total size of each table was restricted to \nbe below 75,000 cells by decreasing the number of samples for large \nnumbers of features.\nWe chose the hyperparameters for the prior based on random \nsearches, in which we use only a single GPU per training and evaluate \non our development set, see section ‘Quantitative analysis’ . We used \nthe Adam optimizer24 with linear warmup and cosine annealing63 and \ntested a set of learning rates in [0.0001, 0.0005], using the one with \nthe lowest final training loss.\nInference details\nT o get the most performance out of TabPFN, it is crucial to optimize \nits inference pipeline. We generally always apply TabPFN in a small \nensemble, in which we perform pre-processing or post-processing of \nthe data differently for each ensemble member.\nAs our models are not fully permutation invariant, for each ensemble \nmember, we shuffle the feature order, approximating order invari-\nance64. For classification tasks, we additionally randomly permute the \nlabels. We also apply a temperature to the softmax distribution of our \nmodel outputs for calibration.\nApart from the above, we use a subset of the following for each of \nour default ensemble members:\n1. Quantile + Id: we quantize the inputs to equally spaced values bet-\nween 0 and 1, but keep a copy of each original feature. This effectively \ndoubles the number of features passed to TabPFN.\n2. Category shuffling: the labels of categorical features with low cardi-\nnality are shuffled.\n3. SVD: an SVD compression of the features is appended to the features.\n4. Outlier removal: all outliers, more than 12 standard deviations from \nthe mean, are removed.\n5. Power transform: each feature (or the label for regression) is trans-\nformed using a Yeo–Johnson transformation to stabilize the variance \nand make the data more normally distributed.\n6. One-hot encoding: categorical features are encoded using one-hot \nencoding, in which each category is represented as a binary vector.\nFor PHE and hyperparameter tuning of TabPFN, we use a larger set of \npre-processing techniques that additionally include a logarithmic, an \nexponential and a KDI transformation65. These transformations help \naddress nonlinear relationships, skewed distributions and varying \nscales among features.\nT o calibrate prediction uncertainty, we apply a softmax temperature \n(default T = 0.9) by dividing logits before the softmax calculation:\n∣\n∑\nPy x zT\nzT() = exp( /)\nexp( /) , (2)i\ni\nj j\nwhere zi are the logits, T is the temperature and P(yi∣x) is the calibrated \nprobability. We offer the option to generate second-order polynomial \nfeatures by multiplying up to 50 randomly selected feature pairs:\nSfx xi j=⋅ ,f or (, )∈ , (3)ij ij\nwhere S is the set of randomly chosen feature pairs. This can capture \nnonlinear interactions between features. This option is disabled by \ndefault. T o ensure proper handling of duplicate samples given the \nsample permutation invariance of our architecture, we add a unique \nsample identifier feature. This is a random number drawn from a stand-\nard normal distribution, ensuring each sample is treated distinctly in \nthe attention mechanism. We also provide an option for subsampling \nin each estimator, to increase ensemble diversity, which performs ran-\ndom sampling without replacement. This option is disabled by default.\nRegression details. T o enable our model to do classification on a large \nrange of scales and target distributions, we use the following approach. \nDuring pre-training, we rescale our regression targets to have zero \nmean and a standard deviation of 1 (z-score). T o decide where the bor-\nders between our features lie, we draw a large sample of datasets from \nour prior and choose the 1/5,000 quantiles from this distribution. At \ninference time, we bring the real-world data to a similar range by again \napplying z-score normalization. Furthermore, we allow applying a \nrange of transforms, including a power transform as part of our default. \nAll of the transforms, including the z-score are inverted at prediction \ntime by applying the inverse of the transform to the borders between \nbuckets. This is equivalent to applying the inverse of the transform to \nthe random variable represented by our output distribution but for \nthe half-normals used on the sides for full support22. This is because all \ntransforms are strictly monotone and the borders represent positions \non the cumulative distribution function.\nData grouping based on random forest. T o perform well on very het-\nerogeneous datasets, we also propose to use random trees to split the \ntraining data into smaller more homogeneous datasets. This technique \nis used only when performing HPO or PHE for TabPFN. It is especially \nuseful for TabPFN as our model performs best on small datasets.\nThe pre-processing for a single ensemble member, that is, a single \ntree, works as follows: we use a standard random tree with feature and \nsample bootstrapping and Gini impurity loss. For each leaf node of the \ndecision tree, we store the subset of training samples that fall into that \nnode and train a TabPFN on these. T o predict the class label for a test \nsample x, we determine the TabPFN to use by passing x through the \ndecision tree. We set the minimal leaf size to be large (500–2,000) such \nthat the resulting data groups are large enough to train a strong model.\nTabPFN (PHE)\nT o further enhance the inference performance of TabPFN, in TabPFN \n(PHE), we use PHE for a fixed portfolio of TabPFN configurations from \nour search space detailed in Extended Data Table 5. For TabPFN (PHE), \nwe first use holdout validation to sequentially evaluate models from \nthe portfolio until a time limit is reached. After all models are evaluated \nonce, we repeat holdout validation with new data splits until the time \nlimit is reached. Then, we ensemble all evaluated TabPFN models by \naggregating their predictions with a weighted arithmetic mean. We \nlearn the weights using greedy ensemble selection (GES)42,66 with 25 \niterations on prediction data from holdout validation. Finally, we prune \neach zero-weighted model, refit all remaining models on all data and \nreturn the weighted average of their predictions.\nFollowing standard practice in AutoML, we use GES because its pre-\ndictive performance is often superior to the best individual model43,67–69. \nOwing to its ICL, we expect TabPFN to overfit the training data less \nthan predictions of traditionally trained algorithms; thus, we opt for \n(repeated) holdout validation (as in Auto-Sklearn 1; ref. 67) instead of \n(repeated) cross-validation (as in AutoGluon40). Moreover, as GES usu-\nally produces sparse weight vectors43,69, we expect the final ensemble \nafter pruning each zero-weighted model to consist of a smaller number \nof models than for other ensembling approaches, such as bagging. Con-\nsequently, PHE can also improve the inference efficiency of a TabPFN \nensemble compared with other ensembling approaches.\nFoundation model abilities\nDensity estimation. The combination of a regression and a classifica-\ntion TabPFN can be used as a generative model for tabular data, not \nonly modelling targets but features as well. Let x y={ (, )}i i i\nN\n=1D  denote \nthe original dataset, where x R∈i\nd  is a d-dimensional feature vector \nand yi is the corresponding target value, and let qθ represent our trained \nTabPFN model, either a regression or classification model depending \non the target type. We aim to approximate the joint distribution of a \nnew example and its label xp y(, )D∣ . T o do this, we factorize the joint \ndistribution as\nxx xDD D∣∣ ∣∏py px py(, )= (, )⋅ (, ) (4)\nj\nd\njj\n=1\n<\n∏ qx qy≈( ,) ⋅( ,) , (5)\nj\nd\nθ jj j θ\n=1\n<: ,<xx DD∣∣\nwhere we only condition on a subset of the features in the training set \n( j:,<D ). The feature order of the joint density factorization influences \nArticle\nthe estimated densities. T o reduce variance from this source, we apply \na permutation sampling approximation of Janossy Pooling at inference \ntime, in which we average the outputs of Nj feature permutations, with \nNj = 24 in our experiments64.\nAs we cannot condition on an empty feature set for technical reasons, \nwe condition the prediction of the first feature x1, on a feature with \nrandom noise, that is, no information.\nThe above factorization of the density of a sample (equation (5)) is \ncompletely tractable and we thus use it to estimate the likelihood for \ndata points. This enables tasks such as anomaly detection and outlier \nidentification.\nSynthetic data generation. We can leverage the generative abilities \nof TabPFN (see section ‘Density estimation’) to synthesize new tabu-\nlar data samples that mimic the characteristics of a given real-world \ndataset, by simply following the factorization in equation ( 5) and \nsampling each feature step by step. The generated synthetic samples \n(x*, y*) can be used for various purposes, such as data augmentation, \nprivacy-preserving data sharing and scenario simulation.\nEmbeddings. TabPFN can be used to retrieve meaningful feature rep-\nresentations or embeddings. Given a dataset x y={ (, )}i i i\nN\n=1D , the goal \nis to learn a mapping RRf :→θ\ndk  that transforms the original \nd-dimensional feature vectors xi into an embedding space of dimen-\nsion k. The resulting embeddings f () ∈θ i\nkRx  capture the learned  \nrelationships between features and can be used for downstream tasks. \nT o use TabPFN for this problem, we simply use the target-column \nrepresentations of its final layer as embeddings.\nDetailed evaluation protocol\nT o rigorously assess the performance and robustness of TabPFN, we \nconduct a comprehensive quantitative evaluation on standard tabular \ndataset benchmarks, comparing against state-of-the-art baselines \nunder a standardized protocol.\nDefault configuration of TabPFN. Unlike traditional algorithms, \nin-context-learned algorithms do not have hyperparameters that \ndirectly control their training procedure. Instead, hyperparameters \nfor inference of TabPFN only control the pre-processing of data and \npost-processing of predictions (for example, feature scaling or softmax \ntemperature). Our default configuration (TabPFN (default)) for both \nclassification and regression is optimized for accurate predictions with \nminimal fitting time. Here, we apply the same model multiple times \nwith different pre- and post-processors and take the average over the \npredictions, yielding a four-way (eight-way for regression) ensemble. \nThe settings for our data processing were obtained through a hyper-\nparameter search optimized on our development datasets. The exact \nsettings chosen are listed in Extended Data Table 5. We emphasize that, \nas for other foundation models (such as GPT), we trained our TabPFN \nmodel once and used the same model to perform ICL in a forward pass \non all new datasets.\nBaselines. We compare with tree-based methods, such as random \nforests38, XGBoost7, CatBoost9 and LightGBM8, the state of the art for \nexperts to perform predictions on tabular data14,15. We also compare \nwith simpler methods, such as ridge regression70, logistic regression \nand SVMs39. Although standard neural networks, which unlike TabPFN \ndo not use ICL, were shown to underperform for small (<10,000 sam-\nples) tabular data1,14,71, as a point of reference, we still consider a simple \nneural network, the MLP.\nTabular dataset benchmarks. We perform our analysis on two widely \nused and publicly available benchmark suites: the standard AutoML \nbenchmark36 and the recent regression benchmark OpenML-CTR23 \n(ref. 37). Both benchmarks comprise a diverse set of real-world tabular \ndatasets, carefully curated to be representative of various domains and \ndata characteristics. The authors of the benchmark suite selected these \ndatasets based on criteria such as sufficient complexity, real-world \nrelevance, absence of free-form text features and diversity of problem \ndomains.\nFor our quantitative analysis of TabPFN for classification tasks, we \nuse a set of test datasets comprising all 29 datasets from the AutoML \nbenchmark with up to 10,000 samples, 500 features and 10 classes. \nFor regression tasks, the AutoML benchmark contains only 16 data-\nsets matching these constraints. T o increase statistical power, we \naugmented this set with all datasets matching our constraints from \nthe recent OpenML-CTR23 benchmark, yielding a test set of 28 unique \nregression datasets in total. Extended Data Tables 3 and 4 provide \nfull details for our test sets of classification and regression datasets, \nrespectively.\nWe further evaluated additional benchmark suites from refs. 14,15. \nIn ref. 14, there are 22 tabular classification datasets selected based on \ncriteria such as heterogeneous columns, moderate dimensionality and \nsufficient difficulty. In ref. 15, there is a collection of 176 classification \ndatasets, representing one of the largest tabular data benchmarks. \nHowever, the curation process for these datasets may not be as rigorous \nor quality controlled as for AutoML Benchmark and OpenML-CTR23. We \nalso evaluated five Kaggle competitions with less than 10,000 training \nsamples from the latest completed Tabular Playground Series.\nDevelopment datasets. T o decide on the hyperparameters of TabPFN, \nas well as our hyperparameter search spaces, we considered another set \nof datasets, our development datasets. We carefully selected datasets \nto be non-overlapping with our test datasets described above. The \nlist of development datasets can be found in Supplementary Tables 5 \nand 6. We considered the mean of normalized scores (ROC/RMSE) \nand rank quantiles and chose the best model configurations on these \ndevelopment datasets.\nMetrics and cross-validation. T o obtain scores for classification tasks, \nwe use two widely adopted evaluation metrics: ROC AUC (One-vs-Rest) \nand accuracy. ROC AUC averages performance over different sensi-\ntivity–specificity trade-offs, and accuracy measures the fraction of \nsamples labelled correctly.\nFor regression tasks, we use R2 and negative RMSE as evaluation met-\nrics. R2 represents the proportion of variance in the target column \nthat the model can predict. RMSE is the root of the average squared \nmagnitude of the errors between the predicted and actual values. As \nwe use negative RMSE, for all our four metrics higher values indicate \na better fit.\nT o increase statistical validity, for each dataset and method in our \ntest datasets, we evaluated 10 repetitions, each with a different random \nseed and train–test split (90% train and 10% test samples; all methods \nused the same cross-validation splits, defined by OpenML72). We average \nthe scores of all repetitions per dataset. Then, to average scores across \ndatasets, we normalize per dataset following previous benchmarks36,40. \nThe absolute scores are linearly scaled such that a score of 1.0 corre-\nsponds to the highest value achieved by any method on that dataset, \nwhereas a score of 0 represents the lowest result. This normalization \nallows for building meaningful averages across datasets with very dif-\nferent score ranges. We provide absolute performance numbers in \nSupplementary Data Tables 1–2. All confidence intervals shown are \n95% confidence intervals.\nWe tuned all methods with a random search using five-fold cross- \nvalidation with ROC AUC/RMSE up to a given time budget, ranging from \nhalf a minute to 4 h. The first candidate in the random search was the \ndefault setting supplied in the implementation of the method and was \nalso used if not a single cross-validation run finished before the time \nbudget was consumed. See the section ‘Qualitative analysis’ for the used \nsearch spaces per method. All methods were evaluated using 8 CPU \ncores. Moreover, TabPFN makes use of a 5-year-old consumer-grade \nGPU (RTX 2080 Ti). We also tested GPU acceleration for the baselines. \nHowever, as Extended Data Fig. 2 shows, this did not improve perfor-\nmance, probably because of the small dataset sizes.\nData availability\nAll datasets evaluated are publicly available on openml.org or kaggle.\ncom. We have provided scripts in our code repository that automate \nthe process of downloading and evaluating the datasets. These scripts \ncontain dataset identifiers, as well as exact data splitting and processing  \nprocedures.\nCode availability\nOur code is available at https://priorlabs.ai/tabpfn-nature/ (https://\ndoi.org/10.5281/zenodo.13981285). We also provide an API that allows \nusers to run TabPFN with minimal coding experience or without the \navailability of specific computing hardware such as a GPU. The code \nis designed to be modular and easily installable in a standard Python \nenvironment. The code to generate synthetic pre-training data has \nnot been released with our models. We aim to enable researchers and \npractitioners to easily integrate TabPFN into their workflows and apply \nit to their specific tabular data tasks. We encourage users to provide \nfeedback, report issues, and contribute to the further development of \nTabPFN. This open release aims to facilitate collaboration and acceler-\nate the adoption and advancement of TabPFN in various research and \napplication domains.\n \n55. Müller, A., Curino, C. & Ramakrishnan, R. Mothernet: a foundational hypernetwork for \ntabular classification. Preprint at https://arxiv.org/abs/2312.08598 (2023).\n56. Shazeer, N. Fast transformer decoding: one write-head is all you need. Preprint at https://\narxiv.org/abs/1911.02150 (2019).\n57. Krapivsky, P. L. & Redner, S. Organization of growing random networks. Phys. Rev. E 63, \n066123 (2001).\n58. Glorot, X. & Bengio, Y. Understanding the difficulty of training deep feedforward neural \nnetworks. In Proc. 13th International Conference on Artificial Intelligence and Statistics \n249–256 (JMLR, 2010).\n59. Nair, V. & Hinton, G. Rectified linear units improve restricted Boltzmann machines. In  \nProc. 27th International Conference on Machine Learning (eds Fürnkranz, J. & Joachims, T.) \n807–814 (Omnipress, 2010).\n60. Quinlan, J. R. Induction of decision trees. Mach. Learn. 1, 81–106 (1986).\n61. Müller, S., Feurer, M., Hollmann, N. & Hutter, F. PFNS4BO: in-context learning for Bayesian \noptimization. In Proc. 40th International Conference on Machine Learning 25444–25470 \n(PMLR, 2023).\n62. Dietterich, T. G. & Bakiri, G. Solving multiclass learning problems via error-correcting \noutput codes. J. Artif. Intell. Res. 2, 263–286 (1994).\n63. Loshchilov, I. & Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In Proc. \n5th International Conference on Learning Representations (ICLR, 2017).\n64. Murphy, R. L., Srinivasan, B., Rao, V. A. & Ribeiro, B. Janossy pooling: learning deep \npermutation-invariant functions for variable-size inputs. In Proc. 7th International \nConference on Learning Representations (ICLR, 2019).\n65. McCarter, C. The kernel density integral transformation. Transact. Mach. Learn. Res. \nhttps://openreview.net/pdf?id=6OEcDKZj5j (2023).\n66. Caruana, R., Munson, A. & Niculescu-Mizil, A. Getting the most out of ensemble selection. \nIn Proc. 6th IEEE International Conference on Data Mining (eds Clifton, C. et al.) 828–833 \n(IEEE, 2006).\n67. Feurer, M. et al. in Automated Machine Learning: Methods, Systems, Challenges  \n(eds Hutter, F. et al.) Ch. 6 (Springer, 2019).\n68. Purucker, L. & Beel, J. Assembled-OpenML: creating efficient benchmarks for ensembles \nin AutoML with OpenML. In Proc. First International Conference on Automated Machine \nLearning (AutoML, 2022).\n69. Purucker, L. & Beel, J. CMA-ES for post hoc ensembling in AutoML: a great success and \nsalvageable failure. In Proc. International Conference on Automated Machine Learning \nVol. 224, 1–23 (PMLR, 2023).\n70. Hoerl, A. E. & Kennard, R. W. Ridge regression: biased estimation for nonorthogonal \nproblems. Technometrics 12, 55–67 (1970).\n71. Shwartz-Ziv, R. & Armon, A. Tabular data: deep learning is not all you need. Inf. Fusion 81, \n84–90 (2022).\n72. Vanschoren, J., van Rijn, J. N., Bischl, B. & Torgo, L. OpenML: networked science in machine \nlearning. SIGKDD Explor. 15, 49–60 (2014).\n73. Fix, E. & Hodges, J. L. Discriminatory analysis. Nonparametric discrimination: consistency \nproperties. Int. Stat. Rev. 57, 238–247 (1989).\nAcknowledgements We express our gratitude to the following individuals for their valuable \ncontributions and support. We thank E. Bergman for his assistance with the evaluation of \nTabPFN, for helping implement the random forest pre-processing, and for his efforts in \nimproving the code quality and documentation. His contributions were instrumental in \nbenchmarking TabPFN and ensuring the reproducibility of our results. We thank A. Gupta \nand D. Otte for their work on the Inference Server, which enables the fast deployment  \nof TabPFN without the need for a local GPU. Their efforts have greatly enhanced the \naccessibility and usability of TabPFN. We thank L. Schweizer for his work on exploring the \nrandom forest pre-processing for TabPFN further. We thank D. Schnurr and K. Helli for their \nwork on visualization, and D. Schnurr for his specific contributions related to handling \nmissing values. We thank S. M. Lundberg for the collection of visualization methods for \nfeature attribution that we adapted for our work. We thank A. Müller for the insightful \ndiscussions related to TabPFN training and for his guidance on identifying and mitigating \nbiases in the prior. His expertise has been invaluable in refining the TabPFN methodology.  \nWe are very grateful to C. Langenberg and M. Pietzner for providing insights on medical \napplications, interpreting model results and offering general advice. Their continued \nsupport has been instrumental in shaping this work. We thank S. Stäglich for his outstanding \nmaintenance and support with the cluster infrastructure. We thank B. Lake for his general \npaper writing advice. We are grateful for the computational resources that were available for \nthis research. Specifically, we acknowledge support by the state of Baden-Württemberg \nthrough bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 \nFUGG (bwForCluster NEMO), and by the Deutsche Forschungsgemeinschaft (DFG, German \nResearch Foundation) under grant no. 417962828. We acknowledge funding by the Deutsche \nForschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), \ngrant no. 499552394, and by the European Union (through ERC Consolidator Grant \nDeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of \nthe authors only and do not necessarily reflect those of the European Union or the European \nResearch Council. Neither the European Union nor the granting authority can be held \nresponsible for them. F.H. acknowledges the financial support of the Hector Foundation.\nAuthor contributions N.H. improved the prior of the model; added regression support, \nunsupervised capabilities and inference optimizations; and contributed to the experiments \nand wrote the paper. S.M. improved the neural network architecture, training and efficiency; \nadded inference optimizations; and contributed to experiments and wrote the paper. L.P. \nimproved the inference interface of the model; contributed to hyperparameter tuning; added \npost hoc ensembling of TabPFN models; contributed to benchmarking; and wrote the paper. \nA.K. added inference optimizations and Kaggle experiments. M.K. contributed to inference \noptimizations. S.B.H. contributed to the usability of our code. R.T.S. contributed to preliminary \narchitectural experiments to speed up inference and helped revise the first draft of the paper. \nF.H. contributed technical advice and ideas, contributed to the random forest pre-processing, \nmanaged collaborations and funding, and wrote the paper.\nCompeting interests The following patent applications invented by S.M. and F.H. and filed by \nR. Bosch are related to this work: DE202021105192U1 and DE102021210775A1. The authors do \nnot have any ownership rights to these patent applications. F.H. and N.H. are affiliated with \nPriorLabs, a company focused on developing tabular foundation models. The authors declare \nno other competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-024-08328-6.\nCorrespondence and requests for materials should be addressed to Noah Hollmann,  \nSamuel Müller or Frank Hutter.\nPeer review information Nature thanks Duncan McElfresh, Oleksandr Shchur and the other, \nanonymous, reviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\nArticle\nExtended Data Fig. 1 | Performance comparison across additional dataset \ncharacteristics, extending Fig.  5. This figure shows the relative performance \nof different methods when datasets are split based on specific attributes. Error \nbars represent 95% confidence intervals. While performance differences are \ngenerally subtle across these splits, the most notable variation is observed for \ndatasets with outliers in the target variable, though confidence intervals still \noverlap.\nExtended Data Fig. 2 | Performance comparisons of TabPFN and baselines \non additional benchmark datasets and with GPU support.  (a) Classification \nperformance on the Grinsztajn medium-sized benchmark with categorical \nfeatures, across 7 datasets. (b) Classification performance on the Grinsztajn \nmedium-sized benchmark with numerical features, across its 15 datasets.  \n(c) Classification performance on the TabZilla benchmark, consisting of 102 \ndatasets with fewer than 10,000 rows of data, 500 features, and 10 classes. \nDuplicated datasets and those with fewer than 5 samples per class were \nremoved to enable 5-fold cross-validation. (d) Performance Over Time \nComparison with CPU vs. GPU Hardware: The performance over time when \nrunning our strongest baselines with eight CPUs (CPU) vs. eight CPUs and on \none GPU (+GPU) on our classification test benchmark. AutoGluon automatically \ndecides which models to train with what resources. For CatBoost and XGB, we \nspecified that the models should train with GPU. Intervals represent 95% CI.\nArticle\nExtended Data Fig. 3 | Comparing SHAP (SHapley Additive exPlanations) \nsummary plots between TabPFN and baselines. We compare SHAP feature \nimportance and impact for Logistic Regression, TabPFN, and CatBoost on the \n“Default of Credit Card Clients” dataset. The top features visualized are credit \namount, age, and duration. Each point represents a single instance, with the \ncolor indicating the value of the checking status feature (blue for low, red for \nhigh), illustrating its interaction with the respective feature on the x-axis.  \nWe see that Logistic Regression is most interpretable due to the simple underlying \nfunctions. However, Logistic Regression has poor predictive accuracy, and the \nlearned functions are unintuitive when looking at the outer bounds of features. \nTabPFN has good predictive accuracy and learns simple, interpretable functions. \nCatBoost is the least interpretable, with unclear patterns and wide variation in \nSHAP values per sample. This figure is adapted from Lundberg et al. 47.\nExtended Data Fig. 4 | Finetuning TabPFN on 2-dimensional sine curve \ndatasets.  (a) Examples of 2D sine curve datasets with different offsets.  \n(b) Finetuning loss curves for 50 runs with random train-test offsets. Colors \nindicate the offset between train and test. TabPFN shows positive transfer,  \nwith better performance for more similar distributions. For a dataset shift of π, \nthe inverse label needs to be predicted in the test set, compared to the finetuning \ndata. However, TabPFN still generalizes when finetuned on this data.\nArticle\nExtended Data Table 1 | Aggregated results on the 29 AMLB classification Benchmark datasets\nScores are normalized on all the baselines shown in this table, with the weakest score set to 0.0 and the highest to 1.0, per dataset. All baselines are optimized for ROC AUC thus trading-off \nrepresentativeness of secondary metrics. Times for TabPFN refer to times on GPU. Datasets are available via OpenML https://www.openml.org/search?type=data&sort=runs&id={OPENML_ID}. \nExact train-test splits defined by OpenML tasks with task numbers in our code datasets/benchmark_dids.py.\nExtended Data Table 2 | Aggregated results on the 28 AMLB and OpenML-CTR23 regression Benchmark datasets\nScores are normalized on all the baselines shown in this table, with the weakest score set to 0.0 and the highest to 1.0, per dataset. K-Nearest Neighbors73 performed significantly worse than  \nthe considered baselines. All baselines are optimized for RMSE as an objective thus trading-off representativeness of secondary metrics. Times for TabPFN refer to times on GPU. Datasets  \nare available via OpenML https://www.openml.org/search?type=data&sort=runs&id={OPENML_ID}. Exact train-test splits defined by OpenML tasks with task numbers in our code datasets/\nbenchmark_dids.py.\nArticle\nExtended Data Table 3 | List of test datasets used for primary evaluation of classification tasks\nAll classification tasks from the AutoML Benchmark36 with fewer 10,000 samples and 500 features. The benchmark comprises diverse real-world tabular datasets, curated for complexity, \nrelevance, and domain diversity.\nExtended Data Table 4 | List of test datasets used for primary evaluation of regression tasks\nAll regression tasks from the AutoML36 and OpenML-CTR2337 Benchmarks with fewer 10,000 samples and 500 features. The benchmark comprises diverse real-world tabular datasets, curated \nfor complexity, relevance, and domain diversity.\nArticle\nExtended Data Table 5 | Hyperparameter defaults and search space for TabPFN and our baselines\n(a) TabPFN search space (b, c) Baseline search spaces.\nExtended Data Table 6 | Performance on Kaggle Data Science Challenges\nPerformance of default CatBoost and default TabPFN on all 5 Kaggle classification or regression competitions from the Tabular Playground Series Season 3 with late submission enabled, fewer \nthan 10,000 rows of data, 500 features, and 10 classes. We report the private score averaged over 5 seeds. For Episode 5, as ordinal regression can be treated as a classification or regression \ntask, for both CatBoost and TabPFN we tried both the regression and the classification model and chose the better of the two (regression for CatBoost; classification for TabPFN). Arrows indicate \nthe optimization direction for each metric. We emphasize that these results only compare Catboost and TabPFN on the raw competition data, not using any of the tricks the ingenious Kaggle \ncommunity applies, such as use of domain knowledge, data cleaning, special feature engineering, postprocessing and ensembling; nevertheless, these techniques can be combined with \nTabPFN, and we hope that TabPFN’s improved base model performance will allow Kagglers to achieve even better results with them.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6671597361564636
    },
    {
      "name": "Raw data",
      "score": 0.5811752080917358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5230104327201843
    },
    {
      "name": "Deep learning",
      "score": 0.49303194880485535
    },
    {
      "name": "Machine learning",
      "score": 0.492350697517395
    },
    {
      "name": "Ensemble learning",
      "score": 0.4294127821922302
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4283597469329834
    },
    {
      "name": "Data mining",
      "score": 0.33902832865715027
    },
    {
      "name": "Data science",
      "score": 0.33484429121017456
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}