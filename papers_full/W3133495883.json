{
  "title": "CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation",
  "url": "https://openalex.org/W3133495883",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4298918401",
      "name": "Magassouba, Aly",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2664316294",
      "name": "Sugiura, Komei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2601675640",
      "name": "Kawai Hisashi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3107094551",
    "https://openalex.org/W2964339842",
    "https://openalex.org/W3192162805",
    "https://openalex.org/W2991186560",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W2963922343",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035232877",
    "https://openalex.org/W3023306062",
    "https://openalex.org/W3035691840",
    "https://openalex.org/W3013640429",
    "https://openalex.org/W2953626316",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3121465735",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3034500398",
    "https://openalex.org/W3198196812",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W2951973805",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2069809153"
  ],
  "abstract": "Navigation guided by natural language instructions is particularly suitable for Domestic Service Robots that interacts naturally with users. This task involves the prediction of a sequence of actions that leads to a specified destination given a natural language navigation instruction. The task thus requires the understanding of instructions, such as ``Walk out of the bathroom and wait on the stairs that are on the right''. The Visual and Language Navigation remains challenging, notably because it requires the exploration of the environment and at the accurate following of a path specified by the instructions to model the relationship between language and vision. To address this, we propose the CrossMap Transformer network, which encodes the linguistic and visual features to sequentially generate a path. The CrossMap transformer is tied to a Transformer-based speaker that generates navigation instructions. The two networks share common latent features, for mutual enhancement through a double back translation model: Generated paths are translated into instructions while generated instructions are translated into path The experimental results show the benefits of our approach in terms of instruction understanding and instruction generation.",
  "full_text": "CrossMap Transformer: A Crossmodal Masked Path Transformer\nUsing Double Back-Translation for Vision-and-Language Navigation\nAly Magassouba1, Komei Sugiura 2, and Hisashi Kawai 1\nAbstract— Navigation guided by natural language instruc-\ntions is particularly suitable for Domestic Service Robots that\ninteract naturally with users. This task involves the prediction of\na sequence of actions that leads to a specified destination given a\nnatural language navigation instruction. The task thus requires\nthe understanding of instructions, such as “Walk out of the\nbathroom and wait on the stairs that are on the right”. Visual\nand Language Navigation remains challenging, notably because\nit requires the exploration of the environment and the accurate\nfollowing of a path specified by the instructions to model the\nrelationship between language and vision. To address this, we\npropose the CrossMap Transformer network, which encodes\nthe linguistic and visual features to sequentially generate a\npath. The CrossMap transformer is tied to a Transformer-based\nspeaker that generates navigation instructions. The two net-\nworks share common latent features, for mutual enhancement\nthrough a double back translation model: Generated paths\nare translated into instructions while generated instructions\nare translated into paths. The experimental results show the\nbenefits of our approach in terms of instruction understanding\nand instruction generation.\nI. I NTRODUCTION\nDomestic service robots (DSRs) are promising solutions\nfor the support of older adults and disabled people. Efforts\nare increasingly being made to standardize DSRs to provide\nvarious support functions [1]. Among these functions, the\nability to navigate in an indoor environment is crucial as it\nis a pre-requisite to many daily life tasks such as fetching\na glass of water from the kitchen. However, in the case of\nmost DSRs, the ability to interact through language, while\nbeing user-friendly for the non-expert user, is limited by the\ncomplexity of understanding natural language.\nIn this context, we focus on understanding natural lan-\nguage instructions for indoor navigation. This task involves\npredicting a sequence of actions to reach a goal destination\nfrom instructions such as “ Go down stairs. At the bottom\nof the stairs walk through the living room and to the\nright into the bathroom. Stop at the sink. ” Such a task\npresents several challenges related to the ambiguity of the\ninstructions because the many-to-many nature of mapping\nbetween language and the real world makes it difficult to\naccurately predict user intention. In particular, unlike the\nunderstanding of manipulation instruction [2], [3] based on\na single environment state, this task requires the language to\nbe mapped to changing states as the DSR moves towards the\n1The authors are with the National Institute of Information and Commu-\nnications Technology, 3-5 Hikaridai, Seika, Soraku, Kyoto 619-0289, Japan.\nfirstname.lastname@nict.go.jp\n2The author is with Keio University, 3-14-1 Hiyoshi, Kohoku, Yokohama,\nKanagawa 223-8522, Japan. firstname.lastname@keio.jp\nFig. 1: Our approach, the CrossMap Transformer is used to predict\nthe sequence of actions to navigate to a goal destination given an\ninstruction.\ndestination. Furthermore, navigation instructions are gener-\nally longer than manipulation instructions, which increases\nthe complexity of the task.\nThe task has recently been formalized as visual-and-\nlanguage navigation [4] (VLN), and many approaches based\non data-driven methods [5], [6] have been proposed. Classi-\ncally, these approaches exploit a recurrent neural network to\ninfer a sequence of actions leading to the desired destination.\nAlthough, these approaches have shown promise, their level\nof accuracy remains far from that of a person [4].\nTo narrow this accuracy gap, we propose the Cross-modal\nMasked Path (CrossMap) Transformer. Motivated by the\nrecent development of transformer networks [7] for lan-\nguage modeling [8], [9], the CrossMap Transformer encodes\nlinguistic and environment state features to sequentially\ngenerate actions similarly to recurrent network approaches.\nOur approach uses feature masking to better model the rela-\ntionship between the instruction and environment features.\nAdditionally, we use a double back translation (DBT)\napproach. Unlike [5] where a speaker network is trained\nseparately to generate instructions, the DBT consists in\narXiv:2103.00852v2  [cs.RO]  21 Aug 2023\nmutually training the speaker network, and the CrossMap\nTransformer, by using common latent features.\nThe present work makes the following key contributions\nto the literature:\n• We propose the CrossMap Transformer, which sequen-\ntially generates actions to reach an instructed destina-\ntion from linguistic and environment state features. We\nexplain the method in Section IV.\n• We propose a double back-translation to improve the\nmapping between linguistic and action features as ex-\nplained in Section IV.\n• We apply the CrossMap Transformer on the stan-\ndard Room-to-Room(R2R) dataset [4]. Our approach\nachieves results comparable to those of recently pro-\nposed state-of-the-art recurrent neural network methods.\nWe present the experimental validation in Section V.\nII. R ELATED WORK\nRobot navigation [10] and path planning from natural lan-\nguage instructions have been widely investigated in the field\nof robotics [11]–[13]. Such a task was recently formalized\nadopting data-driven methods [4] with the release of the R2R\ndataset. In this setup, the VLN task [14] is addressed using\nLong Short Term Memory (LSTM) networks structured in an\nencoder-decoder framework. An instruction is encoded first\nand then decoded as a sequence of actions using the current\nenvironment states. Initially, the VLN method uses low-level\naction spaces, where each motion (e.g., left, right or forward\nmotion) of the robot is predicted. The use of a panoramic\naction space [5] has been shown to improve results as the\naction sequence directly moves the robot from one position\nto another. However, the complexity of the VLN task is\nemphasized by the large gap between human performance\nand the performance of neural models. Additionally, the\ntrade-off between exploration and instruction fidelity has\nbeen emphasized in [15] and additional evaluation metrics,\nsuch as the dynamic time warping, have been proposed.\nTo overcome these limitations, several works [5], [16]\nproposed exhaustive exploration of the environment. Al-\nthough these approaches are generally more accurate, they\nare not feasible in real-world environments. Another line of\nwork relates to data augmentation [17]. Indeed, the R2R\ndataset is relatively small and introduces several biases in\nthe training set distribution [4]. To mitigate these biases, the\nRoom-for-Room (R4R) dataset was introduced in [15] by\nsynthetically concatenating several paths and instructions.\nA back-translation model, a speaker [5], has been intro-\nduced to generate additional instructions for unlabeled path.\nThis approach was extended in [6] with the environmental\ndropout, to improve the back-translation model by synthet-\nically generating new environments from dropped features.\nThe exploration of auxiliary tasks [18], such as ensuring the\nconsistency of the path has had positive results. Globally,\npre-training on a large dataset has been proven to improve\nthe generalization of VLN models [19], [20].\nOur approach, using the CrossMap Transformer, is based\non transformer networks. Although such structures have been\nFig. 2: For the VLN task, each environment is given as a navigation\ngraph where each node is a panoramic view and each edge an action\nto move from one node to another one.\nwidely used in language modeling [8], [9], [21], few works\nhave used transformers to address the VLN task. In [19],\ntransformers were used to score the compatibility between\npaths and instructions, in the setting of the pre-exploration of\nan environment. In line of works such as [20], [22], trans-\nformers were used in pretraining the language embedding\nmodel by combining visual and linguistic features. The path\nwas nonetheless predicted from an LSTM-based encoder-\ndecoder. Conversely, [23] proposed to using transformers\nto decode the instructions and environment features as a\nsequence of actions. Nonetheless, there remains a gap in\nperformance between this approach and classic LSTM-based\nnetworks.\nIII. P ROBLEM STATEMENT\nA. Task background\nWe aim to solve the VLN task in real-world home en-\nvironments as described in [4]. More specifically, given a\nnatural navigation instruction, this task involves predicting a\nsequence of actions that leads to a goal destination. This task\nrequires the understanding of instructions such as “Walk out\nof the bathroom and wait on the stairs that are on the right”\nand “Go down stairs. At the bottom of the stairs walk through\nthe living room and to the right into the bathroom. Stop at\nthe sink.” Such a task is particularly relevant to DSRs, as it\ncomplements manipulation tasks [2], [3] for communicative\nrobots that interact naturally with users. Nonetheless, the\nVLN task is challenging because it requires natural and long\nsentences to be addressed, where the relationships among\nindividual words and with the current environment state\nshould be modeled.\nAdditionally, this task requires a trade-off between ex-\nploration of the environment and fidelity to the instruction.\nIndeed, it has been shown that methods based on exploration\noutperform methods that learn only from the ground truth\npaths [4]. This is explained by the difference in the distri-\nbutions of the training and test data when learning only us-\ning ground truth methods. Meanwhile, although exploration\nyields better results, such an approach induces discrepancies\nbetween the instruction and generated path [15] which limits\nthe modeling between the instruction and physical world.\nHence, many studies [5], [19] have propose exhaustively\nexploring the environment and selecting the best path to\nreach the specified destination. However, considering a phys-\nical deployment on DSRs, such methods are cumbersome\nFig. 3: The CrossMap Transformer (CMT) model endows linguistic, visual and action transformers to predict the sequence of actions\ngiven an instruction and a causal masking structure.\nand time consuming as an exhaustive map should be built\nand explored before performing the required task. As so, the\nwork, we consider single run approaches, where only one\npath is generated on-the-fly.\nB. Task Description\nThe task environments of different size are in real home\nscenes as depicted in Fig. 2. Each scene is discretized\ninto connected nodes that build different paths. A node\ncorresponds to a 360 degree panoramic image.\nSimilarly to many other works [5], [20], we consider that\nthe VLN is performed through adopting a panoramic action\nspace instead of atomic actions [4] (i.e., atomic motion).\nHence, the navigation task can be represented as a graph\nwhere each node represents a waypoint in the scene (see\nFig. 2). Each edge of this graph is an action to move from\none node to another. A path is then a sequence of nodes from\nan initial node to a destination node.\nUnlike the aforementioned studies [5], [19] that addressed\nthis task through full knowledge of the navigation graph, we\nconsider a single-run setup where only the current node and\nadjacent edges are known.\nIn this setup, the VLN task is sequential and for every\ntime step t the following inputs and outputs are expected:\n• Inputs (t = 0): A navigation instruction as a sentence.\n• Inputs (t ≥ 0): The current node (waypoint), the\nadjacent edges, and a panoramic image taken at the node\n(waypoint).\n• Output (t ≥ 0): Next action (edge) among the adjacent\nedges.\nFour evaluation metrics are considered in this study, that\nis, the Success Rate (SR) which is the rate of successfully\ngenerating a path (arriving within 3 meters of the desired\ndestination), the navigation error (NE), which is the mean\ndistance of arrival from the desired destination, the Success\nweighted by Path Length (SPL), which is the ratio of\nsuccessful predictions normalized by the path length, and\nthe oracle success rate (OSR), which is the rate of generated\npaths that cross within 3 meters of the desired destination.\nIV. P ROPOSED METHOD\nA. Novelty\nAs explained in the previous section, there is a gap\nin performance between state-of-the-art methods using the\nLSTM architecture [6] and those using transformers [23].\nIndeed, transformers are generally used for feature modeling\n[8], [24], achieving state-of-the-art results. Text generation\nhas also been addressed using transformers [9], [25] and\ncausal masking [26]. In this line of works, each predicted\ntoken attends only tokens at previous positions. Nonetheless\nvery few transformer architectures are optimized for the VLN\ntask. Yet transformers can be trained faster than LSTMs [7]\nfor long sequences and can take advantages of the recently\nreleased pretrained models on generic tasks using linguistic\nand visual modalities [24], [27].\nInspired by this line of work, we propose a transformer\narchitecture for VLN, the CrossMap Transformer (CMT)\nillustrated in Fig. 3. our approach predicts a sequence of\nactions that reaches the specified goal via causal masking and\nat the same time learns the relationship between the instruc-\ntion and the corresponding path. Furthermore, we introduce\na transformer-based speaker, CrossMap Speaker (CMS) to\nimprove the generalization ability of our approach through\na double back-translation (DBT) model. We advocate that\na better model can be obtained when mutually training the\nCMS and CMT networks with common latent features.\nThe CMT has the following characteristics:\n• The CMT combines visual and linguistic modalities to\npredict a path through a sequence of actions.\n• The CMT adopts cross-modal path masking to model\nthe relationship between the navigation path and instruc-\ntion.\n• The CMT and CMS are mutually trained through DBT,\nwhich enhances both networks.\nB. CrossMap Transformer Architecture\n1) Network Inputs: Let us consider an instruction i, such\nthat at each time step t of the sequence, the set of inputs of\nthe network is defined as:\nxi(t) ={xi\nl, xi\nc(t), xi\na(t)}, (1)\nwhere xi\nl and xi\na(t) respectively denote the linguistic and\nprevious action inputs and xc(t) is the current navigation\nnode. In the following, the index i is omitted for simplicity.\nIn detail, xl is the embedded instruction. The current\nnavigation node is given as a set of N image views so that\nxc(t) ={I0(t), I1(t), . . . , In(t), . . . , IN (t)}, (2)\nwhere each image In(t) is a portion of the input panoramic\nimage at step t. In this study, we consider that N = 36 as\nproposed in [4].\nThe previous actions are given as a sequence\nxa(t) ={E(0), E(1), . . . , E(t − 1)}, (3)\nof the (t − 1) steps actions performed. An action E(t) is\ndefined as a given view of the current panorama in the\ndirection of the next navigation node.\n2) Language Encoder: Each instruction xl is initially\ntokenized into subwords that are then embedded as 384-\ndimensional vectors using MiniLM network [21]. MiniLM\nis the distillated version of UniLM [9], which is a state-\nof-the-art method of masked language modeling based on\ntransformers. These vectors are then processed through the\nlanguage encoder. The language encoder is a two-layer\ntransformer that takes as input the tokens features and the\ncorresponding positional encoding. The maximum instruc-\ntion sequence length is set to 42, that is, 40 tokens and\nas the beginning-of-sentence ( <CLS>) and end-of-sentence\n(<EOS>) tokens. Each sentence is padded according to its\nlength with the padding token ( <PAD>).\nIn the language encoder, self-attention heads process the\ninput vectors and are followed by a fully connected feed-\nforward network. The output of an attention head is given\nby [7]:\nA = V softmax\n\u0012QKT\n√dk\n+ M\n\u0013\n, (4)\nwhere Q, K and V are the queries, keys and values, whereas\nM is a mask matrix, controlling where each token can\nattend. The mask M takes a value of 0, −∞ to allow or\nprevent attention. Within this transformer, self-attention and\nFig. 4: CrossMap Speaker (CMS) configuration that combines the\nsequences of latent actions features from the CrossMap Transformer\n(CMT) and the instructions. The CMS uses both causal and bidi-\nrectional masking.\nlinguistic cross-modal attention are applied as defined in\nEquation (4) by also considering the current environment\nstate through the xc(t) to condition the language input. In the\nself-attention configuration, M corresponds to a linguistic\nmask matrix while Q, K and V are projections of the\ntransformer hidden vector. For the linguistic cross-modal\nattention, the queries Q and values V are projections of\nthe visual features xc(t). The <CLS> is used as the current\nrepresentation of the instruction and is output as a vector\nhl0 (t) of dimension 1 ×384.\n3) Visual Encoder: Each image is encoded as the concate-\nnation of the semantic view provided in [28] and the ResNet-\n152 [29] features as provided in [4]. The intuition behind this\napproach is to use different granularity of features. Indeed,\nResNet-152 provides low-level features while the semantic\nfeatures provides to high-level features.\nA positional feature is also input to the visual encoder\nby concatenating each visual feature with the set of features\n[cos(θn), sin(θn), cos(ϕn), sin(ϕn)] that encodes the relative\nazimuth θn and elevation ϕn of a view In with respect to\nthe current robot orientation.\nSimilarly to the linguistic encoder, cross-modal attention\nis used in the visual encoder. Indeed, following the self-\nattentive transformer layer, each attention head is conditioned\nby linguistic features obtained from the linguistic encoder. A\nvisual representation at the step t is obtained using <CLS>\ntoken. This token is processed and output as a vector hv0 (t)\nof dimension 1 ×384.\n4) Action Decoder: The action decoder predicts the like-\nlihood of all candidate actions given the linguistic and visual\nfeatures at step t and the previous sequence of actions. Each\naction is embedded similarly for each panoramic view, that\nis by concatenating the high-level semantic features and\nlow-level ResNet-152 features. Additionally, two types of\npositional encoding are used for the action decoder. The first\npositional encoding corresponds classically to the position of\nthe action in the sequence. For the second type of positional\nencoding, each action is concatenated with the set of features\n[cos(θm), sin(θm), cos(ϕm), sin(ϕm), ρm] that encodes the\nrelative azimuth θm and elevation ϕm and distance ρm to\nan adjacent node m with respect to the current robot pose.\nFor the stop action, the azimuth and elevation angles as well\nas the distance are set to zero.\nThese inputs are processed by the action decoder.\nThe first layer outputs {oa(0), . . .oa(t)}, which is\nthen concatenated with the corresponding context feature\nC(t) = hl0 (t) ⊕ hv0 (t). The features {oa(0) ⊕\nC(0), . . . ,oa(t) ⊕ C(t)} are then processed by the second\nlayer of the transformer. It is noted that the intermediate\noutputs {oa(0), . . . ,oa(t)} are also used for the CMS net-\nwork. Such an architecture allows training both the CMT and\nCMS. For the action decoder, a causal masking is used, that\nis M corresponds to a mask where each embedded feature\nattends only the previous features of the sequence. The last\noutput ha(t) of the transformer is then used to compute the\nlikelihood p(y)(t + 1)given as\np(y)(t + 1) =ha(t)y(t + 1), (5)\nwhere y corresponds to the set of candidate actions from the\ncurrent robot position.\n5) Loss function: To predict the likelihood p(y(t)) of an\naction, the loss function L as the cross-entropy function is\nexpressed as:\nL = −\nX\nn\nX\nm\ny∗\nnm log p(ynm), (6)\nwhere y∗\nnm denotes the label given to the m-th dimension\nof the n-th sample, and ynm denotes its prediction. As\nperformed in [4], the next action xa(t + 1)in the sequence\nis sampled from p(y(t)) to allow the agent to explore the\nenvironment.\n6) Cross-modal Path Masking: A novelty of the CMT is\nthe cross-modal path masking. In addition to the loss function\ndescribe above, the CMT learns the relationship between the\nground-truth paths and the corresponding instruction. Simi-\nlarly to Natural Language Processing (NLP) methods such\nas BERT [8], a ground-truth sequence of action is randomly\nmasked, and predicted by the network. Given a ground-truth\nsequence of action {y(0), . . .y(T − 1), y(T)} of size T,\nand a masked position m, m ≤ T, the CMT computes\np(y(m)|y(m − 1), . . . ,y(0)) through a causal mask and\nthe same transformer architecture described previously. This\napproach is used in the pre-training phase.\n7) Speaker: In addition to the CMT, we introduce the\nCMS (see Fig. 4) network that is a two-layer transformer\ngenerating a sentence from a sequence of actions. The\nCMS uses a similar architecture as visual and language\ntransformers such as VLP [27]. The inputs of the CMS are\nthe full sequence of latent action features oa(t) that are\ncomputed by the CMT. The CMS generates the sequence\nof words using alternatively causal masking to generate the\nsequence of words and bi-directional masking to learn the\nrelationship between the instruction and the sequence of\nactions.\nTABLE I: Parameter settings and structures of the CrossMap\nTransformer (CMT)\nCMT Adam (lr = 5× 10−4,\nOpt. method β1 = 0.99, β2 = 0.9)\nNb. layers 2\nHidden size 348\nLanguage Model MiniLM [21]\nActivation ReLU\nNb. heads 12\nFeed-forward size 1534\nDropout 0.1\nEnv. Dropout 0.4\nλ (SPICE) 20\nBatch size 50\n8) Double back-translation: Such an architecture where\nthe CMT and CMS share the same features representation\nallows oa(t) to be concurrently processed into a sequence\nof actions and translated into an instruction. This approach\nmutually enhances the CMT and CMS network, differently\nto the method proposed in [5] where the speaker is trained\nseparately. In addition, more classically, we used a second\ntranslation by using the sentences generated by the CMS to\ntrain the CMT. We define these two levels of translation as\nthe double back translation (DBT).\nTo perform the DBT, the CMS minimizes the cross-\nentropy loss between the generated sentence and the original\ninstruction. In this configuration, each input sample oa(t),\nof a full sequence generated by the CMT, is used to train\nthe CMS only if the instructed destination is successfully\nreached. A mask is used to discard the samples that did not\nreach the correct destination.\nIn a second phase the CMS generates sentences from\nlatent features oa(t) obtained from ground-truth paths. These\nsentences are used to train the CMT. Only sentences with a\nscoring metric greater than a threshold λ are used.\nDuring the inference phase, if the average CMS scoring\nmetric of the validation set is greater than λ, for the next\nepoch, the CMS is then used to generate sentence on the\nunlabeled training paths. These newly labeled paths are used\nas augmented data to train the CMT. By filtering both the\nCMT and CMS, we limit the impact of noisy generated\nfeatures as analyzed in [30].\nV. E XPERIMENTS\nA. Experimental Setup\nParameters of the CMT are summarized in Table I. Each of\nthe transformers comprised two layers of 12 attention heads,\nwith a hidden dimension of 384, while the feed-forward\nlayers had a dimension 1,534. We applied a dropout rate\nof 0.1 to each layer. In the visual encoder, we used the\nenvironmental dropout on the visual features with a rate of\n0.4.\nThe training procedure was divided into two phases.\nFirst, both the CMT and CMS were mutually pre-trained\nusing ground truth paths and cross-modal path masking. In\nthe second phase, both networks were fine-tuned following\nTABLE II: Comparison results of the VLN task for LSTM-based decoder and Transformer-based decoder under several metrics: Success\nRate (SR), Navigation Error (NE), Success Path Length (SPL) and Oracle Success Rate (OSR). The results for PREV ALENT* are obtained\nfrom our own implementation.\nDecoder Validation Seen Validation Unseen\nMethod Type SR↑ NE↓ SPL↑ OSR↑ SR↑ NE↓ SPL↑ OSR↑\nSeq2Seq [4] 0.39 6.01 − 0.53 0.22 7.81 − 0.28\nSpeaker-Follower [5] 0.66 3.36 − 0.74 0.35 6.62 − 0.45\nRCM [16] 0.67 3.37 − 0.77 0.43 5.88 − 0.52\nEnvDrop [6] LSTM 0.62 3.99 0.59 − 0.52 5.22 0.48 −\nAuxRN [18] 0.70 3.33 0.67 0.78 0.55 5.28 0.50 0.62\nPREV ALENT [20] 0.69 3.67 0.65 − 0.58 4.71 0.53 −\nPREV ALENT* 0.70 3.48 0.67 0.77 0.57 4.66 0.52 0.65\nCMG-AAL [31] 0.73 2.74 0.69 − 0.59 4.18 0.51 −\nPTA [23] 0.66 3.35 0.64 0.74 0.43 5.95 0.39 0.49\nCMT (BT-only) Transformer 0.67 3.43 0.59 0.74 0.51 4.89 0.40 0.58\nCMT (Ours) 0.73 2.82 0.63 0.80 0.55 4.60 0.44 0.63\nthe setup given in Section IV. Unlike previous works, we\nselected Spice [32] as the CMS scoring metric, and set\nλ = 20. It was shown in [30], that this metric is most\nconsistent with human labelling in the VLN for the task.\nThe CMT was trained on a machine equipped with four\nTesla V100 with 32 GB of memory, 768 GB RAM and an\nIntel Xeon 2.10 GHz processor. The results were reported\nafter 300 epochs. With this setup, it took around 1 day to\ntrain the CMT with a batch size of 50 samples and at learning\nrate of 5 ×10−4.\nB. R2R Dataset\nThe R2R dataset [4] is based on the Matterport3D [28]\nenvironments annotated with navigation instructions. The\ndataset contains 21,567 instructions, which are 29 words\nlong on average, for 90 different environments. The dataset\nwas split with 14,025 instructions in 60 different envi-\nronments as the training set. Two different validation sets\nwere considered. The first one, validation seen, contained\n1,020 instructions for the same environments as the training\nset. The second one, validation unseen, contained 2,349\ninstructions for 11 environments different from the training\nset environments.\nAdditionally, for data augmentation, we used the approx.\n170,000 collected paths given in [5]. These paths were\ncollected in the training environments and were initially\nunlabeled.\nC. Quantitative Results\nWe compared the CMT with state-of-the-art methods in\nterms of the metrics defined in Section III for the two\nvalidation sets of the R2R dataset. It is emphasized that we\nperformed a comparison with approaches that use the same\nsetups as in this study. More explicitly, these setups are the\npanoramic action space and single run setups, without pre-\nexploration or beam-search.\nThe results reported in Table II indicates that our approach\nobtained results, except for the SPL metric, comparable to\nthose of the currently best performing methods, PREV A-\nLENT [20] and CMG-AAL [31]. When compared with\nTABLE III: Effect of path masking and DBT of the CMT for the\nvalidation sets. The values in (·) represent the standard deviations.\nCMT-type1 refers to a model without DBT and path masking, while\nCMT-type2 is a model without DBT.\nValidation Seen\nMethod SR↑ NE↓ SPL↑ OSR↑\nCMT-type1 0.59 4.00 0.51 0.70\n(0.01) (0.16) (0.03) (0.04)\nCMT-type2 0.64 3.71 0.53 0.71\n(0.02) (0.33) (0.03) (0.03)\nValidation Unseen\nMethod SR↑ NE↓ SPL↑ OSR↑\nCMT-type1 0.48 5.44 0.35 0.56\n(0.02) (0.13) (0.02) (0.03)\nCMT-type2 0.50 5.01 0.38 0.55\n(0.02) (0.14) (0.04) (0.02)\nanother Transformer-based approach such as PTA [23], the\nCMT achieved better results for all metrics except the SPL in\nthe case of the validation seen dataset. We hypothesize that\nthe lower performance in terms of SPL metric may be related\nto the fact that we did not optimize the CMT for the shortest\npath unlike [5], [6]. These works adopt a reinforcement\nlearning approach, which penalizes, among other things, the\nnumber of step taken (longer paths). Nonetheless, it is worth\nmentioning that the SPL scores the optimal (shortest) path\nbut fails to take into account the similarity between the\nreference and generated trajectory as analyzed in [15].\nAdditionally our result CMT (BT-only) that uses only\nback-translation as in II), emphasizes the relevance of us-\ning a DBT approach to improve both validation seen and\nunseen metrics. Furthermore, this result shows that using a\ntransformer-based decoder can be beneficial to the VLN task,\nas the CMT (BT-only) generalize better on the validation\ndatasets than the Speaker-Follower or EnvDrop methods.\nIndeed, EnvDrop improves the Speaker-Follower for the\nunseen dataset, but performs worse on the unseen dataset.\n(a) Ground Truth (GT): “Enter the bedroom. Turn left and exit the bedroom through the door. Wait by stairs”. || CMS: “Exit the bathroom and\nturn left. walk past the bed and exit the room. wait there”.\n(b) GT: “Go down the stairs, go slight left at the bottom and go through door, take an immediate left and enter the bathroom, stop just inside in\nfront of the sink”. || CMS: “Go down the stairs and turn left. walk straight and enter the room on the left. stop in front of the sink”.\n(c) GT: “Take a right and walk out of the kitchen. Take a left and wait by the dining room table”. || CMS: “Walk past the kitchen and turn left.\nwalk past the dining room table and chairs and stop”.\n(d) GT: “Head around the table and go the main area left of the long table. Go to the middle of the room next to the ping pong table and stop”.\n|| CMS: “Walk past the ping pong table and wait by the ping pong table”.\nFig. 5: Qualitative results of the CMT represented as a sequence of actions from an initial panoramic image and a ground truth (GT)\ninstruction. The generated sentences from the CMS are also given.\nTABLE IV: Evaluation of generated sentences given the unseen\nvalidation set\nMethod BLEU-4 CIDEr ROUGE SPICE\nSpeaker [5] 13.5 27.2 33.6 19.2\nCMS-type1 6.4 27.9 27.6 20.9\n(Ours) CMS 5.3 23.6 27.3 21.9\nTo gain a better insight into the CMT, we also performed\nan ablation study considering the cross-modal path masking,\nas well as the DBT; results are given in Tables III. We con-\nsidered two ablation conditions, CMT-type1 that is without\nDBT and path masking, and CMT-type2 that is without DBT.\nThe results emphasize that both ablated parts greatly improve\nthe performance of the CMT.\nD. CrossMap Speaker results\nAn alternative way to assess the contributions of this study\nis to evaluate the performance of the CMS network. As\nstated in Table IV, we evaluated the generated sentences with\nfour captioning metrics that are BLEU-4 [33], ROUGE [34],\nCIDEr [35] and SPICE [32]. The CMS is compared with an\nablated architecture that is trained separately from the CMT,\nnamed CMS-type1, and with the Speaker-follower [5],\nAlthough our approach performed better than the two other\nbaselines only for SPICE metric, our results are consistent\nwith the study [30] that claims that standard captioning\nmetrics, except for SPICE, are ineffective for VLN tasks.\nE. Qualitative Results\nThe qualitative results of the CMT and CMS are illustrated\non Fig. 5. Each rows represents a sample instructions from\nthe validation set, and is given as a sequence of actions.\nThe first three samples samples show successfully predicted\npath from the CMT. The CMS also generated consistent\ninstructions, which suggest that the relation between visual\nand linguistic features was correctly modeled. The last row\nillustrates an erroneous sample for both path prediction and\ninstruction generation. Such a sample illustrates one of the\nchallenges of the VLN task, as the “ping pong table”, which\nis a landmark, was seen from far and from several positions.\nSimilarly several tables were in the seen and should be\ndifferentiated from each other. These challenges also affected\nthe generated instruction.\nVI. C ONCLUSION\nIn the context of the increasing demand for DSRs, we\naddressed visual navigation from natural language instruction\nin home environments. The mainresults of the study are\nsummarized as follows:\n• We proposed the CrossMap Transformer, which sequen-\ntially generates actions to reach an instructed destination\ndetermined from linguistic and visual features.\n• We proposed the double back-translation to improve\nthe mapping between linguistic and actions features\nusing a common structure between a CrossMap Speaker\nnetwork, which translates the sequence of actions into\nan instruction, and the CrossMap Transformer.\n• We achieved comparable results as state-of-the-art ap-\nproaches using LSTM decoders\nIn future work, we will combine our present achievements\nwith linguistic explanation, allowing interaction with non-\nexpert users in failure cases.\nACKNOWLEDGEMENT\nThis work was partially supported by JSPS KAKENHI\nGrant Number 20H04269, JST CREST, JST Moonshot R&D\nGrant Number JPMJMS2011, and NEDO.\nREFERENCES\n[1] L. Iocchi, D. Holz, J. Ruiz-del Solar, K. Sugiura, and T. Van Der Zant,\n“RoboCup@ Home: Analysis and Results of Evolving Competitions\nfor Domestic and Service Robots,” Artificial Intelligence, vol. 229, pp.\n258–281, 2015.\n[2] A. Magassouba, K. Sugiura, and H. Kawai, “A Multimodal Classifier\nGenerative Adversarial Network for Carry-and-Place Tasks From\nAmbiguous Language Instructions,” IEEE RA-L , vol. 3, no. 4, pp.\n3113–3120, 2018.\n[3] A. Magassouba, K. Sugiura, A. Trinh Quoc, and H. Kawai, “Under-\nstanding Natural Language Instructions for Fetching Daily Objects\nUsing GAN-Based Multimodal Target-Source Classification,” IEEE\nRA-L, vol. 4, no. 4, pp. 3884–3891, 2019.\n[4] P. Anderson et al. , “Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments,” in\nIEEE CVPR, 2018, pp. 3674–3683.\n[5] D. Fried, R. Hu, V . Cirik, A. Rohrbach, J. Andreas, L.-P. Morency,\nT. Berg-Kirkpatrick, K. Saenko, D. Klein, and T. Darrell, “Speaker-\nfollower models for vision-and-language navigation,” in Advances in\nNeural Information Processing Systems , 2018, pp. 3314–3325.\n[6] H. Tan, L. Yu, and M. Bansal, “Learning to navigate unseen environ-\nments: Back translation with environmental dropout,” arXiv preprint\narXiv:1904.04195, 2019.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdvances in neural information processing systems , 2017, pp. 5998–\n6008.\n[8] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understand-\ning,” arXiv preprint arXiv:1810.04805 , 2018.\n[9] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\nnatural language understanding and generation,” in Advances in Neural\nInformation Processing Systems , 2019, pp. 13 063–13 075.\n[10] F. Xia, W. B. Shen, C. Li, P. Kasimbeg, M. E. Tchapmi, A. Toshev,\nR. Mart´ın-Mart´ın, and S. Savarese, “Interactive gibson benchmark: A\nbenchmark for interactive navigation in cluttered environments,” IEEE\nRobotics and Automation Letters , vol. 5, no. 2, pp. 713–720, 2020.\n[11] T. Kollar, S. Tellex, D. Roy, and N. Roy, “Toward Understanding\nNatural Language Directions,” in ACM/IEEE HRI, 2010, pp. 259–266.\n[12] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J.\nTeller, and N. Roy, “Understanding Natural Language Commands for\nRobotic Navigation and Mobile Manipulation,” in AAAI, 2011, p. 2.\n[13] Y .-L. Kuo, B. Katz, and A. Barbu, “Deep compositional robotic\nplanners that follow natural language commands,” in ICRA, 2020, pp.\n4906–4912.\n[14] A. Mogadala, M. Kalimuthu, and D. Klakow, “Trends in integration\nof vision and language research: A survey of tasks, datasets, and\nmethods,” Journal of Artificial Intelligence Research , 2019.\n[15] V . Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge,\n“Stay on the path: Instruction fidelity in vision-and-language naviga-\ntion,” arXiv preprint arXiv:1905.12255 , 2019.\n[16] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y .-F. Wang,\nW. Y . Wang, and L. Zhang, “Reinforced cross-modal matching and\nself-supervised imitation learning for vision-language navigation,” in\nIEEE CVPR, 2019, pp. 6629–6638.\n[17] F. Yu, Z. Deng, K. Narasimhan, and O. Russakovsky, “Take the scenic\nroute: Improving generalization in vision-and-language navigation,” in\nIEEE CVPR Workshops, June 2020.\n[18] F. Zhu, Y . Zhu, X. Chang, and X. Liang, “Vision-language navigation\nwith self-supervised auxiliary reasoning tasks,” in IEEE CVPR, 2020,\npp. 10 012–10 022.\n[19] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and\nD. Batra, “Improving vision-and-language navigation with image-text\npairs from the web,” arXiv preprint arXiv:2004.14973 , 2020.\n[20] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning a\ngeneric agent for vision-and-language navigation via pre-training,” in\nIEEE/CVF CVPR, 2020, pp. 13 137–13 146.\n[21] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, “Minilm:\nDeep self-attention distillation for task-agnostic compression of pre-\ntrained transformers,” arXiv preprint arXiv:2002.10957 , 2020.\n[22] X. Li, C. Li, Q. Xia, Y . Bisk, A. Celikyilmaz, J. Gao, N. A. Smith, and\nY . Choi, “Robust navigation with language pretraining and stochastic\nsampling,” in Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing , 2019, pp. 1494–\n1499.\n[23] F. Landi, L. Baraldi, M. Cornia, M. Corsini, and R. Cucchiara,\n“Perceive, transform, and act: Multi-modal attention networks for\nvision-and-language navigation,” arXiv preprint arXiv:1911.12377 ,\n2019.\n[24] Y .-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y . Cheng,\nand J. Liu, “Uniter: Universal image-text representation learning,” in\nECCV. Springer, 2020, pp. 104–120.\n[25] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” 2018.\n[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[27] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso, and J. Gao, “Unified\nVision-Language Pre-Training for Image Captioning and VQA.” in\nAAAI, 2020, pp. 13 041–13 049.\n[28] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva,\nS. Song, A. Zeng, and Y . Zhang, “Matterport3d: Learning from RGB-\nD data in indoor environments,” arXiv preprint arXiv:1709.06158 ,\n2017.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Identity Mappings in Deep\nResidual Networks,” in ECCV. Springer, 2016, pp. 630–645.\n[30] M. Zhao, P. Anderson, V . Jain, S. Wang, A. Ku, J. Baldridge, and E. Ie,\n“On the evaluation of vision-and-language navigation instructions,”\narXiv preprint arXiv:2101.10504 , 2021.\n[31] W. Zhang, C. Ma, Q. Wu, and X. Yang, “Language-guided navigation\nvia cross-modal grounding and alternate adversarial learning,” IEEE\nTransactions on Circuits and Systems for Video Technology , 2020.\n[32] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “SPICE:\nSemantic propositional image caption evaluation,” in ECCV, 2016,\npp. 382–398.\n[33] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method\nfor automatic evaluation of machine translation,” in ACL, 2002, pp.\n311–318.\n[34] C.-Y . Lin, “ROUGE: A package for automatic evaluation of sum-\nmaries,” in Text summarization branches out , 2004, pp. 74–81.\n[35] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “CIDEr:\nConsensus-based image description evaluation,” in IEEE CVPR, 2015,\npp. 4566–4575.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7848808765411377
    },
    {
      "name": "Transformer",
      "score": 0.715887188911438
    },
    {
      "name": "Natural language",
      "score": 0.48511064052581787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45888999104499817
    },
    {
      "name": "Machine translation",
      "score": 0.4477156698703766
    },
    {
      "name": "Speech recognition",
      "score": 0.3893739581108093
    },
    {
      "name": "Natural language processing",
      "score": 0.3567855954170227
    },
    {
      "name": "Engineering",
      "score": 0.10534882545471191
    },
    {
      "name": "Electrical engineering",
      "score": 0.09590831398963928
    },
    {
      "name": "Voltage",
      "score": 0.07469439506530762
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90023481",
      "name": "National Institute of Information and Communications Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I203951103",
      "name": "Keio University",
      "country": "JP"
    }
  ],
  "cited_by": 5
}