{
  "title": "Context-faithful Prompting for Large Language Models",
  "url": "https://openalex.org/W4389523675",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2145226641",
      "name": "Wenxuan Zhou",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2097597093",
      "name": "Sheng Zhang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2032387740",
      "name": "Hoifung Poon",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2656641051",
      "name": "Muhao Chen",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2894293047",
    "https://openalex.org/W4404784155",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W3159630167",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4385565472",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2965826089",
    "https://openalex.org/W1824259148",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2118377301",
    "https://openalex.org/W582134693",
    "https://openalex.org/W4385573991",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4385570326",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4306313145",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4280518705",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4386566855",
    "https://openalex.org/W3217305727",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3188983256",
    "https://openalex.org/W3154945374",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3098371839",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3173618889"
  ],
  "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs’ contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs’ faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator’s statement and inquire about the narrator’s opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14544–14556\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nContext-faithful Prompting for Large Language Models\nWenxuan Zhou1, Sheng Zhang2, Hoifung Poon2, Muhao Chen1\n1University of Southern California 2Microsoft Research\n1{zhouwenx,muhaoche}@usc.edu 2{zhang.sheng,hoifung}@microsoft.com\nAbstract\nLarge language models (LLMs) encode para-\nmetric knowledge about world facts and have\nshown remarkable performance in knowledge-\ndriven NLP tasks. However, their reliance on\nparametric knowledge may cause them to over-\nlook contextual cues, leading to incorrect pre-\ndictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper,\nwe seek to assess and enhance LLMs’ contex-\ntual faithfulness in two aspects: knowledge\nconflict and prediction with abstention. We\ndemonstrate that LLMs’ faithfulness can be sig-\nnificantly improved using carefully designed\nprompting strategies. In particular, we iden-\ntify opinion-based prompts and counterfactual\ndemonstrations as the most effective methods.\nOpinion-based prompts reframe the context as\na narrator’s statement and inquire about the nar-\nrator’s opinions, while counterfactual demon-\nstrations use instances containing false facts to\nimprove faithfulness in knowledge conflict sit-\nuations. Neither technique requires additional\ntraining. We conduct experiments on three\ndatasets of two standard NLP tasks, machine\nreading comprehension and relation extraction,\nand the results demonstrate significant improve-\nment in faithfulness to contexts.1\n1 Introduction\nLarge language models (LLMs; Brown et al. 2020;\nWei et al. 2022; Chowdhery et al. 2022; Chung\net al. 2022) have made remarkable advances in\nsolving various NLP problems, particularly in\n(context-free) knowledge-driven tasks such as ques-\ntion answering (Joshi et al., 2017; Kwiatkowski\net al., 2019) and commonsense reasoning (Clark\net al., 2018; Mihaylov et al., 2018). Without ex-\nternal context, LLMs can answer factual questions\nand achieve comparable results to supervised ap-\nproaches (Brown et al., 2020; Wei et al., 2022), in-\n1Code and data are released at https://github.com/\nwzhouad/context-faithful-llm.\nContext: Elon Musk is a business magnate and \ninvestor. He is the owner and CEO of Twitter.\nQuestion: Who is the CEO of Twitter?\nAnswer: Elon Musk\nGPT-3.5: Jack Dorsey\nKnowledge Conflict\nContext: Bill Gates was born in Seattle, \nWashington.\nQuestion: Is Bill Gates the founder of Microsoft?\nAnswer: I don’t know\nGPT-3.5: yes\nPrediction with Abstention\nFigure 1: Examples of knowledge conflict and predic-\ntion with abstention. LLMs may ignore the provided\ncontext and make unfaithful predictions based on their\nparametric knowledge before Q4 2021.\ndicating that LLMs encode parametric knowledge\nabout open-world facts.\nAlthough parametric knowledge can be benefi-\ncial for knowledge-driven tasks, overly relying on\nit can cause problems in context-specific NLP tasks.\nFirst, LLMs may encode misconceptions (Lin et al.,\n2022) or obsolete facts (Lazaridou et al., 2021;\nLiska et al., 2022; Kasai et al., 2022), in which case\nwe expect LLMs to update their predictions when\nprovided with relevant context. Second, when us-\ning LLMs for knowledge acquisition tasks such\nas machine reading comprehension (MRC; Clark\net al. 2019; Rajpurkar et al. 2016) and informa-\ntion extraction (IE; Sang and De Meulder 2003;\nZhang et al. 2017; Zhou and Chen 2022; Lu et al.\n2022), LLMs should always extract the knowledge\nin context instead of relying solely on their para-\nmetric knowledge. In such context-specific applica-\ntion scenarios, we expect LLMs to make decisions\nfaithful to the context and avoid simply parroting\nanswers from pretraining. However, studies have\ndiscovered that LLMs can overlook or ignore con-\ntext (Kasai et al., 2022; Li et al., 2022; Si et al.,\n14544\n2023), posing a significant challenge for their ap-\nplication in these scenarios.\nIn this paper, we aim to investigate techniques\nfor improving the faithfulness of LLMs in context-\nspecific NLP tasks. Conceptually, faithfulness is\nnot simply about how much accuracy the model\ncan offer. Instead, it should concern the validity\nand reliability of its extraction process. Specifi-\ncally, when there is decision-related information\n(e.g., a concept or a relation) to extract, a faithful\nLLM should genuinely induce what is described in\nthe context but not give trivial guesses based on\nparametric knowledge or statistical biases. Besides,\nwhen no known decision-related information is de-\nscribed in the context, the model should selectively\nabstain from predicting. Accordingly, to provide\na realistic assessment of LLMs in terms of faith-\nfulness, we narrow our focus to two sub-problems,\nnamely entity-based knowledge conflict (Longpre\net al., 2021; Wang et al., 2022) and prediction with\nabstention (Rajpurkar et al., 2018), examples of\nwhich are shown in Fig. 1. In cases of knowledge\nconflict, where the given context contains facts dif-\nferent from the pretraining data, LLMs need to\nreturn the facts locally described in the context in-\nstead of the globally memorized ones. For example,\nin Fig. 1, text-davinci-003 identifiesJack Dorseyin-\nstead of Elon Musk as the CEO of Twitter, based on\nits pretrained data before Q4 2021. In cases of pre-\ndiction with abstention, where the context does not\nprovide information to answer the questions, LLMs\nshould abstain from making predictions and notify\nthe users, rather than answering the questions that\nbecome a trivial guess. For example, in Fig. 1,\nwhen asked about the founder of Microsoft based\non an irrelevant context, LLMs should admit that,\nfrom here, they cannot infer the answer.\nWe present various prompting strategies to im-\nprove the faithfulness of LLMs, including design-\ning effective prompts and choosing appropriate in-\ncontext demonstrations. We find that constraining\nthe scope of questions to the context by adding\nphrases (e.g., based on the given context) or natural\nlanguage instructions improve faithfulness in both\nfacets. Particularly, we find that reformulating the\ncontext and questions to opinion-based question-\nanswering problems (Gupta et al., 2019; Bjerva\net al., 2020), where the context is expressed in\nterms of a narrator’s statement, and the question\nasks about this narrator’s opinion, delivers the most\ngains. Additionally, we find that adding counter-\nfactual demonstrations to prompts improves faith-\nfulness in the aspect of knowledge conflict, while\nusing the original (factual) demonstrations leads to\nlimited or negative effects. Finally, combining both\ntechniques delivers the largest gain than using each\none independently.\nWe evaluate our methods based on three datasets,\nincluding Re-TACRED (Stoica et al., 2021) for rela-\ntion extraction, and natural questions (Kwiatkowski\net al., 2019) and RealTime QA (Kasai et al., 2022)\nfor MRC. We find that the proposed strategies\ncan largely improve faithfulness, e.g., reducing\nthe memorization ratio2 of text-davinci-003 from\n35.2% to 3.0% on natural questions. Additionally,\nwe evaluate our methods across LLMs of different\nscales, finding that larger LLMs are more likely to\nupdate memorized answers than smaller ones, both\nwith and without the application of our methods.\n2 Related Work\nWe discuss two topics of related work that are\nclosely relevant to this work.\nKnowledge conflicts. LLMs (Brown et al., 2020;\nWei et al., 2022; Chowdhery et al., 2022) have\nshown promising results in closed-book QA tasks,\nindicating their ability to memorize facts about the\nworld. However, as the world is constantly evolv-\ning, memorized facts may become outdated (Lazari-\ndou et al., 2021; Liska et al., 2022; Kasai et al.,\n2022), emphasizing the need to update LLMs’ pre-\ndictions with new facts. To address this challenge,\nsome studies (Zhu et al., 2020; De Cao et al., 2021;\nMitchell et al., 2022; Meng et al., 2022, 2023) have\nexplored ways to identify and edit the facts stored\nin model parameters. However, it remains unclear\nwhether memory editing methods allow sufficient\ncapacity to encompass all new factual knowledge.\nAnother promising direction is to augment LLM\nprompting with external context containing rele-\nvant knowledge (Lazaridou et al., 2022; Izacard\net al., 2022; Khattab et al., 2022). Coupled with re-\ntrieval systems (Karpukhin et al., 2020; Santhanam\net al., 2022; Gao and Callan, 2022), such meth-\nods have the potential to update LLMs with large\namounts of new facts. However, such methods face\nthe challenge that LLMs may persist with the mem-\norized facts and ignore the provided context (Long-\npre et al., 2021). To tackle this challenge, recent\nworks (Neeman et al., 2022; Li et al., 2022) fine-\n2The percentage of times that LLMs return memorized\nanswers versus answers in the context (Longpre et al., 2021).\n14545\ntune LLMs on counterfactual contexts, where the\noriginal facts are replaced with counterfactual ones.\nThey find that such finetuning processes can effec-\ntively improve the LLMs’ utilization of contexts.\nIn this study, we propose a novel approach using\nprompting to improve context faithfulness in LLMs\nwithout additional finetuning, which offers a more\ngeneral and cost-effective method for LLMs.\nPrediction with abstention. Selective prediction\nwith abstention (Chow, 1970; Fumera and Roli,\n2002; Cortes et al., 2016) is an important prob-\nlem in trustworthy AI. When models are uncertain\nabout their predictions, it is critical that they should\nadmit the uncertainty instead of returning incorrect\npredictions. Selective prediction may be adopted\nin different scenarios, such as when instances are\nclose to the decision boundary (Gal and Ghahra-\nmani, 2016; Lakshminarayanan et al., 2017; Xin\net al., 2021), or when instances are from differ-\nent domains to training (Hendrycks and Gimpel,\n2017; Hendrycks et al., 2020; Zhou et al., 2021).\nIn the scope of context-specific NLP, abstention is\npreferred when the context is irrelevant to the ques-\ntion. For example, SQuAD 2.0 (Rajpurkar et al.,\n2018) introduces unanswerable questions to extrac-\ntive MRC, while Yatskar (2019) finds it focused\non questions of extreme confusion and thus is less\nrelevant to the focus of our study. CoQA (Reddy\net al., 2019) and QuAC (Choi et al., 2018) introduce\nunanswerable questions to conversational question\nanswering. RealTime QA (Kasai et al., 2022) finds\nthat GPT-3 still generates outdated answers when\nprovided with irrelevant documents. To address the\nproblem, Neeman et al. (2022) propose the answer-\nability augmentation where LLMs should predict\nUnanswerable when presented with an empty or\nrandomly sampled document. Several other work\nemploy variants of confidence calibration tech-\nniques to encourage the NLP model to avoid giving\na high confidence on any decisions when encoun-\ntering a case to abstain (Wang et al., 2023, 2022),\nwhich however request white-box accessibility of\nthe incorporated models. We tackle this problem\nwith a part of our prompting method, which we find\nto significantly enhance the LLMs’ ability to make\nselective predictions without need re-calibration or\nwhite-box accessibility of the model.\n3 Method\nWe focus on context-specific NLP tasks. The input\nof these tasks is formulated as (c, q) for free-form\ngeneration tasks, where c is the context and q is the\nquestion, or (c, q, o) for tasks with close decision\nspaces (e.g., multi-choice tasks), where o is the\nset of decisions/choices. The desired output can\nbe either a free-form text or a choice. We solve\nthese tasks by prompting LLMs and study ways of\ndesigning prompting templates and demonstrations\nthat are dedicated to improving the faithfulness of\nLLMs. Specifically, we find two proposed methods,\nopinion-based prompts and counterfactual demon-\nstrations, to be the most effective ones. Our meth-\nods only change the prompts without finetuning\nthe LLMs (Longpre et al., 2021; Li et al., 2022;\nNeeman et al., 2022), targeting a more general and\naffordable solution.\n3.1 Opinion-based Prompting\nGiven an input (c, q, o), we begin with the follow-\ning base prompting template:3\nBase prompt\n{c} Q: {q}? Options: {o} A:\nHere, {.} serves as a placeholder to be filled\nwith specific content during prompting. We investi-\ngate two types of prompting templates for context-\nspecific NLP, namely opinion-based prompts and\ninstructed prompts. Opinion-based prompts trans-\nform original questions into opinion-seeking ques-\ntions, which naturally demand more attention to the\ncontext. Instructed prompts, on the other hand, ex-\nplicitly instruct LLMs to read the context by natural\nlanguage. Details of these templates are discussed\nin the remaining section.\nOpinion-based prompts. We propose to trans-\nform the context to a narrator’s statement and the\nquestion to enquire about the narrator’s opinion\nin this statement. This approach is motivated by\nour own cognitive process for answering different\ntypes of questions. When answering questions that\nseek factual information, we can often rely on our\nown memory and answer without needing to refer\nto the context, as these questions typically have\nonly one correct answer. However, when ques-\ntions are seeking opinions from someone else (in\nthis context, the narrator), it is important to com-\nprehend the narrator’s words before answering the\nquestions, as opinions may vary from person to\nperson. Besides, as opinions are inherently subjec-\n3Options only apply to multiple-choice tasks and are re-\nmoved in free-form text generation tasks.\n14546\ntive and can be influenced by many factors such as\npersonal experiences and beliefs, opinion-seeking\nquestions are sometimes difficult to answer solely\nbased on the narrator’s statement compared to a\nfact-seeking question that typically has definite and\nverifiable answer(s). As a result, transforming fac-\ntual questions into opinion-seeking questions can\nlead to more attention to the context, as memorized\nanswers alone may not suffice. It also helps the\nmodel more selectively predict under cases where\ncontexts do not describe answers. Both factors lead\nto improved faithfulness with LLMs. The opinion-\nbased prompting template is as follows:\nOpinion-based prompt\nBob said, “{c}” Q: {q} in Bob’s opinion? Options:\n{o} A:\nThroughout our experiments, we consistently\nuse Bob to represent the narrator for the context,\nalthough other names could be utilized as well.\nInstructed prompts. We also explicitly instruct\nLLMs to read context by natural language. We start\nby extending questions in prompts with attributive\nphrases such as “based on the given text”, leading\nto the following attributed prompting template:\nAttributed prompt\n{c} Q: {q} based on the given text? Options:{o} A:\nWe also augment the prompts with natural lan-\nguage instructions. Since manually writing instruc-\ntions can be laborious and often fails to account for\nthe compatibility between instructions and LLMs,\nwe leverage automatic prompt engineering (APE;\nZhou et al. 2022) to generate the prompts. Using a\nfew instances and their desired outputs as demon-\nstrations, APE uses LLMs to automatically gener-\nate candidate instructions and select the best one\nbased on the results on a dev set (see Appx. §A for\ngenerated instructions). We then use the following\ninstruction-based prompting template:\nInstruction-based prompt\nInstruction: {Instruction} {c} Q: {q}? Options: {o}\nA:\nExperiments show that all prompting templates\nperform better than the base prompting template.\nSpecifically, opinion-based prompts outperform in-\nstructed prompts in both knowledge conflict and\nprediction with abstention facets, and combining\nthese two prompting methods results in the most\nsignificant improvements.\n3.2 Counterfactual Demonstration\nUsing demonstrations is a standard way to perform\nfew-shot inference on LLMs (Brown et al., 2020).\nTo enhance the faithfulness of language models in\nknowledge conflict scenarios, previous studies (Li\net al., 2022; Neeman et al., 2022) propose to fine-\ntune the models using counterfactual instances,\nwhere the facts in the context are substituted with\nfalse ones, and the model learns to update its predic-\ntions accordingly. Following this strategy, we pro-\npose to use counterfactual instances as demonstra-\ntions for LLMs. To do so, we start with a labeled\nset of counterfactual instances and a test instance\nand then use KATE (Liu et al., 2022) to retrieve the\nmost relevant counterfactual instances as demon-\nstrations. We encode both the test instance and\ncounterfactual instances with RoBERTanli+sts-b (Liu\net al., 2019; Reimers and Gurevych, 2019) and se-\nlect the top counterfactual instances based on co-\nsine similarity. As a part of our analysis, we also\nexperimented with using the original (factual) in-\nstances as demonstrations but found this approach\nto underperform counterfactual demonstrations and\nsometimes even zero-shot inference.\n4 Experiments\nThis section presents our experimental setups (§4.1)\nfor the evaluation of the proposed methods concern-\ning two aspects of faithfulness: knowledge con-\nflict (§4.2) and prediction with abstention (§4.3).\nWe provide additional analysis (§4.4) on results\nacross different model sizes and results on the orig-\ninal datasets. We also show examples of prompts\nand LLMs’ outputs in the case study (§4.5).\n4.1 Experimental Setup\nOur experiments are conducted using the In-\nstructGPT model (text-davinci-003, 175B parame-\nters) and LLama-2-7B-chat (Touvron et al., 2023).\nWe use the base prompt as our baseline, and\ncompare it against the proposed prompting tem-\nplates described in §3.1, including attributed\nprompt (ATTR ), instruction-based prompt (INSTR ),\nopinion-based prompt ( OPIN ), and the combina-\ntion of opinion-based prompt and instruction-based\nprompt (OPIN + I NSTR ). We evaluate the effec-\ntiveness of these templates in both zero-shot and\n14547\nGPT-3.5 MRC RE\nps ↑ po ↓ MR ↓ EM↑ ps ↑ po ↓ MR ↓ F1↑\nZero-shot\nBase 59.0 32.1 35.2 6.2 73.9 21.5 22.5 81.0\nAttr 71.9 14.4 16.6 29.6 72.4 23.6 24.6 80.0\nInstr 74.2 16.0 17.7 27.1 75.8 15.6 17.1 81.6\nOpin 79.4 9.8 11.0 24.9 76.0 19.6 20.5 82.9\nOpin + Instr 79.1 7.9 9.1 48.6 79.4 15.0 15.9 84.7\nOriginal\nBase 43.3 49.4 53.3 35.1 76.2 19.8 20.6 83.3\nAttr 54.1 37.7 41.0 45.5 76.5 19.7 20.5 83.7\nInstr 54.6 37.7 40.8 45.8 77.3 18.4 19.2 84.2\nOpin 60.6 28.7 32.1 51.1 76.8 18.4 19.3 83.8\nOpin + Instr 64.7 26.8 29.3 53.8 78.2 17.1 17.9 84.9\nCounter\nBase 86.9 6.5 7.0 80.2 78.7 13.7 14.8 83.9\nAttr 89.1 4.6 4.9 83.0 79.7 13.0 14.0 84.3\nInstr 86.2 6.3 6.8 80.1 78.0 12.8 14.1 82.9\nOpin 90.1 3.7 3.9 84.3 79.7 12.8 13.8 84.4\nOpin + Instr 90.9 2.8 3.0 85.2 80.0 10.5 11.6 85.1\nTable 1: Results (in %) on GPT-3.5-175B in the knowledge conflict setting. The overall best results are highlighted\nin bold. The best and the second best results in each setting are highlighted in green and orange , respectively.\nLLama-2 MRC RE\nps ↑ po ↓ MR ↓ EM↑ ps ↑ po ↓ MR ↓ F1↑\nZero-shot\nBase 50.8 40.9 44.6 3.5 15.3 67.6 81.6 12.8\nAttr 66.2 23.8 26.4 4.7 13.2 66.5 83.5 10.9\nInstr 77.7 19.7 20.2 27.0 19.6 9.2 75.1 13.2\nOpin 74.6 14.6 16.4 9.4 20.7 63.4 75.4 14.4\nOpin + Instr 77.8 13.9 15.1 13.7 21.6 57.9 72.8 11.8\nOriginal\nBase 56.7 39.7 41.1 19.4 27.6 62.3 69.4 9.4\nAttr 61.7 34.5 35.9 25.2 29.4 58.9 66.7 11.2\nInstr 59.4 35.7 37.5 25.5 34.6 53.6 60.8 13.2\nOpin 67.1 32.1 32.4 18.5 32.2 57.1 63.9 10.9\nOpin + Instr 70.6 26.8 27.5 27.6 35.7 51.3 59.0 11.5\nCounter\nBase 84.4 7.8 8.4 39.2 76.3 14.8 16.2 38.9\nAttr 85.9 7.0 7.6 44.1 76.5 14.2 15.7 39.5\nInstr 85.5 6.7 7.3 47.1 76.0 14.4 15.9 37.3\nOpin 86.7 6.2 6.7 38.1 76.3 13.8 15.4 41.7\nOpin + Instr 88.1 4.9 5.2 49.6 77.3 14.2 15.5 36.9\nTable 2: Results (in %) on LLama-2-7B-chat in the knowledge conflict setting. The overall best results are\nhighlighted in bold. The best and the second best results in each setting are highlighted in green and orange ,\nrespectively.\nfew-shot settings (with demonstrations).\n4.2 Knowledge Conflict\nDatasets. We evaluate in the knowledge conflict\nsetting using counterfactual datasets that contain in-\ncorrect facts, which can conflict with what the LLM\nhas memorized. We use two datasets based on\nreal-world texts: natural questions (Kwiatkowski\net al., 2019) for MRC and Re-TACRED (Stoica\net al., 2021) for relation extraction (RE). To create\ncounterfactuals, we adopt the framework proposed\nby Longpre et al. (2021), which modifies the con-\ntext to support a counterfactual answer. Specifi-\ncally, for MRC, we follow Longpre et al. (2021)\nand replace the gold entity answer in the context\nwith a randomly sampled entity of the same entity\ntype from the corpus. For RE, we first randomly\nsample a context that has the entity mentions of the\nsame type but different relations from the original\none, and then insert the original entities into the\nsampled context. In this scenario, a faithful LLM\nshould update its prediction to the new answer in-\nstead of returning the original one. Moreover, to\nmeasure LLMs’ ability to update answers, we need\nto ensure that they have memorized the knowledge\nof the original answers in the first place. Therefore,\nwe only evaluate LLMs on a subset of instances\non which these models can correctly predict the\n14548\noriginal answers without additional contexts.\nTask setup. We use the same set of evaluation\nmetrics as Longpre et al. (2021). Specifically, we\nmeasure the frequency that the LLMs’ predictions\ncontain an exact match of the original answers\n(po) and the substituted answers ( ps), after both\npredictions and answers have been normalized by\nremoving stop words and punctuation To assess\nthe model’s reluctance to update its prediction, we\nuse the memorization ratio ( MR), which is cal-\nculated as MR = po\npo+ps . A completely faithful\nLLM should have an MR of 0. We also report\ntask-specific metrics, including exact match (EM)\nfor MRC and F1 for RE. For EM, we also use\nnormalized predictions and answers, but the re-\nquirement is that the prediction and answer must\nbe exactly the same, rather than just containing the\nanswer. We conduct experiments in three different\nsettings: zero-shot, demonstration using original\ninstances, and demonstration using counterfactual\ninstances. We retrieve demonstrations from the\noriginal/counterfactual training set, and evaluate\nLLMs on the counterfactual test set. In the few-\nshot setting, we utilize a maximum of 16 demon-\nstration instances, up to the limit of the LLM’s\ncontext window.\nResults and discussion. The results in Tab. 1 and\nTab. 2 demonstrate that the combination of OPIN\n+ INSTR prompting and counterfactual demonstra-\ntions is generally the most effective. Compared\nto the zero-shot base prompts, there is a reduction\nof 32.2% in MR for MRC and a 10.9% reduction\nfor RE on GPT-3.5. Similarly, on LLaMA-2-7B-\nchat, there is a 39.4% reduction in MR for MRC\nand a 57.3% reduction for RE. We also find that\nopinion-based prompts generally perform better\nthan other templates, achieving the second-best re-\nsults on 17 out of 24 metrics on GPT-3.5, and 9 out\nof 24 metrics on LLama-2, indicating that LLMs\nare more faithful to the context when answering\nopinion-seeking questions. Combining opinion-\nbased prompts and instruction-based prompts fur-\nther improves faithfulness, with the best results\nobtained in 23 out of 24 metrics on GPT-3.5, and\n19 out of 24 metrics on LLama-2.\nWhen it comes to few-shot settings, counterfac-\ntual demonstrations lead to further improved per-\nformance. Using the original (factual) instances as\ndemonstrations, on the other hand, leads to limited\neffects or may even impair faithfulness in MRC.\nThis finding suggests that demonstrations do not\nalways improve the generalization of LLMs’ infer-\nence, especially when they contain dataset bias.\nIn the MRC experiments, the natural questions\ndataset used is constructed based on Wikipedia,\nwhich mainly consists of world facts. This poten-\ntially allows for a simplicity bias of LLMs where\nquestions can be answered without contexts. There-\nfore, our study suggests the importance of using\ncounterfactual demonstrations in knowledge con-\nflict scenarios.\n4.3 Prediction with Abstention\nDatasets. As for the second aspect of faithful-\nness, we evaluate LLMs’ ability to selectively ab-\nstain from making uncertain predictions based on\nirrelevant context. Since existing datasets such\nas SQuAD 2.0 (Rajpurkar et al., 2018) generally\ncontain questions with confusion (Yatskar, 2019)\nand are less related to our problem setting, we cu-\nrate our own evaluation data based on RealTime\nQA (Kasai et al., 2022), a dataset that inquires\nabout novel information from June 2022 onwards.\nIn this formulation, LLMs are presented with a\nquestion and multiple choices, and they need to\nchoose the correct answer based on several re-\ntrieved documents. These documents were ob-\ntained using tools like Google custom search and\nmay not contain the answer to the question. To\nadapt this dataset to our setting, we added a new\n“I don’t know” choice and relabeled the dataset.\nInstances where the retrieved documents do not an-\nswer the question are relabeled to “I don’t know”.\nWe used questions in the first six weeks of 2022\nas the test set and randomly picked three questions\nof 2023 as demonstration instances. This process\nresults in a total of 113 test instances, including 63\nanswerable questions and 50 unanswerable ones.\nTask setup. We calculate the probability of a\nchoice as P(choice|prompt) followed by normal-\nization across all choices.4 We report accuracy on\nthe entire dataset (All), accuracy on the subset of\nquestions that can be answered based on retrieved\ndocuments (HasAns), and accuracy on questions\nthat cannot be answered based on retrieved docu-\nments (NoAns). The latter two metrics measure\nLLMs’ ability to extract answers from context and\n4We tried three methods to calculate P(choice|prompt):\njoint probability, per-token probability (joint probability nor-\nmalized by length), and unconditional probability as done\nin Brown et al. (2020). We find that joint probability works\nthe best for GPT-3.5, while per-token probability works the\nbest for LLama-2.\n14549\nMethod GPT-3.5 LLama-2\nAcc↑ Brier↓ Acc↑ Brier↓NoAns All HasAns NoAns All\nZero-shot\nBase 30.6 68.5 29.4 88.7 14.3 55.9 30.0\nAttr 65.3 84.4 14.6 87.1 16.3 55.9 29.7\nInstr 81.6 91.7 7.7 91.9 26.5 63.1 27.4\nOpin 83.3 92.6 6.6 85.5 30.6 61.3 27.8\nOpin + Instr 87.8 94.4 5.2 88.7 36.7 65.8 27.4\nFew-shot\nBase 73.5 88.2 11.2 56.5 69.4 62.2 27.9\nAttr 81.6 91.9 8.0 59.7 67.3 63.1 27.0\nInstr 85.7 93.7 6.1 51.6 81.6 64.9 26.6\nOpin 87.8 94.6 4.1 50.0 87.8 66.6 26.6\nOpin + Instr 89.8 95.5 3.4 43.5 91.8 64.9 26.0\nTable 3: Results (in %) for GPT-3.5 and LLama-2 on RealTime QA. The overall best results are highlighted in bold.\nThe best and the second best results in each setting are highlighted in green and orange , respectively. As all\nprompts achieve perfect accuracy (100%) on the HasAns subset for GPT-3.5, it is not included in the table.\ntheir ability to abstain from making predictions\nwhen the context does not describe the answer, re-\nspectively. Besides, we use the probability of “I\ndon’t know” as LLM’s probability estimation of\nwhether the question can be answered. We use the\nBrier score to evaluate the accuracy of the estima-\ntion, which measures the mean squared difference\nbetween the estimation and the true binary outcome\nof answerability. We use three demonstrations for\neach instance in the few-shot setting, where some\ninstances are filtered out during evaluation due to\nexceeding the context length of LLMs.\nResults and discussion. The results detailed\nin Tab. 3 reveal that the OPIN + I NSTR prompt\noutperforms all others on GPT-3.5, both in the zero-\nshot and few-shot settings, surpassing base prompts\nby 57.2% and 16.3% in NoAns subset accuracy, re-\nspectively. For LLama-2, this approach similarly\noutperforms base prompts by 22.4% in both set-\ntings. Furthermore, the Brier score is reduced by\n24.2% and 7.8% compared to base prompts for\nGPT-3.5 in the two settings, respectively, and by\n2.6% and 1.9% on LLama-2. The OPIN prompt is\nthe second best in terms of these metrics. These\nfindings demonstrate that opinion-based prompts\ncan enhance the LLMs’ ability to make selective\npredictions. In addition, The use of demonstrations\nconsistently improves the LLMs’ ability to make se-\nlective predictions, as evidenced by the lower Brier\nscores and higher NoAns accuracy in the few-shot\nsetting compared to the zero-shot setting.\n4.4 Additional Analysis\nMemorization by different sizes of LLMs. Fig. 2\nshows the memorization ratio MR across different\nsizes of InstructGPTs under the zero-shot evalua-\ntion of natural questions.5 Overall, OPIN + INSTR\nconsistently outperforms other prompts across dif-\nferent model sizes. In the upper plot, results are\nshown for filtered evaluation sets where the cor-\nresponding LLMs can correctly predict the orig-\ninal answers without additional contexts, thereof\nthe size of evaluation sets varies across different\nLLMs.6 We observe that MR generally decreases\nwith increased model size, showing that larger\nLLMs are better at updating memorized answers\nbased on given contexts in knowledge conflicts.\nHowever, the lower plot reveals that larger LLMs\nhave more severe memorization on the full (unfil-\ntered) evaluation set. This is because larger LLMs\ncan memorize more answers than smaller ones, as\nevidenced by the number of instances in the fil-\ntered evaluation set where larger LLMs have more\ninstances. Our analysis suggests that while larger\nLLMs are better at updating memorized answers,\nthey still tend to have more memorization due to\nthe larger number of memorized answers. There-\nfore, we need to pay more attention when using\nlarger LLMs in scenarios with new or potentially\nconflicting knowledge.\nSelective prediction by different sizes of LLMs.\nFig. 3 shows the Brier score across different sizes\nof InstructGPTs under the zero-shot evaluation of\nRealTime QA. On smaller LLMs, opinion-based\nprompt achieves similar or even higher Brier score\n5The 0.3B, 1.3B, 6.7B models refer to text-ada-001, text-\nbabbage-001, text-curie-001, respectively. We do not perform\nfew-shot evaluation as different sizes of LLMs have different\nmaximum input lengths and can take different numbers of\ndemonstrations, thus hard to be compared to each other.\n6The sizes of the filtered evaluation sets, in the order of\nincreased model sizes, are 121, 132, 756, and 2,773.\n14550\n0\n10\n20\n30\n40MR\nFiltered Evaluation Set\nBase\nAttr\nInstr\nOpin\nOpin+Instr\n0.3B 1.3B 6.7B 175B\nModel size\n0\n10\n20\n30\n40MR\nFull Evaluation Set\nFigure 2: Memorization ratios across different sizes of\nInstructGPTs, evaluated in the zero-shot setting.\nthan base prompts, indicating it does not improve\nthe selective prediction ability of LLMs. We hy-\npothesize that this is because smaller LLMs have\ninferior reading comprehension ability, resulting\nin uncertainty in many instances. Opinion-based\nprompts change uncertain predictions of answer-\nable questions to I don’t know, which could lead\nto worse results. For other prompting templates,\nwe do not observe a consistent improvement across\ndifferent LLMs either. This analysis shows that\nwhile the selective prediction ability can be more\neasily activated by zero-shot prompting for LLMs\nsuch as text-davinci-003, smaller LLMs may re-\nquire dedicated adaptations such as calibration and\nfinetuning to activate this ability.\nResults on original datasets. While our main ex-\nperiments demonstrate the effectiveness of the pro-\nposed methods in resolving knowledge conflicts,\nLLMs in real-world applications may also see in-\nstances without knowledge conflicts. Therefore,\nwe investigate how our methods affect inference\nwhen the memorized answers align with the given\ncontexts. To do so, we evaluate LLMs on the same\nset of filtered evaluation set used in the main re-\nsults section (§4.2), but we use the original contexts\nand answers instead of counterfactual ones. The\nresults in Tab. 4 show that opinion-based prompts\nyield similar or better results in all settings. Fur-\nthermore, using either counterfactual or original\ndemonstrations does not significantly impact re-\nsults on the original (factual) dataset. This analysis\n0.3B 1.3B 6.7B 175B\nModel size\n0\n10\n20\n30\n40\n50Brier\nRealTime QA\nBase\nAttr\nInstr\nOpin\nOpin+Instr\nFigure 3: Brier scores across different sizes of Instruct-\nGPTs in the zero-shot setting of RealTime QA.\nMethod po↑ EM↑\nZero-shot\nBase 92.1 11.1\nOpin 91.3 25.2\nOpin + Instr 90.5 57.2\nOriginal\nBase 93.2 77.8\nOpin 92.7 78.7\nOpin + Instr 93.9 80.1\nCounter\nBase 93.6 82.1\nOpin 92.8 82.3\nOpin + Instr 92.7 82.1\nTable 4: Results (in %) on the filtered evaluation set of\nnatural questions with original contexts and answers.\nreveals that our methods do not impair performance\non instances without knowledge conflicts.\n4.5 Case Study\nTab. 5 shows some examples of prompts and the\ncorresponding answers generated by text-davinci-\n003. The left column of the table presents a knowl-\nedge conflict case where the original answer, Lady\nGaga, is replaced with a counterfactual answer,\nBosco. When using base prompts, LLM ignores\nthe context and return the memorized answer Lady\nGaga. However, using opinion-based prompts and\ntheir combination with instructions leads to a more\nfaithful response, with the language model return-\ning Bosco in the given context. The right column\npresents a scenario where the retrieved context\nfrom Google search is irrelevant to the given ques-\ntion. In such cases, base prompts still return a\nchoice, leading to a potentially incorrect answer.\nHowever, opinion-based prompts and their combi-\nnation with instructions can abstain from making\npredictions and return I don’t know . These ex-\namples demonstrate the effectiveness of proposed\n14551\nKnowledge Conflict Prediction with Abstention\nContext (Counterfactual passage) The Super Bowl LI Half-\ntime show took place on February 5, 2017, at NRG\nStadium in Houston, Texas as part of Super Bowl LI.\nThe show was headlined byBosco, who performed a\nmedley of her songs, including newer material from\nher most recent studio album Joanne.\nTara Connolly is senior gas campaigner at Global\nWitness, an international NGO working towards a\nmore sustainable, just and equal planet. She has over\na decade of experience in EU energy policy. The\nviews expressed in this commentary are her own.\nPrompt Instruction: read the given information and answer\nthe corresponding question.\nInstruction: answer a question based on the provided\ninput-output pairs.\nBob said, “The Super Bowl ... album Joanne.” Bob said, “Tara Connolly ... are her own.”\nQ: who performed the halftime show at Super Bowl\n51 in Bob’s opinion based on the given text?\nQ: Mo Farah made public that he was trafficked from\nwhich African country to the UK in Bob’s opinion\nbased on the given text?\nChoices: Somaliland; Djibouti; Ethiopia; Somalia; I\ndon’t know\nBase Lady Gaga✗ Somalia✗\nAttr Lady Gaga✗ Somalia✗\nInstr Lady Gaga✗ Somaliland✗\nOpin Bosco✓ I don’t know✓\nInstr + OpinBosco✓ I don’t know✓\nAnswer Bosco I don’t know\nTable 5: Examples of prompts and LLMs’ corresponding predictions. In the “Prompt” row, we show and highlight\nthe added parts from different prompting templates including attributed prompts, instruction-based prompts, and\nopinion-based prompts.\nprompts in generating context-faithful responses.\n5 Conclusion\nIn this paper, we focus on addressing the faithful-\nness issue of LLMs in context-specific NLP tasks,\nparticularly in scenarios with knowledge conflict\nand prediction with abstention. We propose that\ntwo methods, opinion-based prompts and counter-\nfactual demonstrations, are effective in improving\nLLMs’ faithfulness to contexts. We evaluate our\nmethods on three datasets of two tasks, namely\nmachine reading comprehension and relation ex-\ntraction, and observed significant improvement in\nfaithfulness to contexts. Future work includes eval-\nuating the effectiveness of proposed methods on a\nbroader range of NLP tasks such as open-domain\nQA and summarization, and studying other tech-\nniques to improve faithfulness further.\nAcknowledgement\nWe appreciate the reviewers for their insightful\ncomments and suggestions. Wenxuan Zhou and\nMuhao Chen are supported by the NSF Grant IIS\n2105329 and the DARPA MCS program under Con-\ntract No. N660011924033 with the United States\nOffice Of Naval Research.\nLimitations\nIn this study, our main focus is on the utilization of\ncontext-augmented prompting, assuming the relia-\nbility of the provided context. However, real-world\nscenarios can be more complicated, which may\ninvolve retrieved contexts that contain erroneous\nor conflicting information. Assessing the factu-\nality of the context solely based on the provided\ninformation becomes challenging, as it depends\non additional factors such as trustworthiness and\ntimeliness of the information source. Due to the\ncomplexity and challenges associated with veri-\nfying context reliability, we do not address this\nissue within the scope of this work. Furthermore,\nit is important to note that our paper primarily con-\ncentrates on the capability of LLMs to generate\nupdated answers or decisions for given questions,\nrather than exploring more intricate tasks that re-\nquire the model to apply the updated knowledge in\nmulti-hop reasoning.\nEthical Considerations\nDue to the availability of test data, the experiments\nconducted in this work has been in English, while\nfuture work can consider extending the use of pro-\nposed techniques to tasks in other languages. The\n14552\ndatasets used in this work are public datasets that\nmay not be free of inherent biases. However, the in-\ntroduced context-faithful prompting techniques in\nthis work do not introduce additional biases beyond\nwhat the data have presented.\nReferences\nJohannes Bjerva, Nikita Bhutani, Behzad Golshan,\nWang-Chiew Tan, and Isabelle Augenstein. 2020.\nSubjQA: A Dataset for Subjectivity and Review Com-\nprehension. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 5480–5494, Online. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-\ntau Yih, Yejin Choi, Percy Liang, and Luke Zettle-\nmoyer. 2018. QuAC: Question answering in context.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2174–2184, Brussels, Belgium. Association for Com-\nputational Linguistics.\nC Chow. 1970. On optimum recognition error and reject\ntradeoff. IEEE Transactions on information theory,\n16(1):41–46.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nCorinna Cortes, Giulia DeSalvo, and Mehryar Mohri.\n2016. Learning with rejection. In Algorithmic Learn-\ning Theory: 27th International Conference, ALT\n2016, Bari, Italy, October 19-21, 2016, Proceedings\n27, pages 67–82. Springer.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nGiorgio Fumera and Fabio Roli. 2002. Support vector\nmachines with embedded reject option. In Pattern\nRecognition with Support Vector Machines: First\nInternational Workshop, SVM 2002 Niagara Falls,\nCanada, August 10, 2002 Proceedings, pages 68–82.\nSpringer.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In international conference\non machine learning, pages 1050–1059. PMLR.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2843–2853,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMansi Gupta, Nitish Kulkarni, Raghuveer Chanda,\nAnirudha Rayasam, and Zachary C Lipton. 2019.\nAmazonqa: A review-based question answering task.\nIn International Joint Conference on Artificial Intelli-\ngence.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline for\ndetecting misclassified and out-of-distribution exam-\nples in neural networks. In International Conference\non Learning Representations.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\n14553\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What’s the answer right now?\narXiv preprint arXiv:2207.13332.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles.\nAdvances in neural information processing systems,\n30.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,\nDevang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Ko-\ncisky, Sebastian Ruder, et al. 2021. Mind the gap:\nAssessing temporal generalization in neural language\nmodels. Advances in Neural Information Processing\nSystems, 34:29348–29363.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin\nWang, Michal Lukasik, Andreas Veit, Felix Yu,\nand Sanjiv Kumar. 2022. Large language models\nwith controllable working memory. arXiv preprint\narXiv:2211.05110.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, D’Autume\nCyprien De Masson, Tim Scholtes, Manzil Zaheer,\nSusannah Young, et al. 2022. Streamingqa: A bench-\nmark for adaptation to new knowledge over time in\nquestion answering models. In International Con-\nference on Machine Learning, pages 13604–13622.\nPMLR.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in question\nanswering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7052–7063, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nKeming Lu, I-Hung Hsu, Wenxuan Zhou,\nMingyu Derek Ma, and Muhao Chen. 2022.\nSummarization as indirect supervision for relation\nextraction. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , pages\n6575–6594, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nKevin Meng, David Bau, Alex J Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in gpt. In Advances in Neural Information\nProcessing Systems.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2023. Mass-\nediting memory in a transformer. In International\nConference on Learning Representations.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In International Conference on\nLearning Representations.\n14554\nElla Neeman, Roee Aharoni, Or Honovich, Leshem\nChoshen, Idan Szpektor, and Omri Abend. 2022.\nDisentqa: Disentangling parametric and contextual\nknowledge with counterfactual question answering.\narXiv preprint arXiv:2211.05655.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249–266.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nErik F Sang and Fien De Meulder. 2003. Introduction\nto the conll-2003 shared task: Language-independent\nnamed entity recognition. arXiv preprint cs/0306050.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-\njuan Wang. 2023. Prompting gpt-3 to be reliable. In\nInternational Conference on Learning Representa-\ntions.\nGeorge Stoica, Emmanouil Antonios Platanios, and\nBarnabás Póczos. 2021. Re-tacred: Addressing short-\ncomings of the tacred dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 13843–13850.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nHaoyu Wang, Hongming Zhang, Yuqian Deng, Jacob\nGardner, Dan Roth, and Muhao Chen. 2023. Extract-\ning or guessing? improving faithfulness of event tem-\nporal relation extraction. In Proceedings of the 17th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 541–553,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nYiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun\nCai, Yuxuan Liang, Dayiheng Liu, Baosong Yang,\nJuncheng Liu, and Bryan Hooi. 2022. Should we\nrely on entity mentions for relation extraction? debi-\nasing relation extraction with counterfactual analysis.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3071–3081, Seattle, United States. Association\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. The art of abstention: Selective prediction and\nerror regularization for natural language processing.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1040–1051, Online. Association for Computational\nLinguistics.\nMark Yatskar. 2019. A qualitative comparison of CoQA,\nSQuAD 2.0 and QuAC. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2318–2323, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n35–45, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nWenxuan Zhou and Muhao Chen. 2022. An improved\nbaseline for sentence-level relation extraction. InPro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 2: Short\nPapers), pages 161–168, Online only. Association for\nComputational Linguistics.\n14555\nWenxuan Zhou, Fangyu Liu, and Muhao Chen. 2021.\nContrastive out-of-distribution detection for pre-\ntrained transformers. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1100–1111, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\narXiv preprint arXiv:2012.00363.\nAppendices\nA Settings of Automatic Prompt\nEngineering\nWe run APE using their official code7 and default\nhyperparameters. In the knowledge conflict setting,\nwe use counterfactual datasets to generate instruc-\ntions. While the APE paper recommends using\ninstructions generated by the same model in infer-\nence, we find that smaller LLMs do not generate\nmeaningful instructions for our datasets. Therefore,\nwe use instructions generated by text-davinci-003\nacross different scales of LLMs in additional anal-\nysis. The top three instructions generated by APE\non each dataset are listed below. We use the top\none instruction in experiments.\nNatural questions:\n1. read the given information and answer the\ncorresponding question.\n2. read a piece of text and then use the informa-\ntion in the text to answer a question.\n3. \"Read the given information and answer the\nquestions that follow.\"\nRe-TACRED:\n1. identify the relationship between two entities\nfrom a list of options.\n2. identify the relationship between two entities\nbased on the given input-output pairs.\n3. identify the relationship between two entities\ngiven the input-output pairs.\n7https://github.com/keirp/automatic_prompt_\nengineer\nRealTime QA:\n1. answer a question based on the provided input-\noutput pairs.\n2. ask a question with a set of choices and ask\nthe friend to provide the correct answer.\n3. answer a question related to a news article.\n14556",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.7332414388656616
    },
    {
      "name": "Computer science",
      "score": 0.7109987735748291
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6336239576339722
    },
    {
      "name": "Natural language processing",
      "score": 0.5780085921287537
    },
    {
      "name": "Cognitive reframing",
      "score": 0.5427595973014832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4706380069255829
    },
    {
      "name": "Cognitive psychology",
      "score": 0.44314876198768616
    },
    {
      "name": "Comprehension",
      "score": 0.4380190372467041
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.41518062353134155
    },
    {
      "name": "Psychology",
      "score": 0.33345404267311096
    },
    {
      "name": "Social psychology",
      "score": 0.21162089705467224
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}