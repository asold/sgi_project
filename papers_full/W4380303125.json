{
  "title": "Fostering transparent medical image AI via an image-text foundation model grounded in medical literature",
  "url": "https://openalex.org/W4380303125",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105947135",
      "name": "ChanWoo Kim",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": null,
      "name": "Soham U. Gadgil",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2898429367",
      "name": "Alex J DeGrave",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2993599751",
      "name": "Zhuo Ran Cai",
      "affiliations": [
        "Stanford University",
        "Center for Clinical Research (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2097074997",
      "name": "Su-In Lee",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2105947135",
      "name": "ChanWoo Kim",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Soham U. Gadgil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2898429367",
      "name": "Alex J DeGrave",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2993599751",
      "name": "Zhuo Ran Cai",
      "affiliations": [
        "Center for Clinical Research (United States)",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2097074997",
      "name": "Su-In Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3171849353",
    "https://openalex.org/W3081038317",
    "https://openalex.org/W4281681378",
    "https://openalex.org/W3159068118",
    "https://openalex.org/W2963198001",
    "https://openalex.org/W2061253660",
    "https://openalex.org/W2797527544",
    "https://openalex.org/W2963946669",
    "https://openalex.org/W2794825826",
    "https://openalex.org/W3121732873",
    "https://openalex.org/W4281783336",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W4290852327",
    "https://openalex.org/W3011967257",
    "https://openalex.org/W2965875799",
    "https://openalex.org/W3212197959",
    "https://openalex.org/W2969096242",
    "https://openalex.org/W3092053493",
    "https://openalex.org/W4225143830",
    "https://openalex.org/W3109762849",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W4376637715",
    "https://openalex.org/W4297502708",
    "https://openalex.org/W2806487945",
    "https://openalex.org/W2068748159",
    "https://openalex.org/W4362454777",
    "https://openalex.org/W4224242326",
    "https://openalex.org/W3199500316",
    "https://openalex.org/W3175126073",
    "https://openalex.org/W3175183715",
    "https://openalex.org/W1968426398",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2150593711",
    "https://openalex.org/W3167456680",
    "https://openalex.org/W4229008032",
    "https://openalex.org/W3011721937",
    "https://openalex.org/W3102785203",
    "https://openalex.org/W4385948838",
    "https://openalex.org/W4399739169",
    "https://openalex.org/W4226255141"
  ],
  "abstract": "Abstract Building trustworthy and transparent image-based medical AI systems requires the ability to interrogate data and models at all stages of the development pipeline: from training models to post-deployment monitoring. Ideally, the data and associated AI systems could be described using terms already familiar to physicians, but this requires medical datasets densely annotated with semantically meaningful concepts. Here, we present a foundation model approach, named MONET ( M edical c ON cept r ET riever), which learns how to connect medical images with text and generates dense concept annotations to enable tasks in AI transparency from model auditing to model interpretation. Dermatology provides a demanding use case for the versatility of MONET, due to the heterogeneity in diseases, skin tones, and imaging modalities. We trained MONET on the basis of 105,550 dermatological images paired with natural language descriptions from a large collection of medical literature. MONET can accurately annotate concepts across dermatology images as verified by board-certified dermatologists, outperforming supervised models built on previously concept-annotated dermatology datasets. We demonstrate how MONET enables AI transparency across the entire AI development pipeline from dataset auditing to model auditing to building inherently interpretable models.",
  "full_text": "Fostering transparent medical image AI via an image-text foundation1\nmodel grounded in medical literature2\nChanwoo Kim1, Soham U. Gadgil1, Alex J. DeGrave1,2, Zhuo Ran Cai3, Roxana Daneshjou4,5,*, and3\nSu-In Lee1,*\n4\n1Paul G. Allen School of Computer Science and Engineering, University of Washington5\n2Medical Scientist Training Program, University of Washington6\n3Program for Clinical Research and Technology, Stanford University7\n4Department of Dermatology, Stanford School of Medicine8\n5Department of Biomedical Data Science, Stanford School of Medicine9\n*indicates co-senior authorship10\nAbstract11\nBuilding trustworthy and transparent image-based medical AI systems requires the ability to interrogate data and12\nmodels at all stages of the development pipeline: from training models to post-deployment monitoring. Ideally,13\nthe data and associated AI systems could be described using terms already familiar to physicians, but this requires14\nmedical datasets densely annotated with semantically meaningful concepts. Here, we present a foundation model15\napproach, named MONET (Medical cONcept rETriever), which learns how to connect medical images with text and16\ngenerates dense concept annotations to enable tasks in AI transparency from model auditing to model interpretation.17\nDermatology provides a demanding use case for the versatility of MONET, due to the heterogeneity in diseases, skin18\ntones, and imaging modalities. We trained MONET on the basis of 105,550 dermatological images paired with natural19\nlanguage descriptions from a large collection of medical literature. MONET can accurately annotate concepts across20\ndermatology images as verified by board-certified dermatologists, outperforming supervised models built on previously21\nconcept-annotated dermatology datasets. We demonstrate how MONET enables AI transparency across the entire AI22\ndevelopment pipeline from dataset auditing to model auditing to building inherently interpretable models.23\nIntroduction24\nEnsuring the transparency and robustness of medical AI systems involves assessing data and models at every stage, from25\nmodel training to post-deployment monitoring. However, the tools and methods needed to promote AI transparency26\nand to de-mystify “black-box” models often require medical datasets with dense annotations of human-understandable27\nconcepts. For example, for building a melanoma classifier, it would be medically meaningful to understand the28\ndata and model using concepts such as “darker pigmentation”, “atypical pigment networks”, and “multiple colors”.29\nUnfortunately, obtaining such labels requires a significant amount of time from domain experts, and consequently,30\nmost medical datasets limit annotations to little more than diagnoses. In contrast, rich annotation with the extensive31\nand highly descriptive clinical concepts developed by the medical community could enable numerous benefits. Such32\nrich annotations could promote understanding of key biases in datasets, empower detection of undesirable behavior in33\nmedical AI devices, and foster the development of AI devices that better align with physicians’ expectations. However,34\nfew medical image datasets include such extensive annotations, and the time expended in existing efforts [1] argues35\nthat obtaining this data via large-scale efforts by human experts is infeasible.36\nHere, we instead leverage the collective knowledge of the medical community, as encapsulated in publicly available37\nmedical literature and medical textbooks, to teach an AI model, MONET ( Medical cONcept rETriever), to richly38\nannotate medical images with semantically meaningful and medically relevant concepts (Fig. 1A-B). We focus on39\nthe application of dermatology to showcase its versatility since dermatology has heterogeneity in disease appearance40\nacross diverse skin tones and has no standardized imaging practices, leading to significant heterogeneity in imaging41\nconditions (e.g., lighting, blurriness). In this setting, examples of clinical concepts include lesion color (e.g., brown) and42\n1\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nmorphology (e.g., nodule). MONET’s automatic concept generation capability empowers us to perform meaningful43\ntrustworthiness analysis across all stages of the medical AI pipeline, as demonstrated by three use cases (Fig.1C-E).44\nContrastiveLearning\nImageEncoder\nThis lesion is pigmented ... and asymmetric\nA. Training model using contrastive loss\nImages\nThis lesion is pigmented ... and asymmetricThis lesion is pigmented ... and asymmetricThis lesion is pigmented ... and asymmetricThis lesion is pigmented ···and asymmetric\nCaptionsTextEncoder\nB. Overview of MONET –Automated concept generation and explanation\nC. Data AuditingD. Model AuditingE. Developing inherently interpretable model\nMalignancy\nLinear model\nAudit dataAudit modelDevelop interpretable model•••\n: Image: Concept\nTask\nConceptsMalignant setBenign set\nMore present in malignant\nUlcerBlue\nReference sticker\nMONETDifferentially expressed concept analysis\n...\nAI modelDiagnosisProb.Malignant0.21Benign0.79\nMONET\n MONET\nPubMed articles& textbooks\n• Asymmetric• Ulcer• Pigmented• Erythema...\nMONET (Medical concept retriever)Image\nList of concepts of interest\nGenerated concepts\nDistance\nMONETEnabling concept-drivenanalyses at multiple phases\nAsymmetricErythemaPigmented\nJointrepresentationspace ofimage & text (concept)• Asymmetric • Ulcer• Pigmented • Erythema..                .Ulcer\nUlcerationBlackCrust• • •0.580.750.64• • •\nWeight of linear model\nBlack boxAI pipeline(Model, Data, etc.)\n...More present in benign\nTrue: BenignPredicted: Malignant\nErythema, RedWhich input characteristic leads to model error?\n•••True: MalignantPredicted: Benign\nWarty, Comedo\nFig. 1 | Overview of MONET framework and its usage examples. (A)Training procedure.MONET is trained\nusing contrastive learning on an extensive set of dermatology image and text pairs collected from PubMed articles\nand medical textbooks. During the training process, the paired image and text are forced to be close in the joint\nrepresentation space, while those from different pairs are forced to be far apart. (B) Automatic concept generation.\nMONET can map medical concepts and images onto a joint representation space, allowing it to determine the degree\nto which a concept is present in an image for any given concept by measuring the distance between the image and\nconcept text prompts in the representation space. Its concept generation capability enables various concept-driven\nanalyses at multiple stages of the medical AI pipeline. (C) Concept-level data auditing. MONET’s automatic concept\ngeneration capability makes it possible to explain the distinguishing features between two sets of data in the language\nof human-interpretable concepts. This approach facilitates the auditing of large-scale datasets with ease. (D) Concept-\nlevel model auditing. MONET can be used to identify which input characteristic leads to the errors of medical AI.\n(E) Developing inherently interpretable models. MONET can be used to develop inherently interpretable medical\nAI models that operate on human-interpretable concepts aligning with physicians’ expectations. These models allow\nphysicians to easily decipher the factors influencing the models’ decisions, ensuring high transparency.\nDataset auditing can identify biases in the data before using it for any clinically relevant task, thereby improving45\nthe quality of the data, providing an opportunity for preliminary bias mitigation, and improving overall trustworthiness46\nin the data. Prior work in dataset auditing has identified how particular concepts are associated, either appropriately47\nor inappropriately, with data labels [2–4]. In medicine, data auditing has identified spurious correlations in AI training48\ndata [5, 6]. For example, the overrepresentation of chest drain in the x-rays of patients with pneumothorax led to AI49\nalgorithms that relied on their presence. However, chest tubes are a treatment used after a physician had diagnosed50\npneumothorax and not a causal feature [5]. Because data is not static, dataset auditing also allows the detection of51\ndataset shifts or drifts by identifying the changes in the representation of a concept in the data [7–9]. MONET enables52\n2\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nus to examine datasets on the basis of a rich set of automatically retrieved concepts (Fig.1C).53\nModel auditing involves demystifying the “black-box” of AI models – understanding the factors involved in AI54\ndecision-making [10–13]. AI models that fail during real-world deployment can lead to worse outcomes for patients.55\nAI models have been shown to make systematic errors on a subset of data with shared features, resulting in uneven56\nperformance across the data [2–6, 13, 14]. To prevent this and make appropriate adjustments, models should be57\naudited to understand their failure modes prior to deployment. Model auditing is not only important immediately58\nafter model development, but is a continual process, especially since models may undergo updating over time [7–9].59\nMONET powers model auditing: the dense concept annotations generated by MONET can be used to understand60\nwhich input characteristic leads to model errors (Fig.1D).61\nWhile most existing AI models are black boxes, newer methods in the field of explainable AI have attempted to62\ncreate inherently interpretable models that use concept-level features as input [15, 16]. The automatic generation of63\na rich set of semantically meaningful concepts allows us to leverage and enhance these recent models (Fig.1E).64\nBecause of the need to develop and test the aforementioned methods and tools, there have been prior attempts65\nto create densely annotated datasets in medicine. Such datasets include SkinCon [1], PH2 [17], derm7pt [18], and66\nOsteoarthritis Institute Knee X-ray dataset (OAI) [19]. However, because these medical datasets are annotated by67\nhumans and usually require domain expertise, the number of densely annotated datasets, the number of images in68\neach dataset, and the number of “concepts” that can be labeled in each image are limited. MONET overcomes these69\nchallenges by generating medical concepts automatically. Our framework is built uponcontrastive learning, a recent AI70\nbreakthrough that enables the direct utilization of natural language descriptions on images [20]. Since this approach71\ndoes not require manual labeling, it can unlock the potential of vast numbers of image and text pairs, allowing for the72\nharnessing of data of much larger scale than was possible with supervised learning.73\nTo train MONET, we collect an extensive set of dermatology image and text pairs (n = 105, 550) from PubMed74\narticles and medical textbooks. We map an image (or text) into a lower-dimensional vector or arepresentation through75\na neural network, namely the encoder (Fig.1A), creating a representation space. During the training process, an image76\nand text from the same pair are forced into close proximity in the representation space, while those from different pairs77\nare forced to be farther apart (Methods). Once trained, MONET’s zero-shot capability (i.e., the ability to generate78\na medical concept without a separate learning procedure) generates concepts (Fig. 1B and Methods). When a user79\nprovides an image and a list of concepts to generate, MONET determines the presence of each concept in the image80\nby calculating the distances between the image and concept text prompts in the joint representation space, where81\nimages and texts are jointly mapped.82\nMONET’s automatic concept generation capability enables a whole range of capabilities in medical AI that were83\npreviously infeasible in practice. We showcase MONET’s versatility by demonstrating its use in auditing data, auditing84\nmodels, and creating inherently interpretable models. MONET enables a sophisticated multi-point analysis and can be85\nused to probe any part of the medical processing workflow. For data auditing, we apply MONET to the International86\nSkin Imaging Collaboration (ISIC) dataset [21–26], the most widely used data in dermatology AI [27], to confirm87\nknown trends and discover new ones. We also use MONET to identify which input characteristics lead to errors88\nin medical AI models. Finally, we integrate MONET with the concept bottleneck model (CBM) [15], a well-known89\napproach for building inherently interpretable models, and show MONET+CBM’s advantages over supervised models90\nin terms of both performance and interpretability. All of these tasks are central to the development and deployment91\nof trustworthy and transparent AI models in medicine.92\nResults93\nAutomatic concept generation94\nWe first assess MONET’s concept generation capability before demonstrating how this capability can improve the95\ntransparency and interpretability of medical AI pipelines. The fundamental mechanism in MONET’s concept gener-96\nation is the mapping of medical concepts and images onto a joint representation space. This allows the generation97\nof a concept presence score, i.e., the degree to which a concept is present in an image, by measuring the distance98\nbetween the image and concept text prompts in the joint representation space (Methods). We evaluate MONET’s99\nconcept generation ability by identifying those images with the highest concept presence scores using both clinical100\nand dermoscopic images, the two widely used dermatological image types. Dermoscopic images are captured using101\ndigital photography with a specialized dermatological instrument called a dermoscope that magnifies skin lesions to102\ncapture fine details, while clinical images are often taken at least 6 cm away with a digital camera. For our evaluation,103\nwe employ clinical images (n = 4, 960) from the Fitzpatrick17k and Diverse Dermatology Images (DDI) datasets and104\ndermoscopic images (n = 71, 242) from the ISIC dataset (Methods).105\nFig. 2 and Supplementary Fig. 1-2 display clinical and dermoscopic images with high concept presence scores106\n3\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nfor each concept. These represent examples of widely used medical concepts in dermatology. Dermatologists use a107\nstandardized terminology to describe the morphology, color, configuration and distribution of skin lesions. MONET108\nexcels at recognizing these medical concepts in clinical and dermoscopic images. For example, “erythema” is a term109\nused by dermatologists to describe a red or violaceous color, which usually occurs in the presence of inflammation.110\nIt can be found in various skin diseases, such as atopic dermatitis, psoriasis, and rosacea. Two board-certified111\ndermatologists confirmed that the images with large presence scores for erythema exhibit skin redness in both clinical112\nand dermoscopic images (Fig. 2). Similarly, images with the concept “blue” show dark blue lesions with pigmentation113\nin the dermis, resulting from the Tyndall effect. Moreover, MONET was able to retrieve images with primary114\nmorphological features such as bullae (large, tense fluid-filled blisters) and pustules (small, pus-filled blisters), as well115\nas secondary morphological features including ulcers (open sores) and xerosis (dry, scaly skin).116\nWe assess the performance of MONET’s concept generation using ground truth concept labels in SkinCon (Table117\n1). Of the 48 concepts in the dataset, we exclude any with less than 30 positive examples, leaving 21 concepts for our118\nanalysis. We use 1,645 images from Fitzpatrick17k and DDI datasets with ground truth SkinCon concept labels. We119\ncompare MONET’s performance to a supervised learning approach, training a ResNet-50 model using ground-truth120\nconcept labels from SkinCon [28], and to a pre-existing contrastive image-text model that was not specifically trained121\non dermatology images but on 400 million available image-text pairs on the web - the CLIP (Contrastive Language-122\nImage Pretraining) model by OpenAI [20] (Methods). We find that MONET outperforms the ResNet-50 model and123\nthe CLIP model in terms of concept generation. Specifically, we compare the mean of the area under the receiver124\noperating characteristic curve (AUROC) across concepts with ground truth labels and count how many concepts125\nachieved an AUROC higher than 0.7. MONET achieves a mean AUROC of 0.766; in contrast, CLIP achieves a mean126\nAUROC of 0.692. The ResNet-50 model, trained to predict concept labels, achieves a mean AUROC of 0.692. Of the127\n21 concepts analyzed, MONET remarkably displays 19 concepts with an AUROC over 0.7, compared to 9 for CLIP128\nand 11 for the fully supervised model. Additionally, we conduct the same comparative analysis using disease labels,129\nwhich can be viewed as the most fine-grained concept labels; we map the disease labels instead of SkinCon concepts130\nto the image-text joint representation space. Our findings indicate that MONET’s performance is still comparable to131\nthat of supervised models in this case (Supplementary Table 2). This observation is consistent with a previous study,132\nwhich found that a contrastive learning model trained on radiology images demonstrates comparable performance in133\npredicting pathologies in chest X-rays to that of supervised learning models [29].134\nWe also evaluate the performance of MONET’s concept generation across diverse skin tones (Methods). A recent135\nstudy revealed that state-of-the-art dermatology AI models exhibit uneven performance across skin tones, particularly136\nunderperforming on dark skin tones, potentially due to the insufficient representation of diverse skin tones in training137\ndata [30]. One advantage of contrastive learning, the technique used to train MONET, is its ability to easily harness138\nheterogeneous data from diverse sources for training. This can help reduce performance disparities across demographics139\ncompared to training on a single data source. To determine whether MONET is free from this issue, we compared140\nits performance per skin tone using the Fitzpatrick skin type labels included in the Fitzpatrick17k and DDI datasets.141\nMONET demonstrated even performance across skin tones (Supplementary Table 3).142\nFinally, we also explore MONET’s capability to recognize non-clinical concepts, such as artifacts that are irrelevant143\nto the diagnosis. Many studies have shown that medical AI systems use such non-clinical concepts to make predictions,144\nparticularly when a spurious correlation exists between the artifacts and prediction labels [6, 31]. In dermatology AI,145\nit has been shown that artifacts, such as clinical marking or size reference stickers, can have a detrimental effect on146\nthe model’s generalizability [32–34]. The ability of MONET to identify such artifacts, in addition to clinical concepts,147\nwill facilitate more fine-grained auditing and debugging of medical AI pipelines. Supplementary Fig. 3 shows images148\nfrom the ISIC dataset that MONET identified as containing non-clinical concepts. Supplementary Fig. 3A shows149\nimages with purple pen ink marking that MONET automatically identified; in dermatology, lesions that are biopsied150\nare often routinely marked with purple ink markers. Supplementary Fig. 3B shows orange stickers that MONET151\nidentified; they serve as a lesion marker. In ths ISIC dataset, these orange stickers predominantly show up in the152\npediatric cases, which are largely benign. As these artifacts predominantly appear in certain types of images, they153\nmay inadvertently cause AI algorithms to associate purple ink markings with malignancy and orange stickers with154\nbenign lesions [34]. Also, MONET automatically identifies images with body location features (such as nails and155\nhair) (Supplementary Fig. 3C, D). A recent study has shown that anatomic locations may play a critical role in156\nthe performance of dermatology AI algorithms; however, most datasets lack these annotations [35]. Further,MONET157\nidentifies images with dermoscopic borders, which appear on a subset of dermoscopic images depending on the image158\nprocessing process (Supplementary Fig. 3E).159\nIn the following sections, we showcase how MONET can be used to improve the transparency and trustworthiness160\nof dermatology AI in real-world scenarios.161\n4\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nErythema\nA\nBulla\nXerosis\nPustule\nUlcer\nErythema\nB\nBlue\nNodule\nUlcer\nWarty\nFig. 2 | Images with high concept presence scores calculated using MONET. The concept presence score\nrepresents the degree to which a concept is present in an image. Each row displays the top 10 images for each concept.\n(A) Clinical images from the Fitzpatrick17k and DDI datasets. We exclude images inappropriate for public display\ndue to the inclusion of sensitive body parts; for completeness, we denote the filenames of these files in Supplementary\nTable 1 (B) Dermoscopy images from the ISIC dataset.\nMethod Mean AUROC\nMONET 0.766 (19/21)\nCLIP 0.692 (9/21)\nResNet-50 (Fully supervised) 0.692 (11/21)\nTable 1 | Performance of MONET’s concept generation as compared to the baselines. We use concept\nlabels in the SkinCon dataset as ground truth. Of the 48 concepts in the dataset, we exclude any with less than 30\npositive examples, leaving 21 concepts for our analysis. The baselines are CLIP, an image-text model not specifically\ntrained on dermatology images, and the ResNet-50 model trained on ground truth labels in a fully supervised manner.\nThe numbers in parentheses represent the count of concepts for which the method achieves an AUROC over 0.7 over\nthe total number of concepts examined.\n5\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nData auditing162\nEnsuring that training data aligns with users’ expectations is a crucial first step toward developing AI models since163\nmany unreasonable model behaviors stem from unidentified pitfalls in the training data [6, 32, 34]. For example, in164\ndermatology, when preparing a dataset for training an AI model to diagnose malignancy, the differentiating features165\nin the data between classes (i.e., malignant and benign images) should not contain any biases or spurious correlations,166\nsuch as the pen markings used to identify biopsied lesions [34]. Upon identifying any irregularities, adjustments can167\nbe made, such as improving the data collection and processing [36, 37] or applying optimization techniques to improve168\ngeneralizability [31, 38].169\nHowever, examining large-scale datasets for irregularities is challenging and labor-intensive. One approach is to170\nmanually label features of interest and create a contingency table between each feature and the target label to check for171\nspurious correlations; however, this is subjective and not easily scalable [5]. Another approach is to train a generative172\nmodel, such as CycleGAN [39], to learn the distribution of data for each class [6]; the trained generative model can173\nmodify an image from one class to resemble an image from another class. By observing these changes, a data examiner174\ncan identify the distinguishing features of each diagnostic group. However, generative models are difficult to train and175\nnecessitate manual inspection of the transformed images.176\nTo address the issue, we can employ MONET to automate the data examination process. MONET’s automatic177\nconcept generation capability can explain the distinguishing features between any two arbitrary sets of images in the178\nlanguage of human-interpretable concepts, which we refer to as concept differential analysis (see Methods). Supple-179\nmentary Fig. 4 shows benchmark analysis results.180\nAs a practical use case, we employ MONET to analyze the ISIC dataset, the largest dermoscopic image dataset,181\nwhich consists of over 70, 000 publicly available images that are commonly used to train dermatology AI models [21–182\n27]. We divide the images into a malignant ( n = 10, 091) and a benign set ( n = 61, 151), assuming malignancy as183\nthe prediction target, and examine which concepts were more present in which set (Fig. 3A). We test for 48 concepts184\nlisted in SkinCon along with eight artifacts, including red coloration, pinkish coloration, and purple ink markings,185\nnails, hair, orange sticker, gel, and dermoscopic border.186\nThe top 5 concepts in the malignant images are ulcer, erosion, warty, pinkish coloration and blue coloration; in187\ncontrast, the top 5 concepts in the benign images are orange sticker, hypopigmentation, the color salmon, xerosis, and188\nhyperpigmentation. These concepts represent key features that a prediction model, trained on the ISIC dataset, may189\npotentially use to differentiate benign from malignant lesions. They encompass both clinically pertinent features such190\nas ulcer, crust, erosion, warty, and black coloration, as well as irrelevant confounders such as orange sticker and nail.191\nSkin ulcerations and erosions are commonly linked to malignant skin tumors such as melanoma, basal cell carcinoma192\nand squamous cell carcinoma, making their association with malignancy logical. On the other hand, prediction models193\nlearning from confounding concepts may lead to biases and detrimental consequences. For example, dermatologists use194\norange stickers as a lesion marker, and with the ISIC dataset, this was predominantly used in the pediatric population195\nwhich had mostly benign lesions. The bias in the data could lead a model to erroneously associate orange stickers196\nwith a low likelihood of malignancy.197\nFurthermore, we can use this approach to assess distinctive trends specific to different data sources. In medicine,198\ndata sharing across institutions is limited due to the sensitive nature of medical data and regulatory constraints.199\nIn many cases, a medical AI is developed within a few institutions and then distributed to other institutions for200\ndeployment. For this reason, it is important to understand and monitor the shifts in the concept representations201\nbetween data and identify factors that can potentially compromise the transferability of medical AI. By doing so,202\nnecessary adjustments can be made preemptively. The ISIC dataset is a collection of images from multiple hospitals203\nand research institutions, which serves as an ideal resource for simulating situations where the development and204\ndeployment sites differ. In this analysis, we focus on two cohorts released in the ISIC Challenge 2019—the Medical205\nUniversity of Vienna (Med U. Vienna, malignant: n = 1, 824 / benign: n = 8, 049) and the Hospital Cl´ ınic de206\nBarcelona (Hospital Barcelona, malignant: n = 6, 097 / benign: n = 6, 205)—since they represent the two largest207\ncohorts in the entire ISIC dataset when stratified by release year and data source. We perform concept differential208\nanalysis between malignant and benign images, as noted above, for each cohort separately. We then compare the209\nobtained concept differential expression scores between the two cohorts (Fig. 3B).210\nWhen we sort the test concepts in the order of absolute differences, the top-listed concept was a “red” hue.211\nRedness is positively correlated with malignancy for the images from Hospital Barcelona but negatively correlated212\nwith malignancy for the images from Med U. Vienna. This means that redness has the potential to compromise the213\ntransferability of medical AI between two institutions. This trend is clearly visible in the sampled images from each214\ncohort, as well. Fig. 3C displays images sampled from the top 100 images with high concept expression scores for the215\nred coloration for each cohort, along with their diagnostic labels. The images that have more redness collected from216\nMed U. Vienna are often benign, while those collected from Hospital Barcelona are often malignant. The top 500 and217\ntop 1,000 red images in the Med U. Vienna contain more benign than malignant ones, while the top 500 and top 1,000218\n6\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nred images from Hospital Barcelona still contain more malignant than benign samples. (Fig. 3D).219\nIn sum, we demonstrate how MONET can assist with auditing large-scale datasets. Since concept differential220\nanalysis is conducted simply by describing a concept in a natural language, the approach fosters the scalable discovery221\nof trends within the data. Using the insights gained through this process, AI model developers can enhance data222\ncollection, processing, and optimization techniques, ultimately yielding more reliable and trustworthy medical AI223\nmodels.224\n7\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nAll ISIC images (N=71,242)\nA. All institutions B.  Per institution\nMed U. Vienna (N=9,873) Hosp. Barcelona (N=12,302)\nMalignant Benign\nMONET\nMalignant Benign Malignant Benign\nMONETMONET\nA B\n0.02 0.01 0.00 0.01 0.02\nExpression difference\nUlcer\nErosion\nWarty\nPinkish\nBlue\nNail\nCrust\nPedunculated\nPurple\nBlack\nHyperpigmentation\nXerosis\nSalmon\nHypopigmentation\nOrange sticker\nAll institutions\n0.02 0.01 0.00 0.01 0.02\nExpression difference\nRed\nTelangiectasia\nErythema\nWheal\nHyperpigmentation\nPustule\nPinkish\nAbscess\nWarty\nHypopigmentation\nPurpura/Petechiae\nPigmented\nCrust\nTranslucent\nBlack\nGray\nPatch\nPer institution\nMedical University of Vienna\nHospital Clínic de Barcelona\nC\nMaligant Benign\nMedical University of Vienna\nHospital Clínic de Barcelona\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Precision\nD\nTop 100\nTop 500\nTop 1000\nMedical University of Vienna\nHospital Clínic de Barcelona\nFig. 3 | Concept-level data auditing. (A) We perform concept differential analysis between malignant images\nand benign images in the ISIC dataset. We show the top 10 concepts with positive values and the top 5 concepts with\nnegative values. A positive value means the concept was more present in the malignant images than in the benign\nimages, and vice versa. (B) We perform concept differential analysis between malignant and benign images per data\nsource in the ISIC dataset to identify data-source-specific trends. The purple bar represents the output from the\nMedical University of Vienna, and the green bar represents the output from the Hospital Clinic de Barcelona. We\nshow the top 15 concepts based on their absolute differences between the two cohorts. (C) Examples of red images in\neach cohort. We display 10 randomly selected images from the top 100 images in each cohort that had high concept\nexpression scores for redness. (D) Precision-recall curve for images in each cohort. The images in each cohort are\nsorted based on their concept presence scores for redness and then compared to their malignancy labels. Precision is\ndefined as the proportion of malignant images above a certain threshold out of all images above that threshold, while\nrecall rate is defined as the proportion of malignant images above the threshold out of all malignant images. The top\n500 and top 1000 red images from Barcelona Hospital still contain more malignant than benign samples.\n8\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nModel auditing225\nVarious techniques for auditing AI models have been developed to understand the factors involved in AI decision-226\nmaking. One classical approach is the use of saliency maps, which highlight regions in an input image that significantly227\ncontribute to the model’s prediction [40–42]. The saliency maps of each image help to identify which pixel-level228\nfeatures lead to a correct or incorrect prediction. However, the highlighted pixels are often not easily translated into229\nsemantically meaningful concepts understandable to a human [43].230\nTo address this issue, we can use MONET to audit AI models through the lens of medical concepts. We developed231\na method “model auditing with MONET” (MA-MONET) that uses MONET to automatically detect semantically232\nmeaningful medical concepts that lead to model errors (Methods). MA-MONET starts by sorting images from a test233\nset into groups based on their visual similarity. It then labels the clusters that perform below the overall accuracy as234\nlow-performing. For each low-performing cluster, MONET identifies medical concepts. Each low-performing cluster235\nis then compared to a high-performing counterpart containing similar images, with concepts separately identified in236\nthe high-performing cluster. If two visually similar clusters (one high-performing, the other low-performing) differ in237\nterms of a few concepts, these differing concept terms can be hypothesized as leading to high error rates. Finally, we238\nproduce a ranked list of medical concepts identified by MONET that differentiate the two clusters.239\nTo validate our model auditing, we first perform a benchmarking analysis, using a situation where the ground240\ntruth ( i.e., the concepts leading to model error) is already known (Fig. 4A and Methods). We create a training241\ndataset with spurious correlations from the Fitzpatrick17k and DDI datasets: 500 malignant images that feature a242\nparticular SkinCon concept, while the 500 benign images do not. After training a CNN model to predict malignancy243\non this confounded dataset, we test it on a dataset where the correlation is reversed (Methods); in the test set, 500244\nsampled benign images have the SkinCon concept, while 500 sampled malignant images do not. We cluster images245\nin the test set into 40 clusters, and about 20 of these clusters underperform, meaning their accuracy falls below the246\naverage accuracy. For these low-performing image clusters, we apply the MONET-based error explanation method247\nto obtain a ranked list of medical concepts that would explain the model error. Finally, we observe if the concept of248\nspurious correlation we know is recovered.249\nWe conduct the analysis using 5 concepts that remain after filtering out concepts with fewer than 30 samples in each250\ncategory required for creating the confounded training and test sets (i.e., malignant–with concept, malignant–without251\nconcept, benign–with concept, and benign–without concept) : “crust”, “hyperpigmentation”, “plaque”, “erythema”,252\nand “papule”. For each of the 5 concepts, we repeat this analysis 20 times with different random seeds changing the253\ntraining and test sets. Consequently, we test 100 settings in total. Across these settings, the mean AUROC of the254\ntrained model is 0.779 for validation sets, but decreases to a mean of 0.458 for test sets.255\nWe measure the frequency of the known spurious correlation being recovered by MA-MONET (Fig. 4B), checking256\nif the top-N concept lists of any low-performing clusters include the known spurious correlation concept. We compare257\nthis outcome with that of the out-of-the-box CLIP model, which was not specifically trained on dermatology data258\n[20]. The low-performing clusters being analyzed in each setting are the same for both methods, but the enumeration259\nof concepts associated with errors is done using CLIP instead of MONET. The results for the top 1, 2, and 3 rankings260\nare markedly higher with MONET, at 0.590, 0.800, and 0.890, respectively, compared to those obtained with CLIP,261\nwhich are 0.270, 0.520, and 0.660, respectively.262\nTo showcase its use in real-world scenarios, we consider a widely occurring situation where a model is trained263\nat one institution and deployed at another [44, 45] (Fig. 4C). For training and testing, we use the same datasets264\nwe used in the data auditing section, specifically the two largest cohorts in the ISIC dataset: the Hospital Cl´ ınic de265\nBarcelona (n = 12, 302) and the Medical University of Vienna ( n = 9, 873). We train CNN models on the data from266\none institution using a standard training regimen and test them on the data from the other institution, and vice versa.267\nFor the AI model trained on Med U. Vienna, it showed an AUROC of 0.885 in the internal validation, but the268\nvalue dropped to 0.707 during the external validation. This decline in performance would prompt AI model developers269\nto question which input characteristics led to model errors. To elucidate this, we use MA-MONET to pinpoint which270\nconcepts are associated with model errors. For each cluster with high error rates, the misclassified images and the271\nterms associated with errors are shown in Fig. 4D (displaying the top 5 clusters sorted in the order of high error rates)272\nand Supplementary Fig. 5 (displaying the top 15 clusters sorted in the order of high error rates). For example, the273\ncluster with the highest error rate, displayed in the first row of Fig. 4D and Supplementary Fig. 5A, is characterized by274\nthe concepts “blue”, “black”, “gray”, “pigmented”, and “flat-topped”. Remarkably, we notice several clusters where275\nhigh error rates are explained by concepts related to “red”. For instance, the cluster shown in Supplementary Fig. 5F276\nare characterized by “erythema”, “salmon”, “sclerosis”, “scar”, and “translucent”. Interestingly, we also find that the277\nmalignant images are predominantly misclassified as benign. Out of the 74 malignant images in the cluster, 55 images278\nare misclassified to be benign. This observation aligns with the trends we noted in the data auditing experiment,279\nwhere red images from Med U. Vienna ( i.e., training dataset) were benign, while the red images in Hosp. Barcelona280\n(i.e., test dataset) were malignant.281\n9\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nConversely, we also train an AI model on Hosp. Barcelona data and tested it on Med U. of Vienna data. In282\nthis case, the AUROC of 0.844 in internal validation drops to 0.741 during external validation. For each cluster with283\nhigh error rates, the misclassified images and the identified terms associated with errors are shown in Fig. 4E and284\nSupplementary Fig 6. The cluster with the highest error rate, shown in the first row of Fig. 4E, is characterized by the285\nconcepts “pinkish”, “erythema”, “gray”, “red”, “atrophy”. Out of the 197 benign images in the cluster, 103 images286\nare misclassified as malignant. This observation also aligns with the trends we noted in the data auditing experiment.287\n10\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nA. Training dataset with spurious \ncorrelation: Malignant ≃ Crust\nTrain model\nMONET\n+ Crust\n+ Nodule\n+ Papule\nAI model\nMalignant Benign\n.\n.\n. .\n.\n.\nClusters of images with a high error rate\nList of concepts that lead to model error\nTest model\nTest dataset with the reversed \ncorrelation: Benign ≃ Crust\n...\n1 2 3\nTop-N\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Freq. of recovering spurious corr.\nB. Do the top-N concept explanations recover spurious correlations?\n(across all low-performing clusters)\nMONET CLIP\nC. Training at Med U. Vienna\nAI model AI model\n[ 1, 0, 1, • • •, 0 ]\nAUROC = 0.885   \nTesting at Hosp. Barcelona \nImages\nTrue labels\n0\n0\n1\n0\nImages\nAUROC = 0.707\n.\n.\n.\n.\n.\n.\n[ 0, 1, 1, • • •, 1 ]\nTrue labels Predicted\n1\n0\n1\n0\n.\n.\n.\nPredicted\nWhich input characteristic leads to model error?\n.\n.\n.\nD. Trained at Med U. Vienna / Tested at Hosp. Barcelona \nBlue, Black, Gray, Pigmented, Flat topped\nPigmented\nUlcer, Comedo, Crust\nBlue, Black, Pigmented\nUlcer, Atrophy, Dermoscopic border\nE. Trained at Hosp. Barcelona / Tested at Med U. Vienna\nPinkish, Erythema, Gray, Red, Atrophy\nGray, Pigmented, Hyperpigmentation\nGray, Hyperpigmentation, Pigmented, Hypopigmentation\nPoikiloderma, Gray, Hypopigmentation, Fungating, Atrophy\nGray, Pigmented, Hyperpigmentation, Atrophy\nE. Trained at Med U. Vienna / Tested at Hosp. Barcelona \nBlue, Black, Gray, Pigmented, Flat topped\nMaligant Benign   (Upper left: True, Lower right: Pred)\nTrue: 76 / 17  Pred: 11 / 82 \nFig.\n4 | Concept-level model auditing. (A) We perform a benchmark analysis to see how well “model auditing\nwith MONET” (MA-MONET) can identify the semantically meaningful concepts that lead to model error. To this\nend, we generate settings where we know the ground truth ( i.e., concepts that lead to model errors); we create a\ntraining and test dataset with spurious correlation. We use MA-MONET to identify which concepts lead to model\nerror for an AI model trained on this confounded dataset. MA-MONET returns a ranked list of concepts that explain\nmodel errors. (B) The frequency of the known spurious correlation being recovered by MA-MONET is shown. (D)-\n(E) Each row displays one of the top 5 clusters, sorted by high error rates. For each cluster, we show the misclassified\nimages and the corresponding concepts associated with errors. We represent the true and predicted labels for each\nimage by the color of the upper left and lower right triangles in the small box, respectively. The numbers at the top\nright compare the number of malignant and benign samples for the true and predicted labels. The 5 misclassified\nimages shown for each are selected based on the average concept presence of the identified concepts.\n11\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nInherently interpretable model building288\nIn medicine, inherently interpretable models are of particular interest since they allow physicians to easily decipher289\nthe factors influencing a model’s decision. Rather than training a complex black-box model that requires post-hoc290\nexplanation, an inherently interpretable model offers greater transparency and control in model behavior. Concept291\nbottleneck models (CBMs) are a well-known type of inherently interpretable model [15]. CBMs make predictions in292\na two-step manner: first, they predict concepts from the input using a complex model such as a CNN (i.e., input293\n→ concept); then, they use these predicted concepts to predict the target output via a linear model (i.e., concept294\n→ output). As each node in the bottleneck layer represents a human-interpretable concept, CBMs offer greater295\nexplainability. Further, CBMs facilitate the incorporation of domain knowledge into models by imposing constraints296\non the concepts used, thereby improving the ability to control model behavior.297\nHowever, CBMs have a significant limitation: they require concept annotations in the training data. To achieve298\nhigh performance with CBMs, it is essential to train them on a sufficient number of samples and ensure they operate299\nwith an adequate number of concept labels that are relevant to the prediction task. This constraint has hindered the300\npractical application of CBMs. We address this issue by using MONET’s automatic concept generation to eliminate301\nthe need for manually annotated concept labels in the original training procedure of the CBMs.302\nWe explore the application of MONET and CBMs for melanoma and malignancy prediction tasks, the most preva-303\nlent prediction tasks in dermatology AI. MONET+CBM approach predicts the target (i.e., melanoma or malignancy)304\nby combining automatically generated concepts by MONET ( i.e., the concept presence score) via a linear model305\n(Fig. 5A and Methods). This gives CBM access to many concepts and many samples compared to a manual labeling306\napproach. The following comparison makes use of 4,960 clinical images sourced from the Fitzpatrick17k and DDI307\ndatasets. For melanoma prediction, we further filter images to ensure that data mirrors a well-defined clinical task,308\nresulting in 775 images (Methods). For each setting, we repeated evaluations with 20 different train-test splits.309\nWe observe that access to a large number of concepts and samples offers performance advantages. Fig. 5B-E310\ncompares the performance of MONET+CBM to that of using manual concept annotations. For a fair comparison, we311\nuse both methods on the same set of concepts, specifically the 48 concepts in SkinCon. We chose SkinCon concepts312\nbecause they already have manual annotations provided by experts. For MONET+CBM, we use all training samples313\nand concepts because it can automatically generate concepts without expert annotation. As we increase the number314\nof manually labeled samples used or the number of concepts used, the performance of the CBM created from manual315\nlabeling improves. However, even when all manually labeled concepts and training samples are used, the manual316\napproach is not able to match the performance of MONET+CBM, which has access to more samples due to the317\nability to produce automatic concept labels. As concepts in SkinCon are not annotated for all images, the manual318\nlabel approach is limited to 1,316 malignancy and 294 melanoma images that have manual concept labels; in contrast,319\nMONET makes use of all 3,968 malignancy and 620 melanoma images in our training set.320\nWe compare the performance MONET+CBM to the other baselines, such as supervised models and CLIP-based321\nCBM, for the same prediction targets (as described in Methods) (Fig. 5F and G). Dermatologists selected 11 target-322\nrelevant curated concepts for the bottleneck layer to compare MONET+CBM and CLIP+CBM, which can both323\nflexibly label concepts. Compared to using all 48 SkinCon concepts, the mean AUROC across runs using the 11 curated324\nconcepts decreased from 0.854 to 0.805 for malignancy prediction and decreased from 0.896 to 0.892 for melanoma325\nprediction. Still, for both predicting malignancy and melanoma, MONET+CBM outperforms all other baseline326\nmethods in terms of the mean AUROC (for malignancy, 0.805 with a standard deviation of 0.014; for melanoma,327\n0.892 with a standard deviation of 0.019). Out of 20 runs with different random splits of the train and test data,328\nMONET+CBM outperformed all other methods in 15 runs for malignancy prediction and 18 for melanoma prediction,329\nwith the linear probing method outperforming in the remaining runs. We also conduct one-sided paired t-tests,330\ncomparing the AUROC values of MONET+CBM to those of other methods, where the alternative hypothesis is that331\nMONET+CBM’s AUROC is higher than the other method. In all cases, the resulting p-values are less than 0.001.332\nThus, MONET’s ability to automatically generate concepts enables the creation of models that are both interpretable333\nand high-performing.334\nWhile the concepts used for the concept bottleneck model were selected by dermatologists based on factors that can335\nhelp predict melanoma, we wanted to check that the way these concepts were used by this model align with established336\nclinical rules for the same task. Fig. 5H and I show the weights of the trained linear classifier corresponding to the337\nconcepts used in the bottleneck layer. For the Melanoma target, the results are consistent with the ABCDEs of338\nmelanoma [46], which define easily recognizable features—namely, asymmetry, border irregularity, color variation,339\ndiameter, and evolution—that differentiate malignant melanomas from benign melanocytic nevi. From the concept340\nweights obtained, all concepts that coincide with the ABCDEs get a positive weight as expected, indicating a positive341\ncorrelation with the melanoma prediction target. The concept “blue” also has a positive weight referring to the342\ndermoscopy concept of blue-white veils observed in melanomas. The concepts “white” and “tiny” get an almost zero343\nweight, while the concept “regular” gets a negative weight, consistent with prior knowledge shared by dermatologists,344\n12\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nthat regular borders and color indicate a benign lesion. For the malignancy target, no well-defined guidelines exist for345\nderiving concepts; thus, the same concept list as the Melanoma target is used. The results are similar, with a majority346\nof the concepts retaining their directionality, except for increased sparsity in concept weights.347\n13\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nImages\nAsymmetric\nConcepts\nMONET Malignancy /\nMelanoma\nLinear Model\nTask\nConcept Layer\n.\n.\n.\n.\n.\n.\nImages\nAsymmetric\nConcepts\nConcept Layer\nMalignancy /\nMelanoma\nLinear Model\nTask\n.\n.\n.\n.\n.\n.\nA \nA\n0 10 20 30 40 50\nNum. of concepts\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Area under the ROC curve\nB Malignancy\n0 500 1000\nNum. of expert-labeled samples\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nC Malignancy\nMONET+CBM (SkinCon) Manual Label (SkinCon)\n0 10 20 30 40 50\nNum. of concepts\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nD Melanoma\n0 100 200 300\nNum. of expert-labeled samples\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nE Melanoma\nF\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Area under the ROC curve\n***\n******\n***\nMalignancy\nMONET+CBM (Curated) Manual Label (SkinCon) Supervised (ResNet-50) Linear probing (ResNet-50) CLIP+CBM\nG\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n***\n***\n******\nMelanoma\nH\n3.0 2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0\nCoefficients of linear model\nAsymmetry\nIrregular\nErosion\nBlack\nBlue\nWhite\nBrown\nMultiple Colors\nTiny\nRegular\nMalignancy\n3.80 (±0.22)\n4.18 (±0.07)\nI\n3.0 2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0\nCoefficients of linear model\nAsymmetry\nIrregular\nErosion\nBlack\nBlue\nWhite\nBrown\nMultiple Colors\nTiny\nRegular\nMelanoma\n3.84 (±0.16)\n8.66 (±0.28)\nFig.\n5 | Concept bottleneck model. (A) Concept bottleneck model built using concepts generated by MONET\n(blue). The model first generates concepts using MONET and then predicts disease labels by combining them via\na linear model. Concept bottleneck model built using concepts manually labeled by experts (green). The model\nuses manually annotated concept labels to predict disease labels using a linear model. Manual annotations take a\nlot longer than concept generation using MONET. (B)-(C) Performance of a malignancy prediction model trained\nusing manual labels with respect to the number of concepts and the number of expert-labeled samples. (D)-(E)\nPerformance of a melanoma prediction model trained using manual labels with respect to the number of concepts\nand the number of expert-labeled samples. (B)-(E) MONET+CBM is shown as a cross mark because it can utilize\nall concepts without expert annotation. The shaded area represents the 95% confidence interval. (F) Performance\ncomparison of malignancy prediction models. (G) Performance comparison of melanoma prediction models. (F)-(G)\nUnlike (B)-(E), MONET+CBM uses task-relevant concepts curated by dermatologists. Each dot represents the AUC\nmeasure for individual runs with a different train-test split. The box represents the interquartile range with its lower\nand upper bounds corresponding to the first quartile and third quartile, respectively. p values derived from one-sided\npaired t-tests comparing MONET+CBM and other methods are indicated: * <0.05, **<0.01, ***<0.001; n=20 runs\nof each method. (H) Coefficient of the linear model in MONET+CBM for malignancy prediction. (I) Coefficient\nof the linear model in MONET+CBM for melanoma prediction. (H)-(I) The error bars present the 95% confidence\ninterval.\n14\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nDiscussion348\nEven with the regulatory approval of AI-supported medical devices, much of the AI pipeline is not transparent - from349\nlarge-scale datasets that may contain biases to so-called “black-box” models that are not easily audited or interpretable350\n[27]. One approach to improving transparency and trustworthiness is identifying semantically meaningful, human-351\nunderstandable concepts that are present in datasets or used by models. However, to date, all datasets and methods352\ndeveloped using concepts have relied on human labeling and domain experts, which is not tractable for large-scale,353\nreal-world deployment.354\nHere, we demonstrate the ability to develop automated concept labeling in a medical domain that would usually355\nrequire domain expertise, and we showcase how these automated concepts can be used to perform tasks for trustworthy356\nAI development at all stages, from developing new models to auditing existing datasets and models. We focus on357\nthe field of dermatology due to the heterogeneity of the image data, the large number of potential concepts, and the358\nability to validate our methods on existing datasets.359\nPrior work using image-text models in medicine focused on self-supervised training of diagnostic models that can360\nidentify a handful of disease labels, such as in radiology or pathology [29, 47]. However, our challenge is to develop a361\nmodel that can label a vast number of human-understandable concepts across two image modalities within dermatology:362\nclinical images and dermoscopic images. To solve this challenge, we collect a large number of dermatology image-text363\npairs from PubMed articles and medical textbooks and train an image-text model, MONET. This dermatology image-364\ntest model facilitates automatic generation of concepts, and we show how it can be used to improve the transparency365\nof dermatology AI models. To our knowledge, we are the first to use a large biomedical image-text model to improve366\nthe transparency and explainability of medical AI systems.367\nFor a concept generation task where we had domain expert labels as the ground truth, we find that MONET,368\nwhich requires no domain expert labeling, outperforms the baseline CLIP model and a supervised model trained369\nfrom domain-expert labeled images. These findings are significant since the bottlenecks of data labeling and domain370\nexpertise time can be overcome with image-text models developed from existing medical corpora.371\nAfter demonstrating the ability to generate concepts on par with supervised models, we showcase MONET’s ability372\nto facilitate AI auditing and transparency in the dermatology domain. For example, artifacts such as pen markings,373\nstickers, and hair are known to affect dermatology model performance [34, 48]. However, most studies do not assess374\nthe influence of artifacts on their models because their datasets are not labeled for these anomalies. We demonstrate375\nMONET’s ability to automatically identify these artifacts, which can be useful for data and model auditing. As376\nan example of how this kind of auditing is useful, we analyzed data from the ISIC 2018 challenge and find that a377\n“red” hue appears more often in benign images for the Medical University of Vienna while images from the Hospital378\nCl´ ınic de Barcelona more often have a “red” hue associated with malignant images. This leads to confounding if a379\nmodel is trained on one site’s dataset and tested on the other, as we see when we implement MA-MONET for model380\nauditing. These insights derived from using MA-MONET might not be readily achievable via conventional saliency381\nmap techniques [43]. For instance, the “red” hue noticed using MA-MONET is not a localized attribute, so a saliency382\nmap approach would not necessarily highlight this aspect [43]. Utilizing the insights gained through MONET and383\nMA-MONET, AI model developers can refine data collection and processing and also improve optimization techniques,384\nconsequently fostering the development of more reliable and trustworthy medical AI models.385\nIn medicine, inherently interpretable models are of particular interest since they allow physicians to easily decipher386\nthe factors influencing a model’s decision. Concept bottleneck models (CBMs) are one such inherently interpretable387\nmodel but have been limited because they require a priori concept labels, which only a handful of medical datasets388\ncontain. MONET overcomes this issue with automatic concept labeling, allowing the creation of CBMs that were not389\npreviously possible.390\nMONET demonstrates the ability to automatically label numerous concepts across heterogeneous disease states and391\nacross two modalities (clinical and dermoscopic) in dermatology. A limitation of our experiments is the availability392\nof diverse skin tones in dermoscopic images since no public datasets exist with diverse dermoscopic images [49].393\nThus, when assessing MONET on clinical images, we utilize two datasets known to include a diversity of skin tones,394\nFitzpatrick 17k and DDI, and find that MONET performs well with these datasets.395\nWhile MONET covers heterogeneous dermatology data across two modalities, clinical and dermoscopic, future396\niterations can extend to other forms of medical imaging to improve AI transparency for those use cases. MONET397\ndemonstrates that AI transparency and trustworthiness at scale is feasible in a way that was previously impossible:398\nthrough image-text models tailored to the medical domain of interest.399\n15\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nMethods400\nDataset401\nOverview402\nWe trained MONET on 105,550 pairs of image and text collected from PubMed articles and medical textbooks [50, 51].403\nWe evaluated MONET using images from the International Skin Imaging Collaboration (ISIC) [21–26], Fitzpatrick17k404\n[52], and Diverse Dermatology Images (DDI) datasets [30].405\nPubMed406\nThe PMC Open Access Subset is a dataset with millions of scientific articles released by PubMed Central (PMC) [50].407\nFirst, to find dermatology articles in the dataset, we queried papers in PMC using dermatology-related terms (i.e.,408\ndermatology, melanoma, skin cancer), 114 disease labels in the Fizpatrick17k dataset [52], and 48 concept labels in409\nSkinCon [1]. We downloaded 496,510 articles found via this query. In total, the articles contained 3,172,490 figures.410\nNext, we filtered out non-dermatology-related figures (e.g., graphs, illustrations, diagrams, slide images, and X-ray411\nimages). To this end, we repeated the process of running a clustering algorithm on the images and manually excluding412\ngroups of non-dermatology ones. Specifically, we carried out the following procedure. The clustering features were413\n50 principal components of the embedding of the penultimate layer of the EfficientNetV2-S model pre-trained on414\nImageNet [53]. Using the features, we ran a K-means clustering algorithm with the K (i.e., the number of clusters) of415\n20. For fine-grained filtering, we further applied the K-means algorithm with the K of 20 on each cluster, resulting in416\n400 clusters in each step. For the K-means algorithm, we used the implementation in scikit-learn Python package (ver.417\n1.2.2) [54]. We manually inspected 50 samples of each cluster and filtered out clusters with non-dermatology images.418\nAfter going through this step three times, we determined that the remaining clusters contained mostly dermatology419\nimages. Post-filtering, 50,265 images remained. Finally, we paired the figure captions to their corresponding images420\nbased on the provided XML-formatted file. This file stores the article’s structure with components such as abstract,421\nsections, figures, and figure legends tagged.422\nTextbook423\nWe first extracted images from 55 medical textbooks, yielding a total of 104,223 images. After undergoing the same424\nfiltering procedure as we did for PubMed images, 55,285 images remained. The PDF format of the textbooks made425\nmatching images with associated text difficult, since PDFs lack the structure information provided by XML formats.426\nTo address this issue, we implemented the following procedure. We used PyMuPDF (ver. 1.21.1), an open-source PDF427\nrendering software, to parse the PDF files, extracting text and image objects along with their respective coordinates.428\nThen, we assigned text to images appropriately based on the following criteria. First, we included text that starts429\nwith words indicative of figure legends, such as “Fig” or “Figure”. Next, we excluded text based on font and font size.430\nAlso, since each textbook maintained a consistent layout for placing figure legends (i.e., legends positioned above or431\nbelow the figure), we incorporated this into our filtering process. Lastly, from the remaining text, we selected the one432\nclosest to the image. We customized the specific parameters ( i.e., figure identifier, font, font size, and caption position433\nrelative to the image) for each textbook to ensure accurate text-image associations.434\nISIC435\nThe International Skin Imaging Collaboration (ISIC) archive is a repository of digital skin images, primarily consisting436\nof dermoscopic images, sourced from various institutions. ISIC represents the largest and the most commonly used437\ndataset for the development of dermatology AI [27]. We downloaded 71,671 images in total from all of the ISIC438\ncollections, including ISIC Challenge datasets 2016, 2017, 2018, 2019, and 2020 [21–26]. The images have diagnostic439\nattributes, including binary malignancy versus benign labels and 27 granular disease labels. For per-institution440\nanalysis, we grouped images by data sources based on the attribution column in the metadata. We selected the441\ntwo largest cohorts: the Department of Dermatology at the Medical University of Vienna (9,873 samples) and the442\nDepartment of Dermatology at the Hospital Cl´ ınic de Barcelona (12,302 samples).443\nFitzpatrick17k444\nSince the PubMed and textbook datasets contain clinical ( i.e., non-dermoscopic) images, for evaluation purposes,445\nwe required additional clinical images with ground-truth annotations. As the first of these datasets, we chose Fitz-446\npatrick17k[52], which contains dermatological images collected from online dermatology atlases accompanied by disease447\nannotations and Fitzpatrick skin type labels. To reduce the impact of artifacts in the images, we filtered the dataset448\n16\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nto exclude images with visible patient clothing, visible anatomy ( e.g., fingers, ears, eyes, etc.), or other elements449\nexcept for lesions and background skin. To filter these images, we first manually annotated 10% of the full dataset450\n(1,657 of 16,577 images), marking each image as include or exclude, then trained a machine-learning model to classify451\nthe remaining 90% of images. In particular, we fine-tuned a DenseNet-121[55] (pre-trained on ImageNet) to predict452\nexclusions using our 80% of our hand-labeled data, then chose an operating point to maximize the F1 score (maximum453\n= 0.81) on the remaining 20% of the hand-labeled data. This filtering resulted in a total of 4,951 images, incorporating454\nboth those that passed the classifier’s screening and 462 images from our hand-labeled set. Fitzpatrick17k contains455\nnear duplicate images with slight differences in angle or cropping; we filter for duplicate images to prevent overlap456\nbetween the train and test sets for the concept bottleneck model experiments. To measure the distance between457\nimages, we obtained the 50 principal components of the embedding of the penultimate layer of the EfficientNetV2-S458\nmodel (pre-trained on ImageNet) [53]. Then, we calculated cosine similarity between the 50 principal components of459\nEfficientNet embedding. To rigorously filter out duplicates, we used a loose threshold (cosine similarity = 0.9) and460\nmanually identified any false positives. In total, among the 4,951 images in Fitzpatrick17k “clean” set, we identified461\n523 sets of duplicate images, with some sets containing up to 6 duplicates. When selecting which images to keep462\namong the duplicates, we prioritized keeping those images with SkinCon annotations. After this filtering, we had a463\ntotal of 4,386 images. Lastly, we excluded 62 images that were marked as ‘Do not consider this image’ ( i.e., images of464\nlow quality or considered not appropriate) in the SkinCon dataset. This led to a final dataset containing 4,324 images.465\nAdditionally, for melanoma prediction tasks, amongst the 113 fine-grained diagnosis labels, we further refined the466\ndata to include only melanomas and melanoma look-alikes, such that the data mirrors a well-defined clinical task. In467\nline with the disease filtering criteria outlined by Degrave et al. [43], we included melanomas, benign melanocytic468\nlesions, seborrheic keratoses, and dermatofibromas, resulting in a total of 500 images.469\nDDI470\nAs a second set of clinical images with ground truth labels, we chose the Diverse Dermatology Images (DDI) dataset.471\nDDI contains 656 clinical images of diverse skin tones, obtained from Stanford Clinics [30], accompanied by anno-472\ntations of Fitzpatrick skin type and histopathologically proven diagnoses. Again, we excluded 20 images that were473\nmarked as ‘Do not consider this image’ in the SkinCon dataset, resulting in the final dataset of 636 images. For474\nmelanoma prediction tasks, we narrowed the dataset to include only melanomas and melanoma look-alikes, resulting475\nin a total of 275 images, in accordance with the approach taken by Degrave et al. [43]. Among the 78 fine-grained476\ndiagnosis labels in DDI, the melanoma category comprises the general label “melanoma” as well as the more spe-477\ncific labels acral-lentiginous melanoma, melanoma in situ , and nodular melanoma. Melanoma look-alikes consist of478\nacral melanotic macule, atypical spindle cell nevus of reed, benign keratosis, blue nevus, dermatofibroma, dysplastic479\nnevus, epidermal nevus, hyperpigmentation, keloid, inverted follicular keratosis, melanocytic nevi, nevus lipomatosus480\nsuperficialis, pigmented spindle cell nevus of reed, seborrheic keratosis, irritated seborrheic keratosis, and solar lentigo.481\nSkinCon482\nSkinCon is at present the most comprehensive dataset on dermatology concepts [1]. The dataset features 48 concepts,483\ncurated by board-certified dermatologists, that are frequently used to describe skin lesion attributes such as shape,484\nsize, color, and texture. The dermatologists manually annotated the ground-truth labels for these concepts on 3,230485\nimages from the Fitzpatrick17k dataset, which originally consisted of 16,577 images, and all 656 images from the DDI486\ndataset. Of the 4,324 images in the Fitzpatrick17k dataset we obtained after filtering, 1,009 had SkinCon annotations.487\nAmong the 500 images in the dataset used for the melanoma prediction task, 95 had annotations.488\nMONET489\nFormally, letfimage : Ximage → Rd be the MONET image encoder and ftext : Xtext → Rd be the MONET text encoder.490\nGiven a dataset of paired images I ∈ Ximage and text descriptions T ∈ Xtext, Dpaired = (I i, Ti)npaired\ni=1 , our goal is491\nto train the two encoders such that the distances between pairs of embeddings dist(fimage(Ii), ftext(Tj)) reflect the492\nsemantic similarity between Ii and Tj for all i, j≤ npaired.493\nArchitecture494\nWe use the vision transformer architecture, ViT-L/14, as our image encoder [56]. This encoder takes an input image495\nof size 224 x 224 and outputs a 768-dimensional embedding. For the text encoder, we use a transformer architecture496\nwith 12 self-attention layers. It takes tokenized text with a maximum limit of 77 tokens as input and outputs a 768-497\ndimensional embedding. We use the same architecture as CLIP [20] to take advantage of the weights from pre-trained498\nmodels.499\n17\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nPreprocessing500\nTo meet the input requirements for the encoder architectures, we process image and text inputs as follows. Each input501\nimage is re-sized and center-cropped to be 224x224 dimensions. It is then normalized using the mean and standard502\ndeviation used in CLIP [20]. Throughout the training phase, we applied standard data augmentation steps instead,503\nsuch as random resized crops, vertical flips, horizontal flips, and color jittering for brightness, contrast, and saturation.504\nFor each input text, we apply tokenization using lower-cased byte pair encoding [57]. In cases the text encountered505\nduring training was longer than the text encoder’s maximum token limit of 77, we split the text into sentences and506\nchose half of them from the beginning. We repeated this process until the token count was reduced to fewer than 77.507\nTraining508\nWe use cosine similarity as the distance metric. Both encoders are jointly trained to maximize the cosine similarity509\nbetween the image and text embeddings of the correct pairs while minimizing the cosine similarity between the510\nembeddings of incorrect pairings. To this end, we use a symmetric cross-entropy loss on cosine similarity scores; after511\ncalculating the cosine similarities between embeddings, we scale them by a temperature parameter λ and normalized512\nthem into a probability distribution with the softmax function. The temperature parameter λ was also updated during513\ntraining. We optimize the loss using the Adam optimizer [58] with a cosine learning rate schedule for 10 epochs. This514\nimplementation detail follows that of CLIP [20].515\nFor hyper-parameter tuning, we split the dataset into training and validation sets and find hyper-parameters that516\nresult in the best validation loss; we use validation loss for hyper-parameter tuning because there is no large-scale517\nground truth label for evaluating concept generation performance. We tune the hyper-parameter of batch size (128,518\n256, 512, 1024) and learning rate (1e-3, 1e-4, 1e-5, 1e-6). We find that the larger batch size results in lower validation519\nloss until a batch size of 512 is reached. We also find that the learning rate of 1e-5 leads to the lowest validation loss.520\nUsing the tuned hyper-parameters, we train the model on the whole dataset for 10 epochs. We use 6 Nvidia A40521\nGPUs with data parallelism. Model training takes 1 hour and 40 minutes.522\nAutomatic concept generation523\nDuring the training procedure of image and text encoders, an image and a text from the same pair are forced to be524\nclose to each other in the embedding space, while ones from different pairs are forced to be far apart. After training,525\nMONET can measure the proximity between an image and any arbitrary text. We use this capability to automatically526\ngenerate concepts for images.527\nTo generate a concept c for a given batch of N images I1, I2, ··· , IN , we first compute the image embeddings528\nfimage(I1), fimage(I2), ··· , fimage(IN ) using the image encoder fimage. We also compute the concept prompt embedding529\nftext(Tc) and reference prompt embedding ftext(Tr) using the text encoder, where Tc is a concept prompt (e.g., “This530\nis a skin image of {}”) and Tr is a reference prompt (e.g., “This is a skin image of”). Supplementary Table 4 shows the531\nterms used for each concept for filling templates. Next, we calculate the cosine similarity between image embeddings532\nand prompt embeddings. When multiple terms are used for each concept, we calculate the cosine similarity for each533\nterm and average them. Finally, we obtain concept presence score pi,c that represents the degree to which a concept534\nis present in the image as follows:535\npi,c = exp(simcos(Ii, Tc)/λ)\nexp(simcos(Ii, Tc)/λ) + exp(simcos(Ii, Tr))/λ) (1)\nwhere sim cos(·) is the cosine similarity score between image embeddings and text embeddings, sim cos(Ii, Tc) =536\nfimage(Ii)T ftext(Tc)\n|ftext(Ii)||ftext(Tc)| , and λ is the temperature parameter learned during the training. We normalize by reference prompt537\nto remove the effect of templates being used. Further, we use multiple templates to minimalize the effects of templates.538\nFor clinical images, we used templates: “This is skin image of {}”, “This is dermatology image of {}”, and “This is539\nimage of {}”. For dermoscopic images, we used the templates “This is dermatoscopy of {}” and “This is dermoscopy540\nof {}”. We use concept presence scores averaged across different templates in the end.541\nQuantitative evaluation542\nWe use 1,645 images with SkinCon labels from Fitzpatrick17k and DDI datasets for the task of predicting SkinCon543\nconcepts. We use 4,324 images from Fitzpatrick17k and 636 images from DDI datasets for the task of predicting disease544\nlabels, respectively. We compare the performance of MONET’s concept generation to that of a supervised learning545\napproach, training a ResNet-50 model [28], and to that of a pre-existing image-text model CLIP [20]. MONET and546\nCLIP do not require additional training to perform these tasks; the output from MONET and CLIP models is obtained547\n18\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nvia the automatic concept generation procedure described above. In contrast, we need to train a supervised learning548\nmodel. We train the model using a standard training recipe as follows. We initialize the model using ImageNet pre-549\ntrained weights. We then replace the last layer of the model with a new MLP layer that matches the dimension of the550\nprediction target; for SkinCon concepts, we train each concept one by one (the dimension is 1), and for disease labels,551\nwe train disease labels considered all at once (the dimension is 113 for Fitzpatrick17k and 78 for DDI). We finally552\ntrain the model using cross-entropy loss for 20 epochs. We use the Adam optimizer [58] with a ReduceLROnPlateau553\nlearning rate scheduler implemented in Pytorch (ver. 1.13.0); the initial learning rate is 1e-3, and it is reduced based554\non validation loss with the patience parameter of 2. Also, we use EarlyStopper implemented in PyTorch, which stops555\nthe training when the validation loss does not improve 5 times. The available data for each task is split into train/test556\nsets with a ratio of 4:1, and 20% of the train set is left for calculating validation loss. While for MONET and CLIP,557\nwe calculate AUROC across all available samples in one go, for the ResNet-50 model, we repeat the evaluation with558\n20 different train-test splits and calculate the average AUROC for each target to leverage all samples fully.559\nData auditing560\nConcept differential analysis561\nMONET’s ability to map images and texts onto the co-embedding space enables us to describe the different char-562\nacteristics between two sets of images in natural language. Assume we have two sets of images, denoted as I+ =563\n{I1, I2, ··· , IN+ } and I− = {I1, I2, ··· , IN−}, and a list of concepts we want to investigate [ c1, c2, ..., cNc]. We564\nfirst obtain the prototype embedding of each image set by computing an average of normalized image embeddings,565\nm+ = P\nIi∈{I+}\nfimage(Ii)\n∥fimage(Ii)∥ and m− = P\nIi∈{I−}\nfimage(Ii)\n∥fimage(Ii)∥. Then, we calculate the displacement vector from m−566\nto m+ by subtracting out the two prototype embedding m∆ = m+ − m−. Finally, we get a differential concept567\nexpression score by computing the dot product between the prototype and normalized embeddings of concept prompt568\nC∆,i = mT\n∆ · ftext(Ti)\n∥ftext(Ti)∥. This score measures how much more each concept is differentially expressed in S+ than in S−.569\nA similar technique for converting a set of images to text has been previously used by Eyuboglu et al. [13].570\nBenchmark analysis571\nTo perform a benchmark study on concept differential analysis, we construct synthetic data using ground-truth concept572\nlabels in the SkinCon dataset. For each concept in SkinCon, we create a dataset split into two groups: one with 100573\nimages, many of which are associated with the concept, and another with 100 images, many of which are not associated574\nwith the concept. We use the noise level parameter to control the degree to which the concept is correlated with the575\ngrouping; it indicates the probability that images are randomly sampled from the opposite group. We run simulations576\n20 times for each combination of parameters with different random seeds.577\nModel auditing578\nModel auditing with MONET579\nWe can use MONET to automatically detect semantically meaningful medical concepts that lead to model errors.580\nModel auditing with MONET (MA-MONET) starts by sorting images from a test set into groups based on their581\nvisual similarity. To this end, we run the K-means clustering algorithm implemented in the scikit-learn Python582\npackage (ver. 1.2.2) [54, 59]. We use 50 principal components of the embedding of the penultimate layer of the583\nEfficientNetV2-S model (pre-trained on ImageNet) [53] as clustering features. Next, we calculate the accuracy across584\nall samples and also per cluster; for thresholding the probability output from the trained classifier, we choose an585\noperating point that maximizes the F1 score. Following this, we identify medical concepts for the “low-performing”586\ncluster; we define low-performing clusters as ones with accuracy lower than overall accuracy. Each low-performing587\ncluster is compared to a high-performing counterpart containing similar images to understand what differentiates588\nthem; among the clusters that perform better than the overall accuracy, we choose one whose centroid is closest in589\nEuclidean distance to the low-performing cluster. We conduct a concept differential analysis between the high and590\nlow-performing clusters to pinpoint concepts that are more presented in the low-performing cluster. If two visually591\nsimilar clusters (one high-performing, the other low-performing) differ in terms of a few concepts, these differing592\nconcept terms can be hypothesized as leading to high error rates. We then filter out concepts with a concept presence593\nscore below 0.5 in the low-performing. Finally, we obtain a ranked list of medical concepts identified by MONET that594\ndifferentiate the two clusters.595\n19\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nBenckmark analysis596\nFor benchmarking analysis, we use a situation where the ground truth ( i.e., the concepts leading to model error) is597\nalready known. We create a training dataset with spurious correlations from the Fitzpatrick17k and DDI datasets:598\n500 malignant images that feature a particular SkinCon concept, while the 500 benign images do not. For the test set,599\nwe reverse the correlation; 500 sampled benign images have the SkinCon concept, while 500 sampled malignant images600\ndo not. For concepts of spurious correlation, we use 5 concepts that remain after filtering out concepts with fewer601\nthan 30 samples in each category required for creating the confounded training and test sets ( i.e., malignant–with602\nconcept, malignant–without concept, benign–with concept, and benign–without concept): “crust”, “hyperpigmenta-603\ntion”, “plaque”, “erythema”, and “papule”. For each of the 5 selected concepts, we repeat this analysis 20 times604\nwith different random seeds varying the training and test sets. Consequently, we conduct analysis for a total of 100605\nsettings. In addition to the concept we intentionally introduce as a confounder, there are other concepts that also606\ninadvertently become confounders. For example, when we had “papule” to be associated with malignancy in the train607\nset, “plaque” was associated with benign images in the training set. In such cases, we define all of them as the ground608\ntruth. On average, there are two concepts across all 100 test settings. We consider the spurious correlations are609\nrecovered if the top-N concepts identified by MA-MONET include at least one of these ground truth concepts across610\nall low-performing clusters.611\nBuilding inherently interpretable neural network612\nConcept Bottleneck Model613\nConcept Bottleneck Models (CBMs) [15] are inherently interpretable models that identify the importance of each con-614\ncept for the classifier’s prediction. They use a bottleneck layer to extract compact and discriminative representations615\nof the input data. The bottleneck layer, typically composed of a small number of units, imposes a constraint on616\nthe amount of information transmittable through the network, forcing it to make predictions by using interpretable617\nfeatures that align well with the users’ expectations. This technique can be used to reduce the dimensionality of the618\ndata and improve the efficiency of the model while preserving its predictive power. CBMs have been successfully619\napplied to a wide range of tasks, including image and video classification [60, 61], natural language processing [62],620\nand being applied in different medical settings [1, 63, 64]. However, a caveat is that these models need a large set of621\nconcept annotations to perform well, and collecting these labels is laborious and time intensive.622\nMONET lets us automatically generate concepts for images that can be used to scale to a large concept dataset623\nwith an arbitrary number of concepts. Specifically, MONET helps to create the bottleneck layer, denoted by bc :624\nXimage → RNc, that maps an input image Ii to a vector of dimension Nc, the number of concepts, where each625\ndimension corresponds to one of the Nc concepts. An interpretable linear model is then trained on the prediction626\ntarget to get importance scores for each concept corresponding to the trained model weights.627\nTo create the bottleneck layer, we start with a concept list [ c1, c2, ..., cNc], chosen with guidance from our derma-628\ntologist collaborators, containing concepts that are predictive of the target. Ideally, the bottleneck layer is binarized629\nusing the concept labels. However, we lack access to the concept annotations, and thresholding the similarity score of630\neach concept with the input image is non-trivial. Instead, for each concept cj, we curate a set of reference concepts631\n[rj1, rj2, ..., rjNj ] where Nj is the number of reference concepts for concept cj. Each reference concept is selected632\nsuch that it is sufficiently far from the concept of interest in the representation space while being closer to the other633\nreference concepts. We do this by choosing antonyms of the concept of interest as the reference concepts.634\nOnce the set of reference concepts is created, MONET calculates the similarity scores of the input image to the635\nconcept of interest and the corresponding reference concepts. The former is then normalized by taking a softmax636\nwith the reference concept scores. The resulting normalized score, p′\ni,cj , is then used in the bottleneck node for that637\nconcept, as shown in Equation 2.638\np′\ni,cj = exp(simcos(Ii, cj)/λ)\nexp(simcos(Ii, cj)/λ) + P\nk exp(simcos(Ii, rjk)/λ) (2)\nwhere sim cos(·) is the cosine similarity score obtained using MONET, and λ is a temperature parameter used to639\nmagnify the differences in similarity scores. λ is manually tuned to the value that performs the best on the train640\nset. Once the bottleneck layer is created, we train a simple linear classifier on the prediction target using stochastic641\ngradient descent. Specifically, for a classifier f and a given sample x ∈ RNc, the prediction obtained is wT f(x)+ b. We642\napply L1 regularization to favor sparsity in the trained model weights and make the model more interpretable. Once643\nthe linear classifier is trained, the learned weights w can be analyzed to understand the importance of each concept644\nfor the prediction target.645\n20\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nTo demonstrate the efficacy of MONET, we use two different prediction targets: (1) Melanoma vs. Melanoma look-646\nalike, and (2) Malignant vs. Benign. We differentiate between these two targets since all melanomas are malignant,647\nbut not all malignant lesions are melanoma. For this experiment, we use the clean Fitzpatrick 17k[52] and DDI[30]648\ndatasets. We use 80% of the data for training and reserving the rest for testing. To create the bottleneck layer, we use649\n11 concepts that are known to be correlated to the prediction targets; specifically, we use the ABCDEs of melanoma650\n[46] as a guideline to compile the list of concepts for the bottleneck layer. Supplementary Table 5 lists these concepts651\nalong with the reference concepts used for normalization. MONET’s ability to automatically generate concepts for652\nimages lets us easily add more data or concepts as needed without any manual annotations.653\nWe compare MONET+CBM to several other baseline methods of obtaining target predictions from input images:654\n• Vanilla CLIP+CBM: We use an out-of-the-box CLIP model to create the bottleneck layer and trains a linear655\nclassifier, similar to MONET+CBM. The only difference is that the vanilla CLIP model is not fine-tuned on656\ndermatology images and thus lacks the context of the setting in which we run the experiment; as a result, it657\ncannot adequately capture the semantic differences between technical dermatology terms.658\n• Supervised: We train a deep learning model using the standard fully supervised approach without incorporating659\nconcepts. We use ResNet-50 pre-trained on the ImageNet where the last classification head was replaced to match660\nthe dimension of the prediction target. We train the model end-to-end to classify the input images into the target661\nclasses. The implementation details are the same as described in the Qualitative evaluation subsection under662\nAutomatic concept generation. We only change the maximum training epoch from 20 to 50663\n• Linear Probing We use the representation of the penultimate layer of ResNet-50 pre-trained on ImageNet664\nas the input for a linear model. The difference with supervised is that during the training, the backbone of665\nResNet-50 is frozen.666\n• Manual Labeling We use the SkinCon dataset [1], which applies concept annotations covering 48 concepts for667\n3230 images from the Fitzpatrick 17k dataset to create the bottleneck layer. These concepts were chosen by two668\nboard-certified dermatologists considering the clinical descriptor terms used to describe skin lesions.669\nData availability670\nPMC Open Access Subset is publicly available from https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ .671\nEvaluation datasets are all publicly available and can be accessed from: ISIC ( https://challenge.isic-archive.c672\nom/data/), Fitzpatrick17k (https://github.com/mattgroh/fitzpatrick17k), and DDI(https://stanfordaimi.a673\nzurewebsites.net/datasets/35866158-8196-48d8-87bf-50dca81df965).674\nCode availability675\nThe code used in our analysis is available at https://github.com/suinleelab/MONET. It includes various scripts for676\ndata collection and preprocessing, training the MONET model, and conducting benchmark studies. Also, it provides677\nthe MONET model weights.678\nAcknowledgements679\nWe thank Chris Lin and other people in the Lee Lab for helpful discussions.680\nFunding681\nC.K., S.U.G., A.J.D., and S.-I.L. were supported by the National Science Foundation (CAREER DBI-1552309 and682\nDBI-1759487) and the National Institutes of Health (R35 GM 128638 and R01 AG061132). C.K. was supported by683\nthe Asan Foundation Biomedical Science Scholarship. R.D. was supported by the National Institutes of Health (5T32684\nAR007422-38) and the Stanford Catalyst Program.685\n21\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nEthics declarations686\nCompeting interests687\nR.D. reports fees from L’Oreal, Frazier Healthcare Partners, Pfizer, DWA, and VisualDx for consulting; stock options688\nfrom MDAcne and Revea for advisory board; and research funding from UCB.689\nReferences690\n1. Daneshjou, R., Yuksekgonul, M., Cai, Z. R., Novoa, R. & Zou, J. Y. SkinCon: A skin disease dataset densely an-691\nnotated by domain experts for fine-grained debugging and analysis in Advances in Neural Information Processing692\nSystems (eds Koyejo, S. et al.) 35 (Curran Associates, Inc., 2022), 18157–18167.693\n2. Goel, K., Gu, A., Li, Y. & R´ e, C.Model Patching: Closing the Subgroup Performance Gap with Data Augmentation694\nin 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021695\n(OpenReview.net, 2021).696\n3. Sagawa*, S., Koh*, P. W., Hashimoto, T. B. & Liang, P. Distributionally Robust Neural Networks in International697\nConference on Learning Representations (2020).698\n4. Rajpurkar, P. et al. MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs May 22,699\n2018. arXiv: 1712.06957[physics].700\n5. Oakden-Rayner, L., Dunnmon, J., Carneiro, G. & Re, C. Hidden stratification causes clinically meaningful failures701\nin machine learning for medical imaging in Proceedings of the ACM Conference on Health, Inference, and Learning702\nACM CHIL ’20: ACM Conference on Health, Inference, and Learning (ACM, Toronto Ontario Canada, Apr. 2,703\n2020), 151–159. isbn: 978-1-4503-7046-2.704\n6. DeGrave, A. J., Janizek, J. D. & Lee, S.-I. AI for radiographic COVID-19 detection selects shortcuts over signal.705\nNature Machine Intelligence 3. Number: 7 Publisher: Nature Publishing Group, 610–619. issn: 2522-5839 (July706\n2021).707\n7. Pianykh, O. S. et al. Continuous Learning AI in Radiology: Implementation Principles and Early Applications.708\nRadiology 297. PMID: 32840473, 6–14. eprint: https://doi.org/10.1148/radiol.2020200038 (2020).709\n8. Feng, J. et al. Clinical artificial intelligence quality improvement: towards continual monitoring and updating of710\nAI algorithms in healthcare. npj Digital Medicine 5, 66. issn: 2398-6352 (May 2022).711\n9. Vokinger, K. N., Feuerriegel, S. & Kesselheim, A. S. Continual learning in medical devices: FDA’s action plan712\nand beyond. The Lancet Digital Health 3. Publisher: Elsevier, e337–e338. issn: 2589-7500 (June 1, 2021).713\n10. Kim, B. et al. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors714\n(TCAV) in Proceedings of the 35th International Conference on Machine Learning International Conference on715\nMachine Learning. ISSN: 2640-3498 (PMLR, July 3, 2018), 2668–2677.716\n11. Crabb´ e, J. & van der Schaar, M. Concept Activation Regions: A Generalized Framework For Concept-Based717\nExplanations in NeurIPS (2022).718\n12. Abid, A., Yuksekgonul, M. & Zou, J. Meaningfully debugging model mistakes using conceptual counterfactual719\nexplanations in Proceedings of the 39th International Conference on Machine Learning International Conference720\non Machine Learning. ISSN: 2640-3498 (PMLR, June 28, 2022), 66–88.721\n13. Eyuboglu, S. et al. Domino: Discovering Systematic Errors with Cross-Modal Embeddings in The Tenth Interna-722\ntional Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 (OpenReview.net,723\n2022).724\n14. Chung, Y., Kraska, T., Polyzotis, N., Tae, K. & Whang, S. Automated Data Slicing for Model Validation: A Big725\nData - AI Integration Approach. IEEE Transactions on Knowledge and Data Engineering 32, 2284–2296. issn:726\n1558-2191 (Dec. 2020).727\n15. Koh, P. W. et al. Concept Bottleneck Models in Proceedings of the 37th International Conference on Machine728\nLearning International Conference on Machine Learning. ISSN: 2640-3498 (PMLR, Nov. 21, 2020), 5338–5348.729\n16. Post-hoc Concept Bottleneck Models in The Eleventh International Conference on Learning Representations,730\nICLR 2023, Rwanda, May 1-5, 2023 (2023).731\n17. Mendon¸ ca, T., Ferreira, P. M., Marques, J. S., Marcal, A. R. & Rozeira, J. PH 2-A dermoscopic image database732\nfor research and benchmarking in 2013 35th annual international conference of the IEEE engineering in medicine733\nand biology society (EMBC) (2013), 5437–5440.734\n22\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \n18. Kawahara, J., Daneshvar, S., Argenziano, G. & Hamarneh, G. Seven-point checklist and skin lesion classification735\nusing multitask multimodal neural nets. IEEE journal of biomedical and health informatics (2018).736\n19. Nevitt, M., Felson, D. & Lester, G. The osteoarthritis initiative. Protocol for the cohort study 1 (2006).737\n20. Radford, A. et al. Learning Transferable Visual Models From Natural Language Supervision in Proceedings of738\nthe 38th International Conference on Machine Learning International Conference on Machine Learning. ISSN:739\n2640-3498 (PMLR, July 1, 2021), 8748–8763.740\n21. Gutman, D. et al. Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium741\non Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC) May 4, 2016.742\narXiv: 1605.01397[cs].743\n22. Codella, N. C. F. et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 International744\nsymposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC) in 2018745\nIEEE 15th International Symposium on Biomedical Imaging (ISBI 2018) 2018 IEEE 15th International Sympo-746\nsium on Biomedical Imaging (ISBI 2018). ISSN: 1945-8452 (Apr. 2018), 168–172.747\n23. Codella, N. et al. Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the Interna-748\ntional Skin Imaging Collaboration (ISIC) Mar. 29, 2019. arXiv: 1902.03368[cs].749\n24. Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic750\nimages of common pigmented skin lesions. Scientific Data 5. Number: 1 Publisher: Nature Publishing Group,751\n180161. issn: 2052-4463 (Aug. 14, 2018).752\n25. Combalia, M. et al. BCN20000: Dermoscopic Lesions in the Wild Aug. 30, 2019. arXiv: 1908.02288[cs,eess].753\n26. Rotemberg, V. et al. A patient-centric dataset of images and metadata for identifying melanomas using clinical754\ncontext. Scientific Data 8. Number: 1 Publisher: Nature Publishing Group, 34. issn: 2052-4463 (Jan. 28, 2021).755\n27. Jones, O. T. et al. Artificial intelligence and machine learning algorithms for early detection of skin cancer in756\ncommunity and primary care settings: a systematic review. The Lancet Digital Health 4. Publisher: Elsevier,757\ne466–e476. issn: 2589-7500 (June 1, 2022).758\n28. He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition in 2016 IEEE Conference759\non Computer Vision and Pattern Recognition (CVPR) 2016 IEEE Conference on Computer Vision and Pattern760\nRecognition (CVPR) (IEEE, Las Vegas, NV, USA, June 2016), 770–778. isbn: 978-1-4673-8851-1.761\n29. Tiu, E. et al. Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised762\nlearning. Nature Biomedical Engineering. Publisher: Nature Publishing Group, 1–8. issn: 2157-846X (Sept. 15,763\n2022).764\n30. Daneshjou, R. et al. Disparities in dermatology AI performance on a diverse, curated clinical image set. Science765\nAdvances 8, eabq6147 (2022).766\n31. Janizek, J. D., Erion, G., DeGrave, A. J. & Lee, S.-I. An Adversarial Approach for the Robust Classification of767\nPneumonia from Chest Radiographs in Proceedings of the ACM Conference on Health, Inference, and Learning768\n(Association for Computing Machinery, Toronto, Ontario, Canada, 2020), 69–79. isbn: 9781450370462.769\n32. Bissoto, A., Fornaciali, M., Valle, E. & Avila, S. (De) Constructing Bias on Skin Lesion Datasets in 2019770\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 2019 IEEE/CVF771\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW) (IEEE, Long Beach, CA, USA,772\nJune 2019), 2766–2774. isbn: 978-1-72812-506-0.773\n33. Cassidy, B., Kendrick, C., Brodzicki, A., Jaworek-Korjakowska, J. & Yap, M. H. Analysis of the ISIC image774\ndatasets: Usage, benchmarks and recommendations. Medical Image Analysis 75, 102305. issn: 1361-8415 (Jan. 1,775\n2022).776\n34. Winkler, J. K. et al. Association Between Surgical Skin Markings in Dermoscopic Images and Diagnostic Perfor-777\nmance of a Deep Learning Convolutional Neural Network for Melanoma Recognition. JAMA Dermatology 155,778\n1135–1141. issn: 2168-6068 (Oct. 1, 2019).779\n35. Navarrete-Dechent, C., Liopyris, K. & Marchetti, M. A. Multiclass Artificial Intelligence in Dermatology: Progress780\nbut Still Room for Improvement. Journal of Investigative Dermatology 141, 1325–1328. issn: 0022-202X (2021).781\n36. Singh, C., Balakrishnan, G. & Perona, P. Matched sample selection with GANs for mitigating attribute confound-782\ning Mar. 24, 2021. arXiv: 2103.13455[cs,stat].783\n37. Leming, M., Das, S. & Im, H. Construction of a confounder-free clinical MRI dataset in the Mass General Brigham784\nsystem for classification of Alzheimer’s disease. Artificial Intelligence in Medicine 129, 102309. issn: 0933-3657785\n(July 1, 2022).786\n23\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \n38. Zhao, Q., Adeli, E. & Pohl, K. M. Training confounder-free deep learning models for medical applications. Nature787\nCommunications 11. Number: 1 Publisher: Nature Publishing Group, 6010. issn: 2041-1723 (Nov. 26, 2020).788\n39. Zhu, J., Park, T., Isola, P. & Efros, A. A. Unpaired Image-to-Image Translation Using Cycle-Consistent Ad-789\nversarial Networks in IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October790\n22-29, 2017 (IEEE Computer Society, 2017), 2242–2251.791\n40. Lundberg, S. M. & Lee, S.-I. A Unified Approach to Interpreting Model Predictions in Proceedings of the 31st792\nInternational Conference on Neural Information Processing Systems (Curran Associates Inc., Long Beach, Cali-793\nfornia, USA, 2017), 4768–4777. isbn: 9781510860964.794\n41. Sundararajan, M., Taly, A. & Yan, Q. Axiomatic Attribution for Deep Networks in Proceedings of the 34th795\nInternational Conference on Machine Learning - Volume 70 (JMLR.org, Sydney, NSW, Australia, 2017), 3319–796\n3328.797\n42. Selvaraju, R. R. et al. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization in798\n2017 IEEE International Conference on Computer Vision (ICCV) (2017), 618–626.799\n43. DeGrave, A. J., Cai, Z. R., Janizek, J. D., Daneshjou, R. & Lee, S.-I. Dissection of medical AI reasoning processes800\nvia physician and generative-AI collaboration. medRxiv. eprint: https://www.medrxiv.org/content/early/2801\n023/05/16/2023.05.12.23289878.full.pdf (2023).802\n44. Han, S. S. et al. The degradation of performance of a state-of-the-art skin image classifier when applied to803\npatient-driven internet search. Scientific Reports 12. Number: 1 Publisher: Nature Publishing Group, 16260.804\nissn: 2045-2322 (Sept. 28, 2022).805\n45. Navarrete-Dechent, C. et al. Automated Dermatological Diagnosis: Hype or Reality? Journal of Investigative806\nDermatology 138. Publisher: Elsevier, 2277–2279. issn: 0022-202X, 1523-1747 (Oct. 1, 2018).807\n46. Rigel, D. S., Friedman, R. J., Kopf, A. W. & Polsky, D. ABCDE—an evolving concept in the early detection of808\nmelanoma. Archives of dermatology 141, 1032–1034 (2005).809\n47. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. & Zou, J. Leveraging medical Twitter to build a vi-810\nsual–language foundation model for pathology AI Pages: 2023.03.29.534834 Section: New Results. Apr. 1, 2023.811\n48. Combalia, M. et al. Validation of artificial intelligence prediction models for skin cancer diagnosis using der-812\nmoscopy images: the 2019 International Skin Imaging Collaboration Grand Challenge. The Lancet Digital Health813\n4. Publisher: Elsevier, e330–e339. issn: 2589-7500 (May 1, 2022).814\n49. Daneshjou, R., Smith, M. P., Sun, M. D., Rotemberg, V. & Zou, J. Lack of Transparency and Potential Bias in815\nArtificial Intelligence Data Sets and Algorithms: A Scoping Review. JAMA Dermatology 157, 1362–1369. issn:816\n2168-6068 (Nov. 1, 2021).817\n50. National Library of Medicine. PMC Open Access Subset https://www.ncbi.nlm.nih.gov/pmc/tools/openft818\nlist/ (2022).819\n51. Gamper, J. & Rajpoot, N. M. Multiple Instance Captioning: Learning Representations From Histopathology820\nTextbooks and Articles in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,821\nJune 19-25, 2021 (Computer Vision Foundation / IEEE, 2021), 16549–16559.822\n52. Groh, M. et al. Evaluating Deep Neural Networks Trained on Clinical Images in Dermatology with the Fitz-823\npatrick 17k Dataset in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops824\n(CVPRW) 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)825\n(IEEE, Nashville, TN, USA, June 2021), 1820–1828. isbn: 978-1-66544-899-4.826\n53. Tan, M. & Le, Q. V. EfficientNetV2: Smaller Models and Faster Training in Proceedings of the 38th International827\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (eds Meila, M. & Zhang, T.) 139828\n(PMLR, 2021), 10096–10106.829\n54. Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12, 2825–830\n2830 (2011).831\n55. Huang, G., Liu, Z., Maaten, L. V. D. & Weinberger, K. Q. Densely Connected Convolutional Networks in 2017832\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE Computer Society, Los Alamitos,833\nCA, USA, July 2017), 2261–2269.834\n56. Dosovitskiy, A. et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale in 9th In-835\nternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 (Open-836\nReview.net, 2021).837\n24\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \n57. Sennrich, R., Haddow, B. & Birch, A. Neural Machine Translation of Rare Words with Subword Units in Pro-838\nceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)839\n(Association for Computational Linguistics, Berlin, Germany, Aug. 2016), 1715–1725.840\n58. Kingma, D. P. & Ba, J. Adam: A Method for Stochastic Optimization in 3rd International Conference on Learning841\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (eds Bengio,842\nY. & LeCun, Y.) (2015).843\n59. Lloyd, S. Least squares quantization in PCM. IEEE Transactions on Information Theory 28, 129–137 (1982).844\n60. Lanchantin, J., Wang, T., Ordonez, V. & Qi, Y. General multi-label image classification with transformers in845\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021), 16478–16488.846\n61. Jeyakumar, J. V. et al. Automatic Concept Extraction for Concept Bottleneck-based Video Classification. arXiv847\npreprint arXiv:2206.10129 (2022).848\n62. Sun, X. et al. Interpreting deep learning models in natural language processing: A review. arXiv preprint849\narXiv:2110.10470 (2021).850\n63. Klimiene, U. et al. Multiview Concept Bottleneck Models Applied to Diagnosing Pediatric Appendicitis in 2nd851\nWorkshop on Interpretable Machine Learning in Healthcare (IMLH) (2022).852\n64. Wu, C., Parbhoo, S., Havasi, M. & Doshi-Velez, F. Learning Optimal Summaries of Clinical Time-series with853\nConcept Bottleneck Models in Machine Learning for Healthcare Conference (2022), 648–672.854\n25\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nErythema\nA.\nBulla\nB.\nXerosis\nC.\nPustule\nD.\nUlcer\nE.\nSupplementary Fig. 1 | Clinical images with high concept presence scores calculated using MONET.\nWe show the top 30 images for each concept. (A) Erythema. (B) Bulla. (C) Xerosis. (D) Pustule. (E) Ulcer.\n26\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nErythema\nA.\nBlue\nB.\nNodule\nC.\nUlcer\nD.\nWarty\nE.\nSupplementary Fig. 2 | Dermoscopic images with high concept presence scores calculated using\nMONET. We show the top 30 images for each concept. (A) Erythema. (B) Blue. (C) Nodule. (D) Ulcer.\n(E) Warty.\n27\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nPurple pen\nA.\nOrange sticker\nB.\nNail\nC.\nHair\nD.\nDermoscopic border\nE.\nSupplementary Fig. 3 | Dermoscopic images with artifacts as determined by high concept presence\nscores calculated using MONET. We show the top 30 images for each artifact. (A) Purple pen. (B) Orange\nsticker. (C) Nail. (D) Hair. (E) Dermoscopic border.\n28\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \n1 2 3 4 5\nTop-N\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy\nNoise Level\n0.0\n0.1\n0.2\n0.4\nSupplementary Fig. 4 | Accuracy of concept differential analysis. We perform a benchmark analysis to\nassess MONET’s ability to identify presented concepts correctly. To do this, we generate two paired datasets with\nknown ground truth (i.e., a specific concept is differentially presented) and conduct concept differential analysis on\nthese datasets, letting us determine how accurately the analysis recognizes the intended concept. This experiment is\nconducted on 21 out of 48 concepts from SkinCon that remained after excluding those with fewer than 30 positive\nexamples. For each concept, we sample a set of 100 images where a concept is highly presented and another set of\n100 images where a concept is highly absent from Fitzpatrick17k and DDI datasets, with replacement. Additionally,\nwe varied the noise parameters (0, 0.1, 0.2, and 0.4), which control how correlated the concept is to each grouping.\nFor example, noise = 0.1 means that in the “concept present” set, 90% of the images have the concept, while in the\n“concept absent” set, only 10% of the images have the concept. For each combination of settings (i.e., 21 intended\nconcepts and 4 noise levels), we repeat this evaluation 20 times with different random seeds. The error bars represent\nthe 95% confidence interval.\n29\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nA. Blue, Black, Gray, Pigmented, Flat topped\n True: 76 / 17  Pred: 11 / 82 \nB. Pigmented\n True: 88 / 36  Pred: 31 / 93 \nC. Ulcer, Comedo, Crust\n True: 91 / 39  Pred: 37 / 93 \nD. Blue, Black, Pigmented\n True: 59 / 22  Pred: 14 / 67 \nE. Ulcer, Atrophy, Dermoscopic border\n True: 167 / 51  Pred: 68 / 150 \nF. Erythema, Salmon, Sclerosis, Scar, Translucent\n True: 74 / 31  Pred: 21 / 84 \nG. Atrophy, Dermoscopic border\n True: 166 / 61  Pred: 64 / 163 \nH. Atrophy, Dermoscopic border\n True: 117 / 78  Pred: 30 / 165 \nI. Ulcer, Crust, Dermoscopic border, Dome-shaped, Atrophy\n True: 32 / 28  Pred: 19 / 41 \nJ. Translucent, Ulcer, Erythema, Telangiectasia, Red\n True: 89 / 31  Pred: 42 / 78 \nK. Dome-shaped, Atrophy, Dermoscopic border, Cyst\n True: 124 / 65  Pred: 45 / 144 \nL. Atrophy, Dermoscopic border, Dome-shaped\n True: 141 / 96  Pred: 47 / 190 \nM. Dermoscopic border\n True: 49 / 52  Pred: 13 / 88 \nN. Crust, Atrophy, Dermoscopic border\n True: 76 / 38  Pred: 44 / 70 \nO. Atrophy, Gray, Black, Flat topped, Dermoscopic border\nMaligant Benign   (Upper left: True, Lower right: Pred)\nTrue: 72 / 38  Pred: 30 / 80 \nTrue: # Malignant / # Benign  Pred: # Malignant / # Benign\nSupplementary Fig. 5 | Concept-level model auditing. We train a model on the Med U. of Vienna dataset and\ntest it on the Hosp. Barcelona dataset. Each row displays one of the top 15 clusters, sorted by high error rates. For\neach cluster, we show the misclassified images and the corresponding concepts associated with errors. We represent\nthe true and predicted labels for each image by the color of the upper left and lower right triangles in the small box,\nrespectively. The numbers at the top right compare the number of malignant and benign samples for the true and\npredicted labels. The 10 misclassified images shown for each are selected based on the average concept presence of the\nidentified concepts.\n30\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nA. Pinkish, Erythema, Gray, Red, Atrophy\n True: 9 / 197  Pred: 111 / 95 \nB. Gray, Pigmented, Hyperpigmentation\n True: 64 / 139  Pred: 128 / 75 \nC. Gray, Hyperpigmentation, Pigmented, Hypopigmentation\n True: 67 / 182  Pred: 145 / 104 \nD. Poikiloderma, Gray, Hypopigmentation, Fungating, Atrophy\n True: 123 / 168  Pred: 205 / 86 \nE. Gray, Pigmented, Hyperpigmentation, Atrophy\n True: 55 / 229  Pred: 167 / 117 \nF. Red, Black, Hyperpigmentation, Pigmented\n True: 12 / 140  Pred: 79 / 73 \nG. Black, Hyperpigmentation, Pigmented\n True: 38 / 134  Pred: 76 / 96 \nH. Hyperpigmentation, Pigmented, Red\n True: 15 / 173  Pred: 92 / 96 \nI. Gray, Pigmented, Hyperpigmentation\n True: 75 / 124  Pred: 128 / 71 \nJ. Gray, Pigmented, Hyperpigmentation, Atrophy\n True: 77 / 164  Pred: 152 / 89 \nK. Poikiloderma, Gray, Pigmented\n True: 31 / 89  Pred: 68 / 52 \nL. Gray, Black, Pigmented\n True: 41 / 90  Pred: 76 / 55 \nM. Erythema, Red, Atrophy\n True: 23 / 432  Pred: 196 / 259 \nN. Poikiloderma, Gray, Atrophy\n True: 103 / 171  Pred: 179 / 95 \nO. Hyperpigmentation\nMaligant Benign   (Upper left: True, Lower right: Pred)\nTrue: 25 / 164  Pred: 65 / 124 \nTrue: # Malignant / # Benign  Pred: # Malignant / # Benign\nSupplementary Fig. 6 | Concept-level model auditing. We train a model on the Hosp. Barcelona dataset and\ntest it on the Med U. of Vienna dataset. Each row displays one of the top 15 clusters, sorted by high error rates.\nFor each cluster, we show the misclassified images and the corresponding concepts associated with errors. The 10\nmisclassified images shown for each are selected based on the average concept presence of the identified concepts.\n31\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nConcept File name (Dataset)\nErythema 58b4bc079ca94e6e9377a42ca7564b40.jpg (Fitzpatrick17k)\n720cf31558966c82c118ab75b50632eb.jpg (Fitzpatrick17k)\n5f046cda32a3cc547205662e7be774f9.jpg (Fitzpatrick17k)\nUlcer d8bf377acc45a3beb0c6e81bf7ac1ff5.jpg (Fitzpatrick17k)\nSupplementary Table 1 | Images excluded from figures. We exclude 4 images inappropriate for public display\ndue to the inclusion of sensitive body parts, such as genitals, breasts, and buttocks, from Fig. 2 and Supplementary\nFig. 1. Their file names, as well as the dataset they belong to, are noted.\nMethod Mean AUROC\nLabels in Fitzpatrick17k Labels in DDI\nMONET 0.830 (52/59) 0.701 (4/6)\nCLIP 0.680 (28/59) 0.595 (0/6)\nFully supervised (ResNet-50) 0.856 (58/59) 0.700 (2/6)\nSupplementary Table 2| Performance of MONET in annotating disease labels as compared to baselines\nWe use disease labels in the clinical image datasets, Fitzpatrick17k and DDI datasets, as ground truth. We exclude\nany with less than 30 positive examples, leaving 59 labels in Fitzpatrick17k and 6 labels in DDI for our analysis.\nWe use 4,324 samples from Fitzpatrick17k and 636 samples from DDI. The baselines are CLIP, an image-text model\nnot specifically trained on dermatology images, and the ResNet-50 model trained on ground truth labels in a fully\nsupervised manner. The numbers in parentheses represent the count of concepts for which the method achieves an\nAUROC over 0.7 over the total number of diseases examined.\nFitzpatrick skin type Mean AUROC\nFST I–II 0.767 (17/21)\nFST III–IV 0.759 (18/21)\nFST V–VI 0.768 (16/21)\nSupplementary Table 3 | Evaluation of MONET’s concept generation performance per skin tone. We\ncalculate AUROC metrics per each Fitzpatrick skin type (FST) separately: FST I–II (light skin tone, n = 717), FST\nIII–IV (intermediate skin tone, n = 607), and FST V–VI (dark skin tone, n = 283). The numbers in parentheses\nrepresent the count of concepts for which the method achieves an AUROC over 0.7 over the total number of concepts\nexamined.\n32\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nConcept Terms\nabscess abscess, swollen, pus-filled lump\nacuminate acuminate\natrophy atrophic\nblack black color, black\nblue blue, blue color\nbrown(hyperpigmentation) hyperpigmented, hyperpigmentation, brown(hyperpigmentation)\nbulla bullae, blister\nburrow scabies, burrow\ncomedo whitehead, blackhead\ncrust dried crust, crust\ncyst cyst\ndome-shaped like dome\nerosion erosive, erosion, breakdown of the outer layers, impetigo\nerythema redness, erythematous\nexcoriation excoriation\nexophytic/fungating fungating\nexudate exudate\nfissure dry and cracked skin\nflat topped flat topped\nfriable friable\ngray gray\ninduration edema, oedema\nlichenification lichenification, thickened and leathery\nmacule freckle, macular, lentigo, macule\nnodule nodular, cyst, nodule\npapule papular\npatch hyperpigmented, melasma, vitiligo\npedunculated pedunculated\npigmented pigmented\nplaque plaque, dermatitis, psoriasis\npoikiloderma sun aging\npurple purple\npurpura/petechiae purpura\npustule pustule\nsalmon salmon patch\nscale flaky and scaly, scaly, hyperkeratosis\nscar scar, keloid scars, hypertrophic scars, contractures scars, acnescars scars\nsclerosis scleroderma, crest syndrome\ntelangiectasia dilated or broken blood vessels\ntranslucent translucent, this bump is translucent\nulcer ulcer, ulcerated\numbilicated umbilicated\nvesicle vesicle, fluid-containing\nwarty/papillomatous warty and papillomatous\nwheal urticaria\nwhite(hypopigmentation) white(hypopigmentation), hypopigmentation\nxerosis dry skin, abnormally dry skin, xerosis\nyellow yellow\npurple pen purple pen\nnail nail\npinkish pinkish\nred red\nhair hair\norange sticker orange sticker\ndermoscope border dermoscopy\nSupplementary Table 4 | Terms used to generate concept prompts\n33\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint \nConcept of Interest Reference Concepts\nAsymmetry Symmetry, Regular, Uniform\nIrregular Regular, Smooth\nBlue Green, Red\nWhite Black, Colored, Pigmented\nBrown Pale, White\nBlack White, Creamy, Colorless, Unpigmented\nErosion Deposition, Buildup\nMultiple Colors Single Color, Unicolor\nTiny Large, Big\nRegular Irregular\nSupplementary Table 5 | Concepts used in the bottleneck layer for the Concept Bottleneck Model\n34\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 12, 2023. ; https://doi.org/10.1101/2023.06.07.23291119doi: medRxiv preprint ",
  "topic": "Software deployment",
  "concepts": [
    {
      "name": "Software deployment",
      "score": 0.7901666760444641
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.7792590260505676
    },
    {
      "name": "Pipeline (software)",
      "score": 0.7002624273300171
    },
    {
      "name": "Trustworthiness",
      "score": 0.6770683526992798
    },
    {
      "name": "Computer science",
      "score": 0.66962730884552
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5801973342895508
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4395814836025238
    },
    {
      "name": "Data science",
      "score": 0.4197064936161041
    },
    {
      "name": "Information retrieval",
      "score": 0.39716482162475586
    },
    {
      "name": "Computer security",
      "score": 0.2120119035243988
    },
    {
      "name": "Software engineering",
      "score": 0.20592036843299866
    },
    {
      "name": "History",
      "score": 0.09344109892845154
    },
    {
      "name": "Programming language",
      "score": 0.08541882038116455
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210152177",
      "name": "Center for Clinical Research (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210137306",
      "name": "Stanford Medicine",
      "country": "US"
    }
  ],
  "cited_by": 11
}