{
    "title": "A Review of Automatic Item Generation Techniques Leveraging Large Language Models",
    "url": "https://openalex.org/W4394580931",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2121296513",
            "name": "Bin Tan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5095042856",
            "name": "Nour Armoush",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5092602297",
            "name": "Elisabetta Mazzullo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2120159399",
            "name": "Okan Bulut",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A652587890",
            "name": "Mark J. Gierl",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4388140488",
        "https://openalex.org/W4368276316",
        "https://openalex.org/W4200025781",
        "https://openalex.org/W4293147552",
        "https://openalex.org/W2182880692",
        "https://openalex.org/W2075950485",
        "https://openalex.org/W4286561208",
        "https://openalex.org/W4312097335",
        "https://openalex.org/W1967555382",
        "https://openalex.org/W4381956373",
        "https://openalex.org/W4295846739",
        "https://openalex.org/W4309232742",
        "https://openalex.org/W2906432682",
        "https://openalex.org/W4385573815",
        "https://openalex.org/W4321260681",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W7061537676",
        "https://openalex.org/W4289523162",
        "https://openalex.org/W4214509822",
        "https://openalex.org/W4361733316",
        "https://openalex.org/W4312863132",
        "https://openalex.org/W3112417698",
        "https://openalex.org/W4318480024",
        "https://openalex.org/W4226228060",
        "https://openalex.org/W2142413505",
        "https://openalex.org/W6625268989",
        "https://openalex.org/W2562551526",
        "https://openalex.org/W6793391678",
        "https://openalex.org/W4379781152",
        "https://openalex.org/W4302041232",
        "https://openalex.org/W4367844670",
        "https://openalex.org/W3129150706",
        "https://openalex.org/W7025181778",
        "https://openalex.org/W4381956360",
        "https://openalex.org/W4389010697",
        "https://openalex.org/W6962852072",
        "https://openalex.org/W4292549551",
        "https://openalex.org/W2907125152",
        "https://openalex.org/W3202810978",
        "https://openalex.org/W4285503625",
        "https://openalex.org/W4320151791",
        "https://openalex.org/W4220822134",
        "https://openalex.org/W2183581773",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3010246167",
        "https://openalex.org/W2908927666",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4282567490",
        "https://openalex.org/W4290973226",
        "https://openalex.org/W4317795002",
        "https://openalex.org/W3093858354",
        "https://openalex.org/W4388729351",
        "https://openalex.org/W4382246105",
        "https://openalex.org/W4310995245",
        "https://openalex.org/W2094719514",
        "https://openalex.org/W4294768397",
        "https://openalex.org/W4283314185",
        "https://openalex.org/W3092999391",
        "https://openalex.org/W4379806099",
        "https://openalex.org/W3033487789",
        "https://openalex.org/W4205497773",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4297412105",
        "https://openalex.org/W4297796135",
        "https://openalex.org/W4286639707",
        "https://openalex.org/W2233859745",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2999777802",
        "https://openalex.org/W4309955162",
        "https://openalex.org/W2985620815",
        "https://openalex.org/W4287123055",
        "https://openalex.org/W3083239953",
        "https://openalex.org/W4298084493",
        "https://openalex.org/W4313022557",
        "https://openalex.org/W3176011152",
        "https://openalex.org/W4323075761",
        "https://openalex.org/W4288059420",
        "https://openalex.org/W2145216917",
        "https://openalex.org/W4205528697",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W4361193274",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4295278406",
        "https://openalex.org/W2220688940",
        "https://openalex.org/W3101000947",
        "https://openalex.org/W4210850835",
        "https://openalex.org/W4292197005",
        "https://openalex.org/W4254429661",
        "https://openalex.org/W2989613245"
    ],
    "abstract": "Over a decade ago, automatic item generation (AIG) was introduced to meet the increasing need for high-quality items in educational measurement. Around the same time, a new area of research in computer science began to develop questions for educational use. Historically, researchers from these two domains had little knowledge or communication with one another. However, the development of pre-trained large language models (LLMs) has sparked the interest of researchers from both domains in applying these models for automatically creating items. With similar objectives and methodologies, these two research domains appear to be converging on how to address the problems in this field. The purpose of this study is to provide a review of the current state of research by synthesizing existing studies on the use of LLMs for AIG. By combining research from both domains, we examine the utility and potential of LLMs for AIG. We performed a comprehensive literature review in seven research databases, selected studies based on predefined criteria, and summarized 60 relevant studies that employed LLMs in the AIG process. We identified the most commonly used LLMs in current AIG literature, their specific applications in the AIG process, and the characteristics of the generated items. We found that LLMs are flexible and effective in generating various types of items based on different languages and subject domains. However, many studies have overlooked the quality of the generated items, indicating a lack of a solid educational foundation. This review emphasizes the urgent need for greater integration of learning and measurement theories in future AIG research. We share two suggestions to enhance the educational foundation for leveraging LLMs in AIG, advocating for interdisciplinary collaborations to exploit the utility and potential of LLMs.",
    "full_text": "1  \n  \nA Review of Automatic Item Generation Techniques Leveraging Large Language Models     Bin Tan Measurement, Evaluation, and Data Science University of Alberta btan4@ualberta.ca  Nour Armoush Measurement, Evaluation, and Data Science University of Alberta armoush@ualberta.ca  Elisabetta Mazzullo Measurement, Evaluation, and Data Science University of Alberta mazzullo@ualberta.ca  Okan Bulut Centre for Research in Applied Measurement and Evaluation University of Alberta bulut@ualberta.ca  Mark J. Gierl Centre for Research in Applied Measurement and Evaluation University of Alberta mgierl@ualberta.ca     \n2  \n  \nPractitioner Notes What is already known about this topic ● The traditional process of item development is challenged by the increasing need for high-quality items. Thus, automatic item generation has been proposed in the field of educational measurement as a solution to create large banks of high-quality items while reducing costs. ● Coincidentally, automatic question generation in the applied natural language processing field has been explored for educational purposes. However, historically, researchers from these two domains had little knowledge or communication with one another. They also tried to address similar problems using different terminologies, which hindered their advancement. ● The introduction of large language models has led to the convergence of research from these two domains, with both exploring innovative methods for automatically generating assessment items. What this paper adds ● This review aims to map the current literature by bringing together experiences from both research domains to explore the utility and potential of large language models for automatic item generation. ● We identify the most commonly used large language models and their specific uses in three stages of the automatic item generation process, reflecting their inherent features and strengths. The pre-generation stage focuses on preparing, processing, and understanding texts for subsequent item generation to ensure high quality. In the post-generation stage, large language models are primarily used to filter out low-quality items or determine the usefulness of the generated items. ● Our findings reveal that using large language models to generate items is an effective and flexible solution, with few constraints on item type, language, subject domain, or the data source used for training large language models to create items. Due to the exceptional language understanding abilities of large language models, the generated items are typically free from grammar errors and contextually relevant to the desired content domain. \n3  \n  \n● We note that there was a lack of solid educational foundation for many reviewed studies because they did not incorporate measurement and learning theories in the process of automatic item generation. Accordingly, many of their items were generated without deep consideration of their measurement purposes and item properties, which are essential to meet the requirements of educational assessment. ● We share suggestions for future research. The first suggestion is to clarify the assessment context and measurement goals for automatic item generation. The second suggestion for future research is to examine the measurement properties and pedagogical soundness of items. Implications for practice and/or policy ● Automatic item generation should not end with generating a large number of items but with ensuring that these items are of high quality and can fulfill their demands for educational purposes and contexts. ● Automatic item generation is still a developing and promising research domain, and its further development and application will depend on the integration of learning and measurement theories. ● Future involvement of human experts and interdisciplinary collaborations will help exploit the utility and potential of large language models for automatic item generation. ● Future research should aim for the production of items that are not only grammatically correct and contextually relevant but also reliable, valid, and pedagogically sound.           \n4  \n  \nAbstract Over a decade ago, automatic item generation (AIG) was introduced to meet the increasing need for high-quality items in educational measurement. Around the same time, a new area of research in computer science began to develop questions for educational use. Historically, researchers from these two domains had little knowledge or communication with one another. However, the development of pre-trained large language models (LLMs) has sparked the interest of researchers from both domains in applying these models for automatically creating items. With similar objectives and methodologies, these two research domains appear to be converging on how to address the problems in this field. The purpose of this study is to provide a review of the current state of research by synthesizing existing studies on the use of LLMs for AIG. By combining research from both domains, we examine the utility and potential of LLMs for AIG. We performed a comprehensive literature review in seven research databases, selected studies based on predefined criteria, and summarized 60 relevant studies that employed LLMs in the AIG process. We identified the most commonly used LLMs in current AIG literature, their specific applications in the AIG process, and the characteristics of the generated items. We found that LLMs are flexible and effective in generating various types of items based on different languages and subject domains. However, many studies have overlooked the quality of the generated items, indicating a lack of a solid educational foundation. This review emphasizes the urgent need for greater integration of learning and measurement theories in future AIG research. We share two suggestions to enhance the educational foundation for leveraging LLMs in AIG, advocating for interdisciplinary collaborations to exploit the utility and potential of LLMs. Keywords: Automatic item generation, automatic question generation, educational measurement, educational technology, large language models \n5  \n  \nA Review of Automatic Item Generation Techniques Leveraging Large Language Models Introduction Assessments are a foundational component of the modern education system, serving multiple purposes such as facilitating student learning, evaluating teaching outcomes, and informing educational policy (Black, 1998; Lee et al., 2020). Advancements in educational technology, such as e-learning platforms, adaptive testing, and formative assessments, have increased the demand for high-quality assessment items. However, traditional item development methods are struggling to keep pace with this demand. For example, the cost of developing a single item for high-stakes assessments can reach up to $2,000 (Rudner, 2010), while items for low-stakes assessments often suffer from low-quality content, failing to provide comprehensive or valuable feedback to students (Lim, 2019; Wylie & Lyon, 2015). In response to the growing demand for high-quality items, the field of educational measurement first turned its attention toward a solution called automatic item generation (AIG). The goal of AIG is to generate large banks of high-quality items while reducing overall costs (Gierl & Lai, 2012; Lai et al., 2009). The template-based method proposed by Gierl et al. (2012) is the most widely used method for AIG, which involves a three-step process that requires the construction of cognitive models and item models (Gierl & Lai, 2012, 2016). This method has been successfully implemented in various educational practices (e.g., medical licensing examinations), demonstrating its viability and cost-effectiveness in creating high-quality items (Kosh et al., 2018; Pugh et al., 2020). Coincidentally, there is a related line of research in the domain of computer science, specifically focusing on automatic question generation (AQG) for educational purposes. Originally rooted in natural language processing (NLP) research for developing chatbots and conversational agents, AQG for educational purposes has evolved to \n6  \n  \nshare a similar goal with AIG: the automatic generation of a large number of assessment items using computer algorithms. Despite this shared goal, there are differences in terminology between the two fields. For instance, the term “items” can be presented in diversified types to measure students' knowledge and skills, while “questions” typically have a much narrower format, usually ending with a question mark (Gierl et al., 2021). Despite this distinction, researchers of AQG for educational purposes often refer to the created items as questions. The work conducted in AQG research for educational purposes has been summarized in two recent review studies (Ch & Saha, 2018; Kurdi et al., 2020). Ironically, in both reviews, there were no keywords of “automatic item generation” or even word variants of “items” in the search terms used. As a result, many significant publications in the AIG literature were not included in their review. Furthermore, educational measurement researchers have been less familiar with AQG due to differences in research interests, technical skill sets, and limited participation in NLP conferences. This gap is evident in the minimal overlap between the references and citations used in the two research domains. For example, nearly all references presented in the review paper of Kurdi et al. (2020) were published in computer science venues, while the references used in a review paper of AIG (Falcao et al., 2022) were mostly in education, psychology, and testing journals. As a result, historically, researchers in both domains have had limited knowledge of each other’s work or were not even aware of each other’s existence. More importantly, the use of different terminology (e.g., “items” vs. “questions”) to describe and address similar problems has created communication barriers between the AIG and AQG research domains, hindering their advancement. Marked as recent breakthroughs in NLP research, pre-trained language models (PLMs) and large language models (LLMs) have been developed and introduced to the public. Due to \n7  \n  \ntheir flexibility and extraordinary performance in understanding the context of language and in various language processing tasks (Ackerman & Balyan, 2023), they have significantly sparked the interest of both educational measurement researchers and applied NLP researchers in employing them as an innovative methodology for automating the generation of a large number of items. LLMs are essentially scaled-up versions of PLMs, characterized by an increased number of parameters and larger training data sizes. However, there is no strict cutoff value that differentiates the number of parameters of PLMs from those of LLMs (Zhao et al., 2023). Therefore, in the remaining sections of this review, the term “LLMs” is used to refer to both PLMs and LLMs, as it is a more commonly recognized term. As both AIG and AQG research domains now aim to automate item/question generation and increasingly use similar methodologies involving LLMs, they are converging in many aspects, blurring the distinctions between them. Therefore, it is now crucial to raise awareness of both AIG and AQG research and map the literature of both fields to gain a comprehensive understanding of the utility and potential of LLMs. The purpose of this review is not only to identify what has been achieved in the literature regarding the use of LLMs for AIG but also to explore the knowledge gaps between the two research domains. To ensure consistent terminology, we will use “AIG” to refer to both AIG and AQG for educational purposes in this paper, as the term “item” is more inclusive than “question” (Gierl et al., 2021). However, it is important to note that these terms have distinct historical origins and development trends, but they are used interchangeably here due to their similar research objectives and the focus on leveraging LLMs in this review. To map the current state of research on leveraging LLMs for AIG, our review is guided by the following research questions (RQs): \n8  \n  \nRQ1: What are the most used LLMs for AIG, and how do their underlying architectures and features differ? RQ2: In what specific ways are LLMs most frequently used, and how do the LLMs’ features influence their use in the AIG process? RQ3: What types of items were generated in the reviewed studies in terms of item type, subject domain, and language? Can LLMs generate valid, reliable, and high-quality items? Methods Study Search and Selection  The study search and selection process is displayed in Figure 1. Keyword searches were conducted in seven research databases, which were selected for their comprehensive coverage of literature on AIG (Alsubait, 2015) or relevance to LLMs. The searches used two keyword components: AIG and LLMs, connected by an \"AND\" operator, as detailed in Table 1. Our search targeted full texts, with no restrictions on publication types in the search strategy, and was limited to publications after 2018, marking the emergence of pre-trained language models (Zhao et al., 2023). The last search was conducted on August 1st, 2023, which yielded 831 unique study records from these research databases. After the study search, two reviewers independently screened the titles, abstracts, and keywords of the 831 identified studies, using predefined inclusion criteria for selection. We only retained studies with full texts in English and focused exclusively on empirical studies, excluding editorials, commentaries, and position papers. Given that question generation has applications in other domains like chatbots and customer service, we confined our selection to studies in the education field. This focus was achieved by identifying education-related words or phrases in the title, abstracts, or keywords. Additionally, we selected publications that focused on item \n9  \n  \ngeneration, excluding other forms of text generation, such as the creation of reading passages without items generated. After the initial screening, we noted 47 disagreements in selection decisions, resulting in a high agreement rate of 93.44%. The disagreements primarily resulted from vague mentions of education-related phrases in abstracts from papers published in computer science venues. These disagreements were resolved through further discussion and consultation of the full texts of the studies, resulting in the selection of 65 studies for full-text review and information extraction. After a detailed review, 5 studies were excluded based on our criteria, leaving 60 studies in this review. Figure 1.  The Flow Diagram of Search Process \n \n\n10  \n  \nTable 1. Keywords Used for Study Search \nNote: We focused our search on publications written in English and published in or after 2018. We conducted searches using titles, abstracts, and keywords. “*” represents variants of the word, such as plurals.  Information Extraction To address the proposed research questions, a coding framework was developed through an iterative process to extract information from the included studies. Initially, one reviewer proposed a preliminary coding framework and piloted it by extracting information from 20 of the included studies. Refinements to the coding framework, such as adding new coding subsections, removing redundant or irrelevant ones, and defining coding categories within each subsection, were conducted. Subsequently, with the refined framework, two reviewers independently coded all the studies after training. Disagreements between the two reviewers were resolved through discussions, and a third reviewer was involved when necessary. While conducting the information extraction, we realized some information was missing in the current literature for us to answer our research questions. Particularly, many studies did \nKeywords related to AIG Keywords related to LLMs Item generation*, AIG, Question generation, Distractor generation, Test development, Item development, Generat* Item*, Generat* Question* \nLarge language model, Pre-trained language model, PLM, LLM, language model, BERT, GPT*, ChatGPT, T5, mT5, PanGu, T0, CodeGen, Tk-Instruct, UL2, OPT, NLLB, GLM, Flan-T5, BLOOM, mT0, Galactia, BLOOMZ, OPT-IML, LLaMA, CodeGeeX, Pythia, GShard, Codex, ERNIE, Jurassic-1, HyperCLOVA, FLAN, Yuan, Anthropic, WebGPT, Gopher, GlaM, LaMDA, MT-NLG, AlphaCode, InstructGPT, Chinchilla, PaLM, AlexaTM, Sparrow, WeLM, U-PaLM, Flan-U-PaLM, (Transformer AND Model) \n11  \n  \nnot report the measurement properties of the generated items. Therefore, we employed a simple search strategy to identify the severity of this issue – searching for keywords that are expected to appear in the reviewed studies. We came up with a set of basic keywords that are related to the measurement properties of items as well as persons who are often in charge of evaluating item quality such as content specialists and SMEs. Then, we built a simple information extractor using the Python programming language and used it to summarize the patterns of occurrences of keywords across the reviewed studies. Results Our review indicated that there were two studies published in 2019, followed by an increase to five in 2020, 9 in 2021, a peak of 33 in 2022, and finally, eleven studies in 2023. The included studies can be found in Appendix A. Over half of these studies (n = 31) were papers published at conferences, such as the International Conference on Artificial Intelligence in Education and the European Conference on Technology Enhanced Learning. Fourteen of the reviewed studies were journal articles, which span two broad research domains, with education technology and assessment journals like Education and Information Technologies and International Journal of Assessment Tools in Education, and applied artificial intelligence journals such as Frontiers in Artificial Intelligence and IEEE Access. Moreover, there were non-refereed studies, including two master’s theses, one doctorate dissertation, and 12 preprints. These avenues imply the multidisciplinary and developing nature of studies in this field. RQ1: What are the most used LLMs for AIG, and how do their underlying architectures and features differ?  We identified Google’s Text-to-Text Transfer Transformer (T5; n = 32), Bidirectional Encoder Representations from Transformers (BERT; n = 26), and OpenAI’s Generative Pre-\n12  \n  \nTrained Transformer (GPT; n = 19) as three major base types of LLMs commonly used in AIG. In addition to these most common LLMs, we also found models that were only used once or a couple of times such as BART and PEGASUS. Because there were cases where more than one LLM was used in a study, we summarized the frequency of use by instance. That is, each study could be counted multiple times depending on how many LLMs they used.  Though all follow the transformer-learning paradigm, each type of LLM employs a distinct approach to language modeling, encompassing unique features and strengths for specific tasks. Due to the transfer learning paradigm, these LLMs often undergo further training with additional text data to enhance the base model. This results in a variety of variants and series. For a better understanding of their use in the AIG stages, the features and variants of each base type of LLM are outlined in the following subsections. T5 Variants Introduced by Raffel et al. in 2020, the T5 model treats all language processing tasks as text-to-text conversions, where the model receives text input and produces text output. The original dataset for training T5 was the C4 dataset which contains approximately 750GB of English texts sourced from the public Common Crawl web scrape (Raffel et al., 2020). The T5 model is available in several sizes, each differing in the number of parameters. Commonly used variants in the current AIG studies include T5-small, T5-base, and T5-large. The different sizes of these models enable their application in varied contexts, accommodating diverse requirements in terms of computational resources, training time, and performance capabilities. BERT Variants Differing from many other language models that process text in a single direction, BERT (Devlin et al., 2018) employs a bidirectional approach to understanding the context of words \n13  \n  \nwithin a sentence. By analyzing context from both left to right and right to left, BERT examines the full context of a word by considering all other words in the sentence, leading to a more comprehensive understanding of sentence contexts. Using 3.3 billion words sourced from Wikipedia and BooksCorpus, BERT was trained based on two key training strategies: Masked language model and next-sentence prediction. Similar to other LLMs, BERT could be further trained with additional text data or training strategies, possessing differential performance in certain specific contexts. For example, multilingual BERT (Devlin et al., 2019) are trained with additional text data from multiple languages to improve performance in multilingual understanding. The size of training datasets, training architecture, and the number of parameters can also lead to variants such as ALBERT (BERT with reduced parameters and sentence order prediction; Lan et al., 2019), DistilBERT (BERT with distillation; Sanh et al., 2019), RoBERTa (BERT without NSP but dynamic masking; Liu et al., 2019), and XLNet (BERT with all tokens masked but in random order; Yang et al., 2019).  GPT Series  GPT (Open AI, 2018) was developed based on predicting the next word in a sentence, taking into account the full context of the sentence. Its next-word prediction strategy allows GPT models to generate each word sequentially, conditioned on both the original input it receives (i.e., the prompt) and the words it has previously generated. The series of GPT models has seen remarkable advancements over the last few years, each iteration enhancing capabilities and complexity. In the series, GPT-1 contained 117 million parameters, GPT-2 expanded to 1.5 billion parameters, while GPT-3 further increased the scale with 175 billion parameters. The latest version, GPT-4, marks the latest advancement with an impressive count of approximately \n14  \n  \n1.7 trillion parameters, representing a substantial growth in the model’s capability in various language processing tasks. Building upon the core models of GPT-3 and GPT-4, the well-known ChatGPT (Open AI, 2022) is specifically fine-tuned to engage in interactive conversational tasks. This fine-tuning involves encoding given human language inputs (prompts) into rich, contextual-embedded textual representations that the machine system can understand. Subsequently, it decodes these textual representations sequentially, conditioned on the original prompts and the previously generated words, to produce coherent and contextually appropriate responses in human languages. Similarly, Codex, built based on GPT-3, is trained to specialize in understanding and generating codes of programming languages (Open AI, 2021). Summary of Findings for RQ1 Our review found that the most used base types of LLMs are T5, BERT, and GPT. These models exhibit differences in their training data sources, the size of their training sets, their training architectural frameworks, and their unique features.  RQ2: In what specific ways are LLMs most frequently used, and how do the LLMs’ features influence their use in the AIG process?  As depicted in Figure 2, we summarized and synthesized the specific uses of LLMs into three stages: pre-generation, item generation, and post-generation. In the pre-generation stage, LLMs are used to prepare text data before generating items. This stage involves cleaning, structuring, and understanding the original input text data which can be lengthy and cover various topics. This preparation is essential for the subsequent item generation stage to produce items that are contextually relevant, accurate, and coherent. In the item generation stage, items are created either directly by LLMs or through traditional methods like template-based \n15  \n  \napproaches. Studies using template-based approaches were included because they employed LLMs in either the pre- or post-generation stages. After item generation is the post-generation stage, where items are selected based on certain criteria such as quality and difficulty. With LLMs, this stage can be automated, resulting in filtered items as the final output of the AIG process. The majority of these studies (n = 44) employed LLMs in only one of the three AIG stages, 13 studies used LLMs in two stages, and 3 studies used LLMs across all three stages.  Pre-Generation Stage In the pre-generation stage, BERT was used 20 times, T5 was used 13 times, and GPT was used 9 times. We identified and categorized the following specific uses of LLMs: (1) text embedding (n = 8), (2) key phrase identification (n = 8), (3) text summarization (n = 6), (4) word tokenization (n = 3), (5) prompt engineering (n = 2), (6) word sense disambiguation (n = 2), (7) label extraction (n = 1), and (8) discourse segmentation (n = 1). These specific uses make up a toolbox for processing texts for subsequent tasks of generating desired items. The specific uses of LLMs were categorized into three themes, as described below. The first theme is data transformation, which converts original texts into computationally tractable and manageable units. Word tokenization is the process of transforming texts into smaller individual units, known as tokens. This task aims to break down given texts into manageable units and is often required before many subsequent preprocessing tasks. LLM tokenizers have several advantages over traditional rule-based tokenizers such as understanding context (Singh et al., 2019). In addition, text embedding transforms text into numerical vectors, known as textual representations, which capture the semantic meaning of words, phrases, and sentences. Consequently, texts with similar meanings are positioned closely in the embedding space, and these numerical vectors are used and processed in later language processing tasks. For \n16  \n  \nexample, these numerical vectors can be used for subsequent NLP tasks such as calculating the cosine similarity metric of items or distractors of multiple-choice questions to evaluate their semantic similarity (e.g., Min et al., 2021). Figure 2.  The Specific Use of LLMs in the AIG Process \n The second theme is related to semantic processing, which involves understanding the context of the text data or specific sentences. Key phrase identification involves using LLMs to extract important and relevant phrases from texts, crucial for framing the focus of the item generation. For example, Tsai et al. (2021) employed BERT to extract keywords from an input textbook, which were then used to construct important complete sentences as preparation for the item generation stage. In addition, word sense disambiguation is the process of determining the actual meaning of a word with multiple meanings in a given sentence and context. This step is crucial for computers to interpret words correctly in context, ensuring that later generated items are contextually appropriate and unambiguous. Moreover, we found that discourse segmentation was performed in two studies. This task divides texts into coherent segments such as sentences, \n\n17  \n  \nparagraphs, or topics, which helps to structure the text and generate items in a way that reflects the logical and semantic composition of the input texts. Furthermore, label extraction assigns labels to texts according to categories like reading difficulty, content domain, or question type. The extracted labels serve as control labels for creating targeted and relevant items. For example, Zhao et al. (2022) trained an LLM to extract one of the seven question types based on input texts, aiding in generating questions that match the intended type. Lastly, text summarization aims to convey the main points of the original text while significantly reducing its length, thereby improving the efficiency of subsequent item-generation tasks (Malhar et al., 2022). The last theme is prompt engineering, defined as the process of constructing commands or instructions that effectively communicate a task to LLMs. This process is essential in guiding subsequent LLMs to generate items that meet specific characteristics or assessment purposes, cater to various contexts, or assess across different domains. As an example, in the study of Ghanem et al. (2022), T5 was trained on how to ask questions and what to ask for subsequent item generation. Item Generation Stage We identified that 53 studies used LLMs during the item generation stage. Some studies used multiple LLMs, either to complement each other in different tasks or to compare and identify the best-performing models in the same task. Despite BERT and GPT-2 being introduced earlier than T5, the latter emerged as the most frequently employed LLM for item generation, being used 33 times. For example, Akyon et al. (2022) trained a variant of T5 (mT5) to first extract answers and then used these extracted answers as text input to generate questions as text output. GPT also comes as an option, having been used 15 times. GPT-variants predict the next word in a text sequence, enabling prompt-based generation. For instance, Wang et al. \n18  \n  \n(2022) compared the performance of GPT-3 in item generation when using different prompts. BERT was used in 9 instances. Given BERT’s characteristics, BERT variants were employed to predict masked tokens, creating specific item types such as fill-in-the-blank and cloze questions (e.g., Matsumori et al., 2023). In addition, we identified instances where other LLMs, such as BART and PEGASUS, were used.  Post-Generation Stage  To ensure the quality and relevance of the generated items for a given educational context, LLMs can be used in the post-generation stage to evaluate, filter, or classify the previously generated items. Among them, evaluation was conducted by calculating specific criteria or metrics. For instance, Jiao et al. (2023) utilized GPT-2 to calculate perplexity values, reflecting the fluency of the generated items. They also employed BERT-large for coherence evaluation and BertScore for assessing creativity by calculating semantic differences among items. Having the evaluation metrics calculated, filtering involves removing low-quality or irrelevant questions or distractors of multiple-choice questions that were generated by the LLM. For example, Offerijns et al. (2020) used GPT-2 to generate question-answer pairs and then used BERT to remove the questions that could not be answered or did not make sense. Lastly, classification refers to categorizing questions by type or topic. For example, Nguyen et al. (2022) first employed T5 to generate questions and then used GPT-3 to classify these questions based on their utility in learning specific topics. Summary of Findings for RQ2 We revealed distinct usage patterns of BERT, GPT, and T5 across the three AIG stages, reflecting the inherent features and strengths of these LLMs. As BERT excels at language understanding, it was predominantly utilized in the pre-generation stage. On the other hand, GPT \n19  \n  \nwas notably more employed in the item-generation stage, highlighting its strength in text generation. T5 demonstrated its flexibility by being used comparably in all three AIG stages. RQ3: What types of items were generated in the reviewed studies in terms of item type, subject domain, and language? Can LLMs generate valid, reliable, and high-quality items? Item Type We found 39 studies that generated constructed-response items; 29 studies generated selected-response items. Eight studies generated both types. The constructed-response items can be further categorized as Wh-questions, cloze questions, and Fill-in-the-blank questions, whereas selected-response items included True-False and multiple-choice questions, including their distractor generation.  Subject Domain Our review revealed that the majority of items generated primarily focus on two subject domains: language learning (n = 24) and general knowledge acquisition (n = 17). General knowledge items are often developed using general datasets, such as the Stanford Question Answering Dataset (SQuAD), which contains passages from Wikipedia and thus does not focus on a specific subject domain. Following closely behind are science-related disciplines (n = 9) such as agronomy, biology, chemistry, physics, and science history. In addition, five studies and four studies generated items for computer science education and mathematics education, respectively. We also found studies addressing other subject domains, including social science, medicine, and even literary experiences such as fairy tales.  Language  Overall, we identified a total of 12 different languages in the items generated. This finding suggests the potential for generalizability and the wide linguistic context in which LLMs \n20  \n  \ncan be utilized for AIG. In most instances, items were generated in English (n = 51), but there were also instances of generation in other languages, including Arabic, Chinese, French, German, Hindi, Indonesian, Korean, Lao, Marathi, Spanish, Swedish, Turkish, and Vietnamese. Data Source LLMs are typically developed for general language processing purposes and often require additional training to effectively perform specific tasks, such as generating context-relevant questions. Such additional training involves training LLMs on new datasets to adapt them to different tasks or content domains. In the reviewed studies, the most commonly utilized datasets for additional training have included SQuAD (n = 24), which encompasses a collection of passages with corresponding reading comprehension questions, and the ReAding Comprehension dataset from Examinations (RACE) dataset (n = 10), which focuses on English language exams. Additionally, some researchers have crafted their self-collected datasets by aggregating educational materials. These materials consisted of open-access journal articles (e.g., von Davier 2019), questions extracted from online learning platforms or communities (e.g., Stack Overflow; Tsai et al., 2021), LLM-generated texts (Bulut & Yildirim-Erbasli, 2022), teacher-created questions (Matsumori et al., 2023), stories and fairy tales (e.g., Ghanem et al. 2022), knowledge maps (Aigo et al., 2021), slides (Chughtai et al., 2022), textbooks (e.g., Steuer et al. 2020), and other course materials (e.g., Gopal, 2022). Item Properties As noted in the methodology section, we did not find many studies reporting the measurement properties of the generated items. Therefore, instead, we searched for keywords pertinent to measurement properties across the 60 reviewed studies. Figure 3 depicts the number of studies containing each keyword. Notably, only 10 out of the 60 studies mentioned “validity”. \n21  \n  \nThis was followed by “reliability” and “pedagogical”, which found their places in eight studies. Other keywords were used even less frequently in the reviewed studies. The infrequent occurrences of these keywords signal a concerning issue: the majority of the reviewed studies seem to neglect measurement properties of items when generating items for educational purposes, which potentially impacts the validity and reliability of the assessment results. Moreover, this could result in generating questions that are too simple and do not require higher cognitive thinking to answer, failing to meet the measurement or pedagogical purposes (e.g., delivering feedback or evaluating achievement). We further extracted sentences containing the keywords. However, we found that many occurrences of these keywords were not related to the measurement properties of the generated items. For instance, most descriptions of “reliability” were in contexts other than the reliability of items or assessment results. They commonly referred to terms like “inter-annotator reliability” and “the reliability of the data collection”. Similarly, the instances of “fairness” were exclusively related to the fairness of experiments comparing model performance, rather than to the fairness of the assessment items themselves. Therefore, the issue of lacking sufficient consideration for the measurement properties of items is even more severe than it appears in Figure 3. Summary of Findings for RQ3 We found that LLMs can be an effective and flexible solution to generating a large number of items, with few constraints on item type, language, subject domain, or the data source used for training LLMs to create items. However, we did not find many studies reporting the measurement properties of the generated items.   \n22  \n  \nFigure 3.  Number of Studies Containing Each Keyword \n Discussion Technological advancements such as e-learning platforms and computer-based assessments have ushered in unprecedented learning opportunities for students, transforming traditional educational practices and assessments. This transformation necessitates a substantial demand for high-quality assessment items, which are vital for supporting student learning and effectively evaluating educational outcomes (Bulut & Yildirim-Erbasli, 2022; Mazzullo et al., 2023). Therefore, AIG has been proposed and gradually developed by measurement researchers as a solution to reduce the cost of developing a large number of assessment items (Alves et al., 2010; Gierl & Lai, 2015; Gierl et al., 2021; Lai et al., 2009). Concurrently, recent advancements in LLMs have allowed applied NLP researchers to create items for educational purposes (Akyon et al., 2022; Pochiraju et al., 2023). As research in both domains expands, their intersection \n\n23  \n  \nwithin literature has become evident. To provide a more comprehensive view of research on the use of LLMs in AIG, this review aims to map the current literature by bringing together experiences from both research domains to explore the utility and potential of LLMs for AIG. The Current State of Research on AIG As a summary of the findings from this review, we identified the most commonly used LLMs in the current AIG literature, such as T5, BERT, GPT, and their variants. We described the characteristics and features of each base type of LLM, linking them to their specific uses in the AIG process. Regarding the AIG process, the pre-generation stage focuses on preparing, processing, and understanding texts for subsequent item generation to ensure high quality. In the post-generation stage, LLMs are primarily used to filter out low-quality items (i.e., mostly focusing on the correctness of grammar and syntax as well as the semantic relevancy and similarity) or determine the usefulness of the generated items.  After reviewing the existing studies, we conclude that LLMs prove useful in generating large banks of items. Additionally, we revealed that LLMs offer a highly flexible solution for AIG, as they have virtually no constraints in terms of item type, language, the subject domain of the items to be generated, or the data source used for further training of LLMs. Current Research Gaps in the Literature While the reviewed studies often show that LLMs are effective and flexible in creating a large number of items, we found that many studies applying LLMs in AIG often lacked a solid educational foundation. This might be because many of the authors were NLP researchers who possessed limited recognition and knowledge of learning or measurement theories. Alternatively, it could be because creating high-quality items that are readily usable for educational contexts was not their primary interest or research focus. Accordingly, many of those items are generated \n24  \n  \nwithout deep consideration of their measurement purposes and item properties, which are essential to meet the requirements of educational assessment. In traditional item development or template-based AIG, item generation starts with a clear definition of what to measure (i.e., identifying the construct to be measured by considering the expected learner outcomes and instructional objectives), why to measure (i.e., the purpose of assessment), and how to measure (i.e., assessment design and item format), while only a few studies leveraging LLMs for AIG considered these important aspects. For example, many of those generated items in the existing studies do not attempt to evaluate the higher-level cognitive processes specified in Bloom’s taxonomy, such as applying, analyzing, evaluating, or creating. This is because they are mostly created by researchers from the AQG area whose original purpose was to develop chatbots and conversational agents. Therefore, their item generation predominantly focuses on the levels of remembering or understanding, similar to the goal of conversational agents, which does not always meet or fit the measurement purposes and goals that are to evaluate the complex learning progress and outcomes of human students. Moreover, while some studies invited human participants to evaluate the quality of the items after being generated, only a few involved SMEs or measurement specialists in the AIG process. Consequently, most item evaluations did not address important measurement properties such as item difficulty and item discrimination. Thus, the current literature mostly provides evidence that LLMs can be leveraged to generate a large number of items, but little is known about whether these items possess the high quality necessary for educational purposes such as pedagogical teaching and assessment. Considering the goal of AIG, which is to generate large banks of high-quality items while reducing overall costs (Gierl & Lai, 2012; Lai et al., 2009), we argue that a thorough item evaluation after generation is missing in the current literature. That is, \n25  \n  \nAIG does not end with generating a large number of items but with ensuring that these items are of high quality and can fulfill their demands for educational purposes and contexts. Thus, we argue that AIG is still a developing and promising research domain, and its further development and application will depend on the integration of learning and measurement theories. Suggestions for Future Research The current research gaps led us to advocate for two suggestions for future research in this area. The first suggestion is to clarify the assessment context and measurement goals for AIG. While AIG benefits from advancements in NLP, such as LLMs, it differs from general text generation by creating items that meet the specific purposes of assessments in real educational contexts. Newton (2007) distinguished 18 different educational assessment applications, such as formative, diagnostic, qualification certification, and comparative purposes. For example, formative assessments take place during students’ learning processes and support learning by providing effective feedback, while summative assessments aim to collect information about students’ learning outcomes after the learning process has occurred. As the assessment purposes differ, the desired characteristics of the generated items also differ. When generating items for a specific educational and assessment context, researchers must identify the nature and desired characteristics of these items. For instance, in the case of formative assessments aimed at aiding students’ learning, items should prioritize pedagogical value by providing feedback to help identify misconceptions. To assess students’ learning outcomes following a teaching program, items should be balanced and cover a broad range of key concepts taught, rather than disproportionately focusing on concepts within a narrow content area. Furthermore, regardless of whether items are formative or summative or created for other assessment purposes, they must demonstrate high quality, with strong reliability and validity, to be effective, because they \n26  \n  \ndirectly impact the feedback provided to students or the ability to draw inferences about students’ learning progress and outcomes. Our second suggestion for future AIG research is to examine the measurement properties and pedagogical soundness of items. From a practical standpoint, item development does not end with item generation; the evaluation of item quality is crucial to ensure usability in educational contexts. For example, measurement properties of items (e.g., difficulty and discrimination parameters) and tests (e.g., reliability and validity) can be examined using measurement theories such as classical test theory and item response theory. Finally, after items are generated and evaluated for their measurement properties, the focus should shift back to meeting the intended measurement purposes or pedagogical value of assessments. Educators and SMEs can assess the educational or pedagogical value of automatically generated items. It is important to ensure that the generated items can effectively serve their intended purpose. If not, it is necessary to revisit previous item development stages for revisions to create items that better align with the learning objectives and measurement goals. Limitations  We did not conduct a formal assessment of the methodological quality of the included studies, which is generally considered a limitation of scoping reviews. This is because the primary purpose of this study, and of scoping reviews in general, is to map the existing literature on a particular topic to explore the range, nature, and extent of research activities related to the topic, rather than to evaluate the quality of the studies (Arksey & O'Malley, 2005). Additionally, we acknowledge that the field of leveraging LLMs is rapidly evolving. As we completed our keyword search, new LLMs like Google’s Gemini and OpenAI’s ChatGPT with the new GPT4-Turbo have been introduced, boasting significantly larger parameter sizes and enhanced \n27  \n  \nfunctionalities. This rapid development has not only expanded the utility and potential of LLMs but has also made advanced technology more accessible to users with varying levels of programming skills. Consequently, the use of LLMs in education continues to evolve, and understanding how these models can best serve the AIG process remains an ongoing journey. Conclusion Through the mapping and synthesis of the reviewed studies on leveraging LLMs for AIG, we identified that the most commonly used LLMs are T5, BERT, GPT, and their variants. We have also categorized the current applications of LLMs into three stages of the AIG process: pre-generation, item generation, and post-generation. The findings reveal that using LLMs to generate items is an effective and flexible solution, with few constraints on item type, language, subject domain, or the data source used for training LLMs to create items. Due to the exceptional language understanding abilities of LLMs, the generated items are typically free from grammar errors and contextually relevant to the desired content domain. However, we also noted a lack of a solid educational foundation in many of the reviewed studies, as they did not incorporate learning and measurement theories into the item generation process. We attribute this issue to the absence of involvement of human experts such as SMEs and measurement specialists. Although one part of the goal of AIG was to reduce financial costs and human burdens, it was never meant to exclude human involvement from the item development process. Importantly, AIG is still considered an augmented intelligence approach, which means it requires both human expertise and the capabilities of a computer. Considering the goal of AIG, we not only want to generate a large number of items but also care more about the item quality and characteristics. Hence, future researchers should consider enhancing the educational foundation in the AIG process, \n28  \n  \nleading to the production of items that are not only grammatically correct and contextually relevant but also reliable, valid, and pedagogically sound.   \n29  \n  \nReferences Ackerman, R., & Balyan, R. (2023). Automatic multilingual question generation for health data using LLMs. In International Conference on AI-generated Content (pp. 1-11). Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-99-7587-7_1 Agrawal, A., & Shukla, P. (2023). Context aware automatic subjective and objective question generation using Fast Text to text transfer learning. International Journal of Advanced Computer Science and Applications, 14(4). Retrieved from https://pesquisa.bvsalud.org/global-literature-on-novel-coronavirus-2019-ncov/resource/pt/covidwho-2321413 Aigo, K., Tsunakawa, T., Nishida, M., & Nishimura, M. (2021). Question generation using knowledge graphs with the T5 language model and masked self-attention. In 2021 IEEE 10th Global Conference on Consumer Electronics (pp. 85-87). IEEE. https://doi.org/10.1109/GCCE53005.2021.9621874 Akyön, F. Ç., Cavusoglu, A. D. E., Cengiz, C., Altinuç, S. O., & Temizel, A. (2022). Automated question generation and question answering from Turkish texts. Turkish Journal of Electrical Engineering and Computer Sciences, 30(5), 1931-1940. https://doi.org/10.55730/1300-0632.3914 Alsubait, T., Parsia, B., & Sattler, U. (2016). Ontology-based multiple choice question generation. KI-Künstliche Intelligenz, 30, 183-188. https://doi.org/10.1007/s13218-015-0405-9 Alves, C. B., Gierl, M. J., & Lai, H. (2010). Using automated item generation to promote principled test design and development. [paper]. Annual meeting of American Educational Research Association, Denver, CO, USA. \n30  \n  \nArksey, H., & O'malley, L. (2005). Scoping studies: towards a methodological framework. International Journal of Social Research Methodology, 8(1), 19-32. https://doi.org/10.1080/1364557032000119616 Attali, Y., Runge, A., LaFlair, G. T., Yancey, K., Goodwin, S., Park, Y., & Von Davier, A. A. (2022). The interactive reading task: Transformer-based automatic item generation. Frontiers in Artificial Intelligence, 5, 903077. https://doi.org/10.3389/frai.2022.903077 Berger, G., Rischewski, T., Chiruzzo, L., & Rosá, A. (2022). Generation of English question answer exercises from texts using transformers-based models. In 2022 IEEE Latin American Conference on Computational Intelligence (pp. 1-5). IEEE. https://doi.org/10.1109/LA-CCI54402.2022.9981171 Black, P., & Wiliam, D. (1998). Assessment and classroom learning. Assessment in Education: Principles, Policy & Practice, 5(1), 7-74. https://doi.org/10.1080/0969595980050102 Bulathwela, S., Muse, H., & Yilmaz, E. (2023). Scalable educational question generation with pre-trained language models. In International Conference on Artificial Intelligence in Education (pp. 327-339). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-36272-9_27 Bulut, O., Gorgun, G., Yildirim‐Erbasli, S. N., Wongvorachan, T., Daniels, L. M., Gao, Y., ... & Shin, J. (2023). Standing on the shoulders of giants: Online formative assessments as the foundation for predictive learning analytics models. British Journal of Educational Technology, 54(1), 19-39. https://doi.org/10.1111/bjet.13276 Bulut, O., & Yildirim-Erbasli, S. N. (2022). Automatic story and item generation for reading comprehension assessments with transformers. International Journal of Assessment Tools in Education, 9(Special Issue), 72-87. https://doi.org/10.21449/ijate.1124382 \n31  \n  \nCh, D. R., & Saha, S. K. (2018). Automatic multiple choice question generation from text: A survey. IEEE Transactions on Learning Technologies, 13(1), 14-25. https://doi.org/10.1109/TLT.2018.2889100 Chiang, S. H., Wang, S. C., & Fan, Y. C. (2024). Cdgp: Automatic cloze distractor generation based on pre-trained language model. arXiv preprint arXiv:2403.10326. https://doi.org/10.18653/v1/2022.findings-emnlp.429 Chughtai, R., Azam, F., Anwar, M. W., But, W. H., & Farooq, M. U. (2022). A lecture centric automated distractor generation for post-graduate software engineering courses. In 2022 International Conference on Frontiers of Information Technology (FIT) (pp. 100-105). IEEE. https://doi.org/10.1109/FIT57066.2022.00028 Chung, H. L., Chan, Y. H., & Fan, Y. C. (2020). A BERT-based distractor generation scheme with multi-tasking and negative answer training strategies. arXiv preprint arXiv:2010.05384. https://arxiv.org/abs/2010.05384 Desai, T. (2021). Discourse parsing and its application to question generation. The University of Texas at Dallas. Retrieved from https://utd-ir.tdl.org/server/api/core/bitstreams/19febde1-bb49-43e5-bf58-63435bf69cdf/content Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. https://doi.org/10.48550/arXiv.1810.04805 Dijkstra, R., Genç, Z., Kayal, S., & Kamps, J. (2022). Reading comprehension quiz generation using generative pre-trained transformers. In iTextbooks@ AIED (pp. 4-17). Retrieved from https://e.humanities.uva.nl/publications/2022/dijk_read22.pdf \n32  \n  \nDrori, I., Zhang, S., Shuttleworth, R., Tang, L., Lu, A., Ke, E., ... & Strang, G. (2022). A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32), e2123433119. https://doi.org/10.1073/pnas.2123433119 Falcão, F., Costa, P., & Pêgo, J. M. (2022). Feasibility assurance: a review of automatic item generation in medical assessment. Advances in Health Sciences Education, 27(2), 405-425. https://doi.org/10.1007/s10459-022-10092-z Femi, J. G., & Nayak, A. K. (2022). EQGTL: An Ensemble Model for Relevant Question Generation using Transfer Learning. In 2022 International Conference on Machine Learning, Computer Systems and Security (pp. 253-258). IEEE. https://doi.org/10.1109/MLCSS57186.2022.00054 Fuadi, M., & Wibawa, A. D. (2022). Automatic question generation from indonesian texts using text-to-text transformers. In 2022 International Conference on Electrical and Information Technology (IEIT) (pp. 84-89). IEEE. https://doi.org/10.1109/IEIT56384.2022.9967858 Fung, Y. C., Kwok, J. C. W., Lee, L. K., Chui, K. T., & U, L. H. (2020). Automatic question generation system for english reading comprehension. In Technology in Education. Innovations for Online Teaching and Learning: 5th International Conference, ICTE 2020, Macau, China, August 19-22, 2020, Revised Selected Papers 5 (pp. 136-146). Springer Singapore. https://doi.org/10.1007/978-981-33-4594-2_12 Fung, Y. C., Lee, L. K., & Chui, K. T. (2023). An automatic question generator for Chinese comprehension. Inventions, 8(1), 31. https://doi.org/10.3390/inventions8010031 \n33  \n  \nGhanem, B., Coleman, L. L., Dexter, J. R., von der Ohe, S. M., & Fyshe, A. (2022). Question generation for reading comprehension assessment by modeling how and what to ask. arXiv preprint arXiv:2204.02908. https://doi.org/10.48550/arXiv.2204.02908 Gierl, M. J., & Lai, H. (2012). The role of item models in automatic item generation. International Journal of Testing, 12(3), 273-298. https://doi.org/10.1080/15305058.2011.635830 Gierl, M. J., & Lai, H. (2015). Automatic item generation. In Handbook of test development (pp. 410-429). Routledge. Gierl, M. J., & Lai, H. (2016). A process for reviewing and evaluating generated test items. Educational Measurement: Issues and Practice, 35(4), 6-20. https://doi.org/10.1111/emip.12129 Gierl, M. J., Lai, H., & Tanygin, V. (2021). Advanced methods in automatic item generation. Routledge. Godslove, J. F., & Nayak, A. K. (2023). Generative model for formulating relevant questions and answers using transfer learning. In AIP Conference Proceedings (Vol. 2819, No. 1). AIP Publishing. https://doi.org/10.1063/5.0136892 Gopal, A. (2022). Automatic question generation for Hindi and Marathi. In 2022 International Conference on Advanced Learning Technologies (ICALT) (pp. 19-21). IEEE. https://doi.org/10.1109/ICALT55010.2022.00012 Goyal, R., Kumar, P., & Singh, V. P. (2023). Automated question and answer generation from texts using text-to-text transformers. Arabian Journal for Science and Engineering, 1-15. https://doi.org/10.1007/s13369-023-07840-7 \n34  \n  \nGrover, K., Kaur, K., Tiwari, K., Rupali, & Kumar, P. (2021). Deep learning based question generation using t5 transformer. In Advanced Computing: 10th International Conference, IACC 2020, Panaji, Goa, India, December 5–6, 2020, Revised Selected Papers, Part I 10 (pp. 243-255). Springer Singapore. https://doi.org/10.1007/978-981-16-0401-0_18 Han, Z. (2022). Unsupervised multilingual distractor generation for fill-in-the-blank questions. [Thesis]. Uppsala University. Retrieved from urn:nbn:se:uu:diva-476855.  Jiao, Y., Shridhar, K., Cui, P., Zhou, W., & Sachan, M. (2023). Automatic educational question generation with difficulty level controls. In International Conference on Artificial Intelligence in Education (pp. 476-488). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-36272-9_39 Kalpakchi, D., & Boye, J. (2021). BERT-based distractor generation for Swedish reading comprehension questions using a small-scale dataset. arXiv preprint arXiv:2108.03973. https://doi.org/10.48550/arXiv.2108.03973 Kasakowskij, R., Kasakowskij, T., & Seidel, N. (2022). Generation of multiple true false questions. 20. Fachtagung Bildungstechnologien. https://doi.org/10.18420/delfi2022-026. Khandait, K., Bhura, S., & Asole, S. S. (2022). Automatic question generation through word vector synchronization using lamma. Indian Journal of Computer Science and Engineering, 13(4), 1083-1095. https://doi.org/10.21817/indjcse/2022/v13i4/221304046 Kosh, A. E., Simpson, M. A., Bickel, L., Kellogg, M., & Sanford‐Moore, E. (2019). A cost–benefit analysis of automatic item generation. Educational Measurement: Issues and Practice, 38(1), 48-53. https://doi.org/10.1111/emip.12237 Kumar, A., Kharadi, A., Singh, D., & Kumari, M. (2021). Automatic question-answer pair generation using deep learning. In 2021 Third International Conference on Inventive \n35  \n  \nResearch in Computing Applications (pp. 794-799). IEEE. https://doi.org/10.1109/ICIRCA51532.2021.9544654 Kumar, N. S., Mali, R., Ratnam, A., Kurpad, V., & Magapu, H. (2022). Identification and addressal of knowledge gaps in students. In 2022 3rd International Conference for Emerging Technology (pp. 1-6). IEEE. https://doi.org/10.1109/INCET54531.2022.9824483 Kumar, S., Chauhan, A., & Kumar C, P. (2022). Learning enhancement using question-answer generation for e-book using contrastive fine-tuned T5. In International Conference on Big Data Analytics (pp. 68-87). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-24094-2_5 Kumari, V., Keshari, S., Sharma, Y., & Goel, L. (2022). Context-based question answering system with suggested questions. In 2022 12th International Conference on Cloud Computing, Data Science & Engineering (pp. 368-373). IEEE. https://doi.org/10.1109/Confluence52989.2022.9734207 Kurdi, G., Leo, J., Parsia, B., Sattler, U., & Al-Emari, S. (2020). A systematic review of automatic question generation for educational purposes. International Journal of Artificial Intelligence in Education, 30, 121-204. https://doi.org/10.1007/s40593-019-00186-y Lai, H., Alves, C., & Gierl, M. J. (2009). Using automatic item generation to address item demands for CAT. In D. J. Weiss (Ed.), Proceedings of the 2009 GMAC Conference on Computerized Adaptive Testing. Retrieved from www.psych.umn.edu/psylabs/CATCentral/ \n36  \n  \nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942. https://doi.org/10.48550/arXiv.1909.11942 Lee, H., Chung, H. Q., Zhang, Y., Abedi, J., & Warschauer, M. (2020). The effectiveness and features of formative assessment in US K-12 education: A systematic review. Applied Measurement in Education, 33(2), 124-140. https://doi.org/10.1080/08957347.2020.1732383 Lim, Y. S. (2019). Students’ perception of formative assessment as an instructional tool in medical education. Medical Science Educator, 29(1), 255-263. https://doi.org/10.1007/s40670-018-00687-w Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. https://doi.org/10.48550/arXiv.1907.11692 Maheen, F., Asif, M., Ahmad, H., Ahmad, S., Alturise, F., Asiry, O., & Ghadi, Y. Y. (2022). Automatic computer science domain multiple-choice questions generation based on informative sentences. PeerJ Computer Science, 8, e1010. https://doi.org/10.7717/peerj-cs.1010 Malhar, A., Sawant, P., Chhadva, Y., & Kurhade, S. (2022). Deep learning-based Answering Questions using T5 and Structured Question Generation System. In 2022 6th International Conference on Intelligent Computing and Control Systems (pp. 1544-1549). IEEE. https://doi.org/10.1109/ICICCS53718.2022.9788264 Mathur, A., & Suchithra, M. (2022). Application of abstractive summarization in multiple choice question generation. In 2022 International Conference on Computational Intelligence \n37  \n  \nand Sustainable Engineering Solutions (pp. 409-413). IEEE. https://doi.org/10.1109/CISES54857.2022.9844396 Matsumori, S., Okuoka, K., Shibata, R., Inoue, M., Fukuchi, Y., & Imai, M. (2023). Mask and cloze: Automatic open cloze question generation using a masked language model. IEEE Access, 11, 9835-9850. https://doi.org/10.1109/ACCESS.2023.3239005 Maurya, K. K., & Desarkar, M. S. (2020). Learning to distract: A hierarchical multi-decoder network for automated generation of long distractors for multiple-choice questions for reading comprehension. In Proceedings of the 29th ACM international conference on information & knowledge management (pp. 1115-1124). https://doi.org/10.1145/3340531.3411997 Mazzullo, E., Bulut, O., Wongvorachan, T., & Tan, B. (2023). Learning Analytics in the Era of Large Language Models. Analytics, 2(4), 877-898. https://doi.org/10.3390/analytics2040046 Min, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T. H., Sainz, O., ... & Roth, D. (2023). Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2), 1-40. https://doi.org/10.1145/3605943 Muse, H., Bulathwela, S., & Yilmaz, E. (2022). Pre-training with scientific text improves educational question generation. arXiv preprint arXiv:2212.03869. https://doi.org/10.48550/arXiv.2212.03869 Newton, P. E. (2007). Clarifying the purposes of educational assessment. Assessment in education, 14(2), 149-170. https://doi.org/10.1080/09695940701478321 Nguyen, H. A., Bhat, S., Moore, S., Bier, N., & Stamper, J. (2022). Towards generalized methods for automatic question generation in educational domains. In European \n38  \n  \nconference on technology enhanced learning (pp. 272-284). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-16290-9_20 Nittala, S., Agarwal, P., Vishnu, R., & Shanbhag, S. (2022). Speaker Diarization and BERT-Based Model for Question Set Generation from Video Lectures. In Information and Communication Technology for Competitive Strategies ICT: Applications and Social Interfaces (pp. 441-452). Singapore: Springer Nature Singapore. https://doi.org/10.1007/978-981-19-0095-2_42 Offerijns, J., Verberne, S., & Verhoef, T. (2020). Better distractions: Transformer-based distractor generation and multiple-choice question filtering. arXiv preprint arXiv:2010.09598. https://doi.org/10.48550/arXiv.2010.09598 Pochiraju, D., Chakilam, A., Betham, P., Chimulla, P., & Rao, S. G. (2023). Extractive summarization and multiple-choice question generation using XLNet. In 2023 7th International Conference on Intelligent Computing and Control Systems (pp. 1001-1005). IEEE. https://doi.org/10.1109/ICICCS56967.2023.10142220 Pugh, D., De Champlain, A., Gierl, M., Lai, H., & Touchie, C. (2020). Can automated item generation be used to develop high quality MCQs that assess application of knowledge?. Research and Practice in Technology Enhanced Learning, 15, 1-13. https://doi.org/10.1186/s41039-020-00134-8 Qiu, X., Xue, H., Liang, L., Xie, Z., Liao, S., & Shi, G. (2021). Automatic generation of multiple-choice cloze-test questions for lao language learning. In 2021 International Conference on Asian Language Processing (pp. 125-130). IEEE. https://doi.org/10.1109/IALP54817.2021.9675153 \n39  \n  \nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1-67. https://doi.org/10.48550/arXiv.1910.10683 Raina, V., & Gales, M. (2022). Multiple-choice question generation: Towards an automated assessment framework. arXiv preprint arXiv:2209.11830. https://doi.org/10.48550/arXiv.2209.11830  Ratcheva, M. G., Navale, R., & Desai, B. C. (2022). An online MCQ sub-system for CrsMgr. In Proceedings of the 26th International Database Engineered Applications Symposium (pp. 128-133). https://doi.org/10.1145/3548785.3548789 Rodriguez-Torrealba, R., Garcia-Lopez, E., & Garcia-Cabot, A. (2022). End-to-end generation of multiple-xhoice questions using text-to-text transfer transformer models. Expert Systems with Applications, 208, 118258. https://doi.org/10.1016/j.eswa.2022.118258 Rudner, L. M. (2009). Implementing the graduate management admission test computerized adaptive test. In Elements of adaptive testing (pp. 151-165). New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-85461-8_8 Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. https://doi.org/10.48550/arXiv.1910.01108 Shan, J., Nishihara, Y., Maeda, A., & Yamanishi, R. (2022). Question generation for reading comprehension test complying with types of question. Journal of Information Science & Engineering, 38(3). https://doi.org/10.6688/JISE.202205_38(3).0005 Shan, J., Nishihara, Y., Yamanishi, R., & Maeda, A. (2019). Question generation for reading comprehension of language learning test: A method using Seq2Seq approach with \n40  \n  \ntransformer model. In 2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (pp. 1-6). IEEE. https://doi.org/10.1109/TAAI48200.2019.8959903 Shridhar, K., Macina, J., El-Assady, M., Sinha, T., Kapur, M., & Sachan, M. (2022). Automatic generation of socratic subquestions for teaching math word problems. arXiv preprint arXiv:2211.12835. https://doi.org/10.48550/arXiv.2211.12835 Singh, J., McCann, B., Socher, R., & Xiong, C. (2019). BERT is not an interlingua and the bias of tokenization. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019) (pp. 47-55). https://doi.org/10.18653/v1/D19-6106 Srivastava, M., & Goodman, N. (2021). Question generation for adaptive education. arXiv preprint arXiv:2106.04262. https://doi.org/10.48550/arXiv.2106.04262 Steuer, T., Filighera, A., & Rensing, C. (2020). Exploring artificial jabbering for automatic text comprehension question generation. In Addressing Global Challenges and Quality Education: 15th European Conference on Technology Enhanced Learning, EC-TEL 2020, Heidelberg, Germany, September 14–18, 2020, Proceedings 15 (pp. 1-14). Springer International Publishing. https://doi.org/10.1007/978-3-030-57717-9_1 Tsai, D. C., Chang, W., & Yang, S. (2021). Short answer questions generation by Fine-Tuning BERT and GPT-2. In Proceedings of the 29th International Conference on Computers in Education Conference (Vol. 64). Retrieved from https://icce2021.apsce.net/wp-content/uploads/2021/12/ICCE2021-Vol.II-PP.-508-514.pdf von Davier, M. (2019). Training Optimus prime, MD: Generating medical certification items by fine-tuning OpenAI's gpt2 transformer model. arXiv preprint arXiv:1908.08594. https://doi.org/10.48550/arXiv.1908.08594 \n41  \n  \nVu, N., & Van Nguyen, K. (2022). Enhancing Vietnamese question generation with reinforcement learning. In Asian Conference on Intelligent Information and Database Systems (pp. 559-570). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-21743-2_45 Wang, B., Yao, T., Chen, W., Xu, J., & Wang, X. (2021). Multi-lingual question generation with language agnostic language model. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (pp. 2262-2272). Retrieved from https://aclanthology.org/2021.findings-acl.199.pdf Wang, H. C., Maslim, M., & Kan, C. H. (2023). A question–answer generation system for an asynchronous distance learning platform. Education and Information Technologies, 28(9), 12059-12088. https://doi.org/10.1007/s10639-023-11675-y Wang, Z., Valdez, J., Basu Mallick, D., & Baraniuk, R. G. (2022). Towards human-like educational question generation with large language models. In International conference on artificial intelligence in education (pp. 153-166). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-11644-5_13 Wylie, E. C., & Lyon, C. J. (2015). The fidelity of formative assessment implementation: Issues of breadth and quality. Assessment in Education: Principles, Policy & Practice, 22(1), 140-160. https://doi.org/10.1080/0969594X.2014.990416 Xie, J., Peng, N., Cai, Y., Wang, T., & Huang, Q. (2021). Diverse distractor generation for constructing high-quality multiple choice questions. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30, 280-291. https://doi.org/10.1109/TASLP.2021.3138706 \n42  \n  \nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32. https://dl.acm.org/doi/10.5555/3454287.3454804 Zhang, C. (2023). Automatic Generation of Multiple-Choice Questions. arXiv preprint arXiv: 2303.14576v1. https://doi.org/10.48550/arXiv.2303.14576 Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., ... & Wen, J. R. (2023). A survey of large language models. arXiv preprint arXiv:2303.18223. https://doi.org/10.48550/arXiv.2303.18223 Zhao, Z., Hou, Y., Wang, D., Yu, M., Liu, C., & Ma, X. (2022). Educational question generation of children storybooks via question type distribution learning and event-centric summarization. arXiv preprint arXiv:2203.14187. https://doi.org/10.48550/arXiv.2203.14187 "
}