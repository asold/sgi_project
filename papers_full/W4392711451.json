{
  "title": "Comparing the Performance of Popular Large Language Models on the National Board of Medical Examiners Sample Questions",
  "url": "https://openalex.org/W4392711451",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2010522326",
      "name": "Ali Abbas",
      "affiliations": [
        "Saint Thomas Midtown Hospital",
        "Baptist Hospital",
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5101316081",
      "name": "Mahad S Rehman",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Syed S Rehman",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200544039",
    "https://openalex.org/W2958109836",
    "https://openalex.org/W3017000889",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4384389802",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4385988139",
    "https://openalex.org/W4386553312",
    "https://openalex.org/W4372047097",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W3040702025",
    "https://openalex.org/W2035080614",
    "https://openalex.org/W4361282369",
    "https://openalex.org/W4381855196",
    "https://openalex.org/W3086667591",
    "https://openalex.org/W2980030301",
    "https://openalex.org/W2998175747",
    "https://openalex.org/W3002705197",
    "https://openalex.org/W3013692475",
    "https://openalex.org/W2946185430",
    "https://openalex.org/W4292656665",
    "https://openalex.org/W4377563830",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4388722291",
    "https://openalex.org/W2982580298"
  ],
  "abstract": null,
  "full_text": "Review began\n 02/19/2024 \nReview ended\n 03/08/2024 \nPublished\n 03/11/2024\n© Copyright \n2024\nAbbas et al. This is an open access article\ndistributed under the terms of the Creative\nCommons Attribution License CC-BY 4.0.,\nwhich permits unrestricted use, distribution,\nand reproduction in any medium, provided\nthe original author and source are credited.\nComparing the Performance of Popular Large\nLanguage Models on the National Board of\nMedical Examiners Sample Questions\nAli Abbas \n, \nMahad S. Rehman \n, \nSyed S. Rehman \n1.\n Medical School, University of Texas Southwestern Medical School, Dallas, USA \n2.\n Nephrology, Baptist Hospitals of\nSoutheast Texas, Beaumont, USA\nCorresponding author: \nAli Abbas, \nali.abbas@utsouthwestern.edu\nAbstract\nIntroduction: Large language models (LLMs) have transformed various domains in medicine, aiding in\ncomplex tasks and clinical decision-making, with OpenAI's GPT-4, GPT-3.5, Google’s Bard, and Anthropic’s\nClaude among the most widely used. While GPT-4 has demonstrated superior performance in some studies,\ncomprehensive comparisons among these models remain limited. Recognizing the significance of the\nNational Board of Medical Examiners (NBME) exams in assessing the clinical knowledge of medical\nstudents, this study aims to compare the accuracy of popular LLMs on NBME clinical subject exam sample\nquestions.\nMethods: The questions used in this study were multiple-choice questions obtained from the official NBME\nwebsite and are publicly available. Questions from the NBME subject exams in medicine, pediatrics,\nobstetrics and gynecology, clinical neurology, ambulatory care, family medicine, psychiatry, and surgery\nwere used to query each LLM. The responses from GPT-4, GPT-3.5, Claude, and Bard were collected in\nOctober 2023. The response by each LLM was compared to the answer provided by the NBME and checked for\naccuracy. Statistical analysis was performed using one-way analysis of variance (ANOVA).\nResults: A total of 163 questions were queried by each LLM. GPT-4 scored 163/163 (100%), GPT-3.5 scored\n134/163 (82.2%), Bard scored 123/163 (75.5%), and Claude scored 138/163 (84.7%). The total performance of\nGPT-4 was statistically superior to that of GPT-3.5, Claude, and Bard by 17.8%, 15.3%, and 24.5%,\nrespectively. The total performance of GPT-3.5, Claude, and Bard was not significantly different. GPT-4\nsignificantly outperformed Bard in specific subjects, including medicine, pediatrics, family medicine, and\nambulatory care, and GPT-3.5 in ambulatory care and family medicine. Across all LLMs, the surgery exam\nhad the highest average score (18.25/20), while the family medicine exam had the lowest average score\n(3.75/5). \nConclusion: GPT-4's superior performance on NBME clinical subject exam sample questions underscores its\npotential in medical education and practice. While LLMs exhibit promise, discernment in their application\nis crucial, considering occasional inaccuracies. As technological advancements continue, regular\nreassessments and refinements are imperative to maintain their reliability and relevance in medicine.\nCategories:\n Other, Medical Education, Healthcare Technology\nKeywords:\n chatgpt, google's bard, gpt-4, claude, artificial intelligence and education, artificial intelligence in\nmedicine, united states medical licensing examination (usmle), nbme subject exam, large language model, artificial\nintelligence (ai)\nIntroduction\nAdvances in artificial intelligence (AI), particularly the development of large language models (LLMs), have\nrevolutionized numerous fields, including medicine. AI has found its way into diverse medical\nspecializations, including oncology, radiology, and pathology, showcasing its evolving clinical uses \n[1]\n.\nThese advanced models assist healthcare professionals in complex tasks, from cancer detection and\ncategorization to adjusting insulin regimens in diabetic patients \n[2-4]\n. As the development of new models\npersists, AI will continue to revolutionize our comprehension and approach to medicine. As of June 2023,\nover 20 LLMs are available for public use, with OpenAI's GPT-4, GPT-3.5, Google’s Bard (based on PaLM 2),\nand Anthropic’s Claude among the most widely used \n[5]\n.\nAlthough previous studies have shown the ability of individual LLMs to pass certain medical licensing\nexams \n[6]\n, there is limited research comparing the performance of different LLMs. Most recently, GPT-4 has\nshown to outperform other LLMs in answering questions relating to various medical specialties, such as\nneurosurgery, orthopedics, and general surgery \n[7-9]\n. Furthermore, GPT-4 outperformed its predecessor,\nChatGPT, on the United States Medical Licensing Exam (USMLE) soft skills exam \n[10]\n, displaying its capacity\nfor empathy in addition to technical knowledge. Considering this potential, there is growing interest in\n1\n1\n2\n \n Open Access Original\nArticle\n \nDOI:\n 10.7759/cureus.55991\nHow to cite this article\nAbbas A, Rehman M S, Rehman S S (March 11, 2024) Comparing the Performance of Popular Large Language Models on the National Board of\nMedical Examiners Sample Questions. Cureus 16(3): e55991. \nDOI 10.7759/cureus.55991\nassessing the capabilities of LLMs in medical education and practice. When students or educators use LLMs,\nit is important to know which LLM would provide the most accurate information in a broad variety of\nsubjects.\nThe National Board of Medical Examiners (NBME) clinical subject exams serve as an important measure of a\nmedical student's knowledge and clinical capabilities \n[11-12]\n. The NBME subject exams are taken by medical\nstudents, typically at the conclusion of a clinical rotation in the respective field. For example, a neurology\nsubject exam would be taken by a medical student during their neurology clerkship rotation. This study aims\nto compare the performance of popular LLMs on sample questions from the NBME clinical subject exams.\nGiven the capabilities of LLMs, the outcomes of this study could highlight the potential advantages of\ncertain LLMs over others.\nMaterials And Methods\nThis study was a comparative analysis, aiming to evaluate the performance of various LLMs in answering\nmedical multiple-choice questions. The questions used in this study were multiple-choice questions\nobtained from the official NBME website and are free and publicly available \n[13]\n. Questions from the NBME\nclinical science subject exams in medicine, pediatrics, obstetrics and gynecology, clinical neurology,\nambulatory care, family medicine, psychiatry, and surgery were used along with questions from the NBME\nsample comprehensive clinical science exam. Each exam has a set of sample questions consisting of either\n19 or 20 questions, with the exception of family medicine, which has five questions. All questions from each\nexam were used and no questions were omitted.\nThe LLMs queried include GPT-4, GPT-3.5, Claude, and Bard. None of the LLMs had Internet access enabled.\nAll data were collected in October 2023. For consistency, each NBME question, including its lettered answer\nchoices, was individually inputted into the designated text field of each LLM studied with no additional text\nin the prompt. Using a systematic approach, each question was inputted individually in the order in which\nthey were presented on the NBME sample exam, and all parts of each question were provided as they\nappeared on the exam. Each LLM was prompted with each individual question one time. All questions were\nmultiple-choice-style questions with one correct answer. The answer choice provided in the response from\neach LLM was recorded and compared to the correct answer provided by NBME. The total number of\nquestions answered correctly by each LLM was recorded.\nThe one-way analysis of variance (ANOVA) was used to determine statistically significant differences\nbetween LLM performance for each subject and overall performance. In situations where the ANOVA\nindicated significant differences, a post-hoc analysis was conducted using the Bonferroni correction to\nadjust for multiple comparisons, with the significance level set at alpha <0.0125. All statistical evaluations\nwere performed using the R statistical software, specifically version 3.5.1 (R Foundation for Statistical\nComputing, Vienna, Austria).\nResults\nA total of 163 questions were queried by each LLM. GPT-4 scored 163/163 (100%), GPT-3.5 scored 134/163\n(82.21%), Claude scored 138/163 (84.66%), and Bard scored 123/163 (75.46%) (Figure \n1\n). In terms of total\nperformance, GPT-4 significantly outperformed GPT-3.5 by 17.8% (p\n \n<\n \n0.001), Claude by 15.3% (p\n \n<\n \n0.001),\nand Bard by 24.5% (p\n \n<\n \n0.001). No significant difference was observed between the total performance of GPT-\n3.5, Claude, and Bard. \n2024 Abbas et al. Cureus 16(3): e55991. DOI 10.7759/cureus.55991\n2\n of \n7\nFIGURE\n 1: Overall performance of the LLMs on all NBME sample\nquestions\nLLM: large language model, NBME: National Board of Medical Examiners\nGPT-4 scored 100% on all subject exams. GPT-3.5 performed best on the medicine and obstetrics/gynecology\nexams (18/20) and lowest on the family medicine exam (3/5). Claude performed best on the surgery exam\n(19/20) and lowest on the ambulatory care exam (15/20). Bard performed best on the surgery and neurology\nexams (17/20) and lowest on the family medicine exam (3/5). Comprehensive results are depicted in Table \n1\nand Figure \n2\n. \n \nInternal Medicine\nObstetrics and Gynecology \nPediatrics\nPsychiatry \nSurgery\nFamily Medicine\nClinical Neurology\nAmbulatory Care\nComprehensive Clinical Science \nGPT-4\n20\n20\n19\n20\n20\n5\n20\n20\n19\nGPT-3.5\n18\n18\n16\n17\n17\n3\n17\n14\n14\nBard\n14\n15\n12\n16\n17\n3\n17\n13\n16\nClaude\n15\n18\n16\n17\n19\n4\n18\n15\n16\nTotal Questions\n20\n20\n19\n20\n20\n5\n20\n20\n19\nTABLE\n 1: Number of questions answered correctly by the LLMs on each subject exam\nLLM: large language model\n2024 Abbas et al. Cureus 16(3): e55991. DOI 10.7759/cureus.55991\n3\n of \n7\nFIGURE\n 2: Performance of the LLMs on each subject exam\nLLM: large language model\nGPT-4 significantly outperformed Bard on medicine, pediatrics, family medicine, and ambulatory care (p\n \n<\n \n0.01). GPT-4 also significantly outperformed GPT-3.5 on ambulatory care and family medicine (p <\n \n0.01).\nThere was no statistically significant difference in the performance of GPT 3.5, Claude, and Bard among the\ndifferent subject exams. The highest average score of all LLMs on a single subject exam was on the surgery\nexam (18.25/20), and the lowest average score of all LLMs on a single subject exam was on the family\nmedicine exam (3.75/5). The average score received across all LLMs for each subject exam is depicted in\nTable \n2\n.\n \nMedicine\nObstetrics and Gynecology\nPediatrics\nPsychiatry \nSurgery\nFamily Medicine\nClinical Neurology\nAmbulatory Care\nComprehensive Clinical Science \nAverage Score\n16.75/20\n17.75/20\n15.75/19\n17.5/20\n18.25/20\n3.75/5\n18/20\n15.5/20\n16.25/19\nAverage Percent Correct\n83.75%\n88.75%\n82.89%\n87.50%\n91.25%\n75%\n90%\n77.50%\n85.53%\nTABLE\n 2: Average score across all the LLMs on each NBME subject exam\nLLM: large language model, NBME: National Board of Medical Examiners\nDiscussion\nThis study presents a comprehensive analysis comparing the performance of prominent LLMs on sample\nquestions from the NBME clinical subject exams. GPT-4 performed the best, achieving a perfect score of\n100% across all questions. One potential reason for GPT-4's standout performance may be its vast training\ndata, exceeding 45 terabytes by September 2021 \n[14]\n. The model, despite not being specifically fine-tuned for\nmedical data, demonstrated an ability to respond to intricate medical queries \n[15]\n. OpenAI has vast amounts\nof data on its website that showcases GPT-4 outperforming GPT-3.5 across different disciplines, including\nlaw, language, math, and social and political studies \n[14]\n. The findings of this study expand on the data\nprovided by OpenAI to include GPT-4's superiority over GPT-3.5 in answering questions stemming from\nclinical vignettes related to various subjects within medicine. \nGPT-3.5, Bard, and Claude, despite their respectable performances, were outpaced by GPT-4. This could hint\nat the importance of the sheer volume and diversity of training data seen in GPT-4. However, it is also\nessential to consider other factors like model architecture, fine-tuning strategies, and the nature of the\nquestions in the exams \n[16]\n. The results underline the potential of LLMs in medical education and practice.\nTheir capabilities in answering intricate medical questions could pave the way for innovative applications in\nclinical decision support, research, and education. Although this study provides support for the use of GPT-4\nover other LLMs in the everyday practice of students and educators, it is the only LLM in this study that is\n2024 Abbas et al. Cureus 16(3): e55991. DOI 10.7759/cureus.55991\n4\n of \n7\nnot free to use, which could be a barrier to the use by a large proportion of students \n[17]\n.\nThe remarkable performance of GPT-4 in responding to medical queries underscores the growing\nsignificance of LLMs in healthcare, resonating with emerging literature on AI applications in diagnostics\nand clinical practice. Recent studies have detailed the landscape of FDA-approved AI devices and\nalgorithms, suggesting an accelerating integration of AI in medical diagnostics and patient care and\nreflecting a broader trend of digital transformation in healthcare \n[18]\n. Similarly, deep learning models have\nimproved diagnostic accuracies in breast cancer screening, highlighting the potential of this technology to\nenhance the precision of medical diagnoses alongside human experts \n[19,20]\n. Moreover, the transformative\nimpact of AI extends beyond diagnostics. Recent work has shown that AI can interpret complex medical\ndata, such as echocardiograms and cardiac function assessments, with high accuracy, suggesting its utility in\ncardiology\n \n[21,22]\n. Furthermore, studies have shown that AI can support early lung cancer detection,\npotentially improving patient outcomes \n[23]\n. These advancements signify a shift toward more AI-integrated\nmedical practices, promising improved efficiencies and patient outcomes across various medical specialties.\nFuture research on the application of LLMs in medicine should aim to deepen our understanding of their\npotential and limitations. This includes exploring a broader array of AI models, particularly those\nspecialized in distinct medical fields or capable of interpreting complex medical imagery, to uncover models\nthat excel in specific areas. Expanding the diversity of medical subjects on which LLMs are trained is also\nvital, as studies have shown that current algorithmic-based systems may reinforce gender biases and affect\nmarginalized communities in healthcare-related applications \n[24]\n. Longitudinal studies would also be\nvaluable in tracking the progress and adaptation of LLMs over time, shedding light on their learning curves,\nimprovements in accuracy, and ability to integrate new medical knowledge. Such research efforts will not\nonly enhance the utility of LLMs in clinical and educational settings but also contribute to safer, more\neffective healthcare delivery.\nWhile LLMs display promise, it is vital to approach their use with caution. LLMs can occasionally provide\nanswers with unwarranted confidence, even if incorrect \n[25]\n. Therefore, while they can be invaluable tools,\nclinicians and students must critically evaluate their responses. Any implementation of LLMs in a clinical\nsetting must prioritize patient safety and the validation of the information provided \n[26,27]\n.\nIn addition to the demonstrated capabilities of LLMs, it is pivotal to acknowledge this study's limitations.\nNotably, the absence of sample questions from the NBME featuring an image or other media components,\nprevalent in real exams, may pose a limitation to the external validity of our findings. Our sample size was\nalso limited due to a limited number of publicly available NBME sample questions, which can affect the\ngeneralizability of our results. In addition, our study did not explore the efficiency of LLMs when exposed to\nreal-time clinical scenarios, which often encompass multifaceted clinical reasoning and not just recall-based\nknowledge \n[28]\n. Moreover, while GPT-4's extensive training data undoubtedly contributes to its superiority,\nthe inability to assess it against niche, specialized medical models could limit our understanding of its true\npotential within the medical field. Lastly, as technological advancements march forward, it is worth noting\nthat models like GPT-4 might become outdated, emphasizing the continuous need for reassessment and\nadaptation in this rapidly advancing field.\nConclusions\nGPT-4 demonstrated superior performance, achieving a remarkable 100% accuracy rate in responding to\nsample questions from NBME clinical subject exams, significantly surpassing other LLMs, such as GPT-3.5,\nClaude, and Bard. Overall, the LLMs performed best on sample questions from the surgery clinical subject\nexam and worst on sample questions from the family medicine clinical subject exam.\nDespite these promising results, the study emphasizes the necessity for cautious integration of LLMs into\nmedical practice and education. Given the rapid evolution of AI technologies and their potential\nimplications for patient safety and information validity, critical appraisal by healthcare professionals and\neducators is imperative. The study acknowledges limitations, including the exclusion of image-based\nquestions and reliance on a limited pool of publicly available questions, potentially affecting the findings'\ngeneralizability.\nThis research contributes significantly to the literature on AI in healthcare, outlining both the capabilities\nand challenges associated with the use of LLMs in medical education and practice. It underscores the\ntransformative potential of AI in the medical domain while advocating for a balanced approach to ensure the\nethical and safe application of these technologies, underscoring the need for continuous evaluation and\nvalidation to effectively harness the full potential of LLMs in healthcare.\nAdditional Information\nAuthor Contributions\nAll authors have reviewed the final version to be published and agreed to be accountable for all aspects of the\nwork.\n2024 Abbas et al. Cureus 16(3): e55991. DOI 10.7759/cureus.55991\n5\n of \n7\nConcept and design:\n  \nAli Abbas, Mahad S. Rehman, Syed S. Rehman\nAcquisition, analysis, or interpretation of data:\n  \nAli Abbas, Mahad S. Rehman, Syed S. Rehman\nDrafting of the manuscript:\n  \nAli Abbas, Mahad S. Rehman, Syed S. Rehman\nCritical review of the manuscript for important intellectual content:\n  \nAli Abbas, Mahad S. Rehman,\nSyed S. Rehman\nDisclosures\nHuman subjects:\n All authors have confirmed that this study did not involve human participants or tissue.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n All authors have declared that no financial support was received from\nany organization for the submitted work. \nFinancial relationships:\n All authors have declared that they have\nno financial relationships at present or within the previous three years with any organizations that might\nhave an interest in the submitted work. \nOther relationships:\n All authors have declared that there are no\nother relationships or activities that could appear to have influenced the submitted work.\nAcknowledgements\nAli Abbas and Mahad Rehman contributed equally to the work and should be considered co-first authors.\nReferences\n1\n. \nLiu PR, Lu L, Zhang JY, Huo TT, Liu SX, Ye ZW: \nApplication of artificial intelligence in medicine: an\noverview\n. Curr Med Sci. 2021, 41:1105-15. \n10.1007/s11596-021-2474-3\n2\n. \nGarcia-Vidal C, Sanjuan G, Puerta-Alcalde P, Moreno-García E, Soriano A: \nArtificial intelligence to support\nclinical decision-making processes\n. EBioMedicine. 2019, 46:27-9. \n10.1016/j.ebiom.2019.07.019\n3\n. \nEllahham S: \nArtificial intelligence: the future for diabetes care\n. Am J Med. 2020, 133:895-900.\n10.1016/j.amjmed.2020.03.033\n4\n. \nSinghal K, Azizi S, Tu T, et al.: \nLarge language models encode clinical knowledge\n. Nature. 2023, 620:172-80.\n10.1038/s41586-023-06291-2\n5\n. \nNaveed H, Khan AU, Qiu S, et al.: \nA comprehensive overview of large language models\n. arXiv. 2023,\n10.48550/arXiv.2307.06435\n6\n. \nKung TH, Cheatham M, Medenilla A, et al.: \nPerformance of ChatGPT on USMLE: potential for AI-assisted\nmedical education using large language models\n. PLOS Digit Health. 2023, 2:e0000198.\n10.1371/journal.pdig.0000198\n7\n. \nGuerra GA, Hofmann H, Sobhani S, et al.: \nGPT-4 artificial intelligence model outperforms ChatGPT, medical\nstudents, and neurosurgery residents on neurosurgery written board-like questions\n. World Neurosurg. 2023,\n179:e160-5. \n10.1016/j.wneu.2023.08.042\n8\n. \nKung JE, Marshall C, Gauthier C, Gonzalez TA, Jackson JB 3rd: \nEvaluating ChatGPT performance on the\northopaedic in-training examination\n. JB JS Open Access. 2023, 8:\n10.2106/JBJS.OA.23.00056\n9\n. \nOh N, Choi GS, Lee WY: \nChatGPT goes to the operating room: evaluating GPT-4 performance and its\npotential in surgical education and training in the era of large language models\n. Ann Surg Treat Res. 2023,\n104:269-73. \n10.4174/astr.2023.104.5.269\n10\n. \nBrin D, Sorin V, Vaid A, et al.: \nComparing ChatGPT and GPT-4 performance in USMLE soft skill assessments\n.\nSci Rep. 2023, 13:16492. \n10.1038/s41598-023-43436-9\n11\n. \nTracy BM, Hazen BJ, Ward CR, Winer JH, Pettitt BJ: \nSustained clinical performance during surgical rotations\npredicts NBME shelf exam outcomes\n. J Surg Educ. 2020, 77:e116-20. \n10.1016/j.jsurg.2020.06.033\n12\n. \nKossoff EH, Hubbard TW, Gowen CW Jr: \nEarly clinical experience enhances third-year pediatrics clerkship\nperformance\n. Acad Med. 1999, 74:1238-41. \n10.1097/00001888-199911000-00019\n13\n. \nClinical Science Subject Exams\n. (2023). Accessed: 11/04/2023: \nhttps://www.nbme.org/assessment-\nproducts/assess-learn/subject-exams/clinical-science\n.\n14\n. \nGPT-4\n. (2023). Accessed: 10/24/2023: \nhttp://openai.com/research/gpt-4\n.\n15\n. \nSallam M, Salim NA, Barakat M, et al.: \nChatGPT applications \n​\nin medical, dental, pharmacy, and public health\neducation: a descriptive study \n​\nhighlighting the advantages and limitations\n. Narra J. 2023, 3:103-10.\n10.52225/narra.v3i1.103\n16\n. \nBorji A, Mohammadian M: \nBattle of the Wordsmiths: comparing ChatGPT, GPT-4, Claude, and Bard\n. Soc Sci\nRes Net. 2023, \n10.2139/ssrn.4476855\n17\n. \nIntroducing ChatGPT Plus\n. (2023). Accessed: 10/24/2023: \nhttps://openai.com/blog/chatgpt-plus\n.\n18\n. \nBenjamens S, Dhunnoo P, Meskó B: \nThe state of artificial intelligence-based FDA-approved medical devices\nand algorithms: an online database\n. NPJ Digit Med. 2020, 3:118. \n10.1038/s41746-020-00324-0\n19\n. \nWu N, Phang J, Park J, et al.: \nDeep neural networks improve radiologists' performance in breast cancer\nscreening\n. IEEE Trans Med Imaging. 2020, 39:1184-94. \n10.1109/TMI.2019.2945514\n20\n. \nMcKinney SM, Sieniek M, Godbole V, et al.: \nInternational evaluation of an AI system for breast cancer\nscreening\n. Nature. 2020, 577:89-94. \n10.1038/s41586-019-1799-6\n21\n. \nGhorbani A, Ouyang D, Abid A, et al.: \nDeep learning interpretation of echocardiograms\n. NPJ Digit Med.\n2020, 3:10. \n10.1038/s41746-019-0216-8\n22\n. \nOuyang D, He B, Ghorbani A, et al.: \nVideo-based AI for beat-to-beat assessment of cardiac function\n. Nature.\n2020, 580:252-6. \n10.1038/s41586-020-2145-8\n2024 Abbas et al. Cureus 16(3): e55991. DOI 10.7759/cureus.55991\n6\n of \n7\n23\n. \nArdila D, Kiraly AP, Bharadwaj S, et al.: \nEnd-to-end lung cancer screening with three-dimensional deep\nlearning on low-dose chest computed tomography\n. Nat Med. 2019, 25:954-61. \n10.1038/s41591-019-0447-x\n24\n. \nFosch-Villaronga E, Drukarch H, Khanna P, Verhoef T, Custers B: \nAccounting for diversity in AI for\nmedicine\n. Comput Law Secur Rev. 2022, 47:105735. \n10.1016/j.clsr.2022.105735\n25\n. \nUz C, Umay E: \n\"Dr ChatGPT\": Is it a reliable and useful source for common rheumatic diseases?\n. Int J Rheum\nDis. 2023, 26:1343-9. \n10.1111/1756-185X.14749\n26\n. \nMeskó B, Topol EJ: \nThe imperative for regulatory oversight of large language models (or generative AI) in\nhealthcare\n. NPJ Digit Med. 2023, 6:120. \n10.1038/s41746-023-00873-0\n27\n. \nSchwartz IS, Link KE, Daneshjou R, Cortés-Penfield N: \nBlack box warning: large language models and the\nfuture of infectious diseases consultation\n. Clin Infect Dis. 2023, \n10.1093/cid/ciad633\n28\n. \nKelly CJ, Karthikesalingam A, Suleyman M, Corrado G, King D: \nKey challenges for delivering clinical impact\nwith artificial intelligence\n. BMC Med. 2019, 17:195. \n10.1186/s12916-019-1426-2\n2024 Abbas et al. Cureus 16(3): e55991. DOI 10.7759/cureus.55991\n7\n of \n7",
  "topic": "Subject (documents)",
  "concepts": [
    {
      "name": "Subject (documents)",
      "score": 0.5036115050315857
    },
    {
      "name": "Sample (material)",
      "score": 0.44456547498703003
    },
    {
      "name": "Medical education",
      "score": 0.4123741686344147
    },
    {
      "name": "Psychology",
      "score": 0.40142953395843506
    },
    {
      "name": "Medicine",
      "score": 0.3997631371021271
    },
    {
      "name": "Family medicine",
      "score": 0.3890146315097809
    },
    {
      "name": "Computer science",
      "score": 0.14694169163703918
    },
    {
      "name": "Library science",
      "score": 0.12964099645614624
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ]
}