{
  "title": "MATE: Multi-view Attention for Table Transformer Efficiency",
  "url": "https://openalex.org/W3197798882",
  "year": 2021,
  "authors": [
    {
      "id": null,
      "name": "Julian Eisenschlos",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2903211142",
      "name": "Maharshi Gor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1775435747",
      "name": "Thomas Müller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989912291",
      "name": "William Cohen",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3103940211",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3124424060",
    "https://openalex.org/W2970609357",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963643655",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2995744795",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2964224049",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W2612228435",
    "https://openalex.org/W2996657743",
    "https://openalex.org/W3106009088",
    "https://openalex.org/W2140602286",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W4287214436",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3103667349",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3099873751",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3167748596",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2161002933",
    "https://openalex.org/W3173197792",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W3045733172"
  ],
  "abstract": "This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7606–7619\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n7606\nMATE: Multi-view Attention for Table Transformer Efﬁciency\nJulian Martin Eisenschlos1, Maharshi Gor2∗, Thomas Müller3∗, William W. Cohen1\nGoogle Research1\n{eisenjulian,wcohen}@google.com\nDept. of Computer Science, University of Maryland2\nmgor@cs.umd.edu\nSymanto Research, Valencia, Spain3\nthomas.mueller@symanto.com\nAbstract\nThis work presents a sparse-attention Trans-\nformer architecture for modeling documents\nthat contain large tables. Tables are ubiquitous\non the web, and are rich in information. How-\never, more than 20% of relational tables on the\nweb have 20 or more rows (Cafarella et al.,\n2008), and these large tables present a chal-\nlenge for current Transformer models, which\nare typically limited to 512 tokens. Here we\npropose M ATE, a novel Transformer architec-\nture designed to model the structure of web ta-\nbles. M ATE uses sparse attention in a way that\nallows heads to efﬁciently attend to either rows\nor columns in a table. This architecture scales\nlinearly with respect to speed and memory, and\ncan handle documents containing more than\n8000 tokens with current accelerators. M ATE\nalso has a more appropriate inductive bias for\ntabular data, and sets a new state-of-the-art\nfor three table reasoning datasets. For H Y-\nBRID QA (Chen et al., 2020b), a dataset that in-\nvolves large documents containing tables, we\nimprove the best prior result by 19 points.\n1 Introduction\nThe Transformer architecture (Vaswani et al., 2017)\nis expensive to train and run at scale, especially for\nlong sequences, due to the quadratic asymptotic\ncomplexity of self-attention. Although some work\naddresses this limitation (Ainslie et al., 2020; Ki-\ntaev et al., 2020; Zaheer et al., 2020), there has been\nlittle prior work on scalable Transformer architec-\ntures for semi-structured text.1 However, although\nsome of the more widely used benchmark tasks in-\nvolving semi-structured data have been restricted to\nmoderate size tables, many semi-structured docu-\nments are large: more than 20% of relational tables\non the web have 20 or more rows (Cafarella et al.,\n∗Work done at Google Research.\n1The term \"semi-structured text\" refers to text that has\nstructure that does not reﬂect a known data schema. Typically\nsemi-structured text is organized as an HTML tree or variable\nlength lists and tables.\nPick # Player College\n27 Tom Wilfrid Laurier\n28 Mark York\n29 Pedro California\n30 Rob York\n... ... ...\nQuery: Which college got the most players?\nColumn Head\nRow Head\nFigure 1: Sparse self-attention heads on tables in MATE\nare of two classes: Row heads attend to tokens inside\ncells in the same row, as well as the query. Column\nheads attend to tokens in the same column and in the\nquery. Query tokens attend to all other tokens.\n2008), and would pose a problem for typical Trans-\nformer models.\nHere we study how efﬁcient implementations for\ntransformers can be tailored to semi-structured data.\nFigure 1 highlights our main motivation through an\nexample: to obtain a contextual representation of a\ncell in a table, it is unlikely that the information in\na completely different row and column is needed.\nWe propose the MATE architecture2 (Section 3),\nwhich allows each attention head to reorder the\ninput so as to traverse the data by multiple points of\nview, namely column or row-wise (Figure 2). This\nallows each head to have its own data-dependent\nnotion of locality, which enables the use of sparse\nattention in an efﬁcient and context-aware way.\nThis work focuses on question answering (QA)\nand entailment tasks on tables. While we apply\nour model to several such tasks (see section 6),\nHYBRID QA (Chen et al., 2020b) is particularly\ninteresting, as it requires processing tables jointly\nwith long passages associated with entities men-\ntioned in the table, yielding large documents that\nmay not ﬁt in standard Transformer models.\n2Pronounced mah-teh, as in mate tea.\n7607\nFlattened table tokens\nColumn IDs\nSorted by column\nColumn Buckets\nAttention within \nneighbor buckets\ngather(·,argsort(column_ids))\nreshape(·,[...,bucket_size])\nattention(·,concat([\n  roll(·,shift=-1,axis=-2),\n  roll(·,shift=+1,axis=-2),\n  ·,\n],axis=-1))\nFigure 2: Efﬁcient implementation for M ATE. Each attention head reorders the tokens by either column or row\nindex and then applies a windowed attention mechanism. This ﬁgure omits the global section that attends to and\nfrom all other tokens. Since column/row order can be pre-computed, the method is linear for a constant block size.\nOverall, our contributions are the following:\ni) We show that table transformers naturally fo-\ncus attention according to rows and columns, and\nthat constraining attention to enforce this improves\naccuracy on three table reasoning tasks, yielding\nnew state-of-the-art results in SQA and TABFACT.\nii) We introduce MATE, a novel transformer ar-\nchitecture that exploits table structure to allow run-\nning training and inference in longer sequences.\nUnlike traditional self-attention, MATE scales lin-\nearly in the sequence length.\niii) We propose POINT R (Section 4), a novel\ntwo-phase framework that exploits MATE to tackle\nlarge-scale QA tasks, like HYBRID QA, that require\nmulti-hop reasoning over tabular and textual data.\nWe improve the state-of-the-art by 19 points.\nAll the code is available as open source.3\n2 Related Work\nTransformers for tabular data Traditionally,\ntasks involving tables were tackled by searching for\nlogical forms in a semantic parsing setting. More\nrecently Transformers (Vaswani et al., 2017) have\nbeen used to train end-to-end models on tabular\ndata as well (Chen et al., 2020a). For example,\nTAPAS (Herzig et al., 2020) relies on Transformer-\nbased masked language model pre-training and spe-\ncial row and column embeddings to encode the ta-\nble structure. Chen et al. (2021) use a variant of\nETC (Ainslie et al., 2020) on an open-domain ver-\nsion of HYBRID QA to read and choose an answer\nspan from multiple candidate passages and cells,\nbut the proposed model does not jointly process the\n3github.com/google-research/tapas\nfull table with passages.\nIn order to overcome the limitations on sequence\nlength Eisenschlos et al. (2020) propose heuristic\ncolumn selection techniques, and they also pro-\npose pre-training on synthetic data. Krichene et al.\n(2021) propose a model based cell selection tech-\nnique that is differentiable and trained end-to-end\ntogether with the main task model. Our approach\nis orthogonal to these methods, and can be usefully\ncombined with them, as shown in Table 4.\nRecently, Zhang et al. (2020) proposed SAT,\nwhich uses an attention mask to restrict attention to\ntokens in the same row and same column.SAT also\ncomputes an additional histogram row appended\nat the bottom of the table and encodes the table\ncontent as text only (unlike TAPAS). The proposed\nmethod is not head dependent as ours is, which pre-\nvents it from being implemented efﬁciently to allow\nscaling to larger sequence lengths. Controlling for\nmodel size and pre-training for a fair comparison,\nwe show that our model is both faster (Table 4) and\nmore accurate (Table 6) than SAT.\nEfﬁcient Transformers There is prior work that\ntries to improve the asymptotic complexity of the\nself-attention mechanism in transformers. Tay et al.\n(2020) review the different methods and cluster\nthem based on the nature of the approach. We\ncover some of the techniques below and show a\ntheoretical complexity comparison in Table 1.\nThe LINFORMER model Wang et al. (2020) uses\nlearned projections to reduce the sequence length\naxis of the keys and value vectors to a ﬁxed length.\nThe projections are then anchored to a speciﬁc\ninput length which makes adapting the sequence\n7608\nModel Complexity Class\nTransformer-XL O(n2) RC\nREFORMER O(nlog n) LP\nLINFORMER O(n) LR\nBIGBIRD O(ng+ nb) FP+RP+M\nETC O(ng+ nb) FP+M\nMATE (Ours) O(ng+ nb) FP\nTable 1: Comparison of transformer models follow-\ning Tay et al. (2020). Class abbreviations include: FP\n= Fixed Patterns, RP = Random Patterns, M = Memory,\nLP = Learnable Pattern, LR = Low Rank and RC = Re-\ncurrence. The block size for local attention is denoted\nas band gthe size of the global memory. For our MATE\nmodel, a nlog nsorting step can be pre-computed be-\nfore training or inference for known tables so it is omit-\nted.\nlength during pre-training and ﬁne-tuning challeng-\ning, and makes the model more sensitive to position\noffsets in sequences of input tokens.\nREFORMER (Kitaev et al., 2020) uses locality\nsensitive hashing to reorder the input tokens at ev-\nery layer in such a way that similar contextual em-\nbeddings have a higher chance of being clustered\ntogether. We instead rely on the input data struc-\nture to deﬁne ways to cluster the tokens. Although\nthe limitation can be circumvented by adapting the\nproposed architecture, REFORMER was originally\ndeﬁned for auto-regressive training.\nAinslie et al. (2020) introduce ETC , a frame-\nwork for global memory and local sparse attention,\nand use the mechanism of relative positional atten-\ntion (Dai et al., 2019) to encode hierarchy. ETC\nwas applied to large document tasks such as Nat-\nural Questions (Kwiatkowski et al., 2019). The\nmethod does not allow different dynamic or static\ndata re-ordering. In practice, we have observed that\nthe use of relative positional attention introduces a\nlarge overhead during training. BIGBIRD (Zaheer\net al., 2020) presents a similar approach with the\naddition of attention to random tokens.\n3 The M ATE model\nFollowing TAPAS (Herzig et al., 2020), the trans-\nformer input in MATE, for each table-QA example,\nis the query and the table, tokenized and ﬂattened,\nseparated by a [SEP] token, and preﬁxed by a\n[CLS]. Generally the table comprises most of the\nthe input. We use the same row, column and rank\nembeddings as TAPAS.\nTo restrict attention between the tokens in the\ntable, we propose having some attention heads\nlimited to attending between tokens in the same\nrow (plus the non-table tokens), and likewise for\ncolumns. We call these row heads and column\nheads respectively. In both cases, we allow atten-\ntion to and from all the non-table tokens.\nFormally, if X ∈Rd×n is the input tensor for a\nTransformer layer with sequence length n, the k-th\nposition of the output of the i-th attention head is:\nHeadi\nk (X) = Wi\nV XAi\nk\nσ\n[(\nWi\nKXAi\nk\n)⊺\nWi\nQXk\n]\nwhere Wi\nQ,Wi\nK,Wi\nV ∈Rm×d are query, key and\nvalue projections respectively,σis the softmax\noperator, and Ai\nk ⊆{1,··· ,n}represents the set\nof tokens that position kcan attend to, also known\nas the attention pattern. Here XAi\nk\ndenotes gather-\ning from Xonly the indexes in Ai\nk. When Ai\nk con-\ntains all positions (except padding) for all heads i\nand token index kthen we are in the standard dense\ntransformer case. For a token position k, we deﬁne\nrk,ck ∈N0 the row and column number, which is\nset to 0 if kbelongs to the query set Q: the set of\ntoken positions in the query text including [CLS]\nand [SEP] tokens.\nIn MATE, we use two types of attention patterns.\nThe ﬁrst hr ≥0 heads are row heads and the re-\nmaining hc are column heads:\nAi\nk =\n\n\n\n{1,··· ,n} if k∈Q, else\nQ∪{j : rj = rk} if 1 ≤i≤hr\nQ∪{j : cj = ck} otherwise.\nOne possible implementation of this is an at-\ntention mask that selectively sets elements in the\nattention matrix to zero. (Similar masks are used\nfor padding tokens, or auto-regressive text genera-\ntion.) The ratio of row and column heads is a hyper-\nparameter but empirically we found a 1 : 1 ratio\nto work well. In Section 6 we show that attention\nmasking improves accuracy on four table-related\ntasks. We attribute these improvements to better in-\nductive bias, and support this in Section 7 showing\nthat full attention models learn to approximate this\nbehavior.\n3.1 Efﬁcient implementation\nAlthough row- and column-related attention mask-\ning improves accuracy, it does not improve Trans-\nformer efﬁciency—despite the restricted attention,\nthe Transformer still uses quadratic memory and\ntime. We thus also present an approximation of\nrow and column heads that can be implemented\n7609\nmore efﬁciently. Inspired by Ainslie et al. (2020),\nthe idea is to divide the input into a global part of\nlength Gthat attends to and from everything, and\na local (typically longer) part that attends only to\nthe global section and some radius Raround each\ntoken in the sequence. ETC does this based on a\nﬁxed token order. However, the key insight used in\nMATE is that the notion of locality can be conﬁg-\nured differently for each head: one does not need to\nchoose a speciﬁc traversal order for tokens ahead\nof time, but instead tokens can be ordered in a data-\ndependent (but deterministic) way. In particular,\nrow heads can order the input according to a row\norder traversal of the table, and column heads can\nuse a column order traversal. The architecture is\nshown in Figure 2.\nAfter each head has ordered its input we split off\nthe ﬁrst Gtokens and group the rest in evenly sized\nbuckets of length R. By reshaping the input matrix\nin the self-attention layer to have R as the last\ndimension, one can compute attention scores from\neach bucket to itself, or similarly from each bucket\nto an adjacent one. Attention is further restricted\nwith a mask to ensure row heads and column heads\ndon’t attend across rows and columns respectively.\nSee model implementation details in Appendix D.\nWhen Gis large enough to contain the question part\nof the input and Ris large enough to ﬁt an entire\ncolumn or row, then the efﬁcient implementation\nmatches the mask-based one.\nAs observed in Ainslie et al. (2020), asymptotic\ncomplexity improvements often do not materialize\nfor small sequence lengths, given the overhead of\ntensor reshaping and reordering. The exact break-\neven point will depend on several factors, including\naccelerator type and size as well as batch size. In\nthe experiments below the best of the two function-\nally equivalent implementations ofMATE is chosen\nfor each use case.\n3.2 Compatibility with B ERT weights\nThe sparse attention mechanism of MATE adds no\nadditional parameters. As a consequence, a MATE\ncheckpoint is compatible with any BERT or TAPAS\npre-trained checkpoint. Following Herzig et al.\n(2020) we obtained best results running the same\nmasked language model pre-training used inTAPAS\nwith the same data but using the sparse attention\nmask of MATE.\nFor sequence lengths longer than 512 tokens,\nwe reset the index of the positional embeddings at\nSplit Train Dev Test Total\nIn-Passage 35,215 2,025 20,45 39,285\nIn-Table 26,803 1,349 1,346 29,498\nMissing 664 92 72 828\nTotal 62,682 3,466 3,463 69,611\nTable 2: Statistics for H YBRID QA. In-Table and In-\nPassage groups mark the location of the answer. Miss-\ning denotes answers that do not match any span and\nmay require complex computations.\nthe beginning of each cell. This method removes\nthe need to learn positional embeddings for larger\nindexes as the maximum sequence length grows\nwhile avoiding the large computational cost of rela-\ntive positional embeddings.\n3.3 Universal approximators\nYun et al. (2020a) showed that Transformers\nare universal approximators for any continuous\nsequence-to-sequence function, given sufﬁcient lay-\ners. This result was further extended by Yun et al.\n(2020b); Zaheer et al. (2020) to some Sparse Trans-\nformers under reasonable assumptions.\nHowever, prior work limits itself to the case of\na single attention pattern per layer, whereas MATE\nuses different attention patterns depending on the\nhead. We will show that MATE is also a universal\napproximator for sequence to sequence functions.\nFormally, let Fbe the class of continuous func-\ntions f : D ⊂Rd×n →Rd×n with D compact,\nwith the p-norm ||·||p. Let TMATE be any family\nof transformer models with a ﬁxed set of hyper-\nparameters (number of heads, hidden dimension,\netc.) but with an arbitrary number of layers. Then\nwe have the following result.\nTheorem 1. If the number of heads is at least 3\nand the hidden size of the feed forward layer is at\nleast 4, then for any f ∈F and ϵ∈R+ there exists\nˆf ∈TMATE such that ||ˆf −f||p <ϵ.\nSee the Appendix C for a detailed proof, which\nrelies on the fact that3 heads will guarantee at least\ntwo heads of the same type. The problem can then\nbe reduced to the results of Yun et al. (2020b).\n4 The P OINT R architecture\nMany standard table QA datasets (Pasupat and\nLiang, 2015; Chen et al., 2020a; Iyyer et al., 2017),\nperhaps by design, use tables that can be limited to\n512 tokens. Recently, more datasets (Kardas et al.,\n2020; Talmor et al., 2021) requiring parsing larger\nsemi-structured documents have been released.\n7610\nOriginal Input Table:\nPos No Driver Constructor Time Gap\n1 2 Rubens Barrichello Ferrari 1:10.223 -\n2 1 Michael Schumacher Ferrari 1:10.400 +0.177\n3 10 Takuma Sato Honda 1:10.601 +0.378\n4 9 Jenson Button ⋆ Honda 1:10.820 +0.597\nEntity Descriptions (some are omitted for space):\n• Rubens Barrichello is a Brazilian racing driver who competed in Formula One between 1993 and 2011.\n• Jenson Alexander Lyons Button MBE is a British racing driver. He won the 2009 F1 World Championship.\n• Scuderia Ferrari S.p.A. is the racing division of luxury Italian auto manufacturer Ferrari. Ferrari supplied cars\ncomplete with V8 engines for the A1 Grand Prix series from the 2004 season.\n• Honda Motor Company, Ltd is a Japanese public multinational conglomerate manufacturer of automobiles,\nmotorcycles, and power equipment, headquartered in Minato, Tokyo, Japan.\n...\nQuestion:\nThe driver who ﬁnished in position 4 in the 2004 Grand Prix was of what nationality? British⇓\nExpanded Table with kDescription Sentences Most Similar to the Question:\nPos No Driver Constructor Time Gap\n1 2 Rubens Barrichello Ferrari (Ferrari supplied cars complete with V8 engines\nfor the A1 Grand Prix series from the 2004 season.)\n1:10.223 -\n2 1 Michael Schumacher Ferrari (Ferrari supplied cars complete with V8 engines\nfor the A1 Grand Prix series from the 2004 season.)\n1:10.400 +0.177\n3 10 Takuma Sato Honda 1:10.601 +0.378\n4 9 Jenson Button (Jenson Alexander Lyons Button\nMBE is aBritishracing driver.)⋆\nHonda 1:10.820 +0.597\nPOINT R inference pipeline:\nExpand table cells content \nwith entity descriptions\nCell selection model picks a \ncell that contains answer\nKeep entity descriptions and \ncontent of the selected cell\nReader model extracts \na span with the answer\nFigure 3: An example from the H YBRID QA dataset processed by P OINT R. The ﬁrst paragraph in the Wikipedia\npage for each underlined entity was available to the dataset authors. We expand the text in the cells with this\ndescriptions for the top-kmost relevant sentences, as shown in the second table, and train a model to ﬁnd the cell\ncontaining or linking to the answer (marked here with a ⋆). The goal is to provide the model with all the context\nneeded to locate the answer. A second model extracts a span from the selected cell content and linked text.\nAmong them, we focus on HYBRID QA (Chen\net al., 2020b). It uses Wikipedia tables with entity\nlinks, with answers taken from either a cell or a\nhyperlinked paragraph. Dataset statistics are shown\nin Table 2. Each question contains a table with\non average 70 cells and 44 linked entities. Each\nentity is represented by the ﬁrst 12 sentences of\nthe Wikipedia description, averaging 100 tokens.\nThe answer is often a span extracted from the table\nor paragraphs but the dataset has no ground truth\nannotations on how the span was obtained, leaving\naround 50% of ambiguous examples where more\nthan one answer-sources are possible. The total\nrequired number of word pieces accounting for\nthe table, question and entity descriptions grows\nto more than 11,000 if one intends to cover more\nthan 90% of the examples, going well beyond the\nlimit of traditional transformers.\nTo apply sparse transformers to the HYBRID QA\ntask, we propose POINT R, a two stage framework\nin a somewhat similar fashion to open domain\nQA pipelines (Chen et al., 2017; Lee et al., 2019).\nWe expand the cell content by appending the de-\nscriptions of its linked entities. The two stages of\nPOINT R correspond to (Point)ing to the correct\nexpanded cell and then (R)eading a span from it.\nSee Figure 3 for an example. Full set-up details are\ndiscussed in Appendix A.\n4.1 P OINT R: Cell Selection Stage\nIn the ﬁrst stage we train a cell selection model us-\ning MATE whose objective is to select the expanded\ncell that contains the answer. MATE accepts the\nfull table as input; therefore, expanding all the cells\nwith their respective passages is impractical. In-\nstead, we consider the top-ksentences in the entity\ndescriptions for expansion, using a TF-IDF metric\nagainst the query. Using k = 5, we can ﬁt 97%\nof the examples in 2048 tokens; for the remaining\nexamples, we truncate the longest cells uniformly\nuntil they ﬁt in the budget.\nThe logit score Sfor each cell cis obtained by\n7611\nmean-pooling the logits for each of the tokens t\ninside it, which are in turn the result of applying a\nsingle linear layer to the contextual representation\nof each token when applying MATE to the query q\nand the expanded table e.\nS(t) = MLP(MATE(q,e)[t])\nS(c) = avgt∈cS(t)\nP(c) = exp(S(c))∑\nc′∈e exp(S(c′))\nWe use cross entropy loss for training the model\nto select expanded cells that contain the answer\nspan. Even though the correct span may appear\nin multiple cells or passages, in practice many of\nthese do so only by chance and do not correspond\nto a reasoning path consistent with the question\nasked. In Figure 3 for instance, there could be\nother British divers but we are only interested in\nselecting the cell marked with a star symbol (⋆). In\norder to handle these cases we rely on Maximum\nMarginal Likelihood (MML ) (Liang et al., 2013;\nBerant et al., 2013). As shown by Guu et al. (2017)\nMML can be interpreted as using the online model\npredictions (without gradients) to compute a soft\nlabel distribution over candidates. For an input\nquery x, and a set Cof candidate cells, the loss is:\nL(Θ,x, C) =\n∑\nz∈C\n−q(z) logpΘ(z|x)\nwith q(z) = pΘ(z|x,z ∈C) the probability distri-\nbution given by the model restricted to candidate\ncells containing the answer span, taken here as a\nconstant with zero gradient.\n4.2 P OINT R: Passage Reading Stage\nIn the second stage we develop a span selection\nmodel that reads the answer from a singleexpanded\ncell selected by the POINT R Cell Selector. In order\nto construct the expanded cell for each example, we\nconcatenate the cell content with all the sentences\nof the linked entities and keep the ﬁrst 512 tokens.\nFollowing various recent neural machine read-\ning works (Chen et al., 2017; Lee et al., 2019;\nHerzig et al., 2021), we ﬁne-tune a pre-trained\nBERT-uncased-large model (Devlin et al., 2019)\nthat attempts to predict a text span from the text\nin a given table cell c(and its linked paragraphs)\nand the input query q. We compute a span rep-\nresentation as the concatenation of the contextual\nembeddings of the ﬁrst and last token in a span s\nTABFACT WIKITQ SQA\nExamples 118,275 22,033 17,553\nTables 16,573 2,108 982\nTable 3: Statistics for SQA, W IKI TQ and TABFACT.\nand score it using a multi-layer perceptron:\nhstart = BERTr(q,c)[START(s)]\nhend = BERTr(q,c)[END(s)]\nSread(q,c) = MLP([hstart,hend])\nA softmax is computed over valid spans in the\ninput and the model is trained with cross entropy\nloss. If the span-text appears multiple times in\na cell we consider only the ﬁrst appearance. To\ncompute EM and F1 scores during inference, we\nevaluate the trained reader on the highest ranked\ncell output predictions of the POINT R Cell Selector\nusing the ofﬁcial evaluation script.\n5 Experimental Setup\nWe begin by comparing the performance of MATE\non HYBRID QA to other existing systems. We fo-\ncus on prior efﬁcient transformers to compare the\nbeneﬁts of the table-speciﬁc sparsity. We follow\nHerzig et al. (2020); Eisenschlos et al. (2020) in\nreporting error bars with the interquartile range.\n5.1 Baselines\nThe ﬁrst baselines for HYBRID QA are Table-\nOnly and Passage-Only, as deﬁned in Chen et al.\n(2020b). Each uses only the part of the input in-\ndicated in the name but not both at the same time.\nNext, the HYBRIDER model from the same authors,\nconsists of four stages: entity linking, cell ranking,\ncell hopping and ﬁnally a reading comprehension\nstage, equivalent to our ﬁnal stage. The ﬁrst three\nstages are equivalent to our single cell selection\nstage; hence, we use their reported error rates to\nestimate the retrieval rate. The simpler approach\nenabled by MATE avoids error propagation and\nyields improved results.\nWe also consider two recent efﬁcient transformer\narchitectures as alternatives for the POINT R Cell\nSelector, one based on LINFORMER (Wang et al.,\n2020) and one based on ETC (Ainslie et al., 2020).\nIn both cases we preserve the row, column and rank\nembeddings introduced by Herzig et al. (2020).\nLINFORMER learns a projection matrix that re-\nduces the sequence length dimension of the keys\n7612\nand values tensor to a ﬁxed length of 256 (which\nperformed better than 128 and 512 in our tests.)\nETC is a general architecture which requires some\nchoices to be made about how to allocate global\nmemory and local attention (Dai et al., 2019) Here\nwe use a 256-sized global memory to summarize\nthe content of each cell, by assigning each token\nin the ﬁrst half of the global memory to a row, and\neach token in the second half to a column. Tokens\nin the input use a special relative positional value\nto mark when they are interacting with their corre-\nsponding global row or column memory position.\nWe will refer to this model as TABLE ETC.\nFinally we consider two non-efﬁcient models: A\nsimple TAPAS model without any sparse mask, and\nan SAT (Zhang et al., 2020) model pretrained on\nthe same MLM task asTAPAS for a fair comparison.\nFor the cell selection task TAPAS obtains similar\nresults to MATE, but both TAPAS and SAT lack the\nefﬁciency improvements of MATE.\n5.2 Other datasets\nWe also apply MATE to three other datasets involv-\ning tables to demonstrate that the sparse attention\nbias yields stronger table reasoning models. SQA\n(Iyyer et al., 2017) is a sequential QA task, WIK-\nITQ (Pasupat and Liang, 2015) is a QA task that\nsometimes also requires aggregation of table cells,\nand TABFACT (Chen et al., 2020a) is a binary en-\ntailment task. See Table 3 for dataset statistics. We\nevaluate with and without using the intermediate\npre-training tasks (CS) (Eisenschlos et al., 2020).\n6 Results\nIn Figure 4 we compare inference speed of different\nmodels as we increase the sequence length. Similar\nresults showing number of FLOPS and memory\nusage are in Appendix A. The linear scaling of\nLINFORMER and the linear-time version MATE\ncan be seen clearly. Although LINFORMER has a\nslightly smaller linear constant, the pre-training is\n6 times slower, as unlike the other models, LIN-\nFORMER pretraining must be done at the ﬁnal se-\nquence length of 2048.\nTable 5 shows the end-to-end results of our sys-\ntem using POINT R with MATE on HYBRID QA,\ncompared to the previous state-of-the-art as well\nas the other efﬁcient transformer baselines from\nSection 5. MATE outperforms the previous SOTA\nHYBRIDER by over 19 points, and LINFORMER ,\nthe next best efﬁcient-transformer system, by over\nFigure 4: Comparison of inference speed on a cloud\nVM with 64GB. At a sequence length of 2048, M ATE\nis nearly twice as fast as TAPAS.\nModel SQAALL SQASEQ WIKITQ T ABFACT\nTAPAS 67.2±0.5 40.4±0.9 42.6±0.8 76.3±0.2\nMATE 71.6±0.1 46.4±0.3 42.8±0.8 77.0±0.3\nTAPAS+ CS 71.0±0.4 44.8±0.8 46.6±0.3 81.0±0.1\nMATE+ CS 71.7±0.4 46.1±0.4 51.5±0.2 81.4±0.1\nTable 4: Test results of using MATE on other table pars-\ning datasets show improvements due to the sparse atten-\ntion mechanism. Using Counterfactual + Synthethic\npretraining (CS) in combination with M ATE achieves\nstate-of-the-art in SQA and T ABFACT. Errors are esti-\nmated with half the interquartile range over 5 runs.\n2.5 points, for both exact-match accuracy and F1.\nWe also applied MATE to three tasks involv-\ning table reasoning over shorter sequences. In Ta-\nble 4 we see that MATE provides improvements\nin accuracy, which we attribute to a better induc-\ntive bias for tabular data. When combining MATE\nwith Counterfactual + Synthetic intermediate pre-\ntraining (CS) (Eisenschlos et al., 2020) we often\nget even better results. For TABFACT and SQA\nwe improve over the previous state-of-the-art. For\nWIKI TQ we close the gap with the best published\nsystem TABERT (Yin et al., 2020) (51.8 mean test\naccuracy), which relies on traditional semantic pars-\ning, instead of an end-to-end approach. Dev re-\nsults show a similar trend and can be found in Ap-\npendix B. No special tuning was done on these\nmodels—we used the same hyper-parameters as\nthe open source release of TAPAS.\n7 Analysis\nHYBRID QA Error analysis We randomly sam-\nple 100 incorrectly answered examples from the\ndevelopment set. 55% of the examples have lex-\nical near-misses—predictions have the correct in-\nformation, but have slightly different formatting\n(e.g. (Q)uestion: In what round was the Okla-\n7613\nModel Dev Test\nIn-Table In-Passage Total In-Table In-Passage Total\nEM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nTable-Only 14.7 19.1 2.4 4.5 8.4 12.1 14.2 18.8 2.6 4.7 8.3 11.7\nPassage-Only 9.2 13.5 26.1 32.4 19.5 25.1 8.9 13.8 25.5 32.0 19.1 25.0\nHYBRIDER(τ=0.8) 54.3 61.4 39.1 45.7 44.0 50.7 56.2 63.3 37.5 44.4 43.8 50.6\nPOINTR + SAT 66.5±0.33 71.8±0.28 60.3±0.11 69.2±0.04 61.2±0.29 68.7±0.31 64.6 70.1 59.6 68.5 60.1 67.4\nPOINTR + TAPAS 68.1±0.33 73.9±0.37 62.9±0.25 72.0±0.21 63.3±0.25 70.8±0.12 67.8 73.2 62.0 70.9 62.7 70.0\nPOINTR +TABLEETC 36.0±1.26 42.4±1.13 37.8±1.19 45.3±1.53 36.1±1.30 42.9±1.36 35.8 40.7 38.8 45.7 36.6 42.6\nPOINTR + LINFORMER 65.5±0.78 71.1±0.55 59.4±0.59 69.0±0.68 60.8±0.68 68.4±0.63 66.1 71.7 58.9 67.8 60.2 67.6\nPOINTR + MATE 68.6±0.37 74.2±0.26 62.8±0.25 71.9±0.20 63.4±0.16 71.0±0.17 66.9 72.3 62.8 71.9 62.8 70.2\nHuman 88.2 93.5\nTable 5: Results of different large transformer models on HYBRID QA. In-Table and In-Passage subsets refer to the\nlocation of the answer. For dev, we report errors over 5 runs using half the interquartile range. Since the test set is\nhidden and hosted online, we report the results corresponding to the model with the median total EM score on dev.\nhoma athlete drafted in? (G)old answer: “second”,\n(P)redicted: “second round”). While around 30%\nof such misses involved numerical answers (eg:\n“1” vs “one”), the predictions for the rest of them\nprominently (58% of the near misses) either had\nredundant or were missing auxiliary words (e.g.,\nQ: What climate is the northern part of the home\ncountry of Tommy Douglas? G: “Arctic” P: “Arctic\nclimate”). The inconsistency in the gold-answer\nformat and unavailability of multiple gold answers\nare potential causes here.\nAmong the non near-misses, the majority pre-\ndictions were either numerically incorrect, or were\nreferencing an incorrect entity but still in an rele-\nvant context—especially the questions involving\nmore than 2 hops. (e.g. Q: In which sport has an\naward been given every three years since the ﬁrst\ntournament held in 1948-1949? G: “Badminton”,\nP: “Thomas Cup”). Reassuringly, for a huge major-\nity (> 80%), the entity type of the predicted answer\n(person, date, place, etc.) matches the type of the\ngold answer. The observed errors suggest potential\ngains by improving the entity (Xiong et al., 2020)\nand numerical (Andor et al., 2019) reasoning skills.\nAblation Study In Table 6 we compare architec-\ntures for cell selection on HYBRID QA. Hits@k\ncorresponds to whether a cell containing an answer\nspan was among the top-k retrieved candidates. As\nan ablation, we remove the sparse pre-training and\ntry using only row/column heads. We observe a\ndrop also when we discard the ambiguous exam-\nples from training instead of having MML to deal\nwith them. Unlike the other datasets, TAPAS shows\ncomparable results to MATE, but without any of\nthe theoretical and practical improvements.\nModel Hits@1 Hits@3 Hits@5\nHYBRIDER 68.5 - -\nSAT 77.9 87.4 90.3\nTAPAS 80.1 89.5 91.4\nTABLE ETC 51.1 72.0 78.9\nLINFORMER 77.1 86.5 90.0\nMATE 80.1 89.2 91.5\nMATE (– row heads) 78.3 87.7 90.3\nMATE (– col heads) 77.8 87.1 90.0\nMATE (– sparse pretrain) 75.5 86.5 89.9\nMATE (– ambiguous) 76.7 84.2 86.6\nTable 6: Retrieval results over H YBRID QA (dev set)\nfor models used in P OINT R Cell Selection stage. Ef-\nﬁcient transformer models are grouped together. HY-\nBRIDER results are obtained from Chen et al. (2020b)\nby composing the errors for the ﬁrst components.\nObserved Attention Sparsity Since we are in-\nterested to motivate our choices on how to sparsify\nthe attention matrix, we can inspect the magnitude\nof attention connections in a trained dense TAPAS\nmodel for table question answering. It is important\nto note that in this context we are not measuring at-\ntention as an explanation method (Jain and Wallace,\n2019; Wiegreffe and Pinter, 2019). Instead we are\ntreating the attention matrix in the fashion of mag-\nnitude based pruning techniques (Han et al., 2015;\nSee et al., 2016), and simply consider between\nwhich pairs of tokens the scores are concentrated.\nGiven a token in the input we can aggregate the\nattention weights ﬂowing from it depending on\nthe position of the target token in the input ( CLS\ntoken, question, header, or table) and whether the\nsource and target tokens are in the same column or\nrow, whenever it makes sense. We average scores\nacross all tokens, heads, layers and examples in the\ndevelopment set. As a baseline, we also compare\nagainst the output of the same process when using\n7614\nType Same\ncolumn\nSame\nrow\nAttention /\nUniform\nAttention % Uniform %\n[CLS] 44.46 12.45 0.28\nQuestion 10.13 37.08 3.66\nHeader \u0017 1.32 3.89 2.94\n\u0013 3.37 1.72 0.51\nTable\n\u0017 \u0017 0.34 23.59 68.18\n\u0017 \u0013 0.87 3.51 4.02\n\u0013 \u0017 0.70 13.16 18.84\n\u0013 \u0013 2.93 4.60 1.57\nTable 7: Average attention ﬂow from a token in the\ntable to other token types. We compare to an uniform\nattention matrix as a baseline. Attention to tokens in\ndifferent rows and columns is the relative smallest with\none third of the baseline. Computed on W IKI TQ Dev.\na uniform attention matrix, discarding padding.\nIn Table 7, we show the obtained statistics con-\nsidering only table tokens as a source. We use the\nWIKI TQ development set as a reference. While we\nsee that 23% of the attention weights are looking at\ntokens in different columns and rows, this is only\nabout one third of the baseline number one would\nobtain with a uniform attention matrix. This effect\ncorroborates the approach taken in MATE.\n8 Conclusion\nWe introduce MATE, a novel method for efﬁciently\nrestricting the attention ﬂow in Transformers ap-\nplied to Tabular data. We show in both theory\nand practice that the method improves inductive\nbias and allows scaling training to larger sequence\nlengths as a result of linear complexity. We im-\nprove the state-of-the-art on TABFACT, SQA and\nHYBRID QA, the last one by 19 points.\nEthical Considerations\nAlthough one outcome of this research is more ef-\nﬁcient Transformers for table data, it remains true\nthat large Transformer models can be expensive\nto train from scratch, so experiments of this sort\ncan incur high monetary cost and carbon emissions.\nThis cost was reduced by conducting some exper-\niments at relatively smaller scale, e.g. the results\nof Figure 4. To further attenuate the impact of this\nwork, we plan release all the models that we trained\nso that other researchers can reproduce and extend\nour work without re-training.\nAll human annotations required for the error\nanalysis (Section 7) are provided by authors, and\nhence a concern of fair compensation for annota-\ntors did not arise.\nAcknowledgments\nWe would like to thank Yasemin Altun, Ankur\nParikh, Jordan Boyd-Graber, Xavier Garcia, Syrine\nKrichene, Slav Petrov, and the anonymous review-\ners for their time, constructive feedback, useful\ncomments and suggestions about this work.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. In Proceedings of Empirical Methods\nin Natural Language Processing , pages 268–284,\nOnline. Association for Computational Linguistics.\nDaniel Andor, Luheng He, Kenton Lee, and Emily\nPitler. 2019. Giving BERT a calculator: Finding\noperations and arguments with reading comprehen-\nsion. In Proceedings of Empirical Methods in Natu-\nral Language Processing, pages 5947–5952, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of Empiri-\ncal Methods in Natural Language Processing, pages\n1533–1544, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nMichael J. Cafarella, Alon Halevy, Daisy Zhe Wang,\nEugene Wu, and Yang Zhang. 2008. Uncovering the\nrelational web. In WebDB.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the Associ-\nation for Computational Linguistics , pages 1870–\n1879, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\nWilliam Yang Wang, and William W. Cohen. 2021.\nOpen question answering over tables and text. In\nInternational Conference on Learning Representa-\ntions.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020a. Tabfact: A large-scale\ndataset for table-based fact veriﬁcation. In Proceed-\nings of the International Conference on Learning\nRepresentations.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\nXiong, Hong Wang, and William Yang Wang. 2020b.\nHybridQA: A dataset of multi-hop question answer-\ning over tabular and textual data. In Findings of the\nAssociation for Computational Linguistics: EMNLP,\npages 1026–1036, Online. Association for Computa-\ntional Linguistics.\n7615\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the Asso-\nciation for Computational Linguistics , pages 2978–\n2988, Florence, Italy. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJulian Eisenschlos, Syrine Krichene, and Thomas\nMüller. 2020. Understanding tables with interme-\ndiate pre-training. In Findings of the Association\nfor Computational Linguistics: EMNLP, pages 281–\n296, Online. Association for Computational Linguis-\ntics.\nKelvin Guu, Panupong Pasupat, Evan Liu, and Percy\nLiang. 2017. From language to programs: Bridg-\ning reinforcement learning and maximum marginal\nlikelihood. In Proceedings of the Association for\nComputational Linguistics, pages 1051–1062, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefﬁcient neural network. In Proceedings of Ad-\nvances in Neural Information Processing Systems ,\nvolume 28, pages 1135–1143. Curran Associates,\nInc.\nJonathan Herzig, Thomas Müller, Syrine Krichene, and\nJulian Eisenschlos. 2021. Open domain question an-\nswering over tables via dense retrieval. In Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics , pages 512–519,\nOnline. Association for Computational Linguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the Association for\nComputational Linguistics , pages 4320–4333, On-\nline. Association for Computational Linguistics.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.\nSearch-based neural structured learning for sequen-\ntial question answering. In Proceedings of the Asso-\nciation for Computational Linguistics , pages 1821–\n1831, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics, pages 3543–3556, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMarcin Kardas, Piotr Czapla, Pontus Stenetorp, Se-\nbastian Ruder, Sebastian Riedel, Ross Taylor, and\nRobert Stojnic. 2020. AxCell: Automatic extraction\nof results from machine learning papers. InProceed-\nings of Empirical Methods in Natural Language Pro-\ncessing, pages 8580–8594, Online. Association for\nComputational Linguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Pro-\nceedings of the International Conference on Learn-\ning Representations.\nSyrine Krichene, Thomas Müller, and Julian Eisensch-\nlos. 2021. DoT: An efﬁcient double transformer\nfor NLP tasks with tables. In Findings of the Asso-\nciation for Computational Linguistics: ACL , pages\n3273–3283, Online. Association for Computational\nLinguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\nAssociation for Computational Linguistics , pages\n6086–6096, Florence, Italy. Association for Compu-\ntational Linguistics.\nPercy Liang, Michael I. Jordan, and Dan Klein. 2013.\nLearning dependency-based compositional seman-\ntics. Computational Linguistics, 39(2):389–446.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the Association for Computational\nLinguistics, pages 1470–1480, Beijing, China. As-\nsociation for Computational Linguistics.\nAbigail See, Minh-Thang Luong, and Christopher D.\nManning. 2016. Compression of neural machine\ntranslation models via pruning. In Proceedings\nof The 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 291–301, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Mul-\ntimodal{qa}: complex question answering over text,\ntables and images. In Proceedings of the Interna-\ntional Conference on Learning Representations.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\n7616\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of Empirical\nMethods in Natural Language Processing, pages 11–\n20, Hong Kong, China. Association for Computa-\ntional Linguistics.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In Proceedings of the International Confer-\nence on Learning Representations.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. In Pro-\nceedings of the Association for Computational Lin-\nguistics, pages 8413–8426, Online. Association for\nComputational Linguistics.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat, Sashank Reddi, and Sanjiv Kumar. 2020a.\nAre transformers universal approximators of\nsequence-to-sequence functions? In Proceed-\nings of the International Conference on Learning\nRepresentations.\nChulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli,\nAnkit Rawat, Sashank Reddi, and Sanjiv Kumar.\n2020b. O(n) connections are expressive enough:\nUniversal approximability of sparse transformers.\nIn Proceedings of Advances in Neural Information\nProcessing Systems.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. In Proceedings of Advances in\nNeural Information Processing Systems, volume 33.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact veriﬁcation with structure-aware trans-\nformer. In Proceedings of Empirical Methods in\nNatural Language Processing , pages 1624–1629,\nOnline. Association for Computational Linguistics.\n7617\nAppendix\nWe provide all details on our experimental setup\nto reproduce the results in Section A. In Section B\nwe show the development set results for our exper-\niments. The proof for Theorem 1 is described in\nSection C and in Section D we include the main\ncode blocks for implementing MATE efﬁciently on\na deep learning framework.\nA Experimental setup\nA.1 Pre-training\nPre-training for MATE was performed with con-\nstrained attention with a masked language model-\ning objective applied to the corpus of tables and\ntext extracted by Herzig et al. (2020). With a se-\nquence length of 128 and batch size of 512, the\ntotal training of 1 million steps took 2 days.\nIn contrast, for LINFORMER the pre-training was\ndone with a sequence length of 2048 and a batch\nsize of 128, and the total training took12 days for 2\nmillion steps. For TABLE ETC we also pre-trained\nfor 2 million steps but the batch size had to be\nlowered to 32. In all cases the hardware used was\na 32 core Cloud TPUs V3.\nA.2 Fine-tuning\nFor all experiments we use Large models over 5\nrandom seeds and report the median results. Errors\nare estimated with half the interquartile range. For\nTABFACT, SQA and WIKI TQ we keep the origi-\nnal hyper-parameters used in TAPAS and provided\nin the open source release. In Figure 5 we show\nthe ﬂoating point operation count of the different\nTransformer models as we increase the sequence\nlength, as extracted from the execution graph. We\nalso measure the memory doing CPU inference in\nﬁgure 6. The linear scaling of LINFORMER and\nMATE can be observed. No additional tuning or\nsweep was done to obtain the published results. We\nset the global size Gto 116 and the radius Rfor\nlocal attention to 42. We use an Adam optimizer\nwith weight decay with the same conﬁguration as\nBERT. The number of parameters for MATE is the\nsame as for BERT: 340M for Large models and\n110M for Base Models.\nIn the HYBRID QA cell selection stage, we use a\nbatch size of 128 and train for 80,000 steps and a\nsequence length of 2048. Training requires 1 day.\nWe clip the gradients to 10 and use a learning rate\nof 1×10−5 under a 5% warm-up schedule. For the\nFigure 5: Comparison of inference FLOPS obtained\nfrom execution graph. While T ABLE ETC is linear, rel-\native attention adds a high computation cost so keep it\nout of range in the ﬁgure.\nFigure 6: Comparison of the peak memory usage dur-\ning CPU inference shows the linear asymptotic curve\nof the memory footprint of MATE.\nreader stage use a learning rate of 5 ×10−5 under\na 1% warm-up schedule, a batch size of 512 and\ntrain for 25,000 steps, which takes around 6 hours.\nB Development set results for SQA,\nWIKI TQ and TABFACT\nWe show in Table 8 the dev set results for all\ndatasets we attempted, which show consistent re-\nsults with the test set reported in the main paper.\nC Proof of Theorem 1\nIn this section we discuss the proof that MATE are\nuniversal approximators of sequence functions.\nTheorem. If the number of heads is at least 3 and\nModel SQAALL SQASEQ WIKITQ T ABFACT\nTAPAS 64.9±0.5 40.0±1.0 41.6±1.0 76.9±0.4\nMATE 67.0±0.1 43.2±0.2 42.9±0.6 77.5±0.3\nTAPAS+ CS 68.0±0.2 45.8±0.3 46.2±0.2 81.0±0.1\nMATE+ CS 68.0±0.4 44.9±0.4 50.1±0.7 81.3±0.1\nTable 8: Dev results of using M ATE on other table\nparsing datasets. Errors are estimated with half the in-\nterquartile range over 5 runs.\n7618\nthe hidden size of the feed forward layer is at least\n4, then for any f ∈F and ϵ ∈R+ there exists\nˆf ∈TMATE such that ||ˆf −f||p <ϵ\nProof. When the number of heads is at least 3,\nthere are at least 2 heads of the same type. Fixing\nthose two heads, we may restrict the value of the\nprojection weights WV to be 0 for the rest of the\nheads. This is equivalent to having only those two\nheads with the same attention pattern to begin with.\nThis restriction only makes the family of functions\nmodelled by MATE smaller. In a similar way, we\ncan assume that the hidden size of the feed-forward\nlayer is exactly 4 and that the head size is 1.\nNote that the attention pattern of the two heads,\nregardless of its type contains a token (the ﬁrst one)\nwhich attends to and from every other token. We\nalso have that every token attends to itself. Then\nAssumption 1 of Yun et al. (2020b) is satisﬁed.\nHence we rely on Theorem 1 of Yun et al. (2020b),\nwhich asserts that sparse transformers with2 heads,\nhidden size 4 and head size 1 are universal approx-\nimators, which concludes the proof.\nD TensorFlow Implementation\nIn ﬁgure 7 we provide an approximate implemen-\ntation of MATE in the TensorFlow library. For the\nsake of simplicity we omit how attention is masked\nbetween neighbor buckets for tokens in difference\ncolumns or rows. We also omit the tensor manipula-\ntion steps to reorder and reshape the sequence into\nequally sized buckets to compute attention across\nconsecutive buckets. The full implementation will\nbe part of the open source release.\n7619\nimport dataclasses\nimport tensorflow as tf\n@dataclasses.dataclass\nclass MultiViewEmbedding():\n\"\"\"Results of sorting and reshaping an embedding tensor.\nDifferent views of the tensor created to facilitate attention across tokens\nfrom global/long parts. First two dimensions are ‘batch_size‘ and ‘num_heads‘:\nAttributes:\nfull: <float32>[..., seq_length, embedding_size].\nOriginal tensor without any bucketing.\nglobal: <float32>[..., global_length, embedding_size].\nlong: <float32>[..., long_length/radius, radius, embedding_size]\nwindow: <float32>[..., long_length/radius, 3*radius, embedding_size]\nSame as ‘long‘ but also a rotation to the left and right, in order\nto achieve attention to the previous and next bucket.\n\"\"\"\nfull: tf.Tensor\nglobal: tf.Tensor\nlong: tf.Tensor\nwindow: tf.Tensor\ndef multi_view_attention(\nQ: MultiViewEmbedding,\nK: MultiViewEmbedding,\nV: MultiViewEmbedding,\nembedding_size: int,\nglobal_length: int,\nlong_length: int,\nnum_heads: int,\n):\n# <float32>[batch_size, num_heads, global_length, sequence_length]\nattention_prob_from_global = tf.nn.softmax(tf.einsum(\n’BHFE,BHTE->BHFT’, Q.global, K.full) / sqrt(embedding_size))\n# <float32>[batch_size, num_heads, long_length, global_length]\nattention_score_to_global = tf.einsum(’BHNFE,BHTE->BHNFT’,\nQ.long, K.global) / sqrt(embedding_size)\n# <float32>[batch_size, num_heads, long_length, 3 * radius]\nattention_score_to_window = tf.einsum(’BHNFE,BHNTE->BHNFT’,\nQ.long, K.window) / sqrt(embedding_size)\n# <float32>[batch_size, num_heads, long_length, global_length + 3 * radius]\nattention_prob_from_long = tf.nn.softmax(tf.concat(\n[attention_score_to_global, attention_score_to_window], axis=-1))\nattention_prob_to_global = attention_prob_from_long[..., :global_length]\nattention_prob_to_window = attention_prob_from_long[..., global_length:]\n# <float32>[batch_size, num_heads, global_length, embedding_size]\ncontext_layer_from_global = tf.einsum(’BHFT,BHTE->BHFE’,\nattention_prob_from_global, V.full)\n# <float32>[batch_size, num_heads, long_length / radius, radius, embedding_size]\ncontext_layer_to_global = tf.einsum(’BHNFT,BHTE->BHNFE’,\nattention_prob_to_global, V.global)\n# <float32>[batch_size, num_heads, long_length / radius, radius, embedding_size]\ncontext_layer_to_window = tf.einsum(’BHNFT,BHNTE->BHNFE’,\nattention_prob_to_window, V.window)\ncontext_layer_from_long = tf.reshape(\ncontext_layer_to_first + context_layer_to_window,\n[-1, num_heads, long_length, embedding_size])\nreturn tf.concat(\n[context_layer_from_global, context_layer_from_long], axis=-1)\nFigure 7: Implementation of M ATE in TensorFlow. The creation of MultiViewEmbedding is ommited and\nrelies on tf.gather for ordering the input. We also omit the use of the input mask and column and row index\nto further mask the sparse attention matrix.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7997028827667236
    },
    {
      "name": "Transformer",
      "score": 0.6516729593276978
    },
    {
      "name": "Architecture",
      "score": 0.5754402875900269
    },
    {
      "name": "Row",
      "score": 0.5516454577445984
    },
    {
      "name": "Row and column spaces",
      "score": 0.44222480058670044
    },
    {
      "name": "Table (database)",
      "score": 0.4277484714984894
    },
    {
      "name": "Information retrieval",
      "score": 0.4028499722480774
    },
    {
      "name": "Data mining",
      "score": 0.39458751678466797
    },
    {
      "name": "Database",
      "score": 0.19991546869277954
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}