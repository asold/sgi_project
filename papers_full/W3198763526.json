{
  "title": "LigGPT: Molecular Generation using a Transformer-Decoder Model",
  "url": "https://openalex.org/W3198763526",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5083698352",
      "name": "Viraj Bagal",
      "affiliations": [
        "Indian Institute of Science Education and Research Pune",
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A5016226933",
      "name": "Rishal Aggarwal",
      "affiliations": [
        "Indian Institute of Science Education and Research Pune",
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A5060336051",
      "name": "P. K. Vinod",
      "affiliations": [
        "Indian Institute of Science Education and Research Pune",
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A5034329125",
      "name": "U. Deva Priyakumar",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3027664180",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226348722",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2998374989",
    "https://openalex.org/W2995422623",
    "https://openalex.org/W2963609389",
    "https://openalex.org/W2177317049",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W4289436753",
    "https://openalex.org/W2618625858",
    "https://openalex.org/W2805002767",
    "https://openalex.org/W3030948478",
    "https://openalex.org/W3025593963",
    "https://openalex.org/W4231080513",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2023818227",
    "https://openalex.org/W4230627164",
    "https://openalex.org/W2803526748",
    "https://openalex.org/W2963676163",
    "https://openalex.org/W2953128081",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W2883583109",
    "https://openalex.org/W2786722833",
    "https://openalex.org/W3097865746",
    "https://openalex.org/W2765224015",
    "https://openalex.org/W4233151585",
    "https://openalex.org/W4241126451",
    "https://openalex.org/W3120994384",
    "https://openalex.org/W4241677786",
    "https://openalex.org/W2736137960",
    "https://openalex.org/W3100751385",
    "https://openalex.org/W2793945656",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2891868449",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2746340587",
    "https://openalex.org/W4244409986",
    "https://openalex.org/W2956961449",
    "https://openalex.org/W4242836807",
    "https://openalex.org/W2972741532",
    "https://openalex.org/W2604296437",
    "https://openalex.org/W2094750196",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2989615256",
    "https://openalex.org/W3211335638",
    "https://openalex.org/W4297951436",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W2041686943",
    "https://openalex.org/W2886791556",
    "https://openalex.org/W2903564615",
    "https://openalex.org/W2991736596"
  ],
  "abstract": "Application of deep learning techniques for the de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in Natural Language Processing, such as the Transformers, for molecular design in general. Inspired by Generative Pre-Training (GPT) model that have been shown to be successful in generating meaningful text, we train a Transformer-Decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, LigGPT, outperforms other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to optimize multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties, by passing these structures as conditions, which has potential applications in lead optimization in addition to de novo molecular design. Using saliency maps, we highlight the interpretability of the generative process of the model.",
  "full_text": "LigGPT: Molecular Generation using a\nTransformer-Decoder Model\nViraj Bagal,†,‡ Rishal Aggarwal,† P. K. Vinod,† and U. Deva Priyakumar∗,†\n†International Institute of Information Technology, Hyderabad 500 032, India\n‡Indian Institute of Science Education and Research, Pune 411 008, India\nE-mail: deva@iiit.ac.in\nAbstract\nApplication of deep learning techniques for the de novo generation of molecules,\ntermed as inverse molecular design, has been gaining enormous traction in drug design.\nThe representation of molecules in SMILES notation as a string of characters enables\nthe usage of state of the art models in Natural Language Processing, such as the Trans-\nformers, for molecular design in general. Inspired by Generative Pre-Training (GPT)\nmodel that have been shown to be successful in generating meaningful text, we train\na Transformer-Decoder on the next token prediction task using masked self-attention\nfor the generation of druglike molecules in this study. We show that our model, Lig-\nGPT, outperforms other previously proposed modern machine learning frameworks for\nmolecular generation in terms of generating valid, unique and novel molecules. Fur-\nthermore, we demonstrate that the model can be trained conditionally to optimize\nmultiple properties of the generated molecules. We also show that the model can be\nused to generate molecules with desired scaﬀolds as well as desired molecular prop-\nerties, by passing these structures as conditions, which has potential applications in\nlead optimization in addition to de novo molecular design. Using saliency maps, we\nhighlight the interpretability of the generative process of the model.\n1\nIntroduction\nIt has been postulated that the total number of potential drug like candidates range from\n1023 to 1060 molecules1 of which, only about 108 molecules have been synthesized2. Since it is\ndiﬃcult to screen a practically inﬁnite chemical space, and there is a a huge disparity between\nsynthesized and potential molecules, generative models are used to model a distribution\nof molecules for the purpose of sampling molecules that have desirable properties. Deep\ngenerative models have made great strides in modeling data distributions in general data\ndomains such as Computer Vision 3,4 and Natural Language Processing (NLP) 5,6. Such\nmethods have also been adopted to model molecular distributions 7,8. Such models learn\nprobability distributions over a large set of molecules and therefore are able to generate novel\nmolecules by sampling from these distributions 7,9. The rapid adoption of deep generative\nmodel has also led to the development of benchmark datasets such as the Molecular Sets\n(MOSES)10 and GuacaMol 9 datasets.\nThe representation of molecules in Simpliﬁed Molecular Input Line Entry System (SMILES)11\nnotation as a string of characters enables the usage of modern NLP deep learning models for\ntheir computation 12. Some of the earliest deep learning architectures for molecular gener-\nation involved the usage of Recurrent Neural Networks (RNNs) on molecular SMILES 13,14.\nSuch models have also previously been trained on large corpus of molecules and then focused\nthrough the usage of reinforcement learning 15,16 or transfer learning 13 to generate molecules\nof desirable properties and activity.\nAuto-Encoder variants such as the Variational Auto-Encoder(VAE)17–21 and Adversarial\nAuto-Encoder(AAE)22–25 have also been employed for molecular generation. These models\ncontain an encoder that encodes molecules to a latent vector representation and a decoder\nthat maps latent vectors back to molecules. Molecules can then be generated by sampling\nfrom these latent spaces. However, SMILES based methods often lead to a discontinuous\nlatent space 20 since similar molecules can have very diﬀerent SMILES representations. To\naddress this problem, SMILES strings have also been randomized as inputs for such mod-\n2\nels26–28. Junction Tree VAE (JT-VAE) 20 is also an alternative solution which represents\nmolecules as graph tree structures. JT-VAE also ensures 100% validity of generated mo-\nlecules by maintaining a vocabulary of molecular components that can be added at each\njunction of the molecule tree. Conditional Variational Auto-Encoders have also been used\nto generate molecules with desired properties 29.\nGenerative Adversarial Networks (GANs) have also gained traction for molecular design30–34.\nThis is mainly because of their ability to generate highly realistic content 4. GANs are com-\nposed of generators and discriminators that work in opposition of each other. While the\ngenerator tries to generate realistic content, the discriminator tries to distinguish between\ngenerated and real content. ORGAN31 was the ﬁrst usage of GANs for molecular generation.\nRANC34 introduced reinforcement learning alongside a GAN loss to generate molecules of\ndesirable properties. LatentGAN 30 is a more recent method which uses latent vectors as\ninput and outputs. These latent vectors are mapped to molecules by the decoder of a pre-\ntrained auto-encoder. This ensures that the model can work with latent representations and\ndoesn’t have to handle SMILES syntax. Most of these methods have been benchmarked on\nthe MOSES and GuacaMol datasets for easy comparison.\nOften, methods use Bayesian optimization 35,36, reinforcement learning 15,34 or other op-\ntimization methods37,38 to generate molecules exhibiting desirable properties. Mol-CycleGAN39\nis a generative model that utilises the JT-VAE architecture and applies the CycleGAN 40\nloss to generate molecules of required properties using given molecule templates. Only a\nfew methods explicitly deﬁne the values of properties for generated molecules. Conditional\nRNNs13,41, Deep learning enabled inorganic material generator (DING) 29 and Conditional\nAdversarially Regularized Autoencoder (CARAE) 25 are three such methods that sample\nmolecules based on exact values. RNNs have also been previously used to generate mo-\nlecules based on given scaﬀolds 42. A graph based method has been designed that ensures\nthe presence of desired scaﬀolds while generating molecules with exact property values 43.\nA novel NLP architecture called the Transformer5 has shown state-of-the-art performance\n3\nin language translation tasks. Transformers consist of encoder and decoder modules. The\nencoder module gains context from all the input tokens through self attention mechanisms.\nThe decoder module gains context from both the encoder as well as previously generated\ntokens by attention. Using this context the decoder is able to predict the next token. The\ndecoder module has also been previously used independently for language modeling task and\nis known as the Generative Pre-Training Transformer model (GPT) 44. The GPT model has\nbeen shown to develop better language embedding that model longer-distance connections.\nDue to this, the embeddings have shown top performance when used for multiple language\nmodelling tasks such as natural language inference, question answering, sentence similarity\nand classiﬁcation 45.\nTo yield the added beneﬁts of this architecture, we train a a GPT model, named LigGPT,\nto predict a sequence of SMILES tokens for molecular generation. To the best of our know-\nledge, this is the ﬁrst work that has used the GPT architecture for molecular generation.\nFor this, we use a SMILES tokenizer to break SMILES strings into a set of relevant tokens.\nSince predicted tokens are a result of attention applied to all previously generated tokens,\nwe believe, that the model easily learns the SMILES grammar and therefore, can focus on\nhigher level understanding of molecular properties. To this end, we also train our models\nconditionally to explicitly learn certain molecular properties. The model displays perform-\nance that is at par with other methods benchmarked on the MOSES and Guacamol datasets.\nFurthermore, we show that LigGPT controls user speciﬁed molecular properties and scaﬀolds\nwith extremely high accuracy, leading to our conclusion that it learns a good representation\nof the chemical space. Due to this learned representation, the utility of LigGPT in molecular\ndesign processes such as lead optimization is also highlighted.\n4\nMethods\nIn this section, we ﬁrst present the datasets used for all the experiments. We discuss the\nproperties used for conditional generation. This is then followed by the overview of the\nproposed model. Schematic of the training and generation pipeline is shown in this section.\nAt last, the details of the experiments and the metrics used for the evaluation of diﬀerent\nmodels are provided.\nDatasets\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\nFigure 1: Probability distributions of properties (log P, molecular weight, QED, SAS, TPSA\nand SMILES length) of molecules in the MOSES and GuacaMol datasets.\nIn this work, we used two benchmark datasets, MOSES and GuacaMol for training and\nevaluation of our model. MOSES is a dataset composed of 1.9 million lead-like molecules\nwith molecular weight ranging from 250 to 350 Daltons, number of rotatable bonds lower\nthan 7 and and XlogP below 3.5. GuacaMol on the other hand is a subset of the ChEMBL\n2446 database that contains 1.6 Million molecules. We used the rdkit toolkit 47 to calculate\n5\nFigure 2: Pipeline for Training and Generation using the LigGPT model.\nmolecular properties and to extract Bemis-Murcko scaﬀolds. 48 The MOSES dataset was\ncreated mainly to represent lead like molecules and therefore has a distribution of molecules\nwith ideal druglike properties. However, to test the models control on conditional generation\nwe require a larger distribution of property values available in the Guacamol dataset as can\nbe seen in Figure 1. Therefore, we use the GuacaMol dataset to test property conditional\ngeneration. The MOSES dataset also provides a test set of scaﬀolds which we use to evaluate\nscaﬀold and property conditional generation.\nThe models were trained to learn some properties of the molecules for controlled gener-\nation and optimisation. The properties used are the following:\n• logP: the logarithm of the partition coeﬃcient. Partition coeﬃcient compares the\nsolubilities of the solute in two immiscible solvents at equilibrium. If one of the solvents\nis water and the other is a non-polar solvent, then logP is a measure of hydrophobicity.\n• Synthetic Accessibility score (SAS 49): measures the diﬃculty of synthesizing a\ncompound. It is a score between 1 (easy to make) and 10 (very diﬃcult to make).\n• Topological Polar Surface Area (TPSA) : the sum of surface area over all polar\natoms. It measures the drug’s ability to permeate cell membranes. Molecules with\nTPSA greater than 140 ˚A2 tend to be poor in permeating cell membranes.\n6\nFigure 3: LigGPT model architecture.\n• Quantitative Estimate of Drug-likeness (QED 50): This quantiﬁes drug-likeness\nby taking into account the main molecular properties. It ranges from zero (all proper-\nties unfavourable) to one (all properties favourable).\nModel Overview\nThe model schematic of LigGPT for training and generation is given in Figure 2. For training,\nwe extract molecular properties and scaﬀolds from molecules using rdkit and pass them as\nconditions alongside the molecular SMILES. For generation, we provide the model a set of\nproperty and scaﬀold conditions along with a start token to sample a molecule.\nOur model is illustrated in Figure 3. The model is essentially a mini version of the\nGenerative Pre-Training Transformer (GPT) model44. Unlike GPT1 that has around 110M\nparameters, LigGPT has only around 6M parameters. LigGPT comprises stacked decoder\nblocks, each of which, is composed of a masked self-attention layer and fully connected neural\nnetwork. Each self-attention layer returns a vector of size 256, that is taken as input by the\nfully connected network. The hidden layer of the neural network outputs a vector of size\n1024 and uses a GELU activation and the ﬁnal layer again returns a vector of size 256 to be\n7\nused as input for the next decoder block. LigGPT consists of 8 such decoder blocks.\nTo keep track of the order of the input sequence, position values are assigned to each\ntoken. During conditional training, segment tokens are provided to distinguish between the\ncondition and the SMILES tokens. All the tokens are mapped to the same space using their\nrespective embedding layers. All the embeddings are then added, resulting in a vector of\nsize 256, which is then passed as input to the model.\nGPT architectures work on a masked self-attention mechanism. Self-attention is calcu-\nlated through ’Scaled Dot Product Attention’. This involves 3 sets of vectors, the query,\nkey and value vectors. Query vectors are used to query the weights of each individual value\nvector. They are ﬁrst sent through a dot product with key vectors. These dot products\nare scaled by the dimensions of these vectors and then a softmax function is applied to get\nthe corresponding weights. The value vectors are multiplied by their respective weights and\nadded. The query, key and value vectors for each token are computed by weight matrices\npresent in each decoder block. Attention can be represented by the following formula:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V\nWhere Q, K and V are query, key and value vectors respectively. dk here is the dimension\nof query and key vectors and T is transpose of the matrix.\nSelf-attention provides attention to all the tokens of a sequence for prediction. However,\nthis is not ideal when we are training a model to predict the next token in a sequence.\nTherefore, masked self attention is applied to mask attention to all sequence tokens that\noccur in future time steps. It is essential because, during generation, the network would\nhave access only to the tokens predicted in the previous time-steps. Moreover, instead of\nperforming a single masked self-attention operation, each block performs multiple masked\nself-attention operations (multi-head attention) in parallel and concatenates the output.\n8\nMulti-head attention provides better representations by attending to diﬀerent representation\nsubspaces at diﬀerent positions.\nWe train this model on molecules represented as SMILES string. For this, we use a\nSMILES tokenizer to break up the string into a sequence of relevant tokens. Property\nconditions are also sent through a linear layer that maps the condition to a vector of 256\ndimensions. The resultant vector is then concatenated at the start of the sequence of the\nembeddings of the SMILES tokens. For scaﬀold condition, we use an embedding layer to\nmap the tokens to 256-dimensional vectors as well. This embedding layer is shared with the\nmolecule embedding layer. Similar to property condition, the scaﬀold representation is then\nconcatenated at the start of the sequence of the embeddings of the SMILES tokens. The\nmodel is trained such that the predicted tokens are a result of attention to both the previous\ntokens as well as the conditions.\nTraining Procedure and Evaluation Metrics\nEach model is trained for 10 epochs using the Adam optimizer with a learning rate of 6 e−4.\nDuring generation a start token of a single carbon atom is provided to the network along\nwith the conditions.\nWe trained and tested LigGPT on both the MOSES 10 and GuacaMol 9 datasets. We\nalso conducted experiments to check LigGPT’s capacity to control molecular properties\nand core structures. The models were trained on a NVIDIA 2080Ti GPU. Most of the\nmodels converged and showed best performance after 10 epochs. However, we noticed that\ntraining them for slightly fewer epochs led to similar results in terms of validity, novelty and\nuniqueness of generated molecules, which are the metrics used here (details below).\n• Validity: the fraction of generated molecules that are valid. We use rdkit for validity\ncheck of molecules. Validity measures how well the model has learnt the SMILES\ngrammar and the valency of atoms.\n9\n• Uniqueness: the fraction of valid generated molecules that are unique. Low unique-\nness highlights repetitive molecule generation and low level of distribution learning by\nthe model.\n• Novelty: the fraction of valid unique generated molecules that are not in the training\nset. Low novelty is a sign of overﬁtting. We don’t want the model to memorize the\ntraining data.\n• Internal Diversity (IntDiv p): measures the diversity of the generated molecules.\nThis uses the Tanimoto similarity (T) between the ﬁngerprints of each pair of molecules\nin the generated set ( S).\nIntDivp(S) = 1 −p\n√\n1\n|S|2\n∑\ns1,s2∈S\nT(s1, s2)p\nThe method implementation is available at https://github.com/devalab/liggpt\nResults and Discussion\nIn this section, we ﬁrst present the results on unconditional generation of molecules. Lig-\nGPT’s performance is then compared with other state-of-the-art approaches, followed by\nsome insights on the interpretability of our model. We then demonstrate our model’s ability\nof conditional generation based on property alone and scaﬀold alone. This is followed with\nthe results on conditional generation based on property and scaﬀold together. At last, a\npotential use case of our model is demonstrated for one-shot optimization of QED value of\na starting molecule.\nUnconditional Molecular Generation\nThe chemical space is practically iniﬁnite due to which, the models are required to exhibit\nhigh validity, uniqueness and novelty scores with respect to molecular generation. High values\n10\nTable 1: Comparison of the diﬀerent metrics corresponding to unconditional generation of\nmolecules using diﬀerent approaches trained on MOSES dataset. Temperature value of 1.6\nwas used for LigGPT.\nModels Validity Unique@10K Novelty IntDiv 1 IntDiv2\nCharRNN 0.975 0.999 0.842 0.856 0.85\nVAE 0.977 0.998 0.695 0.856 0.85\nAAE 0.937 0.997 0.793 0.856 0.85\nLatentGAN 0.897 0.997 0.949 0.857 0.85\nJT-VAE 1.0 0.999 0.9143 0.855 0.849\nLigGPT 0.9 0.999 0.941 0.871 0.865\nTable 2: Comparison of the diﬀerent metrics corresponding to unconditional generation of\nmolecules using diﬀerent approaches trained on GuacaMol dataset. Temperature value of\n0.9 was used for LigGPT.\nModels Validity Unique Novelty\nSMILES LSTM 0.959 1.0 0.912\nAAE 0.822 1.0 0.998\nOrgan 0.379 0.841 0.687\nVAE 0.870 0.999 0.974\nLigGPT 0.986 0.998 1.0\nof these metrics would ensure that the models have learnt the molecule grammar well and\nare not overﬁtting to the training data simultaneously. Internal diversity scores give an idea\nabout the extent of chemical space traversed by diﬀerent models. So, LigGPT is compared\nwith previous approaches on these criteria. We compare the performance of LigGPT on the\nMOSES dataset to that of CharRNN, VAE, AAE, LatentGAN and JT-VAE. JT-VAE uses\ngraphs as input while the others use SMILES. To get the optimal model for each dataset,\nthe generative performance of LigGPT is checked for several sampling temperature values\nbetween 0.7 and 1.6. The model performs best at a temperature of 1.6 for MOSES and 0.9\nfor GuacaMol. The optimal model performance on each dataset is reported in Table 1 and\nTable 2.\nOn the MOSES benchmark, LigGPT performs the best in terms of the two internal\ndiversity metrics. This indicates that even though LigGPT learns from the same chemical\nspace as other models, it is better than others in generating molecules with lower redundancy.\nIn case of validity, as mentioned earlier, JT-VAE always generates a valid molecule because\n11\nit checks validity at every step of generation. Barring JT-VAE, we observe that CharRNN,\nVAE and AAE have high validity but low novelty. Compared to these three & JT-VAE,\nLigGPT has lower validity but much higher novelty which is more crucial for the purposes\nof molecular design. We ﬁnd that the performance of LigGPT is comparable to LatentGAN.\nLatentGAN involves training of an autoencoder followed by the training of GAN on the\nlatent space of the trained autoencoder. This is a 2-step process while on the other hand,\nLigGPT is trained end-to-end. LigGPT’s validity and uniqueness are slightly higher than\nLatentGAN, but novelty scores are comparable. On the GuacaMol benchmark, LigGPT is\neasily the preferred method when compared to other methods tested on it. It returns very\nhigh validity, uniqueness and novelty scores on generation with a sampling temperature of 0.9.\nWe believe this boost in performance, as compared to MOSES, is due to a larger diversity\nin molecules in the GuacaMol dataset (see Figure 1). Moreover, even though GuacaMol\ndataset has larger molecules as compared to MOSES dataset, LigGPT generates molecules\nwith very high validity which indicates that this method handles long range dependencies\nvery well. LigGPT takes only about 10 minutes to be trained on an epoch of the GuacaMol\ndataset. This is due to the parallel nature in which it computes self-attention as opposed\nto the sequential nature of training for other models. Thus, it can quickly be retrained on a\nnew dataset or on a diﬀerent set of properties.\nWhile it is important to develop machine learning frameworks and pipelines for making\ntasks more eﬃcient, it is desirable to also demonstrate that these models allow for interpret-\nation. Figure 4 shows input saliency maps for some of the generated tokens of the shown\ngenerated molecule. Input saliency methods assign a score to each input token that indic-\nates the importance of that token in generating the next token. ’(’, ’C’ and ’c’ refer to the\nbranching from chain, non-aromatic carbon and aromatic carbon respectively. From Figure\n4, we see that when generating the ’O’ atom in the ﬁrst saliency map, the model rightly\nattends to the previous double bond and ’N’ atoms. Double bond satisﬁes the valency of\nthe oxygen atom and the ’N’ atom participates in the formation of tautomer (Lactam and\n12\nFigure 4: Input saliency maps for the shown generated molecule. The dark purple underline\nare the tokens under consideration for saliency maps. The intensity of color of each token\nindicates the importance of that token for generating the underlined token.\nLactim) which increases the stability of the structure. When generating the ’C’ atom in the\nsecond saliency map, the model attends to ’(’, ’)’ to check if they are balanced, and also\nattends to the atoms in the non-aromatic ring. In the non-aromatic ring, it attends mostly\nto the immediate neighbors - ’2’ and ’N’ atoms. When generating ’2’ token, it attends to\nthe immediate previous ’C’ token and the tokens in the non-aromatic ring. For the fourth\nand ﬁfth saliency map, when generating ’c’ tokens, the model rightly attends to the atoms\nin the aromatic ring since that ring is still incomplete. Thus, these saliency maps provide\nsome insight on the chemical interpretability of the generative process.\nFurther, to evaluate the robustness of the proposed framework, LigGPT’s performance\nis compared with CharRNN in the low data regime. Here, we train both the models only\non 10% of the MOSES training set and evaluate the metrics by generating 10,000 molecules.\nThe results are reported in Table S1 of the Supporting Information. With temperature\n0.9, LigGPT outperforms CharRNN on validity as well as novelty. With temperature 1.6,\nLigGPT has similar novelty as CharRNN but much better validity. Moreover, LigGPT\nhas only 50% of the trainable parameters of CharRNN. This indicates greater eﬃciency of\n13\n(a)\n (b)\n(c)\n (d)\nFigure 5: Distribution of property of generated molecules conditioned on: (a) logP (b)\nTPSA (c) SAS (d) QED . Distribution depicted using a solid red line corresponds to the\nwhole dataset. Trained on GuacaMol dataset with Temperature=0.9.\nLigGPT owing to its masked self-attention.\nGeneration based on Single and Multiple Properties\nMany processes in biology and chemistry require molecules to have certain property values\nin order to perform some functions. For example, for molecules to penetrate the blood–brain\nbarrier (and thus act on receptors in the central nervous system), a TPSA value less than\n90 ˚A2 is usually needed 51. This motivates the need for models to have accurate conditional\ngeneration. So, the next objective is to evaluate the ability of LigGPT to generate molecules\nthat exhibit speciﬁc properties (conditional generation). Since GuacaMol has a wider range\n14\nin property values, we test the model’s ability to control molecular properties trained on it.\nWhile only logP, SAS, TPSA and QED are used for property control, we would like to note\nthat the model can be trained to learn any property that is inferred from the molecule’s 2D\nstructure. For each condition, 10,000 molecules are generated to evaluate property control.\nTable 3: Comparison of diﬀerent metrics while generating molecules conditioned on single\nproperty based on training on GuacaMol dataset. Temperature value of 0.9 was used.\nCondition Validity Unique Novelty MAD\nlogP 0.992 0.975 1.0 0.217\nTPSA 0.992 0.966 1.0 3.339\nSAS 0.993 0.965 1.0 0.108\nQED 0.995 0.973 1.0 0.049\nDistributions of molecular properties of LigGPT generated molecules while controlling\na single property are depicted in Figure 5. The Mean Average Deviation (MAD), Validity,\nuniqueness and novelty values for each property are reported in Table 3. As seen in Figure\n5, the distribution of properties are centered around the desired value. This is further\nexempliﬁed by the low MAD scores (relative to the range of the property values) in the\nTable 3.\nTable 4: Multi-property conditional training on GuacaMol dataset. Temperature 0.9 was\nused.\nCondition Validity Unique Novelty MAD TPSA MAD logP MAD SAS\nSAS+logP 0.991 0.935 1.0 - 0.241 0.12\nSAS+TPSA 0.992 0.929 1.0 3.543 - 0.133\nTPSA+logP 0.989 0.942 1.0 3.528 0.227 -\nTPSA+logP+SAS 0.99 0.874 1.0 3.629 0.254 0.158\nWhile generating molecules for speciﬁc purposes, in other words de novo design of mol-\ncules, it is necessary to optimize more than one property. For example, one may want mo-\nlecules that have a speciﬁc values for logP and TPSA values. Hence, we check the model’s\ncapacity to control multiple properties simultaneously. For this, SAS, logP and TPSA are\nused. We evaluate the model’s ability to generate desired distributions using two and three\nproperty controls at a time. Generated distribution of molecule properties is depicted in\n15\n(a)\n (b)\n(c)\n (d)\nFigure 6: Distribution of property of generated molecules conditioned on: (a) TPSA + logP\n(b) SAS + logP (c) SAS + TPSA (d) TPSA + logP + SAS. The values that are conditioned\nto are given in the Figures.\nFigure 6. Well separated clusters centered at the desired property values are observed. As\nbefore, the low MAD values for each property combination, reported in Table 4, (as com-\npared to the range of property values) indicate the strong control LigGPT has over multiple\nproperties for accurate generation.\n16\nFigure 7: Boxplot of the evaluation metrics for the scaﬀold conditioned results\nGeneration based on Scaﬀold\nThe above section demonstrated the ability of LigGPT to generate molecules with desired\nproperties. In certain exercises, for eg. lead optimization, chemists intend to generate mo-\nlecules containing a speciﬁc scaﬀold/skeleton and at the same time achieve desired property\nvalues. We evaluate the ability of LigGPT to generate structures with certain property val-\nues while maintaining the structure of the scaﬀold, results of which are presented in this\nand the next sections. We conduct these experiments on the MOSES benchmark dataset as\nit contains a set of test scaﬀolds that are non overlapping with the set of scaﬀolds that are\npresent in the training set. We select a random set of 100 test scaﬀolds, then generate 100\nmolecules for each scaﬀold followed by calculation of validity, uniqueness, novelty and ‘sim-\nilarity ratio’. ‘Similarity ratio’ is deﬁned as the fraction of valid generated molecules having\nTanimoto similarity of the scaﬀold of the generated molecule and the conditioned scaﬀold\ngreater than 0.8. The distribution of each of the metrics in terms of box plot, with the\nswarm plot overlaid on it, is shown in Figure 7. From the boxplot, it is seen that even after\nusing high temperature of 1.6, around 75 of the scaﬀolds have validity greater than 0.8. By\n17\nvirtue of high temperature, almost all the scaﬀolds have uniqueness greater than 0.8. All the\nscaﬀolds have novelty greater than 0.8. Around 75 scaﬀolds have ’similarity ratio’ greater\nthan 0.9, which suggests most of the generated valid molecules have very similar scaﬀold\nto the scaﬀold used for condition. Some examples of generated molecules for two scaﬀolds\nare given in Figure S2 of the Supporting Information. In all the generated molecules, the\nconditioned scaﬀold strucutre is maintained.\nGeneration based on Scaﬀold and Property\nWe evaluate the models ability to generate structures containing desired scaﬀolds while\ncontrolling molecular properties. For our experiments, ﬁve scaﬀolds of diﬀerent sizes were\nrandomly chosen from the MOSES test set (Figure S1 of the Supporting Information). In\nthese experiments, we deﬁne valid molecules as those molecular graphs that satisfy chemical\nvalencies and contain scaﬀolds that have a Tanimoto similarity of at least 0.8 to the desired\nscaﬀold. The validity score of all scaﬀold based experiments are calculated based on this\ndeﬁnition.\nGenerated distributions for single property control can be seen in Figure 8. Tanimoto\nsimilarity is calculated between the scaﬀold of the generated molecule and the conditional\nscaﬀold. Distribution of these Tanimoto similarity scores are also plotted in Figure 8. The\ndistribution plots peak at 1 for all the scaﬀolds and properties. Since scaﬀold based genera-\ntion is more constraining for property control, generated distributions are not as narrow and\nwell separated as before. The quantitative results for single property control are reported\nin Table S2 in the Supporting Information. The low MAD scores still show that LigGPT\ndeviates only slightly from intended values despite the constraints. QED is a function that\nis dependant on multiple molecular properties simultaneously. Therefore, QED is greatly\ninﬂuenced by the structure of the scaﬀold itself making it very hard to control under such\nconstraints. We believe such competing objectives are the reason for large overlap between\ndistributions generated for QED control. Figure S3 of the Supporting Information shows\n18\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\n(g)\n (h)\nFigure 8: Distribution of property of generated molecules conditioned on: Scaﬀold + (a)\nlogP (c) SAS (e) TPSA (g) QED . Distribution of tanimoto similarity of the scaﬀolds of\nthe generated molecules and the scaﬀold used for condition for (b) logP (d) SAS (f) TPSA\n(h) QED. Trained on MOSES dataset. Temperature 1.6 used.\n19\n(a)\n (b)\n(c)\n (d)\nFigure 9: Distribution of property of generated molecules conditioned on: Scaﬀold + (a)\nTPSA + logP (b) SAS + TPSA (c) SAS + logP (d) TPSA + logP + SAS. Trained on\nMOSES dataset and temperature 1.6 used.\nthe molecules conditioned on scaﬀold + logP and scaﬀold + SAS. LigGPT adds diﬀerent\nfunctional groups to the scaﬀold in order to get the desired property value. Multi-property\ncontrol clusters are plotted in Figure 9. Even when using multiple properties, we see the\nTanimoto similarity distributions peaking at 1 in Figure 10. Understandably, property-based\nclusters are not as well formed as before. However, there is a good separation between the\nclusters for two property control. The intended values of molecular properties are close to the\n20\n(a)\n (b)\n(c)\n (d)\nFigure 10: Distribution of tanimoto similarity of the scaﬀolds of the generated molecules\nand the scaﬀold used for condition for (a) TPSA + logP (b) SAS + TPSA (c) SAS + logP\n(d) TPSA + logP + SAS. Trained on MOSES dataset and temperature 1.6 used.\ncenters of these clusters. This can further be veriﬁed by results reported for multi-property\ncontrol in Table S3 in the Supporting Information. For three property control one of the\nclusters (red) is not well formed due to highly constraining property values. We see that the\nrest of the clusters are largely well formed and separated.\nOne Shot Lead Optimization\nDue to LigGPT’s strong ability to control molecular properties, we believe it has potential\nusage in real world problems such as lead optimization. We call this ‘one shot optimization’\nas it’s a one step process of providing the desired scaﬀold and properties for optimized\n21\nFigure 11: One shot optimization of QED value conditioned on the scaﬀold\nmolecule generation. To demonstrate this, three scaﬀolds from test set having QED around\n0.4 are sampled. Using these scaﬀolds and QED 0.9 as the condition, we generate molecules\nusing LigGPT. Sample generated molecules are shown in Figure 11. The scaﬀold structure\nis maintained in the generated molecules and their QED values are around 0.9. While the\nusefulness is demonstrated here only on QED, we would like to note that it could be adopted\nfor other molecular properties as well. Furthermore, it could also be used in other molecular\ndesign task such generation of active molecules with good docking scores for a particular\nprotein, provided, a dataset for the protein is available for training beforehand.\nConclusion\nIn this work, we designed a Transformer-Decoder model called LigGPT for molecular gen-\neration. This model utilises masked self-attention mechanisms that make it simpler to learn\nlong range dependencies between string tokens. This is especially useful to learn the se-\nmantics of valid SMILES strings that satisﬁes valencies and ring closures. We see through our\nbenchmarking experiments that LigGPT shows very good validity scores even with sampling\n22\ntemperature as high as 1.6 for the MOSES dataset and 0.9 for the GuacaMol dataset. Fur-\nthermore, as shown, this also allows the model to do well in low data regimes. The high\nsampling temperatures enables the model to generate large amounts of novel and unique\nmolecules. Therefore, LigGPT is able to show good performance on both datasets with it\noutperforming all other methods benchmarked on the GuacaMol dataset.\nWe also show that the model learns higher level chemical representations through mo-\nlecular property control. LigGPT is able to generate molecules with property values that\ndeviate only slightly from the exact values that are passed by the user. It’s also able to gen-\nerate molecules containing user speciﬁed scaﬀolds while controlling these properties. It does\nthis with good accuracy despite the constraining conditions of scaﬀold based drug design.\nThrough this, we convey LigGPT’s real world utility for molecular generation. Consequently,\nwe believe that the LigGPT model should be considered a strong architecture to be used by\nitself or incorporated into other molecular generation techniques.\nAcknowledgement\nThe authors thank Manasa Kondamadugu, Yashaswi Pathak, Sarvesh Mehta and Manan\nGoel for their comments during the preparation of the manuscript. We thank IHub-Data,\nIIIT Hyderabad for ﬁnancial support.\nSupporting Information Available\nSupporting Information contains results of training on 10,000 molecules and results of scaf-\nfold and property conditioning. Furthermore ﬁgures of scaﬀolds, generated molecules from\nscaﬀold conditioning as well as scaﬀold and property conditioning experiments are provided.\n23\nReferences\n(1) Polishchuk, P. G.; Madzhidov, T. I.; Varnek, A. Estimation of the size of drug-like\nchemical space based on GDB-17 data. Journal of computer-aided molecular design\n2013, 27, 675–679.\n(2) Kim, S.; Thiessen, P. A.; Bolton, E. E.; Chen, J.; Fu, G.; Gindulyte, A.; Han, L.; He, J.;\nHe, S.; Shoemaker, B. A., et al. PubChem substance and compound databases. Nucleic\nacids research2016, 44, D1202–D1213.\n(3) Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.;\nCourville, A.; Bengio, Y. Generative adversarial nets. Advances in neural information\nprocessing systems2014, 27, 2672–2680.\n(4) Karras, T.; Laine, S.; Aila, T. A style-based generator architecture for generative ad-\nversarial networks. Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 2019; pp 4401–4410.\n(5) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser,  L.;\nPolosukhin, I. Attention is all you need. Advances in neural information processing\nsystems. 2017; pp 5998–6008.\n(6) Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bid-\nirectional transformers for language understanding. arXiv preprint arXiv:1810.04805\n2018,\n(7) Chen, H.; Engkvist, O.; Wang, Y.; Olivecrona, M.; Blaschke, T. The rise of deep\nlearning in drug discovery. Drug discovery today2018, 23, 1241–1250.\n(8) Sanchez-Lengeling, B.; Aspuru-Guzik, A. Inverse molecular design using machine learn-\ning: Generative models for matter engineering. Science 2018, 361, 360–365.\n24\n(9) Brown, N.; Fiscato, M.; Segler, M. H.; Vaucher, A. C. GuacaMol: benchmarking models\nfor de novo molecular design. Journal of chemical information and modeling2019, 59,\n1096–1108.\n(10) Polykovskiy, D.; Zhebrak, A.; Sanchez-Lengeling, B.; Golovanov, S.; Tatanov, O.; Bely-\naev, S.; Kurbanov, R.; Artamonov, A.; Aladinskiy, V.; Veselov, M., et al. Molecular\nsets (moses): A benchmarking platform for molecular generation models. Frontiers in\npharmacology 2020, 11 .\n(11) Weininger, D. SMILES, a chemical language and information system. 1. Introduction\nto methodology and encoding rules. Journal of chemical information and computer\nsciences 1988, 28, 31–36.\n(12) Pathak, Y.; Laghuvarapu, S.; Mehta, S.; Priyakumar, U. D. Chemically interpretable\ngraph interaction network for prediction of pharmacokinetic properties of drug-like\nmolecules. Proceedings of the AAAI Conference on Artiﬁcial Intelligence. 2020; pp\n873–880.\n(13) Segler, M. H.; Kogej, T.; Tyrchan, C.; Waller, M. P. Generating focused molecule\nlibraries for drug discovery with recurrent neural networks. ACS central science2018,\n4, 120–131.\n(14) Gupta, A.; M¨ uller, A. T.; Huisman, B. J.; Fuchs, J. A.; Schneider, P.; Schneider, G.\nGenerative recurrent networks for de novo drug design. Molecular informatics 2018,\n37, 1700111.\n(15) Popova, M.; Isayev, O.; Tropsha, A. Deep reinforcement learning for de novo drug\ndesign. Science advances2018, 4, eaap7885.\n(16) Olivecrona, M.; Blaschke, T.; Engkvist, O.; Chen, H. Molecular de-novo design through\ndeep reinforcement learning. Journal of cheminformatics2017, 9, 48.\n25\n(17) Liu, Q.; Allamanis, M.; Brockschmidt, M.; Gaunt, A. Constrained graph variational\nautoencoders for molecule design. Advances in neural information processing systems\n2018, 31, 7795–7804.\n(18) Kusner, M. J.; Paige, B.; Hern´ andez-Lobato, J. M. Grammar variational autoencoder.\narXiv preprint arXiv:1703.019252017,\n(19) Simonovsky, M.; Komodakis, N. Graphvae: Towards generation of small graphs using\nvariational autoencoders. International Conference on Artiﬁcial Neural Networks. 2018;\npp 412–422.\n(20) Jin, W.; Barzilay, R.; Jaakkola, T. Junction tree variational autoencoder for molecular\ngraph generation. arXiv preprint arXiv:1802.043642018,\n(21) Lim, J.; Ryu, S.; Kim, J. W.; Kim, W. Y. Molecular generative model based on condi-\ntional variational autoencoder for de novo molecular design.Journal of cheminformatics\n2018, 10, 1–9.\n(22) Kadurin, A.; Nikolenko, S.; Khrabrov, K.; Aliper, A.; Zhavoronkov, A. druGAN: an\nadvanced generative adversarial autoencoder model for de novo generation of new mo-\nlecules with desired molecular properties in silico. Molecular pharmaceutics 2017, 14,\n3098–3104.\n(23) Putin, E.; Asadulaev, A.; Vanhaelen, Q.; Ivanenkov, Y.; Aladinskaya, A. V.; Aliper, A.;\nZhavoronkov, A. Adversarial threshold neural computer for molecular de novo design.\nMolecular pharmaceutics2018, 15, 4386–4397.\n(24) Polykovskiy, D.; Zhebrak, A.; Vetrov, D.; Ivanenkov, Y.; Aladinskiy, V.; Mamoshina, P.;\nBozdaganyan, M.; Aliper, A.; Zhavoronkov, A.; Kadurin, A. Entangled conditional\nadversarial autoencoder for de novo drug discovery. Molecular pharmaceutics 2018,\n15, 4398–4405.\n26\n(25) Hong, S. H.; Ryu, S.; Lim, J.; Kim, W. Y. Molecular Generative Model Based on an\nAdversarially Regularized Autoencoder. Journal of Chemical Information and Modeling\n2019, 60, 29–36.\n(26) Ar´ us-Pous, J.; Johansson, S. V.; Prykhodko, O.; Bjerrum, E. J.; Tyrchan, C.; Rey-\nmond, J.-L.; Chen, H.; Engkvist, O. Randomized SMILES strings improve the quality\nof molecular generative models. Journal of cheminformatics2019, 11, 1–13.\n(27) Bjerrum, E. J. SMILES enumeration as data augmentation for neural network modeling\nof molecules. arXiv preprint arXiv:1703.070762017,\n(28) Bjerrum, E. J.; Sattarov, B. Improving chemical autoencoder latent space and molecular\nde novo generation diversity with heteroencoders. Biomolecules 2018, 8, 131.\n(29) Pathak, Y.; Juneja, K. S.; Varma, G.; Ehara, M.; Priyakumar, U. D. Deep learning\nenabled inorganic material generator. Physical Chemistry Chemical Physics2020, 22,\n26935–26943.\n(30) Prykhodko, O.; Johansson, S. V.; Kotsias, P.-C.; Ar´ us-Pous, J.; Bjerrum, E. J.; Engk-\nvist, O.; Chen, H. A de novo molecular generation method using latent vector based\ngenerative adversarial network. Journal of Cheminformatics2019, 11, 74.\n(31) Guimaraes, G. L.; Sanchez-Lengeling, B.; Outeiral, C.; Farias, P. L. C.; Aspuru-\nGuzik, A. Objective-reinforced generative adversarial networks (ORGAN) for sequence\ngeneration models. arXiv preprint arXiv:1705.108432017,\n(32) Sanchez-Lengeling, B.; Outeiral, C.; Guimaraes, G. L.; Aspuru-Guzik, A. Optimizing\ndistributions over molecular space. An objective-reinforced generative adversarial net-\nwork for inverse-design chemistry (ORGANIC). 2017,\n(33) De Cao, N.; Kipf, T. MolGAN: An implicit generative model for small molecular graphs.\narXiv preprint arXiv:1805.119732018,\n27\n(34) Putin, E.; Asadulaev, A.; Ivanenkov, Y.; Aladinskiy, V.; Sanchez-Lengeling, B.; Aspuru-\nGuzik, A.; Zhavoronkov, A. Reinforced adversarial neural computer for de novo mo-\nlecular design. Journal of chemical information and modeling2018, 58, 1194–1204.\n(35) G´ omez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hern´ andez-Lobato, J. M.; S´ anchez-\nLengeling, B.; Sheberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.;\nAspuru-Guzik, A. Automatic chemical design using a data-driven continuous repres-\nentation of molecules. ACS central science2018, 4, 268–276.\n(36) Mehta, S.; Laghuvarapu, S.; Pathak, Y.; Sethi, A.; Alvala, M.; Priyakumar, U. D.\nEnhanced Sampling of Chemical Space for High Throughput Screening Applications\nusing Machine Learning.\n(37) Winter, R.; Montanari, F.; Steﬀen, A.; Briem, H.; No´ e, F.; Clevert, D.-A. Eﬃcient\nmulti-objective molecular optimization in a continuous latent space. Chemical science\n2019, 10, 8016–8024.\n(38) Kim, K.; Kang, S.; Yoo, J.; Kwon, Y.; Nam, Y.; Lee, D.; Kim, I.; Choi, Y.-S.; Jung, Y.;\nKim, S., et al. Deep-learning-based inverse design model for intelligent discovery of\norganic molecules. npj Computational Materials2018, 4, 1–7.\n(39) Maziarka,  L.; Pocha, A.; Kaczmarczyk, J.; Rataj, K.; Danel, T.; Warcho l, M. Mol-\nCycleGAN: a generative model for molecular optimization.Journal of Cheminformatics\n2020, 12, 1–18.\n(40) Zhu, J.-Y.; Park, T.; Isola, P.; Efros, A. A. Unpaired image-to-image translation using\ncycle-consistent adversarial networks. Proceedings of the IEEE international conference\non computer vision. 2017; pp 2223–2232.\n(41) Kotsias, P.-C.; Ar´ us-Pous, J.; Chen, H.; Engkvist, O.; Tyrchan, C.; Bjerrum, E. J.\nDirect steering of de novo molecular generation with descriptor conditional recurrent\nneural networks. Nature Machine Intelligence2020, 2, 254–265.\n28\n(42) Ar´ us-Pous, J.; Patronov, A.; Bjerrum, E. J.; Tyrchan, C.; Reymond, J.-L.; Chen, H.;\nEngkvist, O. SMILES-based deep generative scaﬀold decorator for de-novo drug design.\nJournal of Cheminformatics2020, 12, 1–18.\n(43) Lim, J.; Hwang, S.-Y.; Kim, S.; Moon, S.; Kim, W. Y. Scaﬀold-based molecular design\nusing graph generative model. arXiv preprint arXiv:1905.136392019,\n(44) Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I. Improving language under-\nstanding by generative pre-training.\n(45) Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language models\nare unsupervised multitask learners. OpenAI blog2019, 1, 9.\n(46) Gaulton, A.; Hersey, A.; Nowotka, M.; Bento, A. P.; Chambers, J.; Mendez, D.;\nMutowo, P.; Atkinson, F.; Bellis, L. J.; Cibri´ an-Uhalte, E., et al. The ChEMBL data-\nbase in 2017. Nucleic acids research2017, 45, D945–D954.\n(47) Landrum, G. RDKit: A software suite for cheminformatics, computational chemistry,\nand predictive modeling. 2013.\n(48) Bemis, G. W.; Murcko, M. A. The properties of known drugs. 1. Molecular frameworks.\nJournal of medicinal chemistry1996, 39, 2887–2893.\n(49) Ertl, P.; Schuﬀenhauer, A. Estimation of synthetic accessibility score of drug-like mo-\nlecules based on molecular complexity and fragment contributions. Journal of chemin-\nformatics 2009, 1, 8.\n(50) Bickerton, G. R.; Paolini, G. V.; Besnard, J.; Muresan, S.; Hopkins, A. L. Quantifying\nthe chemical beauty of drugs. Nature chemistry 2012, 4, 90–98.\n(51) Clark, D. E. Rapid calculation of polar molecular surface area and its application to\nthe prediction of transport phenomena. 1. Prediction of intestinal absorption. Journal\nof Phatmaceutical Sciences1999,\n29\nGraphical TOC Entry\n30",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7600834369659424
    },
    {
      "name": "Interpretability",
      "score": 0.6534489393234253
    },
    {
      "name": "Transformer",
      "score": 0.6469918489456177
    },
    {
      "name": "Generative grammar",
      "score": 0.5383315086364746
    },
    {
      "name": "Security token",
      "score": 0.5377562046051025
    },
    {
      "name": "Notation",
      "score": 0.5328652858734131
    },
    {
      "name": "Generative model",
      "score": 0.51735919713974
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4771242141723633
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3460357189178467
    },
    {
      "name": "Engineering",
      "score": 0.10980463027954102
    },
    {
      "name": "Electrical engineering",
      "score": 0.07987883687019348
    },
    {
      "name": "Arithmetic",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}