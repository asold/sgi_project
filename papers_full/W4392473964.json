{
    "title": "Evaluation of Transformer-Based Neural Language Models for Writing Feedback and Automated Essay Scoring",
    "url": "https://openalex.org/W4392473964",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Temesgen Abraha",
            "affiliations": [
                "Zayed University"
            ]
        },
        {
            "id": "https://openalex.org/A2158610270",
            "name": "Amril Nazir",
            "affiliations": [
                "Zayed University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4281567365",
        "https://openalex.org/W4291034094",
        "https://openalex.org/W4205752798",
        "https://openalex.org/W4281750788",
        "https://openalex.org/W4300716842",
        "https://openalex.org/W3141704971",
        "https://openalex.org/W3111733452",
        "https://openalex.org/W4210411337",
        "https://openalex.org/W4253281431",
        "https://openalex.org/W3088026650",
        "https://openalex.org/W2792654535",
        "https://openalex.org/W3087889967",
        "https://openalex.org/W3035982729",
        "https://openalex.org/W3164655521",
        "https://openalex.org/W4206450273",
        "https://openalex.org/W3201077663",
        "https://openalex.org/W4212813292",
        "https://openalex.org/W4205523551",
        "https://openalex.org/W4206068088",
        "https://openalex.org/W4296079463",
        "https://openalex.org/W4200246476",
        "https://openalex.org/W4293526662",
        "https://openalex.org/W4288400169",
        "https://openalex.org/W3117359318",
        "https://openalex.org/W4212949095",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2296609147",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3095645723",
        "https://openalex.org/W2952060378",
        "https://openalex.org/W3104108820",
        "https://openalex.org/W2064471662",
        "https://openalex.org/W2511117069",
        "https://openalex.org/W4280510965",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3098654368",
        "https://openalex.org/W4285071183",
        "https://openalex.org/W2168727593",
        "https://openalex.org/W4220783775",
        "https://openalex.org/W4206183159",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W3170567868",
        "https://openalex.org/W3095483082",
        "https://openalex.org/W4285284811",
        "https://openalex.org/W3139572500"
    ],
    "abstract": "<title>Abstract</title> Writing remains a challenging skill for many students due to inadequate feedback tools. There is a need to develop more effective tools for supporting students’ writing skill development. This study aims to evaluate the effectiveness of transformer-based neural language models for assessing and automatically scoring argumentative essays written by 8th-12th grade English Language Learners (ELLs). The students’ English essays were assessed and scored based on six criteria, including cohesion, syntax, vocabulary, phraseology, grammar, and conventions. The models were trained on real teacher feedback from 2700 scored essays. We also compared various transformer-based neural language models to find the most effective model. Several metrics were used for evaluation, with the root mean square error (RMSE) as the primary measure. The results show that a specific model, DeBERTa-v3-large, outperforms others in most categories. In conclusion, this study suggests that transformer-based neural language models, especially when using the DeBERTa-v3-large model, hold significant promise in improving automated essay scoring and feedback, potentially leading to enhanced writing skills among English language learners.",
    "full_text": "Evaluation of Transformer-Based Neural Language\nModels for Writing Feedback and Automated Essay\nScoring\nTemesgen Abraha \nZayed University\nAmril Nazir \nZayed University\nResearch Article\nKeywords: Automated Essay Scoring (AES), Transformer-Based Neural Language Models, DeBERTa-v3-\nlarge\nPosted Date: March 5th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3979085/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nEvaluation of Transformer-Based Neural\nLanguage Models for Writing Feedback and\nAutomated Essay Scoring\nTemesgen Abraha1 and Amril Nazir1*†\n1*College of Technological Innovation, Zayed University, Khalifa City,\nAbu Dhabi, United Arab Emirates.\n*Corresponding author(s). E-mail(s): mohd.nazir@zu.ac.ae;\nContributing authors: temesgen.mikael@gmail.com;\n†These authors contributed equally to this work.\nAbstract\nWriting remains a challenging skill for many students due to ina dequate feed-\nback tools. There is a need to develop more eﬀective tools for sup porting\nstudents’ writing skill development. This study aims to eval uate the eﬀective-\nness of transformer-based neural language models for assessing an d automatically\nscoring argumentative essays written by 8th-12th grade English Language Learn-\ners (ELLs). The students’ English essays were assessed and sco red based on six\ncriteria, including cohesion, syntax, vocabulary, phraseolog y, grammar, and con-\nventions. The models were trained on real teacher feedback from 270 0 scored\nessays. We also compared various transformer-based neural languag e models to\nﬁnd the most eﬀective model. Several metrics were used for evalua tion, with the\nroot mean square error (RMSE) as the primary measure. The results show that\na speciﬁc model, DeBERTa-v3-large, outperforms others in most cat egories. In\nconclusion, this study suggests that transformer-based neural l anguage models,\nespecially when using the DeBERTa-v3-large model, hold signi ﬁcant promise in\nimproving automated essay scoring and feedback, potentially l eading to enhanced\nwriting skills among English language learners.\nKeywords: Automated Essay Scoring (AES), Transformer-Based Neural L anguage\nModels, DeBERTa-v3-large\n1\n1 Introduction\nEnglish is an essential language to master in today’s globalized world, wi th writing\nbeing a particularly crucial aspect for securing admission to prest igious universities and\nattaining top job opportunities [\n1–3]. The demand for students to improve their writing\nskills has grown signiﬁcantly due to globalization [ 4]. However, writing often proves\nto be the weakest skill among many English Language Learners (ELLs) [ 5]. Learning\nevaluations and feedback are vital components of the teaching process, c ontributing\nsubstantially to student learning. The primary purpose of teaching as sessment is to\naccurately gauge students’ understanding of course materials and provi de tailored\nsupport accordingly [ 6]. Unfortunately, the number of qualiﬁed English teachers is\nlimited compared to the high number of ELLs [ 7]. As a result, it is necessary to\nharness technology, speciﬁcally transformer-based neural language mode ls, to address\nthis resource deﬁciency.\nSeveral studies [ 6, 8–10] have investigated how automated writing evaluation using\ntransformer-based neural language models can assist ELLs in enhancing the ir writing\nskills. The ultimate goal is to provide customized teaching material s to help students\nimprove their writing abilities. In this research paper, the obj ective is to compare the\npredictions made by various transformer-based neural language models to determine\nthe most eﬀective approach for evaluating student writing and provid ing valuable\nfeedback. The potential of transformer-based neural language model tech niques for\nenhancing automated essay scoring and feedback tools will be explored . The study\nfocuses on a comprehensive evaluation of student writing by examini ng six key analyt-\nical measures: cohesion, syntax, vocabulary, phraseology, grammar, and conv entions.\nThis holistic approach distinguishes the research from previous st udies, which typi-\ncally concentrated on a single or a couple of linguistic aspects. For examp le, earlier\nresearch has investigated speciﬁc measures such as cohesion [ 11, 12] or spelling and\ngrammar [ 13, 14].\nPrevious works have implemented BERT and RoBERTa for automated essay scor-\ning [ 15–18] to address the challenges faced by ELLs and the insuﬃcient number of\nEnglish teachers. However, BERT and RoBERTa do not consider the relat ive and abso-\nlute positions of words in sentences, which may limit their eﬀect iveness. DeBERTa, in\ncontrast, employs techniques that account for both relative and absolut e positions of\nwords [ 19], resulting in improved performance on natural language processing ( NLP)\ntasks. We believe that by utilizing DeBERTa, we can achieve enhance d performance\nand more meaningful results, ultimately leading to better support f or students in their\nwriting skill development. By leveraging state-of-the-art natural language processing\nmodels, such as DeBERTa-v3-large, we aspire to create more accurate fee dback tai-\nlored to each student’s unique competencies. This improvement in automated feedback\ntools can then be utilized to generate personalized writing tasks, u ltimately fostering\nbetter writing skills among English language learners.\nIn our dataset, essays are evaluated based on six analytical measures, wit h scores\nranging from 1.0 to 5.0, highlighting proﬁciency in each domain. Cohesion is the ﬁrst\nmeasure, emphasizing the connectedness of ideas. For instance, a se ntence like ”The\nmovie was enthralling from start to ﬁnish, smoothly transitioning b etween scenes,”\n2\nreﬂects high cohesion, whereas ”The movie was enthralling. The scene s were ran-\ndom,” indicates a lack of connectedness. Next is Syntax, which revolv es around the\narrangement of words to form coherent sentences. A sentence like ”Alth ough it was\nraining, we decided to go for a walk in the park,” exempliﬁes good Synt ax, in contrast\nto the jumbled ”Although raining, walk park we go.” Vocabulary assesses the range\nand speciﬁcity of words used. ”The verdant landscape was dotted with vi brant ﬂow-\ners” showcases a richer vocabulary than ”The green place had many prett y ﬂowers.”\nPhraseology delves into the proper use of idiomatic expressions. Using ”The early bird\ncatches the worm” correctly demonstrates good phraseology, unlike the misphrased\n”The fast bird gets the small creature.” Grammar underscores the impor tance of lan-\nguage rules, with ”She quickly ﬁnished her homework and then went outs ide to play”\nbeing grammatically sound, in contrast to the error-laden ”She quickly ﬁnished home-\nwork and went out to play with friends.” Lastly, Conventions stress t he correct use\nof formatting, spelling, and capitalization. A sentence like ”It’s esse ntial to proof-\nread your work” observes good conventions, while ”Its important to proof read youre\nwork” exhibits errors. Each of these measures is crucial in evaluatin g the overall qual-\nity and clarity of an essay. By encompassing a broader range of analytical meas ures,\nthis research paper aims to provide a more comprehensive understan ding of student\nwriting and deliver more eﬀective feedback for their improvemen t.\nIn this paper, our overarching aim is to further the existing body of knowledge in\nautomated essay scoring by leveraging transformer-based neural language m odel tech-\nniques, with the goal of enhancing feedback generated by contemporary to ols. To fulﬁll\nthis aim, our objectives encompass a rigorous literature review on automat ed essay\nscoring, emphasizing the breadth of analytical measure coverage. We the n introduce\nthe use of DeBERTa, a state-of-the-art transformer model, for an all-en compassing\nevaluation of student writing, integrating six analytical measures. A s ide-by-side anal-\nysis is conducted, explicitly utilizing a dataset of argumentative essays, contrasting the\neﬃcacy of DeBERTa with other state-of-the-art transformer models su ch as BERT,\nLongformer, and RoBERTa. We prioritize RMSE as our primary evaluation met-\nric, ensuring larger errors are penalized, yielding a genuine portr ayal of a model’s\ncapabilities. Diﬀering from prior research, our approach holistically evaluates student\nessays via six analytical measures, oﬀering a more detailed and preci se assessment and\nsubsequently enhancing feedback for learners. By comparing DeBER Ta with other\nrenowned models, this research highlights the most potent method ologies for student\nessay evaluation, providing invaluable guidance for future research and the improve-\nment of automated essay scoring systems. Ultimately, the insights de rived from this\nstudy hold the potential to spark the creation of advanced feedback tools , capable\nof creating individualized writing exercises that resonate with e ach learner’s distinct\nrequirements, thereby improving the writing proﬁciency of En glish language learners.\nIn the upcoming sections, we ﬁrst discuss a detailed literature r eview in Section 2,\nexploring the previous research in the ﬁeld of automated essay scorin g and transformer-\nbased neural language models. Next, Section 3 outlines the methodology, d etailing the\ndataset, models, and evaluation metrics. Then, in Section 4, we prese nt and discuss\nthe results and ﬁndings, comparing the performance of the various natur al language\nprocessing models. Section 5 concludes the report, summarizing the main ﬁndings\n3\nand discussing our study’s potential contributions. Finally, Sect ion 6 presents future\nwork, discussing possible directions for further research. This structure aims to oﬀer a\nthorough exploration of automated essay scoring, highlighting our approach, re sults,\nand contributions to the ﬁeld.\n2 Related Work\nThis section provides a detailed review of recent research in the ﬁeld of automated\nessay evaluation systems and their practical applications.\n2.1 The Imperative for Automated Essay Scoring Systems and\nTheir Advancements\nThe ﬁeld of automated essay scoring (AES) has witnessed a resurgence of interest,\ndriven by signiﬁcant advancements in natural language processing and tr ansformer-\nbased neural language models. These studies collectively underscor e the eﬀectiveness\nof AES systems in strengthening students’ writing skills. Early work by [\n20] demon-\nstrated the potential of AES for score prediction in text-based analytic al writing. They\ndesigned a rubric-based model surpassing competitive baselines and showed general-\nizability, underlining the importance of feature groups aligning wit h rubric criteria.\nSubsequently, [ 21] further substantiated the reliability of AES systems by applying\nthe Syntactic Complexity Analyzer (SCA) and Coh-Metrix, two widely used computa-\ntional tools for assessing language complexity in written English. As the ﬁeld matured,\nresearch began to investigate AES in practical educational settings. [ 22] studied the\nuse of MI Write, an automated writing evaluation software, in elementary schools,\nﬁnding that teachers and students held positive attitudes despit e limited use. In the\nsame year, [ 23] showcased the eﬀectiveness of Pigai.org, an AES system in China, in\nimproving students’ writing proﬁciency.\nResearch carried out by [ 24] and [ 25] compared the agreement levels between\nhuman graders and an AES system. Remarkably, the results revealed a str ong simi-\nlarity in scoring, with AES systems requiring signiﬁcantly less ti me. This led to the\nassertion that AES systems could outperform human evaluators in terms of r eliability\nwith continued improvement and training. Building on this foundati on, recent work\nby [ 26], [ 27], and [ 28] showcased the potential of transformer-based neural language\nmodel algorithms in AES systems that grade written work based on various li nguistic\nfeatures. [ 10] compared the eﬀectiveness of automated evaluation tools with human\nevaluation, ﬁnding that timely and comprehensive feedback from automat ed tools\nimproved student writing skills more than human evaluations. Furt hermore, [ 29] pro-\nvided additional evidence of the reliability of AES systems through an i nvestigation\nof MI Write, demonstrating the system’s consistency and suggesting that such tools\ncan greatly enhance student writing skills by providing valuable an d timely feedback.\n4\n2.2 The Role of Advanced Language Models in AES Systems\nSeveral studies have shown the advancements in natural language proce ssing (NLP)\nand transformer-based neural language model have signiﬁcantly inﬂuenc ed the devel-\nopment and eﬀectiveness of automated essay scoring (AES) systems [\n6, 9, 13, 15, 18,\n30–38].\nEarly eﬀorts by [ 30] examined the potential of transformer-based neural lan-\nguage model techniques, especially neural networks, in automated tex t scoring. They\ndesigned a model that utilized a convolutional neural network for learn ing textual\nfeatures and an LSTM layer for capturing context. [ 13] performed a comprehen-\nsive literature review of AES using machine learning, emphasizing t he shift toward\ntransformer-based neural language model techniques. Their system e mployed 22 high-\npredictive-power features and the Random Forest algorithm, showcasi ng promising\nresults in score prediction when compared to human graders. This high lights the\npotential of NLP and machine learning for creating reliable and eﬃcient sc oring\nsystems for writing tasks. The momentum continued with the work of [ 31], who devel-\noped an Essay Scoring System that primarily uses linear regression and the Na¨ ıve\nBayes classiﬁer algorithm for evaluation. In the next year, [ 15] investigated the use of\nBERT and RoBERTa models for AES, ﬁnding RoBERTa superior in terms of ac cu-\nracy and eﬃciency. This was followed by studies from [ 32] and [ 33], which utilized\ntransformer models for AES tasks. [ 32] demonstrated that these models outperformed\nregression-based approaches in politeness classiﬁcation tasks.\n[34] conducted an extensive survey on transformer-based neural language mo del\ntechniques for automated evaluation of students’ free-text response s. They identiﬁed\nthe incorporation of wider contexts into neural representations as an em erging trend.\n[35] developed an AES system called ”FactoGrade” that employs NLP and LSTM\ntechniques to grade and provide feedback on various aspects of student s’ writing. Stud-\nies by [ 9], [ 6], and [ 36] underscored the potential of NLP in providing students with\ncomprehensive and timely feedback. [ 9] found that students who received feedback\ngenerated by an NLP-based system produced higher-quality essays. [ 6] emphasized\nthe use of modern technologies such as corpus analysis and artiﬁcial inte lligence for\nrapid assessment and automatic reﬁnement of students’ essays. Expl oring the poten-\ntial of transformer models in AES has been a recent focus. [ 18] also used BERT and\nRoBERTa for AES tasks. [ 38] proposed a multi-task learning approach for AES that\noutperformed the baseline system in key statistical measures. Lastl y, [ 37] highlighted\nthe time-consuming nature of manual essay grading and the potential of aut omated\ngraders built on top of language models like BERT, RoBERTa, and DeBERTa. The ir\nstudy found the DeBERTa-based grader to be the most eﬀective, espec ially in deter-\nmining sentence similarity and grading domain-speciﬁc data. These s tudies collectively\ndemonstrate the crucial role of advanced language models in the developm ent and\neﬀectiveness of AES systems.\n2.3 Analytical Measures and Their Impact on AES Systems\nThe eﬀectiveness and accuracy of automated essay scoring (AES) system s often hinge\non the particular analytical measures incorporated into the systems. T hese measures\n5\nencompass various linguistic aspects such as text cohesion, grammar, spe lling, and\ntypes of errors [ 11–14, 31, 39, 40]\nIn the early stages, [ 11] explored the use of computational indices of text cohesion\nto gauge the impact of Intelligent Tutoring Systems (ITSs) and Automated Writing\nEvaluation (AWE) systems on student writing development. This wor k was extended\nby [ 39] with the introduction of TAACO, a text analysis tool with over 150 ind ices\nrelated to text cohesion. The study found a positive correlation betw een expert judg-\nments of text coherence and quality and global cohesion indices, suggesti ng that expert\nraters perceive coherence as a feature of global rather than local cohesion . Following\na similar trajectory, [ 40] examined the linguistic features that could enhance AES in\nnon-native English essay datasets. Although their feature set was eﬀec tive, the most\npredictive features varied across datasets, hinting at the depende ncy of predictive\nfeature power on the accuracy of underlying tools and the writer’s nat ive language.\n[12] later proposed a machine-learning approach for automated essay grading fo cus-\ning on textual cohesion. They underlined the importance of normalizati on and class\nbalancing techniques to improve results with a limited dataset. [13] demonstrated the\npotential of NLP and transformer-based neural language models in creating re liable\nand rapid scoring systems. They devised an AES approach that combined ru le-based\ngrammar and consistency tests with semantic sentence similarity, y ielding promising\nresults in score prediction compared to human graders. In a diﬀerent approach con-\ncerning grammar and spelling, [ 31] developed an Essay Scoring System that assesses\nessays based on these elements, primarily using linear regression and the Na¨ ıve Bayes\nclassiﬁer algorithm.\nOn the other hand, [ 14] found that strategy feedback with revision opportunities\nimproved essay quality, while spelling and grammar feedback oﬀered modest yet com-\nplementary advantages. This study underscores the importance of inte grating both\nfeedback types in automated writing evaluation and tutoring systems f or high school\nstudents. The research by [ 41] in the ﬁeld of Automatic Essay Scoring (AES) pri-\nmarily addressed essays penned by native English speakers while al so incorporating\ncontributions from non-native English speakers. They extended the scoring categories\nto include cohesion, syntax, vocabulary, phraseology, grammar, and conven tions, thus\nbroadening the scoring detail. [ 41] implemented several models and devised a novel\ntwo-step training method for their combined model. Furthermore, they found that pre-\ntrained models consistently outperformed handcrafted models. Th eir use of Quadratic\nWeighted Kappa (QWK) as a performance evaluation metric resulted in an aver-\nage QWK score of 0.669. Contrasting with [ 41], the current research paper employs\nRoot Mean Square Error (RMSE) as the primary evaluation metric, supple mented\nby four additional evaluation metrics. This approach oﬀers a more precise gauge of\nmodel performance, allowing for a nuanced understanding of the mode ls’ strengths\nand weaknesses across multiple dimensions. Moreover, while [ 41] compared hand-\ncrafted models with pre-trained models, this research paper engage s in a comparative\nanalysis of the latest state-of-the-art models, including deberta- v3-large, deberta-v3-\nbase, roberta-large and other equivalents. This up-to-date comparison pr ovides a more\ncomprehensive evaluation framework and a focus on the latest AES models , thus\ncontributing to a more precise and nuanced understanding of model performance in\nAES.\n6\nThe collective ﬁndings from these studies underscore the critic al impact of var-\nious analytical measures on the performance and reliability of AES system s. They\nhighlight the need for AES systems to integrate various types of analytic al measures\nand feedback methods to ensure a comprehensive assessment and eﬀe ctive learning\noutcomes.\n2.4 The Eﬃcacy and Limitations of Current AES Systems\nCurrent automated essay scoring (AES) systems demonstrate eﬃcacy in multiple\naspects, yet they carry inherent limitations that necessitate a m ore comprehensive\napproach [\n22, 33, 42–44].\n[42] and [ 45], pointed out that some Automated Essay Evaluation (AEE) systems\ntend to assign high scores to lengthy essays that exhibit syntactic c omplexity and a\nrich vocabulary, which can mislead the accurate evaluation of the conten t. Extending\nthis observation, [ 22] found a strong correlation between the scores assigned by Project\nEssay Grade (PEG), an AES technology, and the length of an essay. Both thes e studies\nunderscore a critical ﬂaw of machine-learning-based AES systems, wh ich often equate\nessay quality with the number of words in the essay and syntactic com plexity, poten-\ntially misrepresenting the actual quality of the content. Another li mitation of AES\nsystems was brought to light by [ 43], who revealed that these systems, often used to\nassess short answer tasks, can be easily manipulated by unrelated resp onses or ran-\ndom word rearrangement. They demonstrated that even basic adversarial te chniques\ncan disrupt these systems’ performance. While adversarial trainin g has been proposed\nto strengthen system resilience, it has yet to entirely resolve the issue.\nDespite these limitations, the eﬃcacy of AES systems should not be di scounted. [33]\nstressed the need for automated graders due to the time-consuming nat ure of manual\nessay grading and its impact on the learning experience of students. They compared\nthe performance of automated graders based on BERT, RoBERTa, and DeBERTa\nlanguage models using three datasets and speciﬁc evaluation metrics. Am ong these\nmodels, the DeBERTa-based grader outperformed the rest, especial ly in determining\nsentence similarity and handling domain-speciﬁc data. However, [\n44], while acknowl-\nedging the signiﬁcant potential of automated writing evaluation systems to positively\nimpact students’ writing skills, maintained that feedback from te achers or peers is\nsuperior. The current research paper diﬀers from and advances beyond these studies\nin several critical ways. Unlike [ 33], who relied on three diﬀerent language models,\nour research undertakes a more expansive comparison of eight diﬀerent s tate-of-the-\nart NLP models, which are extensions of BERT, RoBERTa, and DeBERTa. This\nbroader comparison allows us to thoroughly evaluate and leverage each model’ s unique\nstrengths. Additionally, our approach is more comprehensive as it evaluat es student\nwriting across six diﬀerent analytical measures rather than primaril y assessing simi-\nlarity rating predictions. This ensures a more holistic and nuanced understanding of\nthe quality of student writing. We also employ a dataset of argumentativ e essays from\nEnglish Language Learners, which might present unique challenges and oppor tunities\ncompared to the datasets used in the previous study. These diﬀeren ces underline the\nenhanced depth, speciﬁcity, and comprehensiveness of our approach, promising a more\naccurate and robust essay evaluation.\n7\nIn spite of the recognized eﬃcacy and limitations of existing AES syste ms, it is\nclear that a more comprehensive solution is necessary to address thes e shortcomings\neﬀectively. This paper aims to ﬁll that gap by proposing a novel and holi stic approach\nto AES, which critically assesses and integrates six diﬀerent analyt ical measures rather\nthan relying solely on one or a few. This multifaceted evaluation allows us to cap-\nture a broader spectrum of quality indicators in student essays, t hereby providing\na more accurate and comprehensive evaluation of their writing skills. Moreover, our\napproach extends beyond the breadth of evaluation criteria. We impleme nt a com-\nparative study of eight diﬀerent state-of-the-art NLP models, which ar e extensions of\nBERT, RoBERTa, and DeBERTa. This expansive comparison allows us to lev erage\nthe unique strengths of each model, thereby improving the robust ness and accuracy\nof our AES system. By taking into consideration a broader set of analytical m easures\nand leveraging the latest advancements in NLP models, our approach aims to o ver-\ncome some of the limitations observed in current AES systems. This com prehensive\nand innovative approach promises to provide more meaningful feedback t o students,\nempowering them to enhance their writing skills eﬀectively. The synthesis of the poten-\ntial and drawbacks of existing AES systems, as highlighted in this rev iew, underscores\nthe crucial need for the comprehensive and holistic approach proposed i n this research.\nBy addressing these limitations head-on, we aim to make a signiﬁcant con tribution to\nthe ﬁeld of automated essay scoring, ultimately leading to improved s tudent outcomes.\n3 Methodology\nIn this section, the methodology for developing an automated writing ev aluation sys-\ntem that leverages transformer-based neural language model techniques to provide\ncomprehensive feedback on student essays is outlined. The primar y objectives include\napplying DeBERTa as a novel transformer model for comprehensive ev aluation of stu-\ndent writing, incorporating six key analytical measures, and conducti ng a comparative\nanalysis of the DeBERTa model with other state-of-the-art transforme r models, such\nas BERT, Longformer, and RoBERTa. This analysis evaluates their strengths and\nweaknesses in predicting diﬀerent analytical measures using a dat aset of argumenta-\ntive essays and a range of evaluation metrics, with a focus on RMSE as the primary\nmetric. The dataset, methods, and evaluation metrics will be discu ssed in detail in the\nfollowing subsections.\n3.1 Dataset\nThe dataset utilized in this study is obtained from a Kaggle competition focused on\nEnglish Language Learning [\n46]. As summarized in table 1 below, the dataset consists\nof argumentative essays written by English Language Learners (ELLs) from grade s\n8 to 12, which have been evaluated and scored according to six analytical m easures.\nThese measures encompass cohesion, syntax, vocabulary, phraseology, gram mar, and\nconventions, with each representing a unique aspect of essay writ ing expertise. The\nscores for these criteria range from 1.0 to 5.0 in increments of 0.5, wh ere a higher\nscore signiﬁes a higher proﬁciency in the corresponding measure. In addition to the\nsix analytical measures, the dataset includes the full text attribute, containing the\n8\nentire essay, and the text id attribute, which assigns a unique identiﬁer to each piece.\nThe dataset features a total of 2700 essays in the test set and has been util ized and\nimplemented for the objectives of this study.\nTable 1 Dataset Description\nAttribute Description\nfull text Contains the entire essay text\ntext id Assigns a unique identiﬁer to each essay\ncohesion Score evaluating the logical ﬂow and organization of the ess ay (1.0 to 5.0,\nincrements of 0.5)\nsyntax Score assessing sentence structure and complexity (1.0 to 5 .0, increments of\n0.5)\nvocabulary Score measuring the range and appropriate use of words (1.0 t o 5.0, increments\nof 0.5)\nphraseology Score examining the proper use of phrases and idiomatic expr essions (1.0 to\n5.0, increments of 0.5)\ngrammar Score evaluating the accurate use of grammar rules (1.0 to 5. 0, increments of\n0.5)\nconventions Score assessing the correct use of punctuation, capitaliza tion, and spelling\n(1.0 to 5.0, increments of 0.5)\nTotal Essays The test set consists of 2700 essays\n3.2 Methods\nIn this research paper, we employed various state-of-the-art transf ormer-based neural\nlanguage models for natural language processing tasks to evaluate their eﬀe ctive-\nness in automated essay scoring. The models under consideration incl ude RoBERTa,\nDeBERTa-v3-base, DeBERTa-v3-small, DeBERTa-v3-large, XML-RoBERTa-base ,\nXML-RoBERTa-longformer-base, and BigBird-RoBERTa-base. BERT and DeBERT a\nare transformer-based models that process input text by tokenizin g it, creating embed-\ndings, and feeding them through multiple layers. BERT uses standar d transformer\nlayers with self-attention mechanisms. DeBERTa, on the other hand, incorporates a\nDisentangled Attention mechanism with Relative Position Bias to und erstand word\nrelationships and structure better. It also includes an Enhanced M asked Language\nModel (EMLM) to improve context understanding during decoding. The ﬁnal outputs\nfrom both models are passed to an output layer tailored for speciﬁc tasks such as clas-\nsiﬁcation or regression. See ﬁgures\n1 and 2 below for the diﬀerence in transformation\nfor Bert and DeBERTa.\nBERT (Bidirectional Encoder Representations from Transformers) i s a powerful\nmachine-learning model designed for a wide range of NLP tasks [ 47]. It operates by\nutilizing bidirectional training of the Transformer architecture , which allows it to cap-\nture the context from both directions, giving it an advantage over other u nidirectional\nlanguage models [ 48]. BERT has been successfully applied in various NLP tasks, such\nas sentiment analysis, named entity recognition, and question answeri ng. RoBERTa\n(Robustly Optimized BERT) builds upon BERT by modifying the pr etraining process,\nwhich includes removing the next sentence prediction task and u sing longer training\nsequences, resulting in improved performance across several NLP be nchmarks [ 49].\n9\nFig. 1 A High-Level Overview of the BERT Model’s Architecture\nFig. 2 A High-Level Overview of the DeBERTa Model’s Architecture\nXML-RoBERTa-base and XML-RoBERTa-longformer-base are RoBERTa-based\nmodels that incorporate XML, a method for learning expressive embedd ings for text.\nThese models can eﬃciently process long sequences and are particu larly useful for\ntasks requiring a deep understanding of the context. BigBird-RoBE RTa-base is another\nRoBERTa-based model that employs the BigBird architecture, design ed to handle\nlonger text sequences eﬃciently using sparse attention mechanism s.\nThe current research’s focus was on DeBERTa, as it further improve s the perfor-\nmance of BERT and RoBERTa through two novel techniques: disentangled attention\nand enhanced mask decoder [ 19]. The disentangled attention method represents each\nword using two vectors—one for its content and one for its position. This ap proach\nallows the model to learn the relationships between words more eﬀec tively by calculat-\ning attention weights using disentangled matrices based on their cont ent and relative\nposition. The enhanced mask decoder mechanism introduces an absolut e position\n10\nencoding in the decoding layer before training, which helps the model better under-\nstand the positions of words in a sequence. DeBERTa is available in di ﬀerent versions,\nsuch as DeBERTa-v3-base, DeBERTa-v3-small, and DeBERTa-v3-large, whic h vary in\nthe number of layers and hidden sizes. For example, DeBERTa-v3-bas e has 12 layers\nand a hidden size of 768 [ 50]. Larger versions, such as DeBERTa-v3-large, have more\nlayers and hidden sizes, oﬀering greater capacity for learning comple x patterns in the\ndata.\n3.3 Evaluation\nIn this study, a range of evaluation metrics were used to assess the pe rformance of\nthe models. These metrics include R2, MSE, RMSE, MAE, and MAPE. By using\nmultiple evaluation metrics, we can get a more comprehensive unders tanding of the\nperformance of the transformer models and compare their results with each other.\nEach metric provides a diﬀerent perspective on the accuracy and pr ecision of the\nmodels, and together they can help us identify the strengths and weak nesses of each\nmodel. We used cross-validation technique to train and test the mod els.\nR2 or R-squared, or the coeﬃcient of determination, is a measure of how we ll the\nmodel ﬁts the data. It represents the proportion of the variance in the dependent\nvariable that can be explained by the independent variables [\n51]. The best value in\nR-squared is +1, and the poorest value is −∞ . In the formulas below, assuming we\nhave n samples of model error and the error sample set is unbiased, xi represents the\npredicted ith value, and yi represents the ith actual value. The formula for R 2 is:\nR2 = 1 −\n∑ n\ni=1(xi − yi)2\n∑ n\ni=1(¯y − yi)2 (1)\nMSE (Mean Squared Error) measures the average of the squared diﬀerenc es\nbetween the actual and predicted values. MSE is a useful metric for detecting outliers\nbecause the squaring part of the function ampliﬁes the error if the mo del outputs a\nsingle very poor prediction [\n51]. The best value in MSE is 0, while the poorest value\nis + ∞ . The formula for MSE is:\nMSE = 1\nn\nn∑\ni=1\n(xi − yi)2 (2)\nRMSE is the square root of the square of all the model errors, it is the s quare root\nof the MSE [ 51]. It provides a measure of the standard deviation of the residuals, or t he\ndiﬀerence between the actual and predicted values. Like MSE, highe r values indicate a\nlarger discrepancy between the actual and predicted values. Similar to MSE, the best\nvalue in RMSE is 0 and the poorest value is + ∞ . The formula for RMSE is:\nRMSE =\n\n\n\n√\n1\nn\nn∑\ni=1\n(xi − ˆyi)2 (3)\n11\nMAE, or mean absolute error, measures the average of the absolute diﬀerenc es\nbetween the actual and predicted values. It provides a measure of th e magnitude of\nthe errors, regardless of their direction. MAE is a suitable metric t o use in cases where\noutliers may represent corrupted portions of the data. The ideal score for MAE is 0,\nwhile the poorest score is + ∞ . The formula for MAE is:\nMAE = 1\nn\nn∑\ni=1\n|xi − yi| (4)\nMAPE (Mean Absolute Percentage Error) is an intuitive performance metr ic that\nmeasures the average absolute percentage diﬀerence between actual and predicted\nvalues. It is useful in tasks where it is more important to be sensit ive to relative\nvariations than to absolute variations [ 52]. Higher MAPE values indicate a greater\ndeviation from the actual values, the best value being 0 and the poorest +∞ . The\nformula for MAPE is:\nMAPE = 1\nn\nn∑\ni=1\n⏐\n⏐\n⏐\n⏐\nyi − xi\nyi\n⏐\n⏐\n⏐\n⏐ (5)\n4 Results and Discussion\nOne of the main purposes of this study was to evaluate the performance of s everal\npretrained state-of-the-art transformer models in evaluating stud ents’ writings. The\nevaluation was based on several metrics, including R2 score, MSE, RMS E, MAE,\nand MAPE. The models tested on this research paper require a large amoun t of\ncomputation to train, and training for multiple epochs can be very tim e-consuming and\nresource-intensive. The number of epochs required to obtain opti mal performance can\nvary depending on the speciﬁc task, dataset, and model architecture . When it comes\nto transformer models, several studies [\n53–56] have shown that there is no substantial\nbeneﬁt in training transformer-based models for more than a single ep och. Not only\ndid these studies show that one epoch is suﬃcient, they were also able to achieve\nstate-of-the-art performance and results. Additionally, [ 55], highlights that training\ntransformer models for just one epoch can help prevent overﬁtting as we are reducing\nthe number of times the model is seeing the training dataset, ther eby reducing the\nrisk of the model memorizing it and generalizing better to unseen d ata.\nTable 2 displays the evaluation results of various transformer-based models on the\ntask of automated essay scoring. The top three models based on the R2 score are\nDeBERTa-v3-large, Roberta-large, and BigBird-Roberta-base. These mode ls perform\nbetter than other models in terms of ﬁtting the data, as reﬂected by t heir higher R2\nscores. Furthermore, they also outperform the other models in term s of MSE, RMSE,\nMAE, and MAPE scores, which indicates that they are more accurate in pred icting\nthe target variable. The high number of parameters of these models is a l ikely factor in\ntheir superior performance. In addition to the performance metrics , it is important to\nconsider the computational requirements of these models. The thre e top-performing\nmodels in this study - DeBERTa-v3-large, Roberta-large, and BigBird-R oberta-base\n- are all large transformer-based models with a signiﬁcant number of param eters.\n12\nAs a result, they require considerable computational resources and lon ger training\ntimes. However, as we mentioned above, a single epoch is suﬃcient f or these models to\nproduce great results. Among these models, DeBERTa-v3-large is the b est performing\nmodel based on its high R-squared score and low MSE, RMSE, MAE, and MAPE\nscores.\nFor a more detailed analysis of the results, we will be referring to t he RMSE\nscores due to the metric’s emphasis on larger errors, consistency wi th the evalua-\ntion scale, comparability with other research, and favorable mathematic al properties.\nRMSE penalizes larger errors more heavily, ensuring the model’s s ensitivity to sub-\nstantial deviations, and is measured in the same unit as the original score s, making\nit easier to interpret. Furthermore, its widespread use allows f or easy comparisons\nwith other studies, and its diﬀerentiability aids in eﬀective mo del optimization. How-\never, it is essential to consider additional evaluation metrics to gain a comprehensive\nunderstanding of the model’s performance. The average RMSE scores ac ross all lin-\nguistic aspects, such as cohesion, syntax, vocabulary, and phraseology, oﬀ er valuable\ninsights into the overall performance of various models in automated es say scoring.\nDeberta-v3-large performs the best, with the lowest average RMSE sc ore of 0.455243.\nThis indicates that the model is more accurate and consistent in evalu ating essays\nacross all linguistic aspects compared to its counterparts. The second -best model is\nRoberta-large, with an average RMSE score of 0.462514. Although its performance is\ncompetitive, it is marginally higher than deberta-v3-large. On the othe r hand, xml-\nroberta-longformer-base has the highest average RMSE score of 0.530286, reﬂecti ng\nthe weakest performance among the models. This suggests that it may not be as eﬀec-\ntive in evaluating essays’ linguistic aspects as other models. Sim ilarly, xml-roberta-base\nalso performs poorly, with an average RMSE score of 0.525482.\nThe other evaluation metrics, including R2, MSE, MAE, and MAPE, also su pport\nour ﬁndings regarding the performance of the models in automated essay s coring. We\nwill discuss these metrics regarding the top-performing and weake st models identiﬁed\nbased on the average RMSE scores. Deberta-v3-large, the top-performing model based\non average RMSE, also shows strong performance across other evaluation met rics. Its\naverage R2 score (0.511783) is the highest among all models, while its average M SE\n(0.207803) and MAE (0.365246) scores are the lowest, indicating accurate and con-\nsistent predictions. Roberta-large, the second-best model, demon strates competitive\nperformance across these metrics as well. With an average R2 score of 0.496031, i t\nis slightly behind deberta-v3-large. Its average MSE (0.214473) and MAE (0.369800)\nscores are also relatively low, reinforcing its strong performance i n automated essay\nscoring. On the other hand, xml-roberta-longformer-base and xml-robert a-base, the\nweakest models based on average RMSE scores, underperform across all ev aluation\nmetrics. Xml-roberta-longformer-base has the lowest average R2 score ( 0.336935) and\nthe highest average MSE (0.282393) and MAE (0.423329) scores. Xml-roberta-base\nalso performs poorly, with an average R2 score of 0.349976, MSE of 0.277043, and\nMAE of 0.418717.\nOur analysis of the results presented in Table\n2 reveals interesting insights into the\nstrengths and weaknesses of these models in scoring speciﬁc analyti c measures, such\n13\nModel Metrics Cohesion Syntax Vocabulary Phraseology Gramm ar Conventions Average\ndeberta-v3-large R2 0.450876 0.515898 0.493883 0.518194 0 .540198 0.55165 0.511783\nMSE 0.240641 0.200759 0.171808 0.206816 0.224696 0.202098 0.207803\nRMSE 0.490552 0.448062 0.414498 0.45477 0.474021 0.449553 0.455243\nMAE 0.396566 0.359372 0.329818 0.364598 0.379945 0.361177 0.365246\nMAPE 13.533288 12.662783 10.668627 12.575023 13.366952 12.530228 12.55615\nroberta-large R2 0.432312 0.504565 0.475774 0.505238 0.530933 0.527363 0.4 96031\nMSE 0.248776 0.205459 0.177956 0.212377 0.229223 0.213046 0.2 14473\nRMSE 0.498775 0.453276 0.421848 0.460844 0.478772 0.461569 0.4 62514\nMAE 0.402416 0.362303 0.334776 0.368202 0.382638 0.368465 0.3 698\nMAPE 13.790925 12.720342 10.762909 12.546485 13.318074 12.743328 12.647011\ndeberta-v3-base R2 0.422769 0.497986 0.460769 0.494219 0.485972 0.515775 0.4 79582\nMSE 0.252959 0.208188 0.18305 0.217107 0.251195 0.218269 0.22 1795\nRMSE 0.50295 0.456276 0.427843 0.465948 0.501194 0.467193 0.47 0234\nMAE 0.405946 0.366705 0.339411 0.37307 0.401333 0.374291 0.37 6793\nMAPE 13.802933 12.795438 10.845182 12.721782 13.91881 12.9330 01 12.836191\ndeberta-v3-small R2 0.40933 0.490336 0.452182 0.477526 0.4823 0.523596 0.4725 45\nMSE 0.258848 0.21136 0.185964 0.224273 0.252989 0.214744 0.22 4696\nRMSE 0.508771 0.459739 0.431236 0.473574 0.50298 0.463405 0.47 3284\nMAE 0.410939 0.368967 0.342449 0.378866 0.404285 0.37149 0.37 9499\nMAPE 14.027356 12.820205 10.946273 12.990369 14.101374 12.879 358 12.960823\nroberta-base R2 0.380291 0.47012 0.409197 0.455557 0.464098 0.47432 0.442 264\nMSE 0.271574 0.219744 0.200556 0.233703 0.261884 0.236955 0.2 37403\nRMSE 0.521127 0.468768 0.447835 0.483429 0.511746 0.48678 0.48 6614\nMAE 0.417925 0.373602 0.355531 0.385952 0.409048 0.388524 0.3 8843\nMAPE 14.283031 12.985313 11.380272 13.190436 14.222158 13.360 952 13.237027\nbigbird-roberta-base R2 0.374601 0.458735 0.408109 0.450527 0.447328 0.469552 0.4 34808\nMSE 0.274067 0.224465 0.200926 0.235862 0.270079 0.239104 0.2 40751\nRMSE 0.523514 0.473778 0.448247 0.485657 0.519692 0.488983 0.4 89979\nMAE 0.419541 0.377973 0.352555 0.387244 0.414119 0.389972 0.3 90234\nMAPE 14.340923 13.195495 11.286766 13.196849 14.407913 13.457 242 13.314198\nxml-roberta-base R2 0.347153 0.382231 0.313868 0.313704 0.319423 0.423477 0.3 49976\nMSE 0.286096 0.256192 0.232917 0.294594 0.332584 0.259873 0.2 77043\nRMSE 0.534879 0.506154 0.482615 0.542765 0.576701 0.509778 0.5 25482\nMAE 0.427915 0.403513 0.379041 0.433755 0.462541 0.405535 0.4 18717\nMAPE 14.712799 14.16362 12.186013 14.972713 16.144361 14.1121 28 14.381939\nxml-roberta-longformer-base R2 0.324691 0.360324 0.326225 0.321261 0.281069 0.408039 0.3 36935\nMSE 0.294539 0.263647 0.229372 0.288604 0.350751 0.267442 0.2 82393\nRMSE 0.542715 0.513466 0.478928 0.537219 0.592243 0.517148 0.5 30286\nMAE 0.435011 0.410174 0.374301 0.429235 0.478018 0.413234 0.4 23329\nMAPE 14.875921 14.371236 12.068629 14.786631 16.925385 14.455 714 14.580586\nTable 2 Performance Comparison of Transformer-based Models (Best scores are highlighted in bold)\n14\nModel No. of Parameters\ndeberta-v3-large 335 million parameters\nroberta-large 355 million parameters\ndeberta-v3-base 125 million parameters\ndeberta-v3-small 25 million parameters\nroberta-base 125 million parameters\nbigbird-roberta-base 128 million parameters\nxml-roberta-base 270 million parameters\nxml-roberta-longformer-base 150 million parameters\nTable 3 Number of Parameters for the Models\nas Cohesion, Syntax, Vocabulary, Phraseology, Grammar, and Conventions. De berta-\nv3-base performs relatively well in syntax and conventions, with RMS E scores of\n0.456276 and 0.467193, respectively. However, it shows weaknesses in assessin g cohe-\nsion, vocabulary, and phraseology, as demonstrated by higher RMSE scores i n these\ncategories. On the other hand, Deberta-v3-large, boasting a more complex ar chitec-\nture with additional layers, demonstrates a pronounced advantage, espe cially in the\ndomain of vocabulary prediction. In contrast, Deberta-v3-large exhibit s signiﬁcantly\nbetter performance in areas where Deberta-v3-base struggled, especi ally in predicting\nvocabulary scores. This distinction suggests that the additional laye rs in Deberta-v3-\nlarge, beyond just the shared training dataset, contribute to its enh anced capability\nin handling vocabulary and other linguistic aspects. Deberta-v3-smal l exhibits similar\nstrengths and weaknesses to deberta-v3-base. It performs reasonably w ell in syntax\nand conventions, with RMSE scores of 0.459739 and 0.463405, respectively. However ,\nit struggles more with cohesion, vocabulary, and phraseology, where its R MSE scores\nare higher.\nRoberta-base has strengths in syntax and conventions, with RMSE scores of\n0.468768 and 0.486780, respectively. However, it shows weaknesses in cohesion, vocab-\nulary, and phraseology, with relatively higher RMSE scores in these cat egories.\nRoberta-large performs well across all analytic measures, especially in syntax, vocabu-\nlary, phraseology, and conventions. Its performance in cohesion is slight ly weaker, with\nan RMSE score of 0.498775, but it still demonstrates overall strong performanc e. Xml-\nroberta-base struggles in most categories, with its main weakness being in phraseology,\nwhere it has the highest RMSE score of 0.576701 among all models. It also perfor ms\npoorly in cohesion, syntax, vocabulary, and grammar. Xml-roberta-longforme r-base\nexhibits the weakest performance across all models in most categories, particularly in\nvocabulary and phraseology, with RMSE scores of 0.478928 and 0.537219, respectively.\nIt also performs poorly in cohesion, syntax, grammar, and conventions. Bi gbird-\nroberta-base has strengths in syntax and conventions, with RMSE scores of 0.473778\nand 0.488983, respectively. However, it shows weaknesses in cohesion, vo cabulary, and\nphraseology, as demonstrated by higher RMSE scores in these categories.\nWhen selecting a model for a particular application, it is essential to consider the\ntrade-oﬀs between performance and computational requirements. DeBE RTa-v3-large\nand Roberta-large, while demonstrating superior performance, have lar ger numbers of\nparameters and thus require substantial computational resources and lon ger training\ntimes. However, as noted earlier, a single epoch is suﬃcient for the se models to produce\n15\nimpressive results, mitigating some of the computational concerns. O verall, deberta-\nv3-large and roberta-large consistently demonstrate strong performance ac ross most\nof the analytic measures, while xml-roberta-base and xml-roberta-longf ormer-base lag\nbehind in terms of overall performance. The other models, includi ng deberta-v3-base,\ndeberta-v3-small, and bigbird-roberta-base, perform well in some are as but exhibit\nweaknesses in others. Based on the various evaluation metrics used in t his study,\ndeberta-v3-large and roberta-large consistently demonstrate strong per formance, while\nxml-roberta-longformer-base and xml-roberta-base lag behind in terms of accuracy\nand overall prediction quality in automated essay scoring. Furthermor e, our ﬁndings\nreveal that based on the RMSE scores for DeBERTa-v3-large, cohesion remain s a chal-\nlenging analytical measure to predict. This is likely due to the in tricacies involved in\nunderstanding an essay’s logical connections and relationships betwe en sentences and\nideas. Accurately capturing these relationships requires a deep c omprehension of con-\ntext, discourse structure, and the deployment of various cohesive devices. Conversely,\nvocabulary stands out as a more predictable measure. This enhanced per formance isn’t\nmerely due to the training on extensive text corpora, which both the base and large\nversions share, but primarily owes to the reﬁned architecture of t he DeBERTa-v3-large\nmodel. The additional layers in the larger model enrich its capabili ties, allowing it to\ngrasp and predict vocabulary-related features in essays with greater accuracy. Addi-\ntionally, their pre-training on masked language modeling tasks bolster s their ability\nto comprehend contextual word representations, further enhancing their proﬁciency\nin predicting vocabulary across diverse contexts.\nTo provide a comprehensive understanding of model performance, t he above models\nwere evaluated using ﬁve essential metrics: R2, MSE, RMSE, MAE, and MAPE, across\nsix analytical measures, namely cohesion, syntax, vocabulary, phraseol ogy, grammar,\nand conventions. A rank aggregation, inspired by the Borda count method [ 57], was\nemployed to derive a consensus ranking. Unlike the traditional Borda count method,\nwhere candidates accrue points based on rank (with the top rank receivi ng the most\npoints), this adapted method awarded the best-performing model t he least points\n(1 for the top rank, 2 for the second, and so forth), thus favoring model s with the\nlowest total points across metrics. This distinction ensures that t he model with the\nlowest average rank is acknowledged as the top performer, as seen in Tabl e 4. The\nvisual representation in Figure 3 further conﬁrms the relative strengths of each model,\naccentuating the eﬃcacy of the adapted Borda count method in encapsulat ing the\nmulti-metric, multi-measure assessment into an easily interpr etable ranking.\nOur ﬁndings from this research suggest that the architecture of the mo del plays a\nmore critical role in automated essay scoring (AES) than the sheer numb er of parame-\nters. As clearly demonstrated in Table 4, the DeBERTa-v3-small model outperformed\nthe Roberta-base model despite having fewer parameters (See Table 3). The supe-\nrior performance of DeBERTa-v3-small can be attributed to its unique ar chitecture,\nunderscoring the importance of the underlying structure of the mo del in AES tasks.\nHowever, it would be remiss not to acknowledge the signiﬁcance of the n umber of\nparameters. For instance, within the same architectural framework, D eBERTa-v3-\nlarge outperformed both DeBERTa-v3-base and DeBERTa-v3-small. This obse rvation\n16\nModel Average Rank\ndeberta-v3-large 1.066667\nroberta-large 1.933333\ndeberta-v3-base 3.166667\ndeberta-v3-small 3.833333\nroberta-base 5.066667\nbigbird-roberta-base 5.933333\nxml-roberta-base 7.333333\nxml-roberta-longformer-base 7.666667\nTable 4 Average Ranking of the Models based on\nPerformance Across Multiple Metrics and\nAnalytical Measures (Lower Values indicate\nSuperior Performance)\nFig. 3 Visual Representation of Average Model Rankings (Lower Val ues indicating Superior Perfor-\nmance)\nsuggests that while architecture is paramount, the number of parameter s can also sub-\nstantially inﬂuence the model’s performance. When comparing mode ls with the same\narchitecture, the model with more parameters generally delivered better performance.\nTherefore, our research concludes that both the model’s architectu re and the number\nof parameters are vital factors in AES, with the architecture taking sli ght precedence.\nIn the presented analysis, root mean square error (RMSE) has been uti lized as the\nprimary evaluation metric, As reﬂected in Figures\n4-10. As mentioned in the section\nabove, the choice of RMSE is driven by its ability to convey the aver age magnitude\nof the error, rendering it a highly eﬀective measure for comparing th e performance of\ndiﬀerent models. The graphical representations in these ﬁgures pr ovide a clear, at-a-\nglance comparison of RMSE scores for all models across diﬀerent aspects of l anguage\nuse, including cohesion, syntax, vocabulary, phraseology, grammar, and c onventions.\nThese visualizations oﬀer a detailed perspective on the relative pe rformance of each\n17\nmodel, thereby aiding in the comprehension and interpretation of th e underlying\nnumerical data.\nFig. 4 Visualization of Average RMSE Scores across all the Models\nFig. 5 Visualization of Cohesion RMSE Scores across all the Models\nIn our experiments with the DeBERTa-v3-large model, we decided to test its sensi-\ntivity to changes in the ’evaluate n times per epoch’ parameter by increasing its value\nfrom 2 to 4. This parameter controls the number of times the validation dat aset is eval-\nuated per epoch during training. Our aim was to evaluate the model more frequently\nand obtain a more accurate estimate of its performance. This change result ed in longer\ntraining times, as the model had to evaluate the validation dataset more of ten. By\ntesting the sensitivity of the model to this parameter, we can gain i nsights into how the\nmodel behaves under diﬀerent evaluation frequencies and identi fy the optimal value\n18\nFig. 6 Visualization of Syntax RMSE Scores across all the Models\nFig. 7 Visualization of Vocabulary RMSE Scores across all the Mode ls\nfor this parameter. This information can be useful in improving the o verall perfor-\nmance of the model and optimizing its training process. Looking at the results shown\nin Table 5, we can observe that changing the ’evaluate n times per epoch’ value from\n2 to 4 did not result in any signiﬁcant improvement or deterioration in the perfor-\nmance of the DeBERTa-v3-large model. The average RMSE scores indicate on ly a\nslight improvement, from 0.455243 to 0.455067, when the parameter is increased f rom\n2 to 4 evaluations per epoch. Similarly, other evaluation metrics, su ch as average MAE,\nMAPE, and R2 scores, show marginal changes, suggesting that the increase i n eval-\nuation frequency during training does not have a substantial impact on the model’s\nperformance. These ﬁndings indicate that, in this particular case, i ncreasing the eval-\nuation frequency does not lead to signiﬁcant improvements or deteri orations in the\nmodel’s performance across various evaluation metrics. Nevertheless , it is still essential\nto explore diﬀerent training strategies and hyperparameters to ide ntify the optimal\nsettings for achieving the best possible performance in automated ess ay scoring tasks.\n19\nModel Metrics Cohesion Syntax Vocabulary Phraseology Grammar Conventions Average\ndeberta-v3-large\n(evaluate n times per epoch: 2) RMSE 0.490552 0.448062 0.414498 0.45477 0.474021 0.449553 0.455243\nR2 0.450876 0.515898 0.493883 0.518194 0.540198 0.55165 0.511783\nMSE 0.240641 0.200759 0.171808 0.206816 0.224696 0.202098 0.207803\nMAE 0.396566 0.359372 0.329818 0.364598 0.379945 0.361177 0.365246\nMAPE 13.533288 12.662783 10.668627 12.575023 13.366952 12.530228 12.55615\ndeberta-v3-large\n(evaluate n times per epoch: 4) RMSE 0.490777 0.447444 0.414349 0.454769 0.473886 0.449178 0.455067\nR2 0.450373 0.517233 0.494247 0.518196 0.54046 0.552397 0.512151\nMSE 0.240862 0.200206 0.171685 0.206815 0.224568 0.201761 0.207649\nMAE 0.396919 0.359162 0.329804 0.364671 0.379871 0.360937 0.365227\nMAPE 13.536199 12.641897 10.663627 12.570421 13.369219 12.522283 12.550608\nTable 5 Table 5: Parameter Sensitivity Test - Comparing the results for DeBERTa-v3-large when altering parameters in the conﬁg ﬁle\n20\nFig. 8 Visualization of Phraseology RMSE Scores across all the Mod els\nFig. 9 Visualization of Grammar RMSE Scores across all the Models\n5 Conclusion and Future Work\nThis research paper aims to advance the current state of research in th e ﬁeld of\nautomated essay scoring by proposing and evaluating a novel transformer model,\nDeBERTa, incorporating six key analytical measures. Our objectives w ere to conduct\na thorough literature review of existing studies, propose the DeBER Ta model, compare\nits eﬀectiveness with state-of-the-art transformer models, and e valuate its performance\nusing a dataset of argumentative essays and a range of evaluation metrics.\nThrough an extensive literature review, it was found that many existi ng studies\nfocused on speciﬁc linguistic aspects or used limited analytical me asures when evaluat-\ning student writing with transformer models. This leaves a gap in understanding how\nwell these models perform when considering a more comprehensive set of writing cri-\nteria. Furthermore, the potential of novel transformer models, suc h as DeBERTa, has\nnot been thoroughly explored in the context of automated essay scoring. To address\nthese gaps, DeBERTa was proposed as a novel transformer model for automate d essay\n21\nFig. 10 Visualization of Conventions RMSE Scores across all the Mod els\nscoring, incorporating six key analytical measures to provide a more com prehensive\nassessment of student writing. A comparative analysis was conducted to evaluate\nthe DeBERTa model alongside state-of-the-art transformer models, i ncluding Big-\nBird, Longformer, and RoBERTa, in order to identify their strengths and weaknesses\nin predicting diﬀerent analytical measures. This approach aimed to c ontribute to a\nmore complete understanding of the capabilities and limitations of vari ous transformer\nmodels in the context of comprehensive writing evaluation.\nOur ﬁndings revealed an intriguing aspect; the architecture of the m odel appeared\nto have more impact on AES than the number of parameters. For example,\nDeBERTa-v3-small, despite having fewer parameters, outperformed RoBERTa-base.\nThis superior performance is attributed to its unique architectu re, underscoring the\npivotal role of the underlying structure of the model in AES tasks. How ever, we\nalso observed that the number of parameters can also inﬂuence the mode l’s perfor-\nmance. For instance, DeBERTa-v3-large outperformed both DeBERTa-v3-bas e and\nDeBERTa-v3-small, indicating that when the architecture is held c onstant, models\nwith more parameters yield better performance.\nIn terms of speciﬁc model performance, DeBERTa-v3-large outperforme d the\nother models in all the analytical measures, making it the best-perf orming model\nbased on various evaluation metrics, including R2 score, MSE, RMSE, M AE,\nand MAPE. Additionally, we observed that a single epoch of training was su ﬃ-\ncient to achieve impressive results for the top-performing mode ls, mitigating some\ncomputational concerns. While DeBERTa-v3-large and RoBERTa-large consist ently\ndemonstrated strong performance across most of the analytic measures, mo dels like\nXML-RoBERTa-base and XML-RoBERTa-Longformer-base lagged behind in terms\nof overall performance. Other models, such as DeBERTa-v3-base, DeBE RTa-v3-small,\nand BigBird-RoBERTa-base, performed well in some areas but exhibited weaknesses\nin others.\nOverall, this research paper has successfully achieved its object ives by proposing\nand evaluating the DeBERTa model for automated essay scoring and compari ng its\neﬀectiveness with other state-of-the-art transformer models. We have demonstrated\n22\nthe strengths and weaknesses of these models in predicting various analytical measures\nand provided valuable insights into their performance. DeBERTa-v3- large emerged\nas the top-performing model, showcasing its potential for practical ap plications in\nautomated essay scoring.\nIn light of the promising results obtained with the DeBERTa-v3-large m odel for\nautomated essay scoring, there is a clear opportunity for further impr ovements in this\narea. One of the primary challenges in utilizing transformer-based ne ural language\nmodels like DeBERTa for educational applications is their lack of inte rpretability.\nUnderstanding the reasoning behind the model’s predictions is es sential for providing\nmeaningful feedback to English Language Learners (ELLs) and fostering their growth.\nConsequently, the integration of the current state-of-the-art NLP mod el, DeBERTa,\nwith explainable machine learning (XAI) techniques, such as SHapley Ad ditive exPla-\nnations (SHAP), represents a valuable path for future research. By inc orporating XAI\nmethods like SHAP into the DeBERTa model, we can decipher the mod el’s predictions\nbetter and identify which features contribute most signiﬁcantly t o the essay scores.\nThis enhanced understanding will allow us to provide more targeted and meaningful\nfeedback to ELLs, helping them identify their strengths and areas for improvement\nmore eﬀectively. Moreover, our future work aims to create an improve d and FATE\n(Fair, Accountable, Transparent, and Ethical) automated essay evaluation system\nusing Explainable Machine Learning. By ensuring that our model is not on ly accu-\nrate but also transparent and ethical in its predictions, we can enhanc e the trust and\nadoption of automated essay-scoring systems by educators and students al ike. Further-\nmore, a FATE evaluation system can help mitigate potential biases, promot e fairness,\nand ensure that the system is accountable for its performance.\n6 Competing Interests\nThe authors declare that they have no competing interests.\n7 Funding\nThis research received no external funding.\nReferences\n[1] Chan, S.M.H., Mamat, N.H., Nadarajah, V.D.: Mind your language: the\nimportance of english language skills in an international medical pro-\ngramme (imp). BMC Medical Education 22 (2022)\nhttps://doi.org/10.1186/\nS12909-022-03481-W\n[2] Lee, S., Schmidgall, J.: The importance of english writing skills in the inter-\nnational workplace. research memorandum. ets rm-20-07. Educational Testi ng\nService (2020)\n[3] Sophia, M.S.C., Nirmala, M., English, A.P.: Creating awareness and i mportance\nof ’english’ for students hailing from rural areas 25, 1583–6258 (2021)\n23\n[4] Khadka, S.: Meaning and teaching of writing in higher education\nin nepal. Higher Education in Nepal: Policies and Perspec-\ntives, 201–212 (2019)\nhttps://doi.org/10.4324/9781351139205-15/\nMEANING-TEACHING-WRITING-HIGHER-EDUCATION-NEPAL-SANTOSH-KHADKA\n[5] Huwari, I.F., Al-Khasawneh, F.M., Fathi, I., Fadi, H., Al-Khasawne h, M.: The\nreasons behind the weaknesses of writing in english among pre-year st udents’ at\ntaibah university. Esp-world.info 14 (2013)\n[6] Wang, P., Huang, X.: An online english writing evaluation system using d eep\nlearning algorithm. Mobile Information Systems 2022 (2022) https://doi.org/10.\n1155/2022/7605989\n[7] Wang, X., Zhong, W.: Error correction based on deep learning (2022) https:\n//doi.org/10.1155/2022/4082082\n[8] Park, Y.H., Choi, Y.S., Park, C.Y., Lee, K.J.: Essaygan: Essay data augme ntation\nbased on generative adversarial networks for automated essay scoring. Appli ed\nSciences (Switzerland) 12 (2022) https://doi.org/10.3390/APP12125803\n[9] Wambsganss, T., Janson, A., Leimeister, J.M.: Enhancing argumentati ve writ-\ning with automated feedback and social comparison nudging. Computers and\nEducation 191 (2022)\nhttps://doi.org/10.1016/J.COMPEDU.2022.104644\n[10] Wang, Z.: Computer-assisted eﬂ writing and evaluations based on artiﬁ cial intel-\nligence: a case from a college reading and writing course. Library Hi Tech 40,\n80–97 (2022) https://doi.org/10.1108/LHT-05-2020-0113/FULL/PDF\n[11] Crossley, S.A., Varner, L.K., Roscoe, R.D., McNamara, D.S.: Using aut omated\nindices of cohesion to evaluate an intelligent tutoring system and an aut omated\nwriting evaluation system. Lecture Notes in Computer Science (incl uding sub-\nseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioi nformatics)\n7926 LNAI , 269–278 (2013) https://doi.org/10.1007/978-3-642-39112-5 28/\nCOVER\n[12] Lima, F.S., Filho, A.H., Prado, H.A.D., Ferneda, E.: Automatic evaluation of\ntextual cohesion in essays (2019)\n[13] Bhatt, R., Patel, M., Srivastava, G., Mago, V.: A graph based approach to\nautomate essay evaluation. Conference Proceedings - IEEE Internation al Con-\nference on Systems, Man and Cybernetics 2020-October, 4379–4385 (2020)\nhttps://doi.org/10.1109/SMC42975.2020.9282902\n[14] McCarthy, K.S., Roscoe, R.D., Allen, L.K., Likens, A.D., McNamara, D .S.:\nAutomated writing evaluation: Does spelling and grammar feedback suppor t\nhigh-quality writing and revision? Assessing Writing 52, 100608 (2022) https:\n24\n//doi.org/10.1016/J.ASW.2022.100608\n[15] Beseiso, M.: Essay scoring tool by employing roberta architect ure; essay scoring\ntool by employing roberta architecture. International Conference on Data Science,\nE-learning and Information Systems 2021, 54–57 (2021) https://doi.org/10.1145/\n3460620\n[16] Beseiso, M., Alzahrani, S.: An empirical analysis of bert embedding for automated\nessay scoring. IJACSA) International Journal of Advanced Computer Scien ce and\nApplications 11 (2020)\n[17] Li, H., Dai, T.: Explore deep learning for chinese essay automated sc oring. Jour-\nnal of Physics: Conference Series 1631, 12036 (2020) https://doi.org/10.1088/\n1742-6596/1631/1/012036\n[18] Wang, Y., Wang, C., Li, R., Lin, H.: On the use of bert for automated essay\nscoring: Joint learning of multi-scale essay representation\n[19] He, P., Liu, X., Gao, J., Chen, W., Dynamics, M.: Deberta: Decoding- enhanced\nbert with disentangled attention (2020) https://doi.org/10.48550/arxiv.2006.\n03654\n[20] Rahimi, Z., Litman, D., Correnti, R., Wang, E., Matsumura, L.C.: Asse ssing stu-\ndents’ use of evidence and organization in response-to-text writing: Us ing natural\nlanguage processing for rubric-based automated scoring. International Jou rnal of\nArtiﬁcial Intelligence in Education 27, 694–728 (2017) https://doi.org/10.1007/\nS40593-017-0143-2/TABLES/19\n[21] Polio, C., Yoon, H.J.: The reliability and validity of automated tools f or examining\nvariation in syntactic complexity across genres. International Journal of Applied\nLinguistics 28, 165–188 (2018) https://doi.org/10.1111/IJAL.12200\n[22] Wilson, J., Rodrigues, J.: Classiﬁcation accuracy and eﬃciency of w riting screen-\ning using automated essay scoring. Journal of School Psychology 82, 123–140\n(2020) https://doi.org/10.1016/J.JSP.2020.08.008\n[23] Li, Z., Yan, D.: Eﬀect of pigai.org on english majors’ writing self-e ﬃcacy and\nwriting performance. Journal of Physics: Conference Series 1533, 042086 (2020)\nhttps://doi.org/10.1088/1742-6596/1533/4/042086\n[24] Kumar, V.S., Boulanger, D.: Automated essay scoring and the deep learn ing\nblack box: How are rubric scores determined? International Journal of Ar ti-\nﬁcial Intelligence in Education 31, 538–584 (2021) https://doi.org/10.1007/\nS40593-020-00211-5/TABLES/17\n[25] Stephen, T.C., Gierl, M.C., King, S.: Automated essay scoring ( aes) of constructed\nresponses in nursing examinations: An evaluation. Nurse Education in Pr actice\n25\n54, 103085 (2021) https://doi.org/10.1016/j.nepr.2021.103085\n[26] ¨Oncel, P., Flynn, L.E., Sonia, A.N., Barker, K.E., Lindsay, G.C., Mc clure, C.M.,\nMcnamara, D.S., Allen, L.K.: Automatic student writing evaluation: Inv estigating\nthe impact of individual diﬀerences on source-based writing. ACM In ternational\nConference Proceeding Series, 620–625 (2021) https://doi.org/10.1145/3448139.\n3448207\n[27] Wilson, J., Huang, Y., Palermo, C., Beard, G., MacArthur, C.A.: Automated\nfeedback and automated scoring in the elementary grades: Usage, attitudes , and\nassociations with writing outcomes in a districtwide implementat ion of mi write.\nInternational Journal of Artiﬁcial Intelligence in Education 31, 234–276 (2021)\nhttps://doi.org/10.1007/S40593-020-00236-W/TABLES/9\n[28] Ramesh, D., Sanampudi, S.K.: An automated essay scoring systems: a systematic\nliterature review. Artiﬁcial intelligence review 55, 2495–2527 (2022) https://doi.\norg/10.1007/S10462-021-10068-2\n[29] Wilson, J., Myers, M.C., Potter, A.: Investigating the promise of automated writ-\ning evaluation for supporting formative writing assessment at scale. As sessment in\nEducation: Principles, Policy and Practice 29, 183–199 (2022) https://doi.org/10.\n1080/0969594X.2022.2025762/SUPPL FILE/CAIE A 2025762 SM4925.DOCX\n[30] Alikaniotis, D., Yannakoudakis, H., Rei, M.: Automatic text scoring us ing neural\nnetworks. 54th Annual Meeting of the Association for Computational Linguis-\ntics, ACL 2016 - Long Papers 2, 715–725 (2016)\nhttps://doi.org/10.18653/V1/\nP16-1068\n[31] Pathak, P., Raghav, S., Jain, S., Jalal, S.: Essay rating system usi ng machine\nlearning. 2021 5th International Conference on Information Systems and Com-\nputer Networks, ISCON 2021 (2021)\nhttps://doi.org/10.1109/ISCON52037.2021.\n9702504\n[32] Ludwig, S., Mayer, C., Hansen, C., Eilers, K., Brandt, S., Robitz sch, A., Battauz,\nM.: Automated essay scoring using transformer models. Psych 2021, Vol. 3, Pages\n897-915 3, 897–915 (2021) https://doi.org/10.3390/PSYCH3040056\n[33] Ye, X., Manoharan, S.: Performance comparison of automated essay graders bas ed\non various language models. 2021 IEEE International Conference on Comput-\ning, ICOCO 2021, 152–157 (2021)\nhttps://doi.org/10.1109/ICOCO53166.2021.\n9673585\n[34] Bai, X., Stede, M.: A survey of current machine learning approaches to\nstudent free-text evaluation for intelligent tutoring. Internation al Journal\nof Artiﬁcial Intelligence in Education, 1–39 (2022) https://doi.org/10.1007/\nS40593-022-00323-0/TABLES/3\n26\n[35] Das, L.B., Raghu, C.V., Jagadanand, G., George, R.A.R., Yashasawi, P.,\nKumaran, N.A.A., Patnaik, V.K.: Factograde: Automated essay scoring system,\n42–48 (2022)\nhttps://doi.org/10.1109/IAICT55358.2022.9887447\n[36] Nunes, A., Cordeiro, C., Limpo, T., Castro, S.L.: Eﬀectiveness of autom ated\nwriting evaluation systems in school settings: A systematic revie w of studies from\n2000 to 2020. Journal of Computer Assisted Learning 38, 599–620 (2022) https:\n//doi.org/10.1111/JCAL.12635\n[37] Matta, M., Keller-Margulis, M.A., Mercer, S.H.: Cost analysis and cos t-\neﬀectiveness of hand-scored and automated approaches to writing scree ning.\nJournal of School Psychology 92, 80–95 (2022) https://doi.org/10.1016/J.JSP.\n2022.03.003\n[38] Sun, J., Song, T., Song, J., Peng, W.: Improving automated essay scor ing by\nprompt prediction and matching. Entropy 24 (2022) https://doi.org/10.3390/\nE24091206\n[39] Crossley, S.A., Kyle, K., McNamara, D.S.: The tool for the automatic anal ysis\nof text cohesion (taaco): Automatic assessment of local, global, and text cohe-\nsion. Behavior Research Methods 48, 1227–1237 (2016)\nhttps://doi.org/10.3758/\nS13428-015-0651-7/TABLES/6\n[40] Vajjala, S.: Automated assessment of non-native learner essays: Inv estigating the\nrole of linguistic features. International Journal of Artiﬁcial Intelli gence in Edu-\ncation 28, 79–105 (2018) https://doi.org/10.1007/S40593-017-0142-3/TABLES/\n12\n[41] Peng, Y., Jung, S., Kim, Y.: Eecs 595 ﬁnal project report: Aes(automati c essay\nscoring) for ells(english language learners)\n[42] Wilson, J., Czik, A.: Automated essay evaluation software in english l anguage\narts classrooms: Eﬀects on teacher feedback, student motivation, and w riting\nquality. Computers & Education 100, 94–109 (2016) https://doi.org/10.1016/J.\nCOMPEDU.2016.05.004\n[43] Ding, Y., Riordan, B., Horbach, A., Cahill, A., Zesch, T.: Don’t take “nsw vt-\nnvakgxpm” for an answer –the surprising vulnerability of automatic conte nt\nscoring systems to adversarial input, 882–892 (2020) https://doi.org/10.18653/\nV1/2020.COLING-MAIN.76\n[44] Fu, Q.K., Zou, D., Xie, H., Cheng, G.: A review of awe feedback: typ es, learn-\ning outcomes, and implications. https://doi.org/10.1080/09588221.2022.2033787\n(2022)\nhttps://doi.org/10.1080/09588221.2022.2033787\n[45] Babel generator — les perelman, ph.d.\n27\n[46] Feedback prize - english language learning — kaggle\n[47] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-traini ng of deep\nbidirectional transformers for language understanding. NAACL HLT 2019 - 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies - Proceedings of the Conference 1,\n4171–4186 (2018)\nhttps://doi.org/10.48550/arxiv.1810.04805\n[48] Horev, R.: Bert explained: State of the art language model for nlp — by ran i\nhorev — towards data science (2018)\n[49] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Le wis, M.,\nZettlemoyer, L., Stoyanov, V., Allen, P.G.: Roberta: A robustly opti mized bert\npretraining approach (2019)\n[50] microsoft/deberta-v3-base · hugging face\n[51] Chicco, D., Warrens, M.J., Jurman, G.: The coeﬃcient of determ ination r-squared\nis more informative than smape, mae, mape, mse and rmse in regression analy -\nsis evaluation. PeerJ Computer Science 7, 1–24 (2021) https://doi.org/10.7717/\nPEERJ-CS.623/SUPP-1\n[52] Myttenaere, A., Golden, B., Grand, B.L., Rossi, F.: Mean absolute pe rcentage\nerror for regression models. Neurocomputing 192, 38–48 (2016) https://doi.org/\n10.1016/J.NEUCOM.2015.12.114\n[53] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariw al, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win-\nter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Ch ess, B., Clark,\nJ., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amode i, D.: Language\nmodels are few-shot learners. Advances in Neural Information Processi ng Systems\n2020-December (2020) https://doi.org/10.48550/arxiv.2005.14165\n[54] Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H.,\nBrown, T.B., Dhariwal, P., Gray, S., Hallacy, C., Mann, B., Radford, A. , Ramesh,\nA., Ryder, N., Ziegler, D.M., Schulman, J., Amodei, D., Openai, S.M .: Scal-\ning laws for autoregressive generative modeling (2020) https://doi.org/10.48550/\narxiv.2010.14701\n[55] Komatsuzaki, A.: One epoch is all you need (2019) https://doi.org/10.48550/\narxiv.1906.06669\n[56] Zanzotto, F.M., Santilli, A., Ranaldi, L., Onorati, D., Tommasino, P., F allucchi,\nF.: Kermit: Complementing transformer architectures with enco ders of explicit\nsyntactic interpretations. EMNLP 2020 - 2020 Conference on Empirical Metho ds\nin Natural Language Processing, Proceedings of the Conference, 256–267 (2020)\n28\nhttps://doi.org/10.18653/V1/2020.EMNLP-MAIN.18\n[57] Black, D.: Partial justiﬁcation of the borda count. Public Choice, 1–15 (1976)\n29"
}