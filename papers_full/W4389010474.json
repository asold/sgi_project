{
  "title": "Speaker Role Identification in Call Centre Dialogues: Leveraging Opening Sentences and Large Language Models",
  "url": "https://openalex.org/W4389010474",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4314745421",
      "name": "Minh-Quoc Nghiem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2775898566",
      "name": "Nichola Roberts",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2181470311",
      "name": "Dmitry Sityaev",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3127127511",
    "https://openalex.org/W2100374502",
    "https://openalex.org/W2800118084",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W4312206757",
    "https://openalex.org/W2143229091",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285787895",
    "https://openalex.org/W4309857275",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3024400986",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3118639295",
    "https://openalex.org/W4300705248",
    "https://openalex.org/W2153875174",
    "https://openalex.org/W2116634027",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "This paper addresses the task of speaker role identification in call centre dialogues, focusing on distinguishing between the customer and the agent. We propose a text-based approach that utilises the identification of the agent’s opening sentence as a key feature for role classification. The opening sentence is identified using a model trained through active learning. By combining this information with a large language model, we accurately classify the speaker roles. The proposed approach is evaluated on a dataset of call centre dialogues and achieves 93.61% accuracy. This work contributes to the field by providing an effective solution for speaker role identification in call centre settings, with potential applications in interaction analysis and information retrieval.",
  "full_text": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 388–392\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n388\nSpeaker Role Identification in Call Centre Dialogues:\nLeveraging Opening Sentences and Large Language Models\nMinh-Quoc Nghiem, Nichola Roberts, Dmitry Sityaev\nConnex One Limited\nBauhaus, 27 Quay St, Manchester M3 3GY , United Kingdom\n{minh-quoc.nghiem,nichola.roberts,\ndmitry.sityaev}@connexone.co.uk\nAbstract\nThis paper addresses the task of speaker role\nidentification in call centre dialogues, focus-\ning on distinguishing between the customer\nand the agent. We propose a text-based ap-\nproach that utilises the identification of the\nagent’s opening sentence as a key feature for\nrole classification. The opening sentence is\nidentified using a model trained through active\nlearning. By combining this information with\na large language model, we accurately classify\nthe speaker roles. The proposed approach is\nevaluated on a dataset of call centre dialogues\nand achieves 93.61% accuracy. This work con-\ntributes to the field by providing an effective\nsolution for speaker role identification in call\ncentre settings, with potential applications in\ninteraction analysis and information retrieval.\n1 Introduction\nSpeaker role identification is a fundamental process\nthat involves recognizing different speaker roles\nin a conversation. Its significance has grown in\nvarious settings, such as call centres, where distin-\nguishing between agents and customers in a call\ntranscript is critical. Speaker role identification\nhas numerous uses, including interaction and dia-\nlogue analysis, summarisation, and information re-\ntrieval (Lavalley et al., 2010; Jahangir et al., 2021).\nThis paper concentrates exclusively on speaker role\nidentification in call centre conversations. Table 1\ndemonstrates an example of the input and output\nof this process.\nIn our application, speaker role identification\ntakes place after speaker diarisation, where speaker\nturn information has been added to the transcripts.\nWithin a call centre dialogue, two specific roles\nare present: the customer and the agent. While a\ntypical call involves a single customer and a sin-\ngle agent, it is common for calls to involve more\nthan one customer (as in Table 1), or more than\none agent (such as when an agent transfers a cus-\ntomer to another agent). Identifying speaker roles\nSample input dialogue:\nPerson_01 hello\nPerson_02 hello good morning is that\n[NAME]\nPerson_01 if you hang on a sec while i just\nget him\nPerson_02 sorry\nPerson_01 who’s calling\nPerson_02 it’s [NAME] calling you from\n[ORG] it’s for an application\nPerson_03 oh hi there hi\nPerson_02 hi [NAME] it’s just for an appli-\ncation\nPerson_03 yes this is [NAME] yes . . .\nTable 1: An example input of identifying speaker roles.\nThe output should indicate that Person_01 and Per-\nson_03 are the Customers, and Person_02 is the Agent\nis a challenging task due to various factors, such\nas transcription errors, interruptions, repetitions,\nmulti-party conversations, and diverse topics.\nNumerous studies have been undertaken to ad-\ndress the issue of speaker role identification. These\nefforts involve utilising text-based features (Barzi-\nlay et al., 2000; Liu, 2006; Wang et al., 2011; Sapru\nand Valente, 2012; Flemotomos et al., 2019), or\nemploying multimodal approaches that integrate\nboth text and audio features (Rouvier et al., 2015;\nBellagha and Zrigui, 2020; Guo et al., 2023). In\nboth cases, the goal is to classify each speaker in a\nconversation into a predefined role category. This\nclassification task is typically accomplished using\nmachine learning algorithms that are trained on\nlabelled datasets, which consist of conversations\nwhere each speaker is annotated with their corre-\nsponding role category. This paper focuses on the\ntext-based approach and formulate the task as a\nbinary classification problem, with the categories\nbeing “customer” and “agent”.\nIn call centre dialogues, distinguishing between\n389\nthe agent and the customer can be achieved by ex-\nploiting the language differences between them.\nThe call centre agent typically starts the conversa-\ntion by introducing themselves as a representative\nof their company or organization, which is referred\nto as the “opening sentence”. We propose utilising\nthe identification results of the opening sentence\nto identify the speaker roles. By combining this\ninformation with a large language model, we can\naccurately classify the speaker roles in call centres.\nThis paper makes the following two key contri-\nbutions:\n• We propose a model for predicting the open-\ning sentence used by call centre agents, and\nprovide details on how to efficiently construct\nthe training data for this task using active\nlearning.\n• We introduce a practical approach for identify-\ning the speaker roles in call centre dialogues\nby combining the opening sentence identifica-\ntion with a large language model.\nThe remainder of this paper is organised as fol-\nlows. Section 2 provides a brief overview of the\nrelated work. Section 3 presents details of our\nmethodology. Section 4 describes the experimental\nresults and discussions. Section 5 concludes the\npaper and points to avenues for future work.\n2 Related Work\nText-based speaker role identification often takes\nthe form of a text classification task, aiming to\ncategorise each speaker in a conversation into pre-\ndefined role categories. Traditionally, text clas-\nsification has been accomplished using machine\nlearning algorithms trained on labelled datasets.\nHowever, with the advent of the Transformer neu-\nral network (Vaswani et al., 2017), many studies\nhave adopted pre-trained large language models for\ntext classification (Devlin et al., 2019; Liu et al.,\n2019; Yang et al., 2019). Fine-tuning these pre-\ntrained models still requires a certain amount of\nlabelled data. Active learning provides a means to\nquickly build labelled data by involving the model\nin the data labelling process (Settles, 2010).\nOn the other hand, zero-shot text classification\nis an approach that requires no labelled data at\nall (Pourpanah et al., 2022). In this method, a\nmodel is trained on a set of existing labelled ex-\namples and can subsequently classify new exam-\nples from previously unseen classes. This offers\nthe advantage of categorising text into arbitrary\ncategories without the requirement of data prepro-\ncessing and training. BART (Lewis et al., 2020),\nBLOOM (Muennighoff et al., 2022), and FLAN-\nT5 (Wei et al., 2021) are notable pre-trained large\nlanguage models available for research purposes,\noffering the ability to perform zero-shot learning.\n3 Method\n3.1 Opening Sentence Identification\nOur approach for identifying the opening sentence\ninvolves using active learning methods to acquire\nthe necessary labelled data and constructing a clas-\nsifier by fine-tuning a pre-trained large language\nmodel with the labelled data.\n3.1.1 Data Preparation\nAn active learning approach was employed to cre-\nate labelled data for opening sentence identifica-\ntion using a dataset of 437,135 utterances extracted\nfrom 67,719 dialogues from different call centre\ndomains. The initial seed set of 100 samples was\nmanually annotated using keyword searches with\nphrases like “calling from” and “speaking to”. The\nidentified key phrases were combined with negative\nexamples to form a seed set. Following that, the\nseed set was used to train SVM classifiers, utilising\ntwo distinct embedding strategies: BERT (Bidirec-\ntional Encoder Representations from Transform-\ners) sentence embedding and TF-IDF. This selec-\ntion was primarily made to facilitate rapid train-\ning/retraining of the classifiers during the labelling\nprocess.\nThe classifiers are used to classify each unla-\nbelled sample, and based on the confidence scores,\nhuman annotators decide which samples to label\nusing a combination of two sampling strategies:\nExpected model change and Query-by-Diversity.\nGiven the dataset’s substantial class imbalance with\nonly a few positive samples, the focus was on la-\nbelling positive samples. This approach aimed\nto identify the opening sentences that were most\nlikely to have a significant impact on improving\nthe current model. However, Query-by-Diversity\nsampling (Kee et al., 2018) was also employed to\nensure a diverse range of opening sentences was\nidentified. The classifiers underwent retraining ei-\nther after labelling every 100 samples or when no\nsamples had a score exceeding a threshold (0.7 in\nour specific case).\n390\n3.1.2 Classification\nThe system employed to identify the opening sen-\ntence comprises three key components: an input\nlayer, a BERT model, and a classification layer. In\nthis process, the input layer receives an utterance\nfrom the dialogue, and the input representation\nis generated by incorporating the corresponding\ntoken, segment, and position embeddings. The pro-\ncedure adheres to the recommendations outlined in\nthe work by Devlin et al. (2019). A fully connected\nneural network, positioned on top of the BERT\noutput, functions as the classification layer to deter-\nmine whether the utterance is an opening sentence\nor not. During the training phase, the BERT layer\nis initialised with pre-trained parameters, and all\nparameters are then fine-tuned using labelled data\nfrom the data preparation step.\n3.2 Speaker Role Identification\nThe FLAN-T5 model is used as the baseline, using\na zero-shot prompting approach. Although other\nmodels could be utilised, our experiments reveal\nthat the FLAN-T5 yields the most favourable out-\ncomes. The prompt provided to the model is\n{utterances} (from a speaker)\nBased on the utterances above,\n{speaker} is\nOPTIONS\n- an agent from a call centre\n- a customer\nBy inputting the utterances of each speaker, the\nmodel is able to assign them a role, either “cus-\ntomer” or “agent”. This process is repeated for\nall speakers in the dialogue. Additionally, experi-\nments were conducted using the entire conversation\nas input (dialogue as context), and results for both\napproaches are reported (the first 2 rows in Table 3).\nTo identify the role of speakers in a dialogue, we\nuse a combination of the FLAN-T5 model and the\nopening sentence identification approach. First, we\nidentify the opening sentences of the dialogue and\ndesignate their speakers as “agents”. If a speaker\ndoes not have an opening sentence, they are la-\nbelled as “customers”. However, in cases where\nthere are no agents (i.e., no opening sentences de-\ntected) or no customers (i.e., all speakers have an\nopening sentence), we rely on the FLAN-T5 model\nto assign speaker roles. By combining the strengths\nof both approaches, we can improve the accuracy\nand reliability of speaker role identification.\n4 Results and Discussion\n4.1 Evaluation Dataset\nWe use the dataset described in 3.1.1 as the evalua-\ntion dataset. The conversations are typical of those\nencountered in call centre scenarios, e.g. buying\nmobile phones, insurance, foods, etc. A total of\n867 opening utterances were labelled as positive\nexamples, indicating they were opening sentences,\nwhile 1,982 utterances were labelled as negative\nexamples, representing non-opening sentences, us-\ning the active learning approach. A subset of 321\ndialogues from seven domains was selected for\nspeaker role identification, which includes speaker\ndiarisation information.\n4.2 Opening Sentence Identification\nTo ensure balanced representation of positive and\nnegative samples, we divided the opening sentence\nidentification data into a train set and a test set,\nfollowing an 80-20 split while maintaining an equal\nratio of positive and negative samples between the\ntwo sets. Since the data was generated through\nactive learning, there is a potential bias due to the\ndeliberate selection of samples for labelling. To\naddress this, we generated an additional test set\nby randomly selecting 100 dialogues and manually\nassigning labels to them. We presented the results\nobtained from the SVM classifiers as well as the\nclassification performance using BERT (bert-base-\nuncased and bert-large-uncased). We trained the\nBERT classifier model for 3 epochs.\nTable 2: The accuracy, precision, recall and F1 scores of\ndifferent classifiers on opening sentence identification\nMethod Acc Pre Rec F1\nTest Set\nSVM-TF-IDF 91.78 86.88 80.35 83.48\nSVM-BERT 94.92 92.12 87.86 89.94\nBERT base 96.41 92.09 94.22 93.14\nBERT large 95.37 86.60 97.11 91.55\nAdditional Test Set\nSVM-TF-IDF 99.88 98.91 86.67 92.39\nSVM-BERT 99.85 91.43 91.43 91.43\nBERT base 99.05 94.23 93.33 93.78\nBERT large 99.89 90.27 97.14 93.58\nThe SVM-TF-IDF method achieved an accuracy\nof 91.78%, highlighting its proficiency in accu-\nrately identifying opening sentences. In contrast,\nthe SVM-BERT approach outperformed the SVM-\nTF-IDF method with an accuracy of 94.92%. This\n391\nimprovement can be attributed to the utilisation of\nBERT embeddings, which incorporate the seman-\ntic meaning and contextual information of words.\nHowever, the SVM-BERT approach only utilises\nthe last layer of BERT for embedding, resulting\nin slightly lower performance compared to other\nBERT models.\nAmong the evaluated methods, the BERT base\nmodel achieved the highest accuracy of 96.41%.\nThis demonstrates the effectiveness of leveraging\npre-trained language models like BERT for open-\ning sentence identification. Although the BERT\nlarge model achieved a slightly lower accuracy of\n95.37% compared to BERT base, it excelled in\nrecall with a score of 97.11%. This indicates its\nstrength in correctly identifying positive samples,\nalbeit with a slightly lower precision compared to\nBERT base. Furthermore, the precision, recall, and\nF1 scores are notably high, highlighting a well-\nbalanced trade-off in accurately identifying both\npositive and negative samples. The results obtained\nfrom the additional test set further validate this\nobservation.\n4.3 Speaker Role Identification\nFor speaker role identification, a subset of 321 di-\nalogues from seven domains was selected. The\nevaluation focused on measuring the accuracy of\ntwo approaches: FLAN-T5 and the combined use\nof opening sentence identification and FLAN-T5.\nTwo FLAN-T5 models were employed in the evalu-\nation: FLAN-T5-Large and FLAN-T5-XL. The re-\nsults obtained from these evaluations are presented\nin Table 3.\nTable 3: Accuracy of different approaches on speaker\nrole identification\nMethod Acc\nFLAN-T5-Large dialogue as context 70.17\nFLAN-T5-Large utterances 81.25\nFLAN-T5-XL utterances 86.36\nUsing Opening Sentence 89.49\nOpening Sentence + FLAN-T5-XL 93.61\nFLAN-T5-Large (770M parameters), when con-\nsidering the whole dialogue as context, achieved an\naccuracy of 70.17%. However, when using utter-\nances which belong to a specific speaker as context,\nFLAN-T5-Large demonstrated improved perfor-\nmance with an accuracy of 81.25%. This approach\noutperformed the dialogue-level context approach,\nhighlighting the benefits of considering individual\nutterances. The FLAN-T5-XL variant (3B parame-\nters) achieved an accuracy of 86.36%, surpassing\nthe previous approaches. This improvement can be\nattributed to its larger model configuration, which\nenhances its ability to capture complex patterns and\nrepresentations.\nThe utilisation of the opening sentence identifi-\ncation approach resulted in an accuracy of 89.49%.\nThis method leverages labelled data, providing an\nadvantage over FLAN-T5, which is a zero-shot\napproach. Combining the opening sentence iden-\ntification with FLAN-T5-XL yielded the highest\naccuracy of 93.61%. This combination proves to\nbe the most effective for accurate identification.\nClassification errors were further analysed, and\nthe following primary causes were identified: (1)\nInaccurate speaker identifiers in the input data,\nparticularly due to speech diarisation errors. (2)\nComplex contextual scenarios that pose challenges\neven for human understanding. (3) Instances where\nagents engage in conversations with each other,\nmaking it difficult to distinguish their roles. (4)\nSituations involving business numbers being con-\ntacted, which often share the same opening sen-\ntence pattern and are prone to misidentification as\nagents.\n5 Conclusion\nThis paper proposes a text-based approach for\nspeaker role identification in call centre dialogues.\nBy combining the identification of the agent’s\nopening sentence with a large language model,\nour approach achieves high accuracy in classify-\ning speaker roles. This has practical implications\nfor call centre applications, enabling improved\ncustomer-agent interaction analysis and call pat-\ntern analysis.\nThe use of active learning allows for efficient\nconstruction of the training dataset for opening\nsentence identification. Integrating this informa-\ntion into the classification process significantly im-\nproves the accuracy of speaker role identification.\nFuture work can explore enhancements to the\nsystem, such as incorporating additional contex-\ntual features and exploring multimodal approaches.\nEvaluating the approach on larger and more diverse\ndatasets would also provide a better understanding\nof its generalisability.\n392\nReferences\nRegina Barzilay, Michael Collins, Julia Hirschberg, and\nSteve Whittaker. 2000. The rules behind roles: Identi-\nfying speaker role in radio broadcasts. In AAAI/IAAI,\npages 679–684.\nMohamed Lazhar Bellagha and Mounir Zrigui. 2020.\nSpeaker naming in tv programs based on speaker role\nrecognition. In 2020 IEEE/ACS 17th International\nConference on Computer Systems and Applications\n(AICCSA), pages 1–8. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNikolaos Flemotomos, Panayiotis Georgiou, and\nShrikanth Narayanan. 2019. Linguistically aided\nspeaker diarization using speaker role information.\narXiv preprint arXiv:1911.07994.\nDongyue Guo, Jianwei Zhang, Bo Yang, and Yi Lin.\n2023. A comparative study of speaker role identifica-\ntion in air traffic communication using deep learning\napproaches. ACM Transactions on Asian and Low-\nResource Language Information Processing, 22(4):1–\n17.\nRashid Jahangir, Ying Wah Teh, Henry Friday Nweke,\nGhulam Mujtaba, Mohammed Ali Al-Garadi, and\nIhsan Ali. 2021. Speaker identification through artifi-\ncial intelligence techniques: A comprehensive review\nand research challenges. Expert Systems with Appli-\ncations, 171:114591.\nSeho Kee, Enrique Del Castillo, and George Runger.\n2018. Query-by-committee improvement with diver-\nsity and density in batch active learning. Information\nSciences, 454:401–418.\nRémi Lavalley, Chloé Clavel, Patrice Bellot, and Marc\nEl-Beze. 2010. Combining text categorization and\ndialog modeling for speaker role identification on\ncall center conversations. In Eleventh Annual Con-\nference of the International Speech Communication\nAssociation.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYang Liu. 2006. Initial study on automatic identification\nof speaker role in broadcast news speech. In Proceed-\nings of the Human Language Technology Conference\nof the NAACL, Companion Volume: Short Papers,\npages 81–84.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nFarhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei\nZhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang,\nand QM Jonathan Wu. 2022. A review of generalized\nzero-shot learning methods. IEEE transactions on\npattern analysis and machine intelligence.\nMichael Rouvier, Sebastien Delecraz, Benoit Favre,\nMeriem Bendris, and Frederic Bechet. 2015. Mul-\ntimodal embedding fusion for robust speaker role\nrecognition in video broadcast. In 2015 IEEE Work-\nshop on Automatic Speech Recognition and Under-\nstanding (ASRU), pages 383–389. IEEE.\nAshtosh Sapru and Fabio Valente. 2012. Automatic\nspeaker role labeling in ami meetings: recognition\nof formal and social roles. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5057–5060. IEEE.\nBurr Settles. 2010. Active learning literature survey.\nUniversity of Wisconsin, Madison, 52.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nWen Wang, Sibel Yaman, Kristin Precoda, and Colleen\nRichey. 2011. Automatic identification of speaker\nrole and agreement/disagreement in broadcast con-\nversation. In 2011 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 5556–5559. IEEE.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8395158052444458
    },
    {
      "name": "Identification (biology)",
      "score": 0.7158385515213013
    },
    {
      "name": "Sentence",
      "score": 0.6675246953964233
    },
    {
      "name": "Task (project management)",
      "score": 0.6450539827346802
    },
    {
      "name": "Natural language processing",
      "score": 0.6240957975387573
    },
    {
      "name": "Call centre",
      "score": 0.6188694834709167
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5752809047698975
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5655630826950073
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.560496985912323
    },
    {
      "name": "Speaker diarisation",
      "score": 0.48108431696891785
    },
    {
      "name": "Speaker recognition",
      "score": 0.43931013345718384
    },
    {
      "name": "Speech recognition",
      "score": 0.400422602891922
    },
    {
      "name": "Linguistics",
      "score": 0.23069173097610474
    },
    {
      "name": "Engineering",
      "score": 0.06802991032600403
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}