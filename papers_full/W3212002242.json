{
  "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on\\n HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
  "url": "https://openalex.org/W3212002242",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3124133520",
      "name": "Kim Boseop",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149725702",
      "name": "Kim, HyoungSeok",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1985784390",
      "name": "Lee, Sang-Woo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225158067",
      "name": "Lee, Gichang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3027196477",
      "name": "Kwak, Donghyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4257533305",
      "name": "Jeon Dong Hyeon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A241748875",
      "name": "Park, Sunghyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1951152649",
      "name": "Kim Sung‐Ju",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320713776",
      "name": "Kim, Seonhoon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301201804",
      "name": "Seo, Dongpil",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lee, Heungsub",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3123277723",
      "name": "Jeong Minyoung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2199701592",
      "name": "Lee Sung-Jae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296973805",
      "name": "Kim, Minsub",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ko, Suk Hyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745357015",
      "name": "Kim Seok-Hun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2384730125",
      "name": "Park Tae-Yong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A922184168",
      "name": "Kim, Jinuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2602600822",
      "name": "kang soyoung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3194529392",
      "name": "Ryu Na hyeon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3041857571",
      "name": "Yoo, Kang Min",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3182008222",
      "name": "Chang, Minsuk",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Suh, Soobin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "In, Sookyo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2242458201",
      "name": "Park Jin-Seong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222966867",
      "name": "Kim, Kyungduk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301201799",
      "name": "Kim, Hiun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3010775306",
      "name": "Jeong Jisu",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Yeo, Yong Goo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225588740",
      "name": "Ham, Donghoon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3041609951",
      "name": "park dongju",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2282316916",
      "name": "Lee Min Young",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746902769",
      "name": "Kang Jae-Wook",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1903223452",
      "name": "Kang In-ho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3172613083",
      "name": "Ha，jung woo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225158068",
      "name": "Park Woomyoung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225158070",
      "name": "Sung, Nako",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3156891177",
    "https://openalex.org/W3116216579",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3115690768",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W3162296828",
    "https://openalex.org/W3035408261",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W3168149265",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3133101440",
    "https://openalex.org/W2972340899",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W2935711174",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3089263616",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W3134678353",
    "https://openalex.org/W2963250244"
  ],
  "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language\\nmodels (LMs) trained on hundreds of billion scale data. Here we address some\\nremaining issues less reported by the GPT-3 paper, such as a non-English LM,\\nthe performances of different sized models, and the effect of recently\\nintroduced prompt optimization on in-context learning. To achieve this, we\\nintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric\\ncorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA\\nwith our training configuration shows state-of-the-art in-context zero-shot and\\nfew-shot learning performances on various downstream tasks in Korean. Also, we\\nshow the performance benefits of prompt-based learning and demonstrate how it\\ncan be integrated into the prompt engineering pipeline. Then we discuss the\\npossibility of materializing the No Code AI paradigm by providing AI\\nprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,\\nan interactive prompt engineering interface. Lastly, we demonstrate the\\npotential of our methods with three successful in-house applications.\\n",
  "full_text": "What Changes Can Large-scale Language Models Bring?\nIntensive Study on HyperCLOV A: Billions-scale Korean Generative\nPretrained Transformers\nBoseop Kim∗,1 HyoungSeok Kim∗,1 Sang-Woo Lee∗,1,2 Gichang Lee1\nDonghyun Kwak1 Dong Hyeon Jeon3 Sunghyun Park4 Sungju Kim1,3\nSeonhoon Kim3 Dongpil Seo1 Heungsub Lee1 Minyoung Jeong1 Sungjae Lee1\nMinsub Kim1 Suk Hyun Ko1 Seokhun Kim1 Taeyong Park1 Jinuk Kim1\nSoyoung Kang1 Na-Hyeon Ryu1 Kang Min Yoo1,2 Minsuk Chang2 Soobin Suh1,3\nSookyo In1,3 Jinseong Park1,3 Kyungduk Kim1,3 Hiun Kim1 Jisu Jeong1,2\nYong Goo Yeo1 Donghoon Ham1 Dongju Park1 Min Young Lee1 Jaewook Kang1\nInho Kang1,3 Jung-Woo Ha1,2 Woomyoung Park1 Nako Sung1\nNA VER CLOV A1 NA VER AI Lab2 NA VER Search3 Search Solutions, Inc.4\nAbstract\nGPT-3 shows remarkable in-context learning\nability of large-scale language models (LMs)\ntrained on hundreds of billion scale data. Here\nwe address some remaining issues less re-\nported by the GPT-3 paper, such as a non-\nEnglish LM, the performances of different\nsized models, and the effect of recently in-\ntroduced prompt optimization on in-context\nlearning. To achieve this, we introduce Hy-\nperCLOV A, a Korean variant of 82B GPT-3\ntrained on a Korean-centric corpus of 560B\ntokens. Enhanced by our Korean-speciﬁc to-\nkenization, HyperCLOV A with our training\nconﬁguration shows state-of-the-art in-context\nzero-shot and few-shot learning performances\non various downstream tasks in Korean. Also,\nwe show the performance beneﬁts of prompt-\nbased learning and demonstrate how it can\nbe integrated into the prompt engineering\npipeline. Then we discuss the possibility of\nmaterializing the No Code AI paradigm by\nproviding AI prototyping capabilities to non-\nexperts of ML by introducing HyperCLOV A\nstudio, an interactive prompt engineering inter-\nface. Lastly, we demonstrate the potential of\nour methods with three successful in-house ap-\nplications.\n1 Introduction\nDue to its remarkable zero-shot and few-shot per-\nformances, GPT-3’s in-context learning has gained\nsigniﬁcant attention in the AI community (Brown\net al., 2020). In the in-context learning approach,\ndiscrete prompts that consist of a natural language\ntask description and few-shot examples control\n∗Equal contribution.\nlarge-scale language models (LMs) to infer pre-\ndictions for the target task. Using OpenAI’s GPT-3,\nstudies have proposed methods that can further\nboost in-context learning performance of GPT-3\n(Zhao et al., 2021; Liu et al., 2021a). Recently,\nprompt-based learning methods have been reported\nto improve the performances of BERT, GPT-3, and\nT5 without any parameter updates of the main\nmodel (Liu et al., 2021b; Lester et al., 2021; Shin\net al., 2020).\nWe consider the three following practical issues\nof using GPT-3. First, the language composition\nof the training corpus is heavily skewed towards\nEnglish with 92.7%. This makes it difﬁcult to ap-\nply it to tasks in other languages. We also know\nlittle about how to train similar models in another\nlanguage with different linguistic properties, and\nwhere the originally proposed methods will be nat-\nurally applied and where they might fail. Second,\nwhile it is pragmatic and useful to know the ca-\npabilities of various sized models considering the\noperation costs of using large-scale LMs, we only\nhave access to a thorough analysis of models of\n13B and 175B (Brown et al., 2020) but none in\nbetween. Lastly, advanced prompt-based learning\nmethods that require backward gradients of inputs,\nincluding continuous prompt-based tuning, have\nnot yet been experimented for an in-context large-\nscale LM learner.\nHere we address these issues by introducing a\nnon-English GPT-3 with various parameter sizes\nand intensively investigating their capabilities on di-\nverse real-world classiﬁcation and generation tasks\nunder in-context few-shot learning and prompt-\nbased optimization. We introduce a Korean in-\narXiv:2109.04650v2  [cs.CL]  28 Nov 2021\ncontext large-scale LM with 82B parameters, i.e.,\nHyperCLOV A. This is the ﬁrst discovery on near\n100B-scale non-English LM. We present the cor-\npus composition of Korean datasets used for Hy-\nperCLOV A, and describe how we crawl and re-\nﬁne such data to collect 561B tokens of Korean\ncorpus (§3.1). We also design a new Korean tok-\nenization method based on the agglutinative prop-\nerty for HyperCLOV A. We use byte-level BPE\n(Kudo and Richardson, 2018) with a morpheme\nanalyzer (§3.3). Our results show that such tok-\nenization strategy is important for the performance\nof downstream tasks in large-scale in-context learn-\ning (§4.4).\nWe report the state-of-the-art in-context learn-\ning performance of our model on Korean datasets\nin zero and few-shot settings (§4.2). In addition,\nwe are the ﬁrst to discovery the applicability of\nthe continuous prompt-based optimization tech-\nniques, such as p-tuning (Liu et al., 2021b), to large-\nscale LMs. HyperCLOV A leveraged by p-tuning\nachieves outstanding results for both classiﬁcation\nand generation tasks. Also, we investigate the ef-\nfects of p-tuning on two mid-size HyperCLOV A\n(§4.3).\nSubsequently, we illustrate the versatility of op-\nerating a single large-scale LM in the AI industry.\nDeveloping an AI product involves heavy collab-\noration among non-technical experts. This incurs\nsubstantial communication overhead because the\nlevel of technical abstraction varies across job func-\ntions.\nWe introduce HyperCLOV A Studio, an inter-\nactive prompt engineering interface which pro-\nvides GUI and API interfaces like the OpenAI\nplayground1. The interactive interface helps non-\nexperts of ML to easily use HyperCLOV A for pro-\ntotyping AI products. We also share three in-house\napplication scenarios using HyperCLOV A Studio\nas novel task environments. With minimal efforts of\na domain expert in these scenarios, HyperCLOV A\npresents performances qualitatively comparable to\nhuman experts, despite their difﬁculty in designing\ntheir objective function and training data (§5.2).\nWe then discuss how the functionality of Hy-\nperCLOV A Studio can be extended. For example,\nHyperCLOV A Studio can provide input gradient\nfunctionality, to ﬁne-tune small prompt encoder\nwith few number of instances, thus enabling any\nuser to achieve state-of-the-art performance using\n1https://beta.openai.com/\nHyperCLOV A (§5.3). Finally, we discuss the possi-\nbility of No/Low Code AI paradigm using Hyper-\nCLOV A Studio, in which one large LM empowers\npeople to create AI systems with no need for train-\ning individual deep learning models or collecting\nand labeling suitable datasets (§5.4).\nOur contributions are summarized as:\n1. We introduce HyperCLOV A, a large-scale\nKorean in-context learning-based LM with\nnearly 100B parameters, by constructing a\nlarge Korean-centric corpus of 560B tokens.\n2. We discover the effect of language-speciﬁc\ntokenization on large-scale in-context LMs\nfor training corpus of non-English languages.\n3. We explore the zero-shot and few-shot capabil-\nities of mid-size HyperCLOV A with 39B and\n82B parameters and ﬁnd that prompt-based\ntuning can enhance the performances, out-\nperforming state-of-the-art models on down-\nstream tasks when backward gradients of in-\nputs are available.\n4. We argue the possibility of realizing No Code\nAI by designing and applying HyperCLOV A\nStudio to our in-house applications. We will\nrelease HyperCLOV A Studio with input gradi-\nents, output ﬁlters, and knowledge injection.\n2 Previous Work\n2.1 Prompt Optimization\nPrompt-based approaches involve constructing op-\ntimal prompts for language models to best elicit\nknowledge and maximize prediction performances\n(Radford et al., 2019; Brown et al., 2020; Schick\nand Schütze, 2020). As the scale of language mod-\nels grows, the potential of replacing the full ﬁne-\ntuning paradigm with the prompt-based approach\nhas been reported (Reynolds and McDonell, 2021;\nLi and Liang, 2021), as learning via prompts is ef-\nﬁcient regarding time and space complexity. How-\never, language models are highly sensitive to the\nprompt design, motivating methodologies for opti-\nmizing prompts.\nPrompt optimization can be categorized into dis-\ncrete and continuous approaches. The discrete ap-\nproach optimizes directly on the token space (Ben-\nDavid et al., 2021; Shin et al., 2020) and has the\nadvantage of transferability. However, Shin et al.\n(2020) showed that the discrete space has poor in-\nterpretability and can be suboptimal. These limita-\ntions spurred a new direction that aims to optimize\nprompts in the continuous space. Recent work (Li\nand Liang, 2021; Hambardzumyan et al., 2021;\nLiu et al., 2021b; Lester et al., 2021) proposed op-\ntimizing the contextualized token spaces without\nﬁne-tuning the main LM parameters. Notably, Liu\net al. (2021b) found that p-tuning for autoregressive\nLMs outperforms MLM-based ﬁne-tuning in cer-\ntain downstream tasks. Lester et al. (2021) further\nshowed that well-optimized prompt-based learn-\ning achieves state-of-the-art performance on key\nbenchmarks.\n2.2 Language Models\nAlthough multilingual language models have been\npublicly available (Devlin et al., 2019), language-\nspeciﬁc language models are still in demand, as\nthey provide an edge over language-agnostic mod-\nels (Martin et al., 2020; Nguyen and Nguyen, 2020;\nDelobelle et al., 2020). However, due to high cost,\nlanguage-speciﬁc language models other than En-\nglish are limited in availability.\nAs such, the community has an untapped un-\nderstanding of non-English in-context learners. To\nthe best of our knowledge, multilingual in-context\nlearners are not even explored yet, and the research\non in-context learners is focused on few major\nlanguages. Recently, a GPT-like language model\ntrained on Chinese corpora is being actively re-\nsearched concurrently with our work (Zeng et al.,\n2021). They successfully trained LMs of 2.6B and\n13B parameters using a Chinese corpus. They also\nshare their on-going work for training the 207B\nmodel, the corresponding infrastructure, and the\ntraining techniques.\n3 Pre-training\n3.1 Data Description\nThe ratio of Korean data for OpenAI GPT-3 is\nvery small, with less than 0.02% by character\ncount.2 Therefore, it is crucial to construct a large\nKorean-centric corpus in advance to training Hy-\nperCLOV A.\nThe major corpus used for pre-training Hyper-\nCLOV A is listed in Table 1. To build a large-scale\ncorpus comparable to that for training OpenAI\n2https://github.com/openai/gpt-\n3/blob/master/dataset_statistics/languages_by_word_count.csv\nName Description Tokens\nBlog Blog corpus 273.6B\nCafe Online community corpus 83.3B\nNews News corpus 73.8B\nComments Crawled comments 41.1B\nKiN Korean QnA website 27.3B\nModu Collection of ﬁve datasets 6.0B\nWikiEn, WikiJp Foreign wikipedia 5.2B\nOthers Other corpus 51.5B\nTotal 561.8B\nTable 1: Descriptions of corpus for HyperCLOV A.\n# Param nlayers dmodel nheads dhead lr\n137M 12 768 16 48 6.0e-4\n350M 24 1024 16 64 3.0e-4\n760M 24 1536 16 96 2.5e-4\n1.3B 24 2048 16 128 2.0e-4\n6.9B 32 4096 32 128 1.2e-4\n13B 40 5120 40 128 1.0e-4\n39B 48 8192 64 128 0.8e-4\n82B 64 10240 80 128 0.6e-4\nTable 2: Detailed conﬁguration per size of Hyper-\nCLOV A.\nGPT-3, we gathered all available text data includ-\ning user-generated content (UGC) and contents pro-\nvided by external partners, with no violation of le-\ngal issues, from both diverse services of NA VER3\nand external sources.\nWe reﬁned the datasets and collected a total of\n561B tokens as the ﬁnal corpus. The corpus was\nrandomly sampled for pre-training. Appendix A.1\ndescribes the detailed data description and discus-\nsion. Appendix A.2, A.3, and A.4 thoroughly de-\nscribe how to clean, anonymize, and preprocess the\ncrawled raw data, respectively.\n3.2 Model and Learning\nWe employ the same transformer decoder architec-\nture as GPT-3 of OpenAI (Brown et al., 2020). Ta-\nble 2 describes the detailed conﬁgurations of differ-\nent model sizes. We make our model design similar\nto GPT-3, and we set near exponential interpolation\nfrom 13B to 175B OpenAI GPT-3. In particular,\nwe aim to explore the capability and representa-\ntion power of the models with mid-size parameters,\nwhich have not yet been addressed by other studies\non large-scale LMs (Brown et al., 2020), but practi-\ncally useful in many applications. These mid-size\nmodels can contribute to not only understanding\nthe model properties with several tens of billion\n3https://www.naver.com/\nparameters, but also practical usages in real-world\napplications due to their more plausible sizes.\nOur model is based on megatron-LM (Shoeybi\net al., 2019) and trained on the NVIDIA Su-\nperpod, which includes 128 strongly clustered\nDGX servers with 1,024 A100 GPUs. We use\nAdamW (Loshchilov and Hutter, 2019) with co-\nsine learning rate scheduling and weight decay as\nan optimizer. All models use the mini-batch size of\n1,024 and the minimum learning rate is 1/10 of the\noriginal learning rate. It takes 13.4 days to train a\nmodel with 82B parameters with 150B tokens. For\nexperiments in Section 4, the model trained with\n150B is used for fair comparison, because not all\nmodels are ﬁnished training at the same iteration.\nHowever, experiments in Section 5.2 use the model\ntrained with 300B tokens, as HyperCLOV A Stu-\ndio provided the 39B and 82B models trained with\n300B tokens.\nIn our test loss from the encyclopedia corpus not\nincluded in HyperCLOV A corpus, we also observe\nthe scaling law, as discovered in previous research\n(Brown et al., 2020; Kaplan et al., 2020). Figure\n2 in Appendix B shows that increasing model size\nand training longer give advantage.\n3.3 Korean Tokenization\nKorean is an agglutinative language where noun is\nfollowed by particle and stem of verb or adjective\nis followed by endings, expressing various gram-\nmatical properties. Properly tokenizing noun and\nparticle, and stems and endings clariﬁes the seman-\ntics of each token. Park et al. (2020) introduce an\nempirical report that tokenization inﬂuences on per-\nformances of Korean LM. Overall, we need to de-\nsign a sophisticated tokenization strategy suitable\nfor Korean LM, different from its English counter-\npart.\nWe use morpheme-aware byte-level BPE as\nour tokenization method. GPT-2 and GPT-3 use\nbyte-level BPE. However, unlike in English, non-\nEnglish characters like ‘ㅎ’, ‘하’, or ‘한’ are all\nsplit into three different unicode bytes. We alle-\nviate the problem of byte-level BPE by applying\nmorpheme analyzers. See Figure 5 in Appendix E\nfor motivation and detail.\nWe pre-split sentences by using space and mor-\npheme obtained by an in-house morpheme analyzer.\nOur morpheme analyzer excludes most of non-\nKorean characters. Using parts of the sentence pre-\nsplit by our morpheme analyzer, our morpheme-\naware byte-level BPE learns the sentence in which\nmost non-Korean characters are expressed as single\nbyte characters. We use HuggingFace’s tokenizers\nlibrary.4\n4 Experimental Results\n4.1 Experimental Setting\nWe mainly use ﬁve datasets for evaluating in-\ncontext few-shot learning performance. Two of the\nﬁve datasets come from KLUE (Park et al., 2021),\nwhich is a massive benchmark of Korean NLU\ntasks and a work concurrent to our paper. We also\nuse one additional in-house dataset for evaluating\nprompt-based optimization performance.\nNSMC is a movie review dataset from NA VER\nMovies.5 The task is binary sentiment classiﬁca-\ntion, like SST-2 (Socher et al., 2013). It contains\n150K of training data and 50K of test data. For few-\nshot experiments, we generate 12 sets, and each set\nconsists of 70 examples randomly sampled from\nthe training set. We average the test accuracies of\n12 in-context 70-shot learning models.\nKorQuAD 1.0 (Lim et al., 2019) is a Korean ver-\nsion of machine reading comprehension dataset.6\nIt consists of 10,645 training passages with 66,181\ntraining questions and 5,774 validation questions.\nThe format of the dataset is similar to SQuAD 1.0\n(Rajpurkar et al., 2016). We follow the evaluation\nscheme of SQuAD v2.0 used in the work of Brown\net al. (2020), which uses test paragraph, correspond-\ning four question-answer pairs, and test question as\nthe input to GPT-3. In other words, our model is a\nzero-shot learner in the perspective of passage, but\na four-shot learner in the perspective of question.\nWe performed a single trial for each model size.\nAI Hub Korean-English corpus consists of\nKorean-English parallel sentences from news, gov-\nernment websites, legal documents, etc.7 The cor-\npus consists of 800K sentence pairs, and we ran-\ndomly sample 1K pairs for evaluating on Ko →En\nand En →Ko translation tasks. We performed three\nrandom trials for each translation task. Our model\nis evaluated in four-shot learning and we use four\ndifferent examples for each trial. We use BLEU\nscore for evaluation, where Moses and MeCab are\nused for comparison with the result of Park et al.\n(2020).\n4https://github.com/huggingface/tokenizers\n5https://github.com/e9t/nsmc\n6https://korquad.github.io/KorQuad%201.0/\n7https://aihub.or.kr/aidata/87\nNSMC KorQuAD AI Hub (BLEU) YNAT KLUE-STS\n(Acc) (EM / F1) Ko →En En →Ko (F1) (F1)\nBaseline 89.66 74.04 86.66 40.34 40.41 82.64 75.93\n137M 73.11 8.87 23.92 0.80 2.78 29.01 59.54\n350M 77.55 27.66 46.86 1.44 8.89 33.18 59.45\n760M 77.64 45.80 63.99 2.63 16.89 47.45 52.16\n1.3B 83.90 55.28 72.98 3.83 20.03 58.67 60.89\n6.9B 83.78 61.21 78.78 7.09 27.93 67.48 59.27\n13B 87.86 66.04 82.12 7.91 27.82 67.85 60.00\n39B 87.95 67.29 83.80 9.19 31.04 71.41 61.59\n82B 88.16 69.27 84.85 10.37 31.83 72.66 65.14\nTable 3: Results of in-context few-shot tasks on sentiment analysis, question answering, machine translation, topic\nclassiﬁcation, and semantic similarity per model size. As baselines, we report the results of BERT-base for NSMC\nand KorQuAD, and Transformer for AI Hub from Park et al. (2020). mBERT is used for YNAT and KLUE-STS\nfrom Park et al. (2021).\nYNAT (Yonhap News Agency Topic Classiﬁcation\nor KLUE-TC), one of the KLUE Benchmark tasks,\nis a topic classiﬁcation problem with seven classes\n(Park et al., 2021). It consists of 45K, 9K, and 9K\nannotated headlines for training, valid, and test sets,\nrespectively. We average the test accuracies of 3\nin-context 70-shot learners.\nKLUE-STS, another KLUE benchmark task, is a\ntask to predict a sentence similarity between each\npair of sentences, where the similarity score has a\nvalue between 0 and 5 (Park et al., 2021). We use\nF1 score after binarizing the real-valued similarity\nas suggested in the KLUE paper. We average the\ntest accuracies of 3 in-context 40-shot learners.\nQuery modiﬁcation task is a query modiﬁcation\ntask for AI speaker users. The task targets the case\nwhere a single-turn FAQ system is already operat-\ning in AI Speakers. With the query that requires\nunderstanding of multi-turn information, the goal\nof the task is to convert the multi-turn query to a\nsingle-turn query, which can then be understood\nby a single-turn AI speaker. There are 1,326 test\ninstances in total. See Appendix C.3 for detail.\nBaseline We use the scores for baseline models,\nBERT and Transformer from Park et al. (2020),\nand mBERT (BERT-Multilingual) from Park et al.\n(2021), for in-context learning experiments in Ta-\nble 3, whereas mBERT (Devlin et al., 2019) and\nRoBERTa (Kang et al., 2020) are also used for\np-tuning experiments in Table 4.\n4.2 In-context Few-shot Learning\nTable 3 presents the results of few-shot learning\non six tasks. In particular, we explore the perfor-\nmances of HyperCLOV A with mid-size parameters\nincluding 39B and 82B, which is not addressed in\nOpenAI GPT-3 paper (Brown et al., 2020) but can\nbe more practical for real-world applications. Ap-\npendix C.1 and C.2 further explains more results of\nstandard deviation and max performance of trials.\nTable 3 shows that the performances of various\nin-context learning tasks monotonically increases\nas the model size increases. However, in-context\nlearning ability of Ko→En translation and KLUE-\nSTS is much lower than baseline. Especially for\ntranslation, we conjecture the poor performances\non Ko→En might result from lack of English ratio\nof our corpus. Also, more sophisticated prompt\nengineering might improve the results, which is\nfuture research direction.\n4.3 Prompt-based Tuning\nTable 4 shows the results of prompt-based tuning (p-\ntuning) (Liu et al., 2021b) on NSMC. Although in-\ncontext few-shot learning has already achieved near\nstate-of-the-art performance on NSMC, p-tuning\nenables HyperCLOV A to outperform comparatives\nwith no parameter update of the main model. It\nis worth noting that p-tuning with only 4K exam-\nples only provides comparable results to RoBERTa\nﬁne-tuned on 150K data. Considering the results in\nTable 3 and Table 9 in Appendix C.1, we conjecture\nthat p-tuning signiﬁcantly enhances the robustness\nof HyperCLOV A as well as the accuracy.\nFurthermore, we explore the effects of p-tuning\nat the input side on performances for generation\ntasks with the experiments on our in-house query\nmodiﬁcation. As shown in Table 5, p-tuning en-\nables HyperCLOV A to consistently improve the\ninput query qualities with a signiﬁcant margin for\nboth zero and three-shot scenarios. In larger mod-\nels, the inﬂuence of the discrete prompt seems to be\nMethods Acc\nFine-tuning\nmBERT (Devlin et al., 2019) 87.1\nw/ 70 data only 57.2\nw/ 2K data only 69.9\nw/ 4K data only 78.0\nBERT (Park et al., 2020) 89.7\nRoBERTa (Kang et al., 2020) 91.1\nFew-shot\n13B 70-shot 87.9\n39B 70-shot 88.0\n82B 70-shot 88.2\np-tuning\n137M w/ p-tuning 87.2\nw/ 70 data only 60.9\nw/ 2K data only 77.9\nw/ 4K data only 81.2\n13B w/ p-tuning 91.7\nw/ 2K data only 89.5\nw/ 4K data only 90.7\nw/ MLP-encoder 90.3\n39B w/ p-tuning 93.0\nTable 4: Comparison results of p-tuning with ﬁne-tuned\nLMs and in-context few-shot learning on NSMC. MLP-\nencoder means the result of replacing LSTM with MLP\nas the p-tuning encoder on 150K NSMC training data.\nModel sizes Few-shots p-tuning BLEU\n13B\nzero-shot × 36.15\nO 58.04\n3-shot × 45.64\nO 68.65\n39B\nzero-shot × 47.72\nO 73.80\n3-shot × 65.76\nO 71.19\nTable 5: Results of p-tuning on in-house query modiﬁ-\ncation task.\nless. This result is similar to the trend discovered\nin (Lester et al., 2021), that as the scale of LM in-\ncreases, competitive performance can be obtained\neven if the discrete prompt is not used at all. To\nthe best of our knowledge, this is the ﬁrst report\nof applying input-side p-tuning to generation tasks\nwith an in-context LM learner.\nThese results also imply that when the backward\ngradients of GPT-3-scale model on input data are\naccessible, prompt optimization methods are feasi-\nble alternatives for enhancing representation power\nof large-scale LMs for NLP researchers and practi-\ntioners without large-scale GPU clusters.\n4.4 Effect of Tokenization\nWe analyze the effects of morpheme-aware byte-\nlevel BPE, our tokenization method considering\nKorean linguistic characteristics. As baselines, we\nemploy byte-level BPE and char-level BPE, two\nprevalent tokenization methods for pre-training\nLMs with English-centric corpora. It is noticeable\nthat char-level BPE refers to the original BPE. It\nyields out-of-vocabulary (OOV), and some Korean\ncharacter like ‘젝’ is not included in char-level BPE\ntokens. The other two tokenization strategies do not\nmake OOV tokens. We use models of 1.3B param-\neters, which is a relatively small size, considering\nthe heavy computation time of pre-training. Never-\ntheless, it is enough to ﬁnd evidence of tokenization\neffects.\nAs shown in Table 6, our method improves the\nperformance of most tasks compared to the base-\nlines. However, in Ko→En task, morpheme ana-\nlyzer makes the performance worse. On the other\nhand, char-level BPE makes much lower perfor-\nmance than byte-level BPE in YNAT. It is because\nthat char-level BPE makes some OOV tokens, and\nsome important words in a headline of YNAT data\nbecome hard to understand. For example, a char-\nacter ‘젝’ (jec) in a word ‘프로젝트’ (project in\nEnglish) is an OOV token in char-level BPE, which\nmakes the test headline including ‘ 프로젝트’ in-\ncomprehensive. Overall, it is worth noting that care-\nfully designing language-speciﬁc tokenization is\nessential for training large-scale LMs for languages\nquite different from English in terms of their lin-\nguistic properties.\n5 Discussion on Industrial Impacts\nWhat change can large-scale LMs bring? We claim\n“accelerating the life-cycle of NLP ML operation”\nas one of the possible answers. Unlike the protocol\nof most deep learning research where a model is\ntrained with a well-collected dataset by ML experts\nand its corresponding well-deﬁned objective func-\ntion, there are several additional steps to make an\nAI product in a production-level pipeline, which\nyield tremendous communication overhead and\ncosts. A platform with large-scale LMs may make\nhuge progress by allowing only one non-developer,\nsuch as a service designer, to build the prototype\nsystem.\nSection 5.1 introduces HyperCLOV A Studio as\nour distribution method of HyperCLOV A. Section\n5.2 introduces our three in-house usages of Hy-\nKorQuAD AI Hub (BLEU) YNAT KLUE-STS\n(EA / F1) Ko →En En →Ko (F1) (F1)\nOurs 55.28 72.98 3.83 20.03 58.67 60.89\nbyte-level BPE 51.26 70.34 4.61 19.95 48.32 60.45\nchar-level BPE 45.41 66.10 3.62 16.73 23.94 59.83\nTable 6: Effects of tokenization approaches on ﬁve tasks. HyperCLOV A-1.3B is used for evaluation.\nperCLOV A Studio. Section 5.3 discusses possible\nextensions of HyperCLOV A Studio, prompt-based\noptimization, input module, and output module.\nUsing the evidence above, Section 5.4 discusses\nNo/Low Code AI paradigm.\n5.1 HyperCLOV A Studio\nHyperCLOV A Studio is the place for building and\ncommunicating the shared artifact generated by\nHyperCLOV A. HyperCLOV A Studio serves two\nfunctions, 1) it can provide a GUI interface, like\nthe OpenAI Playground, and 2) support API end\npoint in which the output can be easily acquired by\nan API call with diverse functions, including ones\nnot yet provided by OpenAI Playground. These\nadvanced functions are speciﬁed in Section 5.3.\nFigure 3 in Appendix D shows our GUI interface.\nThe biggest advantage of HyperCLOV A Studio is\nthat it allows rapid prototyping of AI-based ser-\nvices while minimizing the involvement of ML\nengineers.\n5.2 Case Studies on HyperCLOV A Studio\nThis section shares three in-house applications pow-\nered by HyperCLOV A Studio, which are novel\ntasks with a large-scale LM as illustrated in Figure\n1. The three in-house usages share three properties\nbelow. First, it is non-trivial to deﬁne the objective\nfunction or to evaluate the models automatically.\nSecond, the style of the inputs and outputs is eas-\nily controlled. Lastly, a product designer, without\nprogramming skill nor knowledge of AI, can easily\nmake Proof-of-Concept (PoC) systems within few\nhours.\n5.2.1 Rapidly Prototyping Chatbots with\nPersonalities\nThis subsection discusses rapid prototyping of chat-\nbots with personalities (Smestad and V olden, 2018)\nusing HyperCLOV A. Our chatbot designers found\nthat HyperCLOV A allows them to build a chatbot\nwith the persona of a speciﬁc character using one or\ntwo lines of description on the character property\nand few dialog examples. This process can be used\nZero-shot (Acc)\n# of augmented samples (k)\nn 5(1) 10(2) 15(3) 25(5) 125(30)\n0(0) 60.8 9.3 68.94.0 71.92.7 74.82.5 78.02.3\nFew-shot (Acc)\n# of original samples (n)\nk 1(1) 2(1) 3(1) 4(1) 5(1)\n0(0) 26.8 6.0 52.04.9 64.75.2 76.54.4 83.03.0\n25(5) 79.2 2.5 81.22.5 82.62.6 83.41.9 84.32.0\n125(30) 80.7 2.2 82.71.9 83.72.1 86.31.5 87.21.7\nTable 7: Zero-shot and few-shot performances in zero-\nshot transfer data augmentation. ndenotes the number\nof original training (validation) instances per class, and\nk denotes the number of generated instances for train-\ning (validation) per class. Subscripted values are stan-\ndard deviation.\nfor producing many bots in metaverse applications.\nFigure 1 (a) shows an example.\nThe style of the character can be controlled\neasily by changing a few dialog examples in the\nprompt. Knowledge in HyperCLOV A can also be\nimplicitly extracted using the beginning of the\nprompt. For example, the knowledge of the famous\ncan be reﬂected. Detailed discussion can be found\nin Appendix C.4.\nPoC can be easily available, and the following\nhuman-in-the-loop process can accelerate making\na bot. Based on these functions, it is possible to\nquickly build a dialogue system of various charac-\nteristics. HyperCLOV A Studio also supports these\nfunctionalities.\n5.2.2 Zero-shot Transfer Data Augmentation\nThe task is to build utterances tailored to user in-\ntent. Given the natural language name of the user’s\nintent, corresponding utterances are generated. For\nexample, if you give “reservation query with one\nperson” as the user intent name, HyperCLOV A\nwill output sentences like “Is it OK for reserva-\ntion with one person?” We formulate this problem\nas in-context zero-shot transfer data augmentation.\nWe give source-domain classes and correspond-\nFigure 1: Examples generated by HyperCLOV A with the prompts under three different tasks. Italic implies given\nprompts and non-italic corresponds to generated outputs. The examples are translated into English.\ning examples for each source-domain class to the\nprompt. Source-domain classes are different from\ntarget-domain classes.\nThe name of intent can be simple, like “reserva-\ntion inquiry” or complex, like “Complaints about\nthe degree of steak doneness”. In in-house usages,\na team for managing the quality of the product uses\nthis function to make diverse utterances to vali-\ndate the dialog system. The team reported that they\ncould easily make diverse utterances of a intent\nwith the complicated situation using HyperCLOV A\nStudio.\nWe design a simple experiment to obtain quanti-\ntative results. We select 20 classes in an in-house\nintent corpus as the target domain and 6 classes\nwith 5 examples each for the source domain. Quan-\ntitative results using the 39B model are illustrated\nin Table 7. See the details and discussions in Ap-\npendix C.5.\n5.2.3 Event Title Generation\nEvent title generation is to generate the titles of\nan event for enhancing product advertisement in\nour e-commerce platforms. Similar to the signif-\nicant effect of the product titles on CTR and rev-\nenue (Zhang et al., 2019), the product event title\nhas a crucial inﬂuence on the product’s success.\nEvent title generation is formulated as a sequence-\nto-sequence task to transform keywords describing\nthe product characteristics into an impressive event\ntitle.\nFor achieving this, we ask an event designer to\nprepare ﬁve examples including event date and key-\nwords as a prompt to HyperCLOV A. Within less\nthan 10 minutes of designers’ effort, HyperCLOV A\nStudio was able to generate the candidates of sales\nevent titles with high quality. Table 8 presents the\nquantitative results of the event title generation. We\nemploy mT5-base (Multilingual T5) model (Xue\net al., 2020) as a baseline. mT5-base has a size of\nBLEU Win Lose Tie\nmT5 vs. GT 13.28 0.311 0.433 0.256\nHyperCLOV A vs. mT5 - 0.456 0.350 0.194\nGT vs. HyperCLOV A 5.66 0.311 0.333 0.356\nTable 8: Results of event title generation. GT denotes\nthe ground truth title written by human experts. Win\nmeans X wins against Y under X vs. Y. BLEU is the\nBLEU score of each model with its corresponding GT.\n580M and is ﬁne-tuned with 400K training data.\nFor human evaluation, we asked nine human ex-\nperts to pick the best expression among the titles\ngenerated by GT, mT5, and HyperCLOV A. As\nshown in Table 8, HyperCLOV A can yield high-\nquality titles comparable to GT. Interestingly, we\nﬁnd that higher BLEU scores with respect to GT do\nnot guarantee higher qualities (Mathur et al., 2020).\nOn the contrary, it is worth noting that lower BLEU\nof HyperCLOV A implies that it can generate more\ncreative titles, not using the exact words of GTs yet\nsatisfying their qualities. Our system is also easy\nto control the theme that each designer wants to\nemphasize for the same keyword, such as discount-\ning promotion, item brand, and product values. The\ndetailed results are presented in Appendix C.6.\nUnlike ﬁne-tuned models, HyperCLOV A is easy\nto be adapted to the events of other domains by\nmodifying the prompts. We also share usage of\nthe advertisement headline task in the Appendix\nC.6, where few training examples are available,\nbut the prompt similar to the event title generation\ntask achieves 99% of appropriateness for the real\nservice.\n5.3 Opportunity of HyperCLOV A Studio\nHyperCLOV A Studio can boost the ability of Hy-\nperCLOV A by multiple additional AI functions.\nFirst, input gradient API, which gives input gradi-\nent of HyperCLOV A can be applied to enhance the\nperformance of local downstream tasks. Even for\nthe downstream task that the in-context learner per-\nforms well, prompt-based optimization can further\nboost the performance. Section 4.3 shows the pos-\nsibility. Our studio can be extended to supply input\ngradient function to support prompt-tuning in local\nmachines. Then each developer can also train their\nown prompt encoder using prompt-optimization\nmethods, such as Autoprompt (Shin et al., 2020), p-\ntuning (Liu et al., 2021b), or prompt tuning (Lester\net al., 2021).\nSecond, prompt injection module can be applied.\nHyperCLOV A can be used for an open-domain\nQA reader by using adequate documents retrieved\nby a retriever. In general, retrieving knowledge or\nsimilar examples can boost the performance of Hy-\nperCLOV A.\nFinally, ﬁlters for input and output are helpful\nfor preventing misuse of HyperCLOV A. OpenAI\nAPI also provides a ﬁlter to monitor generations of\nsensitive or ethically inadequate sentences.\n5.4 No/Low Code AI Paradigm\nA typical machine learning development pipeline\ninvolves (1) problem deﬁnition and user research,\n(2) data gathering and annotation, (3) training and\nvalidating models, (4) deploying and operating ma-\nchine learning systems (MLOps), (5) error analysis\nand user monitoring. It is an iterative process where\nany issue in one step propagates to other steps, and\nthe need for revisiting the steps for revision and\nupdate constantly arises even after the model de-\nployment.\nThis is especially tedious and resource-heavy,\nnot only because this pipeline involves different\nexpertise and different roles, but also because there\nis not a shared grounded artifact to facilitate the\ncommunication between the experts.\nA single large-scale LM with GUI interfacing on\na prompt, like HyperCLOV A Studio, can remark-\nably alleviate this problem. Speciﬁcally, the 2 ∼\n4th steps of the previous ﬁve processes can be com-\nbined into one step. In the uniﬁed phase, curating\nexamples, prompt design, API parameter tuning,\nand API integration can take place at once.\nIt is notable that an approach with a single large-\nscale LM makes communication costs of experts\nbe dramatically reduced. Through this, the proto-\ntype of desired AI product can be created within\nfew hours. Though many companies want to use\nAI technology, it is costly to make the companies\nand teams to use AI techniques and gather data for\nAI, Therefore, there have been several discussions\nabout strategies for adopting AI technology (Raffel\net al., 2020). An approach with a single large-scale\nLM provides a novel paradigm to research commu-\nnities and industries.\nNo Code AI approach is powerful when fast\niteration on PoC is beneﬁcial or when services\ncan be solely built with pure generation ability\nof large-scale model. Low Code AI approach can\nbe used where it uses some training dataset (Liu\net al., 2021a) following by pre-processing code or\ninput/output modules are required.\nWe discuss the challenges of achieving No/Low\nCode AI paradigm with large-scale LMs in Section\nF of the Appendix with detail.\n6 Conclusion\nWe present HyperCLOV A, various billions-scale\nKorean-centric LMs. In particular, HyperCLOV A\nwith 82B parameters shows state-of-the-art in-\ncontext zero-shot and few-shot performance and\ncan further be boosted by prompt-based learning\nmethod. We will share our model by HyperCLOV A\nStudio where non-developers can easily build their\nown AI-backed products. We argue that a frame-\nwork like HyperCLOV A Studio can potentially\nachieve No Code AI paradigm and hope that cases\nof such paradigm become popular, although oppor-\ntunities and challenges coexist.\nOur goal is to create an ecosystem using Hy-\nperCLOV A studio in Korea and help people not\nfamiliar with machine learning make their own AI\nmodels.\nAcknowledgment\nThe authors thank all the members of CLOV A,\nAI Lab, and Search of NA VER for devoted sup-\nporting and discussion. In particular, they thank\nYonghwa Kim, Jin Hwan Suk, Jinsu Park, Hanah\nLim, and the members of CLOV A ML X for qual-\nitative evaluation. In addition, the authors thank\nNA VER Cloud for technically supporting the train-\ning environments of HyperCLOV A. Finally, the\nauthors thank Reinald Kim Amplayo, Hwaran Lee,\nand Sohee Yang for proofreading.\nBroader Impact Statement\nSince GPT3 was released, NLP and AI communi-\nties were impressed by the capability of its vari-\nants remarkably overwhelming the previous work.\nDespite their great success, these hyperscale pre-\ntrained LMs raise several severe concerning issues,\nwhich may harm the sustainability of AI and soci-\nety.\nMisuse of large-scale LMs: The case of Tay,\nthe chatbot developed by Microsoft in 2016 8, is\none of the most well-known misusing examples.\nRecently, Luda, a Korean chatbot developed by a\nKorean startup, suffered from serious sexual abuse\nby malicious users9. This situation brought a fun-\ndamental and social problem of whether AI can\nbe an abused target to the surface. In Luda service,\nprivacy issues were more critical from a legal per-\nspective caused by incomplete data preprocessing\nfor privacy-preserving. In addition to private infor-\nmation, hate speech data can lead to malicious mis-\nuse of language models when used as training data.\nSeveral GPT3 API applications also have reported\nthese malicious usages and problematic generation\nresults10.\nFairness, Bias, and Representation: Another\ncritical problem of Luda was biased and repulsive\nresponses on various sensitive social values includ-\ning gender and racism. Many studies have already\nreported that these biases from training data have\nsigniﬁcant inﬂuences on large-scale language mod-\nels as well (Abid et al., 2021; Garrido-Muñoz et al.,\n2021; Shwartz and Choi, 2020). To overcome these\nissues, many researchers argue the necessity of\ncontrollability when generating sentences such as\nﬁltering and investigate how to more effectively\nreﬁne the data for debiasing (Tamkin et al., 2021).\nExcessive Energy Consumption: Many re-\nsearchers have serious concerns about too heavy\nenergy consumption for training large-scale mod-\nels, which have been recently reported by several\nanalysis papers (Patterson et al., 2021; Bender et al.,\n2021). Scaling raw presents more parameters and\ntraining data are essential for better performance,\nwhich inevitably makes the energy issue worse. A\nplausible alternative is to use energy-efﬁcient hard-\nware such as FPGA.\nEfforts for Positive Directions: Despite all\nthese concerns and side effects, large-scale LMs\ncan provide signiﬁcant and innovative beneﬁts\nwhich cannot be expected from previous AI tech-\nnologies. One of the most valuable functions of\nlarge-scale LMs is the possibility of No/Low Code\n8https://bit.ly/3b6bL3o\n9https://bit.ly/3tp1Rjs\n10https://www.wired.com/story/ai-fueled-dungeon-game-\ngot-much-darker/\nAI. Despite many open-source AI libraries, devel-\noping AI systems and models with a certain level\nof quality still requires considerable effort, experi-\nence, and corresponding data, which are an entry\nbarrier for AI democratization. However, No/Low\nCode AI allows industrial engineers and online\nservice designers not familiar with machine learn-\ning to make a simple AI system or its prototypes\nrapidly. This contribution is a similar case to the\nsuccess of ofﬁce programs such as Microsoft of-\nﬁce. We provided our HyperCLOV A Studio for\nour platform service designers, who showed sur-\nprising results and performances using our Studio\nwith their creativity. The outputs and data gener-\nated by HyperCLOV A Studio are applied to our AI\nservices. From this result, we found the possibility\nof No/Low Code AI with our HyperCLOV A, which\nis a meaningful step to realize AI democratization.\nTherefore, we need strong efforts to alleviate the\nproblematic issues while beneﬁting from the values\nthat large-scale LMs can provide.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nPersistent anti-muslim bias in large language models.\narXiv preprint arXiv:2101.05783.\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nJihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo\nHa, and Jonghyun Choi. 2021. Rainbow memory:\nContinual learning with a memory of diverse sam-\nples. In CVPR.\nEyal Ben-David, Nadav Oved, and Roi Reichart. 2021.\nPADA: A prompt-based autoregressive approach\nfor adaptation to unseen domains. arXiv preprint\narXiv:2102.12206.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. RobBERT: a dutch roberta-based language\nmodel. In EMNLP Findings.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nIsmael Garrido-Muñoz, Arturo Montejo-Ráez, Fer-\nnando Martínez-Santiago, and L Alfonso Ureña-\nLópez. 2021. A survey on bias in deep NLP. Ap-\nplied Sciences, 11(7):3184.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level adversar-\nial reprogramming. In ACL.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In ICLR.\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In EACL.\nDong-Chan Kang, Seung-Hoon Na, Yun-Su Choi, Hye-\nWoo Lee, and Du-Seong Chang. 2020. Knowledge\ndistillation for lightweight roberta of korean. In\nKCC.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nEMNLP: System Demonstrations.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nACL.\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee.\n2019. Korquad1.0: Korean qa dataset for ma-\nchine reading comprehension. arXiv preprint\narXiv:1909.07005.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021a. What\nmakes good in-context examples for GPT- 3? arXiv\npreprint arXiv:2101.06804.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yu-\njie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Benoît\nSagot. 2020. CamemBERT: a tasty french language\nmodel. In ACL.\nNitika Mathur, Timothy Baldwin, and Trevor Cohn.\n2020. Tangled up in BLEU: Reevaluating the eval-\nuation of automatic machine translation evaluation\nmetrics. In ACL.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhoBERT: Pre-trained language models for viet-\nnamese. In EMNLP Findings.\nKyubyong Park, Joohong Lee, Seongbo Jang, and Da-\nwoon Jung. 2020. An empirical study of tokeniza-\ntion strategies for various korean nlp tasks. In\nAACL.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Jiyoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Yongsook Song, Taehwan Oh, et al.\n2021. Klue: Korean language understanding eval-\nuation. arXiv preprint arXiv:2105.09680.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\narXiv preprint arXiv:2104.10350.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nLaria Reynolds and Kyle McDonell. 2021. Prompt\nprogramming for large language models: Be-\nyond the few-shot paradigm. arXiv preprint\narXiv:2102.07350.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nfor building an open-domain chatbot. arXiv preprint\narXiv:2004.13637.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In NAACL.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. arXiv\npreprint arXiv:2104.07540.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-LM: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation re-\nduces hallucination in conversation. arXiv preprint\narXiv:2104.07567.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias? In COL-\nING, pages 6863–6870.\nTuva Lunde Smestad and Frode V olden. 2018. Chatbot\npersonalities matters. In International Conference\non Internet Science, pages 170–181.\nEric Michael Smith, Diana Gonzalez-Rico, Emily\nDinan, and Y-Lan Boureau. 2020. Control-\nling style in generated dialogue. arXiv preprint\narXiv:2009.10855.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP.\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep\nGanguli. 2021. Understanding the capabilities, lim-\nitations, and societal impact of large language mod-\nels. arXiv preprint arXiv:2102.02503.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,\nand Colin Raffel. 2020. mT5: A massively multi-\nlingual pre-trained text-to-text transformer. arXiv\npreprint arXiv:2010.11934.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woomyeong Park. 2021. GPT3Mix:\nLeveraging large-scale language models for text aug-\nmentation. arXiv preprint arXiv:2104.08826.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang,\nYi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang,\nKaisheng Wang, Xiaoda Zhang, et al. 2021. Pangu-\nα: Large-scale autoregressive pretrained chinese lan-\nguage models with auto-parallel computation. arXiv\npreprint arXiv:2104.12369.\nJianguo Zhang, Pengcheng Zou, Zhao Li, Yao Wan, Xi-\numing Pan, Yu Gong, and S Yu Philip. 2019. Multi-\nmodal generative adversarial network for short prod-\nuct title generation in mobile e-commerce. In\nNAACL.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nICML.\nA Details on Data\nA.1 Data Description\nAs shown in Table 1, 49%, 15%, and 13% of the\ncorpus come from blogs, community sites, and\nNews corpus, respectively. 7% of the corpus con-\nsists of comments from various websites mentioned\nabove. 5% of the corpus comes from KiN11, which\nis an online social QnA service similar to Quora.\nKiN corpus consists of open questions and an-\nswers written by users. Note that our corpus also\nincludes Korean Wikipedia, but the portion is very\nsmall (0.04%). We also use Wikipedia for English\nand Japanese to enhance the ability of foreign lan-\nguages. Modu-corpus12 is a collection of various\ndatasets collected by National Institute of Korean\nLanguage (NIKL). We use ﬁve datasets, including\nmessenger, news, spoken language corpus, writ-\nten language corpus, and web corpus from Modu-\ncorpus. The data ratio per language is 97%, 2%,\n0.5%, 0.5% in Korean, English, Japanese, and other\nlanguages, respectively.\nA.2 Data Cleaning\nIn a similar way to the work of Brown et al. (2020),\nwe train a logistic regression model that can mea-\nsure the quality of each document. BERT feature\nof the document is used as an input. We assume\nhigh-quality encyclopedia documents as positive\nexamples and crawled web documents as negative\nones. We exclude the documents predicted as low-\nquality. To remove duplicated documents, we cal-\nculate the similarity of the documents with a hash\nfunction. We also utilize an in-house spam ﬁltering\ntechnique to remove undesired advertisements and\ndocuments. Moreover, we exclude low-quality doc-\numents too short in length or too repetitive at levels\nof graphemes, numbers, or special characters. In\nparticular, we observe the review-type documents\noften contain too repetitive expressions because\nthere is a policy on the length of writing a review.\nAlso, if the document contains too many swear\nwords and slang, it is excluded. Within the doc-\nument, we remove duplicated sentences between\ntitle and content. In the case of KiN corpus, if multi-\nple answers are registered for one question, only the\nanswers adopted by the questioner or the answers\nfrom certiﬁed experts, such as doctors or lawyers,\nwere used. Even if the answer was adopted, it was\nexcluded if the author’s reputation score was low.\n11https://kin.naver.com/\n12https://corpus.korean.go.kr/\nMean Std Max\n137M 73.11 3.11 77.19\n350M 77.55 4.68 82.93\n760M 77.64 7.25 85.03\n1.3B 83.90 1.90 86.03\n6.9B 83.78 2.76 87.62\n13B 87.86 0.52 88.79\n39B 87.95 0.54 88.87\n82B 88.16 0.75 89.16\nTable 9: Mean, standard derivation, and max accuracy\non NSMC.\nWe parse the HTML source code and use only\nmeaningful parts of the HTML page for training the\nmodel. For news-type documents, we remove typi-\ncal parts that have insigniﬁcant information, such\nas the ﬁrst line and the last phrase for afﬁliation.\nA.3 Data Anonymization\nWe mask the personal information such as resident\nregistration number, email address, phone number,\nbank account number, credit card number, passport\nnumber, driver’s license number, etc. However, we\nremain non-critical parts of the numbers that can’t\nbe used to identify a person. For example, we ex-\ntract the age and gender from resident registration\nnumber, location information from driver’s license\nnumber, dialing code from a phone number, and\ndomain address from email.\nA.4 Data Postprocessing\nFor preprocessing, we also add preﬁx on the\ndocument like “Web, Title: ${title_name}, Body:\n${body_text}”, following the CTRL paper (Keskar\net al., 2019).\nB Scaling Law\nFigure 2 shows the training and test loss patterns\nfrom a Korean encyclopedia corpus of Hyper-\nCLOV A. The results of HyperCLOV A are consis-\ntent to the scaling law pattern of GPT-3 (Kaplan\net al., 2020; Brown et al., 2020).\nC Details on Experiments\nC.1 NSMC\nTable 9 shows the statistics on the performance of\nHyperCLOV A in NSMC.\nC.2 AI Hub Translation\nTable 10 shows the statistics on the performance of\nHyperCLOV A in AI Hub translation tasks.\nFigure 2: Scaling law (Kaplan et al., 2020; Brown et al., 2020) in training HyperCLOV A models with various\nparameters. The left ﬁgure presents the training and the right ﬁgure shows the loss on the testset of the Korean\nencyclopedia not contained in the training corpus.\nKo→En En →Ko\nMean Std Max Mean Std Max\n137M 0.80 0.06 0.87 2.78 0.35 3.09\n350M 1.44 0.06 1.52 8.89 0.37 9.24\n760M 2.63 0.10 2.75 16.89 0.83 17.80\n1.3B 3.83 0.08 3.92 20.03 0.13 20.15\n6.9B 7.09 0.20 7.29 27.93 0.58 28.42\n13B 7.91 0.33 8.19 27.82 0.25 28.10\n39B 9.19 0.26 9.49 31.04 0.78 31.93\n82B 10.37 0.22 10.53 31.83 0.70 32.63\nTable 10: Mean, standard derivation, and max accuracy\non AI Hub translation tasks.\nC.3 Query Modiﬁcation Task\nTable 11 and Table 12 show the example and the\nprompt for the query modiﬁcation task.\nC.4 Discussions on Making Persona Chatbot\nRecent chit-chat with the neural model, like Meena\nand Blender, shows impressive conversational per-\nformance (Humeau et al., 2020; Adiwardana et al.,\n2020; Roller et al., 2020). However, such a con-\nversation system uses a lot of data, and it cannot\nmake a new style of conversational system in an\ninstant. There are also plenty of researches on style\ntransfer. However, these methods do not control the\ndetailed style of the conversational system (Smith\net al., 2020).\nThere also exist some hallucination issues. Re-\ntrieved knowledge can alleviate this problem (Shus-\nter et al., 2021). A pre-trained reader can also get\nadvantages if the pre-trained LM itself also per-\nExample 1:\n사용자: 아이유노래 틀어줘\n(User: Play IU’s track)\n스피커: 노래를 재생합니다.\n(AI Speaker: I am playing the track.)\n사용자: 몇 살이야\n(User: How old?)\n사용자의최종의도: 아이유몇 살이야\n(Modiﬁed query: How old is IU?)\nExample 2:\n사용자: 비행기는누가만들었어\n(User: Who invented airplane?)\n스피커: 라이트형제요.\n(AI Speaker: Wright brothers did.)\n사용자: 동생 이름 뭐야\n(User: What is the younger’s name?.)\n사용자의최종의도: 라이트 형제동생 이름 뭐야?\n(Modiﬁed query: What is the younger one’s name of\nWright brothers?)\nTable 11: Examples of user query modiﬁed by Hyper-\nCLOV A. English sentences are translated by a human\nexpert.\nforms well for open-domain QA, as shown in T5\nand FiD in open-domain question answering (Raf-\nfel et al., 2020; Izacard and Grave, 2020).\nC.5 Zero-shot Transfer Data Augmentation\nHyperCLOV A does not always make sentences\nwhich is ﬁt to the target intent class. However,\neven when people simply ﬁll in the utterances that\nﬁt their intent, it is difﬁcult to create various pat-\nterns, and data collectors struggle to make many\nutterances because of this problem. Data collec-\ntors can easily make a corpus by selecting sentence\n[P][P][P][P][P][P][P][P][P][P]\n# 예제1\n(# Example 1)\n사용자: 아이유앨범뭐있어\n(User: What are the names of some albums of IU?)\n스피커: 아이유의대표 앨범으로는Love poem, Palette,\nCHAT-SHIRE가있어요.\n(AI Speaker: IU’s signiture albums include Love poem,\nPalette, and CHAT-SHIRE.)\n사용자: 가장신나는앨범이뭐야\n(User: Which one is the most exciting album?)\n–\n[P][P][P] 사용자의[P][P] 의도: Love poem, Palette,\nCHAT-SHIRE중가장신나는앨범이뭐야\n([P][P][P] User’s [P][P] intent: Among Love poem,\nPalette, and CHAT-SHIRE, which one is the\nmost exciting album?)\n# 예제2\n(# Example 2)\n사용자: 평창 동계올림픽은몇년에 했어?\n(User: When did the PyeongChang Olympics take place?)\n스피커: 2018년입니다.\n(AI Speaker: It is 2018.)\n사용자: 그때미국대통령이누구야\n(User: Who was the president of the United States at that\ntime?)\n–\n[P][P][P] 사용자의[P][P] 의도: 2018년미국대통령이\n누구야\n([P][P][P] User’s [P][P] intent: Who was the president of\nUS in 2018?)\n# 예제3\n(Example 3)\n사용자: 삼성전자주가얼마야\n(User: What is Samsung Electronics’ share price?)\n스피커: 8만2천원입니다.\n(AI Speaker: It is 82,000 Won.)\n사용자: LG전자는\n(User: How about LG Electronics?)\n–\n[P][P][P] 사용자의[P][P] 의도: LG전자주가얼마야\n([P][P][P] User’s [P][P] intent: What is LG Electronics’\nshare price?)\n# 예제4\n(Example 4)\nTable 12: Used prompts of query modiﬁcation task. [P]\ndenotes a token for continuous prompt.\ncandidates created by HyperCLOV A. Our corpus\ndesigner also found that generating dialect or con-\nverting standard language to dialect is also easily\navailable, showing the capability of data augmenta-\ntion with HyperCLOV A.\nNote that this experiment is zero-shot transfer\ndata augmentation, and examples of a different\nclass from target classes are used as in-context ex-\namples. We use a total of 30 examples from six\nsource classes and randomly sample three source\nclasses and corresponding 15 examples to put into\nthe prompt. For classiﬁcation, an in-house BERT-\nbased model is used.\nIn our experiment, sentences for 18 classes are\ngenerated well (like 80% ∼90%), whereas sen-\ntences for 2 classes are not generated well (like\n10% ∼20%).\nSimilar concurrent works are conducted from\nSchick and Schütze (2021). However, their study\ncan only be applicable for NLI, which is a well-\ndeﬁned task, has good datasets, and has pre-trained\nmodels for the task.\nC.6 Advertisement Design\nTable 14 and 18 show the example prompt for the\nevent title generation task. Table 17 shows a quali-\ntative comparison between mT5 and our model.\nSimilar to the event title generation task, the\nproduct designer also does the advertisement head-\nline generation task in a similar way. In this task,\nthere is no training data which could be used due\nto data privacy issue. Nevertheless, HyperCLOV A\nwith a similar style of event title generation task\nsuccessfully generates an advertisement headline.\nTable 15 shows the prompt. Three different\nprompts are used for advertisement headline gen-\neration, and the generated sentence which is most\nsimilar to the product name, which is an input of\nthe task, is selected. A similarity score is calculated\nby the cosine similarity score using a feature of\nthe in-house BERT. The product designer evaluates\nthat 99% of generated sentences are appropriate for\nthe real service.\nD Details on Studio\nFigure 3 shows the GUI interface of HyperCLOV A\nStudio. Figure 4 illustrates No Code AI paradigm\nin HyperCLOV A Studio.\nE Motivation of Tokenization\nFigure 5 shows our motivation and importance of\nmorpheme-aware tokenization. Though we used an\nin-house morpheme analyzer, an alternative open-\nsource morpheme analyzer like Mecab-ko 14 can\nalso be used.\nF Challenges of No/Low Code AI\nParadigm\nSome researchers doubt the performances of GPT-\n3 less competitive than existing ﬁnetuning-based\n14https://bitbucket.org/eunjeon/mecab-ko\nFigure 3: An example interface of HyperCLOV A Studio.\nFigure 4: No Code AI paradigm in HyperCLOV A Studio.\nLMs for various downstream tasks. For example,\ntask-speciﬁc neural structure like FiD (Izacard and\nGrave, 2020) achieves state-of-the-art open-domain\nQA, whereas GPT-3 does not. It is still under-\ndiscovered that a prompt-based method makes\nlarge-scale LMs competitive. To resolve this prob-\nlem, further discovery on general large model capa-\nbility and prompt-based optimization is required.\nThere also exists a problem with dependency on\npre-training data. If the corpus does not contain\ncode generation, it is unfair to expect the LM gen-\nerates source codes, even where a prompt-based\noptimization is applied. The maintainer of Hyper-\nCLOV A Studio may discover many requirements\nof users and further train corpus with common\nneeds. To incorporate these corpora, research on\npre-training under continual learning setup (Bang\net al., 2021) is required.\nThough we mentioned No Code AI earlier, pro-\ngramming further the functions of HyperCLOV A\nStudio still exists for the remaining part of com-\nplete AI system. Also, knowledge of ML is still\nrequired implicitly to design an effective prompt\nand few-shot examples. An easier guideline for Stu-\ndio and incentives on sharing user’s own prompts\ncan boost to spread the ecosystem.\nIn order to support a full-ﬂedged ML develop-\nment, we also need additional features for Hyper-\nCLOV A Studio - experimentation and user feed-\nback. In this function, a user can easily distribute\nPoC service by an appropriate interface, like a text\neditor or messenger, and make the user can feed-\nback on responses of HyperCLOV A. For example,\nuser can rate the response of the chatbot turn by\nFigure 5: Motivation of our morpheme-aware byte-level BPE tokenization. (Top) A conceptual example of making\nsubword from three tokenization methods. (Middle) An example of tokenization, where subword from byte-level\ntokenizer is represented as a byte. (Bottom) The same example of (middle), but subword from byte-level tokenizer\nis represented as a character.\nturn.\nExpensive inference or prompt-based optimiza-\ntion costs are still an obstacle for using large-scale\nLMs. However, there is a trade-off on costs between\ntraining many small-scale LMs and inferencing one\nlarge-scale LM. The outputs by one large-scale LM\ncan also be input to small-scale LMs (Yoo et al.,\n2021). Research on distilling generative transform-\ners or energy-efﬁcient hardware is essential for sus-\ntainability. Further discussion several issues are in\nthe Broader Impact Statement section.\n사용자인텐트에 맞는문장5개를 만드시오.\n(Create ﬁve sentences which match the user intent.)\n@ 사용자인텐트 : 포장가능문의\n(@ User intent: Inquiry on takeout)\n예시발화\n(Example utterances)\n1. 칼국수나 돈까스같은음식도포장되요?\n(1. Can I get food like Kalguksu or pork cutlet to go?)\n2. 죄송한데테이크아웃되죠?\n(2. Excuse me, can I takeout this?)\n3. 메뉴포장되나요?\n(3. Can I get this menu to go?)\n4. 아이스크림 포장해주세요\n(4. I’d like to get this ice cream to go.)\n5. 집에서도먹을수있게포장해주시나요?\n(5. Can I get this menu to go so I can eat this at home?)\n@ 사용자인텐트 : 배달음식환불\n(@ User intent: refund on delivery food)\n예시발화\n(Example utterances)\n1. 보쌈에서시큼한냄새가나는데환불부탁드립니다\n(1. Bossam smells sour, please give me a refund.)\n2. 메뉴가잘못 배달되었습니다. 환불부탁드립니다\n(2. The menu was delivered incorrectly. Please give me a)\nrefund.)\n3. 간장게장맛이이상해요. 환불가능한가요?\n(3. Soy Sauce Marinated Crab tastes weird. Can I get a\nrefund?)\n4. 치킨이너무 식어서왔어요. 환불부탁드려요\n(4. The chicken is too cold. I’d like a refund, please.)\n5. 음식에서벌레가나왔네요. 환불해주세요\n(5. There’s a bug in the food. Please give me a refund.)\n@ 사용자인텐트 : 예약 좌석문의\n(@ User intent: Inquiry on seat reservation)\n예시발화\n(Example utterances)\n1. 20명이가려는데, 자리가충분한가요?\n(1. There are 20 people going, is there enough room?)\n2. 조용히 식사하고싶은데조용한자리가있을까요?\n(2. I’d like to have a quite meal, is there a quiet table?)\n3. 유모차를 가지고들어가서아이눕혀놓고싶은데\n마땅한자리가있을까요?\n(I’d like to take the stroller in and put the child down, is\nthere a suitable seat?)\n4. 15명정도간단하게가족친척들과아기돌잔치할만한\n자리있을까요?\n(4. Is there a place for 15 people to have a ﬁrst birthday)\nparty with their family and relatives?)\n5. 분리된예약석이있을까요?\n(5. Is there a separate reserved seat?)\n@ 사용자인텐트 : {intent-name}\n(@ User intent:{intent-name})\n예시발화\n(Example utterances)\n1.\nTable 13: A prompt for zero-shot transfer data augmen-\ntation.\n태그: 댕댕이옷, 크리스마스, 따뜻한강아지옷,\n강아지코스튬\n(Tags: daengdaeng-i13 clothes, Christmas, warm puppy\nclothes, puppy costume)\n날짜: 12월23일\n(Date: December 23)\n제목: 겨울시즌댕댕이를 위해\n(Title: For puppies in the winter season)\n###\n태그: 암막커튼, 침실커튼, 방한커튼, 인테리어커튼,\n아이방커튼, 거실커튼, 방풍커튼,\n가리개커튼, 작은창커튼\n(Tags: blackout curtains, bedroom curtains, cold protection\ncurtains, interior curtains, children’s room curtains,\nliving room curtains, windproof curtains, blind\ncurtains, small window curtains)\n날짜: 11월1일\n(Date: November 1)\n제목: 찬바람 막는방한커튼\n(Title: Cold protection curtains for blocking cold winds)\n###\n태그: 명품구두, 여자들의로망, 여름구두\n(Tags: luxury shoes, women’s dreams, summer shoes)\n날짜: 7월7일\n(Date: July 7)\n제목: 보기만 해도행복한명품 슈즈\n(Title: Luxury shoes that make you happy\njust by looking at them)\n###\n태그: 유아동복, 주니어쇼핑몰, 아동원피스,\n아동맨투맨, 아동바지, 아동레깅스, 아동모자,\n아동가방,아동양말, 아동신발\n(Tags: children’s clothes, junior shopping mall, children’s\ndresses, children’s sweatshirts, children’s pants,\nchildren’s leggings, children’s hats, children’s bags,\nchildren’s socks, children’s shoes)\n날짜: 2월26일\n(Date: February 26)\n제목: 주목받는신학기코디제안\n(Title: New semester style suggestion for attracting\nattention)\n###\n태그: 스마트워치, 스마트밴드, 웨어러블디바이스\n(Tags: Smart watch, smart band, wearable device)\n날짜: 12월7일\n(Date: December 7)\n제목: 시계도스마트하게사용해봐\n(Title: Try to use your watch smartly)\n###\n태그: 커피머신, 에스프레소머신, 커피메이커\n(Tags: coffee machine, espresso machine, coffee maker)\n날짜: 4월13일\n(Date: April 13)\n제목: 커피 한잔의여유를 위한 커피머신\n(Title: Coffee machine for relaxing with a cup of coffee)\nTable 14: Prompt for product event title generation.\n상품명: 디퓨저꽃 디퓨져스틱 방향제리드스틱 머스타드\n7종\n날짜: 2021년3월29일\n카테고리: 기타아로마/캔들용품\n브랜드: 캔들날다메이릴리\n태그: 72993∧방향제만들기|64225∧디퓨저diy|189638∧\n디퓨저리드|139746∧디퓨저만들기|198335∧\n디퓨저만들기재료|379365∧인테리어디퓨저\n속성: |\n광고문구: 봄을부르는향기\n######\n상품명: LYNN 린 차이나 프릴 블라우스\n날짜: 2021년3월29일\n카테고리: 블라우스/셔츠\n브랜드: 린\n태그: |\n속성: 핏∧기본핏|패턴∧무지|디테일∧프릴/러플|총기장∧\n기본/하프|주요소재∧폴리에스테르|소매기장∧반팔\n광고문구: 여성스러운프릴 블라우스\n######\n상품명: 맥 아이섀도우1.5g\n날짜: 2021년3월29일\n카테고리: 아이섀도\n브랜드: 맥\n태그: 75984∧선물로좋은|76503∧포인트주기좋은|281615\n∧자연스러운발색|240838∧지속력좋은|235326∧\n포인트연출|665375∧파우더리|1228492∧\n부드러운사용감|836046∧자연스러운스모키|5279∧\n청순메이크업|78091∧선물포장\n속성: 형태∧압축/팩트형|세부제품특징∧고운입자|\n세부제품특징∧은은함|세부제품특징∧웜톤용|색상∧\n골드|주요제품특징∧고발색|색상∧핑크|세부제품특징∧\n눈매연출|세부제품특징∧펄있음|주요제품특징∧\n부드러운발림|색상∧브라운|타입∧싱글|주요제품특징∧\n지속력\n광고문구: 매트한질감과선명한발색\n######\n상품명: 케이스아쿠아텍스이지클린 패브릭 원단저상형\n패밀리 침대SS,Q\n날짜: 2021년05월17일\n카테고리: 패밀리침대\n브랜드: ss퍼니처\n태그: 사이즈∧슈퍼싱글+퀸|부가기능∧안전가드포함|\n프레임∧저상형|자재등급∧E0(친환경)|\n부가기능∧유해물질차단|프레임소재∧패브릭\n속성: 5554855641\n광고문구: 안전한 소재로 제작된 저상형 패밀리 침대\nTable 15: Prompt for advertisement headline design.\nProduct name: Diffuser ﬂower diffuser stick air\nfreshener reed stick mustard 7 kinds\nDate: March 29, 2021\nCategory: other aroma/candle supplies\nBrand: Candlenalda maylily\nTag: 72993∧making air freshener|64225∧diffuser diy|\n189638∧diffuser reed|139746∧making diffuser|\n198335∧diffuser making material|\n379365∧interior diffuser\nAttribute: |\nAds. headline: The scent of calling spring\n######\nProduct name: LYNN lynn china frill blouse\nDate: March 29, 2021\nCategory: blouse/shirt\nBrand: Lynn\nTag: |\nAttribute: ﬁt∧basic ﬁt|pattern∧plain|details∧frill/rufﬂe|\nlegnth∧basic/half|material∧Polyester|sleeve∧short\nAds. headline: Feminine frilled blouse\n######\nProduct name: Mac eye shadow 1.5g\nDate: March 29, 2021\nCategory: eye shadow\nBrand: Mac\nTag: 75984∧good for gifts|76503∧good for points|\n281615∧natural color|240838∧long lasting|\n235326∧point makeup|665375∧foundary|\n1228492∧soft feeling|836046∧natural smokey|\n5279∧pure makeup|78091∧gift wrapping\nAttribute: form∧compressed/fact-type|detailed features\n∧ﬁne particles|detailed features∧subtlety|\ndetailed features∧warm tone|color∧gold|\nmain features∧high color|color∧pink|detailed\nfeatures∧eye makeup|detailed features∧pearl|\nmain features∧soft application|color∧brown|\ntype∧single|main features∧long lasting\nAds. headline: Matte texture and vivid color\n######\nProduct name: Case aquatex easy-clean fabric\nlow-rise family bed SS,Q\nDate: May 17, 2021\nCategory: family bed\nBrand: ssfurniture\nTag: size∧super single+queen|Additional function\n∧include a safe guard|frame∧low-rise|\nmaterial grade∧E0(eco)|additional function∧\nblock harmful substances|frame material∧fabric\nAttribute: 5554855641\nAds. headline: Low-rise family bed made of safe materials\nTable 16: English translation for Table 15.\nModels Product event titles\nmT5 봄맞이인테리어 발매트 모음전\n(Interior foot-mat event for spring season.)\nHyperCLOV A 욕실분위기를 바꿔줄아이템\n(Items that can change bathroom mood.)\nmT5 타이니러브바운서\n(Tiny love bouncer.)\nHyperCLOV A 엄마와아기를 위한편안함\n(Comfort for mommy and baby.)\nmT5 한끼 요리 탕요리 반조리\n(A meal, stew, semi-cooked.)\nHyperCLOV A 저녁걱정뚝! 간편한탕요리\n(No worry on dinner!\nSimple semi-cooked stew.)\nmT5 가을맞이면접룩기획전\n(Interview fashion event for fall season.)\nHyperCLOV A 면접때입을옷고민하지마세요\n(No worry on your fashion\nfor the interview.)\nTable 17: Examples of product event titles generated by\nmT5 and HyperCLOV A. English phrases in parenthesis\nare translated by human experts for preserving their nu-\nances and semantics.\n키워드: 캔디주얼리, 프로포즈목걸이, 커플링, 은반지,\n다이아가드링, 로즈골드목걸이, 하트귀걸이,\n하트목걸이\n(Keywords: candy jewelry, proposal necklace, coupling,\nsilver ring, diamond guard ring, rose gold necklace,\nheart earring, heart necklace\n날짜: 2021년3월7일\n(Date: March 7, 2021)\n제목: 화이트데이커플주얼리 세일\n(Title: White Day Couple Jewelry Sale)\n키워드: 수입그릇, 빈티지그릇, 법랑냄비, 수저세트,\n튼튼한컵, 레트로냄비\n(Keywords: imported bowl, vintage bowl, enamel pot,\nspoon and chopsticks set, strong cup, retro pot\n날짜: 2020년4월21일\n(Date: April 21, 2020)\n제목: 주방용품 해외직구할인전\n(Title: Kitchenware overseas direct purchase\ndiscount exhibition)\n키워드: 미세먼지, 차량용핸드폰거치대, 세차용품,\n자동차용품, 차량용품, 차량무선충전거치대,\n차량악세사리, 논슬립패드, 자동차악세사리\n(Keywords: ﬁne dust, mobile phone holder for vehicles,\ncar washing products, automobile supplies, vehicle\nsupplies, vehicle wireless charging cradle\nvehicle accessories, non-slip pads, car accessories)\n날짜: 2021년4월1일\n(Date: April 1, 2021)\n제목: 각종차량용품 할인모음전\n(Title: Collection of discounts on various vehicle supplies)\n키워드: 슬리퍼, 실내용슬리퍼, 사무용슬리퍼, 하이힐,\n봄신상신발, 봄신발, 여자슬리퍼, 여성슬리퍼,\n여성하이힐, 여자하이힐\n(Keywords: slippers, indoor slippers, ofﬁce slippers, high\nheels, spring new arrival shoes, spring shoes, women’s\nslippers, female slippers, women’s high heels,\nfemale high heels)\n날짜: 2021년3월1일\n(Date: March 1, 2021)\n제목: 봄여성사무용슬리퍼하이힐 SALE\n(Title: Spring women’s ofﬁce slippers high heels SALE)\n키워드: 봄신상, 명품악세사리, 링귀걸이, 꽃머리끈,\n명품키링, 머리끈, 악세사리\n(Keywords: spring new arrival, luxury accessories, ring\nearrings, ﬂower headbands, luxury key ring, headband,\naccessories)\n날짜: 2020년8월8일\n(Date: August 8, 2020)\n제목: 악세서리 인기제품 할인전\n(Title: Accessory popular products discount exhibition)\nTable 18: Controlling style by change in-context exam-\nples for product event title generation.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7534434199333191
    },
    {
      "name": "Transformer",
      "score": 0.6594188809394836
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5573604106903076
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5196667909622192
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4758955240249634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4602166414260864
    },
    {
      "name": "Generative grammar",
      "score": 0.4511869549751282
    },
    {
      "name": "Inference",
      "score": 0.42684459686279297
    },
    {
      "name": "Language model",
      "score": 0.42110446095466614
    },
    {
      "name": "Programming language",
      "score": 0.22723448276519775
    },
    {
      "name": "Engineering",
      "score": 0.1526312232017517
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}