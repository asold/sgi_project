{
  "title": "Towards artificial general intelligence via a multimodal foundation model",
  "url": "https://openalex.org/W4281643269",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2904126237",
      "name": "Nanyi Fei",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2134475493",
      "name": "Zhiwu Lu",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2104839185",
      "name": "Yizhao Gao",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2146282143",
      "name": "Guoxing Yang",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2152485585",
      "name": "Yuqi Huo",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2097020016",
      "name": "Jingyuan Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2096803518",
      "name": "Haoyu Lu",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2163477531",
      "name": "Ruihua Song",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2103988188",
      "name": "Xin Gao",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1960361089",
      "name": "Tao Xiang",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2070768212",
      "name": "Hao Sun",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2904126237",
      "name": "Nanyi Fei",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2134475493",
      "name": "Zhiwu Lu",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2104839185",
      "name": "Yizhao Gao",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2146282143",
      "name": "Guoxing Yang",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2152485585",
      "name": "Yuqi Huo",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2097020016",
      "name": "Jingyuan Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2096803518",
      "name": "Haoyu Lu",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2163477531",
      "name": "Ruihua Song",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2103988188",
      "name": "Xin Gao",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1960361089",
      "name": "Tao Xiang",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2070768212",
      "name": "Hao Sun",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017561954",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2624614404",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2101295242",
    "https://openalex.org/W2123046721",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W6600617704",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2981586349",
    "https://openalex.org/W3035454331",
    "https://openalex.org/W3118694826",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W2998388430",
    "https://openalex.org/W3205803342",
    "https://openalex.org/W6796487566",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4229494842",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1980038761",
    "https://openalex.org/W2515866431",
    "https://openalex.org/W2999659281",
    "https://openalex.org/W2607508964",
    "https://openalex.org/W2134873765",
    "https://openalex.org/W3021636956",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W3177934633",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2978223337",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035588244",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W3105577662"
  ],
  "abstract": null,
  "full_text": "ARTICLE\nTowards artiﬁcial general intelligence via a\nmultimodal foundation model\nNanyi Fei1,2,3, Zhiwu Lu 1,2✉, Yizhao Gao1,2, Guoxing Yang1,2, Yuqi Huo2,3, Jingyuan Wen1,2, Haoyu Lu1,2,\nRuihua Song1,2, Xin Gao 4, Tao Xiang5, Hao Sun 1,2✉ & Ji-Rong Wen 1,2,3✉\nThe fundamental goal of artiﬁcial intelligence (AI) is to mimic the core cognitive activities of\nhuman. Despite tremendous success in the AI research, most of existing methods have only\nsingle-cognitive ability. To overcome this limitation and take a solid step towards artiﬁcial\ngeneral intelligence (AGI), we develop a foundation model pre-trained with huge multimodal\ndata, which can be quickly adapted for various downstream cognitive tasks. To achieve this\ngoal, we propose to pre-train our foundation model by self-supervised learning with weak\nsemantic correlation data crawled from the Internet and show that promising results can be\nobtained on a wide range of downstream tasks. Particularly, with the developed model-\ninterpretability tools, we demonstrate that strong imagination ability is now possessed by our\nfoundation model. We believe that our work makes a transformative stride towards AGI, from\nour common practice of“weak or narrow AI” to that of“strong or generalized AI”.\nhttps://doi.org/10.1038/s41467-022-30761-2 OPEN\n1 Gaoling School of Artiﬁcial Intelligence, Renmin University of China, Beijing, China.2 Beijing Key Laboratory of Big Data Management and Analysis Methods,\nBeijing, China. 3 School of Information, Renmin University of China, Beijing, China.4 Computer, Electrical and Mathematical Sciences and Engineering\nDivision, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.5 Department of Electrical and Electronic Engineering, University of\nSurrey, Guildford, UK. ✉email: luzhiwu@ruc.edu.cn; haosun@ruc.edu.cn; jrwen@ruc.edu.cn\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 1\n1234567890():,;\nS\ncience ﬁctions and sci-ﬁﬁ lms, that describe highly intelli-\ngent computer minds, robots, or even human-shaped ones,\ncan be said to understand or have primitive cognitive\nabilities analogous to human intelligence. Since this form of\nhuman-level artiﬁcial intelligence (AI) is too far from reality\nhitherto, researchers change to set a less ambitious goal of\nachieving artiﬁcial general intelligence (AGI). Despite not being\nprecisely deﬁned, AGI is broadly agreed to have several key\nfeatures\n1 including: (1) matching or exceeding human perfor-\nmance across a broad class of cognitive tasks (e.g., perception,\nreading comprehension, and reasoning) in a variety of contexts\nand environments; (2) possessing the ability to handle problems\nquite different from those anticipated by its creators; and (3)\nbeing able to generalize/transfer the learned knowledge from one\ncontext to others. As we can imagine, devising and obtaining an\nAGI system would not only accelerate the AI research itself, but\nalso beneﬁt a wide range of AI+ ﬁelds including neuroscience,\nhealthcare, and biomedicine.\nIn recent years, deep learning2 has achieved tremendous suc-\ncesses in various AI research areas such as computer vision (CV)\nand natural language processing (NLP). For example, deep resi-\ndual networks (ResNets)3 have already surpassed human per-\nformance on image classiﬁcation. The language model RoBERTa4\nhas also outperformed human on several natural language\nunderstanding tasks of the GLUE benchmark 5. Relation\nnetworks6 devised by DeepMind have achieved super-human\nperformance on a relational reasoning dataset. However, most of\nexisting AI advances only focus on approaching or exceeding\nhuman intelligence on single cognitive ability (e.g., image classi-\nﬁcation, language understanding, or relational reasoning). To\novercome such a limitation and take a solid step to AGI, we\ndevelop a foundation model pre-trained with huge multimodal\n(visual and textual) data such that it can be quickly adapted for a\nbroad class of downstream cognitive tasks.\nOur motivations are two-fold: (1) Foundation models7 (also\nwell-known as pre-trained models) are established because they\nare exactly designed to be adapted (e.g., ﬁnetuned) to various\ndownstream cognitive tasks by pre-training on broad data at\nscale. Importantly, foundation models are closely related to two\nbreakthroughs of MIT Technology Review’s “10 Breakthrough\nTechnologies 2021”8: GPT-39 (a pre-trained language model) and\nmulti-skilled AI. (2) Our choice of learning from huge multi-\nmodal data is inspired by the fact that most human intelligent\nbehaviors are exhibited in a multimodal context using visual-\ntextual content as the primary carrier of knowledge and means of\ncommunication (see Fig. 1a). Indeed, researchers have reported\nthat a subset of neurons in the human medial temporal lobe can\nbe selectively activated by representations of a speciﬁc object/\nscene across different sensory modalities (e.g., pictures, written\nnames, and spoken names)10,11. Although the mechanism of\ncross-modal alignment in our brain is unknown, this still suggests\nthat human brain neurons are able to process multimodal\ninformation and encode concepts into invariant representations.\nOverall, we believe that pre-training a large-scale multimodal\nfoundation model is indeed a potential approach to achieving\nAGI.\nMultimodal (visual and textual) foundation models12,13 typi-\ncally take image-text pairs as input and model the correlation\nbetween two different modalities in their pre-training data.\nAlthough existing multimodal foundation models have shown\npromising results on fast learning/transfer and cross-modal\nunderstanding tasks, the majority of them 12,14–18 make the\nassumption of strong semantic correlation over the input image-\ntext pairs (e.g., image-caption pairs) and expect exact matches\nbetween the objects/regions in an image and the words in a piece\nof text (see Fig. 1b). This seriously limits these models ’\ngeneralization abilities because the strong semantic correlation\nassumption is often invalid in the real world and multimodal data\nfollowing this assumption is limited (e.g., only millions of image-\ncaption pairs are collected by years of human annotation). This\nsituation becomes worse when latest multimodal foundation\nmodels\n12,17,19–21 often employ object detectors to obtain mean-\ningful image regions and adopt a single-tower network archi-\ntecture for better modeling the ﬁne-grained region-word\nmatching (i.e., taking the concatenation of image regions and text\nwords as input). These two common practices (i.e., object\ndetectors and the single-tower architecture) are both computa-\ntionally costly and thus unsuited for real-world applications.\nParticularly, as for the latter, given a query in cross-modal\nretrieval (text-to-image or image-to-text), all possible query-\ncandidate pairs need to be fed into the model to compute\nmatching scores, resulting in large latency in retrieval.\nTo address the above issues, we develop a large-scale multi-\nmodal foundation model dubbed Bridging-Vision-and-Language\n(BriVL) by self-supervised learning22–25 from huge multimodal\ndata. Firstly, to build our pre-training data collection, we choose\nto exploit weak semantic correlation data (see Fig.1b) available\non the Internet without any need of human annotation (i.e., we\ncrawl a total of 650 million image-text pairs from the web).\nImportantly, such huge weak semantic correlation data contains\ncomplicated/abstract human emotions and thoughts. Therefore,\ncomparing to modeling strong semantic correlation data by direct\nimage-to-text “translation” in previous works12,17,19–21, modeling\nweak semantic correlation data by image-text matching would\nhelp us obtain a more cognitive model. Secondly, to design our\nnetwork architecture, since there do not necessarily existﬁne-\ngrained region-word matches between image and text modalities,\nwe drop the time-consuming object detectors and adopt a simple\ntwo-tower architecture (instead of the single-tower one), which\nencodes image and text inputs using two separate encoders (see\nFig. 1a). Note that the two-tower architecture has a clear\nadvantage in efﬁciency during inference, as the embeddings of\ncandidates can be computed and indexed before querying,\nmeeting the latency requirement of real-world applications.\nThirdly, with the advancement of large-scale distributed training\ntechniques\n26,27 and self-supervised learning22–25, learning from\nhuge unannotated multimodal data becomes possible. Speciﬁcally,\nto model the weak image-text correlation and learn a uniﬁed\nsemantic space where global-level image/text embeddings are\naligned, we devise a cross-modal contrastive learning (CL) algo-\nrithm, where CL is a special form of self-supervised learning that\nis initially developed in single-modality (i.e., images)28–31 with\nthe learning objective of keeping the positive samples close and\npushing away the negative ones. The proposed network and\nalgorithm designs are detailed in Methods.\nAlthough OpenAI CLIP 13 and Google ALIGN 32 are most\nclosely related to our BriVL, there exist two main differences\nbetween BriVL and these two latest models: (1) We follow the\nweak semantic correlation assumption and construct a huge\ndataset crawled from the Internet, and only pornographic/sensi-\ntive data areﬁltered out in our collected dataset. In contrast, CLIP\nonly keeps image-text pairs with high word frequency (i.e., the\nlong-tail concepts are discarded), while ALIGN alsoﬁlters its pre-\ntraining dataset by some rules (e.g., excluding texts shared by\nmore than 10 images, excluding texts with extremely low word\nfrequency, and excluding texts that are too long or too short). Our\ndataset thus preserves a data distribution closer to that of the real\nworld. (2) Inspired by the single-modal contrastive learning (CL)\nalgorithm MoCo 29, our BriVL model adopts a momentum\nmechanism to dynamically maintain queues of negative samples\nacross different training batches. In this way, we have a large\nnegative sample size (crucial for CL) while using a relatively small\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2\n2 NATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications\nbatch size (to reduce GPU memory footprint). On the contrary,\nboth CLIP and ALIGN use negative samples within each training\nbatch, requiring a large batch size (i.e., a great demand for GPU\nmemories/resources). More technical differences can be found in\nMethods.\nWe conduct extensive experiments on various downstream\ncognitive tasks (e.g., news classiﬁcation in single-modal and visual\nquestion answering in cross-modal) and show that our founda-\ntion model BriVL achieves promising results, demonstrating its\ncross-modal understanding ability and cross-domain learning/\ntransfer ability. Although our BriVL is only pre-trained with an\nimage-text matching learning objective, its strong generalization\nability has already satisﬁed some of the key features that an AGI\nsystem should have. Importantly, with a couple of model-\ninterpretability tools developed in this work, we manage to\nvisually reveal how a multimodal foundation model reasonably\nand logically imagines when words or sentences are told, showing\nthat our BriVL exhibits strong imagination ability. A closer\nexamination reveals that the possession of strong imagination is\nmainly due to the fact that our BriVL leverages weak semantic\ncorrelation data in large-scale multimodal pre-training. Overall,\nthese ﬁndings indicate that pre-training a multimodal (visual and\ntextual) foundation model can make a giant stride towards AGI.\nWith more sensory modalities exploited for multimodal pre-\ntraining and further exploration on more advancing foundation\nmodels, we believe that we are approaching AGI and our work\nwill have a broad impact on a variety of AI+ ﬁelds including\nneuroscience, healthcare, and biomedicine.\nResults\nOur BriVL model has been pre-trained based on a huge weak\nsemantic correlation dataset collected from public web sources.\nThe resulting model possesses excellent imagination ability, evi-\ndenced by Neural Network Visualization, Text-to-Image Gen-\neration and multiple downstream tasks (i.e., remote sensing scene\nclassiﬁcation, news classiﬁcation, cross-modal retrieval, and visual\nquestion answering), which are discussed in detail in this section.\nPre-training data collection. We construct a huge web-crawled\nmulti-source image-text dataset called weak semantic correlation\ndataset (WSCD) as our pre-training data collection. WSCD\ncollects Chinese image-text pairs from multiple sources on the\nweb, including news, encyclopedia, and social media. Concretely,\nimages from these data sources, together with their correspond-\ning/surrounding text descriptions, are used to form image-text\npairs. Since the obtained image-text pairs are crawled from the\nweb, the image and the text of each pair are expected to be weakly\ncorrelated. For example, an image from social media that contains\npeople having a good time with friends tends to have a simple\ntitle of“What a nice day!”, without anyﬁner-grained description\nof the image content. Note that we onlyﬁlter out the porno-\ngraphic/sensitive data from WSCD, without any form of editing/\nmodiﬁcation to the raw data to preserve the natural data dis-\ntribution. Totally, WSCD has around 650 million image-text pairs\ncovering a wide range of topics such as sports, lifestyle, and movie\nposters. Since WSCD is based on Chinese, English texts of all\nexperiments in this section are translated into Chinese for our\nBriVL. Furthermore, we pre-train our BriVL on an English\ndataset and show results on English tasks in Supplementary Note\nFig. S3, indicating that our foundation model also provides a\nfeasible solution closer to AGI beyond speciﬁc languages.\nNeural network visualization. Humans have the ability (or even\ninstinct) that scenes, e.g., in the context of images, come into our\nminds when we hear words or descriptive sentences. As for our\nBriVL, once pre-trained on such a vast amount of loosely aligned\nimage-text pairs (see Methods), we are fascinated by what exactly\nit would imagine when texts are given. Instead of examining it\nindirectly through downstream tasks, we extend Feature Visua-\nlization (FeaVis)33 to see the visual responses (i.e., imagination)\nof BriVL to semantic inputs directly. FeaVis is an algorithm\ndesigned only to visualize the features of convolutional neural\nnetworks (CNNs). However, given a large-scale cross-modal\nfoundation model like our BriVL, we can visualize any text input\nby using the joint image-text embedding space as the bridge.\nConcretely, we ﬁrst input a piece of text and obtain its text\nembedding through the text encoder of BriVL. Next, we ran-\ndomly initialize a noisy image and also get an image embedding\nthrough the image encoder. Since the input image is randomly\ninitialized, its embedding does not match that of the input text.\nWe thus deﬁne the objective of matching the two embeddings\nand back-propagate the resultant gradients to update the input\nimage. Note that we do not use any additional module or data for\nFig. 1 Overarching concept of our BriVL model with weak training data assumption. aComparison between the human brain and our multimodal\nfoundation model BriVL (Bridging-Vision-and-Language) for coping with both vision and language information.b Comparison between modeling weak\nsemantic correlation data and modeling strong semantic correlation data.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2 ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 3\nvisualization, while the pre-trained BriVL is frozen during the\nwhole process. The ﬁnally obtained image thus depicts a clear\npicture of what BriVL imagines about the input text. The visua-\nlizations of different semantic inputs are shown in Fig.2. Note\nthat the input texts are originally in Chinese and translated into\nEnglish for illustration purpose.\nWe ﬁrst present the imagination ability of BriVL to high-level\nconcepts in Fig. 2a. It can be seen that, even though these\nconcepts are rather abstract, the visualizations are able to show\nconcrete embodiment of these concepts (e.g.,“nature”: plants like\ngrass; “time”: a clock;“science”: a face with glasses and a conical\nﬂask; “dream”: cloud, a bridge leading to a door, and the dream-\nlike atmosphere). This ability to generalize an abstract concept to\na series of more concrete objects is a sign of learned common\nsense and an indication of the effectiveness of our multimodal\npre-training using only weak semantic correlation data (which\nexpose the model with abstract concepts).\nIn Fig. 2b, we show the imagination of sentences. The\nvisualization of “Every cloud has a silver lining. ” not only\nembodies the sunlight behind dark clouds literally, but also seems\nto show a dangerous situation on the sea (the ship-like object and\nthe waves on the left), expressing the implicit meaning of this\nsentence. In the visualization of“Let life be beautiful like summer\nﬂowers.”, we can see aﬂower shrub. The next two text inputs\ndescribing more complicated scenes are both from ancient\nChinese poems written with completely different grammar from\nmost other texts in the dataset. It seems that BriVL also\nunderstands them well: for“A few peachﬂowers start to blossom\noutside the bamboo grove.”, there are bamboos and pinkﬂowers;\nfor “The sun goes down below the mountains, and the Yellow\nRiver ﬂows into the sea.”, we can see mountains with trees hiding\nthe sunset, and a small boat on the river. Overall, weﬁnd that\nBriVL possesses strong capability of imagination given a\ncomplicated sentence as prompt.\nIn Fig.2c, a few similar text inputs containing a shared prompt\nare used for network visualization. For“mountains with forests”,\nthere is more green area in the image; for “mountains with\nstones”, the image is more rocky; for“mountains with snow”, the\nground turns into white/blue around the trees in the center; for\n“mountains with waterfall”, we can see blue water falling down\nwith even vapor visible. These imagination results indicate that\nour model is capable of linking speciﬁc objects with more general\nvisual context.\nWe also present the neuron visualization results with semantic\nconstraints in Fig.2d. Concretely, in addition to the image-text\nmatching loss described above, we select neurons (i.e., channels)\nin the feature map of the last layer before the pooling layer (LLP,\nshort for “Last Layer before Pooling”) in our image encoder and\nmaximize the value of each neuron. Since each text input may\ncontain many semantic contents, we can see what it is equivalent\nto activating one neuron under certain semantic constraint. Three\nneurons LLP-108, LLP-456, and LLP-678 (the number means the\nposition of each channel in the feature map) are selected for\nneuron visualization. The two columns in Fig. 2d show the\nvisualizations with text inputs “forest” and “mountains”,\nrespectively. We can clearly see that even with the same semantic\nconstraint, activating different neurons leads to different\nimagination results, indicating that each text input has rich\nsemantics with different aspects being captured by different\nneurons.\nFig. 2 Network and neuron visualizations of our BriVL’s imagination. aVisualizations of theﬁnal embedding layer of BriVL w.r.t. high-level concepts.\nb Visualizations of theﬁnal embedding layer of BriVL w.r.t. free-form text inputs.c Visualizations of theﬁnal embedding layer of BriVL with semantic\nrestrictions related to“mountains with”. d Visualizations for different neurons of BriVL with semantic restrictions“forest” and “mountains”.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2\n4 NATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications\nText-to-image generation. Network/neuron visualizations of the\nimagination are straightforward but sometimes can be hard to\ninterpret. Here, another visualization/interpretability method is\ndeveloped to make the imagined visual contents of our BriVL\nbetter understood by us human. Speciﬁcally, we utilize VQGAN34\nto generate images under the guidance of our BriVL and contrast\nthem with those generated with CLIP13. A VQGAN pre-trained\non the ILSVRC-201235 dataset is excellent in generating photo-\nrealistic images given a sequence of tokens. Each of such token is\na vector from the pre-trained token set (i.e., codebook) of\nVQGAN. We ﬁrst randomly sample a sequence of tokens, and\nobtain a generated image from the pre-trained VQGAN. Next, we\ninput the generated image into the image encoder of CLIP/BriVL\nand also input a piece of text into the text encoder. Finally, we\ndeﬁne the objective of matching the image and text embeddings,\nand back-propagate the resultant gradients to update the initial\ntoken sequence. Like network/neuron visualization, both\nVQGAN and CLIP/BriVL are frozen during the generation\nprocess. The generated examples are presented in Fig.3.\nIn Fig. 3a, b, we select four text inputs and show the results\nobtained by CLIP and our BriVL, respectively. CLIP and BriVL\nboth understand the texts well; however, we also observe two\nmajor differences. Firstly, cartoon-styled elements tend to appear\nin the generated images of CLIP, while images generated by our\nBriVL are more real and natural. Secondly, CLIP tends to simply\nput elements together while BriVL-generated images are more\ncoherent globally. The ﬁrst difference may be due to the\ndifferences in the training data used by CLIP and BriVL. The\nimages in our training data are crawled from the Internet (most\nare real photos), while there may be a fair amount of cartoon\nimages in the training data of CLIP. The second difference lies in\nthe fact that CLIP uses image-text pairs with strong semantic\ncorrelation (by word ﬁltering) while we use weakly correlated\ndata. This means that during multimodal pre-training, CLIP is\nmore likely to learn the correspondence between objects (in\nimages) and words (in texts) while BriVL is trying to understand\neach image with the given text as a whole.\nIn Fig. 3c, we consider a signiﬁcantly more challenging task\nwhere a series of images should be generated according to\nmultiple coherent sentences. Although each image in Fig.3ci s\ngenerated independently, we can observe that all four generated\nimages are visually coherent and of the same style. Thisﬁnding\nFig. 3 Text-to-image generation examples of clearer imagination. aGeneration examples of VQGAN inversion with CLIP (w/ ResNet-50x4).\nb Generation examples of VQGAN inversion with our BriVL.c A series of generation examples by VQGAN inversion with our BriVL.d More generation\nexamples by VQGAN inversion with our BriVL, where concepts/scenes are rarely seen by us humans (e.g.,“blazing sea” and “glowing forest”) or even do\nnot exist in real life (e.g.,“cyberpunk-styled city” and “castle in the clouds”). Note that VQGAN is pre-trained on ILSVRC-2012. BriVL, CLIP and VQGAN are\nall frozen during text-to-image generation.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2 ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 5\ndemonstrates another advantage of our BriVL model: although\nthe environment and background in an image are hard to\nexplicitly mention in the associated text, they are not neglected in\nour large-scale multimodal pre-training.\nWe present more text-to-image generation examples obtained\nby VQGAN inversion with our BriVL in Fig.3d. Speciﬁcally, we\nchoose concepts/scenes rarely seen by us humans (e.g.,“blazing\nsea” and “glowing forest”) or even those not existing in real life\n(e.g., “cyberpunk-styled city” and “castle in the clouds”). Weﬁnd\nthat our proposed model can generate images quite consistent\nwith our imagination about the input concepts/scenes, indicating\nits strong generalization/imagination ability. This also provides\nevidence that the superior performance of BriVL is not due to\noverﬁtting the pre-training data since the text inputs here\ncorrespond to concepts/scenes that even do not exist in real life.\nIn addition, these generation examples again demonstrate the\nadvantage of pre-training BriVL on weak semantic correlation\ndata (otherwise the ﬁne-grained region-word matching would\nharm the imagination ability of BriVL).\nRemote sensing scene classiﬁcation. To show the cross-domain\nknowledge transfer ability and the out-of-domain imagination\nability of our pre-trained BriVL, we conduct zero-shot experi-\nments on two remote sensing scene classiﬁcation benchmarks.\nThe ﬁrst dataset is UC Merced Land-Use (UCM)36, which has 21\nclasses and 100 images for each class. The size of each image in\nUCM is 256 × 256. The second dataset is AID37, which has 30\nclasses and 10,000 images in total. The size of each image in AID\nis 600 × 600. AID is a multi-source dataset, which makes it more\nchallenging for scene classiﬁcation than the single-source UCM.\nConcretely, images of each class in AID are extracted from dif-\nferent countries and regions around the world, and also at dif-\nferent times and seasons of the year under different imaging\nconditions. This leads to larger intra-class data diversity in AID.\nFor each dataset, weﬁrst obtain class embeddings by inputting\nclass names into the text encoder of CLIP/BriVL. Then for each\ntest image, we obtain its image embedding via the image encoder\nof CLIP/BriVL, and compute its cosine similarity with each class\nembedding to predict the class that it belongs to. Note that since\nthe class names of these two datasets are all English, we need to\ntranslate them into Chinese toﬁt our BriVL (but the original class\nnames are directly used for CLIP).\nIn the ﬁeld of zero-shot learning (ZSL)38, datasets typically\nfollow the split of unseen and seen classes. Conventional ZSL\nmodels are thus trained with seen class data and evaluated on\nunseen class data. Although we do not need to train on seen\nclasses, we still follow the common practice and split each dataset\nwith different unseen/seen class ratios (the seen classes are simply\nnot used). Under the split settings where the number of seen\nclasses are not zero, we randomly sample 25 splits and report the\nstandard deviations in brackets along with average accuracy.\nThe zero-shot classiﬁcation results on UCM are shown in\nthe table of Fig.4a. Our BriVL is compared to a strong baseline\nZSSC39 specially designed for zero-shot remote sensing scene\nclassiﬁcation, and also CLIP with different CNN backbones.\nWe can see that large-scale cross-modal foundation models\nachieve far higher rates compared with ZSSC, indicating their\nstrong cross-domain knowledge transfer abilities. Moreover, our\nclassiﬁcation rates are also higher than those of all CLIP models\nwith different CNNs, which is impressive considering the loss in\nEnglish-to-Chinese translation and also cultural differences (CLIP\nis trained on English data while we use data crawled from Chinese\nInternet). Results on another dataset AID are shown in the table\nof Fig. 4b. Since we did not ﬁnd methods conducting ZSL\nexperiments on AID, we only make comparisons with CLIP\nvariations. As we have mentioned, AID is more challenging than\nUCM, which is also reﬂected by the much worse performance of\nCLIP variations on AID than on UCM. However, our BriVL\nachieves similar performance on the two datasets when evaluated\nover all data, and the gap between BriVL and CLIP is larger on\nAID than that on UCM. This means that BriVL has stronger\ngeneralization ability and can cope with more complicated\nsituations.\nFig. 4 Zero-shot remote sensing scene classiﬁcation results. aZero-shot accuracies (%) on UCM with different unseen/seen class ratios.b Zero-shot\naccuracies (%) on AID with different unseen/seen class ratios.c Visualizations of “baseball ﬁeld”. For (a) and (b), we report standard deviations in\nbrackets over 25 random splits. Highest results in (a) and (b) are highlighted in bold.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2\n6 NATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications\nFurthermore, we deploy the aforementioned network visuali-\nzation technique to clarify the visual responses of our BriVL to\nremote sensing related concepts. Concretely, we select one class\n“baseball ﬁeld”, and add the prompt“viewed from above” to the\nclass name as the text input. The imagined visual content of our\nBriVL is shown in Fig.4c along with one example of this class.\nWe can see that remote sensing scenes are very different from\ntraditional photos, mainly in the perspective of cameras. Despite\nthis, we can observe from BriVL’s imagination that there is a\nsmall sector-shaped area (marked with red lines) in“baseball ﬁeld\nviewed from above”. This provides direct explanation to the\nimpressive performance of our BriVL on remote sensing scene\nclassiﬁcation. In addition, we search the keyword“baseball ﬁeld”\nin our pre-training dataset WSCD and ﬁnd that most of the\nrelated images are taken in a normal camera perspective. Given\nthat there is hardly any remote sensing data in our WSCD, this\nﬁnding suggests that BriVL has somehow learned to generalize\ntransformation of perspectives to unseen domains during pre-\ntraining. This again shows the strong imagination ability and\neven hints of common sense reasoning ability of our BriVL.\nNews classiﬁcation. To demonstrate how large-scale multimodal\nlearning can beneﬁt single-modal skills and also improve the\nimagination ability on single-modal tasks, we conduct zero-shot\nexperiments on two Chinese news classiﬁcation datasets. Theﬁrst\ndataset is Toutiao News40, which has 15 classes and a total of\naround 380K samples. The second dataset is THUCNews 41,\nwhich has 14 classes and around 840K samples in total. Since the\ncontents in these two datasets are all texts, we only need the text\nencoder of our BriVL. Concretely, we ﬁrst obtain class\nembeddings by inputting class names into the text encoder.\nFurther, for each piece of news, we only use its title to obtain its\nembedding via the text encoder. Finally, we compute the cosine\nsimilarities between each title embedding and class embeddings to\nmake predictions.\nThe following methods are chosen for comparison: (1)\nRoBERTa-base:42 it is an off-the-shelf Chinese language model\npre-trained by the original authors on a large Chinese dataset\nwith a total of 5.4B words. (2) RoBERTa-base (ﬁnetune): we\nﬁnetune the pre-trained RoBERTa-base on a subset of our WSCD\ndataset (i.e., only the text data of 22M image-text pairs is used).\n(3) BriVL w/ RoBERTa-base: it is a small version of our standard\nBriVL as we reduce the CNN from Ef ﬁcientNet-B743 to\nEfﬁcientNet-B5 and also the text backbone from RoBERTa-\nlarge to RoBERTa-base. We pre-train this small version with the\naforementioned 22M image-text pairs. (4) RoBERTa-large: it is\nthe larger version of RoBERTa-base and is also pre-trained by the\noriginal authors. Its pre-training data is the same as that of\nRoBERTa-base. (5) BriVL w/ RoBERTa-large: our standard BriVL\npre-trained on the whole WSCD.\nThe zero-shot classi ﬁcation results on Toutiao News and\nTHUCNews are shown in Fig.5a. It can be seen that: (1) The\nresults of RoBERTa-base are lower than those of RoBERTa-large,\nwhich is expected since the latter has more parameters and a\nlarger model capacity. (2) On both datasets, RoBERTa-base\n(ﬁnetune) has limited performance gains over RoBERTa-base,\nwhile BriVL w/ RoBERTa-base outperforms RoBERTa-base by\nlarge margins. This clearly indicates the advantage of cross-modal\nlearning over single-modal learning, given that the ﬁnetuning\ndata of RoBERTa-base (ﬁnetune) and BriVL w/ RoBERTa-base is\nboth from the 22M subset of WSCD. (3) When it comes to\nFig. 5 Zero-shot news classiﬁcation results. aZero-shot news classiﬁcation results (%) on two Chinese news datasets.b Zero-shot accuracy gain/loss\n(%) of BriVL w/ RoBERTa-large comparing to RoBERTa-large on each category of Toutiao News.c Top-30 phrase retrieval results of“sports” (top) and\n“automobile” (bottom) using RoBERTa-large and BriVL w/ RoBERTa-large, respectively. The candidate phrase list is obtained from Jieba, which consists of\n347,728 Chinese phrases. We translate the results into English for presentation clarity. Highest results in (a) are highlighted in bold.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2 ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 7\nRoBERTa-large, our BriVL w/ RoBERTa-large also leads to much\nbetter results than RoBERTa-large.\nMoreover, in Fig.5b, we present the performance gain/loss of\nour BriVL w/ RoBERTa-large comparing to RoBERTa-large on\neach category of Toutiao News. We can observe that the\nperformance of BriVL decreases only on 5 categories but\nincreases on the other 10, validating that the single-modal\nimagination/association ability can be improved by multimodal\nlearning. Further, in Fig. 5c, we show top-30 phrase retrieval\nresults of the category names “sports” and “automobile” using\nthese two models to take a closer look. Concretely, we use a\nChinese phrase list from Jieba44 as the candidate list, which\ncontains 347,728 phrases. Then we obtain the text embeddings of\nall candidates using RoBERTa-large and BriVL w/ RoBERTa-\nlarge, respectively. For each model and each category name, we\ncompute the category name embedding and retrieve top-30\nphrases by comparing it with all candidate embeddings using the\ncosine similarity. Finally, we visualize the results with the UMAP\nalgorithm45. For“sports”, we can see that our BriVL relates it to\nphrases with a higher variety than RoBERTa-large does. However,\nfor “automobile”, the retrieved top-30 phrases of our BriVL are\nmore monotonous.\nCross-modal retrieval. Here we conduct experiments on the\ncross-modal retrieval downstream task, which is exactly what we\ntrain our BriVL to do. Since our BriVL is pre-trained with Chi-\nnese data, we choose the only available multimodal Chinese\ndataset AIC-ICC46 for performance evaluation. AIC-ICC is ori-\nginally designed for image captioning, which wasﬁrst released in\nAI Challenger 2017, a competition organized by Sinovation\nVentures, Sogou, and Toutiao (ByteDance). The training set of\nAIC-ICC has 300K images and the validation set has 30K images.\nEach image has 5 Chinese captions. Since the test set is not\nreleased, we take the ﬁrst 10K images along with their corre-\nsponding 50K pieces of texts from the validation set for testing.\nThe cross-modal retrieval results on AIC-ICC are shown in the\ntable of Fig.6a. The method“BriVL (direct training)” means that\nwe directly train a randomly-initialized BriVL model on the\ntraining set of AIC-ICC rather than using the pre-trained BriVL.\nMoreover, the results of three “BriVL (pre-train & ﬁnetune)”\nvariations are all obtained byﬁnetuning our pre-trained BriVL on\nthe training set of AIC-ICC with differentﬁnetuning strategies.\nWe only consider twoﬁnetuning factors: whether toﬁx all the\nbatch normalization (BN) layers in the CNN (i.e., EfﬁcientNet-\nB7), and how many blocks should be unﬁxed in the CNN. We\nperform both image-to-text and text-to-image retrieval for\nevaluation, and report the results with Recall@k (k = 1, 5, 10) as\nwell as Recall@SUM (i.e., the summation of six Recall@k metrics).\nWe can observe from the table of Fig.6a that image-to-text\nretrieval results are generally higher than text-to-image ones,\nwhich is expected because like us humans, describing a given\nimage is easier than imagining a picture from a sentence. We can\nalso see that three “BriVL (pre-train & ﬁnetune)” variations\nachieve far better results than “BriVL (direct training)” for all\nevaluation metrics, indicating the usefulness of large-scale\nmultimodal pre-training. In addition, using pre-trained models\nlike our BriVL is more beneﬁcial to image-to-text retrieval than to\ntext-to-image retrieval, which may be due to the fact that image-\nto-text retrieval is an easier task. From the performance of three\nFig. 6 Cross-modal retrieval and visual question answering (VQA) results. aCross-modal retrieval results (%) on the Chinese dataset AIC-ICC.b VQA\nresults on Visual7W. Overall accuracies (%) along with results on each question type are reported. The dataset is translated into Chinese.c VQA examples\nof our BriVL model regarding whether it is pre-trained to validate the strong imagination ability of our pre-trained BriVL. Highest results in (a) and (b) are\nhighlighted in bold.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2\n8 NATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications\n“BriVL (pre-train &ﬁnetune)” variations, we ﬁnd that different\nﬁnetuning strategies do affect theﬁnal results, which should be\nkept in mind when weﬁnetune pre-trained models for different\ndownstream tasks.\nVisual question answering. We consider another multimodal\ndownstream task called visual question answering (VQA)47 to\nfurther validate the strong imagination ability of our pre-trained\nBriVL on the Visual7W dataset48. Visual7W has 47.3K images\nfrom MSCOCO49 and each image comes with a question and four\nanswer candidates, where only one is the correct answer. The\nwhole dataset can be divided into “Telling” questions and\n“Pointing” ones. Since“Pointing” questions rely on the bounding\nboxes of objects in images, we only conduct experiments on the\n“Telling” part, which can be further divided into six question\ntypes: “What”, “Where”, “When”, “Who”, “Why”, and “How”.\nWe randomly make the training and test splits with 70% and\n30%, respectively. Since Visual7W is an English dataset, we\ntranslate all of the questions and answer candidates into Chinese.\nIn the table of Fig.6b, we report the overall accuracies on the\ntest set, as well as the results on each question type. Similar to the\nsituation on the cross-modal retrieval task, three“BriVL (pre-\ntrain & ﬁnetune)” variations achieve much better results than\n“BriVL (direct training)” for all question types, again indicating\nthe usefulness of large-scale pre-training on downstream tasks.\nWe also notice that the bestﬁnetuning strategy for cross-modal\nretrieval (i.e.,ﬁxing BN and keeping 4 blocks of the CNN unﬁxed)\nis no longer the best for VQA. In addition, although the strategy\nof not ﬁxing BN and keeping 2 blocks unﬁxed obtains the best\noverall result, it does not achieve the best for all question types.\nThis is expected as different tasks require different ﬁnetuning\nstrategies.\nFurthermore, we present four VQA examples in Fig.6c. From\nthese examples, we see our pre-trained BriVL clearly showing the\nstrong imagination ability and even hints of common sense as it\nknows that the train in the picture looks blurry because it is\nmoving fast, the picture of horses was taken in aﬁeld rather than\nin a zoo, the boats being tied to the dock are simply not moving\ninstead of ﬂoating, and the trafﬁc is stopped because of the red\nlight instead of trafﬁc jam. We believe that this is achieved by pre-\ntraining with our weak semantic correlation data: the texts are not\ndetailed descriptions of their corresponding images, and thus our\nBriVL has to ﬁgure out the complicated connections hidden\namong this weak correlation during pre-training. With large pre-\ntraining data as much as 650 million, our BriVLﬁnally succeeds\nin acquiring the ability of reasonably and logically imagining/\nassociating, and also manages to learn some common sense.\nDiscussion\nWe have developed a large-scale multimodal foundation model\ncalled BriVL, which is efﬁciently trained on weak semantic cor-\nrelation dataset (WSCD) consisting of 650 million image-text\npairs. We have identiﬁed the direct evidence of the aligned image-\ntext embedding space by neural network visualizations and text-\nto-image generation. In addition, we have visually revealed how a\nmultimodal foundation model understands language and how it\nmakes imagination or association about words and sentences.\nMoreover, extensive experiments on other downstream tasks\nshow the cross-domain learning/transfer ability of our BriVL and\nthe advantage of multimodal learning over single-modal learning.\nParticularly, our BriVL appears to acquire abilities in imagination\nand reasoning. Last but not least, we believe that all of these\nadvantages are mainly due to the weak semantic correlation\nassumption followed by our BriVL. That is, by effectively fusing\nthe complex human emotions and thoughts from those weakly\ncorrelated image-text pairs, our BriVL is made more cognitive\nand general (i.e., much closer to AGI).\nWe believe that the solid step we take towards AGI would have\na broad impact not only on the AI development community but\nalso on a wide range of AI+ ﬁelds. For the AI researchﬁeld itself,\nbased on our GPU-resource-saving multimodal pre-training fra-\nmework, researchers could easily extend our BriVL to a larger\ncapacity with more modalities, leading to more general founda-\ntion models. Moreover, with the help of large-scale multimodal\nfoundation models, it would also be much easier for researchers\nto explore novel tasks (especially those without abundant human-\nannotated samples). For AI + ﬁelds, such foundation models\ncould be quickly adapted to speciﬁc working context or envir-\nonment, thanks to their strong generalization abilities. For\nexample, in healthcare, multimodal foundation models could\nmake full use of case data in multi-modality (e.g., computed\ntomography data, and blood routine examination data) to\nimprove the diagnosing accuracy. Moreover, in neuroscience,\nmultimodal foundation models could even help ﬁnd out the\nmechanism of how multimodal data connect and fuse since\nartiﬁcial neural networks are much simpler to examine than real\nneural systems in human brains.\nNevertheless, multimodal foundation models still face potential\nrisks and challenges. Since the performance of foundation models\nis based on the data that they are pre-trained on, it is likely that\nthe models learn prejudices and stereotypes about certain issues,\nwhich should be carefully handled before model training and\nmonitored/addressed in downstream applications. Moreover, as\nfoundation models master more and more skills, creators of these\nmodels should be aware of model misuse by ill-intentioned\npeople (e.g., manipulating or generating fake contents), which\nwould have a negative inﬂuence on the society. In addition, on the\nevolution challenges of foundation models academically, it is of\ngrand challenge for (1) developing model-interpretability tools\ndeeper into the foundation models, (2) constructing huge pre-\ntraining datasets with more modalities, as well as (3) applying\nfoundation models to various downstream tasks with more\neffective adaptation/ﬁnetuning techniques.\nOur understanding of what BriVL (or any large-scale multi-\nmodal foundation model) has learned and what it is capable of\nhas only just started. There is still much room for further study to\nbetter understand the foundation model and develop more novel\nuse cases. For instance, since the image can be regarded as a\nuniversally-understood “language”, soliciting an even larger\ndataset containing multiple languages could result in a language\ntranslation model obtained as a by-product of multimodal pre-\ntraining. Moreover, additional modalities (e.g., videos and audios)\ncan be also explored to pre-train a more intelligent model, taking\nus even closer to AGI.\nMethods\nArchitecture overview. The notion of pre-training a large-scale machine learning\nmodel and then using it for downstream tasksﬁrst appeared in natural language\nprocessing (NLP). As shown in Supplementary Note Fig. S1c, large-scale NLP\nmodels like GPT50, BERT51, and their variants, take Transformers52 as text\nencoders to encode input texts into text embeddings, and then design pre-training\nobjectives on top of these embeddings (e.g., generative loss and masked language\nmodeling loss). In computer vision (CV), large-scale pre-training also becomes\npopular. Models like BiT\n53 and ViT54 use convolutional neural networks (CNNs)\nor Transformers as image encoders to obtain embeddings of input images. Simi-\nlarly, pre-training losses are computed using the image embeddings (e.g., image\nclassiﬁcation loss and masked image patch prediction loss). However, these models\nare single-modal and thus only beneﬁt downstream tasks in one modality. For\nmultimodal (e.g., image, text, and audio) pre-training\n12–18,32, existing works can be\ndivided into two groups according to their network architectures: single-tower\nmodels (e.g., UNITER17, OSCAR12, and M618) and two-tower ones (e.g., CLIP13\nand ALIGN32). Our BriVL can also be categorized into the two-tower models since\nwe use separate image and text encoders. But note that we actually adopt two\nadditional momentum encoders to help with the pre-training process (i.e., to\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2 ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 9\ndynamically maintain negative sample queues across training batches), resulting in\na four-tower pre-training architecture.\nThe pre-training goal of our BriVL is to learn two encoders that can embed\nimage and text inputs into the same semantic space for effective image-text\nretrieval. To enforce the image and text encoders to learn better representations in\nthe same embedding space, we introduce cross-modal contrastive learning with the\nInfoNCE loss\n23 into our BriVL. Speciﬁcally, our learning objective is toﬁnd the\ncorresponding image embedding from a batch of them for a given text embedding\nand vice versa. By maximizing the cosine similarity of the image and text\nembeddings for each ground-truth pair while minimizing the cosine similarities of\nthe embeddings from negative pairs, we jointly train the image and text encoders to\nlearn an aligned cross-modal embedding space.\nFormally, for the image-text retrieval task, we denote the training set as\nD ¼f ð x\nðiÞ\ni ; xðtÞ\ni Þji ¼ 1; /C1/C1/C1 ; Ng, whereðxðiÞ\ni ; xðtÞ\ni Þ is a matched image-text pair, andN\nis the size ofD. Our BriVL leverages contrastive learning by applying MoCo29 into\nthe cross-modal scenario, as illustrated in Supplementary Note Fig. S1a. Each\nimage xðiÞ\ni (or each textxðtÞ\ni ) is encoded by the image encoderf(i) (or the text\nencoder f(t)) to obtain itsd-dimensional embeddingzðiÞ\ni (or zðtÞ\ni ). The image encoder\n(see Supplementary Note Fig. S1b) contains a CNN backbone, a successive self-\nattention (SA) block, and a multi-layer perceptron (MLP). A sequence of patch\nembeddings areﬁrst obtained by applying multi-scale patch pooling (MSPP) to the\nfeature map from CNN. They are then fused/encoded by the SA block. The text\nencoder, on the other hand, is stacked by several SA blocks such as BERT51 and\nRoBERTa4. A two-layer MLP block with a ReLU55 activation layer is used for\nmapping each encoder’s representation into the joint cross-modal embedding\nspace. The parameters off(i) and f(t) are denoted asθ(i) and θ(t), respectively.\nImage encoder. To obtain better performance in the image-text retrieval task,\nmost previous methods19–21,56 utilize a bottom-up attention mechanism57 with\nobject features extracted by the Faster R-CNN detector58. However, extracting\nregion/object features with a heavy detector is computationally expensive, e.g., a\nFaster R-CNN detector typically costs 0.064s (15.6 fps) to extractﬁne-grained\nregion information from an image of moderate size. Meanwhile, the image-text\nretrieval would be inevitably limited by the detector’s performance, which is not\nadaptable to real-world applications. In this paper, we thus introduce a simple yet\neffective module named Multi-Scale Patch Pooling (MSPP) to address this\nproblem.\nFor each input imagex(i),w eﬁrst split it into multiple patches at different scales\nand record the patch coordinates. In all experiments, we take two scales as 1 × 1\nand 6 × 6, resulting in a total of 37 patches. Next, we project each set of patch\ncoordinates onto the feature map that is obtained by the CNN backbone (e.g.,\nEfﬁcientNet\n43) and generate a sequence of 37 region feature maps. Finally, we\napply average pooling to each region feature map and obtain a sequence of patch\nfeatures S 2 Rc ´ Np , where each column corresponds to a patch,Np is the number\nof patches (i.e.,Np = 37 in this paper), andc is the number of channels in the\nfeature map.\nTo better capture the relationship of image patch features, we deploy a self-\nattention (SA) block containing multiple Transformer52 encoder layers. Each\nTransformer encoder layer consists of a multi-head attention (MultiHeadAttn)\nlayer and a feed forward network (FFN) layer:\nS\n0 ¼ LayerNormðS þ MultiHeadAttnðSÞÞ ð 1Þ\nS ¼ LayerNormðS0 þ FFNðS0ÞÞ: ð2Þ\nWe then fuse the extracted patch features by applying an average pooling layer:\nrðiÞ ¼ 1\nNp\n∑\nNp\nj¼1\nSj 2 Rc; ð3Þ\nwhere Sj is the j-th column ofS. A two-layer MLP block with a ReLU activation\nlayer is adopted to projectr(i) to the joint cross-modal embedding space, resulting\nin the ﬁnal d-dimensional image embeddingzðiÞ 2 Rd.\nText encoder. Given a sentencex(t),w e ﬁrst tokenize it to obtain a sequence of\ntokens T ¼f tjjj ¼ 1; :::;lg, where l denotes the length of the sentence (e.g., the\nnumber of words) andtj denotes the j-th token ofT . A pre-trained Transformer\nencoder (e.g., RoBERTa42) is then used to map text tokens to a sequence of feature\nvectors (each feature vector corresponds to a word). Similarly, to better capture the\nrelationship between words, we use the same self-attention mechanism as in the\nimage encoder to extract the text representationr\n(t). A two-layer MLP block with a\nReLU activation layer is also used for mapping the text representationr(t) to the\njoint cross-modal embedding space, resulting in theﬁnal d-dimensional text\nembedding zðtÞ 2 Rd .\nContrastive loss. The cross-modal contrastive loss in our BriVL is deﬁned based\non MoCo29, which provides a mechanism of building dynamic sample queues for\ncontrastive learning. Since the two negative queues used in our BriVL decouple the\nqueue size from the mini-batch size, we can have a much larger negative sample\nsize than the mini-batch size (thus GPU-resource-saving).\nTo maintain large queues of samples coming from different mini-batches and\naddress the problem that sample features are extracted by encoders with very\ndifferent parameters, we need two more smoothly updated encoders, that is,\nmomentum encoders. The parametersθðiÞ\nm (or θðtÞ\nm ) of the momentum image\nencoder f ðiÞ\nm (or the momentum text encoderf ðtÞ\nm ) are updated in each training\niteration with a momentum hyper-parameterm:\nθðiÞ\nm ¼ m /C1 θðiÞ\nm þð 1 /C0 mÞ/C1 θðiÞ; ð4Þ\nθðtÞ\nm ¼ m /C1 θðtÞ\nm þð 1 /C0 mÞ/C1 θðtÞ: ð5Þ\nFurther, we maintain two negative sample queuesQðiÞ and QðtÞ, which contain\nNq image negatives andNq text negatives for contrastive learning, respectively. In\neach pre-training iteration with the batch sizeNb, all Nb image negatives andNb\ntext negatives are separately pushed into these two queues. Meanwhile, there areNb\nearliest samples being popped out of each queue. Concretely, at iterationt, the\nimage and text negatives from the current data batchðBðiÞ\nt ; BðtÞ\nt Þ are computed by\nrespectively forwarding the momentum encodersf ðiÞ\nm and f ðtÞ\nm :\nNðiÞ\nt ¼ f ðiÞ\nm ðxðiÞ\ni ÞjxðiÞ\ni 2 BðiÞ\nt\nno\n; ð6Þ\nNðtÞ\nt ¼ f ðtÞ\nm ðxðtÞ\ni ÞjxðtÞ\ni 2 BðtÞ\nt\nno\n; ð7Þ\nwhere jBðiÞ\nt j¼j BðtÞ\nt j¼ Nb. The obtainedNðiÞ\nt and NðtÞ\nt are then pushed intoQðiÞ\nand QðtÞ, respectively. Note that although we generally callNðiÞ\nt (or NðtÞ\nt ) image\nnegatives (or text negatives), there is still one sample being positive to each text (or\nimage). Here, we denote the positive image sample (or text sample) for thej-th\ninput text xðtÞ\nj (or the j-th input imagexðiÞ\nj ) of the current mini-batch as:\npðiÞ\nj ¼ f ðiÞ\nm ðxðiÞ\nj Þ2 NðiÞ\nt ; ð8Þ\npðtÞ\nj ¼ f ðtÞ\nm ðxðtÞ\nj Þ2 NðtÞ\nt : ð9Þ\nWith the two negative queues, the loss function in each training iteration is thus\ncomputed as follows. For each input imagexðiÞ\ni ,w ed eﬁne the contrastive loss\nbetween its image embeddingzðiÞ\ni and all positive/negative texts in the queueQðtÞ as\nan InfoNCE loss23:\nLi2t ¼/C0 1\nNb\n∑\nNb\ni\nlog\nexp zðiÞ\ni /C1 pðtÞ\ni =τ\n/C16/C17\nexp zðiÞ\ni /C1 pðtÞ\ni =τ\n/C16/C17\nþ ∑\nnðtÞ\nexp zðiÞ\ni /C1 nðtÞ=τ\n/C16/C17 ; ð10Þ\nwhere nðtÞ 2 QðtÞ nf pðtÞ\ni g denotes a text negative for each image,τ is the\ntemperature hyper-parameter, and the vector similarity is measured by dot product\n(⋅). Similarly, for each input textxðtÞ\ni , the InfoNCE loss is given by:\nLt2i ¼/C0 1\nNb\n∑\nNb\ni\nlog\nexp zðtÞ\ni /C1 pðiÞ\ni =τ\n/C16/C17\nexp zðtÞ\ni /C1 pðiÞ\ni =τ\n/C16/C17\nþ ∑\nnðiÞ\nexp zðtÞ\ni /C1 nðiÞ=τ\n/C16/C17 ; ð11Þ\nwhere nðiÞ 2 QðiÞ nf pðiÞ\ni g denotes an image negative for each text.\nThe total loss function for pre-training our BriVL is then deﬁned as:\nLtotal ¼ Li2t þ Lt2i: ð12Þ\nIn the test/evaluation stage, given each query image (or text), the cross-modal\nretrieval results are obtained simply by the dot product deﬁned over the outputs of\nthe text (or image) encoder.\nImplementation details. Over the input images, we adopt random graying and\nrandom color jittering for data augmentation. All images are resized to 600 × 600\npixels. We adopt EfﬁcientNet-B7\n43 as the CNN backbone in the image encoder and\nRoBERTa-Large42 as the basis Transformer in the text encoder. For both image and\ntext encoders, the self-attention block consists of 4 Transformer encoder layers and\nthe MLP block has two fully-connected layers with a ReLU activation layer. The\nﬁnal embedding size of the joint cross-modal space is 2,560. We select the hyper-\nparameters heuristically for pre-training our BriVL model due to the computa-\ntional constraint: the temperature hyper-parameterτ = 0.07, momentumm = 0.99,\nand the queue sizeN\nq = 13, 440. We adopt the Adam optimizer59, with the weight\ndecay 1e-5 and the learning rate 1e-4. We use a mini-batch size of 192 for each of\nthe 14 machines (each machine has 8 NVIDIA A100 GPUs), resulting in a total\nbatch size of 2688 (far smaller thanNq). The resource-saving advantages of such\nbatch setting are shown by the ablation study results in Supplementary Note Fig.\nS2. We also deploy the latest distributed-training framework DeepSpeed26 to\naccelerate the pre-training process and save the GPU memories. With 112 NVIDIA\nA100 GPUs in total, it takes about 10 days to pre-train our BriVL model over our\nWSCD of 650 million image-text pairs.\nDifferences from CLIP/ALIGN. We have stated two main differences between our\nBriVL and CLIP/ALIGN in the Introduction section. Below we give more detailed\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2\n10 NATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications\ndifferences technically. (1) We adopt a four-tower network architecture (see Sup-\nplementary Note Fig. S1a) for pre-training. By extending the original single-modal\ncontrastive learning (CL) algorithm MoCo29, we introduce momentum encoders\nand negative sample queues for multimodal pre-training in a more GPU-resource-\nsaving way. In contrast, both CLIP and ALIGN employ the standard two-tower\narchitecture, which requires large batch size (thus enough negative samples) to be\neffective, taking up a mass of GPU memories. (2) We additionally devise a multi-\nscale patch pooling (MSPP) module (see Supplementary Note Fig. S1b) to capture\nﬁne-grained image region representations without using object detectors. While\nCLIP and ALIGN only consider global-level image embeddings, which impedes\ntheir ability to learnﬁne-grained/local image features.\nFormalization of neural network visualization. Neural network visualization is\ndeveloped to directly show the visual response/imagination of BriVL w.r.t. the\nsemantic input. Formally, given the pre-trained image and text encodersf\n(i) and f(t)\nof BriVL, weﬁrst input a piece of textx(t) and obtain its text embedding\nzðtÞ ¼ f ðtÞðxðtÞÞ2 Rd. In the mean time, we randomly initialize a noisy image\nxðiÞ 2 R600 ´ 600 ´ 3, which contains all the learnable parameters throughout the\nentire visualization process. Further, we obtain the image embeddingzðiÞ ¼\nf ðiÞðxðiÞÞ2 Rd and deﬁne the learning objective by matching the two embeddings:\nLvis ¼/C0 cos zðiÞ; zðtÞ/C0/C1\n; ð13Þ\nwhere cosð/C1 ; /C1Þ computes the cosine similarity between two vectors. With the\nresultant gradients, we are able to update the input imagex(i) by back-propagation.\nAfter repeating the above updating step with multiple iterations, weﬁnally obtain\nan image x(i), which can be regarded as BriVL’s response/imagination about the\ninput text. The algorithm for neural network visualization is summarized in\nAlgorithm 1.\nAlgorithm 1. Neural Network Visualization\nInput: The pre-trained image and text encoderf\n(i) and f(t) of our BriVL\nA piece of textx(t)\nA randomly initialized imagex(i)\nA learning rate parameterλ\nOutput: The updated input image\n1: Obtain the text embeddingz(t) = f(t)(x(t));\n2: for all iteration = 1, 2,⋯ , MaxIteration do\n3: Obtain the image embedding z(i) = f(i)(x(i));\n4: Compute Lvis with Eq. (13);\n5: Compute the gradients ∇xðiÞ Lvis;\n6: Update x(i) using gradient descent withλ;\n7: end for\n8: return the updated input image.\nFormalization of text-to-image generation. To make BriVL’s response/imagina-\ntion on input texts better understood, we further adopt VQGAN34 to help generate\nmore photo-realistic images. The reason of utilizing VQGAN instead of other\nGANs60 is as follows. Although classic GANs are able to generate high quality images\nunder speciﬁc domains (e.g., natural sceneries or human faces), they tend to fail when\ncomplex scenarios are involved. In contrast, VQGAN alleviates this problem and\nperforms better under complex scenarios by combining VQVAE61 and GAN. For our\ntext-to-image generation, we only need a codebookC and a CNN generatorg of the\nVQGAN pre-trained on ILSVRC-201235. The pre-trained codebookC ¼f ck 2\nRdc jk ¼ 1; 2; /C1/C1/C1 ; Ncg is a collection of tokens/codes, wheredc is the dimension of\neach code andNc is the number of codes in the codebook (dc = 256 andNc = 1, 024 in\nour case). The pre-trained CNN generatorg takes a spatial collection of codebook\nentries U 2 Rh ´ w ´ dc as input to generate an image (U can also be regarded as a\nsequence ofhw codes, h = w = 16 in our case), where each elementuij 2 Rdc\n(i = 1, 2,/C1/C1/C1 ; h and j = 1, 2,/C1/C1/C1 ; w) must come from the codebookC (i.e., uij 2 C).\nWith the pre-trained image and text encodersf(i) and f(t) of BriVL, weﬁrst input a\npiece of textx(t) and obtain its text embeddingzðtÞ ¼ f ðtÞðxðtÞÞ2 Rd. Meanwhile, we\nrandomly initialize an input code collectionU, which is the only parameter matrix to\nbe learned. Afterwards, we generate an image from the generatorx(i) = g(U)a n d\nfurther obtain its image embeddingzðiÞ ¼ f ðiÞðxðiÞÞ2 Rd . The learning objective is to\nmaximize the similarity between two embeddings:\nLt2i ¼/C0 cos zðiÞ; zðtÞ/C0/C1\n: ð14Þ\nAfter updating the inputU and obtaining U0 by back-propagation, we need to\nperform an element-wise quantization of each spatial codeu0\nij 2 Rdc in U0 onto its\nclosest codebook entryck:\nuij ¼ arg min\nck 2C\nku0\nij /C0 ckk: ð15Þ\nBy repeating the above updating step with multiple iterations, weﬁnally obtain\nan image x(i) generated with the updatedU. The algorithm for text-to-image\ngeneration is summarized in Algorithm 2.\nAlgorithm 2. Text-to-Image Generation\nInput: The pre-trained image and text encoderf(i) and f(t) of our BriVL\nThe codebook C and the CNN generatorg of the pre-trained VQGAN\nA piece of textx(t)\nA randomly initialized collection of codebook entriesU\nA learning rate parameterλ\nOutput: The image generated with the updatedU\n1: Obtain the text embeddingz(t) = f(t)(x(t));\n2: for all iteration = 1, 2,⋯ , MaxIteration do\n3: Generate an image x(i) = g(U);\n4: Obtain the image embedding z(i) = f(i)(x(i));\n5: Compute Lt2i with Eq. (14);\n6: Compute the gradients ∇ULt2i;\n7: Obtain U0 by updating U using gradient descent withλ;\n8: Obtain U by performing element-wise quantization onU0 with Eq. (15);\n9: end for\n10: return the image generated with the updatedU.\nNeural network visualization vs. text-to-image generation. The intrinsic dif-\nference between neural network visualization and text-to-image generation lies in\nthat they produce images following different data distributions. Not utilizing extra\nmodules or data, neural network visualization exhibits BriVL’s primitive visual\nunderstanding of a given piece of text. However, the VQGAN34 used for text-to-\nimage generation is pre-trained on ILSVRC-201235 (i.e., the classic ImageNet\ndataset), which generates images following the data distribution of ImageNet and\nthus being more photo-realistic. Due to such an intrinsic difference, we present the\nvisualization results of these two tasks for different purposes in this paper. Speci-\nﬁcally, neural network visualization allows us to see what exactly a pre-trained\nmulti-modal foundation model imagines about semantic concepts and sentences,\nwhile text-to-image generation is used to generate images matched with given texts\nin a more human-friendly way.\nData availability\nThe availability of datasets used in this study is detailed as follows: (1). Two remote\nsensing scene classiﬁcation datasets: UC Merced Land-Use (UCM,http://weegee.vision.\nucmerced.edu/datasets/landuse.html) and AID (https://captain-whu.github.io/AID/). (2).\nTwo news classiﬁcation datasets: THUCNews (http://thuctc.thunlp.org/) and Toutiao\nNews (https://github.com/aceimnorstuvwxz/toutiao-text-classﬁcation-dataset). (3). The\nChinese cross-modal retrieval dataset AIC-ICC is available athttps://github.com/neilfei/\nbrivl-nmi. (4). The VQA dataset Visual7W is available athttp://ai.stanford.edu/yukez/\nvisual7w/. (5). The dataset used for pre-training our model is available athttps://resource.\nwudaoai.cn/home. Please note that the available datasets (1)– (4) are sufﬁcient for\nﬁnetuning our pre-trained BriVL model in order to interpret, verify and extend our\nresearch.\nCode availability\nThe pre-trained BriVL and its inference code are available athttps://github.com/neilfei/\nbrivl-nmi under Creative Commons Attribution-Non Commercial-No Derivatives 4.0\nInternational Licence (CC BY-NC-ND).\nReceived: 4 December 2021; Accepted: 17 May 2022;\nReferences\n1. Goertzel, B. Arti ﬁcial general intelligence: Concept, state of the art, and future\nprospects. J. Artif. Gen. Intell.5,1 –48 (2014).\n2. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning.Nature 521, 436–444\n(2015).\n3. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image\nrecognition. InIEEE Conference on Computer Vision and Pattern Recognition,\n770–778 (2016).\n4. Liu, Y. et al. RoBERTa: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692 (2019).\n5. Wang, A. et al. GLUE: A multi-task benchmark and analysis platform for\nnatural language understanding. InInternational Conference on Learning\nRepresentations (2019).\n6. Santoro, A. et al. A simple neural network module for relational reasoning. In\nAdvances in Neural Information Processing Systems, 4967–4976 (2017).\n7. Bommasani, R. et al. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2 ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 11\n8. Editors of MIT Technology Review. 10 breakthrough technologies 2021.\n(2021).\n9. Brown, T. B. et al. Language models are few-shot learners. InAdvances in\nNeural Information Processing Systems, 1877–1901 (2020).\n10. Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C. & Fried, I. Invariant visual\nrepresentation by single neurons in the human brain.Nature 435, 1102–1107\n(2005).\n11. Quian Quiroga, R., Kraskov, A., Koch, C. & Fried, I. Explicit encoding of\nmultimodal percepts by single neurons in the human brain.Current Biology\n19, 1308–1313 (2009).\n12. Li, X. et al. Oscar: Object-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision, 121–137 (2020).\n13. Radford, A. et al. Learning transferable visual models from natural language\nsupervision. In International Conference on Machine Learning, 8748–8763\n(2021).\n14. Li, L. H., Yatskar, M., Yin, D., Hsieh, C. & Chang, K. VisualBERT: A simple\nand performant baseline for vision and language. arXiv preprint\narXiv:1908.03557 (2019).\n15. Li, G., Duan, N., Fang, Y., Gong, M. & Jiang, D. Unicoder-VL: A universal\nencoder for vision and language by cross-modal pre-training. InAAAI\nConference on Artiﬁcial Intelligence, 11336–11344 (2020).\n16. Su, W. et al. VL-BERT: Pre-training of generic visual-linguistic\nrepresentations. In International Conference on Learning Representations\n(2020).\n17. Chen, Y.-C. et al. UNITER: Universal image-text representation learning. In\nEuropean Conference on Computer Vision, 104–120 (2020).\n18. Lin, J. et al. M6: A chinese multimodal pretrainer. arXiv preprint\narXiv:2103.00823 (2021).\n19. Wu, Y., Wang, S., Song, G. & Huang, Q. Learning fragment self-attention\nembeddings for image-text matching. InACM International Conference on\nMultimedia, 2088–2096 (2019).\n20. Chen, H. et al. Imram: Iterative matching with recurrent attention memory for\ncross-modal image-text retrieval. InIEEE Conference on Computer Vision and\nPattern Recognition, 12655–12663 (2020).\n21. Diao, H., Zhang, Y., Ma, L. & Lu, H. Similarity reasoning andﬁltration for\nimage-text matching. InAAAI Conference on Artiﬁcial Intelligence, 1218–1226\n(2021).\n22. Wu, Z., Xiong, Y., Yu, S. X. & Lin, D. Unsupervised feature learning via non-\nparametric instance discrimination. InIEEE Conference on Computer Vision\nand Pattern Recognition, 3733–3742 (2018).\n23. Oord, A. v. d., Li, Y. & Vinyals, O. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748 (2018).\n24. Hjelm, R. D. et al. Learning deep representations by mutual information\nestimation and maximization. InInternational Conference on Learning\nRepresentations (2019).\n25. Zhuang, C., Zhai, A. L. & Yamins, D. Local aggregation for unsupervised\nlearning of visual embeddings. InInternational Conference on Computer\nVision, 6002–6012 (2019).\n26. Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S. & He, Y. Zero-inﬁnity:\nBreaking the GPU memory wall for extreme scale deep learning. In\nInternational Conference for High Performance Computing, Networking,\nStorage and Analysis(2021).\n27. Riquelme, C. et al. Scaling vision with sparse mixture of experts. InAdvances\nin Neural Information Processing Systems, 8583–8595 (2021).\n28. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework for\ncontrastive learning of visual representations. InInternational Conference on\nMachine Learning, 1597–1607 (2020).\n29. He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. Momentum contrast for\nunsupervised visual representation learning. InIEEE Conference on Computer\nVision and Pattern Recognition, 9729–9738 (2020).\n30. Grill, J.-B. et al. Bootstrap your own latent— a new approach to self-supervised\nlearning. InAdvances in Neural Information Processing Systems, 21271–21284\n(2020).\n31. Chen, X. & He, K. Exploring simple siamese representation learning. InIEEE\nConference on Computer Vision and Pattern Recognition, 15750–15758 (2021).\n32. Jia, C. et al. Scaling up visual and vision-language representation learning with\nnoisy text supervision. InInternational Conference on Machine Learning,\n4904–4916 (2021).\n33. Olah, C., Mordvintsev, A. & Schubert, L. Feature visualization. Distill,https://\ndoi.org/10.23915/distill.00007 (2017).\n34. Esser, P., Rombach, R. & Ommer, B. Taming transformers for high-resolution\nimage synthesis. InIEEE Conference on Computer Vision and Pattern\nRecognition, 12873–12883 (2021).\n35. Russakovsky, O. et al. ImageNet large scale visual recognition challenge.Int. J.\nComput. Vision 115, 211–252 (2015).\n36. Yang, Y. & Newsam, S. D. Bag-of-visual-words and spatial extensions for\nland-use classiﬁcation. InInternational Symposium on Advances in Geographic\nInformation Systems, 270–279 (2010).\n37. Xia, G.-S. et al. AID: A benchmark data set for performance evaluation of\naerial scene classiﬁcation. IEEE Transactions on Geoscience and Remote\nSensing 55, 3965–3981 (2017).\n38. Guan, J. et al. Zero and few shot learning with semantic feature synthesis and\ncompetitive learning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 43, 2510–2523 (2021).\n39. Li, A., Lu, Z., Wang, L., Xiang, T. & Wen, J. Zero-shot scene classiﬁcation for\nhigh spatial resolution remote sensing images.\nIEEE Transactions on\nGeoscience and Remote Sensing55, 4157–4167 (2017).\n40. Contributors of Toutiao News. Toutiao text classiﬁcation dataset. (2021).\n41. Li, J., Sun, M. & Zhang, X. A comparison and semi-quantitative analysis of\nwords and character-bigrams as features in chinese text categorization. In\nProc. 21st International Conference on Computational Linguistics and 44th\nAnnual Meeting of the Association for Computational Linguistics, 545–552\n(2006).\n42. Cui, Y. et al. Revisiting pre-trained models for chinese natural language\nprocessing. In Conference on Empirical Methods in Natural Language\nProcessing: Findings, 657–668 (2020).\n43. Tan, M. & Le, Q. EfﬁcientNet: Rethinking model scaling for convolutional\nneural networks. InInternational Conference on Machine Learning,\n6105–6114 (2019).\n44. Sun, J. “Jieba” Chinese text segmentation.https://github.com/fxsjy/jieba\n(2020).\n45. McInnes, L., Healy, J., Saul, N. & Großberger, L. UMAP: Uniform manifold\napproximation and projection.J. Open Source Softw.3, 861 (2018).\n46. Wu, J. et al. AI challenger: A large-scale dataset for going deeper in image\nunderstanding. arXiv preprint arXiv:1711.06475 (2017).\n47. Niu, Y. et al. Counterfactual VQA: A cause-effect look at language bias. In\nIEEE Conference on Computer Vision and Pattern Recognition, 12700–12710\n(2021).\n48. Zhu, Y., Groth, O., Bernstein, M. S. & Fei-Fei, L. Visual7W: Grounded\nquestion answering in images. InIEEE Conference on Computer Vision and\nPattern Recognition, 4995–5004 (2016).\n49. Lin, T.-Y. et al. Microsoft coco: Common objects in context. InEuropean\nConference on Computer Vision, 740–755 (2014).\n50. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language\nunderstanding by generative pre-training. OpenAI Blog (2018).\n51. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of deep\nbidirectional transformers for language understanding. InConference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 4171–4186 (2019).\n52. Vaswani, A. et al. Attention is all you need. InAdvances in Neural Information\nProcessing Systems, 5998–6008 (2017).\n53. Kolesnikov, A. et al. Big Transfer (BiT): General visual representation\nlearning. In European Conference on Computer Vision, 491–507 (2020).\n54. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image\nrecognition at scale. InInternational Conference on Learning Representations\n(2021).\n5 5 . N a i r ,V .&H i n t o n ,G .E .R e c t iﬁed linear units improve restricted boltzmann\nmachines. In International Conference on Machine Learning, 807–814\n(2010).\n56. Wei, X., Zhang, T., Li, Y., Zhang, Y. & Wu, F. Multi-modality cross attention\nnetwork for image and sentence matching. InIEEE Conference on Computer\nVision and Pattern Recognition, 10941–10950 (2020).\n57. Anderson, P. et al. Bottom-up and top-down attention for image captioning\nand visual question answering. InIEEE Conference on Computer Vision and\nPattern Recognition, 6077–6086 (2018).\n58. Ren, S., He, K., Girshick, R. B. & Sun, J. Faster R-CNN: Towards real-time\nobject detection with region proposal networks. InAdvances in Neural\nInformation Processing Systems,9 1–99 (2015).\n59. Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. In\nInternational Conference on Learning Representations(2015).\n60. Goodfellow, I. J. et al. Generative adversarial nets. InAdvances in Neural\nInformation Processing Systems, 2672–2680 (2014).\n61. van den Oord, A., Vinyals, O. & Kavukcuoglu, K. Neural discrete\nrepresentation learning. InAdvances in Neural Information Processing\nSystems, 6306–6315 (2017).\nAcknowledgements\nZ.L. acknowledges National Natural Science Foundation of China (61976220). J.R.W.\nacknowledges National Natural Science Foundation of China (61832017), Beijing Out-\nstanding Young Scientist Program (BJJWZYJH012019100020098), and Large-Scale Pre-\nTraining Program 468 of Beijing Academy of Artiﬁcial Intelligence (BAAI). N.F.\nacknowledges the Outstanding Innovative Talents Cultivation Funded Programs 2021 of\nRenmin Univertity of China. We acknowledge the WenLan Data Group for helping us\ncollect the pre-training dataset.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2\n12 NATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications\nAuthor contributions\nZ.L. contributed the original idea, model design, and experimental analysis. Z.L. and N.F.\nwrote the majority of the manuscript. N.F., Y.G. and Y.H. contributed the source code.\nThe experiments were conducted by N.F., G.Y., J.W., H.L. and Y.G. The review and\nediting of the manuscript were carried out by H.S., T.X., X.G., R.S. and J.R.W. The entire\nproject was supervised by J.R.W.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41467-022-30761-2.\nCorrespondence and requests for materials should be addressed to Zhiwu Lu, Hao Sun\nor Ji-Rong Wen.\nPeer review informationNature Communications thanks Qingming Huang, and the\nother, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer\nreviewer reports are available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2022\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30761-2 ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3094 | https://doi.org/10.1038/s41467-022-30761-2 | www.nature.com/naturecommunications 13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.754175066947937
    },
    {
      "name": "Interpretability",
      "score": 0.7387174963951111
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.6522632241249084
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6415383815765381
    },
    {
      "name": "Transformative learning",
      "score": 0.6147568225860596
    },
    {
      "name": "Cognition",
      "score": 0.524731457233429
    },
    {
      "name": "Artificial general intelligence",
      "score": 0.48322638869285583
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I71920554",
      "name": "King Abdullah University of Science and Technology",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    }
  ],
  "cited_by": 217
}