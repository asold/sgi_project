{
    "title": "Neural Random Projections for Language Modelling",
    "url": "https://openalex.org/W2811383539",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5022856736",
            "name": "Davide Nunes",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5048190616",
            "name": "Luís Antunes",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950133940",
        "https://openalex.org/W2157306293",
        "https://openalex.org/W2070862086",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W1916559533",
        "https://openalex.org/W1573706465",
        "https://openalex.org/W2138204974",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W1933990309",
        "https://openalex.org/W1575384945",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2979473749",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W1711419975",
        "https://openalex.org/W2093390569",
        "https://openalex.org/W2315665485",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W117618596",
        "https://openalex.org/W2566563465",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2068858721",
        "https://openalex.org/W2117756735",
        "https://openalex.org/W2950797609",
        "https://openalex.org/W2080021477",
        "https://openalex.org/W1423339008",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2553318503",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3097609957",
        "https://openalex.org/W889023230",
        "https://openalex.org/W2621208586",
        "https://openalex.org/W2250189634",
        "https://openalex.org/W2035238564",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2176412452",
        "https://openalex.org/W2135508918",
        "https://openalex.org/W1775434803",
        "https://openalex.org/W2037757210",
        "https://openalex.org/W2950577311"
    ],
    "abstract": "Neural network-based language models deal with data sparsity problems by mapping the large discrete space of words into a smaller continuous space of real-valued vectors. By learning distributed vector representations for words, each training sample informs the neural network model about a combinatorial number of other patterns. In this paper, we exploit the sparsity in natural language even further by encoding each unique input word using a fixed sparse random representation. These sparse codes are then projected onto a smaller embedding space which allows for the encoding of word occurrences from a possibly unknown vocabulary, along with the creation of more compact language models using a reduced number of parameters. We investigate the properties of our encoding mechanism empirically, by evaluating its performance on the widely used Penn Treebank corpus. We show that guaranteeing approximately equidistant (nearly orthogonal) vector representations for unique discrete inputs is enough to provide the neural network model with enough information to learn --and make use-- of distributed representations for these inputs.",
    "full_text": "Neural Random Projections for Language Modelling\nDavide Nunes1 Luis Antunes1\nAbstract\nNeural network-based language models deal with\ndata sparsity problems by mapping the large dis-\ncrete space of words into a smaller continuous\nspace of real-valued vectors. By learning dis-\ntributed vector representations for words, each\ntraining sample informs the neural network model\nabout a combinatorial number of other patterns.\nIn this paper, we exploit the sparsity in natural\nlanguage even further by encoding each unique\ninput word using a ﬁxed sparse random represen-\ntation. These sparse codes are then projected onto\na smaller embedding space which allows for the\nencoding of word occurrences from a possibly\nunknown vocabulary, along with the creation of\nmore compact language models using a reduced\nnumber of parameters. We investigate the proper-\nties of our encoding mechanism empirically, by\nevaluating its performance on the widely used\nPenn Treebank corpus. We show that guarantee-\ning approximately equidistant (nearly orthogonal)\nvector representations for unique discrete inputs\nis enough to provide the neural network model\nwith enough information to learn –and make use–\nof distributed representations for these inputs.\n1. Introduction\nThe goal of computational semantics is to automate the\nlearning of meaningful representations for natural language\nexpressions. In particular, representation learning is integral\nto state-of-the-art Language Modelling (LM), which is a\nkeystone in computational semantics used for a wide range\nof Natural Language Processing (NLP) tasks, from informa-\ntion retrieval (Ponte & Croft, 1998) to speech recognition\n(Arisoy et al., 2012), or machine translation (Koehn, 2010).\nBetter language models frequently lead to improved perfor-\nmance on underlying downstream tasks, which makes LM\nvaluable in itself. Moreover, certain domains allow language\n1BioISI—Instituto de Biosistemas e Ci ˆencias Integrativas,\nFaculdade de Ciˆencias, Universidade de Lisboa, Portugal.. Corre-\nspondence to: Davide Nunes <davex@ciencias.ulisboa.pt>, Luis\nAntunes <xarax@ciencias.ulisboa.pt>.\nmodels to extract knowledge implicitly encoded in the train-\ning data. For example, in (Serban et al., 2016), learning from\nﬁlm subtitles allows models to answer basic questions about\ncolours and people. Learning representations for discrete\nsymbols is particularly useful in the context of language\nmodels because capturing regularities in relationships be-\ntween words (which are not only complex but recursive\nin nature) allows us to build better models. The LM, as a\ntask, can thus be seen as a framework for the study of more\ngeneral learning approaches operating in fairly complex and\nsometimes dynamic scenarios.\nThe goal of LM is to model the joint probability distribution\nof words in a text corpus. Models are trained by maximising\nthe probability of a word given all the previous words in the\ntraining dataset, formally:\nP(w1,...,w N ) =\nN∏\ni=1\nP(wi|hi),\nwhere wi is the current word and ci = (w1,··· ,wi−1) is\nthe current word history or context. A Markov assumption\nis usually used as approximation: instead of using the full\nword history as context, the probability of observing the ith\nword wi is approximated by the probability of observing it\nin a shortened context of n−1 preceding words. To train\na language model under this assumption, a sliding window\n(wi,ci) of size nis applied sequentially across the textual\ndata. Each window is also commonly referred to as n-gram.\nTraditional n-gram models (Brown et al., 1992; Kneser &\nNey, 1995) are learned by building conditional probability\ntables for each P(wi|ci). This approach is popular due to\nits simplicity and overall good performance, but this formu-\nlation poses some obstacles. More speciﬁcally, we can see\nthat even with large training corpora, extremely small or\nzero probabilities can be given to valid sequences, mostly\ndue to rare word occurrences. Dealing with this sparseness\nand unseen data requires smoothing techniques that reallo-\ncate probability mass from observed to unobservedn-grams,\nproducing better estimates for unseen data (Kneser & Ney,\n1995). The fundamental issue is that classic n-gram models\nlack a notion of word similarity. Words are treated as equal-\nity likely to occur discrete entities with no relation to each\nother. This makes density estimation inherently difﬁcult, as\nthere is no straightforward way to perform smoothing. As\narXiv:1807.00930v4  [cs.CL]  26 Sep 2018\nNeural Random Projections\nan example, there is no way to estimate the probability for\ntwo sequences differing only in a synonymic expression.\nNeural networks have been used as a way to deal with both\nthe sparseness and smoothing problems. The work in (Ben-\ngio et al., 2003) represents a paradigm shift for language\nmodelling and an example of what we call Neural Network\nLanguage Models (NNLM). In a NNLM, the probability dis-\ntribution for a word given its context is modelled as a smooth\nfunction of learned real-valued vector representations for\neach word in that context. The neural network models\nlearn a conditional probability function P(wi|ci; θF ,θL),\nwhere θF is a set of parameters representing the vector\nrepresentations for each word and θL parameterises the log-\nprobabilities of each word based on those learned features.\nA softmax function is then applied to these log-probabilities\nto produce a categorical probability distribution over the\nnext word given its context. (In section 3, we will present\ntwo instances of neural network models following this for-\nmulation.)\nThe resulting probability estimates are smooth functions of\nthe continuous word vector representations, and so, a small\nchange in such vector representations results in a small\nchange in the probability estimation. Smoothing is learned\nimplicitly, leading NNLM to achieve better generalisation\nfor unseen contexts. The resulting word vector representa-\ntions are also commonly referred to as embeddings, since\nthey represent a mapping from the large discrete space (a\nvocabulary) to a smaller continuous space of real-valued\nvectors where they are “embedded”. This constitutes a ge-\nometric analogy for meaning with the that, with proper\ntraining, words that are semantically or grammatically re-\nlated will be mapped to similar locations in the continuous\nspace.\nEnergy-based models provide a different perspective on sta-\ntistical language modelling with parametric neural networks\nas a way to estimate joint discrete probability densities.\nEnergy-based models such as (Mnih & Hinton, 2007; Mnih\net al., 2009) capture dependencies between variables by\nassociating a scalar energy score to each variable conﬁgu-\nration. In this case, making predictions consists on setting\nthe values for observed or visible variables and ﬁnding the\nvalue for the remaining (hidden) variables that minimise\nthis energy score.\nThe key difference between the energy-based models in\n(Mnih & Hinton, 2007; Mnih et al., 2009) and the NNLM\nin (Bengio et al., 2003) is that instead of estimating a cate-\ngorical distribution based on higher-level features produced\nby the NNLM hidden layer based on the distributed rep-\nresentations for the context words, an energy-based model\ntries to predict the distributed representation of a target word\nand attributes an energy score to the output conﬁguration\nof the model based on how close the prediction is from the\nactual representation for the target word. The correct rep-\nresentation for the target word is dependent on the current\nstate of the model embedding parameters, but two separate\nparameter matrices can also be used for context and target\nwords (Mnih & Teh, 2012). Most probabilistic models can\nin fact be viewed as a special type of energy-based model in\nwhich the energy function has to satisfy certain normalisa-\ntion conditions, along with a loss function with a particular\nform. Moreover, energy-based learning provides a uniﬁed\nframework for learning, and can be seen as an alternative to\nprobabilistic estimation for prediction or classiﬁcation tasks\n(Lecun et al., 2006).\nIn this paper, we describe an encoding mechanism that can\nbe used with neural network probabilistic language models\nto represent unique discrete inputs while using a reduced\nparameter space. Our models use random projections as\na way to encode input patterns, using a ﬁxed size sparse\nrandom vector. The idea is to maintain the expressive power\nof the neural networks while reducing the parameter space\nand expanding the types of discrete patterns the network\ncan use to make predictions. Following the principles from\n(Achlioptas, 2003) and (Kanerva, 2009), our random projec-\ntion encoding reduces the input dimension |V|to a lower\nk-dimensional dimensional space while approximately pre-\nserving the distances between expected input patterns. This\nmeans that instead of using the 1-of-V orthogonal encoding,\nwe use a ﬁxed k-dimensional sparse vector with each input\nword being nearly-orthogonal to any other words. Our main\nhypothesis is that in order for a neural network to learn\ndistributed representations for words, we do not require one\nunique vector representation per word; instead, we just need\nthe representations of each pair of unique patterns to be\napproximately orthogonal. Each unique pattern should (by\nitself) be indistinguishable from any other unique pattern.\nBy using random projections, we guarantee that each new\nrepresentation is equally probable and nearly-orthogonal to\nany other representation. This encapsulates the idea that\nwhat makes a word unique in language is not its representa-\ntion, but the way in which we use the word, since language\nis essentially a system of differences, where meaning in\nwords arises through social social interaction (Feyerabend,\n1955), and biological biases (Pinker, 2010).\n2. Background and Related Work\nIn this paper, the goal is two fold: to reduce the dimension-\nality of the input space, and to allow neural-network models\nto capture patterns that are usually intractable to represent\nusing the typical count-based n-gram models. In the case of\nneural language modelling, each word is usually represented\nby a 1-of-V vector. The input space is an orthogonal unit\nmatrix where each word is represented by a single value 1\non the column corresponding to its index. This means that\n2\nNeural Random Projections\nif we want to represent a more complex structured space\nsuch as syntactic dependency trees, this would require ei-\nther to use structured neural network (Socher et al., 2011),\nor to index all possible unique combinations of syntactic\nstructures which is intractable for large datasets. This article\nis a ﬁrst step in the exploration the possibility of encoding\ncomplex structured discrete spaces. We take an incremental\nand bottom-up approach to the problem by exploring how\nto build an encoder that can be used in word-level language\nmodels.\nOne cornerstone of neural-network-based models is the no-\ntion of distributed representation. This is, the fact that neural\nnetwork can learn vector representations for discrete inputs\nas weights that are learned during the training process. In a\ndistributed representation, each ”thing” is represented as a\ncombination of multiple factors. Learning distributed rep-\nresentations of concepts as patterns of activity in a neural\nnetwork has been object of study in (Hinton, 1986). The\nmapping between unique concept ids and respective vector\nrepresentations has been later referred to as embeddings.\nThe idea of using embeddings in language modelling is\nexplored in the early work or Bengio et al. (Bengio et al.,\n2003) and later popularised by the word2vec method for\nword representation learning (Mikolov et al., 2013b). Both\nthese methods encode input words using a 1-of-V encoding\nscheme, where each word is represented by its unique index.\nThe size of the embedding space is thus proportional to the\nsize of the vocabulary.\nIn the case of language modelling, there have been multiple\ndevelopments to deal with large or unknown vocabular-\nies. One straightforward method is to consider character or\nsubword-level (e.g. morphemes) modelling instead of con-\nsidering words the discrete input units (Ling & Dyer, 2015;\nKim et al., 2016; Zhang & LeCun, 2015). The main motiva-\ntion behind using subword-based modelling is that training\naccurate character-level models is extremely difﬁcult and\noften more computationally expensive than working with\nmodels that use word-level inputs (Mikolov et al., 2012),\nbut Elman shows that most of the entropy if concentrated at\nthe ﬁrst few characters of each word (Elman, 1990). More-\nover, word-based models often fail to capture regularities\nin many inﬂectional and agglutinative languages. In this\npaper we focus on word-based modelling but the proposed\nencoding mechanism can be incorporated in models that use\nmorphemes instead of words as inputs.\nDespite the good results obtained with word-level neural\nprobabilistic language models (Bengio et al., 2003; Mnih &\nHinton, 2007), these have notoriously long training times,\neven for moderately-sized datasets. This is due to the fact\nthat in order for these models to output probability dis-\ntributions, they require explicit normalisation, which re-\nquires us to consider all words in the vocabulary to compute\nlog-likelihood gradients. Solutions to this problem include\nstructuring the vocabulary into a tree, which speeds up word\nprobability computations –but ties model predictive perfor-\nmance to the tree used (Morin & Bengio, 2005). Another\napproach is to use a sampling approach to approximate gradi-\nent computations. In particular, Noise Constrastive Estima-\ntion (NCE) (Gutmann & Hyv¨arinen, 2012) has been shown\nto be a stable approach to speed up unnormalised language\nmodels (Mnih & Teh, 2012). While we use full normali-\nsation to analyse the predictive capability of our proposed\nmodels, much like in the previous case, the computational\ncomplexity introduced by the normalisation requirements\ncan be alleviated using techniques like NCE. Nevertheless,\nusing random projections to model unique discrete inputs\nwith neural networks, changes the dynamics of the language\nmodelling problem. This warrants an assessment of how\neffective approximation methods are –which we will do in\nfuture work.\nExploiting low-dimensional structure in high-dimensional\nproblems has become a highly active area of research in ma-\nchine learning, signal processing, and statistics. In summary,\nthe goal is to use a low-dimensional model of relevant data\nin order to achieve, better prediction, compression, or esti-\nmation compared to more complex ”black-box” approaches\nto deal with high-dimensional spaces (Hegde et al., 2016).\nIn particular, we are interested in the compression aspect\nof these approaches. Usually, exploiting low-dimensional\nstructure comes at a cost, which is the fact that incorporating\nstructural constraints into a statistical estimation procedure\noften results in a challenging algorithmic problem. Neural\nnetwork models have been notoriously successful at dealing\nwith these kind of constraints, and even at ﬁnding geometric\nanalogies for complex patterns such as syntactic relation-\nships (Mikolov et al., 2013a). But the use of neural networks\nhas been limited in the types of patterns that it can process\nand encode. Particularly, there is no straightforward way to\nencode priors about discrete input qualiﬁers or to encode\nstructure within the input space –although some work has\nbeen done in that direction (Vaswani et al., 2017).\nIn this article we show that neural network models can be\nused to tackle a challenging statistical estimation problem\nsuch as language modelling, while using an approximation\nmethod to encode a large number of discrete inputs –and\npossibly allowing for the additional encoding of linguistic-\nbased structural constraints. To achieve this, we turn to a\nsimple yet efﬁcient class of dimensionality reduction meth-\nods, random projections.\nThe use of random projections as a dimensionality reduction\ntool has been extensively studied before, especially in the\ncontext of approximate matrix factorisation –the reader can\nrefer to (Halko et al., 2011) for a more in depth review of\nthis technique. The basic idea of random projections as\n3\nNeural Random Projections\ndimensionality reduction technique comes from the work\nof Johnson and Lindenstrauss, in which it was shown that\nthe pairwise distances among a collection of N points in\nan Euclidean space are approximately maintained when\nthe points are mapped randomly to an Euclidean space of\ndimension O(log N). In other words, random embeddings\npreserve Euclidean geometry (Johnson & Lindenstrauss,\n1984). In the case of language modelling, we assume that\nﬁnding representations for words is a problem of geometric\nnature (see (Levy et al., 2014)) that can be translated to a\nlower-dimensional space and solved there.\nRandom projections have been used in multiple machine\nlearning problems as a way to speed-up computationally\nexpensive methods. One example is the use of random\nmappings for fast approximate nearest-neighbour search\n(Indyk, 2001). Random projections have also been applied\nto document retrieval or representation learning based on\nmatrix factorisation. In (Papadimitriou et al., 1998), ran-\ndom projections are used as a ﬁrst step for Latent Semantic\nAnalysis (LSA), which is essentially a matrix factorisation\nof word co-occurrence counts (Landauer & Dumais, 1997).\nThis makes the matrix factorisation algorithm much more\ntractable, because the projections preserve pair-wise dis-\ntances used by the algorithm to ﬁnd latent vector represen-\ntations for documents – representations that explain word\nfrequency variance well. A similar approach is followed\nin (Kaski, 1998): a random mapping projection is applied\nto document vectors to reduce their dimensionality; these\nvectors then serve as input to a Self-Organising Map (SOM)\nin order to cluster the documents according to their simi-\nlarity. This approach was motivated by experiments from\n(Ritter & Kohonen, 1989), where instead of performing\ndocument clustering based on word co-occurrence frequen-\ncies, the authors performed word clustering after reducing\nthe context vector dimensionality using random projections.\nIn the later work of Kanerva, an incremental random pro-\njection method called random indexing, is used to gather\nco-occurrence patters in an incremental fashion (Kanerva\net al., 2000). This was motivated by previous work on as-\nsociative memory models (Kanerva, 1988), and the work\nin (Achlioptas, 2003), which showed that a random pro-\njection matrix can be constructed incrementally using a\nsimple sparse ternary distribution (while still respecting\nthe bound-preserving requirements from the Johnson and\nLindenstrauss lemma (Johnson & Lindenstrauss, 1984)).\nOur proposal of using random projections as the encoding\nmechanism for neural network language models is also re-\nlated to dictionary learning. Dictionary learning consists of\nmodelling signals as sparse linear combinations of atoms\nselected from a learned dictionary (Gribonval et al., 2015).\nIn this paper, we focus on exploring simple feedforward\nneural network architectures for multiple reasons. First, us-\ning incrementally complex models allows us to determine\nwhich architectural components allow neural network mod-\nels to deal with random-projection encoding. The work\nfrom Mnih et al. in energy-based neural language models\n(Mnih & Hinton, 2007; Mnih et al., 2009) is a good starting\npoint for our baseline because the energy-based model gives\nus a way to translate random vector predictions into word\nprobability distributions.\nRecurrent architectures have been successfully used in lan-\nguage modelling and related tasks (Jozefowicz et al., 2016)\n–with these successes often being attributed to their capacity\n(in principle) to capture unbound contexts (retaining long\nterm dependencies and ensuring that information can be\npropagated through many time steps). We remain cautious\nabout their use –as a baseline– for our proposed approach. In\npractice, it is not clear whether or not the capacity to capture\nlong-term dependencies (in principle) is key to achieve good\nresults in all tasks related to LM. For example, neural net-\nworks with a simple architecture and convolution operations,\ncapable of dealing with ﬁxed contexts, have been proposed\nas a competitive alternative to Long Short Term Memory\n(LSTM) networks (Dauphin et al., 2016). As another exam-\nple, good performance can be achieve in a translation task by\nusing attention mechanism with a feedforward architecture\n(Vaswani et al., 2017).\nMost importantly, using over-parametrised/complex archi-\ntectures undermines our ability to distinguish between a\nmodel that performs well with a new component, and a\nmodel that performs well despite the new component. We\ndo intend to explore the proposed encoding with recurrent\narchitectures in the future, but for now, this is beyond the\nscope of this paper.\n3. NRP Model\nIn this section, we describe our neural random projection\napproach to language modelling. The main idea is to use a\nneural network model to learn lower-dimensional representa-\ntions for discrete inputs while learning a discrete probability\ndistribution of words similarly to (Bengio et al., 2003; Mnih\n& Hinton, 2007). By contrast with the existing work, in-\nstead of encoding a word using a 1-of-V vector, each word\nis encoded using a random sparse high-dimensional vec-\ntor (Kanerva et al., 2000; Kanerva, 2009). This allows us\nto create neural-network models with a smaller number of\nparameters, but crucially, it also allows for more ﬂexible\npatterns to be represented and for a model to be trained with\na vocabulary of unknown size.\n3.1. Random Input Encoding\nThe distributed representations or embeddings in our model\nare k×msubspace, with k<< |V|(we have less base vec-\n4\nNeural Random Projections\ntors than words in the known dictionary). A representation\nfor each word as learned by the neural network is the linear\ncombination of (s×m)-dimensional basis where sis the\nnumber of non-zero entries for each vector. Each unique\nword is assigned a randomly generated sparse ternary vector\nwe refer to as random index vector. Random indices are\nsampled from the following distribution with mean 0 and\nvariance 1:\nri =\n\n\n\n+1 with probability α/2\n0 with probability 1 −α\n−1 with probability α/2\n(1)\nwhere αis the proportion of non-zero entries in the random\nindex vector. While similar to the distribution proposed in\n(Achlioptas, 2003), we use very sparse random vectors and\nno scaling factor. Since the input space is an orthogonal unit\nmatrix, the inner product between any two random vectors\n(ri,rj) is expected to be concentrated at 0. The motivation\nfor this is to make any discrete input indistinguishable from\nany other input in terms of distance between them. Instead\nof referring the sparseness parameter a, we will use sas the\nnumber of non-zero entries for randomly generated index\nvectors. This means that for s = 2 we have exactly one\nentry with value 1, one with value −1 and all the rest have\nvalue 0.\n−4 0 4\n# non−zero entries\n2\n8\n16\n32\n64\nFigure 1.Distribution of inner products between pairs of randomly\ngenerated 1000-dimensional random indices.\nFigure 1 shows the distribution of inner products for a\n1000 −kdimensional random indices. A larger number of\nnon-zero entries leads to a greater probability of a collision\nto occur, but the inner products remain well concentrated at\n0 – any two random sparse vectors are expected to be either\northogonal or near orthogonal.\nHaving more non-zero entries adds to the proabability of\nhaving collisions (same columns between two vectors hav-\ning non-zero values), which leads to larger inner products,\nnevertheless the inner product distribution does not deviate\nmuch from 0. We will see later that having random map-\npings with more non-zero entries is not necessarily bad as\nit adds to the robustness of the input encoding – making it\neasier for the neural network model to distinguish between\ndifferent input patters despite the collisions.\nIf we take the distributions in ﬁgure 1 as a proﬁle for inner\nproduct concentration for a random index with dimension\nk (in this case 1000), and we vary this k. We can see\nhow the inner product becomes more concentrated on 0 for\nhigher dimensional random indices (ﬁgure 2). In section\n4 we will show how different sizes of random indices and\nnumber of non-zero entries affect the predictive capabilities\nof our neural network language models. One of the goals is\nto ﬁgure out if neural networks can still make predictions\nbased on a compressed signal of its input, and to ﬁgure out if\nadding more non-zero entries (increasing the probability of\ncollisions) hurts performance. Another question we explore\nis the importance (or irrelevance) of using binary (ﬁgure 2b)\nvs ternary (ﬁgure 2a) random indices.\n3.2. Baseline Model Architecture\nIn order to assess the performance of our added random pro-\njection module, we build a baseline model without random\nprojections. The architecture for this model is based in the\none in (Bengio et al., 2003) but instead of predicting the\nprobability of each word directly we use the energy-based\ndeﬁnition from Mnih’s work (Mnih & Hinton, 2007). The\nbaseline model learns to predict the feature vector (embed-\nding) for a target word along with a probability distribution\nfor all the known words conditioned on the learned fea-\nture vectors of the input context words. Figure 3 shows an\noverview of our baseline model.\nThe baseline language model in ﬁgure 3 converts each word\ninto a vector of real-valued features using a feature lookup\ntable F. The goal is to learn a set of feature vectors inF that\nare good at predicting the feature vector for the target word\nwn. Each word in a context of sizen−1 is converted ton−1\nm-dimensional feature vectors that are then concatenated\ninto a (n−1) ×mvector and passed to the next layer h\nthat applies a non-linear transformation:\nhj = σ(Whf + bh) (2)\nwhere f is a vector of n−1 concatenated feature vectors,\nbis a bias for the layer hand σis a non-linear transforma-\ntion (e.g. sigmoid, hyperbolic tangent, or rectiﬁer linear\nunit (ReLU)). We found ReLU units (equation 3) to per-\nform well and at an increased training speed, so we use this\ntransformation in our models.\n5\nNeural Random Projections\n−5 0 5\nk\n4000\n3000\n2000\n1000\n(a) Ternary random indices\n0.0 2.5 5.0 7.5 10.0 12.5\nk\n4000\n3000\n2000\n1000 (b) Binary random indices\nFigure 2.Inner product distribution for different values of random index dimension k, aggregating the distributions for multiple numbers\nof non-zero entries. Ternary random indices 2a have entries with values ∈{−1, 0, 1}, while binary random indices have entries with\nvalues ∈{0, 1}.\n1\n2\nn -1\n... ...\nh\n1\n2\nn-1\nx\nn \nn\nF Lookup\nn\nŷ\nσ\n En\nFigure 3.Baseline feedforward energy model based on (Bengio\net al., 2003; Mnih & Hinton, 2007).\nf(x) =max(0,x) (3)\nThe result of the non-linear transformation in h is then\npassed to a linear layer ˆythat predicts the feature vector ˆfn\nfor the target word wn as follows:\nˆyn = Wˆyh+ bˆy (4)\nThe energy function Eis deﬁned as the dot product between\nthe predicted feature vector ˆfn, and the actual current feature\nvector for the target word fn:\nEn = ˆfT\nn fn (5)\nTo obtain a word probability distribution in the network out-\nput, the predicted feature vector is compared to the feature\nvectors of all the words in the known dictionary by com-\nputing the energy value (equation 5) for each word. This\nresults in a set of similarity scores which are exponentiated\nand normalised to obtain the predicted distribution for the\nnext word:\nP(wn|w1 : n−1) = exp(En)\n∑|V |\nj exp(Ej)\n(6)\nThe normalisation factor is the sum of the energy values\nfor all the words in the dictionary. Contrary to the work in\n(Mnih et al., 2009), we didn’t ﬁnd any advantage in adding\nbias values based on the frequencies of the known words in\nthe dictionary, so we use a pure energy-based deﬁnition.\nThe described baseline model can be though of as the stan-\ndard feedforward one, as deﬁned in (Bengio et al., 2003)\nwith an added linear layer ˆy of the same size as the em-\nbedding dimensions m. Furthermore, instead of using two\ndifferent sets of embeddings to encode context words and\ncompute the word probability distributions, we learn a single\nset of feature vectors F that are shared between input and\noutput layers to compute the energy values and consequently\nthe probability distribution over words.\n3.3. NRP Model Architecture\nWe incorporate our random projection encoding into the\npreviously deﬁned energy-based neural network language\nmodel following the formulation in (Mnih & Hinton, 2007;\nMnih et al., 2009). We chose this formulation because it\nallows us to use maximum-likelihood training and evaluate\n6\nNeural Random Projections\nour contribution in terms of perplexity. This makes it easier\nto compare it to other approaches that use the same intrin-\nsic measure of model performance. It is well known that\ncomputing the normalisation constant (the denominator in\nequation 6) is expensive, but we are working with relatively\nsimple models, so we compute this anyway. It should be\nnoted that the random projection deﬁnition is compatible\nwith much more economical approximation methods such as\nNoise Constrastive Estimation (NCE) (Mnih & Teh, 2012),\nand as such, we plan to test this method in the future along\nwith other evaluation methods.\nHaving deﬁned the baseline model, we now extend it with\na random projection lookup layer. The overview of the\nresulting architecture can be seen in ﬁgure 4.\nw1\nw2\nwn -1\nr1\nr2\nrn - 1\n∑\n∑\n∑\n... ... ...\nh\nŷ\nσ\nf1\nf2\nfn-1\nEn\nwn \n ∑\nfn^\nrn\nR Lookup\nfn\nx\nF Lookup\nFigure 4.Neural random projection feedforward energy model.\nThere are two major differences between this and the previ-\nous model. First of all, we include a random index lookup\ntable that generates a new random index for each new word\nthat it encounters. Tho get the feature representation for\na given word wi we multiply its k−dimensional random\nindex by the k×mfeature matrix F. The resulting vec-\ntor representation for a word wi is given by the sum of all\nthe feature vectors extracted for each non-zero entry in the\nrandom index. Much like in existing models, only the rows\ncorresponding to the non-zero entries are modiﬁed during\ntraining for each input sample. This adds to the computa-\ntional complexity of the ﬁrst layer since we are no longer\nselecting a single feature vector for each word, but it is still\nquite efﬁcient ,since for n−1 words in a context, the fea-\nture vectors for the context can be retrieved as single sparse\nmultiplication.\nThe second difference is that to compute the output prob-\nability normalisation constant, we can no longer compute\nthe dot product between the predicted feature vector ˆfn and\nthe feature matrix F as ˆfT\nn F because we no longer have\none per word. Instead, we must ﬁrst ﬁnd the feature vectors\nfor all the words by multiplying all the random indexes for\nall the words stored in the random index lookup Rby F\nas F′= RT Rand then ﬁnd the normalisation constant by\ncomputing the dot product between the predicted feature\nvector and F′. The probability normalisation function is\nthus given by:\n|V |∑\nj\nexp( ˆfT\nj rj), (7)\nwhere rj (as we can see in ﬁgure 4) is the sparse random\nindex for word wj. While all these operations can be imple-\nmented using sparse matrix multiplications, the computa-\ntional complexity of each training or inference step scales\nlinearly with the number of non-zero entries in the random\nindices s. As we stated previously, techniques like NCE,can\nalleviate the problem since the issue lies in the fact that\nwe need to compute the partition term to learn an output\nprobability distribution.\nThe fact that we can learn features for a vocabulary incre-\nmentally does not eliminate the fact that we need to compute\nthe normalisation constant or the fact that the model does\nnot deal with out-of-vocabulary words. Nevertheless, we\nare adding to neural network models, is the possibility of\nbuilding predictive models based on a compression of the\ninput space and enabling the possibility for the encoding\nof more complex (and possibly structured) inputs using\nrandom projections similarly to (Basile et al., 2011).\nSince all models output probability distributions, they are\ntrained using a maximum likelihood criterion:\nL= 1\nT\n∑\nt\nlog P(wt,wt −1,...,w t −n+ 1;θ) (8)\nWhere θis the set of parameters learned by the neural net-\nwork in order to maximise a corpus likelihood.\nIn the following section, we present our experimental setup.\nWe start by describing the used dataset, evaluation metrics,\nalong with a brief description of the hyper-parameters found\nin exploratory model ﬁne-tuning. We proceed to show how\nrandom index encoding inﬂuences model performance and\ncompare the model with the previously described baseline.\n4. Experimental Results\n4.1. Experimental Setup\nWe evaluate our models using a subset of thePenn Treebank\ncorpus, more speciﬁcally, the portion containing articles\nfrom the Wall Street Journal (Marcus et al., 1993). We use a\ncommonly used set-up for this corpus, with pre-processing\nas in (Mikolov et al., 2011) or (Kim et al., 2016), resulting\nin a corpus with approximately 1 million tokens (words) and\na vocabulary of size |V|= 10K unique words The dataset\nis divided as follows: sections 0-20 were used as training\n7\nNeural Random Projections\ndata (930K tokens), sections 21-22 as validation data (74K\ntokens) and 23-24 as test data ( 82K tokens). All words\noutside the 10K vocabulary were mapped to a single token\n<unk >. We train the models by dividing the PTB corpus\ninto n-gram samples (sliding windows) of size n. All the\nmodels here reported were 5 −textg−ram models. To\nspeed up training and evaluation, we batch the samples in\nmini-batches of size 128 and the training set is randomised\nprior to batching.\nWe train the models by using thecross entropy loss function.\nCross entropy measures the efﬁciency of the optimal encod-\ning for the approximating (model predicted) distribution q\nunder the true (data) distribution p:\nH(p,q) =−\n∑\nx\np(x)log(q(x)) (9)\nWe evaluate and compare the models based on their perplex-\nity:\nPPL = exp\n(\n−1\nN\n∑\nw1:n\nlogP(wn|w1:n−1)\n)\n(10)\nwhere the sum is performed over all the n-gram windows of\nsize n. Another way to formulate perplexity is as follows:\n2H(p,q) = 2−∑\nx p(x)log(q(x))1 (11)\nPerplexity is essentially a geometric average of inverse prob-\nabilities; it measures the predictability of some distribu-\ntions, given the number of possible outcomes of an equally\n(un)predictable distribution, if each of those outcomes was\ngiven the same probability. (Example: if a distribution has\nperplexity 6, it is as unpredictable as a fair dice. Since all\nthe models are trained using mini-batches, all perplexities\nare model perplexities, this is, the average perplexity for all\nthe batches.)\nAlong with model performance, we also report the approxi-\nmate number of trainable parameters of each model conﬁg-\nuration which can be computed as:\n#p= (|x|×m) + ((m×(n−1))×h+h) + (h×m+m)\nwhere |x|s the dimension of input vectors, mis the dimen-\nsion of the feature lookup layer (or embeddings), nis the\nn-gram size (in this case 5), and his the number of hidden\nunits.\n1If ln is used instead of log to compute the cross-entropy,\nperplexity must be computed as eH(p,q) instead.\nWe train our models using Stochastic Gradient Descent\n(SGD) without momentum. In early experiments, we found\nthat SGD produced overall better models and generalised\nbetter than adaptive optimisation procedures such as ADAM\n(Kingma & Ba, 2014). Model selection and early stopping\nis based on the perplexity measured on the validation set.\nWe used a step-wise learning rate annealing ,where the\nlearning rate is kept ﬁxed during a single epoch, but we\nshrink it by0.5 every time the validation perplexity increases\nat the end of a training epoch. If the validation perplexity\ndoes not improve, we keep repeating the process until a\ngiven number of epochs have passed without improvement\n(early stop). The patience parameter (number of epochs\nwithout improvement before stopping) is set to 3 and it\nresets if we ﬁnd a lower validation perplexity at the end of\nan epoch. We consider that the model converges when it\nstops improving and the patience parameter reaches 0\nAs for regularisation, dropout (Srivastava et al., 2014) and\nℓ2 weight penalty performed similarly. We opted for using\ndropout regularisation in all the models. Dropout is applied\nto all weights including the feature lookup (embeddings).\nIn early experiments, we used a small dropout probability\nof 0.05 following the methodology in (Pham et al., 2016)\n(where the authors use smaller dropout probabilities due to\nthe Penn Treebank corpus being smaller. We later found\nthat,dropout probabilities interact non-linearly with other\narchitectural parameters such as embedding size and number\nof hidden units. Higher dropout probability values can be\nused (and give us better models) in conjunction with larger\nembedding and hidden layer sizes.\nWe found that Rectifying Linear Units (ReLU) activation\nunits (He et al., 2015) performed better than saturating non-\nlinear functions like hyperbolic tangent, or alternatives like\nExponential Linear Units (ELUs) (Clevert et al., 2015). Fig-\nure 5 shows the distribution of test perplexity for models\nusing random projections with different activation functions\non the non-linear projection layer. Overall, ELUs are the\nmost sensitive to larger dropout regularisation and didn’t\nproduce signiﬁcantly better results than the models using\nthe hyperbolic tangent. Rectiﬁer linear units seem to outper-\nform other types of non-linearities both in our baseline and\nthe models using random projections (NRP).\nOne problem encountered when using ReLU units was that\nthey caused gradient values toexplode easily during training.\nTo mitigate this, we added local gradient norm clipping:\neach time a gradient norm is greater than a given threshold\n(in our case 1.0) we clip the gradient to the threshold value.\nClipping gradients by global norm resulted in worse models,\nso we clip their value by their individual norms. Gradient\nclipping allowed us to start with higher learning rate values.\nSetting the initiallearning rate to 0.5 yielded the best results,\nboth in terms of model validation perplexity and number of\n8\nNeural Random Projections\n150\n200\n250\nelu relu tanh\nhidden layer activation\nPerplexity\ndropout prob.\n0.05\n0.25\n0.5\nFigure 5.Average test perplexity of best NRP models for different\nhidden activation functions for a feature lookup size (embeddings)\nm = 128, a number of hidden units h = 256, random indices of\ndimension k = 5000and number of non-zero units s = 8\nepochs for convergence.\nAll network weights are initialised randomly on a range\n[−0.01,0.01] in a random uniform fashion. Bias values are\ninitialised with 0. We found it beneﬁcial to initialise the\nnon-linear layers with the procedure described in (He et al.,\n2015) for the layers using ReLU units.\nWe performed multiple experiments using different sizes\nfor both feature vectors (embeddings) and hidden layer size,\nbut larger feature vector spaces do not lead to better models\nunless more aggressive regularisation is applied (higher\ndropout probability).\n4.2. NRP properties: initial experiments\nUpon selecting a good set of hyper-parameters, we initially\nexplored the effects of different random index (RI) conﬁg-\nurations in the model. This inﬂuence is measured in terms\nof perplexity (less is better). In these ﬁrst experiments, the\nembedding size is set to m = 128 and the hidden layer\nto h = 256, while the dropout probability is also ﬁxed to\n0.05. We span three different parameters, varying the em-\nbedding size m∈{128,256,512,1024}, the RI dimension\nk ∈[1000,10000] and the RI number of non-zero entries\ns ∈{2,8,16}. We run each model to convergence (early\nstop using validation perplexity). We save the best mod-\nels upon convergence based on validation perplexity and\npresent the perplexity scores on the test set.\nFigure 6 shows the results for the average test perplexity\nscores aggregated by the number of non-zero entries in\nthe RIs. The perplexity score follows an exponential de-\ncay as the RI dimension k increases. From k >= 5000,\nthe perplexity values converge to an approximate range\nPPL ≈[170,180] (see table 1). This is worse than the best\nbaseline model, but we subsequently found out that a more\naggressive regularisation process improves both the NRP\nand the baseline models.\n●\n●\n●\n●\n●\n● ●\n●\n● ● ● ●\n● ● ● ● ● ● ●\nbaseline\n140\n160\n175\n200\n225\n250\n275\n300\n325\n350\n375\n400\n420\n2500 5000 7500 10000\nRI dimension (k)\nAvg. Perplexity\nembedding dim.\n● 128\n256\n512\n1024\nFigure 6.Average test perplexity of best models selected based\non validation perplexity for multiple embedding sizes m ∈\n{128, 256, 512, 1024}aggregated by number of non-zero entries\ns ∈{2, 8, 16}. The number of hidden units was set to 256.\nIncreasing the size of the embedding features mseems to\nyield worse perplexity. We found that models with more\nparameters where overﬁtting the training set, thus explaining\nthe worse results. More aggressive regularisation (higher\ndropout probabilities) seems to alleviate the problem, but\nfor embedding and hidden layer sizes of 1024 there is either\nno improvement, or the improvement is marginal.\nTable 1.Average test perplexity for the best models selected based\non validation perplexity aggregated by number of active units (see\nﬁgure 6). The number of hidden units was set to 256.\nPPL Epoch\nk m #p Avg. SD Avg. SD\n5,000 128 0 .8 182 0 .2 12 2 .0\n256 1 .6 190 5 .9 9 1 .0\n512 3 .2 198 9 .6 8 0 .0\n1,024 6.3 191 2 .9 8 0 .0\n7,500 128 1 .1 168 3 .2 12 0 .5\n256 1 .6 176 7 .6 8 0 .0\n512 3 .2 176 5 .2 7 0 .5\n1,024 8.9 178 4 .6 7 1 .2\n10,000 128 1 .4 164 1 .4 13 2 .5\n256 2 .8 167 8 .1 8 0 .0\n512 5 .7 172 12 .6 8 0 .6\n1,024 11 177 8 .4 7 1 .0\nbaseline\n10,000 128 1 .4 141 1 .4 14 1 .1\nWe then looked at the inﬂuence of number of non-zero\n9\nNeural Random Projections\nentries sin the random index vectors for the NRP models, by\naggregating the results by embedding size. Figure 7 shows\nthat different values of s do not affect the performance\nsigniﬁcantly. We should note that this parameter does affect\nthe model performance in terms of perplexity (although\nmarginally) and should be optimised if the goal is to produce\nthe best possible model for a given data set.\n●\n●\n●\n●\n●\n●\n● ●\n● ● ● ●\n● ● ● ● ● ● ●\nbaseline\n140\n166\n200\n225\n250\n275\n300\n325\n350\n375\n400\n430\n2500 5000 7500 10000\nRI dimension (k)\nAvg. Perplexity\n# non−zero entries\n● 2\n8\n16\nFigure 7.Average test perplexity of the best models selected based\non validation perplexity for random indices with a number of\nnon-zero entries s ∈{2, 8, 16}aggregated by embedding size\nm ∈{128, 256, 512, 1024}. The number of hidden units was set\nto 256.\nThe number of epochs to convergence is correlated with\nthe number of trainable parameters (as we will see when\ncomparing these early explorations with later results), but\nit does not differ signiﬁcantly between NRP and baseline\nmodels (see tables 1 and 2).\nFinally, in our early explorations, we looked at two different\nquestions. First, we wanted to ﬁnd out if there was a dif-\nference in performance between NRP models using ternary\nrandom indices with values ∈{−1,0,1}) and models using\nbinary random indices (with values ∈{0,1}). Second, we\nwanted to ﬁnd if redundancy (having more non-zero entries)\nis key to NRP model performance. To answer this second\nquestion, we tested models that used random indices with a\nsingle non-zero value. Under this scenario, different words\ncan share the same feature vector. Figure 8 shows the results\nfor this experiment, in which we tested models with these 3\nconﬁgurations.\nWe can see that models having a single non-zero positive en-\ntry s= 1do much worse than any other conﬁguration. Hav-\ning just two non-zero entries s>= 2seems to be enough to\nimprove the model perplexity. A second result we got from\nthis experiment was that using binary and ternary random\nindices yields similar results, with binary indices display-\ning an increased variance for lower values of random index\ndimension k. We think this increase in variance is due to\nTable 2.Average test perplexity of the best models selected based\non validation perplexity, aggregated by embedding size m (see\nﬁgure 7) for multiple values of input vector dimension k and\nnumber of non-zero input values s. The number of hidden units\nwas set to 256. Dropout probability is set to 0.05. #p is the\napproximate number of trainable parameters in millions.\nPPL Epoch\nk s #p Avg. SD Avg. SD\n5,000 2 0 .8 186 3 .4 10 3 .3\n8 189 7 .8 9 1 .9\n16 195 10 .0 9 1 .5\n7,500 2 1 .1 170 3 .8 9 2 .5\n8 175 6 .2 9 3 .1\n16 179 6 .0 9 2 .5\n10,000 2 1 .4 165 1 .8 9 3 .5\n8 168 9 .8 8 1 .2\n16 177 8 .7 9 3 .1\nbaseline\n10,000 1 1 .4 141 1 .4 14 1 .1\nthe fact that lower-dimensional binary random indices have\na higher probability of collisions and thus disambiguation\nof such collisions becomes harder for our neural network\nmodels. As for why the results are similar, we should note\nthat the near-orthogonality properties of random indices are\nstill maintained whether or not we use ternary representa-\ntions. Moreover, the feature layer (or embeddings) is itself\nsymmetric, having equally distributed positive and negative\nweights, so the symmetry in the input space should not have\nmuch impact on the model performance.\n4.3. The role of regularisation\nIn the next set of experiments, we look at the role ofdropout\nprobability in both the baseline and NRP models. Figure\n9 shows the test perplexity scores for the baseline model\nusing different embedding sizes mand dropout probability\nvalues p= {0.05,0.25,0.3}. The number of hidden units\nis ﬁxed at h= 256.\nWe can see that keeping the dropout probability at the previ-\nously used value 0.05 makes the model perform worse for\nembedding size values m> 128. We found out that larger\nmodels were heavily overﬁtting the training set, which ex-\nplains why more aggressive dropout rates give us better\nresults for larger feature spaces. After extensive testing of\nthis hypothesis and parameter tuning, we recommend the\nuse of higher dropout when increasing the size of the hidden\nlayer (ﬁgure 10, and table 3).\nLooking at the results for NRP models (ﬁgure 11, table 4),\nwe can see that the best results are achieved with larger mod-\n10\nNeural Random Projections\n●\n●\n●\n● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●baseline150\n250\n350\n450\n550\n650\n750\n850\n950\n1050\n1150\n1250\n2500 5000 7500 10000\nRI dimension (k)\nAvg. Perplexity\nnon−zero values\n●● {−1,+1}  s = {2,8,16}\n{+1}      s = {1}\n{+1}      s = {2,8,16}\nFigure 8.Average test perplexity of best models selected based on\nvalidation perplexity for random indices constructed from typical\nternary random indices with non-zero entries having values ∈\n{+1, 1}; random indices with a single positive non-zero entry\n{+1}s ∈{1}; and binary random index vectors with non-zero\nentries ∈{+1}.\nels and higher dropout probability. We should mention that\neven with half the embedding parameters (using random\nindices of dimension k= 5kto encode a vocabulary of 10k\nwords), we get reasonably good models with test perplexi-\nties of 145 to 150 using only, 1.6 to 1.9 million parameters\n(table 4). We can compare this with the perplexity scores\nobtain in (Pham et al., 2016) when using the same dataset\nand partitions. The model from (Bengio et al., 2003) with\napproximately 4.5 million parameters gets a test perplexity\nof 147 to 152. This is of course not a fair comparison, be-\ncause we would need to optimise both models using the best\npossible hyper-parameter values and regularisation methods,\nbut it does conﬁrm our main hypothesis: that we can we\ncan learn language models using a random projection of the\ninput space.\nTable 3.Average test perplexity and convergence epoch for differ-\nent conﬁgurations of the baseline model with multiple values of\nembedding size m, number of hidden units h and weight dropout\nprobability drop. #p is the approximate number of trainable pa-\nrameters in millions.\nPPL Epoch\nm h #p drop Avg. SD Avg. SD\n256 256 2 .8 .25 125 0 .3 22 1 .5\n.40 128 1 .8 22 2 .5\n512 3.2 .25 122 0 .3 19 2 .0\n.40 121 0 .9 23 1 .6\n512 256 5 .8 .25 126 0 .3 20 1 .6\n.40 125 1 .3 22 1 .9\n512 6.4 .25 125 1 .0 16 1 .8\n.40 118 0.4 22 1 .3\n●\n●●\n●\n●\n●\n●●\n●\n130\n140\n150\n64 128 256 512 1024\nm\nPerplexity\ndropout prob.\n0.05\n0.25\n0.3\nFigure 9.Baseline model test perplexity of best models selected\nbased on validation perplexity for multiple values of embedding\nsize m and dropout probability.\n●\n●\n●\nh dim. = 256 h dim. = 512\n256 512 256 512\n117.5\n120.0\n122.5\n125.0\n127.5\n130.0\nm dim.\nPerplexity\ndropout prob.\n0.25\n0.4\nFigure 10.Baseline model test perplexity of best models selected\nbased on validation perplexity for multiple values of embedding\nsize m, hidden layer number of units h and dropout probability.\nAdditional tests showed us that we cannot go much further\nwith dropout probabilities, so we stuck to a maximum value\nof 0.4, since it yielded the best results for both the baseline\nand models using random projections. One weakness in\nour methodology was that hyper-parameter exploration was\ndone based on grid search and intuition about model be-\nhaviour – mostly because each model run is very expensive.\nWhile our goal is not to ﬁnd the best possible baseline and\nNRP models, but to show that we can achieve qualitatively\nsimilar results with random projections and a reduced em-\nbedding space, in future work this methodological aspect\nwill be improved by replacing grid search by a more appro-\npriate method such as Bayesian Optimisation (Snoek et al.,\n2012). This way, we can compare the best possible models,\nsince many hyper-parameters should be tuned to particular\nmodel architectures.\n11\nNeural Random Projections\n●\n●\n●\n●\n●\n●\n●\n●\nk = 5000\nh dim. = 256\nk = 5000\nh dim. = 512\nk = 7500\nh dim. = 256\nk = 7500\nh dim. = 512\n256 512 256 512 256 512 256 512\n130\n140\n150\n160\nm dim.\nPerplexity\ndropout prob.\n0.25\n0.4\nFigure 11.NRP model test perplexity of best models selected\nbased on validation perplexity for multiple values of random index\nsize k, embedding size m, hidden layer number of units h and\ndropout probability.\nTable 4.Average test perplexity and convergence epoch for differ-\nent conﬁgurations of NRP models with multiple values of random\nindex dimension k embedding size m, number of hidden units h\nand weight dropout probability drop. The number of non-zero\nentries in the RI is set fo 4. #p is the approximate number of\ntrainable parameters in millions.\nPPL Epoch\nk m h #p drop Avg. SD Avg. SD\n5,000 256 256 1 .6 .25 149 0 .6 23 1 .2\n.40 156 3 .5 25 3 .4\n512 1.9 .25 145 0 .8 22 1 .7\n.40 147 0 .9 25 1 .1\n512 256 3 .2 .25 150 0 .4 21 1 .5\n.40 151 0 .9 24 0 .9\n512 3.8 .25 149 0 .8 19 2 .3\n.40 142 0.9 24 1 .0\n7,500 256 256 2 .2 .25 136 0 .5 22 1 .3\n.40 140 1 .9 26 2 .9\n512 2.5 .25 132 0 .3 21 0 .8\n.40 133 1 .6 24 2 .3\n512 256 4 .4 .25 137 0 .8 21 0 .8\n.40 137 0 .8 24 1 .7\n512 5.1 .25 137 0 .8 18 1 .8\n.40 129 1.2 23 1 .5\n4.4. Best model approximation\nFinally, we analyse the performance of an NRP model with\nthe best conﬁguration we found in previous experiments,\nand compare it with the best baseline model found. We\nperformed multiple runs with different values of random\nindex dimension k. The results can be seen in ﬁgure 12.\n●\n●\n●\n●\n●\n●\n●\n●\n● ● ● ● ● ● ● ● ● ● ● ●\nbaseline100\n200\n300\n400\n500\n2500 5000 7500 10000\nRI dimension (k)\nAvg. Perplexity\nmodel\n●\n●\nbaseline\nnrp\nFigure 12.Average test perplexity for the best NRP and baseline\nconﬁgurations found (see tables 3, and 4) with m = h = 512\nand dropout probability 0.6 for multiple NRP random index vector\ndimension values k.\nAs with our early experiments, the perplexity decays expo-\nnentially with the increase of the random index dimension\nk. One key difference is that this decay is much more accen-\ntuated and model test perplexity converges to values closer\nto the baseline, with lower-dimensional random indices. In\nsummary, random projections seem to be a viable technique\nto build models with comparable predictive power, with a\nmuch lower number of parameters and without the necessity\nof a priori knowledge of the dimension of the lexicon to be\nused. Proper regularisation and model size tuning was key\nto achieve good results. NRP models are also more sensitive\nto hyper-parameter values and model size than the baseline\nmodel. This was an expected result, since the problem of\nestimating distributed representations using a compressed\ninput space is harder than using dedicated feature vectors\nfor each unique input.\n5. Conclusion and future work\nIn this work ,we investigated the potential of using random\nprojections as an encoder for neural network models for lan-\nguage modelling. The results extend beyond this sequence\nprediction task and can applicable to any neural-network-\nbased modelling problem that involves the modelling of\ndiscrete input patterns.\nWe showed that using random projections allows us to obtain\nperplexity scores comparable to a model that uses an1-of-V\nencoding, while reducing the number of necessary trainable\nparameters. On top of this, our approach opens multiple\ninteresting research directions.\nRandom projections introduce additional computational\ncomplexity in the output layer ,because to obtain the output\nprobabilities we need the vector representations for all the\n12\nNeural Random Projections\nexisting words –and these representations are a combina-\ntion of multiple features. The problem only arises because\nwe need to output a probability distribution over words,.\nHowever,our approach is compatible with approximation\ntechniques such as Noise Constrastive Estimation (NCE)\n(Mnih & Teh, 2012)., with beneﬁts that we will demonstrate\nin a short while. Another future research direction is to\nanswer the question of whether or not more powerful neural\nnetwork architectures (such RNNs or LSTMs) can yield bet-\nter results using random projections –possibly using even\nsmaller random index dimensions and embedding spaces.\nWe have introduced a new simple baseline on the small, but\nwidely used Penn Treebank dataset. Our baseline model\ncombined the energy-based principles from the work in\n(Mnih & Hinton, 2007) with the simple feedforward semi-\nnal neural network architecture proposed in (Bengio et al.,\n2003), and with a recent regularisation technique (dropout\n(Srivastava et al., 2014)). Model performance and training\nspeed were also improved by using rectiﬁer linear units\n(ReLUs) in the hidden layers, similarly to the baseline pro-\nposed in (Pham et al., 2016). The result was a very simple\narchitecture with a low number of parameters, and a perfor-\nmance comparable to similar models in the literature. We\ntested our random projection encoder in an energy-based\nneural network architecture, because this would allow us\nto extend this work beyond the prediction of vocabulary\ndistributions. The fact that we can predict distributed repre-\nsentations,means that we can encode –and make predictions\nabout– more complex patterns. In future work, we intend\nto explore whether or not we can use random projections to\nexploit sequence information, linguistic knowledge, or struc-\ntured input patterns similarly to the approaches in (Plate,\n1995; Cohen et al., 2009; Basile et al., 2011).\nReferences\nAchlioptas, Dimitris. Database-friendly random projections:\nJohnson-lindenstrauss with binary coins. Journal of com-\nputer and System Sciences, 66(4):671–687, 2003.\nArisoy, Ebru, Sainath, Tara N., Kingsbury, Brian, and Ram-\nabhadran, Bhuvana. Deep neural network language mod-\nels. In Proceedings of the NAACL-HLT 2012 Workshop:\nWill We Ever Really Replace the N-gram Model? On\nthe Future of Language Modeling for HLT , pp. 20–28.\nAssociation for Computational Linguistics, 2012.\nBasile, Pierpaolo, Caputo, Annalina, and Semeraro, Gio-\nvanni. Encoding syntactic dependencies by vector permu-\ntation. In Proceedings of the GEMS 2011 Workshop on\nGeometrical Models of Natural Language Semantics, pp.\n43–51. Association for Computational Linguistics, 2011.\nBengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and\nJanvin, Christian. A neural probabilistic language model.\nThe Journal of Machine Learning Research , 3:1137–\n1155, 2003.\nBrown, Peter F., Desouza, Peter V ., Mercer, Robert L.,\nPietra, Vincent J. Della, and Lai, Jenifer C. Class-based\nn-gram models of natural language. Computational lin-\nguistics, 18(4):467–479, 1992.\nClevert, Djork-Arn´e, Unterthiner, Thomas, and Hochreiter,\nSepp. Fast and Accurate Deep Network Learning by\nExponential Linear Units (ELUs). arXiv:1511.07289\n[cs], November 2015.\nCohen, Trevor, Schvaneveldt, Roger W, and Rindﬂesch,\nThomas C. Predication-based semantic indexing: Per-\nmutations as a means to encode predications in semantic\nspace. In AMIA Annual Symposium Proceedings, volume\n2009, pp. 114. American Medical Informatics Associa-\ntion, 2009.\nDauphin, Yann N., Fan, Angela, Auli, Michael, and Grang-\nier, David. Language modeling with gated convolutional\nnetworks. arXiv:1612.08083 [cs], 2016.\nElman, Jeffrey L. Finding structure in time. Cognitive\nscience, 14(2):179–211, 1990.\nFeyerabend, Paul. Wittgenstein’s Philosophical Investiga-\ntions. The Philosophical Review, 64(3):449, July 1955.\nGribonval, R´emi, Jenatton, Rodolphe, and Bach, Francis.\nSparse and spurious: Dictionary learning with noise and\noutliers. IEEE Transactions on Information Theory, 61\n(11):6298–6319, 2015.\nGutmann, Michael U. and Hyv ¨arinen, Aapo. Noise-\ncontrastive estimation of unnormalized statistical models,\nwith applications to natural image statistics. Journal of\nMachine Learning Research, 13:307–361, 2012.\nHalko, Nathan, Martinsson, Per-Gunnar, and Tropp, Joel A.\nFinding structure with randomness: Probabilistic algo-\nrithms for constructing approximate matrix decomposi-\ntions. SIAM review, 53(2):217–288, 2011.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Delving deep into rectiﬁers: Surpassing human-level\nperformance on imagenet classiﬁcation. In Proceedings\nof the 2015 IEEE ICCV, pp. 1026–1034. IEEE Computer\nSociety, 2015. doi: 10.1109/ICCV .2015.123.\nHegde, Chinmay, Indyk, Piotr, and Schmidt, Ludwig. Fast\nrecovery from a union of subspaces. In Advances in\nNeural Information Processing Systems, pp. 4394–4402,\n2016.\nHinton, Geoffrey E. Learning distributed representations\nof concepts. In Proceedings of the eighth annual confer-\nence of the cognitive science society, volume 1, pp. 12.\nAmherst, MA, 1986.\n13\nNeural Random Projections\nIndyk, P. Algorithmic aspects of low-distortion geometric\nembeddings. In Annual symposium on foundations of\ncomputer science (FOCS), 2001.\nJohnson, William B and Lindenstrauss, Joram. Extensions\nof lipschitz mappings into a hilbert space. Contemporary\nmathematics, 26(189):1, 1984.\nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the Limits of Lan-\nguage Modeling. arXiv preprint arXiv:1602.02410, 2016.\nKanerva, Pentti. Sparse distributed memory. MIT press,\n1988.\nKanerva, Pentti. Hyperdimensional computing: An intro-\nduction to computing in distributed representation with\nhigh-dimensional random vectors. Cognitive Computa-\ntion, 1(2):139–159, 2009.\nKanerva, Pentti, Kristofersson, Jan, and Holst, Anders. Ran-\ndom indexing of text samples for latent semantic analysis.\nIn Proceedings of the 22nd annual conference of the cog-\nnitive science society, 2000.\nKaski, Samuel. Dimensionality reduction by random map-\nping: fast similarity computation for clustering. In Neu-\nral Networks Proceedings, volume 1, pp. 413–418, May\n1998.\nKim, Yoon, Jernite, Yacine, Sontag, David, and Rush,\nAlexander M. Character-aware neural language models.\nIn AAAI, 2016.\nKingma, Diederik P. and Ba, Jimmy. Adam: A method\nfor stochastic optimization. In Proceedings of the 3rd\nICLR, 2014. URL http://arxiv.org/abs/1412.\n6980.\nKneser, Reinhard and Ney, Hermann. Improved backing-off\nfor m-gram language modeling. InAcoustics, Speech, and\nSignal Processing, 1995. ICASSP-95., 1995 International\nConference on, volume 1, pp. 181–184. IEEE, 1995.\nKoehn, Philipp. Statistical machine translation . Cam-\nbridge University Press, 2010. ISBN 0521874157,\n9780521874151.\nLandauer, Thomas K. and Dumais, Susan T. A solution to\nPlato’s problem: The latent semantic analysis theory of\nacquisition, induction, and representation of knowledge.\nPsychological review, 104(2):211, 1997.\nLecun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato,\nMarc Aurelio, and Huang, Fu Jie. A tutorial on energy-\nbased learning. In Predicting structured data. MIT Press,\n2006.\nLevy, Omer, Goldberg, Yoav, and Ramat-Gan, Israel. Lin-\nguistic Regularities in Sparse and Explicit Word Repre-\nsentations. In CoNLL, pp. 171–180, 2014.\nLing, Yulia Tsvetkov Manaal Faruqui Wang and Dyer, Guil-\nlaume Lample Chris. Evaluation of word vector repre-\nsentations by subspace alignment. In Proceedings of\nthe 2015 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 2049–2054, 2015.\nMarcus, Mitchell P., Marcinkiewicz, Mary Ann, and San-\ntorini, Beatrice. Building a large annotated corpus of\nEnglish: The Penn Treebank. Computational linguistics,\n19(2):313–330, 1993.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg,\nand Dean, Jeffrey. Distributed representations of words\nand phrases and their compositionality. In Proceedings of\nthe 26th International Conference on Neural Information\nProcessing Systems - Volume 2, NIPS’13, pp. 3111–3119,\nUSA, 2013a. Curran Associates Inc.\nMikolov, Tom´aˇs, Deoras, Anoop, Kombrink, Stefan, Burget,\nLuk´aˇs, and ˇCernock`y, Jan. Empirical evaluation and\ncombination of advanced language modeling techniques.\nIn Twelfth Annual Conference of the International Speech\nCommunication Association, 2011.\nMikolov, Tom´aˇs, Chen, Kai, Corrado, Greg, and Dean, Jef-\nfrey. Efﬁcient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781, 2013b.\nMikolov, Tom ´aˇs, Sutskever, Ilya, Deoras, Anoop, Le,\nHai-Son, Kombrink, Stefan, and Cernocky, Jan. Sub-\nword language modeling with neural networks. preprint\n(http://www. ﬁt. vutbr. cz/imikolov/rnnlm/char. pdf), 2012.\nMnih, Andriy and Hinton, Geoffrey. Three new graphical\nmodels for statistical language modelling. In Proceedings\nof the 24th international conference on Machine learning,\npp. 641–648. ACM, 2007.\nMnih, Andriy and Teh, Yee Whye. A fast and simple al-\ngorithm for training neural probabilistic language mod-\nels. arXiv preprint arXiv:1206.6426, 2012. URL http:\n//arxiv.org/abs/1206.6426.\nMnih, Andriy, Yuecheng, Zhang, and Hinton, Geoffrey.\nImproving a statistical language model through non-linear\nprediction. Neurocomputing, 72(7-9):1414–1418, March\n2009. ISSN 09252312.\nMorin, Frederic and Bengio, Yoshua. Hierarchical proba-\nbilistic neural network language model. In Proceedings\nof the international workshop on artiﬁcial intelligence\nand statistics, pp. 246–252, 2005.\n14\nNeural Random Projections\nPapadimitriou, Christos H., Tamaki, Hisao, Raghavan, Prab-\nhakar, and Vempala, Santosh. Latent semantic indexing:\nA probabilistic analysis. In Proceedings of the Seven-\nteenth ACM SIGACT-SIGMOD-SIGART Symposium on\nPrinciples of Database Systems, PODS ’98, pp. 159–168.\nACM, 1998.\nPham, Ngoc-Quan, Kruszewski, German, and Boleda,\nGemma. Convolutional Neural Network Language Mod-\nels. In EMNLP, pp. 1153–1162, 2016.\nPinker, Steven. The cognitive niche: Coevolution of intel-\nligence, sociality, and language. In Proceedings of the\nNational Academy of Sciences , volume 107, pp. 8993–\n8999, 2010.\nPlate, Tony A. Holographic reduced representations. IEEE\nTransactions on Neural networks, 6(3):623–641, 1995.\nPonte, Jay M and Croft, W Bruce. A language modeling\napproach to information retrieval. In Proceedings of\nthe 21st annual international ACM SIGIR conference\non Research and development in information retrieval ,\npp. 275–281. ACM, 1998.\nRitter, Helge and Kohonen, Teuvo. Self-organizing semantic\nmaps. Biological cybernetics, 61(4):241–254, 1989.\nSerban, Iulian V ., Sordoni, Alessandro, Bengio, Yoshua,\nCourville, Aaron, and Pineau, Joelle. Building end-to-\nend dialogue systems using generative hierarchical neural\nnetwork models. arXiv:1507.04808 [cs], pp. 3776–3784,\n2016.\nSnoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Pract-\ncal bayesian optimization of machine learning algorithms.\nIn Advances in neural information processing systems ,\npp. 2951–2959, 2012.\nSocher, Richard, Lin, Cliff C, Manning, Chris, and Ng,\nAndrew Y . Parsing natural scenes and natural language\nwith recursive neural networks. InProceedings of the 28th\ninternational conference on machine learning (ICML-11),\npp. 129–136, 2011.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\nA simple way to prevent neural networks from overﬁtting.\nJournal of Machine Learning Research, 15:1929–1958,\n2014.\nVaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszko-\nreit, Jakob, Jones, Llion, Gomez, Aidan N., Kaiser,\nLukasz, and Polosukhin, Illia. Attention Is All You Need.\narXiv:1706.03762 [cs], June 2017.\nZhang, Xiang and LeCun, Yann. Text understanding from\nscratch. arXiv preprint arXiv:1502.01710, 2015. URL\nhttp://arxiv.org/abs/1502.01710.\n15"
}