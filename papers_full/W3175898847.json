{
  "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA",
  "url": "https://openalex.org/W3175898847",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2969024387",
      "name": "Zewen Chi",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2612910427",
      "name": "Shaohan Huang",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2099570760",
      "name": "Shuming Ma",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2100574101",
      "name": "Bo Zheng",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A3043020393",
      "name": "Saksham Singhal",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2517586992",
      "name": "Payal Bajaj",
      "affiliations": [
        "Microsoft (Finland)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2102676534",
      "name": "Xia Song",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A4208379621",
      "name": "Xian-Ling Mao",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2101832271",
      "name": "Heyan, Huang",
      "affiliations": [
        "Microsoft (Finland)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft (Finland)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3094300367",
    "https://openalex.org/W3169425228",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3105813095",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W3092973241",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W3168481568",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W3177035927",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3105492289",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W3051275803",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W2963877297",
    "https://openalex.org/W4288284086",
    "https://openalex.org/W2970413168",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W630532510",
    "https://openalex.org/W3185293939",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W3118942129",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3097879195",
    "https://openalex.org/W3170865226",
    "https://openalex.org/W4301187301",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W3105425516",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3199404008",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2960374072",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3186903869",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2742113707"
  ],
  "abstract": "Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, Furu Wei. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6170 - 6182\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nXLM-E: Cross-lingual Language Model Pre-training via ELECTRA\nZewen Chi†∗, Shaohan Huang ‡∗, Li Dong ‡, Shuming Ma ‡, Bo Zheng ‡, Saksham Singhal‡\nPayal Bajaj‡, Xia Song ‡, Xian-Ling Mao †, Heyan Huang †, Furu Wei ‡\n†Beijing Institute of Technology\n‡Microsoft Corporation\nhttps://github.com/microsoft/unilm\nAbstract\nIn this paper, we introduce ELECTRA-style\ntasks (Clark et al., 2020b) to cross-lingual lan-\nguage model pre-training. Speciﬁcally, we\npresent two pre-training tasks, namely multi-\nlingual replaced token detection, and transla-\ntion replaced token detection. Besides, we\npretrain the model, named as XLM-E, on\nboth multilingual and parallel corpora. Our\nmodel outperforms the baseline models on\nvarious cross-lingual understanding tasks with\nmuch less computation cost. Moreover, anal-\nysis shows that XLM-E tends to obtain better\ncross-lingual transferability.\n1 Introduction\nIt has become a de facto trend to use a pretrained\nlanguage model (Devlin et al., 2019; Dong et al.,\n2019; Yang et al., 2019b; Bao et al., 2020) for\ndownstream NLP tasks. These models are typically\npretrained with masked language modeling objec-\ntives, which learn to generate the masked tokens of\nan input sentence. In addition to monolingual rep-\nresentations, the masked language modeling task is\neffective for learning cross-lingual representations.\nBy only using multilingual corpora, such pretrained\nmodels perform well on zero-shot cross-lingual\ntransfer (Devlin et al., 2019; Conneau et al., 2020),\ni.e., ﬁne-tuning with English training data while di-\nrectly applying the model to other target languages.\nThe cross-lingual transferability can be further im-\nproved by introducing external pre-training tasks\nusing parallel corpus, such as translation language\nmodeling (Conneau and Lample, 2019), and cross-\nlingual contrast (Chi et al., 2021b). However, pre-\nvious cross-lingual pre-training based on masked\nlanguage modeling usually requires massive com-\nputation resources, rendering such models quite\nexpensive. As shown in Figure 1, our proposed\n∗ Equal contribution. Zewen Chi contributes during in-\nternship at Microsoft Research.\n0 20 40 60 80 100 120\nFLOPs (1e20)\n75.0\n75.2\n75.4\n75.6\n75.8\n76.0\n76.2\n76.4\n76.6XNLI Acc\nXLM-R\nInfoXLM\nXLM-Align\nXLM-R+TLM (0.3M)\nXLM-R+TLM (0.6M)\nXLM-R+TLM (0.9M)\nXLM-R+TLM (1.2M)\nXLM-R+TLM (1.5M)\nXLM-E (45K)\nXLM-E (90K)\nXLM-E (125K) Speedup 130x\nFigure 1: The proposed XLM-E pre-training (red line)\nachieves 130×speedup compared with an in-house pre-\ntrained XLM-R augmented with translation language\nmodeling (XLM-R + TLM; blue line), using the same\ncorpora and code base. The training steps are shown\nin the brackets. We also present XLM-R (Conneau\net al., 2020), InfoXLM (Chi et al., 2021b), and XLM-\nAlign (Chi et al., 2021c). The compared models are all\nin Base size.\nXLM-E achieves a huge speedup compared with\nwell-tuned pretrained models.\nIn this paper, we introduce ELECTRA-style\ntasks (Clark et al., 2020b) to cross-lingual language\nmodel pre-training. Speciﬁcally, we present two\ndiscriminative pre-training tasks, namely multilin-\ngual replaced token detection, and translation re-\nplaced token detection. Rather than recovering\nmasked tokens, the model learns to distinguish the\nreplaced tokens in the corrupted input sequences.\nThe two tasks build input sequences by replac-\ning tokens in multilingual sentences, and transla-\ntion pairs, respectively. We also describe the pre-\ntraining algorithm of our model, XLM-E , which is\npretrained with the above two discriminative tasks.\nIt provides a more compute-efﬁcient and sample-\nefﬁcient way for cross-lingual language model pre-\ntraining.\n6170\nWe conduct extensive experiments on the\nXTREME cross-lingual understanding benchmark\nto evaluate and analyze XLM-E . Over seven\ndatasets, our model achieves competitive results\nwith the baseline models, while only using 1% of\nthe computation cost comparing to XLM-R. In ad-\ndition to the high computational efﬁciency, our\nmodel also shows the cross-lingual transferability\nthat achieves a reasonably low transfer gap. We\nalso show that the discriminative pre-training en-\ncourages universal representations, making the text\nrepresentations better aligned across different lan-\nguages.\nOur contributions are summarized as follows:\n• We explore ELECTRA-style tasks for cross-\nlingual language model pre-training, and pre-\ntrain XLM-E with both multilingual corpus\nand parallel data.\n• We demonstrate that XLM-E greatly reduces\nthe computation cost of cross-lingual pre-\ntraining.\n• We show that discriminative pre-training\ntends to encourage better cross-lingual trans-\nferability.\n2 Background: ELECTRA\nELECTRA (Clark et al., 2020b) introduces the\nreplaced token detection task for language model\npre-training, with the goal of distinguishing real in-\nput tokens from corrupted tokens. That means the\ntext encoders are pretrained as discriminators rather\nthan generators, which is different from the previ-\nous pretrained language models, such as BERT (De-\nvlin et al., 2019), that learn to predict the masked\ntokens.\nELECTRA trains two Transformer (Vaswani\net al., 2017) encoders, serving as generator and\ndiscriminator, respectively. The generator Gis typ-\nically a small BERT model trained with the masked\nlanguage modeling (MLM; Devlin et al. 2019) task.\nConsider an input sentence x= {xi}n\ni=1 contain-\ning ntokens. MLM ﬁrst randomly selects a subset\nM⊆{ 1,...,n }as the positions to be masked,\nand construct the masked sentence xmasked by re-\nplacing tokens in Mwith [MASK]. Then, the gen-\nerator predicts the probability distributions of the\nmasked tokens pG(x|xmasked). The loss function\nof the generator Gis:\nLG(x; θG) = −\n∑\ni∈M\nlog pG(xi|xmasked). (1)\nThe discriminator D is trained with the replaced\ntoken detection task. Speciﬁcally, the discrimina-\ntor takes the corrupted sentences xcorrupt as input,\nwhich is constructed by replacing the tokens in M\nwith the tokens sampled from the generator G:\n{\nxcorrupt\ni ∼pG(xi|xmasked), i ∈M\nxcorrupt\ni = xi, i ̸∈M (2)\nThen, the discriminator predicts whether xcorrupt\ni is\noriginal or sampled from the generator. The loss\nfunction of the discriminator Dis\nLD(x; θD) = −\nn∑\ni=1\nlog pD(zi|xcorrupt) (3)\nwhere zi represents the label of whether xcorrupt\ni is\nthe original token or the replaced one. The ﬁnal\nloss function of ELECTRA is the combined loss\nof the generator and discriminator losses, LE =\nLG + λLD.\nCompared to generative pre-training, ELECTRA\nuses more model parameters and training FLOPs\nper step, because it contains a generator and a dis-\ncriminator during pre-training. However, only the\ndiscriminator is used for ﬁne-tuning on downstream\ntasks, so the size of the ﬁnal checkpoint is similar\nto BERT-like models in practice.\n3 Methods\nFigure 2 shows an overview of the two discrimina-\ntive tasks used for pre-training XLM-E . Similar to\nELECTRA described in Section 2, XLM-E has\ntwo Transformer components, i.e., generator and\ndiscriminator. The generator predicts the masked\ntokens given the masked sentence or translation\npair, and the discriminator distinguishes whether\nthe tokens are replaced by the generator.\n3.1 Pre-training Tasks\nThe pre-training tasks of XLM-E are multilingual\nreplaced token detection (MRTD), and translation\nreplaced token detection (TRTD).\nMultilingual Replaced Token Detection The\nmultilingual replaced token detection task requires\nthe model to distinguish real input tokens from\n6171\nGenerator\n<M> 好 世界 <M> Hello <M> .\n你 ？ earth\nDiscriminator\nMasked\nOriginal 你好世界。\nReplaced 你 好 世界 ？ Hello earth .\nYes Yes Yes No Yes No YesIs original?\n(b) Translation replaced token detection (TRTD)\nHello world.\nGenerator\nAttention <M> all need<M> <M>Masked\nOriginal\nReplaced\nIs original?\n(a) Multilingual replaced token detection (MRTD)\nAttention is all you need.\nis we ?\nAttention is all needwe ?\nDiscriminator\nYes Yes Yes YesNo No\nFigure 2: Overview of two pre-training tasks of XLM-E, i.e., multilingual replaced token detection, and trans-\nlation replaced token detection. The generator predicts the masked tokens given a masked sentence or a masked\ntranslation pair, and the discriminator distinguishes whether the tokens are replaced by the generator.\ncorrupted multilingual sentences. Both the gener-\nator and the discriminator are shared across lan-\nguages. The vocabulary is also shared for different\nlanguages. The task is the same as in monolin-\ngual ELECTRA pre-training (Section 2). The only\ndifference is that the input texts can be in various\nlanguages.\nWe use uniform masking to produce the cor-\nrupted positions. We also tried span masking (Joshi\net al., 2019; Bao et al., 2020) in our preliminary\nexperiments. The results indicate that span mask-\ning signiﬁcantly weakens the generator’s prediction\naccuracy, which in turn harms pre-training.\nTranslation Replaced Token Detection Paral-\nlel corpora are easily accessible and proved to be\neffective for learning cross-lingual language mod-\nels (Conneau and Lample, 2019; Chi et al., 2021b),\nwhile it is under-studied how to improve discrimi-\nnative pre-training with parallel corpora. We intro-\nduce the translation replaced token detection task\nthat aims to distinguish real input tokens from trans-\nlation pairs. Given an input translation pair, the\ngenerator predicts the masked tokens in both lan-\nguages. Consider an input translation pair (e,f).\nWe construct the input sequence by concatenating\nthe translation pair as a single sentence. The loss\nfunction of the generator Gis:\nLG(e,f; θG) = −\n∑\ni∈Me\nlog pG(ei|[e; f]masked)\n−\n∑\ni∈Mf\nlog pG(fi|[e; f]masked)\nwhere [; ] is the operator of concatenation, and\nMe,Mf stand for the randomly selected masked\npositions for eand f, respectively. This loss func-\ntion is identical to the translation language model-\ning loss (TLM; Conneau and Lample 2019). The\ndiscriminator D learns to distinguish real input\ntokens from the corrupted translation pair. The\ncorrupted translation pair (ecorrupt,fcorrupt) is con-\nstructed by replacing tokens with the tokens sam-\npled from Gwith the concatenated translation pair\nas input. Formally, ecorrupt is constructed by\n{\necorrupt\ni ∼pG(ei|[e; f]masked), i ∈Me\necorrupt\ni = ei, i ̸∈Me\n(4)\nThe same operation is also used to construct\nfcorrupt. Then, the loss function of the discrimi-\nnator Dcan be written as\nLD(e,f; θD) = −\nne+nf∑\ni=1\nlog pD(ri|[e; f]corrupt)\n(5)\nwhere ri represents the label of whether the i-th\ninput token is the original one or the replaced one.\nThe ﬁnal loss function of the translation replaced\ntoken detection task is LG + λLD.\n3.2 Pre-training XLM-E\nThe XLM-E model is jointly pretrained with the\nmasked language modeling, translation language\nmodeling, multilingual replaced token detection\nand the translation replaced token detection tasks.\nThe overall training objective is to minimize\nL = LMLM(x; θG) + LTLM(e,f; θG)\n+ λLMRTD(x; θD) + λLTRTD(e,f; θD)\nover large scale multilingual corpus X= {x}and\nparallel corpus P= {(e,f)}. We jointly pretrain\n6172\nthe generator and the discriminator from scratch.\nFollowing Clark et al. (2020b), we make the gener-\nator smaller to improve the pre-training efﬁciency.\n3.3 Gated Relative Position Bias\nWe propose to use gated relative position bias in\nthe self-attention mechanism. Given input tokens\n{xi}|x|\ni=1, let {hi}|x|\ni=1 denote their hidden states in\nTransformer. The self-attention outputs {˜hi}|x|\ni=1\nare computed via:\nqi,ki,vi = hiWQ,hiWK,hiWV (6)\naij ∝exp{qi ·kj√dk\n+ ri−j} (7)\n˜hi =\n|x|∑\nj=1\naijvi (8)\nwhere ri−j represents gated relative position\nbias, each hi is linearly projected to a triple of\nquery, key and value using parameter matrices\nWQ,WK,WV ∈Rdh×dk , respectively.\nInspired by the gating mechanism of Gated Re-\ncurrent Unit (GRU; Cho et al. 2014), we compute\ngated relative position bias ri−j via:\ng(update),g(reset) = σ(qi ·u),σ(qi ·v)\n˜ri−j = wg(reset)di−j\nri−j = di−j + g(update)di−j + (1 −g(update))˜ri−j\nwhere di−j is learnable relative position bias, the\nvectors u,v ∈Rdk are parameters, σis a sigmoid\nfunction, and wis a learnable value.\nCompared with relative position bias (Parikh\net al., 2016; Raffel et al., 2020; Bao et al., 2020),\nthe proposed gates take the content into considera-\ntion, which adaptively adjusts the relative position\nbias by conditioning on input tokens. Intuitively,\nthe same distance between two tokens tends to play\ndifferent roles in different languages.\n4 Experiments\n4.1 Setup\nData We use the CC-100 (Conneau et al., 2020)\ndataset for the replaced token detection task. CC-\n100 contains texts in 100 languages collected from\nthe CommonCrawl dump. We use parallel corpora\nfor the translation replaced token detection task,\nincluding translation pairs in 100 languages col-\nlected from MultiUN (Ziemski et al., 2016), IIT\nBombay (Kunchukuttan et al., 2018), OPUS (Tiede-\nmann, 2012), WikiMatrix (Schwenk et al., 2019),\nand CCAligned (El-Kishky et al., 2020).\nFollowing XLM (Conneau and Lample, 2019),\nwe sample multilingual sentences to balance the\nlanguage distribution. Formally, consider the pre-\ntraining corpora in N languages with mj examples\nfor the j-th language. The probability of using an\nexample in the j-th language is\npj =\nmα\nj\n∑N\nk=1 mα\nk\n(9)\nThe exponent αcontrols the distribution such that\na lower α increases the probability of sampling\nexamples from a low-resource language. In this\npaper, we set α= 0.7.\nModel We use a Base-size 12-layer Trans-\nformer (Vaswani et al., 2017) as the discrimina-\ntor, with hidden size of 768, and FFN hidden\nsize of 3,072. The generator is a 4-layer Trans-\nformer using the same hidden size as the discrim-\ninator (Meng et al., 2021). See Appendix A for\nmore details of model hyperparameters.\nTraining We jointly pretrain the generator and\nthe discriminator of XLM-E from scratch, using\nthe Adam (Kingma and Ba, 2015) optimizer for\n125K training steps. We use dynamic batching\nof approximately 1M tokens for each pre-training\ntask. We set λ, the weight for the discriminator\nobjective to 50. The whole pre-training procedure\ntakes about 1.7 days on 64 Nvidia A100 GPU cards.\nSee Appendix B for more details of pre-training\nhyperparameters.\n4.2 Cross-lingual Understanding\nWe evaluate XLM-E on the XTREME (Hu et al.,\n2020b) benchmark, which is a multilingual multi-\ntask benchmark for evaluating cross-lingual un-\nderstanding. The XTREME benchmark contains\nseven cross-lingual understanding tasks, namely\npart-of-speech tagging on the Universal Dependen-\ncies v2.5 (Zeman et al., 2019), NER named en-\ntity recognition on the Wikiann (Pan et al., 2017;\nRahimi et al., 2019) dataset, cross-lingual natu-\nral language inference on XNLI (Conneau et al.,\n2018), cross-lingual paraphrase adversaries from\nword scrambling (PAWS-X; Yang et al. 2019a), and\ncross-lingual question answering on MLQA (Lewis\net al., 2020), XQuAD (Artetxe et al., 2020), and\nTyDiQA-GoldP (Clark et al., 2020a).\n6173\nModel Structured Prediction Question Answering Classiﬁcation AvgPOS NER XQuAD MLQA TyDiQA XNLI PAWS-X\nMetrics F1 F1 F1 / EM F1 / EM F1 / EM Acc. Acc.\nPre-training on multilingual corpus\nMBERT(Hu et al., 2020b) 70.3 62.2 64.5 / 49.4 61.4 / 44.2 59.7 / 43.9 65.4 81.9 63.1\nMT5 (Xue et al., 2021) - 55.7 67.0 / 49.0 64.6 / 45.0 57.2 / 41.2 75.4 86.4 -\nXLM-R 75.6 61.8 71.9 / 56.4 65.1 / 47.2 55.4 / 38.3 75.0 84.9 66.4\nXLM-E (w/o TRTD) 74.2 62.7 74.3 / 58.2 67.8 / 49.7 57.8 / 40.6 75.1 87.1 67.6\nPre-training on both multilingual corpus and parallel corpus\nXLM (Hu et al., 2020b) 70.1 61.2 59.8 / 44.3 48.5 / 32.6 43.6 / 29.1 69.1 80.9 58.6\nINFOXLM (Chi et al., 2021b) - - - / - 68.1 / 49.6 - / - 76.5 - -\nXLM-ALIGN(Chi et al., 2021c)76.0 63.7 74.7 / 59.0 68.1 /49.8 62.1 / 44.8 76.2 86.8 68.9\nXLM-E 75.6 63.5 76.2 / 60.2 68.3 / 49.8 62.4 / 45.7 76.6 88.3 69.3\nTable 1: Evaluation results on XTREME cross-lingual understanding tasks. We consider the cross-lingual transfer\nsetting, where models are only ﬁne-tuned on the English training data but evaluated on all target languages. The\ncompared models are all in Base size. Results of XLM-E and XLM-R are averaged over ﬁve runs.\nBaselines We compare our XLM-E model with\nthe cross-lingual language models pretrained\nwith multilingual text, i.e., Multilingual BERT\n(MBERT; Devlin et al. 2019), MT5 (Xue et al.,\n2021), and XLM-R (Conneau et al., 2020), or\npretrained with both multilingual text and par-\nallel corpora, i.e., XLM (Conneau and Lample,\n2019), INFO XLM (Chi et al., 2021b), and XLM-\nALIGN (Chi et al., 2021c). The compared models\nare all in Base size. In what follows, models are\nconsidered as in Base size by default.\nResults We use the cross-lingual transfer setting\nfor the evaluation on XTREME (Hu et al., 2020b),\nwhere the models are ﬁrst ﬁne-tuned with the En-\nglish training data and then evaluated on the tar-\nget languages. In Table 1, we report the accuracy,\nF1, or Exact-Match (EM) scores on the XTREME\ncross-lingual understanding tasks. The results are\naveraged over all target languages and ﬁve runs\nwith different random seeds. We divide the pre-\ntrained models into two categories, i.e., the models\npretrained on multilingual corpora, and the mod-\nels pretrained on both multilingual corpora and\nparallel corpora. For the ﬁrst setting, we pretrain\nXLM-E with only the multilingual replaced token\ndetection task. From the results, it can be observed\nthat XLM-E outperforms previous models on both\nsettings, achieving the averaged scores of 67.6 and\n69.3, respectively. Compared to XLM-R , XLM-E\n(w/o TRTD) produces an absolute 1.2 improve-\nment on average over the seven tasks. For the sec-\nond setting, compared to XLM-A LIGN , XLM-E\nproduces an absolute 0.4 improvement on average.\nXLM-E performs better on the question answering\nModel XNLI MLQA\nXLM (reimplementation) 73.4 66.2 / 47.8\n−TLM 70.6 64.0 / 46.0\nXLM-E 76.6 68.3 / 49.8\n−TRTD 75.1 67.8 / 49.7\n−TRTD−Gated relative position bias 75.2 67.4 / 49.2\nTable 2: Ablation studies of XLM-E. We studies the\neffects of the main components of XLM-E, and com-\npare the models with XLM under the same pre-training\nsetup, including training steps, learning rate, etc.\ntasks and sentence classiﬁcation tasks while pre-\nserving reasonable high F1 scores on structured pre-\ndiction tasks. Despite the effectiveness of XLM-E ,\nour model requires substantially lower computation\ncost than XLM-R and XLM-A LIGN . A detailed\nefﬁciency analysis in presented in Section 4.5.\n4.3 Ablation Studies\nFor a deeper insight to XLM-E , we conduct abla-\ntion experiments where we ﬁrst remove the TRTD\ntask and then remove the gated relative position\nbias. Besides, we reimplement XLM that is\npretrained with the same pre-training setup with\nXLM-E , i.e., using the same training steps, learn-\ning rate, etc. Table 2 shows the ablation results\non XNLI and MLQA. Removing TRTD weakens\nthe performance of XLM-E on both downstream\ntasks. On this basis, the results on MLQA further\ndecline when removing the gated relative position\nbias. This demonstrates that XLM-E beneﬁts from\nboth TRTD and the gated relative position bias dur-\ning pre-training. Besides, XLM-E substantially\noutperform XLM on both tasks. Notice that when\nremoving the two components from XLM-E , our\n6174\nModel Size Params XNLI MLQA\nXLM-E Base 279M 76.6 68.3 / 49.8\nXLM-E Large 840M 81.3 72.7 / 54.2\nXLM-E XL 2.2B 83.7 76.2 / 57.9\nXLM-R XL 3.5B 82.3 73.4 / 55.3\nMT5 XL 3.7B 82.9 73.5 / 54.5\nTable 3: Results of scaling-up the model size.\nModel XTREME Params FLOPs\nMBERT 63.1 167M 6.4e19\nXLM-R 66.4 279M 9.6e21\nINFO XLM* - 279M 9.6e21 + 1.7e20\nXLM-A LIGN * 68.9 279M 9.6e21 + 9.6e19\nXLM-E 69.3 279M 9.5e19\n−TRTD 67.6 279M 6.3e19\nTable 4: Comparison of the pre-training costs. The\nmodels with ‘*’ are continue-trained from XLM-R\nrather than pre-training from scratch.\nmodel only requires a multilingual corpus, but still\nachieves better performance than XLM, which uses\nan additional parallel corpus.\n4.4 Scaling-up Results\nScaling-up model size has shown to improve per-\nformance on cross-lingual downstream tasks (Xue\net al., 2021; Goyal et al., 2021). We study the scal-\nability of XLM-E by pre-training XLM-E models\nusing larger model sizes. We consider two larger\nmodel sizes in our experiments, namely Large and\nXL. Detailed model hyperparameters can be found\nin Appendix A. As present in Table 3, XLM-EXL\nachieves the best performance while using signiﬁ-\ncantly fewer parameters than its counterparts. Be-\nsides, scaling-up the XLM-E model size consis-\ntently improves the results, demonstrating the ef-\nfectiveness of XLM-E for large-scale pre-training.\n4.5 Training Efﬁciency\nWe present a comparison of the pre-training re-\nsources, to explore whether XLM-E provides a\nmore compute-efﬁcient and sample-efﬁcient way\nfor pre-training cross-lingual language models. Ta-\nble 4 compares the XTREME average score, the\nnumber of parameters, and the pre-training com-\nputation cost. Notice that INFO XLM and XLM-\nALIGN are continue-trained from XLM-R, so the\ntotal training FLOPs are accumulated over XLM-R.\nTable 4 shows that XLM-E substantially re-\nduces the computation cost for cross-lingual lan-\nguage model pre-training. Compared to XLM-R\nand XLM-A LIGN that use at least 9.6e21 training\nModel Tatoeba-14 Tatoeba-36\nen →xx xx →en en →xx xx →en\nXLM-R 59.5 57.6 55.5 53.4\nINFOXLM 80.6 77.8 68.6 67.3\nXLM-E 74.4 72.3 65.0 62.3\n−TRTD 55.8 55.1 46.4 44.6\nTable 5: Average accuracy@1 scores for Tatoeba cross-\nlingual sentence retrieval. The models are evaluated un-\nder two settings with 14 and 36 of the parallel corpora\nfor evaluation, respectively.\nFLOPs, XLM-E only uses 9.5e19 training FLOPs\nin total while even achieving better XTREME per-\nformance than the two baseline models. For the set-\nting of pre-training with only multilingual corpora,\nXLM-E (w/o TRTD) also outperforms XLM-R us-\ning 6.3e19 FLOPs in total. This demonstrates the\ncompute-effectiveness of XLM-E , i.e., XLM-E as\na stronger cross-lingual language model requires\nsubstantially less computation resource.\n4.6 Cross-lingual Alignment\nTo explore whether discriminative pre-training im-\nproves the resulting cross-lingual representations,\nwe evaluate our model on the sentence-level and\nword-level alignment tasks, i.e., cross-lingual sen-\ntence retrieval and word alignment.\nWe use the Tatoeba (Artetxe and Schwenk, 2019)\ndataset for the cross-lingual sentence retrieval task,\nthe goal of which is to ﬁnd translation pairs from\nthe corpora in different languages. Tatoeba con-\nsists of English-centric parallel corpora covering\n122 languages. Following Chi et al. (2021b) and\nHu et al. (2020b), we consider two settings where\nwe use 14 and 36 of the parallel corpora for eval-\nuation, respectively. The sentence representations\nare obtained by average pooling over hidden vec-\ntors from a middle layer. Speciﬁcally, we use\nlayer-7 for XLM-R and layer-9 for XLM-E . Then,\nthe translation pairs are induced by the nearest\nneighbor search using the cosine similarity. Ta-\nble 5 shows the average accuracy@1 scores under\nthe two settings of Tatoeba for both the xx →en\nand en →xx directions. XLM-E achieves 74.4\nand 72.3 accuracy scores for Tatoeba-14, and 65.0\nand 62.3 accuracy scores for Tatoeba-36, provid-\ning notable improvement over XLM-R . XLM-E\nperforms slightly worse than INFO XLM . We be-\nlieve the cross-lingual contrast (Chi et al., 2021b)\ntask explicitly learns the sentence representations,\nwhich makes INFO XLM more effective for the\ncross-lingual sentence retrieval task.\n6175\nModel Alignment Error Rate↓ Avgen-de en-fr en-hi en-ro\nfast align 32.14 19.46 59.90 - -\nXLM-R 17.74 7.54 37.79 27.49 22.64\nXLM-A LIGN 16.63 6.61 33.98 26.97 21.05\nXLM-E 16.49 6.19 30.20 24.41 19.32\n−TRTD 17.87 6.29 35.02 30.22 22.35\nTable 6: Alignment error rate scores (lower is better)\nfor the word alignment task on four language pairs. Re-\nsults of the baseline models are from Chi et al. (2021c).\nWe use the optimal transport method to obtain the re-\nsulting word alignments, where the sentence represen-\ntations are from the 9-th layer of XLM-E.\nFor the word-level alignment, we use the word\nalignment datasets from EuroParl 1, WPT2003 2,\nand WPT20053, containing 1,244 translation pairs\nannotated with golden alignments. The pre-\ndicted alignments are evaluated by alignment error\nrate (AER; Och and Ney 2003):\nAER = 1 −|A∩S|+ |A∩P|\n|A|+ |S| (10)\nwhere A,S, and P stand for the predicted align-\nments, the annotated sure alignments, and the anno-\ntated possible alignments, respectively. In Table 6\nwe compare XLM-E with baseline models, i.e.,\nfast align (Dyer et al., 2013), XLM-R, and XLM-\nALIGN . The resulting word alignments are ob-\ntained by the optimal transport method (Chi et al.,\n2021c), where the sentence representations are\nfrom the 9-th layer of XLM-E . Over the four lan-\nguage pairs, XLM-E achieves lower AER scores\nthan the baseline models, reducing the average\nAER from 21.05 to 19.32. It is worth mentioning\nthat our model requires substantial lower compu-\ntation costs than the other cross-lingual pretrained\nlanguage models to achieve such low AER scores.\nSee the detailed training efﬁciency analysis in Sec-\ntion 4.5. It is worth mentioning thatXLM-E shows\nnotable improvements over XLM-E (w/o TRTD)\non both tasks, demonstrating that the translation\nreplaced token detection task is effective for cross-\nlingual alignment.\n4.7 Universal Layer Across Languages\nWe evaluate the word-level and sentence-level\nrepresentations over different layers to explore\n1www-i6.informatik.rwth-aachen.de/\ngoldAlignment/\n2web.eecs.umich.edu/˜mihalcea/wpt/\n3web.eecs.umich.edu/˜mihalcea/wpt05/\n2 4 6 8 10 12\nLayer\n10\n20\n30\n40\n50\n60Averaged accuracy\nXLM-R\nXLM-E\nFigure 3: Evaluation results on Tatoeba cross-lingual\nsentence retrieval over different layers. For each layer,\nthe accuracy score is averaged over all the 36 language\npairs in both the xx →en and en →xx directions.\n0 2 4 6 8 10 12\nLayer\n20\n25\n30\n35\n40Alignment Error Rate (AER)\nXLM-R\nXLM-E\nFigure 4: Evaluation results of cross-lingual word\nalignment over different layers. Layer-0 stands for the\nembedding layer.\nwhether the XLM-E tasks encourage universal rep-\nresentations.\nAs shown in Figure 3, we illustrate the accu-\nracy@1 scores of XLM-E and XLM-R on Tatoeba\ncross-lingual sentence retrieval, using sentence rep-\nresentations from different layers. For each layer,\nthe ﬁnal accuracy score is averaged over all the\n36 language pairs in both the xx →en and en\n→xx directions. From the ﬁgure, it can be ob-\nserved that XLM-E achieves notably higher aver-\naged accuracy scores than XLM-R for the top lay-\ners. The results of XLM-E also show a parabolic\ntrend across layers, i.e., the accuracy continuously\nincreases before a speciﬁc layer and then continu-\nously drops. This trend is also found in other cross-\nlingual language models such as XLM-R and XLM-\nAlign (Jalili Sabet et al., 2020; Chi et al., 2021c).\nDifferent from XLM-R that achieves the highest\naccuracy of 54.42 at layer-7, XLM-E pushes it to\nlayer-9, achieving an accuracy of 63.66. At layer-\n10, XLM-R only obtains an accuracy of 43.34 while\nXLM-E holds the accuracy score as high as 57.14.\nFigure 4 shows the averaged alignment error rate\n6176\nModel XQuAD MLQA TyDiQA XNLI PA WS-X\nMBERT 25.0 27.5 22.2 16.5 14.1\nXLM-R 15.9 20.3 15.2 10.4 11.4\nINFOXLM - 18.8 - 10.3 -\nXLM-ALIGN 14.6 18.7 10.6 11.2 9.7\nXLM-E 14.9 19.2 13.1 11.2 8.8\n−TRTD 16.3 18.6 16.3 11.5 9.6\nTable 7: The cross-lingual transfer gap scores on the\nXTREME tasks. A lower transfer gap score indicates\nbetter cross-lingual transferability. We use the EM\nscores to compute the gap scores for the QA tasks.\n(AER) scores of XLM-E and XLM-R on the word\nalignment task. We use the hidden vectors from\ndifferent layers to perform word alignment, where\nlayer-0 stands for the embedding layer. The ﬁnal\nAER scores are averaged over the four test sets\nin different languages. Figure 4 shows a similar\ntrend to that in Figure 3, where XLM-E not only\nprovides substantial performance improvements\nover XLM-R, but also pushes the best-performance\nlayer to a higher layer, i.e., the model obtains the\nbest performance at layer-9 rather than a lower\nlayer such as layer-7.\nOn both tasks, XLM-E shows good perfor-\nmance for the top layers, even though bothXLM-E\nand XLM-R use the Transformer (Vaswani et al.,\n2017) architecture. Compared to the masked lan-\nguage modeling task that encourages the top layers\nto be language-speciﬁc, discriminative pre-training\nmakes XLM-E producing better-aligned text rep-\nresentations at the top layers. It indicates that the\ncross-lingual discriminative pre-training encour-\nages universal representations inside the model.\n4.8 Cross-lingual Transfer Gap\nWe analyze the cross-lingual transfer gap (Hu et al.,\n2020b) of the pretrained cross-lingual language\nmodels. The transfer gap score is the difference\nbetween performance on the English test set and\nthe average performance on the test set in other\nlanguages. This score suggests how much end task\nknowledge has not been transferred to other lan-\nguages after ﬁne-tuning. A lower gap score indi-\ncates better cross-lingual transferability. Table 7\ncompares the cross-lingual transfer gap scores on\nﬁve of the XTREME tasks. We notice thatXLM-E\nobtains the lowest gap score only on PAWS-X.\nNonetheless, it still achieves reasonably low gap\nscores on the other tasks with such low computation\ncost, demonstrating the cross-lingual transferability\nof XLM-E . We believe that it is more difﬁcult to\nachieve the same low gap scores when the model\nobtains better performance.\n5 Related Work\nLearning self-supervised tasks on large-scale mul-\ntilingual texts has proven to be effective for pre-\ntraining cross-lingual language models. Masked\nlanguage modeling (MLM; Devlin et al. 2019) is\ntypically used to learn cross-lingual encoders such\nas multilingual BERT (mBERT; Devlin et al. 2019)\nand XLM-R (Conneau et al., 2020). The cross-\nlingual language models can be further improved\nby introducing external pre-training tasks using\nparallel corpora. XLM (Conneau and Lample,\n2019) introduces the translation language model-\ning (TLM) task that predicts masked tokens from\nconcatenated translation pairs. ALM (Yang et al.,\n2020) utilizes translation pairs to construct code-\nswitched sequences as input. InfoXLM (Chi et al.,\n2021b) considers an input translation pair as cross-\nlingual views of the same meaning, and proposes\na cross-lingual contrastive learning task. Several\npre-training tasks utilize the token-level alignments\nin parallel data to improve cross-lingual language\nmodels (Cao et al., 2020; Zhao et al., 2021; Hu\net al., 2020a; Chi et al., 2021c).\nIn addition, parallel data are also employed for\ncross-lingual sequence-to-sequence pre-training.\nXNLG (Chi et al., 2020) presents cross-lingual\nmasked language modeling and cross-lingual auto-\nencoding for cross-lingual natural language gener-\nation, and achieves the cross-lingual transfer for\nNLG tasks. VECO (Luo et al., 2020) utilizes cross-\nattention MLM to pretrain a variable cross-lingual\nlanguage model for both NLU and NLG. mT6 (Chi\net al., 2021a) improves mT5 (Xue et al., 2021) by\nlearning the translation span corruption task on\nparallel data. ∆LM (Ma et al., 2021) proposes to\nalign pretrained multilingual encoders to improve\ncross-lingual sequence-to-sequence pre-training.\n6 Conclusion\nWe introduce XLM-E , a cross-lingual language\nmodel pretrained by ELECTRA -style tasks.\nSpeciﬁcally, we present two pre-training tasks, i.e.,\nmultilingual replaced token detection, and trans-\nlation replaced token detection. XLM-E outper-\nforms baseline models on cross-lingual understand-\ning tasks although using much less computation\ncost. In addition to improved performance and com-\nputational efﬁciency, we also show that XLM-E\n6177\nobtains the cross-lingual transferability with a rea-\nsonably low transfer gap.\n7 Ethical Considerations\nOur work introduces ELECTRA-style tasks for\ncross-lingual language model pre-training, which\nrequires much less computation cost than previous\nmodels and substantially reduces the energy cost.\nAcknowledgements\nHeyan Huang is the corresponding author. Zewen\nChi, Xian-Ling Mao, and Heyan Huang are\nsupported by National Key R&D Plan (No.\n2018YFB1005100), National Natural Science\nFoundation of China (No. U19B2020, 62172039,\n61732005, 61602197 and L1924068), the funds\nof Beijing Advanced Innovation Center for Lan-\nguage Resources (No. TYZ19005), and in part\nby CCF-AFSG Research Fund under Grant\nNo.RF20210005, and in part by the fund of Joint\nLaboratory of HUST and Pingan Property & Casu-\nalty Research (HPL).\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7(0):597–610.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. In Proceedings\nof the 37th International Conference on Machine\nLearning, pages 7006–7016.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In International Conference on Learning Rep-\nresentations.\nZewen Chi, Li Dong, Shuming Ma, Shaohan Huang,\nXian-Ling Mao, Heyan Huang, and Furu Wei.\n2021a. mT6: Multilingual pretrained text-to-text\ntransformer with translation pairs. arXiv preprint\narXiv:2104.08692.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7570–7577. AAAI Press.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. 2021b. In-\nfoXLM: An information-theoretic framework for\ncross-lingual language model pre-training. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3576–3588, Online. Association for Computational\nLinguistics.\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-\nLing Mao, Heyan Huang, and Furu Wei. 2021c.\nImproving pretrained cross-lingual language mod-\nels via self-labeled word alignment. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3418–3430,\nOnline. Association for Computational Linguistics.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020a. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020b. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057–7067. Curran Associates, Inc.\n6178\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems , pages 13063–13075. Cur-\nran Associates, Inc.\nChris Dyer, Victor Chahuneau, and Noah A Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of ibm model 2. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–648.\nAhmed El-Kishky, Vishrav Chaudhary, Francisco\nGuzm´an, and Philipp Koehn. 2020. CCAligned: A\nmassive collection of cross-lingual web-document\npairs. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 5960–5969, Online. Associa-\ntion for Computational Linguistics.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale trans-\nformers for multilingual masked language modeling.\narXiv preprint arXiv:2105.00572.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2020a. Explicit align-\nment objectives for multilingual bidirectional en-\ncoders. arXiv preprint arXiv:2010.07972.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020b. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. arXiv preprint arXiv:2003.11080.\nMasoud Jalili Sabet, Philipp Dufter, Franc ¸ois Yvon,\nand Hinrich Sch ¨utze. 2020. SimAlign: High qual-\nity word alignments without parallel training data us-\ning static and contextualized embeddings. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1627–1643, Online. As-\nsociation for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Span-\nBERT: Improving pre-training by representing and\npredicting spans. arXiv preprint arXiv:1907.10529.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation, Miyazaki, Japan. European Language\nResources Association.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nVECO: Variable encoder-decoder pre-training for\ncross-lingual understanding and generation. arXiv\npreprint arXiv:2010.16046.\nShuming Ma, Li Dong, Shaohan Huang, Dong-\ndong Zhang, Alexandre Muzio, Saksham Singhal,\nHany Hassan Awadalla, Xia Song, and Furu Wei.\n2021. DeltaLM: Encoder-decoder pre-training for\nlanguage generation and translation by augmenting\npretrained multilingual encoders. arXiv preprint\narXiv:2106.13736.\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song.\n2021. COCO-LM: Correcting and contrasting text\nsequences for language model pretraining. arXiv\npreprint arXiv:2102.08473.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational linguistics, 29(1):19–51.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\n6179\nAnkur Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2249–2255,\nAustin, Texas. Association for Computational Lin-\nguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm ´an. 2019. Wiki-\nMatrix: Mining 135M parallel sentences in 1620\nlanguage pairs from wikipedia. arXiv preprint\narXiv:1907.05791.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation, pages 2214–2218, Istanbul, Turkey. Eu-\nropean Language Resources Association.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998–6008. Curran As-\nsociates, Inc.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 483–498, Online. Association for Computa-\ntional Linguistics.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020. Alternating\nlanguage modeling for cross-lingual pre-training. In\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019a. PAWS-X: A cross-lingual ad-\nversarial dataset for paraphrase identiﬁcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687–\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Advances in Neu-\nral Information Processing Systems, volume 32. Cur-\nran Associates, Inc.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams,\nand et al. 2019. Universal dependencies 2.5.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntute of Formal and Applied Linguistics (´UFAL), Fac-\nulty of Mathematics and Physics, Charles Univer-\nsity.\nWei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle\nAugenstein. 2021. Inducing language-agnostic mul-\ntilingual representations. In Proceedings of *SEM\n2021: The Tenth Joint Conference on Lexical and\nComputational Semantics , pages 229–240, Online.\nAssociation for Computational Linguistics.\nBo Zheng, Li Dong, Shaohan Huang, Saksham Sing-\nhal, Wanxiang Che, Ting Liu, Xia Song, and Furu\nWei. 2021. Allocating large vocabulary capacity for\ncross-lingual language model pre-training. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3203–\n3215, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In LREC, pages 3530–3534.\n6180\nAppendix\nA Model Hyperparameters\nTable 8 and Table 9 shows the model hyperparam-\neters of XLM-E in the sizes of Base, Large, and\nXL. For the Base-size model, we use the same vo-\ncabulary with XLM-R (Conneau et al., 2020) that\nconsists of 250K subwords tokenized by Sentence-\nPiece (Kudo and Richardson, 2018). For the mod-\nels in Large size and XL size, we use V oCap (Zheng\net al., 2021) to allocate a 500K vocabulary for mod-\nels in Large size and XL size.\nHyperparameters Base Large XL\nLayers 4 6 8\nHidden size 768 1,024 1,536\nFFN inner hidden size 3,072 4,096 6,144\nAttention heads 12 16 24\nTable 8: Model hyperparameters of XLM-E generators\nin different sizes.\nHyperparameters Base Large XL\nLayers 12 24 48\nHidden size 768 1,024 1,536\nFFN inner hidden size 3,072 4,096 6,144\nAttention heads 12 16 24\nTable 9: Model hyperparameters of XLM-E discrimi-\nnators in different sizes.\nB Hyperparameters for Pre-Training\nAs shown in Table 10, we present the hyperparam-\neters for pre-training XLM-E . We use the batch\nsize of 1M tokens for each pre-training task. In\nmultilingual replaced token detection, a batch is\nconstructed by 2,048 length-512 input sequences,\nwhile the input length is dynamically set as the\nlength of the original translation pairs in translation\nreplaced token detection.\nC Hyperparameters for Fine-Tuning\nIn Table 11, we report the hyperparameters for ﬁne-\ntuning XLM-E on the XTREME end tasks.\nHyperparameters Value\nTraining steps 125K\nBatch tokens per task 1M\nAdam ϵ 1e-6\nAdam β (0.9, 0.98)\nLearning rate 5e-4\nLearning rate schedule Linear\nWarmup steps 10,000\nGradient clipping 2.0\nWeight decay 0.01\nTable 10: Hyperparameters used for pre-training\nXLM-E.\n6181\nPOS NER XQuAD MLQA TyDiQA XNLI PAWS-X\nBatch size {8,16,32} 8 32 32 32 32 32\nLearning rate {1,2,3}e-5 {5,...,9}e-6 {2,3,4}e-5 {2,3,4}e-5 {2,3,4}e-5 {5,...,8}e-6 {8,9,10,20}e-6\nLR schedule Linear Linear Linear Linear Linear Linear Linear\nWarmup 10% 10% 10% 10% 10% 12,500 steps 10%\nWeight decay 0 0 0 0 0 0 0\nEpochs 10 10 4 {2,3,4} {10,20,40} 10 10\nTable 11: Hyperparameters used for ﬁne-tuning on the XTREME end tasks.\n6182",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.5914614796638489
    },
    {
      "name": "Computer science",
      "score": 0.5439751744270325
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46724653244018555
    },
    {
      "name": "Association (psychology)",
      "score": 0.4656810164451599
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4353407025337219
    },
    {
      "name": "Language model",
      "score": 0.434631884098053
    },
    {
      "name": "Linguistics",
      "score": 0.4309886395931244
    },
    {
      "name": "Cognitive science",
      "score": 0.36819028854370117
    },
    {
      "name": "Psychology",
      "score": 0.2951035499572754
    },
    {
      "name": "Philosophy",
      "score": 0.14204862713813782
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}