{
  "title": "Exploration of Open Large Language Models for eDiscovery",
  "url": "https://openalex.org/W4389523969",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3037470943",
      "name": "Sumit Pai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2509908781",
      "name": "Sounak Lahiri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098822920",
      "name": "Ujjwal Kumar",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Krishanu Baksi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092031169",
      "name": "Elijah Soba",
      "affiliations": [
        "Deloitte (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4382504536",
      "name": "Michael Suesserman",
      "affiliations": [
        "Deloitte (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2092647833",
      "name": "Nirmala Pudota",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116884679",
      "name": "Jon Foster",
      "affiliations": [
        "Deloitte (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2163995663",
      "name": "Edward A Bowen",
      "affiliations": [
        "Deloitte (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2133338754",
      "name": "Sanmitra Bhattacharya",
      "affiliations": [
        "Deloitte (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4313547549",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4376654514",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2123545015",
    "https://openalex.org/W2791020301",
    "https://openalex.org/W2913178420",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W2155961949",
    "https://openalex.org/W2019805164",
    "https://openalex.org/W3195680661",
    "https://openalex.org/W2029075138",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3108142956",
    "https://openalex.org/W2035773980",
    "https://openalex.org/W2305138248",
    "https://openalex.org/W2126105412",
    "https://openalex.org/W4290059185",
    "https://openalex.org/W2083605078",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W1480224957",
    "https://openalex.org/W2533861448",
    "https://openalex.org/W4384918448"
  ],
  "abstract": "Sumit Pai, Sounak Lahiri, Ujjwal Kumar, Krishanu Baksi, Elijah Soba, Michael Suesserman, Nirmala Pudota, Jon Foster, Edward Bowen, Sanmitra Bhattacharya. Proceedings of the Natural Legal Language Processing Workshop 2023. 2023.",
  "full_text": "Exploration of Open Large Language Models for eDiscovery\nSumit Pai1⇤, Sounak Lahiri1⇤, Ujjwal Kumar1⇤, Krishanu Das Baksi1,\nElijah Soba2, Michael Suesserman2, Nirmala Pudota1, Jonathan Foster2,\nEdward Bowen2, Sanmitra Bhattacharya2\n1Deloitte & Touche Assurance & Enterprise Risk Services India Private Limited, India\n2Deloitte & Touche LLP, United States\n{sumpai, sanmbhattacharya}@deloitte.com\nAbstract\nThe rapid advancement of Generative Ar-\ntiﬁcial Intelligence (AI), particularly Large\nLanguage Models (LLMs), has led to their\nwidespread adoption for various natural lan-\nguage processing (NLP) tasks. One crucial\ndomain ripe for innovation is the Technology-\nAssisted Review (TAR) process in Electronic\ndiscovery (eDiscovery). Traditionally, TAR\ninvolves manual review and classiﬁcation of\ndocuments for relevance over large document\ncollections for litigations and investigations.\nThis process is aided by machine learning\nand NLP tools which require extensive train-\ning and ﬁne-tuning. In this paper, we ex-\nplore the application of LLMs to TAR, specif-\nically for predictive coding. We experiment\nwith out-of-the-box prompting and ﬁne-tuning\nof LLMs using parameter-efﬁcient techniques.\nWe conduct experiments using open LLMs\nand compare them to commercially-licensed\nones. Our experiments demonstrate that open\nLLMs lag behind commercially-licensed mod-\nels in relevance classiﬁcation using out-of-the-\nbox prompting. However, topic-speciﬁc in-\nstruction tuning of open LLMs not only im-\nprove their effectiveness but can often out-\nperform their commercially-licensed counter-\nparts in performance evaluations. Additionally,\nwe conduct a user study to gauge the prefer-\nences of our eDiscovery Subject Matter Special-\nists (SMS) regarding human-authored versus\nmodel-generated reasoning. We demonstrate\nthat instruction-tuned open LLMs can generate\nhigh quality reasonings that are comparable to\ncommercial LLMs.\n1 Introduction\nElectronic discovery (eDiscovery) (Oard et al.,\n2013) refers to the process of identifying, col-\nlecting, and preserving electronic documents and\ndata for the purpose of legal proceedings, inves-\ntigations, or regulatory compliance. During legal\n*These authors contributed equally to this work\nprocedures, including litigation proceedings and\ncorporate mergers and acquisitions, the court often\nissues aproduction request. The request mandates\nthe parties involved to produce documents perti-\nnent to the case. A large team of legal practitioners\nmeticulously examines millions of digital records\nto identify the ones that are relevant to the produc-\ntion request. This step is a crucial phase of the\neDiscovery process, commonly referred to as re-\nsponsiveness determination. During this early step,\nlegal practitioners often employ Artiﬁcial Intelli-\ngence (AI)-based tools to help them identify and\nprioritize the documents for review. This essential\ncomponent of the eDiscovery process is referred to\nas Technology-Assisted Review (TAR) (Cormack\nand Grossman, 2014).\nAnother related domain focused on management\nand extraction of relevant information from elec-\ntronic documents for legal purposes is Legal In-\nformation Retrieval (LIR). However, the important\ndistinctions between eDiscovery and the general\ndomain of LIR (Ganguly et al., 2023) are in their\npurpose, and nature of documents under considera-\ntion. eDiscovery is related to request for production\nof documents and data that are often in the form of\nemail correspondences, text messages, articles, and\nﬁnancial declarations. In contrast, LIR is related\nto ﬁnding legal information in response to speciﬁc\nqueries from legal databases, case law reposito-\nries, legislative archives, and other legal resources.\nThe language used in the documents for these two\ntasks differ with the latter being richer in legal ter-\nminology. In this paper, our primary focus is on\ndocuments pertinent to eDiscovery.\nTraditional approaches to TAR have focused on\nBoolean querying (Blair and Maron, 1985; Baron\net al., 2007), information retrieval (Oard et al.,\n2013) and active learning methodologies (Cormack\nand Grossman, 2016; McDonald et al., 2018). In\nrecent years, the use of supervised learning on\nlabeled documents, referred to aspredictive cod-\ning (Brown, 2015; Yang et al., 2021), has gained\nmore popularity within TAR workﬂows. Predic-\ntive coding often employs binary text classiﬁcation\nbased on textual, syntactic, semantic, and other\ndata-driven features. Motivated by the application\nof LLMs in zero-shot and few-shot learning in var-\nious domains, including recommender systems and\ndocument annotation (Hou et al., 2023; Törnberg,\n2023; Dai et al., 2022; Ahmed and Devanbu, 2022),\nwe propose a novel approach to predictive coding\nusing LLMs. Furthermore, we go beyond the pre-\ndictive capabilities of LLMs, and leverage them to\nprovide reasonings for the predictions. This capa-\nbility can assist human reviewers in making more\ninformed decisions. The use of LLMs for predic-\ntive coding and reasoning represents a paradigm\nshift in a domain burdened by the ever-increasing\nvolume of documents and escalating review costs\n(Yang et al., 2021).\nIn this paper, we utilize open LLMs, notably\nLarge Language Model Meta AI v2 (LLaMA2\n- 13B and 70B versions) (Touvron et al., 2023)\nand Falcon-7B1, as well as the commercially li-\ncensed Generative Pre-trained Transformer (GPT)-\n3.5-turbo model2, for both predictive coding and\nreasoning. We explore various methods to use these\nLLMs effectively, including zero-shot prompting,\nﬁne-tuning for instruction following (i.e., instruc-\ntion tuning), and reasoning generation. Addition-\nally, we conduct a user study involving eDiscovery\nSubject Matter Specialists (SMSs) to evaluate the\nquality of reasoning generated by these models, and\nthose authored by a legal SMS. The publicly avail-\nable Enron email dataset (Grossman et al., 2011) is\nused in our experiments. This dataset comprises of\na large corpus of emails and attachments that had to\nbe reviewed for their responsiveness to production\nrequests. We present experimental results of the\nvarious LLM-based predictive coding approaches\non this dataset.\nThe paper presents related research in Section 2,\ndescribes the datasets used in Section 3, outlines\nthe experimental approaches in Section 4, and re-\nports the results of classiﬁcation models as well as\nuser preferences for generated reasonings in Sec-\ntion 5.\n1https://falconllm.tii.ae/\n2https://platform.openai.com/docs/models/gpt-3-5\n2 Related Work\nThe application of AI in eDiscovery for the pur-\nposes of document prioritization, reasoning, and\ndecision-making are not a recent development\n(Araszkiewicz et al., 2022; Ashley and Bridewell,\n2010; Conrad, 2010). Notably, the Text Retrieval\nConference (TREC) had consistently featured a\ndedicated Legal track from 2006 to 2011 (Baron\net al., 2006; Tomlinson et al., 2007; Oard et al.,\n2008; Hedin et al., 2009; Cormack et al., 2010;\nGrossman et al., 2011), where eDiscovery was\ngiven prime importance, with emphasis on retrieval\nand ranking-based approaches. Several shared\ntasks related to eDiscovery, including TAR and\nprivilege review, were organized to stimulate and\nadvance research in this specialized domain.\nTypical TAR solutions can be decomposed\ninto retrieval-based ( Oard et al. , 2013) and\nclassiﬁcation-based approaches (Barnett et al.,\n2009)( Lewis, 2010). Both of these approaches are\ninstrumental in facilitating the prioritization of doc-\numents for subsequent review processes. Along-\nside these methods, active learning strategies (Cor-\nmack and Grossman, 2016; McDonald et al., 2018)\nhave been extensively explored to boost the opera-\ntional efﬁciency of these models. These strategies\nincorporate relevance feedback provided by human\nreviewers to enhance model performance. How-\never, it’s imperative to acknowledge that certain\nfeedback-based processes can be inherently slow,\nexpensive and inconsistent. In this paper, we pri-\nmarily focus on the classiﬁcation-based approach,\nalso referred to as predictive coding, which has\ngained prominence in recent years.\nConventional TAR approaches are often per-\nceived as black box systems, making it challeng-\ning to trust their decisions. The generation of\nreasoning and explanation for TAR models has\nreceived limited attention. Previous approaches\n(Chhatwal et al., 2018; Villata et al., 2020) focused\non training models to identify snippets (at sen-\ntence level) within documents that are classiﬁed\nas either responsive or non-responsive. However,\nthese approaches necessitate training on annotated\ndata, which is both time-consuming and resource-\nintensive.\nIn this work, we focus on leveraging the capabil-\nities of generative LLMs for both classiﬁcation and\nreasoning. These architectures, rooted in the Trans-\nformer framework (Vaswani et al., 2017), are char-\nacterized by their auto-regressive decoders. Typi-\ncally, these models are trained on vast and diverse\ncorpora of textual data, demonstrating remarkable\nproﬁciency in comprehending natural language in-\nstructions across diverse domains. These LLMs\nare able to generate coherent natural language re-\nsponses when provided with appropriate prompts\n(Liu et al., 2023). They can interpret the topics\nspeciﬁed in the prompts and execute them effec-\ntively. In this study, we experiment with some open\nLLMs, such as LLaMA2 (13B and 70B versions)\nand Falcon-7B, as well as commercially licensed\nmodels like GPT-3.5-turbo.\n3 Dataset\nIn this section, we describe the dataset used for our\nexperiments. We used the Electronic Discovery\nReference Model (EDRM) Enron Email Data Set\nVersion 2, which is the post-processed version em-\nployed in the TREC 2011 Legal eDiscovery track\n(Grossman et al., 2011). This dataset serves as a\nrich resource for exploring various facets of eDis-\ncovery, particularly in the context of email commu-\nnication.\n3.1 Source and Composition\nThe EDRM Enron Email Data Set Version 2 orig-\ninates from the Enron Corporation, once a lead-\ning energy company in the United States that col-\nlapsed in 2001 due to accounting malpractices.\nThe dataset was used in the TREC Legal eDiscov-\nery track from 2009 to 2011. The post-processed\ndataset is meticulously organized into 159 direc-\ntories, each capturing the email communication\nrecords of individuals associated with the company,\noffering a distinct insight into corporate communi-\ncation.\n3.2 Formats\nThe dataset is divided into two primary categories:\n• Emails: This subset comprises a substantial\nportion of the dataset, with a total of 455,449\nemail messages. Each email has a distinct\ndocument ID and is stored as plain text with\nthe naming conventiondoc_id.txt. These\nemails are central to our analyses and experi-\nments.\n• Attachments: In addition to the emails, the\ndataset contains 230,143 attachment ﬁles.\nEach attachments is linked to a speciﬁc\nemail message and follows the naming for-\nmat doc_id.number.txt, wherenumber in-\ndicates its sequential order.\nAttachments are treated as separate documents.\n3.3 Experimental Focus\nFollowing the speciﬁcations of the Learning Task\nof the TREC 2011 Legal Track, we focused on\nthree available topics, numbered 401, 402, and 403.\nDetailed description of each topic is provided in\nAppendix A.1. The task organizers offered a choice\nto use either emails, attachments, or both. Evalu-\nations of the submitted runs for this shared task\nencompassed these three options. In our experi-\nments, we concentrate on analyzing emails from\neach of the aforementioned topics. For simplicity\nand consistency, attachments were intentionally ex-\ncluded due to their potentially large size and varied\nformats, such as spreadsheets, presentations, and\nwebpages.\nFigure 1: Distribution of emails across the seed dataset\nand qrels. The color-coded stacked bars show the num-\nber of relevant and non-relevant emails for each topic.\n3.4 Training and Testing Data\nFor our experiments, we used two primary compo-\nnents from the Learning Task of the TREC 2011\nLegal Track:\n• Seeds: The seed data was used to ﬁne-tune\nthe LLMs to adapt to the eDiscovery domain,\nespecially in the context of Enron’s email com-\nmunications.\n• Qrels: The query relevance judgments (qrels)\nserved as our test set. This dataset allowed\nus to evaluate the effectiveness of the LLMs\nin identifying relevant documents within the\nEnron Dataset for the given topics.\nFigure 1 shows the distribution of emails across\nthe three topics used in our study. The seed data\nand qrels for each topic are highly imbalanced with\na majority of documents being non-relevant.\nWe focused on topics from the 2011 TREC Le-\ngal track, rather than prior years (i.e. 2009 and\n2010 TREC Legal tracks), for two main reasons.\nFirstly, each topic from this year featured a rela-\ntively larger number of labeled documents in both\nthe seed and qrel sets, and showcased a diverse\nrange of label distributions. For instance, Topic 401\ndisplayed a fairly balanced distribution between rel-\nevant and non-relevant documents, whereas other\ntopics exhibited signiﬁcant imbalances. Secondly,\nthe dataset from this year provided a sufﬁcient\nnumber of topic statements to effectively evaluate\nthe capability and generalizability of our approach\nwhen applied to predictive coding.\n4 Methodology\nIn this study, we primarily focus on examining sev-\neral generative models from the decoder-only auto-\nregressive family, a subset of the broader category\nof LLMs. These models have gained signiﬁcant at-\ntention and adoption following the introduction of\nChatGPT3. Legal departments and law ﬁrms have\nexpressed interest in utilizing these models to en-\nhance their document review processes, achieving\nboth scalability and cost-efﬁciency. We explore\nfour models: Falcon (7B version), LLaMA2 (both\n13B and 70B versions), and GPT (3.5 turbo ver-\nsion). Our experimentation involves two distinct\nmethodologies: out-of-the-box (OOB) prompting\napproach and ﬁne-tuning of open LLMs, with a\nspecial emphasis on Falcon 7B and LLaMA2 13B,\nwhich have been shown to outperform other open\nLLMs of similar size across various benchmarks4.\nWe detail these methodologies in the subsequent\nsections.\n4.1 Prompt Engineering\nA common method to interact with a decoder-\nbased auto-regressive LLMs is throughprompts.\nA prompt is a natural language instruction that\ncombines topic speciﬁcations, contextual infor-\nmation, and input parameters for executing the\nspeciﬁed tasks. The LLM interprets the task de-\nscribed in the prompt and generates correspond-\ning responses. Prompting greatly improves the\nusability of decoder-only models, allowing users\n3https://chat.openai.com/\n4https://huggingface.co/spaces/HuggingFaceH4/open\n_llm_leaderboard\nwithout technical knowledge to effectively interact\nwith them using just natural language. Without\nthe knowledge of the technical intricacies, such as\ncoding or statistical methodologies, one can easily\ninteract with these models using this zero-shot ap-\nproach. The art of crafting effective prompts has\npaved the way for a new area of research known\nas prompt engineering. Prompt engineering (Liu\net al., 2023) is an iterative process which entails\nthe creation, evaluation, and reﬁnement of prompts,\nall aimed at enhancing performance outcomes for\nthe targeted topic.\nIn this paper, we present a systematic approach\ntowards prompt engineering grounded on perfor-\nmance evaluation on a sample set of qrels. We\nstarted with an initial prompt for each topic, and\niteratively reﬁned it to derive a ﬁnal optimized\nprompt. To ensure that the prompt template, topic\nstatement, and email content all ﬁt within the con-\ntext length limitations of the LLMs used in this\nstudy, we truncated each emails to its ﬁrst 300 to-\nkens (emails from our dataset have a median word\ncount of < 50). To evaluate the effectiveness of\nincremental updates to the prompts, we computed\nmacro F1 score on a randomly chosen subset of\n50 emails from the qrels for each topic, and then\ncompared the results through each iteration. The\nprompt which gave us optimal performance on this\nsmaller prompt-evaluation subset was then applied\non the entire set of qrels (test set) for a compre-\nhensive evaluation. In the early stages of prompt\nengineering, our primary emphasis was on classiﬁ-\ncation metrics. However, after achieving optimal\nperformance, we shifted our focus to producing\nhigh-quality reasoning, achieved by providing the\nmodel with explicit guidance through the prompt.\n4.2 Instruction Tuning\nInstruction tuning is the process of ﬁne-tuning\nLLMs to follow instructions for targeted tasks (Wei\net al., 2021). The process necessitates the use of an\ninstruction dataset, comprised of instruction-output\npairs that function as the training data. Within the\nscope of the predictive coding application, our pri-\nmary objective is to evaluate the relevance of emails\nin relation to topic statements (typically character-\nized as production requests). To facilitate this, in-\nstructions or prompts are crafted in simple English\nby describing the topic of relevance determination,\nand providing the email and topic statement, and\nﬁnally providing the expected output in plain text\n– either ‘Yes’ or ‘No’ – based on the email’s rele-\nvance to the given topic. To ﬁne-tune the models,\nwe create the prompt and its corresponding output\nusing the seed set from our dataset. The evaluation\nis conducted on the qrels.\nLLMs can be instruction tuned through various\nmethods. While a typical approach involves full\nﬁne-tuning, it often comes with signiﬁcant infras-\ntructure requirements. As an alternate, parameter-\nefﬁcient techniques like Low Rank Adaptation\n(LoRA) (Hu et al., 2021) and Quantized LoRA\n(qLoRA) (Dettmers et al., 2023) offer a way to\nﬁne-tune on relatively modest infrastructure, while\nmaintaining performance that’s comparable to full\nﬁne-tuning (Liu et al., 2022). This can be further\naugmented by aligning the model with human pref-\nerences using Reinforcement learning with Human\nFeedback (RLHF). However, RLHF demands a\nvast collection of manually curated preference data.\nIn this paper, our primary delve into the explo-\nration of LoRA and qLoRA. Their minimal infras-\ntructure requirements and cost-effectiveness make\nthese techniques especially ﬁtting for our research\nobjectives.\nLoRA is a ﬁne-tuning methodology that decom-\nposes the model’s weight matrix into a low-rank ap-\nproximation. This approximation is subsequently\ntrained on topic-speciﬁc data, capturing nuanced,\ntopic-speciﬁc intricacies while the original weights\nare kept frozen. As a result, this approach intro-\nduces fewer trainable parameters, requiring signiﬁ-\ncantly less compute power. It also requires a lower\nvolume of training data compared to more exten-\nsive ﬁne-tuning approaches.\nUpon the completion of the learning process,\nthe resultant LoRA adapter weights can be merged\nwith the original LLM weights. This produces an\nupdated set of weights for subsequent inference.\nLoRA adapters can either be integrated across the\nmodel weight layers or be selectively applied to spe-\nciﬁc layers, offering ﬂexibility in adaptation. This\nchoice dictates the number of trainable parameters\nutilized during the learning process. We conducted\na hyperparameter search on various LoRA parame-\nters, to identify the optimal combination of these\nvalues for each topic statement.\nqLoRA is a ﬁne-tuning technique designed to\nreduce the Graphics Processing Unit (GPU) mem-\nory requirements of a model by employing weight\nquantization. In essence, this method involves con-\nverting the base model’s wide range of weight val-\nues into a more compact range through min-max\nscaling. As a result, the transformed weights re-\nquire less memory for storage. To illustrate this,\nconsider the storage of integers ranging from -128\nto 127, which requires 8 bits of storage. However,\nby mapping these values to a narrower range, like\n-16 to 15, only 4 bits are needed. This reduction\nin bit width effectively decreases the memory foot-\nprint by a factor of 2. When applied across the\nmodel’s weight matrix, this quantization leads to\nsigniﬁcant memory savings. As an example, if\nthe initial model occupies 50GB of memory, the\nquantized model only needs 25GB of GPU memory.\nImportantly, the original weights can be restored by\nimplementing an inverse scaling operation, albeit\nwith a marginal loss of information.\n4.3 Explanation Generation\nOne notable advantage of using LLMs over tra-\nditional text classiﬁcation-based models is their\nability to generate explanations. By providing\nappropriate prompts, these models can generate\ncoherent reasoning to explain model predictions.\nSmaller LLMs, such as LLaMA2-13B, typically\nexhibit limited proﬁciency in reasoning through\nOOB prompting, especially when compared to their\nlarger counterparts like LLaMA2-70B (Wei et al.,\n2022). These smaller models are often less verbose\nand tend to repetitively reiterate the topic statement.\nNonetheless, there’s potential to improve the rea-\nsoning capabilities of these models through instruc-\ntion tuning. Such improvement demands access to\nannotated reasoning data, which is not readily avail-\nable for the Enron emails dataset. In this dataset,\nour seed set is restricted to binary labels (yes/no) of\nrelevance. To address this limitation, we utilize the\nLLaMa2-70B model with OOB prompting to gen-\nerate reasoning on the seed set. Subsequently, the\ngenerated reasoning (on correctly predicted labels)\nserves as the foundation for instruction tuning of\nthe LLaMA2-13B model for improved explanation\ngeneration.\n4.4 User Study\nWe conducted a user study to evaluate the qual-\nity of reasoning that different models generated\nconcerning the relevance of emails to given topic\nstatements. For this assessment, we curated a set of\n20 distinct predictions by randomly sampling from\nthe qrels. We focused solely on cases where every\nmodel made accurate predictions. We created a\nquestionnaire that was distributed to ﬁve eDiscov-\nery SMSs who has knowledge in the domain of\nlegal document review.\nThe primary objective of this study was to gauge\nhuman preferences for reasoning generated by dif-\nferent models. Each annotator was presented with a\nquestion comprising the email text, the topic state-\nment, the actual label, and ﬁve separate reasoning\noutputs generated by ﬁve different techniques. A\nsnapshot of this survey is shown in AppendixA.2.\nOf the ﬁve techniques used, one reasoning in-\nstance was crafted by a SMS from our eDiscovery\nteam (not involved in the subsequent survey). The\nother four were generated using the following meth-\nods: LLaMA2-13B OOB, LLaMA2-70B OOB,\nLLaMA2-13B ﬁne-tuned, and GPT-3.5 OOB. To\nreduce biases, these reasoning outputs were pre-\nsented to annotators in a random sequence. Anno-\ntators were then asked to rate each reasoning on a\nscale of 1 to 5, where 5 denoted the top preference\nand 1 the lowest. Annotators were urged to avoid\nassigning tied scores whenever possible, aiming to\nderive a clear ranking indicative of preference.\nInterestingly, the 13B ﬁne-tuned model occasion-\nally generated outputs similar to those of the 70B\nOOB model. This overlap was expected since the\n70B OOB model was used to create the ground\ntruth data for the 13B model. Consequently, occa-\nsional ties in the reasoning scores were anticipated.\nAfter gathering the preference data, we analysed\nthe results and ranked the models based on the\npreferences of the SMSs.\n5 Results\nIn this section, we present the results of applying\nLLMs to the predictive coding process. The train-\ning and evaluation were conducted on the seed data\nand qrels for topics 401, 402, and 403 from the\nTREC 2011 Legal track. Model performance was\nevaluated on the qrels using standard classiﬁcation\nmetrics, such as precision, recall, and macro F1\nscores.\n5.1 Prompt Engineering\nTable 1 shows the initial prompt, which through\nseveral iterations of performance improvements\non a sample set of qrels, was reﬁned to derive\nthe ﬁnal prompt. For the sake of brevity, we fo-\ncus exclusively on results for the LLaMA2-13B\nmodel. In the ﬁnal prompt, several elements col-\nlectively contribute to achieving the improved out-\ncome: the strategic emphasis on distinct terms such\nas “email”, “topic”, and “relevant”, the consistent\nuse of these terms throughout the prompt, a clear\ndemarcation between inputs and their surrounding\ncontext, precise speciﬁcation of the desired output\nstructure, and intentional guidance provided to the\nmodel for task execution.\nThe results of OOB prompting for Falcon-7B,\nLLaMA2-13B, LLaMA2-70B, and GPT-3.5 are\npresented in the top third of Table2. The results\nclearly show that GPT-3.5 outperforms other open\nLLMs when used through OOB prompting (even\nwith the more effective model-speciﬁc prompt en-\ngineering).\nWe make an interesting observation that a chosen\nprompt for one model might not produce favorable\nresults when used with other models. As a result,\nwe crafted prompts tailored to each model. The un-\nderlying rationale behind these varied outcomes are\nstill a subject of ongoing research and fall outside\nthe scope of this paper.\nIn terms of GPU memory requirements, we were\nable to conduct OOB inference on two 40GB A100\nGPUs for both Falcon-7B and LLaMA2-13B. How-\never, for the LLaMA2-70B model, a more substan-\ntial hardware conﬁguration with eight 40GB A100\nGPUs was required.\n5.2 Instruction Tuning\nWe limited our model selection for instruction tun-\ning to those compatible with two 40GB A100\nGPUs. Consequently, the instruction tuning pro-\ncess was solely performed on the Falcon-7B and\nLLaMa2-13B models. Due to the signiﬁcant cost\nof instruction tuning for GPT-3.5, we opted not\nto include it in this study. The instruction tuning\nprocess followed the methodologies outlined in\nSection 4.2, and the improved results achieved af-\nter extensive hyperparameter tuning (described in\nAppendix A.3) are detailed in the last four rows of\nTable 2.\nIn order to instruction tune our models, we used\nthe improved prompt described in the preceding\nsubsection. This ﬁne-tuning process utilized the\nseed set and was subsequently evaluated against the\nqrels. As shown in Table2, for both LLaMA2-13B\nand Falcon-7b models, the LoRA instruction tuned\nmodels outperform their OOB counterparts in de-\ntermining relevancy. This is quite signiﬁcant for\ntopic 401 as the seed set was sufﬁciently balanced.\nFor Falcon, we saw an improvement from an F1\nscore of 0.39 to 0.79, while for LLaMA2-13B this\nYou are a subject matter expert reviewing a As a subject matter expert, youQr task is to read the\ndocument to evaluate if it is related to a topic.following email and determine from its contents\nRespond with Yes if the document is directlywhether it is related to the provided topic.\nor indirectly related to the topic or No if notEmail: \"\"\"email_text\"\"\"\nrelated. On a new line, give a reason. Topic: \"\"\"topic_statement\"\"\"\nTopic: topic_statement Task: Decide whether the email is related to the\nEmail: email_text topic or not. Provide a simple ’yes’ or ’no’ answer.\nAdditionally, give a reason to support your answer,\nby summarizing parts of email that helped\nyou make this decision.\nAnswer and Reason:\n(a) (b)\nTable 1: The initial prompt (a) and the ﬁnal improved prompt (b) were iteratively developed for the LLaMA2-13B\nmodel. An evaluation was conducted using a random sample of 50 questions spanning the three topics. The initial\nprompt yielded a macro average F1 score of 0.29, whereas the improved prompt achieved a signiﬁcantly improved\nF1 score of 0.51. Improved prompt (b) exhibits a deliberate emphasis on speciﬁc terms, such as “email”, “topic”,\nand “related”, which are consistently employed throughout the prompt. Notably, the email and topic statements are\ndistinctly delineated by the use of triple quotes. Additionally, a structured output format is outlined. The model is\ndirected to provide its reasoning by “summarizing parts of the emails”. This iterative reﬁnement process represents\na systematic and methodical approach to enhance the prompt, resulting in improved performance for classiﬁcation\nand reasoning.\n401 402 403\nType Model F1 Precision Recall F1 Precision Recall F1 Precision Recall\nOOB\nFalcon 7B 0.39 0.51 0.50 0.33 0.48 0.46 0.28 0.47 0.42\nLLaMa v2 13B0.56 0.56 0.56 0.54 0.55 0.59 0.47 0.52 0.56\nLLaMa v2 70B0.58 0.71 0.66 0.63 0.69 0.61 0.66 0.66 0.65\nGPT 3.5 0.61 0.76 0.62 0.66 0.82 0.62 0.66 0.64 0.68\nLoRA Falcon 7B 0.79 0.79 0.80 0.60 0.59 0.60 0.60 0.59 0.66\nLLaMa v2 13B0.85 0.84 0.86 0.59 0.59 0.66 0.68 0.68 0.67\nqLoRA Falcon 7B 0.74 0.79 0.78 0.60 0.76 0.58 0.53 0.58 0.77\nLLaMa v2 13B0.69 0.71 0.69 0.62 0.61 0.66 0.63 0.61 0.69\nTable 2: Comparison of LLM Performance Metrics on TREC 2011 Topics 401, 402, and 403 for the assessment\nof emails relevance to topic statements. The uppermost four rows pertain to Out-of-Box (OOB) prompting, the\nsubsequent two rows are dedicated to LoRA-based ﬁne-tuned models, and the ﬁnal two rows concern qLoRA\nﬁne-tuned models.\njumped from 0.56 to 0.85. Both these models, sur-\npassed GPT-3.5-turbo OOB results on this topic\nstatement. These results empirically highlight the\nadvantages of LoRA instruction tuning. Similarly,\nfor qLoRA ﬁnetuned models, we see a performance\nimprovement compared to OOB prompting across\nall the topics. However, this is not as signiﬁcant as\nLoRA ﬁnetuned models.\n5.3 Quality of reasoning\nThe summary of the user study for evaluating the\nquality of reasoning can be found in Fig3. The\nﬁgure shows the mean score obtained by different\ntechniques that were used for reasoning genera-\ntion. Notably, of the ﬁve reasoning techniques\nassessed, the rationale generated by the LLaMA2-\n70B OOB model was the one preferred by the\nSMSs of our eDiscovery team, achieving a mean\npreference score of 3.58 with a standard error of\n0.12. On the other hand, the reasoning produced\nby the LLaMA2-13B OOB model was the least\nfavored, as reﬂected by its mean score of 2.08\nand a standard error of 0.14. Furthermore, the rea-\nsoning generated by the LLaMA2-70B OOB and\nLLaMA2-13B ﬁne-tuned models were preferred\nover SMS generated reasoning, which had a mean\nscore of 3.01 and standard error of 0.15.\nIt is interesting to observe that the reasoning ca-\npability of the LLaMA2-13B model signiﬁcantly\nimproved after ﬁne-tuning, as supported by the\n(a)\n (b)\n (c)\nFigure 2: Frequency distribution of reasoning preference scores ascribed to various techniques by the 5 Subject\nMatter Specialists (SMSs) engaged in the survey. A total of 20 email samples were presented to each SMSs, along\nwith their relevance to a topic (401). They were provided with ﬁve distinct rationales, generated by ﬁve distinct\ntechniques, in support of the email’s topical relevance. SMSs were instructed to score the reasonings on a scale\nof 1-5, wherein 5 signiﬁed the highest preference and 1 indicated the lowest preference.2(a) shows the number\nof times a reasoning generation technique is ranked at the ﬁrst position by SMSs.2(b) shows the sum of scores\nreceived by each technique on the 20 questions for each SMSs.2(c) shows the distribution of scores received by\neach model across the SMSs and questions.\nFigure 3: Mean and standard error of reasoning prefer-\nence scores for each model as rated by SMSs\nSMS preferences. This improvement is reﬂected in\na mean score of 3.4 and a standard error of 0.12, fur-\nther corroborating our initial hypothesis that ﬁne-\ntuning contributes to improved performance. Sur-\nprisingly, the average preference score for GPT-3.5\nwas notably lower, coming in at 2.94 with a stan-\ndard error of 0.13.\nDetailed results of the survey are shown in Fig-\nure 2. Figure 2(a) provides insights into the fre-\nquency with which a technique was ranked high-\nest by a SMS. Figure2(b) shows the cumulative\nscores accrued by various techniques during when\nassessed by different SMSs. Figure2(c) displays\nthe distribution of scores that individual models\nreceived from the entire pool of SMSs and ques-\ntions. Notably, of all the techniques reviewed, the\nLLaMA2-13B OOB model consistently registered\na low score in a majority of evaluations conducted\nby the SMSs.\n6 Conclusion\nThis study is among of the ﬁrst works to assess\nthe efﬁcacy of generative LLMs in the eDiscov-\nery document review process. We compare the\nclassiﬁcation performance of various open and\ncommercially-licensed LLMs, and demonstrate\nthat although OOB performance of open LLMs are\nworse compared to GPT-3.5, these models can be\nﬁnetuned to achieve comparable results to GPT-3.5.\nMoreover, we conduct a user study and show that\nthe SMSs favored AI-generated reasoning over hu-\nman reasoning, underscoring the viability of these\napproaches in eDiscovery.\nIn terms of scalability, we suggest that there is\nno inherent need to deploy large models such as\nGPT-3.5 or LLaMa2-70B for production purposes.\nThe LLaMA2-13B model, which ﬁts on just two\n40GB A100 GPUs, can be ﬁne-tuned to yield sim-\nilar classiﬁcation performance and reasoning that\nrivals human reasoning – as supported by our user\nstudy. This approach offers signiﬁcant savings in\ninfrastructure costs associated with model deploy-\nment.\n7 Future Work\nIn future, we aim to extend the scope of this work\nby including all the TREC topics from 2009 to\n2011. Additionally, we intend to enhance our\nframework by including attachments, allowing for\na deeper analysis. We also plan to undertake the\nprioritization task, wherein we focus on comput-\ning ranking metrics such as F1@k, precision@k,\nrecall@k, wherek represents the number of doc-\numents reviewed. This will allow us to conduct a\nthorough examination of model performance across\nthe Enron emails corpus and in turn help us iden-\ntify the speciﬁc ‘k’ at which recall surpasses certain\ncourt-mandated thresholds.\nFurthermore, we plan to conduct a large-scale\nuser study, involving a broader cohort of annota-\ntors and an expanded array of survey questions.\nWe also plan to assess the degree of inter-annotator\nagreement within this extended study, employing\nestablished metrics such as Cohens Kappa (Smee-\nton, 1985) or Krippendorf Alpha (Gwet, 2011) to\nquantify the level of consensus among annotators.\nReferences\nTouﬁque Ahmed and Premkumar Devanbu. 2022.\nFew-shot training llms for project-speciﬁc code-\nsummarization. In Proceedings of the 37th\nIEEE/ACM International Conference on Automated\nSoftware Engineering, pages 1–5.\nMichał Araszkiewicz, Trevor Bench-Capon, Enrico\nFrancesconi, Marc Lauritsen, and Antonino Rotolo.\n2022. Thirty years of artiﬁcial intelligence and\nlaw: overviews. Artiﬁcial Intelligence and Law,\n30(4):593–610.\nKevin D Ashley and Will Bridewell. 2010. Emerg-\ning ai & law approaches to automating analysis and\nretrieval of electronically stored information in dis-\ncovery proceedings.Artiﬁcial Intelligence and Law,\n18:311–320.\nThomas Barnett, Svetlana Godjevac, Jean-Michel Ren-\nders, Caroline Privault, John Schneider, and Robert\nWickstrom. 2009. Machine learning classiﬁcation\nfor document review. InDESI III: The ICAIL Work-\nshop on Globaal E-Discovery/E-Disclosure. Citeseer\nPrinceton, NJ, USA.\nJason R Baron, R Braman, K Withers, T Allman, M Da-\nley, and G Paul. 2007. The sedona conference® best\npractices commentary on the use of search and in-\nformation retrieval methods in e-discovery. InThe\nSedona conference journal, volume 8.\nJason R Baron, David D Lewis, and Douglas W Oard.\n2006. Trec 2006 legal track overview. InTREC.\nCiteseer.\nDavid C Blair and Melvin E Maron. 1985. An evalua-\ntion of retrieval effectiveness for a full-text document-\nretrieval system. Communications of the ACM,\n28(3):289–299.\nShannon Brown. 2015. Peeking inside the black box:\nA preliminary survey of technology assisted review\n(tar) and predictive coding algorithms for ediscovery.\nSuffolk J. Trial & App. Advoc., 21:221.\nRishi Chhatwal, Peter Gronvall, Nathaniel Huber-Fliﬂet,\nRobert Keeling, Jianping Zhang, and Haozhen Zhao.\n2018. Explainable text classiﬁcation in legal docu-\nment review a case study of explainable predictive\ncoding. In 2018 IEEE international conference on\nbig data (Big Data), pages 1905–1911. IEEE.\nJack G Conrad. 2010. E-discovery revisited: the need\nfor artiﬁcial intelligence beyond information retrieval.\nArtiﬁcial Intelligence and Law, 18(4):321–345.\nGordon V Cormack and Maura R Grossman. 2014.\nEvaluation of machine-learning protocols for\ntechnology-assisted review in electronic discovery.\nIn Proceedings of the 37th international ACM SIGIR\nconference on Research & development in informa-\ntion retrieval, pages 153–162.\nGordon V Cormack and Maura R Grossman. 2016. Scal-\nability of continuous active learning for reliable high-\nrecall text classiﬁcation. InProceedings of the 25th\nACM international on conference on information and\nknowledge management, pages 1039–1048.\nGordon V Cormack, Maura R Grossman, Bruce Hedin,\nand Douglas W Oard. 2010. Overview of the trec\n2010 legal track. InTREC.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B\nHall, and Ming-Wei Chang. 2022. Promptagator:\nFew-shot dense retrieval from 8 examples.arXiv\npreprint arXiv:2209.11755.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023.Qlora: Efﬁcient ﬁnetuning\nof quantized llms.\nDebasis Ganguly, Jack G Conrad, Kripabandhu Ghosh,\nSaptarshi Ghosh, Pawan Goyal, Paheli Bhattacharya,\nShubham Kumar Nigam, and Shounak Paul. 2023.\nLegal ir and nlp: the history, challenges, and state-\nof-the-art. InEuropean Conference on Information\nRetrieval, pages 331–340. Springer.\nMaura R. Grossman, Gordon V . Cormack, Bruce Hedin,\nand Douglas W. Oard. 2011.Overview of the TREC\n2011 legal track. In Proceedings of The Twentieth\nText REtrieval Conference, TREC 2011, Gaithers-\nburg, Maryland, USA, November 15-18, 2011, vol-\nume 500-296 ofNIST Special Publication. National\nInstitute of Standards and Technology (NIST).\nKilem L Gwet. 2011. On the krippendorff’s alpha co-\nefﬁcient. Manuscript submitted for publication. Re-\ntrieved October, 2(2011):2011.\nBruce Hedin, Stephen Tomlinson, Jason R Baron, and\nDouglas W Oard. 2009. Overview of the trec 2009\nlegal track. InTREC.\nYupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu,\nRuobing Xie, Julian McAuley, and Wayne Xin\nZhao. 2023. Large language models are zero-shot\nrankers for recommender systems.arXiv preprint\narXiv:2305.08845.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021.Lora: Low-rank adaptation of\nlarge language models.\nDavid D Lewis. 2010. Afterword: data, knowledge,\nand e-discovery. Artiﬁcial Intelligence and Law,\n18(4):481–486.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n2022. Few-shot parameter-efﬁcient ﬁne-tuning is\nbetter and cheaper than in-context learning.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nGraham McDonald, Craig Macdonald, and Iadh Ounis.\n2018. Active learning strategies for technology as-\nsisted sensitivity review. InEuropean Conference on\nInformation Retrieval, pages 439–453. Springer.\nDouglas W Oard, Björn Hedin, Stephen Tomlinson, and\nJason R Baron. 2008. Overview of the trec 2008\nlegal track. InTREC, pages 500–277.\nDouglas W Oard, William Webber, et al. 2013. Infor-\nmation retrieval for e-discovery.Foundations and\nTrends® in Information Retrieval, 7(2–3):99–237.\nNigel C. Smeeton. 1985.Early history of the kappa\nstatistic. Biometrics, 41(3):795–795.\nStephen Tomlinson, Douglas W Oard, Jason R Baron,\nand Paul Thompson. 2007. Overview of the trec 2007\nlegal track. InTREC.\nPetter Törnberg. 2023. Chatgpt-4 outperforms experts\nand crowd workers in annotating political twitter\nmessages with zero-shot learning. arXiv preprint\narXiv:2304.06588.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023.Llama 2: Open foundation and ﬁne-\ntuned chat models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.Advances in neural information processing\nsystems, 30.\nS Villata et al. 2020. Sentence embeddings and high-\nspeed similarity search for fast computer assisted an-\nnotation of legal documents. InLegal Knowledge and\nInformation Systems: JURIX 2020: The Thirty-third\nAnnual Conference, Brno, Czech Republic, Decem-\nber 9-11, 2020, volume 334, page 164. IOS Press.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners.arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models.Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nEugene Yang, David D. Lewis, and Ophir Frieder. 2021.\nOn minimizing cost in legal document review work-\nﬂows. InProceedings of the 21st ACM Symposium\non Document Engineering. ACM.\nA Appendix\nA.1 Topic Statements\nThe three topics, numbered 401, 402, and 403, used\nin the TREC 2011 Legal Track learning task are\ndescribed below:\nA.1.1 Topic 401\nAll documents or communications that describe,\ndiscuss, refer to, report on, or relate to the design,\ndevelopment, operation, or marketing of enronon-\nline, or any other online service offered, provided,\nor used by the Company (or any of its subsidiaries,\npredecessors, or successors-in-interest), for the pur-\nchase, sale, trading, or exchange of ﬁnancial or\nother instruments or products, including but not\nlimited to, derivative instruments, commodities, fu-\ntures, and swaps.\nA.1.2 Topic 402\nAll documents or communications that describe,\ndiscuss, refer to, report on, or relate to whether the\npurchase, sale, trading, or exchange of over-the-\ncounter derivatives, or any other actual or contem-\nplated ﬁnancial instruments or products, is, was,\nwould be, or will be legal or illegal, or permitted or\nprohibited, under any existing or proposed rule(s),\nregulation(s), law(s), standard(s), or other proscrip-\ntion(s), whether domestic or foreign.\nA.1.3 Topic 403\nAll documents or communications that describe,\ndiscuss, refer to, report on, or relate to the environ-\nmental impact of any activity or activities under-\ntaken by the Company, including but not limited\nto, any measures taken to conform to, comply with,\navoid, circumvent, or inﬂuence any existing or pro-\nposed rule(s), regulation(s), law(s), standard(s), or\nother proscription(s), such as those governing envi-\nronmental emissions, spills, pollution, noise, and/or\nanimal habitats.\nA.2 User Survey\nFigure 4 shows the snapshot of the survey spread-\nsheet provided to the SMSs. The ﬁrst part contains\nthe instructions and the topic statement. These\npanes were frozen while the annotator could scroll\nthrough the rest of the data. Each row consisted\nof an email content, followed by the relevancy to\nthe topic statement, and the ﬁve reasonings which\nthe annotators had to rate. The reasonings were\nrandomly shufﬂed to prevent annotator bias.\nA.3 Hyperparameter Ranges\nWe conducted an extensive hyperparameter search\nto tune model parameters for instruction tuning. We\nprimarily tuned LoRA parameters such as rank (r),\nscaling factor (alpha), target modules (the layers\nwhere the adapters are inserted), learning rate, and\nnumber of epochs. For the rank parameter, we\nconsidered values of 4, 8, 16, and 32. The scaling\nfactor was checked for values of 16 and 32. For\nadapter placement, we examined insertion into one\nor more of the following layers: query-key-value,\nattention output, upsampling, or downsampling.\nThe learning rate was varied across three orders\nof magnitude: 1e-3, 1e-4, and 1e-5. Similarly, the\nnumber of epochs was chosen from the range of\n1 to 4. To ensure a detailed exploration of the\nhyperparameter space, we randomly sampled 16\ndistinct combinations from these ranges for each\nmodel. All the qLoRA experiments were done in 4\nbit precision with same hyperparameter ranges as\ndescribed above.\nFigure 4: Snapshot of the survey spreadsheet.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5771327614784241
    },
    {
      "name": "Natural language",
      "score": 0.4890654981136322
    },
    {
      "name": "Programming language",
      "score": 0.40025269985198975
    },
    {
      "name": "Natural language processing",
      "score": 0.37033790349960327
    },
    {
      "name": "Art history",
      "score": 0.36247187852859497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32562610507011414
    },
    {
      "name": "Art",
      "score": 0.23631033301353455
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145325580",
      "name": "Deloitte (United States)",
      "country": "US"
    }
  ],
  "cited_by": 2
}