{
  "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models",
  "url": "https://openalex.org/W3201662444",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5091793300",
      "name": "Mohammed Saeed",
      "affiliations": [
        "EURECOM"
      ]
    },
    {
      "id": "https://openalex.org/A5046145551",
      "name": "Naser Ahmadi",
      "affiliations": [
        "EURECOM"
      ]
    },
    {
      "id": "https://openalex.org/A5012055259",
      "name": "Preslav Nakov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011336242",
      "name": "Paolo Papotti",
      "affiliations": [
        "EURECOM"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3112170794",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W1606891084",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3013770059",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4287758766",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W4211148418",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3108531607",
    "https://openalex.org/W2009718960",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102094970",
    "https://openalex.org/W2087451659",
    "https://openalex.org/W2743151379",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2946267351",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2307268612",
    "https://openalex.org/W2154474435",
    "https://openalex.org/W2898844580",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W2107306718",
    "https://openalex.org/W3118069529",
    "https://openalex.org/W2725395424",
    "https://openalex.org/W2995359496",
    "https://openalex.org/W2963763958",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W3214897310",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3172053684",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2964194917",
    "https://openalex.org/W4229658977",
    "https://openalex.org/W1493657719",
    "https://openalex.org/W3104178968",
    "https://openalex.org/W3101204082",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W2997359684",
    "https://openalex.org/W2785119431",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3011762034",
    "https://openalex.org/W2952068915",
    "https://openalex.org/W3127227595"
  ],
  "abstract": "While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460–1476\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1460\nRULE BERT: Teaching Soft Rules to Pre-Trained Language Models\nMohammed Saeed† Naser Ahmadi† Preslav Nakov‡ Paolo Papotti†\n†Eurecom, France ‡Qatar Computing Research Institute, HBKU, Qatar\n{ FirstName.LastName }@eurecom.fr, pnakov@hbku.edu.qa\nAbstract\nWhile pre-trained language models (PLMs)\nare the go-to solution to tackle many natural\nlanguage processing problems, they are still\nvery limited in their ability to capture and to\nuse common-sense knowledge. In fact, even if\ninformation is available in the form of approx-\nimate (soft) logical rules, it is not clear how to\ntransfer it to a PLM in order to improve its per-\nformance for deductive reasoning tasks. Here,\nwe aim to bridge this gap by teaching PLMs\nhow to reason with soft Horn rules. We in-\ntroduce a classiﬁcation task where, given facts\nand soft rules, the PLM should return a pre-\ndiction with a probability for a given hypoth-\nesis. We release the ﬁrst dataset for this task,\nand we propose a revised loss function that en-\nables the PLM to learn how to predict precise\nprobabilities for the task. Our evaluation re-\nsults show that the resulting ﬁne-tuned models\nachieve very high performance, even on logi-\ncal rules that were unseen at training. More-\nover, we demonstrate that logical notions ex-\npressed by the rules are transferred to the ﬁne-\ntuned model, yielding state-of-the-art results\non external datasets.\n1 Introduction\nPre-trained language models (PLMs) based on\ntransformers (Devlin et al., 2019; Liu et al., 2020)\nare established tools for capturing both linguistic\nand factual knowledge (Clark et al., 2019b; Rogers\net al., 2020). However, even the largest models\nfail on basic reasoning tasks. If we consider com-\nmon relations between entities, we see that such\nmodels are not aware of negation, inversion (e.g.,\nparent-child), symmetry (e.g., spouse), implica-\ntion, and composition. While these are obvious\nto a human, they are challenging to learn from\ntext corpora as they go beyond linguistic and fac-\ntual knowledge (Ribeiro et al., 2020; Kassner and\nSchütze, 2020). We claim that such reasoning prim-\nitives can be transferred to the PLMs by leveraging\nlogical rules, such as those shown in Figure 1.\nInput facts:\nMike is the parent of Anne. Anne lives with Mark.\nAnne is the child of Laure. Anne lives with Mike.\nInput rules:\n(r1, .1) Two persons living together are married.\n(r2, .7) Persons with a common child are married.\n(r3, .9) Someone cannot be married to his/her child.\n(r4, 1) Every person is the parent of his/her child.\nTest 1: Laure and Mike are married.\nAnswer: True with probability 0.7 [r4,r2]\nTest 2: Anne and Mark are married.\nAnswer: False with probability 0.9 [r1]\nTest 3: Anne and Mike are married.\nAnswer: False with probability 0.9 [r1,r3,r4]\nFigure 1: Examples of hypotheses that require reason-\ning using facts and possibly conﬂicting soft rules (rule\nid and conﬁdence shown in brackets).\nWhile there have been initial attempts to teach\nreasoning with rules to PLMs (Clark et al., 2020;\nKassner et al., 2020), such approaches model only\na subclass of logical rules. In fact, current solutions\nfocus on exact rules, i.e., rules that hold in all cases.\nIn reality, most of the rules are approximate, or soft,\nand thus have a certain conﬁdence of being correct.\nFor example, across the 7,015 logical rules deﬁned\non the DBpedia knowledge graph, only 11% have a\nconﬁdence above 95%. In the example, rules r1–r3\nare soft, i.e., cover knowledge that is not true in\nall circumstances. Consider rule r2, stating that if\ntwo persons have a child in common, they are most\nlikely married. As r2 has a conﬁdence of being\ncorrect of 0.7, this uncertainty is reﬂected in the\nprobability of the prediction.\nWith the above considerations in mind, here we\nshow how to reason over soft logical rules with\nPLMs. We provide facts and rules expressed in\nnatural language, and we ask the PLM to come up\nwith a logical conclusion for a hypothesis, together\nwith the probability for it being true.\n1461\nUnlike previous approaches (Clark et al., 2020),\nwe enable deductive reasoning for a large class of\nsoft rules with binary predicates and an unrestricted\nnumber of variables. Our model can even reason\nover settings with conﬂicting evidence, as shown\nin Test 3 in Figure 1. In the example, as Anne and\nMike live together, they have a 0.1 probability of\nbeing married because of soft rule r1. However,\nwe can derive from exact rule r4 that Anne is the\nchild of Mike and therefore they cannot be married,\naccording to soft rule r3.\nTo model uncertainty, we pick one ﬂavor of prob-\nabilistic logic programming languages, LPMLN,\nfor reasoning with soft rules (Lee and Wang, 2016).\nIt assigns weights to stable models, similarly to\nhow Markov Logic assigns weights to models.\nHowever, our method is independent of the logic\nprogramming approach at hand, and different mod-\nels can be ﬁne-tuned with different programming\nsolutions. Our proposal makes use of synthetic\nexamples that “teach” the desired formal behavior\nthrough ﬁne-tuning. In particular, we express the\nuncertainty in the loss function used for ﬁne-tuning\nby explicitly mimicking the results for the same\nproblem modeled with LPMLN.\nOur contributions can be summarized as follows:\n• We introduce the problem of teaching soft\nrules expressed in a synthetic language to\nPLMs through ﬁne-tuning (modeled as binary\nclassiﬁcation).\n• We create and release the ﬁrst dataset for this\ntask, which contains 3.2M examples derived\nfrom 161 rules describing real common-sense\npatterns with the target probability for the task\nobtained from a formal reasoner (Section 4).\n• We introduce techniques to predict the cor-\nrect probability of the reasoning output for the\ngiven soft rules and facts. Our solution re-\nlies on a revised loss function that effectively\nmodels the uncertainty of the rules (Section 5).\nOur approach handles multi-variable rules and\nnicely extends to examples that require rea-\nsoning over multiple input rules.\n• We show that our approach enables ﬁne-tuned\nmodels to yield prediction probability very\nclose to that produced by a formal reasoner\n(Section 6). Our PLM ﬁne-tuned on soft rules,\nRULE BERT, can effectively reason with facts\nand rules that it has not seen at training, even\nwhen ﬁne-tuned with only 20 rules.\n• We demonstrate that our ﬁne-tuning approach\neffectively transfers knowledge about predi-\ncate negation and symmetry to the lower lev-\nels of the transformer, which beneﬁts from\nthe logical notions in the rules. In particu-\nlar, RULE BERT achieves new state-of-the-art\nresults on three external datasets.\nThe data, the code, and the ﬁne-tuned model\nare available at http://github.com/MhmdSaiid/\nRuleBert.\n2 Related Work\nPLMs have been shown to have some reasoning\ncapabilities (Talmor et al., 2020b), but fail on ba-\nsic reasoning tasks (Talmor et al., 2020a) and are\ninconsistent (Elazar et al., 2021), especially when\nit comes to negation (Kassner and Schütze, 2020).\nOur work focuses on deductive reasoning. Note\nthat it is different from previous work, e.g., on\nmeasuring the factual knowledge of PLMs (Petroni\net al., 2019), on probing the commonsense capa-\nbilities of PLMs at the token or at the sentence\nlevel (Zhou et al., 2020), or on testing the rea-\nsoning capabilities of PLMs on tasks such as age\ncomparison and taxonomy conjunction (Talmor\net al., 2020a). Our work relates to Task #15 in\nthe bAbI dataset (Weston et al., 2016) and to Rule-\nTakers (Clark et al., 2020). However, we differ\n(i) by using a larger subclass of ﬁrst-order logic\nrules (with more variables and various forms), and\n(ii) by incorporating soft rules.\nOur proposal is different from work on Question\nAnswering (QA) with implicit reasoning based on\ncommon-sense knowledge (Clark et al., 2019a), as\nwe rely purely on deductive logic from explicitly\nstated rules.\nOur approach also differs from methods that se-\nmantically parse natural language into a formal\nrepresentation on which a formal reasoner can be\napplied (Liang, 2016), as we directly reason with\nlanguage. Yet, we are also different from Natural\nLanguage Inference (NLI) and textual entailment,\nwhich work with text directly, but cannot handle\nHorn rules (MacCartney and Manning, 2009; Da-\ngan et al., 2013).\nUnlike previous work (Hamilton et al., 2018;\nYang et al., 2017; Minervini et al., 2020), we do not\ndesign a new, ad-hoc module for neural reasoning,\nbut we rely solely on the transformer’s capability\nto emulate algorithms (Wang et al., 2019b; Lample\nand Charton, 2020).\n1462\n3 Background\nLanguage Models. We focus on language mod-\nels pre-trained with bidirectional transformer en-\ncoders using masked language modeling (Devlin\net al., 2019). For ﬁne-tuning, we create examples\nfor sequence classiﬁcation to teach the models how\nto emulate reasoning given facts and soft rules.\nLogical Rules. We rely on existing corpora of\ndeclarative Horn rules mined from large RDF\nknowledge bases (KBs) (Galárraga et al., 2015;\nOrtona et al., 2018; Ahmadi et al., 2020). An RDF\nKB is a database representing information with\ntriples (or facts) p(s,o), where a predicate pcon-\nnects a subject sand an object o. An atom in a rule\nis a predicate connecting two universally quantiﬁed\nvariables. A Horn rule (or clause) has the form:\nB →h(x,y), where h(x,y) is a single atom (head\nor conclusion of the rule) and B(body or premise\nof the rule) is a conjunction of atoms. Positive\nrules identify relationships between entities, e.g.,\nr1, r2, r4 in Figure 1. Negative rules identify con-\ntradictions, e.g., r3 in Figure 1. Rules can contain\npredicates comparing numerical values, such as <.\nFor example, negative rule r5: birthYear(b,d) ∧\nfoundYear(a,c) ∧<(c,d) →negfounder(a,b) states\nthat any person (variable b) with a birth year ( d)\nhigher than the founding year ( c) of a company\n(a) cannot be its founder. A fact is derived from\na rule if all the variables in the rule body are re-\nplaced with constants from facts. For r5, facts\n“foundYear(Ford,1903), birthYear(E. Musk,1971),\n>(1971,1903)” trigger the rule that derives the fact\nnegFounder(E. Musk, Ford).\nRule Conﬁdence. Exact rules, such as r4, apply\nin all cases, without exception. However, most\nrules are approximate, or soft, as they apply with a\ncertain likelihood. For example, r3 in Figure 1 is\ntrue in most cases, but there are historical excep-\ntions in royal families. Rules are annotated with a\nmeasure of this likelihood, either manually or with\na computed conﬁdence (Galárraga et al., 2015).\nProbabilistic Answer Set Programming. As\nwe deal with soft rules, we adopt LPMLN (Lee\nand Wang, 2016) to create the dataset. LPMLN is\na probabilistic extension of answer set programs\n(ASP) with the concept of weighted rules from\nMarkov Logic (Baral, 2010). In ASP, search prob-\nlems are reduced to computing stable models (an-\nswer sets), a set of beliefs described by the program.\nA weight (or conﬁdence) is assigned to each rule,\nso that the more rules a stable model satisﬁes, the\nlarger weight it gets, and the probability of the sta-\nble model is computed by normalizing its weight\namong all stable models. Given a set of soft rules\nand facts, we measure how much the hypothesis is\nsupported by the stable model.\n4 Dataset\nWe start by deﬁning the reasoning task. We then\ndiscuss example generation methods for three sce-\nnarios: single rule as input, multiple (possibly con-\nﬂicting) rules that require reasoning for the same\nconclusion, and multiple rules that require a se-\nquence (chain) of reasoning steps. Examples of the\ndata generation procedures are in the Appendix.\n4.1 Reasoning Task\nEach example is a triple (context, hypothesis, con-\nﬁdence). Context is a combination of rule(s) and\ngenerated facts, such as “ If the ﬁrst person lives\ntogether with the second person, then the ﬁrst per-\nson is the spouse of the second person.” and “Anne\nlives with Mike.” Hypothesis is the statement to\nbe assessed based on the context, e.g., “ Laure is\nthe spouse of Mike.” Conﬁdence is the probability\nthat the hypothesis is valid given by the reasoner,\ne.g., 0.7. As we generate the examples, we know\nthe conﬁdence for each hypothesis.\n4.2 Single-Rule Dataset Generation\nGiven a rule, we generate examples of different\nhypotheses to expose the model to various contexts.\nEach example contains the context cand a hypoth-\nesis hwith its probability of being true as obtained\nfor the (c,h) pair from the LPMLN reasoner. The\nintuition is that the examples show the expected be-\nhavior of a formal reasoner for every combination\nof possible facts for a given rule. This process is\nnot about teaching the model speciﬁc facts to recall\nlater, but teaching it reasoning patterns.\nUnlike previous work (Clark et al., 2020), our\nrules allow for multiple variables. This introduces\nadditional complexity as examples must show how\nto deal with the symmetry of the predicate. For\nexample, child(Alice,Bob) and child(Bob,Alice) are\nnot equivalent since child is not symmetric, while\nspouse(Alice,Bob) and spouse(Bob,Alice) are equiv-\nalent as spouse is symmetric. We assume that meta-\ndata about the symmetry and the types is available\nfrom the KB for the predicates in the rules.\n1463\nGiven as input (i) a rule r, (ii) a desired number\nn of examples, ( iii) an integer m to indicate the\nmaximum number of facts given as a context, and\n(iv) a pool of values for each type involved in r’s\npredicate pools, Algorithm 1 outputs a dataset D\nof generated examples.\nAlgorithm 1: Generate Synthetic Data\nInput: rule r; // child(a,b)→parent(b,a)\nn; // # of examples\nm; // max # of facts\npools; // pools of names\nOutput: Generated Dataset D\n1 D= {}, i= 1; // initialize\n2 while i≤ceiling(n/8) do\n3 F = GenFacts(r,m,pools) ;\n// child(Eve,Bob),parent(Amy,Sam)\n4 O= LPMLN (r,F) ; // reasoner output\n5 h1 = f ∈F ; // child(Eve,Bob)\n6 h2 = Alter(f) ; // negchild(Eve,Bob)\n7 h3 = r(F) ; // parent(Bob,Eve)\n8 h4 = Alter(r(F)) ; // parent(Eve,Bob)\n9 h5 = pos.fl /∈F ; // child(Joe,Garry)\n10 h6 = ¬h5 ; // negchild(Joe,Garry)\n11 h7 = fr /∈O; // parent(Alice,Joe)\n12 h8 = ¬h7 ; // negparent(Alice,Joe)\n13 D.add(h1−8);\n14 i←i+ 1;\n15 Function GenFacts(r,m,pools):\n16 F = GetRandomFacts(r,pools,m);\n17 F.add(GetRuleFacts(r,pools));\n18 return F\n19 Function Alter(p(s,o)):\n20 if p is symmetric then return ¬p(s,o) ;\n21 if random()>0.5 then return ¬p(s,o) ;\n22 else return p(o,s) ;\nWe start at line 3 by generating facts, such as\nchild(Eve,Bob), using the function GenFacts (lines\n15–18), which takes as input a ruler, the maximum\nnumber of facts mto generate, and the pools. A\nrandom integer less thanmsets the number of facts\nin the current context. The generated facts F have\npredicates from the body ofr, their polarity (true or\nnegated atom) is assigned randomly, and variables\nare instantiated with values sampled from the pool\n(line 16). Facts are created randomly, as we are\nnot interested in teaching the model speciﬁc facts\nto recall later, but instead we want to teach it how\nto reason with different combinations of rules and\nfacts. We then ensure that the rule is triggered in\nevery context, eventually adding more facts to F\nusing the function GetRuleFacts in line 17. After\nobtaining F, we feed rule r along with facts F\nto the LPMLN reasoner, and we obtain a set O\ncontaining all satisﬁed facts and rule conclusions\n(line 4).\nWe generate different hypotheses, where each\none leads to an example in dataset D. For each\ncontext, we add an example with different facts\nwith respect to the given rule according to three\ndimensions. A fact can be ( i) for a predicate in\nthe premise or in the conclusion of a rule, could\nbe (ii) satisﬁed or unsatisﬁed given the rule, and\ncould have (iii) positive or negative polarity. This\nmakes eight different possibilities, thus leading to\nthe generation of eight different hypotheses (one\nfor each context).\nThe ﬁrst hypothesis h1 is obtained by sampling\na fact from the set F (line 5). We then produce\nthe counter hypothesis h2 by altering the fact (line\n6) using the function Alter (lines 19-22). Given a\nhypothesis p(s,o) (line 19), we return its negated\nform if pis symmetric (line 20). Otherwise, if pis\nnot symmetric, we produce a counter hypothesis\neither by negation (line 21), or by switching the\nsubject and the object in the triple as the predicate\nis not symmetric (line 22). We rely on a dictionary\nto check whether a predicate is symmetric or not.\nWe then produce hypothesis h3 (line 7), which\nis the outcome of triggering rule rwith the facts\nadded in line 17. The counter hypothesis h4 is gen-\nerated by altering h3 (line 8). Moreover, we gen-\nerate hypothesis h5 by considering any unsatisﬁed\npositive fact outside F. Following a closed-world\nassumption (CW A), we assume that positive triples\nare false if they cannot be proven, meaning that\ntheir negation is true. We sample a fact fl from the\nset of all possible positive facts that do not have the\nsame predicate of the rule head (line 9). Thus, h5\nwill never be in the output Oof the reasoner, as it\ncannot be derived. We then produceh6 by negating\nh5 in line 10. We further derive h7 by sampling a\nfact fr that has the same predicate as that of the\nrule head, but does not belong to the output of the\nreasoner O(line 11). For a positive (negative) rule,\nsuch a fact is labelled as False (True). h7 is then\nnegated to get the counter hypothesis h8 (line 12).\nAll generated hypotheses are added to D(line 13),\nand the process repeats until we obtain nexamples.\nFinally, we automatically convert the examples\nto natural language using predeﬁned templates. A\nbasic template for atom predicate p(type t1, type\nt2) is “If the 1 st t1 is p of the 2 nd t2.” (“If the\nﬁrst person is spouse of ... ”). For the single-rule\nscenario, we release a dataset for 161 rules with a\ntotal of 3.2M examples and a 80%:10%:10% split\nfor training, validation, and testing.\n1464\n4.3 Rules with Overlapping Conclusion\nWhen multiple rules are in the context, there could\nbe facts that trigger more than one rule for a given\nhypothesis. The triggered rules might be all of the\nsame polarity (positive or negative), eventually ac-\ncumulating their conﬁdence, or could be a mix of\npositive and negative rules that oppose each other.\nWhile the data generation procedure in Section 4.2\ncan be extended to handle multiple rules, this raises\nan efﬁciency problem. Given a set of Rrules, it\nwould generate 8|R| examples for each (facts,rule)\npair in order to cover all rule combinations. This is\nvery expensive, e.g., for ﬁve rules, it would gener-\nate 85 = 32,768 examples for a single context.\nGiven this challenge, we follow a different ap-\nproach. We ﬁrst generate data for each rule in-\ndividually using Algorithm 1. We then generate\nmore examples only for combinations of two or\nmore rules having all their rule conclusions as\nhypotheses. For every input context, we produce\nrule-conclusion hypotheses (positive and negative)\nwhile varying the rules being ﬁred. Thus, we gener-\nate 2 ∗∑|R|\nx=2\n(|R|\nx\n)\nexamples with at least two rules\ntriggered. Adding the single-rule data, we generate\n8∗|R|+2 ∗∑|R|\nx=2\n(|R|\nx\n)\nfor every (facts,rules) pair,\nwhich is considerably smaller than 8r for |R|≥ 2,\naccording to the binomial theorem. For example,\nfor |R|=5, we generate 92 examples per context.\nFor the overlapping rules scenario, we release a\ndataset for 5 rules with a total of 300K examples,\nand a 70%:10%:20% split for training, validation,\nand testing.\n4.4 Chaining of Rule Executions\nFor certain hypotheses, an answer may be obtained\nby executing rules in a sequence, i.e., one on the\nresult of the other, or in a chain. To be able to\nevaluate a model in this scenario, we generate hy-\npotheses that can be tested only by chaining a num-\nber of rules (an example is shown in Appendix H).\nGiven a pool of rules over different relations and a\ndepth D, we sample a chain of rules with length D.\nWe then generate hypotheses that would require a\ndepth varying between 0 and D. We generate a\nrule-conclusion hypothesis (h3) and its alteration\n(h4) for each depth d ≤D. A depth of 0 means\nthat the hypothesis can be veriﬁed using the facts\nalone without triggering any rule. We also generate\ncounter-hypotheses by altering the hypotheses at\na given depth, and we further include hypotheses\nthat are unsatisﬁed given the input.\nFor the chaining rules scenario, we start with a\npool of 64 soft rules, and we generate hypotheses\nthat would need at most ﬁve chained rules to verify\nthem. The dataset for d≤5 contains a total of 70K\nexamples, and a 70%:10%:20% split for training,\nvalidation, and testing.\n5 Teaching PLMs to Reason\nIn this section, we explain how we teach a PLM\nto reason with one or more soft rules. Note that\nuncertainty stems from the rule conﬁdence. One\napproach to teach how to estimate the probability\nof a prediction is to treat each conﬁdence value\n(or bucket of conﬁdence values) as a class and to\nmodel the problem as a k-way classiﬁcation in-\nstance (or regression), but this is intractable when\nmultiple rules are considered. Instead, we keep the\nproblem as a two-class one by altering how the in-\nformation is propagated in the model to incorporate\nuncertainty from the rule conﬁdence.\nLet D= {(xi,yi)}m\ni=1 be our generated dataset,\nwhere xi is one example of the form (con-\ntext,hypothesis,conﬁdence) and yi is a label indi-\ncating whether the hypothesis is validated or not\nby the context (facts and rules in English), and m\nis the size of the training set. A classiﬁer f is a\nfunction that maps the input to one of the labels in\nthe label space. Let h(x,y) be a classiﬁcation loss\nfunction. The empirical risk of the classiﬁer f is\nRh(f) =ED(h(x,y)) =−1\nm\nm∑\ni=1\nh(xi,yi)\nWe want to introduce uncertainty in our loss func-\ntion, using the weights computed by the LPMLN\nsolver as a proxy to represent the probability of\npredicting the hypothesis as being true. To do so,\nwe apply a revised empirical risk:\nR′\nh(f) =ED(h(x,y)) =\n− 1\nm\nm∑\ni=1\n(w(xi) ∗h(xi,1) + (1−w(xi)) ∗h(xi,0))\nwhere w(xi) is the probability of xi being True.\nWe now state that each example is considered as\na combination both of a weighted positive example\nwith a weight w(xi) provided by theLPMLN solver\nand a weighted negative example with a weight\n1 −w(xi).\nWhen trained to minimize this risk, the model\nlearns to assign the weights to each output class,\nthus predicting the conﬁdence for the true class\nwhen given the satisﬁed rule head as a hypothesis.\n1465\nDataset Total Train Dev Test\nSingle Rule (Section 6.2) 20K 16K 2K 2K\nOverlap (Section 6.3) 300K 210K 30K 60K\nChaining (Depth=5) (Section 6.4) 70K 56K 4.6K 9.4K\nRULE BERT (Section 7) 3.2M 2.56M .32M .32M\nTable 1: Datasets for the experiments and their splits.\n6 Experiments\nWe ﬁrst describe the experimental setup (Sec-\ntion 6.1). We then evaluate the model on single\n(Section 6.2) and on multiple rules (Sections 6.3\nand 6.4). We show that a PLM ﬁne-tuned on soft\nrules, namely RULE BERT, makes accurate predic-\ntions for unseen rules (Section 6.5), and it is more\nconsistent than existing models on three external\ndatasets (Section 7). We report the values of the\nhyper-parameters, as well as the results for some\nablation experiments in the Appendix. The datasets\nfor all experiments are summarized in Table 1.\n6.1 Experimental Setup\nRules. We use a corpus of 161 soft rules mined\nfrom DBpedia. We chose a pool of distinct rules\nwith varying number of variables, number of predi-\ncates, rule conclusions, and conﬁdences.\nReasoner. We use the ofﬁcial implementation 1\nof the LPMLN reasoner. We set the reasoner to\ncompute the exact probabilities for the triples.\nPLM. We use the HuggingFace pre-trained\nRoBERTaLARGE (Liu et al., 2020) model as our\nbase model, as it is trained on more data compared\nto BERT (Devlin et al., 2019), and is better at learn-\ning positional embeddings (Wang and Chen, 2020).\nWe ﬁne-tune the PLM2 with the weighted binary\ncross-entropy (wBCE) loss from Section 5. More\ndetails can be found in Appendix C.\nEvaluations Measures. For the examples in the\ntest set, we use accuracy (Acc) and F1 score (F1)\nfor balanced and unbalanced settings, respectively.\nAs these measures do not take into account the\nuncertainty of the prediction probability, we fur-\nther introduce Conﬁdence Accuracy@k (CA@k),\nwhich measures the proportion of examples whose\nabsolute error between the predicted and the actual\nprobabilities is less than a threshold k:\nCA@k= #{xi,|wi −ˆwi|<k}\n#{xi}\n1http://github.com/azreasoners/lpmln\n2The prompt is <s>context</s></s>hypothesis</s>.\nwhere xi is the ith example of dataset, wi is the ac-\ntual conﬁdence of the associated hypothesis given\nby the LPMLN reasoner, ˆwi is the predicted conﬁ-\ndence by the model, and kis a chosen threshold.\nThe measure can be seen as the ordinary accu-\nracy measure, but true positives and negatives are\ncounted only if the condition is satisﬁed, where\nlower values for kindicate stricter evaluation.\n6.2 Single Soft Rule\nWe ﬁne-tune 16 models for 16 different positive\nand negative rules (one model per rule) using 16k\ntraining samples per rule. We compare the accuracy\nof each model (i) without teaching uncertainty us-\ning binary cross-entropy (RoBERTa), and (ii) with\nteaching soft rules using wBCE.\nResults. Every row in Table 2 shows a rule with\nits conﬁdence, followed by accuracy and CA@ k\n(for k= 0.1 and k= 0.01) for both loss functions.\nWe see that models ﬁne-tuned using RoBERTa-\nwBCE perform better on CA@k. In terms of ac-\ncuracy, both models perform well, with RoBERTa-\nwBCE performing better for all rules. Interestingly,\nthe best performing rules are two rules that involve\ncomparison of numerical values (birth years against\ndeath and founding years), which suggests that our\nmethod can handle comparison predicates.\n6.3 Rules Overlapping on Conclusion\nThe dataset contains ﬁve soft rules with spouse or\nnegspouse in the rule conclusion, and a conﬁdence\nbetween 0.30 and 0.87 (shown in Figure 2). We\ntrain a model on the dataset and test it (i) on a test\nset for each of the ﬁve rules separately, (ii) on test\nsets with U triggered rules, where U ∈{2,3,4,5}.\n(r1, .87) child(a,c) ∧parent(c,b) →spouse(A,B)\n(r2, .64) child(a,b) →negspouse(a,b)\n(r3, .3) relative(a,b) →spouse(a,b)\n(r4, ,78) child(a,c) ∧child(b,c) →spouse(a,b)\n(r5, .67) predecessor(a,b) →negspouse(a,b)\nFigure 2: The ﬁve overlapping soft rules.\nResults. Table 3 shows that the model achieves\nhigh scores both on the single test sets (top ﬁve\nrows) and on the sets with interacting rules. The\ntest sets with U = 2 and U = 3 are most chal-\nlenging, as they contain\n(5\n2\n)\n= 10 and\n(5\n3\n)\n= 10\ncombinations of rules, respectively, while the one\nwith U = 5 has only one possible rule combination.\nThe high scores indicate that PLMs can actually\nlearn the interaction between multiple soft rules.\n1466\nRoBERTa-wBCE RoBERTa\nRule Conf. Acc. CA@k Acc. CA@k\n.10 .01 .10 .01\nbirthYear(a,c) ∧deathYear(b,d) ∧>(c,d)→negspouse(a,b) .990 .995 .993 .993 .970 .490 .486\nbirthYear(b,d) ∧foundYear(a,c) ∧<(c,d)→negfounder(a,b) .990 .928 .927 .927 .908 .486 .456\nspouse(c,a) ∧parent(b,c) →negspouse(a,b) .923 .974 .963 .747 .875 .491 .279\nrelative(a,c) ∧spouse(b,c) ∧child(b,a) →relative(a,b) .860 .922 .844 .801 .866 .342 .146\nparent(c,a) ∧child(b,c) →spouse(a,b) .825 .944 .828 .444 .842 .342 .146\npublisher(c,b) ∧subsequentWork(c,a) →publisher(a,b) .721 .909 .834 .765 .905 .358 .219\nsuccessor(b,a) →negspouse(a,b) .718 .972 .896 .693 .949 .369 .313\nchild(c,b) ∧relative(c,a) →negchild(a,b) .644 .935 .880 .693 .905 .310 .303\nchild(c,b) ∧spouse(a,c) →negrelative(a,b) .562 .920 .907 .608 .915 .255 .250\nrelation(a,b) →negchild(a,b) .549 .904 .886 .737 .902 .371 .366\nchild(c,b) ∧spouse(c,a) →child(a,b) .492 .901 .827 .422 .658 .223 .107\nknownFor(b,a) →founder(a,b) .387 .882 .601 .477 .839 .372 .215\nfounder(c,b) ∧publisher(c,a) →negfounder(a,b) .246 .886 .795 .665 .802 .311 .297\npublisher(a,c) ∧parentCompany(b,c) →negpublisher(a,b) .235 .812 .748 .643 .811 .313 .271\nsuccessor(c,a) ∧spouse(c,d) ∧successor(d,b)→spouse(a,b) .221 .927 .738 .628 .761 .248 .215\nrelative(a,c) ∧parent(c,b) →child(a,b) .135 .841 .704 .552 .727 .227 .182\nTable 2: Evaluation results for single-rule models.\nTest Size F1 CA@.15 CA@.1 CA@.05\nr1 1.6k .990 .987 .986 .954\nr2 1.6k .999 .997 .996 .946\nr3 1.6k .995 .994 .994 .992\nr4 1.6k .990 .989 .988 .935\nr5 1.6k 1 .999 .998 .979\nU=2 20k .985 .997 .993 .968\nU=3 20k .925 1 .998 .949\nU=4 10k .956 1 1 .988\nU=5 2k 1 1 1 .980\nTable 3: Results for a model trained on ﬁve rules shar-\ning the same predicate, and tested on multiple test sets.\n6.4 Rule Chaining\nHere, we assess models ﬁne-tuned on various chain-\ning depths. We construct six datasets for this sce-\nnario with increasing depths ( D = 0 , D ≤ 1,\nD ≤2, D ≤3, D ≤4, D ≤5), i.e., dataset\nD ≤ x contains hypotheses that need at most\nx chained rules. We thus train six models (one\nper dataset), and we test them ( i) on their own\ntest dataset (Test), (ii) on the test set with D ≤5\nthat contains all examples up to depth 5 (All), and\n(iii) on test sets with a chaining of depth x(Depx).\nResults. The results are shown in Table 4. We\ncan see that the models achieve high F1 scores\non the respective test sets for Depth 0. The red\nborderline indicates F1 scores for models tested\non chaining depths higher than the ones they have\nbeen trained on. We see that Mod3 and Mod4 do\nfairly well on Depth 5. However, there is a decrease\nfor higher depths, possibly due to the need for more\ntraining examples in order to learn such depths.\nData Mod0 Mod1 Mod2 Mod3 Mod4 Mod5\nTest .996 .926 .883 .852 .856 .831\nAll .589 .743 .772 .811 .831 .831\nDep0 .993 .974 .973 .982 .978 .973\nDep1 .264 .860 .884 .887 .889 .889\nDep2 .396 .655 .730 .751 .750 .720\nDep3 .438 .581 .636 .684 .690 .656\nDep4 .538 .468 .547 .626 .666 .627\nDep5 .552 .356 .496 .703 .785 .744\nTable 4: F1 scores for models trained on varying depths\nand tested on six datasets. The boxed area indicates\nmodels tested on unseen chaining depths.\nMoreover, since we sample a chain of rules each\ntime, it is likely that every model has been trained\non certain chains of rules. This yields lower scores\nin the constant-depth test sets as the models are\nbeing tested on unseen rule chains.\nNote that Mod0 shows a counter-intuitive in-\ncrease in the F1 score for higher unseen depths.\nChaining soft rules may lead to a low probability\nfor the associated hypothesis, and thus eventually\nto a False label. However, Mod0 is not trained on\nchaining and sees a hypothesis that requires chain-\ning as an unsatisﬁed fact, thus eventually labelling\nit as False, while in fact it is the chaining of the soft\nrules that is the cause for this label. This is never\nthe case with hard rules, as the actual label there\nwould be True.\n6.5 Testing RULE BERT on Unseen Rules\nWe have seen that a PLM can be successfully ﬁne-\ntuned with rules. We now study the performance on\nthe PLM after it has been ﬁne-tuned on 161 (single)\nrules. We call this ﬁne-tuned model RULE BERT.\n1467\nRule FT-PLM RULE BERT20 FT-RULE BERT20\nKnown predicates\nchild(a,b) →parent(b,a) .719 .869 .989\nrelative(a,b) →negspouse(b,a) .885 .885 .963\nchild(a,b) ∧child(b,c) →negchild(a,c) .835 .888 .918\nparent(a,b) ∧parent(a,c) →spouse(b,c) .754 .757 .814\nparent(a,b) →negchild(a,b) .923 .933 .963\nUnknown predicates\nknownFor(b,a) →founder(a,b) .817 .795 .971\nworksFor(b,a) →negfounder(a,b) .951 .915 .952\noccupation(a, b) →negalmaMater(a, b) .939 .917 .972\nauthor(c,b) ∧series(a,c) →author(a,b) .965 .937 .989\ncity(a,b) →negstate(a,b) .923 .912 .971\nTable 5: Evaluation on unseen rules (accuracy). The ﬁrst group contains rules with predicates seen by RULE BERT\namong the 20 rules used for ﬁne-tuning, while the second group has rules with unseen predicates.\nchild(a,b) →negparent(a,b)\nchild(a,b) →nespouse(a,b)\nchild(a,b) →negchild(b,a)\nchild(a,b) →negrelation(b,a)\nparent(a,b) →negparent(b,a)\nparent(a,b) →nespouse(a,b)\nspouse(a,b) →relative(b,a)\nsuccessor(a,b) →predecessor(b,a)\npredecessor(a,b) →negsuccessor(a,b)\nsuccessor(a,b) →negspouse(a,b)\npredecessor(a,b) →negspouse(a,b)\nchild(a,c) ∧parent(c,b) →spouse(a,b)\nchild(b,a) ∧child(c,a) →spouse(b,c)\nparent(a,b) ∧parent(b,c) →negparent(a,c)\nparent(a,b) ∧child(c,a) →spouse(b,c)\nspouse(a,b) ∧parent(c,a) →negspouse(b,c)\nspouse(a,b) ∧child(a,c) →negspouse(b,c)\nsuccessor(a,c) ∧successor(b,c) →negspouse(a,b)\npublisher(c,b) ∧subsequentwork(c,a) →publisher(a,b)\npublisher(c,b) ∧previouswork(c,a) →publisher(a,b)\nFigure 3: The 20 random rules used for RULE BERT20.\nWe ﬁrst evaluate RULE BERT on unseen rules.\nWe ﬁne-tune it with only twenty randomly se-\nlected rules (shown in Figure 3) and call it\nRULE BERT20. We then select ten new rules di-\nvided into two groups: (i) ﬁve rules containing pred-\nicates that were used in the rules for ﬁne-tuning\nRULE BERT20, and ( ii) ﬁve rules that share no\npredicates with the ﬁne-tuning rules. For each\nrule in the test sets, we run a model ﬁne-tuned\n(with 4k examples) only for that rule (FT-PLM),\nthe model ﬁne-tuned on the twenty original rules\n(RULE BERT20), and the same model ﬁne-tuned\nagain for the rule at hand (FT-RULE BERT20).\nResults. Table 5 shows that RULE BERT20 out-\nperforms the ﬁne-tuned model (FT-PLM) on the\nﬁrst group. Even though ﬁne-tuned on 20 rules, it\nlearned enough about (i) symmetric/transitive pred-\nicates and (ii) rule conﬁdence to predict correctly,\neven better than rule-speciﬁc models.\nFor the second rule group, the accuracy of\nRULE BERT20 is high, but FT-PLM performs better.\nApplying the same ﬁne-tuning on RULE BERT20\nyields the best results in all scenarios.\n7 RULE BERT on External Datasets\nAs our ﬁne-tuning propagates information in the\nlayers of the encoder, we hypothesize that RULE -\nBERT effectively “learns” logical properties of the\nconcepts represented in the rules, such as negation\nand symmetry, and thus it could perform better on\ntasks testing such properties of PLMs. To study the\nnegation of predicates, we use the Negated LAMA\ndatasets, which test how PLMs distinguish a Cloze\nquestion and its negation (Kassner and Schütze,\n2020). In most cases, PLMs make the same predic-\ntion both for a positive statement (“Relativity was\ndeveloped by Einstein.”) and for its negation (“Rel-\nativity was not developed by Einstein. ”). To test\nthe symmetry relationship between predicates, we\nuse the SRL test in CheckList (Ribeiro et al., 2020),\nwhich focuses on behavioral testing of NLP mod-\nels; we use its test set for the duplicate-question\ndetection task (QQP) (Wang et al., 2019a). Finally,\nwe test deductive reasoning on the bAbI dataset\nand its Task #15 (Weston et al., 2016).\n7.1 Negated LAMA Experiments\nFor Negated LAMA, we do not ﬁne-tune RULE -\nBERT for the task; instead, we replace its original\nclassiﬁcation layer by an MLM head with weights\nidentical to those of RoBERTa (not ﬁne-tuned).\nNote that this conﬁguration is biased in favor of\nRoBERTa, as the parameters of the MLM head\nand of the RoBERTa encoder have been trained in\nconjunction and thus good values have been found\nfor this combination, which is not the case for our\nRULE BERT.\n1468\nFine-Tuned RoBERTa RULE BERT\nbAbI\n1 epoch .401 .477\n2 epochs .676 .863\n3 epochs .827 .825\nNeg. LAMA - .684 .852\nCheckList QQP 1 epoch .000 .422\n3 epochs .000 .000\nTable 6: Evaluation on external datasets (accuracy).\nResults Yet, even in this arguably unfair setting,\nRULE BERT outperforms RoBERTa on all datasets\nof Negated LAMA, as shown in Table 7. We can\nsee that RULE BERT performs better on both eval-\nuation measures used in (Kassner and Schütze,\n2020). It achieves a lower mean Spearman rank\ncorrelation (ρ) and a much smaller percentage of\npositive and negated answers overlap (%).\n7.2 CheckList QQP Experiments\nThe CheckList tests (Ribeiro et al., 2020) have\nshown that PLMs fail in many basic cases. We\nhypothesize that RULE BERT can perform better\non tasks and examples that deal with symmetric\nand asymmetric predicates, if such predicates have\nbeen shown to it during pre-ﬁne-tuning. We exper-\niment with the QQP dataset, which asks to detect\nwhether two questions are duplicates. We identify\na few rules that can teach a model about symmetric\npredicates, and we pre-ﬁne-tune RULE BERT on\nthem; then, we ﬁne-tune it on the QQP dataset.\nResults Table 6 shows the results on the challeng-\ning CheckList QQP test set: we can see that RULE -\nBERT achieves accuracy of 0.422 after one epoch,\nwhile RoBERTa is at 0.0. However, after three\nepochs RULE BERT is also at 0.0,3 i.e., it started to\nunlearn what it had learned at pre-ﬁne-tuning (Kirk-\npatrick et al., 2017; Kemker et al., 2018; Biesial-\nska et al., 2020). Learning a new task often leads\nto such catastrophic forgetting (Ke et al., 2021).\nWhile there are ways to alleviate this (Ke et al.,\n2021), this is beyond the scope of this paper.\n7.3 bAbI Task #15 Experiments\nFinally, we experiment with task #15 of the bAbI\ndataset, where the goal is to assess whether a model\ncan perform deductive reasoning. However, as men-\ntioned in the original bAbI paper (Weston et al.,\n2016), it is not only desirable to perform well on\nthe task, but also to use the fewest examples.\n3On the much easier QQP test set, RULE BERT achieved\n0.89 accuracy after one epoch, and 0.91 after three epochs.\nFacts RoBERTa RULE BERT\nρ % ρ %\nGR\nbirthplace 2,404 90.99 18.51 71.72 4.20\nbirthdate 1,565 82.87 1.40 63.55 0.13\ndeathplace 649 86.44 0.31 71.13 0.00\nT-REx\n1-1 973 78.95 61.38 51.21 32.96\nN-1 20,006 87.56 43.80 67.63 11.48\nN-M 13,096 89.39 50.78 72.59 28.90\nConceptNet — 2,996 42.61 9.00 37.43 4.83\nSQ — 286 89.71 44.76 75.05 26.32\nTable 7: Negated LAMA: Mean Spearman rank cor-\nrelation (ρ) and mean percentage of overlap in the ﬁrst\nranked predictions (%) for original vs. negated queries.\nThus, we use the smallest dataset consisting of\nabout 2,000 data points. We hypothesize that under\nthe same conditions and hyper-parameters, RULE -\nBERT should be able to generalize faster and to\nlearn in fewer epochs. As PLMs produce varying\nscores when ﬁne-tuned on small datasets, we repeat\nthe experiment ten times and we report the average\nscores. We then compare to RoBERTa. Both mod-\nels contain two classiﬁcation layers to predict start\nand end spans of the input context.\nResults We can see in Table 6 thatRULE BERT\nachieves accuracy of 0.863 in two epochs, while\nRoBERTa achieves 0.676. On the third epoch,\nRoBERTa catches up with accuracy of 0.827, while\nRULE BERT starts to overﬁt (goes down to 0.825),\nindicating that fewer epochs should be used, proba-\nbly due to catastrophic forgetting.\n8 Conclusion and Future Work\nWe studied whether PLMs could reason with soft\nrules over natural language. We experimented with\none ﬂavor of probabilistic answer set programming\n(LPMLN), but other semantics can be also used\nwith the proposed methodology. We further ex-\nplored the inference capabilities of Transformer-\nbased PLMs, focusing on positive and negative\ntextual entailment.\nWe leave non-entailment for future work. We\nalso leave open the development of explainable\nmodels. Some approaches use occlusion that re-\nmoves parts of the input and checks the impact on\nthe output (Clark et al., 2020) or build proof itera-\ntively using 1-hop inference (Tafjord et al., 2021).\nAcknowledgments\nThis work is partially supported by a Google Fac-\nulty Research Award and the ANR JCJC Grant\nInfClean.\n1469\nEthics and Broader Impact\nData Collection While we generated the facts in\nour examples, the logical rules have been mined\nfrom the data in the DBpedia knowledge graph,\nwhich in turn has been generated from Wikipedia.\nBiases We are aware of ( i) the biases and abu-\nsive language patterns (Sheng et al., 2019; Zhang\net al., 2020; Bender et al., 2021; Liang et al., 2021)\nthat PLMs impose, and (ii) the imperfectness and\nthe biases of our rules as data from Wikipedia has\nbeen used to mine the rules and compute their con-\nﬁdences (Janowicz et al., 2018; Demartini, 2019).\nHowever, our goal is to study PLM’s capability of\ndeductive soft reasoning. For ( i), there has been\nsome work on debiasing PLMs (Liang et al., 2020),\nwhile for (ii), we used mined rules to have more\nvariety, but could resort to user-speciﬁed rules vali-\ndated by consensus to relieve the bias.\nEnvironmental Impact The use of large-scale\nTransformers requires a lot of computations and\nGPUs/TPUs for training, which contributes to\nglobal warming (Strubell et al., 2019; Schwartz\net al., 2020). This is a smaller issue in our case, as\nwe do not train such models from scratch; rather,\nwe ﬁne-tune them on relatively small datasets.\nMoreover, running on a CPU for inference, once\nthe model is ﬁne-tuned, is less problematic as CPUs\nhave a much lower environmental impact.\nReferences\nNaser Ahmadi, Thi-Thuy-Duyen Truong, Le-Hong-\nMai Dao, Stefano Ortona, and Paolo Papotti. 2020.\nRuleHub: A public corpus of rules for knowledge\ngraphs. J. Data and Information Quality, 12(4).\nChitta Baral. 2010. Knowledge Representation, Rea-\nsoning and Declarative Problem Solving, 1st edition.\nCambridge University Press, USA.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, Virtual Event, Canada.\nAssociation for Computing Machinery.\nMagdalena Biesialska, Katarzyna Biesialska, and\nMarta R. Costa-jussà. 2020. Continual lifelong\nlearning in natural language processing: A survey.\nIn Proceedings of the 28th International Confer-\nence on Computational Linguistics , COLING ’20,\npages 6523–6541, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nEkaba Bisong. 2019. Google colaboratory. In Build-\ning Machine Learning and Deep Learning Models\non Google Cloud Platform: A Comprehensive Guide\nfor Beginners, pages 59–64. Apress.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , NAACL-HLT ’19, pages\n2924–2936, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019b. What does BERT\nlook at? An analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, BlackboxNLP ’19, pages 276–286, Florence,\nItaly. Association for Computational Linguistics.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson.\n2020. Transformers as soft reasoners over lan-\nguage. In Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence,,\nIJCAI ’20, pages 3882–3890, Online. International\nJoint Conferences on Artiﬁcial Intelligence Organi-\nzation.\nIdo Dagan, Dan Roth, Mark Sammons, and Fabio Mas-\nsimo Zanzotto. 2013. Recognizing Textual Entail-\nment: Models and Applications . Synthesis Lec-\ntures on Human Language Technologies. Morgan\nand Claypool publishers.\nGianluca Demartini. 2019. Implicit bias in crowd-\nsourced knowledge graphs. In Proceedings of the\n2019 World Wide Web Conference: Companion Vol-\nume, WWW ’19, page 624–630, San Francisco, Cal-\nifornia, USA. Association for Computing Machin-\nery.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\nNAACL-HLT ’19, pages 4171–4186, Minneapolis,\nMinnesota, USA. Association for Computational\nLinguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv:2002.06305.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Ab-\nhilasha Ravichander, Eduard H. Hovy, Hinrich\nSchütze, and Yoav Goldberg. 2021. Measuring and\n1470\nimproving consistency in pretrained language mod-\nels. arXiv:2102.01017.\nLuis Galárraga, Christina Teﬂioudi, Katja Hose, and\nFabian M. Suchanek. 2015. Fast rule mining in onto-\nlogical knowledge bases with AMIE++. The VLDB\nJournal, 24(6):707–730.\nMartin Gebser, Roland Kaminski, Benjamin Kauf-\nmann, and Torsten Schaub. 2014. Clingo = ASP +\ncontrol: Preliminary report. arXiv:1405.3694.\nWilliam L. Hamilton, Payal Bajaj, Marinka Zitnik, Dan\nJurafsky, and Jure Leskovec. 2018. Embedding\nlogical queries on knowledge graphs. In Proceed-\nings of the 32nd International Conference on Neu-\nral Information Processing Systems, NIPS’18, page\n2030–2041, Montréal, Canada. Curran Associates\nInc.\nKrzysztof Janowicz, Bo Yan, Blake Regalia, Rui Zhu,\nand Gengchen Mai. 2018. Debiasing knowledge\ngraphs: Why female presidents are not like female\npopes. In Proceedings of the International Semantic\nWeb Conference, ISWC ’18, Monterey, California,\nUSA.\nNora Kassner, Benno Krojer, and Hinrich Schütze.\n2020. Are pretrained language models symbolic\nreasoners over knowledge? In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, CoNLL ’20, pages 552–564, On-\nline. Association for Computational Linguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL ’20, pages 7811–7818,\nOnline. Association for Computational Linguistics.\nZixuan Ke, Hu Xu, and Bing Liu. 2021. Adapting\nBERT for continual learning of a sequence of as-\npect sentiment classiﬁcation tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT ’21,\npages 4746–4755, Online. Association for Compu-\ntational Linguistics.\nRonald Kemker, Marc McClure, Angelina Abitino,\nTyler L. Hayes, and Christopher Kanan. 2018. Mea-\nsuring catastrophic forgetting in neural networks. In\nProceedings of the Thirty-Second AAAI Conference\non Artiﬁcial Intelligence , AAAI ’18, pages 3390–\n3398, New Orleans, Louisiana, USA. AAAI Press.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521–3526.\nGuillaume Lample and François Charton. 2020. Deep\nlearning for symbolic mathematics. In Proceed-\nings of the 8th International Conference on Learning\nRepresentations, ICLR ’20, Addis Ababa, Ethiopia.\nOpenReview.net.\nJoohyung Lee, Samidh Talsania, and Y . Wang. 2017.\nComputing LPMLN using ASP and MLN solvers.\nTheory and Practice of Logic Programming , 17(5–\n6):942–960.\nJoohyung Lee and Yi Wang. 2016. Weighted rules\nunder the stable model semantics. In Proceedings\nof the Fifteenth International Conference on Prin-\nciples of Knowledge Representation and Reasoning,\nKR ’16, page 145–154, Cape Town, South Africa.\nAAAI Press.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sen-\ntence representations. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL ’20, pages 5502–5515, Online. As-\nsociation for Computational Linguistics.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency,\nand Ruslan Salakhutdinov. 2021. Towards under-\nstanding and mitigating social biases in language\nmodels. In Proceedings of the International Confer-\nence on Machine Learning, ICML ’21, pages 6565–\n6576, Online. PMLR.\nPercy Liang. 2016. Learning executable semantic\nparsers for natural language understanding. Com-\nmun. ACM, 59(9):68–76.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin. 2020. RoBERTa: A\nrobustly optimized BERT pretraining approach. In\nProceedings of the 8th International Conference on\nLearning Representations, ICLR ’20, Addis Ababa,\nEthiopia. OpenReview.net.\nBill MacCartney and Christopher D. Manning. 2009.\nAn extended model of natural logic. In Proceedings\nof the Eight International Conference on Computa-\ntional Semantics , IWCS-WS ’09, pages 140–156,\nTilburg, The Netherlands. Association for Computa-\ntional Linguistics.\nPasquale Minervini, Matko Bošnjak, Tim Rocktäschel,\nSebastian Riedel, and Edward Grefenstette. 2020.\nDifferentiable reasoning on large knowledge bases\nand natural language. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , 34(04):5182–\n5190.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of ﬁne-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. In Proceedings of the 9th International Con-\nference on Learning Representations, ICLR ’21, Vir-\ntual Event, Austria. OpenReview.net.\n1471\nStefano Ortona, Venkata Vamsikrishna Meduri, and\nPaolo Papotti. 2018. Robust discovery of positive\nand negative rules in knowledge bases. In Proceed-\nings of the 2018 IEEE 34th International Conference\non Data Engineering, ICDE ’18, pages 1168–1179,\nParis, France. IEEE.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing , EMNLP-\nIJCNLP ’19, pages 2463–2473, Hong Kong, China.\nAssociation for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of NLP models with CheckList. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL ’20, pages\n4902–4912, Online. Association for Computational\nLinguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and\nOren Etzioni. 2020. Green AI. Commun. ACM ,\n63(12):54–63.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP ’19, pages\n3407–3412, Hong Kong, China. Association for\nComputational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL ’19, pages 3645–3650, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics, ACL-IJCNLP ’21, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020a. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020b. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, volume 33 of NeurIPS ’20, pages 20227–\n20237, Online.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 7th International Conference on\nLearning Representations, ICLR ’19, New Orleans,\nLouisiana, USA. OpenReview.net.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019b. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL ’19, pages 1810–1822, Florence, Italy.\nAssociation for Computational Linguistics.\nYu-An Wang and Yun-Nung Chen. 2020. What do po-\nsition embeddings learn? An empirical study of pre-\ntrained language model positional encoding. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing , EMNLP ’20,\npages 6840–6849, Online. Association for Compu-\ntational Linguistics.\nJason Weston, Antoine Bordes, Sumit Chopra, and\nTomás Mikolov. 2016. Towards AI-complete ques-\ntion answering: A set of prerequisite toy tasks.\nIn Proceedings of the 4th International Conference\non Learning Representations , ICLR ’16, San Juan,\nPuerto Rico.\nFan Yang, Zhilin Yang, and William W. Cohen. 2017.\nDifferentiable learning of logical rules for knowl-\nedge base reasoning. In Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, California, USA ,\nNeurIPS ’17, pages 2319–2328.\nHaoran Zhang, Amy X. Lu, Mohamed Abdalla,\nMatthew McDermott, and Marzyeh Ghassemi. 2020.\nHurtful words: Quantifying biases in clinical con-\ntextual word embeddings. In Proceedings of the\nACM Conference on Health, Inference, and Learn-\ning, CHIL ’20, page 110–120, Toronto, Ontario,\nCanada. Association for Computing Machinery.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , vol-\nume 34 of AAAI ’20, pages 9733–9740, Online.\nAAAI Press.\n1472\nA More on Reasoning with Soft Rules\nLet σ be a signature as in ﬁrst-order logic. An\nLPMLN program Π is a ﬁnite set of weighted rules\nof the form:\nw: A←B (1)\nwhere A is a disjunction of atoms of σ, B is a\nconjunction of literals (atoms and negated atoms)\nof σ, and wis a real number or the symbol α.\nWhen Ais ⊥(the empty disjunction), the rule as-\nserts that Bshould be false in the stable model. An\nLPMLN rule (1) is called soft if wis a real number\nor hard if wis α. An LPMLN program is ground if\nits rules contain no variables. An LPMLN program\nΠ that contains variables is identiﬁed with a ground\nLPMLN program grσ[Π], which is obtained from\nΠ by replacing every variable with every ground\nterm of σ. The weight of a ground rule in grσ[Π] is\nthe same as the weight of the corresponding rule in\nΠ. By Π we denote the unweighted logic program\nobtained from Π, i.e., Π = {R|w: R∈Π}.\nFor a ground LPMLN program Π, ΠI denotes\nthe set of rules w : R in Π such that I satis-\nﬁes R (denoted I |= R) and SM [Π] denotes the\nset {I |I is a (deterministic) stable model of ΠI}.\nThe (unnormalized) weight of Iunder Π is deﬁned\nas follows:\nWΠ(I) =\n\n\n\nexp( ∑\nw:R∈ΠI\nw) if I ∈SM[Π];\n0 otherwise.\nThe probability of I under Π is the normalized\nweight deﬁned as follows:\nPΠ(I) = lim\nα→∞\nWΠ(I)∑\nJ∈SM[Π] WΠ(J).\nIn Answer Set programming (ASP), search prob-\nlems are reduced to computingstable models (a.k.a.\nanswer sets), a set of beliefs described by the pro-\ngram. In the case of a Horn program, the stable\nmodels coincide with the minimal models. LPMLN\nprograms are transformed to meet the needs of an\nASP solver (Lee et al., 2017; Gebser et al., 2014).\nB Rule Support\nWe designed an experiment to show the impact\nof increasing the number of overlapping rules on\nthe same target predicate. The goal is to measure\nhow often multiple rules are triggered for the same\ntarget triple.\n1 2 3 4 5\n0\n1\n2\n3\n·104\nNumber of rules\nAverage Support\nspouse child relative\nFigure 4: Support of the overlapping rules.\nWe measure this with the support of a rule,\ni.e., the number of triples in the knowledge base\nthat satisfy all the atoms in the rule.\nTo compute the support for more than one rule,\nwe combine the premises of the rules. In this experi-\nment, we picked three predicates (spouse, child and\nrelative), and for each one we selected ten rules ran-\ndomly. Next, we used DBpedia online endpoint 4\nto compute the support for each combination of n\n(n=1,2,...,5) rules for each predicate. The results\nin Figure 4 show that by increasing the number\nof rules, the support decreases for all predicates.\nFor combinations with more than three rules, the\nsupport is very small.\nC More Experimental Details\nFor ﬁne-tuning our models, we use Google Colabo-\nratory (Bisong, 2019), which assigns random GPU\nclusters of various types. The number of param-\neters of our models is about 355M. We select the\nvalues of our hyper-parameters (shown in Table 8)\non the development sets, by maximizing accuracy.\nThe execution times vary largely depending on\nthe GPU at hand and on the scenario, with ﬁne-\ntuning on a Tesla V100 taking from one hour for\na single rule to a few hours for all the chaining\nexperiments. The training/validation/testing splits\nare shown in Table 1. Table 9 shows the sizes of\nthe used test datasets.\nD Ablation\nD.1 Impact of the Data Size\nSetting. We report the impact of the size of the\nﬁne-tuning data on the model performance. As\nshown in Table 2, the accuracy of the ﬁne-tuned\nmodel is higher for rules with higher conﬁdence.\n4http://dbpedia.org/sparql\n1473\nHyper-Parameter Value\nLearning Rate 1e-6\nWeight Decay 0.1\nNumber of Epochs 3\nBatch Size 16\nLearning Rate Decay Linear\nWarmup Ratio 0.06\nTable 8: Hyper-parameters for ﬁne-tuning our model.\nDataset Size\nMod0 Test(own) 2,667\nMod1 Test(own) 4,000\nMod2 Test(own) 5,334\nMod3 Test(own) 6,667\nMod4 Test(own) 8,000\nMod5 Test(own) 9,334\nTest(D≤5) 9,334\nDepth=0 16,057\nDepth=1 6,608\nDepth=2 5,389\nDepth=3 3,993\nDepth=4 2,619\nDepth=5 1,336\nTable 9: Number of examples in each of the test\ndatasets for the chaining experiment.\nWe therefore divide the rules in three categories:\nHigh contains rules with conﬁdence greater than\n0.8, Medium has rules with conﬁdence between\n0.4 and 0.8, and Low is for the rest. There are six\nrules in the Medium category and the other two\ncategories have ﬁve rules each. For each rule, we\nﬁne-tune seven models with 1k, 2k, 5k, 10k, 15k,\n20k, and 30k examples.\nResults. Figure 5 shows that having more train-\ning data improves the accuracy in all scenarios.\nFor all categories, there is a sizable increase going\nfrom 10k to 15k examples; the impact is smaller for\nhigher values. The highest increase is for rules with\nhigh conﬁdence, and rules with medium conﬁdence\ndemonstrate larger increase than low conﬁdence.\n12 5 10 15 20 30\n0.6\n0.8\n1\nTraining Data (Thousands)\nAccuracy High Medium Low\nFigure 5: Impact of the training data size.\nD.2 Role of the Example Format\nSetting. When we teach rules to PLMs, we rely\non examples with real names from a ﬁxed pool.\nHowever, our goal is to teach PLMs the seman-\ntics of the soft rule, not the facts in our examples.\nThus, we further design an experiment to assess\nthe impact of the format used in the example facts\non the behavior of the model. We distinguish two\nformats for the generated facts: (i) real names such\nas Alice and IBM, and (ii) letters such as A and B.\nWe ﬁrst use each format in ﬁne-tuning and we then\ntest both formats. We end up with two test/train\nscenarios: one with the same format and one with\ndifferent formats. For this study, we use just one\nrule: child(a,c) ∧parent(c,b) →spouse(a,b), with\n30K examples for ﬁne-tuning, and 2k for testing.\nTrain Letter Train Name\nTest Letter .981 .932\nTest Name .977 .985\nTable 10: Impact of the example format on accuracy.\nResults. The results in Table 10 show that the\nmodel performance does not depend heavily on\nusing the same fact format for training and testing.\nWith examples using letters in training, the results\nare slightly better in the case with two formats. We\nultimately use names for testing and training in our\ndefault conﬁguration as it yields better results.\nE Impact of the Random Seed\nPre-trained transformers often suffer from insta-\nbility of the results across multiple reruns with\ndifferent random seeds. This usually happens with\nsmall training datasets (Dodge et al., 2020; Mos-\nbach et al., 2021). In such cases, typically multiple\nreruns are performed, and the average value over\nthese reruns is reported.\nHowever, the numbers for the main experiments\nwe report in this paper are not averaged over mul-\ntiple reruns as our datasets are considerably large\nand the models did not suffer from instability due to\nrandom seeds. For example, when we reran RULE -\nBERT on a single-rule experiment three times, we\nobtained accuracy of 0.98959, 0.99551, 0.99636\nwith a standard deviation of only 0.003.\nYet, for the small dataset bAbI, we observed a\nmuch higher standard deviation of 0.17. Thus, in\nthis case we report results that are averaged over\nten reruns.\n1474\nF Data Generation Example\nWe show an example of data generation for Algo-\nrithm 1. For simplicity, here we show an example\nof a hard rule, i.e., one whose conﬁdence is implic-\nitly set to one.5 We begin by setting the values of\nthe input parameters:\nAlgorithm 1 Input:\n• r = child(A,C) ∧ parent(C,B) →\nspouse(A,B)\n• n= 8\n• m= 5\n• pools={Alice,Bob,Carl,David,Eve}\nWe setn= 8 to generate all the eight hypotheses.\nWe start by generating a set of facts F (line 3),\nhaving predicates from the body of the rule with\nrandom polarity. We ensure that there are facts that\ntrigger the rule. Their number should not exceed\nm. Here is an example of generated facts F:\nGenerated Facts F:\n• f1: negparent(Eve,Carl)\n• f2: child(Eve,David)\n• f3: parent(Carl,Bob)\n• f4: child(Alice,Carl)\nFour facts are generated in total. Facts f3 and f4\ntrigger rule r. We then feed the rule rand facts F\ninto the LPMLN reasoner (line 4). The output Ois\nthen:\nLPMLN Reasoner Output O:\n• o1: child(Eve,David)\n• o2: child(Alice,Carl)\n• o3: parent(Carl,Bob)\n• o4: spouse(Alice,Bob)\n• o5: negchild(Eve,Carl)\nWe start generating the hypotheses:\nGenerated Hypotheses H:\n• h1: child(Eve,David)\n• h2: child(David,Eve)\n• h3: spouse(Alice,Bob)\n• h4: negspouse(Alice,Bob)\n• h5: child(David,Carl)\n• h6: negchild(David,Carl)\n• h7: spouse(Bob,Eve)\n• h8: negspouse(Bob,Eve)\n5We show an example of a soft rule in Section G below.\nHypothesis h1 is obtained by sampling from F\n(line 5), and thus it is a valid hypothesis. Then,\nthe hypothesis h2 is generated by altering h1 with\nthe function Alter (line 19-22). In this example,\nsince child is not symmetric, h2 is produced us-\ning a switch of the subject and the object of h1 to\ngenerate a false hypothesis (line 6).\nHypothesis h3 is the outcome of rule r being\ntriggered by facts f3 and f4 (line 7). In a similar\nfashion to h2, we produce h4 (line 8).\nHypothesis h5 is sampled from the universe of\nall unsatisﬁed positive facts having a different pred-\nicate than that of the rule body (line 9), which\nmakes it an invalid hypothesis, as it is not found\nin the O. Hypothesis h6 is the negation of h5, and,\nfollowing CW A, it is a valid hypothesis (line 10).\nFinally, hypothesis h7 is sampled from the uni-\nverse of unsatisﬁed rule-head atoms (line 11), and\nit is negated to produce hypothesis h8.\nOverall, we obtain eight different examples rep-\nresented in symbolic knowledge, where each exam-\nple contains the set of generated facts F, the rule\nr, and a single hypothesis hi. The following is one\nexample in symbolic knowledge:\nExample #1 (Symbolic):\n• Rule r = child(A,C) ∧ parent(C,B)\n→ spouse(A,B)\n• FactsF :\n– f1: negparent(Eve,Carl)\n– f2: child(Eve,David)\n– f3: parent(Carl,Bob)\n– f4: child(Alice,Carl)\n• Hypothesish 3 : spouse(Alice,Bob)\nWe then convert each example to synthetic En-\nglish using a set of pre-deﬁned templates for the\nfacts and for the rules. Here is the above Example\n#1, but now rewritten in synthetic English:\n1475\nExample #1 (Synthetic English):\n• Ruler = If the child of the ﬁrst person is\nthe third person, and the parent of the third\nperson is the second person, then the ﬁrst\nperson is the spouse of the second person.\n• FactsF :\n– f1: The parent of Eve is not Carl.\n– f2: The child of Eve is David.\n– f3: The parent of Carl is Bob.\n– f4: The child of Alice is Carl.\n• Hypothesish 3 : The spouse of Alice is\nBob.\nThe Contextis deﬁned as the combined set of\nfacts and rule(s). Both Contextand Hypothesis\nare fed as an input to the model.\nExample #1 (Model Input):\n• Context : The parent of Eve is not Carl.\nThe child of Eve is David. If the child\nof the ﬁrst person is the third person, and\nthe parent of the third person is the second\nperson, then the ﬁrst person is the spouse\nof the second person. The parent of Carl is\nBob. The child of Alice is Carl.\n• Hypothesis : The spouse of Alice is Bob.\nG Rule Overlap Example\nAfter generating the data for every rule in Figure 2,\nwe generate additional examples using combina-\ntions of rules. Below, we show how to handle the\ninteraction of two rules: r2 and r3. We follow the\nprocedure in Algorithm 1 by generating facts that\ntrigger the rules, but we only take into considera-\ntion hypotheses that deal with rule conclusions.\nFor example, consider the following facts:\nGenerated Facts\n• f1: negparent(Eve,Carl)\n• f2: child(Eve,David)\n• f3: relative(Eve,David)\n• f4: predecessor(Eve,David)\nWe can generate an example that triggers two\nrules: f2 triggers r2, and f3 triggers r3. Feeding\nthe above facts and rules r2 and r3 to the reasoner,\nwe obtain the following output (the numbers in\nparentheses indicate the likelihood of the triple):\nLPMLN Reasoner Output O:\n• o1: relative(Eve,David) (1.0)\n• o2: child(Eve,David) (1.0)\n• o3: negparent(Eve,Carl) (1.0)\n• o4: spouse(Eve,David) (0.134)\n• o5: negspouse(Eve,David) (0.55)\n• o6: predecessor(Eve,David) (1.0)\nWe produce hypotheses that trigger both rules\ntogether. For example, here we generate two hy-\npotheses coming from o4 and o5. The conﬁdence\n(weight) of a hypothesis is given by the LPMLN\nreasoner. Taking o5 as a hypothesis, we feed the\nfollowing example to the model:\nExample #2 (Model Input):\n• Context : The parent of Eve is not Carl.\nThe child of Eve is David. If the child\nof the ﬁrst person is the second person,\nthen the ﬁrst person is not the spouse of\nthe second person. The relative of Eve is\nDavid. If the relative of the ﬁrst person\nis the second person, then the ﬁrst person\nis the spouse of the second person. The\npredecessor of Eve is David.\n• Hypothesis : The spouse of Eve is not\nDavid.\n• Weight : 0.55\nWe also generate an example, where three rules\nare triggered: In addition to r2 and r3, r5 is trig-\ngered by f4. We then repeat the same procedure to\ngenerate the following example:\nExample #3 (Model Input):\n• Context : The parent of Eve is not Carl.\nThe child of Eve is David. If the child of\nthe ﬁrst person is the second person, then\nthe ﬁrst person is not the spouse of the sec-\nond person. The relative of Eve is David. If\nthe relative of the ﬁrst person is the second\nperson, then the ﬁrst person is the spouse\nof the second person. The predecessor of\nEve is David. If the predecessor of the\nﬁrst person is the second person, then the\nﬁrst person is not the spouse of the second\nperson.\n• Hypothesis : The spouse of Eve is not\nDavid.\n• Weight : 0.6\n1476\nThis procedure is repeated for all combinations\nof two or more rules. In case when all rules have the\nsame head polarity, we generate a false example by\naltering the hypothesis and ﬁnding the complement\nof the initial (1- Weight) weight. For example,\nr2 and r5 can occur together and both have the\nsame rule head, and thus no conﬂict occurs. The\ngenerated valid example would be as follows:\nExample #4 (Model Input):\n• Context : The parent of Eve is not Carl.\nThe child of Eve is David. If the child\nof the ﬁrst person is the second person,\nthen the ﬁrst person is not the spouse of\nthe second person. The relative of Eve is\nDavid. The predecessor of Eve is David.\nIf the predecessor of the ﬁrst person is the\nsecond person, then the ﬁrst person is not\nthe spouse of the second person.\n• Hypothesis : The spouse of Eve is not\nDavid.\n• Weight : 0.64\nAn invalid example is generated from the valid\nexample by altering the hypothesis. Here is an\ninvalid example:\nExample #4 (Model Input):\n• Context : The parent of Eve is not Carl.\nThe child of Eve is David. If the child\nof the ﬁrst person is the second person,\nthen the ﬁrst person is not the spouse of\nthe second person. The relative of Eve is\nDavid. The predecessor of Eve is David.\nIf the predecessor of the ﬁrst person is the\nsecond person, then the ﬁrst person is not\nthe spouse of the second person.\n• Hypothesis : The spouse of Eve is not\nDavid.\n• Weight : 0.36 (1-0.64)\nH Rule Chaining Example\nHere is an example that illustrates rule chaining:\nExample #5 (Symbolic):\n• RulesR =\n– r1: child(A,C) ∧ parent(C,B)\n→ spouse(A,B)\n– r2: child(B,A) →\nparent(A,B)\n• FactsF :\n– f1: negparent(Eve,Carl)\n– f2: child(Bob,Carl)\n– f3: child(Alice,Carl)\n• Hypothesish : spouse(Alice,Bob)\nf2 triggers r2 which produces t=\nchild(Bob,Carl).\nt and f3 trigger r1 to validate the hypothesis\nh. r1 and r2 have been chained to validate the\nhypothesis. Since we used two rules to validate the\nhypothesis, we say that this is a chain of depth = 2.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8100873231887817
    },
    {
      "name": "Task (project management)",
      "score": 0.7193866968154907
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.6410930156707764
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5897858142852783
    },
    {
      "name": "Function (biology)",
      "score": 0.5049763321876526
    },
    {
      "name": "Natural language",
      "score": 0.49250611662864685
    },
    {
      "name": "Machine learning",
      "score": 0.4520551562309265
    },
    {
      "name": "Natural language processing",
      "score": 0.36765819787979126
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1902872",
      "name": "EURECOM",
      "country": "FR"
    }
  ]
}