{
  "title": "Understanding Transformer Memorization Recall Through Idioms",
  "url": "https://openalex.org/W4386566901",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5065238853",
      "name": "Adi Haviv",
      "affiliations": [
        "Bar-Ilan University",
        "Tel Aviv University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5102000230",
      "name": "Ido Cohen",
      "affiliations": [
        "Bar-Ilan University",
        "Tel Aviv University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5009457488",
      "name": "Jacob Gidron",
      "affiliations": [
        "Bar-Ilan University",
        "Tel Aviv University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5011769908",
      "name": "Roei Schuster",
      "affiliations": [
        "Bar-Ilan University",
        "Tel Aviv University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5028476919",
      "name": "Yoav Goldberg",
      "affiliations": [
        "Bar-Ilan University",
        "Tel Aviv University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5065717258",
      "name": "Mor Geva",
      "affiliations": [
        "Bar-Ilan University",
        "Tel Aviv University",
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205460703",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4285254489",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W3198711991",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2966176804",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W2788844247",
    "https://openalex.org/W3167352803",
    "https://openalex.org/W2952604841",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W4285220249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W4285280911",
    "https://openalex.org/W3201344816",
    "https://openalex.org/W4281483318",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4294955582",
    "https://openalex.org/W3185437894",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W3097252660",
    "https://openalex.org/W2517996181",
    "https://openalex.org/W3082470847",
    "https://openalex.org/W3026997957",
    "https://openalex.org/W3030374439",
    "https://openalex.org/W3172669006",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3048045781",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W3183582945",
    "https://openalex.org/W2566079294",
    "https://openalex.org/W4385573691",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4226137521",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3100283070"
  ],
  "abstract": "To produce accurate predictions, language models (LMs) must balance between generalization and memorization. Yet, little is known about the mechanism by which transformer LMs employ their memorization capacity. When does a model decide to output a memorized phrase, and how is this phrase then retrieved from memory? In this work, we offer the first methodological framework for probing and characterizing recall of memorized sequences in transformer LMs. First, we lay out criteria for detecting model inputs that trigger memory recall, and propose idioms as inputs that typically fulfill these criteria. Next, we construct a dataset of English idioms and use it to compare model behavior on memorized vs. non-memorized inputs. Specifically, we analyze the internal prediction construction process by interpreting the model's hidden representations as a gradual refinement of the output probability distribution. We find that across different model sizes and architectures, memorized predictions are a two-step process: early layers promote the predicted token to the top of the output distribution, and upper layers increase model confidence. This suggests that memorized information is stored and retrieved in the early layers of the network. Last, we demonstrate the utility of our methodology beyond idioms in memorized factual statements. Overall, our work makes a first step towards understanding memory recall, and provides a methodological basis for future studies of transformer memorization.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 248–264\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nUnderstanding Transformer Memorization Recall Through Idioms\nAdi Havivτ Ido Cohenτ Jacob Gidronτ Roei Schusterµ\nYoav Goldbergαβ Mor Gevaα∗\nτTel Aviv University µWild Moose βBar-Ilan University αAllen Institute for AI\nadi.haviv@cs.tau.ac.il, roei@wildmoose.ai, pipek@google.com,\n{its.ido, jacob.u.gidron, yoav.goldberg}@gmail.com\nAbstract\nTo produce accurate predictions, language mod-\nels (LMs) must balance between generalization\nand memorization. Yet, little is known about\nthe mechanism by which transformer LMs em-\nploy their memorization capacity. When does\na model decide to output a memorized phrase,\nand how is this phrase then retrieved from mem-\nory? In this work, we offer the first methodolog-\nical framework for probing and characterizing\nrecall of memorized sequences in transformer\nLMs. First, we lay out criteria for detecting\nmodel inputs that trigger memory recall, and\npropose idioms as inputs that typically fulfill\nthese criteria. Next, we construct a dataset of\nEnglish idioms and use it to compare model\nbehavior on memorized vs. non-memorized\ninputs. Specifically, we analyze the internal\nprediction construction process by interpreting\nthe model’s hidden representations as a gradual\nrefinement of the output probability distribu-\ntion. We find that across different model sizes\nand architectures, memorized predictions are a\ntwo-step process: early layers promote the pre-\ndicted token to the top of the output distribution,\nand upper layers increase model confidence.\nThis suggests that memorized information is\nstored and retrieved in the early layers of the\nnetwork. Last, we demonstrate the utility of\nour methodology beyond idioms in memorized\nfactual statements. Overall, our work makes a\nfirst step towards understanding memory recall,\nand provides a methodological basis for future\nstudies of transformer memorization.1\n1 Introduction\nTransformer language models (LMs) memorize in-\nstances from their training data (Carlini et al., 2021;\nZhang et al., 2021b), and evidence is building that\nsuch memorization is an important precondition for\ntheir predictive abilities (Lee et al., 2022; Feldman,\n∗ Now at Google Research.\n1Our code and data are available at https://github.\ncom/adihaviv/idiomem/.\n2020; Feldman and Zhang, 2020; Raunak et al.,\n2021; Raunak and Menezes, 2022). Still, it is un-\nknown when models decide to output memorized\nsequences, and how these sequences are being re-\ntrieved internally from memory. Current methods\nfor analyzing memorization (Feldman and Zhang,\n2020; Zhang et al., 2021b; Carlini et al., 2022)\nuse definitions that are based on models perfor-\nmance, which changes between models and often\nalso between training runs. Moreover, these meth-\nods study memorization behavior in terms of the\nmodel’s “black-box” behavior rather than deriving\na behavioral profile of memory recall itself.\nOur first contributions are to provide a definition\nand construct a dataset that allows probing memo-\nrization recall in LMs. We define a set of criteria\nfor identifying memorized sequences that does not\ndepend on model behavior:2 sequences that have\na single plausible completion that is independent\nof context and can be inferred only given the entire\nsequence. We show that many idioms (e.g., “play\nit by ear”) fulfill these conditions, allowing us to\nprobe and analyze memorization behavior. Fur-\nthermore, we construct a dataset of such English\nidioms, dubbed IDIO MEM, and release it publicly\nfor the research community.\nNext, to analyze memory recall behavior, we\ncompare the construction process of predictions\nthat involve memory recall with those that do not.\nTo this end, given a LM, we create two sets of mem-\norized and non-memorized idioms from IDIO MEM\n(Fig. 1, A). We then adopt a view of the transformer\ninference pass as a gradual refinement of the output\nprobability distribution (Geva et al., 2021; Elhage\net al., 2021). Concretely, the token representation\nat any layer is interpreted as a “hidden” probabil-\n2Literature often purports to “define memorization”, result-\ning in a multitude of technical definitions with subtle differ-\nences, although we would expect this concept to be consistent\nand intuitive. Thus, instead of explicitly defining “memoriza-\ntion”, we will define sufficient criteria for detecting memo-\nrized instances.\n248\nCandidate \nPromotion\nConfidence \nboosting\nIdioMem\nplay it by\n(A) Splitting IdioMem to two sets of memorized \nand non-memorized idioms\nthink outside the\nyourself\nmilk\nbox\ncrying over spilt\n(B) Tracking the prediction’s rank \nand probability at each layer\n(C) Visualization: memorized idioms \ndisplay two-steps prediction process\nspiltovercrying\nmilk\n___\n0.7 milk\n0.03 blood\n0.004 wine\n0.003 coffee\n0.1 unden\n0.04 streng\n>0.0001 milk\n…\n…\n…\n...\n ...\n ...\n ...\n…\nFigure 1: Our methodological framework for probing and analyzing memorized predictions of a given LM: (A) we\ncreate two sets of memorized (mem-idiom) and non-memorized (non-mem-idiom) idioms by probing the LM with\ninstances from IDIO MEM, (B) for each instance, we extract hidden features of the prediction computation – the\nrank and probability of the predicted token across layers, and (C) we compare the prediction process of memorized\nidioms versus non-memorized idioms and short sequences from Wikipedia (wiki). Memorized predictions exhibit\ntwo characteristic phases: candidate promotion and confidence-boosting.\nity distribution over the output vocabulary (Geva\net al., 2022) (Fig. 1, B). This interpretation allows\ntracking the prediction across layers in the evolving\ndistribution. We find a clear difference in model be-\nhavior between memorized and non-memorized\npredictions (Fig. 1, C). This difference persists\nacross different transformer architectures and sizes:\nretrieval from memory happens in two distinct\nphases, corresponding to distinct roles of the trans-\nformer parameters and layers: (1) candidate promo-\ntion of memorized predictions’ rank in the hidden\ndistribution in the first layers, and (2) confidence\nboosting where, in the last few layers, the predic-\ntion’s probability grows substantially faster than\nbefore. This is unlike non-memorized predictions,\nwhere the two phases are less pronounced and of-\nten indistinct. We further confirm these phases of\nmemorized predictions through intervention in the\nnetwork’s FFN sublayers, which have been shown\nto play an important role in the prediction construc-\ntion process (Geva et al., 2022; Mickus et al., 2022).\nConcretely, zeroing-out hidden FFN neurons in\nearly layers deteriorate memory-recall, while inter-\nvention in upper layers does not affect it.\nLast, we show our findings extend to types of\nmemory recall beyond idioms by applying our\nmethod to factual statements from the LAMA-\nUHN dataset (Poerner et al., 2020) (e.g. “The\nnative language of Jean Marais is French”). For\nfactual statements that were completed correctly\nby the LM, we observe the same two phases as in\nmemorized idioms, further indicating their connec-\ntion to memory recall.\nTo summarize, we construct a novel dataset of\nidioms, usable for probing LM memorization irre-\nspective of the model architecture or training pa-\nrameterization. We then design a probing method-\nology that extracts carefully-devised features of the\ninternal inference procedure in transformers. By ap-\nplying our methodology and using our new dataset,\nwe discover a profile that characterizes memory\nrecall, across transformer LMs and types of mem-\norized instances. Our released dataset, probing\nframework, and findings open the door for future\nwork on transformer memorization, to ultimately\ndemystify the internals of neural memory in LMs.\n2 Criteria for Detecting Memory Recall\nTo study memory recall, we require a set of inputs\nthat trigger this process. Prior work on memoriza-\ntion focused on detecting instances whose inclu-\nsion in the training data has a specific influence\non model behavior, such as increased accuracy on\nthose instances (Feldman and Zhang, 2020; Ma-\ngar and Schwartz, 2022; Carlini et al., 2022, 2021,\n2019). As a result, memorized instances differ\nacross models and training parameterization. Our\ngoal is instead to find a stable dataset of sequences\nthat correctly predicting their completion indicates\nmemorization recall. This will greatly reduce the\noverhead of studying memorization and facilitate\nuseful comparisons across models and studies.\nTo build such a dataset, we start by defining\na general set of criteria that are predicates on se-\nquence features, entirely independent of the LM\nbeing probed. Given a textual sequence of n words,\n249\nwe call the first n −1 words the prompt and the\nnth word the target. We focus on the task of pre-\ndicting the target given the prompt, i.e., predicting\nthe last word in a sequence given its prefix.3 Such\npredictions can be based on either generalization\nor memorization, and we are interested in isolat-\ning memorized cases to study model behavior on\nthem. Particularly, we are looking for sequences\nfor which success in this task implies memorization\nrecall.\nWe argue that the following criteria aresufficient\nfor detecting such memorized sequences:\n1. Single target, independent of context: We\nrequire that the target is the only correct contin-\nuation, regardless of the textual context where\nthe prompt is placed.4\n2. Irreducible prompt: The target is the single\ncorrect completion only if the entire prompt is\ngiven exactly. Changing or removing parts from\nthe prompt would make the correct target non-\nunique.\nClaim 2.1. Assume a sequence fulfills the above\ncriteria. Then, if a LM correctly predicts the tar-\nget, it is highly likely that this prediction involves\nmemory recall.\nJustification. First, observe that most natural-\nlanguage prompts have many possible continua-\ntions. For example consider the sentence “to get\nthere fast, you can take this ____” . Likely con-\ntinuations include “route”, “highway”, “road”,\n“train”, “plane”, “advice”, inter alia. Note that\nthere are several divergent interpretations or con-\ntexts for the prompt, and for each, language offers\nmany different ways to express similar meaning.\nA prediction that is a product of generalization\n— i.e., it is derived from context and knowledge\nof language — always has plausible alternatives,\ndepending on the context and stylistic choice of\nwords. Hence, the relationship between the entire\nprompt and the target, where the target is the single\ncorrect continuation, is something that needs to be\nmemorized rather than derived via generalization.\nA LM that predicts the single correct continuation\neither memorized this relationship, or used “cues”\nfrom the prompt that happen to provide indica-\ntion towards the correct continuation. To illustrate\nthe latter, consider the sequence “it’s raining cats\n3In cases where tokenization divides the target to sub-\ntokens, our task becomes predicting the target’s first token.\n4We assume that contexts are naturally-occurring and not\nadversarial.\nand ____” which has a single correct continuation,\n“dogs”, but a LM might predict it without observing\nthis sequence during training, due to the seman-\ntic proximity of “cats” and “dogs”. Our second\ncriterion excludes such cases by requiring that the\ncorrect continuation is only likely given the entire\nsequence.\nTherefore, a LM that correctly completes a se-\nquence that fulfills both criteria, is likely to have\nrecalled it from memory.\nIn the next section, we argue that idioms are a\nspecial case of such sequences, and are thus useful\nfor studying memorization (§3).\n3 The Utility of Idioms for Studying\nMemorization\nAn idiom is a group of words with a meaning that\nis not deducible from the meanings of its individual\nwords. For example, consider the phrase “play it\nby ear” — there is a disconnect between its non-\nsensical literal meaning (to play something by a\nhuman-body organ called ‘ear’) and its intended\nidiomatic meaning (to improvise).\nA key observation is thatidioms often satisfy our\ncriteria (§2), and therefore can probe memoriza-\ntion. First, by definition, idioms are expected to\nbe non-compositional (Dankers et al., 2022). They\nare special “hard-coded” phrases that carry a spe-\ncific meaning. As a result, their prompts each have\na single correct continuation, regardless of their\ncontext (criterion 1). For example, consider the\nprompt “crying over spilt ____” — a generaliz-\ning prediction would predict that this slot may be\nfilled by any spillable item, like wine, water or\njuice, while a memorized prediction will retrieve\nonly milk in this context. Notably, while this is an\nempirical characterization of many idioms, there\nmight be exceptions, e.g., contexts that are adver-\nsarially chosen to change the completion. Second,\nmany idioms are “irreducible”, for example the\nsub-sequences “crying over” or “over spilt” by\nthemselves have but scant connection to the word\n“milk”.\nStill, not all idioms fulfill the criteria. For exam-\nple, even when the idiom is far from literal, its con-\nstituents sometimes strongly indicate the correct\ncontinuation, such as with the case of “it’s raining\ncats and ____” (as explained in §2). To construct\na dataset of memorization-probing sequences, we\nwill carefully curate a set of English idioms and\nfilter out ones that do not fulfill our criteria.\n250\nSource # of Idioms Idiom Length (words)\nMAGPIE 590 4.5 ± 0.9\nLIDIOMS 149 5.1 ± 1.2\nEF 97 5.6 ± 1.9\nEPIE 76 4.4 ± 0.7\nTotal (unique) 814 4.7 ± 1.8\nTable 1: Statistics per data source in IDIO MEM.\n3.1 The I DIO MEM Dataset\nWe begin with existing datasets of English idioms:\nMAGPIE (Haagsma et al., 2020),5 EPIE (Saxena\nand Paul, 2020), and the English subset of LID-\nIOMS (Moussallem et al., 2018). We enrich this\ncollection with idioms scraped from the website\n“Education First” (EF).6 We then split each idiom\ninto a prompt containing all but the last word, and\na target that is the last word. Next, we filter out\nidioms that do not comply with our criteria (§2) or\nwhose target can be predicted from their prompt\nbased on spurious correlations rather than memo-\nrization. To this end, we use three simple rules:\n• Short idioms. We observe that prompts of id-\nioms with just a few words often have multiple\nplausible continuations, that are not necessar-\nily the idiom’s target, violating our first crite-\nrion. For example, the prompt “break a ____”\nhas many possible continuations (e.g. “win-\ndow”, “promise”, and “heart”) in addition to its\nidiomatic continuation “leg”. To exclude such\ncases, we filter out idioms with < 4 words.\n• Idioms whose target is commonly predicted\nfrom the prompt’s subsequence. We filter such\ncases to ensure the prompt fulfills our second\ncriterion (prompt irreducibility).\nTo detect these cases, we use an ensemble of\npretrained LMs: GPT2 M, ROBERTA-BASE\n(Liu et al., 2019), T5- BASE (Kale and Ras-\ntogi, 2020) and ELECTRA- BASE -GENERATOR\n(Clark et al., 2020), and check for each model if\nthere is an n-gram ( 1 ≤n ≤4) in the prompt\nfrom which the model predicts the target. We\nfilter out idioms for which a majority ( ≥3) of\nmodels predicted the target (for some n-gram).\n• Idioms whose targets are semantically simi-\nlar to tokens in the prompt. To further ensure\nprompt irreducibility, we embed the prompt’s\ntokens and the target token using GloVe word\n5We take idioms with an annotation confidence of > 75%\nand exclude frequently occurring literal interpretations.\n6https://www.ef.com/wwen/english-resources/\nenglish-idioms/\nPrompt Target Pred. Sim. IDIO MEM\n“make a mountain\nout of a”\nmolehill ✓\n“think outside the” box ✓\n“there’s no such\nthing as a free”\nlunch ✓\n“go back to the\ndrawing”\nboard ✓\n“boys will be” boys ✓\n“take it or leave” it ✓ ✓\nTable 2: Example English idioms included and ex-\ncluded from IDIO MEM by our filters of predictable tar-\nget (Pred.) and prompt-target similarity (Sim.).\nembeddings (Pennington et al., 2014). We mea-\nsure the cosine distance between the target token\nto each token in the prompt separately and take\nthe maximum of all the tokens. We filter out\nidioms where this number is higher than 0.75\n(this number was tuned manually using a small\nvalidation set of idioms).\nOverall, 55.7% of the idioms were filtered out,\nincluding 48.5% by length, 6.1% by the predictable-\ntarget test and an additional 1.6% by the prompt-\ntarget similarity, resulting in an 814 idioms dataset,\nnamed IDIO MEM. Further statistics are provided\nin Tab. 1, and example idioms in Tab. 2.\n4 Probing Methodology\nBackground and Notation Assuming a trans-\nformer LM with L layers, a hidden dimension d\nand an input/output-embedding matrixE ∈R|V|×d\nover a vocabulary V . Denote by s = ⟨s1, ..., st⟩\nthe input sequence to the LM, and let hℓ\ni be the\noutput for token i at layer ℓ, for all ℓ ∈1, ..., Land\ni ∈1, ..., t. The model’s prediction for a tokensi is\nobtained by projecting its last hidden representation\nhL\ni to the embedding matrix, i.e. softmax(EhL\ni ).\nFollowing (Geva et al., 2021, 2022), we in-\nterpret the prediction for a token si by viewing\nits corresponding sequence of hidden representa-\ntions h1\ni, ...,hL\ni as an evolving distribution over\nthe vocabulary. Concretely, we read the “hidden”\ndistribution at layer ℓ by applying the same pro-\njection to the hidden representation at that layer:\npℓ\ni = softmax(Ehℓ\ni). Using this interpretation,\nwe track the probability and rank of the predicted\ntoken in the output distribution across layers. A\ntoken’s rank is its position in the output distribution\nwhen sorted by probability from highest to lowest\n(e.g. the rank of the final predicted token is zero).\n251\nGPT2 M GPT2 L BERT B BERT L\nMemorized idioms (mem-idiom) 364 44.7% 392 48.2% 230 28.3% 305 37.5%\nNon-memorized idioms (non-mem-idiom) 450 55.3% 422 51.8% 584 71.7% 509 62.5%\nTable 3: Number of memorized idioms vs. non-memorized idioms from the IDIO MEM dataset for each model. An\ninstance is considered a memorized example if the model correctly predicts the target.\nProbing Procedure Our key method to under-\nstand how transformer LMs retrieve information\nfrom memory is comparing features of memory re-\ncall to inference that does not necessarily include\nmemory recall. Given a set of sequences that fulfill\nthe criteria in §2, we split them into a “memo-\nrized” set whose targets’ first token is predicted\ncorrectly by the model being analyzed given (and\nare therefore memorized), and a “non-memorized”\nset whose target is predicted incorrectly. We addi-\ntionally include a second set of “non-memorized”\ninstances: natural-language sequences randomly\nsampled from a large corpus (we assume most\nnaturally-occurring sequences are not memorized).\nTo probe a LM, we run it on the 3 sets, and for\neach set and each layer, we (a) extract the rank and\nprobability of the final predicted token in the hid-\nden distribution for each prompt, and (b) compute\nthe mean rank and probability over all prompts.\n5 Probing Memorization Using Idioms\n5.1 Experimental Setup\nDatasets For each LM under analysis (see be-\nlow), we split IDIO MEM into two disjoint subsets\nof memorized and non-memorized idioms, denoted\nas mem-idiom and non-mem-idiom, respectively,\naccording to whether or not the LM succeeds in\ncompleting them. We produce an additional set\nof non-memorized instances, wiki, by sampling\nprompts from the WIKI TEXT-103 dataset (Merity\net al., 2017) of the same length distribution as in\nIDIO MEM (see Tab. 1).\nModels We use multiple transformer LMs that\nare different in size, architecture, and optimiza-\ntion objective. We use GPT2 (medium and large)\n(Radford et al., 2019), an autoregressive trans-\nformer decoder, and BERT (base and large) (De-\nvlin et al., 2019), a transformer encoder trained\nwith a masked language modeling (MLM) objec-\ntive. To evaluate BERT on a specific idiom, we\nfeed the idiom’s prompt concatenated with the spe-\ncial mask token and a period (e.g. “think outside\nthe [MASK]. ”). Further details on each model are\npresented in Tab. 6. The number of memorized and\nnon-memorized idioms from IDIO MEM for each\nmodel are provided in Tab. 3.\n5.2 Memorized Predictions are a Two-Step\nProcess\nFig. 2 shows the probability and rank of the\noutput token across layers, for memorized and\nnon-memorized idioms and short sequences from\nWikipedia, by GPT2 M, GPT2 L, BERT B, and\nBERT L. Naturally, the prediction’s rank decreases\nacross layers as the prediction probability increases.\nHowever, for memorized predictions these trends\noccur as two distinct and sharp inference phases. In\nlower layers, the prediction’s rank decreases from a\nhigh rank to near zero, while its probability is also\nclose to zero. For example, in GPT2 L the rank de-\ncreases until layer 20 while the probability remains\nbelow 0.1. We refer to this phase as candidate pro-\nmotion, as the predicted token is being promoted\nto be a top candidate in the output distribution.\nCompared to non-memorized predictions, the\ninitial rank of memorized predictions is generally\nhigher, especially in GPT2 (6000 vs. 3000 in\nGPT2 M, and 3000 vs. 1500 in GPT2 L). A po-\ntential explanation would be a generally lower fre-\nquency of the predicted token for memorized id-\nioms. However, we find there is only low negative\ncorrelation between the initial rank of the predicted\ntoken and its frequency (see Tab. 4). We there-\nfore offer a different explanation: non-memorized\npredictions are often promoted in early layers that\ndetect local “shallow” patterns, such as common\nbigrams (Geva et al., 2021), while predictions for\nmemorized idioms are not local as they requires\nprocessing of the entire input.\nIn the middle layers, once the predicted token\nreaches the top of the hidden distribution, its proba-\nbility increases until the last layer. We refer to this\nphase as confidence boosting, as the distribution\nshifts towards the predicted token. For memorized\nidioms, this increase is abrupt and dramatic, with\na final probability of > 0.6 across all models. In\ncomparison, predictions on short sequences from\nWikipedia and non-memorized idioms have a sub-\nstantially lower probability of ∼0.2. This can be\n252\n0\n2000\n4000\n6000rank\nGPT2M\nmem-idiom non-mem-idiom wiki\n1 6 11 16 21\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\n0\n1000\n2000\n3000\nrank\nGPT2L\n1 6 11 16 21 26 31 36\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\n0\n2000\n4000rank\nBERTB\n1 3 5 7 9 11\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\n0\n5000\n10000\n15000rank\nBERTL\n1 6 11 16 21\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\nFigure 2: The predicted token’s probability and rank across layers, for memorized idioms ( mem-idiom), non-\nmemorized idioms (non-mem-idiom) and short sequences from Wikipedia ( wiki). Memory recall exhibits two\ncharacteristic phases of candidate promotion and confidence boosting.\nexplained by the fact that memorized idioms have\na single correct target, rather than many possible\ncontinuations, as in the instances from Wikipedia.\nIn addition, low-probability predictions for non-\nmemorized idioms are expected as the model did\nnot memorize the idioms and does not know their\ncontinuation. In §B, we provide more fine-grained\nanalysis of these trends via a log-scaled view of the\nprediction’s rank and a visualization of the ranks\nand probabilities for separate clusters of the memo-\nrized predictions.\nWe further verify that our extracted hidden-\ndistribution features distinguish between memo-\nrized and non-memorized predictions by training\nlinear classifiers over combinations of these fea-\ntures (details in §A). We observe that, indeed, our\nfeatures enable separation between memorized and\nnon-memorized predictions at high accuracy (77%-\n85% across models). Moreover, classifiers that use\nhidden distribution features are more accurate than\nthose relying only on the model’s output. Over-\ncorrelation p-value\nGPT2 M -0.22 2.9e−21\nGPT2 L -0.18 5.4e−14\nBERT B -0.19 1.8e−15\nBERT L 0.15 3.5e−10\nTable 4: Pearson correlation between the predicted to-\nken’s rank at the first layer and its general frequency in\nWikipedia.\nall, these findings provide a profile of memorized\npredictions, suggesting that the memorized infor-\nmation is retrieved in early layers at inference.\n5.3 Testing the Roles of Different Layers\nThrough Intervention\nOur analysis in the previous section interprets hid-\nden representations as distributions over the output\nvocabulary. We now conduct intervention experi-\nments to verify that this interpretation is meaning-\nful for studying memory recall, and to test layers’\nroles in that process. Concretely, we zero out FFN\n253\n1 3 5 7 9 11 13 15 17 19 21 23\nend layer\n1357911131517192123\nstart layer\nnon-dominant FFN sub-updates\n1 3 5 7 9 11 13 15 17 19 21 23\nend layer\n1357911131517192123\nstart layer\ndominant FFN sub-updates\n0%\n20%\n40%\n60%\n80%\n100%\n0%\n20%\n40%\n60%\n80%\n100%\nFigure 3: Intervention in non-dominant (left) and dominant (right) FFN sub-updates in GPT2 M. Each cell shows\nthe percentage of memorized idioms for which the prediction was changed by zeroing-out the FFN sub-updates\nbetween the start and end layers.\nsub-updates to the hidden representation (§4) and\nmeasure changes in memorized predictions.\nA Short Primer on Transformer FFN Sublayers.\nFFN sublayers are the final computation in trans-\nformer layers, which output the hidden distribution\nat the center of our analysis. In general, they have\na key role in capturing knowledge in transformer\nLMs (Dai et al., 2022; Meng et al., 2022). We\nfollow Geva et al. (2022) and view the computa-\ntion of each FFN sublayer as a weighted collection\nof dm sub-updates to the output distribution, each\npromoting a concept in the vocabulary space, e.g.\n“past-tense verbs” or “female athletes”. To under-\nstand this, consider the computation of the FFN\nat layer ℓ, given by FFNℓ(hℓ\ni) = f(Wℓ\nKhℓ\ni)Wℓ\nV,\nwhere Wℓ\nK, Wℓ\nV ∈ Rdm×d are parameter matri-\nces, dm is the intermediate hidden dimension, and\nf is a point-wise non-linearity activation func-\ntion. This computation can be decomposed as:\nFFNℓ(hℓ\ni) = ∑dm\nj=1 f(hℓ\ni ·kℓ\nj)vℓ\nj = ∑dm\nj=1 mℓ\njvℓ\nj,\nwhere kℓ\nj and vℓ\nj are the j-th row in Wℓ\nK and the\nj-th column in Wℓ\nV, respectively. Geva et al. argue\nthat each weight mℓ\nj is the score assigned by the\nmodel for some textual pattern, and each vector vj\npromotes a concept that follows that pattern.\nExperiment First, we sample 100 random in-\nstances from IDIO MEM that the model memorized.\nThen, for each range of up to 3 consecutive lay-\ners, we perform two complementary experiments,\nwhere we run GPT2 M’s inference on the 100 in-\nstances while intervening in the chosen layers to\ncancel the contribution of FFN sub-updates to the\nprediction. Specifically, for each layerℓ in the layer\nrange, we perform the following two interventions:\nfirst, we zero out (i.e. artificially set to 0 during\ninference) the weights of the 10 most dominant\nsub-updates, which are known to be particularly\nsalient for predictions (Geva et al., 2022) (there are\ndm sub-updates per layer). Concretely, we sort the\nsub-updates by their contribution to the FFN out-\nput |mℓ\ni|||vℓ\ni||∀i ∈[1, ..., dm], and set mℓ\ni = 0for\nthe 10 sub-updates with the highest contribution.\nNext, we zero out non-dominant sub-updates, i.e.\nall the sub-updates except for the 10 most dominant\nones. For each intervention, we measure how often\nit changes the predicted token. Further measure-\nments of the change in rank and probability of the\ntarget token are reported in §C.\nResults Fig. 3 shows, for each layer range, the\npercentage of memorized idioms where the pre-\ndicted token has changed. Focusing on zeroing out\nnon-dominant FFN sub-updates (Fig. 3, left), we\nobserve a two-phase pattern of decreasing “layer\nimportance” which corresponds to the two-phase\npattern of decreasing rank and increasing prob-\nability during inference (§5.2) : layers’ effect on\nmemory recall drops precipitously in the first 10\nlayers, corresponding to the candidate promotion\nphase. Then, from around layer 10 onwards, the\ndrop in effect is much less steep, corresponding to\nthe confidence-boosting phase. Intervention in up-\nper layers rarely changes the predicted token, and\nits effect is limited to reducing the model’s con-\nfidence (§C). We further visualize this two-phase\nbehavior in Fig. 7 in §C.\nThese findings suggest that the candidate pro-\nmotion phase, while having a smaller effect on\nthe prediction’s assigned probability compared to\nthe later confidence-boosting stage, in fact, has a\ncrucial role in memory recall.\nWe also observe that interventions in the first\nlayer are by far the most destructive, with 100%\nchange in prediction for non-dominant updates\n254\n(Fig. 3, left). Unlike for other layers, this is also\nobservable when zeroing out the dominant sub-\nupdates (Fig. 3, right), which constitute only 0.1%\nof layer sub-updates. This suggests the first layer\nis especially critical for memorized predictions.\n6 Memorization of Factual Statements\nWe now examine if our findings generalize beyond\nidioms to other types of memory recall, focusing\non the completion of statements expressing facts.\nData Datasets for evaluating memorization of\nfactual knowledge typically contain simple queries\nsuch as “The continent of Kuwait is”, where pre-\ndicting the next token requires knowledge of the\ntriplet ⟨s, r, t⟩where s is a source entity (e.g.,\nKuwait), t a target entity (e.g., Asia), and r is the\nrelation between them (e.g., is the continent\nof). However, unlike idioms, such queries are not\nsuitable for probing memorization since they often\nfail to satisfy the criteria in §2. Concretely, queries\noften include “clues” that could make the predic-\ntion easy to guess and based on generalization\n(Poerner et al., 2020) (e.g. predicting a Spanish-\nspeaking country for the query “Federico López\nwas born in”), and the same fact can be expressed\nin multiple different ways (e.g. “Kuwait is a coun-\ntry in Western Asia” also encodes the above fact).\nTo test memorization of facts, we, therefore,\ncollect factual statements where such cases are\nexcluded. We use LAMA-UHN (Poerner et al.,\n2020), a subset of theLAMA dataset (Petroni et al.,\n2019), where “easy-to-guess” queries are filtered\nout. LAMA comprises of queries structured as\n“fill-in-the-blank” cloze statements (e.g. “Gordon\nScholes is a member of the ____political party. ”).\nTo accommodate autoregressive LMs, we consider\nonly queries where the blank appears at the end. In\naddition, we keep only queries with a single cor-\nrect completion (based on LAMA ). This turns our\ndefinition of memorized and non-memorized sets\n(§4) equivalent to separating based on the evalu-\nation metric of LAMA: an instance is considered\nmemorized if the model predicts the single correct\ncompletion and non-memorized otherwise. Overall,\nthe resulting collection consists of 17,855 factual\nstatements with 22 unique relations.\nMemorized Facts Exhibit a Similar Predic-\ntion Process to Idioms We repeat our analysis\n(§5.1), using the collected factual statements and\nGPT2 M. Splitting the statements to those memo-\n0\n1000\n2000\n3000rank\nmem-fact\nnon-mem-fact\nwiki\n1 6 11 16 21\nlayer\n0.0\n0.2\n0.4\n0.6\nprobability\nFigure 4: The predicted token’s probability and rank\nacross layers of GPT2 M, for memorized ( mem-fact)\nand non-memorized ( non-mem-fact) facts and short\nsequences from Wikipedia (wiki).\nrized and non-memorized by the model results in\n786 mem-facts vs. 17,069 non-mem-facts state-\nments.\nFig. 4 shows the rank and probability of the pre-\ndicted token across layers. Like idioms, memorized\nfacts exhibit a clear two-phase prediction process,\nwhere the prediction probability rapidly increases\nonce the candidate reaches a low rank (at layer\n16). This is in contrast to non-memorized facts and\nshort sequences from Wikipedia, where the rank\n(probability) gradually decreases (increases) across\nlayers without a distinct two phases.\nDifferences from Memorized Idioms Stem from\nIll-Defined Targets There is one major differ-\nence compared to memorized idioms (Fig. 4 vs.\nFig. 2 upper left), which is a substantial drop in\nprobability (0.62 →0.21) in the last two layers.\nWe hypothesize that this is because the input query\nhas multiple plausible completions that were not\nspecified as “correct” targets in LAMA . We verify\nthis by manually analyzing predictions, and find\nthat for 82 out of 100 queries there is more than\none correct continuation in top 5 predicted tokens.\nWe posit that the above deficiency of LAMA is\ninherent because, in violation of our criterion (§2),\nfactual statements can usually be expressed in many\nways so their prompt has no single correct target.\n7 Related Work\nMemorization as Training-Data Influence.\nMemorization in LMs has attracted immense atten-\ntion due to their rich sensitive training data (Carlini\net al., 2019; Song and Shmatikov, 2019; Carlini\n255\net al., 2021; Zhang et al., 2021b; Carlini et al.,\n2022; Tirumala et al., 2022; Tänzer et al., 2022;\nRaunak and Menezes, 2022). Recent work suggests\nthat memorization is necessary for performant ML\ndue to the “long tail” of infrequently-observed pat-\nterns (Zhang et al., 2021a; Feldman, 2020; Brown\net al., 2021; Raunak et al., 2021). This line of\nwork has two key limitations: (a) only black-box\nbehavior is measured rather than looking at models’\ninternal prediction process, and (b) it detects mem-\norized instances by measuring the effect of their\ninclusion in the training set on inference behavior.\nThis results in a set of memorized examples that\nis specific to the model, training data, and even\ntraining pass, making it difficult to build on these\nresults in future research.\nTransformers and Idioms. Nedumpozhimana\nand Kelleher (2021) showed that idioms are iden-\ntified using textual cues within the expression;\nDankers et al. (2022) showed that idioms tend to\nbe internally processed as single units of meaning.\nIt has also been known (Fakharian and Cook, 2021;\nSalton et al., 2016) that LM contextual embeddings\nencode information about whether or not an expres-\nsion is idiomatic (vs. literal). (Shwartz and Dagan,\n2019; Chakrabarty et al., 2022) also studied repre-\nsentations and interpretation of non-compositional\nsequences, such as idioms. No prior work used\nidioms to probe LM memorization, which is one of\nthe main contributions of this work. We release our\ndataset, IDIO MEM, to facilitate future research on\nmemorization recall in LMs. Diagnostic datasets,\nsuch as IDIO MEM, have often proven useful in the\npast (Sugawara et al., 2022; Parrish et al., 2021).\nMemorization of Factual Knowledge. An ex-\ntensive line of work (Petroni et al., 2019; Jiang\net al., 2020; Poerner et al., 2019; Lewis et al.,\n2020; Elazar et al., 2021) studied LMs’ capacity\nto acquire relational knowledge during training.\nSome attention has also been given to understand-\ning the inner workings of factual-memory recall:\nWallat et al. (2020) showed that some facts are\nretrieved from the bottom and intermediate layers,\nand Meng et al. (2022) localized factual-knowledge\nrecall within feed-forward-component computation.\nSince factual statements do not fulfill our criteria, it\nis difficult to convincingly argue that correct predic-\ntions indicate memory recall, making it impossible\nto use them to isolate the effect of memorization.\n8 Conclusion\nWe introduce a methodological framework for de-\ntecting and analyzing memorized predictions in\ntransformer LMs. This includes a set of criteria\non textual sequences for probing memorized pre-\ndictions, the IDIO MEM dataset of idioms fulfilling\nthese criteria, and an interpretation method of pre-\ndiction internals. We characterize a behavioral pro-\nfile that is unique to predictions involving memory\nrecall and is observable across different LMs and\ndata types. By providing these fundamental tools\nand initiating a thread of research on the phenom-\nena we observe, we hope to empower future work\ntowards demystifying transformer memorization.\nLimitations\nOur criteria for detecting memorized instances are\nsufficient but not necessary, which raises the ques-\ntion of what other sequences that trigger memory\nrecall satisfy them.\nAdditionally, while correct prediction for se-\nquences that fulfill our criteria implies memory\nrecall, incorrect prediction does not necessarily im-\nply that no memory was used. This means that\nour set non-mem-idiom might include some por-\ntion of memorized sequences. This does not qual-\nitatively affect our results as long as mem-idiom\nstill contains more memorized predictions than\nnon-mem-idiom, as is evidenced by the stark differ-\nences we observe between LMs’ internal behaviors\non these sets.\nOur work focuses on showing the utility of id-\nioms for probing memorization, and opening up a\nnew thread of research in this vein. We therefore\nleave further investigation of the above gaps for\nfuture work.\nAcknowledgements\nWe thank Gabriel Stanovsky for helpful feedback.\nThis project has received funding and resources\nfrom CIFAR through the Canada CIFAR AI Chair\nprogram, the Province of Ontario, companies spon-\nsoring the Vector Institute, as well as the Com-\nputer Science Scholarship granted by the Séphora\nBerrebi Foundation, and the European Research\nCouncil (ERC) under the European Union’s Hori-\nzon 2020 research and innovation programme,\ngrant agreement No. 802774 (iEXTRACT).\n256\nReferences\nGavin Brown, Mark Bun, Vitaly Feldman, Adam Smith,\nand Kunal Talwar. 2021. When is memorization of\nirrelevant training data necessary for high-accuracy\nlearning? In Proceedings of the 53rd Annual ACM\nSIGACT Symposium on Theory of Computing, pages\n123–132.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX Security Symposium\n(USENIX Security 19), pages 267–284, Santa Clara,\nCA. USENIX Association.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nUSENIX Association.\nTuhin Chakrabarty, Yejin Choi, and Vered Shwartz.\n2022. It’s not rocket science: Interpreting figurative\nlanguage in narratives. Transactions of the Associa-\ntion for Computational Linguistics, 10:589–606.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nVerna Dankers, Christopher Lucas, and Ivan Titov. 2022.\nCan transformer be too compositional? analysing id-\niom processing in neural machine translation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3608–3626, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread. Https://transformer-\ncircuits.pub/2021/framework/index.html.\nSamin Fakharian and Paul Cook. 2021. Contextualized\nembeddings encode monolingual and cross-lingual\nknowledge of idiomaticity. In Proceedings of the\n17th Workshop on Multiword Expressions (MWE\n2021), pages 23–32.\nVitaly Feldman. 2020. Does learning require memoriza-\ntion? a short tale about a long tail. In Proceedings\nof the 52nd Annual ACM SIGACT Symposium on\nTheory of Computing, STOC 2020, page 954–959,\nNew York, NY , USA. Association for Computing\nMachinery.\nVitaly Feldman and Chiyuan Zhang. 2020. What neural\nnetworks memorize and why: Discovering the long\ntail via influence estimation. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n2881–2891. Curran Associates, Inc.\nMor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav\nGoldberg. 2022. Transformer feed-forward layers\nbuild predictions by promoting concepts in the vo-\ncabulary space. arXiv preprint arXiv:2203.14680.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nHessel Haagsma, Johan Bos, and Malvina Nissim. 2020.\nMAGPIE: A large corpus of potentially idiomatic ex-\npressions. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference, pages 279–287,\nMarseille, France. European Language Resources\nAssociation.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In Proceedings of\n257\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 97–102, Dublin, Ireland.\nAssociation for Computational Linguistics.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445, Dublin, Ireland. Association for\nComputational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nInbal Magar and Roy Schwartz. 2022. Data contamina-\ntion: From memorization to exploitation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 157–165, Dublin, Ireland. Association\nfor Computational Linguistics.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nTimothee Mickus, Denis Paperno, and Mathieu Con-\nstant. 2022. How to dissect a Muppet: The struc-\nture of transformer embedding spaces. Transactions\nof the Association for Computational Linguistics ,\n10:981–996.\nDiego Moussallem, Mohamed Ahmed Sherif, Diego\nEsteves, Marcos Zampieri, and Axel-Cyrille\nNgonga Ngomo. 2018. LIdioms: A multilingual\nlinked idioms data set. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018) , Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nVasudevan Nedumpozhimana and John Kelleher. 2021.\nFinding bert’s idiomatic key. In Proceedings of the\n17th Workshop on Multiword Expressions (MWE\n2021), pages 57–62.\nAlicia Parrish, Sebastian Schuster, Alex Warstadt, Omar\nAgha, Soo-Hwan Lee, Zhuoye Zhao, Samuel R. Bow-\nman, and Tal Linzen. 2021. NOPE: A corpus of\nnaturally-occurring presuppositions in English. In\nProceedings of the 25th Conference on Computa-\ntional Natural Language Learning, pages 349–366,\nOnline. Association for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2019.\nBert is not a knowledge base (yet): Factual knowl-\nedge vs. name-based reasoning in unsupervised qa.\narXiv preprint arXiv:1911.03681, 3.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: Efficient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 803–818,\nOnline. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nVikas Raunak and Arul Menezes. 2022. Finding memo:\nExtractive memorization in constrained sequence\ngeneration tasks. ArXiv, abs/2210.12929.\nVikas Raunak, Arul Menezes, and Marcin Junczys-\nDowmunt. 2021. The curious case of hallucinations\nin neural machine translation. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1172–1183,\nOnline. Association for Computational Linguistics.\nGiancarlo Salton, Robert Ross, and John Kelleher. 2016.\nIdiom token classification using sentential distributed\nsemantics. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 194–204.\nPrateek Saxena and Soma Paul. 2020. Epie dataset: A\ncorpus for possible idiomatic expressions. In Text,\nSpeech, and Dialogue, pages 87–94, Cham. Springer\nInternational Publishing.\nVered Shwartz and Ido Dagan. 2019. Still a pain in\nthe neck: Evaluating text representations on lexical\ncomposition. Transactions of the Association for\nComputational Linguistics, 7:403–419.\n258\nCongzheng Song and Vitaly Shmatikov. 2019. Audit-\ning data provenance in text-generation models. In\nProceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Min-\ning, KDD ’19, page 196–206, New York, NY , USA.\nAssociation for Computing Machinery.\nSaku Sugawara, Nikita Nangia, Alex Warstadt, and\nSamuel Bowman. 2022. What makes reading com-\nprehension questions difficult? In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n6951–6971, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nMichael Tänzer, Sebastian Ruder, and Marek Rei. 2022.\nMemorisation versus generalisation in pre-trained\nlanguage models. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7564–7578,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nKushal Tirumala, Aram H Markosyan, Luke Zettle-\nmoyer, and Armen Aghajanyan. 2022. Memoriza-\ntion without overfitting: Analyzing the training dy-\nnamics of large language models. arXiv preprint\narXiv:2205.10770.\nJonas Wallat, Jaspreet Singh, and Avishek Anand. 2020.\nBERTnesia: Investigating the capture and forgetting\nof knowledge in BERT. In Proceedings of the Third\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP, pages 174–183, On-\nline. Association for Computational Linguistics.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin\nRecht, and Oriol Vinyals. 2021a. Understanding\ndeep learning (still) requires rethinking generaliza-\ntion. Communications of the ACM, 64(3):107–115.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramèr, and Nicholas Car-\nlini. 2021b. Counterfactual memorization in neural\nlanguage models. arXiv preprint arXiv:2112.12938.\n259\nA Distinguishing Memorization Using\nHidden-Distribution Features\n§5.2 shows differences in our extracted hidden-\ndistribution features, namely the rank and probabil-\nity across layers, between memorized idioms and\nnon-memorized sequences. To verify that these fea-\ntures are distinguishing between memorized and\nnon-memorized predictions, we build a classifier\nthat receives them as input, as follows.\nExperiment To answer the above, we represent\nevery instance in IDIO MEM, for every LM we ex-\nperiment with, as a sequence of probabilities and\nranks assigned to the predicted token at each layer.\nThis results in a feature vector for each instance in\nIDIO MEM for each of our LMs (GPT2 M, GPT2 L,\nBERT B, and BERT L). We then append a class\nlabel for each LM’s instances corresponding to\nwhether it memorized them. Then, for each LM’s\ndataset, we perform 10-fold cross-validation with\nan 80%-20% train-test split to evaluate the accuracy\nof a logistic-regression classifier using the Logis-\nticRegresion classifier of scikit-learn,7 specifying\nL1 penalty and a bilinear solver as constructor pa-\nrameters. After each split and before evaluating the\nclassifier, we balance the test set by replacing the\nlarger class with a random subsample the size of the\nsmaller class. To isolate the distinguishing utility\nof ranks from that of probabilities, we repeat this\nprocess while only taking either of them as features\nat each time. We also repeat this process while sep-\narately considering just the last-layer probability\nas a single feature, the last hidden state vector, and\nfinally, the ranks in layers of layers 1-12 (omitting\nranks in layers 13-16 where ranks are usually 0)\nappended to all layer probabilities.\nResults Results are given in Tab. 5. We observe\nthat, across all models, most classifiers perform\nwell over the 50%-accuracy baseline for their class-\nbalanced test sets. The output probability alone is\noften highly distinguishing with around 78% accu-\nracy, but the vector of 16 hidden-distribution prob-\nabilities seems to contain additional distinguish-\ning information, as using it alongside ranks results\nin higher accuracy, typically around 84%. Using\nthe ranks in addition to probabilities usually de-\ncreases accuracy, but omitting the ranks in layers\n13-16 (which we know are mostly 0 as this is the\nconfidence-boosting phase) attenuates this effect.\n7https://scikit-learn.org/\n0\n10000\n20000\n30000\n40000rank\nCluster 0 (191) \nCluster 1 (18) \nCluster 2 (9) \nCluster 3 (3) \nCluster 4 (70) \nCluster 5 (39) \nCluster 6 (13) \n1 6 11 16 21\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nprobability\nFigure 5: The predicted token’s probability and rank\nacross layers for each cluster, after clustering the mem-\norized idioms in GPT2 M according to rank and proba-\nbility across layers.\nWe conjecture that ranks have little meaningful in-\nformation, especially in the confidence-boosting\nphase.\nB Fine-grained Analysis of Memorized\nPredictions\n§5.2 shows how recall of memorized predictions\nexhibits two characteristic phases (specifically, see\nFig. 2). To shed light on this phenomenon, we\nconduct additional analysis.\nB.1 Memorized Idioms In-depth Breakdown\nIn our analysis, we address all memorized predic-\ntions jointly. We now check whether these aver-\naged results are consistent across subgroups of the\nmemorized idioms. To this end, we cluster the\nmemorized idioms by GPT2 M, using the same hid-\nden features as in §A, i.e., each instance is repre-\nsented by the predicted token probability and rank\nacross layers. We then cluster the idioms into seven\ngroups, using K-mean clustering, and visualize the\nprediction rank and probability across layers for\neach group. We set the number of clusters to k = 7\nbased on manual inspection, and as we observed\nno substantial differences in the resulting clusters\nfor larger values of k.\nResults are shown in Fig. 5. We find that all\ngroups exhibit the confidence-boosting phase, as\nthe prediction probability quickly increases starting\nfrom the intermediate layers. Notably, the lowest\nfinal probability observed is > 0.5, which is sub-\nstantially higher than the average probability of\n260\nGPT2 M GPT2 L BERT B BERT L\nprobability 84.6 ±2.8 84 .0 ±2.3 84 .2 ±1.8 77 .6 ±3.3\nranks 63.2 ± 1.6 59 .5 ± 2.7 73 .3 ± 3.3 64 .5 ± 3.6\nprobability + ranks 83.6 ± 2.7 82 .9 ± 2.9 82 .7 ± 2.2 76 .9 ± 2.9\nranks layer 1-12 + probability 84.3 ±2.8 83.4 ± 2.7 82 .7 ± 2.2 77.2 ±4.2\nprobability last layer 79.5 ± 3.5 83 .3 ± 2.3 77 .7 ± 2.3 74 .2 ± 4.8\nfinal hidden state 72 ± 2.1 72 .5 ± 3.7 72 .7 ± 2.9 66 .0 ± 3.0\ntoken ids 58 ± 0.2 59 .4 ± 0.2 58 .8 ± 0.2 71 .6 ± 0.2\nrandom 49.5 ± 3.8 49 .7 ± 4.7 50 .4 ± 4.9 50 .1 ± 3.5\nTable 5: Cross-validation accuracy of a logistic-regression classifier trained to distinguish between memorized and\nnon-memorized idioms.\n∼0.2 for non-memorized predictions (§5.2). How-\never, considering the prediction rank for the differ-\nent groups, we observe a relatively large variation.\nSpecifically, we observe that 55% of the instances\n(cluster 0) have a low initial rank. This further\nsupports the findings in §A.\nB.2 Log-Scale Visualization\nFig. 6 shows a log-scaled view of the graphs from\nFig. 2. We observe that, in terms of orders of mag-\nnitude (vs. absolute value), the differences in initial\nranks between memorized and non-memorized pre-\ndictions are more minor, especially in BERT mod-\nels, whereas rank differences measured in upper\nlayers are more stark.\n261\n1\n100\n10000rank (log-scaled)\nGPT2M\n1 6 11 16 21\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\n0\n1\n10\n100\n1000rank (log-scaled)\nGPT2L\n1 6 11 16 21 26 31 36\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\n10\n1000\nrank (log-scaled)\nBERTB\nmem-idiom non-mem-idiom wiki\n1 3 5 7 9 11\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\n10\n1000rank (log-scaled)\nBERTL\n1 6 11 16 21\nlayer\n0.0\n0.2\n0.5\n0.8\n1.0\nprobability\nFigure 6: The predicted token’s probability and log-scaled rank across layers, for memorized idioms (mem-idiom),\nnon-memorized idioms (non-mem-idiom) and short sequences from Wikipedia (wiki).\n262\n0 5 10 15 20\nstart layer\n0.2\n0.4\n0.6\n0.8\n1.0% of modified preditions\nFigure 7: We visualize the effect of intervening in each\n3-consecutive-layer range according to the procedure\nin §5.3.\nC Intervention Experiments: Additional\nAnalysis\nC.1 Additional Visualization\nIn §5.3, we performed an intervention-based ex-\nperiment to test the effect of zeroing out FFN sub-\nupdates in layer computation. This produced a\nheat map of values corresponding to intervention’s\neffect for each layer range. Here, we plot the in-\ntervention’s effect across all 3-layer layer ranges.\nNote that since there are 24 layers, there are a to-\ntal of 22 ranges of 3 consecutive layers. Fig. 7\nvisualizes the effect of intervening in each such\nrange.\nAs discussed in §5.3, we observe a steep drop\nin effect in the first 10 layers, followed by a more\nleveled slope of decrease in the upper layers.\nC.2 Analyzing Changes in Rank and\nProbability of the Target Token\nIn addition to measuring how often an intervention\nchanges memorized predictions (§5.3), we further\nmeasure the average change in the rank and prob-\nability of the target token. Note that the original\ntarget rank for memorized predictions is always\nzero, as the target token is the top candidate in the\noriginal output distribution.\nChange in target rank Fig. 8 shows the change\nin the target token’s rank for all intervention ex-\nperiments. Overall, we observe similar trends as\nin §5.3. First, zeroing out either dominant or non-\ndominant FFN sub-updates in upper layers (layers\n11-24) does not affect memory recall, as the target\ntoken is still ranked as the top candidate in the out-\nput distribution. Moreover, zeroing out in early lay-\ners (1-10) damages memory recall as the target rank\nincreases by > 20 positions. Specifically, zeroing-\nout non-dominant FFN sub-updates in layers 2-4\nincreases the target rank by 60, and disabling either\ndominant or non-dominant sub-updates in the first\nlayer completely eliminates memory recall as the\nrank increases to > 6000.\nChange in target probability Fig. 9 shows the\nchange in the target token’s probability for all in-\ntervention experiments. Unlike the prediction rank,\nwhich is mostly influenced by the lower layers dur-\ning memory recall, the prediction probability is\nhighly influenced by the intermediate and upper\nlayers, where confidence boosting happens. Dis-\nabling FFN sub-updates in only three of these lay-\ners reduces the prediction probability by up to 33%.\nConsidering the lower layers (1-9), zero-outs lead\nto a large probability decrease (up to 50%). This is\nexpected since these interventions often change the\nprediction, i.e. they eliminate the target from the\ntop of the output distribution.\nD Experimental Setting Details\nTab. 6 shows the evaluated models’ hyperparame-\nters.\n263\n1 3 5 7 9 11 13 15 17 19 21 23\nend layer\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23 start layer\nnon-dominant FFN sub-updates\n1 3 5 7 9 11 13 15 17 19 21 23\nend layer\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23 start layer\ndominant FFN sub-updates\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nend layer\n24681012141618202224\nstart layer\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nend layer\n24681012141618202224\nstart layer\n0\n2000\n4000\n6000\n8000\n0\n1000\n2000\n3000\n4000\n5000\n6000\n0\n10\n20\n30\n40\n50\n60\n0\n10\n20\n30\n40\n50\n60\nFigure 8: Change in the rank of the target token following intervention zeroing out non-dominant (left) and dominant\n(right) FFN sub-updates in GPT2 M. Each cell shows the average change in rank of the target token after zeroing out\nthe sub-updates in the layers between the start and end layers. For readability, we provide plots with the first layer\n(top) and without (bottom).\n1 3 5 7 9 11 13 15 17 19 21 23\nend layer\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23 start layer\nnon-dominant FFN sub-updates\n1 3 5 7 9 11 13 15 17 19 21 23\nend layer\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23 start layer\ndominant FFN sub-updates\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nend layer\n24681012141618202224\nstart layer\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\nend layer\n24681012141618202224\nstart layer\n0%\n20%\n40%\n60%\n80%\n100%\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\n0%\n20%\n40%\n60%\n80%\n100%\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\nFigure 9: Change in the probability of the target token following intervention zeroing out non-dominant (left) and\ndominant (right) FFN sub-updates in GPT2 M. Each cell shows the average change in probability of the target token\nafter zeroing-out the sub-updates in the layers between the start and end layers. For readability, we provide plots\nwith the first layer (top) and without (bottom).\nGPT2 M GPT2 L BERT B BERT L\nLayers 24 36 12 24\nModel hidden dimensions (d) 1024 1280 768 1024\nFeed-forward dimensions (dm) 4096 5120 3072 4096\nAttention heads 12 20 12 16\nParameters 345M 774M 110M 340M\nV ocabulary size (# of tokens) 50,256 50,256 30,522 30,522\nTable 6: The models’ hyperparameters.\n264",
  "topic": "Memorization",
  "concepts": [
    {
      "name": "Memorization",
      "score": 0.8421518802642822
    },
    {
      "name": "Computer science",
      "score": 0.7886298894882202
    },
    {
      "name": "Transformer",
      "score": 0.6659315824508667
    },
    {
      "name": "Recall",
      "score": 0.6580445766448975
    },
    {
      "name": "Phrase",
      "score": 0.6487113237380981
    },
    {
      "name": "Artificial intelligence",
      "score": 0.499645471572876
    },
    {
      "name": "Natural language processing",
      "score": 0.4757256805896759
    },
    {
      "name": "Security token",
      "score": 0.47395941615104675
    },
    {
      "name": "Machine learning",
      "score": 0.3919651508331299
    },
    {
      "name": "Speech recognition",
      "score": 0.35286617279052734
    },
    {
      "name": "Cognitive psychology",
      "score": 0.1818193793296814
    },
    {
      "name": "Psychology",
      "score": 0.1256011724472046
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}