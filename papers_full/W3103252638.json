{
  "title": "Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of Regression and Ranking",
  "url": "https://openalex.org/W3103252638",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2125879798",
      "name": "Ruosong Yang",
      "affiliations": [
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2146822238",
      "name": "Jiannong Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228905580",
      "name": "Zhiyuan Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109328759",
      "name": "Youzheng Wu",
      "affiliations": [
        "JDSU (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2122755126",
      "name": "Xiaodong He",
      "affiliations": [
        "JDSU (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980708516",
    "https://openalex.org/W1954968711",
    "https://openalex.org/W23863916",
    "https://openalex.org/W2962851685",
    "https://openalex.org/W2567547739",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2004881074",
    "https://openalex.org/W2970217403",
    "https://openalex.org/W2923978210",
    "https://openalex.org/W2740592764",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964413085",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2251191978",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2108862644",
    "https://openalex.org/W2954278700",
    "https://openalex.org/W2963830885",
    "https://openalex.org/W2898637464",
    "https://openalex.org/W2124725212",
    "https://openalex.org/W2088696944",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665"
  ],
  "abstract": "2020 Conference on Empirical Methods in Natural Language Processing, 16th-20th November 2020, Online",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1560–1569\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1560\nEnhancing Automated Essay Scoring Performance via Fine-tuning\nPre-trained Language Models with Combination of Regression and\nRanking\nRuosong Yang†‡, Jiannong Cao†, Zhiyuan Wen†, Youzheng Wu‡, Xiaodong He‡\n†Department of Computing, The Hong Kong Polytechnic University\n‡JD AI Research\n†{csryang, csjcao, cszwen}@comp.polyu.edu.hk\n‡{wuyouzheng1, xiaodong.he}@jd.com\nAbstract\nAutomated Essay Scoring (AES) is a critical\ntext regression task that automatically assigns\nscores to essays based on their writing qual-\nity. Recently, the performance of sentence\nprediction tasks has been largely improved by\nusing Pre-trained Language Models via fus-\ning representations from different layers, con-\nstructing an auxiliary sentence, using multi-\ntask learning, etc. However, to solve the AES\ntask, previous works utilize shallow neural net-\nworks to learn essay representations and con-\nstrain calculated scores with regression loss or\nranking loss, respectively. Since shallow neu-\nral networks trained on limited samples show\npoor performance to capture deep semantic of\ntexts. And without an accurate scoring func-\ntion, ranking loss and regression loss mea-\nsures two different aspects of the calculated\nscores. To improve AES’s performance, we\nﬁnd a new way to ﬁne-tune pre-trained lan-\nguage models with multiple losses of the same\ntask. In this paper, we propose to utilize a pre-\ntrained language model to learn text represen-\ntations ﬁrst. With scores calculated from the\nrepresentations, mean square error loss and the\nbatch-wise ListNet loss with dynamic weights\nconstrain the scores simultaneously. We uti-\nlize Quadratic Weighted Kappa to evaluate our\nmodel on the Automated Student Assessment\nPrize dataset. Our model outperforms not only\nstate-of-the-art neural models near 3 percent\nbut also the latest statistic model. Especially\non the two narrative prompts, our model per-\nforms much better than all other state-of-the-\nart models.\n1 Introduction\nAutomated Essay Scoring (AES) automatically\nevaluates the writing quality of essays. Essay as-\nsignments evaluation costs lots of time. Besides,\nthe same instructor scoring the same essay at dif-\nferent times may assign different scores (intra-rater\nvariation), different raters scoring the same essay\nmay assign different scores (inter-rater variation)\n(Smolentzov, 2013). To alleviate teachers’ bur-\nden and avoid intra-rater variation, as well as inter-\nrater variation, AES is necessary and essential. An\nearly AES system, e-rater (Chodorow and Burstein,\n2004), has been used to score TOEFL writings.\nRecently, large pre-trained language models,\nsuch as GPT (Radford et al., 2018), BERT (Devlin\net al., 2019), XLNet (Yang et al., 2019), etc. have\nshown the extraordinary ability of representation\nand generalization. These models have gained bet-\nter performance in lots of downstream tasks such as\ntext classiﬁcation and regression. There are many\nnew approaches to ﬁne-tune pre-trained language\nmodels. Sun et al. (2019a) proposed to construct\nan auxiliary sentence to solve aspect-based senti-\nment classiﬁcation tasks. Cohan et al. (2019) added\nextra separate tokens to obtain representations of\neach sentence to solve sequential sentence classiﬁ-\ncation tasks. Sun et al. (2019b) summarized several\nﬁne-tuning methods, including fusing text represen-\ntations from different layers, utilizing multi-task\nlearning, etc. To our knowledge, there are no exist-\ning works to improve AES tasks with pre-trained\nlanguage models. Before introducing our new way\nto use pre-trained language models, we brieﬂy re-\nview existing works in AES ﬁrstly.\nExisting works utilize different methods to learn\ntext representations and constrain scores, which are\nthe two key steps in AES models. For text represen-\ntation learning, various neural networks are used\nto learn essay representations, such as Recurrent\nNeural Network (RNN) (Taghipour and Ng, 2016;\nTay et al., 2018), Convolutional Neural Network\n(CNN) (Taghipour and Ng, 2016), Recurrent Con-\nvolutional Neural Network (RCNN) (Dong et al.,\n2017), etc. However, simple neural networks like\nRNN and CNN focus on word-level information,\nwhich is difﬁcult to capture word connections in\n1561\nlong-distance dependency. Besides, shallow neu-\nral networks trained on a small volume of labeled\ndata are hard to learn deep semantics. As for score\nconstraints, prediction and ranking are two popular\nsolutions. From the prediction perspective, the task\nis a regression or classiﬁcation problem (Taghipour\nand Ng, 2016; T a ye ta l ., 2018; Dong et al., 2017).\nBesides, from the recommendation perspective,\nlearning-to-rank methods ( Yannakoudakis et al.,\n2011; Chen and He, 2013) aim to rank all essays\nin the same order as that ranked by gold scores.\nHowever, without precise score mapping functions,\nonly regression constraints could not ensure the\nright ranking order. And only ranking based mod-\nels could not guarantee accurate scores. In general,\nthere are two key challenges for the AES task. One\nis how to learn better essay representations to eval-\nuate the writing quality, the other one is how to\nlearn a more accurate score mapping function.\nMotivated by the great success of pre-trained lan-\nguage models such as BERT in learning text repre-\nsentations with deep semantics, it is reasonable to\nutilize BERT to learn essay representations. Since\nself-attention is a key component of the BERT\nmodel, it can capture the interactions between any\ntwo words in the whole essays (long texts). Pre-\nvious work (Sun et al., 2019b) shows that fusing\ntext representations from different layers does not\nimprove the performance effectively. For the AES\ntask, the length of essays approximates the length\nlimit of the BERT model, so it is hard to construct\nan auxiliary sentence. Meanwhile, only score la-\nbels are available; it is also difﬁcult to utilize multi-\ntask learning. Summarized existing works in AES,\nthey utilize regression loss or ranking loss, respec-\ntively. Regression loss requires to obtain accurate\nscore value, and ranking loss aims to get precise\nscore order. Unlike multi-task learning requires dif-\nferent fully-connected networks for different tasks,\nwe propose to constrain the same task with multiple\nlosses to ﬁne-tune the BERT model. In addition, it\nis impossible to rank all essays in one batch so that\nthe model is required to learn more accurate scores.\nDuring training, the weight of the regression loss is\nincreasing while that of ranking loss is decreasing.\nIn this paper, we propose R 2BERT (BERT\nModel with Regression and Ranking). In our\nmodel, BERT is used to learn text representa-\ntions to capture deep semantics. Then a fully con-\nnected neural network is used to map the repre-\nsentations to scores. Finally, regression loss and\nbatch-wise ranking loss constrain the scores to-\ngether, which are jointly optimized with dynamic\ncombination weights. To evaluate our model,\nan open dataset, Automated Student Assessment\nPrize (ASAP), is used. With the measurement of\nQuadratic Weighted Kappa (QWK), our model out-\nperforms state-of-the-art neural models on average\nQWK score of all eight prompts near 3 percent and\nalso performs better than the latest statistical model.\nEspecially on the two narrative Prompts (7 and 8),\nonly the regression based model performs compara-\nbly even better compared with other models. And\nour model with combined loss gains much better\nperformance. To explain the model’s effectiveness,\nwe also illustrate the attention weights on two ex-\nample essays (an argumentative essay and a nar-\nrative essay). The self-attention can capture most\nconjunction words that reveal the logical structure,\nand most key concepts that show the topic shifting\nof the narratives.\nIn summary, our contributions are:\n• We propose a new method called multi-loss to\nﬁne-tune BERT models in AES tasks. We are\nalso the ﬁrst one to combine regression and\nranking in these tasks. The experiment results\nshow that the combined loss could improve\nthe performance signiﬁcantly.\n• Experiment results also show that our model\nachieves the best average QWK score and out-\nperforms other state-of-the-art neural models\nalmost on each prompt.\n• To show the effectiveness of self-attention in\nthe BERT model, we illustrate the weights\nof different words on two examples, includ-\ning one argumentative essay and one narrative\nessay.\n2 Related Works\nKe and Ng (2019) summarized recent works on au-\ntomated essay scoring. In general, there are three\nparts to solve the AES task, namely text represen-\ntation learning, score mapping function, and score\nconstraints. Almost all works utilize a linear com-\nbination function to map each text representation\nto a score. In the rest, we introduce various score\nconstraints with used approaches for text represen-\ntation learning.\nAccording to different score constraints, existing\nworks fall into three categories, namely prediction,\n1562\nrecommendation, and reinforcement learning based\nmodels.\nPrediction is the most general approach, includ-\ning classiﬁcation and regression. For classiﬁca-\ntion, the models directly predict labels that point\nto different scores. In comparison, regression mod-\nels constrain calculated scores to be the same as\ngold ones. Generally, hand-crafted features and\nneural network based features are two popular\nmethods to learn text representations. Early works\nmainly focus on the construction of hand-crafted\nfeatures such as statistical features and linguistic\nfeatures. There are several early AES systems in-\ncluding e-rater ( Chodorow and Burstein , 2004),\nPEG (Project Essay Grade) (Shermis and Burstein,\n2003), and IntelliMetric (Elliot, 2003). e-rater uti-\nlized ten linguistic features, including eight repre-\nsenting aspects of writing quality and two repre-\nsenting content. PEG used a larger feature set with\nmore than 30 elements of writing quality. Intelli-\nMetric aggregated all the features into ﬁve types,\nnamely Focus/Coherence, Organization, Elabora-\ntion/Development, Sentence Structure, and Me-\nchanics/Conventions. Cozma et al. (2018) com-\nbined string kernel and word embeddings to extract\nfeatures. With the success of deep learning, re-\nsearchers start to utilize various neural networks\nto learn text representations. Taghipour and Ng\n(2016) explored several neural networks, such as\nLong Short-Term Memory (LSTM) and CNN. Fi-\nnally, they found that the ensemble model combin-\ning LSTM and CNN performs best. Dong et al.\n(2017) proposed a hierarchical text model that uti-\nlized CNN to learn sentence representations, and\nLSTM was used to learn text representations. Tay\net al. (2018) introduced a model called SKIPFLOW,\nwhich aimed to capture neural coherence features\nof the text via considering the adjacent hidden\nstates in the LSTM model.\nIn the recommendation view, learning to rank\napproaches is another popular method to solve this\ntask. Yannakoudakis et al. (2011) ﬁrstly addressed\nthis problem as a rank preference problem. Based\non statistical features, RankSVM, a pairwise learn-\ning to rank model, was used as score constraint.\nChen and He (2013) utilized listwise learning to\nrank model to learn a ranking model based on sev-\neral linguistic features.\nReinforcement learning based models are also\npossible solutions. Wang et al. (2018b) uti-\nlized dilated LSTM to learn text representations.\nThen scores calculation was guided by quadratic\nweighted kappa based reward function.\nFor text representation, previous works only con-\nsider the relations among sentences. In this paper,\nwe focus on all the interactions between any two\nwords. Besides, existing works only utilize regres-\nsion or ranking loss, respectively. We combine two\nlosses dynamically in our model.\n3R 2BERT\nIn this section, we ﬁrst introduce the framework\nof our model, brieﬂy review the BERT model, as\nwell as self-attention. In addition, we will illustrate\nthe regression model as well as some useful tricks.\nFinally, we will show batch-wise learning to rank\nmodel and the combination metric.\nOur model, as shown in Figure 1, takes a batch\nof essays as input. With preprocessing (adding a\nspecial token, [CLS], at the beginning of each es-\nsay), each token is transformed into its embedding\nand sent into the BERT model. The representa-\ntions of all essays are the output vectors mapping\nto [CLS]. Essay scores could be obtained by passing\nthe representations into the Score Mapping Func-\ntion. They are constrained by regression loss and\nranking loss, which are optimized jointly with the\ndynamic combination. As shown in the color bar,\nthe weight of regression loss is gradually increas-\ning, while that of ranking loss is decreasing.\nE[CLS] E1 E2 EN\nC T1 T2 … TN\n[CLS] Tok 1 Tok 2 Tok N\nSentence 1\n瀖\nScore Mapping \nFunction\nRegression Ranking澾\nBERT\nFigure 1: R2BERT Framework\n3.1 BERT\nBERT ( Devlin et al. , 2019) refers to Bidirec-\ntional Encoder Representations from Transform-\ners, which is one of the most popular models in\nrecent years. More speciﬁcally, BERT is an ex-\ntremely large pre-trained language model, which\n1563\nwas trained on enormous corpora, totally more\nthan 3000M words. Meanwhile, two target tasks,\nnamely masked language model and next sentence\nprediction, are used to train the model. Many down-\nstream tasks of natural language processing have\ngained beneﬁts by utilizing pre-trained BERT to\nget text representation such as sentence classiﬁca-\ntion, question answer, common sense inference, etc.\nTo beneﬁt regression problems, the target task is\nreplaced by a fully connected neural network. Then\nthe whole BERT model is ﬁne-tuned on the new\ndataset.\nGenerally BERT has two parameter intensive\nsettings:\nBERTBASE: 12 layers, 768 hidden dimensions\nand 12 attention heads (in transformer) with the\ntotal number of parameters, 110M;\nBERTLARGE: 24 layers, 1024 hidden dimensions\nand 16 attention heads (in transformer) with the\ntotal number of parameters, 340M.\n3.2 Self-attention\nSelf-attention (Vaswani et al., 2017)i st h ek e yt o\nthe success of BERT, which is a mechanism that\na sequence calculates the word weights with itself.\nGiven a text, we construct a matrix W with three\ncopies Q, K, V, referring to query, key, and value,\nin which each column is the word embedding. The\nnew words’ representations are calculated via the\nattention as shown in Formula 1, and Formula 2,\nwhere d is the size of word embedding, nQ, nK\nand nV denote the number of words in each text,\nQ[i]is the ith word representation in the query text\nQ.\nAtt(Q,K)=[ softmax(Q[i]·KT\n√\nd\n)]nQ−1\ni=0 (1)\nVatt(Q,K,V )= Att(Q,K)·V ∈ RnQ×d (2)\n3.3 Feature Extraction\nGiven a sample essay t = {w1,w2,..,w N} as\ninput, where N is the number of the words,\nwe preprocess it to a new sequence t′ =\n{[CLS],w1,w2,..,w N}, where [CLS] is a special\ntoken. Assuming BERT(·) is the pre-trained BERT\nmodel, we can obtain the hidden representations\nof all the input words, h = BERT(t′) ∈ Rrh∗|t′|,\nwhere |t′| is the length of the input sequence and\nrh is the dimension of the hidden state. Finally, the\nhidden representation mapping to[CLS], r = h[CLS],\nis used as the text representation.\n3.4 Regression\nWith obtained text representation r, a fully con-\nnected neural network FCNN(·)is used as the score\nmapping function. More speciﬁcally, FCNN is a lin-\near combination function, where W is the weight\nmatrix and b is the bias as shown in Formula 3.\nTo learn better parameters, the mean score of all\ntraining essays is used to initialize the bias b.I n\naddition, σ = Sigmoid(·), a non-linear activation\nfunction is used to normalize the calculated score\ninto [0,1] as shown in Formula 4.\nFCNN(r)= W r +b (3)\ns′= σ(FCNN(r)) (4)\nMean square error is a widely used loss func-\ntion for regression tasks. Given a dataset D =\n{(ti,si)|i ∈ [1 :m]}, mis the number of samples,\nand ti refers to the ith essay. Besides, si is the gold\nscore of the ith essay. The regression objective Lm\nis shown in Formula 5.\nLm = MSE(s,s′)= 1\nm\nm∑\ni=1\n(si −s′\ni)2 (5)\n3.5 Batchwise Learning to Rank Model\nListNet (Cao et al., 2007) ranks a list of objectives\neach time and measures the accordance between the\npredicted ranking list and the ground truth label. In\nour problem, all the essays are a large list. However,\nit is impossible to rank all the essays in one batch.\nWe sacriﬁce the accuracy and only rank essays in\neach batch, which we called batch-wise ListNet.\nBefore introducing the objective of ListNet, we\nwill give several basic deﬁnitions. Suppose that\ngiven a set of essays which are identiﬁed with the\nnumbers {1,2,...,m }. A permutation π on the es-\nsays is deﬁned as a bijection from {1,2,...,m }\nto itself. The permutation is written as π =<\nπ(1),π(2),...,π (m) >, where π(i) refers to the\nessay at position i in the permutation. And we also\nassume any permutation is possible. The set of all\npossible permutations is denoted as Ωm. As afore-\nmentioned, we assume the batch size is m, and\nthe calculated score of the essay pointed by π(i)\nis s′\nπ(i). As given by the original paper (Cao et al.,\n2007), the permutation probability is deﬁned as\nFormula 6. And Φ(·) is an increasing and strictly\npositive function.\nPs′ (π)=\nn∏\nj=1\nΦ(s′\nπ(j))\n∑n\nk=j Φ(s′\nπ(k)) (6)\n1564\nThe top one probability Ps′ (j) is deﬁned as For-\nmula 7, where j refers to each essay in the batch.\nPs′ (j)= Φ(s′\nj)\n∑n\nk=1 Φ(s′\nk) (7)\nWith the use of top one probability, given two lists\nof scores sand s′as aforementioned, Cross Entropy\ncould be used to represent the distance (batchwise\nloss function Lr) between the two score lists as\nshown in Formula 8.\nLr = CE(s,s′)= −\nn∑\nj=1\nPs(j)log(Ps′ (j)) (8)\n3.6 Combination of Regression and Ranking\nThe key problem of loss combination is to deter-\nmine the weight of each loss. In the scoring sce-\nnario, teachers always prefer to score each essay\nrather than ranking all the essays. Besides, using\nbatch-wise learning to rank approach could not\nguarantee precise global order. Referring to the\ncombination method proposed in (Wu et al., 2009),\nthe weight of ranking loss is decreasing, and that of\nregression loss is increasing during training. The\nweight calculation is followed by Formula9, where\nτe is a σ function about ecalculated as Formula 10.\nL = τe ×Lm +(1 −τe)×Lr (9)\nτe = 1\n1+exp( γ(E/2−e)) (10)\nIn Formula 10, E is the total number of the\nepochs, and e is the value of current epoch, γ\nis a hyper-parameter which is chosen such that\nτ1 =0 .000001.\n4 Experiment\nIn this section, the ASAP dataset is introduced\nﬁrstly. Then we illustrate experiment settings and\nevaluation metrics. In addition, baseline models,\nexperiment results, and analyses are shown. Fur-\nthermore, we also visualize the attention weights\nof different words in two examples.\n4.1 Dataset\nThe automated Student Assessment Prize dataset\ncomes from a Kaggle competition 1, which con-\ntained eight essay prompts with different genres,\nincluding argumentative essays, response essays,\nand narrative essays. Each essay was given a score\nby the instructors. Some statistical information is\nshown in Table 1.\n1https://www.kaggle.com/c/asap-aes/data\nSet #Essays Genre Avg Len. Range\n1 1783 ARG 350 2-12\n2 1800 ARG 350 1-6\n3 1726 RES 150 0-3\n4 1772 RES 150 0-3\n5 1805 RES 150 0-4\n6 1800 RES 150 0-4\n7 1569 NAR 250 0-30\n8 723 NAR 650 0-60\nTable 1: Statistics of the ASAP dataset; Range means\nthe score range, For genre, ARG, RES, and NAR map\nto argumentative essays, response essays and narrative\nessays respectively.\n4.2 Experiment Settings\nFollowing previous work, we also utilize 5-\nfold cross-validation to evaluate all models with\n60/20/20 split for train, validation, and test sets,\nwhich are provided by (Taghipour and Ng, 2016).\nWe implement our model based on the Pytorch\nimplementation of BERT 2 and use the BERTBASE\nmodel due to the limit of GPU memory. Besides,\nwe truncate all the essays with the max length of\n512 words, following the setting of BERT. Also,\nfor the limit of our GPU memory, the batch size is\nset to 16. Since essays in the ASAP dataset is much\nlonger than that in GLUE (Wang et al., 2018a), we\nﬁne-tune our model for 30 epochs and select the\nbest model based on the performance on the vali-\ndation set. We adjust the ﬁne-tuning learning rate\nfrom 1e-5 to 9e-5 with the step 1e-5, and 4e-5 per-\nforms best. And γ in Formula 10 is set to 0.99999.\nFor tokenization and vocabulary, we all use the pre-\nprocessing tools provided by the BERT model. We\nalso normalize all score ranges to within [0,1]. All\nthe scores are rescaled back to the original prompt-\nspeciﬁc scale for calculating Quadratic Weighted\nKappa scores. Following previous works, we con-\nduct the evaluation in prompt-speciﬁc fashion.\n4.3 Evaluation Metric\nFollowing previous works, Quadratic Weighted\nKappa (QWK) is used as the evaluation metric,\nwhich measures the agreement between calculated\nscores and gold ones.\nTo calculate QWK, a weight matrix W is con-\nstructed ﬁrstly, as shown in Formula 11, where i\nand j are gold scores and calculated scores respec-\n2https://github.com/huggingface/pytorch-transformers\n1565\nDataset/Prompts\nID Model 1 2 3 45678 A v erage\n1 LSTM(last) 0.165 0.215 0.231 0.436 0.381 0.299 0.323 0.149 0.275\n2 BiLSTM(last) 0.226 0.276 0.239 0.502 0.375 0.412 0.361 0.188 0.322\n3 LSTM(mean) 0.582 0.517 0.516 0.702 0.604 0.670 0.661 0.566 0.602\n4 BiLSTM(mean) 0.591 0.491 0.498 0.702 0.643 0.692 0.683 0.563 0.608\n5 *EASE(SVR) 0.781 0.630 0.621 0.749 0.782 0.771 0.727 0.534 0.699\n6 *EASE(BLRR) 0.761 0.621 0.606 0.742 0.784 0.775 0.730 0.617 0.705\n7 CNN+LSTM 0.821 0.688 0.694 0.805 0.807 0.819 0.808 0.644 0.761\n8 LSTM-CNN-att 0.822 0.682 0.672 0.814 0.803 0.811 0.801 0.705 0.764\n9 RL1 0.766 0.659 0.688 0.778 0.805 0.791 0.760 0.545 0.724\n10 SKIPFlOW 0.832 0.684 0.695 0.788 0.815 0.810 0.800 0.697 0.764\n11 *HISK+BOSWE 0.845 0.729 0.684 0.829 0.833 0.830 0.804 0.729 0.785\n12 RankingOnly 0.791 0.687 0.665 0.811 0.797 0.821 0.821 0.651 0.756\n13 RegressionOnly 0.800 0.679 0.679 0.822 0.803 0.797 0.837 0.725 0.768\n14 R\n2BERT 0.817 0.719 0.698 0.845 0.841 0.847 0.839 0.744 0.794\nTable 2: QWK evaluation scores on ASAP dataset (* means statistical model)\ntively, and N is the number of possible ratings.\nWi,j = (i−j)2\n(N −1)2 (11)\nIn addition, a matrix O is calculated, such that Oi,j\ndenotes the number of essays obtained a rating iby\nthe human annotator and a rating j by the AES sys-\ntem. Another matrix E with the expected count is\ncalculated as the outer product of histogram vectors\nof the two ratings. The matrix\nE is then normalized\nsuch that the sum of elements in E is the same as\nthat of elements in O. Finally, with given matrices\nO and E, the QWK score is calculated according\nto Formula 12.\nκ =1 −\n∑\ni,j Wi,jOi,j\n∑\ni,j Wi,jEi,j\n(12)\n4.4 Baselines and Implementation Details\nIn this section, we list several baseline models as\nwell as state-of-the-art models.\n• *EASE A statistical model called Enhanced\nAI Scoring Engine (EASE) is an AES system\nthat is publicly available, open-source3, and\nalso achieved excellent results in the ASAP\ncompetition. EASE utilizes hand-crafted fea-\ntures such as length-based features, POS tags,\nand word overlap, as well as different regres-\nsion techniques. Following previous works,\n3https://github.com/edx/ease\nwe report the results of EASE with the set-\ntings of Support Vector Regression (SVR) and\nBayesian Linear Ridge Regression (BLRR).\n• LSTM We use two layers LSTM and biL-\nSTM, as well as mean pooling and last out-\nput to obtain the essay representations. Mean\npooling means the average vector of all the\nhidden states, while the last output refers to\nthe last hidden state. Then, a fully connected\nlinear layer, as well as σ activation function,\nis used to gain scores. In these four models,\nGloVe (Pennington et al., 2014) is used to ini-\ntialize the word embedding matrix, and the\ndimension is 300.\n• CNN+LSTM This model is proposed in\nTaghipour and Ng (2016), which assembled\nCNN and LSTM to gain scores. We use the\nperformance reported in the paper.\n• LSTM-CNN-att Dong et al. (2017) proposed\nto use attention mechanisms and hierarchical\nneural networks to learn the representation of\nthe essays. We also use the experiment results\nreported in their paper.\n• RL1 Wang et al. (2018b) proposed a rein-\nforcement learning based model. In that pa-\nper, QWK is used as the reward function, and\nclassiﬁcation is used to gain the scores. The\nperformance reported in the paper is used.\n• SKIPFlOW T a ye ta l .(2018) proposed the\n1566\nID Model First 512 Last 512\n1 RankingOnly 0.657 0.644\n2 RegressionOnly 0.724 0.725\n3R\n2BERT 0.743 0.745\nTable 3: QWK evaluation scores on Prompt 8 of ASAP\nDataset with different parts of the whole essays\nmodel, which considered the coherence when\nlearning text representations. Experiment re-\nsults in the paper are used.\n• *HISK+BOSWE Cozma et al. (2018) pro-\nposed another statistical model. It utilized\nstring kernel and word embedding to extract\ntext features and gained higher performance.\nOur models We not only show the performance\nof R2BERT but also the results of the regression\nonly version (RegressionOnly) and the ranking\nonly version (RankingOnly). All experiments are\nconducted on a Linux machine running a single\nTesla P40 GPU.\n4.5 Experiment Results and Analysis\nTable2 shows the empirical results of all deep learn-\ning models as well as the statistical models. First,\nthe comparison between LSTM based models is\ndiscussed. The mean pooling performs better than\nthe last output in all LSTM based models. Since\nessays in the dataset contain hundreds of words,\nit is difﬁcult for LSTM to capture longer depen-\ndency. Compared with the last output, average\npooling could alleviate the aforementioned prob-\nlem. Meanwhile, bidirectional LSTM based mod-\nels perform comparably even better than the unidi-\nrectional ones. Because the bidirectional models\ncould capture complete context information. How-\never, these models show lower performance than\nthat of EASE. It means well-designed hand-crafted\nfeatures are more effective than simple neural net-\nworks. These models still perform worse than state-\nof-the-art models.\nAdditionally, we ﬁrstly compare published state-\nof-the-art results. RL1 ( Wang et al., 2018b), the\nreinforcement learning based model, shows pretty\nlower performance in recent works. Since it uti-\nlizes dilated LSTM to learn essay representations,\nwhich ignores sentence-level structure informa-\ntion. It still outperforms basic LSTM based mod-\nels, which shows the effectiveness of the QWK\nreward function. CNN+LSTM (Taghipour and Ng,\n2016) is an ensemble model that shows compara-\nble performance compared with LSTM-CNN-att\n(Dong et al. , 2017), the hierarchical model, on\nPrompt 1,2,4,5,6,7, and even gains much higher\nperformance on Prompt 3. Both models outper-\nform LSTM based models. It means that the en-\nsemble model could make up shortages of single\nneural networks and performs comparably with hi-\nerarchical models. Besides, LSTM-CNN-att and\nSKIPFLOW (T a ye ta l ., 2018) both are hierarchi-\ncal models. They capture the explicit structure\nthrough modeling the relationship of adjacent sen-\ntences (semantics) in each essay. So they perform\nbetter in Prompt 1 and 2, which contain argumen-\ntative essays. Especially the SKIPFLOW model\neven gains much better performance on Prompt 1.\nLSTM-CNN-att also performs better on Prompt\n8. However, a well-designed statistical model,\nHISK+BOSWE Cozma et al. (2018), outperforms\nall previous neural models, which also performs\nbest on the two argumentative prompts.\nCompared with previous state-of-the-art neural\nmodels, the RegressionOnly model outperforms all\nother neural models on the average QWK score,\nwhich shows the great power of the pre-trained\nlanguage model (BERT) in capturing deeply se-\nmantic information. Especially on the two narra-\ntive prompts (Prompt 7 and 8), the RegressionOnly\nmodel outperforms other models by a large mar-\ngin, which shows that self-attention is more suit-\nable for narrative essays since it can capture key\nconcepts in narrative essays as shown in Figure\n2. RankingOnly model shows much lower perfor-\nmance on Prompt 8 as well as average QWK score,\nmaybe because it is difﬁcult to utilize batch-wise\norder to reconstruct the global order perfectly.\nR2BERT outperforms RegressionOnly and\nRankingOnly models on each prompt by a large\nmargin except Prompt 7. The result means that\nranking and regression are surely two complemen-\ntary objectives, and a combination via dynamic\nweights could improve the performance effectively.\nIn general, R2BERT gains a much higher average\nQWK score compared with the aforementioned\nneural models and almost performs best on each\nprompt except Prompt 1. It illustrates a success-\nful way to enhance BERT on downstream tasks.\nOnly utilizing BERT to learn text representations\nis not enough. Suitable auxiliary objectives are\nalso necessary. More importantly, our model also\noutperforms HISK+BOSWE, the latest statistical\n1567\nPrompt ID Prompt 1 Prompt 7\nPrompt Write a letter to your local newspaper in which you \nstate your opinion on the effects computers have on \npeople. Persuade the readers to agree with you.\nWrite a story about a time when you were patient or\nwrite a story about a time when someone you know was \npatient or write a story in your own way about patience.\nAttention \nExample\ndear newspaper, computers have a positive effect on \npeople because they teach hand\n-eye coordination, give \npeople the ability to learn about faraway places and \npeople and allow people to talk online with other \npeople. the invention of computers is the single most \nimportant event of the @date1. @person1, a professor \nat @organization3 says that \"the invention of \ncomputers has led to hundreds even thousands of new \ndiscoveries. this week alone, @caps3 have discovered \n@num1 new drugs that could put an end to cancer.\"\nhave you ever been in a situation when you know \nsomething good is coming or is going to happen and you \njust @caps1t control yourself? you ask your parents, \nwhen and they say, soon just have some patience! well \nthis has happened to me multiple times, such as when \nwe were going to @location1 or @location2, but on this \nspecial occasion, getting our new dog. i decided to be a \nmature teenager and be patient. it was @date1 @time1, \nthe day my family was getting a dog and iw a s  s o  \nexcited. my stomach was filled with butterflies…\nsituation\ngood coming \ncontrol parents,\npatience well\nto \nour new dog.\nstomach \ndecided \nteenager patient\nfamily dog\nbutterflies\npeople because teach \nabout faraway places \nand people online other \npeople invention of the \nof the @date1\nthat invention \neven \nan cancer.\nwe\nFigure 2: Self-attention visualization on examples of Prompt 1 and 7\nmodel, which proves the great power of neural net-\nworks.\nBERT limits the length of each input text with a\nmaximum of 512 words. In Prompt 8, the average\nlength of all essays is about 650 words, which is\nlarger than the limit. We use the ﬁrst 512 words or\nthe last 512 words instead of the whole essay. Table\n3 shows the experimental results. Our three models\nachieve similar performance. How to fully use the\nwhole essays with BERT is a direction in future\nworks. In Table 2, we use the average performance\nas the result of Prompt 8 in each model.\nIn Figure 2, we visualize the word weights of\nself-attention of two essays, including an argu-\nmentative essay from Prompt 1 and a narrative\nessay from Prompt 7. For the limit of the page,\nwe only demonstrate part of each essay. In the\nﬁgure, the word in darker red gains lower atten-\ntion weight. The argumentative example needs\nto convince people that computers can beneﬁt our\nlife. Self-attention has identiﬁed several connectors\nsuch as ”because”, ”and”, ”even”, and some words\nindicating arguments including ”about”, ”that” etc.\nThese words show the explicit logical structure\nof argumentative essays. The narrative example\nuses the example of getting a dog to show his/her\npatience. Self-attention capture the story details\nsuch as ”dog”, ”parent”, ”family”, ”stomach”, ”but-\nterﬂies”, as well as the topic words ”patient” and\n”patience”. All these words show the topics shifting\nof narratives.\n4.6 Runtime and Memory\nIn this section, we analyze the runtime and memory,\nwhich means the total number of parameters. Since\nlittle previous work provided the source code so\nthat it is difﬁcult to estimate the total number of pa-\nrameters accurately. Our three models only utilize\nModel TR IPS #Param\nLSTM 2m53s 0.0013s 1.4M\nBiLSTM 3m15s 0.0014s 1.4M\nR2BERT 22m20s 0.9103s 110M\nTable 4: Comparison of Runtime and Memory. TR\nmeans the total training runtime on the train set and IPS\nmeans inference runtime per each test sample. #Param\nrefers to the number of parameters.\ndifferent losses, so they have the same number of\nparameters. In summary, we only compare LSTM,\nBiLSTM, and R2BERT model. Firstly, we estimate\nthe total number of parameters for the three mod-\nels. Then we record the total training time on all\ntraining samples in Prompt 6. Since simple neural\nnetworks need more training epochs to converge,\nyet BERT model only needs less training epochs\nto ﬁne-tune. To compare the inference time, we\nrecord the time for inference per sample. All results\nare shown in table 4. It is obvious that BERT has\nmore parameters and spends much more training\nand inference time. However, the inference time of\neach sample is near 1 second, which is practical in\nthe real educational scenarios.\n5 Conclusion and Future works\nFrom experimental results, we can obtain several\nconclusions: 1) BERT is a signiﬁcantly effective\nmodel to improve the performance of downstream\nnatural language processing tasks. 2) Regression\nloss and ranking loss are two complementary losses.\n3) Simply ﬁne-tuning on BERT is not enough.\nMulti-loss objective is an effective approach to ﬁne-\ntune the BERT model. 4) Self-attention is useful\nto capture conjunction words and key concepts in\nessays. In the future, we will investigate how to uti-\n1568\nlize the whole long text with the pre-trained BERT\nmodel.\nAcknowledgments\nThe work was supported by Hong Kong RGC Col-\nlaborative Research Fund with project code C6030-\n18GF and Hong Kong Red Swastika Society Tai\nPo Secondary School with project code P20-0021.\nReferences\nZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and\nHang Li. 2007. Learning to rank: from pairwise ap-\nproach to listwise approach. In Proceedings of the\n24th international conference on Machine learning,\npages 129–136. ACM.\nHongbo Chen and Ben He. 2013. Automated essay\nscoring by maximizing human-machine agreement .\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1741–1752, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nMartin Chodorow and Jill Burstein. 2004. Beyond\nessay length: evaluating e-rater R⃝ ’s performance\non toeﬂ R⃝ essays. ETS Research Report Series ,\n2004(1):i–38.\nArman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi,\nand Daniel S Weld. 2019. Pretrained language mod-\nels for sequential sentence classiﬁcation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3684–3690.\nM˘ad˘alina Cozma, Andrei Butnaru, and Radu Tudor\nIonescu. 2018. Automated essay scoring with string\nkernels and word embeddings .I n Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (V olume 2: Short Papers) ,\npages 503–509, Melbourne, Australia. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.I n Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nT echnologies, V olume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nFei Dong, Yue Zhang, and Jie Yang. 2017. Attention-\nbased recurrent convolutional neural network for au-\ntomatic essay scoring .I n Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 153–162, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nScott Elliot. 2003. Intellimetric: From here to valid-\nity. Automated essay scoring: A cross-disciplinary\nperspective, pages 71–86.\nZixuan Ke and Vincent Ng. 2019. Automated essay\nscoring: a survey of the state of the art. In Pro-\nceedings of the 28th International Joint Conference\non Artiﬁcial Intelligence, pages 6300–6308. AAAI\nPress.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper . pdf.\nMark D Shermis and Jill C Burstein. 2003. Auto-\nmated essay scoring: A cross-disciplinary perspec-\ntive. Routledge.\nAndr´e Smolentzov. 2013. Automated essay scoring:\nScoring essays in swedish.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019a. Utiliz-\ning bert for aspect-based sentiment analysis via con-\nstructing auxiliary sentence. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language T echnologies, V olume 1 (Long and\nShort Papers), pages 380–385.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019b. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nKaveh Taghipour and Hwee Tou Ng. 2016. A neural\napproach to automated essay scoring .I n Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 1882–1891,\nAustin, Texas. Association for Computational Lin-\nguistics.\nYi Tay, Minh C. Phan, Luu Anh Tuan, and Siu Che-\nung Hui. 2018. Skipﬂow: Incorporating neural co-\nherence features for end-to-end automatic text scor-\ning.I n Proceedings of the Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, (AAAI-18), the\n30th innovative Applications of Artiﬁcial Intelli-\ngence, pages 5948–5955.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\n1569\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nYucheng Wang, Zhongyu Wei, Yaqian Zhou, and Xuan-\njing Huang. 2018b. Automatic essay scoring incor-\nporating rating schema via reinforcement learning .\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n791–797, Brussels, Belgium. Association for Com-\nputational Linguistics.\nMingrui Wu, Yi Chang, Zhaohui Zheng, and Hongyuan\nZha. 2009. Smoothing dcg for learning to rank: A\nnovel approach using smoothed hinge functions. In\nProceedings of the 18th ACM conference on Infor-\nmation and knowledge management , pages 1923–\n1926. ACM.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automatically\ngrading ESOL texts.I n Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language T echnologies, pages\n180–189, Portland, Oregon, USA. Association for\nComputational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8005441427230835
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6839897632598877
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6569259166717529
    },
    {
      "name": "Task (project management)",
      "score": 0.6437751054763794
    },
    {
      "name": "Regression",
      "score": 0.572534441947937
    },
    {
      "name": "Sentence",
      "score": 0.5500136017799377
    },
    {
      "name": "Machine learning",
      "score": 0.5495943427085876
    },
    {
      "name": "Natural language processing",
      "score": 0.48815709352493286
    },
    {
      "name": "Language model",
      "score": 0.4859987199306488
    },
    {
      "name": "Artificial neural network",
      "score": 0.4468977749347687
    },
    {
      "name": "Mean squared error",
      "score": 0.4159584045410156
    },
    {
      "name": "Statistics",
      "score": 0.17684334516525269
    },
    {
      "name": "Mathematics",
      "score": 0.09102481603622437
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I72427458",
      "name": "JDSU (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    }
  ],
  "cited_by": 95
}