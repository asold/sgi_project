{
  "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection",
  "url": "https://openalex.org/W3170778815",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2211755126",
      "name": "Fang, Yuxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294016934",
      "name": "Liao, Bencheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222247533",
      "name": "Wang, Xinggang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202042627",
      "name": "Fang, Jiemin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2772354204",
      "name": "Qi Jiyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144457390",
      "name": "Wu Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224952121",
      "name": "Niu Jian-wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2023123167",
      "name": "Liu, Wenyu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W8437397",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2991391304",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3135921327",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3039009902",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3015146382",
    "https://openalex.org/W2941956444",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3134206242",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2401231614",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3104401316",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3147352887",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3034098129",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W2102605133"
  ],
  "abstract": "Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.",
  "full_text": "You Only Look at One Sequence:\nRethinking Transformer in Vision through Object Detection\nYuxin Fang1∗ Bencheng Liao1∗ Xinggang Wang1† Jiemin Fang2,1\nJiyang Qi1 Rui Wu3 Jianwei Niu3 Wenyu Liu1\n1 School of EIC, Huazhong University of Science & Technology\n2 Institute of AI, Huazhong University of Science & Technology\n3 Horizon Robotics\n{yxf, bcliao, xgwang}@hust.edu.cn\nAbstract\nCan Transformer perform 2D object- and region-level recognition from a pure\nsequence-to-sequence perspective with minimal knowledge about the 2D spatial\nstructure? To answer this question, we present You Only Look at One Sequence\n(YOLOS), a series of object detection models based on the vanilla Vision Trans-\nformer with the fewest possible modiﬁcations, region priors, as well as induc-\ntive biases of the target task. We ﬁnd that YOLOS pre-trained on the mid-sized\nImageNet-1kdataset only can already achieve quite competitive performance on\nthe challenging COCO object detection benchmark, e.g., YOLOS-Base directly\nadopted from BERT-Base architecture can obtain42.0 box AP on COCO val. We\nalso discuss the impacts as well as limitations of current pre-train schemes and\nmodel scaling strategies for Transformer in vision through YOLOS. Code and\npre-trained models are available at https://github.com/hustvl/YOLOS.\n1 Introduction\nTransformer [59] is born to transfer. In natural language processing (NLP), the dominant approach is\nto ﬁrst pre-train Transformer on large, generic corpora for general language representation learning,\nand then ﬁne-tune or adapt the model on speciﬁc target tasks [ 18]. Recently, Vision Transformer\n(ViT) 1 [21] demonstrates that canonical Transformer encoder architecture directly inherited from\nNLP can perform surprisingly well on image recognition at scale using modern vision transfer learning\nrecipe [33]. Taking sequences of image patch embeddings as inputs, ViT can successfully transfer\npre-trained general visual representations from sufﬁcient scale to more speciﬁc image classiﬁcation\ntasks with fewer data points from a pure sequence-to-sequence perspective.\nSince a pre-trained Transformer can be successfully ﬁne-tuned on sentence-level tasks [7, 19] in NLP,\nas well as token-level tasks [48, 52], where models are required to produce ﬁne-grained output at the\ntoken-level [18]. A natural question is: Can ViT transfer to more challenging object- and region-level\ntarget tasks in computer vision such as object detection other than image-level recognition?\nViT-FRCNN [6] is the ﬁrst to use a pre-trained ViT as the backbone for a Faster R-CNN [50] object\ndetector. However, this design cannot get rid of the reliance on convolutional neural networks (CNNs)\n∗Yuxin Fang and Bencheng Liao contributed equally.†Xinggang Wang is the corresponding author. This\nwork was done when Yuxin Fang was interning at Horizon Robotics mentored by Rui Wu.\n1There are various sophisticated or hybrid architectures termed as “Vision Transformer”. For disambiguation,\nin this paper, “Vision Transformer” and “ViT” refer to the canonical or vanilla Vision Transformer architecture\nproposed by Dosovitskiy et al. [21] unless speciﬁed.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.00666v3  [cs.CV]  27 Oct 2021\nand strong 2D inductive biases, as ViT-FRCNN re-interprets the output sequences of ViT to2D spatial\nfeature maps and depends on region-wise pooling operations (i.e., RoIPool [23, 25] or RoIAlign [27])\nas well as region-based CNN architectures [50] to decode ViT features for object- and region-level\nperception. Inspired by modern CNN design, some recent works [ 39, 60, 63, 66] introduce the\npyramidal feature hierarchy, spatial locality, equivariant as well as invariant representations [24] to\ncanonical Vision Transformer design, which largely boost the performance in dense prediction tasks\nincluding object detection. However, these architectures are performance-oriented and cannot reﬂect\nthe properties of the canonical or vanilla Vision Transformer [21] directly inherited from Vaswani\net al. [59]. Another series of work, the DEtection TRansformer (DETR) families [ 10, 73], use a\nrandom initialized Transformer to encode & decode CNN features for object detection, which does\nnot reveal the transferability of a pre-trained Transformer.\nIntuitively, ViT is designed to model long-range dependencies and global contextual information\ninstead of local and region-level relations. Moreover, ViT lacks hierarchical architecture as modern\nCNNs [26, 35, 53] to handle the large variations in the scale of visual entities [ 1, 37]. Based on\nthe available evidence, it is still unclear whether a pure ViT can transfer pre-trained general visual\nrepresentations from image-level recognition to the much more complicated 2D object detection task.\nTo answer this question, we present You Only Look at One Sequence (YOLOS), a series of object\ndetection models based on the canonical ViT architecture with the fewest possible modiﬁcations,\nregion priors, as well as inductive biases of the target task injected. Essentially, the change from\na pre-trained ViT to a YOLOS detector is embarrassingly simple: (1) YOLOS replaces one [CLS]\ntoken for image classiﬁcation in ViT with one hundred [DET] tokens for object detection. (2) YOLOS\nreplaces the image classiﬁcation loss in ViT with the bipartite matching loss to perform object\ndetection in a set prediction manner following Carion et al. [10], which can avoid re-interpreting the\noutput sequences of ViT to 2D feature maps as well as prevent manually injecting heuristics and prior\nknowledge of object 2D spatial structure during label assignment [72]. Moreover, the prediction head\nof YOLOS can get rid of complex and diverse designs, which is as compact as a classiﬁcation layer.\nDirectly inherited from ViT [21], YOLOS is not designed to be yet another high-performance object\ndetector, but to unveil the versatility and transferability of pre-trained canonical Transformer from\nimage recognition to the more challenging object detection task. Concretely, our main contributions\nare summarized as follows:\n• We use the mid-sized ImageNet-1k[51] as the sole pre-training dataset, and show that a\nvanilla ViT [21] can be successfully transferred to perform the complex object detection\ntask and produce competitive results on COCO [36] benchmark with the fewest possible\nmodiﬁcations, i.e., by only looking at one sequence (YOLOS).\n• For the ﬁrst time, we demonstrate that 2D object detection can be accomplished in a pure\nsequence-to-sequence manner by taking a sequence of ﬁxed-sized non-overlapping image\npatches as input. Among existing object detectors, YOLOS utilizes the minimal2D inductive\nbiases.\n• For the vanilla ViT, we ﬁnd the object detection results are quite sensitive to the pre-train\nscheme and the detection performance is far from saturating. Therefore the proposed YOLOS\ncan be also used as a challenging benchmark task to evaluate different (label-supervised and\nself-supervised) pre-training strategies for ViT.\n2 You Only Look at One Sequence\nAs for the model design, YOLOS closely follows the original ViT architecture [21], and is optimized\nfor object detection in the same vein as Carion et al. [10]. YOLOS can be easily adapted to various\ncanonical Transformer architectures available in NLP as well as in computer vision. This intentionally\nsimple setup is not designed for better detection performance, but to exactly reveal characteristics of\nthe Transformer family in object detection as unbiased as possible.\n2.1 Architecture\nAn overview of the model is depicted in Fig. 1. Essentially, the change from a ViT to a YOLOS\ndetector is simple: (1) YOLOS drops the [CLS] token for image classiﬁcation and appends one\n2\nLinear Projection of Flattened Patches\nTransformer EncoderPat-Tok#NPat-Tok#2Pat-Tok#1 Det-Tok#100Det-Tok#2Det-Tok#1\nMLPHeads\n… ……\nPat-Tok#NPat-Tok#2Pat-Tok#1 Det-Tok#100Det-Tok#2Det-Tok#1… ……Cls&BboxPredictions\nPatchesof anInput Image\n+PE+PE +PE +PE+PE +PE\nFigure 1: YOLOS architecture overview. “Pat-Tok” refers to [PATCH] token, which is the embedding of a\nﬂattened image patch. “Det-Tok” refers to [DET] token, which is a learnable embedding for object binding.\n“PE” refers to positional embedding. During training, YOLOS produces an optimal bipartite matching between\npredictions from one hundred [DET] tokens and ground truth objects. During inference, YOLOS directly outputs\nthe ﬁnal set of predictions in parallel. The ﬁgure style is inspired by Dosovitskiy et al. [21].\nhundred randomly initialized learnable detection tokens ([DET] tokens) to the input patch embeddings\n([PATCH] tokens) for object detection. (2) During training, YOLOS replaces the image classiﬁcation\nloss in ViT with the bipartite matching loss to perform object detection in a set prediction manner\nfollowing Carion et al. [10].\nStem. The canonical ViT [21] receives an 1D sequence of embedded tokens as the input. To handle\n2D image inputs, we reshape the image x ∈RH×W×C into a sequence of ﬂattened 2D image patches\nxPATCH ∈RN×(P2·C). Here, (H,W ) is the resolution of the input image, C is the number of input\nchannels, (P,P ) is the resolution of each image patch, and N = HW\nP2 is the resulting number of\npatches. Then we map xPATCH to Ddimensions with a trainable linear projectionE ∈R(P2·C)×D. We\nrefer to the output of this projection xPATCHE as [PATCH] tokens. Meanwhile, one hundred randomly\ninitialized learnable [DET] tokens xDET ∈R100×D are appended to the [PATCH] tokens. Position\nembeddings P ∈R(N+100)×D are added to all the input tokens to retain positional information. We\nuse the standard learnable 1D position embeddings following Dosovitskiy et al. [21]. The resulting\nsequence z0 serves as the input of YOLOS Transformer encoder. Formally:\nz0 =\n[\nx1\nPATCHE; ··· ; xN\nPATCHE; x1\nDET; ··· ; x100\nDET\n]\n+ P. (1)\nBody. The body of YOLOS is basically the same as ViT, which consists of a stack of Transformer\nencoder layers only [59]. [PATCH] tokens and [DET] tokens are treated equally and they perform global\ninteractions inside Transformer encoder layers.\nEach Transformer encoder layer consists of one multi-head self-attention (MSA) block and one MLP\nblock. LayerNorm (LN) [2] is applied before every block, and residual connections [26] are applied\nafter every block [3, 62]. The MLP contains one hidden layer with an intermediate GELU [29]\nnon-linearity activation function. Formally, for the ℓ-th YOLOS Transformer encoder layer:\nz′\nℓ = MSA (LN (zℓ−1)) +zℓ−1,\nzℓ = MLP (LN (z′\nℓ)) +z′\nℓ. (2)\nDetector Heads. The detector head of YOLOS gets rid of complex and heavy designs, and is as\nneat as the image classiﬁcation layer of ViT. Both the classiﬁcation and the bounding box regression\nheads are implemented by one MLP with separate parameters containing two hidden layers with\nintermediate ReLU [41] non-linearity activation functions.\n3\nDetection Token. We purposefully choose randomly initialized [DET] tokens as proxies for object\nrepresentations to avoid inductive biases of 2D structure and prior knowledge about the task injected\nduring label assignment. When ﬁne-tuning on COCO, for each forward pass, an optimal bipartite\nmatching between predictions generated by [DET] tokens and ground truth objects is established. This\nprocedure plays the same role as label assignment [10, 72], but is unaware of the input 2D structure,\ni.e., YOLOS does not need to re-interpret the output sequence of ViT to an 2D feature maps for label\nassignment. Theoretically, it is feasible for YOLOS to perform any dimensional object detection\nwithout knowing the exact spatial structure and geometry, as long as the input is always ﬂattened to a\nsequence in the same way for each pass.\nFine-tuning at Higher Resolution.When ﬁne-tuning on COCO, all the parameters are initialized\nfrom ImageNet-1kpre-trained weights except for the MLP heads for classiﬁcation & bounding box\nregression as well as one hundred [DET] tokens, which are randomly initialized. During ﬁne-tuning,\nthe image has a much higher resolution than pre-training. We keep the patch size P unchanged, i.e.,\nP×P = 16×16, which results in a larger effective sequence length. While ViT can handle arbitrary\ninput sequence lengths, the positional embeddings need to adapt to the longer input sequences with\nvarious lengths. We perform 2D interpolation of the pre-trained position embeddings on the ﬂy2.\nInductive Bias. We carefully design the YOLOS architecture for the minimal additional inductive\nbiases injection. The inductive biases inherent from ViT come from the patch extraction at the\nnetwork stem part as well as the resolution adjustment for position embeddings [21]. Apart from that,\nYOLOS adds no non-degenerated (e.g., 3 ×3 or other non 1 ×1) convolutions upon ViT 3. From the\nrepresentation learning perspective, we choose to use[DET] tokens to bind objects for ﬁnal predictions\nto avoid additional 2D inductive biases as well as task-speciﬁc heuristics. The performance-oriented\ndesign inspired by modern CNN architectures such as pyramidal feature hierarchy, 2D local spatial\nattention as well as the region-wise pooling operation is not applied. All these efforts are meant to\nexactly unveil the versatility and transferability of pre-trained Transformers from image recognition\nto object detection in a pure sequence-to-sequence manner, with minimal knowledge about the input\nspatial structure and geometry.\nComparisons with DETR. The design of YOLOS is deeply inspired by DETR [10]: YOLOS uses\n[DET] tokens following DETR as proxies for object representations to avoid inductive biases about\n2D structures and prior knowledge about the task injected during label assignment, and YOLOS is\noptimized similarly as DETR.\nMeanwhile, there are some key differences between the two models: (1) DETR adopts a Transformer\nencoder-decoder architecture, while YOLOS chooses an encoder-only Transformer architecture. (2)\nDETR only employs pre-training on its CNN backbone but leaves the Transformer encoder & decoder\nbeing trained from random initialization, while YOLOS naturally inherits representations from any\npre-trained canonical ViT. (3) DETR applies cross-attention between encoded image features and\nobject queries with auxiliary decoding losses deeply supervised at each decoder layer, while YOLOS\nalways looks at only one sequence for each encoder layer, without distinguishing [PATCH] tokens and\n[DET] tokens in terms of operations. Quantitative comparisons between the two are in Sec. 3.4.\n3 Experiments\n3.1 Setup\nPre-training. We pre-train all YOLOS / ViT models on ImageNet-1k[51] dataset using the data-\nefﬁcient training strategy suggested by Touvron et al. [58]. The parameters are initialized with a\ntruncated normal distribution and optimized using AdamW [40]. The learning rate and batch size\nare 1 ×10−3 and 1024, respectively. The learning rate decay is cosine and the weight decay is 0.05.\nRand-Augment [14] and random erasing [70] implemented by timm library [65] are used for data\naugmentation. Stochastic depth [32], Mixup [69] and Cutmix [67] are used for regularization.\n2The conﬁgurations of position embeddings are detailed in the Appendix.\n3We argue that it is imprecise to say Transformer do not have convolutions. All linear projection layers in\nTransformer are equivalent to point-wise or 1 ×1 convolutions with sparse connectivity, parameter sharing, and\nequivalent representations properties, which can largely improve the computational efﬁciency compared with the\n“all-to-all” interactions in fully-connected design that has even weaker inductive biases [5, 24].\n4\nFine-tuning. We ﬁne-tune all YOLOS models on COCO object detection benchmark [ 36] in a\nsimilar way as Carion et al. [10]. All the parameters are initialized from ImageNet-1kpre-trained\nweights except for the MLP heads for classiﬁcation & bounding box regression as well as one\nhundred [DET] tokens, which are randomly initialized. We train YOLOS on a single node with\n8 ×12G GPUs. The learning rate and batch sizes are 2.5 ×10−5 and 8 respectively. The learning\nrate decay is cosine and the weight decay is 1 ×10−4.\nAs for data augmentation, we use multi-scale augmentation, resizing the input images such that the\nshortest side is at least 256 and at most 608 pixels while the longest at most 864 for tiny models.\nFor small and base models, we resize the input images such that the shortest side is at least 480\nand at most 800 pixels while the longest at most 1333. We also apply random crop augmentations\nduring training following Carion et al. [10]. The number of [DET] tokens are 100 and we keep the loss\nfunction as well as loss weights the same as DETR, while we don’t apply dropout [54] or stochastic\ndepth during ﬁne-tuning since we ﬁnd these regularization methods hurt performance.\nModel Variants. With available computational resources, we study several YOLOS variants. De-\ntailed conﬁgurations are summarized in Tab. 1. The input patch size for all models is 16 ×16.\nYOLOS-Ti (Tiny), -S (Small), and -B (Base) directly correspond to DeiT-Ti, -S, and -B [58]. From\nthe model scaling perspective [20, 56, 61], the small and base models of YOLOS / DeiT can be seen\nas performing width scaling (w) [30, 68] on the corresponding tiny model.\nDeiT [58] Layers Embed. Dim.Model Model (Depth) (Width)\nPre-train\nResolution Heads Params. FLOPs f(Lin.)\nf(Att.)\nYOLOS-Ti DeiT-Ti\n12\n192\n224\n3 5.7 M 1.2 G 5.9\nYOLOS-S DeiT-S 384 6 22.1 M 4.5 G 11.8\nYOLOS-B DeiT-B 768 12 86.4 M 17.6 G 23.5\nYOLOS-S (dwr) – 19 240 272 6 13.7 M 4.6 G 5.0\nYOLOS-S (dwr) – 14 330 240 6 19.0 M 4.6 G 8.8\nTable 1: Variants of YOLOS. “dwr” and “dwr” refer to uniform compound model scaling and fast model\nscaling, respectively. The “ dwr” and “dwr” notations are inspired by Dollár et al. [20]. Note that all the\nnumbers listed are for pre-training, which could change during ﬁne-tuning, e.g., the resolution and FLOPs.\nBesides, we investigate two other model scaling strategies which proved to be effective in CNNs.\nThe ﬁrst one is uniform compound scaling (dwr) [20, 56]. In this case, the scaling is uniform w.r.t.\nFLOPs along all model dimensions (i.e., width (w), depth (d) and resolution (r)). The second one is\nfast scaling (dwr) [20] that encourages primarily scaling model width (w), while scaling depth (d)\nand resolution (r) to a lesser extent w.r.t. FLOPs. During the ImageNet-1kpre-training phase, we\napply dwr and dwrscaling to DeiT-Ti (∼1.2G FLOPs) and scale the model to ∼4.5G FLOPs to\nalign with the computations of DeiT-S. Larger models are left for future work.\nFor canonical CNN architectures, the model complexity or FLOPs (f) are proportional to dw2r2 [20].\nFormally, f(CNN) ∝dw2r2. Different from CNN, there are two kinds of operations that contribute to\nthe FLOPs of ViT. The ﬁrst one is the linear projection (Lin.) or point-wise convolution, which fuses\nthe information across different channels point-wisely via learnable parameters. The complexity is\nf(Lin.) ∝dw2r2, which is the same asf(CNN). The second one is the spatial attention (Att.), which\naggregates the spatial information depth-wisely via computed attention weights. The complexity is\nf(Att.) ∝dwr4, which grows quadratically with the input sequence length or number of pixels.\nNote that the available scaling strategies are designed for architectures with complexity f ∝dw2r2,\nso theoretically the dwras well as dwrmodel scaling are not directly applicable to ViT. However,\nduring pre-training phase the resolution is relatively low, therefore f(Lin.) dominates the FLOPs\n(f(Lin.)\nf(Att.) > 5). Our experiments indicate that some model scaling properties of ViT are consistent\nwith CNNs when f(Lin.)\nf(Att.) is large.\n3.2 The Effects of Pre-training\nWe study the effects of different pre-training strategies (both label-supervised and self-supervised)\nwhen transferring ViT (DeiT-Ti and DeiT-S) from ImageNet- 1k to the COCO object detection\nbenchmark via YOLOS. For object detection, the input shorter size is 512 for tiny models and is 800\nfor small models during inference. The results are shown in Tab. 2 and Tab. 3.\n5\nModel Pre-train Method Pre-train\nEpochs\nFine-tune\nEpochs\nPre-train\npFLOPs\nFine-tune\npFLOPs\nTotal\npFLOPs\nImNet\nTop-1 AP\nYOLOS-Ti\nRand. Init. 0 600 0 14.2 ×102 14.2 ×102 – 19.7\nLabel Sup. [58] 200\n300\n3.1 ×102\n7.1 ×102\n10.2 ×102 71.2 26.9\nLabel Sup. [58] 300 4.7 ×102 11.8 ×102 72.2 28.7\nLabel Sup. (C) [58] 300 4.7 ×102 11.8 ×102 74.5 29.7\nYOLOS-S\nRand. Init. 0 250 0 5.9 ×103 5.9 ×103 – 20.9\nLabel Sup. [58] 100\n150\n0.6 ×103\n3.5 ×103\n4.1 ×103 74.5 32.0\nLabel Sup. [58] 200 1.2 ×103 4.7 ×103 78.5 36.1\nLabel Sup. [58] 300 1.8 ×103 5.3 ×103 79.9 36.1\nLabel Sup. (C) [58] 300 1.8 ×103 5.3 ×103 81.2 37.2\nTable 2: The effects of label-supervised pre-training. “pFLOPs” refers to petaFLOPs (×1015). “ImNet” refers to\nImageNet-1k. “C” refers to the distillation method from Touvron et al. [58].\nModel Self Sup. Pre-train Method Pre-train Epochs Fine-tune Epochs Linear Acc. AP\nYOLOS-S MoCo-v3 [13] 300 150 73.2 33.6\nDINO [11] 800 150 77.0 36.2\nTable 3: Study of self-supervised pre-training on YOLOS-S.\nNecessity of Pre-training. At least under prevalent transfer learning paradigms [10, 58], the pre-\ntraining is necessary in terms of computational efﬁciency. For both tiny and small models, we\nﬁnd that pre-training on ImageNet-1ksaves the total theoretical forward pass computations (total\npre-training FLOPs & total ﬁne-tuning FLOPs) compared with training on COCO from random\ninitialization (training from scratch [28]). Models trained from scratch with hundreds of epochs still\nlag far behind the pre-trained ViT even if given more total FLOPs budgets. This seems quite different\nfrom canonical modern CNN-based detectors, which can catch up with pre-trained counterparts\nquickly [28].\nLabel-supervised Pre-training. For supervised pre-training with ImageNet-1kground truth labels,\nwe ﬁnd that different-sized models prefer different pre-training schedules: 200 epochs pre-training\nfor YOLOS-Ti still cannot catch up with 300 epochs pre-training even with a 300 epochs ﬁne-tuning\nschedule, while for the small model 200 epochs pre-training provides feature representations as good\nas 300 epochs pre-training for transferring to the COCO object detection benchmark.\nWith additional transformer-speciﬁc distillation (“C”) introduced by Touvron et al.[58], the detection\nperformance is further improved by∼1 AP for both tiny and small models, in part because exploiting\na CNN teacher [ 47] during pre-training helps ViT adapt to COCO better. It is also promising to\ndirectly leverage [DET] tokens to help smaller YOLOS learn from larger YOLOS on COCO during\nﬁne-tuning in a similar way as Touvron et al. [58], we leave it for future work.\nSelf-supervised Pre-training. The success of Transformer in NLP greatly beneﬁts from large-scale\nself-supervised pre-training [18, 44, 45]. In vision, pioneering works [12, 21] train self-supervised\nTransformers following the masked auto-encoding paradigm in NLP. Recent works [11, 13] based on\nsiamese networks show intriguing properties as well as excellent transferability to downstream tasks.\nHere we perform a preliminary transfer learning experiment on YOLOS-S using MoCo-v3 [13] and\nDINO [11] self-supervised pre-trained ViT weights in Tab. 3.\nThe transfer learning performance of 800 epochs DINO self-supervised model on COCO object\ndetection is on a par with 300 epochs DeiT label-supervised pre-training, suggesting great potentials\nof self-supervised pre-training for ViT on challenging object-level recognition tasks. Meanwhile,\nthe transfer learning performance of MoCo-v3 is less satisfactory, in part for the MoCo-v3 weight\nis heavily under pre-trained. Note that the pre-training epochs of MoCo-v3 are the same as DeiT\n(300 epochs), which means that there is still a gap between the current state-of-the-art self-supervised\npre-training approach and the prevalent label-supervised pre-training approach for YOLOS.\nYOLOS as a Transfer Learning Benchmark for ViT.From the above analysis, we conclude that\nthe ImageNet-1kpre-training results cannot precisely reﬂect the transfer learning performance on\nCOCO object detection. Compared with widely used image recognition transfer learning benchmarks\nsuch as CIFAR-10/100 [34], Oxford-IIIT Pets [43] and Oxford Flowers-102 [42], the performance of\n6\nYOLOS on COCO is more sensitive to the pre-train scheme and the performance is far from saturating.\nTherefore it is reasonable to consider YOLOS as a challenging transfer learning benchmark to evaluate\ndifferent (label-supervised or self-supervised) pre-training strategies for ViT.\n3.3 Pre-training and Transfer Learning Performance of Different Scaled Models\nWe study the pre-training and the transfer learning performance of different model scaling strategies,\ni.e., width scaling (w), uniform compound scaling (dwr) and fast scaling (dwr). The models are\nscaled from ∼1.2G to ∼4.5G FLOPs regime for pre-training. Detailed model conﬁgurations and\ndescriptions are given in Sec. 3.1 and Tab. 1.\nWe pre-train all the models for 300 epochs on ImageNet-1kwith input resolution determined by the\ncorresponding scaling strategies, and then ﬁne-tune these models on COCO for 150 epochs. Few\nliteratures are available for resolution scaling in object detection, where the inputs are usually oblong\nin shape and the multi-scale augmentation [10, 27] is used as a common practice. Therefore for each\nmodel during inference, we select the smallest resolution (i.e., the shorter size) ranging in [480,800]\nproducing the highest box AP, which is784 for dwrscaling and 800 for all the others. The results\nare summarized in Tab. 4.\nImage Classiﬁcation @ ImageNet-1k Object Detection @ COCO val\nScale FLOPs f(Lin.)\nf(Att.) FPS Top-1 FLOPs f(Lin.)\nf(Att.) FPS AP\n– 1.2 G 5.9 1315 72.2 81 G 0.28 12.0 29.6\nw 4.5 G 11.8 615 79.9 194 G 0.55 5.7 36.1\ndwr 4.6 G 5.0 386 80.5 163 G 0.35 4.5 36.2\ndwr 4.6 G 8.8 511 80.4 172 G 0.49 5.7 37.6\nTable 4: Pre-training and transfer learning performance of different scaled models. FLOPs and FPS data of\nobject detection are measured over the ﬁrst 100 images of COCO val split during inference following Carion\net al. [10]. FPS is measured with batch size 1 on a single 1080Ti GPU.\nPre-training. Both dwr and dwr scaling can improve the accuracy compared with simple w\nscaling, i.e., the DeiT-S baseline. Other properties of each scaling strategy are also consistent with\nCNNs [20, 56], e.g., wscaling is the most speed friendly. dwrscaling achieves the strongest accuracy.\ndwris nearly as fast as wscaling and is on a par with dwrscaling in accuracy. Perhaps the reason\nwhy these CNN model scaling strategies are still appliable to ViT is that during pre-training the linear\nprojection (1 ×1 convolution) dominates the model computations.\nTransfer Learning. The picture changes when transferred to COCO. The input resolution r is\nmuch higher so the spatial attention takes over and linear projection part is no longer dominant in\nterms of FLOPs (f(Lin.)\nf(Att.) ∝w\nr2 ). Canonical CNN model scaling recipes do not take spatial attention\ncomputations into account. Therefore there is some inconsistency between pre-training and transfer\nlearning performance: Despite being strong on ImageNet-1k, the dwrscaling achieves similar box\nAP as simple wscaling. Meanwhile, the performance gain from dwrscaling on COCO cannot be\nclearly explained by the corresponding CNN scaling methodology that does not takef(Att.) ∝dwr4\ninto account. The performance inconsistency between pre-training and transfer learning calls for\nnovel model scaling strategies for ViT considering spatial attention complexity.\n3.4 Comparisons with CNN-based Object Detectors\nIn previous sections, we treat YOLOS as a touchstone for the transferability of ViT. In this section, we\nconsider YOLOS as an object detector and we compare YOLOS with some modern CNN detectors.\nComparisons with Tiny-sized CNN Detectors.As shown in Tab. 5, the tiny-sized YOLOS model\nachieves impressive performance compared with well-established and highly-optimized CNN object\ndetectors. YOLOS-Ti is strong in AP and competitive in FLOPs & FPS even though Transformer is\nnot intentionally designed to optimize these factors. From the model scaling perspective [20, 56, 61],\nYOLOS-Ti can serve as a promising model scaling start point.\nComparisons with DETR. The relations and differences in model design between YOLOS and\nDETR are given in Sec. 2.1, here we make quantitative comparisons between the two.\n7\nMethod Backbone Size AP Params. (M) FLOPs (G) FPS\nYOLOv3-Tiny [49] DarkNet [49] 416 ×416 16.6 8.9 5.6 330\nYOLOv4-Tiny [61] COSA [61] 416 ×416 21.7 6.1 7.0 371\nYOLOS-Ti DeiT-Ti (C) [58] 256 ×∗ 23.1 6.5 3.4 114\nCenterNet [71] ResNet-18 [26] 512 ×512 28.1 – – 129\nYOLOv4-Tiny (3l) [61] COSA [61] 320 ×320 28.7 – – 252\nDef. DETR [73] FBNet-V3 [15] 800 ×∗ 27.9 12.2 12.3 35\nYOLOS-Ti DeiT-Ti (C) [58] 432 ×∗ 28.6 6.5 11.7 84\nTable 5: Comparisons with some tiny-sized modern CNN detectors. All models are trained to be fully converged.\n“Size” refers to input resolution for inference. FLOPs and FPS data are measured over the ﬁrst 100 images of\nCOCO val split during inference following Carion et al. [10]. FPS is measured with batch size 1 on a single\n1080Ti GPU.\nMethod Backbone Epochs Size AP Params. (M) FLOPs (G) FPS\nDef. DETR [73] FBNet-V3 [15] 150 800 ×∗ 27.5 12.2 12.3 35\nYOLOS-Ti DeiT-Ti [58] 300 512 ×∗ 28.7 6.5 18.8 60\nYOLOS-Ti DeiT-Ti (C) [58] 300 432 ×∗ 28.6 6.5 11.7 84\nYOLOS-Ti DeiT-Ti (C) [58] 300 528 ×∗ 30.0 6.5 20.7 51\nDETR [10] ResNet-18-DC5 [26]\n150\n800 ×∗ 36.9 29 129 7.4\nYOLOS-S DeiT-S [58] 800 ×∗ 36.1 31 194 5.7\nYOLOS-S DeiT-S (C) [58] 800 ×∗ 37.2 31 194 5.7\nYOLOS-S (dwr) DeiT-S [58] (dwr Scale [20]) 704 ×∗ 37.2 28 123 7.7\nYOLOS-S (dwr) DeiT-S [58] (dwr Scale [20]) 784 ×∗ 37.6 28 172 5.7\nDETR [10] ResNet-101-DC5 [26] 150 800 ×∗ 42.5 60 253 5.3\nYOLOS-B DeiT-B (C) [58] 42.0 127 538 2.7\nTable 6: Comparisons with different DETR models. Tiny-sized models are trained to be fully converged. “Size”\nrefers to input resolution for inference. FLOPs and FPS data are measured over the ﬁrst 100 images of COCO\nval split during inference following Carion et al. [10]. FPS is measured with batch size 1 on a single 1080Ti\nGPU. The “ResNet-18-DC5” implantation is from timm library [65].\nAs shown in Tab. 6, YOLOS-Ti still performs better than the DETR counterpart, while larger YOLOS\nmodels with width scaling become less competitive: YOLOS-S with more computations is 0.8 AP\nlower compared with a similar-sized DETR model. Even worse, YOLOS-B cannot beat DETR with\nover 2×parameters and FLOPs. Even though YOLOS-S with dwrscaling is able to perform better\nthan the DETR counterpart, the performance gain cannot be clearly explained as discussed in Sec. 3.3.\nInterpreting the Results. Although the performance is seemingly discouraging, the numbers are\nmeaningful, as YOLOS is not purposefully designed for better performance, but designed to precisely\nreveal the transferability of ViT in object detection. E.g., YOLOS-B is directly adopted from the\nBERT-Base architecture [18] in NLP. This12 layers, 768 channels Transformer along with its variants\nhave shown impressive performance on a wide range of NLP tasks. We demonstrate that with minimal\nmodiﬁcations, this kind of architecture can also be successfully transferred (i.e., AP = 42.0) to the\nchallenging COCO object detection benchmark in computer vision from a pure sequence-to-sequence\nperspective. The minimal modiﬁcations from YOLOS exactly reveal the versatility and generality of\nTransformer.\n3.5 Inspecting Detection Tokens\nFigure 2: Visualization of all box predictions on all images from COCO val split for the ﬁrst ten [DET] tokens.\nEach box prediction is represented as a point with the coordinates of its center normalized by each thumbnail\nimage size. The points are color-coded so that blue points corresponds to small objects, green to medium objects\nand red to large objects. We observe that each [DET] token learns to specialize on certain regions and sizes. The\nvisualization style is inspired by Carion et al. [10].\n8\nFigure 3: The statistics of all ground truth object categories (thered curve) and the statistics of all object category\npredictions from all [DET] tokens (the blue curve) on all images from COCO val split. The error bar of the blue\ncurve represents the variability of the preference of different tokens for a given category, which is small. This\nsuggests that different [DET] tokens are category insensitive.\nQualitative Analysis on Detection Tokens.As an object detector, YOLOS uses [DET] tokens to\nrepresent detected objects. In general, we ﬁnd that [DET] tokens are sensitive to object locations and\nsizes, while insensitive to object categories, as shown in Fig. 2 and Fig. 3.\nQuantitative Analysis on Detection Tokens.We give a quantitative analysis on the relation be-\ntween X = the cosine similarity of [DET] token pairs, and Y = the corresponding predicted bounding\nbox centers ℓ2 distances. We use the Pearson correlation coefﬁcient ρX,Y = E[(X−µX)(Y−µY )]\nσXσY\nas a measure of linear correlation between variable X and Y, and we conduct this study on all\npredicted object pairs within each image in COCO val set averaged by all 5000 images. The result\nis ρX,Y = −0.80. This means that [DET] tokens that are close to each other (i.e., with high cosine\nsimilarity) also lead to mostly nearby predictions (i.e., with short ℓ2 distances, given ρX,Y <0).\nWe also conduct a quantitative study on the relation between X = the cosine similarity of [DET]\ntoken pairs, and Y = the corresponding cosine similarity of the output features of the classiﬁer. The\nresult is ρX,Y = −0.07, which is very close to 0. This means that there is no strong linear correlation\nbetween these two variables.\nModel [DET] Tokens Conﬁg AP\nYOLOS-Ti Rand. Init. & Learnable 28.7\nRand. Init. & Detached 28.3\nYOLOS-S Rand. Init. & Learnable 36.1\nRand. Init. & Detached 36.4\nTable 7: Impacts of detaching the [DET] tokens of\nYOLOS during training.\nDetaching Detection Tokens. To further under-\nstand the role [DET] tokens plays, we study impacts\ncaused by detaching the [DET] tokens of YOLOS dur-\ning training, i.e., we don’t optimize the parameters\nof the one hundred randomly initialized [DET] tokens.\nAs shown in Tab. 7, detaching the [DET] tokens has\na minor impact to AP. These results imply that [DET]\ntokens mainly serve as the information carrier for\nthe [PATCH] tokens. Similar phenomena are also ob-\nserved in Fang et al. [22].\n4 Related Work\nVision Transformer for Object Detection.There has been a lot of interest in combining CNNs\nwith forms of self-attention mechanisms [ 4] to improve object detection performance [ 9, 31, 64],\nwhile recent works trend towards augmenting Transformer with CNNs (or CNN design). Beal\net al. [6] propose to use a pre-trained ViT as the feature extractor for a Faster R-CNN [ 50] object\ndetector. Despite being effective, they fail to ablate the CNN architectures, region-wise pooling\noperations [23, 25, 27] as well as hand-crafted components such as dense anchors [ 50] and NMS.\nInspired by modern CNN architecture, some works [39, 60, 63, 66] introduce the pyramidal feature\nhierarchy and locality to Vision Transformer design, which largely boost the performance in dense\nprediction tasks including object detection. However, these architectures are performance-oriented\nand cannot reﬂect the properties of the canonical or vanilla Vision Transformer [ 21] that directly\ninherited from Vaswani et al. [59]. Another series of work, the DEtection TRansformer (DETR)\nfamilies [10, 73], use a random initialized Transformer to encode & decode CNN features for object\ndetection, which does not reveal the transferability of a pre-trained Transformer.\n9\nUP-DETR [16] is probably the ﬁrst to study the effects of unsupervised pre-training in the DETR\nframework, which proposes an “object detection oriented” unsupervised pre-training task tailored\nfor Transformer encoder & decoder in DETR. In this paper, we argue for the characteristics of a\npre-trained vanilla ViT in object detection, which is rare in the existing literature.\nPre-training and Fine-tuning of Transformer.The textbook-style usage of Transformer [ 59]\nfollows a “pre-training & ﬁne-tuning” paradigm. In NLP, Transformer-based models are often pre-\ntrained on large corpora and then ﬁne-tuned for different tasks at hand [18, 44]. In computer vision,\nDosovitskiy et al. [21] apply Transformer to image recognition at scale using modern vision transfer\nlearning recipe [33]. They show that a standard Transformer encoder architecture is able to attain\nexcellent results on mid-sized or small image recognition benchmarks (e.g, ImageNet-1k[51], CIFAR-\n10/100 [34], etc.) when pre-trained at sufﬁcient scale ( e.g, JFT-300M [55], ImageNet-21k [17]).\nTouvron et al. [58] achieves competitive Top-1 accuracy by training Transformer on ImageNet-1k\nonly, and is also capable of transferring to smaller datasets [34, 42, 43]. However, existing transfer\nlearning literature of Transformer arrest in image-level recognition and does not touch more complex\ntasks in vision such as object detection, which is also widely used to benchmark CNNs transferability.\nOur work aims to bridge this gap. We study the performance and properties of ViT on the challenging\nCOCO object detection benchmark [36] when pre-trained on the mid-sized ImageNet-1kdataset [51]\nusing different strategies.\n5 Discussion\nOver recent years, the landscape of computer vision has been drastically transformed by Transformer,\nespecially for recognition tasks [10, 21, 39, 58, 60]. Inspired by modern CNN design, some recent\nworks [39, 60, 63, 66] introduce the pyramidal feature hierarchy as well as locality to vanilla ViT [21],\nwhich largely boost the performance in dense recognition tasks including object detection.\nWe believe there is nothing wrong to make performance-oriented architectural designs for Transformer\nin vision, as choosing the right inductive biases and priors for target tasks is crucial for model design.\nHowever, we are more interested in designing and applying Transformer in vision following the spirit\nof NLP, i.e., pre-train the task-agnostic vanilla Vision Transformer for general visual representation\nlearning ﬁrst, and then ﬁne-tune or adapt the model on speciﬁc target downstream tasks efﬁciently.\nCurrent state-of-the-art language models pre-trained on massive amounts of corpora are able to\nperform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled\ndata [8, 38, 45, 46]. Meanwhile, prevalent pre-trained computer vision models, including various\nVision Transformer variants, still need a lot of supervision to transfer to downstream tasks.\nWe hope the introduction of Transformer can not only unify NLP and CV in terms of the architecture,\nbut also in terms of the methodology. The proposed YOLOS is able to turn a pre-trained ViT into an\nobject detector with the fewest possible modiﬁcations, but our ultimate goal is to adapt a pre-trained\nmodel to downstream vision tasks with the fewest possible costs. YOLOS still needs 150 epochs\ntransfer learning to adapt a pre-trained ViT to perform object detection, and the detection results are\nfar from saturating, indicating the pre-trained representation still has large room for improvement.\nWe encourage the vision community to focus more on the general visual representation learning for\nthe task-agnostic vanilla Transformer instead of the task-oriented architectural design of ViT. We\nhope one day, in computer vision, a universal pre-trained visual representation can be easily adapted\nto various understanding as well as generation tasks with the fewest possible costs.\n6 Conclusion\nIn this paper, we have explored the transferability of the vanilla ViT pre-trained on mid-sized\nImageNet-1kdataset to the more challenging COCO object detection benchmark. We demonstrate\nthat 2D object detection can be accomplished in a pure sequence-to-sequence manner with minimal\nadditional inductive biases. The performance on COCO is promising, and these preliminary results\nare meaningful, suggesting the versatility and generality of Transformer to various downstream tasks.\n10\nAcknowledgment\nThis work is in part supported by NSFC (No. 61876212, No. 61733007, and No. 61773176) and the\nZhejiang Laboratory under Grant 2019NB0AB02. We thank Zhuowen Tu for valuable suggestions.\nAppendix\nPosition Embedding (PE) of YOLOS\nIn object detection and many other computer vision benchmarks, the image resolutions as well as\nthe aspect ratios are usually not ﬁxed as the image classiﬁcation task. Due to the changes in input\nresolutions & aspect ratios (sequence length) from the image classiﬁcation task to the object detection\ntask, the position embedding (PE) in ViT / YOLOS has also to be changed and adapted4. The changes\nin PE could affect the model size and performance. In this work, we study two types of PE settings\nfor YOLOS:\n• Type-I adds randomly initialized PE to the input of each intermediate Transformer layer as\nDETR [10], and the PE is 1D learnable (considering the inputs as a sequence of patches in\nthe raster order) as ViT [21]. For the ﬁrst layer, the PE is interpolated following ViT. The\nsize of PEs is usually smaller than the input sequence size considering the model parameters.\nIn the paper, small- and base-sized models use this setting.\n• Type-II interpolates the pre-trained 1D learnable PE to a size similar to or slightly larger than\nthe input size, and adds no PE in intermediate Transformer layers. In the paper, tiny-sized\nmodels use this setting.\nIn a word, Type-I uses more PEs and Type-II uses larger PE.\nType-I PE. This setting adds PE to the input of each Transformer layer following DETR [ 10],\nand the PE considering the inputs as a sequence of patches in the raster order following ViT [ 21].\nSpeciﬁcally, during ﬁne-tuning, the PE of the ﬁrst layer is interpolated from the pre-trained one, and\nthe PEs for the rest intermediate layers are randomly initialized and trained from scratch. In our\npaper, small- and base-sized models use this setting. The detailed conﬁgurations are given in Tab. 8.\nPE-cls to PE-det Rand. Init. PE-det cls →det\nModel @ First Layer @ Mid. Layer Params. (M)\nYOLOS-S 224\n16 ×224\n16 ↗512\n16 ×864\n16\n512\n16 ×864\n16 22.1 →30.7\nYOLOS-S (dwr) 224\n16 ×224\n16 ↗800\n16 ×1344\n16\n800\n16 ×1344\n16 13.7 →22.0\nYOLOS-S (dwr) 224\n16 ×224\n16 ↗512\n16 ×864\n16\n512\n16 ×864\n16 19.0 →27.6\nYOLOS-B 384\n16 ×384\n16 ↗800\n16 ×1344\n16\n800\n16 ×1344\n16 86.4 →127.8\nTable 8: Type-I PE conﬁgurations for YOLOS models. “PE-cls↗PE-det” refers to performing 2D interpolation\nof ImageNet-1k pre-trained PE-cls to PE-det for object detection. The PEs added in the intermediate (Mid.)\nlayers (all the other layers of YOLOS except the ﬁrst layer) are randomly initialized.\nFrom Tab. 8, we conclude that it is expensive in terms of model size to use intermediate PEs for\nobject detection. In other words, about 1\n3 of the model weights is for providing positional information\nonly. Despite being heavy, we argue that the randomly initialized intermediate PEs do not directly\ninject additional inductive biases and they learn the positional relation from scratch. Nevertheless, for\nmulti-scale inputs during training or input with different sizes & aspect ratios during inference, we\n(have to) adjust the PE size via 2D interpolation on the ﬂy 5. As mentioned in Dosovitskiy et al. [21]\nand in the paper, this operation could introduce inductive biases.\n4PE for one hundred [DET] tokens is not affected.\n5There are some kind of data augmentations that can avoid PE interpolation, e.g., large scale jittering used\nin Tan et al. [57], which randomly resizes images between 0.1×and 2.0×of the original size then crops to\na ﬁxed resolution. However, scale jittering augmentation usually requires longer training schedules, in part\nbecause when the original input image is resized to a higher resolution, the cropped image usually has a smaller\nnumber of objects than the original, which could weaken the supervision signal therefore needs longer training\nto compensate. So there is no free lunch.\n11\nTo control the model size, these intermediate PE sizes are usually set to be smaller than the input\nsequence length, e.g., for typical models YOLOS-S and YOLOS-S (dwr), the PE size is 512\n16 ×864\n16 .\nSince the dwrscaling is more parameter friendly compared with other model scaling approaches, we\nuse a larger PE for YOLOS-S (dwr) than other small-sized models to compensate for the number of\nparameters. For larger models such as YOLOS-Base, we do not consider the model size so we also\nchoose to use larger PE.\nUsing 2D PE can save a lot of parameters, e.g., DETR uses two long enough PE (Length = 50for\nregular models and Length = 100for DC5 models) for both xand yaxes. We don’t consider2D PE\nin this work.\nPE-cls to PE-det Rand. Init. PE-det Params. (M)\nModel PE Type @ First Layer @ Rest Layer cls →det AP\nYOLOS-Ti Type-I 224\n16 ×224\n16 ↗512\n16 ×864\n16\n512\n16 ×864\n16 5.7 →9.9 28.3\nType-II 224\n16 ×224\n16 ↗800\n16 ×1344\n16 No PE 5.7 →6.5 28.7\nYOLOS-S Type-I 224\n16 ×224\n16 ↗512\n16 ×864\n16\n512\n16 ×864\n16 22.1 →30.7 36.1\nType-II 224\n16 ×224\n16 ↗960\n16 ×1600\n16 No PE 22.1 →24.6 36.6\nTable 9: Some instantiations of Type-II PE. They are lighter and better than Type-I counterparts.\nType-II PE. Later, we ﬁnd that interpolating the pre-trained PE at the ﬁrst layer to a size similar to\nor larger than the input sequence length as the only PE can provide enough positional information,\nand is more efﬁcient than using more smaller-sized PEs in the intermediate layers. In other words, it is\nredundant to use intermediate PEs given one large enough PE in the ﬁrst layer. Some instantiations are\nshown in Tab. 9. In the paper, tiny-sized models use this setting. This type of PE is more promising,\nand we will make a profound study about this setting in the future.\nSelf-attention Maps of YOLOS\nWe inspect the self-attention of the [DET] tokens that related to the predictions on the heads of the last\nlayer of YOLOS-S. The visualization pipeline follows Caron et al. [11]. The visualization results are\nshown in Fig. 4 & Fig. 5. We conclude that:\n• For a given YOLOS model, different self-attention heads focus on different patterns &\ndifferent locations. Some visualizations are interpretable while others are not.\n• We study the attention map differences of two YOLOS models, i.e., the 200 epochs\nImageNet-1k [51] pre-trained YOLOS-S and the 300 epochs ImageNet-1k pre-trained\nYOLOS-S. Note that the AP of these two models is the same (AP= 36.1). From the visual-\nization, we conclude that for a given predicted object, the corresponding [DET] token as well\nas the attention map patterns are usually different for different models.\n12\n(a) YOLOS-S, 200 epochs pre-trained, COCO AP = 36.1.\n(b) YOLOS-S, 300 epochs pre-trained, COCO AP = 36.1.\n(c) YOLOS-S, 200 epochs pre-trained, COCO AP = 36.1.\n(d) YOLOS-S, 300 epochs pre-trained, COCO AP = 36.1.\nFigure 4: The self-attention map visualization of the [DET] tokens and the corresponding predictions on the heads\nof the last layer of two different YOLOS-S models.\n13\n(a) YOLOS-S, 200 epochs pre-trained, COCO AP = 36.1.\n(b) YOLOS-S, 300 epochs pre-trained, COCO AP = 36.1.\n(c) YOLOS-S, 200 epochs pre-trained, COCO AP = 36.1.\n(d) YOLOS-S, 300 epochs pre-trained, COCO AP = 36.1.\nFigure 5: The self-attention map visualization of the [DET] tokens and the corresponding predictions on the heads\nof the last layer of two different YOLOS-S models.\n14\nReferences\n[1] Edward H Adelson, Charles H Anderson, James R Bergen, Peter J Burt, and Joan M Ogden.\nPyramid methods in image processing. RCA engineer, 1984.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv preprint arXiv:1809.10853, 2018.\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2015.\n[5] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius\nZambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan\nFaulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint\narXiv:1806.01261, 2018.\n[6] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, and Dmitry Kislyuk. Toward\ntransformer-based object detection. arXiv preprint arXiv:2012.09958, 2020.\n[7] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large\nannotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326,\n2015.\n[8] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[9] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet\nsqueeze-excitation networks and beyond. In ICCV, 2019.\n[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294, 2021.\n[12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In ICML, 2020.\n[13] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\nvision transformers. arXiv preprint arXiv:2104.02057, 2021.\n[14] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical\nautomated data augmentation with a reduced search space. In CVPRW, 2020.\n[15] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan Chen,\nYuandong Tian, Matthew Yu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe search using\nneural acquisition function. arXiv preprint arXiv:2006.02049, 2020.\n[16] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training\nfor object detection with transformers. In CVPR, 2021.\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[19] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential para-\nphrases. In IWP, 2005.\n15\n[20] Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and accurate model scaling. arXiv preprint\narXiv:2103.06877, 2021.\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[22] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-\ntransformer: Exchanging local spatial information by manipulating messenger tokens. arXiv\npreprint arXiv:2105.15168, 2021.\n[23] Ross Girshick. Fast r-cnn. In ICCV, 2015.\n[24] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. TPAMI, 2015.\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016.\n[27] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask r-cnn. In ICCV, 2017.\n[28] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In ICCV, 2019.\n[29] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\n[30] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural\nnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n[31] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018.\n[32] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In ECCV, 2016.\n[33] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\nGelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv\npreprint arXiv:1912.11370, 2019.\n[34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. NeurIPS, 2012.\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[37] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\nFeature pyramid networks for object detection. In CVPR, 2017.\n[38] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\nprocessing. arXiv preprint arXiv:2107.13586, 2021.\n[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n16\n[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[41] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\nIn ICML, 2010.\n[42] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large\nnumber of classes. In ICVGIP, 2008.\n[43] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In\nCVPR, 2012.\n[44] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI blog, 2019.\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n[47] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Design-\ning network design spaces. In CVPR, 2020.\n[48] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[49] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint\narXiv:1804.02767, 2018.\n[50] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015.\n[51] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. IJCV, 2015.\n[52] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-\nindependent named entity recognition. arXiv preprint cs/0306050, 2003.\n[53] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\n[54] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. JMLR, 2014.\n[55] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\neffectiveness of data in deep learning era. In ICCV, 2017.\n[56] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\nnetworks. In ICML, 2019.\n[57] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcientdet: Scalable and efﬁcient object\ndetection. In CVPR, 2020.\n[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need.arXiv preprint arXiv:1706.03762,\n2017.\n17\n[60] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and\nJonathon Shlens. Scaling local self-attention for parameter efﬁcient visual backbones. arXiv\npreprint arXiv:2103.12731, 2021.\n[61] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling\ncross stage partial network. arXiv preprint arXiv:2011.08036, 2020.\n[62] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao.\nLearning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787,\n2019.\n[63] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[64] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn CVPR, 2018.\n[65] Ross Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019.\n[66] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image\ntransformers. arXiv preprint arXiv:2104.06399, 2021.\n[67] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon\nYoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In\nICCV, 2019.\n[68] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint\narXiv:1605.07146, 2016.\n[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n[70] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data\naugmentation. In AAAI, 2020.\n[71] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. arXiv preprint\narXiv:1904.07850, 2019.\n[72] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu, Zeming Li, and Jian\nSun. Autoassign: Differentiable label assignment for dense object detection. arXiv preprint\narXiv:2007.03496, 2020.\n[73] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020.\n18",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.71517014503479
    },
    {
      "name": "Computer science",
      "score": 0.6651672124862671
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5490626692771912
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4842652678489685
    },
    {
      "name": "Object detection",
      "score": 0.4771372973918915
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4146212935447693
    },
    {
      "name": "Computer vision",
      "score": 0.34080007672309875
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3380233645439148
    },
    {
      "name": "Machine learning",
      "score": 0.32819902896881104
    },
    {
      "name": "Engineering",
      "score": 0.15463125705718994
    },
    {
      "name": "Cartography",
      "score": 0.09833937883377075
    },
    {
      "name": "Voltage",
      "score": 0.09427464008331299
    },
    {
      "name": "Geography",
      "score": 0.08031022548675537
    },
    {
      "name": "Electrical engineering",
      "score": 0.06730961799621582
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 194
}