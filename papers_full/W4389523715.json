{
  "title": "LLM-driven Instruction Following: Progresses and Concerns",
  "url": "https://openalex.org/W4389523715",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1970544490",
      "name": "Wenpeng Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157516767",
      "name": "Qinyuan Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000456417",
      "name": "Pengfei Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4383045326",
    "https://openalex.org/W3198490223",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W4312091404",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W4320843360",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W3121854931",
    "https://openalex.org/W4285309087",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4297412056",
    "https://openalex.org/W4287854458",
    "https://openalex.org/W2122223050",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W3212893438",
    "https://openalex.org/W3173781631",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4367701241",
    "https://openalex.org/W2140584963",
    "https://openalex.org/W4385574017",
    "https://openalex.org/W2023286866",
    "https://openalex.org/W4385572124",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4285176332",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4389518761",
    "https://openalex.org/W3216866458",
    "https://openalex.org/W4385570412",
    "https://openalex.org/W4308615640",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4298343034",
    "https://openalex.org/W4225716497",
    "https://openalex.org/W4366330700",
    "https://openalex.org/W2970200208",
    "https://openalex.org/W4285254250",
    "https://openalex.org/W3161024824",
    "https://openalex.org/W3176688854",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4389421376",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4288029342"
  ],
  "abstract": "The progress of natural language processing (NLP) is primarily driven by machine learning that optimizes a system on a large-scale set of task-specific labeled examples. This learning paradigm limits the ability of machines to have the same capabilities as humans in handling new tasks since humans can often solve unseen tasks with a couple of examples accompanied by task instructions. In addition, we may not have a chance to prepare task-specific examples of large-volume for new tasks because we cannot foresee what task needs to be addressed next and how complex to annotate for it. Therefore, task instructions act as a novel and promising resource for supervision. This tutorial targets researchers and practitioners who are interested in AI and ML technologies for NLP generalization in a low-shot scenario. In particular, we will present a diverse thread of instruction-driven NLP studies that try to answer the following questions: (i) What is task instruction? (ii) How is the process of creating datasets and evaluating systems conducted? (iii) How to encode task instructions? (iv) When and why do some instructions work better? (v) What concerns remain in LLM-driven instruction following? We will discuss several lines of frontier research that tackle those challenges and will conclude the tutorial by outlining directions for further investigation.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 19–25\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLLM-driven Instruction Following: Progresses and Concerns\nWenpeng Yin†, Qinyuan Ye‡, Pengfei Liu⋄, Xiang Ren‡ and Hinrich Schütze♯\n†Penn State; ‡USC; ⋄SJTU; ♯LMU Munich\nwenpeng@psu.edu; {qinyuany, xiangren}@usc.edu\nstefanpengfei@gmail.com; hinrich@hotmail.com\nAbstract\nThe progress of natural language processing\n(NLP) is primarily driven by machine learning\nthat optimizes a system on a large-scale set of\ntask-specific labeled examples. This learning\nparadigm limits the ability of machines to have\nthe same capabilities as humans in handling\nnew tasks since humans can often solve unseen\ntasks with a couple of examples accompanied\nby task instructions. In addition, we may not\nhave a chance to prepare task-specific exam-\nples of large-volume for new tasks because we\ncannot foresee what task needs to be addressed\nnext and how complex to annotate for it. There-\nfore, task instructions act as a novel and promis-\ning resource for supervision.\nThis tutorial targets researchers and practition-\ners who are interested in AI and ML technolo-\ngies for NLP generalization in a low-shot sce-\nnario. In particular, we will present a diverse\nthread of instruction-driven NLP studies that\ntry to answer the following questions: (i) What\nis task instruction? (ii) How is the process of\ncreating datasets and evaluating systems con-\nducted? (iii) How to encode task instructions?\n(iv) When and why do some instructions work\nbetter? (v) What concerns remain in LLM-\ndriven instruction following? We will discuss\nseveral lines of frontier research that tackle\nthose challenges and will conclude the tutorial\nby outlining directions for further investigation.\n1 Introduction\nThis proposal is driven by a fundamental question\nof task generalization in NLP: how to comprehend\na new task if labeled examples are pretty limited?\nOne goal of AI is to build a system that can con-\ntinually understand and solve new tasks. Labeled\nexamples, as the mainstream task representation,\nare unlikely to be available in large numbers or\neven do not exist. Then, is there any other task\nrepresentation that can contribute to task compre-\nhension? Task instructions provide another dimen-\nsion of supervision for expressing the task seman-\ntics. Instructions often contain more abstract and\ncomprehensive knowledge of the target task than\nindividual labeled examples. With the availability\nof task instructions, systems can be quickly built\nto handle new tasks, especially when task-specific\nannotations are scarce (Wang et al., 2022; Yin et al.,\n2022). Instruction following is inspired by the typi-\ncal human learning for new tasks, e.g., a little kid\ncan well solve a new mathematical task by learn-\ning from its instruction and a few examples. This\nnew learning paradigm has recently begun to at-\ntract the attention of the machine learning and NLP\ncommunities.\nDespite the importance, frontier research in in-\nstruction following is still struggling with the fol-\nlowing questions. First, should instructions be con-\nstructed to express the target task as detailed as\npossible (e.g., MTurk instructions (Mishra et al.,\n2022)) or to align with the format of supervising\ntasks (e.g., natural language inference (Yin et al.,\n2019) or language modeling (Brown et al., 2020))\nas well as possible? Second, how to effectively en-\ncode instructions that may consist of some specific\nrequirements such as “maximal output length 5”,\nand “do not generate anything else apart from one\nof the following · · ·”? Third, what are the factors\n(e.g., model size, task numbers) that influence a sys-\ntem’s generalization, robustness, etc.? Fourth, how\nto evaluate instruction-following systems? Last,\nwhat is the future for academia and industry in this\nChatGPT era?\nIn this tutorial, we will systematically review\nseveral lines of frontier research on developing\nsystems that are supervised by task instructions.\nBeyond introducing pioneering work that parsed\ninstructions to cope with individual tasks, such as\nsoccer game (Kuhlmann et al., 2004), software con-\ntrol (Branavan et al., 2009, 2011), etc., we will\nfocus on recent LLM-based approaches for cross-\ntask generalization given task instructions. Specifi-\ncally, in light of the heterogeneous formats and dis-\n19\nparate rationales underlying instructions, we shall\nendeavor to establish a unified lens for interpreting\nthe essence of various instructions. Subsequently,\na structured exposition and critical analysis will\nbe undertaken, encompassing a spectrum of as-\npects such as diverse instruction-following datasets,\nrigorous evaluation methodologies, multifaceted\nperformance-influencing factors, and lingering con-\ncerns within this domain.\nParticipants will learn about recent trends and\nemerging challenges in this topic, representative\ntools and learning resources to obtain ready-to-use\nmodels, and how related technologies benefit end-\nuser NLP applications.\n2 Outline of Tutorial Content\nThis half-day tutorial presents a systematic\noverview of recent advancements in NLP with su-\npervision from task instructions. The detailed con-\ntents are outlined below.\n2.1 Background and motivation [20min]\nWe will define the main research problem and moti-\nvate the topic by presenting several real-world NLP\nand instruction-driven AI applications, as well as\nseveral key challenges that are at the core of classic\nmachine learning.\n2.2 What is the essence of instructions?\n[30min]\nVarious researchers may hold differing viewpoints\non the nature of instructions, with some specializ-\ning in particular types of instructions while over-\nlooking the interconnections among various instruc-\ntion categories. In this section, we aim to establish\na unified perspective for understanding the essence\nof instructions.\nWe begin by introducing various typical forms\nof instructions. For instance, some instructions\nserve to elucidate the output labels in classifica-\ntion tasks, as exemplified by NLI-oriented task\ninstructions (Yin et al., 2019; Xu et al., 2022; Li\net al., 2022; Xia et al., 2021; Sainz et al., 2021,\n2022). These instructions treat the outputs as hy-\npotheses and transform the target problems into\nnatural language inference (NLI) to leverage the su-\npervision available in existing NLI datasets. Other\ninstructions aim to enhance the input text, such\nas prompts, which are designed to leverage the\nrich supervision from pretrained language models\n(Radford et al., 2019; Schick and Schütze, 2021b,a,\n2022). Thus, they are referred to as LM-oriented\ninstructions. Additionally, there are more natu-\nral instructions contributed by end-users who lack\nexpertise in machine learning or LLMs. These in-\nstructions attempt to convey the task’s semantics\nregardless of the specific technique to be employed.\nWe categorize these as human-oriented instruc-\ntions (Efrat and Levy, 2020; Mishra et al., 2022;\nWang et al., 2022; Lou et al., 2023). To adhere to\nhuman-oriented instructions, LLMs are frequently\ntrained on a diverse array of instruction-following\ntasks. Consequently, we consolidate these distinct\ntypes of instructions under the umbrella term in-\nstructions as supervision-oriented textual expres-\nsions.\n2.3 Instruction-following datasets and\nevaluations [30min]\nInitially, we introduce a range of crowdsourced\ndatasets, which include P3 (Sanh et al.), Big-bench\n(Srivastava et al., 2022), Dolly (Conover et al.,\n2023), Natural-Instructions (Mishra et al., 2022;\nWang et al., 2022), Multi-Instruct (Xu et al., 2023b),\netc. Nevertheless, human-crafted datasets have\ninherent limitations due to the constraints of hu-\nman effort, making it challenging to expand the\ndiversity and complexity of tasks. Consequently,\nrecent efforts have turned to LLM-generated\ndatasets, as exemplified by Self-Instruct (Wang\net al., 2023), Unnatural-Instruct (Honovich et al.,\n2023), Dynosaur (Yin et al., 2023), WizardLM (Xu\net al., 2023a), LongForm (Köksal et al., 2023), Muf-\nfin (Lou et al., 2023), and others. Irrespective of\nthe datasets’ origin, this tutorial will elucidate their\nobjectives and distinctions from a scaling perspec-\ntive.\nRegarding the evaluation, we commence with\nautomated assessments conducted on a selection\nof high-quality crowdsourced datasets, including\nNatural-Instructions (Mishra et al., 2022; Wang\net al., 2022), T0 (Sanh et al.), Big-bench (Srivas-\ntava et al., 2022), etc. Subsequently, we introduce\nVicuna system (Chiang et al., 2023), which em-\nployed GPT-41 for automated evaluations. Finally,\nwe proceed to human assessments, which take into\naccount various criteria, as demonstrated in works\nsuch as (Wang et al., 2023; Yin et al., 2023; Askell\net al., 2021).\n1https://openai.com/research/gpt-4\n20\n2.4 Methodology for instruction tuning\n[30min]\nAn established experimental framework for instruc-\ntion tuning entails initially training a model on a\nset of provided instructions and subsequently as-\nsessing its performance on unseen instructions. In\nthis context, we will present three distinct method-\nologies for modeling instructions: (i) The Concate-\nnation method, which involves the straightforward\nconcatenation of elements from the instruction and\ntask input to form a lengthy textual sequence. This\ncomposite sequence is then fed into an LLM to\ngenerate the desired output. Representative works\ninclude (Mishra et al., 2022; Wang et al., 2022;\nYin et al., 2022). (ii) Hypernetwork-based ap-\nproaches (Ye and Ren, 2021; Ivison et al., 2022),\nwhere a hypernetwork (Ha et al., 2017) is trained\nto generate instruction-specific model parameters,\nwhich are subsequently integrated into a primary\nnetwork. (iii) Reinforcement learning with hu-\nman feedback methods (Bai et al., 2022; Ouyang\net al., 2022; Stiennon et al., 2020), which involve\nthe utilization of reinforcement learning techniques\nguided by human-provided comparison data.\n2.5 When and why it works [30min]\nMost instruction-driven systems assume that each\ntask has a single instruction. We can imagine that\ndifferent users can convey a task with instructions\nof distinct textual expressions. Some prompt-based\nLLMs also show varying performance in dealing\nwith prompts of different templates (Schick and\nSchütze, 2022; Kojima et al., 2022). A question\narises: how to predict and explain an instruction’\nbehavior? To the end, we first introduce the work\nby Gu et al. (2023) that explored the robustness of\npretrained instruction learning system in handling\n(i) the same task with distinct instructions writ-\nten by different MTurkers, and (ii) instruction of\nvarying degrees of abstractions. Then, we present\na series of works that i) explain prompts perfor-\nmance by LLM-oriented perplexity (Gonen et al.,\n2022), the model bias (Zhao et al., 2021), or ii)\nimprove instructions by reformulating them into\nmore effective ones (Khashabi et al., 2022).\n2.6 Concerns of instruction following [30min]\nIn this section, we will address concerns related\nto instruction following across four distinct dimen-\nsions: (i) The “inverse scaling law” observed in\nLLMs when dealing with negation (Mishra et al.,\n2022; Jang et al., 2022; Hossain et al., 2022). (ii)\nUnanticipated behavior arising in the realm of in-\nstruction comprehension, drawing from human ca-\npabilities in following instructions (Webson and\nPavlick, 2022). (iii) The issue of task-hungry mod-\nels. Despite shifting our research focus from cross-\nexample generalization to cross-task generation,\nthe creation of large-scale instruction-following\ndatasets presents another challenge. To enhance\nLLMs’ instruction-following abilities for new tasks,\nthe collection of extensive training tasks becomes\na necessity. (iv) The emergence of adversarial in-\nstruction attacks (Shu et al., 2023; Wan et al., 2023;\nKang et al., 2023; Li et al., 2023).\n2.7 Future directions [10min]\nIn the last section, we will discuss some critical and\nforeshadowing research directions, such as scalable\noversight and alignment (Hendrycks et al., 2021;\nBowman et al., 2022), explainable instruction learn-\ning, and how to encode instructions without the\nhelp of labeled examples, etc.\n3 Specification of the Tutorial\nThe proposed tutorial is considered a\ncutting-edge tutorial that introduces new\nfrontiers in instruction-driven NLP. The pre-\nsented topic has not been covered by any\nACL/EMNLP/EACL/NAACL/AACL/COLING\ntutorials in the past 4 years. A tiny overlap\nexists between our section “LM-oriented task\ninstructions” and the ACL tutorial (Beltagy\net al., 2022), which presented LLM techniques\nfor NLP. But Beltagy et al. (2022) focused on\nvarious training techniques, such as self-training,\nmeta-training, etc., our tutorial has a broader scope\nof instruction learning, in which prompt-based\nLLM is merely a sub-area.\nAudience and Prerequisites Based on the level\nof interest in this topic, we expect around 150 par-\nticipants. While no specific background knowledge\nis assumed of the audience, it would be best for\nthe attendees to know about basic deep learning\ntechnologies, pre-trained language models (e.g.,\nBERT). A reading list that could help provide\nbackground knowledge to the audience before at-\ntending this tutorial is given in Appendix A.1.\nBreadth We estimate that at least 60% of the\nwork covered in this tutorial is from researchers\nother than the instructors of the tutorial.\n21\nDiversity Considerations This tutorial will\ncover instruction learning for NLP as well as non-\nNLP problems, such as instruction-driven naviga-\ntion, software control, etc. We will also cover con-\ntent applying instruction supervision for individual\ntasks as well as cross-task generation. Our pre-\nsenter team has a diverse background regarding\ngeography and gender. Our team will promote our\ntutorial on social media to diversify our audience\nparticipation.\nMaterial Access Online All the materials are\nopenly available at www.wenpengyin.org/\npublications\n4 Tutorial Instructors\nThe following are biographies of the speaker.\nWenpeng Yin is an Assistant Professor in the\nDepartment of Computer Science and Engineer-\ning at Penn State University. His research focuses\non NLP with three sub-areas: (i) learning from\ntask instructions; (ii) information extraction; (iii)\nNLP for education, bioinformatics, etc. Dr. Yin\nhas presented the tutorial “Indirectly Supervised\nNatural Language Processing” at ACL’23, and tu-\ntorial “Learning from Task Instructions” at KON-\nVENS’23. Additional information is available at\nwww.wenpengyin.org.\nQinyuan Ye is a fifth-year Ph.D. student at the\nUniversity of Southern California, advised by Prof.\nXiang Ren. Her research interest lies in natural\nlanguage processing. In particular she is interested\nin approaches that reduce human annotation efforts,\nincluding methods leveraging distant supervision,\nhigh-level human supervision (e.g., explanations,\ninstructions), and meta-learning. Additional infor-\nmation is available at yeqy.xyz.\nPengfei Liu is an associate professor at Shang-\nhai Jiaotong University and leads the Generative\nArtificial Intelligence Research Lab (GAIR). His\nresearch topics currently focus on information ex-\ntraction, text generation, language pre-training, and\nNLP system evaluation. He won the Best Demo\nPaper award in ACL 2021 and the Outstanding\nDemo Paper award in ACL 2022. Homepage:\nhttp://pfliu.com.\nXiang Ren is an Associate Professor in Com-\nputer Science and the Andrew and Erna Viterbi\nEarly Career Chair at USC. Ren’s research seeks\nto build generalizable NLP systems that can han-\ndle a wide variety of language tasks and situations.\nHe works on new algorithms and datasets to make\nNLP systems cheaper to develop and maintain, arm\nmachine models with common sense, and improve\nmodel’s transparency and reliability to build user\ntrust. His research work has received several best\npaper awards in top NLP and AI conference venues.\nRen has been awarded an NSF CAREER Award,\nmultiple faculty research awards from Google,\nFacebook, Amazon, JP Morgan and Sony, and\nthe 2018 ACM SIGKDD Doctoral Dissertation\nAward. He was named Forbes’ Asia 30 Under 30\nin 2019. Ren has presented a number of tutorials,\nsuch as Knowledge-Augmented Methods for Nat-\nural Language Processing at ACL 2022, Scalable\nConstruction and Reasoning of Massive Knowl-\nedge Bases at NAACL 2018, and other related tu-\ntorials at WWW’18, CIKM’17, etc. Homepage:\nhttps://shanzhenren.github.io.\nHinrich Schütze is Chair of Computational\nLinguistics and co-director of the Center of In-\nformation and Language Processing at Ludwig-\nMaximilians-Universität München (LMU Munich),\nGermany. He was the President of the Association\nfor Computational Linguistics in 2020, and Gen-\neral Chair of ACL 2013. In 2022, Prof. Schütze\nwas elected as ACL Fellow. Prior to joining LMU\nMunich, he was a Professor of Theoretical Compu-\ntational Linguistics at the University of Stuttgart.\nHinrich holds a Ph.D. in computational linguistics\nfrom Stanford University. Additional information\nis available athttps://schuetze.cis.lmu.\nde.\nEthical Considerations\nWe do not anticipate any ethical issues particularly\nto the topics of the tutorial.\nReferences\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Benjamin Mann, Nova DasSarma, Nelson\nElhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom B. Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment. CoRR, abs/2112.00861.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\n22\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom B.\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBenjamin Mann, and Jared Kaplan. 2022. Train-\ning a helpful and harmless assistant with rein-\nforcement learning from human feedback. CoRR,\nabs/2204.05862.\nIz Beltagy, Arman Cohan, Robert Logan IV , Sewon Min,\nand Sameer Singh. 2022. Zero- and few-shot NLP\nwith pretrained language models. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics: Tutorial Abstracts, pages\n32–37, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nSamuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin\nChen, Craig Pettit, Scott Heiner, Kamile Lukosuite,\nAmanda Askell, Andy Jones, Anna Chen, et al. 2022.\nMeasuring progress on scalable oversight for large\nlanguage models. arXiv preprint arXiv:2211.03540.\nS. R. K. Branavan, Harr Chen, Luke Zettlemoyer, and\nRegina Barzilay. 2009. Reinforcement learning for\nmapping instructions to actions. In Proceedings of\nACL, pages 82–90.\nS. R. K. Branavan, David Silver, and Regina Barzi-\nlay. 2011. Learning to win by reading manuals in\na monte-carlo framework. In Proceedings of ACL,\npages 268–277.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nAvia Efrat and Omer Levy. 2020. The turking test: Can\nlanguage models understand instructions? CoRR,\nabs/2010.11982.\nDan Goldwasser and Dan Roth. 2011. Learning from\nnatural instructions. In IJCAI 2011, Proceedings of\nthe 22nd International Joint Conference on Artificial\nIntelligence, Barcelona, Catalonia, Spain, July 16-22,\n2011, pages 1794–1800. IJCAI/AAAI.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,\nand Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation. CoRR,\nabs/2212.04037.\nJiasheng Gu, Hongyu Zhao, Hanzi Xu, Liangyu Nie,\nHongyuan Mei, and Wenpeng Yin. 2023. Robustness\nof learning from task instructions. In Findings of\nACL, pages 13935–13948.\nDavid Ha, Andrew M. Dai, and Quoc V . Le. 2017. Hy-\npernetworks. In ICLR.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning AI with shared human values. In\nICLR.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2023. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. In Pro-\nceedings of ACL, pages 14409–14428.\nMd Mosharaf Hossain, Dhivya Chinnappa, and Eduardo\nBlanco. 2022. An analysis of negation in natural\nlanguage understanding corpora. In Proceedings of\nACL, pages 716–723.\nHamish Ivison, Akshita Bhagia, Yizhong Wang,\nHannaneh Hajishirzi, and Matthew Peters. 2022.\nHint: Hypernetwork instruction tuning for effi-\ncient zero-shot generalisation. arXiv preprint\narXiv:2212.10315.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2022. Can\nlarge language models truly understand prompts? A\ncase study with negated prompts. In Transfer Learn-\ning for Natural Language Processing Workshop, 03\nDecember 2022, New Orleans, Louisiana, USA, vol-\nume 203 of Proceedings of Machine Learning Re-\nsearch, pages 52–62. PMLR.\nDaniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin,\nMatei Zaharia, and Tatsunori Hashimoto. 2023.\nExploiting programmatic behavior of llms: Dual-\nuse through standard security attacks. CoRR,\nabs/2302.05733.\nDaniel Khashabi, Chitta Baral, Yejin Choi, and Han-\nnaneh Hajishirzi. 2022. Reframing instructional\nprompts to gptk’s language. In Findings of ACL ,\npages 589–612.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\n23\nAbdullatif Köksal, Timo Schick, Anna Korhonen, and\nHinrich Schütze. 2023. Longform: Optimizing in-\nstruction tuning for long text generation with corpus\nextraction. CoRR, abs/2304.08460.\nGregory Kuhlmann, Peter Stone, Raymond Mooney,\nand Jude Shavlik. 2004. Guiding a reinforcement\nlearner with natural language advice: Initial results\nin robocup soccer. In The AAAI workshop on supervi-\nsory control of learning and adaptive systems, pages\n30–35.\nBangzheng Li, Wenpeng Yin, and Muhao Chen. 2022.\nUltra-fine entity typing with indirect supervision\nfrom natural language inference. Trans. Assoc. Com-\nput. Linguistics, 10:607–622.\nZekun Li, Baolin Peng, Pengcheng He, and Xifeng\nYan. 2023. Do you really follow me? adversarial\ninstructions for evaluating the robustness of large\nlanguage models. CoRR, abs/2308.10819.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nRenze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice\nAhn, Hanzi Xu, Yu Su, and Wenpeng Yin. 2023.\nMUFFIN: Curating multi-faceted instructions for im-\nproving instruction following.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of ACL, pages 3470–3487.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, An-\nder Barrena, and Eneko Agirre. 2021. Label verbal-\nization and entailment for effective zero and few-shot\nrelation extraction. In Proceedings of EMNLP, pages\n1199–1212.\nOscar Sainz, Itziar Gonzalez-Dios, Oier Lopez de La-\ncalle, Bonan Min, and Eneko Agirre. 2022. Textual\nentailment for event argument extraction: Zero- and\nfew-shot with multi-source learning. In Findings of\nNAACL, pages 2439–2455.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. Multitask\nprompted training enables zero-shot task generaliza-\ntion. In ICLR.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of EACL,\npages 255–269.\nTimo Schick and Hinrich Schütze. 2021b. Few-shot\ntext generation with natural language instructions. In\nProceedings of EMNLP, pages 390–402.\nTimo Schick and Hinrich Schütze. 2022. True few-shot\nlearning with prompts - A real-world perspective.\nTrans. Assoc. Comput. Linguistics, 10:716–731.\nManli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geip-\ning, Chaowei Xiao, and Tom Goldstein. 2023. On\nthe exploitability of instruction tuning. CoRR,\nabs/2306.17194.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Rahane, Anantharaman S.\nIyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmüller, Andrew M. Dai, Andrew La, Andrew K.\nLampinen, Andy Zou, Angela Jiang, Angelica Chen,\nAnh Vuong, Animesh Gupta, Anna Gottardi, Anto-\nnio Norelli, Anu Venkatesh, Arash Gholamidavoodi,\nArfa Tabassum, Arul Menezes, Arun Kirubarajan,\nAsher Mullokandov, Ashish Sabharwal, Austin Her-\nrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and\net al. 2022. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nCoRR, abs/2206.04615.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F. Christiano. 2020. Learn-\ning to summarize from human feedback. In NeurIPS.\nAlexander Wan, Eric Wallace, Sheng Shen, and Dan\nKlein. 2023. Poisoning language models during in-\nstruction tuning. In Proceedings of ICML, volume\n202, pages 35413–35425.\n24\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of ACL, pages 13484–13508.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, Eshaan Pathak, Gi-\nannis Karamanolakis, Haizhi Gary Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuz-\nnia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa-\ntel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-\nhit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit\nVerma, Ravsehaj Singh Puri, Rushang Karia, Savan\nDoshi, Shailaja Keyur Sampat, Siddhartha Mishra,\nSujan Reddy A, Sumanta Patro, Tanay Dixit, and\nXudong Shen. 2022. Super-naturalinstructions: Gen-\neralization via declarative instructions on 1600+ NLP\ntasks. In Proceedings of EMNLP, pages 5085–5109.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan,\nQinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xi-\nang Ren. 2020. Learning from explanations with\nneural execution tree. In Proceedings of ICLR.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of NAACL, pages 2300–\n2344.\nCongying Xia, Wenpeng Yin, Yihao Feng, and Philip S.\nYu. 2021. Incremental few-shot text classification\nwith multi-round new classes: Formulation, dataset\nand system. In Proceedings of NAACL-HLT, pages\n1351–1360.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. CoRR,\nabs/2304.12244.\nHanzi Xu, Slobodan Vucetic, and Wenpeng Yin. 2022.\nOpenstance: Real-world zero-shot stance detection.\nIn Proceedings of CoNLL.\nZhiyang Xu, Ying Shen, and Lifu Huang. 2023b. Multi-\ninstruct: Improving multi-modal zero-shot learning\nvia instruction tuning. In Proceedings of ACL, pages\n11445–11465.\nQinyuan Ye and Xiang Ren. 2021. Learning to generate\ntask-specific adapters from task description. In Pro-\nceedings of ACL/IJCNLP (Volume 2: Short Papers),\npages 646–653.\nDa Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal,\nJiawei Han, and Kai-Wei Chang. 2023. Dynosaur: A\ndynamic growth paradigm for instruction-tuning data\ncuration. CoRR, abs/2305.14327.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Bench-\nmarking zero-shot text classification: Datasets, eval-\nuation and entailment approach. In Proceedings of\nEMNLP-IJCNLP, pages 3912–3921.\nWenpeng Yin, Jia Li, and Caiming Xiong. 2022. Con-\nTinTin: Continual learning from task instructions. In\nProceedings of ACL, pages 3062–3072.\nYichi Zhang and Joyce Chai. 2021. Hierarchical task\nlearning from language instructions with unified\ntransformers and self-monitoring. In Findings of\nACL/IJCNLP, pages 4202–4213.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of ICML , volume 139, pages 12697–\n12706.\nA Appendix\nA.1 Recommended Paper List\nThe following is a reading list that could help pro-\nvide background knowledge to the audience before\nattending this tutorial:\n• Learning from Natural Instructions (Gold-\nwasser and Roth, 2011)\n• Learning from Explanations with Neural Exe-\ncution Tree (Wang et al., 2020)\n• Benchmarking Zero-shot Text Classification:\nDatasets, Evaluation and Entailment Ap-\nproach (Yin et al., 2019)\n• Textual Entailment for Event Argument Ex-\ntraction: Zero- and Few-Shot with Multi-\nSource Learning (Sainz et al., 2022)\n• Pre-train, Prompt, and Predict: A Systematic\nSurvey of Prompting Methods in Natural Lan-\nguage Processing (Liu et al., 2021)\n• True Few-Shot Learning With Prompts—A\nReal-World Perspective (Schick and Schütze,\n2022)\n• The Turking Test: Can Language Models Un-\nderstand Instructions? (Efrat and Levy, 2020)\n• Hierarchical Task Learning from Language\nInstructions with Unified Transformers and\nSelf-Monitoring (Zhang and Chai, 2021)\n• Cross-Task Generalization via Natural Lan-\nguage Crowdsourcing Instructions (Mishra\net al., 2022)\n• MUFFIN: Curating Multi-Faceted Instruc-\ntions for Improving Instruction Following\n(Lou et al., 2023)\n25",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8560472726821899
    },
    {
      "name": "Task (project management)",
      "score": 0.6518977880477905
    },
    {
      "name": "Process (computing)",
      "score": 0.4883725047111511
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4818888306617737
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4704144299030304
    },
    {
      "name": "Human–computer interaction",
      "score": 0.39404791593551636
    },
    {
      "name": "Natural language processing",
      "score": 0.3497496247291565
    },
    {
      "name": "Programming language",
      "score": 0.12350451946258545
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130769515",
      "name": "Pennsylvania State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3018771216",
      "name": "LMU Klinikum",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ]
}