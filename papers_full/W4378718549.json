{
  "title": "Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model",
  "url": "https://openalex.org/W4378718549",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4378735296",
      "name": "Soong, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2512548914",
      "name": "Sridhar, Sriram",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146275537",
      "name": "Si Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378735299",
      "name": "Wagner, Jan-Samuel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378735300",
      "name": "Sá, Ana Caroline Costa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378735301",
      "name": "Yu, Christina Y",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378735302",
      "name": "Karagoz, Kubra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378735303",
      "name": "Guan, Meijian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378735304",
      "name": "Hamadeh, Hisham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3197128863",
      "name": "Higgs Brandon W",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4287855143",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4316116392",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4318899036",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4319048572",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4311537867",
    "https://openalex.org/W4295951577",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4286224767",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4304192721",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4287208373",
    "https://openalex.org/W2250539671"
  ],
  "abstract": "Large language models (LLMs) have made significant advancements in natural language processing (NLP). Broad corpora capture diverse patterns but can introduce irrelevance, while focused corpora enhance reliability by reducing misleading information. Training LLMs on focused corpora poses computational challenges. An alternative approach is to use a retrieval-augmentation (RetA) method tested in a specific domain. To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a custom RetA model were compared using 19 questions on diffuse large B-cell lymphoma (DLBCL) disease. Eight independent reviewers assessed responses based on accuracy, relevance, and readability (rated 1-3). The RetA model performed best in accuracy (12/19 3-point scores, total=47) and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4 received the highest readability scores (17/19, 55), followed by GPT-3 (15/19, 53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34), relevance (32), and readability (38). Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data. These findings suggest that RetA models, supplemented with domain-specific corpora, may outperform general-purpose LLMs in accuracy and relevance within specific domains. However, this evaluation was limited to specific questions and metrics and may not capture challenges in semantic search and other NLP tasks. Further research will explore different LLM architectures, RetA methodologies, and evaluation methods to assess strengths and limitations more comprehensively.",
  "full_text": "Improving accuracy of GPT-3/4 results on biomedical data using a\nretrieval-augmented language model\nDavid Soong*1, Sriram Sridhar*1, Han Si1, Jan-Samuel Wagner1, Ana Caroline Costa Sá1, Christina\nY Yu1, Kubra Karagoz1, Meijian Guan1, Hisham Hamadeh1, Brandon W Higgs1\n1Data Sciences, Genmab, Princeton, NJ\n*Equal contribution\nAbstract\nLarge language models (LLMs) have made significant advancements in natural language processing\n(NLP). Broad corpora capture diverse patterns but can introduce irrelevance, while focused corpora\nenhance reliability by reducing misleading information. Training LLMs on focused corpora poses\ncomputational challenges. An alternative approach is to use a retrieval-augmentation (RetA) method\ntested in a specific domain.\nTo evaluate LLM performance, OpenAI's GPT-3.5, GPT-4, Bing's Prometheus, and a custom RetA\nmodel were compared using 19 questions on diffuse large B-cell lymphoma (DLBCL) disease. Eight\nindependent reviewers assessed responses based on accuracy, relevance, and readability (rated 1-3).\nThe RetA model performed best in accuracy (12/19 3-point scores, total=47) and relevance (13/19,\n50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4 received the highest readability scores (17/19,\n55), followed by GPT-3.5 (15/19, 53) and the RetA model (11/19, 47). Prometheus underperformed\nin accuracy (34), relevance (32), and readability (38).\nBoth GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model\nand Prometheus. Hallucinations were mostly associated with non-existent references or fabricated\nefficacy data.\nThese findings suggest that RetA models, supplemented with domain-specific corpora, may out-\nperform general-purpose LLMs in accuracy and relevance within specific domains. However, this\nevaluation was limited to specific questions and metrics and may not capture challenges in semantic\nsearch and other NLP tasks. Further research will explore different LLM architectures, RetA\nmethodologies, and evaluation methods to assess strengths and limitations more comprehensively.\nIntroduction\nThe development of large language models (LLMs), such as bidirectional encoder representations\nfrom transformer (BERT) and generative pre-trained transformer (GPT), has revolutionized the\nfield of natural language processing [1], [2], [3] [4]. Applications of these LLMs have ranged from\nsentiment analysis and machine translation to code generation and question answering in several\ndomains [5-10] – all demonstrating remarkable performance. However, despite their impressive\nexecution and widespread use, LLMs do not know the information they were not trained on, and\noften lack domain-specific knowledge and vocabulary. They can also perpetuate biases based on\nskewed content in the training data, and need to be further refined through reinforcement learning\nand alignment approaches to understand user intentions while making them more truthful and less\ntoxic [11, 12]. Furthermore, concerns have been raised about the potential for LLMs to generate\nhallucinated or misleading information, which can have severe implications in scientific research and\nled to the critical determinants of distinguishing fact from fiction leading to discontinuation of, as\nwas the case for Meta’s Galactica [13, 14].\n1\narXiv:2305.17116v2  [cs.CL]  30 May 2023\nPopular LLMs with billions of parameters such as GPT-3 [4] , PaLM [15], OPT [16], and LLaMA [17]\nare typically trained on vast amounts of information collected from the Internet (e.g. the Common\nCrawl dataset [18]) and capture a diverse range of language patterns and knowledge. Word and\nsentence embeddings are high-dimensional numerical representations of concepts scaled by the size\nof the corpus and complexity of language usage [19] [20, 21]. This can produce a higher level of\ngenerality and flexibility in the model's ability to yield natural language, making it more robust and\nadaptable to a range of applications. Similarly, a broad corpus can capture the diversity of language\nusage across different domains and genres. For example, a model trained on a broad corpus could\npotentially generate natural language in scientific literature, social media, or news articles, with\nequal ease [3].\nNonetheless, a wide-ranging corpus can inadvertently incorporate a significant amount of noise\nor irrelevant data, resulting in a reduced signal-to-noise ratio [22]. This may adversely affect the\ngenerated text's quality, leading to decreased coherence, meaning, or accuracy. Additionally, biases\nand inaccuracies may arise in the model's comprehension of natural language. A corpus that\npredominantly features one type of language or cultural context may display bias towards that\nspecific domain or culture [23]. Although a corpus may strive to encompass a diverse range of\ndomains, the sheer vastness of the domain space makes it currently unfeasible to include all relevant\ndomains. Moreover, as more domains are incorporated, there is a risk that LLMs trained on such\na comprehensive corpus may struggle to differentiate language from various domains, particularly\nwhen faced with prompts that lack sufficient context.\nOne approach to address these limitations is to retrain or finetune an LLM with a focused corpus\ntailored to a specific domain or application [22] [24] thereby reducing the risk of generating irrelevant\nor misleading information and enhancing the reliability and precision of the LLM's outputs in\nspecialized contexts. Numerous publications have highlighted the efficacy of domain specific LLMs\nin their respective fields. For example, BioBERT [25] targets biomedical text mining tasks, SciBERT\n[26] and PubMedBERT [27] address scientific literature, and Legal-BERT [28] specializes in legal\ntext processing. These approaches minimize noise and irrelevant information in the text, potentially\nreducing hallucinations.\nHowever, retraining LLMs to encompass new documents might be impractical due to the cumulative\ncomputational costs and data scientist resources required per update. The LLM architecture might\nalso need to be updated to incorporate more parameters to memorize more facts [29]. As LLMs\nhave demonstrated extraordinary abilities to learn in-context information purely from its prompt\n[4], RetA approaches have proven promising [24] [30]. These models first retrieve relevant context\nfrom domain-specific corpora based on a user query using lexical search (e.g. BM25 [31]) or a\npretrained/fine-tuned semantic retriever (e.g. Spider[32], OpenAI embeddings [33]), and then seed a\npre-trained LLM with such context to provide grounded answers while avoiding the prohibitive time\nand cost of retraining an LLM.\nIn this study, several LLMs were evaluated to investigate if a retrieval-augmentation approach on a\nfocused corpus could improve the accuracy of LLMs applications in biomedical Q&A. Three scoring\nmetrics were utilized to objectively compare outputs between models using a set of evaluation-based\nquestions focused on disease characterization, genetic subtypes, treatment options, and clinical\noutcomes in diffuse large B-cell lymphoma DLBCL. These observations provide insights into the\npros and cons of each LLM and suggest potential areas for improvement to meet utility requirements\nfor rigorous drug development and scientific research.\nMethods\nEvaluation framework\nThe performance of generically trained LLMs was tested versus a RetA LLM in question answering\n2\n(Q&A) tasks related to disease biology and drug development. A set of 19 questions focused on\nmechanisms and treatments associated with DLBCL were provided to evaluate LLM performance.\nThe questions covered a broad range of topics related to DLBCL disease biology including clinical and\nmolecular subtypes, genetic subsets and relevant biomarkers, clinical management, and standards\nof care and other available therapies. Questions were designed to look for both qualitative and\nquantitative answers (e.g. overall response rate and prevalence of genomic alterations). Each\nquestion was provided to four different LLMs: Open AI’s general ChatGPT-3.5 [34], OpenAI’s\ngeneral GPT-4 [34], Bing’s Prometheus model (referred to in this manuscript as Bing chat, based\non GPT-4 [35]), and a RetA LLM (based on GPT-3) using a custom set of full-text publications\nassociated with DLBCL (Table 1). The questions intentionally varied in detail to assess the ability\nof each LLM to infer the expected result. For example, question #15 provided a concise query for\nDLBCL diagnosis and prognosis, while question #3 asked specific treatments for a target in the\ndisease with accompanying references to support the answer.\nThe two general GPT-based LLMs from OpenAI were only trained on content up to September 2021\n(OpenAI GPT-4 Technical Report [36]), as opposed to Bing’s Prometheus and the RetA models.\nRelease versions of GPT-4 and GPT-3.5 used to answer the questions were from 3/23/23 to 4/28/23\n(updates were released on a weekly or bi-weekly basis and were documented).\nRetA model and dataset\nScientific papers were downloaded from PubMed Central (PMC [37]) using the Entrez E-utilities\n[38]). Each of the following queries was used to retrieve up to 500 articles: ‘diffuse large b-cell\nlymphoma’, ‘follicular lymphoma’, ‘epcoritamab’, ‘glofitamab’, ‘minimal residual disease’, ‘ctDNA’.\nBy default, Entrez returns articles sorted by PMC identifier. The queries used were meant to\ngenerate a corpus specific to DLBCL, related biomarkers, standards of care, and therapeutic options,\nnot to specifically answer the questions used in this evaluation. This created a unique dataset of\n1,868 full-text articles. The documents were first pre-processed to exclude potentially unstructured or\nnoisy text (e.g. figures, tables, references, author disclosure) and split into segments of 4,000 tokens.\nEmbeddings were then calculated using the OpenAI model text-embedding-ada-002 and stored in a\nlocal database. When the user entered a question, the query was transformed into an embedding\nvector and compared to the database of embeddings using cosine similarity. The topk document\nsegments by similarity were retrieved and formed the knowledge context for the user query. The\nsynthesis of the answer to the query was achieved in two stages: in stage one, text-davinci-003 was\nused to answer the query using each of thek context segments with prompt instructions to minimize\ninclusion of non-factual information from the LLM. This generatedk answers which were combined\ninto a final response in the second stage using a call to text-davinci-003 with a summarization\nprompt (Figure 1, Tables 2a,b).\nEvaluation metrics\nAnswers were scored for each question on a three-point scale (1-3, with 3 being highest) based on\nthree metrics: accuracy, relevance, and readability by eight independent reviewers (Table 3), with\neach reviewer scoring a subset of questions. Answers to all questions were searchable. Accuracy and\nrelevance assessments focused on factual correctness of answers, correctness of references or links to\nreferences, or general pieces of knowledge included or not included in an answer. The 3-point scale\nused for each evaluation category also allowed for some granularity in scoring answers. For example,\nan answer might be given a score of “2” if the result was factually correct but links to supporting\nreferences were broken or incorrect. An answer which does not directly address the question being\nasked or contains factually incorrect information (i.e. hallucinations) might garner a score of “1” for\naccuracy. As both the language model and oncology therapeutics fields are constantly evolving, there\nis some recency bias associated with answers to questions and the data which LLMs are trained on.\nThis was in part accounted for through the types of questions chosen and the scale used to assess\n3\nresponses. An emphasis of the evaluation was to specifically look for factually incorrect answers, as\nopposed to incomplete answers which may be a result of recency bias. Reviewers were all Ph.D. level\nscientists with an average of 8 years of biopharma industry experience and 11 years of post-doctoral\nwork experience. All scores were then assessed by one reviewer from the group to adjust for reviewer\nbiases. The prompts were stratified into three high level categories based on relevance to drug\ninformation, disease biology, and clinical information. Prompts were also grouped based on being\ngeneral (i.e. high level) or specific (i.e. asking for details) questions to better attribute subfield\nperformance within DLBCL in comparisons between LLMs.\nResults\nOverall, the performance of the LLMs varied widely across the different questions and metrics. In\nterms of accuracy, the RetA model of GPT-3 on DLBCL publications outperformed the other LLMs\nwith the highest (3-point) scoring answers on 12/19 questions. GPT-4 was the next best performer\nwith 3-point scores on 8/19 questions. Bing’s Prometheus had 7/19 3-point scores for accuracy while\nGPT-3.5 had the fewest high scoring answers (4/19 3-point scores) (Figures 2, 3). The summated\nscores for accuracy showed that the RetA model scored slightly higher than GPT-4 in the categories\nof drug and clinical information (Figure 4). Bing’s Prometheus model did not perform well in\naccuracy compared to all other models with low (1-point) scores on 10/19 questions(Figures 2, 3).\nThis was primarily due to misrepresentation of references in its answers. Conversely, GPT-4 and the\nRetA model had the fewest low scoring answers for accuracy (1/19 and 3/19 respectively) across\nprompts (Figures 2, 3).\nInterestingly, Bing’s Prometheus model was the only one to not score a value of 1 in accuracy for\nquestion #6 (“What is the overall response rate of DLBCL patients treated with glofitamab?”).\nNumerical overall and complete response rates (ORR and CRR, respectively) reported by GPT-3.5\n(ORR=65.1%, CRR=35.1%) and GPT-4 (ORR=62.7%, CRR=39.2%) were not consistent with\ntheir references cited and had either fabricated or provided incorrect references. Bing’s Prometheus\nmodel scored a value of 2 because there was a mixture of accurate and inaccurate answers to the\nquestion, i.e., this model accurately captured the ORR value of glofitamab treatment (52%) in\nDickinson et al, NEJM reference [39], but also incorrectly used the median duration of objective\nresponse rather than median duration of CR. The RetA model result was not accurate in answering\nthis question because the official glofitamab trial efficacy paper [39] was not available on PubMed\nCentral (https://www.ncbi.nlm.nih.gov/pmc/) and therefore not included in the corpus.\nIn terms of relevance, the RetA model performed slightly better than GPT-4 and GPT-3.5. The\nRetA model scored high (3-point) on 13/19 questions, compared to 11/19 and 10/19 in GPT-4 and\nGPT-3.5 respectively. Bing’s Prometheus model performed worst in this category with scores of\n1 in 8/19 questions(Figures 2, 3). The other three LLMs had few-to-no low scoring answers to\nprompts with respect to relevance. The irrelevant answers (i.e. low scoring questions) across all\nLLMs were primarily due to references to other diseases or treatment. For example, in question\n#14 (“Have checkpoint inhibitor treatments in monotherapy or combination therapy settings shown\nefficacy in DLBCL patients? Provide references.”), the GPT-4 model cited three references, one\nof which was in Hodgkin’s lymphoma (DLBCL is a non-Hodgkin’s lymphoma) and another that\ndiscussed CAR-T, which is not a checkpoint inhibiting drug agent, though the model associated this\ntreatment modality with immunotherapies and extended relevance to CAR-T therapies. GPT-3.5\nalso cited a reference evaluating a checkpoint inhibitor treatment in Hodgkin’s lymphoma.\nFinally, for readability, GPT-4 scored the highest with 17/19 scores of 3, followed by GPT-3.5 with\n15/19, and the RetA model with 11/19(Figures 2, 3). The summated scores demonstrated parity\nbetween GPT-4 and GPT-3.5 across all categories (Figure 4). Readability was particularly low\nscoring in the clinical category of questions for the RetA. Bing’s Prometheus model once again\nscored last in this category (7/19 3-point scores), primarily due to concise, yet vague answers, often\n4\nwith little detail. For example, for question #7 (“What is a treatment to use in DLBCL patients\nwho have progressed on CAR-T?”), Bing’s Prometheus model simply reported references without\nsummarization, including one study where multiple drugs were approved, and referenced only those\nof approved agents, ignoring studies evaluating investigational drug agents.\nAcross the 19 questions, both GPT-3.5 and GPT-4 LLMs generated a considerably higher number\nof hallucinations in their responses (31 from 13 questions and 19 from 8 questions, respectively)\ncompared to the RetA model and Bing’ Prometheus model (3 from 3 questions and 2 from 1\nquestion, respectively). These were primarily associated with fabrication of both references and\nclinical results. Although LLMs are known to be behind in mathematical capabilities [40], the\ninaccuracy of numerical results appeared to be due to hallucinations or context understanding rather\nthan limitations in mathematical reasoning.\nThese results suggest that the performance of LLMs can vary widely depending on the specific\ntask and domain, though the RetA model enhanced with domain-specific data may outperform\nmore general-purpose LLMs in accuracy and relevance. However, it should be noted that this\nevaluation was limited to a specific set of questions and metrics, and further research is needed\nto fully understand the strengths and limitations of different LLMs for semantic search and other\nnatural language processing tasks.\nDiscussion\nThe advantages and drawbacks of using LLMs trained on broad corpora versus a RetA approach\nultimately depend on the specific use case and desired outcomes. In biomedical and healthcare\nresearch, it is paramount to have accurate, relevant, and unbiased information supported by published\nliterature. In this study, quantifying the accuracy and utility of LLMs was conducted for answering\nqualitative and quantitative biomedical questions related to the treatment and prognosis of patients\nwith DLBCL. Results here demonstrated that the RetA LLM performed better on biomedical-specific\ntasks than the other LLMs evaluated, specifically with respect to accuracy of results. This suggests\nthat RetA LLMs can provide more accurate and reliable information for specific fields, reducing\nthe likelihood of generating irrelevant or misleading outputs, while maintaining the flexibility and\nadaptability of a general LLM.\nOne major advantage of the RetA model is the easy integration of new domain knowledge that\nthe base LLM was not trained on. When a new document is added to the corpus, the model only\nneeds to calculate the embeddings to facilitate retrieval during future queries. On the other hand,\nfine-tuning or retraining an LLM on a new corpus takes both time and resources, and may not\nalways be possible depending on the choice of LLM – as of the publication of this study, OpenAI\nhas not offered an option to fine-tune their ChatGPT models; Meta’s LLaMA model is also not\navailable for commercial applications [17, 41].\nHowever, since the RetA model needs to prompt a pre-trained LLM into performing specific tasks\nsuch as summarizing across relevant documents and extracting information without using prior\nknowledge, the model typically uses a large amount of tokens as input and multiple iterations of\nbase LLM inference (i.e. text-completion API) calls, which can increase the compute cost in its\napplication. The dependence on a certain LLM (e.g. OpenAI GPT-3) also implies that the desired\nprompt behavior needs to be closely monitored when the LLM backend is updated with new training\ndata, or when the user switches to a different base LLM (e.g. GPT-4, Dolly 2 [42], Open Assistant\n[43], or RedPajama [44]).\nFurthermore, its performance is also bound by the limitations of the base LLM’s vocabulary\n(tokenizer) and internal representation of concepts (embedding). For example, question #13 asked\nabout minimal residual disease (MRD) in DLBCL, but the document retriever returned articles\nabout MRD in multiple myeloma and chronic lymphocytic leukemia - two distinct hematological\n5\nmalignancies from DLBCL. The RetA model relies on GPT-3 as the summarization engine which\nfailed to distinguish between the different disease types, leading to an incorrect answer. These issues\nmay be ameliorated by utilizing more sophisticated document retrieval methods. For biomedical\nliterature, domain specific models such as BioBERT and PubMedBERT can be used for tokenization\nand embedding calculation; additional metadata filters can also be used to improve relevance of\nretrieved documents. As an example, when the retrieval method was modified in the RetA model to\ndirectly search for supporting articles on PubMed by significance, the model provided informative\nand relevant answers detailing the measurement of disease clones with V(D)J sequences, as well as\nthe association with clinical outcomes.\nOverall, general LLMs provide highly readable and coherent text in various subjects. Furthermore,\nthe performance of the RetA model demonstrated the utility of using LLMs as a backend in\nperforming various reasoning tasks through specifically crafted prompts. Indeed, prompt engineering\nhas been an active area of research that continues to expand the capability of pre-trained LLMs\nthrough methods such as: zero-shot [45], few-shot [4], chain of thought [46], self-ask [47], and ReAct\n[48] reasoning. These reasoning properties allow LLMs to be used as programmable agents to\norchestrate and perform tasks across different modalities or domains (e.g. ToolFormer [49], Visual\nChatGPT [50], Langchain [51], GPT plugins [52]).\nThough findings here are informative, this study had several limitations that need to be considered.\nFirst, the assessment included only 19 questions, which accounted for various clinical, therapy,\nand biological content, which was an attempt to address pertinent context in biomedical research,\nthough certainly not exhaustive. Second, the focus was on a single disease (DLBCL), which may\nnot be generalizable to other diseases or domains. Third, the scoring metrics selected included\naccuracy, readability, and relevance, which might not have captured other important aspects of\nthe text such as strength, completeness, and consistency. The scoring was performed across the\nentire answer as opposed to by sentence or phrase within an answer. While scoring questions in this\nmanner can be subjective, we adjusted for this by using multiple reviewers and having an additional\noverarching review to calibrate scores across questions. There was also a range in experience among\nreviewers to account for any bias associated with experience. Questions were also specific enough\nsuch that available literature could be used to assess accuracy of answers. A point of emphasis\nfor the evaluation of responses was to look for factually incorrect answers (hallucinations), which\nwere more likely to garner the lowest score, as opposed to answers which were factually correct but\nnot exhaustive. Last, the RetA model included an arbitrary number of full-text articles (1,868),\nprioritized by PMC identifier, which might not have represented the most relevant or comprehensive\nset of articles for the disease. It is possible that an optima of accuracy, relevance, and readability\ncan be achieved with an RetA model by increasing the size and breadth of the corpus, and future\nwork will be needed to test this hypothesis. Despite these limitations, this study provides valuable\ninsights into the performance of LLMs on different types of corpora and highlights the importance\nof domain-specific knowledge in achieving higher accuracy and relevance.\nWith the rapid advancement and development of foundation models across text, image, video\nand other data modalities, adaptation of AI in a fair, accurate, and reliable fashion can make an\nimmediate impact on healthcare and drug development. In this study, focus was on evaluation of\npre-trained and RetA LLMs for biomedical Q&A in the field of clinical drug development. Future\nresearch could explore methods incorporating biomedical ontology, knowledge graphs, as well as\nother agent-based approaches to further enhance the performance of LLMs [51], [52]. As open-source\ninitiatives democratize AI research [53, 54][42] [43] [44] and new emerging methodologies [55-58] begin\nto offer possibilities to build custom LLMs with reduced compute resources and time requirements,\nfurther integrating with multi-modal approaches that leverage across molecular (e.g. mutations and\ngene expression), imaging (pathology and radiology), electronic health records, and wearable sensor\ndata will provide a deeper understanding of disease biology and accelerate drug development in a\n6\nfair and socially responsible way [59] [60].\nAcknowledgements\nThe authors thank Bryan Ho and Swathi Vangala for their contributions to infrastructure.\nFigures and Tables\nFigure 1. Components and workflow of a RetA LLM. The pre-processing stage splits documents\ninto smaller chunks, creates embeddings and stores them in a database. At the querying stage, a\ndocument retriever finds the most relevant documents in the embeddings database, iteratively seeds\nthe base LLM with context to generate a response.\nTable 1. Questions used for LLM evaluation classified into group and scope categories.\nQuestion # Question Group Scope\n1 What is epcoritamab? Please provide sources for\nyour answer.\nDrug information General\n2 What are the subtypes of DLBCL? Please\nprovide sources for your answer.\nDisease biology General\n3 What are the antibody therapies targeting CD20\nfor treatment of DLBCL? Please provide sources\nfor your answer.\nDrug information General\n4 What is the standard of care for treatment of\nDLBCL?\nClinical\ninformation\nSpecific\n5 What are the approved drugs for treatment of\nDLBCL?\nClinical\ninformation\nSpecific\n6 What is the overall response rate of DLBCL\npatients treated with glofitamab?\nClinical\ninformation\nSpecific\n7 What is a treatment to use in DLBCL patients\nwho have progressed on CAR-T?\nDrug information General\n8 What are common treatments used in patients\nwho have relapsed or were refractory to standard\nof care treatments in DLBCL?\nDrug information General\n9 Do any DLBCL patient subtypes respond more\nfavorably to chemotherapy or CAR-T\ntreatments?\nClinical\ninformation\nSpecific\n7\nQuestion # Question Group Scope\n10 What are the most common adverse events\nobserved in DLBCL patients treated with\nR-CHOP?\nClinical\ninformation\nSpecific\n11 What biomarkers in DLBCL have been reported\nto correlate with either response or progression\nfollowing treatment with R-CHOP?\nClinical\ninformation\nSpecific\n12 What treatment combinations have been shown\nto be effective in DLBCL patients who have\nprogressed on CAR-T treatment? Please provide\nsources for your answer.\nClinical\ninformation\nSpecific\n13 How can minimal residual disease (MRD) be\nused to understand clinical outcomes in DLBCL\npatients? Please provide sources for your answer.\nDisease biology General\n14 Have checkpoint inhibitor treatments in\nmonotherapy or combination therapy settings\nshown efficacy in DLBCL patients? Provide\nreferences.\nDrug information Specific\n15 DLBCL diagnosis and prognosis. Clinical\ninformation\nGeneral\n16 Landscape of DLBCL treatment as SOC. Please\nprovide sources for your answer.\nClinical\ninformation\nSpecific\n17 Emerging novel treatment options for DLBCL\npatients.\nDrug information General\n18 what is the importance of TP53 in DLBCL? Disease biology General\n19 What is the prevalence of double hit mutations in\nlymphoma?\nDisease biology Specific\nTable 2a. Prompts for GPT3 in the RetA workflow.\nStage Prompt\nStage\none\nInstruction: You are a truthful AI assistant. You answer questions only based on\nprovided context below. If the context is not relevant to the question, say you do not\nknow the answer. No need to explain why.\nContext: {segment of article}\nQuestion: {user query}\nAnswer:\nStage\ntwo\nPlease combine the following paper's summaries. Only use the context below and not\nincorporate any prior knowledge.\nPaper #1: {answer 1 based on segment 1}\nPaper #2: {answer 2 based on segment 2}\nTable 2b. Workflow and LLM descriptions used in this study.\nWorkflow Evaluation Base LLM\nRetA LLM Python workflow text-davinci-003\nchatGPT3.5 OpenAI web gpt-3.5-turbo\n8\nWorkflow Evaluation Base LLM\nchatGPT4 OpenAI web gpt-4\nBingChat Microsoft web Custom GPT4\nTable 3. Answer scoring metric descriptions for LLM comparison.\nScore\nMetrics 1 2 3\nAccuracy Mostly inaccurate or\nmisleading content\nA mix of accurate and\ninaccurate content\nFactually accurate and\nreliable content\nRelevance Mostly irrelevant\ncontent\nPartially relevant\ncontent\nHighly relevant and\non-point content\nReadability Difficult to read, unclear\nor convoluted language\nModerately readable,\nwith some unclear\npassages\nEasy to read, clear, and\nconcise language\n9\nFigure 2. Scores for each LLM within 3 metrics (accuracy, relevance, readability) on a three-point\nscale. Questions are ordered by question category (clinical, drug-related, disease-related). Question\nscope (general or specific) is also annotated.\nFigure 3. Count of scores (3-point, 2-point, and 1-point) across the 19 questions for each LLM in\neach score category (Accuracy, Readability, Relevance).\n10\nFigure 4. Summarized scores for each LLM within 3 metrics (accuracy, relevance, readability) and\nquestion categories (clinical information, disease biology, and drug information).\nReferences\n1. Vaswani, A., et al. Attention Is All You Need . 2017. arXiv:1706.03762 DOI:\n10.48550/arXiv.1706.03762.\n2. Devlin, J., et al.BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\nstanding. 2018. arXiv:1810.04805 DOI: 10.48550/arXiv.1810.04805.\n3. Radford, A., et al.Language Models are Unsupervised Multitask Learners. 2019.\n4. Brown, T.B., et al. Language Models are Few-Shot Learners. 2020. arXiv:2005.14165 DOI:\n10.48550/arXiv.2005.14165.\n5. Chen, M., et al.Evaluating Large Language Models Trained on Code. 2021. arXiv:2107.03374\nDOI: 10.48550/arXiv.2107.03374.\n6. Gozalo-Brizuela, R. and E.C. Garrido-MerchanChatGPT is not all you need. A State of the Art\nReview of large Generative AI models. 2023. arXiv:2301.04655 DOI: 10.48550/arXiv.2301.04655.\n7. Phuong, M. and M. HutterFormal Algorithms for Transformers. 2022. arXiv:2207.09238 DOI:\n10.48550/arXiv.2207.09238.\n8. Services, B.P.,Introducing BloombergGPT, Bloomberg’s 50-billion parameter large language model,\npurpose-built from scratch for finance. 2023.\n11\n9. Wu, S., et al.BloombergGPT: A Large Language Model for Finance. 2023. arXiv:2303.17564\nDOI: 10.48550/arXiv.2303.17564.\n10. Yang, X., et al.,A large language model for electronic health records.NPJ Digit Med, 2022.\n5(1): p. 194.\n11. Chung, H.W., et al.Scaling Instruction-Finetuned Language Models. 2022. arXiv:2210.11416\nDOI: 10.48550/arXiv.2210.11416.\n12. Ouyang, L., et al.,Training language models to follow instructions with human feedback.ArXiv,\n2022. abs/2203.02155.\n13. Heaven, W.D.Why Meta’s latest large language model survived only three days online. 2022;\nAvailable from: https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-\nmodel-ai-only-survived-three-days-gpt-3-science/.\n14. Taylor, R., et al.Galactica: A Large Language Model for Science. 2022. arXiv:2211.09085 DOI:\n10.48550/arXiv.2211.09085.\n15. Chowdhery, A., et al.PaLM: Scaling Language Modeling with Pathways. 2022. arXiv:2204.02311\nDOI: 10.48550/arXiv.2204.02311.\n16. Zhang, S., et al.OPT: Open Pre-trained Transformer Language Models. 2022. arXiv:2205.01068\nDOI: 10.48550/arXiv.2205.01068.\n17. Touvron, H., et al. LLaMA: Open and Efficient Foundation Language Models. 2023.\narXiv:2302.13971 DOI: 10.48550/arXiv.2302.13971.\n18. Common Crawl. Available from: https://commoncrawl.org/.\n19. Mikolov, T., et al.Efficient Estimation of Word Representations in Vector Space. inInternational\nConference on Learning Representations. 2013.\n20. Pennington, J., R. Socher, and C. Manning.GloVe: Global Vectors for Word Representation.\n2014. Doha, Qatar: Association for Computational Linguistics.\n21. Reimers, N. and I. Gurevych.Sentence-BERT: Sentence Embeddings using Siamese BERT-\nNetworks. 2019. Hong Kong, China: Association for Computational Linguistics.\n22. Gururangan, S., et al.Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks.\n2020. Online: Association for Computational Linguistics.\n23. Bender, E.M., et al.,On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?,\nin Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021,\nAssociation for Computing Machinery: Virtual Event, Canada. p. 610–623.\n24. Guu, K., et al.,Retrieval Augmented Language Model Pre-Training, inProceedings of the 37th\nInternational Conference on Machine Learning, D. Hal, III and S. Aarti, Editors. 2020, PMLR:\nProceedings of Machine Learning Research. p. 3929--3938.\n25. Lee, J., et al.,BioBERT: a pre-trained biomedical language representation model for biomedical\ntext mining. Bioinformatics, 2019. 36(4): p. 1234-1240.\n26. Beltagy, I., K. Lo, and A. Cohan.SciBERT: A Pretrained Language Model for Scientific Text.\n2019. Hong Kong, China: Association for Computational Linguistics.\n27. Gu, Y., et al.,Domain-Specific Language Model Pretraining for Biomedical Natural Language\nProcessing. ACM Trans. Comput. Healthcare, 2021.3(1): p. Article 2.\n12\n28. Chalkidis, I., et al.LEGAL-BERT: The Muppets straight out of Law School. 2020. Online:\nAssociation for Computational Linguistics.\n29. Kaplan, J., et al. Scaling Laws for Neural Language Models. 2020. arXiv:2001.08361 DOI:\n10.48550/arXiv.2001.08361.\n30. Ram, O., et al.In-Context Retrieval-Augmented Language Models. 2023. arXiv:2302.00083 DOI:\n10.48550/arXiv.2302.00083.\n31. Robertson, S. and H. Zaragoza,The Probabilistic Relevance Framework: BM25 and Beyond.\nFound. Trends Inf. Retr., 2009.3(4): p. 333–389.\n32. Ram, O., et al.Learning to Retrieve Passages without Supervision. 2022. Seattle, United States:\nAssociation for Computational Linguistics.\n33. Neelakantan, A., et al. Text and Code Embeddings by Contrastive Pre-Training. 2022.\narXiv:2201.10005 DOI: 10.48550/arXiv.2201.10005.\n34. OpenAI. ChatGPT. Available from: https://chat.openai.com/.\n35. Microsoft, Bing.\n36. OpenAI GPT-4 Technical Report. 2023. arXiv:2303.08774 DOI: 10.48550/arXiv.2303.08774.\n37. NCBI. PMC. Available from: https://www.ncbi.nlm.nih.gov/pmc/.\n38. NCBI, Entrez E-utilities.\n39. Dickinson, M.J., et al.,Glofitamab for Relapsed or Refractory Diffuse Large B-Cell Lymphoma.\nNew England Journal of Medicine, 2022.387(24): p. 2220-2231.\n40. Frieder, S., et al. Mathematical Capabilities of ChatGPT. 2023. arXiv:2301.13867 DOI:\n10.48550/arXiv.2301.13867.\n41. Research, F.llama github page. Available from: https://github.com/facebookresearch/llama.\n42. Mike Conover, M.H., Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali\nGhodsi, Patrick Wendell, Matei Zaharia and Reynold Xin,Free Dolly: Introducing the World's First\nTruly Open Instruction-Tuned LLM. 2023, Databricks.\n43. Open Assistant. Available from: https://open-assistant.io/.\n44. RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training\ndataset of over 1.2 trillion tokens. 2023.\n45. Kojima, T., et al.Large Language Models are Zero-Shot Reasoners. 2022. arXiv:2205.11916\nDOI: 10.48550/arXiv.2205.11916.\n46. Wei, J., et al.Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2022.\narXiv:2201.11903 DOI: 10.48550/arXiv.2201.11903.\n47. Press, O., et al.Measuring and Narrowing the Compositionality Gap in Language Models. 2022.\narXiv:2210.03350 DOI: 10.48550/arXiv.2210.03350.\n48. Yao, S., et al. ReAct: Synergizing Reasoning and Acting in Language Models. 2022.\narXiv:2210.03629 DOI: 10.48550/arXiv.2210.03629.\n49. Schick, T., et al. Toolformer: Language Models Can Teach Themselves to Use Tools. 2023.\narXiv:2302.04761 DOI: 10.48550/arXiv.2302.04761.\n50. Wu, C., et al.Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models.\n2023. arXiv:2303.04671 DOI: 10.48550/arXiv.2303.04671.\n13\n51. LangChain. Available from: https://python.langchain.com/en/latest/.\n52. ChatGPT plugins. Available from: https://openai.com/blog/chatgpt-plugins.\n53. Biderman, S., et al.Pythia: A Suite for Analyzing Large Language Models Across Training and\nScaling. 2023. arXiv:2304.01373 DOI: 10.48550/arXiv.2304.01373.\n54. Köpf, A., et al.OpenAssistant Conversations -- Democratizing Large Language Model Alignment.\n2023. arXiv:2304.07327 DOI: 10.48550/arXiv.2304.07327.\n55. Hu, E.J., et al.LoRA: Low-Rank Adaptation of Large Language Models. 2021. arXiv:2106.09685\nDOI: 10.48550/arXiv.2106.09685.\n56. Lester, B., R. Al-Rfou, and N. ConstantThe Power of Scale for Parameter-Efficient Prompt\nTuning. 2021. arXiv:2104.08691 DOI: 10.48550/arXiv.2104.08691.\n57. Li, X.L. and P. Liang.Prefix-Tuning: Optimizing Continuous Prompts for Generation. 2021.\nOnline: Association for Computational Linguistics.\n58. Liu, X., et al.P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\nAcross Scales and Tasks. 2021. arXiv:2110.07602 DOI: 10.48550/arXiv.2110.07602.\n59. Moor, M., et al.,Foundation models for generalist medical artificial intelligence.Nature, 2023.\n616(7956): p. 259-265.\n60. Acosta, J.N., et al.,Multimodal biomedical AI.Nat Med, 2022.28(9): p. 1773-1784.\n14",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.805311381816864
    },
    {
      "name": "Relevance (law)",
      "score": 0.6418561339378357
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5011930465698242
    },
    {
      "name": "Natural language processing",
      "score": 0.49536120891571045
    },
    {
      "name": "Machine learning",
      "score": 0.35476964712142944
    },
    {
      "name": "Medicine",
      "score": 0.34183844923973083
    },
    {
      "name": "Computer science",
      "score": 0.26555800437927246
    },
    {
      "name": "Programming language",
      "score": 0.08460500836372375
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I266595603",
      "name": "Genmab (United States)",
      "country": "US"
    }
  ],
  "cited_by": 3
}