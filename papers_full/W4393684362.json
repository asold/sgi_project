{
  "title": "Understanding Large Language Models : Towards Rigorous and Targeted Interpretability Using Probing Classifiers and Self-Rationalisation",
  "url": "https://openalex.org/W4393684362",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2951011333",
      "name": "Jenny Kunz",
      "affiliations": [
        "Linköping University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2529194139",
    "https://openalex.org/W6752948792",
    "https://openalex.org/W6753345517",
    "https://openalex.org/W4385571986",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3163443091",
    "https://openalex.org/W2251803266",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2891012317",
    "https://openalex.org/W4231799185",
    "https://openalex.org/W1539309091",
    "https://openalex.org/W3043865406",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W6803096969",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W6815599981",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2951306478",
    "https://openalex.org/W2739827909",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W6676391964",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W2970442950",
    "https://openalex.org/W3177271869",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W7000626104",
    "https://openalex.org/W6650144174",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2947469743",
    "https://openalex.org/W4303648884",
    "https://openalex.org/W2942630857",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2301095666",
    "https://openalex.org/W4321645967",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W6826842082",
    "https://openalex.org/W3015770160",
    "https://openalex.org/W2946444107",
    "https://openalex.org/W2740285126",
    "https://openalex.org/W2178314882",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W6658697210",
    "https://openalex.org/W2618851150",
    "https://openalex.org/W3212748247",
    "https://openalex.org/W4387075507",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2670253439",
    "https://openalex.org/W4236490218",
    "https://openalex.org/W2898694742",
    "https://openalex.org/W6851781724",
    "https://openalex.org/W6653414727",
    "https://openalex.org/W4316135772",
    "https://openalex.org/W2785327160",
    "https://openalex.org/W2953035981",
    "https://openalex.org/W4210723357",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963609017",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2912512851",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2054658115",
    "https://openalex.org/W2739638526",
    "https://openalex.org/W2945526235",
    "https://openalex.org/W4385573608",
    "https://openalex.org/W4386576819",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3005086430",
    "https://openalex.org/W4382202746",
    "https://openalex.org/W4381573334",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2148437670",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W4230230267",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4287854450",
    "https://openalex.org/W3196392173",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W2913115534",
    "https://openalex.org/W3111165175",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3101422495",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2162303522",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2952984539",
    "https://openalex.org/W4287889353",
    "https://openalex.org/W2164998307",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2155274384",
    "https://openalex.org/W2130059423",
    "https://openalex.org/W4244520098",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W4293768783",
    "https://openalex.org/W2180885055",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W4385573797",
    "https://openalex.org/W253665904",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2146003320",
    "https://openalex.org/W3107855336",
    "https://openalex.org/W3206132631",
    "https://openalex.org/W2948698627",
    "https://openalex.org/W3102035862",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034806906",
    "https://openalex.org/W2034637247",
    "https://openalex.org/W3103054319",
    "https://openalex.org/W4287887656",
    "https://openalex.org/W4287324238",
    "https://openalex.org/W1874182513",
    "https://openalex.org/W2111671092",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4244006142",
    "https://openalex.org/W2098766651",
    "https://openalex.org/W4391505309",
    "https://openalex.org/W2556252833",
    "https://openalex.org/W2842624112",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W4389518688",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W630529707",
    "https://openalex.org/W4385572950",
    "https://openalex.org/W3173650485",
    "https://openalex.org/W4380366109",
    "https://openalex.org/W3153046263",
    "https://openalex.org/W3092292656",
    "https://openalex.org/W2119858020",
    "https://openalex.org/W2152184085",
    "https://openalex.org/W189558000",
    "https://openalex.org/W4388685652",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3102812725",
    "https://openalex.org/W3116152597",
    "https://openalex.org/W4224950688",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2754136456",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W2011906570",
    "https://openalex.org/W4391673261",
    "https://openalex.org/W3110909889",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W2105926775",
    "https://openalex.org/W249567354",
    "https://openalex.org/W4287890953",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W4226325987",
    "https://openalex.org/W2102293055",
    "https://openalex.org/W600465858",
    "https://openalex.org/W4233691971",
    "https://openalex.org/W2970863760",
    "https://openalex.org/W3212191244",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2164114436",
    "https://openalex.org/W1518672703",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2250189634",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2964020032",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W4288359148",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W4220693086",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W195941064",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W3155744586",
    "https://openalex.org/W4391940854",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W3211602321",
    "https://openalex.org/W1496347916",
    "https://openalex.org/W4285310310",
    "https://openalex.org/W4385570095",
    "https://openalex.org/W3099954305",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2148381207",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W4287708700",
    "https://openalex.org/W4362655601",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W2104114539",
    "https://openalex.org/W158494842",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2792922767",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3101155149",
    "https://openalex.org/W3175591618",
    "https://openalex.org/W2164119315",
    "https://openalex.org/W2583774121",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W2970155250",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4297687090",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W4385849240",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3035503910"
  ],
  "abstract": "Large language models (LLMs) have become the base of many natural language processing (NLP) systems due to their performance and easy adaptability to various tasks. However, much about their inner workings is still unknown. LLMs have many millions or billions of parameters, and large parts of their training happen in a self-supervised fashion: They simply learn to predict the next word, or missing words, in a sequence. This is effective for picking up a wide range of linguistic, factual and relational information, but it implies that it is not trivial what exactly is learned, and how it is represented within the LLM. In this thesis, I present our work on methods contributing to better understanding LLMs. The work can be grouped into two approaches. The first lies within the field of interpretability, which is concerned with understanding the internal workings of the LLMs. Specifically, we analyse and refine a tool called probing classifiers that inspects the intermediate representations of LLMs, focusing on what roles the various layers of the neural model play. This helps us to get a global understanding of how information is structured in the model. I present our work on assessing and improving the probing methodologies. We developed a framework to clarify the limitations of past methods, showing that all common controls are insufficient. Based on this, we proposed more restrictive probing setups by creating artificial distribution shifts. We developed new metrics for the evaluation of probing classifiers that move the focus from the overall information that the layer contains to differences in information content across the LLM. The second approach is concerned with explainability, specifically with self-rationalising models that generate free-text explanations along with their predictions. This is an instance of local understandability: We obtain justifications for individual predictions. In this setup, however, the generation of the explanations is just as opaque as the generation of the predictions. Therefore, our work in this field focuses on better understanding the properties of the generated explanations. We evaluate the downstream performance of a classifier with explanations generated by different model pipelines and compare it to human ratings of the explanations. Our results indicate that the properties that increase the downstream performance differ from those that humans appreciate when evaluating an explanation. Finally, we annotate explanations generated by an LLM for properties that human explanations typically have and discuss the effects those properties have on different user groups. While a detailed understanding of the inner workings of LLMs is still unfeasible, I argue that the techniques and analyses presented in this work can help to better understand LLMs, the linguistic knowledge they encode and their decision-making process. Together with knowledge about the models’ architecture, training data and training objective, such techniques can help us develop a robust high-level understanding of LLMs that can guide decisions on their deployment and potential improvements.",
  "full_text": "Understanding Large\nLanguage Models\nLinköping Studies in Science and Technology\nDissertation No. 2364\nJenny Kunz\nJenny Kunz         Understanding Large Language Models                      2024\nFACULTY OF SCIENCE AND ENGINEERING\nLinköping Studies in Science and Technology, Dissertation No. 2364, 2024\nDepartment of Computer and Information Science\nLinköping University\nSE-581 83 Linköping, Sweden\nwww.liu.se Towards Rigorous and Targeted Interpretability\nUsing Probing Classifiers and Self-Rationalisation\n\nLinköping Studies in Science and Technology\nDissertations, No. 2364\nUnderstanding Large Language Models:\nTowards Rigorous and Targeted Interpretability\nUsing Probing Classifiers and Self-Rationalisation\nJenny Kunz\nLinköping University\nDepartment of Computer and Information Science\nDivision of Artificial Intelligence and Integrated Computer Systems\nSE-581 83 Linköping, Sweden\nLinköping 2024\nTypeset using LATEX\nCover illustration by Max Trembczyk and Jenny Kunz, with assistance of the\nDALL\u0004E 2 image generation model.\nPrinted by LiU-Tryck, Linköping 2024\nEdition 1:1\n© Jenny Kunz, 2024\nISBN 978–91–8075–470–5 (print)\nISBN 978–91–8075–471–2 (PDF)\nhttps://doi.org/10.3384/9789180754712\nISSN 0345-7524\nPublished articles have been reprinted with permission from the respective\ncopyright holder.\nii\nThis work is licensed under a Creative Commons Attribution 4.0 \nInternational License. \nhttps://creativecommons.org/licenses/by/4.0/\nPOPULÄRVETENSKAPLIG SAMMANFATTNING\nNeuronala språkmodeller ligger bakom många vardagliga användningar, bland annat mjukvara\nsom kontrollerar stavning och grammatik och som kompletterar text. De mest omtalade systemen\njust nu är dock chatbotar som ChatGPT. Sådana modeller har många imponerande färdigheter:\nMan kan ställa olika typer av frågor till dem, och svaren är ofta både korrekta och välskrivna.\nChatbotar kan även hjälpa med textsammanfattningar, med att omstrukturera och förbättra text,\nmed programmeringsfrågor och med många andra uppgifter.\nEn utmaning med dagens språkmodeller är att de är så stora och tränade på så pass stora\nmängder text att det är omöjligt att fullt ut förstå hur de fungerar och varför man får ett visst\nsvar. Den största delen av sina färdigheter får modellerna från så kallad självövervakad träning:\nDe är inte tränade på att svara på frågor utan att på att predicera nästa ord i en text (eller ord\nsom saknas någonstans i texten) och lär sig själv vilken information som är viktig för att lösa\nden konceptuellt enkla uppgiften. Självövervakad träning har visat sig vara effektiv för att\nmodellerna ska tillägna sig kunskap om lingvistik och fakta, men det är inte trivialt att veta exakt\nvilka kunskaper och egenskaper modellen får. En sådan förståelse är dock nödvändig för att\nkunna förbättra modellerna och för att bedöma när de är pålitliga och kan användas med gott\nsamvete.\nSyftet med den här avhandlingen är att utveckla, förbättra och utvärdera metoder inom för-\nklarbar språkteknologi. Jag presenterar vårt arbete inom två delområden: interpretation av\nmodellernas interna representationer och generering av förklaringar för individuella svar. I\ndet första delområdet har vi utvecklat en metod för att förstå begränsningarna med en populär\nmetod inom modellinterpretation som kallas för probing. Med hänsyn till dessa begränsningar\nhar vi utvecklat metoder för att göra probing mer utmanande samt nya metriker för att mäta\nspråkmodellers kvalitet. I det andra delområdet är fokusen på språkmodeller som har förmågan\natt kunna generera förklaringar för sina svar. Vi har systematiserat egenskaper som förklaringar\nsom genereras av människor har och undersökt om de även finns i automatiskt genererade förkla-\nringarna. Våra resultat visar att vissa egenskaper förekommer hos en stor del av de genererade\nförklaringarna, speciellt olika former av ofullständighet och illustrerande element. Subjektivitet\nförekommer mycket mer sällan i genererade förklaringar, antagligen för att ett senare steg i\nmodellernas träningsprocess stävjar den här egenskapen. Mer generellt kan vi konstatera att\nmänskliga användare och tillämpningssystem som bygger på språkmodeller har olika behov\nmed avseende på vilka egenskaper dessa förklaringar ska ha.\nVår forskning bidrar till en bättre förståelse av språkmodeller, men det är viktigt att vara tydlig\nmed att vi är långt ifrån en detaljerad förståelse. Kompletterad med kunskap om modellens\narkitektur, träningsdata och träningsmål kan kunskapen och metoderna dock vara till hjälp för\natt formulera och testa hypoteser för hur modellerna beter sig.\niii\nABSTRACT\nLarge language models (LLMs) have become the base of many natural language processing (NLP)\nsystems due to their performance and easy adaptability to various tasks. However, much about\ntheir inner workings is still unknown. LLMs have many millions or billions of parameters, and\nlarge parts of their training happen in a self-supervised fashion: They simply learn to predict\nthe next word, or missing words, in a sequence. This is effective for picking up a wide range of\nlinguistic, factual and relational information, but it implies that it is not trivial what exactly is\nlearned, and how it is represented within the LLM.\nIn this thesis, I present our work on methods contributing to better understanding LLMs. The\nwork can be grouped into two approaches. The first lies within the field of interpretability, which\nis concerned with understanding the internal workings of the LLMs. Specifically, we analyse and\nrefine a tool called probing classifiers that inspects the intermediate representations of LLMs,\nfocusing on what roles the various layers of the neural model play. This helps us to get a global\nunderstanding of how information is structured in the model. I present our work on assessing\nand improving the probing methodologies. We developed a framework to clarify the limitations\nof past methods, showing that all common controls are insufficient. Based on this, we proposed\nmore restrictive probing setups by creating artificial distribution shifts. We developed new\nmetrics for the evaluation of probing classifiers that move the focus from the overall information\nthat the layer contains to differences in information content across the LLM.\nThe second approach is concerned with explainability, specifically with self-rationalising models\nthat generate free-text explanations along with their predictions. This is an instance of local\nunderstandability: We obtain justifications for individual predictions. In this setup, however, the\ngeneration of the explanations is just as opaque as the generation of the predictions. Therefore, our\nwork in this field focuses on better understanding the properties of the generated explanations.\nWe evaluate the downstream performance of a classifier with explanations generated by different\nmodel pipelines and compare it to human ratings of the explanations. Our results indicate that the\nproperties that increase the downstream performance differ from those that humans appreciate\nwhen evaluating an explanation. Finally, we annotate explanations generated by an LLM for\nproperties that human explanations typically have and discuss the effects those properties have\non different user groups.\nWhile a detailed understanding of the inner workings of LLMs is still unfeasible, I argue that\nthe techniques and analyses presented in this work can help to better understand LLMs, the\nlinguistic knowledge they encode and their decision-making process. Together with knowledge\nabout the models’ architecture, training data and training objective, such techniques can help us\ndevelop a robust high-level understanding of LLMs that can guide decisions on their deployment\nand potential improvements.\niv\nAcknowledgments\nSince I came to Linköping five years ago, a lot of people have contributed to the work on this\nthesis and to making my PhD experience enjoyable and fun.\nFirst and foremost, I want to thank my supervisor Marco Kuhlmann for your support and\nguidance. It’s been super inspirational to work with someone with such a commitment to\nrigorous research practice, attention to detail, and not least a passion for great teaching.\nA huge thank you to my other colleagues in the NLP group, Ehsan Doostmohammadi, Oskar\nHolmström, Olle Torstensson, Marcel Bollmann, Kevin Glocker and Noah-Manuel Michael, for\nsharing ideas and experiences and for discussions about research and everything else. I also\nwant to thank my amazing master’s thesis students Martin Jirénius and Marc Braun for their\ngreat work on their thesis projects and on the papers we have co-written. And not to forget\nall my former colleagues from NLPLAB, Arne Jönsson, Lars Ahrensberg, Robin Kurtz, Evelina\nRennes, Jody Foo, Jalal Maleki and Riley Capshaw, and my co-supervisor Eva Blomqvist. You all\nprovided invaluable feedback and inspiration especially at the beginning of my PhD journey.\nI thank Richard Johansson, Lovisa Hagström and Tobias Norlund for many research discussions\nespecially during the pandemic and when our own NLP group was still tiny. Thanks to Ryan\nCotterell for the insightful discussions in my mid-term seminar, and to everyone else who\nattended my seminars and presentations over the years and gave feedback to my work.\nAll the lunch and fika breaks wouldn’t have been as fun without the guys (m/f/d) from the\nplanning group and elsewhere at AIICS and HCS. Thank you for your company!\nI thank Karin Baardsen for your assistance with everything concerning travel and money, and\nAnne Moe for your great support with the bureaucratic processes and everything else. Thanks to\nmy colleagues in the PhD Council for constantly working on improving the PhD experience at\nIDA.\nA great thank you to my family for supporting me and my dreams and ideas, even though they\ntend to send me far away from you. And finally, a huge thank you to my three favourite people:\nTo my partner Max for coming to Linköping with me and for all the support over the years, and\nto our kids Josefine and Rufus for making life colourful. I love you!\nv\n\nContents\nAbstract iii\nAcknowledgments v\nContents vii\nList of Figures xi\nList of Tables xiii\nI Introductory Summary 1\n1 Introduction 3\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.1 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.2 Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3 Delimitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.4 Reading Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2 Language Representation Learning 9\n2.1 Neural Network Language Representations . . . . . . . . . . . . . . . . . . . . 9\n2.2 Word Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Contextualized Language Representations . . . . . . . . . . . . . . . . . . . . . 10\n2.3.1 ELMo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.3.2 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.3.3 GPT-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3.4 GPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.3.5 GPT-3.5 and GPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4 Limitations of Large Language Models . . . . . . . . . . . . . . . . . . . . . . . 16\n2.4.1 Generalisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.4.2 Hallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.4.3 Form and Meaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3 Interpretation 19\n3.1 Behavioural Probes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.2 Structural Probes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.3 Mechanistic Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.4 Probing Classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.4.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nvii\n3.4.2 Influential Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.4.3 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.4.4 Comparison to other Methods . . . . . . . . . . . . . . . . . . . . . . . 27\n4 Explanations 29\n4.1 Input Relevance Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.1.1 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.1.2 Other Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.1.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.2 Deductive Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.3 Natural Language Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.3.1 Applications and Datasets . . . . . . . . . . . . . . . . . . . . . . . . . 36\n4.3.2 Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n4.3.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n5 Paper Summaries 43\n5.1 Paper I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n5.2 Paper II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n5.3 Paper III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.4 Paper IV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n5.5 Paper V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n5.6 Other Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n5.6.1 Constructing Surrogate Models for Textual Explanations . . . . . . . 47\n5.6.2 Understanding Cross-Lingual Transfer . . . . . . . . . . . . . . . . . . 47\n6 Conclusion 49\n6.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.2 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n6.2.1 Understanding the Internal Processes of LLMs . . . . . . . . . . . . . 50\n6.2.2 Building LLMs that are More Interpretable by Design . . . . . . . . . 51\n6.2.3 Targeted Explanations that Consider the User’s Needs . . . . . . . . . 51\nBibliography 53\nII Papers 81\n7 Paper I 85\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n7.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n7.2.1 Neural Sentence Encoders . . . . . . . . . . . . . . . . . . . . . . . . . 87\n7.2.2 Probes and Probing Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n7.3 Extracting Linguistic Structure, One Embedding at a Time . . . . . . . . . . . . 88\n7.3.1 Extracting Structure vs. Learning a Task: A Continuum . . . . . . . . 88\n7.3.2 Neighboring Word Identity Probes . . . . . . . . . . . . . . . . . . . . 88\n7.4 A Framework for the Analysis of Probing Experiments . . . . . . . . . . . . . . 91\n7.4.1 The Context-Only Hypothesis . . . . . . . . . . . . . . . . . . . . . . . 91\n7.4.2 Review of Baselines under the Hypothesis . . . . . . . . . . . . . . . . 91\n7.4.3 Review of Model and Training Restrictions under the Hypothesis . . 92\n7.4.4 Empirical Analysis of Training Restrictions . . . . . . . . . . . . . . . . 93\n7.4.5 The Pipeline Argument . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n7.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\nviii\n8 Paper II 105\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n8.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n8.2.1 Probing (and its Limitations) . . . . . . . . . . . . . . . . . . . . . . . . 106\n8.2.2 Interpolation and Extrapolation . . . . . . . . . . . . . . . . . . . . . . 107\n8.2.3 What are Hard Examples? . . . . . . . . . . . . . . . . . . . . . . . . . 107\n8.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n8.3.1 Word Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n8.3.2 Probing Classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n8.3.3 Tasks and Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n8.3.4 Scoring Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n8.3.5 Easy Sets and Hard Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n8.3.6 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n8.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n8.4.1 Sentence Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n8.4.2 Arc Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n8.4.3 Most Frequent Tag and Tag Proportions . . . . . . . . . . . . . . . . . 113\n8.4.4 Speed of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n8.4.5 Sample-specific Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n8.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n8.5.1 Scoring Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n8.5.2 Contributions and Limitations . . . . . . . . . . . . . . . . . . . . . . . 117\n8.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n9 Paper III 127\n9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n9.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n9.3 A Taxonomy of Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n9.3.1 General Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n9.3.2 Global Baselined Probing (GBP) . . . . . . . . . . . . . . . . . . . . . . 130\n9.3.3 Global Conditional Probing (GCP) . . . . . . . . . . . . . . . . . . . . . 130\n9.3.4 Local Baselined Probing (LBP) . . . . . . . . . . . . . . . . . . . . . . . 131\n9.3.5 Local Conditional Probing (LCP) . . . . . . . . . . . . . . . . . . . . . 131\n9.3.6 Emergent Information (EMI) . . . . . . . . . . . . . . . . . . . . . . . . 131\n9.3.7 EMI, Baselined Control (EMI-BL) . . . . . . . . . . . . . . . . . . . . . 132\n9.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n9.4.1 Probing Classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n9.4.2 Language Representation Models . . . . . . . . . . . . . . . . . . . . . 132\n9.4.3 Data and Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n9.4.4 Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n9.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n9.5.1 Max Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n9.5.2 Early Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n9.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n9.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n10 Paper IV 151\n10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n10.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n10.2.1 Automatic Evaluation and Diagnostics . . . . . . . . . . . . . . . . . . 153\n10.2.2 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n10.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n10.3.1 Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\nix\n10.3.2 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n10.3.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n10.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n10.4.1 BERTScores and Surface Features . . . . . . . . . . . . . . . . . . . . . 158\n10.4.2 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n10.4.3 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n10.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n10.5.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n10.5.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n10.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n10.6.1 Hallucinations in GPT-MT . . . . . . . . . . . . . . . . . . . . . . . . . 171\n10.6.2 Template-like explanations in e-SNLI . . . . . . . . . . . . . . . . . . . 171\n10.6.3 Plausible but “incorrect” answer options . . . . . . . . . . . . . . . . . 172\n10.6.4 Uninformative “refute” answers . . . . . . . . . . . . . . . . . . . . . . 173\n11 Paper V 177\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n11.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n11.2.1 Self-Rationalising Models . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n11.2.2 Faithfulness Versus Understandability . . . . . . . . . . . . . . . . . . 179\n11.3 Properties of Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n11.3.1 Incompleteness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n11.3.2 Subjectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n11.3.3 Misleading Explanations for Incorrect Labels . . . . . . . . . . . . . . 182\n11.3.4 Illustrative Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n11.4 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n11.4.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n11.4.2 Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n11.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n11.5.1 Presence of Explanations (Q1 and Q2) . . . . . . . . . . . . . . . . . . . 184\n11.5.2 Properties of Explanations (Q3–Q6) . . . . . . . . . . . . . . . . . . . . 186\n11.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n11.6.1 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n11.6.2 Limitations of our Method . . . . . . . . . . . . . . . . . . . . . . . . . 187\n11.6.3 Implications for Different Goals . . . . . . . . . . . . . . . . . . . . . . 188\n11.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n.1 Full Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n.1.1 Instructions for Annotators . . . . . . . . . . . . . . . . . . . . . . . . . 197\n.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n.2.1 Commonsense Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n.2.2 Selectivity (Q3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n.2.3 Subjectivity (Q4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199\n.2.4 Illustrative Elements (Q5) . . . . . . . . . . . . . . . . . . . . . . . . . 200\n.2.5 Misleading Explanations for Incorrect Labels . . . . . . . . . . . . . . 200\nx\nList of Figures\n2.1 Overview of ELMos architecture. Figure by Devlin et al. (2019). . . . . . . . . . . . 11\n2.2 Example for MLM prediction: The word language is predicted by the BERT model\nbased on its surrounding words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3 Comparison of the high-level architectures of BERT and GPT-3: The constrained\nmulti-head self-attention of the Transformer decoder caps the connections to the\npreceding tokens in GPT-2. Figure by Devlin et al. (2019). . . . . . . . . . . . . . . . 14\n2.4 The three steps of aligning an LLM: Instruction fine-tuning, training the reward\nmodel, and reinforcement learning. Figure taken from Ouyang et al. (2022). . . . . 15\n3.1 Syntax tree recovered from BERT representations with a structural probe. Example\nby Hewitt and Manning (2019). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Results from Blevins et al. (2018), with a probe on a dependency paring and a\nsemantic role labelling model that exposes a hierarchy of tasks: Part-of-speech\ninformation peaks first, then syntactic parents, and then syntactic grand- and great-\ngrandparents. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n4.1 An example for a feature attribution-based explanation in the masked language\nmodelling task, highlighting the most relevant tokens for predicting the masked-out\ntoken. The missing word in this example is live. Created with AllenNLP Interpret\n(E. Wallace et al. 2019b). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n4.2 Example for a deductive explanation: A constrained METGEN (Hong et al. 2022)\ntree for science question answering. The question in this example was: How might\neruptions affect plants?, the answer, as shown in green in the figure:Eruptions can cause\nplants to die. Orange denotes facts; blue intermediate conclusions. Figure adapted\nfrom Hong et al. (2022). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.3 Graphical representation of the categorisation proposed by Hase et al. (2020). x is\nthe input, y the output and e the explanation. Figure by Hase et al. (2020). . . . . . 39\n7.1 Neighboring word identity probes: Results for BERT . . . . . . . . . . . . . . . . . . 90\n7.2\nRestrictions on BERT-based models for label (above) and head-dependent pair (be-\nlow) prediction, trained with representations from the uncontextualized layer BERT-0\nand on BERT-6. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n7.3 Restrictions on ELMo-based models for label prediction, trained with representations\nfrom the uncontextualized layer ELMo-WE and the best performing layer BERT-1,\nas well as an “inflated” version of ELMo-WE that has the same dimensionality as\nELMo-1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n8.1 Extrapolation based on sentence length. From left to right: part-of-speech tagging\n(T1), linguistic criterion; dependency labelling (T2), linguistic; T1, distributional\ncriterion; T2, distributional. In all plots, the x-axis corresponds to the BERT layer\nused for prediction, and the y-axis corresponds to the mean accuracy. . . . . . . . . 112\nxi\n8.2 Extrapolation based on arc length. Left: Standard distributional setup. Right: Modi-\nfied setup. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n8.3 Extrapolation for T1 based on the most frequent tag (left) and tag proportions criteria\n(right). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n8.4 Extrapolation based on speed of learning. Left: Tagging (T1). Right: Dependency\nlabelling (T2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n8.5 Extrapolation based on loss. Left: Tagging (T1). Right: Dependency Labelling (T2). 115\n9.1 Heatmaps illustrating our results for syntactic parent (P) and grandparent (GP)\nprediction (BERT-base, en, layers 1–12): Global metrics peak in middle layers. Local\ncontributions are concentrated in early layers. (Darker shades indicate higher values.)128\n9.2 Part-of-speech tagging, global (a–b) and local (c–d) metrics on the English data. Solid\ngreen line: non-MFTs, dotted orange: MFTs, dashed blue: full development set (all\ntags). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n9.3 As opposed to en BERT and the other four models, for cs and tr, the scores on MFTs\nin GCP drop more over the layers than those for non-MFTs. . . . . . . . . . . . . . . 136\n9.4 For cs and tr BERT, the LCP plots exhibit a pattern that deviates from that we observe\nfor en BERT: They do not decrease steadily. . . . . . . . . . . . . . . . . . . . . . . . 136\n9.5 Ancestors prediction, LCP: fi shows a later peak for grandparents (orange), while tr\nBERT’s curves show a similar pattern for both tasks. . . . . . . . . . . . . . . . . . . 137\n9.6 POS Experiments. Orange: MFT; green:  MFT; blue: all. . . . . . . . . . . . . . . . 146\n9.7 Ancestors Experiments. Blue: P; orange: GP . . . . . . . . . . . . . . . . . . . . . . . . 148\n10.1\nExperimental setup for training (upper half) and testing (lower half) on gold versus\ngenerated explanations as a causal graph (Pearl 1995). Itrain, Idev, Etrain, Ltrain and\nLdev are the inputs, explanations and labels from the train and dev set, respectively.\nEgen are generated explanations from the GPT-models, M is the BERT classification\nmodel, Lpred are the labels predicted by M. All variables affected by the intervention\non Etrain are marked with a red border line. . . . . . . . . . . . . . . . . . . . . . . . 156\n11.1 Distribution of the categories defined in Section 11.4.1 in the evaluation set. . . . . 184\n11.2 Comparison of the yes-answers the three annotators (A1, A2, A3) for Questions Q1\n(“Does the output contain an explanation for the prediction?”) and Q2 (“Would\nyou give an explanation/justify your reasoning if you were asked this question by a\nfriend?”). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\nxii\nList of Tables\n2.1 Architecture, number of parameters and training data size for the LLMs used in our\nexperimental work. For GPT-4, there is no official data available. . . . . . . . . . . 11\n7.1 ELMo Results: Word Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n9.1 Part-of-speech tagging tasks. The numbers give the layer of maximum score across\nmetrics and languages. Bold marks the task (MFT or  MFT) that is higher in the\nhierarchy induced by the model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n9.2 Syntactic ancestors prediction tasks. The numbers give the layer of maximum score\nacross metrics and languages. Bold marks the task (P or GP) that is higher in the\nhierarchy induced by the model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n9.3 Part-of-speech tagging tasks: Contribution of layer 1, 1 + 2 and 1 + 2 + 3 to the\noverall performance of the probe. Bold marks the task (MFT or  MFT) that is is\nhigher in the hierarchy induced by the model (smaller contribution of the lower layers).137\n9.4 Syntactic ancestors prediction tasks: Contribution of layer 1, 1 + 2 and 1 + 2 + 3 to\nthe overall performance of the probe. Bold marks the task (P or GP) that is is higher\nin the hierarchy induced by the model (smaller contribution of the lower layers). . 138\n10.1 Overview of our classification setups. The table indicates the source of the explana-\ntions that the model is trained and tested with. . . . . . . . . . . . . . . . . . . . . . 156\n10.2 BERTScores (F1) for the single-task (GPT-ST) and multi-task (GPT-MT) models. . . 157\n10.3 Surface features: average word and character length, vocabulary size and vocabulary\noverlap with gold explanations for each set of explanations (dev. set). . . . . . . . . 158\n10.4 Results for the classification models, macro-averaged F1 scores. . . . . . . . . . . . 159\n10.5 Results for the classification models, accuracy. . . . . . . . . . . . . . . . . . . . . . . 159\n10.6 Human evaluation: average share of yes answers across all samples that were not\nflagged as invalid. The numbers in parentheses show Krippendorf’s α (n = 3,\ninterval from \u00011 to +1) for inter-rater agreement. . . . . . . . . . . . . . . . . . . . . 160\n11.1 Samples that received at least two yes-Answers from the raters for Questions Q1 and\nQ2 as well as the average output length in tokens. . . . . . . . . . . . . . . . . . . . 185\n11.2 Samples that received at least two yes-Answers from the raters for Questions Q3–Q6.\nTotal is number of explanations for the category (as reported via Q1). . . . . . . . . 186\nxiii\n\nPart I\nIntroductory Summary\n1\n\n1 Introduction\nThis chapter includes the motivation behind the work in this thesis (Section 1.1), its\nscientific contributions (Section 1.2) and its delimitations (Section 1.3), as well as an\noutline of the thesis structure (Section 1.4).\n1.1 Motivation\nOver the course of the recent five to seven years, large language models (LLMs) have\nbecome almost inevitable in natural language processing (NLP). LLMs are huge neural\nnetworks with many millions or billions of parameters that learn to represent language\nby predicting words from their context. By being trained on large amounts of texts and\nmaking such predictions of words billions of times, LLMs learn to capture fine-grained\nstatistical information about language. These features have proven proven useful for\nalmost all kinds of downstream applications that are currently of interest in NLP . LLMs\nare even applied to syntactic tasks but stand out in particular for natural language\nunderstanding (NLU) tasks that depend on complex interactions between words\nand sentences, as well as on commonsense facts about the world we live in. Many\nNLU benchmarks that were once considered challenging, such as extractive question\nanswering (Rajpurkar et al. 2016)1 and the recognition of textual entailment (A. Wang\net al. 2019)2, can now be seen as solved: Current models regularly surpass human\nperformance, although it is important to emphasise that the models perform well\n1https://rajpurkar.github.io/SQuAD-explorer/; last accessed 21/2/2024.\n2https://super.gluebenchmark.com/leaderboard; last accessed 21/2/2024.\n3\n1. I NTRODUCTION\non benchmarks rather than real-world tasks, and that the validity of human-to-system\ncomparisons has been challenged (Tedeschi et al. 2023).\nThe popularity of LLMs comes from performance gains in many tasks, but also from\nthe fact that they make careful feature engineering obsolete as they already appear to\ncontain a lot of useful syntactic, but also semantic information. This broad applicability\nhas brought the first LLMs such as ELMo (Peters et al. 2018) and BERT (Devlin et al.\n2019) wider media attention. In 2018, The New York Times wrote in an article about\nBERT that “computer systems can learn the vagaries of language in general ways and then\napply what they have learned to a variety of specific tasks.”3. The article even noted BERT’s\nhuman-like performance on a commonsense reasoning task.\nMore recently, the focus has shifted towardsgenerative LLMs such as GPT-4 (OpenAI\n2023) and Llama 2 (Touvron et al. 2023). These models do not only solve classification\nand labelling tasks but can generate coherent text for a wide range of user prompts.\nThe largest of these models can do so even without adaption to the specific task or\nwith only few or no task-specific examples (Brown et al. 2020; Chowdhery et al. 2022).\nWith the arrival of easy-to-use conversational interfaces such as OpenAI’s ChatGPT,\nGoogle’s Bard and Anthropic’s Claude, generative LLMs have been widely adopted\neven by non-NLP people. In a survey of university students in Sweden, Malmström et\nal. (2023) report that 63% of the respondents report to use ChatGPT at least occasionally,\nand only 5% are completely unfamiliar with it. While sampling bias may apply, and\nuniversity students may be more likely to adopt new technologies than the general\npopulation, this nonetheless indicates that LLMs are impacting society more and more,\nwith no end in sight.\nAs LLMs are adopted by broader parts of the population and for an increasing number\nof use cases, it is crucial to understand how they work and what they have learned.\nHowever, much of the nature of the features that LLMs encode remains unclear even\nto NLP experts. Neural network-based models in general, and LLMs in particular,\nare opaque. Their size and complexity make a complete, fine-grained understanding\nof the internal processes infeasible. For this reason, they have unexpected failure\nmodes (Bommasani et al. 2021; Mittelstadt et al. 2019). This affects the users’ trust\nin a system and the ability of operators to know when it is a good idea to give a\nsystem control (Lipton 2018), but also the developer’s possibilities to improve it as a\nlimited understanding of models also hinders hypothesis-driven progression (Rogers\net al. 2020). Without knowledge about what features a model relies on, it is also hard\nto assess how fair and ethical its decisions are (Miller 2019). It also leaves open the\nquestion of the true capabilities of current NLP models: The great performance on\nmany tasks could be (and in some cases has been) attributed to statistical cues and\nbiases in the data sets instead of models performing the reasoning that we hope it\nto do. Besides developing more challenging tasks and data sets, we also need more\ninsights about the LLMs’ inner workings in order to get a more robust understanding.\nFor these reasons, an increasing effort within the NLP community is spent on the\ninterpretability and explainability of LLMs. Interpretability methods help to develop a\n3https://www.nytimes.com/2018/11/18/technology/\nartificial-intelligence-language.html; last accessed 15/2/2024.\n4\n1.2. Contributions\nhigher-level understanding of the LLMs’ internal representations and test hypotheses\nabout their inner workings. Explainability methods showcase (actual or potential)\nreasons that led to specific predictions.\nWhile many ideas, approaches and framings of interpretation techniques pop up, their\nmethodology remains largely explorative. Oftentimes, the approaches are limited\nto very focused areas, or build up on cherry-picked examples. A more systematic,\nrigorous and interpretable approach to interpretability is needed for faithful, reliable\ninsights. In this thesis, I present our work on these issues: on better understanding the\ncontributions of current interpretability and explainability methods, and on making\nthem more rigorous. I argue that such approaches, paired with an understanding of\nthe models’ architecture, learning objective(s) and training data, can lead us to a level\nof understanding that allows us to predict the models’ behavior and outputs of future\ntasks. In short, the goal is to demystify LLMs.\n1.2 Contributions\nThis thesis is divided into two parts, which represent two complementary approaches\nto better understanding LLMs and their decision-making process: interpretation and\nexplanation. As those approaches contribute to different subgoals of explainable NLP\nand use different methods, I will first introduce and discuss them separately, before\nbringing them together again in Chapter 6, where I will discuss how the combination\nof both approaches helps us to understand the bigger picture of how LLMs work.\n1.2.1 Interpretation\nIn the first part of this thesis, I investigate how linguistic information is structured\nwithin the model by focusing on the roles that the internal representations at the\nvarious layers of the models play. This gives us an improved understanding of the\nmodel on a global level: We get to understand how the model as a whole represents\nlanguage. We aim to answer the following research questions:\n1. What limitations do currently popular interpretation techniques and explanation\nmechanisms have? How can we assess and expose the weaknesses of such\nmechanisms?\n2. How can interpretation methods be improved so that they are more faithful,\nmore reliable and better suited to draw conclusions about what the language\nrepresentations encode and how information is structured within the models?\nTo answer these questions, our work assesses, improves, and develops the interpreta-\ntion methodologies. It focuses on a popular interpretability tool that is called a probing\nclassifier. A probing classifier is a tool that learns to predict linguistic properties from\nthe internal representations of LLMs and other neural network-based models. An\nimportant but non-trivial distinction is if the classifier learns the probing task from the\ndata it is trained on, either based on memorisation or on contextual patterns, or if it\nextracts relevant linguistic information that is encoded in the LLM’s representations.\n5\n1. I NTRODUCTION\nPopular interpretation methodologies are questioned and theoretically and empirically\ntested on whether they really reveal what they intend. In Paper I, Classifier Probes\nMay Just Learn from Linear Context Features, we develop a framework that exposes that\nan important assumption made in previous work does not hold neither logically nor\nempirically, namely, that if we feed the representation of one word at a time to the\nprobing classifier, it does not have sufficient contextual information to learn the task.\nWe show that information about each word’s surrounding context is extractable from\nits own representation with impressive precision, and that none of the baselines or\nrestrictions commonly used in the literature can disprove the hypothesis that it is\nonly linear context information that enables the probing classifier to learn the task.\nIn conclusion, common probing methodologies are unable to differentiate between\nlearning the task and extracting relevant features.\nBased on our findings from Paper I, we increase the restrictiveness of the probing\nsetup in Paper II Test Harder than You Train: Probing with Extrapolation Splits. Instead of\nevaluating the probe on a similar distribution to the one it was trained on, we create\nextrapolation splits: We define a set of criteria to rank the difficulty of data samples,\nand use only the simpler samples for training and the harder samples for evaluation.\nAs machine learning models, of which the probing classifier is an instance, typically\nonly succeed in an interpolation setup, we argue that success in the extrapolation\nsetup is a sign that the probing classifier decodes task-specific linguistic properties from\nthe representations rather than learning the task.\nIn Paper III, Where Does Linguistic Information Emerge in Neural Language Models? Mea-\nsuring Gains and Contributions across Layers, we overcome the problems that we iden-\ntified in Paper I with new metrics that provide a new perspective on the structure\nof information within models. Instead of comparing the probing classifier’s results\non a specific layer to a global baseline, we focus on local gains: That is, how much\ninformation the representation at this layer contains that the representation of the\npreceding layer did not contain. We show that this perspective changes the focus for\nsyntactic tasks from the middle layers, where the overall performance is highest, to\nthe first layers, where most new information emerges.\n1.2.2 Explanation\nIn the second part of this thesis, I will turn to models that explain their own individual\npredictions, so-called self-rationalising models. I focus on models that generate those\npredictions in free text as this form is both easily understandable to various user\ngroups and applicable to many kinds of NLP tasks. As those generated explanations\nare however not (necessarily) faithful to the predictions but generated by an opaque\nmodel themselves, our work aims to better understand their properties. In particular,\nit focuses on the relation between various, partially conflicting goals of explainable\nNLP: That the explanations should provide us with insights about the LLM’s true\nprediction process, guide the LLM itself in its predictions, but also be understandable\nand useful to different users, such as non-technical end users, but also developers.\nIn this part, our work investigates the following research questions:\n6\n1.3. Delimitations\n1. How does the utility of explanations to a downstream model align with the\nhuman perception of the explanations?\n2. Which properties of human explanations does an LLM adopt and how do these\nrelate to different goals of explainable NLP?\nThese questions are investigated in two papers that are centred around human evalua-\ntions of LLM-generated explanations.\nIn Paper IV ,Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Ex-\nplanations for Model Predictions, we answer the first question by comparatively training\nmodels with different pipeline architectures and performing human evaluations on\nthem. We show that a crucial difference is that a downstream model by tendency ben-\nefits from the inclusion of novel (input-external) information in the explanation, even\nif the information in many cases is factually incorrect, while human raters however\npunish factual incorrectness decisively.\nIn Paper V , Properties and Challenges of LLM-Generated Explanations, we perform a\nhuman evaluation on an LLM’s outputs for a diverse dataset. We annotate the data for\nknown properties of human explanations, specifically such that have been pointed\nout as disadvantageous for explainable NLP . We find that the LLM explanations are\noften incomplete and contain illustrative elements, but are rarely subjective and rarely\nmisleading, but that the latter two findings are highly dependent on the model and\ndataset that we use. We connect the observed properties with different goals and user\ngroups of explainable NLP , showing that all of the properties can have positive or\nnegative implications depending on the use case.\n1.3 Delimitations\nThe aim of this thesis is not to develop fully interpretable language models where\nthe internal decision steps can be understood in detail. Classical explainable artificial\nintelligence methods such as sparse representations, few learnable parameters, or\ndecomposability are not considered. I accept the dense and generally opaque nature\nof the currently most successful representations and explore and develop methods to\nunderstand their properties better. The understanding I aim at is at a relatively high\nlevel. While the methods used may help to predict the success and failure cases of\nmodels, and add context for human users when making decisions, they do not make\nthe model a reliable basis for high-stake decisions without a human expert in the loop.\n1.4 Reading Guide\nThis thesis assumes its readers to have a basic understanding of machine learning and\nnatural language processing concepts. It does not require a deep technical understand-\ning of current models; I introduce them at the level of details that is needed to follow\nour work.\nOutline. This thesis is written in form of a compilation. The original research\nconducted as part of this thesis is described in the five articles found in Part II, in their\n7\n1. I NTRODUCTION\npublished (Paper I–IV) or submitted (Paper V) form. Part I, the introductory summary\n(kappa), provides a more comprehensive background, a broader overview of relevant\nliterature and a more extensive discussion of the methodologies that are the basis of\nour own work. The kappa is structured as follows:\n• In Chapter 2, I give readers an overview of the field of language representation\nlearning. In particular, I introduce the three types of representations that our\npapers build on: the recurrent neural network-based ELMo, the Transformer\nencoder-based BERT and the family of Transformer decoder-based GPT models.\nAs those models are widely known in the NLP community, readers with this\nbackground may skip this chapter.\n• In Chapter 3, I introduce interpretation techniques that are designed to assess\nwhat is learned by the LLMs. I briefly introduce a broader set of methods to\nposition our contributions in the field as a whole but focus on probing classifiers\nthat are the method used in our own work.\n• In Chapter 4, I introduce work on generating and evaluating explanations in\nNLP . I start with an overview of the types of explanations that exist in NLP but\nfocus on free-text explanations that are used our work.\n• Our contributions to the fields of interpretability and explainability are then\nsummarized in Chapter 5, where I give an overview of the findings of the papers\nthat this thesis contains and relate them to each other. I also introduce further\npapers I contributed to during my time as a PhD student.\n• Chapter 6 concludes this thesis with a final summary and discussion of our\ncontributions as well as my view on the future of the field.\nThroughout the thesis, I use the pronoun I when referring to the writing of this kappa.\nWhenever referring to work done with collaborators, I use we.\n8\n2 Language Representation\nLearning\nThe object of study of this thesis are LLMs, the currently most dominant type of neural\nlanguage representations in academic NLP . In this chapter, I give an overview of the\nfast progress in language representation learning over the past ten years.\nLanguage representation models automatically identify and organize re-usable in-\nformation from text corpora to build a representation of natural language. Those\nrepresentations can be built either for internal use in the model itself based on the\ntask-specific dataset that this model is trained on (Section 2.1) or from unlabelled data\nfor the transfer to various models (Section 2.2) or tasks (Section 2.3). The most weight\nwill lie on the latter representations, as the LLMs that we study in our experiments\nare instances of those. The development of interpretability techniques was however\nalready taking off in the context of pre-LLM neural models and word embeddings, as\nwe will discuss in Chapter 3. After having introduced the models, I will conclude the\nchapter by discussing some crucial limitations of current LLMs in Section 2.4.\n2.1 Neural Network Language Representations\nIn NLP , neural networks gained broad popularity in the middle of the last decade.\nIt was recurrent neural networks (RNNs; Elman 1990), and long short-term memory\nnetworks (LSTMs; Hochreiter and Schmidhuber 1997) in particular, that first led\nto major performance improvement on many tasks. The sequential nature of these\narchitectures was a natural fit for language data, which is also generated and processed\nsequentially by humans. In addition to performance gains, neural networks dispensed\nwith the need for manual feature engineering and hand-crafted pipelines by building\n9\n2. L ANGUAGE REPRESENTATION LEARNING\nup their own internal language representation. In the pre-neural era, even shallow\ntasks like syntactic dependency parsing relied on a large set of features, such as part-\nof-speech (POS) tags of the current tokens themselves and surrounding tokens, and\nvarious distance- and direction-based features. Finding the best set of features was an\nimportant and time-consuming part of parser engineering. Kiperwasser and Goldberg\n(2016) in their influential parser use a bidirectional LSTM encoding to represent a\ntoken in its context, and base the representation only on word forms and POS tags,\nand let the LSTM encoder do the rest. But even POS tags were no longer a crucial\nadvantage for syntactic parsing, giving at most a slight performance gain (Dozat et al.\n2017; Lhoneux et al. 2017). Given enough training data, the models can operate on\nraw text without helper systems, and achieve similar accuracy to highly engineered\nmodels. A similar transformation happened in many other tasks, with an important\nsource of influential new ideas being machine translation that brought along RNN-\nbased sequence-to-sequence approaches (Sutskever et al. 2014), attention (Bahdanau\net al. 2015), and finally the Transformer architecture (Vaswani et al. 2017) that is the\ncurrently most widely adapted base model in NLP . Learning to encode a word with\nthe task-specific data set, and to contextualize it, with a neural model proved to be a\nvery successful representation for language. But while this is a form of feature learning\nto get a model-internal language representation, such representations are still trained\ntask-specifically (although the word representation architecture can be shared among\nmany tasks), and usually are not transferred to other purposes.\n2.2 Word Embeddings\nAt the same time as neural networks were popularized in NLP , word embedding (WE)\nmodels appeared as widely used general-purpose word representations. They build\none static vector for each word form based on its immediate contexts occurring in an\nunannotated corpus, using a simple word prediction task based on the context words\nor on a dimensionality-reduced co-occurrence matrix. Popular examples are Turian\nembeddings (Turian et al. 2010), GloVe (Pennington et al. 2014) and the word2vec\nmodels (Mikolov et al. 2013b). Word embeddings can be applied to basically every\nNLP task and in any machine learning model. Using them improved the neural\nnetwork performance on many tasks due to their richer, self-supervised representation\nof words, in particular on tasks with limited data that is not sufficient for the model to\nbuild its own rich language representation.\n2.3 Contextualized Language Representations\nWhile the word embeddings described in Section 2.2 were used as the input to other\nneural network-based models, large contextualized language representations trans-\nformed the field and how models are built fundamentally. Pre-trained on huge\namounts of unlabelled text data and with rapidly increasing model sizes, such a model\ncan with a relatively modest effort be adapted to various kinds of unforeseen tasks.\nThe step of adapting such a model with task-specific data in a second, substantially\nshorter training phase is called fine-tuning. For many tasks, they easily outperform\nprevious neural network models trained from scratch. As Bommasani et al. (2021)\nnote, \"the field of NLP has become largely centred around using and understanding foundation\n10\n2.3. Contextualized Language Representations\nBERT (Ours)\nTrm Trm Trm\nTrm Trm Trm\n...\n...\nTrm Trm Trm\nTrm Trm Trm\n...\n...\nOpenAI GPT\nLstm\nELMo\nLstm Lstm\nLstm Lstm Lstm\nLstm Lstm Lstm\nLstm Lstm Lstm\n T1 T2  TN...\n...\n...\n...\n...\n E1 E2  EN...\n T1 T2 TN...\n E1 E2  EN...\n T1 T2  TN...\n E1 E2  EN...\nFigure 2.1: Overview of ELMos architecture. Figure by Devlin et al. (2019).\nmodels\". Few pre-trained models are re-used and examined tens of thousands of times,\nwith the BERT model currently (as of February 2024) having almost 100,000 citations\non Google Scholar.\nContextualized language representations are largely congruent with LLMs. While\nthe latter term is more often used for generative models, I adopt the definition of\nLuccioni and Rogers (2023) which states that LLMs are neural network models that\nprocess and generate text, that have been trained on at least one billion words, and that\nmake inference based on transfer learning. While there are contextualized language\nrepresentation that do not fulfill the data requirements of this definition, we see in\nTable 2.1 that all of the language representations that were used in the experiments for\nthis thesis project fulfill it. I will therefore use the term LLMs interchangeably.\nModel Architecture Objective #Params Training Data Papers\nELMo BiLSTM Autoreg. LM 94M 1B Words I\nBERTbase Tf. Encoder MLM / NSP 110M 3.3B Words I–IV\nBERTlarge Tf. Encoder MLM / NSP 340M 3.3B Words -\nGPT-2 Tf. Decoder Autoreg. LM 1.5B 9B Tokens IV\nGPT-3 Tf. Decoder Autoreg. LM 175B 300B Tokens -\nGPT-4 ? ? ? ? V\nTable 2.1: Architecture, number of parameters and training data size for the LLMs\nused in our experimental work. For GPT-4, there is no official data available.\n2.3.1 ELMo\nThe autoregressive language model ELMo (Peters et al. 2018) was, along with ULMFiT\n(Howard and Ruder 2018), one of the first contextualized word representation that was\nwidely adopted in the NLP community. ELMo is intended and mostly used as a word\nrepresentation in a downstream model. In contrast to the representations introduced\nin Section 2.2 however, the representation is a function of the entire input sentence\nrather than of a word.\n11\n2. L ANGUAGE REPRESENTATION LEARNING\nELMO is based on a bidirectional LSTM (Hochreiter and Schmidhuber 1997) consisting\nof a forward and a backward language model that predicts the next (respectively the\nprevious) token conditioned on the LSTM accumulation of the preceding (respectively\nthe future) tokens, maximizing the log-likelihood of both directions. For a sequence of\nN tokens (t1, t2, . . ., tN ) and the parameters Θx of the token representation, Ý ÑΘLSTM of\nthe forward-pass LSTM predicting from the left-side context, Ð ÝΘLSTM of the backward-\npass LSTM predicting from the right-side context, and Θs of the softmax layer, ELMo\nmaximises:\nN¸\nk=1\n( log p(tk | t1, . . . ,tk\u00011 ; Θx, Ý ÑΘLSTM , Θs)\n+ log p(tk | tk+1, . . . ,tN; Θx, Ð ÝΘLSTM , Θs) ) .\nELMO has a character-based word representation layer with 512 dimensions and 2\nbi-LSTM hidden layers with 1,024 units. The token representation commonly used\nin downstream tasks is either the top layer or a task-specific weighted sum of the 3\ninternal layers of the LSTM. In the latter case, the weights for each layer are learned by\nthe downstream model, while the parameters of the layers themselves remain frozen.\nELMo is trained on the One Billion Word Benchmark, a sentence-level English-\nlanguage dataset in the news domain with, as the name says, approximately one\nbillion words (Chelba et al. 2013).\n2.3.2 BERT\nBERT (Devlin et al. 2019) is based on a Transformer model’s encoder (Vaswani et al.\n2017) that contextualizes the word representations with multi-head self-attention and\nfully connected layers. Its architecture became the basis of many more Transformer-\nbased language representations. Transformers proved to be more successful than\nother architectures like RNNs because they scale up to very deep models and the self-\nattention makes them more successful at catching long-range interactions of tokens\n(Bommasani et al. 2021).\nAfter the input to the BERT model is tokenised, the embedding of the token itself\nis enriched with an encoding of its position in the input span, as the Transformer\narchitecture does not natively model word order, and an encoding indicating which\nsentence the token belongs to, as BERT can process up to two sentences at a time.\nThe standard BERTbase model consists of 12 layers, while the BERT large model has\n24 layers. The core components of each layer are a multi-head self-attention module\nand a fully connected layer. The self-attention module provides the representation\nwith context, as it aggregates information from the embeddings of the whole input\nsequence. This is done multiple times in parallel (12 times in the case of BERTbase; 16\ntimes for BERTlarge) to be able to capture richer features from the representations (thus\nmulti-head self-attention). The scores of each head are then combined before they are\npropagated to the fully connected layer. A high-level illustration of the architecture\ncan be found in Figure 2.3.\n12\n2.3. Contextualized Language Representations\nFigure 2.2: Example for MLM prediction: The word language is predicted by the BERT\nmodel based on its surrounding words.\nBERT is pre-trained with two objectives: The Masked Language Model (MLM) ran-\ndomly replaces some input tokens with a special MASK token, with the objective of\npredicting the vocabulary ID of the original token at that position. Figure 2.2 shows an\nexample of such a prediction. This approach naturally includes both left and right con-\ntext. The Next Sentence Prediction (NSP) objective makes BERT learn the relationship\nbetween two sentences by predicting if the second sentence is following the first one\nin the original document or not. A special token ([CLS]) is added for the NSP objective,\nwhich can also be used for other downstream classification tasks.\nThe MLM objective and the self-attention mechanism allow BERT to jointly condition\nthe representation on the right and left context of a token, unlike ELMo that models\nthem with two separate LSTMs. BERT thus optimises a simpler expression than ELMo.\nFor N tokens (t1, t2,..., tN ) and the learnable parameters Θ, we get:\nN\nk= 1\nlog p(tk\n t1,..., tk\n 1, tk+ 1,..., tN; Θ)\nBERT is trained on two corpora: the BooksCorpus (Zhu et al. 2015), consisting of books\nwith 800 million words, and English Wikipedia, consisting of 2.5 billion words.\n2.3.3 GPT-2\nThe GPT model family consists of autoregressive LLMs based on the decoder part\nof the Transformer architecture. In contrast to the Transformer encoder that is the\narchitecture of BERT, the decoder is designed fortext generation. Therefore, it computes\nconstraint multi-head self-attention scores that are only based on the context on the\nleft of the token that is to be predicted. The effect of this constraint, as compared to\nBERT’s Transformer encoder architecture, can be seen in Figure 2.3.\nThe conditioning on the left context only gives us the following formula (again, for a\nsequence if N tokens (t1, t2,..., tN ) and the parameters Θ):\nN\nk= 1\nlog p(tk\n t1,..., tk\n 1; Θ)\n13\n2. L ANGUAGE REPRESENTATION LEARNING\nFigure 2.3: Comparison of the high-level architectures of BERT and GPT-3: The\nconstrained multi-head self-attention of the Transformer decoder caps the connections\nto the preceding tokens in GPT-2. Figure by Devlin et al. (2019).\nGPT-2’s architecture and objective were introduced for the original GPT model by\nRadford et al. (2018). Its successor, GPT-2 (Radford et al. 2019), was the last GPT model\nwith a full public release of the parameters. There are GPT-2 models in four different\nsizes, with the smallest one, like BERT, consisting of 12 layers and 12 self-attention\nheads, and the largest of 48 layers and 25 attention heads.\nGPT-2 is trained on the WebText corpus that is also introduced in Radford et al. (2019).\nWebText is a scrape of outbound links from the internet forum Reddit. Radford et al.\n(2019) state that it contains approximately 8 million documents, and 40 gigabytes of\ntext. While the original dataset is not shared and the exact number of training tokens\ntherefore unknown, there exists an open replication by Gokaslan et al. (2019) which\nhas 9 billion tokens.\n2.3.4 GPT-3\nGPT-3 (Brown et al. 2020) adopts GPT-2’s architecture and objective, but it represents\nan enormous upscaling, growing from (at most) 1.5 billion to 175 billion parameters\nfor the largest of the 8 GPT-3 models. The 175 billion parameters are spread over 96\nlayers, and the model has 128 attention heads.\nThe most groundbreaking property of GPT-3 is the ability to do few-shot learning, also\ncalled in-context learning: To adapt the model to a new task, no parameter updates are\nnecessary; providing a limited number of task demonstration examples in the input is\nsufficient. Some success is even reported for one-shot and zero-shot transfer where only\none or no demonstration example is provided to the model when solving a new task.\nIt appears that, with sufficient scale, autoregressive pre-training is sufficient to infer\nthe structure of many tasks. It also improved the performance over smaller models.\nHowever, it is well-documented that the training corpus of GPT-3 has a significant\namount of contamination with common benchmark tasks (Brown et al. 2020; Dodge\net al. 2021), which has a measurable effect on the performance of models (Magar and\nSchwartz 2022). Therefore, comparisons have to be done with caution.\n14\n2.3. Contextualized Language Representations\nFigure 2.4: The three steps of aligning an LLM: Instruction fine-tuning, training the\nreward model, and reinforcement learning. Figure taken from Ouyang et al. (2022).\nThe training corpus for GPT-3 has 300B tokens and consists of is a filtered version of\nthe web archive corpus CommonCrawl (Raffel et al. 2020), WebText2 (an expanded\nversion of the GPT-2 training set), two internet-based corpora named Books1 and\nBooks2 (with no further details released), and English-language Wikipedia.\n2.3.5 GPT-3.5 and GPT-4\nGPT-3.5 and GPT-4 (OpenAI 2023) are, as the names imply, the successors of GPT-3,\nand the base of the now-famous ChatGPT model. They have greatly improved zero-\nshot capabilities compared to GPT-3, probably due to instruction fine-tuning (IFT; Wei\net al. 2021) and reinforcement learning from human feedback (RLHF; Ouyang et al. 2022).\nThose techniques can teach a model to better follow instructions and align them with\nthe users’ expectations. After the general language modelling pre-training phase, the\nmodel is fine-tuned on a supervised dataset containing a diverse set of instructions\nand demonstrations of desired model behaviours. With a sufficiently diverse set of\ntasks in the instructions, the model’s performance is increased even on tasks unseen\nduring IFT (Wei et al. 2021). In the RLHF step, human ratings of model outputs are\nused as a reward signal to the model. A reward model is trained on human rankings\nof multiple output candidates, which is then used to train the model via reinforcement\nlearning. An overview of the process as introduced in the OpenAI’s InstructGPT paper\n(Ouyang et al. 2022) is given in Figure 2.4. The whole process is also called alignment,\nas it does not only improve the zero-shot performance of the model but also aligns its\noutputs better with human expectations.\nHowever, which techniques are used for GPT-3.5 and GPT-4 is speculative. While key\ninformation about the GPT-3 model, like the type and amount of the training data\n15\n2. L ANGUAGE REPRESENTATION LEARNING\nand the model architecture, was still published, for the following models there is no\nreliable data available.\n2.4 Limitations of Large Language Models\nWhile LLMs have impressive coverage and adaptability, they continue to have fun-\ndamental limitations. When the task at hand is specialised and a sufficient amount\nof training data is available, it is often preferable to use a smaller, specialised model\nrather than a very large general-purpose model. One obvious reason is that the infer-\nence costs are substantially lower, but fine-tuning a smaller model may also result in\nbetter performance. The paper Jack of all Trades, Master of Noneby Koco ´ n et al. (2023)\nshows that the GPT-3.5-based ChatGPT model is often outperformed by task-specific\nfine-tuned models.\nWhile this factor may change at any moment, there are more systematic limitations\ninherent to LLMs that I will summarise in this section: The generalistion capabilities,\nhallucinations, and the aspects of meaning they can capture. All of those limitations\nalso relate to the lacking interpretability of the models. They show us that our under-\nstanding about (and control of) how LLMs model language and knowledge, and how\nthey solve specific tasks, is still limited.\n2.4.1 Generalisation\nThe ability to generalise, that is, to transfer representations, knowledge and strategies\nto new tasks, is a key goal of machine learning (Hupkes et al. 2022). Yet, LLMs have\ndeficiencies in various types of generalisation.\nThey are prone to overly relying on superficial cues and annotation artifacts from the\ndataset when making predictions. In the famous paper Right for the Wrong Reasons:\nDiagnosing Syntactic Heuristics in Natural Language Inference, T. McCoy et al. (2019)\nintroduce an evaluation set for the sentence-level entailment prediction task NLI with\nexamples where simple syntactic heuristics fail. While BERT on average performs\nbetter than the other three models that are trained from scratch, the performance is\nstill poor compared to the standard evaluation set, indicating that even BERT relies\ntoo much on shallow heuristics rather than learning proper generalizations. The\ngeneralization problem is also addressed by Niven and Kao (2019). They apply BERT\nto an argument reasoning comprehension task and reach 77%, which is almost human\nperformance and should the authors’ view not be possible without supplying world\nknowledge. However, thet show that BERT relies largely on very simple statistical\ncues like unigrams and bigrams. As in the previously mentioned work however, this\napplies even stronger to models trained from scratch. They create an adversarial\ndataset by negating the claim and inverting the label of each data point, with the\nresult that BERT performs only slightly above the random baseline. E. Wallace et\nal. (2019a) create adversarial examples with simple modifications of the inputs that\nchange the prediction for several tasks and models including ELMo and GPT-2. The\nadversarial modifications let the performance drop substantially. Similarly, Hsieh\net al. (2019) employ five different strategies for developing adversarial examples that\ncould mislead neural models but not humans and find out that Transformer and BERT\n16\n2.4. Limitations of Large Language Models\nmodels are less sensitive to them than recurrent models. Building on such works,\nIlyas et al. (2019) argue that adversarial vulnerability is a natural consequence of the\nsupervised paradigm. Models are trained on standard datasets containing signals that\nare incomprehensible to humans but highly predictive for the given dataset, which\nthey call non-robust features. They emphasise the need to include human priors at\ntraining time to align the models with human expectations, and to make them robust\nand interpretable.\nLarger LLMs have become less reliant on specific properties of the dataset as the\nneed for fine-tuning with extensive task-specific datasets that may contain annotation\nartifacts has decreased. However, they are still heavily reliant on the presence of similar\ntext in the pre-training data. R. T. McCoy et al. (2023) show that for exactly the same\ntask, high-probability input or output sequences lead to much higher performance than\nequivalent low-probability sequences. For example, for linear functions, functions that\noccur often in text data because they e.g. are used for Celsius-to-Fahrenheit conversion\nlead to much more accurate results than a very similar function that uses only slightly\ndifferent numbers in the same tasks. Sun et al. (2023) show that instruction-tuned\nLLMs are sensitive to instruction phrasing: They considerably drop in performance\neven with slight variations of the instructions, indicating that even the most advanced\nLLMs still struggle heavily with generalisation.\n2.4.2 Hallucinations\nThe term hallucinations refers to information that is made up by the LLM. Coined in\nthe field of neural machine translation (K. Lee et al. 2018) and subsequently adapted\nto abstractive text summarisation (Maynez et al. 2020), the term was originally used\nfor information that is not faithful to the input document: The translation or the\nsummary contain information that has not been present in the original text. Today\nhowever, it is frequently applied broadly in the field of text generation, referring\nto the inclusion of unintended information in general, such as factually incorrect or\nirrelevant information (Ji et al. 2023). As LLMs often operate in contexts where there is\nno single input document that the generated text should be closely aligned with, it is\nharder to define or detect a hallucination. It is however a common observation when\nworking with LLMs that the output is plausible, but does not align with the real world.\nA hallucinated, i.e. factually incorrect, answer can even come with a hallucinatory\nexplanation, a potentially plausible-sounding defense of such a statement (Augenstein\net al. 2023).\nHallucinations are a major problem of LLMs not only because they decrease the\nperformance of a system but also because they can pose a safety risk in sensitive\napplications, which limits the potential applications LLMs can realistically have (Ji\net al. 2023). Therefore, reducing them has become the motivation behind a large\nbody of research. Various techniques have been proposed for this purpose, such as\nconfidence-based refinement of the output (Nie et al. 2019), refinement by checking\nagainst facts in a knowledge graph (Dziri et al. 2021) or retrieval-augmented generation\n(Shuster et al. 2021). However, hallucinations will not be fully mitigated in LLMs\nas we currently understand them. Mechanisms that rely on external ground truths\nsuch as knowledge graphs or text retrieval databases will not have full coverage over\n17\n2. L ANGUAGE REPRESENTATION LEARNING\nall types of information that users of a general-purpose LLM such as GPT-4 seek.\nReference-free approaches, such as the confidence-based refinement, can never capture\nall cases of hallucination, not least because making up content is be an inherent (and\noften intended) functionality of generative LLMs.\n2.4.3 Form and Meaning\nA more philosophical debate addresses the question if LLMs are even capable of\ncapturing meaning. The LLMs we use build on the distributional hypothesis after\nwhich words that occur in similar contexts have similar meanings (Firth 1957; Harris\n1954). The distributional approach has limitations in which aspects of meaning it\ncan cover, in particular, it does not have access to the referential meaning of a word\n(Emerson 2020), meaning that they cannot relate the form of language they get to\nsee to any instances of the real world. Bender and Koller (2020) argue that LLMs are\ntherefore unable to achieve a semantic understanding of language.\nThe view that reference determines meaning is however disputed, especially because\nhumans regularly use terms that have no referent in the real world because they\nrefer to abstract concepts or invented entities. Piantadosi and Hill (2022) argue that\nLLMs capture other key aspects of meanings: conceptual role meanings, i.e. meanings\nderived from the relations of concepts to each other. They argue that reference is not\nnecessary to determine meaning as humans can readily reason about many concepts\nwithout referents in the real world. Even Mollo and Millière (2023) emphasise that\nhuman representations often emerge without contact to a reference but by testimony\nbased on the representation of others. They criticise the conflation of grounding and\nother concepts like understanding or agency, and define grounding as the ability to\nrepresent things independently of human interpretation, i.e. that they possess intrinsic\nmeaning. Based on the symbol grounding problem (Harnad 1990) that states that symbols\nin symbolic approaches to artificial intelligence have no intrinsic meaning as they\ndo not interact with the world, they introduce the vector grounding problem for self-\nsupervised models. They claim that LLMs, in contrast to symbolic systems, do in\nfact have meaningful internal representations and can generate meaningful outputs.\nThey argue that referential grounding is in fact the relevant form of grounding as\nit underlies all other forms of grounding (relational, communicative, epistemic and\nsensorimotor grounding), but that it is possible for LLMs to acquire. As the models\nare trained on data that is shaped by human interactions with their environment, they\ncan develop a representation similar to the one humans who do not have access to\na referent can acquire, mediated by the human producers of the data. Moreover, for\nmodels trained with RLHF, there is an additional objective that (among other goals)\nrewards truthfulness and factual correctness with respect to the real world.\nIn any way, the distributional approach is the currently most widespread approach,\nand often successful from a pragmatic perspective. Bengio et al. (2013) argue that\na good representation is one that is useful in its downstream applications. This is\ncertainly the case for LLMs, and it is the reason why while LLMs may not acquire\nunderstanding of the text they create, they are widely used in practice.\n18\n3 Interpretation\nIn Chapter 2, I emphasised the self-supervised nature of the training of neural language\nrepresentations, particularly of LLMs. These models are not explicitly constructed\nwith a defined feature set but their qualities emerge from raw text (Bommasani et\nal. 2021). LLMs are too complex to enable the user to comprehend their decision-\nmaking criteria and rationale, due to the sheer amount of learnable parameters and\nthe non-linear nature of the functions they model (Mittelstadt et al. 2019). As neural\nlanguage representations dispense with the need for manually crafted features, it\nappears natural to ask if the representations have implicitly learned similar features,\neven though trained on a language modelling objective only. The massive performance\ngain on many tasks also raises the question of what additional information causes the\ngain.\nI adopt the definition of interpretability in machine learning by Roscher et al. (2020):\nthe human understandability of some of the internal properties of a model (Roscher\net al. 2020). Interpretability research in NLP and other machine learning-powered\napplications aims at creating methods and models that make machine learning systems\n(more) comprehensible to humans, and that test the models’ behaviour under specific\ncircumstances. Alain and Bengio (2017) argue that probing helps to get a sense of\nhow the training is progressing in a well-behaved model . Inspired by such an improved\nunderstanding, interpretation studies could motivate developments of the architecture\nor the learning objectives that lead to better models in general or models that are better\nadapted for specific use cases. In the case of NLP , interpretability methods often probe\nfor specific syntactic or semantic features that researchers hypothesize are the basis of\ncompleting a task (or a family of tasks).\n19\n3. I NTERPRETATION\nThis chapter is limited to post-hoc probes that are aimed at detecting linguistic knowl-\nedge in pre-trained word representations. While the community has developed a large\nfamily of interpretability methods, I will introduce four widely used categories to give\na taste of the field. The first one, behavioural or zero-shot probing, provides tactical\ninputs to the model and inspects its corresponding output probabilities. The latter\nthree, structural probes, mechanistic interpretability and probing classifiers, operate\non the hidden representations of the model to get insights into the model’s inner\nworkings. I will summarise existing research and discuss each method’s benefits and\ndrawbacks. In Section 3.1 I will introduce behavioural probes that operate within the\nLLMs’ training objective and compare probabilities for different queries. In Section 3.2,\nI introduce structural probes that investigate the geometric properties of the internal\nrepresentations. In section 3.3, the field of mechanistic interpretation aimed at reverse-\nengineering the model to understand at the neuron level what the model is doing.\nFinally, in Section 3.4, I introduce classifier probes. As classifier probes are the method\nthat our work in the interpretability field is about, I will introduce them in more detail,\nproviding a broader overview of the concept and relevant works and discussions.\n3.1 Behavioural Probes\nThe simplest setup for probes is the zero-shot setup, where the model’s native training\nobjective is used to query the model. As the model is left unchanged while targeted\ninputs are being processed and the outputs are used for the analysis, probes in this\nsetup are commonly called behavioural probes. In the case of an autoregressive language\nmodel, the most common setup is the extraction of probabilities for different predic-\ntions for the next token. In the masked language modelling objective, the tokens of\ninterest are masked and the probabilities for different predictions are compared.\nGroundbreaking works in the field have been done by Linzen et al. (2016), Marvin\nand Linzen (2018) and Goldberg (2019). Those papers probe the syntactic abilities of\nlanguage models, for example, linguistic capabilities like subject-verb agreement. A\nsimple example of such a subject-verb agreement probe by Marvin and Linzen (2018)\nis the sentence:\nThe author laughs.\nThe verb is masked out:\nThe author [MASK].\nThe model succeeds in this example if the probability of laughs is higher than the\nprobability of laugh:\nP(laughs) ¡ P(laugh).\nThe same setup has been applied to semantic tasks as well: Talmor et al. (2020) create\na set of tasks that require commonsense reasoning abilities called oLMpics. They test\nvarious abilities of the models such as age comparison and object comparison. An\nexample of the latter is in the following sentence:\n20\n3.2. Structural Probes\nA cat is [MASK] than a mouse.\nWe expect the probability of the tokenlarger to be higher than the probability of smaller:\nP(larger) ¡ P(smaller).\nThe advantage of zero-shot probes is their simplicity: No training is needed, we can\ntest the model directly without any modifications. Besides being easy to use, the fact\nthat they are a direct probe of the model, without additional learning parameters\nthat could influence the outcome, reduces the risk of the probe being influenced by\nexternal factors: The ability that we test for cannot be learned at probing time. The\nonly model-external factor that influences the results is the choice of probing data,\nwhich cannot be avoided in any known setup.\nA downside is that zero-shot probing is limited to certain tasks that can fit into the\nlanguage modelling objective and where a comparison of output probabilities can be\ninterpreted in a meaningful way. And even though no learning is happening in zero-\nshot probes, the design of the prompts affects the outcome. Success in such probes\nis context-dependent; the models often fail where discrepancy from their training\ndistribution becomes too large (Talmor et al. 2020). This can make it easy to draw\nconclusions (both positive and negative) that do not generalize to other experimental\ndesigns.\n3.2 Structural Probes\nAnother line of probing assumes the geometric properties of word representation\nvectors to be meaningful as they reflect similarities of the words’ usage in the training\ndata. It is the first of three interpretability methods that attempt to understand the\nvector space formed by model activations.\nA classical example of structural probes are word analogy tasks such as the widely\nknown work by Mikolov et al. on their word2vec model (Mikolov et al. 2013a,b).\nThey show that simple addition and subtraction of word vectors can (sometimes) give\nmeaningful results, such as in their famous king:queen example: When subtracting\nthe vector for man from the word for king, and adding the vector for woman, the closest\nword vector to the resulting vector is the one for queen.\nvector(”king”) - vector(”man”) + vector(”woman”)\n≈ vector(”queen”).\nThe authors create a data set of various kinds of semantic and syntactic analogies\ncalled Semantic-Syntactic Word Relationship. It includes quadruples from domains such\nas country and capital, adjective and adverb, and singular and plural. They assume\nthat a good word representation model should perform well on this data set. This\nassumption has been disputed, most influentially by Rogers et al. (2017) who argue\nthat assuming linguistic relations to expose such strong regularities is psychologically\nnot plausible: Semantic features are graded and messy, and even humans struggle with\n21\n3. I NTERPRETATION\nFigure 3.1: Syntax tree recovered from BERT representations with a structural probe.\nExample by Hewitt and Manning (2019).\nanalogy tasks due to their ambiguity. And while analogical reasoning is fundamental\nto humans, analogies cannot be represented as binary inference rules. Rogers et al.\n(2017) also show experimentally that the word analogies only work when the source\nand target vectors are close to each other. Other studies (Baroni et al. 2014; Levy and\nGoldberg 2014) show that the relational similarities are not exclusive to neural word\nembeddings but can also occur when using count-based word embedding strategies.\nStructural probes looking for sentence-level features in contextual word representa-\ntions are arguably less famous than relational probes on word embeddings but there\nare some influential works. Hewitt and Manning (2019) develop a structural probe\nthat tests if entire syntax trees can be extracted as a (learned) linear transformation\nfrom ELMo and BERT word representations. They assume that the number of edges\nbetween words (the depth of the parse tree) may be encoded in the representation as\nan L2 norm, finding out that it is indeed to some extent a structural property of the\nword representation space. An example for such a tree from their work is presented in\nFigure 3.1.\nEthayarajh (2019) investigates embeddings of BERT, ELMo, and GPT-2 with respect\nto how contextual they are. The author tests (among some other things) how similar\nthe representations of the same word are in different contexts, and finds that in the\nupper layers, the cosine similarity decreases, suggesting that the upper layers are more\ntask-specific. Kornblith et al. (2019) measure representational similarity between NNs\ntrained from scratch on image classification datasets with a measure that is invariant\nto invertible linear transformations. They find that the representations of different\ndatasets are similar in early, but not in higher layers.\nIt is an intuitive idea that the positioning of representations in the vector space, shaped\nby statistical patterns from the training corpus, is meaningful. However, structural\nprobes have the practical limitation that they require strong assumptions on how the\nproperties of interest are encoded in the representation. Those assumptions are only\npossible to make for certain properties that can be translated into a geometric relation.\n3.3 Mechanistic Interpretation\nClosely related to the structural probes, mechanistic interpretability aims at a fine-\ngrained understanding of models at the neuron level. This field builds on the assump-\ntion that it is possible to reverse engineer a model from its internal representations to\nhuman-understandable algorithms, analogously to the reverse engineering of a binary\ncomputer program. The discovered algorithms are called circuits, and are subgraphs\nof the network that connect features in an interpretable way (Cammarata et al. 2020;\nOlah et al. 2020).\n22\n3.3. Mechanistic Interpretation\nA disputed assumption is that the features and circuits are universal, i.e. that analogous\nvariants exist across similar models. Some works suggest that at least otherwise equal\nmodels with different seeds converge at similar (but not equal) feature representations\n(Y. Li et al. 2015).\nIn NLP , the mechanistic interpretability work is so far centred around discovering\nfeatures. In an early work, Karpathy et al. (2015) find several interpretable memory\ncells in LSTMs via activation statistics, encoding patterns like line length counters\nand cells that activate within brackets and quotes in code. Dalvi et al. (2019) find an\nindividual neuron that activates at month names and one that activates at negation\nwords, with the ten top words that activate at them being, respectively:\nMonth neuron: August, July, January, September, October, presidential, April,\nMay, February, December.\nNegation neuron: no, No, not, nothing, nor, neither, or, none, (Negation)\nwhether, appeal.\nStanczak et al. (2022) probe for the cross-lingual overlap of neurons for morphosyn-\ntactic properties such as gender, number and tense, finding that it is significant. They\nconclude that this result indicates a language-independent representation of these\nfeatures. The ROME method by Meng et al. (2022) employs causal tracing to locate\nfactual knowledge, and to edit it to alter the GPT-2’s output. The authors provide an\nexample where the input text:\nThe Space Needle is located in the city of Seattle.\nThe aim of their paper is to locate where in the model the fact that the Space Needle is\nin Seattle is represented, and to edit it so that the model places the needle in a different\ncity. Their findings using ROME suggest that factual knowledge is concentrated in\nmid-layer feed-forward modules.\nWhile mechanistic interpretability, popularised by the AI company Anthropic with\naccessible and engaging blog posts (Cammarata et al. 2020; Olah et al. 2020), has\nbecome the best-known term, attempts to discover individual neurons that represent\nspecific properties have taken various names. For example, Dalvi et al. (2019) call their\ntechnique with this goal linguistic correlation analysis. Torroba Hennigen et al. (2020)\nname attempts to discover such neurons intrinsic probing, and their specific technique\ndimension selection.\nBricken et al. (2023) argue that, rather than individual neurons, linear combinations\nof neurons should be viewed as the features and building blocks of mechanistic\ninterpretability. This is because individual neurons have a property that is called poly-\nsemanticity: They activate at inputs that do not correlate in a human-understandable\nway. The work uses a sparse autoencoder setup on a one-layer transformer language\nmodel, and finds relatively monosemantic (and thereby interpretable) features. This is\nnot a new but rather a rebranded insight: Combinations or rankings of multiple neu-\nrons have also been the building blocks of the analogous techniques used in academic\nresearch (Dalvi et al. 2019; Sta ´ nczak et al. 2023; Torroba Hennigen et al. 2020).\n23\n3. I NTERPRETATION\nFully understandable circuits have, not surprisingly, so far only been discovered for\nvery well-defined tasks. Nanda et al. (2023) study one-layer transformers for modular\naddition. They are able to fully reverse-engineer the model and find that it performs\nthe task by mapping the inputs to a circle which it uses to combine the inputs with its\nfeed-forward and attention layers. If similar results are possible with much deeper\nmodels, and especially if such interpretable circuits will be possible for real-world\napplications given the noisy nature of natural language and many NLP tasks remains\nto be seen.\n3.4 Probing Classifiers\nThe way to investigate representations that I focus on in this thesis is through the use\nof supervised probing classifiers (Alain and Bengio 2017; Hupkes et al. 2017). Probing\nclassifiers are simple classifiers that are trained to solve diagnostic prediction tasks\nconsidered to require relevant linguistic information, such as parts of speech, syntactic\nstructure, or semantic roles, from frozen model parameters. If the classifier is capable\nof predicting the property of interest to a high degree, it is concluded that the property\n(or, more precisely, information highly correlated to the property) has been learned\nimplicitly by the model.\nProbing classifiers are often used to measure quantities of interest in different parts\nof a model, to see where in a model a specific property is best found. Alain and\nBengio (2017) compare probing to using thermometers to measure the temperature\nsimultaneously at different locations within the model.\n3.4.1 Definition\nA probing classifier is a supervised classifier, typically a simple feed-forward network\nor a linear network, that is trained on a probing task (diagnostic task). It uses datasets\nD = (xi, yi)i, where each xi is the continuous vector representation of a neural lan-\nguage model at some specific layer and yi is the gold-standard label for the probing\ntask. The xi are often word-level representations as typical probing tasks are often\nat the word level, such as parts of speech, syntactic dependencies, semantic roles\nand co-referent entity mentions. The classifier’s performance is often measured with\nclassification metrics such as accuracy and can be compared to baselines or across\nlayers.\nTo give a simple example, the dataset D may contain sentences where each word is\nannotated with a part of speech tag, such as noun, verb or adjective, which become the\nyi. If we feed the sentence through an LLM (e.g. BERT), we can extract each token’s\ninternal representation from a specific layer (e.g. layer 3). As the words may be split\ninto several tokens by BERT’s tokeniser, we may need to realign them, e.g. by adding\nup the representations of the tokens that the word was split into. Now that we have\none representation for one word, we also have our xi. The dataset D can now be used\nto train and evaluate the probing classifier and see how much useful information for\npart-of-speech tagging BERT contains at layer 3. By building analogous datasets for\nthe other layers, we can compare the probe’s performance across layers and see where\nin the model most useful information is found.\n24\n3.4. Probing Classifiers\nFigure 3.2: Results from Blevins et al. (2018), with a probe on a dependency paring\nand a semantic role labelling model that exposes a hierarchy of tasks: Part-of-speech\ninformation peaks first, then syntactic parents, and then syntactic grand- and great-\ngrandparents.\n3.4.2 Influential Works\nHupkes et al. (2017) propose probing classifiers, or diagnostic classifiers as they call\nthem, to investigate how RNNs capture the hierarchical compositional semantics of\nnatural language. To isolate this mechanism from other properties of natural language\n(like structural and lexical ambiguity, irregular paradigms, multi-word units and\nidiomatic expressions), they use a synthetic dataset of nested arithmetic expressions.\nThey find that RNNs roughly follow a cumulative strategy, as the intermediate value\nof the expression up to the point from which the current representation is taken can be\npredicted more accurately than the value within the current brackets, which would be\nnecessary for a recursive strategy of solving the expressions.\nVarious papers found that information is structured hierarchically within the models.\nBelinkov et al. (2017a) probe machine translation models for part-of-speech and full\nmorphological tagging. They find that the lower layers of the encoder perform better\nfor this type of information, while deeper layers have less information about it. As\ndeeper networks are however crucial for translation quality, they hypothesize that\ndeeper layers are specialised in meaning. They also find that the decoder part of the\nnetwork contains little morphological information. In a subsequent work, Belinkov\net al. (2017b) find that indeed, the higher layers contain more semantic tag information.\nBlevins et al. (2018) probe four different LSTM-based models: a syntactic dependency\nparser, a semantic role labelling model, a machine translation model and a language\nmodel. They discover a soft hierarchy within the models, with part-of-speech informa-\ntion more prevalent in the lower layers, followed by syntactic parents, grandparents,\nand finally great-grandparents. Two example results for models they examine are\npresented in Figure 3.2.\nIn the paper that introduces the ELMo model, Peters et al. (2018) find that part-\nof-speech tags are better predicted from the first hidden layer, while word sense\ninformation is more prevalent in the second layer. In a comprehensive probing study\nwith many different tasks, Tenney et al. (2019) discover that in the BERT model, the\ntasks are ordered analogously to a hand-crafted NLP pipeline from pre-neural times:\n25\n3. I NTERPRETATION\nfrom POS tags over syntactic dependencies, named entities and semantic roles to\ncoreference information. Y. Lin et al. (2019) use probing classifiers for tasks that\nrequire different sorts of syntactic information, from linear information like a word’s\nposition to hierarchical information like the main auxiliary or the subject noun of\na sentence. Their results indicate that embeddings from lower layers contain more\nlinear information while higher layers contain more complex hierarchical features.\nThat semantics can be found in the higher layer while the lower layers contain more\nsyntactic information is also reported by Raganato and Tiedemann (2018) who use\nprobing classifiers on part-of-speech tagging, chunking, named entity recognition\nand semantic tagging on Transformer encoders for machine translation. Similarly,\nJawahar et al. (2019) demonstrate that BERT learns surface features at the bottom\nlayers, syntactic features in the middle layers, and semantic features in the top layers,\nsuggesting that BERT requires deeper layers to learn long-distance features.\nBlevins et al. (2022) analyse the pre-training dynamics of a multilingual model by\nprobing various training checkpoints. They find that while monolingual capabilities\nare acquired early, cross-lingual capabilities emerge later in training, to a varying\ndegree depending on the language pair. They also find that linguistic knowledge\nwanders from higher to lower layers during pre-training. K. Zhang and S. Bowman\n(2018) probe representations learned with different learning objectives, including\nlanguage modeling and translation, for syntactic tasks, finding that the representations\ntrained on bidirectional language modelling contain the most useful information.\nProbing has also been used for the evaluation of document or sentence embeddings.\nAdi et al. (2017) argue that downstream task evaluation is very coarse-grained and\ndoes not tell much about the types of information contained in the embeddings, and\ntherefore, generalisable conclusions cannot be drawn. They therefore introduce a set\nof three tasks that capture the most basic properties of a sentence: word order, word\ncontent, and length. Conneau et al. (2018) probe sentence embeddings for ten simple\nlinguistic features, finding that encoders contain a wide range of linguistic information\ncompared to baselines. They argue that for these simple tasks, it is easier to control for\nbiases than for downstream tasks.\nSlobodkin et al. (2023) probe if the embedding space of a model encodes the answer-\nability of a question with the help of a dataset that contains both answerable and\nunanswerable questions. They find that a probe on the last hidden layer reaches a\nmuch higher accuracy than a probe on the first layer, and conclude that there is an\nanswerability subspace within the representations.\n3.4.3 Methodology\nSeveral works have raised concerns that a powerful probe may simply learn the task\nby storing information in its own parameters, rather than exposing features from the\nrepresentations. The efforts in probing were in favour of placing restrictions on the\nclassifiers. Alain and Bengio (2017) explicitly measure the level oflinear separability with\ntheir probes, arguing that a linear classifier avoids the problem of local minima. Hewitt\nand Liang (2019) restrict the model in several ways: In making them linear like Alain\nand Bengio (2017), but also in the hidden size and the amount of training data, arguing\n26\n3.4. Probing Classifiers\nthat a reliable probe should be selective: It should perform well on the actual task, but\non the same time be unable to learn (memorise) a control task that randomly assigns\nword forms labels. Voita and Titov (2020) propose to tackle this problem by using\nthe minimum description length (Rissanen 1978), an information-theoretic estimate\nof complexity that takes into account the complexity of the model (its codelength).\nPimentel et al. (2020a) explicitly take into account the accuracy–complexity trade-off.\nThey argue that optimising for performance alone does not allow for conclusions\nabout the representations, but that optimising for complexity alone will make the\nprobe unable to exploit complex information about a word’s identity. Hence, a probe\nshould be Pareto optimal: In a family of probes, there should be no probe that has a\nhigher accuracy and at the same time a lower complexity.\nOn the other hand, Pimentel et al. (2020b) argue that a probe should be as expressive\nas possible. In their information-theoretic framework, they measure the mutual\ninformation between a representation and a linguistic property (the labels) and argue\nthat it is best measured with a powerful model. Saphra and Lopez (2019) argue that\nthe intermediate representations of neural networks cannot be expected to be linearly\nseparable, and that the non-accessibility of a property to a linear probe cannot be\nexpected to imply that this property does not exist in the representation.\nAnother concern about standard probing methodologies is that they do not take into\naccount if the detected information in the representation is actually relevant to the\nmodel when performing language modelling or downstream tasks. Ravichander et al.\n(2021) show that linguistic properties can be encoded in the representations of a model\n(in their case, a BiLSTM) even if they are not relevant to the models’ training task.\nBased on this problem, Elazar et al. (2021) propose amnesic probing, a technique that\nshifts the focus from the question of whether certain information is encoded in the\nrepresentation to the question of how this information is being used by the model.\nThey use the Iterative Nullspace Projection algorithm (Ravfogel et al. 2020) to delete\nlinear information associated with a task, and measure the effects of the intervention\non the language modelling performance. The results indicate that not all tasks that are\neasily learned by a probing classifier influence the performance: While the impact low-\nlevel syntactic information is the strongest, named entity information has a small and\nphrase detection information no impact. Rozanova et al. (2023) build on this work and\npropose mnestic probing, a method that reverses amnesic probing and keeps only the\ninformation that has been deemed task-relevant. They show that this strategy is more\ninformative for their target task, natural language inference, where high-dimensional\nrepresentations meet a small set of class labels.\n3.4.4 Comparison to other Methods\nCompared to the analysis of attention modules, probing classifiers have the advantage\nthat they are largely agnostic to the architecture of the neural network, as they only\nrely on the parameters of the feed-forward part. The latter can however also be\nseen as a limitation, as probing classifiers ignore the specific function of other parts\nof the architecture. In particular, they do not consider the function of the attention\nheads, which have been shown to exhibit interpretable meanings (K. Clark et al. 2019).\nHowever, the effect of the attention heads should be observable from the representation\n27\n3. I NTERPRETATION\nin the feed-forward part. And in fact, studies on attention and syntactic structure show\nsimilar results to probing: Vig and Belinkov (2019) show that the attention heads in\nthe highest layers capture the most distant syntactic dependency relationships, while\nmost dependency relationships are to be found in the middle layers.\nProbing classifiers may also be applicable to a broader set of tasks, as there may not\nbe a suitable attention head that covers a sufficiently similar structure for every task.\nSome tasks may depend on information aggregated from various attention heads in a\nway that is not easily observable. The broader applicability shows even more for the\ncomparison to structural probes and behavioural probes. Those two techniques are, as\ndiscussed earlier, only applicable to tasks with very specific properties. For probing\nclassifiers, the only limiting factor is that we need annotated data that can be aligned\nwith the representations. Feature discovery techniques in mechanistic interpretability\nare however closely related to probing classifiers. Consequently, they can be applied\non the same set of problems, and give us similar types of insight (and more, as it is\npossible to obtain results on a more fine-grained level).\nA disadvantage compared to behavioural probes is that for probing classifiers, we\nneed parameter access to the model. There is no way to analyse models with probing\nclassifiers, structural probes or mechanistic interpretability if we access them as a\nblackbox via an API. On the other hand, behavioural probes do not allow comparison\nof layers and thereby cannot offer any insights about models’ internal dynamics.\nAs outlined earlier in this section, what can be concluded from the results of a probing\nclassifier is subject to discussion. The usage of a trainable classifier makes the method-\nology more indirect than other methods that work with the model or representation\nas-is. This explains why there is more methodological discussion around probing\nclassifiers than around most other probing methods. This is the motivation behind our\nwork on probing classifiers: How do we know what we can conclude from a probe?\nAnd how can we make the method as reliable and insightful as possible? We approach\nthese questions in Papers I, II and III.\n28\n4 Explanations\nComplementary to analyzing the properties of a model globally as described in Chap-\nter 3, it is insightful to understand the reasons and mechanisms behind individual\npredictions. This can give both developers and users crucial context in order to know if\npredictions are reliable, or if a model uses undesirable shortcuts and biases.\nThe question what constitutes an explanation touches philosophy and the social\nsciences. In social science literature, explanations have been viewed as an answer to\nwhat–questions, how–questions, and why–questions (Miller 2019). For explainable AI,\nexplanations are likely to answer why-questions: Why is A the answer to question or\nquery Q? Or, more specifically: Why does the model infer answer A? By answering\nthis question, explanation methods aim at making the outputs of the NLP model and\nits behaviour more intelligible to humans. The explanation mechanisms can either be\npart of the model itself, or they can have access to the model parameters, or they can\nsimulate its internal processes by collecting outputs for targeted inputs.\nThe currently most widespread approach in explainable NLP is attribution methods\n(Section 4.1), which identify those components of the model input that have the\nmost impact on the output. However, the applicability and clarity of these methods\nhave been criticised as limited (Bansal et al. 2021; Papenmeier et al. 2022; Wiegreffe\net al. 2021). In addition, humans prefer explanations that target the mechanism of\nhow the model arrived at a conclusion over explanations that solely list covariant\nfactors (Lombrozo 2006). An alternative that focuses on both covariant factors and the\nmechanism are procedural explanations (Section 4.2) that offer a complete reasoning\npath from the input to the prediction, but that have an even more limited applicability.\nAs a third alternative, free-text explanations (Section 4.3) have recently gotten traction\n29\n4. E XPLANATIONS\nFigure 4.1: An example for a feature attribution-based explanation in the masked\nlanguage modelling task, highlighting the most relevant tokens for predicting the\nmasked-out token. The missing word in this example is live. Created with AllenNLP\nInterpret (E. Wallace et al. 2019b).\nin NLP (Marasovic et al. 2022; Wiegreffe et al. 2022). They are easily accessible to\nusers, as they imitate human explanations, and are flexible in the tasks they can\nbe used for and the types of reasoning they can express. However, there are also\nsignificant concerns. One is that free-text explanations, like human explanations,\nfocus on proximal mechanisms rather than complete sets of reasons or complete logical\ndeductions (Tan 2022). Explaining a model in this way can seem unintuitive as from a\ntechnical system, many people expect the ability to explain things in a technical, not\na social form. The most widespread concern is however that the connection to the\nmodel’s reasoning process is unclear (Bommasani et al. 2021). This is also the case for\nmany other explanation techniques (as I will outline in this chapter), but even more\nevident for explanations that are largely generated by the same LLM that generates the\npredictions, with the same mechanism, but no easy way to reliably test the connection\nbetween answer prediction and explanation generation.\nThe object of study of our own work has been natural language explanations, which I\nwill therefore discuss most extensively in Section 4.3. To set natural language explana-\ntions in context in the field of explainable NLP , I will however start with introducing\ntwo other impactful approaches: input relevance measures in Section 4.1 and proce-\ndural explanations in Section 4.2, with a focus on their limitations that make natural\nlanguage explanations stand out.\n4.1 Input Relevance Measurements\nInput Relevance Measurements, also called feature attribution techniques, explain\nmodel predictions by identifying critical parts of the input. For a document classifi-\ncation task, this could be a collection of words that were particularly important for\nthe assignment of a document to a certain category. Wiegreffe and Marasovic (2021)\nrefer to explanations produced by feature attribution as highlights, Tan (2022) calls\nthem evidence. They are the most common form of explanation for explainable NLP .\nAn example for a feature attribution-based explanation can be found in Figure 4.1.\nThe idea behind feature attribution is that explanations have classically been de-\nfined as sets of causes, that is, empirical conditions or natural laws that lead to the\nexplanandum—the statement about the phenomenon in need of explanation (Lom-\nbrozo 2006; Moravcsik 1974). However, Lombrozo (2006) argues that humans prefer\nexplanations that cover the mechanism of how causes lead to an outcome. Attribution\nleaves out all this information on how the highlighted information is being used by the\nmodel (Rudin 2019). As such, it does not map to any form of everyday explanation,\nand Tan (2022) even argues that stand-alone evidence is not a form of explanation.\n30\n4.1. Input Relevance Measurements\nInstead, we may think of highlighted parts as causes that have the effect of making the\nmodel produce a certain output (Keil 2006).\nWhen it comes to the utility of attribution methods, studies have arrived at mixed\nconclusions. Bansal et al. (2021) test if explanations increase the performance of human-\nAI teams and find that they do not provide more value than confidence scores alone:\nthey yield an increase in accuracy when the model is correct, but a decrease when it is\nwrong. Papenmeier et al. (2022) report the results of a large user study where faithful\nexplanations did not increase user trust or understanding compared to random or\nno explanations. Chu et al. (2020) show that for image classification, accuracy, trust,\nand understanding are not significantly improved by providing visual explanations.\nHowever, the utility of explanations may vary across tasks and designs. In particular,\nthey may be helpful for tasks that require navigating external information sources.\nFor example, González et al. (2021) retrieve answers for challenging open\"-domain\nquestions from corpora and find that explanations help users predict errors better than\nconfidence only. Shen et al. (2022) show that the length of the explanation is crucial for\nunderstanding.\nApart from its restriction to information in the model input, a further shortcoming of\nfeature attribution is its limitation to tasks where either the input itself or the retrieved\ndocument explicitly contains all relevant context. More open-ended tasks that rely\nextensively on information inferred from the model parameters or other knowledge\nand reasoning, such as open-ended or multiple choice question answering (Talmor\net al. 2019) or algebraic word problems (Ling et al. 2017), cannot be satisfactorily\nexplained using feature attribution.\n4.1.1 Attention\nAn intriguing technique for assessing feature relevance, as it is a pre-existing compo-\nnent of many NLP models including Transformers, is looking at the attention scores.\nHowever, it is subject to discussion how meaningful the scores are as the models are\nnot specifically trained on attending to relevant inputs, let alone to rank inputs in\nattention scores.\nSerrano and Smith (2019) investigate the input and output of an attention layer before\nand after an intervention on the attention weights in order to assess if attention\ndistributions can be used to identify the most relevant information. They perform a\nnumber of experiments in a text classification system in its final attention layer to judge\nattention’s capability of being used as an importance ranking. In the first experiment,\nthey remove the attention weight of the component with the highest attention and\ncompare the effect with a randomly removed attention weight. In this case, the\nconclusion is tentatively positive: The highest-scored element has a much larger effect\nwhen being removed than the random element. In another experiment, they erase\nattention weights subsequently starting at the top of the ranking and observe at which\npoint the model’s decision changes. They compare the attention distribution with\nthree other rankings: a random order, an order determined by the gradient of the\ndecision function with respect to each attention weight, and the attention weights\nsupplemented with information about the gradient. They find out that while the\n31\n4. E XPLANATIONS\nattention weights distribution flips the decision faster than the random order, the\ntwo latter approaches flip faster than the (pure) attention weights, suggesting that\nattention weights are not an optimal predictor for models’ decisions although they do\nsometimes correlate with importance, but in a noisy way.\nThe paper Attention is not Explanation (Jain and B. C. Wallace 2019) also explores how\nwell attention weights can capture the relative importance of an input, but in the\ncontext of the full model instead of the final attention layer as done by Serrano and\nSmith (2019). Based on experiments with comparisons of the attention distributions\nto gradient-based feature importance measures and an evaluation with adversarial\nattention, Jain and B. C. Wallace (2019) claim that attention weights are weak predictors.\nThis paper has been criticised in its methodology and conclusions by Wiegreffe and\nPinter (2019) whose answer paper is calledAttention is not not Explanation. In particular,\nWiegreffe and Pinter (2019) question the adversarial attention experiments as they\ndo not take into account that attention weights are not trained independently but\nend-to-end with the full model, and as the manipulated attention scores are not truly\nadversarial. Wiegreffe and Pinter (2019) provide alternative approaches for testing the\nmeaningfulness of attention distributions, in particular by training examples that they\nshow to be truly adversarial. Based on the poor performance on those new adversarial\nexamples, they conclude that attention scores can provide coherent interpretations.\nIt is apparent that researchers have different perceptions of terms like explainability\nand interpretability. This is also reflected in the definitions that they give. Serrano and\nSmith (2019) write that “[i]n order for a model to be interpretable, it must not only suggest\nexplanations that make sense to people, but also ensure that those explanations accurately\nrepresent the true reasons for the model’s decision. ” Jain and B. C. Wallace (2019) state\nthat their question is “[. . . ] whether attention suffices as a holistic explanation for a model’s\ndecision”. Wiegreffe and Pinter (2019), on the other hand, claim that attention is\nusually treated as an explanation, not the explanation, and that most earlier work\nclaimed attention rather to be “providing plausible rationales” than guaranteed faithful\nexplanations, so that their interpretations of the attention models can still be seen as\nvalid.\nWhile the previously discussed papers studied attention mechanisms in models with\nRNNs, others have investigated attention distributions, but to self-attentive Trans-\nformer models with multiple heads. Voita et al. (2019) look at distributions in the\nindividual attention heads of a transformer model for machine translation, finding that\nthere are heads that take interpretable functions such as positional heads attending to\na specific relative position, attention heads that model specific syntactic relations and\na head attending to rarest tokens in a sentence. In their evaluation of a pruning experi-\nment, the authors show that the interpretable heads are more relevant for the system\nperformance than the others, and that most heads, especially in the encoder, can be\npruned without seriously affecting the system’s performance. A related method is\nused byK. Clark et al. (2019) who analyze BERT’s attention heads and also find several\nfunctional heads that attend for example to the direct objects of verbs, determiners of\nnouns, objects of prepositions, and coreferent entity mentions.\n32\n4.1. Input Relevance Measurements\n4.1.2 Other Attribution Methods\nBesides attention, there are more methods to extract input relevance scores. There\nare two main approaches: perturbation-based surrogate models, and gradient-based\nmethods.\nSurrogate models are smaller, more interpretable models that approximate model\ndecisions. They are model-agnostic, meaning that they treat the model they are\nsupposed to explain as a blackbox and query it using small perturbations around the\ninput data to infer the relevance of each input feature. Examples for such models are\nLocal Interpretable Model-Agnostic Explanations (LIME; Ribeiro et al. (2016)) and\nSHAP (Lundberg and S.-I. Lee 2017). While surrogate models are widely used due to\ntheir simplicity and wide applicability, they have been shown to be unstable for non-\nlinear models (Alvarez-Melis and Jaakkola 2018; E. Lee et al. 2019), and the sampling\nprocedures to perturb the input can lead to uncertainty (Yujia Zhang et al. 2019).\nMoreover, surrogate models are sensitive to adversarial attacks: They can even be\nmanipulated to not reflect the true features and biases underlying the decision-making\n(Dombrowski et al. 2019; Slack et al. 2020).\nAnother family of techniques that is not strictly model-agnostic but can be used for all\nneural network models uses gradients to compute the contribution of input features\nto the model’s decision (J. Li et al. 2016; Simonyan et al. 2014; Sundararajan et al. 2017).\nAs gradient-based models use model parameters only, requiring just a backward\npass without modifications, they are considered more faithful than surrogate models.\nFaithfulness is particularly relevant given that the intended user of the generated score\nis often the model developer (Bastings and Filippova 2020). For other target users\nhowever, it is relevant that even the gradients of a model can be subject to adversarial\nattacks: J. Wang et al. (2020) train models whose gradients place high attribution to\ntokens not relavant to the task without affecting the predictions.\n4.1.3 Evaluation\nAs outlined in this section, no input relevance measure is fully reliable and robust.\nMoreover, different techniques can produce contradictionary results (Atanasova et al.\n2020a). To systematically compare and evaluate them, various criteria and measures\nhave been proposed.\nAlvarez-Melis and Jaakkola (2018) propose measures for the robustness of the expla-\nnations to slight variations of the input. They find that while no method they test\nis robust, gradient-based methods perform better than perturbation-based methods.\nYu et al. (2019) suggest a set of three properties that a rationale should maximise:\nsufficiency, meaning that it is possible to make the prediction with the selected features\nonly, comprehensiveness, meaning that the explanation includes all relevant features,\nand compactness, meaning that the explanation should not be longer than necessary\nand that it should be continuous. Atanasova et al. (2020a) propose a more extensive\nset of criteria, including the agreement with human-annotated rationales, confidence\nmeasures, faithfulness (as measured by performance difference when leaving out the\nmost salient tokens), and consistency. Even they find that gradient-based methods are\nfulfilling their criteria best.\n33\n4. E XPLANATIONS\nFigure 4.2: Example for a deductive explanation: A constrained METGEN (Hong et al.\n2022) tree for science question answering. The question in this example was: How\nmight eruptions affect plants?, the answer, as shown in green in the figure: Eruptions\ncan cause plants to die. Orange denotes facts; blue intermediate conclusions. Figure\nadapted from Hong et al. (2022).\nFor the comparison with human rationales, there exist datasets with annotations for\nrelevant input spans. The benchmark ERASER (DeYoung et al. 2020) collects seven\nsuch datasets. The authors of ERASER propose to measure agreement with these\nhuman annotations, but also faithfulness scores based in sufficiency and comprehen-\nsiveness as the goal of outputting attribution scores is to create not only plausible but\nalso faithful explanations. The importance of the disentanglement between the two has\nbeen noted by Jacovi and Goldberg (2020) who point out that human ratings or gold\nstandards are inappropriate for faithfulness evaluation as plausibility to humans does\nnot indicate what a machine learning model is doing internally.\n4.2 Deductive Procedure\nDeductive procedures are grounded in the input and provide step-by-step rules that\nlead to the prediction. They are less common because they are only applicable to a\nsmall share of NLP tasks (Tan 2022) but provide complete inference chains where\nall intermediate steps can be checked. Long before deep learning became popular,\nprocedural explanations have been used in artificial intelligence to learn generalization\nby capturing the structural relationship of a problem (DeJong and Mooney 1986; Lewis\n1988; Mitchell et al. 1986).\nNarayanan et al. (2018) employ explanations in the form of decision sets, mappings of\ninputs to outputs via a set of rules. In user studies, they search for explanations that\nhumans can utilize best, finding that more complex explanations are harder to process\nand less satisfactory. Hong et al. (2022) build constrained trees consisting of entailment\nsteps for science question answering. An example for such a tree generated with their\nMETGEN system can be found in Figure 4.2. Ling et al. (2017) and Jie et al. (2022)\ngenerate the intermediate steps necessary to solve math word problems. This is similar\nto the recently famous Chain-of-Thought (CoT) generation (Wei et al. 2022), where the\nmodel generates intermediate reasoning steps prior to the prediction in a zero-shot\nsetting. However, in CoT, the completeness and correctness of the intermediate steps\nis neither enforced nor typically evaluated; its main goal is the improvement of the\nprediction accuracy.\n34\n4.3. Natural Language Explanations\nThere are two main limiting factors in deductive procedures. One is applicability: The\nprediction problem needs to be fully formalisable, which is a very strong assumption\nas most dynamical systems are not characterisable as (interpretable) computations\n(Cummins 2000). Also, procedures are limited to problems solvable by simple and\ntransparent algorithms, such as decision trees. Humans, on the other hand, explain at\ndifferent levels of abstraction, even if their understanding is coarse and fragmentary\n(Keil 2006). To understand how rare purely formal reasoning is in humans, we should\nconsider that even discovery-focused parts of mathematics have informal components:\nThey require the refinement of guesses by speculation and criticism, heuristics, and\nexploration (Lakatos 1963).\nThe second limitation of deductive procedures is their understandability: If the ex-\nplanation exceeds a certain length, it will be hard for a human user to follow. The\ncausal and relational complexity of the real world would however require deductive\nprocedures of unbound length, making understandable procedures utopian.\n4.3 Natural Language Explanations\nGenerating natural language explanations has gained relatively little attention until a\nfew years ago. While some works use more restrictive techniques to generate textual\nexplanations, such as template-based approaches (N. Wang et al. 2018; Yongfeng Zhang\net al. 2014) or approaches based on extractive summarisation (Atanasova et al. 2020b),\nit was only with the emergence of GPT-2 and other generative Transformer models\nthat they gained more traction. Datasets with human-written free-text explanations1\nfor the correct label were created for tasks like natural language inference (Camburu\net al. 2018) and multiple-choice question answering (Aggarwal et al. 2021; Rajani et al.\n2019), along with models fine-tuned to imitate these explanations.\nNatural language explanations address many of the limitations that attribution meth-\nods and procedural methods have. They are easily accessible to human users than\nother forms of explanation, especially to end users without a technical background.\nMoreover, they can incorporate forms of reasoning not covered by the other methods:\nThey allow for the inclusion of any input-external knowledge and any type of rea-\nsoning that can be expressed in natural language, while not being restricted to tasks\nwhere it is possible to provide a complete reasoning path.\nThe incorporation of free-text explanations in the learning process has in some cases\nlead to an increase in performance and robustness. The idea behind this approach is\nthat with the explanations as additional supervision, models can be guided to make\ndecisions in a more accurate way and rely less on spurious correlations in the dataset,\nas the decision-making process is better aligned with the expected forms of reasoning.\nThis may also improve the model’s robustness, as it may reduce the dependence on\ncues from one dataset that do not generalise to other datasets. While similar ideas\n1At this point, I use free-text explanations as a narrower term than natural language explanations\nas it excludes template-based or extractive natural language explanations. In the remainder\nof this chapter, I use the two terms interchangeably, both referring to explanations generated\ntoken-by-token by an LLM.\n35\n4. E XPLANATIONS\nare also applied to attribution methods (Chen et al. 2022), the freer form of natural\nlanguage explanations may extend the benefits to more tasks.\nThe need of manual annotations of explanations, the creation of which is costly, is a\nlimiting factor in generating free-text explanations (Belinkov and Glass 2019). This\nproblem may have been partially resolved with LLMs like GPT-3 and beyond that\nhave few-shot and zero-shot capabilities. An objection to free-text explanations that is\nof enduring relevance is however that the explanations have no obvious connection to\nthe label prediction. As Bommasani et al. (2021) write:\n“[...] there are reasons to be skeptical: language models, and now foundation\nmodels, are exceptional at producing fluent, seemingly plausible content without\nany grounding in truth. Simple self-generated “explanations” could follow suit.\nIt is thus important to be discerning of the difference between the ability of a\nmodel to create plausible-sounding explanations and providing true insights into\nits behavior.”\nBommasani et al. (2021)’s sceptical remarks are justified: We cannot trust LLM-\ngenerated explanations to accurately present its inner workings when generating\na label. However, as I will discuss in this section, just as most other explanation\ntechniques, they are able to provide us with some insight about the model’s inner\nworkings. As the kind of insight they provide us with are not currently achievable by\nother techniques, they are a valuable component of model explainability and can be\nuseful to both developers and users.\nIn this section, I introduce influential applications and datasets (§4.3.1) and common\napproaches to generating free-text explanations (§4.3.2). Finally, I discuss the eval-\nuation of free-text explanations and the question how we can determine if they are\nfaithful to the prediction process (§4.3.3).\n4.3.1 Applications and Datasets\nDatasets containing free-text explanations broadly fit into two categories: Tasks with a\nfocus on logical and mathematical reasoning, and tasks that require factual knowledge\nand commonsense reasoning.\nLogical Reasoning\nThe first category of datasets require logical or arithmetic reasoning between state-\nments to solve a task.\nA widely used dataset that pioneered the generation of free-text explanations is eSNLI\nby Camburu et al. (2018). They add explanations to the natural language inference\n(NLI) datasets SNLI (S. R. Bowman et al. 2015). The task of SNLI is to classify the\nentailment relation of two sentences (the premise and the hypothesis) into contradiction,\nneutral or entailment. An example of this is the following, where the explanation points\nat the logical problem in inferring the hypothesis from the premise, resulting in the\nlabel contradiction:\n36\n4.3. Natural Language Explanations\nPremise: A shirtless man is singing into a microphone while a woman next to\nhim plays an accordion.\nHypothesis: He is playing a saxophone.\nLabel: contradiction.\ne-SNLI Explanation: A person cannot be singing and playing a saxophone\nsimultaneously.\nThe SNLI dataset has been shown to contain unintended cues that models can rely\non when making predictions (Gururangan et al. 2018). Other, more diverse datasets\nalso rely on such cues (T. McCoy et al. 2019), making it uncertain how well models\nsolve the actual task even if the model scores high in an in-domain evaluation. Textual\nexplanations that exhibit the expected reasoning can be valuable to assess the potential\nof the model to solve the task accurately and its limitations. In this line, Zhou and Tan\n(2021) extend the adversarial HANS dataset (T. McCoy et al. 2019) with explanations\nand show that model-generated explanations contain a high lexical overlap with the\nhuman-written explanations but hallucinate information and miss crucial relations.\nGuiding models through explanation generation sometimes improves their perfor-\nmance. In chain-of-thought prompting (Wei et al. 2022), the model generates interme-\ndiate reasoning steps before the prediction in a few-shot setting on arithmetic word\nproblems. The approach has been shown to substantially boost the performance for\nappropriate tasks when combined with very large models. The most widely used\ndataset for chain-of-thought prompting is GSM8K (Cobbe et al. 2021). GSM8K consists\nof a question, the reasoning path, and the final answer, as shown in the example below:\nQuestion: Weng earns $12 an hour for babysitting. Yesterday, she just did 50\nminutes of babysitting. How much did she earn?\nAnswer: Weng earns 12/60 = $12/60=0.2 Ñ 0.2 per minute.\nWorking 50 minutes, she earned $0.2 x 50 = $0.2*50=10 Ñ $10.\nFinal Answer: $10\nKojima et al. (2022) do zero-shot prompting on a set of arithmetic reasoning tasks\n(including GSM8K) and other logical reasoning tasks, simply appending Let’s think\nstep by step to the prompt. This alone results in a substantial task performance increase.\nFor e-SNLI however, such increases have not been observed. Zhao and Vydiswaran\n(2020) address this issue and identify two problems: Firstly, unlike humans, past\nmodels did not consider alternative explanations but focus on explaining the correct\nlabel. Secondly, the models do not reason about the correctness of facts but focus\non creating well-formed sentences. They include a candidate explanation for every\npossible label, and use an instance selector to reason about which one is correct. The\nperformance increase they report with this approach is however slight.\nHowever, departing from training on the e-SNLI explanations, He et al. (2023) perform\nfew-shot prompting similar to chain-of-thought prompting on several NLI datasets\nincluding SNLI but also various harder and adversarial datasets. They show that this\nsetup improves task performance and robustness.\n37\n4. E XPLANATIONS\nKnowledge-based Reasoning\nThe second category of datasets containing explanations that I will present are tasks\nthat require commonsense and factual knowledge that is not present in the input for\ntheir reasoning.\nCoS-E (Rajani et al. 2019) and ECQA (Aggarwal et al. 2021) extend the multiple-choice\nquestion answering dataset CommonsenseQA (Talmor et al. 2019) with crowd-sourced\nexplanation. While CoS-E provides a brief explanation only for the correct answer,\nECQA also contains an explanation for each of the four incorrect answers, with a\nreason why this option is wrong. In addition, ECQA features a long-form explanation\nthat contrasts all answer options. The following is a shortened example from ECQA,\nwith the question, the correct answer option along with an explanation, as well as one\nwrong answer option along with an explanation:\nQuestion: What is something that people do early in the day?\nCorrect Answer: Eat eggs.\nExplanation: People generally eat breakfast early morning. People most\noften eat eggs as breakfast.\nNegative Answer (Example): Believe in God.\nExplanation: Believing in God is not restricted to a specific part of a day.\nIn the Commonsense Validation and Explanation (ComVE; C. Wang et al. (2020))\ndataset, the task is to predict and explain whether a natural language statement makes\nsense. In the prediction task, the model is presented with two statements, and needs\nto select the nonsensical one. The explanation task has two modes; a multiple-choice\nmode where the correct explanation must be selected from three explanations with\nsimilar wordings, and a free-text generation mode. An example for a multiple-choice\ninstance containing a nonsensical statement and three options for explanations is the\nfollowing:\nStatement: John put an elephant into the fridge.\nOption 1: An elephant is much bigger than a fridge. (correct)\nOption 2: Elephants are usually white while fridges are usually white.\n(wrong)\nOption 3: An elephant cannot eat a fridge. (wrong)\nFor a similar multiple-choice question answering dataset, Latcinnik and Berant (2020)\ninclude an intermediate textual layer in their model to generate explanations. However,\nthey do not perform supervised training with annotated explanations as a target but\ntrain their models with weak supervision to produce explanations that are useful\nfor the downstream classifier. Park et al. (2018) create the visual question-answering\ndatasets VQA-X and ACT-X by enriching existing datasets with textual justifications\nfor answers as well as relevant positions highlighted in the image. They show that it\ndepends on the sample if visual or textual explanations are more useful, concluding\nthat multimodal explanations are preferable for multimodal tasks.\n38\n4.3. Natural Language Explanations\nTraining Phase 1\nTraining Phase 2\nFigure 4.3: Graphical representation of the categorisation proposed by Hase et al.\n(2020). x is the input, y the output and e the explanation. Figure by Hase et al. (2020).\n4.3.2 Approaches\nIn the earlier works on generating free-text explanations, the explanation is generated\neither completely independently of (Rajani et al. 2019) or prior to the label prediction\nmodel (Camburu et al. 2018; Latcinnik and Berant 2020). As Latcinnik and Berant\n(2020) acknowledge, this is a weak form of explanation as the prediction process cannot\ninfluence the explanation generation. Hase et al. (2020) introduce a categorisation of\nthe approaches, calling the former type of approach that conditions the explanation\nonly on inputs reasoning (RE) mode, and the latter that considers the input as well as\nthe label rationalising (RA) mode. They also consider if the explanations are used as\nan additional input in a pipeline model which they call a serial-task (ST) approach or\nexplanations are generated jointly with the labels, called a multi-task (MT) approach.\nA graphical representation of the four approaches is presented in Figure 4.3. Hase\net al. (2020)’s multi-task reasoning mode has subsequently become known as self-\nrationalising models, the term that I adopt in this thesis.\nWith the increasing multi-task capabilities of LLMs, self-rationalising models have\nbecome increasingly common. Narang et al. (2020) show that they can successfully\ngenerate labels and explanations at the same time, and that the self-rationalisation\ncapabilities can, to some extent, even be transferred to other datasets. Wiegreffe et al.\n(2021) show that self-rationalising models have a higher performance than serial-task\narchitectures.\nThese first approaches all used fine-tuned models. After the release of GPT-3 and the\nincreasing capabilities of models in in-context learning, Marasovic et al. (2022) were\nthe first to propose a few-shot approach that jointly generates model predictions and\nexplanations. They explore how the prompt should be formatted for such a setup, and\nshow that while the prompt has a significant impact on the performance, humans rate\ntheir generated model explanations as significantly less plausible than human-written\nexplanations.\nThese findings have also been exploited for the generation of datasets. Synthetic, LLM-\ngenerated datasets are becoming more common as they reach a reasonable quality for\nmany NLP tasks and applications with a fraction of the cost, and the same appears to be\ntrue for LLM-generated natural language explanations datasets. Wiegreffe et al. (2022)\ngenerate candidate explanations with GPT-3 and train an acceptability filter based on\nhuman ratings of the explanations. They show that human raters often prefer the LLM-\ngenerated over the human-written explanations. A similar result has been reported\nby He et al. (2023) who compare few-shot prompting for NLI with human-annotated\n39\n4. E XPLANATIONS\nversus ChatGPT-annotated explanations. They find that, surprisingly, humans prefer\nthe LLM-annotated explanations while the human-annotated explanations are more\nbeneficial for task performance for four out of five models.\n4.3.3 Evaluation\nSimilar to the evaluation of input relevance measurements in Section 4.1, there are\nseveral properties of natural language explanations that can be evaluated but that\nshould not be conflated. As we evaluate similarity of input attribution methods to\nhuman relevance judgements, we can evaluate the similarity of model-generated\nexplanation to human-written explanations. Faithfulness evaluation is even more\nchallenging and more experimental for free-text explanations than it is for input\nattribution methods, but nonetheless, there exist approaches that attempt to estimate\nthis property.\nSimilarity to Human Explanations and Plausibility\nSimilar to other text generation tasks, the automatic evaluation of explanations has\nthe challenge that the same thing can be expressed in many different ways. For\nmany tasks and data instances, it is even possible to create various valid reasoning\nchains. Therefore, even where human-annotated explanations exist, it is not necessary\nto expect exactly the same explanation from the model, we can at most expect a\nsimilar meaning. Therefore, test generation metrics that measure the semantic overlap\nbetween generated and annotated explanations are commonly used.\nTwo widely used metrics in text generation tasks are BLEU (Papineni et al. 2002),\na group of metrics originally developed for machine translation that measures the\nn-gram precision of generated text and ROUGE (C.\n-Y. Lin 2004), a group of metrics\noriginally developed for text summarisation that measures the n-gram recall. BLEU\nand ROUGE however measure the surface overlap between texts and require exact\nmatches of the n-grams. In consequence, they are not robust to lexical and syntactical\nvariations and are less indicative for criteria such as the acceptability of a generated text\n(Ananthakrishnan et al. 2007; van der Lee et al. 2021). For this reason, the BERTScore\nmetric family (T. Zhang et al. 2020) has been proposed. Instead of surface overlap, it\nmeasures the pairwise cosine similarities between the contextual token embeddings\nfrom the BERT model. T. Zhang et al. (2020) show that BERTScore correlates better\nwith human judgements than metrics based on surface overlap.\nThese metrics can give an idea of how well the generated explanations reflect the\nhuman-written explanations from a dataset. However, although better than previous\nmetrics, even the BERTScore metrics are far from perfect: In particular, even they have\nbeen shown to be overly reliant on lexical overlap (Hanna and Bojar 2021).\nA more general problem is that the reference explanations are expected to have large\nvariance between annotators, given that the possible set of valid explanations is large.\nThis may induce annotator bias into the evaluation (Geva et al. 2019). Therefore\nevaluation approaches that compare generations to a gold standard are not considered\nsuitable as a sole base for the evaluation of natural language generation tasks (Amidei\net al. 2018).\n40\n4.3. Natural Language Explanations\nTo judge the plausibility of the generated explanations independent of specific human\nannotations, it has therefore become common to include a human evaluation of a\nsubset of the evaluation set (Marasovic et al. 2022; Wiegreffe et al. 2022). However,\nwhile human ratings are often treated as a gold standard, even they have been shown\nto rely on surface qualities of the text such as fluency (E. Clark et al. 2021). Moreover,\nthe inter-rater agreement in human evaluations is often low. Amidei et al. (2018)\nstudy what influences the agreement and find that ratings diverge for generation\ntasks due to inter-personal differences in factors such as personal style, attention to\ndetail, background knowledge and personal assumptions. They conclude that such\ndifferences are inherent to the nature of such studies, and that the inter-rater agreement\nis not a measure that is supposed to be maximised.\nFaithfulness\nAs I have outlined in the introduction of this chapter, the faithfulness of a self-\nrationalising model is, by default, uncertain. There is no inherent reason to assume that\nthe generated explanations accurately reflect the models’ prediction process. Therefore,\na line of work has emerged that estimates the explanations’ faithfulness to the label\nprediction process.\nHase et al. (2020) propose the leakage-adjusted simulatability (LAS) metric to evaluate\nhow predictable (simulatable) a model’s decisions are to an observer, given the explana-\ntions. They argue that the LAS score is a measurement of faithfulness because it is an\nindication that the content of the explanations influences the predictions. The authors\ncontrol for predictability from the input alone by comparing to an input-only baseline,\nas well as for label leakage in the explanation by testing if the label can be predicted\nwith the explanation alone. They find that rationalising models achieve better LAS\nscores than reasoning model, and that the serial-task rationalising model gets similar\nscores as humans.\nOverall, experimental faithfulness evaluations based on input interventions have lead\nto mixed conclusions. Wiegreffe et al. (2021) show that there are correlations of the\nlabel prediction and the explanation generation process by adding noise to the input\nand showing that both are affected in similar ways. They conclude that their results\nindicate the potential to generate faithful free-text explanations. Atanasova et al.\n(2023) propose analysing the faithfulness of textual explanations after counterfactual\ninterventions on the input that alter the prediction. They find that for many instances,\nit is possible to find an edit that leads to an unfaithful explanation. They also test for\nthe sufficiency of the reasons provided in the explanations, again finding a substantial\nnumber of unfaithful explanations. Turpin et al. (2023) add biasing features to the\ninput which affect the prediction, e.g. providing few-shot multiple-choice examples\nwhere the selected answer is always the first one. Those biases are never reflected in\nthe explanations generated by the models they test, showing that the predictions are\nnot faithful to the explanations. Critiquing the aforementioned works, Parcalabescu\nand Frank (2024) argue that what they measure is not faithfulness but self-consistency,\nwhich is a necessary, but not a sufficient condition for faithfulness. They argue that\nthe inner workings for generating predictions and explanations could be consistent,\nbut still different, and that therefore, evaluating faithfulness is still an open problem.\n41\n4. E XPLANATIONS\nAn alternative approach to estimating the faithfulness of explanations is comparing\nthe performance of models that include explanations to models that do not provide\nexplanations for their predictions. If self-rationalisation adds performance and ro-\nbustness, this is an indication that the prediction process is at least partly guided\nby the reasoning provided in the explanations. Ross et al. (2022) test the effect of\nself-rationalisation on the robustness to spurious correlations in fine-tuned models by\nevaluating on challenging subsets and datasets that are designed for not containing\nknown spurious correlations in NLI (Gururangan et al. 2018; T. McCoy et al. 2019).\nThey find that it does not generally make models more robust. However, they identify\ntwo factors that make a model more likely to benefit from self-rationalisation: fewer\nfine-tuning resources and larger model sizes.\n4.3.4 Summary\nNatural language explanations can cover a broad set of problems and are commonly\nmore accessible to humans than other forms of explanation. As the capabilities of\nLLMs have increased and few-shot and zero-shot prompting has become possible,\nself-rationalisation is now a simple and powerful method to generate natural language\nexplanations. However, the properties of free-text explanations are still underexplored.\nOur work in Papers IV and V aims at understanding their role and effects better.\n42\n5 Paper Summaries\nIn this chapter, I will summarise the contributions of the five papers included in this\nthesis in Sections 5.1 to 5.5. The published versions of the full papers are included\nin Part II; they are adjusted in formatting but the text is not altered. In Section 5.6, I\nwill give an overview of other papers that I have contributed to during my time as a\nPhD student. These papers are not an official part of this thesis. Although the main\ncontributions of two of these papers are outside this field, I will specifically reflect on\ntheir relation to interpretable and explainable NLP .\n5.1 Paper I\nJenny Kunz and Marco Kuhlmann (Dec. 2020). “Classifier Probes May Just Learn\nfrom Linear Context Features”. In: Proceedings of the 28th International Conference on\nComputational Linguistics. Barcelona, Spain (Online): International Committee on\nComputational Linguistics, pp. 5136–5146. DOI : 10.18653/v1/2020.coling-\nmain.450. URL : https://aclanthology.org/2020.coling-main.450.\nIn this paper, we question an assumption that was often stated in early probing\nwork: That the contextual representation of one token does not contain sufficient\ninformation for a probe to learn common sentence-level probing tasks without specific\nlinguistic structure already being encoded in the representation. However, through the\ncontextualisation with the multi-head self-attention mechanism, the representation\ncan contain detailed information about the whole sentence. This information enables\nthe learning of all sentence-level tasks as crucial features (in particular, information\nabout the surrounding words) are present.\n43\n5. P APER SUMMARIES\nWe show experimentally that detailed linear context features are actually contained\nin the representation by constructing a probing task that, from the representation of\none token at a time, predicts the exact identity of the neighbouring words at a specific\nposition. Our results show that for the BERT model, more than half of the direct\nneighbours of the word in the sentence are recoverable exactly, and even more distant\nneighbours are recoverable to a substantial extent. For ELMo, the numbers are lower\nbut still much higher than for non-contextual baselines.\nBased on these results, we develop a framework that is centred around what we call\nthe context-only hypothesis: We call for probing experiments to disprove the assump-\ntion that the only information that the probe uses to learn the probing task is linear\ninformation about the identity of the neighbouring words. We show theoretically and\nexperimentally that none of the baselines or restrictions imposed on the classifier in\nthe literature can disprove this hypothesis. Therefore, we argue that these methods\nare not a strong foundation for conclusions about linguistic features being explicitly\nencoded in the representations.\n5.2 Paper II\nJenny Kunz and Marco Kuhlmann (Nov. 2021). “Test Harder than You Train: Probing\nwith Extrapolation Splits”. In: Proceedings of the Fourth BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for NLP. Punta Cana, Dominican\nRepublic: Association for Computational Linguistics, pp. 15–25. DOI : 10.18653/\nv1/2021.blackboxnlp-1.2 . URL : https://aclanthology.org/2021.\nblackboxnlp-1.2.\nIn this paper, we take inspiration from our first paper and increase the restrictiveness\nof the probing classifier. As machine learning models typically operate within an\ninterpolation setting, we hypothesise that the ability to extrapolate would be an indi-\ncation that the probe decodes linguistic knowledge rather than learning the task from\nscratch. For this purpose, we take inspiration from curriculum learning and define a\nset of linguistic, statistical and learning-related scoring functions after which we rank\nthe difficulty of training examples. These rankings are used to define train-test splits\nwhere we train only on easy examples and evaluate only on hard examples.\nWe analyse the relative merits of these criteria experimentally and theoretically, finding\nthat the results vary greatly across scoring functions. The linguistic and statistical\ncriteria that allow for the best-motivated splitting points show the clearest differences\nbetween interpolation and extrapolation settings, indicating that arbitrary splitting\npoints do not define true extrapolation setups. The most discriminating ranking\nfunction for the syntactic dependency label prediction probing task is the length of the\nsyntactic dependency arc, where the easy samples are the samples where the syntactic\nhead is a direct neighbour. For the part-of-speech-tagging probing task, the most\ndifferentiating criterion is a binary statistical criterion, where the easy samples are the\nones where a word form is assigned its most frequent label. Our experiments show\nthat for the setups with well-motivated splitting points, the ability to extrapolate to\nharder examples is limited, but not completely absent.\n44\n5.3. Paper III\n5.3 Paper III\nJenny Kunz and Marco Kuhlmann (Oct. 2022). “Where Does Linguistic Information\nEmerge in Neural Language Models? Measuring Gains and Contributions across\nLayers”. In: Proceedings of the 29th International Conference on Computational Lin-\nguistics. Gyeongju, Republic of Korea: International Committee on Computational\nLinguistics, pp. 4664–4676. URL : https://aclanthology.org/2022.coling-\n1.413.\nThe third paper also proposes a way to overcome the question if a task is encoded\nin the representation or learned by the probe. We offer a new perspective on how to\napproach, evaluate and interpret probing results with a new family of metrics that\nfocuses on the information flow through the model instead of the isolated performance\nof one layer at a time. The new metrics focus on the local information gain from one\nlayer to the other, and calculate the contribution of this layer to the overall performance\nof the model.\nThis new focus shows that it is the very first layers of the BERT model that contribute\nthe most new information to the performance on syntactic tasks, and not the middle\nlayers as one may conclude based on traditional, global probing metrics. The informa-\ntion is just preserved in the model, which leads to a higher performance due to more\naccumulated information.\nWe also test if expected hierarchies between probing tasks hold: That information\nuseful for predicting syntactic parents is gained in earlier layers than information\nrelevant for syntactic grandparent prediction, and that information for predicting the\nmost frequent part-of-speech tags for a word form is gained earlier than information\nfor less frequent tags, as those may require more context. Our experimental results\nshow that the hierarchy holds in the new metrics only for the first task pair; for the less\nfrequent part-of-speech tags, the most significant gains happen in the very first layers,\nearlier than for the most frequent tags. We also show that the structure of information\nvaries greatly between models in different languages.\n5.4 Paper IV\nJenny Kunz, Martin Jirenius, Oskar Holmström, and Marco Kuhlmann (Dec. 2022).\n“Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Expla-\nnations for Model Predictions”. In: Proceedings of the Fifth BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for NLP. Abu Dhabi, United Arab\nEmirates (Hybrid): Association for Computational Linguistics, pp. 164–177. DOI :\n10.18653/v1/2022.blackboxnlp-1.14 . URL : https://aclanthology.\norg/2022.blackboxnlp-1.14. Best Paper Award Winner.\nThe fourth paper is our first contribution to the field of natural language explanations.\nWe explore how the utility of explanations for a downstream model compares to\nhuman ratings of the same explanations by training and evaluating explanation-\ngenerating models with two different pipeline architectures: one serial-task model that\ngenerates only the explanation in the first step, and one multi-task model that jointly\n45\n5. P APER SUMMARIES\ngenerates predictions with the explanations. Both pipelines consist of a fine-tuned\nGPT-2 for the generation of the explanations and BERT for the final classification, and\nare trained and evaluated on e-SNLI and ECQA.\nWe find that the explanations by the serial-task model are rated higher by humans\nwith respect to their validity and factual correctness and score higher in similarity\nmeasures compared to human-annotated gold explanations. However, in the utility\nfor a downstream classification model, the explanations by the multi-task model score\nslightly higher. We observe that the latter explanations are a classical example of\nhallucinations, as they contain more novel information and more factually incorrect\ninformation. This is punished by human annotators but slight performance improve-\nments may be an indicator that the novel information can in some cases be useful\ncontext for downstream models.\nA second insight from this paper is that fine-tuning the downstream classifier with\ngold explanations only leads to failure at evaluation time when switching to generated\nexplanation. This indicates an over-reliance on information in the explanations. How-\never, further fine-tuning on generated explanations solves this problem. The classifier\nlearns to handle the less reliable information from the generated explanations and, in\nthe case of the harder ECQA dataset, improves its performance over the model that\ndoes not receive explanations as an additional input substantially.\n5.5 Paper V\nJenny Kunz and Marco Kuhlmann (2024). Properties and Challenges of LLM-Generated\nExplanations. arXiv: 2402.10532 [cs.CL]. URL : https://arxiv.org/abs/\n2402.10532. Under Review.\nIn the fifth paper, we focus on zero-shot explanations generated by GPT-4 and explore\ntheir properties. We annotate an instruction fine-tuning dataset for categories of\ninstruction, and the outputs of GPT-4 for known properties of human explanations\nthat have been pointed out as problematic for the goals of explainable NLP .\nWe find that the explanations often list an incomplete set of contributing reasons,\nbut argue that this is not avoidable due to the open and complex nature of many\ninstructions. We also find a large number of explanations that contain illustrative\nelements, which likely have no connection to the model’s reasoning but on the other\nhand help make the reasoning more accessible to the user. Only few explanations are\nsubjective, which we assume is related to GPT-4’s alignment process that discourages\nthe inclusion of subjective statements in the output. Misleading explanations for\nanswers that are wrong are also rare, but we assume that this is an artefact of the\ndataset we use, where the instructions themselves are LLM-generated and therefore\nvery likely to be answerable correctly by GPT-4.\nWe discuss the effects of the presence or absence of these properties on different goals\nof explainable NLP and different user groups that may use LLMs. We argue that all\nproperties can have positive or negative sides, depending on the use case.\n46\n5.6. Other Works\n5.6 Other Works\nDuring my PhD studies, I also contributed to the following papers.\n5.6.1 Constructing Surrogate Models for Textual Explanations\nMarc Braun and Jenny Kunz (2024). A Hypothesis-Driven Framework for the Analysis\nof Self-Rationalising Models. arXiv: 2402.04787 [cs.CL]. URL : https://arxiv.\norg/abs/2402.04787 To Appear at the Student Research Workshop at the\n18th Conference of the European Chapter of the Association for Computational\nLinguistics.\nIn this work, we build surrogate models based on a Bayesian Network that allow us\nto test hypotheses on how a task (in our case, NLI) is solved. The Bayesian Network\nmodels interactions between phrases of the premise and hypothesis in an interpretable\nway, where its internal states can be translated to textual explanations via templates.\nWe compare the explanations generated with the surrogate models to the explanations\ngenerated by few-shot-prompted GPT-3.5. We find that the explanations can take\nsimilar shapes to the ones in the e-SNLI dataset, but that the performance of the\nBayesian Network model is too low to reach a high similarity to GPT-3.5 in quantitative\nmetrics. Due to the template-like structure of e-SNLI explanations, we do however\nsee the potential to better approximate the GPT-3.5 explanations and decisions with a\nmore sophisticated surrogate model.\n5.6.2 Understanding Cross-Lingual Transfer\nAnother line of work analyses how cross-lingual transfer is done in LLMs. Two\npapers have so far come out of this side project. The first one investigates if GPT-\n3.5’s generations are language-specific or if the same capabilities are shared across\nlanguages. In the second one, we perform extensive ablations on the contribution of\nlanguage adapters to zero-shot cross-lingual transfer.\nTransfer of Capabilities across Languages: Frameworks and Evaluation\nOskar Holmström, Jenny Kunz, and Marco Kuhlmann (May 2023). “Bridging\nthe Resource Gap: Exploring the Efficacy of English and Multilingual LLMs for\nSwedish”. In: Proceedings of the Second Workshop on Resources and Representations\nfor Under-Resourced Languages and Domains (RESOURCEFUL-2023). Tórshavn, the\nFaroe Islands: Association for Computational Linguistics, pp. 92–110. URL : https:\n//aclanthology.org/2023.resourceful-1.13.\nIn our first paper on cross-lingual transfer, we compare GPT-3.5’s English and Swedish\nabilities concerning whether capabilities are shared across languages, or if distinct\nsubmodels are activated depending on the input language.\nWe find that many capabilities are shared: With Swedish input, the performance is\nrelatively close to GPT-3.5 in English, and differences can in part be explained by trans-\nlation errors from the evaluation set. As other LLMs with substantially more Swedish\n47\n5. P APER SUMMARIES\npre-training data have much more limited capabilities on the same tasks, we assume\nthat the Swedish submodel would either be too small or not have the appropriate\ntraining data for these capabilities to be learned. Therefore we conclude that those\ncapabilities must be shared with or transferred to the lower-resource languages in the\nmodel.\nHowever, we still find a distinct behaviour of GPT-3.5 when prompted in Swedish as\ncompared to English. A small study with prompts that we hypothesise can trigger\nculture-specific answers shows that indeed, the output reflects common cultural\npractices of Sweden versus the United States of America. We conclude that the transfer\nbehaviour that GPT-3.5 shows is promising; with transfer happening where it is\ndesired but with specific outputs where appropriate.\nLanguage Adapters as Modular Components\nJenny Kunz and Oskar Holmström (2024). The Impact of Language Adapters in Cross-\nLingual Transfer for NLU. arXiv: 2402.00149 [cs.CL]. URL : https://arxiv.\norg/abs/2402.00149 To Appear at the First Workshop on Modular and Open\nMultilingual NLP (MOOMIN) at the 18th Conference of the European Chapter of\nthe Association for Computational Linguistics.\nIn the second paper on cross-lingual transfer, we explore the effect of language adapters\nwhen performing zero-shot transfer for natural language understanding tasks. Specifi-\ncally, we train task adapters with the language adapter of the source language active\nand exchange the source-language adapter against the target-language adapter at\nevaluation time.\nWe perform an extensive set of ablations on the impact of the target-language adapters.\nWe observe that it is often possible to keep the source-language adapter at evaluation\ntime instead or even to remove the language adapter without substitution without a\nstrong negative effect. Sometimes, keeping the source-language adapter even outper-\nforms using the target-language adapter. For the context of interpretable NLP , this\nsuggests that language adapters trained independently of the model do not play a\nconsistent, and thereby interpretable, role.\n48\n6 Conclusion\nIn this final chapter, I will summarise the contributions of this thesis and set them into a\nwider context in Section 6.1. Finally, in Section 6.2, I will provide an outlook concerning\ncurrent trends and developments in the field of interpretable and explainable NLP .\n6.1 Summary\nIn this thesis, I have presented our contributions to enhancing interpretability tech-\nniques and better understanding the properties of natural language explanations\ngenerated by LLMs.\nOur contributions to the field of interpretability focused on probing classifiers that\nmeasure linguistic information that the internal representation of the model, often\nfrom a specific layer, contains. We developed a framework for a rigid assessment of\nthe results of probing classifiers. A crucial distinction that we discuss in this paper is\nif the probing classifier decodes information from the representation or if it learns the\nprobing task. We show that a classical argument why probes should not be able to\ndo the latter does not hold and that common restrictions to the probing methodology\nare not able to make the distinction either. Building on these results, we developed\nmore challenging probing methods that focus on the ability of the probe to extrapolate\nfrom easy to hard examples. We argue that as classifiers generally operate in an\ninterpolation setup, extrapolation capabilities would be an indication that the probe\nhas found generalisable features from the representation. We designed new evaluation\nmetrics that focus on modelling local information gains throughout the model and\neach layer’s contribution to the model’s overall performance. This shifts the focus on\n49\n6. C ONCLUSION\nwhere syntactic information is located in the model from the middle layers, where the\noverall performance is the highest, to the earliest layers, which contribute the most\nnew information. Both the focus on extrapolation capabilities and the focus on local\ninformation gains provide new perspectives to the probing method which can help us\nto gain more robust insights.\nIn the field of explainability, we focused on the properties and utility of free-text\nexplanations generated by LLMs. We showed that human ratings, particularly of the\nfactual correctness of the explanations, are not indicative of the performance when\nusing the generated explanations in a downstream model. For the latter, including\nmore novel information in the explanations appears to be beneficial, but this comes at\nthe risk of including incorrect or irrelevant information, which human raters punish\ndecisively. We further examined which common properties of human explanations\nare commonly reflected in LLM-generated explanations. The results of our annotation\nstudy indicate that they often list incomplete sets of contributing reasons as well as\nillustrative examples.\nInterpretation and explanation methods can complement each other in shaping a better\nunderstanding of LLMs. The former methods give us a high-level understanding of\nhow the individual tokens are contextualised and, layer for layer, form a representation\nuseful for many applications. The latter methods give us an idea of the context and\nreasoning accessible to the model when making a prediction, even if the explanations\nare not faithful to the model’s decision process. Together with an understanding\nof the LLMs’ architecture and training objectives, such methods make it possible\nto achieve a coarse understanding of the decision-making process and be able to\npredict the models’ behaviour to a certain extent. This understanding is insufficient\nto allow for the deployment of LLMs for high-stakes decisions without a human in\nthe loop. However, it has the potential to enable developers and users to make better\ndecisions on whether the model will be able to perform a certain task, whether its\ndecision-making is sufficiently robust, and how it can be improved.\n6.2 Outlook\nFinally, I outline some questions that I consider central for future work in interpretable\nand explainable NLP , based on our research on these topics.\n6.2.1 Understanding the Internal Processes of LLMs\nProbing classifiers have given us valuable insights into how linguistic information is\nstructured within an LLM and how this representation is formed during the training of\nthe model. We have made contributions to this method in Papers I, II and III. However,\nlike with many other interpretability methods, as outlined in Chapter 3, those insights\nare coarse and need to be interpreted relative to a baseline or to other models or\nlayers. An open question is if it is possible to reach a more general and fine-grained\nunderstanding that is still comprehensible to human observers.\nRecently, mechanistic interpretability has attracted the attention of many researchers,\nwith some successes reported on the reverse engineering of toy models. Large amounts\n50\n6.2. Outlook\nof resources are flowing into this field from both industry and academia. As I discussed\nin Chapter 3.3, results in NLP are so far focused on specific features rather than\nalgorithm discovery and are far from the declared goal of meaningfully explaining\ndecision-making in any real-world application. Whether even this field hits a wall\nwith toy tasks and standalone features or whether it can give us more fine-grained\nmechanistic insights into how LLMs work remains to be seen.\n6.2.2 Building LLMs that are More Interpretable by Design\nAnother promising line of research is building coarsely interpretable models without\nlosing the capabilities LLMs are appreciated for. Such models could be trained to\nhave modules that fulfill specific interpretable functions. Pfeiffer et al. (2022) have\nsuccessfully employed such a modular approach for language modules in encoder\nmodels. Unlike our own experiments with language adapters trained post-hoc, as\nsummarised in Section 5.6.2, such modules that are present already at pre-training\ntime may be an isolated encapsulation of a specific property. It would be insightful,\nalthough more complex, to test a similar approach for capabilities other than handling\ndifferent input languages. Such models are not fully explainable in that all details of\nthe decision-making process are comprehensible but offer many insights that are not\npossible with LLMs by being more controllable as they allow for targeted interventions.\n6.2.3 Targeted Explanations that Consider the User’s Needs\nAs we have outlined in Papers IV and V , explanations can have different goals and\ntarget groups, and properties beneficial for one user group can be disadvantageous\nfor another. However, research that explicitly states the intended target group for the\nexplanations, let alone specifies their needs, is sparse in explainable NLP research.\nThis ignorance contributes to the fact that currently, end users of the systems rarely\nengage with explanations and either ignore the system’s recommendations entirely or\nblindly follow its predictions (Miller 2023). It has become clear that it is not sufficient\nfor researchers and developers to follow their intuitions on how to design explainable\nsystems (maybe unless the target group is themselves). Therefore, two challenges\nneed to be solved: First, the technical challenge to make generated explanations reflect\nthe decision-making process to an extent sufficient for the problem. And second, the\nchallenge to design the explainable system in a way that reflects the actual target\ngroup’s needs and is accessible to them.\n51\n\nBibliography\nAdi, Yossi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg\n(2017). “Fine-grained Analysis of Sentence Embeddings Using Auxiliary\nPrediction Tasks”. In: 5th International Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceed-\nings. OpenReview.net. URL : https://openreview.net/forum?id=\nBJh6Ztuxl.\nAggarwal, Shourya, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh\nKhandelwal, Parag Singla, and Dinesh Garg (Aug. 2021). “Explanations\nfor CommonsenseQA: New Dataset and Models”. In: Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers). Online: Association for Computational Linguistics, pp. 3050–\n3065. DOI : 10.18653/v1/2021.acl- long.238 . URL : https://\naclanthology.org/2021.acl-long.238.\nAlain, Guillaume and Yoshua Bengio (2017). “Understanding intermediate\nlayers using linear classifier probes”. In: 5th International Conference on\nLearning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Work-\nshop Track Proceedings. OpenReview.net. URL : https://openreview.\nnet/forum?id=HJ4-rAVtl.\nAlvarez-Melis, David and Tommi S. Jaakkola (2018). “On the Robustness of\nInterpretability Methods”. In: cite arxiv:1806.08049Comment: presented\nat 2018 ICML Workshop on Human Interpretability in Machine Learning\n(WHI 2018), Stockholm, Sweden. URL : http://arxiv.org/abs/1806.\n08049.\n53\nBIBLIOGRAPHY\nAmidei, Jacopo, Paul Piwek, and Alistair Willis (Aug. 2018). “Rethinking\nthe Agreement in Human Evaluation Tasks”. In: Proceedings of the 27th\nInternational Conference on Computational Linguistics. Santa Fe, New Mex-\nico, USA: Association for Computational Linguistics, pp. 3318–3329. URL :\nhttps://aclanthology.org/C18-1281.\nAnanthakrishnan, R, Pushpak Bhattacharyya, M Sasikumar, and Ritesh M\nShah (2007). “Some issues in automatic evaluation of English-Hindi MT:\nMore blues for BLEU”. In: Icon 64.\nAtanasova, Pepa, Oana-Maria Camburu, Christina Lioma, Thomas\nLukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein (July 2023).\n“Faithfulness Tests for Natural Language Explanations”. In: Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers). Toronto, Canada: Association for Computational\nLinguistics, pp. 283–294. DOI : 10.18653/v1/2023.acl-short.25 .\nURL : https://aclanthology.org/2023.acl-short.25.\nAtanasova, Pepa, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augen-\nstein (Nov. 2020a). “A Diagnostic Study of Explainability Techniques for\nText Classification”. In: Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP). Online: Association for Com-\nputational Linguistics, pp. 3256–3274. DOI : 10.18653/v1/2020.emnlp-\nmain.263. URL : https://aclanthology.org/2020.emnlp-main.\n263.\n— (July 2020b). “Generating Fact Checking Explanations”. In: Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics.\nOnline: Association for Computational Linguistics, pp. 7352–7364.DOI : 10.\n18653/v1/2020.acl-main.656 . URL : https://aclanthology.\norg/2020.acl-main.656.\nAugenstein, Isabelle, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty,\nGiovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara,\nScott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben\nMiguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni\nZagni (2023). Factuality Challenges in the Era of Large Language Models. arXiv:\n2310.05189 [cs.CL].\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio (2015). “Neural\nMachine Translation by Jointly Learning to Align and Translate”. In:3rd\nInternational Conference on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Yoshua Bengio\nand Yann LeCun. URL : http://arxiv.org/abs/1409.0473.\nBansal, Gagan, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi,\nEce Kamar, Marco Tulio Ribeiro, and Daniel Weld (2021). “Does the Whole\nExceed Its Parts? The Effect of AI Explanations on Complementary Team\nPerformance”. In: Proceedings of the 2021 CHI Conference on Human Factors in\nComputing Systems. CHI ’21. Yokohama, Japan: Association for Computing\n54\nBibliography\nMachinery. ISBN : 9781450380966. DOI : 10.1145/3411764.3445717 .\nURL : https://doi.org/10.1145/3411764.3445717.\nBaroni, Marco, Georgiana Dinu, and Germán Kruszewski (June 2014). “Don’t\ncount, predict! A systematic comparison of context-counting vs. context-\npredicting semantic vectors”. In: Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers). Balti-\nmore, Maryland: Association for Computational Linguistics, pp. 238–247.\nDOI : 10.3115/v1/P14-1023 . URL : https://aclanthology.org/\nP14-1023.\nBastings, Jasmijn and Katja Filippova (Nov. 2020). “The elephant in the in-\nterpretability room: Why use attention as explanation when we have\nsaliency methods?” In: Proceedings of the Third BlackboxNLP Workshop on\nAnalyzing and Interpreting Neural Networks for NLP. Online: Association\nfor Computational Linguistics, pp. 149–155. DOI : 10.18653/v1/2020.\nblackboxnlp- 1.14. URL : https://aclanthology.org/2020.\nblackboxnlp-1.14.\nBelinkov, Yonatan, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James\nGlass (July 2017a). “What do Neural Machine Translation Models Learn\nabout Morphology?” In: Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long Papers). Vancouver,\nCanada: Association for Computational Linguistics, pp. 861–872. DOI :\n10.18653/v1/P17-1080. URL : https://aclanthology.org/P17-\n1080.\nBelinkov, Yonatan and James Glass (2019). “Analysis Methods in Neural\nLanguage Processing: A Survey”. In: Transactions of the Association for\nComputational Linguistics 7, pp. 49–72. DOI : 10.1162/tacl_a_00254 .\nURL : https://aclanthology.org/Q19-1004.\nBelinkov, Yonatan, Lluís Màrquez, Hassan Sajjad, Nadir Durrani, Fahim\nDalvi, and James Glass (Nov. 2017b). “Evaluating Layers of Represen-\ntation in Neural Machine Translation on Part-of-Speech and Semantic\nTagging Tasks”. In:Proceedings of the Eighth International Joint Conference\non Natural Language Processing (Volume 1: Long Papers) . Taipei, Taiwan:\nAsian Federation of Natural Language Processing, pp. 1–10. URL : https:\n//aclanthology.org/I17-1001.\nBender, Emily M. and Alexander Koller (July 2020). “Climbing towards NLU:\nOn Meaning, Form, and Understanding in the Age of Data”. In:Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics.\nOnline: Association for Computational Linguistics, pp. 5185–5198.DOI : 10.\n18653/v1/2020.acl-main.463 . URL : https://aclanthology.\norg/2020.acl-main.463.\nBengio, Yoshua, Aaron Courville, and Pascal Vincent (2013). “Representation\nLearning: A Review and New Perspectives”. In: IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35.8, pp. 1798–1828. DOI : 10.1109/\nTPAMI.2013.50.\n55\nBIBLIOGRAPHY\nBlevins, Terra, Hila Gonen, and Luke Zettlemoyer (Dec. 2022). “Analyzing\nthe Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Lan-\nguage Models”. In: Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing. Abu Dhabi, United Arab Emirates: Associa-\ntion for Computational Linguistics, pp. 3575–3590. DOI : 10.18653/v1/\n2022.emnlp-main.234. URL : https://aclanthology.org/2022.\nemnlp-main.234.\nBlevins, Terra, Omer Levy, and Luke Zettlemoyer (July 2018). “Deep RNNs\nEncode Soft Hierarchical Syntax”. In: Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 2: Short Papers). Mel-\nbourne, Australia: Association for Computational Linguistics, pp. 14–19.\nDOI : 10.18653/v1/P18-2003. URL : https://aclanthology.org/\nP18-2003.\nBommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,\nEmma Brunskill, et al. (2021). “On the Opportunities and Risks of Founda-\ntion Models”. In: arXiv preprint arXiv:2108.07258.\nBowman, Samuel R., Gabor Angeli, Christopher Potts, and Christopher D.\nManning (Sept. 2015). “A large annotated corpus for learning natural\nlanguage inference”. In: Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing. Lisbon, Portugal: Association for\nComputational Linguistics, pp. 632–642. DOI : 10.18653/v1/D15-1075.\nURL : https://aclanthology.org/D15-1075.\nBraun, Marc and Jenny Kunz (2024). A Hypothesis-Driven Framework for the\nAnalysis of Self-Rationalising Models. arXiv: 2402.04787 [cs.CL]. URL :\nhttps://arxiv.org/abs/2402.04787.\nBricken, Trenton, Adly Templeton, Joshua Batson, Brian Chen, Adam\nJermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison,\nAmanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas\nSchiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex\nTamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan\nHume, Shan Carter, Tom Henighan, and Christopher Olah (2023). “To-\nwards Monosemanticity: Decomposing Language Models With Dictio-\nnary Learning”. In: Transformer Circuits Thread. https://transformer-\ncircuits.pub/2023/monosemantic-features/index.html.\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei (2020). “Language\nModels are Few-Shot Learners”. In: Advances in Neural Information Process-\ning Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,\n56\nBibliography\nand H. Lin. Vol. 33. Curran Associates, Inc., pp. 1877–1901. URL : https:\n//proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nCamburu, Oana-Maria, Tim Rocktäschel, Thomas Lukasiewicz, and Phil\nBlunsom (2018). “e-SNLI: Natural Language Inference with Natural\nLanguage Explanations”. In: Advances in Neural Information Processing\nSystems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc. URL :\nhttps : / / proceedings . neurips . cc / paper / 2018 / file /\n4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf.\nCammarata, Nick, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov,\nLudwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim (2020).\n“Thread: Circuits”. In: Distill. https://distill.pub/2020/circuits. DOI : 10.\n23915/distill.00024.\nChelba, Ciprian, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants,\nPhillipp Koehn, and Tony Robinson (2013). “One billion word benchmark\nfor measuring progress in statistical language modeling”. In:arXiv preprint\narXiv:1312.3005.\nChen, Howard, Jacqueline He, Karthik Narasimhan, and Danqi Chen (July\n2022). “Can Rationalization Improve Robustness?” In: Proceedings of the\n2022 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies. Seattle, United States: As-\nsociation for Computational Linguistics, pp. 3792–3805. DOI : 10.18653/\nv1/2022.naacl-main.278 . URL : https://aclanthology.org/\n2022.naacl-main.278.\nChowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gau-\nrav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sut-\nton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko,\nJoshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Olek-\nsandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\nSaeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-\nHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel (2022).\nPaLM: Scaling Language Modeling with Pathways. arXiv: 2204 . 02311\n[cs.CL]. URL : https://arxiv.org/abs/2204.02311.\nChu, Eric, Deb Roy, and Jacob Andreas (2020). “Are Visual Explanations\nUseful? A Case Study in Model-in-the-Loop Prediction”. In: CoRR\n57\nBIBLIOGRAPHY\nabs/2007.12248. arXiv: 2007.12248. URL : https://arxiv.org/abs/\n2007.12248.\nClark, Elizabeth, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gu-\nrurangan, and Noah A. Smith (Aug. 2021). “All That’s ‘Human’ Is Not\nGold: Evaluating Human Evaluation of Generated Text”. In: Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers). Online: Association for Computational Linguis-\ntics, pp. 7282–7296. DOI : 10.18653/v1/2021.acl-long.565 . URL :\nhttps://aclanthology.org/2021.acl-long.565.\nClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning\n(Aug. 2019). “What Does BERT Look at? An Analysis of BERT’s Atten-\ntion”. In: Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP. Florence, Italy: Association for\nComputational Linguistics, pp. 276–286. DOI : 10.18653/v1/W19-4828.\nURL : https://aclanthology.org/W19-4828.\nCobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,\nLukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman (2021). “Training Verifiers\nto Solve Math Word Problems”. In: CoRR abs/2110.14168. arXiv: 2110.\n14168. URL : https://arxiv.org/abs/2110.14168.\nConneau, Alexis, German Kruszewski, Guillaume Lample, Loı¨c Barrault, and\nMarco Baroni (July 2018). “What you can cram into a single $&!#* vector:\nProbing sentence embeddings for linguistic properties”. In: Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers). Melbourne, Australia: Association for Computa-\ntional Linguistics, pp. 2126–2136. DOI : 10.18653/v1/P18-1198 . URL :\nhttps://aclanthology.org/P18-1198.\nCummins, Robert C. (2000). “\"How Does It Work\" Versus \"What Are the\nLaws?\": Two Conceptions of Psychological Explanation”. In: Explanation\nand Cognition, 117-145. Ed. by F. Keil and Robert A. Wilson. MIT Press.\nURL : https://philpapers.org/rec/CUMHDI.\nDalvi, Fahim, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau,\nand James Glass (2019). “What is one grain of sand in the desert? analyzing\nindividual neurons in deep NLP models”. In: Proceedings of the Thirty-\nThird AAAI Conference on Artificial Intelligence and Thirty-First Innovative\nApplications of Artificial Intelligence Conference and Ninth AAAI Symposium on\nEducational Advances in Artificial Intelligence. AAAI’19/IAAI’19/EAAI’19.\nHonolulu, Hawaii, USA: AAAI Press. ISBN : 978-1-57735-809-1. DOI : 10.\n1609/aaai.v33i01.33016309. URL : https://doi.org/10.1609/\naaai.v33i01.33016309.\nDeJong, Gerald and Raymond Mooney (1986). “Explanation-based learning:\nAn alternative view”. In: Machine learning 1.2, pp. 145–176. URL : https:\n//link.springer.com/article/10.1007/BF00114116.\n58\nBibliography\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (June\n2019). “BERT: Pre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding”. In: Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers). Minneapolis, Min-\nnesota: Association for Computational Linguistics, pp. 4171–4186. DOI :\n10.18653/v1/N19-1423. URL : https://aclanthology.org/N19-\n1423.\nDeYoung, Jay, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming\nXiong, Richard Socher, and Byron C. Wallace (July 2020). “ERASER: A\nBenchmark to Evaluate Rationalized NLP Models”. In: Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics. On-\nline: Association for Computational Linguistics, pp. 4443–4458. DOI : 10.\n18653/v1/2020.acl-main.408 . URL : https://aclanthology.\norg/2020.acl-main.408.\nDodge, Jesse, Maarten Sap, Ana Marasovi´ c, William Agnew, Gabriel Ilharco,\nDirk Groeneveld, Margaret Mitchell, and Matt Gardner (Nov. 2021). “Doc-\numenting Large Webtext Corpora: A Case Study on the Colossal Clean\nCrawled Corpus”. In:Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing. Online and Punta Cana, Dominican Repub-\nlic: Association for Computational Linguistics, pp. 1286–1305. DOI : 10.\n18653/v1/2021.emnlp-main.98 . URL : https://aclanthology.\norg/2021.emnlp-main.98.\nDombrowski, Ann-Kathrin, Maximilian Alber, Christopher J. Anders, Mar-\ncel Ackermann, Klaus-Robert Müller, and Pan Kessel (2019). “Explana-\ntions can be manipulated and geometry is to blame.” In: NeurIPS. Ed.\nby Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd’Alché Buc, Emily B. Fox, and Roman Garnett, pp. 13567–13578. URL :\nhttp://dblp.uni-trier.de/db/conf/nips/nips2019.html#\nDombrowskiAAAMK19.\nDozat, Timothy, Peng Qi, and Christopher D. Manning (Aug. 2017). “Stan-\nford’s Graph-based Neural Dependency Parser at the CoNLL 2017 Shared\nTask”. In: Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing\nfrom Raw Text to Universal Dependencies. Vancouver, Canada: Association for\nComputational Linguistics, pp. 20–30. DOI : 10.18653/v1/K17-3002 .\nURL : https://aclanthology.org/K17-3002.\nDziri, Nouha, Andrea Madotto, Osmar Zaı¨ane, and Avishek Joey Bose (Nov.\n2021). “Neural Path Hunter: Reducing Hallucination in Dialogue Sys-\ntems via Path Grounding”. In: Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing. Online and Punta Cana,\nDominican Republic: Association for Computational Linguistics, pp. 2197–\n2214. DOI : 10 . 18653 / v1 / 2021 . emnlp - main . 168. URL : https :\n//aclanthology.org/2021.emnlp-main.168.\n59\nBIBLIOGRAPHY\nElazar, Yanai, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg (2021). “Am-\nnesic Probing: Behavioral Explanation with Amnesic Counterfactuals”. In:\nTransactions of the Association for Computational Linguistics 9, pp. 160–175.\nDOI : 10.1162/tacl_a_00359. URL : https://aclanthology.org/\n2021.tacl-1.10.\nElman, Jeffrey L (1990). “Finding structure in time”. In: Cognitive science 14.2,\npp. 179–211.\nEmerson, Guy (July 2020). “What are the Goals of Distributional Semantics?”\nIn: Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Online: Association for Computational Linguistics, pp. 7436–\n7453. DOI : 10.18653/v1/2020.acl- main.663 . URL : https://\naclanthology.org/2020.acl-main.663.\nEthayarajh, Kawin (Nov. 2019). “How Contextual are Contextualized Word\nRepresentations? Comparing the Geometry of BERT, ELMo, and GPT-2\nEmbeddings”. In: Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association\nfor Computational Linguistics, pp. 55–65.DOI : 10.18653/v1/D19-1006.\nURL : https://aclanthology.org/D19-1006.\nFirth, J. R. (1957). “A synopsis of linguistic theory 1930-55.” In: 1952-59, pp. 1–\n32.\nGeva, Mor, Yoav Goldberg, and Jonathan Berant (Nov. 2019). “Are We Mod-\neling the Task or the Annotator? An Investigation of Annotator Bias in\nNatural Language Understanding Datasets”. In: Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\nHong Kong, China: Association for Computational Linguistics, pp. 1161–\n1166. DOI : 10.18653/v1/D19-1107 . URL : https://aclanthology.\norg/D19-1107.\nGokaslan, Aaron, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex (2019).\nOpenWebText Corpus. URL : http : / / Skylion007 . github . io /\nOpenWebTextCorpus.\nGoldberg, Yoav (2019). “Assessing BERT’s syntactic abilities”. In:arXiv preprint\narXiv:1901.05287.\nGonzález, Ana Valeria, Gagan Bansal, Angela Fan, Yashar Mehdad, Robin\nJia, and Srinivasan Iyer (Aug. 2021). “Do Explanations Help Users De-\ntect Errors in Open-Domain QA? An Evaluation of Spoken vs. Visual\nExplanations”. In: Findings of the Association for Computational Linguistics:\nACL-IJCNLP 2021 . Online: Association for Computational Linguistics,\npp. 1103–1116. DOI : 10.18653/v1/2021.findings-acl.95 . URL :\nhttps://aclanthology.org/2021.findings-acl.95.\nGururangan, Suchin, Swabha Swayamdipta, Omer Levy, Roy Schwartz,\nSamuel Bowman, and Noah A. Smith (June 2018). “Annotation Artifacts\nin Natural Language Inference Data”. In: Proceedings of the 2018 Conference\n60\nBibliography\nof the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 2 (Short Papers). New Orleans,\nLouisiana: Association for Computational Linguistics, pp. 107–112. DOI :\n10.18653/v1/N18-2017. URL : https://aclanthology.org/N18-\n2017.\nHanna, Michael and Ondˇ rej Bojar (Nov. 2021). “A Fine-Grained Analysis\nof BERTScore”. In: Proceedings of the Sixth Conference on Machine Transla-\ntion. Online: Association for Computational Linguistics, pp. 507–517. URL :\nhttps://aclanthology.org/2021.wmt-1.59.\nHarnad, Stevan (1990). “The symbol grounding problem”. In: Physica D:\nNonlinear Phenomena 42.1-3, pp. 335–346.\nHarris, Zellig S. (1954). “Distributional Structure”. In: <i>WORD</i> 10.2-3,\npp. 146–162. DOI : 10.1080/00437956.1954.11659520.\nHase, Peter, Shiyue Zhang, Harry Xie, and Mohit Bansal (Nov. 2020). “Leakage-\nAdjusted Simulatability: Can Models Generate Non-Trivial Explanations\nof Their Behavior in Natural Language?” In: Findings of the Association for\nComputational Linguistics: EMNLP 2020. Online: Association for Computa-\ntional Linguistics, pp. 4351–4367. DOI : 10.18653/v1/2020.findings-\nemnlp.390. URL : https://aclanthology.org/2020.findings-\nemnlp.390.\nHe, Xuanli, Yuxiang Wu, Oana-Maria Camburu, Pasquale Minervini, and\nPontus Stenetorp (2023). Using Natural Language Explanations to Improve\nRobustness of In-context Learning for Natural Language Inference. arXiv:2311.\n07556 [cs.CL].\nHewitt, John and Percy Liang (Nov. 2019). “Designing and Interpreting Probes\nwith Control Tasks”. In:Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: As-\nsociation for Computational Linguistics, pp. 2733–2743. DOI : 10.18653/\nv1/D19-1275. URL : https://aclanthology.org/D19-1275.\nHewitt, John and Christopher D. Manning (June 2019). “A Structural Probe\nfor Finding Syntax in Word Representations”. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). Minneapolis, Minnesota: Association for Computational Linguis-\ntics, pp. 4129–4138. DOI : 10.18653/v1/N19-1419 . URL : https://\naclanthology.org/N19-1419.\nHochreiter, Sepp and Jürgen Schmidhuber (1997). “Long short-term memory”.\nIn: Neural computation 9.8, pp. 1735–1780.\nHolmström, Oskar, Jenny Kunz, and Marco Kuhlmann (May 2023). “Bridg-\ning the Resource Gap: Exploring the Efficacy of English and Multilin-\ngual LLMs for Swedish”. In: Proceedings of the Second Workshop on Re-\nsources and Representations for Under-Resourced Languages and Domains\n(RESOURCEFUL-2023). Tórshavn, the Faroe Islands: Association for Com-\n61\nBIBLIOGRAPHY\nputational Linguistics, pp. 92–110. URL : https://aclanthology.org/\n2023.resourceful-1.13.\nHong, Ruixin, Hongming Zhang, Xintong Yu, and Changshui Zhang (July\n2022). “METGEN: A Module-Based Entailment Tree Generation Frame-\nwork for Answer Explanation”. In: Findings of the Association for Compu-\ntational Linguistics: NAACL 2022. Seattle, United States: Association for\nComputational Linguistics, pp. 1887–1905. DOI : 10.18653/v1/2022.\nfindings-naacl.145 . URL : https://aclanthology.org/2022.\nfindings-naacl.145.\nHoward, Jeremy and Sebastian Ruder (July 2018). “Universal Language Model\nFine-tuning for Text Classification”. In: Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers).\nMelbourne, Australia: Association for Computational Linguistics, pp. 328–\n339. DOI : 10.18653/v1/P18-1031 . URL : https://aclanthology.\norg/P18-1031.\nHsieh, Yu-Lun, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and\nCho-Jui Hsieh (July 2019). “On the Robustness of Self-Attentive Mod-\nels”. In: Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics. Florence, Italy: Association for Computational Lin-\nguistics, pp. 1520–1529. DOI : 10.18653/v1/P19-1147 . URL : https:\n//aclanthology.org/P19-1147.\nHupkes, Dieuwke, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai\nElazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi\nSaphra, Arabella Sinclair, et al. (2022). “State-of-the-art generalisation re-\nsearch in NLP: a taxonomy and review”. In:arXiv preprint arXiv:2210.03050.\nHupkes, Dieuwke, Sara Veldhoen, and Willem Zuidema (2017). “Visualisation\nand ‘Diagnostic Classifiers’ Reveal how Recurrent and Recursive Neural\nNetworks Process Hierarchical Structure.” In: Interpreting, Explaining and\nVisualizing Deep Learning, NIPS2017.\nIlyas, Andrew, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Bran-\ndon Tran, and Aleksander Madry (2019). “Adversarial Examples Are Not\nBugs, They Are Features”. In: Advances in Neural Information Processing\nSystems. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-\nBuc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. URL : https:\n//proceedings.neurips.cc/paper_files/paper/2019/file/\ne2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf.\nJacovi, Alon and Yoav Goldberg (July 2020). “Towards Faithfully Interpretable\nNLP Systems: How Should We Define and Evaluate Faithfulness?” In:\nProceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Online: Association for Computational Linguistics, pp. 4198–\n4205. DOI : 10.18653/v1/2020.acl- main.386 . URL : https://\naclanthology.org/2020.acl-main.386.\nJain, Sarthak and Byron C. Wallace (June 2019). “Attention is not Explanation”.\nIn: Proceedings of the 2019 Conference of the North American Chapter of the As-\n62\nBibliography\nsociation for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers). Minneapolis, Minnesota: Association for Compu-\ntational Linguistics, pp. 3543–3556. DOI : 10.18653/v1/N19-1357. URL :\nhttps://aclanthology.org/N19-1357.\nJawahar, Ganesh, Benoit Sagot, and Djame Seddah (July 2019). “What Does\nBERT Learn about the Structure of Language?” In: Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics. Ed. by Anna\nKorhonen, David Traum, and Lluis Marquez. Florence, Italy: Association\nfor Computational Linguistics, pp. 3651–3657. DOI : 10.18653/v1/P19-\n1356. URL : https://aclanthology.org/P19-1356.\nJi, Ziwei, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko\nIshii, Ye Jin Bang, Andrea Madotto, and Pascale Fung (2023). “Survey of\nHallucination in Natural Language Generation”. In: ACM Comput. Surv.\n55.12. ISSN : 0360-0300. DOI : 10.1145/3571730 . URL : https://doi.\norg/10.1145/3571730.\nJie, Zhanming, Jierui Li, and Wei Lu (May 2022). “Learning to Reason Deduc-\ntively: Math Word Problem Solving as Complex Relation Extraction”. In:\nProceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). Dublin, Ireland: Association for Com-\nputational Linguistics, pp. 5944–5955. DOI : 10.18653/v1/2022.acl-\nlong.410 . URL : https://aclanthology.org/2022.acl-long.\n410.\nKarpathy, Andrej, Justin Johnson, and Li Fei-Fei (2015). “Visualizing and\nUnderstanding Recurrent Networks”. In: CoRR abs/1506.02078. arXiv:\n1506.02078. URL : http://arxiv.org/abs/1506.02078.\nKeil, Frank C (2006). “Explanation and understanding”. In: Annual review\nof psychology 57, p. 227. DOI : 10.1146/annurev.psych.57.102904.\n190100. URL : https://www.annualreviews.org/doi/10.1146/\nannurev.psych.57.102904.190100.\nKiperwasser, Eliyahu and Yoav Goldberg (2016). “Simple and Accurate De-\npendency Parsing Using Bidirectional LSTM Feature Representations”. In:\nTransactions of the Association for Computational Linguistics 4, pp. 313–327.\nDOI : 10.1162/tacl_a_00101. URL : https://aclanthology.org/\nQ16-1023.\nKoco ´ n, Jan, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Do-\nminika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arka-\ndiusz Janz, Kamil Kanclerz, Anna Koco ´ n, Bartłomiej Koptyra, Wiktoria\nMieleszczenko-Kowszewicz, Piotr Miłkowski, Marcin Oleksy, Maciej Pi-\nasecki, Łukasz Radli ´ nski, Konrad Wojtasik, Stanisław Wo´ zniak, and Prze-\nmysław Kazienko (2023). ChatGPT: Jack of all trades, master of none. DOI :\n10.48550/ARXIV.2302.10724 . URL : https://arxiv.org/abs/\n2302.10724.\nKojima, Takeshi, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and\nYusuke Iwasawa (2022). “Large Language Models are Zero-Shot Rea-\n63\nBIBLIOGRAPHY\nsoners”. In: Advances in Neural Information Processing Systems. Ed. by\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh.\nVol. 35. Curran Associates, Inc., pp. 22199–22213. URL : https : / /\nproceedings.neurips.cc/paper_files/paper/2022/file/\n8bb0d291acd4acf06ef112099c16f326 - Paper - Conference .\npdf.\nKornblith, Simon, Mohammad Norouzi, Honglak Lee, and Geoffrey Hin-\nton (2019). “Similarity of Neural Network Representations Revisited”.\nIn: Proceedings of the 36th International Conference on Machine Learning.\nEd. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Vol. 97. Proceed-\nings of Machine Learning Research. PMLR, pp. 3519–3529. URL : https:\n//proceedings.mlr.press/v97/kornblith19a.html.\nKunz, Jenny and Oskar Holmström (2024). The Impact of Language Adapters\nin Cross-Lingual Transfer for NLU. arXiv: 2402.00149 [cs.CL]. URL :\nhttps://arxiv.org/abs/2402.00149.\nKunz, Jenny, Martin Jirenius, Oskar Holmström, and Marco Kuhlmann (Dec.\n2022). “Human Ratings Do Not Reflect Downstream Utility: A Study\nof Free-Text Explanations for Model Predictions”. In: Proceedings of the\nFifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP. Abu Dhabi, United Arab Emirates (Hybrid): Association for\nComputational Linguistics, pp. 164–177. DOI :\n10 . 18653 / v1 / 2022 .\nblackboxnlp- 1.14. URL : https://aclanthology.org/2022.\nblackboxnlp-1.14.\nKunz, Jenny and Marco Kuhlmann (Dec. 2020). “Classifier Probes May Just\nLearn from Linear Context Features”. In: Proceedings of the 28th Interna-\ntional Conference on Computational Linguistics. Barcelona, Spain (Online):\nInternational Committee on Computational Linguistics, pp. 5136–5146.\nDOI :\n10 . 18653 / v1 / 2020 . coling - main . 450. URL : https : / /\naclanthology.org/2020.coling-main.450.\n— (Nov. 2021). “Test Harder than You Train: Probing with Extrapolation\nSplits”. In: Proceedings of the Fourth BlackboxNLP Workshop on Analyzing\nand Interpreting Neural Networks for NLP. Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics, pp. 15–25. DOI : 10.18653/\nv1/2021.blackboxnlp-1.2 . URL : https://aclanthology.org/\n2021.blackboxnlp-1.2.\n— (Oct. 2022). “Where Does Linguistic Information Emerge in Neural Lan-\nguage Models? Measuring Gains and Contributions across Layers”. In:\nProceedings of the 29th International Conference on Computational Linguistics.\nGyeongju, Republic of Korea: International Committee on Computational\nLinguistics, pp. 4664–4676. URL : https://aclanthology.org/2022.\ncoling-1.413.\n— (2024). Properties and Challenges of LLM-Generated Explanations. arXiv:2402.\n10532 [cs.CL].URL : https://arxiv.org/abs/2402.10532.\nLakatos, Imre (1963). Proofs and refutations. Nelson London.\n64\nBibliography\nLatcinnik, Veronica and Jonathan Berant (2020). “Explaining Question Answer-\ning Models through Text Generation”. In: CoRR abs/2004.05569. arXiv:\n2004.05569. URL : https://arxiv.org/abs/2004.05569.\nLee, Eunjin, David Braines, Mitchell Stiffler, Adam Hudler, and Daniel Har-\nborne (2019). “Developing the sensitivity of LIME for better machine learn-\ning explanation”. In: Artificial Intelligence and Machine Learning for Multi-\nDomain Operations Applications. Vol. 11006. SPIE, pp. 349–356. URL : https:\n//www.spiedigitallibrary.org/conference- proceedings-\nof-spie/11006/1100610/Developing-the-sensitivity-of-\nLIME-for-better-machine-learning-explanation/10.1117/\n12.2520149.full.\nLee, Katherine, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David\nSussillo (2018). “Hallucinations in neural machine translation”. In: Inter-\npretability and Robustness in Audio, Speech, and Language Workshop. Conference\non Neural Information Processing Systems (NeurIPS 2018), Montreal, Canada.\nURL : https://openreview.net/pdf?id=SJxTk3vB3m.\nLevy, Omer and Yoav Goldberg (June 2014). “Linguistic Regularities in Sparse\nand Explicit Word Representations”. In:Proceedings of the Eighteenth Con-\nference on Computational Natural Language Learning. Ann Arbor, Michigan:\nAssociation for Computational Linguistics, pp. 171–180. DOI : 10.3115/\nv1/W14-1618. URL : https://aclanthology.org/W14-1618.\nLewis, Clayton (1988). “Why and How to Learn Why: Analysis-based Gen-\neralization of Procedures”. In: Cognitive Science 12.2, pp. 211–256. DOI :\nhttps : / / doi . org / 10 . 1207 / s15516709cog1202 \\ _3. eprint:\nhttps : / / onlinelibrary . wiley . com / doi / pdf / 10 . 1207 /\ns15516709cog1202 _ 3\n. URL : https : / / onlinelibrary . wiley .\ncom/doi/abs/10.1207/s15516709cog1202_3.\nLhoneux, Miryam de, Yan Shao, Ali Basirat, Eliyahu Kiperwasser, Sara\nStymne, Yoav Goldberg, and Joakim Nivre (Aug. 2017). “From Raw Text to\nUniversal Dependencies - Look, No Tags!” In:Proceedings of the CoNLL 2017\nShared Task: Multilingual Parsing from Raw Text to Universal Dependencies.\nVancouver, Canada: Association for Computational Linguistics, pp. 207–\n217. DOI : 10.18653/v1/K17-3022 . URL : https://aclanthology.\norg/K17-3022.\nLi, Jiwei, Xinlei Chen, Eduard Hovy, and Dan Jurafsky (June 2016). “Visualiz-\ning and Understanding Neural Models in NLP”. In: Proceedings of the 2016\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. San Diego, California: Associ-\nation for Computational Linguistics, pp. 681–691. DOI : 10.18653/v1/\nN16-1082. URL : https://aclanthology.org/N16-1082.\nLi, Yixuan, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft (2015).\n“Convergent Learning: Do different neural networks learn the same rep-\nresentations?” In: Proceedings of the 1st International Workshop on Feature\nExtraction: Modern Questions and Challenges at NIPS 2015. Ed. by Dmitry\n65\nBIBLIOGRAPHY\nStorcheus, Afshin Rostamizadeh, and Sanjiv Kumar. Vol. 44. Proceedings of\nMachine Learning Research. Montreal, Canada: PMLR, pp. 196–212. URL :\nhttps://proceedings.mlr.press/v44/li15convergent.html.\nLin, Chin-Yew (July 2004). “ROUGE: A Package for Automatic Evaluation\nof Summaries”. In: Text Summarization Branches Out. Barcelona, Spain:\nAssociation for Computational Linguistics, pp. 74–81. URL : https://\naclanthology.org/W04-1013.\nLin, Yongjie, Yi Chern Tan, and Robert Frank (Aug. 2019). “Open Sesame:\nGetting inside BERT’s Linguistic Knowledge”. In: Proceedings of the 2019\nACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP. Florence, Italy: Association for Computational Linguistics, pp. 241–\n253. DOI : 10.18653/v1/W19-4825 . URL : https://aclanthology.\norg/W19-4825.\nLing, Wang, Dani Yogatama, Chris Dyer, and Phil Blunsom (July 2017). “Pro-\ngram Induction by Rationale Generation: Learning to Solve and Explain\nAlgebraic Word Problems”. In: Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). Vancou-\nver, Canada: Association for Computational Linguistics, pp. 158–167. DOI :\n10.18653/v1/P17-1015. URL : https://aclanthology.org/P17-\n1015.\nLinzen, Tal, Emmanuel Dupoux, and Yoav Goldberg (2016). “Assessing the\nAbility of LSTMs to Learn Syntax-Sensitive Dependencies”. In:Transactions\nof the Association for Computational Linguistics 4, pp. 521–535. DOI : 10 .\n1162/tacl_a_00115 . URL : https://aclanthology.org/Q16-\n1037.\nLipton, Zachary C (2018). “The Mythos of Model Interpretability: In machine\nlearning, the concept of interpretability is both important and slippery.”\nIn: Queue 16.3, pp. 31–57.\nLombrozo, Tania (2006). “The structure and function of explanations”. In:\nTrends in cognitive sciences 10.10, pp. 464–470. URL : https://pubmed.\nncbi.nlm.nih.gov/16942895/.\nLuccioni, Alexandra Sasha and Anna Rogers (2023). Mind your Language\n(Model): Fact-Checking LLMs and their Role in NLP Research and Practice.\narXiv: 2308.07120 [cs.CL].\nLundberg, Scott M. and Su-In Lee (2017). “A unified approach to interpreting\nmodel predictions”. In: Proceedings of the 31st International Conference on\nNeural Information Processing Systems. NIPS’17. Long Beach, California,\nUSA: Curran Associates Inc., pp. 4768–4777. ISBN : 9781510860964.\nMagar, Inbal and Roy Schwartz (May 2022). “Data Contamination: From\nMemorization to Exploitation”. In: Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers). Dublin,\nIreland: Association for Computational Linguistics, pp. 157–165. DOI : 10.\n18653/v1/2022.acl-short.18 . URL : https://aclanthology.\norg/2022.acl-short.18.\n66\nBibliography\nMalmström, Hans, Christian Stöhr, and AW Ou (2023). “Chatbots and other AI\nfor learning: A survey of use and views among university students in Swe-\nden”. In: Chalmers Studies in Communication and Learning in Higher Education\n1.10.17196. URL : https://research.chalmers.se/publication/\n535715/file/535715_Fulltext.pdf.\nMarasovic, Ana, Iz Beltagy, Doug Downey, and Matthew Peters (July 2022).\n“Few-Shot Self-Rationalization with Natural Language Prompts”. In: Find-\nings of the Association for Computational Linguistics: NAACL 2022. Seat-\ntle, United States: Association for Computational Linguistics, pp. 410–\n424. DOI :\n10.18653/v1/2022.findings-naacl.31 . URL : https:\n//aclanthology.org/2022.findings-naacl.31.\nMarvin, Rebecca and Tal Linzen (Oct. 2018). “Targeted Syntactic Evaluation\nof Language Models”. In: Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing. Brussels, Belgium: Association for\nComputational Linguistics, pp. 1192–1202. DOI : 10.18653/v1/D18-\n1151. URL : https://aclanthology.org/D18-1151.\nMaynez, Joshua, Shashi Narayan, Bernd Bohnet, and Ryan McDonald (July\n2020). “On Faithfulness and Factuality in Abstractive Summarization”. In:\nProceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Online: Association for Computational Linguistics, pp. 1906–\n1919. DOI : 10.18653/v1/2020.acl- main.173 . URL : https://\naclanthology.org/2020.acl-main.173.\nMcCoy, R Thomas, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas\nL Griffiths (2023). “Embers of Autoregression: Understanding Large Lan-\nguage Models Through the Problem They are Trained to Solve”. In:arXiv\npreprint arXiv:2309.13638. URL : https : / / arxiv . org / pdf / 2309 .\n13638.pdf.\nMcCoy, Tom, Ellie Pavlick, and Tal Linzen (July 2019). “Right for the Wrong\nReasons: Diagnosing Syntactic Heuristics in Natural Language Inference”.\nIn: Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics. Florence, Italy: Association for Computational Lin-\nguistics, pp. 3428–3448. DOI :\n10.18653/v1/P19-1334 . URL : https:\n//aclanthology.org/P19-1334.\nMeng, Kevin, David Bau, Alex Andonian, and Yonatan Belinkov (2022). “Lo-\ncating and Editing Factual Associations in GPT”. In: Advances in Neural\nInformation Processing Systems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal,\nD. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., pp. 17359–\n17372. URL : https://proceedings.neurips.cc/paper_files/\npaper / 2022 / file / 6f1d43d5a82a37e89b0665b33bf3a182 -\nPaper-Conference.pdf.\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean (2013a). “Efficient\nestimation of word representations in vector space”. In.\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff\nDean (2013b). “Distributed Representations of Words and Phrases and\n67\nBIBLIOGRAPHY\ntheir Compositionality”. In: Advances in Neural Information Processing\nSystems. Ed. by C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahra-\nmani, and K. Q. Weinberger. Vol. 26. Curran Associates, Inc. URL :\nhttps : / / proceedings . neurips . cc / paper / 2013 / file /\n9aa42b31882ec039965f3c4923ce901b-Paper.pdf.\nMiller, Tim (2019). “Explanation in artificial intelligence: Insights from the\nsocial sciences”. In: Artificial intelligence 267, pp. 1–38.\n— (2023). “Explainable AI is Dead, Long Live Explainable AI! Hypothesis-\ndriven Decision Support using Evaluative AI”. In: Proceedings of the 2023\nACM Conference on Fairness, Accountability, and Transparency . FAccT ’23.\nChicago, IL, USA: Association for Computing Machinery, pp. 333–342.\nDOI : 10.1145/3593013.3594001 . URL : https://doi.org/10.\n1145/3593013.3594001.\nMitchell, Tom M, Richard M Keller, and Smadar T Kedar-Cabelli (1986).\n“Explanation-based generalization: A unifying view”. In: Machine learning\n1.1, pp. 47–80. URL : https://link.springer.com/article/10.\n1023/A:1022691120807.\nMittelstadt, Brent, Chris Russell, and Sandra Wachter (2019). “Explaining Ex-\nplanations in AI”. In: Proceedings of the Conference on Fairness, Accountability,\nand Transparency. FAT* ’19. Atlanta, GA, USA: Association for Computing\nMachinery, pp. 279–288. ISBN : 9781450361255. DOI : 10.1145/3287560.\n3287574. URL : https://doi.org/10.1145/3287560.3287574.\nMollo, Dimitri Coelho and Raphaël Millière (2023). “The vector grounding\nproblem”. In: arXiv preprint arXiv:2304.01481. URL : https://arxiv.\norg/pdf/2304.01481.pdf.\nMoravcsik, Julius ME (1974). “Aristotle on adequate explanations”. In: Syn-\nthese, pp. 3–17. URL : https : / / www . jstor . org / stable / pdf /\n20114949.pdf.\nNanda, Neel, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt\n(2023). “Progress measures for grokking via mechanistic interpretability”.\nIn: The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. URL : https://\nopenreview.net/pdf?id=9XFSbDPmdW.\nNarang, Sharan, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and\nKarishma Malkan (2020). WT5?! Training Text-to-Text Models to Explain their\nPredictions. arXiv: 2004.14546 [cs.CL].\nNarayanan, Menaka, Emily Chen, Jeffrey He, Been Kim, Sam Gershman,\nand Finale Doshi-Velez (2018). “How do Humans Understand Explana-\ntions from Machine Learning Systems? An Evaluation of the Human-\nInterpretability of Explanation”. In: CoRR abs/1802.00682. arXiv:\n1802.\n00682. URL : http://arxiv.org/abs/1802.00682.\nNie, Feng, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew Lin (July 2019).\n“A Simple Recipe towards Reducing Hallucination in Neural Surface\nRealisation”. In: Proceedings of the 57th Annual Meeting of the Association for\n68\nBibliography\nComputational Linguistics. Florence, Italy: Association for Computational\nLinguistics, pp. 2673–2679. DOI : 10.18653/v1/P19-1256. URL : https:\n//aclanthology.org/P19-1256.\nNiven, Timothy and Hung-Yu Kao (July 2019). “Probing Neural Network\nComprehension of Natural Language Arguments”. In: Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics. Florence,\nItaly: Association for Computational Linguistics, pp. 4658–4664. DOI : 10.\n18653/v1/P19- 1459. URL : https://aclanthology.org/P19-\n1459.\nOlah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov,\nand Shan Carter (2020). “Zoom In: An Introduction to Circuits”. In: Distill.\nhttps://distill.pub/2020/circuits/zoom-in. DOI : 10.23915/distill.\n00024.001.\nOpenAI (2023). “GPT-4 Technical Report”. In: ArXiv abs/2303.08774. URL :\nhttps://arxiv.org/abs/2303.08774.\nOuyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie\nSimens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike,\nand Ryan Lowe (2022). “Training language models to follow instruc-\ntions with human feedback”. In: Advances in Neural Information Process-\ning Systems. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,\nK. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., pp. 27730–27744.\nURL :\nhttps://proceedings.neurips.cc/paper_files/paper/\n2022 / file / b1efde53be364a73914f58805a001731 - Paper -\nConference.pdf.\nPapenmeier, Andrea, Dagmar Kern, Gwenn Englebienne, and Christin Seifert\n(2022). “It’s Complicated: The Relationship between User Trust, Model\nAccuracy and Explanations in AI”. In: ACM Trans. Comput.-Hum. Interact.\n29.4. ISSN : 1073-0516. DOI : 10.1145/3495013 . URL : https://doi.\norg/10.1145/3495013.\nPapineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu (July 2002).\n“Bleu: a Method for Automatic Evaluation of Machine Translation”. In:\nProceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics. Philadelphia, Pennsylvania, USA: Association for Computa-\ntional Linguistics, pp. 311–318. DOI : 10.3115/1073083.1073135. URL :\nhttps://aclanthology.org/P02-1040.\nParcalabescu, Letitia and Anette Frank (2024).On Measuring Faithfulness or Self-\nconsistency of Natural Language Explanations. arXiv:2311.07466 [cs.CL].\nURL : https://arxiv.org/abs/2311.07466.\nPark, Dong Huk, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt\nSchiele, Trevor Darrell, and Marcus Rohrbach (2018). “Multimodal Ex-\nplanations: Justifying Decisions and Pointing to the Evidence”. In: 2018\n69\nBIBLIOGRAPHY\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8779–\n8788. DOI : 10.1109/CVPR.2018.00915.\nPennington, Jeffrey, Richard Socher, and Christopher Manning (Oct. 2014).\n“GloVe: Global Vectors for Word Representation”. In:Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP).\nDoha, Qatar: Association for Computational Linguistics, pp. 1532–1543.\nDOI : 10.3115/v1/D14-1162 . URL : https://aclanthology.org/\nD14-1162.\nPeters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christo-\npher Clark, Kenton Lee, and Luke Zettlemoyer (June 2018). “Deep Con-\ntextualized Word Representations”. In:Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long Papers). New Orleans,\nLouisiana: Association for Computational Linguistics, pp. 2227–2237. DOI :\n10.18653/v1/N18-1202. URL : https://aclanthology.org/N18-\n1202.\nPfeiffer, Jonas, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel,\nand Mikel Artetxe (July 2022). “Lifting the Curse of Multilinguality by\nPre-training Modular Transformers”. In: Proceedings of the 2022 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. Seattle, United States: Association for Com-\nputational Linguistics, pp. 3479–3495. DOI : 10.18653/v1/2022.naacl-\nmain.255. URL : https://aclanthology.org/2022.naacl-main.\n255.\nPiantadosi, Steven T. and Felix Hill (2022). Meaning without reference in large\nlanguage models. arXiv: 2208.02957 [cs.CL].\nPimentel, Tiago, Naomi Saphra, Adina Williams, and Ryan Cotterell (Nov.\n2020a). “Pareto Probing: Trading Off Accuracy for Complexity”. In:Pro-\nceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP). Online: Association for Computational Linguistics,\npp. 3138–3153. DOI : 10.18653/v1/2020.emnlp- main.254 . URL :\nhttps://aclanthology.org/2020.emnlp-main.254.\nPimentel, Tiago, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina\nWilliams, and Ryan Cotterell (July 2020b). “Information-Theoretic Probing\nfor Linguistic Structure”. In: Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics. Online: Association for Computa-\ntional Linguistics, pp. 4609–4622. DOI : 10.18653/v1/2020.acl-main.\n420. URL : https://aclanthology.org/2020.acl-main.420.\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever (2018).\n“Improving language understanding by generative pre-training”. In.\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. (2019). “Language models are unsupervised multitask\nlearners”. In: OpenAI blog 1.8, p. 9.\n70\nBibliography\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu (2020). “Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer”. In:\nJ. Mach. Learn. Res. 21, 140:1–140:67. URL : http://jmlr.org/papers/\nv21/20-074.html.\nRaganato, Alessandro and Jörg Tiedemann (Nov. 2018). “An Analysis of En-\ncoder Representations in Transformer-Based Machine Translation”. In:\nProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP. Brussels, Belgium: Association for Com-\nputational Linguistics, pp. 287–297. DOI : 10.18653/v1/W18-5431. URL :\nhttps://aclanthology.org/W18-5431.\nRajani, Nazneen Fatema, Bryan McCann, Caiming Xiong, and Richard Socher\n(July 2019). “Explain Yourself! Leveraging Language Models for Com-\nmonsense Reasoning”. In: Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics. Florence, Italy: Association for\nComputational Linguistics, pp. 4932–4942. DOI : 10.18653/v1/P19-\n1487. URL : https://aclanthology.org/P19-1487.\nRajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang (Nov.\n2016). “SQuAD: 100,000+ Questions for Machine Comprehension of Text”.\nIn: Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-\nguage Processing. Austin, Texas: Association for Computational Linguis-\ntics, pp. 2383–2392. DOI : 10.18653/v1/D16-1264 . URL : https://\naclanthology.org/D16-1264.\nRavfogel, Shauli, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Gold-\nberg (July 2020). “Null It Out: Guarding Protected Attributes by Iterative\nNullspace Projection”. In: Proceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics. Online: Association for Computational\nLinguistics, pp. 7237–7256. DOI : 10.18653/v1/2020.acl-main.647.\nURL : https://aclanthology.org/2020.acl-main.647.\nRavichander, Abhilasha, Yonatan Belinkov, and Eduard Hovy (Apr. 2021).\n“Probing the Probing Paradigm: Does Probing Accuracy Entail Task Rele-\nvance?” In: Proceedings of the 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume. Online: Association\nfor Computational Linguistics, pp. 3363–3377. DOI : 10.18653/v1/2021.\neacl-main.295 . URL : https://aclanthology.org/2021.eacl-\nmain.295.\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2016). “\"Why Should\nI Trust You?\": Explaining the Predictions of Any Classifier”. In:Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge Discov-\nery and Data Mining. KDD ’16. San Francisco, California, USA: Associa-\ntion for Computing Machinery, pp. 1135–1144. ISBN : 9781450342322. DOI :\n10.1145/2939672.2939778 . URL : https://doi.org/10.1145/\n2939672.2939778.\n71\nBIBLIOGRAPHY\nRissanen, Jorma (1978). “Modeling by shortest data description”. In: Au-\ntom. 14.5, pp. 465–471. DOI : 10.1016/0005-1098(78)90005-5 . URL :\nhttps://doi.org/10.1016/0005-1098(78)90005-5.\nRogers, Anna, Aleksandr Drozd, and Bofang Li (Aug. 2017). “The (too Many)\nProblems of Analogical Reasoning with Word Vectors”. In:Proceedings of\nthe 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017).\nVancouver, Canada: Association for Computational Linguistics, pp. 135–\n148. DOI : 10.18653/v1/S17-1017 . URL : https://aclanthology.\norg/S17-1017.\nRogers, Anna, Olga Kovaleva, and Anna Rumshisky (2020). “A Primer in\nBERTology: What We Know About How BERT Works”. In:Transactions of\nthe Association for Computational Linguistics 8, pp. 842–866. DOI : 10.1162/\ntacl_a_00349 . URL : https://aclanthology.org/2020.tacl-\n1.54.\nRoscher, Ribana, Bastian Bohn, Marco F Duarte, and Jochen Garcke (2020).\n“Explainable machine learning for scientific insights and discoveries”. In:\nIEEE Access 8, pp. 42200–42216.\nRoss, Alexis, Matthew Peters, and Ana Marasovic (Dec. 2022). “Does Self-\nRationalization Improve Robustness to Spurious Correlations?” In:Proceed-\nings of the 2022 Conference on Empirical Methods in Natural Language Process-\ning. Abu Dhabi, United Arab Emirates: Association for Computational Lin-\nguistics, pp. 7403–7416. DOI : 10.18653/v1/2022.emnlp-main.501 .\nURL : https://aclanthology.org/2022.emnlp-main.501.\nRozanova, Julia, Marco Valentino, Lucas Cordeiro, and André Freitas (May\n2023). “Interventional Probing in High Dimensions: An NLI Case Study”.\nIn: Findings of the Association for Computational Linguistics: EACL 2023.\nDubrovnik, Croatia: Association for Computational Linguistics, pp. 2489–\n2500. DOI : 10.18653/v1/2023.findings-eacl.188 . URL : https:\n//aclanthology.org/2023.findings-eacl.188.\nRudin, Cynthia (2019). “Stop explaining black box machine learning models\nfor high stakes decisions and use interpretable models instead”. In: Nature\nmachine intelligence 1.5, pp. 206–215.\nSaphra, Naomi and Adam Lopez (June 2019). “Understanding Learning Dy-\nnamics Of Language Models with SVCCA”. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). Minneapolis, Minnesota: Association for Computational Linguis-\ntics, pp. 3257–3267. DOI : 10.18653/v1/N19-1329 . URL : https://\naclanthology.org/N19-1329.\nSerrano, Sofia and Noah A. Smith (July 2019). “Is Attention Interpretable?”\nIn: Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics. Florence, Italy: Association for Computational Lin-\nguistics, pp. 2931–2951. DOI : 10.18653/v1/P19-1282 . URL : https:\n//aclanthology.org/P19-1282.\n72\nBibliography\nShen, Hua, Tongshuang Wu, Wenbo Guo, and Ting-Hao Huang (May 2022).\n“Are Shortest Rationales the Best Explanations for Human Understand-\ning?” In: Proceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers). Dublin, Ireland: Association for\nComputational Linguistics, pp. 10–19. DOI : 10.18653/v1/2022.acl-\nshort.2. URL : https://aclanthology.org/2022.acl-short.2.\nShuster, Kurt, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston (Nov.\n2021). “Retrieval Augmentation Reduces Hallucination in Conversation”.\nIn: Findings of the Association for Computational Linguistics: EMNLP 2021.\nPunta Cana, Dominican Republic: Association for Computational Linguis-\ntics, pp. 3784–3803. DOI : 10.18653/v1/2021.findings-emnlp.320.\nURL : https://aclanthology.org/2021.findings-emnlp.320.\nSimonyan, Karen, Andrea Vedaldi, and Andrew Zisserman (2014).Deep Inside\nConvolutional Networks: Visualising Image Classification Models and Saliency\nMaps. arXiv: 1312.6034 [cs.CV]. URL : https://arxiv.org/abs/\n1312.6034.\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu\nLakkaraju (2020). “Fooling LIME and SHAP: Adversarial Attacks on\nPost hoc Explanation Methods”. In: Proceedings of the AAAI/ACM Con-\nference on AI, Ethics, and Society. AIES ’20. New York, NY, USA: Associa-\ntion for Computing Machinery, pp. 180–186. ISBN : 9781450371100. DOI :\n10.1145/3375627.3375830 . URL : https://doi.org/10.1145/\n3375627.3375830.\nSlobodkin, Aviv, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Rav-\nfogel (Dec. 2023). “The Curious Case of Hallucinatory (Un)answerability:\nFinding Truths in the Hidden States of Over-Confident Large Language\nModels”. In: Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing. Ed. by Houda Bouamor, Juan Pino, and Ka-\nlika Bali. Singapore: Association for Computational Linguistics, pp. 3607–\n3625. DOI : 10 . 18653 / v1 / 2023 . emnlp - main . 220. URL : https :\n//aclanthology.org/2023.emnlp-main.220.\nStanczak, Karolina, Edoardo Ponti, Lucas Torroba Hennigen, Ryan Cotterell,\nand Isabelle Augenstein (July 2022). “Same Neurons, Different Languages:\nProbing Morphosyntax in Multilingual Pre-trained Models”. In: Proceed-\nings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies. Seattle, United\nStates: Association for Computational Linguistics, pp. 1589–1598. DOI : 10.\n18653/v1/2022.naacl-main.114 . URL : https://aclanthology.\norg/2022.naacl-main.114.\nSta ´ nczak, Karolina, Lucas Torroba Hennigen, Adina Williams, Ryan Cotterell,\nand Isabelle Augenstein (2023). “A latent-variable model for intrinsic\nprobing”. In: Proceedings of the Thirty-Seventh AAAI Conference on Artificial\nIntelligence and Thirty-Fifth Conference on Innovative Applications of Artificial\nIntelligence and Thirteenth Symposium on Educational Advances in Artificial\n73\nBIBLIOGRAPHY\nIntelligence. AAAI’23/IAAI’23/EAAI’23. AAAI Press. ISBN : 978-1-57735-\n880-0. DOI : 10.1609/aaai.v37i11.26593. URL : https://doi.org/\n10.1609/aaai.v37i11.26593.\nSun, Jiuding, Chantal Shaib, and Byron C Wallace (2023). “Evaluating the Zero-\nshot Robustness of Instruction-tuned Language Models”. In: arXiv preprint\narXiv:2306.11270. URL : https://arxiv.org/pdf/2306.11270.pdf.\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan (2017). “Axiomatic attri-\nbution for deep networks”. In: Proceedings of the 34th International Confer-\nence on Machine Learning - Volume 70. ICML’17. Sydney, NSW, Australia:\nJMLR.org, pp. 3319–3328.\nSutskever, Ilya, Oriol Vinyals, and Quoc V Le (2014). “Sequence to Se-\nquence Learning with Neural Networks”. In: Advances in Neural Infor-\nmation Processing Systems. Ed. by Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K. Q. Weinberger. Vol. 27. Curran Associates, Inc.\nURL : https://proceedings.neurips.cc/paper/2014/file/\na14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf.\nTalmor, Alon, Yanai Elazar, Yoav Goldberg, and Jonathan Berant (2020).\n“oLMpics-On What Language Model Pre-training Captures”. In: Trans-\nactions of the Association for Computational Linguistics 8, pp. 743–758. DOI :\n10 . 1162 / tacl _ a _ 00342. URL : https : / / aclanthology . org /\n2020.tacl-1.48.\nTalmor, Alon, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant (June\n2019). “CommonsenseQA: A Question Answering Challenge Targeting\nCommonsense Knowledge”. In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and Short Papers). Minneapolis,\nMinnesota: Association for Computational Linguistics, pp. 4149–4158. DOI :\n10.18653/v1/N19-1421. URL : https://aclanthology.org/N19-\n1421.\nTan, Chenhao (July 2022). “On the Diversity and Limits of Human Explana-\ntions”. In: Proceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technolo-\ngies. Seattle, United States: Association for Computational Linguistics,\npp. 2173–2188. DOI :\n10.18653/v1/2022.naacl- main.158 . URL :\nhttps://aclanthology.org/2022.naacl-main.158.\nTedeschi, Simone, Johan Bos, Thierry Declerck, Jan Hajiˇ c, Daniel Hershcovich,\nEduard Hovy, Alexander Koller, Simon Krek, Steven Schockaert, Rico\nSennrich, Ekaterina Shutova, and Roberto Navigli (July 2023). “What’s the\nMeaning of Superhuman Performance in Today’s NLU?” In:Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers). Toronto, Canada: Association for Computational\nLinguistics, pp. 12471–12491. DOI : 10.18653/v1/2023.acl- long.\n697. URL : https://aclanthology.org/2023.acl-long.697.\n74\nBibliography\nTenney, Ian, Dipanjan Das, and Ellie Pavlick (July 2019). “BERT Rediscovers\nthe Classical NLP Pipeline”. In: Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics. Florence, Italy: Association\nfor Computational Linguistics, pp. 4593–4601. DOI : 10.18653/v1/P19-\n1452. URL : https://aclanthology.org/P19-1452.\nTorroba Hennigen, Lucas, Adina Williams, and Ryan Cotterell (Nov. 2020).\n“Intrinsic Probing through Dimension Selection”. In: Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP).\nOnline: Association for Computational Linguistics, pp. 197–216. DOI : 10.\n18653/v1/2020.emnlp-main.15 . URL : https://aclanthology.\norg/2020.emnlp-main.15.\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and\nGuillaume Lample (2023). LLaMA: Open and Efficient Foundation Language\nModels. cite arxiv:2302.13971. URL : http://arxiv.org/abs/2302.\n13971.\nTurian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio (July 2010). “Word Repre-\nsentations: A Simple and General Method for Semi-Supervised Learning”.\nIn: Proceedings of the 48th Annual Meeting of the Association for Computational\nLinguistics. Uppsala, Sweden: Association for Computational Linguistics,\npp. 384–394. URL : https://aclanthology.org/P10-1040.\nTurpin, Miles, Julian Michael, Ethan Perez, and Samuel R. Bowman (2023).\n“Language Models Don’t Always Say What They Think: Unfaithful Ex-\nplanations in Chain-of-Thought Prompting”. In: Thirty-seventh Conference\non Neural Information Processing Systems. New Orleans, Louisiana, USA:\nConference on Neural Information Processing Systems (NeurIPS). URL :\nhttps://openreview.net/forum?id=bzs4uPLXvi.\nvan der Lee, Chris, Albert Gatt, Emiel van Miltenburg, and Emiel Krah-\nmer (2021). “Human evaluation of automatically generated text: Current\ntrends and best practice guidelines”. In: Computer Speech and Language 67,\np. 101151. ISSN : 0885-2308. DOI : https://doi.org/10.1016/j.csl.\n2020.101151. URL : https://www.sciencedirect.com/science/\narticle/pii/S088523082030084X.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin (2017). “Atten-\ntion is All you Need”. In: Advances in Neural Information Processing Sys-\ntems. Ed. by I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fer-\ngus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc.\nURL :\nhttps://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nVig, Jesse and Yonatan Belinkov (Aug. 2019). “Analyzing the Structure of\nAttention in a Transformer Language Model”. In: Proceedings of the 2019\nACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\n75\nBIBLIOGRAPHY\nNLP. Florence, Italy: Association for Computational Linguistics, pp. 63–76.\nDOI : 10.18653/v1/W19-4808. URL : https://aclanthology.org/\nW19-4808.\nVoita, Elena, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov\n(July 2019). “Analyzing Multi-Head Self-Attention: Specialized Heads Do\nthe Heavy Lifting, the Rest Can Be Pruned”. In: Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics. Florence,\nItaly: Association for Computational Linguistics, pp. 5797–5808. DOI : 10.\n18653/v1/P19- 1580. URL : https://aclanthology.org/P19-\n1580.\nVoita, Elena and Ivan Titov (Nov. 2020). “Information-Theoretic Probing with\nMinimum Description Length”. In: Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP). Online: Asso-\nciation for Computational Linguistics, pp. 183–196. DOI : 10.18653/v1/\n2020.emnlp-main.14 . URL : https://aclanthology.org/2020.\nemnlp-main.14.\nWallace, Eric, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh\n(Nov. 2019a). “Universal Adversarial Triggers for Attacking and Analyzing\nNLP”. In: Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). Hong Kong, China: Association\nfor Computational Linguistics, pp. 2153–2162. DOI : 10.18653/v1/D19-\n1221. URL : https://aclanthology.org/D19-1221.\nWallace, Eric, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner,\nand Sameer Singh (Nov. 2019b). “AllenNLP Interpret: A Framework for Ex-\nplaining Predictions of NLP Models”. In: Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing (EMNLP-IJCNLP):\nSystem Demonstrations. Hong Kong, China: Association for Computational\nLinguistics, pp. 7–12. DOI : 10.18653/v1/D19- 3002 . URL : https:\n//aclanthology.org/D19-3002.\nWang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel R. Bowman (2019). “Super-\nGLUE: a stickier benchmark for general-purpose language understanding\nsystems”. In: Proceedings of the 33rd International Conference on Neural Infor-\nmation Processing Systems. Red Hook, NY, USA: Curran Associates Inc.\nWang, Cunxiang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and\nYue Zhang (Dec. 2020). “SemEval-2020 Task 4: Commonsense Validation\nand Explanation”. In: Proceedings of the Fourteenth Workshop on Semantic\nEvaluation. Barcelona (online): International Committee for Computational\nLinguistics, pp. 307–321. DOI : 10.18653/v1/2020.semeval-1.39 .\nURL : https://aclanthology.org/2020.semeval-1.39.\nWang, Junlin, Jens Tuyls, Eric Wallace, and Sameer Singh (Nov. 2020).\n“Gradient-based Analysis of NLP Models is Manipulable”. In: Findings of\n76\nBibliography\nthe Association for Computational Linguistics: EMNLP 2020. Online: Associ-\nation for Computational Linguistics, pp. 247–258. DOI : 10.18653/v1/\n2020.findings-emnlp.24 . URL : https://aclanthology.org/\n2020.findings-emnlp.24.\nWang, Nan, Hongning Wang, Yiling Jia, and Yue Yin (2018). “Explainable\nRecommendation via Multi-Task Learning in Opinionated Text Data”. In:\nThe 41st International ACM SIGIR Conference on Research & Development\nin Information Retrieval. SIGIR ’18. Ann Arbor, MI, USA: Association for\nComputing Machinery, pp. 165–174. ISBN : 9781450356572. DOI : 10.1145/\n3209978.3210010 . URL : https://doi.org/10.1145/3209978.\n3210010.\nWei, Jason, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu,\nBrian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le (2021). “Finetuned\nLanguage Models Are Zero-Shot Learners”. In: CoRR abs/2109.01652.\narXiv: 2109.01652. URL : https://arxiv.org/abs/2109.01652.\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\nFei Xia, Ed Chi, Quoc Le, and Denny Zhou (Jan. 2022). “Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Models”. In:arXiv e-prints,\narXiv:2201.11903, arXiv:2201.11903. arXiv: 2201.11903 [cs.CL].\nWiegreffe, Sarah, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin\nChoi (July 2022). “Reframing Human-AI Collaboration for Generating\nFree-Text Explanations”. In: Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies. Seattle, United States: Association for Computational\nLinguistics, pp. 632–658. DOI : 10.18653/v1/2022.naacl-main.47 .\nURL : https://aclanthology.org/2022.naacl-main.47.\nWiegreffe, Sarah and Ana Marasovic (2021). “Teach Me to Explain: A Review\nof Datasets for Explainable Natural Language Processing”. In: Proceedings\nof the Neural Information Processing Systems Track on Datasets and Benchmarks.\nEd. by J. Vanschoren and S. Yeung. Vol. 1. URL : https://datasets-\nbenchmarks - proceedings . neurips . cc / paper / 2021 / file /\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf.\nWiegreffe, Sarah, Ana Marasovi´ c, and Noah A. Smith (Nov. 2021). “Measuring\nAssociation Between Labels and Free-Text Rationales”. In:Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing. Online\nand Punta Cana, Dominican Republic: Association for Computational\nLinguistics, pp. 10266–10284. DOI :\n10.18653/v1/2021.emnlp-main.\n804. URL : https://aclanthology.org/2021.emnlp-main.804.\nWiegreffe, Sarah and Yuval Pinter (Nov. 2019). “Attention is not not Explana-\ntion”. In: Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP). Hong Kong, China: Association for\nComputational Linguistics, pp. 11–20. DOI :\n10.18653/v1/D19-1002 .\nURL : https://aclanthology.org/D19-1002.\n77\nBIBLIOGRAPHY\nYu, Mo, Shiyu Chang, Yang Zhang, and Tommi Jaakkola (Nov. 2019). “Rethink-\ning Cooperative Rationalization: Introspective Extraction and Comple-\nment Control”. In: Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association\nfor Computational Linguistics, pp. 4094–4103. DOI : 10.18653/v1/D19-\n1420. URL : https://aclanthology.org/D19-1420.\nZhang, Kelly and Samuel Bowman (Nov. 2018). “Language Modeling Teaches\nYou More than Translation Does: Lessons Learned Through Auxiliary\nSyntactic Task Analysis”. In: Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels,\nBelgium: Association for Computational Linguistics, pp. 359–361. DOI :\n10.18653/v1/W18-5448. URL : https://aclanthology.org/W18-\n5448.\nZhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav\nArtzi (2020). “BERTScore: Evaluating Text Generation with BERT”. In:8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.URL : https://openreview.\nnet/forum?id=SkeHuCVFDr.\nZhang, Yongfeng, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaop-\ning Ma (2014). “Explicit factor models for explainable recommendation\nbased on phrase-level sentiment analysis”. In: Proceedings of the 37th Inter-\nnational ACM SIGIR Conference on Research & Development in Information\nRetrieval. SIGIR ’14. Gold Coast, Queensland, Australia: Association for\nComputing Machinery, pp. 83–92. ISBN : 9781450322577. DOI : 10.1145/\n2600428.2609579 . URL : https://doi.org/10.1145/2600428.\n2609579.\nZhang, Yujia, Kuangyan Song, Yiming Sun, Sarah Tan, and Madeleine Udell\n(2019). “\" Why Should You Trust My Explanation?\" Understanding Un-\ncertainty in LIME Explanations”. In: arXiv preprint arXiv:1904.12991. URL :\nhttps://arxiv.org/pdf/1904.12991.pdf.\nZhao, Xinyan and V . G. Vinod Vydiswaran (2020). “LIREx: Augmenting Lan-\nguage Inference with Relevant Explanation”. In: CoRR abs/2012.09157.\narXiv: 2012.09157. URL : https://arxiv.org/abs/2012.09157.\nZhou, Yangqiaoyu and Chenhao Tan (Nov. 2021). “Investigating the Effect\nof Natural Language Explanations on Out-of-Distribution Generalization\nin Few-shot NLI”. In: Proceedings of the Second Workshop on Insights from\nNegative Results in NLP. Online and Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics, pp. 117–124. DOI : 10.18653/\nv1/2021.insights- 1.17 . URL : https://aclanthology.org/\n2021.insights-1.17.\nZhu, Yukun, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel\nUrtasun, Antonio Torralba, and Sanja Fidler (2015). “Aligning Books and\nMovies: Towards Story-Like Visual Explanations by Watching Movies\n78\nBibliography\nand Reading Books”. In: 2015 IEEE International Conference on Computer\nVision (ICCV), pp. 19–27. URL : https://api.semanticscholar.org/\nCorpusID:6866988.\n79\n\nPart II\nPapers\n81\n\n \n \n \n \n \nPapers \n \nThe papers associated with this thesis have been removed for \ncopyright reasons. For more details about these see:  \nhttps://doi.org/10.3384/9789180754712    \n \n\nDepartment of Computer and Information Science \nLinköpings universitet \n \nDissertations \n \nLinköping Studies in Science and Technology \nLinköping Studies in Arts and Sciences \nLinköping Studies in Statistics \nLinköping Studies in Information Science \n \nLinköping Studies in Science and Technology \nNo 14 Anders Haraldsson: A Program Manipulation \nSystem Based on Partial Evaluation, 1977, ISBN 91-\n7372-144-1. \nNo 17 Bengt Magnhagen: Probability Based Verification of \nTime Margins in Digital Designs, 1977, ISBN 91-7372-\n157-3. \nNo 18 Mats Cedwall: Semantisk analys av process-\nbeskrivningar i naturligt språk, 1977, ISBN 91- 7372-\n168-9. \nNo 22 Jaak Urmi: A Machine Independent LISP Compiler \nand its Implications for Ideal Hardware, 1978, ISBN \n91-7372-188-3. \nNo 33 Tore Risch: Compilation of Multiple File Queries in \na Meta-Database System, 1978, ISBN 91- 7372-232-4. \nNo 51 Erland Jungert: Synthesizing Database Structures \nfrom a User Oriented Data Model, 1980, ISBN 91-\n7372-387-8. \nNo 54 Sture Hägglund: Contributions to the Development \nof Methods and Tools for Interactive Design of \nApplications Software, 1980, ISBN 91-7372-404-1. \nNo 55 Pär Emanuelson: Performance Enhancement in a \nWell-Structured Pattern Matcher through Partial \nEvaluation, 1980, ISBN 91-7372-403-3. \nNo 58 Bengt Johnsson, Bertil Andersson: The Human-\nComputer Interface in Commercial Systems, 1981, \nISBN 91-7372-414-9. \nNo 69 H. Jan Komorowski: A Specification of an Abstract \nProlog Machine and its Application to Partial \nEvaluation, 1981, ISBN 91-7372-479-3. \nNo 71 René Reboh: Knowledge Engineering Techniques \nand Tools for Expert Systems, 1981, ISBN 91-7372-\n489-0. \nNo 77 Östen Oskarsson: Mechanisms of Modifiability in \nlarge Software Systems, 1982, ISBN 91- 7372-527-7. \nNo 94 Hans Lunell: Code Generator Writing Systems, 1983, \nISBN 91-7372-652-4. \nNo 97 Andrzej Lingas: Advances in Minimum Weight \nTriangulation, 1983, ISBN 91-7372-660-5. \nNo 109 Peter Fritzson: Towards a Distributed Programming \nEnvironment based on Incremental Compilation, \n1984, ISBN 91-7372-801-2. \nNo 111 Erik Tengvald: The Design of Expert Planning \nSystems. An Experimental Operations Planning \nSystem for Turning, 1984, ISBN 91-7372- 805-5. \nNo 155 Christos Levcopoulos: Heuristics for Minimum \nDecompositions of Polygons, 1987, ISBN 91-7870-\n133-3. \nNo 165 James W. Goodwin: A Theory and System for Non-\nMonotonic Reasoning, 1987, ISBN 91-7870-183-X. \nNo 170 Zebo Peng: A Formal Methodology for Automated \nSynthesis of VLSI Systems, 1987, ISBN 91-7870-225-9. \nNo 174 Jo\n han Fagerström: A Paradigm and System for \nDesign of Distributed Systems, 1988, ISBN 91-7870-\n301-8. \nNo 192 Dimiter Driankov: Towards a Many Valued Logic of \nQuantified Belief, 1988, ISBN 91-7870-374-3. \nNo 213 Lin Padgham: Non-Monotonic Inheritance for an \nObject Oriented Knowledge Base, 1989, ISBN 91-\n7870-485-5. \nNo 214 Tony Larsson: A Formal Hardware Description and \nVerification Method, 1989, ISBN 91-7870-517-7. \nNo 221 Michael Reinfrank: Fundamentals and Logical \nFoundations of Truth Maintenance, 1989, ISBN 91-\n7870-546-0. \nNo 239 Jonas Löwgren: Knowledge-Based Design Support \nand Discourse Management in User Interface \nManagement Systems, 1991, ISBN 91-7870-720-X. \nNo 244 Henrik Eriksson: Meta-Tool Support for Knowledge \nAcquisition, 1991, ISBN 91-7870-746-3. \nNo 252 Peter Eklund: An Epistemic Approach to Interactive \nDesign in Multiple Inheritance Hierarchies, 1991, \nISBN 91-7870-784-6. \nNo 258 Patrick Doherty: NML3 - A Non-Monotonic \nFormalism with Explicit Defaults, 1991, ISBN 91-\n7870-816-8. \nNo 260 Nahid Shahmehri: Generalized Algorithmic \nDebugging, 1991, ISBN 91-7870-828-1. \nNo 264 Nils Dahlbäck: Representation of Discourse-\nCognitive and Computational Aspects, 1992, ISBN \n91-7870-850-8. \nNo 265 Ulf Nilsson: Abstract Interpretations and Abstract \nMachines: Contributions to a Methodology for the \nImplementation of Logic Programs, 1992, ISBN 91-\n7870-858-3. \nNo 270 Ralph Rönnquist: Theory and Practice of Tense-\nbound Object References, 1992, ISBN 91-7870-873-7. \nNo 273 Björn Fjellborg: Pipeline Extraction for VLSI Data \nPath Synthesis, 1992, ISBN 91-7870-880-X. \nNo 276 Staffan Bonnier: A Formal Basis for Horn Clause \nLogic with External Polymorphic Functions, 1992, \nISBN 91-7870-896-6. \nNo 277 Kristian Sandahl: Developing Knowledge Manage-\nment Systems with an Active Expert Methodology, \n1992, ISBN 91-7870-897-4. \nNo 281 Christer Bäckström: Computational Complexity of \nReasoning about Plans, 1992, ISBN 91-7870-979-2. \nNo 292 Mats Wirén: Studies in Incremental Natural \nLanguage Analysis, 1992, ISBN 91-7871-027-8. \nNo 297 Mariam Kamkar: Interprocedural Dynamic Slicing \nwith Applications to Debugging and Testing, 1993, \nISBN 91-7871-065-0. \nNo 302 Tingting Zhang: A Study in Diagnosis Using \nClassification and Defaults, 1993, ISBN 91-7871-078-\n2. \nNo 312 Arne Jönsson: Dialogue Management for Natural \nLanguage Interfaces - An Empirical Approach, 1993, \nISBN 91-7871-110-X. \nNo 338 Simin Nadjm-Tehrani: Reactive Systems in Physical \nEnvironments: Compositional Modelling and Frame-\nwork for Verification, 1994, ISBN 91-7871-237-8. \nNo 371 Bengt Savén: Business Models for Decision Support \nand Learning. A Stu dy of Discrete -Event \nManufacturing Simulation at Asea/ABB 1968-1993, \n1995, ISBN 91-7871-494-X. \nNo 375 Ulf Söderman: Conceptual Modelling of Mode \nSwitching Physical Systems, 1995, ISBN 91-7871-516-\n4. \nNo 383 Andreas Kågedal: Exploiting Groundness in Logic \nPrograms, 1995, ISBN 91-7871-538-5. \nNo 396 George Fodor: Ontological Control, Description, \nIdentification and Recovery from Problematic \nControl Situations, 1995, ISBN 91-7871-603-9. \nNo 413 Mikael Pettersson: Compiling Natural Semantics, \n1995, ISBN 91-7871-641-1. \nNo 414 Xinli Gu: RT Level Testability Improvement by \nTestability Analysis and Transformations, 1996, ISBN \n91-7871-654-3. \nNo 416 Hua Shu: Distributed Default Reasoning, 1996, ISBN \n91-7871-665-9. \nNo 429 Jaime Villegas: Simulation Supported Industrial \nTraining from an Organisational Learning \nPerspective - Development and Evaluation of the \nSSIT Method, 1996, ISBN 91-7871-700-0. \nNo 431 Peter Jonsson: Studies in Action Planning: \nAlgorithms and Complexity, 1996, ISBN 91-7871-704-\n3. \nNo 437 Johan Boye: Directional Types in Logic \nProgramming, 1996, ISBN 91-7871-725-6. \nNo 439 Cecilia Sjöberg: Activities, Voices and Arenas: \nParticipatory Design in Practice, 1996, ISBN 91-7871-\n728-0. \nNo 448 Patrick Lambrix: Part-Whole Reasoning in \nDescription Logics, 1996, ISBN 91-7871-820-1. \nNo 452 Kjell Orsborn: On Extensible and Object-Relational \nDatabase Technology for Finite Element Analysis \nApplications, 1996, ISBN 91-7871-827-9. \nNo 459 Olof Johansson: Development Environments for \nComplex Product Models, 1996, ISBN 91-7871-855-4. \nNo 461 Lena Strömbäck: User-Defined Constructions in \nUnification-Based Formalisms, 1997, ISBN 91-7871-\n857-0. \nNo 462 Lars Degerstedt: Tabulation-based Logic Program-\nming: A Multi-Level View of Query Answering, \n1996, ISBN 91-7871-858-9. \nNo 475 Fredrik Nilsson: Strategi och ekonomisk styrning - \nEn studie av hur ekonomiska styrsystem utformas \noch används efter företagsförvärv, 1997, ISBN 91-\n7871-914-3. \nNo 480 Mikael Lindvall: An Empirical Study of Require-\nments-Driven Impact Analysis in Object-Oriented \nSoftware Evolution, 1997, ISBN 91-7871-927-5. \nNo 485 Göran Forslund: Opinion-Based Systems: The Coop-\nerative Perspective on Knowledge-Based Decision \nSupport, 1997, ISBN 91-7871-938-0. \nNo 494 Martin Sköld: Active Database Management \nSystems for Monitoring and Control, 1997, ISBN 91-\n7219-002-7. \nNo 495 Hans Olsén: Automatic Verification of Petri Nets in \na CLP framework, 1997, ISBN 91-7219-011-6. \nNo 498 Thomas Drakengren: Algorithms and Complexity \nfor Temporal and Spatial Formalisms, 1997, ISBN 91-\n7219-019-1. \nNo 502 J\n akob Axelsson: Analysis and Synthesis of Heteroge-\nneous Real-Time Systems, 1997, ISBN 91-7219-035-3. \nNo 503 Johan Ringström: Compiler Generation for Data-\nParallel Programming Languages from Two-Level \nSemantics Specifications, 1997, ISBN 91-7219-045-0. \nNo 512 Anna Moberg: Närhet och distans - Studier av kom-\nmunikationsmönster i satellitkontor och flexibla \nkontor, 1997, ISBN 91-7219-119-8. \nNo 520 Mikael Ronström: Design and Modelling of a \nParallel Data Server for Telecom Applications, 1998, \nISBN 91-7219-169-4. \nNo 522 Niclas Ohlsson: Towards Effective Fault Prevention \n- An Empirical Study in Software Engineering, 1998, \nISBN 91-7219-176-7. \nNo 526 Joachim Karlsson: A Systematic Approach for \nPrioritizing Software Requirements, 1998, ISBN 91-\n7219-184-8. \nNo 530 Henrik Nilsson: Declarative Debugging for Lazy \nFunctional Languages, 1998, ISBN 91-7219-197-X. \nNo 555 Jonas Hallberg: Timing Issues in High-Level Synthe-\nsis, 1998, ISBN 91-7219-369-7. \nNo 561 Ling Lin: Management of 1-D Sequence Data - From \nDiscrete to Continuous, 1999, ISBN 91-7219-402-2. \nNo 563 Eva L Ragnemalm: Student Modelling based on Col-\nlaborative Dialogue with a Learning Companion, \n1999, ISBN 91-7219-412-X. \nNo 567 Jörgen Lindström: Does Distance matter? On geo-\ngraphical dispersion in organisations, 1999, ISBN 91-\n7219-439-1. \nNo 582 Vanja Josifovski: Design, Implementation and \nEvaluation of a Distributed Mediator System for \nData Integration, 1999, ISBN 91-7219-482-0. \nNo 589 Rita Kovordányi: Modeling and Simulating \nInhibitory Mechanisms in Mental Image \nReinterpretation - Towards Cooperative Human-\nComputer Creativity, 1999, ISBN 91-7219-506-1. \nNo 592  Mikael Ericsson: Supporting the Use of Design \nKnowledge - An Assessment of Commenting \nAgents, 1999, ISBN 91-7219-532-0. \nNo 593 Lars Karlsson: Actions, Interactions and Narratives, \n1999, ISBN 91-7219-534-7. \nNo 594 C. G. Mikael Johansson: Social and Organizational \nAspects of Requirements Engineering Methods - A \npractice-oriented approach, 1999, ISBN 91-7219-541-\nX. \nNo 595 Jörgen Hansson: Value-Driven Multi-Class Overload \nManagement in Real-Time Database Systems, 1999, \nISBN 91-7219-542-8. \nNo 596 Niklas Hallberg: Incorporating User Values in the \nDesign of Information Systems and Services in the \nPublic Sector: A Methods Approach, 1999, ISBN 91-\n7219-543-6. \nNo 597 Vivian Vimarlund: An Economic Perspective on the \nAnalysis of Impacts of Information Technology: \nFrom Case Studies in Health-Care towards General \nModels and Theories, 1999, ISBN 91-7219-544-4. \nNo 598 Johan Jenvald: Methods and Tools in Computer-\nSupported Taskforce Training, 1999, ISBN 91-7219-\n547-9. \nNo 607 Magnus Merkel: Understanding and enhancing \ntranslation by parallel text processing, 1999, ISBN 91-\n7219-614-9. \nNo 611 S\n ilvia Coradeschi: Anchoring symbols to sensory \ndata, 1999, ISBN 91-7219-623-8. \nNo 613 Man Lin: Analysis and Synthesis of Reactive \nSystems: A Generic Layered Architecture \nPerspective, 1999, ISBN 91-7219-630-0. \nNo 618 Jimmy Tjäder: Systemimplementering i praktiken - \nEn studie av logiker i fyra projekt, 1999, ISBN 91-\n7219-657-2. \nNo 627 Vadim Engelson: Tools for Design, Interactive \nSimulation, and Visualization of Object-Oriented \nModels in Scientific Computing, 2000, ISBN 91-7219-\n709-9. \nNo 637 Esa Falkenroth: Database Technology for Control \nand Simulation, 2000, ISBN 91-7219-766-8. \nNo 639 Per-Arne Persson: Bringing Power and Knowledge \nTogether: Information Systems Design for Autonomy \nand Control in Command Work, 2000, ISBN 91-7219-\n796-X. \nNo 660 Erik Larsson: An Integrated System-Level Design for \nTestability Methodology, 2000, ISBN 91-7219-890-7. \nNo 688 Marcus Bjäreland: Model-based Execution \nMonitoring, 2001, ISBN 91-7373-016-5. \nNo 689 Joakim Gustafsson: Extending Temporal Action \nLogic, 2001, ISBN 91-7373-017-3. \nNo 720 Carl-Johan Petri: Organizational Information Provi-\nsion - Managing Mandatory and Discretionary Use \nof Information Technology, 2001, ISBN 91-7373-126-\n9. \nNo 724 Paul Scerri: Designing Agents for Systems with Ad-\njustable Autonomy, 2001, ISBN 91-7373-207-9. \nNo 725 Tim Heyer: Semantic Inspection of Software \nArtifacts: From Theory to Practice, 2001, ISBN 91-\n7373-208-7. \nNo 726 Pär Carlshamre: A Usability Perspective on Require-\nments Engineering - From Methodology to Product \nDevelopment, 2001, ISBN 91-7373-212-5. \nNo 732 Juha Takkinen: From Information Management to \nTask Management in Electronic Mail, 2002, ISBN 91-\n7373-258-3. \nNo 745 Johan Åberg: Live Help Systems: An Approach to \nIntelligent Help for Web Information Systems, 2002, \nISBN 91-7373-311-3. \nNo 746 Rego Granlund: Monitoring Distributed Teamwork \nTraining, 2002, ISBN 91-7373-312-1. \nNo 757 Henrik André-Jönsson: Indexing Strategies for Time \nSeries Data, 2002, ISBN 917373-346-6. \nNo 747  Anneli Hagdahl: Development of IT-supported \nInterorganisational Collaboration - A Case Study in \nthe Swedish Public Sector, 2002, ISBN 91-7373-314-8. \nNo 749 Sofie Pilemalm: Information Technology for Non-\nProfit Organisations - Extended Participatory Design \nof an Information System for Trade Union Shop \nStewards, 2002, ISBN 91-7373-318-0. \nNo 765 Stefan Holmlid: Adapting users: Towards a theory \nof use quality, 2002, ISBN 91-7373-397-0. \nNo 771 Magnus Morin: Multimedia Representations of Dis-\ntributed Tactical Operations, 2002, ISBN 91-7373-421-\n7. \nNo 772 Pawel Pietrzak: A Type-Based Framework for Locat-\ning Errors in Constraint Logic Programs, 2002, ISBN \n91-7373-422-5. \nNo 758 Erik Berglund: Library Communication Among Pro-\ngrammers Worldwide, 2002, ISBN 91-7373-349-0. \nNo 774 Choong-ho Yi: Modelling Object-Oriented Dynamic \nSystems Using a Logic-Based Framework, 2002, ISBN \n91-7373-424-1. \nNo 779 Ma\n thias Broxvall: A Study in the Computational \nComplexity of Temporal Reasoning, 2002, ISBN 91-\n7373-440-3. \nNo 793 Asmus Pandikow: A Generic Principle for Enabling \nInteroperability of Structured and Object-Oriented \nAnalysis and Design Tools, 2002, ISBN 91-7373-479-9. \nNo 785 Lars Hult: Publika Informationstjänster. En studie av \nden Internetbaserade encyklopedins bruksegenska-\nper, 2003, ISBN 91-7373-461-6. \nNo 800 Lars Taxén: A Framework for the Coordination of \nComplex Systems´ Development, 2003, ISBN 91-\n7373-604-X. \nNo 808 Klas Gäre: Tre perspektiv på förväntningar och \nförändringar i samband med införande av \ninformationssystem, 2003, ISBN 91-7373-618-X. \nNo 821 Mikael Kindborg: Concurrent Comics - \nprogramming of social agents by children, 2003, \nISBN 91-7373-651-1. \nNo 823 Christina Ölvingson: On Development of \nInformation Systems with GIS Functionality in \nPublic Health Informatics: A Requirements \nEngineering Approach, 2003, ISBN 91-7373-656-2. \nNo 828 Tobias Ritzau: Memory Efficient Hard Real-Time \nGarbage Collection, 2003, ISBN 91-7373-666-X. \nNo 833 Paul Pop: Analysis and Synthesis of \nCommunication-Intensive Heterogeneous Real-Time \nSystems, 2003, ISBN 91-7373-683-X. \nNo 852 Johan Moe: Observing the Dynamic Behaviour of \nLarge Distributed Systems to Improve Development \nand Testing – An Empirical Study in Software \nEngineering, 2003, ISBN 91-7373-779-8. \nNo 867 Erik Herzog: An Approach to Systems Engineering \nTool Data Representation and Exchange, 2004, ISBN \n91-7373-929-4. \nNo 872 Aseel Berglund: Augmenting the Remote Control: \nStudies in Complex Information Navigation for \nDigital TV, 2004, ISBN 91-7373-940-5. \nNo 869 Jo Skåmedal: Telecommuting’s Implications on \nTravel and Travel Patterns, 2004, ISBN 91-7373-935-9. \nNo 870 Linda Askenäs: The Roles of IT - Studies of \nOrganising when Implementing and Using \nEnterprise Systems, 2004, ISBN 91-7373-936-7. \nNo 874 Annika Flycht-Eriksson: Design and Use of Ontolo-\ngies in Information-Providing Dialogue Systems, \n2004, ISBN 91-7373-947-2. \nNo 873 Peter Bunus: Debugging Techniques for Equation-\nBased Languages, 2004, ISBN 91-7373-941-3. \nNo 876 Jonas Mellin: Resource-Predictable and Efficient \nMonitoring of Events, 2004, ISBN 91-7373-956-1. \nNo 883 Magnus Bång: Computing at the Speed of Paper: \nUbiquitous Computing Environments for Healthcare \nProfessionals, 2004, ISBN 91-7373-971-5. \nNo 882 Robert Eklund: Disfluency in Swedish human-\nhuman and human-machine travel booking di-\nalogues, 2004, ISBN 91-7373-966-9. \nNo 887 Anders Lindström: English and other Foreign \nLinguistic Elements in Spoken Swedish. Studies of \nProductive Processes and their Modelling using \nFinite-State Tools, 2004, ISBN 91-7373-981-2. \nNo 889 Zhiping Wang: Capacity-Constrained Production-in-\nventory systems - Modelling and Analysis in both a \ntraditional and an e-business context, 2004, ISBN 91-\n85295-08-6. \nNo 893 P\n ernilla Qvarfordt: Eyes on Multimodal Interaction, \n2004, ISBN 91-85295-30-2. \nNo 910 Magnus Kald: In the Borderland between Strategy \nand Management Control - Theoretical Framework \nand Empirical Evidence, 2004, ISBN 91-85295-82-5. \nNo 918 Jonas Lundberg: Shaping Electronic News: Genre \nPerspectives on Interaction Design, 2004, ISBN 91-\n85297-14-3. \nNo 900 Mattias Arvola: Shades of use: The dynamics of \ninteraction design for sociable use, 2004, ISBN 91-\n85295-42-6. \nNo 920 Luis Alejandro Cortés: Verification and Scheduling \nTechniques for Real-Time Embedded Systems, 2004, \nISBN 91-85297-21-6. \nNo 929 Diana Szentivanyi: Performance Studies of Fault-\nTolerant Middleware, 2005, ISBN 91-85297-58-5. \nNo 933 Mikael Cäker: Management Accounting as \nConstructing and Opposing Customer Focus: Three \nCase Studies on Management Accounting and \nCustomer Relations, 2005, ISBN 91-85297-64-X. \nNo 937 Jonas Kvarnström: TALplanner and Other \nExtensions to Temporal Action Logic, 2005, ISBN 91-\n85297-75-5. \nNo 938  Bourhane Kadmiry: Fuzzy Gain-Scheduled Visual \nServoing for Unmanned Helicopter, 2005, ISBN 91-\n85297-76-3. \nNo 945 Gert Jervan: Hybrid Built-In Self-Test and Test \nGeneration Techniques for Digital Systems, 2005, \nISBN 91-85297-97-6. \nNo 946 Anders Arpteg: Intelligent Semi-Structured Informa-\ntion Extraction, 2005, ISBN 91-85297-98-4. \nNo 947  Ola Angelsmark: Constructing Algorithms for Con-\nstraint Satisfaction and Related Problems - Methods \nand Applications, 2005, ISBN 91-85297-99-2. \nNo 963 Calin Curescu: U tility-based Optimisation of \nResource Allocation for Wireless Networks, 2005, \nISBN 91-85457-07-8. \nNo 972 Björn Johansson: Joint Control in Dynamic \nSituations, 2005, ISBN 91-85457-31-0. \nNo 974  Dan Lawesson: An Approach to Diagnosability \nAnalysis for Interacting Finite State Systems, 2005, \nISBN 91-85457-39-6. \nNo 979 Claudiu Duma: Security and Trust Mechanisms for \nGroups in Distributed Services, 2005, ISBN 91-85457-\n54-X. \nNo 983 Sorin Manolache: Analysis and Optimisation of \nReal-Time Systems with Stochastic Behaviour, 2005, \nISBN 91-85457-60-4. \nNo 986 Yuxiao Zhao: Standards-Based Application \nIntegration for Business-to -Business \nCommunications, 2005, ISBN 91-85457-66-3. \nNo 1004 Patrik Haslum: Admissible Heuristics for \nAutomated Planning, 2006, ISBN 91-85497-28-2. \nNo 1005 Aleksandra Tešanovic: Developing Reusable and \nReconfigurable Real-Time Software using Aspects \nand Components, 2006, ISBN 91-85497-29-0. \nNo 1008 David Dinka: Role, Identity and Work: Extending \nthe design and development agenda, 2006, ISBN 91-\n85497-42-8. \nNo 1009 Iakov Nakhimovski: Contributions to the Modeling \nand Simulation of Mechanical Systems with Detailed \nContact Analysis, 2006, ISBN 91-85497-43-X. \nNo 1013 Wilhelm Dahllöf: Exact Algorithms for Exact \nSatisfiability Problems, 2006, ISBN 91-85523-97-6. \nNo 1016 Levon Saldamli: PDEModelica - A High-Level Lan-\nguage for Modeling with Partial Differential Equa-\ntions, 2006, ISBN 91-85523-84-4. \nNo 1017 Dan\n iel Karlsson: Verification of Component-based \nEmbedded System Designs, 2006, ISBN 91-85523-79-8 \nNo 1018  Ioan Chisalita: Communication and Networking \nTechniques for Traffic Safety Systems, 2006, ISBN 91-\n85523-77-1. \nNo 1019 Tarja Susi: The Puzzle of Social Activity - The \nSignificance of Tools in Cognition and Cooperation, \n2006, ISBN 91-85523-71-2. \nNo 1021 Andrzej Bednarski: Integrated Optimal Code Gener-\nation for Digital Signal Processors, 2006, ISBN 91-\n85523-69-0. \nNo 1022 Peter Aronsson: Automatic Parallelization of Equa-\ntion-Based Simulation Programs, 2006, ISBN 91-\n85523-68-2. \nNo 1030 Robert Nilsson: A Mutation-based Framework for \nAutomated Testing of Timeliness, 2006, ISBN 91-\n85523-35-6. \nNo 1034 Jon Edvardsson: Techniques for Automatic \nGeneration of Tests from Programs and \nSpecifications, 2006, ISBN 91-85523-31-3. \nNo 1035 Vaida Jakoniene: Integration of Biological Data, \n2006, ISBN 91-85523-28-3. \nNo 1045 Genevieve Gorrell: Generalized Hebbian \nAlgorithms for Dimensionality Reduction in Natural \nLanguage Processing, 2006, ISBN 91-85643-88-2. \nNo 1051 Yu-Hsing Huang: Having a New Pair of Glasses - \nApplying Systemic Accident Models on Road Safety, \n2006, ISBN 91-85643-64-5. \nNo 1054 Åsa Hedenskog: Perceive those things which cannot \nbe seen - A Cognitive Systems Engineering \nperspective on requirements management, 2006, \nISBN 91-85643-57-2. \nNo 1061 Cécile Åberg: An Evaluation Platform for Semantic \nWeb Technology, 2007, ISBN 91-85643-31-9. \nNo 1073 Mats Grindal: Handling Combinatorial Explosion in \nSoftware Testing, 2007, ISBN 978-91-85715-74-9. \nNo 1075 Almut Herzog: Usable Security Policies for Runtime \nEnvironments, 2007, ISBN 978-91-85715-65-7. \nNo 1079 Magnus Wahlström: Algorithms, measures, and \nupper bounds for Satisfiability and related problems, \n2007, ISBN 978-91-85715-55-8. \nNo 1083 Jesper Andersson: Dynamic Software Architectures, \n2007, ISBN 978-91-85715-46-6. \nNo 1086  Ulf Johansson: Obtaining Accurate and Compre-\nhensible Data Mining Models - An Evolutionary \nApproach, 2007, ISBN 978-91-85715-34-3.  \nNo 1089 Traian Pop: Analysis and Optimisation of \nDistributed Embedded Systems with Heterogeneous \nScheduling Policies, 2007, ISBN 978-91-85715-27-5. \nNo 1091 Gustav Nordh: Complexity Dichotomies for CSP-\nrelated Problems, 2007, ISBN 978-91-85715-20-6. \nNo 1106 Per Ola Kristensson: Discrete and Continuous Shape \nWriting for Text Entry and Control, 2007, ISBN 978-\n91-85831-77-7. \nNo 1110 He Tan: Aligning Biomedical Ontologies, 2007, ISBN \n978-91-85831-56-2. \nNo 1112 Jessica Lindblom: Minding the body - Interacting so-\ncially through embodied action, 2007, ISBN 978-91-\n85831-48-7. \nNo 1113 P\n ontus Wärnestål: Dialogue Behavior Management \nin Conversational Recommender Systems, 2007, \nISBN 978-91-85831-47-0. \nNo 1120 Thomas Gustafsson: Management of Real-Time \nData Consistency and Transient Overloads in \nEmbedded Systems, 2007, ISBN 978-91-85831-33-3. \nNo 1127 Alexandru Andrei: Energy Efficient and Predictable \nDesign of Real-time Embedded Systems, 2007, ISBN \n978-91-85831-06-7. \nNo 1139 Per Wikberg: Eliciting Knowledge from Experts in \nModeling of Complex Systems: Managing Variation \nand Interactions, 2007, ISBN 978-91-85895-66-3. \nNo 1143 Mehdi Amirijoo: QoS Control of Real-Time Data \nServices under Uncertain Workload, 2007, ISBN 978-\n91-85895-49-6. \nNo 1150 Sanny Syberfeldt: Optimistic Replication with For-\nward Conflict Resolution in Distributed Real-Time \nDatabases, 2007, ISBN 978-91-85895-27-4. \nNo 1155 Beatrice Alenljung: Envisioning a Future Decision \nSupport System for Requirements Engineering - A \nHolistic and Human-centred Perspective, 2008, ISBN \n978-91-85895-11-3. \nNo 1156 Artur Wilk: Types for XML with Application to \nXcerpt, 2008, ISBN 978-91-85895-08-3. \nNo 1183 Adrian Pop: Integrated Model-Driven Development \nEnvironments for Equation-Based Object-Oriented \nLanguages, 2008, ISBN 978-91-7393-895-2. \nNo 1185 Jörgen Skågeby: Gifting Technologies - \nEthnographic Studies of End-users and Social Media \nSharing, 2008, ISBN 978-91-7393-892-1. \nNo 1187 Imad-Eldin Ali Abugessaisa: Analytical tools and \ninformation-sharing methods supporting road safety \norganizations, 2008, ISBN 978-91-7393-887-7. \nNo 1204 H. Joe Steinhauer: A Representation Scheme for De-\nscription and Reconstruction of Object \nConfigurations Based on Qualitative Relations, 2008, \nISBN 978-91-7393-823-5. \nNo 1222 Anders Larsson: Test Optimization for Core-based \nSystem-on-Chip, 2008, ISBN 978-91-7393-768-9. \nNo 1238 Andreas Borg: Processes and Models for Capacity \nRequirements in Telecommunication Systems, 2009, \nISBN 978-91-7393-700-9. \nNo 1240 Fredrik Heintz: DyKnow: A Stream-Based Know-\nledge Processing Middleware Framework, 2009, \nISBN 978-91-7393-696-5. \nNo 1241 Birgitta Lindström: Testability of Dynamic Real-\nTime Systems, 2009, ISBN 978-91-7393-695-8. \nNo 1244 Eva Blomqvist: Semi-automatic Ontology Construc-\ntion based on Patterns, 2009, ISBN 978-91-7393-683-5. \nNo 1249 Rogier Woltjer: Functional Modeling of Constraint \nManagement in Aviation Safety and Command and \nControl, 2009, ISBN 978-91-7393-659-0. \nNo 1260 Gianpaolo Conte: Vision-Based Localization and \nGuidance for Unmanned Aerial Vehicles, 2009, ISBN \n978-91-7393-603-3. \nNo 1262 AnnMarie Ericsson: Enabling Tool Support for For-\nmal Analysis of ECA Rules, 2009, ISBN 978-91-7393-\n598-2. \nNo 1266 Jiri Trnka: Exploring Tactical Command and \nControl: A Role-Playing Simulation Approach, 2009, \nISBN 978-91-7393-571-5. \nNo 1268 Ba\n hlol Rahimi: Supporting Collaborative Work \nthrough ICT - How End-users Think of and Adopt \nIntegrated Health Information Systems, 2009, ISBN \n978-91-7393-550-0. \nNo 1274 Fredrik Kuivinen: Algorithms and Hardness Results \nfor Some Valued CSPs, 2009, ISBN 978-91-7393-525-8.  \nNo 1281 Gunnar Mathiason: Virtual Full Replication for \nScalable Distributed Real-Time Databases, 2009, \nISBN 978-91-7393-503-6. \nNo 1290 Viacheslav Izosimov: Scheduling and Optimization \nof Fault-Tolerant Distributed Embedded Systems, \n2009, ISBN 978-91-7393-482-4. \nNo 1294 Johan Thapper: Aspects of a Constraint \nOptimisation Problem, 2010, ISBN 978-91-7393-464-0. \nNo 1306 Susanna Nilsson: Augmentation in the Wild: User \nCentered Development and Evaluation of \nAugmented Reality Applications, 2010, ISBN 978-91-\n7393-416-9. \nNo 1313 Christer Thörn: On the Quality of Feature Models, \n2010, ISBN 978-91-7393-394-0. \nNo 1321 Zhiyuan He: Temperature Aware and Defect-\nProbability Driven Test Scheduling for System-on-\nChip, 2010, ISBN 978-91-7393-378-0. \nNo 1333 David Broman: Meta-Languages and Semantics for \nEquation-Based Modeling and Simulation, 2010, \nISBN 978-91-7393-335-3. \nNo 1337 Alexander Siemers: Contributions to Modelling and \nVisualisation of Multibody Systems Simulations with \nDetailed Contact Analysis, 2010, ISBN 978-91-7393-\n317-9. \nNo 1354 Mikael Asplund: Disconnected Discoveries: \nAvailability Studies in Partitioned Networks, 2010, \nISBN 978-91-7393-278-3. \nNo 1359 Jana Rambusch: Mind Gam es Extended: \nUnderstanding Gameplay as Situated Activity, 2010, \nISBN 978-91-7393-252-3. \nNo 1373 Sonia Sangari: Head Movement Correlates to Focus \nAssignment in Swedish, 2011, ISBN 978-91-7393-154-\n0. \nNo 1374 Jan-Erik Källhammer: Using False Alarms when \nDeveloping Automotive Active Safety Systems, 2011, \nISBN 978-91-7393-153-3. \nNo 1375 Mattias Eriksson: Integrated Code Generation, 2011, \nISBN 978-91-7393-147-2. \nNo 1381 Ola Leifler: Affordances and Constraints of \nIntelligent Decision Support for Military Command \nand Control – Three Case Studies of Support \nSystems, 2011, ISBN 978-91-7393-133-5. \nNo 1386 Soheil Samii: Quality-Driven Synthesis and \nOptimization of Embedded Control Systems, 2011, \nISBN 978-91-7393-102-1. \nNo 1419 Erik Kuiper: Geographic Routing in Intermittently-\nconnected Mobile Ad Hoc Networks: Algorithms \nand Performance Models, 2012, ISBN 978-91-7519-\n981-8. \nNo 1451 Sara Stymne: Text Harmonization Strategies for \nPhrase-Based Statistical Machine Translation, 2012, \nISBN 978-91-7519-887-3. \nNo 1455 Alberto Montebelli: Modeling the Role of Energy \nManagement in Embodied Cognition, 2012, ISBN \n978-91-7519-882-8. \nNo 1465 \nMohammad Saifullah: Biologically-Based Interactive  \nNeural Network Models for Visual Attention and \nObject Recognition, 2012, ISBN 978-91-7519-838-5. \nNo 1490 Tomas Bengtsson: Testing and Logic Optimization \nTechniques for Systems on Chip, 2012, ISBN 978-91-\n7519-742-5. \nNo 1481 David Byers: Improving Software Security by \nPreventing Known Vulnerabilities, 2012, ISBN 978-\n91-7519-784-5. \nNo 1496 Tommy Färnqvist: Exploiting Structure in CSP-\nrelated Problems, 2013, ISBN 978-91-7519-711-1. \nNo 1503 John Wilander: Contributions to Specification, \nImplementation, and Execution of Secure Software, \n2013, ISBN 978-91-7519-681-7. \nNo 1506 Magnus Ingmarsson: Creating and Enabling the \nUseful Service Discovery Experience, 2013, ISBN 978-\n91-7519-662-6. \nNo 1547 Wladimir Schamai: Model-Based Verification of \nDynamic System Behavior against Requirements: \nMethod, Language, and Tool, 2013, ISBN 978-91-\n7519-505-6. \nNo 1551 Henrik Svensson: Simulations, 2013, ISBN 978-91-\n7519-491-2. \nNo 1559 Sergiu Rafiliu: Stability of Adaptive Distributed \nReal-Time Systems with Dynamic Resource \nManagement, 2013, ISBN 978-91-7519-471-4. \nNo 1581 Usman Dastgeer: Performance-aware Component \nComposition for GPU-based Systems, 2014, ISBN \n978-91-7519-383-0. \nNo 1602 Cai Li: Reinforcement Learning of Locomotion based \non Central Pattern Generators, 2014, ISBN 978-91-\n7519-313-7. \nNo 1652 Roland Samlaus: An Integrated Development \nEnvironment with Enhanced Domain-Specific \nInteractive Model Validation, 2015, ISBN 978-91-\n7519-090-7. \nNo 1663 Hannes Uppman: On Some Combinatorial \nOptimization Problems: Algorithms and Complexity, \n2015, ISBN 978-91-7519-072-3. \nNo 1664 Martin Sjölund: Tools and Methods for Analysis, \nDebugging, and Performance Improvement of \nEquation-Based Models, 2015, ISBN 978-91-7519-071-6. \nNo 1666 Kristian Stavåker: Contributions to Simulation of \nModelica Models on Data-Parallel Multi-Core \nArchitectures, 2015, ISBN 978-91-7519-068-6. \nNo 1680 Adrian Lifa: Hardware/Software Codesign of \nEmbedded Systems with Reconfigurable and \nHeterogeneous Platforms, 2015, ISBN 978-91-7519-040-\n2. \nNo 1685 Bogdan Tanasa: Timing Analysis of Distributed \nEmbedded Systems with Stochastic Workload and \nReliability Constraints, 2015, ISBN \n978-91-7519-022-8. \nNo 1691 Håkan Warnquist: Troubleshooting Trucks – \nAutomated Planning and Diagnosis, 2015, ISBN 978-\n91-7685-993-3. \nNo 1702 Nima Aghaee: Thermal Issues in Testing of \nAdvanced Systems on Chip, 2015, ISBN 978-91-7685-\n949-0. \nNo 1715 Maria Vasilevskaya: Security in Embedded Systems: \nA Model-Based Approach with Risk Metrics, 2015, \nISBN 978-91-7685-917-9.  \nNo 1729 Ke Jiang: Security-Driven Design of Real-Time \nEmbedded System, 2016, ISBN 978-91-7685-884-4. \nNo 1733 Victor Lagerkvist: Strong Partial Clones and the \nComplexity of Constraint Satisfaction Problems: \nLimitations and Applications, 2016, ISBN 978-91-7685-\n856-1. \nNo 1734 Chandan Roy: An Informed System Development \nApproach to Tropical Cyclone Track and Intensity \nForecasting, 2016, ISBN \n978-91-7685-854-7. \nNo 1746 Amir Aminifar: Analysis, Design, and Optimization \nof Embedded Control Systems, 2016, ISBN 978-91-\n7685-826-4. \nNo 1747 E khiotz Vergara: Energy Modelling and Fairness for \nEfficient Mobile Communication, 2016, ISBN 978-91-\n7685-822-6. \nNo 1748 Dag Sonntag: Chain Graphs – Interpretations, \nExpressiveness and Learning Algorithms, 2016, ISBN \n978-91-7685-818-9. \nNo 1768 Anna Vapen: Web Authentication using Third-\nParties in Untrusted Environments, 2016, ISBN 978-\n91-7685-753-3. \nNo 1778 Magnus Jandinger: On a Need to Know Basis: A \nConceptual and Methodological Framework for \nModelling and Analysis of Information Demand in \nan Enterprise Context, 2016, ISBN 978-91-7685-713-7. \nNo 1798 Rahul Hiran: Collaborative Network Security: \nTargeting Wide-area Routing and Edge-network \nAttacks, 2016, ISBN 978-91-7685-662-8. \nNo 1813 Nicolas Melot: Algorithms and Framework for \nEnergy Efficient Parallel Stream Computing on \nMany-Core Architectures, 2016, ISBN \n978-91-7685-\n623-9. \nNo 1823 Amy Rankin: Making Sense of Adaptations: \nResilience in High-Risk Work, 2017, ISBN 978-91-\n7685-596-6. \nNo 1831 Lisa Malmberg: Building Design Capability in the \nPublic Sector: Expanding the Horizons of \nDevelopment, 2017, ISBN 978-91-7685-585-0. \nNo 1851 Marcus Bendtsen: Gated Bayesian Networks, 2017, \nISBN 978-91-7685-525-6. \nNo 1852 Zlatan Dragisic: Completion of Ontologies and \nOntology Networks, 2017, ISBN 978-91-7685-522-5. \nNo 1854 Meysam Aghighi: Computational Complexity of \nsome Optimization Problems in Planning, 2017, ISBN \n978-91-7685-519-5. \nNo 1863 Simon Ståhlberg: Methods for Detecting Unsolvable \nPlanning Instances using Variable Projection, 2017, \nISBN \n978-91-7685-498-3. \nNo 1879 Karl Hammar: Content Ontology Design Patterns: \nQualities, Methods, and Tools, 2017, ISBN 978-91-\n7685-454-9. \nNo 1887 Ivan Ukhov: System-Level Analysis and Design \nunder Uncertainty, 2017, ISBN 978-91-7685-426-6. \nNo 1891 Valentina Ivanova: Fostering User Involvement in \nOntology Alignment and Alignment Evaluation, \n2017, ISBN 978-91-7685-403-7. \nNo 1902 Vengatanathan Krishnamoorthi: Efficient HTTP-\nbased Adaptive Streaming of Linear and Interactive \nVideos, 2018, ISBN 978-91-7685-371-9. \nNo 1903 Lu Li: Programming Abstractions and Optimization \nTechniques for GPU-based Heterogeneous Systems, \n2018, ISBN 978-91-7685-370-2. \nNo 1913 Jonas Rybing: Studying Simulations with \nDistributed Cognition, 2018, ISBN 978-91-7685-348-1. \nNo 1936 Leif Jonsson: Machine Learning-Based Bug \nHandling in Large-Scale Software Development, \n2018, ISBN 978-91-7685-306-1. \nNo 1964 Arian Maghazeh: System-Level Design of GPU-\nBased Embedded Systems, 2018, ISBN 978-91-7685-\n175-3. \nNo 1967 Mahder Gebremedhin: Automatic and Explicit \nParallelization Approaches for Equation Based \nMathematical Modeling and Simulation, 2019, ISBN \n978-91-7685-163-0. \nNo 1984 Anders Andersson: Distributed Moving Base \nDriving Simulators – Technology, Performance, and \nRequirements, 2019, ISBN 978-91-7685-090-9. \nNo 1993 Ulf Kargén: Scalable Dynamic Analysis of Binary \nCode, 2019, ISBN 978-91-7685-049-7. \nNo 2001 Tim Overkamp: How Service Ideas Are \nImplemented: Ways of Framing and Addressing \nService Transformation, 2019, ISBN 978-91-7685-025-1. \nNo 2006 Daniel de Leng: Robust Stream Reasoning Under \nUncertainty, 2019, ISBN 978-91-7685-013-8. \nNo 2048 Biman Roy: Applications of Partial Polymorphisms \nin (Fine -Grained) Complexity of Constraint \nSatisfaction Problems, 2020, ISBN 978-91-7929-898-2. \nNo 2051 Olov Andersson: Learning to Make Safe Real-Time \nDecisions Under Uncertainty for Autonomous \nRobots, 2020, ISBN 978-91-7929-889-0. \nNo 2065 Vanessa Rodrigues: Designing for Resilience: \nNavigating Change in Service Systems, 2020, ISBN \n978-91-7929-867-8. \nNo 2082 Robin Kurtz: Contributions to Semantic Dependency \nParsing: Search, Learning, and Application, 2020, \nISBN 978-91-7929-822-7. \nNo 2108 Shanai Ardi: Vulnerability and Risk Analysis \nMethods and Application in Large Scale \nDevelopment of Secure Systems, 2021, ISBN 978-91-\n7929-744-2. \nNo 2125 Zeinab Ganjei: Parameterized Verification of \nSynchronized Concurrent Programs, 2021, ISBN 978-\n91-7929-697-1. \nNo 2153 Robin Keskisärkkä: Complex Event Processing \nunder Uncertainty in RDF Stream Processing, 2021, \nISBN 978-91-7929-621-6. \nNo 2168 Rouhollah Mahfouzi: Security-Aware Design of \nCyber-Physical Systems for Control Applications, \n2021, ISBN 978-91-7929-021-4. \nNo 2205 August Ernstsson: Pattern-based Programming \nAbstractions for Heterogeneous Parallel Computing, \n2022, ISBN 978-91-7929-195-2. \nNo 2218 Huanyu Li: Ontology-Driven Data Access and Data \nIntegration with an Application in the Materials \nDesign Domain, 2022, ISBN \n978-91-7929-267-6. \nNo 2219 Evelina Rennes: Automatic Adaption of Swedish \nText for Increased Inclusion, 2022, ISBN 978-91-7929-\n269-0. \nNo 2220 Yuanbin Zhou: Synthesis of Safety-Critical Real-\nTime Systems, 2022, ISBN 978-91-7929-271-3. \nNo 2247 Azeem Ahmad: Contributions to Improving \nFeedback and Trust in Automated Testing and \nContinuous Integration and Delivery, 2022, ISBN 978-\n91-7929-422-9. \nNo 2248 Ana Kuštrak Korper: Innovating Innovation: \nUnderstanding the Role of Service Design in Service \nInnovation, 2022, ISBN 978-91-7929-424-3. \nNo 2256 Adrian Horga: Performance and Security Analysis \nfor GPU-Based Applications, 2022, ISBN 978-91-7929-\n487-8. \nNo 2262 Mattias Tiger: Safety-Aware Autonomous Systems: \nPreparing Robots for Life in the Real World 2022, \nISBN 978-91-7929-501-1. \nNo 2266 Chih-Yuan Lin: Network-based Anomaly Detection \nfor SCADA Systems: Traffic Generation and \nModeling, 2022, ISBN \n978-91-7929-517-2. \nNo 2280 Filip Strömbäck: Teaching and Learning Concurrent \nProgramming in the Shared Memory Model, 2023, \nISBN 978-91-8075-000-4. \nNo 2298 Fiona Lambe: Devising Capabilities: Service Design \nfor Development Interventions, 2023, ISBN 978-91-\n8075-080-6. \n \n \nNo 2309 Alachew Mengist: Model-Based Tools Integration \nand Ontology-Driven Traceability in Model-Based \nDevelopment Environments, 2023, ISBN 978-91-8075-\n143-8. \nNo 2322 Mariusz Wzorek: Selected Functionalities for \nAutonomous Intelligent Systems in Public Safety \nScenarios, 2023, ISBN 978-91-8075-195-7. \nNo 2351 Johan Källström: Reinforcement Learning for \nImproved Utility of Simulation-Based Training, 2023, \nISBN 978-91-8075-366-1. \nNo 2364 Jenny Kunz: Understanding Large Language \nModels: Towards Rigorous and Targeted \nInterpretability Using Probing Classifiers and Self-\nRationalisation, 2024, ISBN \n978-91-8075-470-5. \nNo 2366 Sijin Cheng: Query Processing over Heterogeneous \nFederations of Graph Data, 2024, ISBN 978-91-8075-\n488-0. \n \n \nLinköping S tudies in Arts and Sciences \nNo 504 Ing-Marie Jonsson: Social and Emotional \nCharacteristics of Sp eech-based I n-Vehicle \nInformation Systems: Impact o n A ttitude and \nDriving Behaviour, 2009, ISBN 978-91-7393-478-7. \nNo 586 Fabian Segelström: Stakeholder Engagement for \nService Design: How service designers identify and \ncommunicate insights, 2013, ISBN 978-91-7519-554-4. \nNo 618 Johan Blomkvist: Representing Future Situations of \nService: Prototyping in Service Design, 2014, ISBN \n978-91-7519-343-4. \nNo 620 Marcus Mast: Human-Robot Interaction for Semi-\nAutonomous Assistive Robots, 2014, ISBN 978-91-\n7519-319-9. \nNo 677 P eter Berggren: Assessing Shared Strategic \nUnderstanding, 2016, ISBN 978-91-7685-786-1. \nNo 695 Mattias Forsblad: Distributed cognition in home \nenvironments: The pr ospective memory and \ncognitive practices of older adults, 2016, ISBN 978-\n91-7685-686-4. \nNo 787 Sara Nygårdhs: Adaptive behaviour in traffic: An \nindividual road user perspective, 2020, ISBN 978-91-\n7929-857-9. \nNo 811 Sam Thellman: Social Robots as Intentional Agents, \n2021, ISBN, 978-91-7929-008-5. \n \nLinköping Studies in Statistics \nNo 9 Davood Shahsavani: Computer Experiments De-\nsigned to Explore and Approximate Complex Deter-\nministic Models, 2008, ISBN 978-91-7393-976-8. \nNo 10 Karl Wahlin: Roadmap for Trend Detection and As-\nsessment of Data Quality, 2008, ISBN 978-91-7393-\n792-4. \nNo 11 Oleg Sysoev: Monotonic regression for large \nmultivariate datasets, 2010, ISBN 978-91-7393-412-1. \nNo 13 Agné Burauskaite-Harju: Characterizing Temporal \nChange and Inter-Site Correlations in Daily and Sub-\ndaily Precipitation Extremes, 2011, ISBN 978-91-7393-\n110-6. \nNo 14 Måns Ma gnusson: Scalable and Effi cient \nProbabilistic Topic Model Inference for Textual Data, \n2018, ISBN 978-91-7685-288-0. \nNo 15 Per Sidén: Scalable Bayesian spatial analysis with \nGaussian Markov random fields, 2020, 978-91-7929-\n818-0. \nNo 16 Caroline Svahn: Prediction Methods for High \nDimensional Data with Censored Covariates, 2022, \n978-91-7929-398-7.  \nNo 17 Héctor Rodriguez Déniz: Bayesian Models for \nSpatiotemporal Data from Transportation \nNetworks, 2023, 978-91-8075-035-6. \n \n \n \nLinköping Studies in Information Science \nNo 1 Karin Axelsson: Metodisk systemstrukturering- att \nskapa samstämmighet mellan informationssystem-\narkitektur och verksamhet, 1998. ISBN 9172-19-296-8. \nNo 2 Stefan Cronholm: Metodverktyg och användbarhet - \nen studie av dator stödd metod baserad \nsystemutveckling, 1998, ISBN 9172-19-299-2. \nNo 3 Anders Avdic: Användare och utvecklare - om \nanveckling med kalkylprogram, 1999. ISBN 91-7219-\n606-8. \nNo 4 Owen Eriksson: Kommunikationskvalitet hos infor-\nmationssystem och affärsprocesser, 2000, ISBN 91-\n7219-811-7. \nNo 5 Mikael Lind: Från system till process - kriterier för \nprocessbestämning vid verksamhetsanalys, 2001, \nISBN 91-7373-067-X. \nNo 6 Ulf Melin: Koordination och informationssystem i \nföretag och nätverk, 2002, ISBN 91-7373-278-8. \nNo 7 Pär J. Ågerfalk: Information Systems Actability - Un-\nderstanding Information Technology as a Tool for \nBusiness Action and Communication, 2003, ISBN 91-\n7373-628-7. \nNo 8 Ulf Seigerroth: Att förstå och förändra system-\nutvecklingsverksamheter - en taxonomi fö r \nmetautveckling, 2003, ISBN 91-7373-736-4.  \nNo 9 Karin Hedström: Spår av datoriseringens värden – \n Effekter av IT i äldreomsorg, 2004, ISBN 91-7373-963-\n4. \nNo 10 E\n wa Braf: K nowledge Demanded for Action - \nStudies on Knowledge Mediation in Organisations, \n2004, ISBN 91-85295-47-7. \nNo 11 Fredrik Karlsson: Method Configuration method \nand computerized tool support, 2005, ISBN 91-85297-\n48-8. \nNo 12 Malin Nordström: Styrbar systemförvaltning - Att \norganisera systemförvaltningsverksamhet med hjälp \nav effektiva förvaltningsobjekt, 2005, ISBN 91-85297-\n60-7. \nNo 13 Stefan Holgersson: Yrke: POLIS - Yrkeskunskap, \nmotivation, IT-system och andra förutsättningar för \npolisarbete, 2005, ISBN 91-85299-43-X. \nNo 14 Benneth Christiansson, Marie-Therese \nChristiansson: Mötet mellan process och komponent \n- mot ett ramverk för en verks amhetsnära \nkravspecifikation vid anskaffning av komponent-\nbaserade informationssystem, 2006, ISBN 91-85643-\n22-X. \nUnderstanding Large\nLanguage Models\nLinköping Studies in Science and Technology\nDissertation No. 2364\nJenny Kunz\nJenny Kunz         Understanding Large Language Models                      2024\nFACULTY OF SCIENCE AND ENGINEERING\nLinköping Studies in Science and Technology, Dissertation No. 2364, 2024\nDepartment of Computer and Information Science\nLinköping University\nSE-581 83 Linköping, Sweden\nwww.liu.se Towards Rigorous and Targeted Interpretability\nUsing Probing Classifiers and Self-Rationalisation",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9439836740493774
    },
    {
      "name": "Rationalisation",
      "score": 0.7856920957565308
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4438612461090088
    },
    {
      "name": "Computer science",
      "score": 0.40842998027801514
    },
    {
      "name": "Natural language processing",
      "score": 0.33315780758857727
    },
    {
      "name": "Mathematics",
      "score": 0.2583470046520233
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}