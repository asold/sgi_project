{
    "title": "A deep learning approach for orphan gene identification in moso bamboo (Phyllostachys edulis) based on the CNN + Transformer model",
    "url": "https://openalex.org/W4229031330",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100356745",
            "name": "Xiaodan Zhang",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5043142532",
            "name": "Jinxiang Xuan",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5089588347",
            "name": "Chensong Yao",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5054680322",
            "name": "Qijuan Gao",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5050632045",
            "name": "Lianglong Wang",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5064144631",
            "name": "Xiu Jin",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A5103075590",
            "name": "Shaowen Li",
            "affiliations": [
                "Anhui Agricultural University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2059223767",
        "https://openalex.org/W2046085083",
        "https://openalex.org/W2107519392",
        "https://openalex.org/W2132706594",
        "https://openalex.org/W2132009717",
        "https://openalex.org/W2968199443",
        "https://openalex.org/W1988485432",
        "https://openalex.org/W2127673162",
        "https://openalex.org/W2109636763",
        "https://openalex.org/W2056699111",
        "https://openalex.org/W2980141724",
        "https://openalex.org/W1972824771",
        "https://openalex.org/W2092776658",
        "https://openalex.org/W2173312143",
        "https://openalex.org/W6628753890",
        "https://openalex.org/W2161153002",
        "https://openalex.org/W3085184829",
        "https://openalex.org/W2900903885",
        "https://openalex.org/W3036973367",
        "https://openalex.org/W2958612150",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3171169846",
        "https://openalex.org/W2055043387",
        "https://openalex.org/W3025339916",
        "https://openalex.org/W3162780970",
        "https://openalex.org/W3213342912",
        "https://openalex.org/W2141292707",
        "https://openalex.org/W2766191963",
        "https://openalex.org/W2998559844",
        "https://openalex.org/W2885583144",
        "https://openalex.org/W2736280136",
        "https://openalex.org/W2963347351",
        "https://openalex.org/W2916877561",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W1963749132",
        "https://openalex.org/W2433743436",
        "https://openalex.org/W3000237001",
        "https://openalex.org/W2999309192",
        "https://openalex.org/W2918408501",
        "https://openalex.org/W2050077513",
        "https://openalex.org/W2165118109",
        "https://openalex.org/W2889887734",
        "https://openalex.org/W4294216483",
        "https://openalex.org/W2969718750",
        "https://openalex.org/W3096828292",
        "https://openalex.org/W1478648646",
        "https://openalex.org/W2128689695",
        "https://openalex.org/W2159482845",
        "https://openalex.org/W4243272828",
        "https://openalex.org/W2108492781",
        "https://openalex.org/W2891563800"
    ],
    "abstract": null,
    "full_text": "A deep learning approach for orphan gene \nidentification in moso bamboo (Phyllostachys \nedulis) based on the CNN + Transformer model\nXiaodan Zhang1,2†, Jinxiang Xuan1,2†, Chensong Yao3†, Qijuan Gao1, Lianglong Wang1,2, Xiu Jin1,2* and \nShaowen Li1,2* \nBackground\nOG are genes that lack homologs in other lineages [1 ]. Thus, genes are generally clas -\nsified as orphans if they lack coding-sequence similarity outside of their own species \n[2]. OG appear to be present in all species and make up 10% to 30% of all genes in a \nAbstract \nBackground: Orphan gene play an important role in the environmental stresses of \nmany species and their identification is a critical step to understand biological func-\ntions. Moso bamboo has high ecological, economic and cultural value. Studies have \nshown that the growth of moso bamboo is influenced by various stresses. Several \ntraditional methods are time-consuming and inefficient. Hence, the development of \nefficient and high-accuracy computational methods for predicting orphan genes is of \ngreat significance.\nResults: In this paper, we propose a novel deep learning model (CNN + Transformer) \nfor identifying orphan genes in moso bamboo. It uses a convolutional neural network \nin combination with a transformer neural network to capture k-mer amino acids and \nfeatures between k-mer amino acids in protein sequences. The experimental results \nshow that the average balance accuracy value of CNN + Transformer on moso bamboo \ndataset can reach 0.875, and the average Matthews Correlation Coefficient (MCC) value \ncan reach 0.471. For the same testing set, the Balance Accuracy (BA), Geometric Mean \n(GM), Bookmaker Informedness (BM), and MCC values of the recurrent neural network, \nlong short-term memory, gated recurrent unit, and transformer models are all lower \nthan those of CNN + Transformer, which indicated that the model has the extensive \nability for OG identification in moso bamboo.\nConclusions: CNN + Transformer model is feasible and obtains the credible predic-\ntive results. It may also provide valuable references for other related research. As our \nknowledge, this is the first model to adopt the deep learning techniques for identifying \norphan genes in plants.\nKeywords: Orphan genes, Moso bamboo, Deep learning, Convolutional neural \nnetwork, Transformer neural network\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nZhang et al. BMC Bioinformatics          (2022) 23:162  \nhttps://doi.org/10.1186/s12859-022-04702-1 BMC Bioinformatics\n†Xiaodan Zhang, Jinxiang \nXuan and Chensong Yao \nhave contributed equally to \nthis work\n*Correspondence:   \njinxiu123@ahau.edu.cn; \nliahau@163.com\n1 Anhui Province Key \nLaboratory of Smart \nAgricultural Technology \nand Equipment, Anhui \nAgriculture University, \nHefei 230001, China\nFull list of author information \nis available at the end of the \narticle\nPage 2 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \ngenome [3 ]. Currently, a growing number of OG are being identified in plants, includ -\ning taxa such as Arabidopsis, Populus, Oryza sativa and sweet orange [ 4–7]. Many \nannotated OG are often differentially expressed in response to stresses and are con -\nsidered to be determinant of species characteristics [8 –10]. Hence, the identification \nof OG may provide a better understand for OG adaptation.\nMoso bamboo (Phyllostachys edulis) belongs to the subfamily Bambusoideae of the \nPoaceae family; it shows characteristics of fast growth and excellent material pro -\nduction and can therefore be used to produce cloths, artwork, paper and food [11]. \nRecent studies have revealed that stresses such as drought and high temperature can \naffect the growth of moso bamboo as well as the yield and quality of moso bamboo \nshoots [12, 13]. Although the identification of OG has been widely carried out in \nmany plant species,  a comprehensive understanding of OG is lacking in moso bam -\nboo. Therefore, the discovery of OG is of great significance for subsequent research in \nthis species.\nCurrently, orphan genes are generally obtained through BLAST sequence alignment, \nwhich compares the sequencing sequence (genome sequence, transcriptome sequence, \netc.) of the studied species with other species, BLAST is a relatively reliable tool for \nidentifying orphan genes [2]. BLAST, including BLASTP and tBLASTn, are often used \nas the alignment tools [14–16]. However, the use of these methods to identify OG \nrequires considerable server and time resources and is greatly affected by the computa -\ntional approach applied [17]. Orphan genes are widely distributed in plant species and \ngenerally exhibit significant differences in gene length, the number of exons, GC con -\ntent, and expression level compared to protein-coding genes [3, 6, 10, 16, 18]. Therefore, \norphan genes and non-orphan genes are distinguishable in terms of protein features. In \ntraditional machine learning methods, features are often selected and extracted manu -\nally, which requires researchers to have prior domain knowledge and keen insight into \nthe relationships between gene essentiality and types of biological data to obtain inform-\native features to train the models. For example, Ying et  al. [19] employed a machine \nlearning-based approach to predict autism spectrum disorder (ASD) risk genes using \nhuman brain spatiotemporal gene expression signatures, gene-level constraint metrics, \nand other gene variation features. Recent study shows that using deep learning to extract \nfeatures often achieves better results compared to its closest machine learning competi -\ntors, the majority of these deep learning algorithms rely on features extracted from raw \nsequences [20]. Hence, it would be significant to develop an efficient method that uses \nonly protein sequences to train such models and produces credible predictive results.\nAs deep learning has gained popularity, it has been applied successfully in many bio -\ninformatics fields, such as gene prediction [21] and medical image segmentation [22]. In \nparticular, deep learning technology is used to automatically extract and learn abstract \ninformation from data to train a model; this approach shows superior performance and \nhigh adaptability and avoids complex feature engineering in natural language process -\ning [23]. Moreover, protein sequences are very similar to natural language; the amino \nacids in proteins are similar to the words in natural language, and the same contextual \nrelationship exists between amino acids in a protein sequence and as between words in \na sentence. In this context, the prediction of OG can be considered a natural language \nprocessing problem.\nPage 3 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \nRecently, transformer models have been widely used to address sequence problems. \nZheng et  al. [24] proposed the Segmentation Transformer (SETR) model, which regards \nsemantic segmentation as a sequence-to-sequence prediction task. Zou et al. [25] proposed \na transformer model for end-to-end target detection.\nIn this paper, OG are predicted by using hybrid deep learning based on convolutional \nneural networks and transformer neural networks. The raw protein sequences are first \nencoded as vectors or matrices by encoder or word2vec encoding, respectively. Then, pro-\ntein features are extracted by CNN and transformer model. Finally, the extracted features \nare input into the fully connected neural network to generate the final recognition result. \nCNN + Transformer only uses protein sequences to train the models to predict moso bam-\nboo OG. It uses two multicore convolution layers to capture high-frequency k-mer features \nin protein sequences. The extracted k-mer features are provided to the transformer layer, \nwhich captures the long-term interaction information between k-mer features through a \nmulti-head self-attention mechanism.\nMethods\nDataset\nBLAST (2.11.0+) [26] was used to identify OG based on previous studies [6, 27–29]. Moso \nbamboo protein sequences were downloaded from Bamboo GDB [30]. First, we used \nBLASTp to search for homologs of all 31,987 proteins annotated in moso bamboo in each \nof the other 136 plant species released in Phytozome v12.1 [31] with an e-value cutoff of \n1e−5. A total of 30,443 moso bamboo genes showed significant similarity to at least one \nsequence, which were defined as Evolutionarily Conserved genes (ECs) [32] and removed \nfrom further analysis (Fig. 1). Second, the remaining 1936 moso bamboo proteins for which \nno homologs could be found in any of the genomes were used for the next step of searches, \nwhich was performed by tBLASTn analysis. In this step, 392 moso bamboo genes were clas-\nsified as ECs. The final sets of ECs and OG contained 30,443 and 1,544 genes (Additional \nfile 1: Table S1), respectively (Fig. 1).\nTo adequately train the deep learning model, the 1544 obtained OG were identified with \nlabel 1, and 30,443 ECs were identified with label 0. These genes were combined to form the \nmoso bamboo orphan gene dataset.\nProtein embedding\nProtein sequences consist of possible 20 amino acids, each of which is represented by a \ncapital letter. To make the protein sequence recognizable by a computer, the first step is to \nencode each amino acid in the protein according to Table 1, mapping each amino acid to \na specific real number, where values of 1–20 represent the amino acid types, and unspeci-\nfied or unknown amino acids are denoted as 21 [33, 34]. The amino acid coding sequence \nin the table does not affect the experimental results. The sequence profiles thus obtained \nfor each sequence search were processed by truncating the profiles of long sequences to \na fixed length (L) and zero padding short sequences, a method that is widely used for data \npreprocessing and effective training [35]. As a result, we obtained a one-dimensional vector \nfor each protein.\n(1)s = (s1 ,s2 ,... ,sL )si ∈{ 0, 1,... , 21}\nPage 4 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \nFor protein sequences, by learning the dense continuous feature representation of \neach amino acid in the sequence, a distributional representation can be learned for the \namino acids. When these embedding vectors are projected in 2D, it can be shown that \namino acids with similarities in hydrophobicity, polarity and net charge, which are fac -\ntors important for covalent chemical bonding, form visually distinguishable groups [36]. \nThis validates the used of distributed representation as an effective method for encoding \namino acids that also helps to preserve important physiochemical properties.\nHence, the sparse feature vectors (S) of a given protein sequence are transformed to \ndense continuous feature representations using word embedding transformation as \nFig. 1 Flow chart of data acquisition for moso bamboo OG\nTable 1 Amino acid embedding cross-reference table\nAmino acids Letters Code Amino acids Letters Code\nHistidine H 1 Methionine M 2\nAlanine A 3 Lysine K 4\nCysteine C 5 Arginine R 6\nLucine L 7 Tyrosine Y 8\nSerine S 9 Aspartic D 10\nGlycine G 11 Valine V 12\nIsoleucine I 13 Glutamic E 14\nAsparagine N 15 Tryptophan W 16\nPhenylalanine F 17 Threonine T 18\nGlutamine Q 19 Proline P 20\nIllegal Amino acids B,J,O,U,X,Z 21\nPage 5 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \nfollows:F 1\ne ∈ RL×e , where e corresponds to the embedding dimension. The self-atten -\ntion mechanism in the transformer cannot distinguish between words at different \npositions, so the input sequence needs to be position-encoded to incorporate the posi -\ntional information into the input sequence. The input sequence is then encoded with \na two-dimensional matrix, F 2\ne ∈ RL×e . F 1\ne and F 2\ne  are added together as the input of \nCNN + Transformer, as follows:\nHere, S i,j corresponds to the jth word embedding number of the ith amino acid of the \nprotein sequence after preprocessing.\nTransformer model\nThe canonicalization model used in this work was based on a transformer architecture \nconsisting of two separate stacks of layers for the encoder and decoder, respectively [37]. \nThe structure of the encoder and decoder, as shown in Fig.  2, mainly includes a multi-\nhead attention mechanism layer, a feed-forward fully connected layer and normalization \nand residual connections [37, 38].\nThe multi-head attention mechanism incorporates some portion of knowledge \nwritten in its internal memory (V ) with indexed access by keys (K ). When new data \n(2)F e = F 1\ne + F 2\ne =\n\n\ns1,1 ··· s1,j ··· s1,e\n... ···\n... ···\n...\nsi,1 ··· si,j ··· si,e\n... ···\n... ···\n...\nsL,1 ··· sL,j ··· sL,e\n\n\nFig. 2 The structure of the encoder and decoder in the transformer model\nPage 6 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \narrive (Q ), the layer calculates attention and modifies the input accordingly, thus \ngenerating the output of the self-attention layer and weighting the parts that carry \nthe essential information. The formulas of Q , K, and V  are as follows:\nFc is the input matrix of the transformer model, W Q\ni  is the query transformation matrix \nweight vector, W K\ni  is the keyword transformation matrix weight vector, and W V\ni  is the \nvalue transformation matrix weight vector.\nQ and V  perform dot product operations, and the result is divided by the scaling \nfactor \n√\nd The result is divided by the scaling factor and then multiplied by V  follow -\ning the application of the softmax function to obtain the result after self-attention. \nThe output (H ) of multi-head self-attention is obtained by splicing the self-attention \nN-head times, and the formula for multi-head self-attention is as follows.\nwhere head i denotes the i-th self-attention mechanism (1 <  = i <  = N-head). LayerNorm \n[39] indicates layer normalization, which mainly serves to speed up the convergence of \nthe model, while the residual network structure is used to reduce the learning load of the \nmodel with the following equation.\nDropout [40] is a stochastic deactivation strategy to prevent overfitting in mod -\nels with a large number of parameters. The feed-forward layer enhances the nonlin -\near capability of the model with two layers of neural networks, and the transformer \nstructure continues after the feed-forward layer with a normalization and residual \nlayer, according to the following equation.\nwhere O is the output vector of the transformer layer in the model, H′ is the normalized \nvector, and W(1) , W(2) , b(1) , and b(2) are the weight coefficients and bias of the 2-layer \nneural network, respectively.\n(3)Q = FcW Q\ni\n(4)K = FcW K\ni\n(5)V = FcW V\ni\n(6)headi = Multihead(F cW Q\ni ,FcW K\ni ,FcW V\ni )\n(7)Attention(Q,K,V ) = softmax\n(QK T\n√\nd\n)\nV\n(8)H = Concat (head1 ,head2 , ...,headN −head)\n(9)H\n′\n= LayerNorm(H + Dropout(H))\n(10)O = LayerNorm(H\n′\n+ Dropout(Relu(H\n′\nW (1) + b(1))W (2) + b(2)))\nPage 7 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \nCNN + Transformer model\nAn important disadvantage of the transformer model is its inefficiency in processing \nlong sequences, mainly due to the computation and memory complexity of the self-\nattention module [41]. CNN can extract local features in the sequence to shorten the \nlength of the sequence [42, 43]. Therefore, this research proposes the CNN + Trans-\nformer model structure, which combines a CNN and a transformer model. As shown in \nFig. 3, the proposed CNN + Transformer model structure is composed of two multicore \nconvolution layers: a transformer layer and a fully connected layer.\nAfter protein embedding, the protein sequences are encoded as dense, continuous \nvectors ( Fe ) as the input of the CNN layer. The CNN layer in the model consists of two \nconvolutional layers, with the first convolutional layer containing six convolution ker -\nnels, as shown in Fig.  3. The variable size of the filter in the convolution is designed to \ncapture k-mer amino acid fragments, where k ranges from 2 (2 peptides) to 7 (7 pep -\ntides). The second layer consists of three convolution kernels, denoted as {\nk n\nj\n}\nn=1,2,3j=3,6,9\n , where n represents the first few kernels, and j represents the size of \nthe corresponding kernel. The kernel size is equal to the size of a convolutional window \nacross j characters, and the parameters are tuned according to the training and valida -\ntion step. The intermediate feature map of the i-th CNN layer is extracted as \nFi\nm = Conv (Fe,K i).\nAfter obtaining the intermediate convolutional feature map ( Fi\nm  ), downsampling is \nperformed using AvgPooling by taking the average of the output subregions of the CNN \nlayer, which helps maintain the integrity of the information and facilitates subsequent \nFig. 3 CNN + Transformer model structure. The discrete raw sequence is transformed into a dense, \ncontinuous vector  Fe through feature embedding and then fed into the CNN layer with multi-scale \nconvolution kernels to capture local amino acid k-mers features. The extracted characteristic map of the CNN \nlayer is passed to Transformer neural network. According the multi-head self attention mechanism to capture \nthe long-range interaction characteristics between k-mers. Finally, the Transformer outputs are passed to the \nfully connected layers to produce identification result\nPage 8 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \nglobal feature extraction. After average pooling, the output from all kernels is concat -\nenated for another average pooling operation, which is used to generate the next layer of \nthe feature maps ( Fc ), with the following equation:\nwhere AvgPooling and Concat are average pooling operations and connection opera -\ntions, respectively.\nThe transformer layer in the CNN + Transformer model is composed of three trans -\nformer encoder-decoder layers. In the transformer layer, the feature representation of \nlong-range interaction information between amino acids is obtained by introducing \na multi-head self-attention mechanism, and the values of two hyperparameters in the \ntransformer layer, the number of self-attention heads (N-head) and the number of trans-\nformer layers (Num-layer), are discussed and explained in the “Results” .\nThe output of the transformer layer is flattened to one dimension. The number of \noutput hidden vectors in the fully connected layer is 2, which indicates the binary clas -\nsification predicted by the model, and the output vector of the binary classification is \ntransformed into the probability (p) by the ReLU activation function.\nwhere σ is the activation function, l >= 1 is the number of layers of the multiconnected \nneural network, and W l and b l are the connection weights and biases of the hidden \nnodes from layer l − 1 to layer l in the fully connected layer, respectively.W l ∈ Rnl−1×nl , \nn l−1 and n l are the numbers of hidden nodes in layers l − 1 and l , respectively. T (l) is the \noutput hidden vector of layer l . W s and bs are the weights and biases, respectively, of the \npenultimate layer of the fully connected neural network, and W s ∈ Rnl×2.\nImplementation details\nThe loss function is cross-entropy loss function and using the Adam optimizer [44]. In \nthe training set, there were approximately 20 times more moso bamboo OG than moso \nbamboo non-OG, which led to an imbalance problem during training. We explored a \ncost-sensitive technique for addressing the imbalance problem when training the clas -\nsifier. The experiments were conducted by adding weights to the different categories of \ndata during training according to the number ratio and using the category weights to \ntrain the model. CNN + Transformer involved multiple hyperparameters. These hyper -\nparameters were tuned on the validation set using agrid search procedure. Their optimal \nvalues are mentioned below:\n1. Embedding dimension: we tested for e ∈{ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100} and \nfound that the optimal model performance was obtained at e = 50.\n2. Convolution filters: at the first convolution layer, we chose six convolution filters, \ns.t. f 1\nk ∈{ 2, 3, 4, 5, 6, 7} . This allowed us to capture amino acid k-mer frequencies \nfor k-mers of lengths, k = 2 to k = 7. These k-mers represent the local contextual \n(11)Fc = AvgPooling(Concat(AvgPooling(F i\nm )))\n(12)T (1) = σ( W lT (l−1) + bl)\n(13)p = σ( W sT (l) + bs)\nPage 9 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \n‘biological’ words. For the second convolution layer, the optimal filter sizes were \nf 2\nk ∈{ 3, 6, 9} . This led to inference of interactions between amino acid k-mers i.e. \ndetect frequencies of local contextual biological phrases consisting of two k-mers \nhaving same or different k. For example, the second convolution layer could appre -\nhend interactions between two different dipeptides as well asestimate frequency of a \nbiological phrase comprising a dipeptide and a tripeptide.\n3. Transformer encoder-decoder layer number L and self-attention mechanism number \nH: We took {2, 3, 4} for L and {5, 10} for H, and formed six combinations of L and \nH, { (2, 5), (2, 10), (3, 5), (3,10), (4,5), (4,10)}. The optimal model performance was \nattained for L = 3 and H = 10.\n4. Fully connected layer dimension: we tested for fc ∈{ 64, 128, 256, 512} and for opti -\nmal model fc was 256.\n5. Learning rate: the learning rate for the Adam optimizer was 0.001.\n6. Number of epochs: the maximum number of epochs was set to 100 but we enforced \nearly stoppage if the validation loss function stopped improving for two consecutive \nepochs.\n7. Batch size: we tested for batch sizes {64, 128, 256}. The optimal model performance \nwas attained for batch size = 128.\nAll deep learning models were implemented in Pytorch (1.7.1) [45]. To speed up the \ntraining process, a GPU version of PyTorch on an NVIDIA Tesla P100 PCIe 16 GB sys -\ntem was used for the experiments.\nEvaluation strategies\nThe identification of OG is an imbalance problem, and the number of non-OG in the \nmoso bamboo orphan gene dataset is approximately 20 times greater than the number \nof OG. Although accuracy and F1 scores are very popular classification evaluation met -\nrics, they can produce misleading results for unbalanced datasets because they do not \ntake into account the ratio between positive and negative samples, and classifiers can \nachieve good results in terms of specificity but show a large number of false positives. \nAn effective solution for overcoming the class imbalance issue comes from the MCC and \nBA [46]. When dealing with an unbalanced dataset, GM and BM are better performance \nmetrics if the classification success rate is of concern [47]. Therefore, BA, BM, GM, and \nMCC were selected as the evaluation metrics in the experiment. The evaluation indica -\ntors used in the article are summarized as follows:\n(14)BA = 1\n2 ×\n( TP\nTP + FP + TN\nTN + FN\n)\n(15)GM =\n√\nTP\nTP + FN × TN\nTN + FP\n(16)BM = TP\nTP + FN + TN\nTN + FP − 1\nPage 10 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \nHere, TP is the number of OG identified as OG, FN is the number of OG identified \nas non-OG, FP is the number of non-OG identified as OG, and TN is the number of \nnon-OG identified as non-OG.\nResults and discussion\nDataset division\nWe use 70% of the data for training, 15% of the data for validation, and the remain -\ning 15% of the data as holdout data for testing. We maintain the same ratio between \nthe number of OG and non-OG during the training, validation, and testing of data. \nEach experiment is executed 10 times to obtain the average performance (i.e., tenfold \ncross-validation with 15% independent data as testing data for each run). The average \nperformance for the independent holdout testing datasets is reported. The datasets \nthat we use in the experiment are shown in Table 2 .\nOrphan genes are widely distributed in plant species and generally exhibit signifi -\ncant differences in gene length [2 , 10]. As can be seen from Fig.  4, the sequence length \ndistribution of the Original Set, Training Set, Validation Set and Testing Set is similar, \nindicating that four data sets are comparable.\n(17)MCC = TP × TN - FP × FN√\n(TP + FP)× (TP + FN) × (TN + FP) × (TN + FN)\nTable 2 Division and construction of datasets\nData OG Non-OG Total\nOriginal set 1544 30,443 31,987\nTraining set 1071 21,317 22,388\nValidation set 235 4566 4801\nTesting set 238 4560 4798\nFig. 4 Sequence length distribution in original set, training set, validation set, and testing set\nPage 11 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \nPerformance comparison of different CNN + Transformer architectures\nThe CNN + Transformer model structure proposed in this study includes an embed -\nding layer and two multicore convolutional layers: a transformer layer and a fully con -\nnected layer. Table 3 shows the performance comparison of CNN + Transformer under \ndifferent model structures. From the table, we can see that the average BA value and \nGM value of the proposed CNN + Transformer architecture for the testing set can reach \n0.877 and 0.881, respectively. Reducing the CNN layer or transformer layer in this struc-\nture will result in a decrease in model recognition performance. Specifically, when the \ntransformer layer is removed from the original structure, the average total BA value and \nGM value are 0.773 and 0.784, respectively. When two multicore convolutional layers are \nremoved from the original structure, the average total BA value and GM value are 0.844 \nand 0.832, respectively. In contrast, removing one 3-core convolutional layer or two mul-\nticore convolutional layers from the original structure will result in a slight decrease in \nthe recognition performance of the model. After adding a fully connected layer of 256 \nneurons on the basis of the original framework, the recognition performance of the \nmodel is basically the same as that of the original framework, but the complexity of the \nmodel structure increases. When a 3-core convolutional layer is added on the basis of \nthe original model structure, the average BA value and GM value are 0.853 and 0.837, \nrespectively, and the recognition performance of the model declines.\nThe effect of hyperparameters on model performance\nWe evaluated the robustness of CNN + Transformer and elucidated the effect of two \nhyperparameters on the model: Max_len and Embedding_dim. The MCC values and \nbalance accuracy were used to evaluate the model as the hyperparameters were adjusted. \nThe first hyperparameter is the maximum sequence length, Max_len. From Fig.  5, it \ncan be seen that when the value of Max_len is less than 1200, the values of MCC and \nBA are positively correlated with Max_len as a whole. At 1200, the average values of \nMCC and BA in the model are as high as 0.469 and 0.876, respectively, because some \nsequences with longer inputs are truncated at shorter lengths, which results in the loss \nof information contained in the sequences. However, when the sequence length contin -\nues to increase, the MCC value and BA value show a slight downward trend, indicating \nthat simply increasing Max_len does not improve the recognition performance of the \nTable 3 The average BA and GM values under different CNN + Transformer structures\nBold values indicate the highest values of different evaluation indicators\nE: word embedding coding. CNN_6: multiscale convolution layer, with a convolution kernel size of {2, 3, 4, 5, 6, 7} for each \nscale. CNN_3: multiscale convolution layer, the convolution kernel size of each scale is {3, 6, 9}. Transformer: three-layer \ntransformer neural network. FC_256: fully connected neural network with 256 neurons\nMethod BA GM Train time \n(min)\nTest time (s)\nE + FC_256 0.677 0.612 25 88\nE + CNN_6 + FC_256 0.748 0.644 29 106\nE + CNN_6 + CNN_3 + FC_256 0.773 0.784 28 99\nE + Transformer + FC_256 0.844 0.832 57 390\nE + CNN_6 + Transformer + FC_256 0.866 0.849 61 377\nE + CNN_6 + CNN_3 + Transformer + FC_256 0.877 0.881 44 342\nE + CNN_6 + CNN_3 + CNN_3 + Transformer + FC_256 0.853 0.837 48 366\nE + CNN_6 + CNN_3 + Transformer + FC_256 + FC_256 0.871 0.865 55 404\nPage 12 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \nmodel but would increase the model recognition time and reduce the model recognition \nefficiency.\nThe second hyperparameter is the word embedding dimension, Embedding_dim. \nWe take every 10 values from 0 to 100 as the word embedding dimension. As shown in \nFig. 6, when Embedding_dim is equal to 50, the best mean MCC values and BA values \nof the model are highest; the best mean MCC values are 0.481 and 0.467, respectively; \nand the best mean BA values are 0.892 and 0.876, respectively. When Embedding_dim \nis equal to 50, increasing Embedding_dim further does not improve the performance of \nthe model but increases the time to train the weights of the embedding matrix. There -\nfore, an Embedding_dim of 50 is selected for the experiment values.\nNext, we study the effect of two other hyperparameters, N-head and Num-layer, in \nthe transformer layer on model performance. Because the head number (N-head) of the \nhyperparameter multi-head self-attention mechanism must be divisible by Embedding_\ndim, N-head values of 5 and 10 are chosen for the experiment. The numbers of layers of \nthe encoder-decoder in the transformer hyperparameter are 2, 3, and 4. There are six \ncombinations of N-head and Num-layer. Figure  7 shows the performance comparison \nof the models in the six different combinations. From the figure, we can see that in the \nCNN + Transformer model, when there are three layers of the transformer encoder-\ndecoder and 10 heads of the attention mechanism, the best, average, and worst BA and \nMCC values of the model in the testing set are highest for the six combinations; the best, \nFig. 5 Comparison of CNN + Transformer performance with different Max_len values\nFig. 6 Comparison of CNN + Transformer performance with different Embedding_dim values\nPage 13 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \naverage, and worst BA values are 0.888, 0.875, and 0.863, respectively; and the highest \nbest, average, and worst MCC values are 0.479, 0.470, and 0.458, respectively. Adding an \nencoder-decoder layer on this basis will cause the model performance to drop dramati -\ncally, with average BA and MCC values of 0.524 and 0.106, respectively. When one layer \nof the encoder-decoder is reduced or the number of multi-head attention mechanisms is \nreduced, the recognition performance of the model will also decrease.\nPerformance comparison with traditional deep learning models and traditional machine \nlearning models\nTo verify the recognition performance of CNN + Transformer, we compared it with four \nbasic models (RNN, LSTM, GRU and transformer) that are widely used in deep learning \nto process sequences and two traditional machine learning models (Support.\nVector Machines (SVM) [48], Random Forest [49]). At the same time, we added the \nCNN layer in CNN + Transformer to fine-tuned RNN, LSTM and GRU models, and \nthe results are shown in Table  4. Each deep learning model was weighted according to \nthe ratio of OG to non-OG. All models were tested ten times, and the average of the \nten test results was used for comparison. As shown in the table, CNN + Transformer \nFig. 7 Performance comparison of CNN + Transformer under different N-head and Num-layer combinations. \nL: transformer encoder-decoder number; H: number of heads of the self-attention mechanism\nTable 4 Model performance comparison\nBold values indicate the highest values of different evaluation indicators\nModel BA GM BM MCC Train time \n(min)\nTest time (s)\nRandom forest 0.667 0.629 0.334 0.227 4 22\nSVM 0.690 0.659 0.380 0.252 23 77\nRNN 0.517 0.512 0.034 0.245 31 123\nCNN + RNN 0.503 0.500 0.007 0.109 18 98\nLSTM 0.829 0.829 0.659 0.418 36 284\nCNN + LSTM 0.775 0.772 0.550 0.376 26 231\nGRU 0.838 0.834 0.667 0.423 33 253\nCNN + GRU 0.777 0.776 0.554 0.373 24 219\nTransformer 0.844 0.838 0.678 0.444 57 387\nCNN + Transformer 0.875 0.871 0.746 0.471 44 343\nPage 14 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \nperformed significantly better than the four basic models according to the four com -\nprehensive indicators of BA, BM, GM and MCC. The MCC value reached 0.471, which \nwas 0.027 higher than the value for of the transformer model, 0.048 higher than that \nfor GRU, 0.053 higher than that for LSTM, and 0.226 higher than that for RNN, and \n0.219 higher than that for SVM, and 0.244 higher than that for Random Forest. Com -\npared with Random Forest, SVM, RNN, LSTM, GRU, and the transformer model, the \nBA values   were increased by 0.208, 0.185 0.358, 0.046, 0.037, and 0.031; the GM values \nwere increased by 0.242, 0.212, 0.359, 0.042, 0.037, and 0.033; and the BM values were \nincreased by 0.412, 0.366, 0.712, 0.087, 0.079 and 0.068, respectively. We noticed that \nthe CNN + Transformer model performed better than the transformer model, which \nproves the importance of the convolution operation in the CNN + Transformer model. \nAmong the basic models, the transformer model showed the best performance, with a \nbalance accuracy of 0.844 and an MCC value of 0.444, which were higher the values for \nthe recurrent neural network. After adding the CNN layer, the recognition performance \nof the LSTM and GRU models for the moso bamboo orphan gene dataset decreased. \nBefore adding the CNN layer, the average BA values of LSTM and GRU were 0.829 and \n0.838, and the average MCC values were 0.418 and 0.423, respectively. After adding the \nCNN layer, the average BA values of the two models dropped to 0.775 and 0.777, and the \naverage MCC values dropped to 0.376 and 0.373, respectively. However, after the trans -\nformer was added to the CNN layer, the model’s recognition ability for the moso bam -\nboo orphan gene dataset was improved and the training and testing time of the model \nwas reduced, the balance accuracy and MCC value were increased from 0.844 and 0.444 \nto 0.875 and 0.471, respectively.\nIn terms of training time and testing time, the Transformer model has slightly higher \ntraining and testing time than the recurrent neural network model, which is caused by \nthe computational complexity of multi-head self-attention mechanism in Transformer. \nAlthough the complexity of CNN + Transformer model structure is higher than that \nof Transformer model, the training and testing time of CNN + Transformer model is \nlower than that of Transformer model. Because the one-dimensional convolution layer \nof CNN + Transforme model performs preliminary feature extraction on the input fea -\nture matrix, the size of the feature matrix is compressed, thus reducing the computa -\ntional complexity of the model. These results further prove that CNN + Transformer is \nan effective deep learning model for moso bamboo OG recognition.\nCNN + Transformer model verification\nThe genome of moso bamboo has been updated to the second edition, which includes \n50,936 protein sequences [50]. The model is tested on the dataset of second edition. \nFirstly, we obtained 1275 orphan genes (Additional file 2: Table S2) from the second edi-\ntion of moso bamboo protein sequences through BLAST sequence alignment. Then, we \ninput all the protein sequences of moso bamboo into the CNN + Transformer model, \nand the model identified 1466 orphan genes of moso bamboo.\nIn order to verify the reliability of CNN + Transformer model in identifying orphan \ngenes of moso bamboo, we compared the 1466 orphan genes with 1275 orphan genes \nwhich were obtained by BLAST method. The results showed that 1106 protein sequences \n(Additional file 3: Table S3) were identical and our method had a high coincidence with \nPage 15 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \nBLAST results. To further validate the performance of CNN + Transformer model, we \ntrained an optimal CNN + Transformer model using 70% data for training, 15% data \nfor verification and 15% data for testing in the second version of moso bamboo orphan \ngenes dataset. The test set contained 194 orphan genes identified by BLAST tools. The \nCNN + Transformer model identified 211 protein sequences as orphan genes in the test \nset, 183 of which were coincident with BLAST results. The above results indicated the \nreliability of CNN + Transformer in identifying orphan genes of moso bamboo.\nOGs functional analyses\nFunctional annotation, classification and enrichment (GO, KEGG) analysis were per -\nformed by the BGI in-house customized data mining system called Dr.Tom (http://  \nreport. bgi. com). The 1254 OGs of moso bamboo were searched against the GO data -\nbase in order to categorize standardized gene functions. Some OGs were classified \ninto “cellular process” , “metabolic process” , “catalytic activity” , “binding” , and “cell” \n(Fig.  8 (A)). In Fig.  8 (B), we performed GO enrichment analysis on OGs, functions \nsuch as “cell wall mannoprotein biosynthetic process” , “box H/ACA snoRNA 3’-end \nprocessing” , “mannose-6-phosphate isomerase activity” and “phosphoribosylformyl -\nglycinamidine cyclo-lingase activity” were enriched. According to KEGG pathway \nannotation, the KEGG pathway classification graph (Fig.  8 (C)) and enrichment graph \n(Fig. 8 (D)) are generated [51– 53], phyper function in the R project was used to calcu -\nlate P values and false discovery rates (FDRs). The 1254 OGs were divided into several \ncategories (Fig.  8 (C)). Among them, “Translation” , “Folding, sorting and degradation” , \nFig. 8 Functional classification and enrichment of OGs (Dr.Tom, BGI, China). A GO (Gene Ontology) \nclassification of OGs. The vertical axis represents the GO terms, and the horizontal axis represents the number \nof OGs. B Bubble graph for GO enrichment (the bigger bubble means the more genes enriched, and the \nincreasing depth of blue means the differences were more obvious; q-value: the adjusted p-value). C KEGG \n(Kyoto Encyclopedia of Genes and Genomes) classification of OGs. D Bubble graph for KEGG enrichment\nPage 16 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \n“Carbohydrate metabolism” and “Environmental adaptation” were the most promi -\nnent. In Fig.  8 (D), we performed KEGG enrichment analysis on OGs, pathways such \nas “Circadian rhythm-plant” , “Protein processing in endoplasmic reticulum” , and \n“Plant-pathogen interaction” were enriched.\nConclusion\nUsing the sequence alignment method to identify OG in species is time-consuming \nand laborious, so it is a great challenge to design a robust and efficient model for iden -\ntifying OG in species. In this study, we propose the sequence-based deep learning \nmodel CNN + Transformer with the aim of exploring whether deep learning shows \nbetter performance in the identification of moso bamboo OG (an unbalanced clas -\nsification problem). The model uses a CNN to capture local k-mer amino acid features \nin the protein sequence and a transformer model to capture remote features between \nk-mer amino acids. CNNs are often used to capture local features, but they show \nsome defects in effectively identifying the interdependence among long-distance \ninput data. In contrast, in the model based on the transformer neural network, the \nlong-term dependency relationships between local features are captured by introduc -\ning a multi-head self-attention mechanism.\nThe performance of CNN + Transformer was evaluated with a moso bamboo \norphan gene dataset, and it achieved very good performance according to four \ncomprehensive evaluation indexes: BA, GM, BM and MCC. Compared with four \nother models (RNN, LSTM, GRU, and transformer) that are widely used to address \nsequence problems in deep learning, the performance of CNN + Transformer was sig -\nnificantly superior, which further proved that CNN + Transformer is an effective gene \nrecognition model for moso bamboo OG. At the same time, we combined the CNN \nlayer of CNN + Transformer with RNN, LSTM, GRU and other models and made \nfine adjustments. The results showed that the recognition performance of the RNN, \nLSTM and GRU models declined to varying degrees after adding the CNN layer. The \nefficiency of the transformer model in capturing the correlation dependence between \nk-mer amino acids in the protein sequence was verified. Subsequently, we compared \nthe results of CNN + Transformer and BLAST on moso bamboo orphan gene dataset \nof the second edition, and verified that CNN + Transformer is a reliable orphan gene \nidentification model of moso bamboo.\nCNN + Transformer model was used to predict orphan genes directly from pro -\ntein sequences, which was essentially different from BLAST method. Therefore, when \nresearchers want to know whether some genes are orphan genes, CNN + Trans-\nformer can assist researchers to further confirm orphan genes as an effective tool. In \nthe future, we will explore and integrate orphan gene data of other species to further \nimprove the performance of CNN + Transformer. At the same time, we are interested \nin how to use deep learning to automatically learn features from biological data rather \nthan manually extracting features heavily based on domain knowledge.\nPage 17 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \nAbbreviations\nCNN: Convolutional neural network; ECs: Evolutionarily conserved genes; GO: Gene Ontology; GRU : Gated recurrent unit; \nKEGG: Kyoto Encyclopedia of Genes and Genomes; LSTM: Long short-term memory; OG: Orphan genes; RNN: Recurrent \nneural network; SVM: Support Vector Machines.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 022- 04702-1.\nAdditional file 1. Includes 1544 orphan genes of the first version moso bamboo.\nAdditional file 2. Includes 1275 orphan genes of the second version moso bamboo\nAdditional file 3. Includes 1106 the second edition orphan genes of moso bamboo further screened by \nCNN+Transformer model.\nAcknowledgements\nNot applicable.\nAuthor contributions\nXZ, JX and CY conceived, designed the experiments and analyzed the results. XZ, QG, XJ, LW and SL conceived, designed \nthe method and wrote the manuscript. All authors read and approved the final manuscript.\nFunding\nPublication costs are funded by Nature Science Research Project of Education Department in Anhui Province \n(KJ2020A0108).\nAvailability of data and materials\nThe datasets generated during and analysed during the current study and the source code can be downloaded from \nGithub (https:// github. com/ xuan2 333/ CNN+ Trans former).\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 Anhui Province Key Laboratory of Smart Agricultural Technology and Equipment, Anhui Agriculture University, \nHefei 230001, China. 2 College of Information and Computer Science, Anhui Agricultural University, Hefei 230001, China. \n3 Graduate School, Anhui Agricultural University, Hefei 230036, China. \nReceived: 14 September 2021   Accepted: 28 April 2022\nReferences\n 1. Tautz D, Domazet-Loso T. The evolutionary origin of orphan genes. Nat Rev Genet. 2011;12(10):692–702.\n 2. Arendsee ZW, Li L, Wurtele E. Coming of age: orphan genes in plants. Trends Plant Sci. 2014;19(11):698–708.\n 3. Wissler L, Gadau J, Simola DF, Helmkampf M, Bornberg-Bauer E. Mechanisms and dynamics of orphan gene emer-\ngence in insect genomes. Genome Biol Evol. 2013;5(2):439–55.\n 4. Campbell MA, Zhu W, Jiang N, Lin H, Ouyang S, Childs KL, Haas BJ, Hamilton JP , Buell CR. Identification and charac-\nterization of lineage-specific genes within the Poaceae. Plant Physiol. 2007;145(4):1311–22.\n 5. Graham MA, Silverstein KAT, Cannon SB, VandenBosch KA. Computational identification and characterization of \nnovel genes from legumes. Plant Physiol. 2004;135(3):1179–97.\n 6. Ma SW, Yuan Y, Tao Y, Jia HY, Ma ZQ. Identification, characterization and expression analysis of lineage-specific genes \nwithin Triticeae. Genomics. 2020;112(2):1343–50.\n 7. Yang XH, Jawdy S, Tschaplinski TJ, Tuskan GA. Genome-wide identification of lineage-specific genes in Arabidopsis, \nOryza and Populus. Genomics. 2009;93(5):473–80.\n 8. Carvunis A-R, Rolland T, Wapinski I, Calderwood MA, Yildirim MA, Simonis N, Charloteaux B, Hidalgo CA, Barbette J, \nSanthanam B, et al. Proto-genes and de novo gene birth. Nature. 2012;487(7407):370–4.\n 9. Colbourne JK, Pfrender ME, Gilbert D, Thomas WK, Tucker A, Oakley TH, Tokishita S, Aerts A, Arnold GJ, Basu MK. The \necoresponsive genome of Daphnia pulex. Science. 2011;331(6017):555–61.\n 10. Donoghue MT, Keshavaiah C, Swamidatta SH, Spillane C. Evolutionary origins of Brassicaceae specific genes in Arabi-\ndopsis thaliana. BMC Evol Biol. 2011;11(1):1–23.\nPage 18 of 19Zhang et al. BMC Bioinformatics          (2022) 23:162 \n 11. Shan X, Yang K, Xu X, Zhu C, Gao Z. Genome-wide investigation of the NAC gene family and its potential association \nwith the secondary cell wall in moso bamboo. Biomolecules. 2019;9(10):609.\n 12. Liu L, Dong D, Yun L, Li X. Investigation of moso bamboo forest under high temperature and drought disaster. World \nBamboo and Rattan. 2014;12(01):24–7.\n 13. Zhang P , Wang J, Zhang H. Measures of water management and increasing drought resistance of moso forests in \nAnji County, Zhejiang Province. World Bamboo Rattan. 2008;6:23–4.\n 14. Lin W-L, Cai B, Cheng Z-M. Identification and characterization of lineage-specific genes in Populus trichocarpa. Plant \nCell Tissue Organ Cult. 2014;116(2):217–25.\n 15. Sadat A, Jeon J, Mir AA, Kim S, Lee YH. Analysis of in planta expressed orphan genes in the rice blast fungus Magna-\nporthe oryzae. Plant Pathol J. 2014;30(4):367–74.\n 16. Xu Y, Wu G, Hao B, Chen L, Deng X, Xu Q. Identification, characterization and expression analysis of lineage-specific \ngenes within sweet orange (Citrus sinensis). BMC Genom. 2015;16(1):1–10.\n 17. Zhang HP , Yin TM. Advances in lineage-specific genes. Yi Chuan = Hereditas. 2015;37(6):544–53.\n 18. Neme R, Tautz D. Phylogenetic patterns of emergence of new genes support a model of frequent de novoevolution. \nBMC Genomics. 2013;14(1):1–13.\n 19. Lin Y, Afshar S, Rajadhyaksha AM, Potash JB, Han S. A machine learning approach to predicting autism risk genes: \nvalidation of known genes and discovery of new candidates. Front Genet. 2020;11:1051.\n 20. Elbasir A, Moovarkumudalvan B, Kunji K, Kolatkar PR, Mall R, Bensmail H. DeepCrystal: a deep learning framework for \nsequence-based protein crystallization prediction. Bioinformatics. 2019;35(13):2216–25.\n 21. Liu TYA, Zhu H, Chen H, Arevalo JF, Hui FK, Yi PH, Wei J, Unberath M, Correa ZM. Gene expression profile prediction \nin uveal melanoma using deep learning: a pilot study for the development of an alternative survival prediction tool. \nOphthalmol Retina. 2020;4(12):1213–5.\n 22. Rong Y, Xiang D, Zhu W, Shi F, Gao E, Fan Z, Chen X. Deriving external forces via convolutional neural networks for \nbiomedical image segmentation. Biomed Opt Express. 2019;10(8):3800–14.\n 23. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.\n 24. Zheng S, Lu J, Zhao H, Zhu X, Luo Z, Wang Y, Fu Y, Feng J, Xiang T. Rethinking semantic segmentation from a \nsequence-to-sequence perspective with transformers. In: Proceedings of the IEEE/CVF conference on computer \nvision and pattern recognition. 2021. P .6881–90.\n 25. Zou C, Wang B, Hu Y, Liu J, Wu Q, Zhao Y, Li B, Zhang C, Zhang C, Wei Y. End-to-end human object interaction detec-\ntion with hoi transformer. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. \n2021. p. 11825–34.\n 26. Altschul SF, Gish W, Miller W, Myers EW, Lipman DJ. Basic local alignment search tool. J Mol Biol. 1990;215(3):403–10.\n 27. Chen K, Tian ZH, Chen P , He H, Jiang FT, Long CA. Genome-wide identification, characterization and expression \nanalysis of lineage-specific genes within Hanseniaspora yeasts. FEMS Microbiol Lett. 2020;367(11):fnaa077.\n 28. Ma DN, Ding QS, Guo ZJ, Zhao ZZ, Wei LF, Li YY, Song SW, Zheng HL. Identification, characterization and expres-\nsion analysis of lineage-specific genes within mangrove species Aegiceras corniculatum. Mol Genet Genom. \n2021;296(6):1235–47.\n 29. Zhao ZZ, Ma DN. Genome-wide identification, characterization and function analysis of lineage-specific genes in \nthe tea plant Camellia sinensis. Front Genet. 2021;12(13):770570–770570.\n 30. Zhao H, Peng Z, Fei B, Li L, Hu T, Gao Z, Jiang Z. BambooGDB: a bamboo genome database with functional annota-\ntion and an analysis platform. Database - J Biol Databases Curation. 2014;2014:bau006.\n 31. Goodstein DM, Shu S, Howson R, Neupane R, Hayes RD, Fazo J, Mitros T, Dirks W, Hellsten U, Putnam N, et al. Phyto-\nzome: a comparative platform for green plant genomics. Nucleic Acids Res. 2012;40(D1):D1178–86.\n 32. Chica C, Louis A, Roest Crollius H, Colot V, Roudier F. Comparative epigenomics in the Brassicaceae reveals two \nevolutionarily conserved modes of PRC2-mediated gene regulation. Genome Biol. 2017;18(1):1–15.\n 33. Guo L, Wang SF, Li MY, Cao ZC. Accurate classification of membrane protein types based on sequence and evolu-\ntionary information using deep learning. BMC Bioinform. 2019;20(25):1–17.\n 34. Li H, Gong XJ, Yu H, Zhou C. Deep neural network based predictions of protein interactions using primary \nsequences. Molecules. 2018;23(8):1923.\n 35. Min X, Zeng W, Chen N, Chen T, Jiang R. Chromatin accessibility prediction via convolutional long short-term \nmemory networks with k-mer embedding. Bioinformatics. 2017;33(14):I92–101.\n 36. Vang YS, Xie X. HLA class I binding prediction via convolutional neural networks. Bioinformatics. \n2017;33(17):2658–65.\n 37. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Adv \nNeural Inf Process Syst. 2017;30:1–11.\n 38. Rush AM. The annotated transformer. In: Proceedings of workshop for NLP open source software (NLP-OSS). 2018. p. \n52–60.\n 39. Ba JL, Kiros JR, Hinton GE. Layer normalization. 2016. arXiv preprint, arXiv:1607.06450.\n 40. Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural networks \nfrom overfitting. J Mach Learn Res. 2014;15:1929–58.\n 41. Lin T, Wang Y, Liu X, Qiu X. A survey of transformers. 2021. arXiv preprint, arXiv:2106.04554.\n 42. Ji LP , Pu XR, Qu H, Liu GS. One-dimensional pairwise CNN for the global alignment of two DNA sequences. Neuro-\ncomputing. 2015;149:505–14.\n 43. Zeng HY, Edwards MD, Liu G, Gifford DK. Convolutional neural network architectures for predicting DNA-protein \nbinding. Bioinformatics. 2016;32(12):121–7.\n 44. Zhou Y, Zhang M, Zhu J, Zheng R, Wu Q. A randomized block-coordinate adam online learning optimization algo-\nrithm. Neural Comput Appl. 2020;32(16):12671–84.\n 45. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, Ainips J, Desmaison \nA. PyTorch: an imperative style, high-performance deep learning library. 2019. arXiv: 1912. 01703.\n 46. Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in \nbinary classification evaluation. BMC Genom. 2020;21(1):1–13.\nPage 19 of 19\nZhang et al. BMC Bioinformatics          (2022) 23:162 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 47. Luque A, Carrasco A, Martin A. de las Heras A: The impact of class imbalance in classification performance metrics \nbased on the binary confusion matrix. Pattern Recognit. 2019;91:216–31.\n 48. Zhu Y, Shen X, Pan W. Network-based support vector machine for classification of microarray samples. BMC Bioin-\nform. 2009;10(1):1–11.\n 49. Pang H, Lin A, Holford M, Enerson BE, Lu B, Lawton MP , Floyd E, Zhao H. Pathway analysis using random forests clas-\nsification and regression. Bioinformatics. 2006;22(16):2028–36.\n 50. Zhao H, Gao Z, Wang L, Wang J, Wang S, Fei B, Chen C, Shi C, Liu X, Zhang H. Chromosome-level reference genome \nand alternative splicing atlas of moso bamboo (Phyllostachys edulis). Gigascience. 2018;7(10):giy115.\n 51. Kanehisa M, Goto S. KEGG: kyoto encyclopedia of genes and genomes. Nucleic Acids Res. 2000;28(1):27–30.\n 52. Kanehisa M. Toward understanding the origin and evolution of cellular organisms. Protein Sci. 2019;28(11):1947–51.\n 53. Kanehisa M, Furumichi M, Sato Y, Ishiguro-Watanabe M, Tanabe M. KEGG: integrating viruses and cellular organisms. \nNucleic Acids Res. 2021;49(D1):D545–51.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}