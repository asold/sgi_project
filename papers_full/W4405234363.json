{
  "title": "Large Language Models in Worldwide Medical Exams: Platform Development and Comprehensive Analysis",
  "url": "https://openalex.org/W4405234363",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2153947353",
      "name": "Hui Zong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161313166",
      "name": "Rongrong Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221937891",
      "name": "Jiaxue Cha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102016149",
      "name": "Jiao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2516711024",
      "name": "Erman Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137131825",
      "name": "Jiakun Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097872777",
      "name": "Yi Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1492746850",
      "name": "Chi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2020096273",
      "name": "Wei-Zhe Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2189545922",
      "name": "Bairong Shen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4367595583",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4380423243",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4395067526",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4394760744",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4394956892",
    "https://openalex.org/W4386807359",
    "https://openalex.org/W2514199499",
    "https://openalex.org/W4229336273",
    "https://openalex.org/W4401250476",
    "https://openalex.org/W4394616404",
    "https://openalex.org/W4400475545",
    "https://openalex.org/W4391811872",
    "https://openalex.org/W4377041488",
    "https://openalex.org/W4399701665",
    "https://openalex.org/W4392755271",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4390617165",
    "https://openalex.org/W4386407038",
    "https://openalex.org/W4392702964",
    "https://openalex.org/W4385476456",
    "https://openalex.org/W4389795202",
    "https://openalex.org/W4391574955",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4389992676",
    "https://openalex.org/W4376866715",
    "https://openalex.org/W4395052272",
    "https://openalex.org/W4391225179",
    "https://openalex.org/W4399162090",
    "https://openalex.org/W4400392484",
    "https://openalex.org/W4397003497",
    "https://openalex.org/W4399387113",
    "https://openalex.org/W4390546602",
    "https://openalex.org/W4393318738",
    "https://openalex.org/W4400111281"
  ],
  "abstract": "Background Large language models (LLMs) are increasingly integrated into medical education, with transformative potential for learning and assessment. However, their performance across diverse medical exams globally has remained underexplored. Objective This study aims to introduce MedExamLLM, a comprehensive platform designed to systematically evaluate the performance of LLMs on medical exams worldwide. Specifically, the platform seeks to (1) compile and curate performance data for diverse LLMs on worldwide medical exams; (2) analyze trends and disparities in LLM capabilities across geographic regions, languages, and contexts; and (3) provide a resource for researchers, educators, and developers to explore and advance the integration of artificial intelligence in medical education. Methods A systematic search was conducted on April 25, 2024, in the PubMed database to identify relevant publications. Inclusion criteria encompassed peer-reviewed, English-language, original research articles that evaluated at least one LLM on medical exams. Exclusion criteria included review articles, non-English publications, preprints, and studies without relevant data on LLM performance. The screening process for candidate publications was independently conducted by 2 researchers to ensure accuracy and reliability. Data, including exam information, data process information, model performance, data availability, and references, were manually curated, standardized, and organized. These curated data were integrated into the MedExamLLM platform, enabling its functionality to visualize and analyze LLM performance across geographic, linguistic, and exam characteristics. The web platform was developed with a focus on accessibility, interactivity, and scalability to support continuous data updates and user engagement. Results A total of 193 articles were included for final analysis. MedExamLLM comprised information for 16 LLMs on 198 medical exams conducted in 28 countries across 15 languages from the year 2009 to the year 2023. The United States accounted for the highest number of medical exams and related publications, with English being the dominant language used in these exams. The Generative Pretrained Transformer (GPT) series models, especially GPT-4, demonstrated superior performance, achieving pass rates significantly higher than other LLMs. The analysis revealed significant variability in the capabilities of LLMs across different geographic and linguistic contexts. Conclusions MedExamLLM is an open-source, freely accessible, and publicly available online platform providing comprehensive performance evaluation information and evidence knowledge about LLMs on medical exams around the world. The MedExamLLM platform serves as a valuable resource for educators, researchers, and developers in the fields of clinical medicine and artificial intelligence. By synthesizing evidence on LLM capabilities, the platform provides valuable insights to support the integration of artificial intelligence into medical education. Limitations include potential biases in the data source and the exclusion of non-English literature. Future research should address these gaps and explore methods to enhance LLM performance in diverse contexts.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.8935868144035339
    },
    {
      "name": "Computer science",
      "score": 0.4718344807624817
    },
    {
      "name": "Data science",
      "score": 0.4164436459541321
    },
    {
      "name": "World Wide Web",
      "score": 0.3639248013496399
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24185976",
      "name": "Sichuan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I116953780",
      "name": "Tongji University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2802734952",
      "name": "First Affiliated Hospital of Xinjiang Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I154093214",
      "name": "Xinjiang Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210089761",
      "name": "West China Hospital of Sichuan University",
      "country": "CN"
    }
  ],
  "cited_by": 34
}