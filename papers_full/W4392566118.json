{
  "title": "Spectraformer: deep learning model for grain spectral qualitative analysis based on transformer structure",
  "url": "https://openalex.org/W4392566118",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2098399583",
      "name": "Zhuo Chen",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2553635177",
      "name": "Rigui Zhou",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2167518597",
      "name": "Pengju Ren",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2098399583",
      "name": "Zhuo Chen",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2553635177",
      "name": "Rigui Zhou",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A2167518597",
      "name": "Pengju Ren",
      "affiliations": [
        "Shanghai Maritime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3200883231",
    "https://openalex.org/W2057561052",
    "https://openalex.org/W1779377117",
    "https://openalex.org/W3127323829",
    "https://openalex.org/W2591465214",
    "https://openalex.org/W2677169684",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3201550658",
    "https://openalex.org/W3036476034",
    "https://openalex.org/W2564339002",
    "https://openalex.org/W2909516836",
    "https://openalex.org/W3152858711",
    "https://openalex.org/W2883273084",
    "https://openalex.org/W4280527943",
    "https://openalex.org/W4299816210",
    "https://openalex.org/W4297734170",
    "https://openalex.org/W2792058264",
    "https://openalex.org/W3137060398",
    "https://openalex.org/W3094153837",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3207373390",
    "https://openalex.org/W3215885522",
    "https://openalex.org/W1988975411",
    "https://openalex.org/W3132214797",
    "https://openalex.org/W3206420289",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4302374119",
    "https://openalex.org/W2612690371"
  ],
  "abstract": "This study used portable near-infrared spectroscopy and various preprocessing techniques to explore universal methods. The spectraformer model showed superior performance among the compared machine learning models.",
  "full_text": "Spectraformer: deep learning model for grain\nspectral qualitative analysis based on transformer\nstructure\nZhuo Chen,ab Rigui Zhou *ab and Pengju Renab\nThis study delves into the use of compact near-infraredspectroscopy instruments for distinguishing between\ndiﬀerent varieties of barley, chickpeas, and sorghum, addressing a vital need in agriculture for precise crop\nvariety identiﬁcation. This identiﬁcation is crucial for optimizing crop performance in diverse environmental\nconditions and enhancing food security and agricultural productivity. We also explore the potential application\nof transformer models in near-infrared spectroscopy and conduct an in-depth evaluation of the impact of\ndata preprocessing and machine learning algorithms on variety classiﬁcation. In our proposed spectraformer\nmulti-classiﬁcation model, we successfully diﬀerentiated 24 barley varieties, 19 chickpea varieties, and ten\nsorghum varieties, with the highest accuracy rates reaching 85%, 95%, and 86%, respectively. These results\ndemonstrate that small near-infrared spectroscopy instruments are cost-eﬀective and eﬃcient tools with the\npotential to advance research in various identiﬁcation methods, but also underscore the value of transformer\nmodels in near-infrared spectroscopy classiﬁc a t i o n .F u r t h e r m o r e ,w ed e l v ei n t ot h ed i s c u s s i o no ft h e\ninﬂuence of data preprocessing on the performanceof deep learning models compared to traditional\nmachine learning models, providing valuable insights for future research in thisﬁeld.\n1 Introduction\nGrains, a fundamental source of human sustenance, are pivotal\nin agriculture and food security. This study explores advanced\ntechniques in grain classi cation and quality assessment,\nwhich are crucial for enhancing agricultural productivity and\nensuring food quality. While various grain types di ﬀer in\nmorphology, chemical composition, and application, precise\nclassication and assessment are key for optimizing production\nand supply chain eﬃciency. However, conventional methods\nare oen time-consuming and complex, highlighting the need\nfor improved approaches.\nWe focus on three grains: barley, sorghum, and chickpeas.\nEach presents unique challenges in classication due to their\nvisual similarities and diverse applications.\nBarley, used in food, animal feed, and beer production,\nrequires precise identication methods due to its visual simi-\nlarity among varieties. While rich in starch, protein, and dietary\nber, its classi cation remains a time-intensive process,\ndependent on expert evaluation.\nChickpeas, high in protein and benecial for soil health, face\nincreasing challenges in variety diﬀerentiation due to growing\nseed similarities.\n1\nSorghum, versatile in food and biofuel production, necessitates\naccurate variety identication, particularly in livestock farming.2\nExisting grain classication methods exhibit certain draw-\nbacks. Firstly, many grain varieties bear an uncanny resem-\nblance, confounding visual diﬀerentiation and thus fostering\nambiguity and classication errors.\nSecondly, prevailing classication methods oen hinge on\nvisual appraisal, rendering results susceptible to the evaluator's\nsubjective judgment and experiential bias. Such subjectivity can\nengender disparate classication outcomes, diminishing the\nclassication process's reliability and precision.\nMoreover, the unique attributes of certain varieties may only\nbecome apparent in high-resolution images or under substan-\ntial magnication, amplifying the complexity and duration of\nthe classication endeavor. These deciencies underscore the\ndiﬃculties inherent in current grain classication methodolo-\ngies when confronted with varietal diversity and the imperative\nfor precise identication. Consequently, the quest for more\naccurate and objective classi cation technologies becomes\ncrucial to enhance eﬃciency and accuracy.\nNear-infrared (NIR) spectroscopy\n3 is a non-destructive and\nrapid method for analyzing sample components, o ﬀering\na simpler alternative to traditional chemical analysis by elimi-\nnating complex preparation like extraction or dilution. The NIR\nspectral region, typically spanning 780 to 2526 nm, aligns with the\nabsorption of hydrogen-containing groups (O–H, N–H, C–H) in\norganic molecules. By scanning this region, NIR spectroscopy\ngathers characteristic information about these organic molecules.\naSchool of Information Engineering, Shanghai Maritime University, Shanghai, 201306,\nChina. E-mail: rgzhou@shmtu.edu.cn\nbResearch Center of Intelligent Information Processing and Quantum Intelligent\nComputing, Shanghai, 201306, China\nCite this:RSC Adv.,2 0 2 4 ,14,8 0 5 3\nReceived 19th November 2023\nAccepted 8th February 2024\nDOI: 10.1039/d3ra07708j\nrsc.li/rsc-advances\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8053\nRSC Advances\nPAPER\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nA near-infrared spectrometer projects NIR light onto\na sample, with diﬀerent molecules absorbing light variably. NIR\nspectroscopy is non-destructive, e ﬃcient, and eco-friendly,\noﬀering speed, accuracy, and cost-eﬀectiveness without chem-\nical reagents. Traditional spectrometers, bulky and limited to\nlabs, restricted NIR use in agriculture,\n4 food safety,5 and envi-\nronmental monitoring,6 particularly for real-time analysis.7,8\nPortable spectrometers, a notable advancement, allow for on-\nsite, real-time NIR analysis but have a narrower spectral\nrange, necessitating new methods for speci c applications.\nConcurrently, advancements in computing and the internet\nhave led to data abundance, fueling deep learning's expansion\ninto variouselds, including image and speech processing and\nnatural language understanding.\nAs computer technology and computational capabilities\nhave rapidly advanced and with the widespread adoption of the\ninternet and other information technologies, large volumes of\ndata have become easily accessible and storable. This data\nabundance has provided ample training samples for deep\nlearning, which has found extensive applications across various\ndomains, including image processing, speech recognition, and\nnatural language processing.\nIn contrast to traditional machine learning approaches, deep\nlearning algorithms can learn features directly from raw data\nthrough multi-layer automatic neural networks, eliminating the\nneed for manual feature engineering. This simpli es the\ncomplexity associated with feature engineering and enables\nthese models to capture intricate nonlinear relationships and\npatterns within the data.\nIn agriculture, the application of deep learning, particularly\nConvolutional Neural Networks (CNNs),\n9 in NIR spectroscopy\nhas been demonstrated in recent studies. For instance, Yang\net al.\n10 introduced the“TeaNet” method for classifying 50 black\nand green tea brands using NIR spectroscopy data. Their\napproach involved preprocessing the NIR spectroscopy data,\ntransforming it into pseudo-images, and feeding it into a four-\nlayer convolutional neural network, achieving an impressive\nclassication accuracy of 99.2%. Similarly, Rong et al.\n11\nproposed a seven-layer convolutional neural network model for\ncategorizing ve types of peaches based on VIS-NIR spectros-\ncopy. Their model achieved a validation dataset accuracy of\n100% and a test dataset accuracy of 91.7%. These examples\nhighlight the increasing prevalence and success of CNNs in NIR\napplications in recent years.\n12–15\nIn the context of spectroscopic data, it is essential to recog-\nnize its inherent sequential nature, which signicantly inu-\nences the predictive accuracy of models. Spectroscopic data\noen exhibits the repetition of spectral features at various\npositions, carrying similar information. However, in the feature\nextraction stage, CNNs typically excel at capturing local\nfeatures.\n16 Still, they may need to pay more attention to global\npositional information, potentially leading to a failure in fully\ncapturing the correlations between wavelengths. Consequently,\ndespite identical spectral features at diﬀerent positions, tradi-\ntional CNNs struggle to establish connections among them,\nresulting in information loss.\nTo address this challenge, introducing the transformer\narchitecture has proven to be a highly eﬀective strategy.\n17 The\ntransformer's attention18,19 mechanism assigns varying weights\nto features at diﬀerent positions, enabling the model to priori-\ntize crucial wavelength positions eﬀectively. By learning and\nincorporating correlations between positions, the attention\nmechanism facilitates capturing sequential features inherent in\nspectroscopic data.\n20–22 This signicantly enhances the model's\ngrasp of the data's sequential nature, improving predictive\naccuracy.\nFurthermore, the adaptive nature of the attention mecha-\nnism allows it to adjust weights dynamically, enabling the\nmodel to accommodate varying levels of information at\ndiﬀerent positions. This adaptability proves particularly\nadvantageous when dealing with noise or variations in spec-\ntroscopic data, as it enhances the model's robustness and\nreduces the impact of noise on predictive performance.\nWe aim to unlock the potential of this fast and non-invasive\nmethod for rapid and precise classication and identication of\nthese grain varieties. In the case of barley, we employed data\nfrom 24 diﬀerent varieties, encompassing ten distinct sorghum\nvarieties, and referenced data from 19 chickpea varieties from\nexisting research.\n23 It's crucial to underscore that each grain\ntype possesses its distinct spectral characteristics, with these\ntraits subject to variation based on diﬀerences in varieties,\ngrowth conditions, and chemical compositions.\nThe primary objective of this study is to utilize a portable\nnear-infrared spectrometer to identify barley, chickpea, and\nsorghum varieties in Ethiopia. Our secondary aim centers on\nexploring the potential of constructing a high-performance\ngrain species recognition model by integrating near-infrared\nspectroscopy with transformer-based deep learning tech-\nniques. The third objective involves a comprehensive exami-\nnation of the role of attention mechanisms in handling near-\ninfrared spectroscopy data. Lastly, we undertake a compara-\ntive evaluation of diﬀerent preprocessing techniques to identify\nthe most suitable algorithm.\n2 Materials and methods\n2.1 Sample preparation\nThe collection and preparation of grain samples were con-\nducted in Ethiopia by the Ethiopian Institute of Agricultural\nResearch (EIAR) in June 2017.\n23 We utilized datasets for barley,\nchickpeas, and sorghum.† A total of 50 samples were collected\nfor each variety, resulting in 24 barley varieties with a total of\n1200 barley samples, 19 chickpea varieties with a total of 950\nchickpea samples, and 10 sorghum varieties with a total of 500\nsorghum samples. All these grains were produced in the same\nyear to eliminate the inuence of seed age.\n2.2 Portable near infrared spectrometer\nThe dataset was obtained using the SCIO Consumer Edition,\na spectrometer designed by Consumer Physics for everyday use.\n† https://github.com/zzd119/cz-data\n8054 | RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nIt functions through a smartphone application and requires an\ninternet connection to upload spectral data to a remote server.\nThis device eﬀectively covers a wavelength range from 740 to\n1070 nanometers, including 331 unique variables.\n2.3 Spectral data preprocessing and data analysis method\nSpectral data preprocessing is an essential step in spectral\nanalysis, focusing on data quality enhancement through\na series of sophisticated methods.24–26 This process commences\nwith the optimization of the data's initial state, involving noise\nreduction and baseline adjustment. Subsequently, standardi-\nzation is employed to ensure the data's uniformity and\ncomparability, which is critical for the accuracy of further\nanalyses and model development. In our methodology, we\nemployed a diverse array of preprocessing techniques to rene\nraw spectral data.\nWe employed the Savitzky–Golay smoothing technique (S) to\neﬀectively reduce random noise and achieve smoother spectral\ncurves. Additionally, the AirPLS baseline correction method (A)\nwas applied, which signicantly minimized the inuence of\nnon-specic signals and improved the baseline quality of our\ndata. To eﬀectively counteract and eradicate any negative values\nfound in our dataset, we deployed a targeted technique specif-\nically designed for the removal of negative values (0). We stan-\ndardized the data using the min–max normalization method\n(M), ensuring uniformity across a common scale and over-\ncoming the challenges associated with varying units or ranges.\nThese preprocessing techniques were applied in various\ncombinations, leading to a suite of methods: Savitzky–Golay\nsmoothing (S), Min–Max normalization (M), Savitzky–Golay +\nAirPLS (SA), Savitzky–Golay + negativity removal (S0), negativity\nremoval + Min-Max Normalization (0M), Savitzky–Golay + Min–\nMax normalization (SM), Savitzky–Golay + AirPLS + negativity\nremoval (SA0), Savitzky–Golay + negativity removal + Min–Max\nnormalization (S0M), Savitzky –Golay + AirPLS + Min –Max\nnormalization (SAM), and Savitzky–Golay + AirPLS + negativity\nremoval + Min–Max normalization (SA0M).\nPost-implementation of these methods, we conducted\na comparative performance analysis ofve models, including\nthe model proposed in this study, to develop a predictive model\ncapable of identifying various grain classes. The evaluated\nmodels comprised a Support Vector Machine with a linear\nkernel (SVM (linear)),\n27 a Support Vector Machine with an RBF\nkernel (SVM (RBF)), a Random Forest algorithm (RF),28 a CNN,\nand our newly proposed spectraformer model. These models\nwere assessed for their e ﬃcacy in analyzing preprocessed\nspectral data.\nTo develop a model boasting superior generalization capa-\nbilities, we implemented a 7 : 3 ratio for training-to-testing data.\nMoreover, we incorporated a 5-fold cross-validation technique\nto enhance the model's robustness for more rigorous verica-\ntion. This study leveraged a spectrum of machine learning\nalgorithms, encompassing deep learning techniques, to delve\ninto the relationship between spectral data and barley classi-\ncations. Fig. 1 meticulously delineates the data analysis work-\now adopted in our research.\n2.3.1 Self-built spectraformer. The CNN is a versatile deep\nlearning algorithm, particularly eﬀective for extracting features\nfrom one-dimensional sequential data. It excels in signal\nsmoothing, noise reduction, and removing unnecessaryuc-\ntuations. Employing appropriate convolutional kernels can\nFig. 1 Flow chart of the data processing used.\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8055\nPaper RSC Advances\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\neﬀectively lter out high-frequency noise from signals, enabling\na more in-depth analysis of trends and patterns within the data.\nHowever, it's worth noting that with only 331 data points, which\nis relatively short in spectroscopy, it's crucial to strike a balance\nand avoid overly complex model structures to ensure accuracy.\nThe challenge lies in precisely analyzing these short yet\ndensely informative data sequences in spectroscopy. While\nCNNs excel in feature extraction and smoothing for one-\ndimensional data, they may face limitations when dealing\nwith longer data sequences or the need to consider long-range\ndependencies. This limitation motivates the introduction of\nthe transformer model.\nTransformer models have already found wide-ranging\napplications in image processing and natural language pro-\ncessing domains.\n18,29–31 Therefore, this paper endeavors to\nintroduce transformer modules into theeld of spectroscopy.\nThis eﬀort seeks to leverage the powerful capabilities of trans-\nformers to enhance the eﬃciency of spectral data analysis and\nprocessing. By incorporating this advanced deep learning\ntechnology into spectroscopy research, the aim is to provide\nscientists with more accurate and eﬃcient tools, ultimately\nfostering further development and innovation within the spec-\ntroscopy eld. Fig. 2 visually illustrates the schematic structure\nof the transformer model, emphasizing its attention mecha-\nnism when processing sequential data.\nUnlike traditional convolution operations that rely onxed-\nsize kernels, transformer models are based on the core concept\nof using attention mechanisms to process sequential data. This\nfundamental diﬀerence makes transformers highly eﬀective in\ncapturing long-range dependencies and easily adaptable to\nsequences of varying lengths.\nFig. 2 presents a transformer model's attention mechanism,\na sophisticated approach within the eld of deep learning\ndesigned to dynamically weigh the signicance of diﬀerent\nparts of input data. At the base of the diagram, we see the input\nembeddings (X\n1, X2, X3), which could represent segments of\na spectroscopic signal. These inputs are then transformed by\nlearnable weights (W\nx) into the query (Q), key (K), and value (V)\nvectors, essential components of the attention mechanism. The\nquery vectors are tasked with identifying relevant parts of the\ndata, the key vectors match these parts, and the value vectors\ncarry the actual content to be focused upon.\nAttention scores (a\n11) are computed through a scaled dot-\nproduct of the Q and K vectors, followed by a somax normal-\nization to ascertain the focus level for diﬀerent input segments.\nThese scores are pivotal as they dictate the weighting of the\nvalue vectors, culminating in a weighted sum output. This\noutput, depicted at the top of the diagram as a combination of\nweighted values (a\n11V1, a12V2, a13V3), is the rened result of the\nattention process.\nBy combining the strengths of both CNN and transformer,\nwe can achieve a powerful synergy: CNN for local feature\nextraction and signal smoothing and transformer for global\ninformation capture and modeling of long-range dependencies.\nThis combination can signicantly enhance the eﬃciency of\nprocessing spectral data and improve the accuracy of analysis,\nproviding scientists in the eld of spectroscopy with more\nrobust tools to drive innovation and progress. Therefore, this\npaper introduces the spectraformer model, which leverages the\ncomplementary capabilities of both architectures to address the\nspecic challenges involved in analyzing and processing spec-\ntroscopic data.\n2.3.2 Traditional machine learning methods. Traditional\nmachine learning methods like SVM and RF have played\na pivotal role in near-infrared spectroscopy for a considerable\ntime, providing robust tools for predicting sample properties\nand ensuring quality control.\n32–35\nThe strengths of traditional machine learning methods in\nnear-infrared spectroscopy analysis are readily apparent. SVM,\nfor instance, is well-suited for classifying and regressing high-\ndimensional data, making it a valuable asset in handling the\nintricacies of spectral data. On the other hand, random forests\nexcel in dealing with complex spectral datasets, allowing for\neﬀective analysis. However, it's worth noting that these\nmethods encounter certain challenges, particularly in feature\nextraction and intricate pattern recognition.\nThe SVM is a powerful algorithm based on the maximum\nmargin principle, aiming to establish an optimal relationship\nbetween observed values. Within the SVM framework, the key\ncomponents are the support vectors, which play a critical role in\ndetermining the model's weights and dening the decision\nboundary.\nTo construct a maximum margin model and identify the\noptimal hyperplane, SVM utilizes a kernel function to gauge the\nsimilarity of data points. During the training process, observed\nvalues are categorized into three groups: those lying outside the\nmargin, those violating the margin, and those residing directly\non the margin. The position and orientation of the separating\nhyperplane are determined by the data points that rest on the\nmargin, and these points are referred to as“support vectors”.\nIn cases where the data is not linearly separable in the\noriginal feature space, SVM employs a technique known as\nkernel trick. This involves mapping the data to a higher-\ndimensional feature space that becomes linearly separable. In\nFig. 2 The architecture of attention.\n8056 | RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nthis scenario, the margin assumption may be relaxed, allowing\nsome observed values to violate the margin condition.\nIt's crucial to emphasize that the choice of the kernel func-\ntion substantially impacts the SVM model's performance.\nDiﬀerent kernel functions can transform the data in various\nways, and selecting the appropriate kernel function is a critical\nparameter in SVM model tuning.\nRF is a machine learning method that leverages an ensemble\nof decision trees generated from a set of induction rules.\nDecision trees are formed from random subsets of variables and\nobserved values. At each node (or decision rule), the attribute\nthat minimizes the average class entropy is chosen, considering\nthe weighted number of observations entering each branch.\nEach tree's leaf node (or terminal node) represents a rule with\nconditions formed by concatenating all edge labels along the\ndecision path. A signicant characteristic of decision trees is\ntheir ability to simultaneously optimize the example distribu-\ntion for all successor nodes within a node.\nIn an RF model, each tree is constructed based on boot-\nstrapped samples from the dataset. The nal classication\nresult is determined through majority voting among the\ngenerated trees. This implies that each tree provides a classi-\ncation prediction for the observed outcome, and the ultimate\nclassication result is determined by the majority vote. This\napproach eﬀectively mitigates the risk of over tting while\nenhancing the model's stability and accuracy.\n2.3.3 So ware tools.A laboratory computer is employed for\ndata analysis and model development, running the Ubuntu\n22.04.3 operating system. The hardware conguration includes\nan Intel Xeon Gold 6133 CPU, 128 GB of RAM, and an NVIDIA\nGeForce RTX 4090 GPU. In the process of building and training\ndeep learning models, we integrate the open-source machine\nlearning framework PyTorch, which can be found at (https://\npytorch.org/). Our primary programming language for\nmodeling and analysis is Python, specically version 3.8.13.\n3 Results and discussion\nWe employed NIR spectroscopy data as our study's primary\ninput, utilizing various grain identication models. Our inves-\ntigation evaluated the impact of the transformer module's\nplacement within these models on their performance.\nFurthermore, we delved into the role of the spectraformer and\nthe transformer module in the context of near-infrared\nspectroscopy.\nIn addition to model architecture considerations, we applied\nvarious data preprocessing techniques and explored diﬀerent\ncombinations of these methods. To assess the eﬃcacy of our\nproposed approach, we conducted a comparative analysis\ninvolving ve distinct models. Our primary objective was to\nascertain whether the model introduced in this paper out-\nperformed other existing models.\nWe implemented a hyperparameter search algorithm to\ndetermine the essential hyperparameters for the SVM (linear),\nSVM (RBF), and RF algorithms precisely. This methodology\nrevealed that the optimal conguration involves setting the\nhyperparameter C to 0.1 for SVM (linear) and to 1 for SVM (RBF)\nto attain the most favorable outcomes. For the RF algorithm,\nsetting n_estimators to 200 emerged as the most e ﬃcient\nconguration.\n3.1 Parameter setting and adjustment\nThe progress made in near-infrared spectroscopy classication\nnecessitates a deeper understanding of how the components\nand choices in model construction impact classi cation\nperformance. A series of experiments were conducted to inves-\ntigate the eﬀectiveness of the components used in the model\npresented in this paper.\nThe model architecture employed in this study, as illustrated\nin Fig. 3, comprises critical components tailored for near-\ninfrared spectroscopy classication tasks. These components\ninclude a transformer module, four layers of three-kernel\nconvolution, four single-kernel convolution, and two fully con-\nnected layers. Each convolutional block consists of one three-\nkernel convolution and one single-kernel convolution, fol-\nlowed by Batch Normalization and ReLU activation, regardless\nof whether it is a three-kernel or single-kernel convolution. A\ntransformer block is introduced aer the initial convolutional\nblock, primarily composed of attention mechanisms and multi-\nlayer perceptrons.\nIn our model, the initial convolutional layer is congured\nwith 16 channels, and each subsequent layer progressively\ndoubles the channel count from its predecessor. We have\nstandardized the stride across all convolutional layers at 2 while\nsetting the padding value to 1 to facilitate optimal data pro-\ncessing. For the self-attention layer, we tested various attention\nheads and ultimately adopted a dual-head attention mecha-\nnism. This specic design choice aims to improve the model's\nability to process and interpret complex information, thereby\nsignicantly enhancing the overall model eﬃcacy.\nAer completing all feature extraction and fusion processes,\nall convolutional features areattened and processed through\ntwo fully connected layers to yield classication results. The\ncross-entropy loss function was chosen as the primary loss\nfunction for the model to ensure superior classication results.\nAdditionally, stochastic gradient descent (SGD) with a learning\nFig. 3 Diagram showing the structure of the spectraformer.\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8057\nPaper RSC Advances\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nrate 0.0001 was employed to guide the gradient descent process,\nowing to its ease of implementation and computational eﬃ-\nciency. To guarantee thorough training of the model while\nsafeguarding against over tting, we established a training\ntermination criterion at 200 epochs. Consequently, the training\nregimen will involve 200 complete cycles through the training\ndataset, with the process halting at this point regardless of\npotential performance gains. This protocol ensures uniformity\nand comparability in the training of the model and judiciously\nmanages the expenditure of computational resources.\nThe experimental design for this study involved systemati-\ncally adding or removing diﬀerent components to evaluate their\ncontributions to the model's performance and their relevance to\nvarious tasks.\n3.1.1 Transformer layer. As depicted in Fig. 4, the experi-\nmental results highlight the accuracy of various components in\nthe model's performance. Notably, the test accuracy of the\nspectraformer model outperforms that of a model solely\nutilizing CNN. This observation underscores the notion that\nintegrating a transformer module into a CNN model can\nsignicantly enhance the overall model performance. This\nnding suggests the potential applicability of transformers in\nspectroscopy for comprehensive global feature extraction and\nfusion purposes.\nIn our exploration of the convolutional neural network\nrelative to the spectraformer model, we endeavored to oﬀset the\nreduction in model complexity caused by excluding the trans-\nformer module. This was attempted by integrating an extra CNN\nmodule, aiming to achieve a balance in the model's architec-\ntural intricacy.\nThe transformer module is instrumental in classifying near-\ninfrared spectroscopy data and is adept at handling the spectral\ndata's blend of global and local features.\n36,37 Its attention\nmechanism excels not only in capturing global features across\nthe entire spectrum but also in assessing the signicance of\neach wavelength within this global context. This dual capability\nallows the model to integrate a comprehensive understanding\nof the spectral range, enhancing classication performance\nsignicantly.\nAdditionally, we treat near-infrared spectroscopy data as\na continuous sequence, akin to processing sentences in natural\nlanguage. This approach enables our model to leverage the\nsequential nature of the data eﬀectively.\nCritically, the transformer's self-attention mechanism\ndynamically weighs each wavelength channel, recognizing\nvarying degrees of importance across the spectrum. This\nprocess doesn't isolate channels; rather, it evaluates them\nwithin the overall spectral context, emphasizing relevant\nfeatures while de-emphasizing lesser ones. This nuanced\napproach not only preserves the integrity of global feature\ncomprehension but also renes it by giving weight to the most\ninformative parts of the spectrum, thereby bolstering the\nmodel's ability to diﬀerentiate between various categories with\nheightened sensitivity and precision.\nFurthermore, features in spectral data may exhibit long-\nrange dependencies, such as interactions between certain\nwavelengths. Traditional convolutional neural networks might\nencounter limitations in capturing these long-range depen-\ndencies. In contrast, the transformer's self-attention mecha-\nnism excels at capturing relationships between distant features,\nthus elevating the modeling capability of the classi cation\nmodel when dealing with complex data.\nAs summarized in Table 1, our experimental ndings\nconrm that placing the transformer module aer the rst\nconvolutional layer yields the most favorable results. We posit\nseveral reasons behind this choice. Firstly, this conguration\nhelps the model swily identify and emphasize critical features\nin the input data at an early stage, thereby enhancing the\nFig. 4 Heatmap of the overall classiﬁcation accuracy for cereal variety identiﬁcation using CNN and spectraformer, with diﬀerent preprocessing\nmethods (in columns) and models (in rows).\n8058 | RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nmodel's representative capacity. Secondly, it plays a pivotal role\nin alleviating gradient vanishing or exploding issues, thereby\nexpediting the training convergence process.\nMoreover, spectral data oen exhibit temporal and contex-\ntual dependencies. The introduction of the attention module\nimmediately aer the initial convolutional layer enables the\nmodel to incorporate contextual information earlier. This\naugmentation assists the model in capturing temporal and\ncontextual relationships within the data more eﬀectively.\nNevertheless, it is imperative to acknowledge that the\noptimal architecture and placement of the attention module\nmay vary depending on the speci c task and data type.\nAchieving the best conguration oen necessitates experimen-\ntation. Additionally, diﬀerent types of attention mechanisms\ncan be explored to adapt to diverse data and tasks eﬀectively.\n3.1.2 Convolutional layer. The signicance of convolu-\ntional operations in feature extraction cannot be overstated.\nThese operations play a pivotal role in eﬀectively capturing\nspatial features within spectra, thus making a substantial\ncontribution to spectral classication. The choice of a kernel\nsize of 3 in the convolutional layers is strategically made to\nfacilitate the capture of local features between adjacent wave-\nlengths in spectral data. This is particularly critical for extract-\ning local information from near-infrared spectra, where specic\nchemical or physical relationships between di ﬀerent wave-\nlengths may exist. By leveraging the convolutional layers, the\nmodel gains a deeper understanding of these local features,\nultimately improving classication performance.\nIncluding four stacked convolutional layers, each equipped\nwith a kernel size of 3, enables the model to extract features at\nvarious scales. Shallow layers tend to capture local details, while\ndeeper layers are adept at encompassing more extensive\ncontextual information. This multi-scale feature extraction\nenhances the model's capacity to diﬀerentiate between diﬀerent\ncategories, enriching the representation of spectral data.\nConvolutional operations involve convolving input data with\nconvolutional kernels and applying non-linear activation func-\ntions. This non-linear transformation empowers the convolu-\ntional layers to acquire abstract feature representations within\nspectral data, consequently elevating the model's classication\nperformance. The progressive non-linear transformations\nacross diﬀerent layers gradually extract higher-level features.\nThe Rectied Linear Unit (ReLU) is widely applied in deep\nlearning models as an eﬀective activation function. Compared to\ntraditional S-shaped activation functions like sigmoid or tanh, the\nmain advantage of ReLU lies in its simplicity and mitigation of the\nvanishing gradient problem. The working principle of ReLU is\nstraightforward: it passes any positive input directly and outputs\nzero for any negative input. This mechanism not only reduces\ncomputational complexity but also, due to its constant gradient in\nthe positive region, helps accelerate the training process of neural\nnetworks. Additionally, ReLU introduces non-linearity, allowing\nthe model to learn more complex representations of data. In the\ncontext of near-infrared (NIR) spectroscopy classication models,\nReLU is particularly benecial due to its ability to handle the high-\ndimensional and complex chemical information present in NIR\ndata eﬀectively. Its non-linear processing helps in capturing\nintricate patterns in the spectral data, which is crucial for accurate\nclassication. Moreover, ReLU's characteristic of mitigating the\ngradient vanishing problem is vital in deep learning models\ndealing with NIR spectroscopy, where layers of the network need\nto learn from vast and intricate datasets.\nBatch Normalization (BN) is a key technique designed to\naddress the issue of internal covariate shis in deep learning\nnetworks. In deep neural networks, the input distribution of\nintermediate layers might change due to continuous updates of\nlayer parameters, a phenomenon known as internal covariate\nshi\n. Batch normalization addresses this issue by applying\nnormalization processing at each layer,i.e., adjusting the mean\nand variance of each mini-batch of data to maintain the stability\nof the input distribution. This normalization process not only\nspeeds up the training of the network but also increases the\nmodel's tolerance to initial weight settings, making the training\nmore robust. In the case of NIR spectroscopy classication\nmodels, BN is particularly advantageous. It ensures consistent\ntraining conditions across diﬀerent layers of the network, which\nis crucial for dealing with the variability in NIR spectral data. By\nstabilizing the learning process, BN allows for the use of higher\nlearning rates, which is essential for quickly processing and\nanalyzing the large volumes of data typical in NIR spectroscopy.\nThis makes BN an essential component in the development of\nrobust and eﬃcient NIR spectroscopy classication models.\nAn additional advantage of convolutional operations is their\nability to reduce the dimensionality of feature maps. This\nreduction serves a dual purpose by mitigating the number of\nmodel parameters, reducing computational costs, and\naddressing overtting concerns while enhancing the model's\ngeneralization performance.\nThe model systematically constructs more robust feature\nrepresentations by applying four layers of one-dimensional\nconvolution with a kernel size of 3. These features can capture\nintricate details, patterns, and correlations within spectral data,\nenabling the model to classify accurately. The four layers of one-\ndimensional convolution with a kernel size of 3 serve as versa-\ntile components in near-infrared spectral classication. They\nfulll multiple roles, encompassing local and global feature\ncapture, multi-scale feature extraction, feature enhancementvia\nnon-linear transformations, and dimensionality reduction.\nThese convolutional layers are indispensable for preprocessing\nand feature extraction in near-infrared spectral data, enriching\nthe input information available to the classication model and\nenhancing classication performance (Fig. 5).\nIncluding one-kernel convolution layers aer each layer of\nthree-kernel convolution layers allows for gradually extracting\nmore abstract features from the original spectral data. Each\nconvolutional layer captures information at diﬀerent scales,\nTable 1 Accuracy of diﬀerent positions of the transformer module\nModel A er 1 A er 2 A er 3\nOur 0.86 0.78 0.80\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8059\nPaper RSC Advances\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nranging from local intricacies to global patterns. The subse-\nquent one-kernel convolution layers further consolidate these\nfeatures, enabling the model to understand better the structural\nnuances and relationships inherent in the spectral data.\nThe one-kernel convolution layers, characterized by a kernel\nsize of 1, excel at fusing features from diﬀerent channels. This\nfusion fosters inter-channel information interactions, enriching\nthe holistic feature representation and elevating classi er\nperformance.\nThe synergy between four layers of one-dimensional convolu-\ntion with a kernel size of 3 and the subsequent one-kernel\nconvolution layers extracts comprehensive insights from diverse\nperspectives within spectral data. This enables the model to\ncomprehend spectral data's diversity and complexity better, ulti-\nmately leading to more precise classication outcomes.\nIn summary, combining four layers of one-dimensional\nconvolution with a kernel size of 3, followed by one-kernel\nconvolution layers, plays a pivotal role in feature extraction,\ninformation integration, and dimension control within near-\ninfrared spectral classication. This amalgamation signicantly\nenhances the model's capacity to abstractly represent spectral\ndata, improving classication accuracy and bolstering general-\nization performance.\n3.1.3 Module summary. We have thoroughly examined the\ncritical components of near-infrared spectroscopy classication\nmodels through a comprehensive series of experiments. Both\nthe transformer and convolutional layers have been shown to\nplay pivotal roles in enhancing the model's performance. These\ndistinct components collaborate harmoniously in the context of\nspectral classication tasks, providing robust support for the\noverall model performance. Consequently, when constructing\nand optimizing near-infrared spectroscopy classi cation\nmodels, it becomes imperative to consider the complementary\nfunctions of each component to achieve greater eﬃciency and\nprecision in classication results.\nOur research oﬀers valuable insights into spectral data anal-\nysis, thereby serving as a valuable guide for the practical appli-\ncation of near-infrared spectroscopy classication challenges.\n3.2 Grain species identication\nWe conducted training and validation procedures involvingve\ndiﬀerent algorithms on the original spectra, coupled with 10\ndistinct preprocessing techniques to validate these algorithms.\nThis comprehensive evaluation involved 330 trials, with each\ncrop type undergoing 110 assessments.\n3.2.1 Barley variety identi cation. The recognition results\nfor barley are visually presented in Fig. 6. Notably, the use of\nSavitzky–Golay in combination with Min–Max normalization\n(SM) as a preprocessing method, along with S as the pre-\nprocessing technique in the spectraformer model, achieved the\nhighest classication accuracy, reaching an impressive 84.7%\non the test dataset. As discerned in Fig. 6, training the model\nFig. 5 The testing accuracy curve for spectraformer with a kernel= 1\nconvolutional layer is (a) and the testing accuracy curve for spectra-\nformer without a kernel= 1 convolutional layer is (b). The spectral data\nused is from barley, and the preprocessing method employed is the SM\nmethod.\nFig. 6 Overall classiﬁcation accuracy heatmap of barley cultivar identiﬁcation (N = 24 varieties andn = 1200 samples) using a combination of\npreprocessing methods (in rows) and models (in columns).\n8060 | RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nwith data preprocessed using S or SM consistently outperforms\nother preprocessing techniques. It's worth highlighting that\nboth the CNN and spectraformer models exhibit superior\nperformance compared to other algorithms.\nThe confusion matrix in Fig. 7 displays the classication\nresults achieved by applying data preprocessed with SM to the\nspectraformer model. Impressively, nine barley varieties ach-\nieved 100% correct classication, while eight varieties achieved\nclassication accuracy surpassing the 85% mark. However,\nseven varieties fell short of the 80% accuracy threshold.\nNotably, the Deribe and Holker varieties exhibited lower\nclassication accuracy, standing at only 60%. Deribe had a 1/3\nprobability of being misclassied as the Explorer variety, and\nHolker was nearly 1/3 likely to be classied as the Beka variety.\nThe Shege variety experienced severe classication errors, with\nShege being incorrectly classied as either the EH 1493 variety\nor the HB-1966 variety.\n3.2.2 Chickpea cultivar identi cation. The chickpea varie-\nties exhibit diverse morphological characteristics, and our\nprediction model achieved its highest accuracy among the three\ncrops studied. As shown in Fig. 8, the use of NIR spectroscopy\ndata, when analyzed with deep learning classiers, consistently\noutperformed traditional machine learning models across all\npreprocessing techniques. Remarkably, the spectraformer\nmodel achieved an outstanding classication accuracy of 95%\nwhen preprocessing spectral data with SM. When using S pre-\nprocessing for spectral data, the CNN model, spectraformer\nmodel, and SVM model also achieved remarkable classication\naccuracies of 94%, surpassing all other preprocessing methods.\nFig. 9 provides a comprehensive view through the confusion\nmatrix for the spectraformer model applied to SM preprocessed\ndata for classi cation. Notably, only one variety displayed\na classication accuracy lower than 80%, with the Ejere variety\npotentially being misclassied as either the Habru or Shasho\nvariety. Impressively, twelve varieties achieved perfect accuracy\nin classication.\n3.2.3 Sorghum cultivar identi cation. Fig. 10 presents the\naccuracy of diﬀerent models for sorghum variety recognition.\nNotably, the 0M preprocessing method achieved the highest\naccuracy, albeit with potential drawbacks related to training\ninstability, as demonstrated in Fig. 11(c), where thetting speed\nis slower. It's noteworthy that our experiments with barley and\nchickpea data, using the 0M preprocessing method, indicate\nFig. 7 Confusion matrix of barley cultivar from the spectraformer\nmodel that achieved the best score (SM). Overall classiﬁcation accu-\nracy is 84.7%.\nFig. 8 Overall classiﬁcation accuracy heatmap for chickpea cultivar identiﬁcation (N = 19 varieties andn = 950 samples), using a combination of\npreprocessing methods (in rows) and models (in columns).\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8061\nPaper RSC Advances\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nthat the eﬀect of epoch= 200 is not as favorable as epoch= 500.\nThis discrepancy is attributed to the additional time provided\nby epoch = 500 for optimal model convergence, as evident in\nFig. 11(c) and (d).\nWhile data preprocessed with the 0M and M methods, as\nwell as raw data input into the spectraformer model, exhibited\nthe highest testing accuracy, their testing accuracy curves were\nobserved to be less stable, as depicted in Fig. 11(a) –(c).\nConversely, data preprocessed with the S and SM methods,\nalthough slightly lower by 1–2 percentage points, displayed\nstable accuracy curves, as evidenced in Fig. 13(a) and (b). It's\nworth noting that while uctuating curves may yield better\nresults in specic cases, predicting such scenarios can be\nchallenging. Smooth curves are generally more interpretable\nand comprehensible, signifying stable model training less\nsusceptible to randomness. This enhances the model's reli-\nability and ensures more consistent performance across diverse\ndatasets and experimental conditions.\nFig. 12 illustrates the confusion matrix for 0M (epoch= 200)\npreprocessing. Among the ten varieties, four achieved perfect\nFig. 9 Confusion matrix of chickpea cultivar from the spectraformer\nmodel that achieved the best score (SM). Overall classiﬁcation accu-\nracy is 95.4%.\nFig. 10 Overall classiﬁcation accuracy heatmap for sorghum cultivar identiﬁcation (N = 10 varieties andn = 500 samples), using a combination\nof preprocessing methods (in rows) and models (in columns).\nFig. 11 The testing accuracy curve obtained with raw data is repre-\nsented as (a), the testing accuracy curve obtained with preprocessed\ndata using method M is represented as (b), the testing accuracy curve\nwith preprocessed data using method 0M and epoch= 200 is rep-\nresented as (c), and the testing accuracy curve with preprocessed data\nusing method 0M and epoch= 500 is represented as (d).\n8062\n| RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nclassication accuracy, with only the Teshale variety falling\nbelow the 80% threshold, having a 1/3 probability of being\nmisclassied as Dekeba.\n3.2.4 k-Fold cross-validation. The k-fold cross-validation\nmethod, a staple in statistical analysis and machine learning,\nwas employed to enhance the accuracy and comprehensiveness\nof the experimental results. This method's core concept involves\ndividing the dataset intok subsets, subsequently using each as\na test set in rotation, with the remaining subsets serving as the\ntraining set. Such an approach facilitates a more thorough\nevaluation of the model's performance and mitigates the\ninuence of data partitioning methods on the results.\nIn this study, a range of k-fold cross-validation methods,\nfrom 2-fold to 10-fold, were meticulously tested. It was conclu-\nsively determined that 5-fold cross-validation yielded the most\noptimal performance in this experimental context. This deci-\nsion was informed by a holistic consideration of factors,\nincluding model stability, accuracy, and computational\neﬃciency.\nUpon reviewing Fig. 14, 15, and 16, it was observed that the\nspectraformer model consistently demonstrated high accuracy\nacross both 5-fold cross-validation and the conventional 7 : 3\ndata split when employing both S and SM preprocessing\nmethods. Delving into specics, the SM preprocessing method\nexhibited superior performance in the barley dataset during 5-\nfold cross-validation, achieving an impressive average accuracy\nrate of 84.7%. Similarly, this method attained the highest\naverage accuracy of 94.7% with the chickpea dataset. In the case\nof the sorghum dataset, the SM method again proved to be the\nmost eﬀective, reaching an average accuracy rate of 84.7%.\nThese ndings underscore the adaptability and eﬃciency of the\nSM preprocessing method across diverse datasets.\nBy comparing the outcomes of the 5-fold cross-validation\nwith the results from splitting the dataset into a validation set\nusing a 7 : 3 ratio, we can observe a notable reduction in accu-\nracy when employing the 5-fold cross-validation in conjunction\nwith the 0M preprocessing condition. This suggests that the 0M\npreprocessing method might compromise the spectraformer\nmodel's performance stability, thereby rendering it less suitable\nas a universal preprocessing approach.\n3.2.5 Experimental discussion. The outcomes of this study\nunderscore the viability of employing small-scale near-infrared\nspectra for crop variety identication, with a notable emphasis\non the eﬀectiveness of models that integrate the transformer\nmodule into CNN architectures.\nThe collective correct classication accuracy for the 24 barley\nvarieties, 19 chickpea varieties, and 10 sorghum varieties\nreached 85%, 95%, and 86%, respectively. It is worth noting\nthat prior research has similarly indicated that while not all\ncategories may achieve perfect classi cation, near-infrared\nspectra retain the potential for robust variety identication.\nFig. 12 Confusion matrix of sorghum cultivar from the spectraformer\nmodel that achieved score (0M, epoch= 200). Overall classiﬁcation\naccuracy is 89.3%.\nFig. 13 The testing accuracy curve obtained with preprocessed data\nusing method S is represented as (a), and the testing accuracy curve\nobtained with preprocessed data using method SM is represented as (b).\nFig. 14 Overall classiﬁcation 5-fold cross-validation accuracy heat-\nmap for barley cultivar identiﬁcation (N = 24 varieties andn = 1200\nsamples), using a combination of preprocessing methods (in rows) and\nmodels (in columns).\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8063\nPaper RSC Advances\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nThe extensive experiments conducted with barley, chickpea,\nand sorghum data show that deep learning models consistently\nexhibit superior robustness compared to traditional machine\nlearning approaches. Deep learning models consistently ach-\nieve accuracies of nearly 80% or even higher across various\npreprocessing methods, underscoring their adaptability and\ncapacity to capture complex patterns.\nIt is noteworthy that traditional machine learning models\nrely more on the relationships within the data. Models\nemploying SVM (linear) and Random Forest generally outper-\nform those using SVM (RBF), implying a clear linear correlation\nwithin the spectral data. The relatively high accuracy of the SVM\n(linear) model suggests that the spectral data exhibits linear\nseparability within the feature space, enabling linear classiers\nto distinguish spectral data from diﬀerent categories eﬀectively.\nIn contrast, SVM (RBF) may be more prone to overtting when\nmapping the data to a higher-dimensional feature space,\npotentially leading to reduced model performance.\nConversely, the high accuracy achieved by the Random Forest\nmodel may indicate the presence of intricate yet linearly separable\npatterns in the spectral data. Random Forest, an ensemble\nlearning method, excels at capturing non-linear data relation-\nships by combining the outcomes of multiple decision trees while\nbeneting from the data's underlying linear structure.\nIt's essential to note that while the accuracy of traditional\nmachine learning models remains consistent when classifying\nthe three types of data (original, preprocessed with M, and\npreprocessed with 0M), this does not imply that M and 0M\npreprocessing had no impact on the data. In the realm of deep\nlearning, training on the original data yields test accuracy\ncurves with more uctuations compared to training on data\npreprocessed with M or 0M, as evidenced in Fig. 11(a)–(c). This\nphenomenon could be attributed to the potential loss or\namplication of information in specic features resulting from\nthe M and 0M preprocessing, as deep learning models are\nsensitive to the scale and distribution of data. Such sensitivity\ncan lead to increaseductuations in test accuracy curves.\nIn contrast, traditional machine learning models, particu-\nlarly linear and tree-based models, tend to be less sensitive to\nthe absolute scale of features. These models emphasize relative\nrelationships and patterns between features, exhibiting\nminimal concern for the absolute numerical values of features.\nConsequently, in traditional machine learning, the loss or\namplication of information induced by normalization has\na relatively insignicant impact on model performance.\nIndeed, deep learning models have demonstrated remark-\nable adaptability, rendering the nature of data relationships less\nlimiting. Deep learning exhibits the capability to learn suitable\nfeature representations for data, irrespective of whether the\ndata exhibits linear or non-linear relationships, resulting in\nexceptional accuracy. The strength of deep learning models lies\nin their multi-layered neural network architectures, which\ninherently possess the capacity to automatically unearth and\nexpress intricate patterns and correlations within the data.\nFurthermore, deep learning models excel in processing high-\nFig. 15 Overall classiﬁcation 5-fold cross-validation accuracy heat-\nmap for barley cultivar identiﬁcation (N = 24 varieties andn = 1200\nsamples), using a combination of preprocessing methods (in rows) and\nmodels (in columns).\nFig. 16 Overall classiﬁcation 5-fold cross-validation accuracy heat-\nmap for barley cultivar identiﬁcation (N = 24 varieties andn = 1200\nsamples), using a combination of preprocessing methods (in rows) and\nmodels (in columns).\n8064\n| RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ndimensional data, extracting valuable information from various\ninput features. This attribute proves particularly advantageous\nfor complex data types such as spectral data.\nDeep learning models are typically trained using the back-\npropagation algorithm, which autonomously ne-tunes the\nmodel's weights and parameters to minimize the loss function,\nenhancing the model's alignment with the data. This adaptability\nempowers deep learning mode ls to consistently achieve\noutstanding performance across diverse data relationship\nscenarios encompassing linear, non-linear, and highly intricate\ndata patterns.\nWhile non-linear discriminative models like SVM possess\nunique capabilities for handling small datasets and can harness\nspectral information to achieve accuracy levels approaching those\nof deep learning models, this underscores the importance of\nspectral information, particularly in the context of barley variety\nidentication. However, it is essential to recognize that deep\nlearning models also oﬀer advantages in addressing non-linearity\nand handling large datasets, rendering them an ideal choice for\nbarley variety identication. Consequently, the selection of the\nappropriate model algorithm should be guided by considerations\nsuch as dataset size and the inherent nature of the data.\n4 Conclusions\nIn this comprehensive study, we harnessed the power of near-\ninfrared hyperspectral imaging in conjunction with deep\nlearning to eﬀectively categorize various crops. Our dataset\nencompassed a substantial volume, featuring 1200 barley grains,\n950 chickpeas, and 500 sorghum grains, facilitating a rigorous\nanalysis.\nThe results obtained in our research underscore the excep-\ntional performance of the transformer module in the domain of\nnear-infrared spectroscopy. While CNNs may occasionally achieve\nsimilar accuracy levels, it is vital to clarify that this does not\ninherently imply superiority over our proposed model. Trans-\nformers, particularly when operating on larger datasets, consis-\ntently exhibit superior performance, showcasing their robustness\nand formidable generalization capabilities.\nMoreover, our multi-model comparative experiments\nunequivocally demonstrate that our proposed model consis-\ntently outperforms other models across various performance\nmetrics, with accuracy being a prominent factor.\nFurthermore, our experiments have unveiled the profound\nimpact of diﬀerent preprocessing methods on spectral data. These\nndings provide substantial empirical evidence supporting the\nutility of small-scale near-infrared spectrometers and machine-\nlearning techniques for precisely identifying crop varieties.\nIn summary, this study not only leverages state-of-the-art\ntechnology and methodologies but also underscores the\nremarkable potential of combining near-infrared hyperspectral\nimaging with deep learning, oﬀering valuable insights into the\ndomain of crop variety identication.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nThis work is supported by the National Key R&D Plan under\nGrant No. 2021YFF0601200 and 2021YFF0601204.\nNotes and references\n1 M. B. Priyadarshi, A. Sharma, K. Chaturvedi, R. Bhardwaj,\nS. Lal, M. Farooqi, S. Kumar, D. Mishra and M. Singh,\nLegume Res., 2023,46, 251–256.\n2 I. Ejaz, S. He, W. Li, N. Hu, C. Tang, S. Li, M. Li, B. Diallo,\nG. Xie and K. Yu,Front. Plant Sci., 2021,12, 720022.\n3 M. Blanco and I. Villarroya,TrAC, Trends Anal. Chem., 2002,\n21, 240–250.\n4 J. S. Shenk, J. J. Workman Jr and M. O. Westerhaus,\nHandbook of near-infrared analysis, CRC Press, 2007, pp.\n365–404.\n5 J. U. Porep, D. R. Kammerer and R. Carle,Trends Food Sci.\nTechnol., 2015,46, 211–230.\n6 A. Munawar, Y. Yunus, D. Devianti and P. Satriyo,IOP Conf.\nSer. Earth Environ. Sci., 2021, 012036.\n7 B. K. Wilson, H. Kaur, E. L. Allan, A. Lozama and D. Bell,Am.\nJ. Trop. Med. Hyg., 2017,96, 1117.\n8 R. Giovanni, Smartphone-Based Food Diagnostic\nTechnologies: A Review,Sensors, 2017,17(6), 1453.\n9 Y. LeCun, L. Bottou, Y. Bengio and P. Haﬀner, Proc. IEEE,\n1998, 86, 2278–2324.\n10 J. Yang, J. Wang, G. Lu, S. Fei, T. Yan, C. Zhang, X. Lu, Z. Yu,\nW. Li and X. Tang, Comput. Electron. Agric., 2021, 190,\n106431.\n11 D. Rong, H. Wang, Y. Ying, Z. Zhang and Y. Zhang,Comput.\nElectron. Agric., 2020,175, 105553.\n12 J. Acquarelli, T. van Laarhoven, J. Gerretzen, T. N. Tran,\nL. M. Buydens and E. Marchiori, Anal. Chim. Acta, 2017,\n954,2 2–31.\n13 X. Zhang, T. Lin, J. Xu, X. Luo and Y. Ying,Anal. Chim. Acta,\n2019, 1058,4 8–57.\n14 L. Ling-qiao, P. Xi-peng, F. Yan-chun, Y. Li-hui, H. Chang-qin\nand Y. Hui-hua, Spectrosc. Spectral Anal., 2019, 39, 3606–\n3613.\n15 X. Zhang, J. Yang, T. Lin and Y. Ying, Trends Food Sci.\nTechnol., 2021,\n112, 431–441.\n16 C. Cui and T. Fearn,Chemom. Intell. Lab. Syst., 2018,182,9 –\n20.\n17 P. Fu, Y. Wen, Y. Zhang, L. Li, Y. Feng, L. Yin and H. Yang,J.\nInnovative Opt. Health Sci., 2022,15, 2250021.\n18 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser and I. Polosukhin, Advances in\nneural information processing systems, 2017, vol. 30.\n19 K. Ahmed, N. S. Keskar and R. Socher,arXiv, 2017, preprint,\narXiv:1711.02132.\n20 K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,\nR. Zemel and Y. Bengio,International conference on machine\nlearning, 2015, pp. 2048–2057.\n21 M.-T. Luong, H. Pham and C. D. Manning, arXiv, 2015,\npreprint, arXiv:1508.04025, DOI:10.48550/arXiv.1508.04025.\n© 2024 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 | 8065\nPaper RSC Advances\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n22 D. Bahdanau, K. Cho and Y. Bengio,arXiv, 2014, preprint,\narXiv:1409.0473, DOI:10.48550/arXiv.1409.0473.\n23 F. Kosmowski and T. Worku,PloS One, 2018,13, e0193620.\n24 L. Hao-xiang, Z. Jing, L. Ling-qiao, L. Zhen-bing, Y. Hui-hua,\nF. Yan-chun and Y. Li-hui,Spectrosc. Spectral Anal., 2021,41,\n1782–1788.\n25 X. Miao, Y. Miao, H. Gong, S. Tao, Z. Chen, J. Wang, Y. Chen\nand Y. Chen,Spectrochim. Acta, Part A, 2021,257, 119700.\n26 P. Mishra, J. M. Roger, F. Marini, A. Biancolillo and\nD. N. Rutledge,Chemom. Intell. Lab. Syst., 2021,212, 104190.\n27 C. Cortes and V. Vapnik,Mach. Learn., 1995,20, 273–297.\n28 L. Breiman,Mach. Learn., 2001,45,5 –32.\n29 S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan and\nM. Shah,ACM Comput. Surv., 2022,54,1 –41.\n30 J. Wang, C.-Y. Hsieh, M. Wang, X. Wang, Z. Wu, D. Jiang,\nB. Liao, X. Zhang, B. Yang, Q. He,et al., Nat. Mach. Intell.,\n2021, 3, 914–922.\n31 V. Venkatasubramanian and V. Mann, Curr. Opin. Chem.\nEng., 2022,36, 100749.\n32 O. Devos, C. Ruckebusch, A. Durand, L. Duponchel and\nJ.-P. Huvenne,Chemom. Intell. Lab. Syst., 2009,96,2 7–33.\n33 V. G. K. Cardoso and R. J. Poppi,Microchem. J., 2021, 164,\n106052.\n34 M. Li, F. Chen, M. Lei and C. Li,Guangpuxue Yu Guangpu\nFenxi, 2016,36, 2793–2797.\n35 H. Qiao, X. Shi, H. Chen, J. Lyu and S. Hong,Soil Tillage Res.,\n2022, 215, 105223.\n36 S. d'Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos, G. Biroli\nand L. Sagun,International Conference on Machine Learning,\n2021, pp. 2286–2296.\n37 Z. Dai, H. Liu, Q. V. Le and M. Tan,Advances in neural\ninformation processing systems, 2021, vol. 34, pp. 3965–3977.\n8066 | RSC Adv.,2 0 2 4 ,14,8 0 5 3–8066 © 2024 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 07 March 2024. Downloaded on 11/5/2025 7:19:51 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online",
  "topic": "Sorghum",
  "concepts": [
    {
      "name": "Sorghum",
      "score": 0.6768178939819336
    },
    {
      "name": "Preprocessor",
      "score": 0.6224371194839478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5448309779167175
    },
    {
      "name": "Computer science",
      "score": 0.4992334842681885
    },
    {
      "name": "Machine learning",
      "score": 0.4920899271965027
    },
    {
      "name": "Agriculture",
      "score": 0.47098609805107117
    },
    {
      "name": "Data pre-processing",
      "score": 0.4294891953468323
    },
    {
      "name": "Agricultural engineering",
      "score": 0.4272356927394867
    },
    {
      "name": "Near-infrared spectroscopy",
      "score": 0.4121144413948059
    },
    {
      "name": "Engineering",
      "score": 0.2403273582458496
    },
    {
      "name": "Agronomy",
      "score": 0.22015950083732605
    },
    {
      "name": "Biology",
      "score": 0.09842965006828308
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96733725",
      "name": "Shanghai Maritime University",
      "country": "CN"
    }
  ]
}