{
  "title": "LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization",
  "url": "https://openalex.org/W3176692111",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5018635864",
      "name": "Weidong Guo",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5103834539",
      "name": "Mingjun Zhao",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5010873419",
      "name": "Lusheng Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5032424832",
      "name": "Di Niu",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5069413253",
      "name": "Jinwen Luo",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100420216",
      "name": "Zhenhua Liu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101409888",
      "name": "Zhenyang Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101998691",
      "name": "Jianbo Tang",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3177365697",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3081031588",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2944852028",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Language model pre-training based on large corpora has achieved tremendous\\nsuccess in terms of constructing enriched contextual representations and has\\nled to significant performance gains on a diverse range of Natural Language\\nUnderstanding (NLU) tasks. Despite the success, most current pre-trained\\nlanguage models, such as BERT, are trained based on single-grained\\ntokenization, usually with fine-grained characters or sub-words, making it hard\\nfor them to learn the precise meaning of coarse-grained words and phrases. In\\nthis paper, we propose a simple yet effective pre-training method named LICHEE\\nto efficiently incorporate multi-grained information of input text. Our method\\ncan be applied to various pre-trained language models and improve their\\nrepresentation capability. Extensive experiments conducted on CLUE and\\nSuperGLUE demonstrate that our method achieves comprehensive improvements on a\\nwide variety of NLU tasks in both Chinese and English with little extra\\ninference cost incurred, and that our best ensemble model achieves the\\nstate-of-the-art performance on CLUE benchmark competition.\\n",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1383–1392\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1383\nLICHEE: Improving Language Model Pre-training\nwith Multi-grained Tokenization\nWeidong Guo1∗, Mingjun Zhao2∗, Lusheng Zhang1∗, Di Niu2, Jinwen Luo1,\nZhenhua Liu1, Zhenyang Li1, Jianbo Tang1\n1Platform and Content Group, Tencent\n2University of Alberta\n{weidongguo, lshzhang, jamsluo, edinliu,\nnickzyli, jianbotang}@tencent.com,\n{zhao2, dniu}@ualberta.ca\nAbstract\nLanguage model pre-training based on large\ncorpora has achieved tremendous success in\nterms of constructing enriched contextual rep-\nresentations and has led to signiﬁcant perfor-\nmance gains on a diverse range of Natural\nLanguage Understanding (NLU) tasks. De-\nspite the success, most current pre-trained lan-\nguage models, such as BERT, are trained based\non single-grained tokenization, usually with\nﬁne-grained characters or sub-words, making\nit hard for them to learn the precise meaning\nof coarse-grained words and phrases. In this\npaper, we propose a simple yet effective pre-\ntraining method named LICHEE to efﬁciently\nincorporate multi-grained information of input\ntext. Our method can be applied to various pre-\ntrained language models and improve their rep-\nresentation capability. Extensive experiments\nconducted on CLUE and SuperGLUE demon-\nstrate that our method achieves comprehensive\nimprovements on a wide variety of NLU tasks\nin both Chinese and English with little extra\ninference cost incurred, and that our best en-\nsemble model achieves the state-of-the-art per-\nformance on CLUE benchmark competition.\n1 Introduction\nPre-trained language models (PLMs) such as GPT\n(Radford et al., 2018), BERT (Devlin et al., 2019)\nand XLNet (Yang et al., 2019) have become enor-\nmously popular and achieved great success on di-\nverse natural language understanding tasks, such\nas sentiment analysis, question answering, and lan-\nguage inference. These models usually utilize a\ntransformer architecture (Vaswani et al., 2017) to\ncapture the dependencies between tokens in the in-\nput text, to model the language information, and\nto learn contextual representations. It is ﬁrst pre-\ntrainined based on large-scale unlabeled corpora,\n∗ ∗Equal contribution.\nand subsequently ﬁne-tuned based on the labeled\ndata from downstream tasks.\nIn many NLU applications, tokenization often\naffects the performance and needs to be chosen\ncarefully. The input tokens for pre-trained lan-\nguage models are usually ﬁne-grained, e.g., words\nand sub-words for English and characters for Chi-\nnese. Compared with coarse-grained tokens such as\nphrases, the advantage of ﬁne-grained tokens is that\nthey form a smaller vocabulary, yielding abundant\ntraining samples per token, and thus alleviating the\ndata sparsity issue and out-of-vocabulary (OOV)\nproblem (Li et al., 2019). However, even trained\non large corpora, it is still hard for language mod-\nels pre-trained with ﬁne-grained tokens to learn\nthe correct attention boundaries of larger semantic\nunits in many languages (Zhang and Li, 2020).\nTo obtain a more accurate model, prior studies\nattempt to incorporate coarse-grained information\ninto models trained with ﬁne-grained tokenization\nby masking sequences of consecutive tokens in the\npre-training stage (Joshi et al., 2020; Cui et al.,\n2019). Zhang and Li (2020) propose AMBERT, a\nSiamese network based on BERT to handle multi-\ngrained input text, and uses two encoders with\nshared weights to separately encode ﬁne-grained to-\nkens and coarse-grained tokens into two sequences\nof contextualized representations. Despite its ef-\nfectiveness, the inference cost of AMBERT almost\ndoubles that of the original BERT due to the dual-\nencoder structure, which is often unacceptable in\nindustrial scenarios.\nIn this paper, we propose a novel method named\nLICHEE designed to efﬁciently leverage the input\ninformation at multiple levels of granularity in the\npre-training stage in order to enhance the repre-\nsentation ability of PLMs. Unlike AMBERT that\nencodes the ﬁne-grained and coarse-grained tokens\nwith two encoders, which signiﬁcantly increases\nthe inference cost, in LICHEE the fusion of multi-\n1384\ngrained information of input text happens at the\nembedding level, which requires no change on the\noriginal model structure of the PLM, and thus in-\nduces little extra inference cost when applied in on-\nline NLP applications. Speciﬁcally, LICHEE ﬁrst\npre-processes the input text into ﬁne-grained and\ncoarse-grained tokens, which are passed through\ntwo embedding layers, respectively, to derive their\ncorresponding vector representations. Both vector\nrepresentations are then merged via pooling to form\nthe multi-grained embedding vector, which serves\nas the input to the PLM encoder. Finally, the en-\nhanced contextual representations generated by the\nPLM encoder, with both ﬁne-grained and coarse-\ngrained information incorporated, are obtained and\nused for downstream tasks.\nWe have applied LICHEE to enhance multi-\nple different pre-trained language models, includ-\ning BERT (Devlin et al., 2019), ALBERT (Lan\net al., 2019), and GPT (Brown et al., 2020), and\nconducted extensive evaluation of the resulted\nlanguage models on Chinese natural language\nunderstanding (NLU) tasks evaluated by CLUE\n(Liang Xu, 2020) benchmarks. Results show\nthat with LICHEE, the resulted pre-trained lan-\nguage models signiﬁcantly outperform their single-\ngrained counterparts on almost all tasks, by taking\nadvantage of multi-grained information to effec-\ntively and efﬁciently produce more accurate repre-\nsentations.\nIn addition, we also participated in the CLUE\nbenchmark competition with our best ensemble\nmodel built upon a collection ofLICHEE-enhanced\nBERT-large models, and achieved the state-of-the-\nart performance of an average score of 80.42 (as\nof January 8, 2021) over 9 different Chinese NLU\ntasks, as well as the best scores on two individual\ntasks: IFLYTEK and CSL.\nMoreover, we have also conducted English natu-\nral language understanding experiments based on\nSuperGLUE (Wang et al., 2019a) benchmarks. Sig-\nniﬁcant improvements are observed when LICHEE\nis employed in the pre-training stage, which demon-\nstrates that the proposed pre-training method is\ngenerally effective in different language settings.\n2 Related Work\nIn this section, we give a brief overview of some\npopular pre-trained language models and studies\non the training techniques related to tokenization.\nPre-trained language models are pre-trained on\nlarge unsupervised corpora and aim to produce\nmeaningful representations for each input token\nnot only considering the meaning of itself, but also\nwith its surrounding contexts anticipated. ELMo\n(Peters et al., 2018) is one of the ﬁrst pre-trained\nlanguage models based on bidirectional LSTMs\nwhich produces the contextual representation of\neach token by concatenating its left-to-right and\nright-to-left representations. GPTs (Radford et al.,\n2018, 2019; Brown et al., 2020) leverage the pow-\nerful Transformer (Vaswani et al., 2017) to build\nan auto-regressive language model predicting the\nnext token given its history context. BERT (Devlin\net al., 2019) is a bidirectional auto-encoding lan-\nguage model also based on transformer. It consists\nof two pre-training objectives: masked language\nmodel (MLM) and next sentence prediction (NSP).\nYang et al. (2019) point out the discrepancy of the\npre-training and ﬁne-tuning stage of BERT due to\nthe masking symbol, and propose a permutation\nlanguage model called XLNet (Yang et al., 2019).\nThe great popularity of BERT draws many re-\nsearchers to make improvements on the architec-\nture. RoBERTa (Liu et al., 2019) improves several\ntraining details of BERT including dynamic mask-\ning and the removal of the NSP pre-training task.\nALBERT (Lan et al., 2019) reduces the model pa-\nrameters with cross-layer weight sharing and ac-\ncelerates the training process. ELECTRA (Clark\net al., 2019) proposes a new token detection task\nand adopts a generator-discriminator framework to\npre-train the language model.\nAlthough most pre-trained language models are\nbuilt on ﬁne-grained tokenization, coarse-grained\ninformation proves to be helpful to the model per-\nformance. Cui et al. (2019) propose a masking\nscheme called “whole word masking” (WWM) for\nChinese BERT, where the consecutive characters\nbelonging to the same word are masked together.\nIn ERNIE (Sun et al., 2019), knowledge graphs\nare added to enhance the model, and entity level\nmasking is used during the pre-training, which is\nbeneﬁcial for language understanding tasks. Span-\nBERT (Joshi et al., 2020) proposes to mask ran-\ndom spans instead of random tokens, and adopts\na new span boundary objective task to replace the\nnext sentence prediction task in the pre-training.\nInstead of focusing on the masking scheme, AM-\nBERT (Zhang and Li, 2020) proposes to adopt two\nencoders with shared parameters to learn the rep-\nresentations of ﬁne-grained and coarse-grained to-\n1385\nkens in parallel. However, even that the weight\nsharing setting reduces the number of model pa-\nrameters, the dual-encoder structure of AMBERT\ninduces twice the inference cost, which remains a\nhuge issue when deployed in online applications.\nDifferent from AMBERT, our work merges the\nﬁne-grained and coarse-grained tokenization at\nembedding level, and achieves signiﬁcant perfor-\nmance gains with little additional computation\ncosts.\n3 Methodology\nIn this section, we present LICHEE, the general\nmulti-grained framework for language model pre-\ntraining, and its detailed implementation, including\nthe pre-training methods for both auto-regressive\nand auto-encoding tasks and ﬁne-tuning details.\n3.1 Model Architecture\nFigure 1 gives an overview of LICHEE where the\ninput information from multiple granularities is\nleveraged to enhance the representation ability for\nmany pre-trained language models.\nThe framework takes in text sequences as input\nwhich are tokenized into token sequences. In this\npaper, we keep two vocabularies and use two tok-\nenizers to perform ﬁne-grained and coarse-grained\ntokenizations, where items in vocabularies are se-\nlected based on their token frequencies in pre-\ntraining corpora. Also, the deﬁnitions of “ﬁne\ngrain” and “coarse grain” vary across languages.\nFor example, in English, words and phrases are\noften used as the ﬁne-grained and coarse-grained\ntokens respectively. And in Chinese, characters\nand words are used instead. Ofﬁcially, for a given\ninput text sequence T, we use tf\ni to denote the\ni-th ﬁne-grained token and tc\nj-k to denote a coarse-\ngrained token that is composed of ﬁne-grained to-\nkens {tf\nj,...,t f\nk}between j and k. For example,\nin ﬁgure 1, the coarse-grained token “New York\nTimes” is composed of the ﬁrst, sencond, and third\nﬁne-grained tokens, and is denoted as tc\n1-3.\nAfter tokenization, two separate embedding lay-\ners are used to map the tokenized tokens to their\nvector representations. Speciﬁcally, each ﬁne-\ngrained token tf\ni is passed into a ﬁne-grained em-\nbedding layer to produce the ﬁne-grained embed-\nding vector ⃗ ef\ni ∈Rd of the token, where ddenotes\nthe dimension of the ﬁne-grained embedding. Sim-\nilarly, the coarse-grained embedding ⃗ ec\nj-k ∈Rd\nis derived with the same dimension dby feeding\ntoken tc\nj-k to the coarse-grained embedding layer,\nshown as:\n⃗ ef\ni = embeddingfine(tf\ni),\n⃗ ec\nj-k = embeddingcoarse(tc\nj-k).\n(1)\nFor each token tf\ni, we construct its multi-grained\nembedding vector ⃗ ei ∈Rd by performing a max-\npooling operation on the derived ﬁne-grained em-\nbedding ⃗ ef\ni and the coarse-grained embedding ⃗ ec\nj-k\nof its corresponding coarse-grained token tc\nj-k:\n⃗ ei = max-pool(⃗ ef\ni,⃗ ec\nj-k), (2)\nwhere j ≤i≤k. Note that dis equal to the orig-\ninal embedding dimension of the single-grained\nPLM, to prove that the performance gain is con-\ntributed to the introduction of multi-grained infor-\nmation other than modiﬁed model structure.\nFinally, the combined embedding vectors ⃗ eare\nfed into the PLM encoder to construct the ﬁnal\ncontextualized representations ⃗h enhanced with\nmulti-grained information:\n⃗h = encode(⃗ e). (3)\n3.2 Pre-training\nWe have applied LICHEE on both auto-regressive\nand auto-encoding PLMs, such as GPT and BERT.\nFor auto-regressive PLMs, the pre-training task\nis Next Token Prediction which aims to predict the\nnext token ti based on its previous context t<i, by\noptimizing the following objective function\nmin\nθ\n−\n∑\ni\nlog pθ(ti|t<i), (4)\nwhere the conditional probability pθ is modeled\nwith a network with parameter θ.\nIn our framework, we adjust the objective func-\ntion to include both ﬁne-grained context tf\n<i and\ncoarse-grained context tc\n<i, shown as:\nmin\nθ\n−\n∑\ni\nlog pθ(ti|tf\n<i,tc\n<i). (5)\nNote that when making predictions on any token\nwithin a coarse-grained span ti ∈tc\nj-k, the token\nembedding ⃗ ei will cause information leakage as it\ninvolves the coarse-grained token embedding⃗ ec\nj-k\nwhich contains information beyond the history con-\ntext. For example, in the case illustrated in ﬁgure\n1, the prediction on token “York” should not rely\n1386\nThe is a newspaperNew York Times\nCoarse-grained Embedding Layer\nThe is a newspaperNew York Times\nThe New York Times is a newspaper\nFine-grained Embedding Layer\nThe New York Times is a newspaper\nFine-grained Tokenization Coarse-grained Tokenization\nThe New York Times is a newspaper\nThe\n+\nThe\nNew\n+\nNew York Times\nYork\n+\nNew York Times\nTimes\n+\nNew York Times\nis\n+\nis\na\n+\na\nnewspaper\n+\nnewspaper\nPLM Encoder\nTokenization\nMulti-grained\nEmbedding\nMerge\nEmbedding\nEncode\nFigure 1: The overall structure of our proposed pre-training framework LICHEE. Fine-grained and coarse-grained\ntokens are ﬁrst derived from the input text by tokenization, and separately passed into two individual embedding\nlayers. The multi-grained embedding vectors are acquired by taking a max-pooling on the ﬁne-grained and coarse-\ngrained embedding vectors, and are fed into the PLM encoder to extract the ﬁnal contextualized representations.\non token “New” and its embedding ⃗ e1 as it dis-\ncloses the entire information of the coarse-grained\ntoken of “New York Times” by the coarse-grained\nembedding ⃗ ec\n1-3. Therefore, we can only exploit\nthe context before the start position of the coarse-\ngrained token to make predictions, illustrated as:\nmin\nθ\n−\n∑\nj≤i≤k\nlog pθ(ti|tf\n<j,tc\n<j), (6)\nwhere jand kare the start and end positions of the\ncoarse-grained token.\nFor auto-encoding PLMs, we only include\nMasked Language Modeling (MLM) task in the\npre-training process, as Next Sentence Prediction\n(NSP) task is shown to have no beneﬁts indicated\nin many recent studies (Lan et al., 2019; Liu et al.,\n2019; Zhang and Li, 2020). In MLM, 15% of the\ntokens are randomly selected and substituted with\na set of tokens, in which 80% are replaced with\n[MASK] token, 10% are replaced with random to-\nkens, and 10% stay unchanged.\nThe objective is to recover the masked tokens\nTm ⊂T from the altered text input sequence ˜T:\nmin\nθ\n−\n∑\ntm∈Tm\nlog pθ(tm|˜T). (7)\nIn our framework, we propose to exploit the\nmulti-grained information of the input in the MLM\ntask, shown as:\nmin\nθ\n−\n∑\ntm∈Tm\nlog pθ(tm|˜Tf, ˜Tc), (8)\nwhere ˜Tf and ˜Tc stand for the ﬁne-grained and\ncoarse-grained altered input text.\nSimilar to the strategy deployed in auto-\nregressive PLMs, we apply a masking strategy that\nwhen a ﬁne-grained token tf\ni is to be masked, its\ncorresponding coarse-grained token tc\nj-k and all the\nﬁne-grained tokens tf\nj,...,t f\nk belonging to it are\nalso masked, in order to avoid information leakage\nfrom the multi-grained embeddings.\n3.3 Fine-tuning\nIn ﬁne-tuning of downstream tasks, we append the\nspecial tokens ([CLS], [SEP]) to both ﬁne-grained\nand coarse-grained vocabularies. In sentence-level\n1387\nclassiﬁcation tasks, [CLS] is attached to the start\nof input sequences in auto-encoding PLMs like\nBERT, and to the end of the input in auto-regressive\nPLMs like GPT. Its multi-grained contextualized\nrepresentation ⃗h[CLS] is used to represent the whole\ninput sequence and is passed into a projecting layer\nfor the ﬁnal prediction.\nSimilarly, for tasks that include token-level span\ndetection, such as Question Answering, the contex-\ntual representation ⃗hi for each token ti is extracted\nand utilized in the task.\n4 Experiments\nWe have carried out extensive experiments on vari-\nous natural language understanding tasks on both\nChinese and English datasets. In the following sec-\ntion, we will ﬁrst introduce the pre-training datasets\nused in our evaluation and provide the implementa-\ntion details of our framework. And we demonstrate\nthe effectiveness of LICHEE by conducting com-\nprehensive experiments on various Chinese NLU\ndatasets with multiple different PLMs, and com-\npare our method with other baseline methods. Next,\nwe perform a thorough ablation study to evaluate\ndifferent approaches of integrating input text in-\nformation from multiple granularities. Finally, we\nadopt LICHEE to an English BERT to verify its\nefﬁcacy on English NLU tasks.\n4.1 Pre-Training Datasets\nFor Chinese language, there is no commonly used\ncorpus for pre-training language models. We utilize\na large corpus consisting of 450G text from a wide\nrange of popular Chinese applications including\nKandian, Zhihu, Wechat, and Weibo, in various\nﬁelds of news, wiki, and blogs.\nSimilar to most Chinese PLMs, characters are\nused as ﬁne-grained tokens due to the language na-\nture of Chinese. For coarse-grained tokens, We use\nQQSeg which is a segmentation tool with an open\nAPI to perform segmentation on text, and the seg-\nmented words are treated as coarse-grained tokens.\nFor the construction of vocabularies, we follow\nGoogle’s Chinese BERT and include 21,128 to-\nkens in the ﬁne-grained vocabulary. And in the\ncoarse-grained vocabulary, we calculate the to-\nken frequencies and trimmed out tokens with fre-\nquency lower than 8, resulting in 210,946 tokens.\nNote that in order to alleviate the out-of-vocabulary\n(OOV) problem, all tokens in the ﬁne-tuned vocabu-\nlary are also included in coarse-grained vocabulary.\nFor English, a corpus with 6.2 million docu-\nments (18.9G compressed text) from Wikipedia is\nleveraged to pre-train the model. We ﬁrst perform\nsub-word tokenization with BPE algorithm (Sen-\nnrich et al., 2015) on the English text, where the\nproduced words and sub-words constitute the ﬁne-\ngrained vocabulary of28,996 tokens. In the coarse-\ngrained vocabulary, we treat high-frequency words\nas coarse-grained tokens, resulting in 136,630 to-\nkens in total, which also include all tokens in the\nﬁne-grained vocabulary for the OOV concern.\n4.2 Benchmarks\nThe evaluation of the pre-trained models is con-\nducted on various downstream NLU tasks. In\nour experiments, all the Chinese PLMs are eval-\nuated on Chinese Language Understanding Eval-\nuation (CLUE) (Liang Xu, 2020) which is a com-\nprehensive language understanding benchmark de-\nveloped for Chinese containing 9 natural language\nunderstanding tasks. Within the 9 tasks, there are\ntwo single-sentence classiﬁcation tasks that are\nTNEWS and IFLYTEK, four sentence-pair classiﬁ-\ncation tasks that are AFQMC, OCNLI, CLUEWSC\nand CSL, and three question answering tasks that\nare CMRC2018, CHID, and C3. Note that OC-\nNLI has replaced CMNLI since Oct 22, 2020. We\ncompare the model performance by reporting the\nperformance score of each task and the average\nscore of all tasks.\nFor English tasks, we use the SuperGLUE bench-\nmarks (Wang et al., 2019a) which is an extension\nof GLUE (Wang et al., 2019b) consisting of a col-\nlection of 8 NLU tasks of higher difﬁculty for com-\nprehensively evaluating the performance of English\nPLMs. SuperGLEU contains a word sense disam-\nbiguation task (WiC), two textual entailment tasks\n(CB and RTE), two reasoning tasks (COPA and\nWSC), and three question answering tasks (BoolQ,\nMultiRC, and ReCoRD).\n4.3 Experiment Setup\nIn order to demonstrate the general applicability\nand effectiveness of our framework, we have imple-\nmented three different pre-trained language models\nwith our method including BERT, ALBERT and\nGPT, and compare the performances with their cor-\nresponding single-grained baseline methods.\nFor BERT and ALBERT, we follow the “base”\nstructure in (Devlin et al., 2019) with an encoder of\n12 layers. And the GPT model in our experiment\nis also made up of a 12-layer transformer decoder.\n1388\nModel Avg. TNEWS IFLYTEK AFQMC OCNLI CLUEWSC CSL CMRC2018 CHID C3\n- acc. acc. acc. acc. acc. acc. EM. acc. acc.\nBERT 71.12 66.62 60 .64 71 .74 73 .45 72 .92 84 .01 73 .08 75 .52 62.08\nBERT-LICHEE 73.92 67.94 60 .94 73 .65 75 .85 81 .03 84 .51 75 .84 77 .65 67.84\nALBERT 67.27 64.45 57 .54 71.35 69.19 68 .80 83 .00 68 .06 68 .97 54.04\nALBERT-LICHEE69.30 66.31 58 .29 70.95 71.05 70 .39 83 .31 72 .87 71 .93 58.65\nGPT 67.41 67.52 60 .84 69 .83 70 .91 63 .76 83 .12 62 .53 73 .31 54.84\nGPT-LICHEE 68.73 68.40 61 .06 70 .00 72 .01 66 .01 83 .23 64 .57 74 .02 59.27\nTable 1: Comparison of the model performances on the CLUE tasks. BERT- LICHEE, ALBERT-LICHEE and\nGPT-LICHEE stand for the multi-grained version of the model with our method incorporated. The average score\nof the nine CLUE tasks are also given.\nModel Avg. TNEWS IFLYTEK AFQMC OCNLI CLUEWSC CSL CMRC2018 CHID C3\n- acc. acc. acc. acc. acc. acc. EM. acc. acc.\nArcher-24E-SINGLE79.19 69.54 62 .27 77.26 83 .57 90.00 85 .73 75 .65 85 .66 83.04\nrobertaselfrun 79.46 69.10 63 .92 76 .09 80 .40 93.10 87.27 79 .20 88 .80 77.29\nUER-ensemble 79.64 72.20 64.00 76 .82 80 .80 90 .35 85 .83 79 .15 86 .03 81.60\nBERTs 79.66 69.94 63 .92 76 .77 82 .09 88 .97 86 .77 80.50 89 .51 78.44\nLICHEE-ensemble 80.06 70.50 64.15 76.98 81 .30 90 .69 87.40 79.80 87 .51 82.22\nTable 2: Top-5 models on the CLUE benchmark leaderboard where our ensemble model achieves the state-of-the-\nart performance on the averaged CLUE score. These results are grabbed from the ofﬁcial CLUE website1on Jan 8,\n2021.\nThen, we apply the following training setting to\nthe training process of all three models. For better\nscalability in large batch, we adopt LAMB (You\net al., 2019) to replace Adam (Kingma and Ba,\n2014) as the optimizer with a batch size of 768\nand a learning rate of 2e−4. We ﬁrst train the\nmodel for 1M steps using 128 as the maximum\nsequence length, and increase the maximum length\nto 512 for another 100k steps, for better capturing\nthe long distance dependencies. To enhance the\ntraining efﬁciency, we adopt mix-precision training\ntechnique (Micikevicius et al., 2017) during pre-\ntraining, which are performed on 4 Nvidia V100\ngpus.\nWe have also implemented a LICHEE-enhanced\nensemble model based on BERT-large to partici-\npate in the CLUE benchmark competition. During\ntraining, we adapt the batch size to 1,024 and the\nmaximum sequence lengths at the ﬁrst and second\nstage are set to 256 and 512. And 64 Nvidia V100\ngpus are used to train the model.\nFor the evaluation of each task, we derive 6 re-\nsults with different random seeds and report the\naverage performance in this paper.\n4.4 Main Results\nIn table 1, we adopt our multi-grained pre-training\nmethod on three pre-trained language models:\n1https://www.cluebenchmarks.com/rank.html\nBERT, ALBERT, and GPT, and compare them\nwith their single-grained baselines on CLUE bench-\nmark. From the results, we can see that our\nmethod achieves signiﬁcant performance gains by\nexploiting the multi-grained information of the\ntext input. The averaged CLUE scores of our\nmulti-grained BERT-LICHEE, ALBERT-LICHEE\nand GPT-LICHEE are 73.92, 69.30 and 68.73 re-\nspectively, producing signiﬁcant absolute improve-\nments of 2.80, 2.03, and 1.32 compared to their\nsingle-grained baseline models. Aside from the\nimprovement on the averaged CLUE score, it\nis also worth to mention that our multi-grained\nBERT-LICHEE and GPT-LICHEE outperforms\ntheir single-grained baselines on all 9 NLU tasks\nin CLUE, while the ALBERT-LICHEE model also\nbeat the single-grained ALBERT in8 out of 9 tasks,\nwhich provides strong evidence that the beneﬁts\nof our method are generally applicable to differ-\nent pre-trained language models and diverse NLU\ntasks.\nIn order to further investigate the potential of\nLICHEE, we apply it on an ensemble model based\non BERT-large and participate in the CLUE bench-\nmark competition. As demonstrated in table 2,\nour method outperforms all other candidates on\nthe average score of 9 CLUE tasks by a signiﬁ-\ncant margin, and also achieves the state-of-the-art\nperformance on two individual NLU tasks of IFLY-\n1389\nModel (BERT) Avg. TNEWS IFLYTEK AFQMC OCNLI CLUEWSC CSL CMRC2018 CHID C3\n- acc. acc. acc. acc. acc. acc. EM. acc. acc.\nSG 71.12 66.62 60 .64 71 .74 73 .45 72 .92 84 .01 73 .08 75 .52 62.08\nSG (WWM) 72.24 66.87 60 .55 72 .62 74 .41 74 .07 84 .12 75 .22 77 .74 64.60\nMG (CAT 384+384)72.86 68.11 61.09 72 .33 75 .08 75 .26 84 .48 75 .35 77 .84 66.17\nMG (CAT 256+512)72.94 67.63 61.55 71.96 74 .97 76 .54 84 .16 75 .31 78 .17 66.15\nMG (CAT 512+256)73.08 67.88 61 .06 73 .07 75 .84 74 .45 84.74 74.44 78.29 67.91\nMG (MEAN) 73.22 67.85 60 .99 73 .44 75.97 76.31 84 .52 75 .54 77 .84 66.53\nLICHEE 73.92 67.94 60 .94 73.65 75.85 81.03 84.51 75.84 77.65 67.84\nTable 3: Ablation study of different pre-training strategies with BERT model on CLUE dataset. Two single-grained\n(SG) baselines and ﬁve multi-grained (MG) methods (LICHEE and its variants) with different ways of integrating\nthe ﬁne-grained and coarse-grained representations are evaluated.\nTEK and CSL. This results further proves that our\nmulti-grained pre-training method is able to bring\nsigniﬁcant improvements on the representation abil-\nity of language models and is generally effective to\na wide range of downstream NLU tasks.\nThe reason ofLICHEE’s success is that we adopt\na multi-grained pre-training strategy to model the\ncontextual information of the input text to leverage\nthe advantages from both granularities, where ﬁne-\ngrained token representations are easier to learn\nconsidering the sufﬁcient training samples, and\ncoarse-grained tokens are more complete as lex-\nical units and provide more accurate contextual\ninformation. Furthermore, in our framework, the\ncombination of the multi-grained information is re-\nalized on the embedding level so that we can keep\nthe model structure unaltered, showing that the ben-\neﬁts are achieved entirely through the information\ngains caused by multi-grained pre-training other\nthan model-level modiﬁcations.\n4.5 Ablation Analysis\nWe have conducted ablation analysis on CLUE\nbenchmarks with BERT, to evaluate the impact\nof our multi-grained design, as well as perform a\ncomprehensive study on the different methods of\nintegrating the multi-grained embedding. Table 3\nlists the performance of model variants with differ-\nent training strategies, including two single-grained\nmethods and ﬁve multi-grained methods.\nThe original single-grained BERT whose mask-\ning scheme is solely based on ﬁne-grained tokens\ngives an average CLUE score of71.12. The Whole\nWord Masking (WWM) technique (Cui et al., 2019)\nperforms masking operations on continuous ﬁne-\ngrained tokens that form a coarse-grained token and\nimproves the performance to 72.24. Note that al-\nthough WWM utilizes coarse-grained token bound-\nary information during the masking operations, it\ndoes not explicitly train representations for coarse-\ngrained tokens. Therefore, we treat WWM also as\na single-grained pre-training method.\nFor multi-grained pre-training methods, we have\nconducted experiments to explore ﬁve different\napproaches of combining embedding representa-\ntions of ﬁne-grained and coarse-grained tokens, in-\ncluding concatenating the embedding vectors with\ndifferent dimension settings, and integrating them\nwith mean-pooling and max-pooling. For the con-\ncatenation approaches, we keep the dimension of\nthe concatenated multi-grained embeddings to 768\nto align with the baseline models, and apply three\nsettings to adjust the dimensions of ﬁne-grained\nand coarse-grained embedding correspondingly to\n(384, 384), ( 256, 512) and ( 512, 256). Empiri-\ncally, we discover that the three concatenation set-\ntings achieve similar performances, while having\nlarger embedding vectors for ﬁne-grained tokens\nand smaller embedding vectors for coarse-grained\ntokens produces a slightly better performance of\n73.08 average CLUE score.\nExploiting mean-pooling to integrate the multi-\ngrained information gives more performance gains\ncompared with concatenation methods and reaches\n73.22 average CLUE score, which may be at-\ntributed to the greater number of embedding param-\neters, as pooling methods do not require a shrink\non the embedding dimension and allow both ﬁne-\ngrained and coarse-grained embedding dimension\nto stay 768. Finally, LICHEE with the max-pooling\nincorporated outperforms all the fore-mentioned\napproaches, attains an overall score of 73.92, and\nachieves the best score on 3 out of 9 CLUE tasks,\ndue to its capability of extracting more representa-\ntive features. Especially for the task of CLUEWSC,\nLICHEE acquires an accuracy of 81.03 while the\nsecond best method only reaches76.54. We believe\nthis is because the small training set of CLUEWSC\n1390\nModel Avg. BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC\n- acc. acc. acc. EM. EM. acc. acc. acc.\nBERT-WWM 63.64 77.13 79 .76 62 .83 24 .88 65 .20 70 .88 64 .50 63 .94\nBERT-LICHEE 65.53 77.98 88 .21 63 .00 25 .41 67 .50 71 .91 65 .45 64 .81\nTable 4: Comparison between our multi-grained BERT- LICHEE and the single-grained BERT-WWM on Super-\nGLUE tasks.\nModel FLOPs Speedup\nBERT 43.5B 1.0x\nAMBERT 87.0B 0.5x\nLICHEE 43.5B 1.0x\nTable 5: Comparison of FLOPs and speedup among the\nsingle-grained BERT, AMBERT, and our method.\nwith only 532 examples makes it more dependent\non powerful pre-trained representations, so that the\nadvantage of the max-pooling method is ampliﬁed.\nOverall, we can see from table 3 that all multi-\ngrained pre-training methods outperform the single-\ngrained baselines by a signiﬁcant margin, which\nagain proves that our idea of incorporating multi-\ngrained information during the pre-training phase\nis efﬁcacious and can beneﬁt model performance\nconsiderably.\n4.6 Inference Speed Analysis\nWe have also studied the inference speed of\nLICHEE and compare it with the original single-\ngrained BERT and another multi-grained method\nAMBERT.\nTable 5 gives a brief comparison in terms of\nFLOPs and speedup, tested on a binary classiﬁca-\ntion task with 512 sequence length. FLOPs indi-\ncates the number of ﬂoating-point operations that\nthe model performs for a single process, where gen-\nerally speaking, the higher the model’s FLOPs is,\nthe slower the inference speed will be.\nWe can see that the FLOPs of the AMBERT is\n87.0 billion, twice the number of the single-grained\nBERT. It means the inference time of AMBERT is\nalmost doubled, which can cost a lot more time and\nresources, and often can be unacceptable for real-\nworld applications. Meanwhile, our multi-grained\nmethod produces a model with 43.5 billion FLOPs\nwith a negligible increase compared with the single-\ngrained baseline, because the additional operations\nonly include an embedding lookup operation for\ncoarse-grained tokens and a max-pooling operation\nto integrate the ﬁne-grained and coarse-grained em-\nbedding vectors. In summary, LICHEE can pro-\nduce signiﬁcant performance gains with negligible\nextra inference time needed.\n4.7 English Tasks\nWe have also conducted experiments on Super-\nGLUE benchmarks to evaluateLICHEE on English\nlanguage tasks, and compared it with the single-\ngrained baseline: BERT-WWM (Cui et al., 2019).\nAs shown in table 4, the BERT model pre-trained\nwith our multi-grained method outperforms the\nsingle-grained BERT-WWM on all8 SuperGLUE\ntasks, and attains an average score of65.53 surpass-\ning the baseline by 1.89. This improvement over\nBERT-WWM demonstrates that the effectiveness\nof LICHEE is attributed greatly to the information\ngain of its multi-grained representations, more than\njust token boundary information. We also notice\nthat, similar to the CLUEWSC task, a huge in-\ncrease of 8.45 on accuracy is achieved for the CB\ndataset of 250 training samples, because our pre-\ntraining method leverages the information gains of\nmulti-grained tokens and produces more accurate\nrepresentations, which is especially effective on\ntasks with small training data.\nThis result evidently illustrates that LICHEE is\nnot only effective on tasks of character based lan-\nguage like Chinese that highly relies on correct\ntokenizations, but can also produce signiﬁcant im-\nprovements on languages that are naturally tok-\nenized such as English.\n5 Conclusion\nIn this paper, we have proposed a novel multi-\ngrained method for language model pre-training\nnamed LICHEE, which can be applied to both auto-\nregressive and auto-encoding PLMs. In our method,\nthe ﬁne-grained embeddings and the coarse-grained\nembeddings are separately learned and integrated\nas the multi-grained embeddings, which is then\npassed into the encoder of the language model. Ex-\nperiments show that LICHEE can signiﬁcantly en-\n1391\nhance the model performance by a great margin on\ndownstream tasks of both Chinese and English, and\nsigniﬁcantly improve the inference speed compared\nto the prior multi-grained method.\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In International Conference on Learning Rep-\nresentations.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nXiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han,\nArianna Yuan, and Jiwei Li. 2019. Is word segmen-\ntation necessary for deep learning of chinese repre-\nsentations? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3242–3252.\nLu Li Hai Hu Chenjie Cao Weitang Liu Junyi Li\nYudong Li Kai Sun Yechen Xu Yiming Cui Cong Yu\nQianqian Dong Yin Tian Dian Yu Bo Shi Jun Zeng\nRongzhao Wang Weijian Xie Yanting Li Yina Pat-\nterson Zuoyu Tian Yiwen Zhang He Zhou Shaowei-\nhua Liu Qipeng Zhao Cong Yue Xinrui Zhang\nZhengliang Yang Zhenzhong Lan Liang Xu, Xuan-\nwei Zhang. 2020. Clue: A chinese language un-\nderstanding evaluation benchmark. arXiv preprint\narXiv:2004.05986.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, et al. 2017. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30:5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in neural informa-\ntion processing systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019b.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\n1392\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2019. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Con-\nference on Learning Representations.\nXinsong Zhang and Hang Li. 2020. Ambert: A pre-\ntrained language model with multi-grained tokeniza-\ntion. arXiv preprint arXiv:2008.11869.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8973211050033569
    },
    {
      "name": "Lexical analysis",
      "score": 0.7599660158157349
    },
    {
      "name": "Language model",
      "score": 0.6934518814086914
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6353673934936523
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6202746033668518
    },
    {
      "name": "Natural language processing",
      "score": 0.6095121502876282
    },
    {
      "name": "Inference",
      "score": 0.5877469182014465
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5675347447395325
    },
    {
      "name": "Natural language understanding",
      "score": 0.5566728115081787
    },
    {
      "name": "Representation (politics)",
      "score": 0.47574493288993835
    },
    {
      "name": "Natural language",
      "score": 0.44374990463256836
    },
    {
      "name": "Machine learning",
      "score": 0.37143605947494507
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    }
  ],
  "cited_by": 5
}