{
  "title": "QuEst: Graph Transformer for Quantum Circuit Reliability Estimation",
  "url": "https://openalex.org/W4307929416",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3083326127",
      "name": "Wang, Hanrui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224526899",
      "name": "Liang, Zhiding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2039403692",
      "name": "Gu Jiaqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2687880597",
      "name": "Li ZiRui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2386970823",
      "name": "Ding, Yongshan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354500975",
      "name": "Jiang, Weiwen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350148245",
      "name": "Shi, Yiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221599880",
      "name": "Pan, David Z.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222063450",
      "name": "Chong, Frederic T.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1972140387",
      "name": "Han Song",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4312121133",
    "https://openalex.org/W2787740662",
    "https://openalex.org/W3043042778",
    "https://openalex.org/W4313169488",
    "https://openalex.org/W2017477211",
    "https://openalex.org/W2755255888",
    "https://openalex.org/W2103956991",
    "https://openalex.org/W4221151504",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2895744665",
    "https://openalex.org/W2170620327",
    "https://openalex.org/W4289785452",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3126796861",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W1532834996",
    "https://openalex.org/W3200960482",
    "https://openalex.org/W3122552506",
    "https://openalex.org/W3198322448",
    "https://openalex.org/W2559394418",
    "https://openalex.org/W3098780233",
    "https://openalex.org/W2161685427",
    "https://openalex.org/W1978553093",
    "https://openalex.org/W4281479206",
    "https://openalex.org/W3199970312",
    "https://openalex.org/W3107868845",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W3205601995",
    "https://openalex.org/W3092618035",
    "https://openalex.org/W4280557388",
    "https://openalex.org/W3103870741",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W3163320491",
    "https://openalex.org/W4311126788",
    "https://openalex.org/W4224942751",
    "https://openalex.org/W3100843411",
    "https://openalex.org/W4286898450",
    "https://openalex.org/W4306815925",
    "https://openalex.org/W1608473410",
    "https://openalex.org/W3198793880",
    "https://openalex.org/W2950805899",
    "https://openalex.org/W3139255692",
    "https://openalex.org/W2084652510",
    "https://openalex.org/W2781738013",
    "https://openalex.org/W4289143671",
    "https://openalex.org/W3213289548",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4301287831",
    "https://openalex.org/W4285723986",
    "https://openalex.org/W1568345435",
    "https://openalex.org/W4280646646"
  ],
  "abstract": "Among different quantum algorithms, PQC for QML show promises on near-term devices. To facilitate the QML and PQC research, a recent python library called TorchQuantum has been released. It can construct, simulate, and train PQC for machine learning tasks with high speed and convenient debugging supports. Besides quantum for ML, we want to raise the community's attention on the reversed direction: ML for quantum. Specifically, the TorchQuantum library also supports using data-driven ML models to solve problems in quantum system research, such as predicting the impact of quantum noise on circuit fidelity and improving the quantum circuit compilation efficiency. This paper presents a case study of the ML for quantum part. Since estimating the noise impact on circuit reliability is an essential step toward understanding and mitigating noise, we propose to leverage classical ML to predict noise impact on circuit fidelity. Inspired by the natural graph representation of quantum circuits, we propose to leverage a graph transformer model to predict the noisy circuit fidelity. We firstly collect a large dataset with a variety of quantum circuits and obtain their fidelity on noisy simulators and real machines. Then we embed each circuit into a graph with gate and noise properties as node features, and adopt a graph transformer to predict the fidelity. Evaluated on 5 thousand random and algorithm circuits, the graph transformer predictor can provide accurate fidelity estimation with RMSE error 0.04 and outperform a simple neural network-based model by 0.02 on average. It can achieve 0.99 and 0.95 R$^2$ scores for random and algorithm circuits, respectively. Compared with circuit simulators, the predictor has over 200X speedup for estimating the fidelity.",
  "full_text": "QuEst: Graph Transformer for Quantum Circuit Reliability\nEstimation\nTorchQuantum Case Study for Robust Quantum Circuits\nHanrui Wang1, Pengyu Liu2, Jinglei Cheng3, Zhiding Liang4, Jiaqi Gu5, Zirui Li6, Yongshan Ding7,\nWeiwen Jiang8, Yiyu Shi4, Xuehai Qian3, David Z. Pan5, Frederic T. Chong9, Song Han1\n1MIT 2Carnegie Mellon University 3 Purdue Univ. 4Univ. of Notre Dame 5Univ. of Taxes at Austin6Rutgers Univ. 7Yale Univ.\n8George Mason Univ. 9Univ. of Chicago\nhttps://qmlsys.mit.edu\nAbstract\nQuantum Computing has attracted much research attention be-\ncause of its potential to achieve fundamental speed and efficiency\nimprovements in various domains. Among different quantum al-\ngorithms, Parameterized Quantum Circuits (PQC) for Quantum\nMachine Learning (QML) show promises to realize quantum ad-\nvantages on the current Noisy Intermediate-Scale Quantum (NISQ)\nMachines. Therefore, to facilitate the QML and PQC research, a re-\ncent python library called TorchQuantum has been released. It can\nconstruct, simulate, and train PQC for machine learning tasks with\nhigh speed and convenient debugging supports. Besides quantum\nfor ML, we want to raise the communityâ€™s attention on the reversed\ndirection: ML for quantum. Specifically, the TorchQuantum library\nalso supports using data-driven ML models to solve problems in\nquantum system research, such as predicting the impact of quan-\ntum noise on circuit fidelity and improving the quantum circuit\ncompilation efficiency.\nThis paper presents a case study of the ML for quantum part\nin TorchQuantum. Since estimating the noise impact on circuit\nreliability is an essential step toward understanding and mitigating\nnoise, we propose to leverage classical ML to predict noise impact\non circuit fidelity. Inspired by the natural graph representation\nof quantum circuits, we propose to leverage a graph transformer\nmodel to predict the noisy circuit fidelity. We firstly collect a large\ndataset with a variety of quantum circuits and obtain their fidelity\non noisy simulators and real machines. Then we embed each circuit\ninto a graph with gate and noise properties as node features, and\nadopt a graph transformer to predict the fidelity. We can avoid\nexponential classical simulation cost and efficiently estimate fidelity\nwith polynomial complexity.\nEvaluated on 5 thousand random and algorithm circuits, the\ngraph transformer predictor can provide accurate fidelity estimation\nwith RMSE error 0.04 and outperform a simple neural network-\nbased model by 0.02 on average. It can achieve 0.99 and 0.95 R2\nscores for random and algorithm circuits, respectively. Compared\nwith circuit simulators, the predictor has over 200Ã—speedup for\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nICCAD â€™22, October 30â€“ November 3, 2022, San Diego, CA, USA\nÂ© 2022 Copyright held by the owner/author(s).\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nâ¾‡0Ê«\nâ¾‡0Ê«\nâ¾‡0Ê«\nâ¾‡0Ê«\nX\nX\nRZ\nCNOT\nCNOT\nCNOT\nSX\nInput Node Measurement Node X Node\nCNOT NodeRZ Node SX Node\nEmbed \nCircuit  \nto Graph\nGraph Transformer\n12/3/2019Electronic Device\nï¬le:///Users/hanruiwang/Downloads/noun_CPU_2880768.svg1/1Created by Muhammad Khoirul Amfrom the Noun Project\nGraph Info\n0 1 3\n2\nDevice   Noise Info\nPredicted Fidelity\nTorch Quantum\nTorchQuantum\nQuantum Device\nFigure 1: The proposed fidelity prediction framework. The\nquantum circuit is firstly embedded into a graph in which the\nnodes are gates and edges are execution orders. The feature\nvector on each node contains the device noise information,\nsuch as gate error rates. The graph is processed by a graph\ntransformer in TorchQuantum to estimate circuit fidelity.\nestimating the fidelity. The datasets and predictors can be accessed\nin the TorchQuantum library.\n1 Introduction\nQuantum Computing (QC) presents a new computational paradigm\nthat has the potential to address classically intractable problems\nwith much higher efficiency and speed. It has been shown to have\nan exponential or polynomial advantage in various domains such\nas combinatorial optimization [12], molecular dynamics [27, 35],\nand machine learning [ 3, 7, 16, 21, 28, 29, 38, 57], etc. By virtue\nof breakthroughs in physical implementation technologies, QC\nhardware has advanced quickly during the last two decades. Mul-\ntiple QC systems with up to 127 qubits have been released re-\ncently [15, 19, 23, 39].\nDespite the promising developments, it is still anticipated that\nbefore we enter the fault-tolerant era, we will spend a number of\nyears in the Noisy Intermediate Scale Quantum (NISQ) [36] stage.\nIn this stage, the qubits and quantum gates suffer from significant\nerror (around 10âˆ’3), which is the bottleneck towards quantum ad-\nvantages. Therefore, Parameterized Quantum Circuits (PQC) have\nattracted increasingly more attention thanks to their flexibility in\narXiv:2210.16724v2  [quant-ph]  27 Jan 2025\nSpearman Correlation=0.993\nFidelity\nPST\nPST can provide accurate \nestimation for fidelity\nFigure 2: Relationship between fidelity and PST of random\ncircuits. The PST of a circuit is obtained by appending the\ninverse circuit to the original one and executing. There is a\nstrong positive correlation (Spearman = 0.993) between the\ntwo metrics, so it is sufficient for the predictor to output PST.\nthe circuit architecture (ansatz) and parameters that provides vast\nspace for noise mitigation and optimizations.\nTo facilitate the robust quantum circuits, especially parameter-\nized quantum circuits for quantum machine learning, the TorchQuan-\ntum library is released, which supports easy construction, simula-\ntion, and fast parameter training of PQCs. Several noise mitigation\ntechniques, such as noise-aware ansatz search [ 49], noise-aware\nparameter training [50], gradient pruning for robust on-chip train-\ning [51], are also supported in the library.\nAlthough plenty of work has been focusing on quantum for ma-\nchine learning with parameterized circuits, little research explores\nanother direction â€“ using machine learning to solve quantum sys-\ntem research problems. To fill this vacancy, the TorchQuantum\nlibrary also provides multiple classical machine learning models to\nperform quantum compilation, reliability estimation tasks, etc.\nIn this paper, we show one case study of machine learning for\nquantum â€“ using graph transformer models to estimate the quan-\ntum circuit fidelity under noise impact, as shown in Figure 1. Due to\nthe limited quantum resources, it is highly desirable to estimate the\ncircuit performance before submitting it for execution. If the fidelity\nof a circuit is lower than a threshold, running it on real quantum\nmachines will not generate any meaningful result. One straightfor-\nward method is to perform circuit simulation on noisy simulators,\nbut the exponentially increasing cost is prohibitive for circuits with\nmany qubits. Therefore, in this work, we propose a polynomial\ncomplexity method in which a data-driven graph transformer is\ntrained to perform fidelity estimation. Intuitively, estimating the\nfidelity does not require precisely computing the complete density\nmatrix. So there are opportunities that the data-driven method can\nprovide accurate enough estimation with low computation costs.\nIn fact, there have been works on predicting circuit reliability using\nsimple machine learning models [31]. However, it considers neither\nany graph information of the circuit nor the noise information and\nthus has less accurate predictions in experimental results.\nThe first step of the framework is to collect a large dataset con-\ntaining various randomly generated circuits and circuits from com-\nmon quantum algorithms. We run the circuits on both noisy simu-\nlators and real quantum machines. On simulators, we change the\nproperties of the qubits, such as T1 and T2, and the error rates of\ngates to diversify the data samples. The dataset contains over 20\nthousand samples on simulators and 25 thousand samples on real\nquantum machines. In order to reduce the overhead of collecting a\ndataset, we use the â€œProbability of Successful Trials\" (PST) [44] as\nthe proxy for the fidelity following the setting in [31]. Specifically,\nfor each circuit, we will concatenate the inverse of the circuit to\nthe original one and execute. Since the original quantum state is\nall zero, the ground truth output of the concatenated circuit will\nstill be all zero. Therefore, the PST will be the frequency of getting\nall zero bit-string. The dataset is embedded in the TorchQuantum\nlibrary and can be easily accessed for future studies.\nSecondly, motivated by the fact that quantum circuits are graphs ,\nwe propose to leverage a graph transformer to process the circuit\ninformation. The nodes of the graph are the quantum gates, in-\nput qubits, and measurements. The edges are determined by the\nsequence of gate executions. The feature vector on each node con-\ntains gate type, qubit index, qubit T1, T2 time, gate error rate, etc.,\nto capture operation and noise information. In one layer of the\ngraph transformer, the attention layer will capture the correlations\nbetween each node and its neighbors according to the graph and\ncompute the updated feature vector. Several fully-connected layers\nare appended at the end to regress the circuit PST.\nOverall, we present a case study on using graph transformer\nmodels in the TorchQuantum library to estimate circuit fidelity\nunder noise. The contributions are summarized as below:\nâ€¢A dataset for circuit fidelity on various noisy simula-\ntors and real machines is presented and embedded in the\nTorchQuantum library to facilitate research on reliability\nestimations. It contains 20K simulation samples and 25K real\nmachine samples.\nâ€¢A graph transformer model is constructed and trained to\nprocess the quantum circuit graph and feature vectors on\nnodes to provide accurate fidelity prediction.\nâ€¢Extensive evaluations on around 2 thousand circuits on\nnoisy simulators and 3 thousand circuits on real machines\ndemonstrate the high accuracy of the predictor. It achieves\n0.04 RMSE and over 0.95 R2 scores with 200Ã—speedup over\ncircuit simulators.\n2 Related Work\n2.1 Quantum Basics\nA quantum bit (qubit) can be in a linear combination of the two\nbasis states 0 and 1, in contrast to a classical bit, |ğœ“âŸ©= ğ›¼|0âŸ©+ğ›½|1âŸ©,\nfor ğ›¼,ğ›½ âˆˆC,where |ğ›¼|2 +|ğ›½|2 = 1. Only one of the 2ğ‘› states can\nbe stored in a classical ğ‘›-bit register. However, we can employ an\nğ‘›-qubit system to describe a linear combination of 2ğ‘› basis states\ndue to the ability to build a superposition of basis states. To perform\ncomputation on a quantum system, we use a quantum circuit to\nmanipulate the state of qubits. A given quantum system can be\nexpressed as a Hamiltonian function and solved by SchrÃ¶dingerâ€™s\nequation, and these operational steps can be performed by vari-\nous quantum gates . Results of a quantum circuit are obtained by\nqubit readout operations called measurements, which collapse a\nqubit state |ğœ“âŸ©to either |0âŸ©or |1âŸ©probabilistically according to the\namplitudes ğ›¼ and ğ›½.\n2.2 Quantum Errors\nQuantum errors are one of the most significant challenges that\nNISQ-era quantum computing experiences. On real quantum ma-\nchines, errors occur because of the interactions between qubits\nand the environment, control errors, and interference from the\nenvironment [4, 25, 33]. Qubits undergo decoherence error over\ntime, and quantum gates introduce operation errors (such as coher-\nent/stochastic errors) into the system. These systems need to be\ncharacterized [33] and calibrated [20] frequently to mitigate the\nquantum noise impacts.\nThe errors seriously interfere with the function of quantum cir-\ncuits and form obstacles to further optimization of quantum circuits.\nA number of noise mitigation techniques have been developed to\nattenuate negative effects [ 8, 10, 17, 26, 29, 37, 49, 50]. [50] pro-\nposes a framework to improve the quantum circuitsâ€™ robustness by\nmaking them aware of noise. It consists of three main techniques:\ninjection of gate errors, regularization, and normalization of mea-\nsurement outcomes. Another literature [ 29] integrates the gate\nerror characteristics into the mapped quantum circuit to improve\nrobustness.\n2.3 Fidelity Estimation and Prediction\nIn order to validate and characterize the states generated by a\nquantum computer, it is crucial to estimate the fidelity of quan-\ntum states [13, 56]. However, calculating fidelities is already quite\ncomputationally expensive. Numerous efforts have been made to\naddress this problem in the past few years. Variational quantum\nalgorithms have been adopted by recent works to perform fidelity\nestimation [5, 6, 42]. Machine learning-based and statistical meth-\nods are also proposed to estimate the fidelity [31, 58, 60]. In addition,\nâ€œclassical shadowâ€ is proposed for more efficient tomography [18],\nwhich can also benefit fidelity estimation. The works mentioned\nabove present various methods for estimating fidelity. Fewer works,\nhowever, have focused on predicting fidelity given a quantum cir-\ncuit and a noisy backend. [31] derives a fidelity prediction model\nusing polynomial fitting and a shallow neural network. The noisy\nbackend is considered as a black box in that work. [34, 43] calculate\nfidelity with a simple equation and use it as a metric to optimize the\ncompilation workflow. These methods are inaccurate and do not\naccount for the structure of quantum circuits or noisy backends.\n2.4 Randomized Benchmarking\nPlenty of techniques have been developed to estimate the fidelity\nof quantum circuits and identify errors in NISQ computers, and\nthey can provide indicators of the quality of quantum circuits and\ndirections for further improvement of quantum hardware. Among\nthem, randomized benchmarking is the most prominent [ 24, 32,\n33] one. Randomized benchmarking can estimate the fidelity of\ncertain gates or circuits and further characterize noises to very high\naccuracy in the presence of state preparation and measurement\nerrors. However, randomized benchmarking has several limitations.\nFor example, it usually requires strong assumptions about the error\npattern, such as assuming the errors are gate-independent, and the\nbenchmarked gate set must have group structures.\n2.5 Transformers\nThe attention [1, 46] based Transformer models [48, 55] have pre-\nvailed in sequence modeling. Recently, it is also widely applied in\nother domains such as vision transformer [11] for computer vision\nand graph transformer (graph attention networks) [47, 52, 54, 59]\nfor graph learning. The graph transformer leverages the attention\nmechanism to generate the updated features of the next layer for\neach node. The Query vectors come from the center node, while\nthe Key and Value vectors are calculated from the neighboring\nnodes. Recently, several variants of traditional transformers have\nbeen proposed, including AGNN, which removes all the FC lin-\near layers in the model [45], Modified-GAT [40], which proposes\ngate-augmented attention for better feature extraction, Linear At-\ntention [41], which reduces the complexity of attention to linear\ncost, and Hardware-Aware Transformer [53] that adjusts the archi-\ntecture according to the hardware latency feedback.\n3 Circuit Fidelity Dataset\nIn classical computing, training datasets must be fed into the ma-\nchine learning algorithms before validation datasets (or testing\ndatasets) can be employed to validate the modelâ€™s interpretation of\nthe input data. However, when dealing with the fidelity prediction\nproblem, we do not have an off-the-shelf dataset that can be used\nto train and evaluate different methods. To address this problem,\nwe present a scheme for generating datasets and incorporating the\ngathered datasets into TorchQuantum in order to provide relevant\nresearchers with appropriate starting points.\n3.1 Metrics\nIn order to accurately estimate the â€œsuccess rateâ€ of quantum circuits\non noisy devices, the conception of fidelity is introduced, which is a\nmeasure of the â€œclosenessâ€ of two quantum states. In noisy quantum\ncomputing, fidelity is adopted to illustrate the difference between\nthe quantum states generated by noisy devices and those generated\nby noiseless classical simulations. Obtaining the fidelity of quantum\ncircuits is, however, computationally costly â€“ exponential to the\nqubit number. Intricate tomography would be required to â€œrestoreâ€\nor â€œdescribeâ€ quantum states[18]. To solve such a problem, we adopt\nthe idea of â€œProbability of Successful Trials\" (PST) [44] as the proxy\nof fidelity.\nğ‘ƒğ‘†ğ‘‡ = #ğ‘‡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ ğ‘¤ğ‘–ğ‘¡â„ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ ğ‘ ğ‘ğ‘šğ‘’ğ‘ğ‘ ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’\n#ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ \nInstead of measuring the fidelity of quantum circuits, we count the\nproportion of unchanged qubits (all zeros) after concatenating the\ncircuits with their inverse. For concatenated circuits, the proportion\nwill be one if we conduct simulations on a noise-free simulator.\nWe compare the PST with fidelity for 1400 quantum circuits on\nsimulators. As shown in Figure 2, they exhibit a strong correlation\nwith a Spearman correlation coefficient of 0.993. Therefore, we can\nconclude that PST can provide accurate fidelity estimations.\nX CNOT\nSX\nX\nX\nRZ\nCNOT\nCNOT\nSX\nâ€¦\nX\nX\nRZ\nCNOT\nCNOT\nSX SXâ€ \nCNOTâ€ \nCNOTâ€ \nXâ€ \nXâ€ \nRZâ€ \nGenerate Random Circuits with Basis Gates Concatenate Inverse Circuit Obtain PST\nTable 2\nObtained Noise-free\n|0000Ê« 0.7 1\n|0001Ê« 0.08 0\n|0010Ê« 0.04 0\n|0011Ê« 0 0\n|0100Ê« 0 0\n|0101Ê« 0 0\n|0110Ê« 0.01 0\n|0111Ê« 0 0\n|1000Ê« 0 0\n|1001Ê« 0.03 0\n|1010Ê« 0.01 0\n|1011Ê« 0 0\n|1100Ê« 0.02 0\n|1101Ê« 0 0\n|1110Ê« 0 0\n|1111Ê« 0.11 0\n0.0\n0.5\n1.0\n|0000\nÊ«\n|0001\nÊ«\n|0010\nÊ«\n|0011\nÊ«\n|0100\nÊ«\n|0101\nÊ«\n|0110\nÊ«\n|0111\nÊ«\n|1000\nÊ«\n|1001\nÊ«\n|1010\nÊ«\n|1011\nÊ«\n|1100\nÊ«\n|1101\nÊ«\n|1110\nÊ«\n|1111\nÊ«\n0.11\n0.00\n0.00\n0.02\n0.00\n0.01\n0.03\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.04\n0.08\n0.70\nObtained Noise-free\nPST = 0.7\nRandom Circuits Design Space\nTarget Device Coupling Map\nTranspile\nObtain Inverse Circuit\nRun on Simulator with \nDiffernet Noise Models\n or on Real Machines\nOutput [oiginal circuit, device\nnoise info, PST] Pairs\nFigure 3: Overview of the dataset generation process. i) Prepare random circuits by mixing basis gate: RZ, SX, X, CNOT, namely\nconstructing native circuits. ii) Inverse all the gates in the transpiled native circuit and then concatenate the inverse circuit\nto the original transpiled native circuit. iii) Calculate PST by dividing the number of trials (shots) with all zero state by the\nnumber of total trials (shots).\nNoise Level\nNumber of CNOT Gates\nNumber of Gates\nCircuit Depth\nNumber of Qubits\nPSTPSTPSTPSTPST\nFigure 4: Dataset property profiling.\n3.2 Dataset Generation\nAs shown in Figure 3, the generation of random datasets can be bro-\nken down into three major steps: initial random circuit generation,\nconcatenation with inverse circuits, and PST calculation.\nNative Circuit Construction. In the first step, random gates\nare generated from the basis gate set {RZ, SX, X, CNOT} and assigned\nto quantum circuits to create an initial version of random circuits.\nSingle-qubit gates are assigned to all possible qubits, and two-qubit\ngates are assigned to all available connections in the quantum\ndevice. After finishing the assignments, the circuits will be compiled\nto eliminate duplicated gates. As a result, we consider the number\nof qubits and gates, the coupling map of quantum devices, and\nthe number of random circuits as parameters during the random\ncircuits generation process.\nConcatenation of Inverse Circuit. Furthermore, the obtained\nrandom circuits will be concatenated with their inverse. The inverse\ncircuit is obtained by reversing the gate sequence of the original\ncircuit and replacing each gate with its inverse gate, as shown\nin Figure 3 middle. The purpose of concatenation is to use PST\nrather than fidelity as our metrics, thereby allowing us to avoid the\ncomputationally expensive state tomography. The detailed reasons\nare elaborated on in the Section 3.1. For example, assuming a circuit\nconsisting of a CNOT gate and an X gate, after concatenation, the\ncircuit will be â€œCNOT + X + barrier + X + CNOTâ€. A barrier is placed to\nprevent gate cancellation. The concatenated circuits will be sent to\nthe backends to obtain PSTs. Note that the dataset only contains the\noriginal circuits without concatenation. As a result, if we need to\nevaluate a new quantum circuit, we will feed it to the ML predictor.\nThen the predicted PST for the concatenated circuit will be returned\nby the ML model, which is highly correlated with the circuitâ€™s\nfidelity.\nPST Calculation. The concatenated circuits will then be passed\nto noisy backends to calculate the PST. We begin with the default ini-\ntial state |00....0âŸ©, and the PST represents the proportion of |00....0âŸ©\nin the output distribution. Our prediction model takes into account\nthe information from both quantum circuits and noisy backends.\nAs a result, the quantum circuits are simulated on backends with\ndiffering noise levels to create our datasets. The backendsâ€™ noise\nconfigurations are derived from real NISQ machines, with random\nconstants to change the noise levels.\nâ¾‡0Ê«\nâ¾‡0Ê«\nâ¾‡0Ê«\nX\nRZ\nCNOT\nCNOT\nSX\nInput Node\nX Node\nRZ Node\nMeasurement Node\nCNOT Node\nSX Node\nShared weights\nEmbed Circuit  \nto Graph (with \ndevice noise info)\nGraph Transformer Attention Layer\n01\n2\n3\n4\nNode 1, 2, 3, 4 are neighbors of node 0\nGlobal Average Pooling\nFC Regressor Layers\nPredicted Fidelity\n12/3/2019Electronic Device\nï¬le:///Users/hanruiwang/Downloads/noun_CPU_2880768.svg1/1Created by Muhammad Khoirul Amfrom the Noun Project0 1 3\n2\nFeature \nVectors\n0\n1 \n2 \n3 \n4\nCompute \nQuery\n0\n1 \n2 \n3 \n4\n1 \n2 \n3 \n4\nCompute \nKey\nCompute \nValue\nV\nK\nQ\nCompute \nAttention \nScores\nD\nD\nD\nD\nD\nSoftmax\nAttention prob x Value\nOutput \nNew Features\n4 4 D\nFigure 5: Overview of the Graph Transformer for fidelity prediction. (i) Generate the graph according to quantum circuit, and\nthen generate the feature vector for each of the node according to the quantum device noise information. (ii) For one Graph\nTransformer layer, we perform graph attention layer to extract information and captures the neighboring correlations. The\nweight matrices are shared across all nodes. (iii) Finally, a regressor containing several FC layers regresses the circuit PST (an\napproximation of fidelity).\n3.3 Dataset Properties\nFigure 4 depicts the relationships between PST and various circuit\nand backend properties. We can anticipate a lower PST as the num-\nber of gates increases. The PST numbers are also influenced by the\nnumber of CNOT gates, circuit depth, and noise level. To cover these\ndimensions, we create random datasets with varying numbers of\nqubits, gates, and backends of different noise levels. The PSTs of\nthese circuits are simulated on backends with five different noise\nlevels. As a result, the random circuits datasets contain 10000 data\npoints on noisy simulators. We also measure the PSTs of these cir-\ncuits from five different real NISQ machines. The dataset on real\nmachines contains around 25000 data points. The performance of\nour graph transformer model on random circuits is demonstrated\nin Figure 7. In addition, our datasets include circuits used in quan-\ntum algorithms such as quantum error correction [30], variational\nquantum eigensolver [ 22], Grover search [ 14], quantum fourier\ntransform [9], quantum approximate optimization algorithm [12]\nand quantum teleportation [2]. We select a total of 30 circuits de-\nrived from quantum algorithms. The simulations are also carried\nout on noisy simulators with varying noise levels to collect data\npoints. The performance of our graph transformer model on these\ncircuits is demonstrated in Figure 8.\n4 Predictor\nThe dataset introduced in the previous section enables a data-driven\napproach to learning the PST from circuit and noise features. This\nsection will continue to present a case study of a deep learning\nmodel, graph transformer, for circuit PST prediction. Figure 5 shows\nthe overview of the framework. A gate graph is firstly extracted\nfrom the circuit. Then the node features are generated according\nto the gate type, noise information, etc. Next, a graph transformer\ncontaining attention operations is introduced to process the node\nAlgorithm 1: Attention in Graph Transformer\nInput: Circuit graph: ğº with ğ¾ nodes\nLength of feature vector: ğ·\nNode features: H âˆˆRğ¾Ã—ğ·\nQuery, Key, Value weights{Wğ‘„,Wğ¾,Wğ‘‰}âˆˆ Rğ·Ã—ğ·\nQ = Wğ‘„ Â·H\nK = Wğ¾ Â·H\nV = Wğ‘‰ Â·H\ndo in parallel\nfor ğ‘– = 0 to ğ¾ do\nObtain neighbor nodes Nğ‘– according to ğº\nğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘–ğ‘— = Qğ‘– Â·Kğ‘‡\nğ‘—,ğ‘— âˆˆNğ‘–\nattention_score = attention_score/sqrt(|Nğ‘–|)\nattention_prob = Softmax(attention_score)\nattention_outğ‘– = Ã\nğ‘—âˆˆNğ‘– ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘ğ‘Ÿğ‘œğ‘ğ‘–ğ‘— Â·ğ‘‰ğ‘—\nattention_outğ‘– âˆˆRğ·\nend\nend\nOutput: attention_out âˆˆRğ¾Ã—ğ·\nfeatures and neighboring relations. Finally, a PST regression layer\noutputs the predicted values.\n4.1 Graph Construction\nWe firstly use directed acyclic graphs (DAG) to represent the topol-\nogy of quantum circuits. Each node represents one qubit, quantum\ngate, or measurement. Edges represent the time-dependent order of\ndifferent gates. One example of extracting the graph from the circuit\nis presented on the left of Figure 5. The connectivity can be encoded\ninto an adjacent matrix. With the TorchQuantum framework, the\nDAG can be conveniently converted from the circuit.\n0, 1, 0, 0, 0, 0,\nOne-Hot \nNode Type\n0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\nOne-Hot \nGate Qubit\n140.3, 200.2, 120.5, 230.6, 0.004,\nFirst Qubit \nT1, T2\nSecond Qubit \nT1, T2\nGate Error \nRate\n0.03,\nReadout \nError 0 - 1\n0.05,\nReadout \nError 1 - 0\n11\nGate \nIndex\nFigure 6: Node feature vector.\nNoisy Simulator IBM Geneva IBM Hanoi\nIBM Montreal IBM Mumbai IBM Toronto\nGround Truth PST Ground Truth PST Ground Truth PST\nGround Truth PSTGround Truth PSTGround Truth PST\nPredicted PSTPredicted PST\nPredicted PST\nPredicted PST\nPredicted PST\nPredicted PST\nR2=0.991 R2=0.955 R2=0.969\nR2=0.977 R2=0.971 R2=0.984\nFigure 7: Scatter plots of PST of randomly generated circuits on noisy simulators and 5 real machines. Our transformer can\nprovide accurate estimations of PST with R 2 higher than 0.95.\n4.2 Node Features\nFor each node in the graph, we generate a vector representing the\nfeatures. The features include gate type, target qubit index, T1 and\nT2 of the target qubit, gate error, and gate index, as shown in Fig-\nure 6. In our experiments, we set the maximum qubit number to\n10, and the feature vector has a length of 24. The first 6 numbers\nare one-hot vectors describing the gate type: initial input qubit,\nmeasurement, RZ, X, SX, or CNOT. Then we use 10 numbers to de-\nscribe the target gate qubit(s). If this gate acts on the ğ‘–th qubit, the\nğ‘–th number of the vector is set to 1 and otherwise 0. That also ap-\nplies to multi-qubit gates. Then we use the following 7 numbers to\ndescribe the calibration information of the backend with the follow-\ning format: [T1, T2 for the first target qubit, T1, T2 for the second\ntarget qubit, gate error rate, readout error10, readout error01]. If a\nfeature is not applicable for a particular node, the corresponding\nvalue is set to 0. For example, RZ acts on only one qubit, so T1\nand T2 for the second target qubit are set to 0. Since RZ is not a\nmeasurement, readout error10 and readout error01 are set to 0 also.\nThe last number is used to encode the index of the node. The whole\nfeatur vector is illustrated in Figure 6.\n4.3 Graph Transformer\nTo process graphs with node features, we propose to use a graph\ntransformer as shown in Figure 5 right. The transformer contains\nmultiple layers, each containing the attention operation. The atten-\ntion is described in Algorithm 1. the Query, Key, and Value vectors\nfor each node are computed with shared weights. Then for one\nnode, we fetch the Key vectors of its neighboring nodes and com-\npute Query Ã—Keyğ‘‡. The outputs are attention scores which are\nthen normalized according to the square root of the number of\nneighbors. Softmax is adopted to normalize the attention scores.\nThe output is called attention probability because the values add\nup to one. The probability vector is then employed as weights to\nperform a weighted sum of the Value vectors of the neighboring\nnodes. The output has the same dimension as the input feature\nof the center node. After that, we perform a residual connection\nbetween input and output of attention with a layer normalization.\nTable 1: Prediction RMSE vs. Whether Using Global Features\nFeatures Noisy Simulator IBM Geneva IBM Hanoi\nw/o Global Features 0.0239 0.0757 0.0506\nw/ Global Features 0.0232 0.0723 0.0500\nThe output will be the feature vector of the next layer. Note that\ncomputations on all nodes are done simultaneously.\nAfter multiple transformer layers, we obtain a learned feature\non each node, with its neighbors influenced. If deep enough, each\nnode can access to features of all nodes in the graph. Finally, we\nperform a global average pooling of the node features and obtain\nan aggregated node feature vector. Then a regressor with three FC\nlayers is appended to output the final regressed PST. Besides node\nfeature, we also leverage global features , representing the circuit\ndepth, width, and counts of RZ, X, SX, and CNOT gates. The global\nfeature vector is concatenated with the aggregated node feature\nvector and fed to the regressor.\nThe computational complexity of the proposed graph trans-\nformer is polynomial to qubit number since the overall number of\ngates is typically polynomial to qubit number.\n5 Evaluation\n5.1 Evaluation Methodology\nModel and Training Setups. In the default setup, we use two\nlayers of graph transformers. The embedding dimension is 24 since\nwe have 24 features. The dimension for the Query, Key, and Value\nvectors is also 24. We use single-head attention layers. The global\naverage pooling across nodes generates a single 24 dimensional\nvector as the aggregated feature for a circuit. If global features are\nenabled, we use two FC layers with hidden and output dimensions\nof 12 to pre-process and concatenate it with the aggregated node\nfeature. The concatenated feature is processed with additional three\nFC layers with hidden dimension 128 and output dimension 1. This\noutput is treated as the predicted PST value. We use ReLU activation.\nWe normalize the node features across the dataset by removing\nthe mean and dividing the standard deviation. We then train the\nmodels with Adam optimizer for 500 epochs with a constant learn-\ning rate of 10âˆ’2, weight decay 10âˆ’4, batch size 2500 and MSE loss.\nThen we choose the model that performs best on the validation set\nto test on the test set.\nDataset Setup. For noisy simulators datasets, we have 10000\nrandom circuits and 350 circuits for 30 quantum algorithms each.\nFor real machine datasets, we collect 5000, 5000, 5450, 2750, and\n6750 random circuits for IBM Geneva, IBM Hanoi, IBM Montreal,\nIBM Mumbai, and IBM Toronto, respectively. We split the dataset\ninto three parts, the training set includes 70% data, the validation\nset includes 20% data, and the test set consists of the last 10%.\n5.2 Experimental Results\nFigure 7 shows the scatter plots of transformer predicted PST vs.\nthe ground truth PST for randomly generated circuits on the test\nset. The red dash line is theğ‘¦ = ğ‘¥line. We train one separate model\nfor each of the backend settings. For results on noisy simulators,\nthe points are close to the ğ‘¦ = ğ‘¥ line with an R2 value of 0.991. On\nTable 2: Importance Comparison of Node Features\nFeatures Noisy Simulator IBM Geneva IBM Hanoi\nAll Features 0.0232 0.0723 0.0500\nw/o Gate Error Rate 0.0235 0.0732 0.0501\nw/o Gate Index 0.0247 0.0730 0.0497\nw/o Gate Type 0.0236 0.0742 0.0512\nw/o Qubit Index 0.0239 0.0736 0.0514\nw/o T1&T2 0.0239 0.0707 0.0491\nreal machines, the difficulty is greater than on noisy simulators.\nAlthough the predictor R2 is lower than noisy simulators, they are\nstill higher than 0.95. Furthermore, as in Figure 8, we select 30 repre-\nsentative quantum algorithms as benchmarks and show the scatter\nplots for predicted PST on the test set. Each color represents one al-\ngorithm circuit under different noise models. We train one common\nmodel for the 30 algorithm circuits. The transformer model can\neffectively track the PST value, especially for those spanning a wide\nrange of PST. The overall R2 for 30 benchmarks is 0.9985. We also\nshow the two representative training curves on noisy simulators\nand the real quantum machine IBM Hanoi in Figure 9. The training\nloss converges after around 200 steps. The convergence speed on\nreal machine data is slightly slower than the noisy simulator data\nand has a higher final RMSE (around 0.05).\nBesides, we also compare our transformer-based model with the\nsimple NN model adapted from [ 31] as in Figure 10. The simple\nNN model only takes 116 features as input, which include circuit\ndepth, width, and counts of RZ, X, SX, and CNOT gates, single-qubit\ngate counts on each qubit, and two-qubit gate counts on each qubit\npair. It uses 3 FC layers with hidden dimension 128 and ReLU\nactivation to regress the PST. We compare the RMSE on the test set\nfor random circuits on 6 benchmarks and 30 algorithm circuits on\nnoisy simulators. On average, the RMSE of the transformer model is\n0.02 better than the simple NN model. On the algorithm circuit, the\ngap is even more apparent â€“ up to 0.05. The R2 on algorithm circuits\nwith transformer is also much higher than simple NN (0.9985 vs.\n0.9110). That shows the effectiveness of involving circuit graph\ninformation in the model.\n5.3 Analysis\nIn Table 1, we show the effectiveness of concatenating the global\nfeatures to the aggregated node features. Adding global features can\nreduce the RMSE loss on the test set with negligible computational\noverhead. The effectiveness is especially significant in IBM Geneva,\nwhere the RMSE is reduced by around 0.003.\nTable 2 further performs an ablation study on the importance of\neach feature in the node feature vectors. We remove one feature\nwhile keeping all other features in each experiment and then train\nthe model again to obtain the results and report the RMSE loss\non the test set. The bold values mark the largest two losses when\nremoving different features. We can see that removing â€˜Qubit Indexâ€™\nseverely degrades the accuracy in all three backends. This may be\nbecause the qubit index helps the transformer model know the\nlocation of the gate. Removing â€˜Gate Typeâ€™ also has a substantial\nGround Truth PST Ground Truth PST Ground Truth PST\nPredicted PST\nPredicted PST\nPredicted PST\nFigure 8: Scatter plots of circuit PST of 30 quantum algorithms on noisy simulators. Our transformer can provide accurate\nestimations of PST with R 2 0.99.\nTraining Steps Training Steps\nRMSE\nRMSE\nNoisy Simulator IBM Hanoi\nFinal RMSE: 0.02239 Final RMSE: 0.04877\nFigure 9: Training curves of transformer models on noisy\nsimulators and IBM Hanoi datasets for random circuits.\nArbitrary state preparation\nSimple baseline Graph transformer\nRandom Noisy Simulator 0.03398304202 0.02316096113\nGeneva 0.07224595911 0.07189490001\nHanoi 0.04983630086 0.04994256536\nMontreal 0.05334645321 0.05000003947\nMumbai 0.05476363433 0.05304554138\nToronto 0.07481231747 0.04626975102\n0.06493324780618780.0123552357277555\nAverage 0.05770299354374110.0438098562996794\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nRandom \nNoisy Simulator\nRandom \nGeneva\nRandom \nHanoi\nRandom \nMontreal\nRandom \nMumbai\nRandom \nToronto\nAlgorithm \nNoisy Simulator\nAverage\n0.04\n0.01\n0.05\n0.05\n0.05\n0.05\n0.07\n0.02\n0.06\n0.06\n0.07\n0.05\n0.05\n0.05\n0.07\n0.03\nSimple NN Graph Transformer\nRMSE\nFigure 10: The proposed graph transformer-based model can\noutperform the simple NN model on various benchmarks.\nTable 3: Prediction RMSE vs. Transformer Layer Number\n# Layers Noisy Simulator IBM Geneva IBM Hanoi\n1 0.0230 0.0720 0.0491\n2 0.0232 0.0723 0.0500\n3 0.0232 0.0719 0.0500\nnegative impact since the model will not know the node type. We\nalso observe that removing some features even improves the accu-\nracy. This only happens on the real machine backend and maybe\nbecause of the large fluctuations of noise on the real backend.\nTable 3 shows the relationship between the number of trans-\nformer layers with the prediction performance. We find that dif-\nferent model sizes do not greatly impact accuracy. On the noisy\nsimulator and IBM Hanoi datasets, the one-layer model slightly\nTable 4: Prediction RMSE vs. Number of Shots\nShots IBM Jakarta IBM Lima IBM Manila\n512 0.0287 0.0266 0.0440\n1024 0.0352 0.0246 0.0403\n2048 0.0305 0.0217 0.0410\n4096 0.0294 0.0250 0.0399\nTable 5: Runtime of Simulation vs. Transformer Predictor\nSimulation Predictor (bsz=1) Predictor (bsz=10)\nLatency (s) 5.57E-1 2.79E-3 3.28E-4\noutperforms the others, while on the IBM Geneva dataset, the three-\nlayer model is the best. Therefore, in most of our experiments, we\nuse a two-layer model as a trade-off.\nFurthermore, we show the performance differences under differ-\nent numbers of shots in noisy simulators as in Table 4. As the shots\nincrease, the precision of the ground truth PST in the training set\nwill be improved and will converge to the true PST when the shots\nare infinity. However, counter-intuitively, we find that increasing\nshot number does not guarantee better model accuracy.\nFinally, besides theoretical proof of lower computation complex-\nity of our model, we also perform empirical runtime comparisons\nas shown in Table 5. We run both the circuit simulator and the\ngraph transformer on an Nvidia 3090 GPU with 24GB memory for\n1000 sampled circuits from the random circuit dataset, and report\naverage runtime. We select batch size 1 or 10 for the graph trans-\nformer predictor. The predictor achieves 200Ã—and 1.7KÃ—speedup\nover classical simulators to obtain the PST for batch size 1 and 10,\nrespectively. That demonstrates the much higher efficiency of our\ngraph transformer-based predictor.\n6 Conclusion\nUsing machine learning to optimize quantum system problems is\npromising. This paper presents a case study of the ML for Quantum\npart of TorchQuantum library. We are inspired by thata quantum\ncircuit is a graph and propose to leverage agraph transformer model\nto predict the circuit fidelity under the influence of quantum noise.\nFirst, we collect a large dataset of randomly generated circuits and\nalgorithm circuits, and measure their fidelity on simulators and\nreal machines. A graph with feature vectors for each node is con-\nstructed according to the circuit. The graph transformer processes\nthe circuit graph and calculates the anticipated fidelity value for the\ncircuit. Instead of the exponential cost of performing whole circuit\nsimulations, we can effectively evaluate the fidelity under polyno-\nmial complexity. The datasets and models have been integrated\ninto the TorchQuantum library, and we hope they can accelerate\nresearch in the ML and Quantum field.\nAcknowledgment\nWe thank National Science Foundation, MIT-IBM Watson AI Lab,\nand Qualcomm Innovation Fellowship for supporting this research.\nThis work is funded in part by EPiQC, an NSF Expedition in Com-\nputing, under grants CCF-1730082/1730449; in part by STAQ under\ngrant NSF Phy-1818914; in part by DOE grants DE-SC0020289 and\nDE-SC0020331; and in part by NSF OMA-2016136 and the Q-NEXT\nDOE NQI Center. We acknowledge the use of IBM Quantum ser-\nvices for this work.\nReferences\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine\ntranslation by jointly learning to align and translate. arXiv:1409.0473 (2014).\n[2] Charles H Bennett, Gilles Brassard, Claude CrÃ©peau, et al. 1993. Teleporting an\nunknown quantum state via dual classical and Einstein-Podolsky-Rosen channels.\nPhysical review letters 70, 13 (1993), 1895.\n[3] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe,\nand Seth Lloyd. 2017. Quantum machine learning. Nature (2017).\n[4] Colin D Bruzewicz, John Chiaverini, Robert McConnell, and Jeremy M Sage. 2019.\nTrapped-ion quantum computing: Progress and challenges. Applied Physics\nReviews 6, 2 (2019), 021314.\n[5] Marco Cerezo, Alexander Poremba, Lukasz Cincio, and Patrick J Coles. 2020.\nVariational quantum fidelity estimation. Quantum 4 (2020), 248.\n[6] Ranyiliu Chen, Zhixin Song, Xuanqiang Zhao, and Xin Wang. 2021. Variational\nquantum algorithms for trace distance and fidelity estimation. Quantum Science\nand Technology 7, 1 (2021), 015019.\n[7] Jinglei Cheng, Haoqing Deng, and Xuehai Qia. 2020. Accqoc: Accelerating\nquantum optimal control based pulse generation. In ISCA (2020). IEEE, 543â€“555.\n[8] Jinglei Cheng, Hanrui Wang, Zhiding Liang, Yiyu Shi, Song Han, and Xuehai\nQian. 2022. TopGen: Topology-Aware Bottom-Up Generator for Variational\nQuantum Circuits. arXiv preprint arXiv:2210.08190 (2022).\n[9] Don Coppersmith. 2002. An approximate Fourier transform useful in quantum\nfactoring. arXiv preprint quant-ph/0201067 (2002).\n[10] Poulami Das, Christopher A Pattison, Srilatha Manne, Douglas M Carmean,\nKrysta M Svore, Moinuddin Qureshi, and Nicolas Delfosse. 2022. AFS: Accurate,\nFast, and Scalable Error-Decoding for Fault-Tolerant Quantum Computers. In\nHPCA (2022). IEEE, 259â€“273.\n[11] Alexey Dosovitskiy, Lucas Beyer, et al. 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv:2010.11929 (2020).\n[12] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. 2014. A quantum approxi-\nmate optimization algorithm. arXiv preprint arXiv:1411.4028 (2014).\n[13] AndrÃ¡s GilyÃ©n and Alexander Poremba. 2022. Improved Quantum Algorithms\nfor Fidelity Estimation. arXiv preprint arXiv:2203.15993 (2022).\n[14] Lov K Grover. 1996. A fast quantum mechanical algorithm for database search.\nIn STOC (1996). 212â€“219.\n[15] Jeremy Hsu. 2018. Intel49. In Intel.\n[16] Zhirui Hu, Peiyan Dong, Zhepeng Wang, Youzuo Lin, Yanzhi Wang, and Weiwen\nJiang. 2022. Quantum Neural Network Compression. ICCAD (2022).\n[17] Fei Hua, Yanhao Chen, Yuwei Jin, Chi Zhang, Ari Hayes, Youtao Zhang, and\nEddy Z Zhang. 2021. Autobraid: A framework for enabling efficient surface code\ncommunication in quantum computing. In Micro (2021). 925â€“936.\n[18] Hsin-Yuan Huang, Richard Kueng, and John Preskill. 2020. Predicting many\nproperties of a quantum system from very few measurements. Nature Physics\n16, 10 (2020), 1050â€“1057.\n[19] IBM. 2022. IBM Unveils Breakthrough 127-Qubit Quantum Processor. In IBM.\n[20] Qiskit IBM. 2021. https://qiskit.org/textbook/ch-quantum-hardware/calibrating-\nqubits-pulse.html\n[21] Weiwen Jiang, Jinjun Xiong, and Yiyu Shi. 2021. A co-design framework of\nneural networks and quantum circuits towards quantum advantage. Nature\ncommunications 12, 1 (2021), 1â€“13.\n[22] Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus\nBrink, Jerry M Chow, and Jay M Gambetta. 2017. Hardware-efficient variational\nquantum eigensolver for small molecules and quantum magnets. Nature (2017).\n[23] Julian Kelly. 2018. A Preview of Bristlecone, Googleâ€™s New Quantum Processor.\nIn Google.\n[24] Emanuel Knill, Dietrich Leibfried, Rolf Reichle, Joe Britton, R Brad Blakestad,\nJohn D Jost, Chris Langer, Roee Ozeri, Signe Seidelin, and David J Wineland. 2008.\nRandomized benchmarking of quantum gates. Physical Review A 77, 1 (2008).\n[25] Philip Krantz, Morten Kjaergaard, Fei Yan, Terry P Orlando, Simon Gustavsson,\nand William D Oliver. 2019. A quantum engineerâ€™s guide to superconducting\nqubits. Applied Physics Reviews 6, 2 (2019), 021318.\n[26] Sebastian Krinner, Nathan Lacroix, Ants Remm, Agustin Di Paolo, Elie Genois,\nCatherine Leroux, Christoph Hellings, Stefania Lazar, Francois Swiadek, Johannes\nHerrmann, et al. 2022. Realizing repeated quantum error correction in a distance-\nthree surface code. Nature 605, 7911 (2022), 669â€“674.\n[27] Zhiding Liang, Jinglei Cheng, Hang Ren, Hanrui Wang, Fei Hua, Yongshan Ding,\nFred Chong, Song Han, Yiyu Shi, and Xuehai Qian. 2022. PAN: Pulse Ansatz on\nNISQ Machines. arXiv preprint arXiv:2208.01215 (2022).\n[28] Zhiding Liang, Hanrui Wang, Jinglei Cheng, Yongshan Ding, Hang Ren, Xuehai\nQian, Song Han, Weiwen Jiang, and Yiyu Shi. 2022. Variational quantum pulse\nlearning. QCE (2022).\n[29] Zhiding Liang, Zhepeng Wang, Junhuan Yang, Lei Yang, Yiyu Shi, and Weiwen\nJiang. 2021. Can noise on qubits be learned in quantum neural network? a case\nstudy on quantumflow. In ICCAD (2021). IEEE, 1â€“7.\n[30] Daniel A Lidar and Todd A Brun. 2013. Quantum error correction. Cambridge\nuniversity press.\n[31] Ji Liu and Huiyang Zhou. 2020. Reliability Modeling of NISQ- Era Quantum\nComputers. In IISWC (2020). 94â€“105.\n[32] Easwar Magesan, Jay M Gambetta, and Joseph Emerson. 2011. Scalable and\nrobust randomized benchmarking of quantum processes. Physical review letters\n106, 18 (2011), 180504.\n[33] Easwar Magesan, Jay M Gambetta, and Joseph Emerson. 2012. Characterizing\nquantum gates via randomized benchmarking. Physical Review A 85, 4 (2012),\n042311.\n[34] Shin Nishio, Yulu Pan, Takahiko Satoh, Hideharu Amano, and Rodney Van Me-\nter. 2020. Extracting success from ibmâ€™s 20-qubit machines using error-aware\ncompilation. JETC (2020) 16, 3 (2020), 1â€“25.\n[35] Alberto Peruzzo, Jarrod McClean, et al. 2014. A variational eigenvalue solver on\na photonic quantum processor. Nature communications 5, 1 (2014), 1â€“7.\n[36] John Preskill. 2018. Quantum Computing in the NISQ era and beyond. Quantum\n2 (2018), 79.\n[37] Gokul Subramanian Ravi, Kaitlin N Smith, Pranav Gokhale, Andrea Mari, Nathan\nEarnest, Ali Javadi-Abhari, and Frederic T Chong. 2022. Vaqem: A variational\napproach to quantum error mitigation. In HPCA (2022). IEEE, 288â€“303.\n[38] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. 2014. Quantum support\nvector machine for big data classification. Physical review letters 113, 13 (2014),\n130503.\n[39] rigetti. 2021. Rigetti Quantum. In rigetti.\n[40] Seongok Ryu, Jaechang Lim, Seung Hwan Hong, and Woo Youn Kim. 2018.\nDeeply learning molecular structure-property relationships using attention-and\ngate-augmented graph convolutional network. arXiv:1805.10988 (2018).\n[41] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. 2021.\nEfficient attention: Attention with linear complexities. In WACV(2021).\n[42] Kok Chuan Tan and Tyler Volkoff. 2021. Variational quantum algorithms to\nestimate rank, quantum entropies, fidelity, and Fisher information via purity\nminimization. Physical Review Research 3, 3 (2021), 033251.\n[43] Swamit S Tannu and Moinuddin Qureshi. 2019. Ensemble of diverse mappings:\nImproving reliability of quantum computers by orchestrating dissimilar mistakes.\nIn Micro (2019). 253â€“265.\n[44] Swamit S Tannu and Moinuddin K Qureshi. 2019. Not all qubits are created\nequal: a case for variability-aware policies for NISQ-era quantum computers. In\nASPLOS (2019). 987â€“999.\n[45] Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. 2018.\nAttention-based graph neural network for semi-supervised learning. arXiv\npreprint arXiv:1803.03735 (2018).\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\nNeed. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc.\n[47] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[48] Hanrui Wang et al. 2020. Efficient algorithms and hardware for natural language\nprocessing. Massachusetts Institute of Technology (2020).\n[49] Hanrui Wang, Yongshan Ding, Jiaqi Gu, Yujun Lin, David Z Pan, Frederic T\nChong, and Song Han. 2022. Quantumnas: Noise-adaptive search for robust\nquantum circuits. In HPCA (2022). IEEE, 692â€“708.\n[50] Hanrui Wang, Jiaqi Gu, Yongshan Ding, Zirui Li, Frederic T Chong, David Z Pan,\nand Song Han. 2022. QuantumNAT: Quantum Noise-Aware Training with Noise\nInjection, Quantization and Normalization. DAC (2022).\n[51] Hanrui Wang, Zirui Li, Jiaqi Gu, Yongshan Ding, David Z Pan, and Song Han.\n2022. QOC: Quantum On-Chip Training with Parameter Shift and Gradient\nPruning. DAC (2022).\n[52] Hanrui Wang, Kuan Wang, Jiacheng Yang, Linxiao Shen, Nan Sun, Hae-Seung\nLee, and Song Han. 2020. GCN-RL Circuit Designer: Transferable Transistor\nSizing with Graph Neural Networks and Reinforcement Learning. In DAC 2020.\n[53] Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan,\nand Song Han. 2020. Hat: Hardware-aware transformers for efficient natural\nlanguage processing. ACL (2020).\n[54] Hanrui Wang, Jiacheng Yang, Hae-Seung Lee, and Song Han. 2018. Learning to\ndesign circuits. NeurIPS MLSys Workshop (2018).\n[55] Hanrui Wang, Zhekai Zhang, and Song Han. 2021. Spatten: Efficient sparse\nattention architecture with cascade token and head pruning. In HPCA 2021.\n[56] Qisheng Wang, Zhicheng Zhang, Kean Chen, Ji Guan, Wang Fang, and Ming-\nsheng Ying. 2021. Quantum algorithm for fidelity estimation. arXiv preprint\narXiv:2103.09076 (2021).\n[57] Zhepeng Wang, Zhiding Liang, Shanglin Zhou, Caiwen Ding, Yiyu Shi, and\nWeiwen Jiang. 2021. Exploration of quantum neural architecture by mixing\nquantum neuron designs. In ICCAD (2021). IEEE, 1â€“7.\n[58] Xiao-Dong Yu, Jiangwei Shang, and Otfried GÃ¼hne. 2022. Statistical Methods\nfor Quantum State Verification and Fidelity Estimation. Advanced Quantum\nTechnologies (2022), 2100126.\n[59] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim.\n2019. Graph transformer networks. NeurIPS 32 (2019).\n[60] Xiaoqian Zhang, Maolin Luo, Zhaodi Wen, Qin Feng, Shengshi Pang, Weiqi\nLuo, and Xiaoqi Zhou. 2021. Direct fidelity estimation of quantum states using\nmachine learning. Physical Review Letters 127, 13 (2021), 130503.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7104431390762329
    },
    {
      "name": "Quantum circuit",
      "score": 0.6445578932762146
    },
    {
      "name": "Electronic circuit",
      "score": 0.5468567609786987
    },
    {
      "name": "Fidelity",
      "score": 0.46666043996810913
    },
    {
      "name": "Computer engineering",
      "score": 0.4383500814437866
    },
    {
      "name": "Quantum",
      "score": 0.41836294531822205
    },
    {
      "name": "Algorithm",
      "score": 0.40279215574264526
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4010336399078369
    },
    {
      "name": "Quantum computer",
      "score": 0.3850451707839966
    },
    {
      "name": "Quantum error correction",
      "score": 0.21827340126037598
    },
    {
      "name": "Electrical engineering",
      "score": 0.11457329988479614
    },
    {
      "name": "Engineering",
      "score": 0.09413793683052063
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I107639228",
      "name": "University of Notre Dame",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210096112",
      "name": "Rutgers Sexual and Reproductive Health and Rights",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I162714631",
      "name": "George Mason University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    }
  ]
}