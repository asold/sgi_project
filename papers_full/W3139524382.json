{
  "title": "Towards a question answering assistant for software development using a transformer-based language model",
  "url": "https://openalex.org/W3139524382",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5070390965",
      "name": "Liliane do Nascimento Vale",
      "affiliations": [
        "Universidade Federal de Goiás"
      ]
    },
    {
      "id": "https://openalex.org/A5010173851",
      "name": "Marcelo de Almeida Maia",
      "affiliations": [
        "Universidade Federal de Uberlândia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2902567760",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964204609",
    "https://openalex.org/W2995229924",
    "https://openalex.org/W2924175345",
    "https://openalex.org/W2545101277",
    "https://openalex.org/W3081943439",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W2154706669",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Question answering platforms, such as Stack Overflow, have impacted substantially how developers search for solutions for their programming problems. The crowd knowledge content available from such platforms has also been used to leverage software development tools. The recent advances on Natural Language Processing, specifically on more powerful language models, have demonstrated ability to enhance text understanding and generation. In this context, we aim at investigating the factors that can influence on the application of such models for understanding source code related data and produce more interactive and intelligent assistants for software development. In this preliminary study, we particularly investigate if a how-to question filter and the level of context in the question may impact the results of a question answering transformer-based model. We suggest that fine-tuning models with corpus based on how-to questions can impact positively in the model and more contextualized questions also induce more objective answers.",
  "full_text": "arXiv:2103.09423v1  [cs.SE]  17 Mar 2021\nT owards a question answering assistant\nfor software development using a\ntransformer-based language model\nLiliane do Nascimento V ale˚:\nF ederal University of Catalão˚\nInstitute of Biotechnology - Computing Science\nGoiás - Brazil\nEmail: lilianevale@ufg.br\nMarcelo de Almeida Maia\nF ederal University of Uberlândia :\nF aculty of Computing\nMinas Gerais - Brazil\nEmail: marcelo.maia@ufu.br\nAbstract—Question answering platforms, such as\nStack Overﬂow, have impacted substantially how de-\nvelopers search for solutions for their programming\nproblems. The crowd knowledge content available from\nsuch platforms has also been used to leverage software\ndevelopment tools. The recent advances on Natural\nLanguage Processing, speciﬁcally on more powerful\nlanguage models, have demonstrated ability to enhance\ntext understanding and generation. In this context,\nwe aim at investigating the factors that can inﬂuence\non the application of such models for understanding\nsource code related data and produce more interactive\nand intelligent assistants for software development. In\nthis preliminary study , we particularly investigate if a\nhow-to question ﬁlter and the level of context in the\nquestion may impact the results of a question answering\ntransformer-based model. W e suggest that ﬁne-tuning\nmodels with corpus based on how-to questions can\nimpact positively in the model and more contextualized\nquestions also induce more objective answers.\nI. Introduction\nQuestion and answer (Q&A) systems have gained ex-\npressive importance over the past decade for software\ndevelopment. Stack Overﬂow (SO) is the most prominent\nexample of such service, serving more than 100M users\nmonthly , with more than 20M registered questions 1. The\ncrowd knowledge available on Stack Overﬂow dumps have\nenabled several studies that leverage such raw content to\nproduce documentation for APIs [1], recommend posts\n[2, 3] and answers from queries [4, 5], understand social\ninteractions [6], and several others 2.\nRecent advances in language models have opened the\nway for designing more intelligent Q&A conversational\nassistants to perform tasks during the software develop-\nment. T ransformers, a special kind of neural network based\nsolely on attention mechanisms, dispensed recurrence and\nconvolutions and have inﬂuenced a large number of other\nspeciﬁc models [7]. Models such as GPT-2 [8] and BER T\n1 https://stackoverﬂow.com/company\n2 https://bit.ly/3oAbcUa\n[9] have demonstrated ability in several applications of\nNatural Language Processing (NLP), including Q&A.\nIn this paper, we propose the investigation of how\ncould an assistant based on transformers could perform to\nanswer automatically development related questions. W e\ninvestigate the limitations on a model trained with the\nGPT-23 to support the capacity to generate English text,\nﬁne-tuned with text (and code snippets) extracted from\nthe SO dataset. Because question answering is still a hard\nchallenge for English text in general, indeed, our goal is\nnot to provide a functional assistant that could be readily\nused by developers, but instead to understand diﬀerent\nfactors that could impact on the accuracy of the answers,\nin order to guide further similar initiatives.\nIn Section II, we describe the steps to train the assistant.\nIn Section III, some preliminary results are presented and\ndiscussed. In Section IV, we present some related work,\nand ﬁnally in Section V, concluding remarks are presented.\nII. Study Setting\nOur proposal is to build a assistant that performs the\ntask of automatically answering questions using the GPT-\n2 language model ﬁne-tuned with crowd knowledge. Thus,\nin the following sections we describe the steps necessary\nto conduct the proposed preliminary experiment. The data\nset used in this study is available in a Github repository 4.\nA. Goals\nThe goal of this paper is not to propose a conversational\nassistant that could pass on a “T uring test\" customized for\ndevelopers, i.e., developers would ask any question on a\nspeciﬁc programming language or API, the assistant would\nanswer, and the developer would not distinguish if it is\na robot or a human. Our primary goal is to understand\nhow a modern transformer-based language model would\nperform in such a scenario, and more importantly , how\n3 https://beta.openai.com/\n4 https://github.com/lascam-UFU/T owards-a-conversational-\nassistant-for-software-development-using-a-tr.git\ndiﬀerent ﬁne-tuning conﬁgurations and diﬀerent questions\nmay impact the result of the answers. So, our ﬁrst goal is\nto understand if the ﬁne-tuning with how-to-oriented Q&A\npairs [1] produces better answers for how-to questions. Our\nsecond goal is to understand if providing more context in\nthe question, i.e., a speciﬁc class or method, induces more\ncontextualized answers.\nB. Corpus Preparation\nIn order to ﬁne-tune the GPT-2 model, we use questions\nand answers from SO. T o build that corpus, initially ap-\nproximately 20 million questions were recovered from the\n2020 dump[10]. F or reasons of computational constraints,\nwe select only the questions with the tag #java and in\nEnglish language. Java was chosen because of popularity\nand the number of systems developed with the language.\nThen, we selected 865k Q&A pairs with accepted answers\n(AcceptedAnswerId=1). T o structure the corpus for ﬁne-\ntunning, we insert the ă | startoftext| ą , rQUESTIONs,\nrANSWERs and ă |endoftext| ą delimiter tokens for each\nQ&A pair. These delimiters work as supervised training,\nallowing the model to learn the diﬀerence of question and\nanswer, and consequently learn that the desired task is\nquestion answering.\nC. Model T raining\nActually , the model training is a ﬁne-tuning of a pre-\ntrained GPT-2 model. W e use the pre-trained 124M model\nof the GPT-2 that makes it possible to learn the En-\nglish language. W e have chosen the smallest 124M model\n(default) because when ﬁne-tuning GPT-2 because it bal-\nances speed, size, creativity and accuracy . Larger models\nwould require more computational power. T o ﬁne-tune\nthe model, we use the corpus described in the previous\nsection. W e have already built diﬀerent text classiﬁers\nto distinguish the intent of a question in SO [2, 1]. A\nprevalent kind of intent the two set of questions is the\nhow-to questions, where the developers aims at solving a\nprogramming problem and expects a how-to solution. So,\nwe segmented the Q&A pairs into two groups: one group\n(500k posts) containing questions with the word ”how”\non the posts named how-to model, and one group (366k\nposts) not containing “how” on the posts named non-how-\nto model. Our goal is to compare the answers produced\nfor the two ﬁne-tuned models. The ﬁne-tuning loads the\nspeciﬁed the SO corpus and trains for the speciﬁed number\nof steps. W e maintained default of 1.000 steps, since it is\nenough to allow distinct text to emerge and takes just\nabout 45 minutes. W e used Google Colab Notebook 5 with\nlocal connection in a particular server conﬁgured with\n90Gb of RAM and 12 cores, and no GPU.\n5 https://colab.research.google.com/drive/1VLG8e7YSEwypxU-\nnoRNhsv5dW4NfTGce?usp=sharing\nD. Question Deﬁnition\nThe ﬁne-tuned models can be used to produce text given\na preamble. In our case, the task is question answering,\nso the preamble is a question and the generated text\nshould be an answer for that question. In sequence,\nwe established some criteria to deﬁne the questions for\nthis study: 1)W e use ”how to”, ”how can“ preﬁxes to\ncompose questions; 2) W e choose simple tasks on popular\nAPIs/tasks, so we can more easily assess the quality of\nthe answer produced by the model; 3) T o increase the\nspeciﬁcity of the context in the questions, we propose\nincremental questions: from a generic question, we added\ninformation progressively to evaluate how diﬀerent could\nbe the produced answers. So, we considered questions:\n1) without context (in this case, we expect the model to\nsuggest an API/class/method); 2) containing class name\n(in this case, we expect the model to explain the class\nand suggest methods of that class); 3) with a class and\na method name (in this case, we expect the model to\nexplain how to use that speciﬁc method). The following\nquestions considered in the study are shown:\nQ1: How to create a window?\nQ2: How to create a window with JFrame\nQ3:How to create a window with JFrame and set dimension?\nQ4:How to open a DataBase connection?\nQ5:How to open a DataBase connection JDBC?\nQ6:How to open a DataBase connection using JDBC and DriverMa nager?\nQ7: How to open a DataBase connection using JDBC and DriverMa nager\nand getConnection method?\nQ8: How can I insert an element in array at a given position?\nQ9: How to open a SSL connection?\nE. Answer Generation\nF or the generation of responses by the GTP-2 model,\nwe maintained the preﬁx to specify how exactly the text\nmust start (example: [QUESTION]: How to open database\nconnection? \\n [ANSWER]:\"), in this case by the question\nitself. The answers produced had a length equals to 250\ntokens and to generate the amount of text generated:\nsize equals to 5; number of samples equals to 5 and\ntemperature equals to 0.7. An increase of temperature\nwould increase creativity of the text, however we would\nprefer less creativity trying to avoid meaningless solutio ns.\nIII. Resul ts and Discussion\nIn all provided answers, we could not ﬁnd a completely\nright one. Next, we report on some encouraging answers,\ntogether with interesting failures in the answers. Initial ly ,\nwe will consider the following the Q1, Q2, and Q3 from\nthe previous section. Regarding the how-to model, answers\nfor Q1 does not mention speciﬁc classes for instantiating\nwindows as JF rame, JPanel. The best that was generated\nwas a suggestion involving paintComponent (Graphics g) .\npublic class Window {\nprivate String userName; private String password;\npublic Window(String userName, String password) {\nthis.userName = userName; }\n@Override public void paintComponent(Graphics g) {\nsuper.paintComponent(g); }\nAnswers, for both Q2 and Q3, present examples of\ninstantiation and use of the JF rame class, but for Q3 there\nare no speciﬁc methods for determining the window size,\nsuch as the setPreferredSize() or setSize() methods. W e\nshow part of the ﬁrst answer for Q2, which mostly seems\nto make sense, and also has learned adequate syntax,\nalthough with some errors. This is a promising example:\nJFrame mainFrame = new JFrame(\"Main\");\nmainFrame.setDefaultCloseOperation (WindowConstants. EXIT_ON_CLOSE);\nmainFrame.setLayout(new BorderLayout());\nJFrame mainFrame.add(mainFrame);\nmainFrame.add(new JLabel(\"Main\"));\nRegarding the non-how-to model, only Q2 has an answer\nthat refers to the JF rame class and the setSize() method.\nF or Q3, there is a reference to the JPanel class. F or the\ngeneric Q1, there is a mention of the use a non-existent\njavax.swing.W indowBuilder class for building windows.\nThis is an interesting limitation. If in English, the use\nof new entities would be acceptable in some contexts, in\nsoftware development the use of new entities seems to\nrequire the proper deﬁnition of that entity .\nF or the second group of questions related to database\nconnection (Q4, Q5, Q6, and Q7), we have the following\nﬁndings for the how-to model.\nQ4 did not indicate an API to use. The suggested\ncode made a confusion of connection with databases and\nnetwork connection:\nclient.connect(WebSocket.getConnectionContext());\nConnectionFactory factory = new ConnectionFactory();\nconnectionFactory.setConnection(new WebSocketConnect ion(client));\nQ5 speciﬁes that JDBC should be used, and presented\nexamples of instantiating Connection, which is not a class\nbut an interface. Moreover, it proposed a snippet to\nsetup users and password, which are entities related to\na connection. Q6 mentioned the class DriverManager and\nshowed better results containing code examples involving\nConnection and DriverManager classes and getConnec-\ntion() method and SQL:\nDriverManager.getConnection().reset();\npublic class DatabaseTableAdapter extends JTableAdapter {\nprivate static final String SQL_NAME_TYPE = \"sql\";\nQ7 presents examples of use of the DriverM anager\nclass, but all inadequate. The snippet use a non-existent\nsetC onnectionpq method, suggesting the existence of the\ngetC onnectionpq method. Moreover, DriverManager was\nnot used properly accessed based on its static methods.\nDriverManager implements DriverManager {\n@Override public void onDriverManagerChanged\n(DriverManager driver) { driver.setConnection\n(driver.getDriverManager()); driver.setDataSource(da taSource);}\n@Override public void onDataSourceChanged (DataSource so urce) {\nconnection = driver.getDataSource();\nRegarding non-how-to model, Q4, which is more general\nquestion, suggests the use of MySQL and Q5 refers to a\nwebservice. Q6 cites the class DriverManager but with\nimproper and meaningless methods:\nString path = DriverManager.getSerializedPath();\nString uri = driverManager.getSerializedURI();\nString key = driverManager.getSerializedKey()\nQ7, which puts DriverManager and getConnection in\ncontext, also shows the invocation of some methods, but\nalso improperly , although with more sense than in Q6:\nqlConnection.ConnectionBuilder());\ndriverManager.setConnection(driverManager.getConnec tion());\ndriverManager.setConnection (driverManager.getConnec tion());\nDriverManager driverManager = DriverManager.getInstanc e();\nDriverManager.createConnection();\nIn Q8, regarding the insertion of elements in an array:\nHow can I insert an element in array at a given position? ,\nthe how-to model retrieved an example related to arrays,\nbut still not a proper one.\nfor (int i = 0; i num.length; i++) {\nfor (int j = 0; j num.length; j++) {\nif (num[i] & 0) { if (num[j] & 0) {\nint num[num.length - j] = num[j].length } }\nfor (int k = 0; k num.length; k++) {\nif (num[k] & 0) {\nint num[k + j] = num[k + j + 1]; }\nRegarding to the non-how-to model, we observed no\nsatisfactory answers on this group. Q9 is a generic question\n(How to open an SSL connection? ), and both models did\nnot presented adequate answer, but a possible answer that\ncan be a guide to formulate new queries was founded\nin the “how-to\" model: ” Open a ssl connection to your\nhosting provider. The fol lowing steps wil l help you: Open\na ssl connection to the URL of the ssl project. In the ssl\nconnection, add the URL of the ssl project to the URL of\nthe ssl server . “ F or the non-how-to model, the best answer\nwas: “ The Java API is to be used in the context where\nthe client is executing. In this case, you need to use a\nclient-side method to send the request to the server. So\nbasical ly, you don ’t need to do any of this, because it’s just\na wrapper around the client. Y ou should have a single cal l\nto send the request to the server. If you want to use a client-\nside method, then you should use a client-side method to\nsend the request to the server. Note the static method that\nreturns the client-side method. ... ” . Although related to the\nnetwork topic, it lacks meaningfulness.\nW e used two diﬀerent models to evaluate the generated\nanswers, and then compare their performance. W e could\nobserve that the how-to model, compared to the non-\nhow-to, was able to produce more contextualized answers,\nsometimes containing legible pieces of code and consistent\nexplanation. One possible reason for that better perfor-\nmance may related to the structure of the question. W e\nselected posts that could contain the word ”how“ in any\npart of the text. Actually , we observed that most posts\nstarted with the word ”how“ . Thus, it is possible that this\nstructure allowed the model to better relate the intent of\nthe questions, inducing their semantic understanding.\nRegarding to our second goal, we observed that the\ngeneration process had diﬃculties in answering generic\nquestions, even though the posts are speciﬁc to the Java\nlanguage. Therefore, we understand that query reformu-\nlation methods would be important to recommend more\nspeciﬁc questions allowing the assistant to be more accu-\nrate in the answers.\nA point observed was the quality of the answers in terms\nof grammar, semantic coherence, etc. W e understand that\nthe pre-trained 124M model for English text works quite\nwell for producing apparently coherent text, but still ther e\nis much to be done to achieve more semantic accuracy .\nIV. Rela ted Work\nIntelliCode [11] it is a general purpose multilingual code\ncompletion tool which is capable of predicting sequences\nof code tokens of arbitrary types, generating up to entire\nlines of syntactically correct code. It leverages generati ve\ntransformer model trained on 1.2 billion lines of source\ncode in Python, C#, JavaScript and TypeScript program-\nming languages. Other commercial tool, T abnine 6 uses\nGPT-2 to serve ranked lists of code sequence suggestions.\nHowever,this tool does not attempt to complete longer\nsequences of 20–30 characters long, up to a whole line of\ncode, and we are not aware of any currently deployed tool\nthat have similar feature.\nSiow et al. [12], a novel deep learning model for rec-\nommending relevant reviews given a code change, named\nas COde REview engine - CORE that is built upon only\ncode changes and reviews without external resources. The\nmotivation is to reduce the workload of developers by pro-\nviding review recommendation without human intervene.\nBy automating the code review process, developers can\ncorrect their code as soon as possible, hence, reducing the\ntime between each revision of code changes.\nRadford et al. [8] demonstrate that language models\nbegin to learn these tasks without any explicit supervision\nwhen trained on a new data set of millions of webpages\ncalled W ebT ext. When conditioned on a document plus\nquestions, the answers generated by the language model.\nTheir idea was to make a model unsupervised \"indepen-\ndent task\", but a diﬃcult problem to apply this unsuper-\nvised \"task independent\" model in a practical solution. T o\nimprove the results are gave some clues of the task for the\nmodel: 1) summary was added the token TL; DR: and one\nmore attempt of summary in the training; 2) in translation\nthe authors pass the task in the context 3) Q&A induce\nthe task also in the context that authors passed.\nThe most of the approaches that employ language and\nlearning models have been used mainly to recover code.\nOur proposal presents an alternative to recover code and\ntext that helps to explain and learn a given concept.\nV. Conclusion\nW e have presented a preliminary study aimed at inves-\ntigating the use of a transformer-based language model\naiming the development of a question answering assistant\nfor problems related to software development. Our ﬁndings\n6 http://www.tabnine.com\nsuggest that the corpus used in ﬁne-tuning should be\ncarefully designed, so the answers can be more accurate.\nMoreover, as expected, we found that the question formu-\nlation still plays a very important role on the generated\nanswer. F or future work, several others factors still need\ninvestigation. F or instance, a lower level of creativity ma y\ninﬂuence the generation of more consolidated solutions.\nOther language models, including newer generation GPT-\n3 trained with more parameters, should be investigated on\ntheir impact on the quality of the generation.\nAcknowledgment\nThe authors would like to thank CNPq and F APEMIG\nfor partially supporting this work.\nReferences\n[1] L. B. L. Souza et al. , “Bootstrapping cookbooks for\napis from crowd knowledge on Stack Overﬂow,” Inf.\nSoftw. T echnol., vol. 111, pp. 37–49, 2019.\n[2] L. B. L. de Souza, E. C. Campos, and M. A. Maia,\n“Ranking crowd knowledge to assist software devel-\nopment,” in Proc. of the 22th ICPC , 2014, pp. 72–82.\n[3] E. C. Campos, L. B. de Souza, and M. A. Maia,\n“Searching crowd knowledge to recommend solutions\nfor API usage tasks,” JSEP, vol. 28, no. 10, pp. 863–\n892, 2016.\n[4] R. F. Silva et al. , “Recommending comprehensive\nsolutions for programming tasks by mining crowd\nknowledge,” in Proc. of the 27th ICPC . IEEE Press,\n2019, pp. 358–368.\n[5] R. F. G. da Silva et al. , “CROKAGE: eﬀective so-\nlution recommendation for programming tasks by\nleveraging crowd knowledge,” Empirical Software En-\ngineering, vol. 25, pp. 4707––4758, Sep. 2020.\n[6] A. May , J. W achs, and A. Hannák, “Gender diﬀer-\nences in participation and reward on Stack Overﬂow,”\nEmpirical Software Engineering , vol. 24, no. 4, pp.\n1997–2019, 2019.\n[7] A. V aswani et al. , “Attention is all you need,” in\nProc. of 31st Conf. on Neural Information Processing\nSystems (NIPS 2017) , Long Beach, CA.\n[8] A. Radford, J. W u, R. Child, D. Luan, D. Amodei,\nand I. Sutskever, “Language models are unsupervised\nmultitask learners,” T ech. Rep., 2019.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. T outanova,\n“Bert: Pre-training of deep bidirectional transformers\nfor language understanding,” in NAACL-HL T, 2019.\n[10] “Stackovreﬂow, \"stack overﬂow data\ndump version september 2020\",\nhttps://archive.org/details/stackexchange. ”\n[11] A. Svyatkovskiy et al. , “Intellicode compose: code\ngeneration using transformer,” in Proc. of. the 28th\nESEC/FSE, 2020, pp. 1433–1443.\n[12] J. Siow, C. Gao, L. F an, S. Chen, and Y. Liu,\n“Core: Automating review recommendation for code\nchanges,” 12 2019.",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.6573803424835205
    },
    {
      "name": "Computer science",
      "score": 0.5693247318267822
    },
    {
      "name": "Transformer",
      "score": 0.5272238254547119
    },
    {
      "name": "Natural language processing",
      "score": 0.3830608129501343
    },
    {
      "name": "Software engineering",
      "score": 0.3523271381855011
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3265467882156372
    },
    {
      "name": "Engineering",
      "score": 0.183089017868042
    },
    {
      "name": "Electrical engineering",
      "score": 0.07100039720535278
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "topic": "Question answering"
}