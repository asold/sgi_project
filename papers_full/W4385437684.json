{
  "title": "Leveraging Large Language Models to Generate Answer Set Programs",
  "url": "https://openalex.org/W4385437684",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2889029425",
      "name": "Adam Ishay",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2291840625",
      "name": "Zhun Yang",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2101096475",
      "name": "JooHyung Lee",
      "affiliations": [
        "Arizona State University",
        "Samsung (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2106080192",
    "https://openalex.org/W6673559633",
    "https://openalex.org/W3104666000",
    "https://openalex.org/W6691777337",
    "https://openalex.org/W3178308100",
    "https://openalex.org/W25469692",
    "https://openalex.org/W2141261170",
    "https://openalex.org/W2183391735",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2091138122",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W1903938722",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2251584889"
  ],
  "abstract": "Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer set programs. The majority of errors made are relatively simple and can be easily corrected by humans, thus enabling LLMs to effectively assist in the creation of answer set programs.",
  "full_text": "Leveraging Large Language Models to Generate Answer Set Programs\nAdam Ishay1 , Zhun Yang1 , Joohyung Lee1,2\n1Arizona State University\n2Samsung Research\n{aishay, zyang90, joolee}@asu.edu\nAbstract\nLarge language models (LLMs), such as GPT-3 and GPT-4,\nhave demonstrated exceptional performance in various natu-\nral language processing tasks and have shown the ability to\nsolve certain reasoning problems. However, their reasoning\ncapabilities are limited and relatively shallow, despite the ap-\nplication of various prompting techniques. In contrast, formal\nlogic is adept at handling complex reasoning, but translating\nnatural language descriptions into formal logic is a challeng-\ning task that non-experts struggle with. This paper proposes a\nneuro-symbolic method that combines the strengths of large\nlanguage models and answer set programming. Specifically,\nwe employ an LLM to transform natural language descrip-\ntions of logic puzzles into answer set programs. We carefully\ndesign prompts for an LLM to convert natural language de-\nscriptions into answer set programs in a step by step man-\nner. Surprisingly, with just a few in-context learning exam-\nples, LLMs can generate reasonably complex answer set pro-\ngrams. The majority of errors made are relatively simple and\ncan be easily corrected by humans, thus enabling LLMs to\neffectively assist in the creation of answer set programs.\n1 Introduction\nTransformer-based large language models (LLMs) have\nrecently shown remarkable success in many downstream\ntasks, demonstrating their general reasoning capability\nacross diverse problems. However, while LLMs excel in\ngenerating System 1 thinking, they struggle with System 2\nthinking, resulting in output that is often inconsistent and\nincoherent (Nye et al. 2021). This is because LLMs are ba-\nsically trained to predict subsequent words in a sequence and\ndo not appear to have a deep understanding of concepts such\nas cause and effect, logic, and probability, which are essen-\ntial for reasoning.\nTo address the issue, Nye et al. (2021) propose a dual-\nsystem model that combines the strengths of LLMs and sym-\nbolic logic to achieve improved performance on reasoning\ntasks. They leverage an LLM to generate a System 1 pro-\nposal and employ symbolic computation to filter these pro-\nposals for consistency and soundness.\nWe are interested in situations where problems are de-\nscribed in natural language and solving them requires deep\nreasoning. A system needs to take into account linguistic\nvariability and be able to perform symbolic reasoning. We\ntake logic puzzles as the testbed as they are well-suited for\nthis purpose.\nWe first note that GPT-3 (Brown et al. 2020) and GPT-\n41 by themselves struggle with solving logic puzzles, de-\nspite various prompts we tried. On the other hand, we find\nthat they can convert the natural language descriptions of\nthe puzzles into declarative answer set programming lan-\nguages (Lifschitz 2008; Brewka, Niemel¨a, and Truszczynski\n2011) surprisingly well. Even the errors these LLMs make\nare mostly simple for humans to correct. We hope that our\nfinding will ease the efforts of writing answer set programs\nand expand the application of answer set programming to a\nbroader audience.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 offers a brief overview of related work on automated\nsolving of logic puzzles. Sections 3 and 4 delve into the pro-\nposed approach in detail. Section 5 presents experimental\nresults and performance evaluations of the approach. Sec-\ntion 6 shows more examples demonstrating the generaliz-\nability of our method.\nThe code is available at https://github.com/azreasoners/\ngpt-asp-rules.\n2 Preliminaries\n2.1 Large Language Models (LLMs)\nLLMs have significantly improved natural language pro-\ncessing, achieving strong performance on a variety of tasks\nusing few-shot learning (Brown et al. 2020). However,\nLLMs remain weak at tasks that involve complex reason-\ning (Creswell, Shanahan, and Higgins 2022; Valmeekam\net al. 2022), and scaling model size alone is not enough\nto achieve good performance (Rae et al. 2021). It has\nbeen shown that various prompting methods improve accu-\nracy on reasoning tasks (Wei et al. 2022; Zhou et al. 2022;\nCreswell, Shanahan, and Higgins 2022). Nye et al. (2021)\npresent a dual-system model which uses an LLM as a se-\nmantic parser and couples it with a custom symbolic mod-\nule to achieve performance gains on reasoning tasks. This\nframework combines the strengths of LLMs for parsing\ncomplex natural language and symbolic logic for handling\n1Throughout the paper, we use GPT-3 to refer to the “text-\ndavinci-003” model and GPT-4 to refer to the “gpt-4-0314” (re-\nleased March, 2023) model in the OpenAI API.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n374\ncomplex reasoning. However, the authors had to use hand-\nengineered set of constraints for the latter part. To our\nknowledge, our work is the first to use LLMs to generate\nlogic rules to solve complex reasoning tasks.\n2.2 Automated Logic Puzzle Solving\nWorks focused on solving logic puzzles typically involve a\nmapping from natural language to logic formalism. This\nprocess often includes problem simplification techniques,\nsuch as tailoring the puzzle to a specific domain, restrict-\ning natural language input to a certain form, or assuming\nadditional inputs like enumerated types. Lev et al. (2004)\nemploy a specialized automated multi-stage parsing process\nto convert natural language text into an intermediate form\ncalled Semantic Logic, which is then converted into First\nOrder Logic to finally evaluate on law school admissions\ntests (LSAT) and the Graduate Records Examination (GRE).\nShapiro (2011) manually encodes the “Jobs Puzzle” in a\nfew different logical formalisms and compare them. Puz-\nzler (Milicevic, Near, and Singh 2012) uses a general link\nparser to translate puzzles into to the Alloy language for\nsolving, primarily through an automated process, albeit with\nassumed types. LogicSolver (Nordstrom 2017) follows a\nsimilar approach to Puzzler but replaces Alloy with a cus-\ntom solver and conducts a more comprehensive evaluation.\nSeveral works utilize translations into the language of\nanswer set programming (ASP) (Lifschitz 2008; Brewka,\nNiemel¨a, and Truszczynski 2011). Schwitter (2013) ad-\ndresses the “Jobs Puzzle” by representing the problem using\ncontrolled natural language (Schwitter 2010), which can be\nfurther turned into ASP. Baral and Dzifcak (2012) employ a\nλ-calculus-based approach and trains a model that converts\na manually simplified version of natural language clues into\nASP rules for solving Zebra puzzle-type logic puzzles. Mi-\ntra and Baral (2015) train a maximum entropy-based model\nto extract relations for each clue, which are then converted\ninto a common ASP rule format, where a stable model corre-\nsponds to the puzzle solution. LGPSolver (Jabrayilzade and\nTekir 2020) uses DistilBERT, a transformer-based model, as\na classifier that can distinguish between representative rule\ntypes. With the clue classification, the authors use a hand-\ncrafted clue to Prolog translation (as opposed to ASP) and\ncompute the solution. The works mentioned involve some\ncombination of manual processing and/or brittle problem-\nspecific translations. Our work distinguishes itself by be-\ning both fully automated and featuring a general pipeline,\nleveraging the extensive translation capacity available from\nLLMs.\n2.3 Generate-Define-Test with ASP\nASP programs are typically written following the Generate-\nDefine-Test structure, which generates potential solutions\n(Generate) and eliminates invalid ones based on certain con-\nstraints (Test). The Generate portion usually includes choice\nrules, while the Test portion consists of a set of constraints\nthat prune out invalid solutions. An additional part of the\nprogram, the Define portion, includes necessary auxiliary\npredicates that are used in the Test portion.\n3 Method\nFigure 1: Flow of Generating Answer Set Programs from Logic\nPuzzle in English\nIn order to find a solution to a logic puzzle, we utilize\nGPT-3 to convert the puzzle into an answer set program\nso that the stable model (a.k.a answer set) encodes the so-\nlution.2 Although GPT-3 exhibits strong capabilities, we\ndiscovered that it cannot generate a correct answer set pro-\ngram without being guided by carefully engineered prompts.\nThese prompts instructs GPT-3 to reliably extract constants\nand generate accurate predicates and rules. In this paper, we\ndetail our prompt engineering efforts.\nFigure 1 illustrates the structure of our pipeline, which uti-\nlizes GPT-3 step by step to generate an ASP program. Sim-\nilar to how a human would approach the task, our pipeline\nfirst extracts the relevant objects and their categories. Then,\nit generates a predicate that describes the relations among\nthe objects from different categories. Using the generated\ninformation, the pipeline further constructs an ASP program\nin the style of Generate-Define-Test.\nLet Fc and Fp denote the Constant Extraction and Predi-\ncate Generationsteps in Figure 1. LetFr1 and Fr2 represent\nthe two parts of the Rule Generation step, i.e., the Generate\npart and the Define&Testpart, respectively. Our pipeline can\nbe modeled by the following equations that map a puzzle\nstory q to an ASP program Π = Πgenerate ∪ Πdefine\ntest.\nc =Fc(q) p =Fp(q, c)\nΠgenerate =Fr1(c, p) Π define test =Fr2(q, c, p).\nHere, c and p denote extracted objects and generated pred-\nicates. Each step F∗ is realized by GPT-3 with 2-shot\nprompting, i.e., only 2 examples in each prompt.\n3.1 Constant Extraction\nThe first step in the pipeline is to extract constants or entities\nfrom the given story along with their corresponding cate-\ngories. To accomplish this, we invoke GPT-3 usingPrompt\nC, which consists of three parts: instruction, examples, and\na query.\nPrompt C:\n2Though this section mostly mentions GPT-3, GPT-4 can be\nused instead.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n375\n1 Given a problem , extract all different\nconstants and their categories in the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n \". Here , the format of each constant\nis turned into either an integer or a string\nsurrounded by double quotes , e.g.,\"some name \".\n2\n3 Problem 1:\n4 Consider N-Queens Puzzle on a chessboard of\nsize 8x8. The goal is to assign 8 queens on\nthe chessboard so that no two queens can share\nthe same row , column , or diagonal.\n5\n6 Constants:\n7 index_of_row : 1; 2; 3; 4; 5; 6; 7; 8.\n8 index_of_column : 1; 2; 3; 4; 5; 6; 7; 8.\n9\n10 Problem 2:\n11 \"Against the Grain\" offers hand -made wooden\nfurniture at reasonable prices. Each item is\nmade by an in -house employee. Using only the\nclues that follow , match each item to the\nemployee who crafted it , and determine its\nprice and the type of wood used to make it.\nRemember , as with all grid -based logic puzzles\n, no option in any category will ever be used\nmore than once.\n12 1. Bonita ’s piece costs $325.\n13 2. The item made of poplar costs more than\nYvette ’s piece.\n14 3. Tabitha ’s item costs 50 dollars less than\nthe piece made of sandalwood.\n15 4. The $275 item is either the piece made of\nash or Yvette ’s item.\n16\n17 Constants:\n18 employee: \"Bonita \"; \"Yvette \"; \"Tabitha \".\n19 price: 225; 275; 325.\n20 wood_type: \"ash\"; \"poplar \"; \"sandalwood \".\n21\n22 Problem 3:\n23 <story>\n24\n25 Constants:\nLine 1 provides a general instruction for the task of ex-\ntracting objects and directing GPT-3 to generate them in the\nform of “category: constant1; . . . ; constantn.” Then, two ex-\namples follow: Lines 6-8 for Problem 1 specified in Lines\n3-4, and Lines 17-20 for Problem 2 specified in Lines 10-\n15. By replacing Line 23 (⟨story⟩) with a new example story\nand invoking GPT-3 with the above prompt, a new list of\ncategories and constants for that story is generated, as with\nthe previous two examples.\nThe above two examples are chosen to cover two cases of\nobject extraction. For the N-Queens problem, the constants\n1, . . . ,8 are not described in the Problem 1 statement (Line\n4) but can be inferred. For the second puzzle, however, all\nconstants in Lines 18-20 are mentioned in the example story\nprovided in Lines 11-15.\nThe second puzzle is also intentionally selected to give an\nexample for GPT-3 so that certain constants (e.g.,$225) can\nbe turned into valid integers (e.g., 225) so that arithmetic\ncan be applied correctly later when generating rules later on,\nwhile others should be surrounded by double quotes. We ex-\nperimented with various prompts to instruct GPT-3 to gener-\nate all non-numeric constants in lowercase and replace spe-\ncial characters with underscores. However, GPT-3 was un-\nable to strictly adhere to these instructions and consequently\nmade more errors.\n3.2 Predicate Generation\nThe next step in the pipeline is to generate predicates p that\ndescribe the relations among the extracted constants. We use\nGPT-3 on the Prompt P below.\nPrompt P:\n1 Given a problem and some categorized constants\nof the form \"category: constant_1; constant_2;\n...; constant_n\", generate the minimum number\nof predicates to define the relations among\nthe categories of constants. Each generated\npredicate is of the form \"predicate(X1 , X2 ,\n..., Xn)\" where X1 , X2 , ..., Xn are different\nvariables and each variable X belongs to one\nof the categories. For each category , there\nmust exist at least one variable of some\npredicate that belongs to this category.\n2\n3 Problem 1:\n4 (Lines 4-8 from Prompt C: Omitted)\n5\n6 Predicates:\n7 % The categories in Constants include\nindex_of_row and index_of_column . We use\ndifferent variables Ir and Ic to represent\nindex_of_row and index_of_column .\n8 % We assign a queen at row Ir and column Ic ,\nwhere Ir belongs to index_of_row and Ic\nbelongs to index_of_column .\n9 assign(Ir , Ic)\n10\n11 Problem 2:\n12 (Lines 11-20 from Prompt C: Omitted)\n13\n14 Predicates:\n15 % The categories in Constants include employee ,\nprice , and wood_type. We use different\nvariables E, P, and W to represent employee ,\nprice , and wood_type.\n16 % We match an employee E with price P and wood\ntype W, where E belongs to employee , P belongs\nto price , and W belongs to wood_type.\n17 match(E, P, W)\n18\n19 Problem 3:\n20 <story>\n21\n22 Constants:\n23 <constants>\n24\n25 Predicates:\nLine 1 is a general instruction describing the task of pred-\nicate generation, and that the generated predicates should\nfollow the form of “predicate(X 1, . . . , Xn)” where each X i\nis a distinct variable that represents a category of constants.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n376\nAgain, the two examples follow. Lines 3–4 are a copy\nof the first example in Lines 3–8 of Prompt C (where we\nomit Lines 4–8 from Prompt C to reduce the space). Lines\n6–9 continue the first example, where it now generates the\npredicates with variables as arguments following the instruc-\ntion. It also contains two comments (starting with symbol\n%). The first comment in Line 7 recalls the categories of\nconstants and assigns a different variable to each category.\nThe second comment in Line 8 gives the English reading of\nthe predicate and variables, and emphasizes the link between\neach variable and a category of constants. Similarly, Lines\n11–17 present the second example.\nNext, the story and constants are given for the third prob-\nlem and GPT-3 is prompted to generate the predicate for that\nexample, given the general instruction and the preceding two\nexamples.\nGiven the extracted constants c and generated predicates\np, the next step in the pipeline is to generate ASP rules Π,\nconsisting of the Generate part and the Define&Test part.\n3.3 Rule Generation: Generate\nThe Generate part of an ASP program defines all possi-\nble mappings of constants from different categories. This\nis done by choice rules. In this step, an ASP program\nΠgenerate is obtained by calling GPT-3 with Prompt R1.\nPrompt R1:\n1 Given some categorized constants in the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\" and some predicates about the\nrelation among different categories of\nconstants , write ASP (Answer Set Programming)\nrules to generate the search space of possible\nrelations.\n2\n3 Constants:\n4 employee: \"Bonita \"; \"Yvette \"; \"Tabitha \".\n5 price: 225; 275; 325.\n6 wood_type: \"ash\"; \"poplar \"; \"sandalwood \".\n7\n8 Predicates:\n9 % The categories include employee , price , and\nwood_type. We use different variables E, P,\nand W to represent employee , price , and\nwood_type.\n10 % We match an employee E with price P and wood\ntype W, where E belongs to employee , P belongs\nto price , and W belongs to wood_type.\n11 match(E, P, W)\n12\n13 ASP Rules:\n14 % Define the constants in each category.\n15 employee (\" Bonita \"; \"Yvette \"; \"Tabitha \").\n16 price (225; 275; 325).\n17 wood_type (\"ash\"; \"poplar \"; \"sandalwood \").\n18\n19 % For each employee E, it matches with exactly\n1 price P and 1 wood type W.\n20 {match(E, P, W): price(P), wood_type(W)}=1 :-\nemployee(E).\n21\n22 Constants:\n23 <constants>\n24\n25 Predicates:\n26 <predicates>\n27\n28 ASP rules:\nIn the above prompt, ⟨constants⟩ and ⟨predicates⟩ are to\nbe replaced for a new example. GPT-3 generates facts and\nchoice rules following the last line of the prompt.\nThe task in this step is to write facts and choice rules based\non the generated constants and predicates. Since this step\ndoesn’t require the details of the story, we omit the story\nfrom the prompt to avoid unnecessary noisy information be-\ning included in the prompt. Each example only consists of\nconstants, predicates, and ASP rules to be generated, i.e.,\nfacts and choice rules.\nSimilar to the previous prompts, Line 1 is a general in-\nstruction, Lines 3–20 provide an example, and Lines 22–28\nare for the queried example. The example ASP rules in Lines\n14–20 contain comments (Lines 14 and 19), which will also\nbe generated for the queried example and help to gather se-\nmantic information before generating a rule.\n3.4 Rule Generation: Define and Test\nThe Define&Test part of an ASP program contains con-\nstraints that “weed out” the stable models that do not cor-\nrespond to valid answers. This step takes as input the puzzle\nstory q, constants c, and predicates p: semantically, the ASP\nrules represent the content in storyq while, syntactically, the\nASP rules must be formed by the extracted constants c and\ngenerated predicates p. The ASP program Πdefine\ntest is\nobtained by calling GPT-3 with Prompt R2.\nPrompt R2:\n1 Consider the constraint in the following form\n2 <C1 >; <C2 >; ...; <Cm > :- <L1 >, <L2 >, ..., <Ln >.\n3 which says that if the conjunction \"<L1 > and <\nL2 > and ... and <Ln >\" is true , then the\ndisjunction of comparisons \"<C1 > or <C2 > or\n... or <Cm >\" must be true.\n4\n5 One can also add a restriction that \"exactly k\nof <C1 >, <C2 >, ..., <Cm > is true\" by using the\nfollowing form\n6 {<C1 >; <C2 >; ...; <Cm >}=k :- <L1 >, <L2 >, ..., <\nLn >.\n7\n8 Given a problem , extract all constraints from\nthe clues in the problem using only the\nprovided constants and predicates.\n9\n10 Problem 1:\n11 \"Against the Grain\" offers hand -made wooden\nfurniture at reasonable prices. Each item is\nmade by an in -house employee. Using only the\nclues that follow , match each item to the\nemployee who crafted it , and determine its\nprice and the type of wood used to make it.\nRemember , as with all grid -based logic puzzles\n, no option in any category will ever be used\nmore than once.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n377\n12 1. Bonita ’s piece costs $325.\n13 2. The item made of poplar costs more than\nYvette ’s piece.\n14 3. Tabitha ’s item costs 50 dollars less than\nthe piece made of sandalwood.\n15 4. The $275 item is either the piece made of\nash or Yvette ’s item.\n16\n17 Constants:\n18 employee: \"Bonita \"; \"Yvette \"; \"Tabitha \".\n19 price: 225; 275; 325.\n20 wood_type: \"ash\"; \"poplar \"; \"sandalwood \".\n21\n22 Predicates:\n23 % The categories include employee , price , and\nwood_type. We use different variables E, P,\nand W to represent employee , price , and\nwood_type.\n24 % We match an employee E with price P and wood\ntype W, where E belongs to employee , P belongs\nto price , and W belongs to wood_type.\n25 match(E, P, W)\n26\n27 Constraints:\n28 % No option in any category will ever be used\nmore than once.\n29 {E1=E2; P1=P2; W1=W2}=0 :- match(E1 ,P1 ,W1),\nmatch(E2 ,P2 ,W2), (E1 ,P1 ,W1)!=(E2 ,P2 ,W2).\n30\n31 % 1. Bonita ’s piece costs $325.\n32 P=325 :- match(E,P,W), E=\" Bonita \".\n33\n34 % 2. The item made of poplar costs more than\nYvette ’s piece.\n35 P1 >P2 :- match(E1 ,P1 ,W1), match(E2 ,P2 ,W2), W1=\"\npoplar\", E2=\" Yvette \".\n36\n37 % 3. Tabitha ’s item costs 50 dollars less than\nthe piece made of sandalwood.\n38 P1=P2 -50 :- match(E1 ,P1 ,W1), match(E2 ,P2 ,W2),\nE1=\" Tabitha\", W2=\" sandalwood \".\n39\n40 % 4. The $275 item is either the piece made of\nash or Yvette ’s item.\n41 {W=\"ash\"; E=\" Yvette \"}=1 :- match(E,P,W), P=275.\n42\n43 (Problem 2 omitted)\n44\n45 Problem 3:\n46 <story>\n47\n48 Constants:\n49 <constants>\n50\n51 Predicates:\n52 <predicates>\n53\n54 Constraints:\nIn the above prompt, ⟨story⟩ is a new puzzle, and\n⟨constants⟩, ⟨predicates⟩ are generated by GPT-3 for that\nstory using Prompt C and Prompt P in Section 3.1 and 3.2.\nLines 1–8 are a general instruction describing the task of\nΠdefine test generation and provides two rule forms for the\ntarget ASP rules. The first rule form\nC1; C2; . . .; Cm ← L1, L2, . . . , Ln\nsays that “C1 or ... or Cm is true if L1 and ... and Ln are\ntrue.” Here, each Li is a literal and each Ci is a comparison\nin the input language of CLINGO , e.g., A > B, A = B + 3,\netc. The second rule form\n{C1; C2; . . .; Cm} = k ← L1, L2, . . . , Ln\nadditionally restricts that “exactly k of {C1, . . . , Cm} must\nbe true.” In principle, the first rule form is enough to rep-\nresent various constraints. However, since the second rule\nform is syntactically closer to certain complex sentences re-\nlated to cardinality, e.g., “either ... or ...”, “neither ... nor ...”,\nor “no ... is ...”, etc, we found that GPT-3 works much better\nwhen we also include the second rule form.\n4 Optional Enhancements to the Pipeline\nSection 3 presented a general pipeline that automatically\nwrites an ASP program for a puzzle in natural language us-\ning LLM. This section explains two optional enhancements\nthat strengthen its robustness.\n4.1 Constant Formatting\nIn the Constant Extraction step (Section 3.1), GPT-3 may\nextract the names of the objects as they appear in the puz-\nzle story, such as $225, Sue Simpson, and 8:30 AM,\nwhich do not conform to the syntax of the input language of\nanswer set solver CLINGO . Also, GPT-3 applies arithmetic\ncomputations (e.g., L1=L2+3) to constants surrounded by\ndouble quotes (e.g., L2 is constant \"9 inches\") instead\nof constants that are integers (e.g., L2 is constant 9).\nA rule-based post-processing could be applied to turn\nthem into the right syntax, but alternatively, we employ\nGPT-3 to generate syntactically correct forms. We found\nthat this method requires significantly less efforts and is\nmore general because GPT-3 applies the constant formatting\ncorrectly even for unforeseen formats using some “common\nsense,” which is lacking in the rule-based approach. We use\nthe following prompt for this.\nThe Constant Formatting step is done by calling GPT-3\nwith the following prompt, where ⟨constants⟩ at the end of\nthe prompt is replaced by the original (extracted) constantsc\nobtained by the Constant Extraction step (Section 3.1). The\nGPT-3 response in this step is the updated constants c, serv-\ning as an input to other steps in the pipeline.\n1 Given categorized constants of the form \"\ncategory: constant_1; constant_2; ...;\nconstant_n\", format the category and constants\nsuch that:\n2 each category consists of only lowercase\nletters and underscores , and\n3 each constant is either an integer or a string\nsurrounded by double quotes , e.g., \"United\nStates \".\n4\n5 There are two ways below to format constants\nand we must use the same way for all constants\nof the same category.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n378\n6 1. Turn all constants of the same category into\nintegers with no space or special character.\n7 2. Add double quotes around all constants of\nthe same category.\n8 Note that the 1st way has a higher priority ,\nmeaning that we must turn all constants of the\nsame category into integers whenever possible\n. For example , twice or second can be turned\ninto 2, September can be turned into 9,\nSeptember 5th can be turned into 5 if all\ndates are in September , but 9:30 am can only be\nturned into \"9:30 am\" since no integer can\nrepresent 9:30 am.\n9\n10 Original constants:\n11 Employees: Bonita; Yvette; Tabitha.\n12 Prices: $225; $275; $325.\n13 Wood types: ash; poplar; sandalwood.\n14\n15 Formatted constants:\n16 employee: \"Bonita \"; \"Yvette \"; \"Tabitha \".\n17 price: 225; 275; 325.\n18 wood_type: \"ash\"; \"poplar \"; \"sandalwood \".\n19\n20 Original constants:\n21 months: January; April; October; December.\n22 times: 8:30 AM; 10:30 AM; 2:30 PM; 3:30 PM.\n23 durations: 1 day; 3 days; 11 days; 12 days.\n24\n25 Formatted constants:\n26 month: 1; 4; 10; 12.\n27 time: \"8:30 AM\"; \"10:30 PM\"; \"2:30 PM\"; \"3:30 PM\".\n28 duration: 1; 3; 11; 12.\n29\n30 Original constants:\n31 ⟨constants⟩\n32\n33 Formatted constants:\n4.2 Sentence Paraphrasing\nSometimes sentences may need to be paraphrased before an\nLLM can correctly generate rules from them. The Sentence\nParaphrasing step provides the opportunity to not only sim-\nplify or formalize the sentences from the original question\nbut also add the hidden information assumed to underlie the\nquestion. For example, the following sentence\n1 Of the person who won the prize in\nbioengineering and Sue Simpson , one won in\n1976 and the other won in 1968.\nis one clue in the example question in Section 3. The cor-\nrect translation requires an LLM to turn the above sentence\ninto at least 3 ASP rules, which would be hard for the cur-\nrent LLMs (e.g., GPT-3). Instead, we can ask GPT-3 to first\nparaphrase such kind of sentence into simpler ones below.\n1 The person who won the prize in bioengineering\nand Sue Simpson are different.\n2 The person who won the prize in bioengineering\nwon in 1976 or won in 1968.\n3 Sue Simpson won in 1976 or won in 1968.\nThe Sentence Paraphrasing step is done by calling GPT-\n3 with the following prompt, where ⟨sentences⟩ at the end\nof the prompt is replaced by the numbered sentences in the\nqueried puzzle story q, and the GPT-3 response in text is\nused to replace the original sentences in q. This prompt is\ndedicated to the logic puzzles from Puzzle Baron and only\nparaphrases one kind of sentence in the form “of A and B,\none is C and the other is D.”\n1 Copy a sequence of numbered sentences.\n2\n3 If a sentence is of the form \"N. Of A and B,\none is C and the other is D\", replace it with\n3 sentences below.\n4 N.1 A and B are different.\n5 N.2 A is C or D.\n6 N.3 B is C or D.\n7\n8 For every sentence , if it is not of the form \"N\n. Of ... and ...\" , simply copy it without\nreplacement. An easy way to determine if a\nsentence is not of the above form is to check\nif its first word is not of.\n9\n10 In the following example , one sentence is of\nthe above form.\n11 Given:\n12 1. The squad from Grenada ended with 2 silver\nmedals.\n13 2. Of the team from Oman and the team that won\n10 silver medals , one finished with 2 gold\nmedals and the other finished with 1 gold\nmedal.\n14 Copy:\n15 1. The squad from Grenada ended with 2 silver\nmedals.\n16 2.1 The team from Oman and the team that won 10\nsilver medals are different.\n17 2.2 The team from Oman finished with 2 gold\nmedals or finished with 1 gold medal.\n18 2.3 The team that won 10 silver medals finished\nwith 2 gold medals or finished with 1 gold\nmedal.\n19\n20 In the following example , no sentence is of the\nabove form.\n21 Given:\n22 1. Tabitha ’s item costs 50 dollars less than\nthe piece made of sandalwood.\n23 2. The $275 item is either the piece made of\nash or Yvette ’s item.\n24 Copy:\n25 1. Tabitha ’s item costs 50 dollars less than\nthe piece made of sandalwood.\n26 2. The $275 item is either the piece made of\nash or Yvette ’s item.\n27\n28 Given:\n29 ⟨sentences⟩\n30 Copy:\n5 Experiments\nWe tested the above pipeline on the logic puzzles dataset\nfrom (Mitra and Baral 2015). Since the constants are pro-\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n379\nMethod train set test set\n(Mitra and Baral 2015) – 71%\nZero-shot GPT-3 0% 2%\nFew-shot GPT-3 4% 3%\nZero-shot GPT-4 12% 21%\nFew-shot GPT-4 6% 7%\nGPT-3 Generated ASP Rules 86% 81%\nGPT-4 Generated ASP Rules 92% 92%\nTable 1: Accuracy on 50 train and 100 test puzzles. GPT-3 refers\nto the model named“text-davinci-003” in the OpenAI API, while\nGPT-4 is the model named “gpt-4.”\nStep Count\nGPT-3 GPT-4\nconstant formatting 3 1\nparaphrasing 2 4\nconstraint generation (syntax) 3 0\nconstraint generation (semantics) 13 3\nTable 2: Mistakes on 100 test puzzles at different pipeline steps.\nvided in the dataset as necessary information to solve each\npuzzle, we apply Constant Formatting directly on the given\nconstants to generate constants c.\nThe dataset consists of 50 training examples and 100 test-\ning examples. When designing our prompts, we only consult\nthe training examples and not the testing examples. Table 1\nshows the performance of our approach to zero-shot GPT-\n3/GPT-4, few-shot GPT-3/GPT-4, and a fully-supervised\nlearning system LOGICIA (Mitra and Baral 2015). 3 In\nthe few-shot setting, we use the first two examples in the\ntraining set as the few-shot examples. GPT-3 with zero-shot\nand few-shot settings didn’t perform well, while zero-shot\nGPT-4 could solve 21% of the test puzzles correctly, which\nis significantly better than GPT-3’s performance. However,\nthis is much lower than our method’s 81%. Interestingly,\nwhile the few-shot setting slightly improves over the zero-\nshot for GPT-3, this is quite different with GPT-4. This\nis likely because GPT-4 with the zero-shot setting was in-\nstructed to solve the puzzles in a step by step manner. How-\never, for the few-shot setting, the examples only include the\nproblem and solution, which may have discouraged GPT-4\nfrom working through the puzzles in steps.\nBesides the fact that the direct execution of the LLMs re-\nsults in low performance, it is hard to understand why they\nfail to solve puzzles; in other words, the results are hard to\ninterpret.\nOn the other hand, in our method, although the LLMs can\nstill produce incorrect ASP programs, the errors are much\nless and it is easy to identify the failures by examining the\ngenerated ASP programs. Table 2 summarizes the number\nof mistakes we encountered on 100 test data at each pipeline\nstep. Below we provide a representative example for each\n3For GPT-3/GPT-4, to avoid randomness, we use a temperature\nof 0 (deterministic) and a top P value of 1 (default setting).\ntype of errors from Table 2 and highlight the mistakes in\nred. 4 The examples are taken from GPT-3, although each\ntype also occurs with GPT-4, except for the syntax error.\n• Failure to convert certain constants into integers dur-\ning constant formatting. The only mistake happens\nwhen, given constants “points: 181 points; 184 points;\n187 points; 190 points”, GPT-3 generated\n1 points: \"181 points\"; \"184 points\";\n2 \"187 points\"; \"190 points\".\nwhile it should generate constants in integers below to be\nable to apply arithmetic operations.\n1 points: 181; 184; 187; 190.\n• Addition of wrong clues during paraphrasing. One\nsuch error happens when paraphrasing the sentence “2.\nThe book published in 2011 is either Orlando’s project\nor Jan’s project”. GPT-3 generated the following three\nsentences.\n1 % 2.1 The book published in 2011 is either\nOrlando ’s project or Jan ’s project.\n2 % 2.2 Orlando’s project is the book\n3 published in 2011 or the 200 page\n4 project.\n5 % 2.3 Jan’s project is the book\n6 published in 2011 or the 200 page\n7 project.\nThe first sentence is simply a copy of the original sentence\nwhile the last two should not be there since they cannot be\ninferred from the original sentence.\n• Syntax error during constraint generation. One typ-\nical syntax error is an incorrect variable name in a gener-\nated rule. For example, given the sentence “The study on\nthe Liberty starts 2 months after Eunice’s project”, GPT-3\ngenerates the following rule\n1 M=M1+2 :- match(S,M,Sh), match(S1 ,M1 ,Sh1),\nS=\"Eunice\", Sh=\" Liberty \".\nwhile the variable S in the second line should have been\nS1.\n• Semantic error during constraint generation. One\ntypical semantic error is caused by a wrong equation. For\nexample, given the sentence “the $35,000 structure is 15\nsq ft smaller than the $29,000 home”, GPT-3 generated\n1 S1=S2+15 :- match(C1 ,S1 ,P1), match(C2 ,S2 ,P2)\n, P1 =35000 , P2 =29000.\nwhile the equation in the head should have been\nS1=S2-15.\nWhile our pipeline doesn’t achieve 100% accuracy on\ngenerated ASP programs, most failed puzzles only have one\n4We have provided a breakdown of the errors we encoun-\ntered on Github at https://github.com/azreasoners/gpt-asp-rules/\ntree/main/error analysis.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n380\nmistake and such a mistake is easy to correct. This means\nthat our pipeline could serve as a good suggestion tool to\nprepare draft ASP programs for users. For example, com-\npared to designing all the ASP programs for 50+100 puzzles\nmanually, it would save a significant amount of time to only\ncheck the correctness of the automatically generated rules\nfor the programs that don’t have a single stable model.\n6 More Examples\nPrevious approaches that automate logic puzzle solving ei-\nther only predict constants and relations (Mitra and Baral\n2015) or treat rule generation as a classification problem on\na small set of rule templates (Jabrayilzade and Tekir 2020).\nIn comparison, our method is generative, where rules are\ngenerated in an open-ended manner under the guidance of a\nfew examples.\nWhile it’s hard to apply the previous methods to other\ndomains without substantial changes, applying our pipeline\nto new domains requires only minor adjustments on the\nprompts. To apply our pipeline to other domains, we make a\nslight adjustment by turning the last sentence in Line 11 of\nPrompt R2 into a numbered clue “0. No option in any cate-\ngory will ever be used more than once.”, since it was specific\nto grid logic puzzles.\nIn the following part of this section, we show how our\npipeline can be further applied to generate ASP programs\nfor Sudoku and the Jobs Puzzle.\n6.1 Sudoku\nIf we describe Sudoku problem with the following story q\n1 The Sudoku problem is a classic computer\nscience problem that involves filling a 9x9\ngrid with digits from 1 to 9.\n2 1. The same number cannot appear twice in the\nsame row.\n3 2. The same number cannot appear twice in the\nsame column.\n4 3. The same number cannot appear twice in the 3\nx3 box. (You can use division / to check if 2\ncells are in the same box.)\nour pipeline generates the following ASP program Π.\n1 % Define the constants in each category.\n2 index_of_row (1; 2; 3; 4; 5; 6; 7; 8; 9).\n3 index_of_column (1; 2; 3; 4; 5; 6; 7; 8; 9).\n4 number (1; 2; 3; 4; 5; 6; 7; 8; 9).\n5 % For each row Ir and column Ic , it assigns\nexactly 1 number N.\n6 {assign(Ir , Ic , N): number(N)}=1 :-\nindex_of_row (Ir), index_of_column (Ic).\n7\n8 % 1. The same number cannot appear twice in the\nsame row.\n9 {N1=N2}=0 :- assign(Ir ,Ic1 ,N1), assign(Ir ,Ic2 ,\nN2), (Ic1 ,N1)!=(Ic2 ,N2).\n10\n11 % 2. The same number cannot appear twice in the\nsame column.\n12 {N1=N2}=0 :- assign(Ir1 ,Ic ,N1), assign(Ir2 ,Ic ,\nN2), (Ir1 ,N1)!=(Ir2 ,N2).\n13\n14 % 3. The same number cannot appear twice in the\n3x3 box.\n15 {N1=N2}=0 :- assign(Ir1 ,Ic1 ,N1), assign(Ir2 ,Ic2\n,N2),(Ir1/3,Ic1/3)=(Ir2/3,Ic2/3), (Ir1 ,\nIc1 ,N1)!=(Ir2 ,Ic2 ,N2).\nThis ASP program Π is almost correct except that the red\npart in Line 16 of Π should be\n1 ((Ir1 -1)/3, (Ic1 -1) /3) = ((Ir2 -1)/3, (Ic2 -1) /3)\nsince the row and column indices start from 1. This for-\nmula seems too difficult for GPT-3 to notice and generate\nunless some examples are provided . On the other hand, if\nwe slightly adjust Lines 7–8 of Prompt C (Section 3.1) to\nmake the indices start from 0, then the generated ASP pro-\ngram Π becomes correct as Lines 2–3 of Π are changed to\nthe following facts.\n1 index_of_row (0; 1; 2; 3; 4; 5; 6; 7; 8).\n2 index_of_column (0; 1; 2; 3; 4; 5; 6; 7; 8).\nGPT-4 also fails to generate the last rule correctly, al-\nthough it makes a different mistake.\n6.2 Jobs Puzzle\nThe Jobs Puzzle studied in (Schwitter 2013) asks one to as-\nsign 8 different jobs to 4 people while satisfying the given\nconstraints. The full puzzle q is shown below.\n1 1. There are four people: Roberta , Thelma ,\nSteve , and Pete.\n2 2. Among them , they hold eight different jobs.\n3 3. Each holds exactly two jobs.\n4 4. The jobs are: chef , guard , nurse , telephone\noperator , police officer (gender not implied),\nteacher , actor , and boxer.\n5 5. The job of nurse is held by a male.\n6 6. The husband of the chef is the telephone\noperator.\n7 7. Roberta is not a boxer.\n8 8. Pete has no education past the ninth grade.\n9 9. Roberta , the chef , and the police officer\nwent golfing together.\n10 Question: Who holds which jobs?\nThis puzzle was considered a challenge for logical express-\nibility and automated reasoning (Shapiro 2011).\nTo apply our method to the Jobs Puzzle, some paraphras-\ning was needed before the Define&Test part of rule genera-\ntion. We manually paraphrased the above puzzle to the fol-\nlowing\n1 There are four people: Roberta , Thelma , Steve ,\nand Pete. Among them , they hold eight\ndifferent jobs. Each holds exactly two jobs.\nThe jobs are: chef , guard , nurse , telephone\noperator , police officer (gender not implied),\nteacher , actor , and boxer.\n2 5. The job of nurse is held by a male.\n3 6. The husband of the chef is the telephone\noperator , which means the chef is a female and\nthe telephone operator is a male.\n4 7. Roberta is not a boxer.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n381\n5 8. Pete has no education past the ninth grade.\n(Three jobs teacher , nurse , and police officer\nrequire higher education .)\n6 9. Roberta , the chef , and the police officer\nwent golfing together , meaning they are\ndifferent people.\n7 10.1 The same job cannot be assigned to\ndifferent people.\n8 10.2 Roberta is a female.\n9 10.3 Thelma is a female.\n10 10.4 Steve is a male.\n11 10.5 Pete is a male.\n12 10.6 The actor is a male.\n13 Question: Who holds which jobs?\nby turning clues 1–4 as background story, clarifying clues 6,\n8, and 9, and adding a few hidden clues numbered 10.X at\nthe end.\nAs for the prompts, we only need to update Line 1 of\nPrompt R1 to the following to allow for{...}=k in a rule.\n1 Given a problem as the background information ,\nturn the provided categorized constants and\npredicates into rules.\n2\n3 Tasks:\n4 1. Write a rule of the form \"category(\nconstant_1; constant_2; ...; constant_n)\" to\nrepresent each row of categorized constants in\nthe form \"category: constant_1; constant_2;\n...; constant_n \".\n5 2. Write a rule of the form \"{ predicate(X_1 ,\nX_2 , ..., X_n): category_2(X_2), ...,\ncategory_n(X_n)}=k :- category_1(X_1)\" for\neach predicate of the form \"predicate(X_1 , X_2\n, ..., X_n)\".\nFinally, GPT-3 generates the following ASP program:\n1 % Task 1. Define the constants in each category\n.\n2 person (\" Roberta \"; \"Thelma \"; \"Steve \"; \"Pete \").\n3 job(\" chef \"; \"guard \"; \"nurse \"; \"telephone\noperator \"; \"police officer \"; \"teacher \"; \"actor\n\"; \"boxer \").\n4 gender (\" male \"; \"female \").\n5 % Task 2. For each person P, there is exactly 2\njobs J and gender G.\n6 {assign(P, J, G): job(J), gender(G)}=2 :-\nperson(P).\n7 % Done tasks 1 and 2.\n8 % 5. The job of nurse is held by a male.\n9 G=\" male\" :- assign(P,J,G), J=\" nurse \".\n10 % 6. The husband of the chef is the telephone\noperator , which means the chef is a female and\nthe telephone operator is a male.\n11 G1=\" female\" :- assign(P1 ,J1 ,G1), assign(P2 ,J2 ,\nG2), J1=\" chef\", J2=\" telephone operator \".\n12 G2=\" male\" :- assign(P1 ,J1 ,G1), assign(P2 ,J2 ,G2)\n, J1=\" chef\", J2=\" telephone operator \".\n13 % 7. Roberta is not a boxer.\n14 J!=\" boxer\" :- assign(P,J,G), P=\" Roberta \".\n15 % 8. Pete has no education past the ninth grade\n. (Three jobs teacher , nurse , and police\nofficer require higher education .)\n16 {J=\" teacher \"; J=\" nurse \"; J=\" police officer \"}=0\n:- assign(P,J,G), P=\" Pete \".\n17 % 9. Roberta , the chef , and the police officer\nwent golfing together , meaning they are\ndifferent people.\n18 P1!=P2 :- assign(P1 ,J1 ,G1), assign(P2 ,J2 ,G2),\nassign(P3 ,J3 ,G3), P1=\" Roberta\", J2=\" chef\", J3\n=\" police officer \".\n19 P2!=P3 :- assign(P1 ,J1 ,G1), assign(P2 ,J2 ,G2),\nassign(P3 ,J3 ,G3), P1=\" Roberta\", J2=\" chef\", J3\n=\" police officer \".\n20 P1!=P3 :- assign(P1 ,J1 ,G1), assign(P2 ,J2 ,G2),\nassign(P3 ,J3 ,G3), P1=\" Roberta\", J2=\" chef\", J3\n=\" police officer \".\n21 % 10.1 The same job cannot be assigned to\ndifferent people.\n22 {P1=P2}=0 :- assign(P1,J,G1),\n23 assign(P2,J,G2), (P1,G1)!=(P2,G2).\n24 % 10.2 Roberta is a female.\n25 G=\" female\" :- assign(P,J,G), P=\" Roberta \".\n26 % 10.3 Thelma is a female.\n27 G=\" female\" :- assign(P,J,G), P=\" Thelma \".\n28 % 10.4 Steve is a male.\n29 G=\" male\" :- assign(P,J,G), P=\" Steve \".\n30 % 10.5 Pete is a male.\n31 G=\" male\" :- assign(P,J,G), P=\" Pete \".\n32 % 10.6 The actor is a male.\n33 G=\" male\" :- assign(P,J,G), J=\" actor \".\nwhich is almost correct with a single mistake in translating\nclue 10.1. If we just replace this constraint (in red) with\n1 P1=P2 :- assign(P1 ,J,G1), assign(P2 ,J,G2).\nthe corrected ASP program has exactly one stable model,\nwhich is the correct solution to the Jobs Puzzle.\nSimilarly, GPT-4 also failed to generate a completely cor-\nrect ASP program. It also couldn’t generate a correct rule for\nconstraint 10.1, and furthermore failed to produce the gen-\nder category in constant extraction stepPrompt C), missing\n“gender: \"male\"; \"female\".”\n7 Conclusion\nLLMs are a relatively recent technology that have shown to\nbe disruptive. Despite their wide range of applications, their\nresponses are not always reliable and cannot be trusted.\nAutomatic rule generation is a difficult problem. How-\never, by using LLMs as a front-end to answer set program-\nming, we can utilize their linguistic abilities to translate nat-\nural language descriptions into the declarative language of\nanswer set programs. Unlike previous methods that use al-\ngorithmic or machine learning techniques, we find that a pre-\ntrained large language model with a good prompt can gener-\nate reasonably accurate answer set programs. We present a\npipeline with general steps that systematically build an ASP\nprogram in a natural way. This method not only leads to\nhigher accuracy but also makes the results interpretable.\nWe expect this type of work to expand the application of\nKR methods that may appear unfamiliar to non-experts. We\nalso anticipate that this pipeline will serve as a suggestion\ntool to help users prepare valid constants, useful predicates,\nor draft ASP programs.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n382\nAcknowledgements\nWe are grateful to the anonymous referees for their useful\ncomments. This work was partially supported by the Na-\ntional Science Foundation under Grant IIS-2006747.\nReferences\nBaral, C., and Dzifcak, J. 2012. Solving puzzles described\nin english by automated translation to answer set program-\nming and learning how to do that translation. InProceedings\nof the Thirteenth International Conference on Principles of\nKnowledge Representation and Reasoning, 573–577.\nBrewka, G.; Niemel ¨a, I.; and Truszczynski, M. 2011. An-\nswer set programming at a glance. Communications of the\nACM 54(12):92–103.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems 33:1877–\n1901.\nCreswell, A.; Shanahan, M.; and Higgins, I. 2022.\nSelection-inference: Exploiting large language models\nfor interpretable logical reasoning. arXiv preprint\narXiv:2205.09712.\nJabrayilzade, E., and Tekir, S. 2020. LGPSolver - solving\nlogic grid puzzles automatically. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020, 1118–\n1123.\nLev, I.; MacCartney, B.; Manning, C. D.; and Levy, R. 2004.\nSolving logic puzzles: From robust processing to precise se-\nmantics. In Proceedings of the 2nd Workshop on Text Mean-\ning and Interpretation, 9–16.\nLifschitz, V . 2008. What is answer set programming? In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, 1594–1597. MIT Press.\nMilicevic, A.; Near, J. P.; and Singh, R. 2012. Puzzler:\nAn automated logic puzzle solver. Technical report, Mas-\nsachusetts Institute of Technology (MIT).\nMitra, A., and Baral, C. 2015. Learning to automatically\nsolve logic grid puzzles. In Proceedings of the 2015 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 1023–1033.\nNordstrom, R. 2017. LogicSolver - Solving logic grid puz-\nzles with part-of-speech tagging and first-order logic. Tech-\nnical report, University of Colorado, Colorado Springs.\nNye, M.; Tessler, M.; Tenenbaum, J.; and Lake, B. M.\n2021. Improving coherence and consistency in neural se-\nquence models with dual-system, neuro-symbolic reason-\ning. Advances in Neural Information Processing Systems\n34:25192–25204.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoff-\nmann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.;\nYoung, S.; et al. 2021. Scaling language models: Meth-\nods, analysis & insights from training gopher.arXiv preprint\narXiv:2112.11446.\nSchwitter, R. 2010. Controlled natural languages for knowl-\nedge representation. In Coling 2010: Posters, 1113–1121.\nSchwitter, R. 2013. The jobs puzzle: Taking on the chal-\nlenge via controlled natural language processing. Theory\nand Practice of Logic Programming 13(4-5):487–501.\nShapiro, S. C. 2011. The jobs puzzle: A challenge for logi-\ncal expressibility and automated reasoning. In AAAI spring\nsymposium: logical formalizations of commonsense reason-\ning.\nValmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambham-\npati, S. 2022. Large language models still can’t plan\n(a benchmark for LLMs on planning and reasoning about\nchange). In NeurIPS 2022 Foundation Models for Decision\nMaking Workshop.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; brian ichter;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022. Chain of\nthought prompting elicits reasoning in large language mod-\nels. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho, K.,\neds., Advances in Neural Information Processing Systems.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Bousquet, O.; Le, Q.; and Chi, E. 2022.\nLeast-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625.\nProceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning\nMain Track\n383",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7990585565567017
    },
    {
      "name": "Natural language",
      "score": 0.6690633296966553
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6320643424987793
    },
    {
      "name": "Answer set programming",
      "score": 0.6249560713768005
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5367971658706665
    },
    {
      "name": "Context (archaeology)",
      "score": 0.528112530708313
    },
    {
      "name": "Logic programming",
      "score": 0.4914964437484741
    },
    {
      "name": "Natural language understanding",
      "score": 0.4780624210834503
    },
    {
      "name": "Natural language programming",
      "score": 0.4732525944709778
    },
    {
      "name": "Task (project management)",
      "score": 0.4241480529308319
    },
    {
      "name": "Programming language",
      "score": 0.4088316857814789
    },
    {
      "name": "Natural language processing",
      "score": 0.4078810214996338
    },
    {
      "name": "Universal Networking Language",
      "score": 0.37167030572891235
    },
    {
      "name": "Comprehension approach",
      "score": 0.155324786901474
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}