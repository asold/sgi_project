{
  "title": "MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning",
  "url": "https://openalex.org/W4401863877",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2914489391",
      "name": "Sanchit Sinha",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2634280861",
      "name": "Yuguang Yue",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1911799498",
      "name": "VÃ­ctor Soto",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2805390021",
      "name": "Mayank Kulkarni",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2181857529",
      "name": "Jianhua Lu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2150231934",
      "name": "Aidong Zhang",
      "affiliations": [
        "University of Virginia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205058818",
    "https://openalex.org/W4385573777",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4285189120",
    "https://openalex.org/W3192405822",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W3167554296",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W4385573261",
    "https://openalex.org/W2747329762"
  ],
  "abstract": "Adapting large language models (LLMs) to unseen tasks with incontext training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only performs well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in metatraining literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.",
  "full_text": null,
  "topic": "Training (meteorology)",
  "concepts": [
    {
      "name": "Training (meteorology)",
      "score": 0.6902651190757751
    },
    {
      "name": "Computer science",
      "score": 0.6802680492401123
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6431338787078857
    },
    {
      "name": "Meta learning (computer science)",
      "score": 0.4466184675693512
    },
    {
      "name": "Context model",
      "score": 0.4209586977958679
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4032916724681854
    },
    {
      "name": "Engineering",
      "score": 0.14072981476783752
    },
    {
      "name": "History",
      "score": 0.09661829471588135
    },
    {
      "name": "Systems engineering",
      "score": 0.09043213725090027
    },
    {
      "name": "Geography",
      "score": 0.07979810237884521
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}