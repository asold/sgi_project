{
    "title": "MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning",
    "url": "https://openalex.org/W4401863877",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2914489391",
            "name": "Sanchit Sinha",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2634280861",
            "name": "Yuguang Yue",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1911799498",
            "name": "VÃ­ctor Soto",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2805390021",
            "name": "Mayank Kulkarni",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2181857529",
            "name": "Jianhua Lu",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2150231934",
            "name": "Aidong Zhang",
            "affiliations": [
                "University of Virginia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3205058818",
        "https://openalex.org/W4385573777",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W4285189120",
        "https://openalex.org/W3192405822",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W3167554296",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3152515526",
        "https://openalex.org/W4385573261",
        "https://openalex.org/W2747329762"
    ],
    "abstract": "Adapting large language models (LLMs) to unseen tasks with incontext training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only performs well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in metatraining literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches.",
    "full_text": null
}