{
  "title": "Towards LLM-driven Dialogue State Tracking",
  "url": "https://openalex.org/W4389523682",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2239412783",
      "name": "Yujie Feng",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2261557960",
      "name": "Zexin Lu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2036346442",
      "name": "Bo Liu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2165234013",
      "name": "Liming Zhan",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2196274784",
      "name": "Xiao-Ming Wu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4303685247",
    "https://openalex.org/W4224310111",
    "https://openalex.org/W4367046802",
    "https://openalex.org/W4385572445",
    "https://openalex.org/W4323706279",
    "https://openalex.org/W2945475330",
    "https://openalex.org/W4287992022",
    "https://openalex.org/W4385570786",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4306317016",
    "https://openalex.org/W3156877449",
    "https://openalex.org/W4281398986",
    "https://openalex.org/W4321276803",
    "https://openalex.org/W4372272550",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4224275713",
    "https://openalex.org/W4225553189",
    "https://openalex.org/W4385823413",
    "https://openalex.org/W4385768065",
    "https://openalex.org/W4288094254",
    "https://openalex.org/W4385573192",
    "https://openalex.org/W4382318248",
    "https://openalex.org/W3200895474",
    "https://openalex.org/W4206496297",
    "https://openalex.org/W4385574016",
    "https://openalex.org/W4323650985",
    "https://openalex.org/W4322759717",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4385573341",
    "https://openalex.org/W3045703328",
    "https://openalex.org/W2962831269",
    "https://openalex.org/W4321472314",
    "https://openalex.org/W4385573382",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4327525855",
    "https://openalex.org/W4308915726",
    "https://openalex.org/W4386566453",
    "https://openalex.org/W4229044077",
    "https://openalex.org/W4226451629",
    "https://openalex.org/W3099827451",
    "https://openalex.org/W4385572807",
    "https://openalex.org/W4224131719",
    "https://openalex.org/W3021096583",
    "https://openalex.org/W2997771882",
    "https://openalex.org/W4404783626",
    "https://openalex.org/W4385572851",
    "https://openalex.org/W3208705495",
    "https://openalex.org/W4290945382",
    "https://openalex.org/W4312469624",
    "https://openalex.org/W3015322406",
    "https://openalex.org/W4367046670",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3168491067",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4365601361",
    "https://openalex.org/W4364383757",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W4386081234",
    "https://openalex.org/W4389009440"
  ],
  "abstract": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT's capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 739–755\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards LLM-driven Dialogue State Tracking\nYujie Feng Zexin Lu Bo Liu Liming Zhan Xiao-Ming Wu ∗\nDepartment of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.\n{yujie.feng, zexin.lu, bokelvin.liu, lmzhan.zhan}@connect.polyu.hk\nxiao-ming.wu@polyu.edu.hk\nAbstract\nDialogue State Tracking (DST) is of paramount\nimportance in ensuring accurate tracking of\nuser goals and system actions within task-\noriented dialogue systems. The emergence of\nlarge language models (LLMs) such as GPT3\nand ChatGPT has sparked considerable inter-\nest in assessing their efficacy across diverse\napplications. In this study, we conduct an ini-\ntial examination of ChatGPT’s capabilities in\nDST. Our evaluation uncovers the exceptional\nperformance of ChatGPT in this task, offer-\ning valuable insights to researchers regarding\nits capabilities and providing useful directions\nfor designing and enhancing dialogue systems.\nDespite its impressive performance, ChatGPT\nhas significant limitations including its closed-\nsource nature, request restrictions, raising data\nprivacy concerns, and lacking local deployment\ncapabilities. To address these concerns, we\npresent LDST, an LLM-driven DST framework\nbased on smaller, open-source foundation mod-\nels. By utilizing a novel domain-slot instruction\ntuning method, LDST achieves performance\non par with ChatGPT. Comprehensive evalua-\ntions across three distinct experimental settings,\nwe find that LDST exhibits remarkable per-\nformance improvements in both zero-shot and\nfew-shot setting compared to previous SOTA\nmethods. The source code1 is provided for re-\nproducibility.\n1 Introduction\nTask-oriented dialogue systems have emerged as\npowerful tools for assisting users in accomplishing\na wide range of tasks (Huang et al., 2020). These\nsystems, such as Apple Siri and Microsoft Cortana,\nfunction as virtual personal assistants, providing\nsupport for tasks like flight reservations, appoint-\nment scheduling, and hotel bookings. Dialogue\nState Tracking (DST) plays a crucial role in task-\noriented dialogue systems by accurately tracking\n∗Corresponding author.\n1https://github.com/WoodScene/LDST\nFigure 1: Example of a multi-domain dialogue. The\nslots “hotel-pricerange” and “restaurant-pricerange”\nhave a co-reference relationship, where the value of the\nformer is inferred from the latter. The slot “restaurant-\narea” demonstrates error propagation behavior.\nthe evolving user goals and system actions during\na conversation. In general, the multi-domain di-\nalogue state is represented as a list of triplets in\nthe form of (domain, slot, value), e.g., “<restau-\nrant, area, east>”. These predefined slot pairs are\nextracted from the dialogue context at each turn.\nA plethora of models have been proposed to\naddress the challenges of multi-domain DST, as\ndocumented in recent studies (Qixiang et al., 2022;\nZhou et al., 2022; Feng et al., 2022b; Guo et al.,\n2022a; Yang et al., 2022; Ma et al., 2023; Xu et al.,\n2023a). These models primarily focus on effective\ntransfer and generalization across diverse domains,\naddressing the crucial challenges of co-reference\n(Feng et al., 2022a) and error propagation prob-\nlem (Wang and Xin, 2022) depicted in Figure 1.\nThe co-reference challenge poses a significant hur-\ndle in enhancing DST performance, as it arises\nfrom the linguistic variations in multi-turn dia-\nlogues where slots and values are often indirectly\n739\nexpressed. Moreover, the error propagation issue\nemerges when the model fails to recognize and\nrectify errors in the previously predicted dialogue\nstate, leading to the persistence of errors in subse-\nquent turns. Despite significant efforts to address\nthese issues, they persist as ongoing challenges.\nIn recent days, the emergence of large-scale pre-\ntrained language models has revolutionized the\nfield of natural language processing (NLP). Models\nlike ChatGPT2 have shown excellent performance,\nsparking significant interest in evaluating their ef-\nfectiveness across different dimensions (Tan et al.,\n2023; Wang et al., 2023; Jiao et al., 2023; Yang\net al., 2023a; Gao et al., 2023; Liu et al., 2023). De-\nspite the significant advancements made by large\nlanguage models (LLMs), their performance in\nmulti-domain DST remains relatively unexplored.\nTo bridge this research gap, we conduct an evalua-\ntion of ChatGPT’s capabilities for DST. The evalu-\nation unveils ChatGPT’s exceptional performance\nin the DST task, offering valuable insights to re-\nsearchers and providing useful directions for fur-\nther exploration.\nWhile ChatGPT demonstrates superb perfor-\nmance, it has significant limitations (Zhou et al.,\n2023; Yang et al., 2023a; Cao et al., 2023). Firstly,\nit is not open source, so the underlying code and\nmodel parameters cannot be modified by users.\nSecond, it is subject to request limitations, which\ncan restrict its usage in high-demand scenarios. Fur-\nthermore, there are concerns regarding strong data\nprivacy protection, as the system may collect and\nstore user data. Lastly, ChatGPT cannot be de-\nployed locally, limiting its availability and control.\nThese limitations hinder the applicability and adop-\ntion of ChatGPT in various practical scenarios for\nbuilding task-oriented dialogue systems.\nTo overcome the limitations of ChatGPT, we in-\ntroduce LDST, a DST framework driven by LLMs\nbut based on smaller, open-source foundation mod-\nels. LDST employs a novel assembled domain-\nslot instruction tuning method and a parameter effi-\ncient tuning technique, enabling it to achieve per-\nformance comparable to ChatGPT while utilizing\na much smaller model and limited computational\nresources. LDST demonstrates exceptional perfor-\nmance across three different experimental settings,\nsurpassing prior state-of-the-art methods by a large\nmargin and demonstrating its remarkable adapt-\nability and generalization capabilities. Our main\n2https://chat.openai.com\ncontributions are concluded as follows:\n• We present the first evaluation of ChatGPT\nin DST task, highlighting its superior perfor-\nmance over prior methods and providing valu-\nable insights for advancing dialogue systems.\n• We propose LLM-driven DST (LDST) based\non smaller, open-source foundation models.\nLDST achieves comparable performance to\nChatGPT by employing an innovative assem-\nbled domain-slot instruction tuning technique.\n• We extensively evaluate LDST on three bench-\nmark datasets across various experimental set-\ntings, revealing significant performance im-\nprovements over previous approaches. In the\nzero-shot scenario, LDST boosts the JGA\nscore by 16.9%, elevating it from 65.3% to an\noutstanding 82.2%. In the few-shot scenario,\nLDST improves the JGA score by 7.5%, rais-\ning it from 47.7% to a notable 55.2%.\n2 Assessing the Capabilities of ChatGPT\nfor DST\nIn this section, we evaluate the effectiveness of\nChatGPT in addressing the DST task. Before going\ninto detail, we first formally define the problem.\nDST: Problem Formulation In task-oriented di-\nalogue systems, a dialogue with T turns of conver-\nsations between the system and the user can be rep-\nresented as {(A1, U1) , (A2, U2) . . . ,(AT , UT )},\nwhere A represents system response and U rep-\nresents user input. A predefined slot set S =\n{S1, . . . , SJ}is given, where J is the total number\nof slots. The dialogue context at turn t includes\nprevious turns of interactions, denoted as Xt =\n{(A1, U1) , (A2, U2) . . . ,(At, Ut)}. The dialogue\nstate at turn t is represented as a set of (slot, value)\npairs, denoted as Bt =\n{(\nS1, Vt\n1\n)\n, . . . ,\n(\nSJ, Vt\nJ\n)}\n,\nwhere V t\nJ is the value of slot SJ. For multi-domain\nDST, following previous works (Lee et al., 2019),\na slot is defined as the concatenation of the specific\ndomain and the slot, e.g., “<restaurant-area>”. If no\ninformation is provided in the dialogue about a spe-\ncific slot, the value associated with that slot is set to\n“NONE”. Essentially, the DST problem is defined\nas learning a dialogue state tracker F: Xt →Bt.\nLeveraging ChatGPT for DST We evaluate the\nperformance of ChatGPT (using the gpt-3.5-turbo\nAPI service) on three multi-domain DST bench-\nmarks, using the JGA and AGA evaluation metrics\n740\nFigure 2: Illustration of different prompt templates used and the corresponding results on the MultiWOZ 2.2 test set.\n(for detailed descriptions of the datasets and met-\nrics, refer to Section 4.1). As shown in Figure 2,\nwe explore various prompt templates and utilize\nthe MultiWOZ 2.2 dataset (Zang et al., 2020) to\nselect the optimal prompt.\nIn Figure 2, “single return” and “multi return”\nrefer to the number of slot values returned in each\nChatGPT API request. “Single return” involves\nrequesting and receiving values for one slot at a\ntime, while “multi return” entails requesting and\nreceiving values for all slots simultaneously. For\ninstance, in the MultiWOZ 2.2 dataset which has\n49 different slots, “multi return” retrieves values for\nall 49 slots in a single request. This causes a signif-\nicant increase API requests for \"single return\" but\nsimplifies the model’s task, resulting in improved\nperformance. Conversely, “multi return” reduces\nAPI requests but increases token count per request.\n\"No/one demo\" denotes whether an example is pro-\nvided in the prompt as a demonstration to aid the\nmodel’s comprehension of the task. Selecting \"one\ndemo\" is similar to adopting the in-context learn-\ning concept. Detailed prompt template design is\nprovided in the Appendix A.1.\nPerformance of ChatGPT As can be seen from\nFigure 2, the first prompt, which retrieves the value\nof a single slot in each request without including\na demo in the input, achieves the highest AGA\nscore. This is attributed to the inherent difficulty\nof the task that necessitates the model to provide\nmultiple slot values in a single request. We have\nobserved that ChatGPT tends to predict “NONE”\nfor slots that should have a specific value. For in-\nstance, in the case of the slot “hotel-leaveat” where\n(a) JGA score\n (b) AGA score\nFigure 3: The results of the best baseline and ChatGPT\non various datasets. The higher the values of the JGA\nand AGA metrics, the better. SOTA results for Multi-\nwoz 2.2, Multiwoz 2.4, JGA score for SGD datasets,\nand AGA score for SGD datasets were obtained from\nprevious works (Bang et al., 2023a; Ye et al., 2022a;\nZhao et al., 2022; Feng et al., 2022a), respectively.\nthe expected value is “14:00”, ChatGPT may in-\ncorrectly predict “NONE”, resulting in lower pre-\ndiction accuracy. Secondly, the addition of a demo\nto the input has a reduced effect, which may seem\ncounter-intuitive. However, our analysis of the er-\nror results suggests that ChatGPT also analyzes the\ndialogue context within the demo, even when the\ndemo and tested sample are clearly differentiated\nin the input. Therefore, we chose the first prompt\nas the best template for the subsequent evaluation.\nThe full evaluation results of ChatGPT on the\nthree datasets3 are shown in Figure 3. Firstly, on\nthe SGD dataset, the AGA score of ChatGPT is sig-\nnificantly superior than the previous SOTA method\n(Feng et al., 2022a), and it achieves a 7.46% ab-\n3The evaluation of the MultiWOZ 2.2 dataset were con-\nducted between April 15th and 18th, 2023. The evaluations of\nMultiWOZ 2.4 occurred between June 10th and 12th, 2023.\nThe SGD was assessed between June 14th and 17th, 2023.\n741\nsolute imporvement in AGA score. In addition,\nthe average improvement on the three datasets is\n0.73% in JGA score and 2.34% in AGA score.\nSecondly, ChatGPT’s performance on the Multi-\nWOZ 2.2 dataset is slightly worse than the previ-\nous SOTA method (Bang et al., 2023a). However,\nthrough careful analysis of the errors, we found\nthat 70% of them were due to annotation errors\nin the original dataset. Thus, on the MultiWOZ\n2.4 dataset which has fixed the annotation errors,\nChatGPT outperforms the best baseline method (Ye\net al., 2022a).\nLimitations of ChatGPT In summary, ChatGPT\nexhibits comparable performance when solving the\nDST task compared to the previous SOTA meth-\nods. This highlights the ability of current LLMs\nto capture and comprehend complex linguistic pat-\nterns and dependencies within multi-turn dialogues.\nHowever, ChatGPT has several significant limita-\ntions that impede its applicability and adoption in\nvarious practical scenarios. Firstly, we observed\nthat ChatGPT often provides responses with a sig-\nnificant amount of explanatory content, or it may\nnot align perfectly with our expected answer for-\nmat. For instance, when the ground truth value is\n“2 pm,” ChatGPT might return “14:00.” While both\nare essentially correct answers, such variations can\naffect the accuracy of the final metrics. And Chat-\nGPT is not open source, which restricts the ability\nof developers and researchers to modify and cus-\ntomize the model. Secondly, ChatGPT is subject to\nrequest limitations, which may impact real-time or\nhigh-volume applications that rely on prompt and\nefficient responses. Furthermore, ChatGPT oper-\nates in a cloud-based environment and lacks strong\ndata privacy protection measures, which raises con-\ncerns about the privacy and security of sensitive\ninformation shared during the dialogue sessions.\nLastly, ChatGPT relies on an internet connection\nfor operation and cannot be deployed locally.\n3 Fine-tuning Smaller Foundation\nModels with Instructions for DST\nTo overcome the aforementioned limitations\nof ChatGPT, we introduce LDST, an LLM-\ndriven DST framework that leverages fine-tuning\nsmaller, open-source foundation models such as\nLLaMa (Touvron et al., 2023) with instructions\nspecially tailored for DST. We first outline the\nprocess of constructing an instruction dataset for\nthe multi-domain DST task. Next, we utilize a\nparameter-efficient fine-tuning (PEFT) technique\nto train the foundation model with the instruction\ndataset. PEFT enables the training of a foundation\nmodel with limited computational resources.\n3.1 Instruction Tuning\nUnlike prompt tuning, instruction tuning (Chung\net al., 2022) provides more explicit and detailed\nguidance to the model through task-specific instruc-\ntions. This allows for finer control over the model’s\nbehavior and leads to improved performance com-\npared to prompt tuning. The core idea of instruction\ntuning is designing the instruction dataset, typically\nincluding instruction, input, and output fields. Usu-\nally, different instructions are assigned for different\ntasks. However, employing a fixed instruction tem-\nplate for multi-domain DST may limit the model’s\nrobustness, as emphasized by Wang et al. (2023),\nwhich highlights the crucial influence of prompt\ndesign on model performance.\nTo address this challenge, we propose a novel\nAssembled Domain-Slot Instruction Generation ap-\nproach for the DST task. This approach generates\ndiverse instruction samples by randomly combin-\ning different instruction and input templates, expos-\ning the model to a rich variety of instruction types\nduring the fine-tuning process to reduce the model’s\nsensitivity to prompts. As shown by the provided\nexample in Figure 4, for each sample in the original\ndataset, it consists of the dialogue contextXt at turn\nt, the current requested slot SJ and its correspond-\ning state V t\nJ . The raw data is then passed through\nthe Instruction Data Generation module to generate\ninstruction samples. The detailed template settings\nfor each field are introduced as follows.\nInstruction Prompt Specifically, two types of\ninstruction templates are defined: (1) Standard\nSlot Tracking Instruction and (2) Customized Slot\nTracking Instruction. The difference between them\nis that the Customized Slot Tracking Instruction\nprovides a more specific domain-slot information.\nAnd the instruction field of each sample is ran-\ndomly selected from these two templates.\nInput Prompt For the input field, the prompt\ntemplate is composed of four main parts: (1) the di-\nalogue context, (2) domain-slot description prompt,\n(3) Possible Value List (PVL) prompt and (4) the\nquery prompt. The green, purple, blue and orange\ntext in the example in Figure 4 refers to these four\nprompts respectively. In particular, we concate-\nnate all sub-sequences with special segment tokens,\n742\nFigure 4: Structure of the LDST model. In the first step, we construct the instruction dataset from the original\ndataset using the Instruction Data Generation module. Next, we utilize the parameter-efficient fine-tuning technique\nto train the foundation model with the instruction dataset.\nsuch as the “[USER]” segment token used to in-\ndicate the start of a system utterance. And both\nthe domain-slot description prompt and the PVL\nprompt are supplementary descriptions of the re-\nquested slot, they all derive from the given schema\nin original dataset (PVL prompts are only available\nfor categorical slots). Here, to simulate the situa-\ntion when the model may not get a description of\nthe requested slot or it’s possible values during the\ntesting phase, we add these two prompt templates\nrandomly with a 50% probability, respectively.\nOuput Prompt Finally, the output field consists\nof the corresponding value V t\nJ of the requested slot\nSJ. By following the aforementioned process, we\nobtained a newly and diverse instruction dataset for\nthe next step of fine-tuning the model.\n3.2 Parameter Efficient Tuning\nIn this part, we describe how to fine-tune the foun-\ndation model using a parameter efficient approach.\nLDST takes the instruction and input field from the\ndataset as inputs and retrieves the corresponding\nslot value V t\nJ as output:\nV t\nJ = Decoder( ˆX) (1)\nwhere Decoder indicates that the foundation model\n(e.g., LLaMa) uses the Transformer-decoder frame-\nwork, and ˆXdenotes the instruction data, i.e., the\ncombination of instruction and input fields.\nAs shown in Figure 4, to enhance the efficiency\nof the fine-tuning process and reduce memory\nrequirements, we utilize Low-Rank Adaptation\n(LoRA) (Hu et al., 2021). LoRA freezes the pre-\ntrained model weights and injects trainable rank de-\ncomposition matrices into each layer of the Trans-\nformer architecture, greatly reducing the number\nof trainable parameters for downstream tasks. For\nexample, in our experiment with LLaMa 7B, the\nnumber of learnable parameters is 8.4M, which\nis only 0.12% of the total model parameters. De-\nnote by the trainable parameters as a weight ma-\ntrix W0 ∈ Rd×k. Unlike traditional methods\nthat directly modify the values of W0, LoRA in-\ntroduces an additional set of trainable parame-\nters, denoted as ∆W, which are directly injected\ninto the original W0. We represent the update as\nW = W0 + ∆W = W0 + BA, where B ∈Rd×r,\nA ∈Rr×k. The rank r ≪min(d, k). During train-\ning, W0 is frozen and does not receive any gradient\nupdates, we only update the parameters in A and\nB. Note both W0 and ∆W = BA are multiplied\nwith the same input, and their respective output\nvectors are summed coordinate-wise. For the origi-\nnal output h = W0x, LoRA modified forward pass\nyields:\nh = W0x + ∆Wx = W0x + BAx. (2)\nFinally, the learning objective of the generation\nprocess in LDST is to minimize the negative log-\nlikelihood of V t\nJ given the context Xt and slot SJ:\nL = −\nT∑\nt\nJ∑\nj\nlog p\n(\nV t\nj |Xt, Sj\n)\n. (3)\n4 Experiments\n4.1 Datasets\nWe conducted experiments using the benchmark\ndatasets for multi-domain task-oriented dialogue,\nand Table 1 gives detailed statics on these datasets.\n743\nCharacteristics SGD MultiWOZ 2.2 MultiWOZ 2.4\nNo. of domains 16 8 7No. of dialogues 16,142 8,438 8,438Total no. of turns 329,964 113,556 113,556Avg. turns per dialogue20.44 13.46 13.46Avg. tokens per turn9.75 13.13 13.38No. of slots 215 61 37Have schema descriptionYes Yes YesUnseen domains in test setYes No No\nTable 1: Statistics of the datasets used for training in\nour experiments.\nSchema-Guided Dialogue (SGD) SGD (Rastogi\net al., 2020) is the most challenging dataset, consist-\ning of over 16,000 conversations between a human-\nuser and a virtual assistant. It encompasses 26\nservices across 16 domains, such as events, restau-\nrants, and media. Notably, SGD introduces unseen\ndomains in its test set, challenging the generaliza-\ntion ability of the model.\nMultiWOZ 2.2 and MultiWOZ 2.4 MultiWOZ\n2.4 (Ye et al., 2022a) is an updated version on top\nof MultiWOZ 2.2 (Zang et al., 2020) to improve\nDST evaluation, and the validation set and test set\nof MultiWOZ 2.4 have been carefully reannotated.\nWe therefore treat it as a clean dataset for testing.\nWe also conduct experiments on MultiWOZ 2.2\nwhich is known to contain annotation noise. We\nused this noisy dataset to test the robustness of the\nmodel and to analyse the ability of the model to\ndetect annotation errors present in the test set.\n4.2 Evaluation Metrics\nWe adopt the following evaluation metrics, consis-\ntent with previous works (Ye et al., 2022b): Joint\nGoal Accuracy ( JGA) and Average Goal Accu-\nracy (AGA). JGA serves as the primary metric for\nDST evaluation and represents the ratio of dialogue\nturns for which the entire state is correctly pre-\ndicted. AGA is the average accuracy of the active\nslots in each turn. A slot becomes active if its value\nis mentioned in the current turn and is not inherited\nfrom previous turns.\n4.3 Main Results\nWe conducted full-scale evaluations of the LLM-\ndriven LDST model in three distinct experimental\nsettings, where the model showed a tremendous\nperformance improvement in both zero-shot and\nfew-shot settings. These findings can provide valu-\nable insights and contribute to the research com-\nmunity through substantial advances in the field of\nDST. The detailed results are as follows:\nDomain SGD-baseline TransferQA SDM-DST D3STLDST\nMessaging 10.2/20.0 13.3/37.9 36.6/61.4 -89.6/90.4Payment 11.5/34.8 24.7/60.7 16.5/62.0 -92.3/96.4Trains 13.6/63.5 17.4/64.9 46.7/86.9 - 81.0/94.0Alarm 57.7/1.8 58.3/81.7 58.3/87.5 - 94.4/96.9\nAverage 20.5/34.2 25.9/61.8 40.4/76.8 83.3/-89.3/94.4\nTable 2: Zero-shot results (JGA(%)/A VG(%)) on SGD.\nDomain TRADE SUMBT SimpleTOD T5DST D3STLDST\nAttraction 19.87 22.60 28.01 33.09 57.10 75.61Hotel 13.70 19.80 17.69 21.21 22.30 63.32Restaurant 11.52 16.50 15.57 21.82 38.90 73.72Taxi 60.58 59.50 59.22 65.09 79.00 91.47Train 22.37 22.50 27.75 35.42 39.60 71.03\nAverage 25.76 28.18 29.65 35.20 47.38 75.03\nTable 3: Zero-shot results (JGA(%)/A VG(%)) on Multi-\nWOZ 2.0.\nZero-shot Cross-domain Results Following pre-\nvious zero-shot settings (Wang et al., 2022c), all\nmodels are first trained on some domains and then\nevaluated on the test-set of the unseen domain.\nHere we compare with the baseline models that can\npredict dialogue state on unseen domains: SGD-\nbaseline (Rastogi et al., 2020), TransferQA (Lin\net al., 2021a), SDM-DST (Wang et al., 2022a),\nSUMBT (Lee et al., 2019), SimpleTOD (Hosseini-\nAsl et al., 2020), T5DST (Lin et al., 2021b) and\nD3ST method (Zhao et al., 2022).\nTables 2 and 3 highlight the exceptional perfor-\nmance of our approach in zero-shot cross-domain\nDST. Specifically, on the SGD dataset, LDST\nachieves a remarkable 6.0% absolute increase in the\nJGA score when compared to the larger T5-XXL\n(11B)-based D3ST model, elevating it from 83.3%\nto an impressive 89.3%. Additionally, the AGA\nscore experiences a substantial surge of 17.6%, es-\ncalating from 76.8% to a remarkable 94.4%.\nOn the MultiWOZ 2.0 dataset, we observe a sig-\nnificant advancement in the average JGA score,\nsurging from 47.38% to 75.03%, reflecting an ab-\nsolute improvement of 27.65%. Notably, the Pay-\nment domain in the SGD dataset displays the most\nprominent improvement, with the JGA metric soar-\ning from 24.7% to an astounding 92.3%. This re-\nmarkable enhancement is attributed to the Payment\ndomain’s independence from the source domains.\nThis significant boost not only demonstrates the\npowerful transfer learning capabilities of the LDST\nmodel but also emphasizes its valuable implications\nfor the DST research community.\nFew-shot Results In the few-shot settings, we\nfollow the multi-domain scenario from Wu et al.\n744\nModels\nMultiWOZ 2.4\n1% 5% 10%\nDS2-BART 30.55 42.53 41.73\nDS2-T5 36.76 49.89 51.05\nIC-DST GPT-Neo17.36 29.62 34.38\nSM2-11b 40.03 51.14 51.97\nLDST 46.77 56.48 62.45\nTable 4: Results (in JGA(%)) of few-shot experiments\non MultiWOZ 2.4.\n(2019), where randomly select 1%, 5%, and 10%\nof training dialogues to train, and evaluation is\nconducted on the full test set of each domain. The\nevaluation results on MultiWOZ 2.4 are shown in\nTable 4, where we compare with SOTA few-shot\nDST methods: DS2-BART (Shin et al., 2022), DS2-\nT5 (Shin et al., 2022), IC-DST GPT-Neo (Hu et al.,\n2022), and SM2-11b (Chen et al., 2023).\nThe results indicate a clear trend: as the amount\nof training data increases, the performance of all\nmodels consistently improves. Notably, our LDST\nmodel stands out in this setting. At the 10%\ndata setting, it achieved significant performance\ngains by elevating the JGA metric from 51.97% to\n62.45%, marking an impressive 10.48% absolute\nimprovement. Even at the 5% data setting, our\napproach surpassed traditional methods that were\nusing 10% of the data. This highlights LDST’s\nremarkable capacity to excel in learning and cap-\nturing the core aspects of the task with a smaller\ndataset.\nResults of Fine-tuning with Full Training Data\nWe also evaluate the performance of LDST using\nthe complete training data, and compare it with\nthe following strong baselines, including SGD-\nbaseline (Rastogi et al., 2020), TRADE (Wu et al.,\n2019), DS-DST (Zhang et al., 2019), TripPy (Heck\net al., 2020), Seq2Seq-DU (Feng et al., 2020),\nMetaASSIST (Ye et al., 2022b), SDP-DST (Lee\net al., 2021), TOATOD (Bang et al., 2023b), DiCoS-\nDST (Guo et al., 2022b), D3ST (Zhao et al., 2022),\npaDST (Ma et al., 2019). And the results are shown\non Table 5.\nWe initially note significant advancements in\nrecent LLMs like ChatGPT and LLaMa. Notably,\nour model achieves competitive performance with\nChatGPT and even surpasses it on the SGD dataset,\nparticularly excelling in the AGA metric with a\nscore exceeding 98%.\nThe paDST method has currently achieved\nMethods Based-model(# Parameters)MultiWOZ 2.2MultiWOZ 2.4SGDJGA AGAJGA AGAJGA AGA\nSGD-baseline- 42.00 - - - 25.40 90.60TRADE - 45.40 - 55.05 - - -DS-DST BERTbase(110M) 51.70 - - - - -TripPy BERTbase(110M) 53.50 - 64.75 - - -Seq2Seq-DUBERTbase(110M) 54.40 90.9067.10 - 30.10 91.00MetaASSISTBERTbase(110M) - - 78.5799.08 - -DiCoS-DSTT5base(220M) 61.13 98.06- - - -TOATODT5base(220M) 63.79 - - - - -SDP-DSTT5large(770M) 57.6098.49 - - - -paDST XLNetlarge(340M) - - - - 86.50-D3ST T5XXL(11B) 58.60 - 75.90 - 86.40-\nChatGPTGPT-3.5-Turbo∗ 61.5297.8683.16 99.2784.8198.46\nLLaMa LLaMa (7B)55.37 95.7175.13 97.5875.32 95.83\nLDST (ours)LLaMa (7B)60.6598.8379.9498.9084.4799.38\nTable 5: Results of fine-tuning with full training data.\n- represents the results are not reported in the original\npaper. ∗means that the exact number of parameters is\nuncertain but definitely exceeds 100 billion.\nTransfer D3ST\n(T5XXL-11B)\nLDST\n(LLaMa-7B)\nSGD→MultiWOZ 2.4 28.9 31.6\nMultiWOZ 2.4→SGD 23.1 25.9\nTable 6: Results (in JGA(%)) of cross-dataset transfer\nbetween SGD and MultiWOZ 2.4.\nSOTA performance on the SGD dataset (with a\nJGA score of 86.5%), surpassing LDST’s 84.47%.\nHowever, it’s important to note that paDST re-\nlies on additional techniques, which contain back-\ntranslation between English and Chinese for data\naugmentation and special manual rules for model\npredictions. In contrast, LDST relies solely on\nthe default SGD dataset without additional aids.\nAnother SOTA method is D3ST, which uses T5-\nXXL as backbone model with 11B parameters (our\nLDST utilizes a 7B model, for outcomes based on\ndifferent foundational models and different model\nsizes, please consult Appendix B). D3ST surpasses\nLDST on the SGD dataset. However, it’s notewor-\nthy that LDST outperforms D3ST on Multiwoz 2.2\nand 2.4. Additionally, our model demonstrates im-\nproved effectiveness when compared to the LLaMa\nbackbone model, underscoring the ongoing impor-\ntance of fine-tuning LLMs in current research.\nResults of Cross-dataset Transfer We further\nperformed experiments to assess cross-dataset\ntransfer capabilities, akin to the experiment out-\nlined in Table 4c by Zhao et al. (2022). The JGA re-\nsults are presented in Table 6, highlighting LDST’s\nexceptional cross-dataset transfer abilities. When\ncompared to the D3ST method, LDST showcases\nan average improvement of 2.7% in terms of JGA.\n745\nModels JGA AGA\nLLaMa (backbone model) 68.61 96.37\n+ Traditional Instruction Tunnig 72.87 97.98\n+ Ours 75.02 99.04\nTable 7: Ablation study. The mean JGA(%) and\nAGA(%) scores on Multiwoz 2.2, Multiwoz 2.4 and\nSGD are reported.\nFigure 5: Comparison of the sensitivity of the models\nto different prompts during the testing phase.\n4.4 Ablation Study\nTo validate the effectiveness of the assembled\ndomain-slot instruction tuning, we conducted a\ncomparison with traditional instruction tuning,\nwhich employs a fixed prompt template contain-\ning all the descriptions for the requested slot (see\ndetails in Appendix A.2). The results, as displayed\nin Table 7, clearly demonstrate that our method\noutperforms traditional instruction tuning. We ob-\nserved a substantial 2.15% improvement in the JGA\nscore and a 1.06% improvement in the AGA score.\nNext, to analyse the sensitivity of the model to\ndifferent prompts during the testing phase. we de-\nsigned six different prompts and evaluated their\neffects, the results are shown in Figure 5. The\nresults clearly indicate that LDST demonstrates\nsignificantly higher accuracy and lower variance\nin test results compared to the other two baseline\nmethods. The mean variance of our method is 0.04,\nin contrast to 0.78 for the LLaMa model, represent-\ning a substantial decrease of 0.74. These findings\nhighlight that the utilization of the assembled tech-\nnique for instruction tuning effectively reduces the\nmodel’s sensitivity to prompts. This not only pro-\nvides a more stable and efficient inference process\nbut also enhances overall robustness.\n4.5 Error Analysis\nWe analyze the types of incorrect predictions in\nLDST by using the 2835 incorrectly predicted sam-\nples on MultiWOZ 2.4. Firstly, 45.72% of the\nFigure 6: JGA score at each turn on MultiWOZ 2.2.\nerrors are related to the values “dontcare” and\n“not mentioned”. For example, in cases where the\nground truth is “dontcare”, the model predicts “not\nmentioned”, and vice versa. Among all 37 slots,\nthe top five with highest error rates are “hotel-type”\n(338 errors), “restaurant-name” (290 errors), “hotel\narea” (225 errors), “hotel name” (221 errors), and\n“attraction name” (205 errors), collectively account-\ning for 45.11% of the total errors. Specifically,\nfor the \"hotel-type\" slot, 78.10% of the errors are\nattributed to the model frequently confusing “not\nmentioned” with the value “hotel”. For instance,\nthe correct value for “hotel-type” was “hotel”, but\nthe model incorrectly predicted as “not mentioned”.\n4.6 Effectiveness of LDST in Addressing the\nMain Challenges of DST\nFor the co-reference challenge, we analyze the\nMultiWOZ 2.3 dataset (Han et al., 2021), which\nincludes 253 test samples annotated with co-\nreference relationships. Our evaluation reveals that\nthe best baseline method achieves an accuracy rate\nof 91.1%, whereas LDST model achieves an im-\npressive accuracy rate of 96.4%, showcasing the\nsignificant improvement offered by our approach.\nAdditionally, we visualize the JGA score for\neach dialogue turn in Figure 6 to demonstrate the\neffectiveness in addressing error propagation. The\nresult clearly shows that as the number of dialogue\nturns increases, the performance of all methods ex-\nperiences a decline. However, our LDST model\ndemonstrates a remarkable resilience to error prop-\nagation, showcasing a significantly slower decline\ncompared to LLaMa and the best baseline method.\nThese results emphasize the LDST model’s capac-\nity to capture and comprehend complex linguistic\npatterns and dependencies in multi-round conver-\nsations, making it a promising solution to mitigate\nthe challenges associated with the DST task.\n746\n5 Related Work\n5.1 Multi-Domain Dialogue State Tracking\nRecent studies in multi-domain DST have exten-\nsively utilized the pre-trained language models to\nachieve high-performance results (Ravuru et al.,\n2022; Yu et al., 2022; Sun et al., 2022; Feng et al.,\n2021; Wang et al., 2022b; Xu et al., 2023c). For ex-\nample, Xie et al. (2022) proposed a multi-stage cor-\nrectable dialogue state tracking method to mitigate\nthe error propagation phenomenon, while Wang\nand Xin (2022) proposed a jointly decision making\nand a dialogue update technique to prevent error\naccumulation. In addition, Wang et al. (2022a) and\nManotumruksa et al. (2022) solve the challenge\nof co-referencing by learning correlations between\nslots, for example, by using a combination of slot\nprompts or hard copy mechanism. However, these\napproaches still have limitations, such as the lack\nof robustness in handling complex dialogue con-\ntexts and the difficulty in capturing fine-grained\nsemantic relationships between slots and values.\n5.2 LLMs for Dialogue State Tracking\nWhile large language models such as GPT-3\n(Brown et al., 2020) and T5 (Raffel et al., 2020)\nhave gained significant popularity, the efficient uti-\nlization of these models remains a significant chal-\nlenge. Recent advancements in parameter-efficient\nfine-tuning (PEFT) techniques have effectively alle-\nviated this problem, such as LoRA(Hu et al., 2021)\nand Prefix Tuning(Liu et al., 2021). For instance,\nboth Lee et al. (2021) and Yang et al. (2023b)\nproposed a prompt-tuning method that leverages\ndomain-specific prompts and context information\nto improve the performance of DST task. Mean-\nwhile, Ma et al. (2023) and Chen et al. (2023) intro-\nduced the prefix tuning approach, which involves\nmodifying the input prompt by adding specific to-\nkens at the beginning of the dialogue, aiming to\nenhance the efficiency of model fine-tuning. How-\never, these methods still face challenges, where the\neffectiveness heavily depends on prompt design.\nRecently, Heck et al. (2023) exclusively tested\nChatGPT’s performance on the Multiwoz 2.1\ndataset. In contrast, our evaluation covers the Mul-\ntiwoz 2.2, 2.4, and SGD datasets, providing a more\ncomprehensive assessment. While both Pan et al.\n(2023) and Hudeˇcek and Dušek (2023) included re-\nsults on the Multiwoz 2.2, Multiwoz 2.4, and SGD\ndatasets, their results were relatively lower due to\ntheir use of the text-davinci-003 API. In contrast,\nwe utilized the latest gpt-3.5-turbo API, a highly\ncapable GPT-3.5 model optimized for chat at a frac-\ntion of the cost. Consequently, we achieved new\nSOTA performance with ChatGPT, showcasing its\nsignificant strengths.\nWith the emergence of open-source large lan-\nguage models, such as LLaMa (Touvron et al.,\n2023), it provides a series of higher-quality back-\nbone models with different parameters. Leveraging\nLLaMa and the technique of instruction tuning has\nproven to achieve better results (Taori et al., 2023),\nopening new avenues for our research.\n6 Conclusion\nIn this study, we conduct an initial examination\nof ChatGPT’s capabilities in multi-domain DST,\nshowcasing its superiority over previous methods.\nThis comprehensive evaluation provides useful di-\nrections for researchers to design and enhance dia-\nlogue systems. To solve the limitations of ChatGPT,\nwe present LDST, an LLM-driven DST framework\nbased on smaller, open-source foundation models.\nBy utilizing a novel assembled domain-slot instruc-\ntion tuning method, LDST achieves performance\non par with ChatGPT. Comprehensive evaluations\nacross three distinct experimental settings demon-\nstrate the remarkable performance improvements\nachieved by LDST compared to previous methods.\nLimitations\nThis work has two main limitations: (1) Subjec-\ntivity in prompt design: Prompt engineering has\nshown significant potential for the application of\nLLMs. However, the prompt designs used in our\nstudy are subjective and may not necessarily rep-\nresent the optimal choices. Consequently, the ef-\nfectiveness of using these prompts for model fine-\ntuning or testing may not always yield the best\nresults. Exploring more systematic automated tech-\nniques for prompt design could enhance the overall\nperformance of the model. (2) Input length con-\nstraints: In our study, we set the input length of\nthe model to 512, which was determined based\non statistical analysis and already contains more\nthan 90% of the samples. While it is possible to\nincrease the input length, doing so may result in\nslower training and inference times. Additionally,\nwhen the dialogue or description content becomes\ntoo long, the challenge of effectively truncating or\nsummarizing the input arises. Further investigation\n747\ninto handling longer input sequences without com-\npromising model efficiency would be beneficial.\nAcknowledgments\nWe thank the anonymous reviewers for their valu-\nable feedback and constructive comments, which\ngreatly contributed to improve the quality of this\nwork. This research was partially supported by the\ngrant of HK ITF ITS/359/21FP.\nReferences\nNamo Bang, Jeehyun Lee, and Myoung-Wan Koo.\n2023a. Task-optimized adapters for an end-to-end\ntask-oriented dialogue system.\nNamo Bang, Jeehyun Lee, and Myoung-Wan Koo.\n2023b. Task-optimized adapters for an end-to-\nend task-oriented dialogue system. arXiv preprint\narXiv:2305.02468.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai,\nPhilip S Yu, and Lichao Sun. 2023. A comprehensive\nsurvey of ai-generated content (aigc): A history of\ngenerative ai from gan to chatgpt. arXiv preprint\narXiv:2303.04226.\nDerek Chen, Kun Qian, and Zhou Yu. 2023. Stabilized\nin-context learning with pre-trained language models\nfor few shot dialogue state tracking. arXiv preprint\narXiv:2302.05932.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nYue Feng, Aldo Lipani, Fanghua Ye, Qiang Zhang, and\nEmine Yilmaz. 2022a. Dynamic schema graph fusion\nnetwork for multi-domain dialogue state tracking.\narXiv preprint arXiv:2204.06677.\nYue Feng, Yang Wang, and Hang Li. 2020. A sequence-\nto-sequence approach to dialogue state tracking.\narXiv preprint arXiv:2011.09553.\nYujie Feng, Jiangtao Wang, Yasha Wang, and\nXu Chu. 2022b. Spatial-attention and demographic-\naugmented generative adversarial imputation network\nfor population health data reconstruction. IEEE\nTransactions on Big Data.\nYujie Feng, Jiangtao Wang, Yasha Wang, and Xu Chu.\n2023. Towards sustainable compressive population\nhealth: A gan-based year-by-year imputation method.\nACM Transactions on Computing for Healthcare ,\n4(1):1–18.\nYujie Feng, Jiangtao Wang, Yasha Wang, and Sumi\nHelal. 2021. Completing missing prevalence rates\nfor multiple chronic diseases by jointly leveraging\nboth intra-and inter-disease population health data\ncorrelations. In Proceedings of the Web Conference\n2021, pages 183–193.\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.\n2023. Exploring the feasibility of chatgpt for event\nextraction. arXiv preprint arXiv:2303.03836.\nJinyu Guo, Kai Shuang, Jijie Li, Zihan Wang, and\nYixuan Liu. 2022a. Beyond the granularity: Multi-\nperspective dialogue collaborative selection for dia-\nlogue state tracking. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2320–\n2332, Dublin, Ireland. Association for Computational\nLinguistics.\nJinyu Guo, Kai Shuang, Jijie Li, Zihan Wang,\nand Yixuan Liu. 2022b. Beyond the granular-\nity: Multi-perspective dialogue collaborative selec-\ntion for dialogue state tracking. arXiv preprint\narXiv:2205.10059.\nTing Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian,\nChongxuan Huang, Dazhen Wan, Wei Peng, and Min-\nlie Huang. 2021. Multiwoz 2.3: A multi-domain task-\noriented dialogue dataset enhanced with annotation\ncorrections and co-reference annotation. In Natural\nLanguage Processing and Chinese Computing: 10th\nCCF International Conference, NLPCC 2021, Qing-\ndao, China, October 13–17, 2021, Proceedings, Part\nII 10, pages 206–218. Springer.\nMichael Heck, Nurul Lubis, Benjamin Ruppik, Re-\nnato Vukovic, Shutong Feng, Christian Geishauser,\nHsien-Chin Lin, Carel van Niekerk, and Milica Gaši´c.\n2023. Chatgpt for zero-shot dialogue state track-\ning: A solution or an opportunity? arXiv preprint\narXiv:2306.01386.\nMichael Heck, Carel van Niekerk, Nurul Lubis, Chris-\ntian Geishauser, Hsien-Chin Lin, Marco Moresi, and\nMilica Gaši´c. 2020. Trippy: A triple copy strategy\nfor value independent neural dialog state tracking.\narXiv preprint arXiv:2005.02877.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A simple\nlanguage model for task-oriented dialogue. Advances\nin Neural Information Processing Systems, 33:20179–\n20191.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\n748\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A Smith, and Mari Ostendorf. 2022. In-context\nlearning for few-shot dialogue state tracking. arXiv\npreprint arXiv:2203.08568.\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020.\nChallenges in building intelligent open-domain di-\nalog systems. ACM Transactions on Information\nSystems (TOIS), 38(3):1–32.\nV ojtˇech Hudeˇcek and Ondˇrej Dušek. 2023. Are llms all\nyou need for task-oriented dialogue? arXiv preprint\narXiv:2304.06556.\nWX Jiao, WX Wang, JT Huang, Xing Wang, and ZP Tu.\n2023. Is chatgpt a good translator? yes with gpt-4 as\nthe engine. arXiv preprint arXiv:2301.08745.\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf.\n2021. Dialogue state tracking with a language model\nusing schema-driven prompting. arXiv preprint\narXiv:2109.07506.\nHwaran Lee, Jinsik Lee, and Tae-Yoon Kim. 2019.\nSumbt: Slot-utterance matching for universal\nand scalable belief tracking. arXiv preprint\narXiv:1907.07421.\nZhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan\nMoon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang,\nZhou Yu, Eunjoon Cho, Rajen Subba, et al. 2021a.\nZero-shot dialogue state tracking via cross-task trans-\nfer. arXiv preprint arXiv:2109.04655.\nZhaojiang Lin, Bing Liu, Seungwhan Moon, Paul\nCrook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,\nAndrea Madotto, Eunjoon Cho, and Rajen Subba.\n2021b. Leveraging slot descriptions for zero-shot\ncross-domain dialogue state tracking. arXiv preprint\narXiv:2105.04222.\nBo Liu, Liming Zhan, Zexin Lu, Yujie Feng, Lei Xue,\nand Xiao-Ming Wu. 2023. How good are large lan-\nguage models at out-of-distribution detection? arXiv\npreprint arXiv:2308.10261.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\nZhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-\ntuning v2: Prompt tuning can be comparable to fine-\ntuning universally across scales and tasks. arXiv\npreprint arXiv:2110.07602.\nMingyu Derek Ma, Jiun-Yu Kao, Shuyang Gao, Arpit\nGupta, Di Jin, Tagyoung Chung, and Nanyun Peng.\n2023. Parameter-efficient low-resource dialogue\nstate tracking by prompt tuning. arXiv preprint\narXiv:2301.10915.\nYue Ma, Zengfeng Zeng, Dawei Zhu, Xuan Li, Yiy-\ning Yang, Xiaoyuan Yao, Kaijie Zhou, and Jian-\nping Shen. 2019. An end-to-end dialogue state\ntracking system with machine reading comprehen-\nsion and wide & deep classification. arXiv preprint\narXiv:1912.09297.\nJarana Manotumruksa, Jeffrey Dalton, Edgar Meij, and\nEmine Yilmaz. 2022. Similarity-based multi-domain\ndialogue state tracking with copy mechanisms for\ntask-based virtual personal assistants. In Proceedings\nof the ACM Web Conference 2022, pages 2006–2014.\nWenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che,\nand Libo Qin. 2023. A preliminary evaluation of\nchatgpt for zero-shot dialogue understanding. arXiv\npreprint arXiv:2304.04256.\nGao Qixiang, Guanting Dong, Yutao Mou, Liwen Wang,\nChen Zeng, Daichi Guo, Mingyang Sun, and Weiran\nXu. 2022. Exploiting domain-slot related keywords\ndescription for few-shot cross-domain dialogue state\ntracking. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2460–2465.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghav Gupta, and Pranav Khaitan. 2020. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8689–8696.\nLohith Ravuru, Seonghan Ryu, Hyungtak Choi, Haehun\nYang, and Hyeonmok Ko. 2022. Multi-domain dia-\nlogue state tracking by neural-retrieval augmentation.\nIn Findings of the Association for Computational\nLinguistics: AACL-IJCNLP 2022, pages 169–175.\nJamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea\nMadotto, and Juneyoung Park. 2022. Dialogue\nsummaries as dialogue states (ds2), template-guided\nsummarization for few-shot dialogue state tracking.\narXiv preprint arXiv:2203.01552.\nZhoujian Sun, Zhengxing Huang, and Nai Ding.\n2022. On tracking dialogue state by inheriting\nslot values in mentioned slot pools. arXiv preprint\narXiv:2202.07156.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan\nHu, Yongrui Chen, and Guilin Qi. 2023. Evalu-\nation of chatgpt as a question answering system\nfor answering complex questions. arXiv preprint\narXiv:2303.07992.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html,\n3(6):7.\n749\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHaoming Wang and Wang Xin. 2022. How to stop an\navalanche? jodem: Joint decision making through\ncompare and contrast for dialog state tracking. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 7030–7041.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zhixu\nLi, Jianfeng Qu, and Jie Zhou. 2023. Cross-\nlingual summarization via chatgpt. arXiv preprint\narXiv:2302.14229.\nQingyue Wang, Yanan Cao, Piji Li, Yanhe Fu, Zheng\nLin, and Li Guo. 2022a. Slot dependency modeling\nfor zero-shot cross-domain dialogue state tracking.\nIn Proceedings of the 29th International Conference\non Computational Linguistics, pages 510–520.\nYifan Wang, Jing Zhao, Junwei Bao, Chaoqun Duan,\nYouzheng Wu, and Xiaodong He. 2022b. LUNA:\nLearning slot-turn alignment for dialogue state track-\ning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 3319–3328, Seattle, United States. Asso-\nciation for Computational Linguistics.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al.\n2022c. Benchmarking generalization via in-context\ninstructions on 1,600+ language tasks. arXiv preprint\narXiv:2204.07705.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-\nAsl, Caiming Xiong, Richard Socher, and Pascale\nFung. 2019. Transferable multi-domain state genera-\ntor for task-oriented dialogue systems. arXiv preprint\narXiv:1905.08743.\nHongyan Xie, Haoxiang Su, Shuangyong Song, Hao\nHuang, Bo Zou, Kun Deng, Jianghua Lin, Zhihui\nZhang, and Xiaodong He. 2022. Correctable-dst:\nMitigating historical context mismatch between train-\ning and inference for improved dialogue state track-\ning. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 876–889.\nJing Xu, Dandan Song, Chong Liu, Siu Cheung Hui,\nFei Li, Qiang Ju, Xiaonan He, and Jian Xie. 2023a.\nDialogue state distillation network with inter-slot con-\ntrastive learning for dialogue state tracking. arXiv\npreprint arXiv:2302.08220.\nYongxin Xu, Xu Chu, Kai Yang, Zhiyuan Wang, Peinie\nZou, Hongxin Ding, Junfeng Zhao, Yasha Wang, and\nBing Xie. 2023b. Seqcare: Sequential training with\nexternal medical knowledge graph for diagnosis pre-\ndiction in healthcare data. In Proceedings of the ACM\nWeb Conference 2023, pages 2819–2830.\nYongxin Xu, Kai Yang, Chaohe Zhang, Peinie Zou,\nZhiyuan Wang, Hongxin Ding, Junfeng Zhao, Yasha\nWang, and Bing Xie. 2023c. Vecocare: Visit\nsequences-clinical notes joint learning for diagno-\nsis prediction in healthcare data. Proceedings of the\nThirty-Second International Joint Conference on Ar-\ntificial Intelligence.\nPuhai Yang, Heyan Huang, Wei Wei, and Xian-Ling\nMao. 2022. Toward real-life dialogue state tracking\ninvolving negative feedback utterances. In Proceed-\nings of the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, pages 2222–2232.\nXianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and\nWei Cheng. 2023a. Exploring the limits of chatgpt\nfor query or aspect-based text summarization. arXiv\npreprint arXiv:2302.08081.\nYuting Yang, Wenqiang Lei, Pei Huang, Juan Cao, Jin-\ntao Li, and Tat-Seng Chua. 2023b. A dual prompt\nlearning framework for few-shot dialogue state track-\ning. In Proceedings of the ACM Web Conference\n2023, pages 1468–1477.\nFanghua Ye, Jarana Manotumruksa, and Emine Yil-\nmaz. 2022a. MultiWOZ 2.4: A multi-domain task-\noriented dialogue dataset with essential annotation\ncorrections to improve state tracking evaluation. In\nProceedings of the 23rd Annual Meeting of the Spe-\ncial Interest Group on Discourse and Dialogue, pages\n351–360, Edinburgh, UK. Association for Computa-\ntional Linguistics.\nFanghua Ye, Xi Wang, Jie Huang, Shenghui Li, Samuel\nStern, and Emine Yilmaz. 2022b. Metaassist: Robust\ndialogue state tracking with meta learning. arXiv\npreprint arXiv:2210.12397.\nDian Yu, Mingqiu Wang, Yuan Cao, Laurent El Shafey,\nIzhak Shafran, and Hagen Soltau. 2022. Knowledge-\ngrounded dialog state tracking. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 3428–3435, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. MultiWOZ 2.2 : A dialogue dataset with\nadditional annotation corrections and state tracking\nbaselines. In Proceedings of the 2nd Workshop on\nNatural Language Processing for Conversational AI,\npages 109–117, Online. Association for Computa-\ntional Linguistics.\nJian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng Wu,\nYao Wan, Philip S Yu, Richard Socher, and Caiming\nXiong. 2019. Find or classify? dual strategy for\nslot-value predictions on multi-domain dialog state\ntracking. arXiv preprint arXiv:1910.03544.\n750\nJeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu,\nMingqiu Wang, Harrison Lee, Abhinav Rastogi,\nIzhak Shafran, and Yonghui Wu. 2022. Description-\ndriven task-oriented dialog modeling. arXiv preprint\narXiv:2201.08904.\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, et al. 2023. A comprehensive survey on\npretrained foundation models: A history from bert to\nchatgpt. arXiv preprint arXiv:2302.09419.\nYihao Zhou, Guoshuai Zhao, and Xueming Qian. 2022.\nDialogue state tracking based on hierarchical slot at-\ntention and contrastive learning. In Proceedings of\nthe 31st ACM International Conference on Informa-\ntion & Knowledge Management, pages 4737–4741.\nA Description of Prompt Templates\nA.1 Prompt Templates for ChatGPT Request\nInitially, we noticed that the results reported in the\nstudies by Pan et al. (2023); Hude ˇcek and Dušek\n(2023) were notably lower in comparison to our re-\nsults. We attribute this observation to two primary\nfactors, as outlined below.\nMitigating the Generation of Excessively\nLengthy Responses ChatGPT often generated\nanswers with excessively detailed explanations, de-\nviating from the expected response format. For\nexample, when asked about the \"train-leaveat\" slot,\nChatGPT might respond with extensive informa-\ntion such as \"Monday at 05:16 for the first train and\nMonday at 16:16 for the last train,\" whereas the cor-\nrect response should be simply \"05:16.\" To address\nthis issue, we introduced a prompt that includes\n\"No explanation!\" as an instruction to ChatGPT\nnot to provide detailed explanations. Experimen-\ntal results indicated a significant improvement in\nanswer accuracy through this approach.\nAPI Version Differences Another factor is the\nutilization of different API versions. The prior\nworks all relied on the text-davinci-003 API, while\nwe utilized a more powerful gpt-3.5-turbo API, a\nhighly capable GPT-3.5 model optimized for chat\nat a fraction of the cost.\nBelow we provide specific samples for the four\ndifferent prompts in Figure 2.\nPrompt type 1: “single return” + “no demo”\n{\n“instruction”: Now you need to perform\nthe task of multi-domain dialogue state\ntracking. You need to return the value of\nthe slot I’m asking about simply based\non the content of the dialogue. No\nexplanation!\n“input”: Input dialogue: [USER] I would\nlike to find a salon. [SYSTEM] In\nwhich city do you prefer the salon to\nbe located? [USER] In SFO will be\ngreat. [domain] Hotels_2, it indicates\nA popular service for searching and\nbooking houses for short term stay [slot]\nnumber_of_adults, it indicates Number of\npeople for the reservation. This slot is\ncategorical and you can only choose from\nthe following available values: 1, 2, 3,\n4, 5. If the slot is not mentioned in the\ndialogue, just return NONE.\nSo the value of slot\n<Hotels_2-number_of_adults> is\n}\nPrompt type 2: “multi return” + “no demo”\n{\n“instruction”: Now you need to\nperform the task of dialogue state\ntracking. And the slot schema is in\nthis list [restaurant-area, hotel-name,\nattraction-name, ...(the remaining slots\nare omitted here)], which is in a\ndomain-slot format. You need to return\nthe slot and its value in dict format if\nthe value is not none, and no explanation!\n“input”: Input dialogue: [USER] I would\nlike to find a salon. [SYSTEM] In which\ncity do you prefer the salon to be\nlocated? [USER] In SFO will be great.\nPlease return the value of slot\nlist [restaurant-area, hotel-name,\nattraction-name, ...(the remaining slots\nare omitted here)].\n}\nPrompt type 3: “single return” + “one demo”\n{\n“instruction”: Now you need to perform\nthe task of multi-domain dialogue state\ntracking. And I will show you an example\nand you need to return to the state of\nthe slot I asked about.\n“input”: The example is: Input dialogue:\n[User]: I need train reservations from\nnorwich to cambridge [SYSTEM]: I have 133\ntrains matching your request. ...\n751\nSo the value of slot<train-departure>\nis\nAnd your result should be Norwich.\nThe following is the dialogue you need\nto test:\nInput dialogue: [USER] I would like to\nfind a salon. [SYSTEM] In which city\ndo you prefer the salon to be located?\n[USER] In SFO will be great. [domain]\nHotels_2, it indicates A popular service\nfor searching and booking houses for\nshort term stay [slot] number_of_adults,\nit indicates Number of people for the\nreservation. This slot is categorical and\nyou can only choose from the following\navailable values: 1, 2, 3, 4, 5. If the\nslot is not mentioned in the dialogue,\njust return NONE. So the value of slot\n<Hotels_2-number_of_adults> is\n}\nPrompt type 4: “multi return” + “one demo”\n{\n“instruction”: Now you need to perform\nthe task of multi-domain dialogue state\ntracking. And I will show you an example\nand you need to return the answer strictly\nin the format of the example.\n“input”: The example is: Input dialogue:\n[User]: I need train reservations from\nnorwich to cambridge [SYSTEM]: I have 133\ntrains matching your request. ...\nOutput result: Train-Departure:\nNorwich, Train-Arrival: Cambridge,\n...(the remaining slots are omitted\nhere)\nAnd you need to test this example:\nInput dialogue: [USER] I would like to\nfind a salon. [SYSTEM] In which city do\nyou prefer the salon to be located? [USER]\nIn SFO will be great.\nPlease return the value of slot\nlist [restaurant-area, hotel-name,\nattraction-name, ...(the remaining slots\nare omitted here)].\n}\nFor practical reasons related to API request costs,\nwe conducted tests using these four prompt tem-\nplates exclusively on the MultiWOZ 2.2 dataset.\nSubsequent evaluations on the MultiWOZ 2.4 and\nSGD datasets focused on the most effective tem-\nplate, i.e., “single return” + “no demo.”\nA.2 Prompt Template for “Traditional”\nInstruction Tuning\nHere, we present the template for traditional in-\nstruction tuning, where \"traditional\" implies the ap-\nplication of instruction tuning directly to the DST\ntask with a fixed prompt template. It’s important to\nhighlight that this fixed prompt template includes\nall slot descriptions, such as the domain-slot de-\nscription and the list of potential values. This fixed\nprompt is utilized during both the fine-tuning and\ntesting phases.\n{\n“instruction”: Track the state of the slot\n<restaurant-area> in the input dialogue.\n“input”: [Dialogue context omitted...]\n[Domain] restaurant, [Slot] area, it\nindicates the area or place of the\nrestaurant. This slot is categorical, and\nyou can only choose from the following\navailable values: north, east, south,\nwest. If the slot is not mentioned in\nthe dialogue, just return NONE. So the\nvalue of slot <restaurant-area> is\n“output”: north\n}\nB Additional Results\nB.1 Comparison of ChatGPT with Zero-shot\nMethods\nEssentially, the evaluation of ChatGPT inherently\nbelongs to the zero-shot setting. However, since\nwe found that ChatGPT’s results have been compa-\nrable to traditional fine-tuning methods, we put its\nresults in Table 5 in the paper. Additionally, we in-\ntroduce ChatGPT’s results from zero-shot settings\nand the results are shown in table 8 and 9 below.\nDomain SDM-DST LDST ChatGPT\nMessaging 36.6 89.6 92.8\nPayment 16.5 92.3 94.1\nTrains 46.7 81.0 83.3\nAlarm 58.3 94.3 95.7\nAverage 40.4 89.3 91.5\nTable 8: Zero-shot results (in JGA(%)) of ChatGPT on\nSGD.\nThe results clearly demonstrate that ChatGPT\noutperforms the two strong baselines, SDM-DST\nand T5DST, by a huge margin. This is primarily\n752\nDomain T5DST LDST ChatGPT\nAttraction 33.09 75.61 78.50\nHotel 21.21 63.32 66.75\nRestaurant 21.82 73.72 77.49\nTaxi 65.09 91.47 92.38\nTrain 35.42 71.03 72.81\nAverage 35.20 75.03 77.58\nTable 9: Zero-shot results (in JGA(%)) of ChatGPT on\nMultiWOZ 2.0.\nbecause the evaluation is conducted in a zero-shot\nenvironment, where ChatGPT inherently holds an\nadvantage. It’s important to note that as an API\nservice, ChatGPT cannot be tuned offline and is\nexclusively used for testing purposes.\nIn the zero-shot setting, the performance of tradi-\ntional methods (e.g., SDM-DST and T5DST) is\nworse due to the lack of domain-specific train-\ning data. ChatGPT, equipped with its extensive\nmodel size and rich pre-trained knowledge, dra-\nmatically surpasses the performance of traditional\nmethods and sets the upper bound of performance\nin the zero-shot setting. It’s also worth mentioning\nthat ChatGPT’s performance approaches the results\nof traditional methods fine-tuned on the complete\ntraining dataset, which is why we include it in Table\n5 for comparison\nIn contrast, our LDST, utilizing a customized\ninstruction tuning method, effectively approaches\nChatGPT’s performance in the zero-shot setting,\nwith an average performance difference of 2.4% in\nthe JGA score.\nB.2 Results with Different Foundation Models\nWe further performed evaluations using an alterna-\ntive foundation model, Llama2-7B (Touvron et al.,\n2023), as depicted in Table 10 below.\nMethods Backbone Multiwoz 2.4 SGD\nChatGPT GPT-3.5-Turbo 83.2 84.8\nLLaMa-7B LLaMa (7B) 75.1 75.3\nLDST_LLaMa LLaMa (7B) 79.9 84.5\nLDST_LLaMa2 LLaMa2 (7B) 81.9 85.1\nTable 10: Results (in JGA(%)) with different backbones.\nThe results show that LDST_LLaMa2 performed\nthe best on SGD, attaining a JGA of 85.1% and\ndemonstrating a performance akin to that of Chat-\nGPT on MultiWOZ 2.4. It suggests that a stronger\nbackbone can lead to better DST performance.\nB.3 Results with LLaMa of different sizes\nIn order to investigate how model size influences\nperformance, we have incorporated supplementary\nexperimental findings involving the LLaMa-13B\nand -30B models on the SGD dataset. These results\nare presented in Table 11 below.\nMethods Backbone # Training Epochs SGD\nChatGPT GPT-3.5-Turbo n/a 84.8\nD3ST T5 XXL (11B) not provided 86.4\nLDST LLaMa (7B) 2 84.5\nLDST LLaMa (13B) 2 86.5\nLDST LLaMa (30B) 0.5 86.9\nTable 11: Results (in JGA(%)) with backbones of vary-\ning sizes.\nThe results provide a clear indication that an\nincrease in model size corresponds to an improve-\nment in the JGA score. However, in practical appli-\ncations, a 7B model not only offers a more suitable\nfit for local deployment but also showcases impres-\nsive performance.\nB.4 Inference Time Analysis\nThe table 12 below provides the results of infer-\nence time. It’s worth highlighting that we employ\n8-bit quantization for the LLMs, which leads to\nslower inference times compared to standard 32-bit\nconfigurations.\nMethods Inference Speed (Samples/Min)\nT5_large-770M 531\nLDST_LLaMa-7B 90\nLDST_LLaMa2-7B 84\nLDST_LLaMa-13B 64\nLDST_LLaMa-30B 35\nTable 12: Inference time for different models.\nT5 large is the backbone model of the SDP-DST\nbaseline method. From the table, it’s clear that\nthe inference speed decreases as the model size in-\ncreases. For example, LDST_LLaMa-7B predicts\nan average of 90 samples per minute. When com-\npared to the baseline method based on T5-large\n(770M), the speed of LDST is approximately one-\nsixth that of the baseline.\nB.5 Effect of LoRA Configurations\nIn our work, we utilized common configurations:\nlora_r = 8 and lora target modules=[query_proj,\nkey_proj, value_proj, output_proj] in each self-\nattention module that needs to be updated.\n753\nTo further clarify the impact of LoRA configura-\ntions on the experimental results, we performed ad-\nditional analysis on the Multiwoz 2.4 dataset using\n1% training set (To save training time, we set the\nepoch to 1). We varied the lora_r parameter with\nvalues of 1, 2, 4, 8, and 16. In addition, we experi-\nmented with two different lora_target_modules con-\nfigurations: [q_proj, v_proj] and [q_proj, k_proj,\nv_proj, o_proj]. This resulted in 10 distinct experi-\nmental setups.\nlora_target_modules \\lora_r 1 2 4 8 16\n[q proj, v proj] 29.75 31.38 33.11 39.07 40.40\n[q proj, k proj, v proj, o proj] 31.59 40.11 36.09 40.19 42.02\nTable 13: Effect of LoRA configurations. All results are\nreported in JGA(%).\nIn these results, a smaller value of “lora_r” in-\ndicates fewer LoRA parameters, while lora target\nmodules determines which modules receive LoRA\nupdate matrices. Generally, updating more atten-\ntion matrices yields better results, and performance\nimproves as “lora_r” increases. However, it’s es-\nsential to note that higher “lora_r” values might\nextend the model’s training time. Hence, selecting\nappropriate hyperparameters is crucial.\nB.6 Results on MultiWOZ 2.1 Dataset\nFor a comprehensive evaluation, refer to Table 14,\nwhich presents the results on the MultiWOZ 2.1\ndataset, comparing ChatGPT by Heck et al. (2023)\nwith the D3ST method by Zhao et al. (2022).\nMethods Based-model MultiWOZ 2.1\nChatGPT GPT-3.5-text-davinci-003 56.44\nD3ST T5 XXL (11B) 57.80\nLDST (ours) LLaMa (7B) 56.69\nTable 14: Results (in JGA(%)) on MultiWOZ 2.1.\nThe results reveal that LDST’s performance is\nmarginally below that of D3ST. This could be at-\ntributed to potential noise in the test set annotations,\nmirroring our observations regarding the Multi-\nWOZ 2.2 dataset.\nC Implementation Details\nC.1 Data Preprocessing and Evaluation\nStep 1 - Standard Preprocessing In line with\nthe approach used by Lee et al. (2021), this initial\nstep involves the extraction of dialogue content and\nslot-value pairs from the raw data. For instance,\nconsider the dialogue labeled \"PMUL4398.json\"\nin the Multiwoz 2.2 training dataset. It comprises\n6 dialogue turns between the system and the user.\nWith Multiwoz 2.2 featuring 49 unique slots, this\ndialogue yields 294 (6*49) training data samples.\nHere is a specific example:\n{\n“dialogue”: [SYSTEM] What can I help you\nwith [USER] i need a place to dine in the\ncenter thats expensive [SYSTEM] I have\nseveral options for you; do you prefer\nAfrican, Asian, or British food? [USER]\nAny sort of food would be fine, as long\nas it is a bit expensive. Could I get\nthe phone number for your recommendation?\n[domain] restaurant find places to dine\nand whet your appetite [slot] area area or\nplace of the restaurant [Possible Values]\ncentre, east, north, south, west\n“state”: centre\n}\nIn this example, the “dialogue” field includes the\ncontent of the dialogue (A1, U1), (A2, U2), the\ntracked slot <restaurant-area>, and it’s description.\nThe “state” field is the value of the corresponding\nslot. For the slots that are not mentioned in the\ndialogue, the “state” field is assigned to NONE.\nStep 2 - Instruction Data Generation While the\npreprocessing in Step 1 provided valuable data, it\ndidn’t entirely align with the required format for\ninstruction tuning. As a result, it led to suboptimal\nexperimental performance. To address this, we in-\ntroduced an additional preprocessing stage known\nas the \"Instruction Data Generation Module,\" as\ndepicted in Figure 4. This module is designed to\nconstruct more suitable prompts.\nThe aforementioned details the entirety of the\npreprocessing procedure, after which it can be\nleveraged for both model training and testing.\nEvaluation Regarding evaluation, we also uti-\nlized the code provided by Lee et al. (2021) to cal-\nculate JGA and AGA scores. A prediction was con-\nsidered correct when it exactly matches the ground\ntruth. During the testing phase, we used a con-\nsistent prompt template that included domain-slot\ndescriptions and lists of potential values. Experi-\nmental findings showed that this template slightly\noutperformed others because of its provision of\nmore comprehensive slot information.\n754\nC.2 Experimental Settings\nDuring the training phase, we utilized a batch size\nof 128 and set the learning rate to 1e-4. The num-\nber of epochs varied depending on the specific ex-\nperiment setup. For zero-shot experiments, we\ntrained for 3 epochs. For few-shot experiments, we\nconducted experiments with different percentages\nof labeled data: 1% few-shot experiments were\ntrained for 10 epochs, 5% few-shot experiments\nfor 3 epochs, and 10% few-shot experiments for 2\nepochs. For fine-tuning on the full dataset experi-\nments, we used 2 epochs.\nThe model’s cutoff length was set at 512, based\non data analysis. This length was determined\nto be optimal as it covered more than 90% of\nthe data. For samples with input lengths exceed-\ning 512 tokens, we truncated them to fit within\nthe cutoff length. Additionally, the parameter\n\"train_on_inputs\" was set to false, indicating that\nthe model solely computed the loss based on the\nfinal output.\nRegarding the hyperparameters of the LORA\nmodule, we set the lora rank to 8, alpha to\n16, dropout rate to 0.05, and selected “q_proj”,\n“k_proj”, “v_proj” and “o_proj” as the LORA tar-\nget modules. Furthermore, in order to reduce the\nmemory usage of the model, we employed 8-bit\nquantization techniques to further optimize the fine-\ntuning process.\nWe would also like to offer further insights into\nthe training time comparison of our model. In ex-\nperiments involving fine-tuning on the full dataset,\nour model had an average training time of 8 hours.\nIn contrast, powerful baseline methods, such as\nSDP-DST (Feng et al., 2023) and DiCoS-DST (Xu\net al., 2023b), required approximately 60 hours\nfor fine-tuning the T5 model based on our testing.\nThis substantial difference in training time under-\nscores the efficiency of our approach. And for the\nTOATOD (Bang et al., 2023b) method, which also\nutilizes the PEFT technique, the fine-tuning process\nonly focuses on soft prompts, reducing the overall\nruntime to 12 hours. This runtime is comparable to\nour method, demonstrating the effectiveness of our\napproach compared to traditional methods.\nIn the case of few-shot experiments, the train-\ning time for 1% labeled data was 5 hours, 5% la-\nbeled data required 8 hours, and 10% labeled data\ntook approximately 10 hours to train. In contrast,\nthe runtime for zero-shot experiments averaged\naround 12 hours. It’s worth noting that our ap-\nproach did not exhibit significant runtime improve-\nments compared to traditional methods in these\nsettings. However, it does illustrate that our LLM-\ndriven approach achieves the most substantial per-\nformance improvements while still maintaining ef-\nficiency. These additional insights into the model’s\nruntime in various experimental setups provide a\ncomprehensive understanding of the time required\nfor training our model and its comparison to other\nbaseline methods.\n755",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7764319181442261
    },
    {
      "name": "Software deployment",
      "score": 0.7212269306182861
    },
    {
      "name": "Task (project management)",
      "score": 0.6492305994033813
    },
    {
      "name": "Open source",
      "score": 0.5390448570251465
    },
    {
      "name": "Code (set theory)",
      "score": 0.4902406334877014
    },
    {
      "name": "Source code",
      "score": 0.4760170578956604
    },
    {
      "name": "Tracking (education)",
      "score": 0.45941174030303955
    },
    {
      "name": "State (computer science)",
      "score": 0.4537917673587799
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.44670072197914124
    },
    {
      "name": "Human–computer interaction",
      "score": 0.37016579508781433
    },
    {
      "name": "Software engineering",
      "score": 0.2807634770870209
    },
    {
      "name": "Programming language",
      "score": 0.13025492429733276
    },
    {
      "name": "Systems engineering",
      "score": 0.1292230486869812
    },
    {
      "name": "Engineering",
      "score": 0.11113354563713074
    },
    {
      "name": "Software",
      "score": 0.10045382380485535
    },
    {
      "name": "Psychology",
      "score": 0.07448029518127441
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    }
  ]
}