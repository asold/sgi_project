{
  "title": "RecExplainer: Aligning Large Language Models for Explaining Recommendation Models",
  "url": "https://openalex.org/W4388890788",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3082541406",
      "name": "Lei, Yuxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3167779009",
      "name": "Lian Jianxun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098019569",
      "name": "Yao Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105179755",
      "name": "Huang Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114070850",
      "name": "Lian DeFu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127526945",
      "name": "Xie Xing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2522925973",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3012754345",
    "https://openalex.org/W2510508396",
    "https://openalex.org/W4376312626",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W4385571411",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4386655575",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W2158599326",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2129888542",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2944854690",
    "https://openalex.org/W1825675169",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W4281396910",
    "https://openalex.org/W2963365341",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W4293865119",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4384655952",
    "https://openalex.org/W2597603852",
    "https://openalex.org/W4386071687",
    "https://openalex.org/W2972317931",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4283219931"
  ],
  "abstract": "Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.",
  "full_text": "RecExplainer: Aligning Large Language Models for Explaining\nRecommendation Models\nYuxuan Lei\nUniversity of Science and Technology\nof China\nHefei, China\nleiyuxuan@mail.ustc.edu.cn\nJianxun Lianâˆ—\nMicrosoft Research Asia\nBeijing, China\njianxun.lian@outlook.com\nJing Yao\nMicrosoft Research Asia\nBeijing, China\njingyao@microsoft.com\nXu Huang\nUniversity of Science and Technology\nof China\nHefei, China\nxuhuangcs@mail.ustc.edu.cn\nDefu Lianâˆ—\nUniversity of Science and Technology\nof China\nHefei, China\nliandefu@ustc.edu.cn\nXing Xie\nMicrosoft Research Asia\nBeijing, China\nxing.xie@microsoft.com\nAbstract\nRecommender systems are widely used in online services, with\nembedding-based models being particularly popular due to their\nexpressiveness in representing complex signals. However, these\nmodels often function as a black box, making them less transparent\nand reliable for both users and developers. Recently, large language\nmodels (LLMs) have demonstrated remarkable intelligence in under-\nstanding, reasoning, and instruction following. This paper presents\nthe initial exploration of using LLMs as surrogate models to explain-\ning black-box recommender models. The primary concept involves\ntraining LLMs to comprehend and emulate the behavior of target\nrecommender models. By leveraging LLMsâ€™ own extensive world\nknowledge and multi-step reasoning abilities, these aligned LLMs\ncan serve as advanced surrogates, capable of reasoning about ob-\nservations. Moreover, employing natural language as an interface\nallows for the creation of customizable explanations that can be\nadapted to individual user preferences. To facilitate an effective\nalignment, we introduce three methods: behavior alignment, inten-\ntion alignment, and hybrid alignment. Behavior alignment operates\nin the language space, representing user preferences and item in-\nformation as text to mimic the target modelâ€™s behavior; intention\nalignment works in the latent space of the recommendation model,\nusing user and item representations to understand the modelâ€™s\nbehavior; hybrid alignment combines both language and latent\nspaces. Comprehensive experiments conducted on three public\ndatasets show that our approach yields promising results in un-\nderstanding and mimicking target models, producing high-quality,\nhigh-fidelity, and distinct explanations. Our code is available at\nhttps://github.com/microsoft/RecAI.\nâˆ—Corresponding authors.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671802\nCCS Concepts\nâ€¢ Information systems â†’Recommender systems; â€¢ Comput-\ning methodologies â†’Natural language generation .\nKeywords\nLarge Language Models, Recommender Systems, Model Explain-\nability\nACM Reference Format:\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie.\n2024. RecExplainer: Aligning Large Language Models for Explaining Rec-\nommendation Models. In Proceedings of the 30th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,\nBarcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.\n1145/3637528.3671802\n1 Introduction\nRecommender systems provide the appropriate information to the\nright individual based on comprehending usersâ€™ preferences and\nintentions [19, 20, 25]. These systems have become an essential\ncomponent in various online services, including e-commerce, news,\nand television & movies. Embedding-based recommender models,\nsuch as collaborative filtering based on latent factors [20, 26] and\nsequential recommenders [16, 22], showcase their remarkable ex-\npressiveness in representing complex signals, and have thus been\nextensively utilized in recommender systems. However, embedding-\nbased models typically function in a black-box manner, resulting\nin a lack of explainability.\nModel explainability is a crucial aspect of building reliable and\ntrustworthy recommender systems. It offers multiple advantages,\nincluding insights into the underlying logic of systems, identifi-\ncation of bugs, detection of biases, and providing clues for model\nimprovement. One mainstream category of techniques for model\nexplanation involves training a surrogate model to align with orig-\ninal black-box model [21, 36, 37, 54]. This surrogate model must\nbe both human-interpretable and maintain (local) fidelity to the\noriginal model. Once trained, it serves as an effective explainer\nfor the original model. However, existing surrogate models typi-\ncally employed, such as sparse linear models and decision trees,\nare inherently explainable but usually compromise fidelity due to\ntheir simplicity. Furthermore, the explanations generated are often\narXiv:2311.10947v2  [cs.IR]  22 Jun 2024\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuxuan Lei et al.\nlimited to basic styles, such as additive weights or multiple deci-\nsion rules, and lack semantic interpretation from human-readable\nperspective.\nRecently, large language models (LLMs) have exhibited excep-\ntional versatility and proficiency in handling complex tasks such as\nquestion answering, code comprehension, reasoning, and instruc-\ntion following [2, 8, 17, 33, 34]. The remarkable capabilities of LLMs\npresent new opportunities to revolutionize various research fields\nwithin machine learning, including explainable AI. With an exten-\nsive repository of world knowledge embedded in their memory\nand powerful multi-step reasoning skills, LLMs are renowned for\ngenerating high-quality, human-readable explanations. As a result,\nthe traditional paradox that self-explainable surrogate models must\nbe simple and low-complexity may no longer hold true.\nIn this paper, we investigate the potential of utilizing an LLM as\na surrogate model for explaining recommender systems. We begin\nwith the traditional training approach, which primarily involves\naligning an LLM with a target recommendation model. The recom-\nmendation model is pre-trained and remains unaltered during this\nprocess. The LLM is then trained to emulate the recommendation\nmodelâ€™s predictive patternsâ€”given a userâ€™s profile as input, the LLM\nis fine-tuned to predict the items that the recommendation model\nwould suggest to the user. We refer to this approach asbehavior\nalignment.\nHowever, similar to traditional surrogate model-based approach,\nbehavior alignment merely mimics predictive observations from\noutside the model, attempting to deduce what is happening within\nthe black-box. We argue that a more profound way to explain\nthe execution logic of models involves enabling the LLM to di-\nrectly comprehend the neural layers of the recommender model.\nTherefore, we propose an alternative approach called intention\nalignment, wherein the embeddings (i.e., activations of neural\nlayers) of the recommender model are incorporated into the LLMâ€™s\nprompts to represent user and item information, and the LLM is\nfine-tuned to understand these embeddings. This approach can\nbe considered as a multimodal model, with textual words and rec-\nommendation model embeddings representing two distinct data\nmodalities. Take [15] from the series of vision-language multimodal\nmodels [23, 31, 43, 44] as an example. The image pixels are trans-\nformed into embeddings by a pre-trained vision model. The lan-\nguage model is then trained to comprehend the contents of the\noriginal image by incorporating these embeddings into the input\ncontext. Eventually, the language model aligns itself with the vision\nmodelâ€™s space, acquiring the ability to understand and respond to\nquestions about the image.\nMerging the advantages of both approaches, we introduce a\nnovel method called hybrid alignment. This strategy effectively\ncounteracts the hallucination issues associated with the intention\nalignment approach by incorporating both explicit titles and im-\nplicit embeddings in the prompt during training and inference\nstages. Thus, hybrid alignment facilitates a more robust and com-\nprehensive understanding of the recommender model, ultimately\nenhancing the LLM to generate highly credible explanations. To\nvalidate the effectiveness of our proposed approaches, we con-\nduct extensive experiments on three public datasets, examining\ntheir alignment effect and explanation generation ability. Empirical\nevidence demonstrates that LLMs can be successfully aligned to\naccurately reflect and comprehend the behavior of recommender\nmodels, highlighting their potential as a new type of surrogate\nmodel for explaining complex systems.\nOur contributions can be summarized as follows:\nâ€¢We propose to align LLMs for explaining recommender mod-\nels, presenting a significant potential to advance explainable AI\nresearch by overcoming the traditional dilemma of requiring\nsurrogate models to be simple for self-explainability.\nâ€¢To enable efficient model alignment, we introduce two distinct\napproaches: behavior alignment and intention alignment, each\nproviding unique benefits. Additionally, we present hybrid align-\nment, a method that combines the advantages of both approaches.\nâ€¢We rigorously evaluate these alignment approaches on three\npublicly available datasets, demonstrating their effectiveness in\nboth comprehension and explanation, highlighting the poten-\ntial of LLMs as a new type of surrogate model for explaining\nrecommender models.\n2 Methodologies\n2.1 Problem Formulation\nIn recommender systems, users are represented by their behavioral\nsequences: xğ‘¢ = âŸ¨ğ‘1,ğ‘2,...,ğ‘ |xğ‘¢ |âŸ©, where ğ‘âˆ— represents an item\nthat user ğ‘¢has interacted with in the past, and items are listed in\nchronological order. A recommender model ğ‘“()learns to assign\na higher score to items that users favor over those they donâ€™t:\nğ‘“(xğ‘¢,ğ‘ğ‘– ) > ğ‘“(xğ‘¢,ğ‘ğ‘— ), where ğ‘ğ‘– denotes a positive item and ğ‘ğ‘—\ndenotes a negative item for the user. To scale large systems, the two-\ntower model paradigm has been extensively employed in industrial\nrecommender systems, particularly in initial steps such as item\nrecall. In this paradigm, users and items are separately encoded\ninto embeddings: eğ‘¢ = ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿğ‘¢ğ‘ ğ‘’ğ‘Ÿ (xğ‘¢ ), eğ‘– = ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘š (ağ‘– ), and\nthe preference score is determined by the similarity betweeneğ‘¢ and\neğ‘– . This paper focuses on the two-tower model paradigm and leaves\nother paradigms such as the single-tower paradigm for future work.\nGiven a trained recommender model ğ‘“(), our objective is to tune\nan LLM ğ‘”()to explain the decision-making process within ğ‘“().\nIn next sections, we detail our methodologies for tuning LLMs\ninto recommendation model explainer (RecExplainer), covering\nthree styles: behavior alignment, intention alignment, and hybrid\nalignment, which are denoted as RecExplainer-B, RecExplainer-I,\nand RecExplainer-H, respectively.\n2.2 Behavior Alignment\nIn this approach, we fine-tune an LLM ğ‘”()such that its predictive\nbehavior aligns with the recommender model ğ‘“(). The hypothesis\nis that if an LLM ideally aligns with a target modelâ€™s predictions,\nit can imitate the execution logic of the target model and make\ncorresponding predictions, then the LLM can leverage its inherent\nknowledge and reasoning capabilities to generate an explanation\nfor its predictions. Fine-tuning tasks include:\nTask 1: Next item retrieval. Given item titles of user history xğ‘– ,\nthis task teaches the LLM about the recommendations the target\nmodel would make to the user. It is important to note that there\nare two major differences between Task 1 for training ğ‘”and the\ntraditional next item prediction task for training model ğ‘“. First, the\nlabel in Task 1 is based on the predicted items from the target model\nRecExplainer: Aligning Large Language Models for Explaining Recommendation Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\n(a) Process for Task 1, 2 and 3\nUser history, Item candidates Prompt\nPredictions\n(rating or ranking)\nPredictions\n(rating or ranking)\nRecommender LLM\nalign\n(b) Process for Task 4\nItem candidates Prompt\nAttributes\n(tags, descriptions, ...)\nGenerated \nattributes\nDatabase LLM\nalign\n(c) Process for Task 4 and  6\nUser history, Item candidates Recovered Info\nUser or item \nembddings Prompt\nRecommender LLM\nalign\nFigure 1: Graphical illustrations for aligning LLM with different tasks.\nLLM\nRecommender model\nWill this user  <user>  love item  <item>  ?\nGive your answer and explain why.\nExplanations\nItem&User \nEmb. MLP\nLoRA\nFrozen\nNot Frozen\nFigure 2: The RecExplainer framework.\nğ‘“, rather than the ground truth in the original dataset. Second, the\ninput and output content do not contain item IDs but are replaced\nwith textual titles. These modifications ensure that the LLM focuses\non understanding the target modelâ€™s decision-making patterns.\nTask 2: Item ranking. Given the item titles of user history xğ‘–\nand a short list of item candidates pğ‘– = âŸ¨ğ‘1,ğ‘2,...,ğ‘ ğ‘˜ âŸ©, this task\nteaches the LLM to reorder pğ‘– to reflect the order provided by\nğ‘“(). Retrieval and ranking are two of the most crucial tasks for\nrecommender systems; therefore, Task 1 and Task 2 are specifically\ndesigned to align an LLM with the recommendation process of the\nrecommender model.\nTask 3: Interest classification. Given the item titles of user his-\ntory xğ‘– and one candidate item ğ‘ğ‘— , the LLM must generate a binary\nlabel: like or dislike, reflecting whether user ğ‘– likes item ğ‘— or not\nfrom the perspective of ğ‘“(). To prepare training data samples, we\nset up two thresholdsğ‘¡+and ğ‘¡âˆ’, selecting items with ğ‘“(xğ‘–,ğ‘ğ‘— )> ğ‘¡+\nas positive samples, and items with ğ‘“(xğ‘–,ğ‘ğ‘— )< ğ‘¡âˆ’as negative sam-\nples. Task 3 serves as a complement to Task 2: while Task 2 teaches\nthe LLM to recognize relative orders between items, it lacks the\nability to discern the absolute sentiment of ğ‘“()towards items.\nTask 4: Item discrimination. A pretrained LLM may not possess\nsufficient knowledge about all items in a recommendation domain.\nThis insufficiency can arise due to various reasons, such as the\npresence of fresh items and domain-specific items that appear less\nfrequently in general knowledge sources. To address this issue of\nmissing item knowledge, we design the item discrimination task:\ngiven an item title, let the LLM describe item details, including tags,\ndescriptions, and related items1. This task helps the LLM to better\nunderstand the characteristics of items.\nTask 5: ShareGPT training. To mitigate catastrophic forgetting,\nwhich leads to a decline in the LLMâ€™s general intelligence during\nfine-tuning, we also incorporate a general-purpose instruction tun-\ning dataset. ShareGPT2 is a publicly available dataset that contains\nconversations between users and ChatGPT, which is gathered from\nShareGPT.com with public APIs. This dataset has been used for\ntraining numerous popular LLMs, such as Vicuna [ 5] and MPT3.\nIncorporating ShareGPT training helps preserve the LLMâ€™s general\nintelligence and adaptability while fine-tuning on specific tasks.\nThis task is important because when generating explanations, not\nonly does LLM need to understand the target recommender model,\nbut it also needs to combine its own reasoning, instruction following\nand other general intelligence abilities.\nAll five tasks play a crucial role in generating training samples\nfor behavior alignment. After fine-tuning, the LLM is prompted\nto produce model explanations. For example, given a prompt like\n\"[some system prompts here] Given a user with history: item title1,\nitem title2, ..., will you recommend item xx to the user and why?\",\nthe LLM can mimic the execution logic of the recommendation\nmodel and generate a well-informed and coherent explanation,\ndemonstrating its understanding of the underlying recommenda-\ntion process and user preferences.\n2.3 Intention Alignment\nNonetheless, LLMs exhibit capabilities far beyond mere behavior\ncloning. Recently, cross-modality training has shown remarkable\nsuccess in enabling LLMs to comprehend multimodal content. For\nexample, vision-language models treat text and images as two dis-\ntinct modalities. By aligning perceptions derived from text and\nimages, the resulting LLM can effectively understand the content\nof images. Consequently, by leveraging the inherent reasoning\n1For each item, we select the top k items as its related items based on item embedding\nsimilarities generated by the target recommender model.\n2https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered\n3https://www.mosaicml.com/blog/long-context-mpt-7b-8k\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuxuan Lei et al.\nabilities of the LLM, it becomes capable of providing linguistic ex-\nplanations for images, such as answering the question, \" Explain\nwhy this photo is funny \".\nBuilding upon these insights, we treat the user and item embed-\ndings generated from ğ‘“ as a unique modality. This data modality\ncaptures the characteristics of items and usersâ€™ preferences. Con-\nsequently, we aim to align the LLMâ€™s perceptions with those origi-\nnating from the user and item embeddings. We term this approach\n\"intention alignment\", and its underlying hypothesis is that if an\nLLM can comprehend the neurons of the target model while retain-\ning its multi-step reasoning capabilities, it holds the potential to\nelucidate the decision-making logic of the target model.\nTo establish an effective connection between the LLM and em-\nbeddings from ğ‘“, we modify the training data for Tasks 1 through\n4 by replacing item names in the query prompts with their corre-\nsponding embeddings. For instance, a prompt of Task 1 becomes\n\"[some system prompts here] Given a user with history: [a vector\nof user embedding], generate the next most likely item title. \". This\nforces the LLM to generate accurate responses based on user and\nitem embeddings. Specifically, for Tasks 1, 2, and 3, we replace the\nsequence of item titles in user history with a special token <user>\nand map it to a single projected user embedding eeğ‘¢ :\neeğ‘¢ = ğºğ¸ğ¿ğ‘ˆ(eğ‘¢ğ‘Š1 +b)ğ‘Š2 (1)\nThe projection operation aims to extend the original user embed-\ndings generated by f (e.g., with a dimension of 32) to match the\nlength of token embeddings in the LLM (e.g., with a dimension\nof 4096). For Tasks 2, 3, and 4, we substitute the candidate item\ntitle with a special token <item> and map it to the projected item\nembedding eeğ‘– , using a projection similar to 1 but with a new set of\nparameters.\nIn addition to Tasks 1 through 5, we design an auxiliary task for\nintention alignment to enhance the information fidelity between\nuser embeddings and the usersâ€™ true history:\nTask 6: History reconstruction. Given a projected user embed-\nding eeğ‘¢ , this task recovers the titles of items in the userâ€™s history or\nthe preference summary of the user history. We leverage GPT-44\n[29] to generate a preference summary for each user based on user\nhistory titles in advance. It is important to note that Tasks 2 and 3\nprimarily concentrate on understanding the relationships between\nuser embeddings and item embeddings, but they do not sufficiently\nexplore the self-contained information within user embeddings.\nTask 6 is designed to address this limitation.\nFor better illustration, Figure 1 provides a comparison among\nthe processes of different tasks to highlight the distinctions, while\nFigure 2 shows the model architecture of the intention alignment.\n2.4 Hybrid Alignment\nThe intention alignment approach entirely depends on user/item\nembeddings to decode preference-related information, which may\nlead to a too strict hypothesis. During the training of model ğ‘“, a\ncertain degree of information will inevitably be lost, such as it is\nhard to fully identify every item in user history from the encoded\nuser embeddings. To mitigate the information lost, we design the\n4The snapshot of GPT-4 is gpt-4-0314.\nthird approach called \"hybrid alignment\", combining both the pre-\nvious approaches. All Task 1 through 6 are included. for tasks that\ninvolve user history or item candidates, hybrid alignment not only\ninclude both data forms of behavior alignment and intention align-\nment, but also add a new data form: simultaneously put both user\nhistory/item candidates and user/item embeddings in the query\nprompt. Thus, a prompt may look like: \"[some system prompts\nhere] Given a user with history: [a vector of user embedding], item\ntitle1, item title2, ..., generate the next most likely item title. \"\n3 Experiments\n3.1 Evaluation Strategies and Metrics\nIn measuring performance of our RecExplainer on explaining rec-\nommendation models, we evaluate from two perspectives:\n3.1.1 Alignment Effect. We first assess the LLMâ€™s alignment effect,\nthat is, to what extent LLMs can understand neurons and predictive\npatterns of the target recommender model. Following the previous\nwork [52], we apply the leave-one-out strategy for evaluation. We\ntake the last item of each userâ€™s interaction sequence as the test\ndata and use the other records for training the LLM and target\nrecommender model. It should be noted that when training the\ntarget recommender model, we use labels from the original dataset,\nbut when training the LLM, we use labels inferred by the well-\ntrained recommendation model. We evaluate four alignment tasks,\nincluding task1 (next item retrieval), task2 (item ranking), task3\n(interest classification), and task6 (history reconstruction). For next\nitem retrieval, we adopt the top-K hit ratio (HR) and top-K nor-\nmalized discounted cumulative gain (NDCG) for evaluation, where\nwe set K to 5. For item ranking, we calculate the NDCG@5. For\ninterest classification, we use classification accuracy. For history\nreconstruction, we define a history coverage ratio (HCR) metric,\nwhich calculates the proportion of items in the user history that\nappear in the predicted sequence. During the inference of LLM, we\nuse greedy decoding to generate texts and consider a successful\noutput when there is a strict string match between generated item\nnames and their ground-truth names.\n3.1.2 Explanation Generation Ability. Considering that there is\nno available ground truth for the explanation, we need a new\nevaluation system to demonstrate the effectiveness of our method\non model explainability. Specifically, we design an instruction to\nprompt the LLM to first evaluate the target item and then generate\na coherent explanation:\n\"The user has the following purchase history: {USER HISTORY} .\nWill the user like the item: {ITEM} ? Please give your answer and\nexplain why you make this decision from the perspective of a recom-\nmender model. Your explanation should include the following aspects:\nsummary of patterns and traits from user purchase history, the con-\nsistency or inconsistency between user preferences and the item. \"\nFollowing [46], we implement a four-level scoring system to\nquantitatively evaluate the response from the LLM. Complete crite-\nria can be found in Appendix A.1.\nâ€¢RATING-0 : Incorrect classification.\nâ€¢RATING-1 : Correct classification, insufficient explanation. LLM\nprovides irrelevant explanations or provide explanations with\nhallucination.\nRecExplainer: Aligning Large Language Models for Explaining Recommendation Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nâ€¢RATING-2 : Correct classification, acceptable explanation with\nminor imperfections such as lack of persuasiveness or informa-\ntiveness.\nâ€¢RATING-3 : Correct classification, satisfying explanation.\nThe evaluation criteria are formulated with a two-step approach:\ninitially assessing correctness of the classification, followed by the\nassessment of explanation quality. The correctness of classification\nholds significant importance as it serves as an indicator of whether\nthe LLM is formulating explanations based on an accurate under-\nstanding of the target model, rather than relying on conjecture.\nConsidering that human annotation is extremely time-consuming\nand labor-intensive, we adopt a combined approach using both hu-\nman annotators and LLM annotators. Some studies [ 4, 7] have\nalready demonstrated that LLM can to some extent replace manual\nannotations. Since GPT-44 is currently the most powerful LLM with\nstrong abilities to follow instructions and perform reasoning tasks,\nwe adopt both GPT-4 scoring and human scoring strategies to evalu-\nate our generated explanations. More specifically, for GPT-4, we use\nthe aforementioned evaluation criteria as prompts, inputting them\ninto GPT-4 to generate scores. We sample 500 test cases for each\ndataset, calculating the mean score of each LLM on each dataset.\nRegarding human scoring, due to cost considerations, we select\na sample of 120 test cases from a single dataset. Given that there\nare five different LLMs for text generation, this results in a total of\n600 generated texts for human evaluation. For more details about\nhuman and GPT-4 annotations, please refer to Appendix A. These\nhuman evaluation results effectively complement the GPT-4 evalu-\nation results, providing a comprehensive assessment.\n3.2 Experimental Setup\n3.2.1 Datasets. We evaluate our model on three public datasets:\nVideo Games and Movies & TV dataset released in Amazon plat-\nform5[28], and Steam6[16, 30]. For the specific tasks, we generate\ndata as follows: for next item retrieval, we treat the top-1 prediction\nof the target recommender model as the ground truth; for item\nranking, we sample five items from the entire item set for each\nsample, and use the ranking order produced by the target model\nas the ground truth; for interest classification, we set the ğ‘¡+and\nğ‘¡âˆ’threshold as the top 20%, bottom 50% respectively, and sample\none positive and one negative item for each user for training and\ntesting. The dataset details can be found in Appendix B.\n3.2.2 Implementation details. Our backbone LLM is vicuna-v1.3-7b\n[5] with a maximum context length of 1024. We employ LoRA [14]\nfor parameter-efficient tuning and leverage DeepSpeedâ€™s ZeRO-2\n[35] to further reduce gpu memory consumption. We use 8 NVIDIA\nV100-32GB GPUs and fp16 for training 10 epochs, with a total batch\nsize of 64. The peak learning rate is 1e-4 with a linear warmup\nfor the first 10% steps. We train our model using the standard\nlanguage modeling objective and only compute loss on the response\ntokens. For the target recommender model, we adopt the powerful\ntransformer-based model SASRec [16], which is lightweight and\neffective. Hyperparameter tuning is performed to train SASRec on\nthree datasets, obtaining a well-trained sequential recommender.\n5https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/\n6https://github.com/kang205/SASRec\nSpecifically, the embedding size is set to 256, 192, 256 on Video\nGames, Movies & TV and Steam datasets, respectively, and the\nmax sequence length is set to 9, 13, 16 respectively. During LLMâ€™s\ntraining, the target SASRec is fixed, which is used only for inferring\nuser embeddings and item embeddings.\n3.2.3 Baselines. For evaluating the alignment effect, we employ\ntwo statistical models, a raw LLM and three aligned models.\nâ€¢Random: Sample k items uniformly from the item set for re-\ntrieval. Random shuffle the candidate items for ranking.\nâ€¢Popularity: Sample k items based on the item popularity dis-\ntribution for retrieval. Sort the candidate items according to the\nitem popularity.\nâ€¢Vicuna-v1.3-7B [ 5]: An open-source LLM obtained by fine-\ntuning the LLaMa model on ShareGPT data. This model serves as\nthe base model which is not fine-tuned on our in-domain dataset.\nâ€¢Vicuna-v1.3-7B-ICL [5]: In-context learning is an effective ap-\nproach to align LLMs to do specific tasks. Specifically, when\nperforming a specific task, we randomly select two instances\nfrom the training set of that task and place them at the beginning\nof the LLMâ€™s context.\nâ€¢GPT4-ICL [29]: Same in-context learning strategy but with the\ncurrently most powerful closed-source LLM from OpenAI.\nâ€¢SASRec [16]: a mainstream traditional sequential recommender\nmodel. To enable the SASRec to align to the target recommender\nmodel and complete several alignment tasks, we use knowledge\ndistillation outlined in EMKD[10]. Specifically, we minimize the\nKullback-Leibler divergence between the teacher logits and the\nstudent logits, where the logits represent the scores given by\nusers for the entire item set.\nFor evaluating the explanation generation ability, we use Vicuna-\nv1.3-7B and ChatGPT7 (gpt-3.5-turbo-0301), as other aforemen-\ntioned methods either are not text generation models or lack readily\navailable examples for in-context learning. Additionally, the three\nproposed alignment methods themselves are suitable for mutual\ncomparisons on both evaluation settings.\n3.3 Performance w.r.t. Alignment\nTo investigate the alignment effect of LLM after training, we eval-\nuate modelâ€™s performance on four recommendation-related tasks,\nwith the results presented in Table 1. It should be noted that for\nTasks 1, 2 and 3 in Table 1, we use the inference results of the target\nrecommender model as ground truth labels and the SASRec base-\nline in Table 1 is actually another model aligned with the target\nrecommender model. We have the following observations:\nRecExplainer-H can achieve comparable performance with the\npowerful SASRec, and often performs better in retrieval (task1)\nand classification (task3) tasks. This demonstrates that our RecEx-\nplainerâ€™s alignment training is sufficiently effective, as only thor-\nough alignment can ensure the reliability of subsequent explanation\ngeneration. For the vicuna-7b model without alignment, the perfor-\nmance is unsatisfactory across all tasks. This suggests that there\nis still a significant gap between the target model and the LLMs,\ndemonstrating the necessity of alignment training. In comparison\nto vicuna-7B, vicuna-7B-ICL with the adoption of the in-context\n7https://chat.openai.com/\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuxuan Lei et al.\nTable 1: Performance w.r.t Alignment to the target recommender model. \"N/A\" represents that the method can not be applied to\ncorresponding task.\nDataset Amazon Video Games Amazon Movies and TV Steam\nTask Task1 Task2 Task3 Task6 Task1 Task2 Task3 Task6 Task1 Task2 Task3 Task6\nMethods H@5 N@5 N@5 ACC HCR H@5 N@5 N@5 ACC HCR H@5 N@5 N@5 ACC HCR\nRandom 0.0023 0.0015 0.6153 0.5030 N/A 0.0050 0.0030 0.6100 0.4987 N/A 0.0060 0.0039 0.6139 0.4977 N/A\nPopularity 0.0077 0.0047 0.6683 N/A N/A 0.0150 0.0088 0.7044 N/A N/A 0.0321 0.0201 0.7971 N/A N/A\nVicuna-7B 0.0026 0.0014 0.2391 0.5026 N/A 0.0091 0.0062 0.2706 0.5011 N/A 0.0028 0.0015 0.3229 0.5000 N/A\nVicuna-7B-ICL 0.0379 0.0304 0.2661 0.5070 N/A 0.0144 0.0104 0.3005 0.5079 N/A 0.0461 0.0332 0.2907 0.5076 N/A\nGPT4-ICL 0.1105 0.0864 0.6492 0.6338 N/A 0.0886 0.0692 0.5954 0.5964 N/A 0.3008 0.2525 0.6750 0.5886 N/A\nSASRec 0.6736 0.5234 0.8759 0.7768 N/A 0.6217 0.5025 0.8252 0.6541 N/A 0.9751 0.8780 0.9577 0.8914 N/A\nRecExplainer-B 0.7460 0.6260 0.7521 0.8365 N/A 0.8106 0.7027 0.7033 0.7818 N/A 0.9310 0.8100 0.8699 0.9554 N/A\nRecExplainer-I 0.8436 0.6994 0.8299 0.9385 0.1162 0.9039 0.7709 0.8290 0.8396 0.1201 0.9615 0.8122 0.9083 0.9904 0.0659\nRecExplainer-H 0.8057 0.6922 0.8458 0.9189 0.1325 0.8773 0.7750 0.7638 0.8109 0.1461 0.9358 0.8242 0.9036 0.9815 0.0707\nlearning method shows some improvements, but its performance\nremains relatively low. On the other hand, gpt4-ICL demonstrates\nsignificantly higher performance than vicuna-7B-ICL, showcas-\ning its strong general intelligence. However, the performance of\ngpt4-ICL still lags far behind RecExplainer-H, as alignment training\nenables the LLM to thoroughly learn the recommendation paradigm\nof the target model on the entire dataset.\nRegarding our three alignment methods, RecExplainer-B per-\nforms the worst across all tasks and datasets, suggesting that merely\nimitating the recommendation behavior of the target model is not\nan optimal solution for understanding the target model. Given\nthat the neurons of the target recommender model (such as user\nand item embeddings) can inherently reflect the recommendation\nparadigms and collaborative signals in the target model, we can\nsee that the performance of RecExplainer-I improves significantly\nwhen these embeddings are used as part of prompts for the LLM. For\nRecExplainer-H, its performance is superior to all other approaches\nexcept on next item retrieval (task1) and interest classification\n(task3) tasks, which are slightly lower than that of RecExplainer-I\nin some datasets. A possible reason is that when both neuron signals\nand textual signals are added to LLMâ€™s inputs, the LLM may overly\nrely on the text and, to some extent, neglect the role of neurons.\nOverall, The performance of RecExplainer-H is very powerful, indi-\ncating that textual and neuron signals can complement each other,\njointly enhancing the LLMâ€™s understanding of the target model.\nIn conclusion, compared to models without alignment, LLMs\nwith alignment training significantly enhance their predictive abil-\nity for the pre-trained target model. They can achieve comparable\nperformance with existing alignment-based recommendation mod-\nels, indicating that LLMs with alignment training have effectively\nlearned the paradigm and neurons of the target model, making it\nsuitable for subsequent recommendation explanation tasks.\n3.4 Performance w.r.t. Explanation\n3.4.1 Overall Ratings. Evaluation results from GPT-4 and human\nexperts are shown in Table 2 and Figure 3 respectively. We have\nthe following observations:\nThe trend of the two evaluation strategies are the same, val-\nidating the credibility of our evaluation method. Among which,\nRecExplainer-H achieves the highest scores on all three datasets,\nindicating that it can mimic the execution logic of the target model\nwell and give satisfying explanations for that logic. RecExplainer-B\ncomes next, suggesting that behavioral imitation is also helpful in\nunderstanding the execution paradigm of target models.\nFor the two unaligned LLMs, Vicuna-7B and ChatGPT, they can\ngenerate reasonably good explanatory texts in some cases through\ntheir powerful reasoning capabilities and internal knowledge. How-\never, since they are unrelated to the target model and are not aware\nof the target modelsâ€™ predictive patterns, they are not sure whether\nthe target model would recommend the item or not. As a result,\ntheir explanations tend to be ambiguous and lack persuasiveness\nand their scores tend to fall under the RATING-2.\nAnother point worth mentioning is that we find RecExplainer-I\nhas the lowest evaluation scores. By examining specific examples,\nwe discover that it generates explanations with hallucination, such\nas mistaking other items as the current userâ€™s history. This indicates\nthat there might be a certain gap in directly reconstructing textual\ncontent from neuron signals, as the information may be insufficient.\nThis is also demonstrated by the relatively low metrics of the history\nreconstruction task in the previous section.\n3.4.2 Distinction and Coherence. To further verify whether Rec-\nExplainer are indeed explaining its own predictions, we conduct\nvalidation from two perspectives: (1) Is RecExplainerâ€™s explanations\ndistinct from other LLMâ€™s explanations? (2) Do RecExplainerâ€™s ex-\nplanations reflect RecExplainerâ€™s predictions?\nWe generate 2500 explanations for each of Vicuna-7B, ChatGPT,\nand RecExplainer-H, and divide them into training and testing sets\nat 4:1 ratio. Firstly, we train a discriminator to prove that the expla-\nnations generated by RecExplainer possess sufficient distinctiveness\nand are different from those produced by models without align-\nment training. The experimental results are illustrated in Figure 4,\nrevealing that this discriminator can easily differentiate explana-\ntions from RecExplainer and other models. Secondly, we develop\nscore predictors to assess the alignment between the explainerâ€™s tex-\ntual explanations and the target recommender modelâ€™s predictions.\nSpecifically, for a given user-item pair (ğ‘¢,ğ‘–), ğ‘“(xğ‘¢,ğ‘ğ‘– )represents\nthe prediction made by the target recommender model. The score\nRecExplainer: Aligning Large Language Models for Explaining Recommendation Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\npredictors are then trained using the explainerâ€™s textual explana-\ntions as input, with the goal of closely approximating ğ‘“(xğ‘¢,ğ‘ğ‘– ).\nThis is a regression task, so we evaluate this using Mean Squared\nError (MSE). The results, as presented in Table 3, indicate that ex-\nplanations produced by RecExplainer offer a significant advantage\nwhen used to predict scores of the target model, confirming that\nRecExplainer effectively utilizes the understanding of the target\nmodelâ€™s behavior patterns during the explanation generation pro-\ncess. Both the discriminator and the score predictor employed in\nthis study are based on the base version of BERT[17].\nTable 2: Performance w.r.t. Explanation (GPT-4). The higher\nscore represents the better performance in explanation, rang-\ning from 0 to 3.\nMethods Games Movies Steam\nVicuna-7B 2.0703 2.0261 2.0341\nChatGPT 1.9320 1.8360 1.9560\nRecExplainer-B 2.3240 2.1360 2.4660\nRecExplainer-I 1.6653 1.4689 1.3394\nRecExplainer-H 2.5240 2.2204 2.4920\nTable 3: Performance w.r.t score predictors. The metric is\nMean Squared Error (MSE).\nMethods Games Movies Steam\nVicuna-7B 3.6970 2.8373 0.9687\nChatGPT 3.4803 2.8393 1.0288\nRecExplainer-H 1.2786 1.9190 0.3248\n0%\n20%\n40%\n60%\n80%\n100%\nVicuna-7B ChatGPT\nRecExplainer-BRecExplainer-IRecExplainer-H\nR 0: bad response\nR 1: insufficient response\nR 2: acceptable response\nR 3: perfect response\nFigure 3: Performance w.r.t Explanation (Human experts) on\nAmazon Video Games dataset.\nVicuna ChatGPT Ours\nPredicted labels\nVicunaChatGPTOurs\nTrue labels\n498 2 0\n7 493 0\n0 0 500\nVideo Games\nVicuna ChatGPT Ours\nPredicted labels\nVicunaChatGPTOurs\nTrue labels\n497 3 0\n5 494 1\n1 0 499\nMovies\nVicuna ChatGPT Ours\nPredicted labels\nVicunaChatGPTOurs\nTrue labels\n493 7 0\n5 495 0\n0 0 500\nSteam\nFigure 4: Confusion matrix of explanation discrimination.\n3.5 Case Study\n3.5.1 Explanation Quality. We show cases of each method for\nstraightforward effect of explanation, illustrated in Figure 5. Both\nRecExplainer-B and RecExplainer-H give convincing explanations,\nwhich point out that firstly the user prefers gaming accessories and\ndevices instead of games, and secondly the user has no explicit en-\ngagement in Xbox-360 platform, exhibiting high consistency with\nthe output of the target recommender model. Nevertheless, Vicuna\nand ChatGPT do not give a satisfying and persuasive explanation.\nThis is because they do not align with the target model and can\nonly rely on their own knowledge and logic to make certain con-\njectures about user preferences, which may cause errors. Notably,\nRecExplainer-I exhibits hallucination in giving non-existing games.\nThis illustrates that although the alignment is effective, solely rely-\ning on hidden neurons to recover the domain information of user\nhistory/items and make explanations are not enough due to the\ninformation compression loss in embeddings of the target model.\n3.5.2 Controllable Explanations. Benefit from the powerful instruc-\ntion following capability and multi-step reasoning ability of LLM,\nour RecExplainer possesses interactive capabilities, enabling itself\nto understand user requirements and dynamically adjust the con-\ntent of its explanations accordingly. Concretely, we instruct RecEx-\nplainer to predict and explain from two different view, i.e. the game\nplatforms and game genres, the cases are shown in Figure 6. The\nmodel could generate consistent predictions with the target model\nin each case, and the two explanations indeed vary correspond-\ning to the instructions, demonstrating the remained instruction\nfollowing capability and the controllability of our RecExplainer.\n3.6 Ablation Study w.r.t Explanation\nIn our method, we design multiple training tasks to help the LLM\nbetter understand target recommender models and domain-specific\ndata. To explore the impact of each task on LLMâ€™s explanation gen-\neration ability, we remove each task to train a RecExplainer and\nevaluate the explanation quality using GPT-4. As shown in Figure\n7, each training task contributes to the final explanation quality.\nSpecifically, task3 (interest classification) and task 6 (history recon-\nstruction) have shown to have the most substantial impacts. On\nthe one hand, compared to item embedding, information in user\nembedding is more severely compressed, and history reconstruc-\ntion effectively facilitates learning this information. On the other\nhand, during explanation generation, the model needs to justify the\nprediction of the target model. Therefore, the classification task also\nhas a significant impact on the modelâ€™s explanation performance.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuxuan Lei et al.\nFigure 5: Case study for explanation on Amazon Video Games\ndataset. Fonts highlighted in red signify the presence of er-\nrors within the generated explanations, while those high-\nlighted in green denote accurate and logical explanations.\n4 Related Work\n4.1 Model Explainability\nDeep neural models have demonstrated state-of-the-art (SOTA)\nperformance in various machine learning tasks, and explaining the\ndecision-making logic behind these black-box models has become\na significant area of research. Existing literature in this field can be\nbroadly divided into two categories [49]. The first category focuses\non identifying the most salient parts of input features that con-\ntribute to the decision result of the target model, with the primary\nchallenge being the effective implementation of score attribution.\n[11] present early work investigating the explainability of deep\nmodels by visualizing what a neuron computes in the input space,\nusing several alternative methods such as sampling from a neuron\nand maximizing the activation of a neuron. Input space visualization\nFigure 6: Case study for controllable explanation on Amazon\nVideo Games dataset.\nOurs w/o task1&2w/o task3 w/o task4 w/o task5 w/o task6\n1\n2\nGPT Score\n2.53\n2.44\n1.91\n2.39 2.41\n2.03\nFigure 7: Ablation study on Amazon Video Games dataset.\nhas since been widely applied in vision tasks [50, 51] to facilitate\nhuman understanding of deep models. [3, 12] propose models that\nselect salient input features by maximizing the mutual information\nbetween salient features and response variables. Additionally, other\npopular feature selection methods include neuron contribution dif-\nference [38], integrated gradients [40], influence functions [18], and\nShapley value estimation [6, 27, 39].\nThe second category of methods involves training a surrogate\nmodel to explain the target model. Surrogate models should main-\ntain high fidelity to the target modelâ€™s predictive patterns while\nalso possessing explainable properties. Consequently, linear models\nand decision trees are the most commonly used surrogate models.\n[21, 37, 54] develop methods for generating a small number of com-\npact decision rules as surrogates to explain the target model. [36]\nproposes a general model explanation framework that allows for\nthe flexible selection of various surrogate model forms, such as\nlinear models or decision trees. However, surrogate models need\nRecExplainer: Aligning Large Language Models for Explaining Recommendation Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nto be structurally simple for explaining complex models, which\ncreates a dilemma with model fidelity. Recently, LLMs have demon-\nstrated strong versatility in terms of predictive accuracy and reason-\ning/explanation ability, providing an opportunity to overcome this\ndilemma. As a novel advancement in the first category of related\nworks, [1] utilizes GPT-4 to automatically generate explanations\nfor neurons based on the attributed input features. To the best of\nour knowledge, this paper is the first to discuss leveraging LLMs\nfor the second category of methods.\n4.2 LLMs and Multimodality\nIn recent years, the Transformer architecture [ 42] has become a\nfundamental building block for language models. Researchers have\ndiscovered that by pretraining large Transformer-based language\nmodels on extensive open data sources, these models exhibit en-\nhanced capabilities in knowledge accumulation, multi-task learning,\nand few-shot or even zero-shot predictions [ 2, 8, 17, 33, 34]. Re-\nmarkably, when the model size reaches a certain scale (e.g., 170\nbillion parameters), language models exhibit emergent abilities [47],\nwhich are unforeseen phenomena, such as instruction following,\nreasoning, and problem-solving skills, indicating a preliminary step\ntowards AGI. Consequently, researchers have begun to investigate\nwhether emergent abilities can also be present in smaller-scale\nmodels (e.g., 7B models) if trained effectively. This inquiry has led\nto the development of several popular models, including OPT [53],\nLlama [41], Vicuna [5], WizardLM [48], and phi-1.5 [24].\nThe intelligence of large models is not solely confined to text;\nother data modalities, such as images [ 9, 13] and audio [32], can\nalso benefit from large-scale pretraining. Multimodal models [23,\n31, 43, 44] seek to break boundaries and bridge different modalities,\nas humans can simultaneously comprehend multimodal knowledge\nand perform complex tasks using all available information. Fun-\ndamentally, multimodal models align the modelsâ€™ representations\nacross different modalities in the form of latent embeddings [15, 45],\nenabling them to not only perceive each individual modality but\nalso the interactions across modalities. With this perspective in\nmind, this paper considers the embeddings of recommender models\nas a new data modality and proposes a novel model explainer based\non aligning LLMs.\n5 Conclusion\nIn this paper, we investigate the potential of employing large lan-\nguage models (LLMs) as surrogate models to enhance the explain-\nability of recommender systems. LLMs, known for generating high-\nquality, human-readable explanations, offer a promising solution\nto the traditional dilemma of necessitating simple models for self-\nexplainability, thus paving the way for more advanced and trans-\nparent AI systems. We introduce three innovative alignment ap-\nproaches â€” behavior alignment, intention alignment, and hybrid\nalignment â€” to facilitate effective model alignment. Each of these\napproaches offers unique advantages in terms of explainability\nand fidelity. Through rigorous evaluation on three publicly avail-\nable datasets, we demonstrate the effectiveness of our proposed\nalignment approaches in both comprehension and explanation. Em-\npirical evidence highlights the potential of LLMs as a new type\nof surrogate model for explainable recommender systems. As an\ninitial attempt, our research contributes to the ongoing efforts in\nexplainable AI, paving the way for future work on leveraging LLMs\nfor a wide range of explainability applications in complex systems.\nAcknowledgments\nThe work was supported by grants from the National Key R&D\nProgram of China (No. 2021ZD0111801) and the National Natural\nScience Foundation of China (No. 62022077).\nReferences\n[1] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel\nGoh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language\nmodels can explain neurons in language models. URL https://openaipublic. blob.\ncore. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)\n(2023).\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[3] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. 2018. Learning\nto explain: An information-theoretic perspective on model interpretation. In\nInternational conference on machine learning . PMLR, 883â€“892.\n[4] David Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be\nan Alternative to Human Evaluations?. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023 . Association for Computational Linguistics,\n15607â€“15631. https://doi.org/10.18653/V1/2023.ACL-LONG.870\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See\nhttps://vicuna. lmsys. org (accessed 14 April 2023) (2023).\n[6] Anupam Datta, Shayak Sen, and Yair Zick. 2016. Algorithmic transparency via\nquantitative input influence: Theory and experiments with learning systems. In\n2016 IEEE symposium on security and privacy (SP) . IEEE, 598â€“617.\n[7] Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty,\nand Lidong Bing. 2023. Is GPT-3 a Good Data Annotator?. InProceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), ACL 2023, Toronto, Canada, July 9-14, 2023 . Association for Computational\nLinguistics, 11173â€“11195. https://doi.org/10.18653/V1/2023.ACL-LONG.626\n[8] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-\ntraining for natural language understanding and generation. Advances in neural\ninformation processing systems 32 (2019).\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. In9th Interna-\ntional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net. https://openreview.net/forum?id=YicbFdNTTy\n[10] Hanwen Du, Huanhuan Yuan, Pengpeng Zhao, Fuzhen Zhuang, Guanfeng Liu, Lei\nZhao, Yanchi Liu, and Victor S. Sheng. 2023. Ensemble Modeling with Contrastive\nKnowledge Distillation for Sequential Recommendation. InProceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023 . ACM, 58â€“67. https://doi.\norg/10.1145/3539618.3591679\n[11] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2009. Visu-\nalizing higher-layer features of a deep network. University of Montreal 1341, 3\n(2009), 1.\n[12] Chaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin Chen, Di He, and Xing Xie.\n2019. Towards a Deep and Unified Understanding of Deep Neural Models in\nNLP. In Proceedings of the 36th International Conference on Machine Learning\n(Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and\nRuslan Salakhutdinov (Eds.). PMLR, 2454â€“2463. https://proceedings.mlr.press/\nv97/guan19a.html\n[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition . 16000â€“16009.\n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of\nLarge Language Models. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.\nhttps://openreview.net/forum?id=nZeVKeeFYf9\n[15] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shum-\ning Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuxuan Lei et al.\nLanguage is not all you need: Aligning perception with language models. arXiv\npreprint arXiv:2302.14045 (2023).\n[16] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-\nommendation. In IEEE International Conference on Data Mining, ICDM 2018,\nSingapore, November 17-20, 2018 . IEEE Computer Society, 197â€“206. https:\n//doi.org/10.1109/ICDM.2018.00035\n[17] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of naacL-HLT , Vol. 1. 2.\n[18] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions\nvia Influence Functions. In Proceedings of the 34th International Conference on\nMachine Learning (Proceedings of Machine Learning Research, Vol. 70) , Doina\nPrecup and Yee Whye Teh (Eds.). PMLR, 1885â€“1894. https://proceedings.mlr.\npress/v70/koh17a.html\n[19] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted\ncollaborative filtering model. InProceedings of the 14th ACM SIGKDD international\nconference on Knowledge discovery and data mining . 426â€“434.\n[20] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-\nniques for recommender systems. Computer 42, 8 (2009), 30â€“37.\n[21] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2017. In-\nterpretable & explorable approximations of black box models. arXiv preprint\narXiv:1707.01154 (2017).\n[22] Yuxuan Lei, Xiaolong Chen, Defu Lian, Peiyan Zhang, Jianxun Lian, Chaozhuo\nLi, and Xing Xie. 2023. Practical Content-aware Session-based Recommendation:\nDeep Retrieve then Shallow Rank. In Amazon KDD Cup 2023 Workshop .\n[23] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.\n2019. Visualbert: A simple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557 (2019).\n[24] Yuanzhi Li, SÃ©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar,\nand Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report.\narXiv preprint arXiv:2309.05463 (2023).\n[25] Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing\nXie. 2020. Lightrec: A memory and search-efficient recommender system. In\nProceedings of The Web Conference 2020 . 695â€“705.\n[26] Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong Rui.\n2014. GeoMF: joint geographical modeling and matrix factorization for point-of-\ninterest recommendation. In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining . 831â€“840.\n[27] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model\npredictions. Advances in neural information processing systems 30 (2017).\n[28] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations\nusing distantly-labeled reviews and fine-grained aspects. In Proceedings of the\n2019 conference on empirical methods in natural language processing and the 9th\ninternational joint conference on natural language processing (EMNLP-IJCNLP) .\n188â€“197.\n[29] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[30] Apurva Pathak, Kshitiz Gupta, and Julian McAuley. 2017. Generating and person-\nalizing bundle recommendations on steam. InProceedings of the 40th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n1073â€“1076.\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning . PMLR, 8748â€“8763.\n[32] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and\nIlya Sutskever. 2023. Robust speech recognition via large-scale weak supervision.\nIn International Conference on Machine Learning . PMLR, 28492â€“28518.\n[33] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018.\nImproving language understanding by generative pre-training. (2018).\n[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO:\nmemory optimizations toward training trillion parameter models. In Proceedings\nof the International Conference for High Performance Computing, Networking,\nStorage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19,\n2020, Christine Cuicchi, Irene Qualters, and William T. Kramer (Eds.). IEEE/ACM,\n20. https://doi.org/10.1109/SC41405.2020.00024\n[36] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i\ntrust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd\nACM SIGKDD international conference on knowledge discovery and data mining .\n1135â€“1144.\n[37] Gregor PJ Schmitz, Chris Aldrich, and Francois S Gouws. 1999. ANN-DT: an\nalgorithm for extraction of decision trees from artificial neural networks. IEEE\nTransactions on Neural Networks 10, 6 (1999), 1392â€“1401.\n[38] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Impor-\ntant Features Through Propagating Activation Differences. In Proceedings of the\n34th International Conference on Machine Learning (Proceedings of Machine Learn-\ning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3145â€“3153.\nhttps://proceedings.mlr.press/v70/shrikumar17a.html\n[39] Erik Å trumbelj and Igor Kononenko. 2014. Explaining prediction models and\nindividual predictions with feature contributions. Knowledge and information\nsystems 41 (2014), 647â€“665.\n[40] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution\nfor Deep Networks. InProceedings of the 34th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and\nYee Whye Teh (Eds.). PMLR, 3319â€“3328. https://proceedings.mlr.press/v70/\nsundararajan17a.html\n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[43] Teng Wang, Wenhao Jiang, Zhichao Lu, Feng Zheng, Ran Cheng, Chengguo\nYin, and Ping Luo. 2022. Vlmixer: Unpaired vision-language pre-training via\ncross-modal cutmix. In International Conference on Machine Learning . PMLR,\n22680â€“22690.\n[44] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu,\nKriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and\nFuru Wei. 2023. Image as a Foreign Language: BEIT Pretraining for Vision\nand Vision-Language Tasks. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023 . IEEE,\n19175â€“19186. https://doi.org/10.1109/CVPR52729.2023.01838\n[45] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan,\nJianfeng Gao, and Furu Wei. 2023. Visually-Augmented Language Modeling.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023 . OpenReview.net. https://openreview.net/pdf?id=\n8IN-qLkl215\n[46] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language\nModels with Self-Generated Instructions. InProceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), ACL\n2023, Toronto, Canada, July 9-14, 2023 , Anna Rogers, Jordan L. Boyd-Graber, and\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, 13484â€“13508.\nhttps://doi.org/10.18653/v1/2023.acl-long.754\n[47] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.arXiv preprint arXiv:2206.07682\n(2022).\n[48] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng,\nChongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).\n[49] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.\n2019. Gnnexplainer: Generating explanations for graph neural networks. Ad-\nvances in neural information processing systems 32 (2019).\n[50] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. 2015.\nUnderstanding neural networks through deep visualization. arXiv preprint\narXiv:1506.06579 (2015).\n[51] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding con-\nvolutional networks. In Computer Visionâ€“ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 . Springer, 818â€“833.\n[52] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-\nRong Wen. 2023. Recommendation as Instruction Following: A Large Language\nModel Empowered Recommendation Approach. CoRR abs/2305.07001 (2023).\nhttps://doi.org/10.48550/arXiv.2305.07001 arXiv:2305.07001\n[53] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt:\nOpen pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n(2022).\n[54] Jan Ruben Zilke, Eneldo Loza MencÃ­a, and Frederik Janssen. 2016. Deepredâ€“rule\nextraction from deep neural networks. In Discovery Science: 19th International\nConference, DS 2016, Bari, Italy, October 19â€“21, 2016, Proceedings 19 . Springer,\n457â€“473.\nA Details for human and GPT-4 annotations\nA.1 Criteria for GPT-4 and Human Experts\nWe ask human experts and GPT-4 to score explanations generated\nby all the LLMs with the same criteria. The prompt is shown in\nFigure A1.\nRecExplainer: Aligning Large Language Models for Explaining Recommendation Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nPlease act as an impartial judge and evaluate the AI assistantâ€™s recommendation decision as well as decision explanation based on the userâ€™s purchase\nhistory, target item, and ground truth label. Assign a score according to the following four levels:\nRATING-0: Incorrect classification - The assistant fails to generate a correct recommendation decision.\nRATING-1: Correct classification, insufficient explanation - The assistant correctly makes the recommendation decision but provides no,\nfew, or irrelevant explanations, or provides explanations with hallucination, some of which do not conform to the actual situation.\nRATING-2: Correct classification, acceptable explanation - The assistant correctly makes the recommendation decision and provides an ex-\nplanation that is logically consistent and aligns with the userâ€™s history and target item. But the explanation still has minor imperfections such as lack of\npersuasiveness or informativeness.\nRATING-3: Correct classification, satisfying explanation - The assistant correctly makes the recommendation decision and provides a satis-\nfactory explanation, including a summary of the userâ€™s historical behavior patterns and characteristics, as well as a thorough analysis of the consistency\nor inconsistency between user preferences and the target item.\nPlease give your score in the form of <br>RATING</br>, for example, if the rating is 1, output <br>RATING-1</br>. Do not allow the\nlength of the explanation to influence your evaluation. Be as objective as possible.\nKnown information: User history: {USER HISTORY}, Target item: {ITEM}, Label: {YES/NO}. Assistantâ€™s output: {EXPLANATIONS}\nFigure A1: Prompt for the evaluation criteria.\nTable B1: Statistics of the datasets.\nDataset #Users #Items #Inters Sparsity\nGames 3,901 1,864 31,672 99.564%\nMovies 3,194 1,170 28,105 99.248%\nSteam 2,493 986 27,498 98.881%\nTable B2: Dataset details for each task.\nDataset Video Games Movies and TV Steam\nSplit # Train # Test # Train # Test # Train # Test\nTask 1 23,870 3,901 21,717 3,194 22,512 2,493\nTask 2 23,870 3,901 21,717 3,194 22,512 2,493\nTask 3 7,802 2,294 6,388 1,888 4,986 1,456\nTask 4 9,177 0 4,363 0 3,856 0\nTask 5 10,000 1,000 10,000 1,000 10,000 1,000\nTask 6 27,771 3,901 24,911 3,194 25,005 2,493\nTotal 102,490 14,997 89,096 12,470 88,871 9,935\nA.2 Human Evaluation Setup\nWe ask three experts, all of whom are master students majoring\nin recommender systems and are not involved in the co-author\nlist, to evaluate the generated results of all LLMs. These three ex-\nperts coordinate the standards of the 4-level rating system before\nstarting annotations and then each of them rates all the instances\nindependently. During the evaluation process, they are presented\nwith the target label, user history, target item, and model responses.\nModel responses are listed in random order, with all the model\ninformation anonymized, ensuring that the experts are unaware of\nthe specific LLM responsible for generating each text.\nA.3 Human and GPT-4 Evaluation Agreement\nWe have also included calculations for both inter-human agreement\nand gpt4-human agreement. When calculating inter-human agree-\nment, we conduct pairwise comparisons among the three human\nannotators and compute the average metric. For the gpt4-human\nagreement calculation, we separately compute the metrics for the\nthree gpt4-human pairs and then average them.\nWe first report Cohenâ€™sğœ…, which is commonly used to measure\ninter-rater agreement for categorical items. When calculating this,\nwe treat the 4-level rating (1-4) as a categorical variable. The ğœ…for\ninter-human and gpt4-human is 0.366 and 0.316 respectively, which\nboth show a moderate agreement.\nWe also compute the Spearman correlation coefficientğœŒbetween\nthe ratings of our two evaluators (human or gpt4) by treating the\nrating as an ordinal variable (4>3>2>1). The coefficient for inter-\nhuman and gpt4-human is 0.563 and 0.714 respectively, which both\nindicate a high correlation between the two evaluators.\nB Details about Dataset generation\nB.1 Templates for Data Generation\nTo better align the LLM to the target recommendation model, we\ndesign several training tasks for the LLM to understand the pre-\ndictive behaviors of the target model and the domain-specific data.\nFollowing [5], All tasks are formed into USER-ASSISTANT format.\nWe list all the templates used in our datasets in Figure B2.\nB.2 Statistics of the Datasets\nConsidering that each dataset has millions of interaction records, we\nreduce their sizes to avoid unacceptable training costs. Concretely,\nwe filter each dataset by first selecting items with top frequency,\nand retaining usersâ€™ interaction history on this item set. Finally, we\nrandomly select a subset of users as our dataset. Following prior\nwork [52], we also apply a 5-core filter, further removing users and\nitems with fewer than five interactions from the dataset. We show\nthe overall data statistics and statistics for each task in Table B1\nand Table B2, respectively.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yuxuan Lei et al.\nTask Name: Next Item Retrieval\nAssistant Response: \"{ITEM TITLE}\"\nUser Question:\n1. \"Given the user purchase history: {USER HISTORY} , generate the next most likely clicked item title. \"\n2. \"What is the next most likely clicked item title for the purchase history: {USER HISTORY} ?\"\n3. \"Predict the item that the user with this history: {USER HISTORY} might like next. \"\n4. \"Considering the purchasing history: {USER HISTORY} , what will be the next item the user click on?\"\n5. \"Based on the buying history {USER HISTORY} , what item is the user likely to click on next?\"\n6. \"With the given purchase records {USER HISTORY} , can you determine the next item the user will click?\"\n7. \"What item is expected to be clicked next by a user who has this purchase history: {USER HISTORY} ?\"\n8. \"Generate the next probable clicked item for a user with the purchase history: {USER HISTORY} . \"\n9. \"For a user with the following purchase background: {USER HISTORY} , which item will he most likely click next?\"\nTask Name: Item Ranking\nAssistant Response: \"{SORTED ITEM TITLES}\"\nUser Question:\n1. \"Given the user history: {USER HISTORY} and next items to be ranked: {ITEMS} , generate the sorted item titles from the userâ€™s favorite to least favorite. \"\n2. \"Considering user: {USER HISTORY} and some items he might like next: {ITEMS} , provide a ranking list of them according to the user preference. \"\n3. \"Please rank the following items: {ITEMS} from what the user likes to dislikes. Here is the user history: {USER HISTORY} . \"\n4. \"For user with purchase history: {USER HISTORY} , please arrange these items in order of preference: {ITEMS} . \"\n5. \"Taking into account userâ€™s history: {USER HISTORY} , create a list of the items: {ITEMS} ranked by the userâ€™s interests. \"\n6. \"With the userâ€™s purchase history given: {USER HISTORY} , sort the items: {ITEMS} based on the userâ€™s taste from best to worst. \"\n7. \"Based on the purchase history: {USER HISTORY} , please provide a ranking of the following items: {ITEMS} according to the userâ€™s preferences. \"\n8. \"Given userâ€™s past history: {USER HISTORY} , rank these items: {ITEMS} from most to least appealing. \"\n9. \"Using the provided user purchase history: {USER HISTORY} , generate a ranked list of items: {ITEMS} in accordance with the userâ€™s likes and dislikes. \"\nTask Name: Interest classification\nAssistant Response: \"{YES/NO}\"\nUser Question:\n1. \"The user has the following purchase history: {USER HISTORY} . Will the user like the item: {ITEM} ?\"\n2. \"Considering user: {USER HISTORY} and item: {ITEM} , will the user like the item?\"\n3. \"Here is the user history: {USER HISTORY} . Do you think the user will prefer the item: {ITEM} ?\"\n4. \"Userâ€™s purchase records are: {USER HISTORY} . Can you tell if the user will enjoy item: {ITEM} ?\"\n5. \"Given the purchase background of the user: {USER HISTORY} , would the user appreciate the item: {ITEM} ?\"\n6. \"The buyer has this purchase history: {USER HISTORY} . Would the user be interested in the product: {ITEM} ?\"\n7. \"With the following purchasing history for the user: {USER HISTORY} , can we predict if the user will like item: {ITEM} ?\"\n8. \"Hereâ€™s the customerâ€™s buying log: {USER HISTORY} . Would you say the user might favor the item: {ITEM} ?\"\nTask Name: Item discrimination\nAssistant Response: \"{TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}\"\nUser Question:\n1. \"What is the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} ?\"\n2. Given the item: {ITEM} , generate its {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}. \"\n3. For the item: {ITEM} , what is its {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}?\"\n4. \"Can you tell me the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} ?\"\n5. \"Please generate the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} . \"\n6. {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} of the item: {ITEM} ?\"\n7. \"Item: {ITEM} , what is its {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE}?\"\n8. \"Could you generate the {TITLE/DESCRIPTION/TAGS/SIMILAR ITEM TITLE/BRAND/FEATURE} for the item: {ITEM} ?\"\nTask Name: History reconstruction\nAssistant Response: \"{USER HISTORY TITLES}\"\nUser Question:\n1. What are the history titles of the user: {USER HISTORY} ?\"\n2. Given the user purchase history: {USER HISTORY} , generate the history titles. \"\n3. Generate the titles of the user history: {USER HISTORY} . \"\n4. Show me the history titles for the user: {USER HISTORY} . \"\n5. Can you list the titles in the purchase history of the user: {USER HISTORY} ?\"\n6. Please generate the titles from the userâ€™s purchase history: {USER HISTORY} . \"\n7. List the titles in the purchase history for user: {USER HISTORY} . \"\n8. What titles can be found in userâ€™s purchase history: {USER HISTORY} ?\"\nFigure B2: Prompt templates for data generation.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.767675518989563
    },
    {
      "name": "Computer science",
      "score": 0.7563985586166382
    },
    {
      "name": "Function (biology)",
      "score": 0.5080215930938721
    },
    {
      "name": "Recommender system",
      "score": 0.5019652843475342
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48500916361808777
    },
    {
      "name": "Language model",
      "score": 0.4769636392593384
    },
    {
      "name": "Space (punctuation)",
      "score": 0.4438297152519226
    },
    {
      "name": "Natural language processing",
      "score": 0.3605231046676636
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.3546043634414673
    },
    {
      "name": "Data science",
      "score": 0.3238009214401245
    },
    {
      "name": "Machine learning",
      "score": 0.3100879192352295
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}