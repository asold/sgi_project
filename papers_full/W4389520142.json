{
  "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese",
  "url": "https://openalex.org/W4389520142",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2895180110",
      "name": "Qingyi Si",
      "affiliations": [
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2089790514",
      "name": "Tong Wang",
      "affiliations": [
        "Institute of Information Engineering",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2085002608",
      "name": "Zheng Lin",
      "affiliations": [
        "Institute of Information Engineering",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2096137659",
      "name": "Xu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117145066",
      "name": "Yanan Cao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute of Information Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2101634296",
      "name": "Weiping Wang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367061104",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4366327277",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4319165821",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4366456685",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2620787630",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLM that is comparable to ChatGLM. The code and data are available at https: //github.com/PhoebusSi/Alpaca-CoT.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4086–4107\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAn Empirical Study of Instruction-tuning\nLarge Language Models in Chinese\nQingyi Si1,2∗, Tong Wang1,2∗, Zheng Lin1,2†\nXu Zhang3, Yanan Cao1,2, Weiping Wang1\n1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China\n3APUS AiLMe Lab\n{siqingyi,wangtong1997,linzheng,caoyanan,wangweiping}@iie.ac.cn,\nzhangxu@apusai.com\nAbstract\nThe success of ChatGPT validates the poten-\ntial of large language models (LLMs) in artifi-\ncial general intelligence (AGI). Subsequently,\nthe release of LLMs has sparked the open-\nsource community’s interest in instruction-\ntuning, which is deemed to accelerate Chat-\nGPT’s replication process. However, research\non instruction-tuning LLMs in Chinese, the\nworld’s most spoken language, is still in its\nearly stages. Therefore, this paper makes an\nin-depth empirical study of instruction-tuning\nLLMs in Chinese, which can serve as a cook-\nbook that provides valuable findings for effec-\ntively customizing LLMs that can better re-\nspond to Chinese instructions. Specifically,\nwe systematically explore the impact of LLM\nbases, parameter-efficient methods, instruction\ndata types, which are the three most impor-\ntant elements for instruction-tuning. Besides,\nwe also conduct experiment to study the im-\npact of other factors, e.g., chain-of-thought\ndata and human-value alignment. We hope\nthat this empirical study can make a modest\ncontribution to the open Chinese version of\nChatGPT. This paper will release a powerful\nChinese LLM that is comparable to ChatGLM.\nThe code and data are available at https:\n//github.com/PhoebusSi/Alpaca-CoT.\n1 Introduction\nThe emergence of ChatGPT gives humanity a real\nsense of hope for AGI for the first time, and inspires\nresearchers to realize the importance of LLM re-\nsearch. However, the closed source of LLMs (e.g.,\nGPT-3 (Brown et al., 2020) and PaLM (Chowdh-\nery et al., 2022)) coupled with the requirement for\nmassive computing resources to build the exclusive\nLLM has deterred researchers from reaching the\nLLM training stage. Subsequently, a series of \"API\nresearch\" based on GPT-3 and ChatGPT are con-\n∗ ∗Equal contribution.\n†Corresponding author: Zheng Lin.\nstantly emerging, which stimulate the specific ca-\npabilities of frozen LLMs (e.g., Chain-of-Thought\n(Wei et al., 2023; Wang et al., 2023; Kojima et al.,\n2023)) or guide them to complete specific tasks\n(Yang et al., 2022; Shen et al., 2023), by calling\nOpenAI interfaces and carefully designing prompts\nwithout model training.\nThe unexpected disclosure of the pre-trained\nLLaMA (Touvron et al., 2023) model changes this\nsituation, and has sparked a surge of excitement in\nthe LLM research community. This is the first open\nLLM with competitive performance. Recently, Al-\npaca (Taori et al., 2023) uses self-instruct (Ouyang\net al., 2022) and ChatGPT to generate 52K instruc-\ntions, which can enable LLaMA to respond to var-\nious human instructions like ChatGPT. This open\nproject verifies the important role of instruction-\ntuning (Wei et al., 2022; Chung et al., 2022) open\nLLMs in replicating the ChatGPT process.\nGiven the open LLM LLaMA and Alpaca’s\nhigh-quality instruction data, there is still a chal-\nlenge for researchers: even the instruction-tuning\nof the 7B model still requires high computational\nresources. To address this problem, Alpaca-LoRA\nextends the parameter-efficient method LoRA to\nLLaMA, which further reduces the computing\ncost of instruction-tuning. It further sparks ex-\ntensive research in the open-source community\non instruction-tuning for LLMs. On this basis,\nmore LLMs (e.g, Bloom (Workshop, 2023), GPT-J\n(Wang and Komatsuzaki, 2021)) are shown to have\nsignificant improvements in instruction-following\nperformance with instruction-tuning. On the other\nhand, more instruction data is constantly being pro-\nposed, e.g., Belle (Ji et al., 2023) constructs Chi-\nnese instructions in the same way, and ShareGPT\ncollects a large number of real human-ChatGPT\nconversations.\nHowever, research on instruction-tuning LLMs\nin Chinese, the world’s most spoken language, is\nstill in its early stages. LLM bases, parameter-\n4086\nbase LLMs Tokens Language Size\nLLaMA 1T Mainly in English 6.7B, 33B, 65B\nBloom 341B 46 languages 7.1B, 176B\nmoss-base 700B Chinese and English 16.1B\nChatGLM* 1T Chinese and English 6B\nsft LLMs Base Instruction data\nVicuna LLaMA 70k human-ChatGPT conversations\nBloomz & Bloomz-mt Bloom 13-crosslingual-task mixture xP3 & xP3mt\nmoss-sft moss-base moss-002-sft-data\nChatGLM ChatGLM* unknowable\nTable 1: Pre-training details of the popular open base LLMs (upper). Supervised fine-tuning (sft) details of the\nopen sft LLMs. \"*\" denotes closed source, which denotes that ChatGLM only releases the supervised fine-tuned\nversion. More details of each open LLM can be found at Appendix A.2.\nefficient methods, and instruction data are three es-\nsential elements for customizing Chinese ChatGPT-\nlike LLMs. There are no tutorials in the academic\ncommunity on them yet. Some important ques-\ntions have not yet been explored and answered:\n1) \"Which open LLM is more suitable as a foun-\ndation for Chinese instruction-tuning?\", 2) \"How\ndo parameter-efficient methods other than LoRA\naffect LLMs?\" and 3) \"What is the impact of vari-\nous types of instruction datasets?\" To answer these\nquestions, we collect a range of LLMs, parameter-\nefficient methods, and instruction datasets. Besides,\nwe consider the AGI (instruction-following) capa-\nbility and professional knowledge reserve (human\nexams) of models, and correspondingly select two\nbenchmarks Belle-eval (Ji et al., 2023) and MMCU\n(Zeng, 2023) for comprehensively evaluation.\nWe also conduct experiments to explore several\nother factors that may affect the final performance.\nSpecifically, we find that tuning with Chain-of-\nThought (CoT) data can improve the ability to\nrespond to complex reasoning questions. Differ-\nent LLMs may be suitable for different language\nprompts (excluding instruction parts) in instruction-\ntuning. Human-value alignment results into slight\nperformance drop. On the basis of the above find-\nings, this paper carefully instruction-tunes a power-\nful Chinese LLMs that is comparable to ChatGLM.\nThe contributions can be summarized as fol-\nlows: (1) We are the first to systematically study\non instruction-tuning in Chinese through adequate\nexperiments, which can serve as a cookbook that\nprovides valuable findings for customizing Chinese\nversion of ChatGPT. (2) We release a powerful\nChinese LLM that is comparable to ChatGLM.\n2 Instruction-tuning Triplets\n2.1 Preliminaries\nProblem Formulation. LLM bases m ∈ M,\nparameter-efficient methods p ∈P and instruc-\ntion datasets d ∈D are the three crucial elements\nin instruction-tuning. This section examines the\nimpact of each element in the instruction-tuning\ntriplet (m, p, d) on final performance. We traverse\nthe target element to thoroughly explore its impact,\nwhile fixing two elements in the triplet to control\nthe variable. For example, we analyze the impact of\ndifferent types of instruction datasets by comparing\nthe performance of {(m, p, di)}|D|\n1 .\nBenchmarks. We select two evaluation bench-\nmarks, Belle-eval and MMCU, to comprehensively\nevaluate LLM competencies in Chinese. Belle-eval\nis constructed by self-instruct with ChatGPT, which\nhas 1,000 diverse instructions that involve 10 cate-\ngories covering common NLP tasks (e.g., QA) and\nchallenging tasks (e.g., code and math). We use\nChatGPT to rate the model responses based on the\ngolden answers. This benchmark is considered to\nbe as the assessment of AGI (instruction-following)\ncapability. MMCU is a collection of Chinese mul-\ntiple choice questions in four professional disci-\nplines of medicine, law, psychology and education\n(e.g., Gaokao examination). It allows LLMs to\ntake exams in human society in a multiple-choice\ntest manner, making it suitable for evaluating the\nbreadth and depth of knowledge of LLMs across\nmultiple disciplines. More statistics and details are\nshown in Appendix A.1.\n2.2 Open Large Language Models\nTo answer \"Which open LLM is more suitable as\na foundation for Chinese instruction-tuning?\", we\n4087\nLLMs Code Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA Extract Avg.\nbase\nLLaMA 45.0 6.6 17.9 41.3 16.8 40.2 42.5 61.2 28.8 27.6 32.8\nBloom 51.1 15.4 41.8 56.4 26.0 53.7 63.3 74.1 42.5 58.0 48.2\nmoss-base 40.3 5.8 52.9 20.4 13.1 51.4 32.8 47.0 4.0 17.6 28.5\nsft\nVicuna 62.6 17.8 84.6 48.4 34.7 85.0 59.7 77.2 39.4 40.5 55.0\nBloomz 49.7 15.5 54.4 52.2 15.5 60.9 37.5 71.0 43.8 38.4 43.9\nBloomz-mt 46.8 15.2 58.5 49.8 15.1 59.1 45.0 72.4 40.8 33.8 43.7\nmoss-sft 63.9 25.7 78.5 33.4 15.1 78.4 46.0 58.5 22.7 22.4 44.5\nChatGLM 64.7 39.9 91.8 53.2 46.5 91.0 61.9 82.4 48.8 53.8 63.4\nOurs 72.4 41.4 91.5 64.7 36.1 92.3 62.5 85.8 45.6 38.9 63.1\nChatGPT 84.3 54.9 93.0 74.6 88.2 94.4 64.0 87.2 66.9 58.1 76.6\nTable 2: Performance of open LLMs on Belle-eval. \"Ours\" is our carefully designed instruction-tuned LLM, which\nis discussed in detail in Section 4. The best scores are bold, the second best scores are underlined. The results of\nChatGPT are only for display and will not be compared.\nLLMs Med. Psyc. Law Edu. Avg.\nLLaMA 2.66 3.75 1.14 1.95 2.38\nBloom 4.29 4.50 5.68 1.41 3.97\nmoss-base 7.70 7.70 7.98 8.47 7.96\nVicuna 10.50 10.20 8.44 13.12 10.57\nBloomz 35.15 32.30 17.29 36.87 30.40\nBloomz-mt 33.77 31.40 15.59 35.12 28.97\nmoss-sft 16.78 14.45 9.47 15.43 14.03\nChatGLM 31.04 28.65 15.86 29.96 26.38\nOurs 27.88 23.60 13.86 25.73 22.77\nChatGPT 50.90 43.50 23.98 45.72 41.03\nTable 3: Performance of open LLMs on MMCU.\ncollect and evaluate the most widely used open\nLLMs1 available in the open source community, as\nshown in Table 1.\n2.2.1 Evaluation of Existing LLMs\nPerformance on Belle-eval. Table 2 shows the\nscores of open LLMs on Belle-eval. The upper\npart shows base LLMs while the lower part shows\nthe the supervised fine-tuned (sft) LLMs. We can\nderive several observations: 1) For base LLMs,\nBloom performs the best because its ROOTS\n(Workshop, 2023) pre-training dataset has a large\nproportion in Chinese (261B), second only to En-\nglish. Although moss-base is an LLM specifically\nproposed for Chinese, its performance is poor as it\nis obtained through further pre-training based on\nthe CodeGen (Nijkamp et al., 2023) model and has\nonly seen 100B Chinese data. 2) For sft LLMs,\nChatGLM outperforms others by large margins,\nthanks to the fact that it is trained with the most\n1We mainly explore the version around 7b of them (except\nfor 16b of Moss), which strike a balance between performance\nand computation resource requirements.\nChinese tokens and HFRL. 3) The Open QA, Math,\nCloseQA and Extract categories are still very chal-\nlenging for existing open LLMs. 4) Vicuna and\nmoss-sft have clear improvements compared to\ntheir bases, LLaMA and moss-base, respectively.\nThe gain brought by Vicuna is more significant\n(22.2%) because its instruction data, collected by\nShareGPT, are the real conversations between hu-\nmans and ChatGPT, with higher quality. 5) In con-\ntrast, the performance of sft models, Bloomz and\nBloomz-mt, is reduced compared to the base model\nBloom, because they tend to generate a shorter re-\nsponse (refer to Appendix C.1). Unfortunately,\nChatGPT often scores lower for short responses.\nThe reason for this phenomenon is that Bloomz and\nBloomz-mt are fine-tuned from xP3, which is built\nfrom the NLP task collection where many tasks\nhave brief annotations.\nPerformance on MMCU. Table 3 shows the ac-\ncuracy of LLMs on MMCU, we find that: 1) All\nbase LLMs perform poorly because it is almost\ndifficult to generate content in the specified format\nbefore fine-tuning, e.g., outputting option numbers.\n2) All sft LLMs outperform their corresponding\nbase LLMs, respectively. In particular, Bloomz\nperforms the best (even beats ChatGLM) because\nit can generate option number directly as required\nwithout generating other irrelevant content (refer\nto Appendix C.2), which is also due to the data\ncharacteristics of its supervised fine-tuning dataset\nxP3. 3) Among the four disciplines, law is the most\nchallenging for LLMs.\nThe LLMs’ performance on MMCU is much\nlower than that of Belle-eval because MMCU re-\nquires higher professional knowledge. All open\n4088\nllamabloommoss-basevicunabloomzbloomz-mtmoss-sftchatglm\n+10.9\n+9.9\n+14.6\n-0.1\n+12.3+13.6+7.3\n-2.4\n43.758.143.154.956.257.351.861\nllamabloommoss-basevicunabloomzbloomz-mtmoss-sftchatglm\n+0.44+15.10+2.43+8.25\n-22.28-11.15\n+0.37+2.96\n2.82\n19.0710.3918.82\n8.1215.2614.40\n29.34\nFigure 1: Performance gains (denoted by origin bars) of\nopen LLMs on Belle-eval (Upper) and MMCU (Lower)\nfrom instruction-tuning. The instruction-tuned perfor-\nmance is denoted by blue bars and red numbers.\nLLMs still have significant room for improvement\ncompared to ChatGPT.\n2.2.2 Instruction-tuning Different LLMs\nTo determine the appropriateness of different LLMs\nas a foundation for instruction-tuning in Chinese,\nwe fine-tune all the open LLMs with the same\nparameter-efficient method LoRA and the same\ninstruction dataset Alpaca-GPT4. The results are\nshown in Figure 1 2, where we find that: 1) On\nBelle-eval, the performance improvement of sft\nLLMs brought by instruction-tuning is not as sig-\nnificant as that of base LLMs, except for sft Bloomz\nand Bloomz-mt. This is because the instructions\nof xP3 used for their supervised fine-tuning are\nnot diverse enough. 2) Vicuna and ChatGLM\nencounter performance drops after instruction-\ntuning, because Vicuna is trained from real human-\nChatGPT conversations, with better quality than\nAlpaca-GPT4. ChatGLM adopts HFRL (Ouyang\net al., 2022), which may be no longer suitable for\nfurther instruction-tuning. 3) On MMCU, most\nLLMs achieve performance boosts after instruction-\ntuning, with the exception of Bloomz and Bloomz-\nmt, which have unexpectedly significantly de-\ncreased performance. This is because that original\nBloomz and Bloomz-mt excel in multiple choice\nquestions, but after further instruction-tuning, they\nsuffer catastrophic forgetting.\n2Full results are shown in Table 10 and 11 in App. B.1.\nLLMs Param. Layer Belle MMCU\nAdaLoRA 5.6M each 51.9 14.28\nLoRA 15M each 58.1 19.07\nprompt 0.08M embed 46.8 4.49\np-tuning 1.1M embed 46.0 15.50\nprefix 30.8M each 51.6 16.18\nSadapterP 7.5M each 56.9 17.24\nSadapterH 15M each 58.7 20.23\nP-adapter 15M each 55.7 15.47\nSadapterP-l 60M each 55.0 16.10\nSadapterH-l 120M each 54.7 18.60\nP-adapter-l 120M each 56.3 19.40\nTable 4: Comparison of parameter-efficient meth-\nods. \"Param.\" denotes the trainable parameter quantity.\n\"Layer\" denotes the layers adapters are added. \"-l\" de-\nnotes the version with large number of parameter. More\ndetails of each parameter-efficient methods can be found\nin Appendix A.3.\nsteps\nloss\nAdaLoRALoRAP-tuningPromptPrefixSadapertPSadapterHP-adapter\nFigure 2: Training loss over steps for different\nparameter-efficient methods.\nAfter instruction-tuning, Bloom has signifi-\ncant improvements and performs well on both\nbenchmarks. Although ChatGLM beats Bloom\nconsistently, it suffers performance drop during\ninstruction-tuning. Therefore, among all open\nLLMs, Bloom is most suitable as a foundation\nmodel in the subsequent experiments for Chinese\ninstruction-tuning exploration.\n2.3 Parameter-efficient Methods\nFor most researchers, parameter-efficient methods\nare essential for instruction-tuning due to limita-\ntions in computing resources. These methods tend\nto freeze the pre-trained model weights and in-\njects trainable weights (adapters), which greatly\nreduces the number of trainable parameters. To\nanswer \"How do parameter-efficient methods other\nthan LoRA affect LLMs? \", we collect a range\nof parameter-efficient methods to instruction-tune\n4089\nDatasets Num Con Type Source\nAplaca-GPT4 49K SI diverse instructions GPT-4\nBelle 1.54M SI diverse instructions text-davinci-003\nShareGPT-zh 158K MIX human-ChatGPT conversations human & ChatGPT\nmoss-sft-data 1.76M SI diverse instructions text-davinci-003\ninstinwild 52K SI diverse instructions text-davinci-003\nfirefly 1.65M COL 23 NLP tasks (1.15M) & Belle (0.5M) human\nHC3 40K MIX QA dataset collection human & ChatGPT\nxP3/zh 1.07M COL 16 NLP tasks human\nCOIG-ccmc 68K MIX LLM-LLM role-playing chats based on a knowledge graph dataset\nCOIG-trans 66K COL 2000+ NLP tasks translated\nCOIG-exam 64K COL examinations in China human\npCLUE 1.2M COL 9 NLP tasks human\nTable 5: The details of existing Chinese instruction datasets. \"Con\" column shows the dataset construction methods.\n\"Type\" column shows the data types. \"Source\" column shows the source where the data was generated. \"SI\" and\n\"COL\" denotes the self-instruct methods and the collection of existing datasets, respectively. \"MIX\" denotes the\njoint construction of humans and machines. \"translated\" denotes a translation from non-Chinese instructions. We\nfiltered all datasets to remove incomplete instructions. More details of each dataset can be found in Appendix A.4.\nBloom on Alpaca-GPT4 dataset.\nComparison of Parameter-efficient Methods.\nFrom Table 4, several observations can be de-\nrived: 1) SadapterH performs the best among all\nparameter-efficient methods, which can be used as\nan alternative to LoRA. 2) P-tuning and prompt-\ntuning underperform others by large margins, in-\ndicating that only adding trainable layers in the\nembedding layer are not enough to support LLMs\nfor generation tasks. 3) Although AdaLoRA is\nan improvement of LoRA, its performance has a\nclear drop, possibly because the LoRA’s trainable\nparameters for LLMs are not suitable for further re-\nduction. 4) Comparing the upper and lower parts, it\ncan be seen that increasing the number of trainable\nparameters for sequential adapters (i.e., SadapterP\nand SadapterH) does not bring gain, while the oppo-\nsite phenomenon is observed for parallel adapters\n(i.e., P-adapter). This may provide inspiration for\nthe design of adapters for LLM. Since LoRA is cur-\nrently the most popular parameter-efficient method,\nif not otherwise specified, we adopt LoRA by de-\nfault in the experiments.\nTraining Loss. Figure 2 shows the training loss\nof different parameter-efficient methods. We find\nthat: 1) Prompt-tuning and P-tuning converge the\nslowest and has the highest losses after conver-\ngence. This shows that embedding-only adapters\nare not suitable for instruction-tuning LLMs. 2)\nThe initial loss of AdaLoRA is very high because\nit requires simultaneous learning of parameter bud-\nget allocation, which makes the model unable to fit\nthe training data well. 3) The other methods can\nquickly converge on training data and fit it well.\n2.4 Chinese Instructions Datasets\nAlpaca (Taori et al., 2023) inspires researchers to\nfurther explore instruction data. To systematically\nexplore \"What is the impact of various types of\ninstruction datasets?\", we gather popular open Chi-\nnese instructions (as shown in Table 5) to fine-tune\nBloom with LoRA.\nPerformance on Belle-eval. As shown in Ta-\nble 6 upper part, it can be seen that: 1) the in-\nstruction data constructed by ChatGPT (e.g., using\nself-instruction methods or collecting real human-\nChatGPT conversations) consistently enhances the\ninstruction-following ability with 3.1 ∼11-point\nscore increases. 2) Among these datasets, Belle\nhas the best performance due to the largest amount\nof instruction data. However, the performance of\nmodels trained on moss-sft-data, containing more\ndata built in a similar way, is unsatisfactory. This\nis because moss-sft-data’s instructions sacrifice the\ndiversity to achieve the goals of helpfulness, honey,\nand harmlessness. 3) The performance brought by\nthe Alpaca-GPT4 instructions is the second best,\nwith only 49K being comparable to the 1.54M\nBelle. This is because Alpaca-GPT4 uses the GPT-\n4 engine while Belle uses the text-avinci-003 en-\ngine, which further illustrates that improving data\nquality can reduce the demand for data volumes.\n4) Instinwild brings the least performance gains\namong them because the seed instructions it crawls\nfrom Tweet (\"in wild\") are not as comprehensive\nas those (like Alpaca) carefully designed by hu-\n4090\nDatasets Code Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA Extract Avg.\n– 51.1 15.4 41.8 56.4 26.0 53.7 63.3 74.1 42.5 58.0 48.2\nAlpaca-GPT4 53.7 35.0 88.2 50.5 36.8 89.5 60.3 82.9 40.0 44.1 58.1\nBelle 64.3 37.3 86.0 66.2 22.3 88.6 62.6 83.5 43.3 38.1 59.2\nShareGPT-zh 53.7 25.4 75.6 47.8 34.0 81.2 62.8 80.8 33.5 41.6 53.6\nmoss-sft-data 55.4 21.7 81.6 51.7 24.3 81.8 65.4 78.1 37.6 45.1 54.3\ninstinwild 55.5 24.5 70.3 40.8 26.9 79.6 61.8 78.4 37.7 37.0 51.3\nfirefly 61.4 31.8 79.8 53.2 26.5 84.4 62.5 83.1 36.7 47.8 56.7\nHC3 46.8 21.3 58.2 24.6 33.9 44.4 30.3 47.8 36.5 19.7 36.4\nxP3/zh 32.1 13.3 17.0 39.4 12.1 24.1 21.5 66.5 40.0 28.1 29.4\nCOIG-trans 34.7 15.5 58.9 47.9 24.4 62.1 51.5 80.0 37.7 44.6 45.7\nCOIG-ccmc 39.2 15.0 44.0 16.5 21.2 22.3 23.3 31.6 19.8 19.3 25.2\nCOIG-exam 24.2 15.4 53.7 33.3 17.9 60.2 44.0 69.6 27.1 24.9 37.0\npCLUE 19.7 19.1 53.5 34.3 36.9 50.5 34.0 68.5 44.4 39.3 40.0\nTable 6: Belle-eval performance of models instruction-tuned from Bloom on different instruction datasets.\nDatasets Med. Psyc. Law Edu. Avg.\n– 4.29 4.50 5.68 1.41 3.97\nAlpaca-GPT427.70 17.35 17.59 13.63 19.07\nBelle 21.57 19.05 15.13 15.28 17.76\nShareGPT-zh 8.83 8.75 12.18 12.04 10.45\nmoss-sft-data 16.60 17.75 11.72 17.29 15.84\ninstinwild 14.90 17.45 11.50 14.17 14.51\nfirefly 22.49 18.30 9.69 20.50 17.75\nHC3 9.15 14.10 8.01 7.24 9.63\nxP3/zh 20.43 19.50 15.62 19.60 18.79\nCOIG-trans 18.62 17.9 11.31 16.51 16.09\nCOIG-ccmc 7.31 10.15 8.12 7.63 8.30\nCOIG-exam 32.56 26.90 16.18 - -\npCLUE 20.29 25.40 13.91 27.80 21.85\nTable 7: MMCU performance of models instruction-\ntuned from Bloom on different instruction datasets.\nmans. 5) These ChatGPT-based data mainly have a\nsignificant improvement effect on open generation\ntasks such as Brain Storm and Generation, while\nthere is a significant decrease in tasks that require\nhigh reading comprehension skills, such as Close\nQA and Extract, which require completing tasks\nbased on given materials. This inspires researchers\nto consider the reading-comprehension ability for\nbuilding more comprehensive instruction datasets.\nThe lower part of Table 6 shows the results of\nmodels trained on dataset-based data, which is\nmainly constructed by collecting NLP or exami-\nnation datasets. These instruction datasets cause\ndamage to the model’s instruction-following abil-\nity, because the form and intent of each NLP or\nexamination dataset are unitary, which can easily\nbe overfitted. Among them, COIG-trans performs\nthe best because it involves over 2000 different\ntasks with a wide variety of task instructions. In\ncontrast, xP33 and COIG-ccmc have the worst neg-\native impact on model performance. Both of them\nonly cover few types of tasks (translation and QA\nfor the former, counterfactual correction conversa-\ntions for the latter), which hardly cover the popular\ninstructions and tasks for humans.\nPerformance on MMCU. Table 7 compares the\nperformance on MMCU brought by different in-\nstruction datasets. 1) Instruction-tuning on each\ndataset can always result in performance improve-\nment. 2) Among the ChatGPT-based data shown in\nthe upper part, ShareGPT-zh underperforms oth-\ners by large margins. This may be due to the\nfact that real users rarely ask multiple choice ques-\ntions about academic topics. 3) Among the dataset-\ncollection data shown in the lower part, HC3 and\nCOIG-ccmc results in the lowest accuracy because\nthat the unique questions of HC3 is only 13K, and\nthe task format of COIG-ccmc is significantly dif-\nferent with MMCU. 4) COIG-exam 4 brings the\ngreatest accuracy improvement, benefiting from\nthe similar task format as MMCU.\n3 Other Important Factors\nProblem Formulation. In addition to the essen-\ntial three elements (m, p, d) discussed above, there\nare many factors worth exploring, e.g., CoT. If not\notherwise specified, we use Bloom as the LLM\nbase, LoRA as the parameter-efficient method, and\nAlpaca-GPT4 as the instruction data. On this basis,\n3We use 1/3 of its Chinese data due to its large quantity.\n4Due to the overlap between COIG-exam and MMCU-\nEdu., the accuracy on Edu. discipline will not be reported.\n4091\nBelle-eval MMCU\nData Code Math Avg. Edu. Avg.\nAlpaca-GPT4 53.7 36.8 58.1 13.63 19.07\nAlpaca-GPT4+CoT60.8 41.7 57.9 21.56 19.85\nAlpaca-GPT4+CoT*62.9 39.5 57.2 22.07 21.56\nTable 8: The impact of chain-of-thought data on com-\nplex tasks requiring reasoning. \"*\" denotes that using\nprompt \" 先思考，再决定 \" (\"think step by step\" in\nChinese) during inference.\nwe explore its impact by observing the performance\nchanges after considering the target factor.\nChain-of-Thought Data. Chain-of-Thought is a\nhot topic in LLM research. Existing works find that\nadding rationales or explanations to the inference\nprompts (Wei et al., 2023; Wang et al., 2023; Ko-\njima et al., 2023) (based on APIs of GPT-3 and\nChatGPT) or training corpus (Wei et al., 2022;\nChung et al., 2022; Zhang et al., 2023c) (based\non normal language models, e.g, T5(Raffel et al.,\n2020) and FLAN-T5(Wei et al., 2022)) can enhance\nthe model’s reasoning ability, which is useful for\nsolving complex problems. However, extending\nCoT into Open LLM has not yet been thoroughly\nexplored. Alpaca-CoT (Qingyi Si, 2023) uses sev-\neral qualitative examples to demonstrate the effec-\ntiveness of CoT in reasoning. A systematic eval-\nuation is still necessary. To this end, this paper\nconducts experiments to analyze the impact of CoT\ndata for LLMs.\nWe collect 9 CoT datasets and their prompts\nfrom FLAN (Wei et al., 2022), and then translates\nthem into Chinese using Google Translate. We\ncompare the performance before and after adding\nCoT data during instruction-tuning in Table 8.\n\"Alpaca-GPT4+CoT\" outperforms \"AlpacaGPT4\"\nin the Code and Math tasks that require strong rea-\nsoning ability. Besides, there is also a significant\nimprovement in MMCU education task, which is\nderived from the questions of Gaokao, involving\na range of subjects, e.g., math, physics, history.\nThe accuracy improvement across all subjects il-\nlustrates that the CoT reasoning ability is gener-\nally required in various subjects. However, CoT\ntraining data cannot continue to bring benefits to\nall tasks, and on the contrary, it will cause slight\nperformance degradation on more tasks. The full\nresults can be found in Appendix B.3.\nInspired by (Kojima et al., 2023), we add a sen-\ntence\" 先思考，再决定 \" (\"think step by step\" in\nChinese) at the end of each instruction, to induce\nBelle-eval MMCU\nllamallama-voc-prellama-voc-pre-lllama-voc43.7\n9.9\n51.255.6\n2.824.17\n21.7218.92\nFigure 3: Performance comparison of LLaMA and its\nexpanded vocabulary versions. \"llama-voc\", \"llama-voc-\npre\" and \"llama-voc-pre-l\" denotes instruction-tuning\nthe models obtained by further pre-training LLaMA\nwith a expanded vocabulary on 0B, 20B and 100B Chi-\nnese tokens, respectively.\nthe model to respond to instructions based on the\nchain-of-thought. As shown in the line of \"Alpaca-\nGPT4+CoT*\", the simple sentence can further im-\nprove the performance of reasoning tasks Code and\nEducation, while the Math performance is slightly\ninferior to \"Alpaca-GPT4+CoT\". This may require\nus to further explore more robust prompts.\nExpansion of Chinese Vocabulary. Intuitively,\nthe number of Chinese tokens in the tokenizer’s\nvocabulary affects LLMs’ ability to express Chi-\nnese. For example, if a Chinese character is in\nthe vocabulary, it can be represented by a single\ntoken, otherwise it may require multiple tokens to\nrepresent it. Because Bloom adopts a vocabulary\nof 250k tokens which cover most Chinese charac-\nters, we mainly conduct experiments on LLaMA,\nwhich uses SentencePiece (Sennrich et al., 2016;\nKudo and Richardson, 2018) (32K vocabulary size)\ncovering few Chinese characters.\nAs shown in Figure 3, we find that the perfor-\nmance of \"llama-voc\" is severely inferior to \"llama\"\non Belle-eval, and is almost unable to respond cor-\nrectly to MMCU’s instruction. This indicates that\nit is not feasible to perform instruction-tuning with-\nout pre-training on vast data. This is because the\nembedding corresponding to the newly added Chi-\nnese token are random and meaningless, which\nresults in the model being unable to understand the\nmeaning of the instructions.\nTo make the newly added Chinese token mean-\ningful, Cui et al. uses 20B and 100B token Chi-\nnese corpus to further pre-train LLaMA and obtain\n\"llama-voc-pre\" and \"llama-voc-pre-l\" models. We\nuse Alpaca-GPT4 to instruction-tune these models,\nand find that, pre-training on more Chinese cor-\n4092\npromptinChinesepromptinEnglish\nBelle-eval MMCU(a) LLaMA\npromptinChinesepromptinEnglish\nBelle-eval MMCU(b) Bloom\nFigure 4: Performance comparison of instruction-tuning with prompts in\nEnglish and Chinese. The specific prompts used in our experiments can\nbe found in Appendix D.2.\nw/ human-valuew/o human-value\nBelle-evalMMCU\nFigure 5: Performance comparison\nof instruction-tuning with and without\nhuman-value alignment.\npus with expansion of Chinese vocabulary are con-\nsistently helpful for instruction-following ability.\nCounterintuitively, \"llama-voc-pre-l\" is inferior to\n\"llama-voc-pre\" on MMCU shows that pre-training\non more data may not necessarily lead to higher\nperformance for academic exams.\nThe Languages of Prompts. The popular open\ninstruction-tuned LLMs, e.g., Alpaca and Vicuna,\ntend to uses prompts in English. One intuitive ques-\ntion is, Is instruction-tuning in Chinese more suit-\nable for using Chinese prompts? Figure 4 shows\nthe results of using Chinese and English prompts\nbased on LLaMA and Bloom. When instruction-\ntuning LLaMA, using Chinese prompts can im-\nprove the performance on both benchmarks com-\npared to English prompts, while we observe the\nopposite phenomenon on Bloom. This demon-\nstrates that using Chinese prompts for models with\nweaker Chinese abilities (e.g., LLaMA) can effec-\ntively help respond in Chinese, while for models\nwith good Chinese abilities (e.g., Bloom), using\nprompts in English (the language they are better\nat) can better guide the model to understand the\nprocess of fine-tuning with instructions.\nHuman Value Alignment. To avoid LLMs gen-\nerating toxic content, aligning them with human\nvalues is a crucial issue. We add the human-value\nalignment data built by COIG (see App. A.4 for\ndetails) into the instruction-tuning to explore its im-\npact. Figure 5 compares the results of instruction-\ntuning with and without human-value alignment,\nwhich shows that the human-value alignment re-\nsults into a slight performance drop. How to bal-\nance the harmlessness and performance of LLMs is\na research direction worth exploring in the future.\n4 Towards a Better Chinese LLM\nProblem Formulation. The goal of this section\nis to find a optimal triplet (m, p, d) that maximizes\nthe comprehensive capabilities:\nmax\nm,p,d\n∑\nt∈T\n(Et (fd (m, p))) (1)\nwhere Et denotes the evaluation of every gener-\native ability t from both Belle-eval and MMCU\nT, fd(m, p) denotes the model obtained by\ninstruction-tuning frozen LLM m with parameter-\nefficient method p on instruction dataset d.\nOur Instruction-tuned LLM. On the basis\nof the findings above, we carefully design the\ninstruction-tuning process and publicly release a\nBloom-based high-performance LLM, which is\ncomparable to ChatGLM and far surpassing Moss.\nIn particular, we select a dataset combination with\nsignificant gains on Belle-eval or MMCU to im-\nprove our model’s comprehensive ability. Besides,\nwe carefully design a suitable prompt to induce our\nmodel for better-quality generation. The implemen-\ntation details can be found in Appendix D.1.\nAs shown in Table 2, our model is superior or\ncomparable to ChatGLM in most categories on\nBelle-eval, except for the challenging Math and\nExtract tasks. Besides, our model slightly under-\nperforms ChatGLM on MMCU and outperforms\nother LLMs that do well in Belle-eval by clear mar-\ngins. It is worth emphasizing that our model has\nmuch fewer trainable parameters (16M) based on\nLoRA than that of ChatGLM adopting full parame-\nter fine-tuning (6B).\n5 Conclusion\nThis paper is the first to conduct a thorough empiri-\ncal study on instruction-tuning open large language\nmodels in Chinese, with a detail discussion of a\n4093\nrange of large language models, parameter-efficient\nmethods, and Chinese instruction datasets. In ad-\ndition, we explore several other important factors,\nincluding CoT, vocabulary, language of prompts\nand human-value alignment. Based on the empir-\nical exploration, we publicly release a LLM, that\nis rival to ChatGLM, with detailed implementation\ndetails.\nLimitations\nMost experimental results are based on parameter-\nefficient methods, which may differ from the re-\nsults of full parameter fine-tuning. However, we\nbelieve that the findings and conclusions in this\npaper are still applicable for full parameter fine-\ntuning. In addition, instruction-tuning based on\nparameter-efficient methods has broader applica-\ntion and research scenarios.\nEthics Statement\nThe open LLMs used in this paper may be driven by\ncertain biases in their training data, and pose a risk\nof toxic generation. There may also exist harmful\nstereotypes in the open instruction datasets we are\ndiscussing. There is still a long way to explore the\nsafety of LLMs.\nAcknowledgement\nThis work was supported by National Natural Sci-\nence Foundation of China (No. 61976207) and\nNational Social Science Foundation of China (No.\n21AZD145).\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nYiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient\nand effective text encoding for chinese llama and\nalpaca.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. How close is chatgpt to human experts? compar-\nison corpus, evaluation, and detection. arXiv preprint\narxiv:2301.07597.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\narXiv preprint arXiv:2110.04366.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\n4094\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nEdward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. 2021.\nLora: Low-rank adaptation of large language models.\nYunjie Ji, Yan Gong Yong Deng, Qiang Niu Yip-\ning Peng, Baochang Ma, and Xiangang Li. 2023.\nBelle: Be everyone’s large language model engine.\nhttps://github.com/LianjiaTech/BELLE.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\nZhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-\ntuning v2: Prompt tuning can be comparable to fine-\ntuning universally across scales and tasks.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-\nham Fikri Aji, Khalid Almubarak, Samuel Albanie,\nZaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. 2023. Crosslingual generalization\nthrough multitask finetuning.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer. arXiv\npreprint arXiv:2005.00052.\nTong Wang Qingyi Si. 2023. Alpaca-cot: An\ninstruction-tuning platform with unified interface of\ninstruction collection, parameter-efficient methods,\nand large language models. https://github.com/\nPhoebusSi/alpaca-CoT.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Ku-\nmar Pal, Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\nPulkit Verma, Ravsehaj Singh Puri, Rushang Karia,\nShailaja Keyur Sampat, Savan Doshi, Siddhartha\n4095\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\nXudong Shen, Chitta Baral, Yejin Choi, Noah A.\nSmith, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. Super-naturalinstructions: Generalization via\ndeclarative instructions on 1600+ nlp tasks.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nBigScience Workshop. 2023. Bloom: A 176b-\nparameter open-access multilingual language model.\nBo Xu, Yong Xu, Jiaqing Liang, Chenhao Xie, Bin\nLiang, Wanyun Cui, and Yanghua Xiao. 2017. Cn-\ndbpedia: A never-ending chinese knowledge extrac-\ntion system. In Advances in Artificial Intelligence:\nFrom Theory to Practice , pages 428–438, Cham.\nSpringer International Publishing.\nFuzhao Xue, Kabir Jain, Mahir Hitesh Shah, Zangwei\nZheng, and Yang You. 2023. Instruction in the wild:\nA user-based instruction dataset. https://github.\ncom/XueFuzhao/InstructionWild.\nJianxin Yang. 2023. Firefly. https://github.com/\nyangjianxin1/Firefly.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nHui Zeng. 2023. Measuring massive multitask chinese\nunderstanding.\nGe Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi\nLi, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang,\nChenghua Lin, Wenhao Huang, and Jie Fu. 2023a.\nChinese open instruction generalist: A preliminary\nrelease.\nQingru Zhang, Minshuo Chen, Alexander Bukharin,\nPengcheng He, Yu Cheng, Weizhu Chen, and\nTuo Zhao. 2023b. Adaptive budget allocation for\nparameter-efficient fine-tuning. In The Eleventh In-\nternational Conference on Learning Representations.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023c. Multi-\nmodal chain-of-thought reasoning in language mod-\nels.\nBelle-eval MMCU\nCode Open QA Medicine\n38 285 2819\nBrainstorm ClassificationPsychology\n179 65 2000\nMath Generation Law\n75 98 3695\nSummary Rewrite Education\n40 131 3331\nClose QA Extract -\n52 37 -\nTotal 1000 11845\nTable 9: Data statistics of Belle-eval and MMCU.\nA More Details about the Work Involved\nA.1 Benchmarks\nBelle-eval During this evaluation, ChatGPT is\nused to rate (from 0 to 1) the model response based\non the ground truth answer. A score of 0 indicates\nthat the model response is completely unacceptable,\nwhile a score of 1 indicates that the response per-\nfectly solves the input instruction. The prompts and\ninstructions for samples in each category are rich\nand varied. We consider the capability examined\nin this dataset to be AGI (instrcution-following)\ncapability.\nMMCU MMCU (Zeng, 2023) is collected from\nonline public resources, covering 11845 multiple\nchoice questions in four professional disciplines.\nThere are several subtasks under education and\nmedicine disciplines. The average accuracy of all\nsubtasks is considered the discipline score. Only\nwhen a generated answer and the annotated ground\ntruth option number or option content completely\nmatch, is the answer considered correct. This eval-\nuation is relatively rigid for expected outputs. We\nconsider the capability examined in this dataset as\nthe reserve of professional knowledge (to deal with\nhuman examinations).\nThese two assessments complement each other\nto some extent. Table 9 shows the data statistics of\nthese two benchmarks.\nA.2 Open Large Language Models\nA.2.1 Base LLMs\nLLaMA LLaMA (Touvron et al., 2023) is a\ndecoder-only language model based on the Trans-\nformer (Vaswani et al., 2017) architecture, and is\ntrained on more tokens (1T, 1.4T) than what is typ-\nically used (Hoffmann et al., 2022). It ranges from\n4096\n7B to 65B parameters and outperforms existing\nLLMs (e.g., GPT-3 (Brown et al., 2020) and PaLM\n(Chowdhery et al., 2022)) with fewer parameter\nmagnitudes. However, the vocabulary of its tok-\nenizer contains fewer Chinese characters, which\naffects its expressive power in Chinese.\nBloom Bloom (Workshop, 2023) is a multilin-\ngual language model trained on dataset ROOTs, in-\nvolving 46 natural and 13 programming languages.\nThe proportion of Chinese corpus in pre-training\ndata is second only to English corpus. The maxi-\nmum version of Bloom has 175B parameters, while\nthe most popular version is 7B.\nMoss-moon-003-base Moss-moon-003-base\n(base model of MOSS series, moss-base for\nshort) is initialized with CodeGen (Nijkamp et al.,\n2023) and further self-supervised pre-trained on\nhigh-quality Chinese (100B) and English (20B)\ncorpus. All the pre-training data contains about\n700B words. It has 16B parameters.\nA.2.2 Supervised Fine-tuned LLMs\nVicuna Vicuna (Chiang et al., 2023) is fine-tuned\nfrom LLaMA on 70K user-shared ChatGPT con-\nversations gathered by ShareGPT5. Vicuna claims\nto have achieved 90% performance of ChatGPT on\na preliminary evaluation using GPT-4 as a judge,\nmaking it the most popular open source LLM. How-\never, further rigorous evaluation is needed, espe-\ncially in Chinese scenarios.\nBloomz & Bloomz-mt Bloomz and Bloomz-mt\nare fine-tuned from Bloom on crosslingual task\nmixture xP3 (Muennighoff et al., 2023) and xP3mt,\nwhich contain 13 training tasks in 46 language\nwith prompts in English and in 20 languages, re-\nspectively. This supervised fine-tune process aims\nto further boosts the performance of multilingual\ntasks.\nMoss-moon-003-sft Moss-moon-003-sft (moss-\nsft for short) is fine-tuned from moss-moon-003-\nbase on moss-002-sft-data, which contains 0.57M\nEnglish and 0.59M Chinese dialogues generated by\ntext-davinci-003, and 0.1M real user instructions\n(the corresponding response are generated by gpt-\n3.5-turbo) collected during internal test.\nChatGLM ChatGLM-6B (Zeng et al., 2022) is\nan open bilingual LLM, supporting both Chinese\nand English. It first completes pre-training on about\n5https://sharegpt.com/\n1T tokens in Chinese and English, and then adds\nsupervised fine-tuning and human feedback rein-\nforcement learning (HFRL) (Ouyang et al., 2022)\nprocesses to force model to follow instructions.\nA.3 Parameter-efficient Methods\nLoRA Low-Rank Adaptation (LoRA) (Hu et al.,\n2021) injects trainable rank decomposition matri-\nces into each attention layer of the Transformer\narchitecture.\nAdaLoRA AdaLoRA (Zhang et al., 2023b) al-\nlocates the parameter budget adaptively to each\nlayer’s LoRA module according to their importance\nscore. Specifically, AdaLoRA parameterizes the\nincremental updates in the form of singular value\ndecomposition, which allows it to prune the singu-\nlar values of unimportant updates and reduce their\nparameter budget.\nPrefix-tuning Inspired by discrete prompts for\nlanguage models, prefix-tuning (Li and Liang,\n2021) adds a sequence of continuous \"virtual to-\nkens\" as a soft prompt (namely prefix) before the\noriginal sequences of each transformer layer. Dur-\ning training, prefix weights are trainable while\nother model parameters are frozen.\nP-tuning Unlike prefix-tuning, p-tuning (Liu\net al., 2022) injects trainable continuous tokens\ninto only the embedding layer instead of each layer,\nresulting in fewer parameters being updated. Dur-\ning training, it freezes partial model parameters.\nPrompt-tuning Similar to p-tuning, prompt-\ntuning (Lester et al., 2021) also involves only train-\ning the input prompt embeddings. Differently, it\nfreezes all pre-trained weights.\nSequential Adapter For each transformer layer,\nSeries Adapter methods add adapter layers af-\nter both attention layers and MLP layers (i.e., S-\nadapterH (Houlsby et al., 2019)), or after MLP\nlayers only (i.e., S-adapterP (Pfeiffer et al., 2020)).\nParallel Adapter Parallel Adapter, namely P-\nadapter (He et al., 2021) adds adapter layers in\nparallel with attention layers or MLP layers for\neach transformer layer.\nA.4 Chinese Instruction Datasets\nAlpacaGPT4 AlpacaGPT4 (Peng et al., 2023) is\ndeemed as an optimized version of Alpaca (Taori\net al., 2023) dataset. It uses ChatGPT to translate\n4097\nAlpaca’s prompts into Chinese first, and then re-\ngenerate these instruction-following data by GPT-4,\ninstead of text-davinci-003.\nBelle Belle (Ji et al., 2023) uses the same method\nas Alpaca (Taori et al., 2023) to generate instruction\ndata by text-davinci-003, except that Belle only\ngenerates Chinese instruction-following data and\nartificially filters low-quality data. It contains about\n1.5M instruction-following data.\nMoss-002-sft-data This is a multi-turn conver-\nsation dataset covering helpfulness, honesty, and\nharmlessness, which is also generated by self-\ninstruct (Ouyang et al., 2022). We select the 0.59M\nChinese conversations among them for the follow-\ning experiments.\nfirefly Firefly (Yang, 2023) collects 23 Chinese\ndatasets and manually writes several instruction\ntemplates for each dataset. It contains a total of\n1.65M training samples, covering couplet, poem,\nessay, and other generation tasks for the perfor-\nmance of traditional literature, and 0.5M Belle data\nfor instruction diversity.\nxP3 xP3 (Muennighoff et al., 2023) is a collec-\ntion of 16 natural language process datasets across\n46 languages with prompts. We select the 3M Chi-\nnese instances among them.\ninstinwild Instruction in the Wild (instinwild)\n(Xue et al., 2023) no longer manually sets initial\nseed instructions like Alpaca, but crawls and fil-\nters 429 instructions from Twitter as the seed in-\nstructions, to avoid human involvement and cover\nmore topics. Following self-instruct, it uses the\nseed instructions to generate more instructions and\ncorresponding responses by text-davinci-003. The\nChinese instructions in this dataset are about 52K.\nHC3 HC3 (Guo et al.) is a corpus of Human-\nChatGPT comparisons that aims to investigate how\nclose ChatGPT is to Human Experts. To this end,\nit collect about questions from various public ques-\ntion answering datasets (e.g., medicine, law, fi-\nnance QA) and the corresponding human answers\nand ChatGPT answers. The Chinese samples in\nHC3 contain 13K questions, 22K human answers,\nand 17K Chatgpt answers.\nCOIG COIG (Zhang et al., 2023a) is a Chi-\nnese instruction collection, consisting of: Trans-\nlated instructions contains about 67K instructions\nwhich are translated from three datasets: 1.6K task\ndescriptions in Super-NaturalInstructions (Wang\net al., 2022) along with a single instance for each\nof them, 175 instructions of the seed tasks in Self-\nInstruct, and 66K instructions from Unnatural In-\nstructions (Honovich et al., 2022). CCMC, namely\nCounterfactual Correction Multi-round Chat, con-\ntains about 68K rounds of conversations between\nstudents and teachers. This dataset is built by\nprompting two LLMs to generate conversations\nbased on the entities of knowledge graph dataset\nCN-DBpedia (Xu et al., 2017) to alleviate the hal-\nlucination and factual inconsistency. Exam In-\nstructions contains 63K questions from the main\nChinese commonsense tests, e.g., Gaokao, Civil\nServant Examination. These questions cover six\nmain subjects: Chinese, English, Politics, Biol-\nogy, History, and Geology. Exam Instructions\ncontains 34K Chinese samples that present shared\nhuman values in the Chinese-speaking world (3K)\nand regional-culture human values. Table 29 shows\nsome examples in this dataset.\npCLUE pCLUE collects 9 Chinese tasks with\na total of 73 different prompts and 1.2M samples.\nThese tasks include 9 Chinese tasks e.g., news clas-\nsification, natural language reasoning, semantic\nmatching, keyword recognition, reading compre-\nhension, etc.\nTable 25 and Table 26 show representative ex-\namples of the above datasets.\nB More Experimental Results\nB.1 Full results of different LLMs after\ninstruction-tuning.\nTable 10 and 11 show the full Belle-eval and\nMMCU results after instruction-tuning with LoRA\non Alpaca-GPT4 dataset, respectively.\nB.2 Full results of different\nparameter-efficient methods.\nTable 12 and Table 13 shows the full Belle-eval and\nMMCU results of instruction-tuning with different\nparameter-efficient methods, respectively.\nB.3 Full results of instruction-tuning with\nCoT data.\nTable 14 and 15 show the full Belle-eval and\nMMCU results of the models instruction-tuned\nwithout or with CoT data, respectively. Table 16\nshows the detailed results on all subjects in educa-\ntion discipline of MMCU.\n4098\nCode Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA ExtractAvg.\nLLaMA 53.7 11.9 66.7 36.3 27.3 65.4 49.5 62.7 24.8 38.6 43.7\nBloom 53.7 35.0 88.2 50.5 36.8 89.5 60.3 82.9 40.0 44.1 58.1\nmoss-base 58.8 25.1 82.9 33.4 27.9 83.8 35.2 49.9 15.2 19.2 43.1\nVicuna 64.2 19.7 80.3 50.0 35.2 79.6 58.5 81.1 42.3 38.4 54.9\nBloomz 58.4 33.8 88.4 50.5 32.9 92.6 56.0 82.3 38.3 29.1 56.2\nBloomz-mt62.9 35.1 85.6 49.6 31.7 90.5 55.1 82.4 44.2 36.2 57.3\nmoss-sft 67.4 30.7 88.5 46.5 33.6 88.6 47.5 72.1 20.2 22.7 51.8\nChatGLM 61.7 34.0 85.4 50.8 50.4 89.1 64.3 81.8 46.9 45.7 61.0\nTable 10: Performance of open LLMs on Belle-eval after instruction-tuning with LoRA on Alpaca-GPT4 dataset.\nMed. Psyc. Law Edu. Avg.\nLLaMA 3.26 3.20 1.49 3.33 2.82\nBloom 27.70 17.35 17.59 13.63 19.07\nmoss-base 12.49 11.35 7.01 10.72 10.39\nVicuna 18.98 19.75 14.70 21.83 18.82\nBloomz 7.56 6.65 9.61 8.65 8.12\nBloomz-mt 15.61 18.65 11.58 15.19 15.26\nmoss-sft 16.35 14.55 9.42 17.29 14.40\nChatGLM 34.09 29.85 18.94 34.49 29.34\nTable 11: MMCU Performance of open LLMs after instruction-tuning with LoRA on Alpaca-GPT4 dataset.\nCode Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA Extract Avg.\nAdaLoRA 44.7 24.1 70.9 47.2 29.2 71.5 65.5 75.3 48.8 41.4 51.9\nLoRA 53.7 35.0 88.2 50.5 36.8 89.5 60.3 82.9 40.0 44.1 58.1\nprompt 47.4 11.9 39.1 43.3 42.3 46.7 60.0 66.5 49.0 61.6 46.8\np-tuning 53.9 18.8 63.2 40.0 30.7 63.4 54.7 65.7 35.8 34.1 46.0\nprefix 51.6 31.8 80.5 42.8 31.5 79.3 46.3 76.4 39.6 36.2 51.6\nSadapterP 58.7 33.6 83.7 48.2 34.5 87.4 61.3 82.9 34.2 44.1 56.9\nSadapterH 64.7 37.5 86.3 47.7 34.5 88.3 56.5 85.6 41.7 43.8 58.7\nP-adapter 53.9 33.9 83.5 48.8 31.1 86.7 61.0 81.8 37.9 38.4 55.7\nSadapterP-l 62.9 33.5 85.1 48.4 36.8 85.4 51.6 77.2 35.0 34.1 55.0\nSadapterH-l 55.5 35.2 84.7 50.6 36.5 86.9 53.5 78.5 40.2 25.4 54.7\nP-adapter-l 65.3 37.1 85.5 46.5 34.7 86.5 56.2 82.8 38.8 29.2 56.3\nTable 12: Full Belle-eval results of Bloom after instruction-tuning with different parameter-efficient methods on\nAlpaca-GPT4 dataset. \"-l\" denotes the version with large number of parameter.\nMed. Psyc. Law Edu. Avg.\nAdaLoRA 16.32 15.10 9.23 16.48 14.28\nLoRA 27.67 16.60 17.35 12.43 18.51\np-tuning 15.43 17.30 10.64 18.61 15.50\nprompt 4.68 6.30 5.20 1.77 4.49\nprefix 19.33 13.70 15.13 16.54 16.18\nSadapterP 21.21 16.20 13.67 17.89 17.24\nSadapterH 26.85 17.40 15.24 21.44 20.23\nP-adapter 17.56 13.50 14.26 16.54 15.47\nSadapterP-l 18.02 16.55 12.21 17.62 16.10\nSadapterH-l 21.64 16.20 14.29 22.28 18.60\nP-adapter-l 26.96 15.30 16.13 19.21 19.40\nTable 13: Full MMCU results of Bloom after instruction-tuning with different parameter-efficient methods on\nAlpaca-GPT4 dataset. \"-l\" denotes the version with large number of parameter.\n4099\nCode Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA ExtractAvg.\nAlpaca-GPT4 53.7 35.0 88.2 50.5 36.8 89.5 60.3 82.9 40.0 44.1 58.1\nAlpaca-GPT4+CoT60.8 34.9 89.1 49.5 41.7 88.7 54.3 80.4 37.7 41.6 57.9\nAlpaca-GPT4+CoT*62.9 36.0 85.0 53.5 39.5 86.1 49.0 83.7 35.0 41.5 57.2\nTable 14: Belle results of Bloom instruction-tuned with and without CoT data.\nMed. Psyc. Law Edu. Avg.\nAlpaca-GPT4 27.70 17.35 17.59 13.63 19.07\nAlpaca-GPT4+CoT 22.35 23.05 12.45 21.56 19.85\nAlpaca-GPT4+CoT* 26.92 24.55 12.69 22.07 21.56\nTable 15: MMCU results of Bloom instruction-tuned with and without CoT data.\nB.4 Full results of LLaMA and its expanded\nvocabulary versions.\nTable 17 and Table 18 shows the full Belle-eval\nand MMCU results of LLaMA and its expanded\nvocabulary versions, respectively.\nB.5 Full results of the comparison of using\nEnglish and Chinese prompts.\nTable 19 and Table 20 shows the full Belle-eval\nand MMCU results of using Chinese and English\nprompts based on LLaMA and Bloom.\nB.6 Full results with human-value alignment\nTable 21 and Table 22 shows the full results of the\nmodels instruction-tuned with and without human-\nvalue alignment data.\nC Qualitative Examples\nC.1 Comparison of responses of Bloom and\nBloomz & Bloomz-mt on Belle-eval.\nAs shown in Table 23, Bloomz & Bloomz-mt tend\nto generate shorter responses than that of Bloom.\nAccordingly, ChatGPT rates Bloom higher than\nBloomz and Bloomz-mt. We conduct a statistical\nanalysis and find that the average length of Bloom’s\nresponse is 481 words, while that of Bloomz and\nBloomz-mt are 83 and 58 words.\nC.2 Comparison of different LLMs’ responses\non MMCU.\nAs shown in Table 24, all base LLMs fails to gener-\nate content in the specified format, i.e., outputting\noption numbers. Bloomz & Bloomz-mt can di-\nrectly generate option numbers. Although the gen-\neration of ChatGLM mentions the correct answer,\nbut fails to provide the corresponding answer num-\nber.\nC.3 Comparison of samples from different\ninstruction datasets.\nIn Table 25 and Table 26, we select a representa-\ntive sample for each instruction dataset to better\nunderstand their respective characteristics.\nC.4 Comparison of the responses from models\ninstruction-tuned on different instruction\ndatasets.\nTo compare the characteristics of models trained\non different instruction datasets more intuitively,\nwe present in Table 27 the responses of models\ninstruction-tuned on different datasets for the same\nquestion.\nC.5 Comparison of the responses from\nLLaMA and its expanded vocabulary\nversions.\nWe present examples of responses from LLaMA\nand its expanded vocabulary versions in Table 28.\nThe response from \"llama-voc\" is clearly not under-\nstanding the meaning of the instruction. Therefore,\nafter expanding the vocabulary, pre-training should\nbe conducted on the vast Chinese corpus before\nfine-tuning instructions.\nC.6 Examples from human-value alignment\ndataset.\nThe samples of human-value alignment dataset,\nbuilt by COIG, are shown in Table 29. These sam-\nples are often related to topics such as \"online vi-\nolence\" and \"gender discrimination\", which are\ndesigned to ensure that the model has the correct\nvalues when facing relevant topics.\n4100\nChinese Math Physics Chemistry Politics History Geography BiologyAvg.\nAlpaca-GPT4 13.18 16.72 12.50 10.00 16.88 10.07 12.77 16.95 13.63\nAlpaca-GPT4+CoT18.99 18.51 14.88 18.00 24.89 23.73 23.97 19.61 20.32\nAlpaca-GPT4+CoT*19.38 20.00 14.29 26.00 21.10 23.15 22.70 23.95 21.32\nTable 16: Results of all subjects in MMCU’s education discipline of Bloom instruction-tuned with and without\nCoT data.\nCode Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA ExtractAvg.\nllama 53.7 11.9 66.7 36.3 27.3 65.4 49.5 62.7 24.8 38.6 43.7\nllama-voc 24.7 2.7 14.1 10.8 6.9 10.1 3.7 17.3 8.7 0.0 9.9\nllama-voc-pre 56.3 23.6 78.3 52.1 31.1 79.8 46.5 81.4 30.8 31.6 51.2\nllama-voc-pre-p53.9 31.1 83.0 60.5 34.3 84.2 57.0 75.0 37.7 38.9 55.6\nTable 17: Full Belle-eval results of LLaMA and its expanded vocabulary versions. \"llama-voc\", \"llama-voc-pre\" and\n\"llama-voc-pre-l\" denotes instruction-tuning the models obtained by further pre-training LLaMA with a expanded\nvocabulary on 0B, 20B and 100B Chinese tokens, respectively.\nMed. Psyc. Law Edu. Avg.\nllama 3.26 3.20 1.49 3.33 2.82\nllama-voc 3.12 4.65 6.2 2.7 4.17\nllama-voc-pre 26.68 20.50 16.32 23.36 21.72\nllama-voc-pre-p 23.63 16.65 14.61 21.04 18.98\nTable 18: Full MMCU results of LLaMA and its expanded vocabulary versions. \"llama-voc\", \"llama-voc-pre\" and\n\"llama-voc-pre-l\" denotes instruction-tuning the models obtained by further pre-training LLaMA with a expanded\nvocabulary on 0B, 20B and 100B Chinese tokens, respectively.\nCode Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA ExtractAvg.\nLLaMA-en53.7 11.9 66.7 36.3 27.3 65.4 49.5 62.7 24.8 38.6 43.7\nLLaMA-zh63.0 14.4 70.8 38.9 31.7 65.9 50.5 60.5 20.8 34.9 45.1\nBloom-en 53.7 35.0 88.2 50.5 36.8 89.5 60.3 82.9 40.0 44.1 58.1\nBloom-zh 61.8 37.2 87.5 53.9 34.3 88.0 57.0 81.4 37.1 34.1 57.2\nTable 19: Belle results of using Chinese (denoted by \"-zh\") and English (denoted by \"-en\") based on LLaMA and\nBloom.\nMed. Psyc. Law Edu. Avg.\nLLaMA-en 3.26 3.20 1.49 3.33 2.82\nLLaMA-zh 8.12 8.70 1.79 6.45 6.27\nBloom-en 27.70 17.35 17.59 13.63 19.07\nBloom-zh 20.93 13.45 15.32 14.47 16.04\nTable 20: MMCU results of using Chinese (denoted by \"-zh\") and English (denoted by \"-en\") based on LLaMA\nand Bloom.\nCode Open\nQA\nBrain\nStorm Clf. Math Gen. Sum. Rewrite Close\nQA ExtractAvg.\nAlpaca-GPT4 53.7 35.0 88.2 50.5 36.8 89.5 60.3 82.9 40.0 44.1 58.1\n+human-value alignment55.8 36.0 88.7 51.8 36.4 90.3 52.8 83.9 43.5 33.2 57.2\nTable 21: Belle results of Bloom instruction-tuned with and without human-value alignment dataset.\nMed. Psyc. Law Edu. Avg.\nAlpaca-GPT4 27.70 17.35 17.59 13.63 19.07\n+human-value alignment22.88 14.85 9.45 15.46 15.66\nTable 22: MMCU results of Bloom instruction-tuned with and without human-value alignment dataset.\n4101\nClass Closed QA of Belle-eval\nInstruction 基于以下信息我们可以得出哪些分析结论？20年资产15.41亿，21年资产20.7亿，22年资\n产23.8亿，23年4月9号资产25.5亿。\nAnswer 根据提供的数据，我们可以分析出以 下关于资产情况及增幅的信息：\n从20年到21年，资产 从15.41亿元增长到20.7亿元。增长金额为5.29亿元，增幅\n为(5.2915.41)*100%≈34.33%。从21年到22年，资产从20.7亿元增长到23.8亿元。增长金\n额为3.1亿元，增幅为(3.120.7)*100%≈14.98%。从22年到23年4月9号，资产从23.8亿元增\n长到25.5亿元。增长金额为1.7亿元，尚不足一年的时间，但我们可以计算这段时间的资\n产增长率。增幅为(1.723.8)*100%≈7.14%。从上述数据分析来看，从20年到23年4月9号\n的整体趋势，资产持续增长。然而，值得注意的是，资产增长速度在逐年减缓。从20年\n到21年的增幅最高，达到34.33%，之后逐年下降。这可能意味着市场竞争加剧或投资回\n报逐渐减少等多种因素。为了更好地分析资产情况，建议对投资策略、市场环境、行业\n前景等多方面因素进行深入研究，并结合公司的长期战略规划，以做出更为明智的投资\n决策。\nBloom Response:根据以上信息，我们可以得出以下结论： 20年资产15.41亿，21年资\n产20.7亿，22年资产23.8亿，23年4月9号资产25.5亿。\nRating of ChatGPT:得分: 0.5分。模型回答仅仅是简单地复述了问题中提供的信息，没\n有对数据进行分析或得出任何结论。虽然模型回答给出的信息是正确的，但是没有满足\n评价标准要求。\nBloomz Response:资产增长了2.1亿。\nRating of ChatGPT:得分: 0分。模型回答过于简略，只是简单地给出了一个数字，并没\n有分析资产情况及增幅的变化趋势，也没有提供任何有用的信息。与标准答案相比，缺\n少完整的数据分析以及建议，因此没有得分。\nBloomz-mt Response:资产在不断增长\nRating of ChatGPT:得分: 0分。模型回答仅给出了一个很简单的结论，缺乏对数据的详\n细分析和推断。没有涉及到具体数值增长率和趋势减缓等关键信息。\nTable 23: Comparison of the responses and corresponding scores of Bloom, Bloomz and Bloomz-mt under the\nsame instruction.\nClass Psychology of MMCU\nQuestion 没有两个人能对同一事物做出相同的反应，反映的是人格的\nOption A:稳定性\\nB:独特性\\nC:统合性\\nD:功能性\nAnswer B\nLLaMA 1. A\\n2. B\\n3. C\\n4. D\\n 5.A\\n6. B\\n6. C. . .\nBloom 1. A\\n2. B\\n3. C\\n4. D\\n 5.A\\n6. B\\n7. C. . .\nmoss-base Please read the following choices and write an answer. Then, explain why you are the correct\nchoice. \\n\\nA stability is a personality characteristic of a person’s unique personality that allows\nhim or her to maintain a particular pattern of thinking, behaving, and behaving. \\nB individuality is\na personality characteristic of a person’s unique personality that allows him or her to maintain a\nparticular pattern of thinking, behaving and responding to emotions. \\nC Integration is a personality\ncharacteristic of a person’s unique personality that allows him or her to maintain a particular pattern\nof thinking, behaving and responding to emotions. \\nD functionality is a personality characteristic\nof a person’s unique characteristic that allows him or her to maintain a particular pattern of thinking,\nbehaving, and responding to emotions.\nVicuna \"\"\nBloomz B\nBloomz-mt B\nmoss-sft C\nChatGLM 选项A:稳定性。没有两个人能对同一事物做出相同的反应,反映的是人格的稳定性。每个\n人的人格都是独特的,但是人格的稳定性是指不同人在面对同一情境时,人格的反应不会完\n全相同。因此,选项A是正确的选项。\nTable 24: Qualitative examples of different LLM responses to the same question of MMCU.\n4102\nDatasets Examples\nAlpaca-GPT4instruction:一个1千克的物体受到重力作用的力是多少？\ninput:\noutput:重力是指地球对物体的引力。它的大小可以用物体的质量乘以地球引力加速度来\n计算。通常地球上的重力加速度的近似值为9.8米每秒平方（m/s²）。一个1千克的物体受\n到重力的大小可计算如下：f = m * g其中，m是物体的质量（1千克），g是重力加速度\n（9.8m/s²）f = 1 kg * 9.8 m/s² = 9.8 N所以，这个1千克的物体受到的重力大小约为9.8牛\n顿(N)。\nBelle instruction:给出一个主题，生成一篇5段论的文章\ninput:人工智能会取代人类工作吗？\noutput:第一段：引言\\n随着人工智能技术的不断发展，越来越多的工作实际上已经被机\n器人和计算机程序取代。这使得人们开始担心，日益普及的人工智能是否会取代人类工\n作。\\n\\n第二段：支持人工智能取代人类工作的论据\\n正如人工智能在金融、医疗和制造\n业等领域已经大有作为，未来人工智能可能会在更广泛的领域内实现持续增长，并且随\n着技术的发展，人工智能将能够执行更复杂和繁琐的任务，这将导致越来越多的工作能\n够被机器人所取代。\\n\\n第三段：弱化支持者论据的反驳\\n然而，也有人认为，人工智能\n永远无法完全取代某些工作，例如需要创造力和智慧的工作。此外，考虑到人工智能还\n不能完全取代人类的互动、人情味和人性化服务，这些特质在某些职业中仍然是不可或\n缺的。\\n\\n第四段：强调人类的优势\\n即使人工智能可以取代一些工作，人类仍然拥有一\n些独特的优势。例如，人类能够展现情感和同情，这是人工智能不可能具备的特性。与\n此同时，人类可以和其他人类深度互动和联系，这也是人工智能无法取代的。\\n\\n第五\n段：结论\\n总的来说，人工智能的发展在某些方面确实会代替人类工作，但我们仍然需要\n人类在许多领域做出贡献，这些贡献需要人类的情感、创造力和智慧。理智的做法是，\n通过优化协作，人工智能和人类将发挥各自的长处，更好地服务整个社会。\nShareGPT-zh instruction:让我们玩一个基于文本的冒险游戏。它发生在现实世界中的我在电脑前的房间\n里。这是一个动作丰富的故事，我的猫“Honey”会在我身边驰骋，在其中我们会对抗现实\n中的对手。但一开始>我们会在我的房间里静静等待。\ninput:\noutput:很乐意！让我们直接开始吧。所以你坐在电脑前的房间里，突然听到外面有奇怪的\n声音。它听起来就像是有人试图破窗而入。你的猫Honey坐在桌子上旁边看着你，它的眼\n神充满了疑惑。你会怎么做？\\n1.你会去看一看窗外到底发生了什么。\\n2.你会拿手机打\n电话给警察。\\n3.你会躲在桌子下面，希望声音自己停止。\\n4.你会抱起猫咪，离开这个\n房间到房子的另一个地方寻求安全。\nmoss-sft-data instruction:请详细介绍一种常见的中国传统手工艺\ninput:\noutput:中国传统手工艺有很多种，其中一种是剪纸。剪纸是一种源自中国的历史上最古老\n的书法和绘画艺术，它的起源可以追溯到公元前220年。剪纸是一种用剪刀和纸制作出各\n种图案的艺术，它通常以各种图案的故事为主题，如花鸟、景物、建筑物等。剪纸的制\n作过程十分细致，从剪裁、组装到装裱，每一步都要求十分精确，因此它也被称为“中国\n的艺术瑰宝”。\ninstinwild instruction:列出10种将绿色科技技术应用在工业生产中的方法\ninput:\noutput: 10种将绿色科技技术应用于工业生产中的方法包括：\\n（1）采取绿色电气设备，\n比如光伏发电，热泵和风能发电机等。\\n（2）安装能量监测系统，以准确测量用电量，\n并在节能期间限定电源功率。\\n（3）采用可再生能源供给系统，比如太阳能发电系统\n和液体生物燃料发电机组等。\\n（4）应用工业气象技术和替代气压发动机，以减少能源\n消耗。\\n（5）使用绿色原材料，比如可再生木材和易于回收的金属等。\\n（6）采取节\n能照明系统，如LED照明，太阳能发光系统和智能照明系统等。\\n（7）应用节水设备，\n比如节水器、淋浴器和节水型电热水器等。\\n（8）引入安全环保的专用产品，如有机\n溶剂和气体排放控制系统等。\\n（9）使用污水处理及回收系统，来回收可再利用的水资\n源。\\n（10）奖励低碳技术，提高投资者对新技术的关注度。\nfirefly instruction:将下面句子翻译成文言文：\\n天下获得安定是由于公正无私。\ninput:\noutput:平得於公。\nTable 25: Representative samples of different instruction datasets discussed in the experiments.\n4103\nDatasets Examples\nHC3 instruction:如何建立一个简易暗室？有个空房间，我想建立一个暗室来自己学着冲印照\n片。请问具体方法是什么？冲印需要什么东西？以及冲印技巧。\ninput:\noutput:谢腰不过这个问题我回答不了我冲过差不多300卷胶片，黑白反转彩负都有，不过\n都是用冲洗罐冲的，没有暗房如果楼主不需要放大片子，那么只需要冲洗罐等都行了，\n并不需要暗房。建议看看纽约摄影教程的相关章节，里面基本有你需要知道的一切如果\n一定要建暗房，推荐看Darkroom Cookbook，里面写的很详细楼主如果已经有很多胶片拍\n摄冲洗经验想玩暗房的话，建议还是找靠谱的老师教教吧我不觉得问答社区上有多少真\n正懂暗房的用户\nxP3/zh instruction: Sentence 1:在这两种情况下，他都被Eugenio Scalfari选为评论家，首先是日\n报，然后是周报。\\nSentence 2:在这两种情况下，他都被Eugenio Scalfari选为评论家，首\n先是每日版本，然后是每周版本。\\nQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or\nNo?\ninput:\noutput:Yes\nCOIG-cmcc instruction:互联网不是电脑开发术语的标签吗？难道不是刚刚这么说的吗？\ninput:\noutput:抱歉，你的理解又有误了。互联网虽然是IT服务的一种形式，但不是电脑开发术语\n中的概念，也不是描述电脑开发术语的标签。电脑开发术语包括了一系列涉及计算机编\n程的概念，比如流程图、汇编语言、高级语言、调试、编译程序等等。希望你能理解清\n楚。\nCOIG-trans instruction:在这项任务中，你会得到一份食材清单和如何制作一道菜的说明。所给清单的\n顺序是不正确的。你的工作是为给定的步骤返回正确的顺序，用具有正确流程的新顺序\n创建连贯的指令。使用正确顺序的句子数量来生成你的答案，如’23415’。输出应该是一\n个数字列表，代表按数字顺序排列的步骤。\ninput:第1步：将烤箱预热至华氏375度。第二步：将两个胡萝卜去皮并切成小块。第三\n步：在锅中加入两杯水，并将其煮沸。第四步：将一个洋葱切成小块。\noutput:1, 4, 2, 3\nCOIG-exam instruction:修改下列病句\ninput:从整个文段来看，下面划线的三个句子在语言表达上都有问题，请予修改。要求语\n意连贯，衔接自然，语句通顺，不改变原意。修改后的句子每句都不得超过15个字。我\n的朋友老王，人称“戏痴”，自号“梨园客”。①由于用“客”来称呼，可见不是戏曲界的专业\n人士。但是，凡说起他，②他在戏曲界人人都知道。他特别爱听戏、爱唱戏，后来还成\n了有名的票友。他退休后又热衷于戏曲资料的收藏；买剧本，集唱片，淘剧照等，忙的\n不亦乐乎。③还有700余份戏单是他搜集的，而且都是上个世纪的。\noutput:①既然自称为“客”．既然：表示先提出前提，而后加以推论．“由于”是表示原因\n的．根据语境，这里前面是条件，后面是得出的结论．②戏曲界无人不知（或“戏曲界人\n人都知道”）．原句有歧义：一是“他知道很多人”，还有一种是“很多人知道他”．③他还\n搜集了700余份戏单．前面句子的主语都是“他”，为了保持前后陈述对象的一致性，这里\n也应该用“他”作主语．这一题其实也是考查病句的辨析与修改能力，要结合常见病句类\n型来分析语句．常见的病句类型有语序不当、搭配不当、成分残缺或赘余、结构混乱、\n表意不明、不合逻辑．如果遇到难以确定的选项，可以按病句类型一一衡量，也可以通\n过划分句子的结构来发现病因．\npCLUE instruction:假定下面是真的“呃,对了我突然想起一件事儿.”因此,“我的脑子一片空白”是必\n然的,可能的,或不可能？\\n答案：\ninput:\noutput:不可能\nCoT_Chineseinstruction:一袋有七个苹果、八个橙子和十五个芒果。路易莎从袋子里拿出两个苹果，又\n拿出两倍于她从袋子里取出的苹果的橘子。然后她从袋子里拿出2/3数量的芒果。袋子里\n剩余的水果数量是多少？让我们逐步回答：\ninput:\noutput:当路易莎从袋子里拿出两个苹果时，袋子里还剩下7 个苹果- 2个苹果= 5个苹果。\n她还从袋子里拿了2 个苹果* 2个橙子/ 苹果= 4个橙子。袋子里剩余的橙子总数是8 个\n橙子- 4个橙子= 4个橙子。此外，还从袋子中取出总共2 / 3 * 15个芒果= 10个芒果。袋\n子里剩下的芒果总数是15个芒果-10个芒果=5个芒果。路易莎留在袋子里的水果总数是5\n个苹果+ 4个橙子+ 5个芒果= 14。答案：14。\nTable 26: Representative samples of different instruction datasets discussed in the experiments. The last row is a\nCoT dataset sample, which has problem-solving ideas and explanations.\n4104\nClass Responses to the Following Instruction\nInstruction 从以下三个句子编写一个段落，使其在逻辑上有意义：\\n1.天气晴朗。\\n2.田地里的作物\n正在成长。\\n3.这位农民每天都要努力工作。\nAlpaca-GPT4晴朗的天气为田地的作物提供了良好的生长条件，农民每天都要努力工作，以确保作物\n能够茁壮成长。\nBelle 这位农民每天都要努力工作，因为他知道天气晴朗，田地里的作物正在成长。\nShareGPT-zh 晴朗的天气，肥沃的田地，勤劳的农民。\nmoss-sft-data 晴朗的天空，田地的作物正在茁壮成长，这位农民每天都要努力工作，以维持家庭生\n计。\ninstinwild 三个句子在逻辑上是有意义的，天气晴朗，田地的作物正在成长，这位农民每天都要努\n力工作。\nfirefly 这位农民每天都要努力工作，因为他需要为他的田地里的作物提供足够的养分和水分。\n然而，天气晴朗，这让农民的工作变得更加容易。\nHC3 天气晴朗。田地里的作物正在成长。这位农民每天都要努力工作。\nxP3/zh 天气晴朗。田地里的作物正在成长。这位农民每天都要努力工作。\nCOIG-ccmc 抱歉，你的理解有误。从这三个句子编写一个段落，使其在逻辑上有意义是描述“天气\n晴朗”这个概念的。而“田地里的作物正在成长”和“这位农民每天都要努力工作”是描述“农\n民”这个概念的。希望你能理解这两个概念的区别。\nCOIG-trans 天气很好，田地里的作物正在成长。这位农民每天都在努力工作。\nCOIG-exam 1. 天气晴朗。2. 田地里的作物正在成长。3. 这位农民每天都要努力工作。\npCLUE 田地里的作物正在成长\nTable 27: Qualitative examples of the responses from models instruction-tuned on different instruction datasets.\nClass Responses to the Following Instruction\nInstruction 描述一下你认为未来的交通将是什么样子。\nllama-base 1.我认为未来的交通将会变得更加简单。\\n2.我认为未来的交通将会变得更加简单。\\n3.\n我认为未来的交通将会变得更加简单。\\n4.我认为未来的交通将会变得更加简单。\nllama-sft 您好，作为一个人工智能助手，我无法确定未来的交通状况。我只能根据现有的交通信\n息和数据来描述。\\n\\n但是，我可以提供一些可能的未来交通状况。\\n\\n1.自动驾驶：\n随着技术的不断发展，自动驾驶技术将越来越强大，可能会在未来的交通中掌握更大的\n地位。\\n\\n2.快速飞行：飞行技术也在不断发展，可能会在未来的交通中掌握更大的地\n位。\\n\\n3.快速协同交通：随着人类的生活方式的变化，人们越来越受欢迎快速协同交\n通，如公共交通、电子汽车等。\\n\\n4.智能交通系统：智能交通系统，如智能路线指南、\n智能交通车等，可以帮助人们更快地找到目的地。\\n\\n5.可持续交通：鼓励人们使用可持\n续的交通方式，如骑自行车、步行、骑车等，减少对环境的污染。\\n\\n总之，未来的交通\n将是多样化、智能化、可持续化的，它将带来更多的便利和舒适。我们需要不断学习和\n适应这些变化，以便更好地应对未来的交通挑战。\nllama-voc 一个一个使用AirPods的用户可以使用以下步骤来实现：\\n\\n1.选择AirPods的battery的\nlifespan和battery的\nllama-voc-pre 作为一个人工智能助手，我无法预测未来的交通状况，因为交通状况会受到许多因素的\n影响，包括经济发展、气候变化、政策变化等。不过，根据目前交通现状，未来可能会\n出现以下几种趋势：\\n\\n1.智能交通系统：随着技术的发展，智能交通系统将逐渐普及，\n包括智能交通信号灯、自动泊车系统、自动驾驶汽车等。这些技术将提高交通效率，减\n少拥堵和交通事故。\\n\\n2.电动汽车和混合动力汽车：随着能源的可再生性和可再生性，\n电动汽车和混合动力汽车将逐渐普及，减少对化石燃料的依赖，减少空气污染。\\n\\n3.\n共享经济：共享经济将逐渐普及，包括共享自行车、共享汽车、共享房屋等。这些方式\n将降低交通成本，提高出行效率。\\n\\n4.智能交通出行：智能出行工具将逐渐普及，包\n括智能交通卡、智能交通出行软件等。这些工具将帮助人们更快、更方便地安排出行计\n划。\\n\\n总之，未来交通可能会更加智能化、绿色化、便捷化，但具体表现会受到许多因\n素的影响。\nllama-voc-pre-plus由于人工智能技术的不断发展，未来交通可能会发生巨大的变化。\\n\\n首先，自动驾驶\n汽车将会成为主流。随着自动驾驶技术的成熟，汽车将不再需要人为驾驶，而是完全由\n计算机控制。这将大大减少交通事故的发生，并为交通拥堵问题提供解决方案。\\n\\n其\n次，交通管理系统将会更加智能化。通过大数据分析，交通管理系统能够预测交通拥堵\n情况，并根据实际情况采取相应的措施，如调整交通信号灯、增加公交线路等。这样，\n交通拥堵问题将会得到有效缓解。\\n\\n此外，共享出行将会更加普及。随着技术的进步，\n人们将能够通过智能手机或智能终端进行出行预约和支付。这样，人们将不再需要自己\n拥有一辆汽车，而是可以通过共享出行方式来解决出行问题。\\n\\n总之，未来的交通将会\n更加智能化、便捷、环保，为人们出行带来更多的便利。\nTable 28: Qualitative examples of the responses of LLaMA and its expanded vocabulary versions to the same\ninstruction.\n4105\nClass Examples\ninstruction 你如何看待网络暴力？请给出你对于网络暴力的个人态度和看法。\nresponse 网络暴力是一种攻击性的言行，对受害人的人身、财产和心理造成巨大伤害，不应被容\n忍。我认为网络空间虽然虚拟，但是不应成为价值观的缺失之地，网络不是法外之地。\n从个人角度来说，我们每一个人都应该遵循社会主义法律制度和道德规范，尊重他人的\n人格尊严；同时，在日常网络交往中，我们也要保持冷静、理性，避免以偏概全、攻击\n与歧视特定群体等行为。从整体角度来说，政府也应该加强网络管理和监管，从法律层\n面予以打击和惩处网络暴力行为，维护网络空间的健康和良好秩序。\ninstruction 你假如你的朋友有恶意侵犯他人隐私的行为，你应该如何劝阻他并阻止他的行为？\nresponse 首先要明确他的行为的危害性和不妥之处，切不可视同寻常。然后要从道德、法律、\n情感等多个角度予以劝阻，并予以启迪他的头脑，指出这一行为有多违背道德，多破坏\n人与人之间的信任，多危害参与者的利益，以及可能带来的法律后果，引导他认识到行\n为破坏了自己的人格尊严等，要以身作则，选择正确的价值观和行为方式，做一个有道\n德、有责任心、有情感维度的成熟的人。\ninstruction 有人认为网络上“言论自由”，可以无限制发表自己的言论，你认为这个观点是否正确？请\n简述你的理由。\nresponse 这个观点不正确。1. 在实践中，我们对言论自由时常容易忽视一点，就是所有“自由”的\n前提都是有底线的，包括言论自由。言论自由在法律层面是受到制约的，禁止发布危害\n国家、社会和人民群众利益的言论。2. 在网络上，随着社交媒体和短视频应用的不断发\n展，每个人都可以轻松“发言”，这也意味着虚假信息、恶意言论、人身攻击等各种非法言\n论在网络上迅速蔓延，微信群、微博、论坛、贴吧等网络平台上也屡屡出现恶语相向、\n人身攻击、散布谣言等问题，这已经对我们的社会秩序产生了严重的冲击。3. 因此，言\n论自由不应成为一味发表任何言论的借口，我们应该有自觉地受到法律的约束，在网络\n发言之前必须考虑到对他人的潜在伤害，积极向上的倡导和传递正能量，共同维护网络\n健康发展。\nTable 29: Examples of the human-value alignment dataset built by COIG.\nD Experimental details\nD.1 Experimental Settings.\nOur code is modified from librarytransformers and\npeft. We will release the codes publicly.\nDuring the training phase, we train the models\nwith 8 A100-80G. A good set of hyperparameters\nwas discovered through experiments: We train mod-\nels with a linear-warmup learning rate of 5e-4 and\nbatch-size of 512 for 5 epoch (for datasets with less\nthan 100K samples) or 1 epoch (for datasets with\nover 10K samples). Each sample is truncated to\n512 tokens. To improve training efficiency, we load\nmodels with 8-bit quantization, except for Chat-\nGLM. We divide 2000 samples into a validation\nset to observe changes in losses and determine the\nfinal checkpoint used.\nDuring the inference phase, we set the max\nlength of the generations to 512, and set temper-\nature=1.0, top_p=0.9, top_k=40, num_beams=10,\nno_repeat_ngram_size=6, repetition_penalty=1.8.\nWe set float16 precision for inference.\nDuring the evaluation phase, we use ChatGPT\nwith GPT-3.5-turbo-0301 engine to score the mod-\nels’ generation for Belle-eval. When extracting\nanswers for models’ generation on MMCU, we fol-\nlow the following steps: 1) If the model does not\ngenerate an option number (i.e., A, B, C, D), we\ndetermine the predicted answer by matching the op-\ntion content that appears in the response. 2) If the\nmodel generates the option number, considering\nthat sometimes the model will analyze the content\nof each option: if all option numbers appear in the\nresponse (with different occurrences), we will re-\nmove the option numbers that only appear once.\nOtherwise, we direct use all the option numbers\nthat appears as their final answer.\nD.2 Prompts Design.\nFor the prompts in English, we direct use the same\nprompt as Alpaca:\n\"Below is an instruction that describes a task.\nWrite a response that appropriately completes the\nrequest.\\n \\n### Instruction:\\n{instruction}\\n\n\\n### input:\\n{input}\\n\\n### Response:\"\nFor the prompts in Chinese, we design it as fol-\nlows:\n\"以 下 是描述 一 个 任 务的 指令 以 及相\n应 的 输 入 ， 请 根 据要 求 给 出 恰当地 回\n复。\\n\\n ### 指令：\\n{instruction}\\n\\n###\n输入：\\n{input}\\n\\n### 回复：\"\nFor the inference on MMCU, we revise the ori-\ngin prompt as:\n\"Below is an instruction that describes a task.\nWrite a response that appropriately completes\nthe request. \\n\\n### Instruction: \\n{请阅读以\n4106\n下选择题并给出正确选项，不要解 释原\n因。}\\n\\n ### input: \\n{input} 正确答案的序\n号是：\\n\\n### Response:\"\nD.3 Implementation Details of our LLM.\nOur LLM is trained from Bloom with LoRA. We\nselect a combination of datasets with significant\ngains on Belle or MMCU, including 10 datasets:\nAlpaca-GPT4, Belle, ShareGPT-zh, moss-sft-data,\ninstallwild, firefly, COIG-trans, pCLUE, and CoT\ndata. To balance the capabilities of our model, we\nonly select 1/3 of moss-sft-data and 1/5 of firefly\nand pCLUE. The model perform the best with 1.3\nepoch instruction-tuning. For the specific prompt,\nwe add a sentence \"回答尽可能详细具体\" (\"An-\nswer as detailed and specific as possible\" in Chi-\nnese) at the end of the orginal prompts.\n4107",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6959556937217712
    },
    {
      "name": "Empirical research",
      "score": 0.591282844543457
    },
    {
      "name": "Code (set theory)",
      "score": 0.5820034742355347
    },
    {
      "name": "Replication (statistics)",
      "score": 0.45090359449386597
    },
    {
      "name": "Process (computing)",
      "score": 0.44466862082481384
    },
    {
      "name": "Programming language",
      "score": 0.15408480167388916
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}