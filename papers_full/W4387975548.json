{
    "title": "SegViT v2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers",
    "url": "https://openalex.org/W4387975548",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2116540305",
            "name": "Bowen Zhang",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A2117670397",
            "name": "Liyang Liu",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A3034912343",
            "name": "Minh Hieu Phan",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A2228339486",
            "name": "Zhi Tian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098201046",
            "name": "Chunhua Shen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2102849999",
            "name": "Yifan Liu",
            "affiliations": [
                "University of Adelaide"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2561196672",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3034435444",
        "https://openalex.org/W4312744085",
        "https://openalex.org/W6600007113",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W3108139472",
        "https://openalex.org/W4312815172",
        "https://openalex.org/W3180659539",
        "https://openalex.org/W2798791840",
        "https://openalex.org/W4313007769",
        "https://openalex.org/W3171888599",
        "https://openalex.org/W3107810305",
        "https://openalex.org/W4225484930",
        "https://openalex.org/W2060277733",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W4386071839",
        "https://openalex.org/W3198020043",
        "https://openalex.org/W4312309344",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W4312910119",
        "https://openalex.org/W3035526186",
        "https://openalex.org/W4386071535",
        "https://openalex.org/W2997564896",
        "https://openalex.org/W2993235622",
        "https://openalex.org/W2473930607",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W4221163392",
        "https://openalex.org/W2563705555",
        "https://openalex.org/W3107497254",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W4313181088",
        "https://openalex.org/W3187986157",
        "https://openalex.org/W3011986058",
        "https://openalex.org/W3166525903",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W2125215748",
        "https://openalex.org/W4301914798",
        "https://openalex.org/W3120075637",
        "https://openalex.org/W4312401347",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W4285299366",
        "https://openalex.org/W3175937066",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W2940262938",
        "https://openalex.org/W4312453657",
        "https://openalex.org/W4312615142",
        "https://openalex.org/W4312633518",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W4312351187",
        "https://openalex.org/W4312238419",
        "https://openalex.org/W3014641072",
        "https://openalex.org/W4313158203",
        "https://openalex.org/W4293680532",
        "https://openalex.org/W3109772732",
        "https://openalex.org/W2884822772",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W3180392831",
        "https://openalex.org/W3109301572",
        "https://openalex.org/W2963727650",
        "https://openalex.org/W4312447529",
        "https://openalex.org/W6797784111",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W4386075882",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2191637929"
    ],
    "abstract": null,
    "full_text": "International Journal of Computer Vision (2024) 132:1126–1147\nhttps://doi.org/10.1007/s11263-023-01894-8\nSegViT v2: Exploring Efﬁcient and Continual Semantic Segmentation\nwith Plain Vision Transformers\nBowen Zhang 1 · Liyang Liu 1 · Minh Hieu Phan 1 · Zhi Tian 3 · Chunhua Shen 2 · Yifan Liu 1\nReceived: 3 March 2023 / Accepted: 10 August 2023 / Published online: 27 October 2023\n© The Author(s) 2023\nAbstract\nThis paper investigates the capability of plain Vision Transformers (ViTs) for semantic segmentation using the encoder–\ndecoder framework and introduce SegViTv2. In this study, we introduce a novel Attention-to-Mask (ATM) module to design\na lightweight decoder effective for plain ViT. The proposed ATM converts the global attention map into semantic masks\nfor high-quality segmentation results. Our decoder outperforms popular decoder UPerNet using various ViT backbones\nwhile consuming only about 5% of the computational cost. For the encoder, we address the concern of the relatively high\ncomputational cost in the ViT-based encoders and propose a Shrunk++ structure that incorporates edge-aware query-based\ndown-sampling (EQD) and query-based up-sampling (QU) modules. The Shrunk++ structure reduces the computational\ncost of the encoder by up to 50% while maintaining competitive performance. Furthermore, we propose to adapt SegViT\nfor continual semantic segmentation, demonstrating nearly zero forgetting of previously learned knowledge. Experiments\nshow that our proposed SegViTv2 surpasses recent segmentation methods on three popular benchmarks including ADE20k,\nCOCO-Stuff-10k and PASCAL-Context datasets. The code is available through the following link: https://github.com/zbwxp/\nSegVit.\nKeywords Vision transformer · Incremental learning · Semantic segmentation · Continual learning\nCommunicated by Kaiyang Zhou.\nBowen Zhang, Liyang Liu and Minh Hieu Phan have contributed\nequally to this work.\nB Yifan Liu\nyifan.liu04@adelaide.edu.au\nBowen Zhang\nb.zhang@adelaide.edu.au\nLiyang Liu\nakide.liu@adelaide.edu.au\nMinh Hieu Phan\nvuminhhieu.phan@adelaide.edu.au\nZhi Tian\nzhi.tian@outlook.com\nChunhua Shen\nchunhuashen@zju.edu.cn\n1 The University of Adelaide, Adelaide, Australia\n2 Zhejiang University, Hangzhou, China\n3 Meituan Inc., Beijing, China\n1 Introduction\nSemantic segmentation is a pivotal computer vision task\nthat aims to assign labels to every pixel on the image.\nWidely adopted state-of-the-art methods like Fully Convo-\nlutional Networks (FCN) (Long et al., 2015) utilize deep\nconvolutional neural networks (ConvNet) as encoders and\nincorporate segmentation decoders for dense predictions.\nPrior works (Wang et al., 2020; Yuan et al., 2020; Chen\net al., 2018) have aimed to enhance performance by aug-\nmenting contextual information or incorporating multi-scale\ninformation, leveraging the inherent multi-scale and hierar-\nchical attributes of the ConvNet architectures.\nThe advent of the Vision Transformer (ViT) (Dosovit-\nskiy et al., 2021) has offered a paradigm shift, serving as\na robust backbone for numerous computer vision tasks.ViT,\ndistinct from ConvNet base models, retains a plain and\nnon-hierarchical architecture while preserving the resolu-\ntion of the feature maps. To conveniently leverage existing\nsegmentation decoders for dense prediction, such as U-Net\n(Ronneberger et al., 2015) or DeepLab (Chen et al., 2018),\nrecent Transformer-based approaches, including Swin Trans-\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1127\nformer (Liu et al., 2021) and PVT (Wang et al., 2021), have\ndeveloped a hierarchical ViT to extract hierarchical feature\nrepresentations.\nHowever, modifying the original ViT structures requires\ntraining the networks from scratch rather than using off-the-\nshelf plain ViT checkpoints due to the discrepancy between\nthe hierarchical and plain architectures, such as spatial down-\nsampling (Xu et al., 2022). Altering the plain ViT architecture\ncompromises the use of rich representations from vision-\nlanguage pre-training methods like CLIP (Radford et al.,\n2021), BEiT (Bao et al., 2022), BEiT-v2 (Peng et al., 2022),\nM V P( W e ie ta l . ,2022), and COTS (Lu et al., 2022).\nHence, there is a clear advantage to developing effec-\ntive decoders for the original ViT structures in order to\nleverage those powerful representations. Previous works,\nsuch as UPerNet (Xiao et al., 2018) and DPT (Ranftl et\nal., 2021), have primarily focused on hierarchical feature\nmaps and neglected the distinctive characteristics of the plain\nVision Transformer. Consequently, these methods introduce\ncomputation-intensive operations while offering limited per-\nformance gains, as shown in Fig. 1.\nA recent trend in several works, such as SETR (Zheng\net al., 2021) or Segmenter (Strudel et al., 2021), aims to\ndevelop decoders speciﬁcally tailored for the Plain ViT archi-\ntecture. However, these designs are often an extension of\nper-pixel classiﬁcation techniques derived from traditional\nconvolution-based decoders. For example, SETR’s decoder\n(Zheng et al., 2021) uses a sequence of convolutions and\nbilinear up-sampling to increase the ViT’s extracted fea-\nture maps gradually. It then applies a naive MLP to the\nextracted features to perform pixel-wise classiﬁcation, which\nisolates the neighboring contexts surrounding the pixel. Cur-\nrent pixel-wise classiﬁcation decoder designs overlook the\nFig. 1 Comparison with previous methods in terms of performance\nand efﬁciency on ADE20K dataset. The and bubbles in the accompany-\ning graph represent the ViT Base and ViT Large models, respectively,\nwith the size of each bubble corresponding to the FLOPs of the variant\nsegmentation methods. SegViT-BEiT v2 Large achieves state-of-the-\nart performance with a 58.0% mIoU on the ADE20K validation set.\nAdditionally, our efﬁcient, optimized version, SegViT-Shrunk-BEiT v2\nLarge, saves half of the GFLOPs compared to UPerNet, signiﬁcantly\nreducing computational overhead while maintaining a competitive per-\nformance of 55.7%\nimportance of contextual learning when assigning labels to\neach pixel.\nAnother prevalent issue in deep networks, including\nTransformer, is ‘catastrophic forgetting’ (French, 1999;\nKirkpatrick et al., 2017), where the model’s performance\non previously learned tasks deteriorates as it learns new\nones (Shao & Feng, 2022; Wang et al., 2022, ?; Phan et\nal., 2022). This limitation poses signiﬁcant challenges for\nthe application of deep segmentation models in dynamic\nreal-world environments. Recently, the rapid development\nof the foundation model pre-trained on large-scale data has\nsparked interest among researchers in studying its transfer-\nability across various downstream tasks (Ostapenko et al.,\n2022). These models are capable of extracting powerful and\ngeneralized representations, which has led to a growing inter-\nest in exploring their extensibility to new classes and tasks\nwhile retaining the previously learned knowledge represen-\ntations (Ramasesh et al., 2022; Wu et al., 2022).\nInspired by these challenges, this paper aims to develop\nplain Vision Transformer-based model for effective seman-\ntic segmentation without resorting to hierarchical backbone.\nAs self-supervision and multi-modality pre-training continue\nto evolve, we anticipate that the plain vision transformer\nwill learn enhanced visual representations. Consequently,\ndecoders for dense tasks are expected to adapt more ﬂexi-\nbly and efﬁciently to these representations.\nIn light of these research gaps, we propose SegViTv2—a\nnovel, efﬁcient segmentation network that features a plain\nVision Transformer and exhibits robustness against forget-\nting. We introduce a novel Attention-to-Mask (ATM) module\nthat operates as a lightweight component for the SegViT\ndecoder. Leveraging the non-linearity of cross-attention\nlearning, our proposed ATM employs learnable class tokens\nas queries to pinpoint spatial locations that exhibit high com-\npatibility with each class. We advocate for regions afﬁliated\nwith a particular class to possess substantial similarity values\nthat correspond to the respective class token.\nAs depicted in Fig. 2, the ATM generates a meaningful\nsimilarity map that accentuates regions with a strong afﬁn-\nity towards the ‘Table’ and ‘Chair’ categories. By simply\nimplementing a Sigmoid operation, we can transform these\nsimilarity maps into mask-level predictions. The computa-\ntion of the mask scales linearly with the number of pixels, a\nnegligible cost that can be integrated into any backbone to\nbolster segmentation accuracy. Building upon this efﬁcient\nATM module, we present a novel semantic segmentation\nparadigm that utilizes the cost-effective structure of plain\nViT, referred to as SegViT. Within this paradigm, multiple\nATM modules are deployed at various layers to extract seg-\nmentation masks at different scales. The ﬁnal prediction is\nthe summation of the outputs derived from these layers.\nTo alleviate the computational burdens of plain Vision\nTransformers (ViTs), we introduce the Shrunk and Shrunk++\n123\n1128 International Journal of Computer Vision (2024) 132:1126–1147\nFig. 2 The overall concept of our Attention-to-Mask decoder. ATM\nlearns the similarity map for each category by capturing the cross-\nattention between the class tokens and the spatial feature map (Left).\nSigmoid is applied to produce category-speciﬁc masks, highlighting\nthe area with high similarity to the corresponding class (Middle). ATM\nenhances the semantic representations by encouraging the feature to be\nsimilar to the target class token and dissimilar to other tokens\nstructures, which incorporate query-based downsampling\n(QD) and query-based upsampling (QU). The proposed QD\nemploys a 2x2 nearest neighbor downsampling technique to\nobtain a sparser token mesh, reducing the number of tokens\ninvolved in attention computations. In Shrunk++, we extend\nQD to edge-aware query-based downsampling (EQD). EQD\nselectively preserves tokens situated at object edges, as they\npossess more discriminative information. Consequently, QU\nrecovers the discarded tokens within the object’s homoge-\nneous body, reconstructing high-resolution features crucial\nfor accurately dense prediction. Integrating the Shrunk++\nstructure with the ATM module as the decoder, our SegViTv2\nachieves computational reductions of up to 50% while main-\ntaining competitive performance.\nWe further adapt our SegViTv2 framework for continual\nlearning. Leveraging the robust, generalized representation\nof the foundational model, this paper investigates its adapt-\nability to new classes and tasks, ensuring retention of prior\nknowledge. Recent techniques in continual semantic seg-\nmentation (CSS) aim to replay old data (Maracani et al.,\n2021; Cha et al., 2021) or distill knowledge from the pre-\nvious model to mitigate model divergence (Cermelli et al.,\n2020; Phan et al., 2022; Zhang et al., 2022). These methods\nﬁne-tune parameters related to old tasks, which can disrupt\nthe previously learned solutions and result in forgetting. In\ncontrast, our proposed SegViT supports learning new classes\nwithout interfering with previously acquired knowledge. We\nstrive to establish a forget-free SegViT framework, achieved\nby incorporating a new ATM module dedicated to new tasks\nwhile freezing all old parameters. Consequently, the pro-\nposed SegViT architecture has the potential to eliminate the\nissue of forgetting.\nOur key contributions can be summarized as follows:\n– We introduce the Attention-to-Mask (ATM) decoder\nmodule, a potent and efﬁcient tool for semantic segmen-\ntation. For the ﬁrst time, we exploit spatial information\npresent in attention maps to generate mask predictions\nfor each category, proposing a new paradigm for seman-\ntic segmentation.\n– We present the Shrunk++ structure, applicable to any\nplain ViT backbone, which alleviates the intrinsically\nhigh computational expense of the non-hierarchical ViT\nwhile maintaining competitive performance, as illus-\ntrated in Fig. 1. We are the ﬁrst work capitalizing on edge\ninformation to decrease and restore tokens for efﬁcient\ncomputation. Our Shrunk++ version of SegViTv2,t e s t e d\non the ADE20K dataset, achieves a mIoU of 55.7%,\nwith a computational cost of 308.8 GFLOPs, marking\na reduction of approximately 50% compared to the orig-\ninal SegViT (637.9 GFLOPs).\n– We propose a new SegViT architecture capable of con-\ntinual learning with nearly zero forgetting. To our knowl-\nedge, we are the ﬁrst work seeking to completely freeze\nall parameters for old classes, thereby nearly obliterating\nthe issue of catastrophic forgetting.\n2 Related Work\n2.1 Semantic Segmentation\nSemantic segmentation aims to partition an image into\nregions with meaningful categories. Fully Convolutional\nNetworks (FCNs) used to be the dominant approach to this\ntask. To enlarge the receptive ﬁeld, several approaches (Zhao\net al., 2017; Chen et al., 2018) propose dilated convolu-\ntions or apply spatial pyramid pooling to capture contextual\ninformation at multiple scales. Most semantic segmentation\nmethods aim to classify each pixel directly using a classiﬁ-\ncation loss. This paradigm naturally partitions images into\ndifferent classes.\nVarious methods have achieved signiﬁcant advancements\nby integrating Transformers into the semantic segmentation\ntask. Early works (Liu et al., 2021; Dong et al., 2022) directly\nadapt the transformer encoder, designed for classiﬁcation,\ninto semantic segmentation by ﬁne-tuning it together with\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1129\nsegmentation decoders such as UPerNet (Xiao et al., 2018).\nRecent approaches (Xie et al., 2021; Strudel et al., 2021;\nCheng et al., 2021) have focused on designing the overall\nsegmentation framework to achieve better adaptation. For\ninstance, SETR (Zheng et al., 2021) views semantic segmen-\ntation as a sequence-to-sequence task and proposes a pure\nTransformer encoder combined with a standard convolution-\nbased decoder. SegFormer (Xie et al., 2021)e m p l o y sa\nhierarchical encoder design to extract features from ﬁne-to-\ncoarse levels and a lightweight decoder design for efﬁcient\nprediction. However, the SegFormer decoder adopts the\npyramid structure by fusing multi-scale features, which\nis specialized for hierarchical ViTs such as Swin Trans-\nformer (Liu et al., 2021). The above-mentioned methods\naim to design either a naive convolution-based decoder or\na pyramid-structure decoder for hierarchical base models.\nNonetheless, designing an effective decoder specialized for\nplain ViTs remains an open research question.\nRecently, several segmentation methods propose a univer-\nsal framework that uniﬁes multiple tasks, including instance\nsegmentation, semantic segmentation, and object detection.\nFor example, Mask DINO (Li et al., 2022) extends DINO\nwith a mask prediction branch, achieving promising results\nin the instance, panoptic, and semantic segmentation tasks.\nMask2Former (Cheng et al., 2022) enhances MaskFormer\n(Cheng et al., 2021) by introducing deformable multi-scale\nattention in the decoder and a masked cross-attention mech-\nanism. OneFormer (Jain et al., 2022) represents a universal\nimage segmentation framework with a multi-task train-once\ndesign, outperforming specialized models in various tasks.\nRecent methods (Cheng et al., 2021; Strudel et al., 2021;\nZhang et al., 2021) propose decoupling the per-pixel clas-\nsiﬁcation into image partitioning and region classiﬁcation.\nFor image partitioning, they use learnable tokens as mask\nembeddings and associate them with the extracted feature\nmap to generate object masks. For region classiﬁcation, the\nlearnable tokens are fed to a classiﬁer to predict the class\ncorresponding to each mask. This paradigm enables global\nsegmentation and alleviates the burden on the decoder to\nperform per-pixel classiﬁcation, resulting in state-of-the-art\nperformance (Cheng et al., 2021). While previous works use\ngeneric tokens for mask generation, this work explicitly uti-\nlizes class-speciﬁc tokens to enhance the semantics of mask\nembeddings, thereby improving segmentation accuracy.\n2.2 Mask-Oriented Segmentation\nCompared to previous mask-oriented segmentation tech-\nniques such as MaskFormer (Cheng et al., 2021) and\nMask2Former (Cheng et al., 2022), our method presents\nseveral novel conceptual differences and advantages. Specif-\nically, our approach is tailored to address semantic segmen-\ntation problems by assigning each class to a ﬁxed token\nand generating the corresponding mask directly. In con-\ntrast, MaskFormer relies on Hungarian matching, with each\nlearnable query corresponding to spatial information instead\nof category information. Our Attention-to-Mask (ATM)\napproach eliminates the need for positional embedding, as\nwe utilize the attention map between the class token and the\nfeature map. Our overarching goal is to adapt Plain Vision\nTransformers for dense prediction, as recent studies have\ndemonstrated that self-supervised learning (He et al., 2022;\nChen et al., 2022; Touvron et al., 2022; Peng et al., 2022) and\nmultimodal learning (Radford et al., 2021) are enhanced by\nhierarchical ViT structures. Our approach enhances the rep-\nresentation ability of class tokens by applying transformer\nblocks.\nPrevious CNN-based decoders, such as OCRNet (Yuan et\nal., 2019) and K-Net (Zhang et al., 2021), have demonstrated\nthe effectiveness of the attention mechanism in modeling\ncontextual information. For example, K-Net utilizes seman-\ntic kernels (one kernel for each class) and performs convo-\nlution operations to generate the semantic mask. In contrast,\nour proposed ATM module integrates cross-attention mecha-\nnisms, allowing for more effective contextual learning. While\nOCRNet (Yuan et al., 2019) applies cross-attention from\nthe class token to the feature map to enhance feature rep-\nresentations, it still employs a standard linear predictor in\nthe decoder to produce the segmentation map. On the other\nhand, our proposed ATM module is speciﬁcally designed\nfor generating segmentation outputs, paving the way for\nfuture research on effective decoders for plain ViT. Addi-\ntionally, existing convolution-based attention networks such\nas OCRNet (Yuan et al., 2019), K-Net (Zhang et al., 2021),\nand DANet (Fu et al., 2019) adopt the traditional per-pixel\nclassiﬁcation framework for segmentation generation. In\ncontrast, our proposed SegViT decouples segmentation into\nmask prediction and classiﬁcation, which proves advanta-\ngeous for establishing connections between the class proxy\nand language representations (Zhou et al., 2022), as well as\nfacilitating continual learning.\n2.3 Transformers for Vision\nIn the realm of image classiﬁcation tasks, attention-based\ntransformer models have emerged as powerful alternatives\nto standard convolution-based networks. The original ViT\n(Dosovitskiy et al., 2021) represents a plain, non-hierarchical\narchitecture. However, there have been several advancements\nin the ﬁeld of hierarchical transformers, such as PVT (Wang\net al., 2021), Swin Transformer (Liu et al., 2021), Twins (Chu\net al., 2021), SegFormer (Xie et al., 2021), and P2T (Wu et al.,\n2022). These hierarchical transformer models inherit certain\ndesign elements from convolution-based networks, including\nhierarchical structures, pooling, and downsampling with con-\nvolutions. Consequently, they can be seamlessly employed\n123\n1130 International Journal of Computer Vision (2024) 132:1126–1147\nas direct replacements for convolutional-based networks and\ncan be coupled with existing decoder heads for tasks such as\nsemantic segmentation.\n2.4 Self-Supervised Vision Transformers\nSelf-supervised learning has emerged as a powerful techni-\nque for pretraining visual models, eliminating the need for\nlabeled data. One notable self-supervised method is MAE\n(He et al., 2022) (Masked Autoencoder), which trains a\nvision transformer to reconstruct masked regions of input\nimages. This approach results in a high generalization capac-\nity. Another signiﬁcant method is CLIP (Radford et al., 2021)\n(Contrastive Language-Image Pre-Training), which involves\njoint training of a vision transformer and a language model\non a large corpus of text and images, leading to the cre-\nation of a comprehensive knowledge store. CAE (Chen et al.,\n2022) aims to learn image representations that are invariant\nto context changes and effectively capture underlying seman-\ntic content. Furthermore, iBot (Zhou et al., 2022) performs\nmasked visual learning using an online tokenizer and self-\ndistillation mechanism, facilitating semantic representation\nlearning. In our approach, we leverage attention to masks\nto optimize the extraction of dense hidden representations,\nthereby enhancing the segmentation capability of our model.\n2.5 Plain-Backbone Decoders\nFor dense prediction tasks, such as semantic segmentation,\nthe high-resolution feature maps produced by the backbone\nare vital for preserving spatial details. In typical hierar-\nchical transformer models, techniques such as FPN (Lin\net al., 2017) or dilated backbone are employed to gener-\nate high-resolution feature maps by merging features from\ndifferent levels. However, when it comes to a plain, non-\nhierarchical transformer backbone, the resolution remains\nthe same across all layers. SETR (Zheng et al., 2021)p r o -\nposed a straightforward approach to address segmentation\ntasks by treating transformer outputs from the base model in\na sequence-to-sequence perspective. Segmenter (Strudel et\nal., 2021) combines class embeddings and transformer patch\nembeddings and applies several self-attention layers on the\ncombined tokens to learn discriminative embeddings. In their\napproach, the class tokens are used as input to the ViT back-\nbone, resulting in increased computational complexity. In\ncontrast, our SegViT introduces the class tokens as input to\nthe ATM, the Attention-to-Mask module, thereby reducing\ncomputational costs while still beneﬁting from the integra-\ntion of class tokens.\n2.6 Continual Learning\nContinual learning (CL) aims to address the issue of forget-\nting, ensuring consistent performance on previously learned\nclasses while adapting to new ones (Chen and Liu, 2016).\nMost CL methods propose regularization techniques for\nconvolution-based networks (Li & Hoiem, 2018; Douillard\net al., 2020; Kang et al., 2022; Peng et al., 2021) or expand\nthe network architectures to accommodate new tasks (Yan\net al., 2021), thereby avoiding the need to store and replay\nold data. In recent years, efforts have also emerged to pre-\nvent forgetting in Transformer models. Dytox (Douillard et\nal., 2022) dynamically learns new task tokens, which are\nthen utilized to make the learned embeddings more relevant\nto the speciﬁc task. Lifelong ViT (Wang et al., 2022) and\ncontrastive ViT (Wang et al., 2022) introduce cross-attention\nmechanisms between tasks through external key vectors, and\nthey slow down the changes to these keys to mitigate for-\ngetting. Despite the use of complex mechanisms to prevent\nforgetting, these methods still require ﬁne-tuning of the net-\nwork for new classes, which can result in interference with\npreviously learned knowledge.\nIn the ﬁeld of semantic segmentation, recent research has\nbeen devoted to addressing the forgetting issue in contin-\nual learning. However, in addition to forgetting, continual\nsemantic segmentation (CSS) also encounters the problem\nof \"background shift.\" This refers to the situation where\nforeground object classes from previous tasks are mistak-\nenly classiﬁed as background in the current task (Cermelli\net al., 2020). REMINDER (Phan et al., 2022) tackles for-\ngetting in CSS by utilizing class similarity to identify the\nclasses that are more likely to be forgotten. It then focuses\non revising those speciﬁc classes to mitigate the forgetting\nproblem. RCIL (Zhang et al., 2022) introduces a two-branch\nconvolutional network, with one branch frozen and the other\ntrained to prevent forgetting. At the end of each learning step,\nthe trainable branch is merged with the frozen branch, which\ncan introduce model interference. However, it is worth noting\nthat existing CSS and CL techniques typically involve ﬁne-\ntuning certain parts of the network dedicated to the old tasks.\nUnfortunately, this ﬁne-tuning process can lead to forgetting\nas the model diverges from the previously learned solution.\n3 Method\nIn this section, we ﬁrst introduce the overall architecture\nof our proposed SegViT model for semantic segmentation.\nThen, we discuss the Shrunk and Shrunk++ architectures\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1131\nFig. 3 The overall SegViT structure with the ATM module. The\nAttention-to-Mask (ATM) module inherits the typical transformer\ndecoder structure. It takes in randomly initialized class embeddings\nas queries and the feature maps from the ViT backbone to generate\nkeys and values. The outputs of the ATM module are used as the input\nqueries for the next layer. The ATM module is carried out sequentially\nwith inputs from different layers of the backbone as keys and values in\na cascade manner. A linear transform is then applied to the output of\nthe ATM module to produce the class predictions for each token. The\nmask for the corresponding class is transferred from the similarities\nbetween queries and keys in the ATM module. We have removed the\nself-attention mechanism in ATM decoder layers further improve the\nefﬁciency while maintaining the performance\ndesigned to reduce the model’s computational cost. Lastly,\nwe explore the adaptation of our SegViT model for the\ncontext of continual semantic segmentation to minimize for-\ngetting.\n3.1 Overall SegViT Architecture\nSegViT comprises a ViT-based encoder responsible for fea-\nture extraction and a decoder used to learn the segmentation\nmap. For the encoder, we designed the ‘Shrunk’ structure\nto reduce the computational overhead associated with the\nplain ViT. Regarding the decoder, we introduce a novel\nlightweight module named Attention-to-Mask (ATM). This\nmodule generates class-speciﬁc masks denoted as M and\nclass predictions denoted as P, which determine the presence\nof a particular class in the image. The mask outputs from a\nstack of ATM modules are combined and then multiplied by\nthe class predictions to obtain the ﬁnal segmentation output.\nFigure 3 illustrates the overall architecture of our proposed\nSegViT.\n3.1.1 Encoder\nGiven an input image I ∈ R\nH×W ×3, the plain vision trans-\nformer backbone reshapes it into a sequence of tokens F0 ∈\nRL×C , where L = HW\nP2 , P is the patch size, and C is the num-\nber of channels. To capture positional information, learnable\nposition embeddings of the same size as F\n0 are added. Subse-\nquently, the token sequence F0 is processed by m transformer\nlayers to produce the output. The output tokens for each\nlayer are deﬁned as [F\n1,F2,..., Fm ]∈ RL×C .F o rap l a i n\nvision transformer like ViT, the number of tokens are high\nand remains constant for each layer. Processing a substantial\nnumber of tokens for every layer results in elevated com-\nputational costs for plain ViT. We denote a plain ViT-based\nencoder as the ’Single’ structure. To mitigate computational\ncosts, we introduce the Shrunk and Shrunk++ structures, tai-\nlored to create a more computationally efﬁcient ViT-based\nencoder. Further details regarding the Shrunk structure can\nbe found in Sect. 3.2.\n3.1.2 Decoder\nAttention-to-Mask (ATM) Cross-attention can be described\nas the mapping between two sequences of tokens, denoted\nas {v\n1,v2}. In our case, we deﬁne two token sequences:\nG ∈ RN ×C with a length N equal to the number of classes,\nand Fi ∈ RL×C . To enable cross-attention, linear transfor-\nmations are applied to each token sequence, resulting in the\nquery (Q), key (K), and value (V) representations. This pro-\ncess is described by Eq. ( 1).\n123\n1132 International Journal of Computer Vision (2024) 132:1126–1147\nQ = φq (G) ∈ RN ×C ,\nK = φk (Fi ) ∈ RL×C ,\nV = φv(Fi ) ∈ RL×C .\n(1)\nThe similarity map is calculated by computing the dot\nproduct between the query and key representations. Fol-\nlowing the scaled dot-product attention mechanism, the\nsimilarity map and attention map are calculated as follows:\nS(Q, K ) = QK\nT\n√dk\n∈ RN ×L ,\nAttention (G,Fi ) = Softmax\n(\nS(Q, K )\n)\nV ∈ RN ×C ,\n(2)\nwhere √dk is a scaling factor with dk equals to the dimension\nof the keys.\nThe shape of the similarity map S(Q, K ) is determined by\nthe lengths of the two token sequences, N and L. The atten-\ntion mechanism updates G by performing a weighted sum\nof V , where the weights are derived from the similarity map\nafter applying the softmax function along the L dimension.\nIn dot-product attention, the softmax function is used\nto concentrate attention exclusively on the token with the\nhighest similarity. However, we believe that tokens other\nthan those with maximum similarity also carry meaning-\nful information. Based on this intuition, we have designed a\nlightweight module that generates semantic predictions more\ndirectly. To this end, we assign G as the class embeddings for\nthe segmentation task, and F\ni as the output of layer i of the\nViT backbone. A semantic mask is paired with each token\nin G to represent the semantic prediction for each class. The\nbinary mask M is deﬁned as follows:\nMask (G,Fi ) = Sigmoid(S(Q, K )) ∈ RN ×L . (3)\nThe masks have a shape of N × L, which can be reshaped to\nN × H\nP × W\nP and bilinearly upsampled to the original image\nsize N ×H ×W . As depicted in the right section of Fig. 3,t h e\nATM mechanism produces masks as an intermediate output\nduring cross-attention.\nThe ﬁnal output tokens Z ∈ RL×C from the ATM module\nare utilized for classiﬁcation. A fully connected layer (FC)\nparameterized by W ∈ RC×2 followed by the Softmax func-\ntion is used to predict whether the object class is present in the\nimage or not. The class predictions P ∈ RN ×2 are formally\ndeﬁned as:\nP = Softmax(WZ ). (4)\nHere, Pc,1 indicates the likelihood of class c appearing in the\nimage. For simplicity, we refer to Pc as the probability score\nfor class c.\nThe output segmentation map for class Os ∈ RH×W\nis obtained by element-wise multiplication of the reshaped\nclass-speciﬁc mask Mc and its corresponding prediction\nscore Pc: Oc = Pc ⊙ Mc. During inference, the label is\nassigned to each pixel i by selecting the class with the high-\nest score using argmax c Oi,c.\nIndeed, plain base models like ViT do not inherently\npossess multiple stages with features of different scales.\nConsequently, structures such as Feature Pyramid Networks\n(FPN) that merge features from multiple scales are not appli-\ncable to them.\nNevertheless, features from layers other than the last one in\nViT contain valuable low-level semantic information, which\ncan contribute to improving performance. In SegViT, we have\ndeveloped a structure that leverages feature maps from differ-\nent layers of ViT to enrich the feature representations. This\nallows us to incorporate and beneﬁt from the rich low-level\nsemantic information present in those feature maps.\nSegViT is trained via the classiﬁcation loss and the binary\nmask loss. The classiﬁcation loss ( L\ncls) minimizes cross-\nentropy between the class prediction and the actual target.\nT h em a s kl o s s( Lmask) consists of a focal loss (Lin et al.,\n2017) and a dice loss (Milletari et al., 2016) for optimizing\nthe segmentation accuracy and addressing sample imbal-\nance issues in mask prediction. The dice loss and focal loss\nrespectively minimize the dice and focal scores between the\npredicted masks and the ground-truth segmentation. The ﬁnal\nloss is the combination of each loss, formally deﬁned as:\nL = L\ncls + λfocalLfocal + λdiceLdice (5)\nwhere λfocal and λdice are hyperparameters that control the\nstrength of each loss function. Previous mask transformer\nmethods such as MaskFormer (Cheng et al., 2021) and DETR\n(Carion et al., 2020) have adopted the binary mask loss and\nﬁne-tuned their hyperparameters through empirical exper-\niments. Hence, for consistency, we directly use the same\nvalues as MaskFormer and DETR for the loss hyperparame-\nters: λ\nfocal = 20.0 and λdice = 1.0.\n3.2 Shrunk Structure for Efficient Plain ViT Encoder\nRecent efforts, such as DynamicViT (Rao et al., 2021),\nTokenLearner (Ryoo et al., 2021), and SPViT (Kong et al.,\n2022), propose token pruning techniques to accelerate vision\ntransformers. However, most of these approaches are specif-\nically designed for image classiﬁcation tasks and, as a result,\ndiscard valuable information. However, when adapting these\ntechniques to semantic segmentation tasks, they may fail to\npreserve high-resolution features that are necessary for accu-\nrate dense prediction tasks.\nIn this paper, we introduce the Shrunk structure. This\nmethod employs query-based down-sampling (QD) to prune\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1133\nFig. 4 Architecture of the proposed query-downsapling (QD) layer\n(blue block) and the query-upsampling (QU) layer (block). The QD\nlayer uses an efﬁcient down-sampling technique (green block) and\nremoves less informative input tokens used for the query. The QU layer\ntakes a set of trainable query tokens and learns to recover the discarded\ntokens using multi-head attention (Color ﬁgure online)\nthe input token sequence Fi and uses query up-sampling\n(QU) to retrieve the discarded tokens, ensuring preservation\nof ﬁne-detail features vital for semantic segmentation. The\noverall architecture of QD and QU is illustrated in Fig. 4.\nFor QD, we have re-designed the Transformer encoder\nblock (Vaswani et al., 2017) and incorporated efﬁcient down-\nsampling operations to speciﬁcally reduce the number of\nquery tokens. In a Transformer encoder layer, the compu-\ntational cost is directly inﬂuenced by the number of query\ntokens, and the output size is determined by the query token\nsize. To mitigate the computational burden while maintain-\ning information integrity, a viable strategy is to selectively\nreduce the number of query tokens while preserving the key\nand value tokens. This approach allows for an effective reduc-\ntion in the output size of the current layer, leading to reduced\ncomputational costs for subsequent layers.\nFor QU, we perform up-sampling using a token sequence\n- either predeﬁned or inherited - that has a higher resolution\nthan the query tokens. The key and value tokens are taken\nfrom the token sequence obtained from the backbone, which\ntypically has a lower resolution. The output size is dictated by\nthe query tokens with higher resolution. Through the cross-\nattention mechanism, information from the key and value\ntokens is integrated into the output. This process facilitates\na non-linear merging of information and demonstrates an\nupsampling behavior, effectively increasing the resolution\nof the output.\nAs illustrated in Fig. 5, our proposed Shrunk structure\nincorporates the QD and QU modules. Speciﬁcally, we inte-\ngrate a QD operation at the middle depth of the ViT backbone,\nprecisely at the 8\nth layer of a 24-layer backbone. The QD\noperation downsamples the query tokens using a 2 ×2 nearest\nneighbor downsampling operation, resulting in a feature map\nsize reduction to 1 /32. However, such downsampling can\npotentially cause information loss and performance degra-\ndation. To mitigate this issue, prior to applying the QD\noperation, we employ a QU operation to the feature map. This\ninvolves initializing a set of query tokens with a resolution of\n1/16 to store the information. Subsequently, as the downsam-\npled feature map progresses through the remaining backbone\nlayers, it is merged and upsampled using another QU oper-\nation alongside the previously stored 1 /16 high-resolution\nfeature map. This iterative process ultimately generates a\nFig. 5 Illustrations of the Shrunk and Shrunk++. In the diagram, the\nand boxes respectively refer to the transformer encoder block and\nthe patch embedding block. In SegVit (Zhang et al., 2022), the pro-\nposed Shrunk structure employs query downsampling (QD) on the\nmiddle-level features to preserve the information. In the new Shrunk++\narchitecture, we introduce the Edged Query Downsampling (EQD) tech-\nnique which consolidates every four adjacent tokens into one token and\nadditionally includes the tokens that contain edges. This enhancement\nenables downsampling operations to take place before the ﬁrst layer\nwithout signiﬁcant performance degradation, offering computational\nsavings for the initial layers of the Shrunk model. The edge information\nis extracted using a lightweight parallel edge detection head\n123\n1134 International Journal of Computer Vision (2024) 132:1126–1147\n1/16 high-resolution feature map enriched with semantic\ninformation processed by the backbone.\nDespite the effectiveness of the proposed Shrunk approach\nin maintaining performance, it requires the integration of the\nQD operation within the intermediate layers of the back-\nbone. This necessity arises due to the fact that shallow layers\nprimarily capture low-level features, and applying downsam-\npling to these layers would result in signiﬁcant information\nloss. Consequently, these low-level layers continue to be\ncomputed at a higher resolution, limiting the potential reduc-\ntion in computational cost.\nTo address this limitation and further optimize the back-\nbone, we introduce SegViTv2 using a novel architecture\ncalled Shrunk++. In this architecture, we incorporate an edge\ndetection module in the QD section and introduce an Edged\nQuery Downsampling (EQD) technique to update the QD\nprocess. In addition to the 2 ×2 nearest downsampling opera-\ntion that eliminates every 4 consecutive tokens, our approach\naims to retain tokens that contain multiple categories, specif-\nically tokens that contain an edge. By preserving the 2 × 2\nsparse tokens, we retain important semantic information,\nwhile also preserving the edge tokens to retain detailed spa-\ntial information. By retaining both types of information, we\nminimize the loss of valuable information and overcome the\nlimitations associated with low-level layers. To extract edges,\nwe add a separate branch using a lightweight multilayer per-\nceptron (MLP) termed as the edge detection head that learns\nto detect edges from the input image. The edge detection\nhead operates as an auxiliary branch, trained simultaneously\nwith the main ATM decoder. This head processes the input\nimage, which has the same dimensions as the backbone. Let\nthe input image have C channels, aligned with the backbone.\nThe Multi-Layer Perceptron (MLP) in this head consists of\nthree layers, with dimensions C, C/2, and 2, respectively. Let\nI represent the input image, and the output of the MLP can be\ndeﬁned as E = MLP(I ; W\n1, W2, W3), where W1, W2, W3\nare the weights for the three layers. The output E is then\npassed through a softmax activation function, resulting in\nS = Softmax(E). To determine the conﬁdence level of a\ntoken belonging to an edge, we apply a threshold τ. In our\nimplementation, we set τ to 0.7. To obtain the ground-truth\n(GT) edge, we perform post-processing on the GT segmenta-\ntion map Y . Since the input has been tokenized with a patch\nsize of P, we tokenize the GT and reshape it into a sequence\nof tokens denoted as Y ∈ R\n(HW /P2)×P×P , where the last two\ndimensions correspond to the patch dimensions. We consider\na patch to contain an edge if there exists any edge pixel within\nthe patch. We deﬁne the edge mask Mask\ni as follows:\nMask i =\n{\n1i f ∑\nj,k Yi,j,k > 0,\n0 otherwise . (6)\nFor each element si in S, we create a binary edge mask\nMi : Mi = 1,if si ≥ τ. The cross-entropy loss is computed\nbetween the generated edge mask Mi and the ground-truth\nedge mask Yi : Ledge =− ∑ iYi log(Mi ) + (1 − Yi )log(1 −\nMi ). By incorporating the Edge Detection head as an aux-\niliary branch, the Shrunk++ architecture effectively retains\ndetailed spatial contexts throughout the query downsam-\npling process, forming an Edge Query Downsampling (EQD)\nstructure. This EQD structure effectively captures and retains\nedge information during sparse downsampling, signiﬁcantly\nreducing computational overhead while maintaining per-\nformance. The integration of EQD enables the Shrunk++\narchitecture to strike a remarkable balance between compu-\ntational efﬁciency and maintaining high-performance levels.\n3.3 Exploration on Continual Semantic\nSegmentation\nContinual semantic segmentation aims to train a segmenta-\ntion model in T steps without forgetting. At step t,w ea r e\ngiven a dataset Dt which comprises a set of pairs (Xt , Y t ),\nwhere Xt is an image of size H × W and Y t is the ground-\ntruth segmentation map. Here, Y t only consists of labels in\ncurrent classes Ct , while all other classes (i.e., old classes\nC1:t−1 or future classes Ct+1:T ) are assigned to the back-\nground. In continual learning, the model at step t should be\nable to predict all classes C1:t in history.\n3.3.1 SegViT for Continual Learning\nExisting continual semantic segmentation methods (Zhang et\nal., 2022; Phan et al., 2022) propose regularization algorithms\nto preserve the past knowledge of a speciﬁc architecture,\nDeepLabV3. These methods focus on continual seman-\ntic segmentation for DeepLabV3 with a ResNet backbone,\nwhich has a less robust visual representation for distin-\nguishing between different categories. Consequently, these\nmethods require ﬁne-tuning model parameters to learn new\nclasses while attempting to retain knowledge of old classes.\nUnfortunately, adapting the old parameters dedicated to the\nprevious task inevitably interferes with past knowledge, lead-\ning to catastrophic forgetting. In contrast, our proposed\nSegViT decouples class prediction from mask segmentation,\nmaking it inherently suitable for a continual learning setting.\nBy leveraging the powerful representation capability of the\nplain vision transformer, we can learn new classes by solely\nﬁne-tuning the class proxy (i.e., the class token) while keep-\ning the old parameters frozen. This approach eliminates the\nneed for ﬁne-tuning old parameters when learning new tasks,\neffectively addressing the issue of catastrophic forgetting.\nDuring training on the current task t, we add a new\nsequence of learnable tokens G\nt ∈ R|Ct |×C , where |Ct | is the\nnumber of classes in the current task. To learn new classes,\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1135\nFig. 6 Overview of SegViT adapted for continual semantic segmenta-\ntion. When learning a new task t, we grow and train a separate ATM\nand fully-connected layer to produce mask and class prediction. All the\nparameters dedicated to the old task t − 1, including ATM, FC layers,\nand the ViT encoder, are frozen. This prevents interfering with the old\nknowledge, which guarantees no forgetting\nwe grow and train new ATM modules and a fully-connected\nlayer for mask prediction and mask classiﬁcation. For sim-\nplicity, we ignore the parallel structure of ATM modules. A\nsingle ATM module refers to multiple ATM modules. Let\nAt and W t denote the ATM module and the weights of the\nfully connected (FC) layer for task t. All parameters for\nprior tasks, including the ViT encoder, the ATM module, and\nthe FC layer, are completely frozen. Figure 6 illustrates the\noverview of our SegViT architecture adapted for continual\nsemantic segmentation.\nGiven the encoder extracted features FT and the class\ntokens Gt , the ATM produces the mask predictions Mt and\nthe output tokens Zt corresponding to the mask:\nMt , Zt = ATM(Gt ,F T ). (7)\nB a s e do nE q .(4), the class prediction P is obtained by apply-\ning FC on the class token Zt .\nThe prediction score St\nc for each class c is multiplied by\nthe corresponding mask Mt\nc to get the segmentation map Ot\nc\nfor class c:\nOt\nc = St\nc ⊙ Mt\nc, (8)\nwhere ⊙ denotes the element-wise multiplication. The seg-\nmentation ˆOt is obtained by taking the class c having the\nhighest score in every pixel, deﬁned as\nˆOt = argmax\nc∈Ct\nOt\ni,c (9)\nBased on the ground truth Y t for task t, SegViT is trained\nusing the loss function deﬁned in Eq. ( 5). To produce the ﬁnal\nsegmentation across all tasks, we concatenate the individual\noutputs O\nt from each task.\n4 Experiments\n4.1 Datasets\nADE20K (Zhou et al., 2017) is a challenging scene parsing\ndataset which contains 20 ,210 images as the training set and\n2,000 images as the validation set with 150 semantic classes.\nCOCO-Stuff-10K (Caesar et al., 2018) is a scene pars-\ning benchmark with 9 ,000 training images and 1 ,000 test\nimages. Even though the dataset contains 182 categories, not\nall categories exist in the test split. We follow the implemen-\ntation of mmsegmentation (MMSegmentation, 2020) with\n171 categories to conduct the experiments.\nPASCAL-Context (Mottaghi et al., 2014) is a dataset with\n4,996 images in the training set and 5 ,104 images in the\nvalidation set. There are 60 semantic classes in total, includ-\ning a class representing ‘background’.\n4.2 Implementation Details\n4.2.1 Transformer Backbone\nWe employ the naive ViT (Dosovitskiy et al., 2021)a st h e\nbackbone for our method. For our ablation studies, we pri-\nmarily utilize the ‘Base’ variation, while also presenting\nresults based on the ‘Large’ variant. Notably, variations in\nperformance can arise due to different pre-trained weights,\nas indicated by Segmenter (Strudel et al., 2021). To ensure\nequitable comparisons, we adopt the pre-trained weights\nprovided by Augreg (Steiner et al., 2021), aligning with\npractices employed in Strudel (Strudel et al., 2021) and\nStructToken (Lin et al., 2022). These weights stem from\ntraining on ImageNet-21k with strong data augmentation and\nregularization techniques (Steiner et al., 2021). To explore\nthe maximum capacity and assess the upper bound of our\nmethod, we also conduct experiments using stronger base\nmodels such as DEiT v3 (Touvron et al., 2022) and BEiT v2\n(Peng et al., 2022).\n4.2.2 Training Settings\nWe use MMSegmentation (MMSegmentation, 2020) and fol-\nlow the commonly used training settings. During training,\nwe apply sequential data augmentation techniques, includ-\ning random horizontal ﬂipping, random resizing within a\nratio of 0 .5t o2 .0, and random cropping. For most settings,\nthe cropping dimensions are set to 512 × 512, except for\nPASCAL-Context where we use 480 × 480, and for ViT-\nlarge backbone on ADE20K where we use 640 × 640. The\nbatch size is set to 16 for all datasets with a total iteration\nof 160 k,8 0 k, and 80 k for ADE20k, COCO-Stuff-10k, and\nPASCAL-Context respectively.\n123\n1136 International Journal of Computer Vision (2024) 132:1126–1147\nTable 1 Experiment results on the ADE20K val. split\nMethod Backbone Crop size GFLOPs mIoU (ss) mIoU (ms) Inf time (fps)\nUPerNet (Xiao et al., 2018) ViT-Base 512 × 512 443 .9 46.6 47.5 16 .07\nDPT* (Ranftl et al., 2021) ViT-Base 512 × 512 219 .8 47.2 47.9 23 .63\nSETR-MLA* (Zheng et al., 2021) ViT-Base 512 × 512 113 .5 48.2 49.3 −\nSegmenter* (Strudel et al., 2021) ViT-Base 512 × 512 129 .6 49.0 50.0 20 .46\nStructToken (Lin et al., 2022) ViT-Base 512 × 512 171 .5 50.9 51.8 14 .22\nMaskFormer (Cheng et al., 2021) Swin-B(21K) 640 × 640 198 .3 52.7 53.9 −\nMask2Former (Cheng et al., 2022) Swin-B(21K) 640 × 640 223 .4 53.9 55.1 12.43\nSegViT (ours) ViT-Base 512 × 512 120 .9 51.3 53.0 31 .52\nSegViT (Shrunk++, Ours) BEiTv2-Base 512 × 512 74.4 52.9 53.3 25 .03\nSegViT (ours) BEiTv2-Base 512 × 512 120 .9 54.0 54.9 23.59\nDPT* (Ranftl et al., 2021)V i T - L a r g e † 640 × 640 800 .0 49.2 49.5 9 .38\nUPerNet (Xiao et al., 2018)V i T - L a r g e † 640 × 640 1993 .9 48.6 50.0 3 .88\nSETR-MLA (Zheng et al., 2021) ViT-Large 512 × 512 368 .6 48.6 50.3 5 .17\nMCIBI (Jin et al., 2021) ViT-Large 512 × 512 > 400 – 50.8 −\nSegmenter (Strudel et al., 2021)V i T - L a r g e † 640 × 640 671 .8 51.8 53.6 4 .73\nStructToken (Lin et al., 2022)V i T - L a r g e † 640 × 640 774 .6 52.8 54.2 4 .1\nKNet+UPerNet (Zhang et al., 2021) Swin-L(21K) 640 × 640 659 .3 52.2 53.3 11 .28\nMaskFormer (Cheng et al., 2021) Swin-L(21K) 640 × 640 378 .1 54.1 55.6 10 .21\nMask2Former (Cheng et al., 2022) Swin-L(21K) 640 × 640 402 .7 56.1 57.3 8 .81\nSegViT (ours) ViT-Large † 640 × 640 637 .9 54.6 55.2 9 .37\nSegViT(Shrunk, ours) ViT-Large † 640 × 640 373 .5 53.9 55.1 10 .18\nSegViT(Shrunk++, ours) ViT-Large † 640 × 640 209 .1 53.0 54.9 10 .26\nSegViT (Shrunk++, ours) BEiTv2-Large 512 × 512 210 .3 55.1 56.1 9 .82\nSegViT (ours) BEiTv2-Large 512 × 512 374 .0 56.5 58.0 9.39\nSegViT (Shrunk++, ours) BEiTv2-Large 640 × 640 308 .8 55.7 57.0 9 .38\nSegViT (ours) BEiTv2-Large 640 × 640 637 .9 58.05 8 .2 6.25\nWe have utilized bold text in the tables to highlight the best or state-of-the-art (SOTA) benchmarks. ‘ms’ means that mIoU is calculated using\nmulti-scale inference. ‘†’ means the models use the backbone weights pre-trained by AugReg (Steiner et al., 2021). ‘*’ represents the model\nreproduced under the same settings as the ofﬁcial repo. The GFLOPs are measured at single-scale inference with the given crop size. We report\ninference speed for our SegViT and reproduce previous methods in terms of Frame Per Second (FPS) on a single A100 device\n4.2.3 Evaluation Metric\nWe use the mean Intersection over Union (mIoU) as the\nmetric to evaluate the performance. ‘ss’ means single-scale\ntesting and ‘ms’ test time augmentation with multi-scaled\n(0.5,0.75,1.0,1.25,1.5,1.75) inputs. All reported mIoU\nscores are in a percentage format. All reported computational\ncosts in GFLOPs are measured using the fvcore\n1 library.\n4.3 Comparisons with the State-of-the-Art Methods\n4.3.1 Results on ADE20K\nTable1 reports the comparison with the state-of-the-art meth-\nods on ADE20K validation set using ViT backbone. The\nSegViT uses the ATM module with multi-layer inputs from\n1 https://github.com/facebookresearch/fvcore\nthe original ViT backbone, while the Shrunk is the one that\nconducts QD to the ViT backbone and saves 50% of the com-\nputational cost without sacriﬁcing too much performance.\nOur approach achieves a state-of-the-art mIoU of 58 .2%\n(MS) with the BEiTv2 Large backbone. To ensure a fair com-\nparison, we evaluate our SegViT module with the BEiT-v2\nlarge backbone on a crop size of 512 × 512, which con-\nsumes 374.0 GFlOPs. Our approach achieves a slightly better\nperformance of 56 .5% mIoU compared to Mask2former-\nSwin-L, which achieves 56 .1% with 402.7 GFlops on a\ncrop size of 640 × 640. Additionally, our Shrunk version\noffers around a 50% reduction in computational cost (308.8\nGFLOPs), while delivering competitive performance with a\nmIoU of 57 .0% (MS). Optimizing SegViT with ViT-Large\nusing the proposed Shrunk++ reduces the computational\ncost of Shrunk by 3.05 times, while preserving the mIoU.\nFigure 7 shows the visual results of different segmentation\nmethods. In contrast to other methods that often confuse\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1137\nFig. 7 Visuals results of\ndifferent segmentation networks\nand plain ViT backbones on the\nADE20K validation set (Zhou et\nal., 2017). It includes the\nfollowing models: a Segmenter\n(Strudel et al., 2021) with ViT\nlarge, b StructToken (Lin et al.,\n2022) with ViT large, c UPerNet\n(Xiao et al., 2018) with BEiT\nlarge, and d SegViT V2 with\nBEiTv2 large. The results\ndemonstrate that our methods\neffectively generate accurate\nsegmentation masks and unlock\nthe potential of plain ViT. Zoom\nin for a better view\nsimilar classes and misclassify related concepts, our SegViT\nstands out by more precise object boundary delineation and\nachieving accurate segmentation of complete objects, even\nin cluttered scenes.\n4.3.2 Results on COCO-Stuff-10K\nTable2 shows the result on the COCO-Stuff-10K dataset. Our\nmethod achieves 50 .3% which is higher than the previous\nstate-to-the-art StrucToken by 1 .2% with less computational\ncost. Our Shrunk version achieves 49 .4% mIoU with 224 .8\nGFLOPs, which is similar to the computational cost of a\ndilated ResNet-101 backbone but with much higher perfor-\nmance. By extending SegViT with the effective Shrunk++,\nwe signiﬁcantly decrease its GFLOPs by 1.82 times, while\nretaining a competitive mIoU.\n4.3.3 Results on PASCAL-Context\nTable 3 shows the results on the PASCAL-Context dataset.\nWe follow HRNet (Sun et al., 2019) to evaluate our method\nand report the results under 59 classes (without background)\n123\n1138 International Journal of Computer Vision (2024) 132:1126–1147\nTable 2 Experiment results on\nthe COCO-Stuff-10K test.\nsplit\nMethod Backbone GFLOPs mIoU (ms)\nDANet (Fu et al., 2019) Dilated-ResNet-101 289.3 39.7\nMaskFormer (Cheng et al., 2021) ResNet-101-fpn 81.7 39.8\nEMANet (Li et al., 2019) Dilated-ResNet-101 247.4 39.9\nSpyGR (Li et al., 2020) ResNet-101-fpn >80 39.9\nOCRNet (Yuan et al., 2020) HRNetV2-W48 167.9 40.5\nGINet (Wu et al., 2020) JPU-ResNet-101 >200 40.6\nRecoNet (Chen et al., 2020) Dilated-ResNet-101 >200 41.5\nISNet (Jin et al., 2021) Dilated-ResNeSt-101 228.3 42.1\nMCIBI (Jin et al., 2021)V i T - L a r g e >380 44.9\nS t r u c t T o k e n( L i ne ta l . ,2022)V i T - L a r g e >400 49.1\nSenFormer (Bousselham et al., 2021)S w i n - L a r g e >400 50.1\nSegViT (Shrunk, ours) ViT-Large 224.8 49.40\nSegViT (ours) ViT-Large 383.9 50.30\nSegViT (Shrunk++, ours) BEiTv2-Large 213.3 50.54\nSegViT (ours) BEiTv2-Large 388.2 53.46\nWe have utilized bold text in the tables to highlight the best or state-of-the-art (SOTA) benchmarks. Following\npublished methods, we report the results with multi-scale inference (denoted by ‘ms’). The GFLOPs is\nmeasured at single scale inference with a crop size of 512 × 512\nTable 3 Experimental results on the PASCAL-Context val. split\nMethod Backbone GFLOPs mIoU 59 (ms) mIoU 60 (ms)\nReﬁneNet (Lin et al., 2017) ResNet-152 − – 47.3\nUNet++ (Zhou et al., 2018) ResNet-101 − 47.7 –\nPSPNet (Zhao et al., 2017) Dilated-ResNet-101 157 .0 47.8 –\nDing et al. (Ding et al., 2018) ResNet-101 − 51.6 –\nEncNet (Zhang et al., 2018) Dilated-ResNet-101 192 .1 52.6 –\nHRNet (Sun et al., 2019) HRNetV2-W48 82 .7 54.0 48.3\nNRD (Zhang et al., 2021) ResNet-101 42 .9 54.1 49.0\nGFFNet (Li et al., 2020) Dilated-ResNet-101 − 54.3 –\nEfﬁcientFCN (Liu et al., 2020) ResNet-101 52 .8 55.3 –\nOCRNet (Yuan et al., 2020) HRNetV2-W48 143 .9 56.2 –\nSETR-MLA (Zheng et al., 2021) ViT-Large 318 .5 – 55.8\nSegmenter (Strudel et al., 2021) ViT-Large 346 .2 – 59.0\nSenFormer (Bousselham et al., 2021)S w i n - L a r g e − 64.0 –\nSegViT (Shurnk, ours) ViT-Large 186 .9 62.3 57.40\nSegViT (ours) ViT-Large 321 .6 65.3 59.30\nSegViT (Shurnk++, ours) BEiTv2-Large 179.3 64.91 59.92\nSegViT (ours) BEiTv2-Large 329 .7 67.14 61 .63\nWe have utilized bold text in the tables to highlight the best or state-of-the-art (SOTA) benchmarks. Following published methods, we report the\nresults with multi-scale inference (denoted by ‘ms’). mIoU 59: mIoU averaged over 59 classes (without background). mIoU 60: mIoU averaged over\n60 classes (59 classes plus background). Both metrics were used in the literature, and we report for the 60 classes. The GFLOPs are measured at\nsingle scale inference with a crop size of 480 × 480\nand 60 classes (with background). Using full SegViT struc-\nture without adopting Shrunk or Shrunk++, we reach mIoU\nof 67 .14% and 61 .63% respectively for those two met-\nrics, outperforming the state-of-the-art methods using the\nViT backbones with less computational cost. By applying\nShrunk and Shrunk++ architecture, the computational cost\nin terms of GLOPs is reduced by 42% and 45%, respectively.\nAmong all approaches evaluated on the PASCAL-Context\ndataset, SegViTv2 with Shrunk++ achieves the best trade-\noff between accuracy and efﬁciency.\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1139\nTable 4 Comparisons between our proposed ATM module with SETR\n(Zheng et al., 2021)\nDecoder Loss mIoU (ss)\nSETR CE loss 46.5\nATM CE loss 47.0 (+ 0.5)\nATM\nLmask loss 49.6 (+ 3.1)\nWe have utilized bold text in the tables to highlight the best or state-\nof-the-art (SOTA) benchmarks. ‘CE loss’ indicates the cross-entropy\nloss commonly used in semantic segmentation. The experiments on the\nADE20k dataset are carried out using the ViT-Base backbone\n4.4 Ablation Study\nIn this section, we conduct extensive ablation studies to show\nthe effectiveness of our proposed methods.\n4.4.1 Effect of the ATM Module\nWe conducted an analysis to evaluate the impact of using\nthe proposed ATM module as an encoder. The results are\nsummarized in Table 4. To establish a baseline for compar-\nison, we introduced SETR-naive, which utilizes two 1 × 1\nconvolutions to directly derive per-pixel classiﬁcations from\nthe ﬁnal layer of the ViT-Base transformer output. From the\nresults, it is evident that applying the ATM module under\nthe supervision of a conventional cross-entropy loss leads\nto a performance improvement of 0.5%. However, the per-\nformance gains become much more substantial when we\ndecouple the classiﬁcation and mask prediction processes,\nsupervising each separately. This approach results in a sig-\nniﬁcant performance boost of 3.1%, highlighting the efﬁcacy\nof the ATM module in enhancing semantic segmentation per-\nformance.\n4.4.2 Ablation of the Feature Levels\nThe effects of using multiple-layer inputs from the back-\nbone to the ATM modules are presented in Table 5.T h e\nincorporation of feature maps from lower layers leads to a\nnotable performance improvement of 1.3%. We further inves-\ntigated the impact of including more layers of features and\nobserved additional gains in performance. After empirical\ntesting, we determined that utilizing three layers yielded opti-\nmal results, resulting in an overall mIoU boost of 1.7%. These\nablation studies conﬁrm the effectiveness of our proposed\nATM decoder and highlight the advantage of incorporat-\ning multi-layer features into the segmentation structure. This\nintegration signiﬁcantly enhances the performance of seman-\ntic segmentation tasks.\nTable 5 Results of using different layer inputs to the SegViT structure\non ADE20K dataset using ViT-Base as the backbone\nUsed layers mIoU (ss)\nSingle layer [12] 49.6\nCascade [6, 12] 50.9 (+ 1.3)\nCascade [6, 8, 12] 51.3 (+ 1.7)\nCascade [3, 6, 9, 12] 51.2 (+ 1.6)\nWe have utilized bold text in the tables to highlight the best or state-\nof-the-art (SOTA) benchmarks. Involving multi-layer features leads to\nobvious performance gains\nTable 6 The experiments use the Swin-Tiny (Liu et al., 2021) backbone\nand are carried out on the ADE20K dataset\nMethod mIoU (ss) GFLOPs\nMaskformer (Cheng et al., 2021) 46.7 57.3\nMask2former (Cheng et al., 2022) 47.7 73.7\nSegViT (ours) 47.1 48.0\nWe have utilized bold text in the tables to highlight the best or state-of-\nthe-art (SOTA) benchmarks. The GFLOPs are measured at single-scale\ninference with a crop size of 512 × 512\n4.4.3 SegViT on Hierarchical Base Models\nWe conducted an analysis to evaluate the performance of\nSegViT on hierarchical base models. For comparison, we\nselected two competitive methods, Maskformer (Cheng et\nal., 2021) and Mask2former (Cheng et al., 2022). The results\npresented in Table 6 indicate that, even though our method\nwas not speciﬁcally designed for hierarchical base models,\nwe are still able to achieve competitive performance while\nmaintaining computational efﬁciency. This demonstrates the\napplicability of our SegViT approach to various types of ViT-\nBase models.\n4.4.4 Ablation of Shrunk and Shrunk++ Strategies\nIn this section, we analyze the effectiveness of the different\nSegViT structures. Table 7 presents the effects of various\ntechniques employed in each SegViT structure, including\nquery upsampling (QU), query downsampling (QD), token-\nsqueezing (TS) techniques, and segmentation heads. Apply-\ning the ATM head to the ’Single’ structure yields a notable\nperformance improvement of 6.67% compared with using the\nSETR head. This demonstrates the effectiveness of the ATM\nhead in enhancing the performance of the baseline struc-\nture. However, applying QD to the ’Single’ structure with the\nATM head leads to a performance drop of 2.7%, suggesting\nthe occurrence of information loss during the downsampling\n123\n1140 International Journal of Computer Vision (2024) 132:1126–1147\nTable 7 Ablation results of\nShrunk and Shrunk++ version\non the ADE20K dataset\nStructure QD QU QD layer QD method Head mIoU (ss) GFLOPs\nSingle – – – – SETR 46.5 107.3\nSingle – – – – ATM 49.6 (+ 3.1) 115.8\nNaive Shrunk ✓ –6 2 × 2 ATM 46.9 (+ 0.4) 74.1\nShrunk ✓✓ 62 × 2A T M 50.0 (+ 3.5) 97.1\nNearest-TS ✓✓ 03 × 3 ATM 38.9( − 7.6) 32.8\nNearest-TS ✓✓ 02 × 2 ATM 43.3( −3.2) 46.1\nShrunk++ ✓✓ 03 × 3-Edge ATM 47.9(+ 1.4) 69.3\nShrunk++ ✓✓ 02 × 2-Edge ATM 49.9(+ 3.4) 74 .6\nWe have utilized bold text in the tables to highlight the best or state-of-the-art (SOTA) benchmarks. We\nexplored various shrink strategies. The GFLOPs are measured at single-scale inference with a crop size of\n512×512 on the ViT-Base backbone. QD: query-based downsampling. QU: query-based upsampling. QD\nlayer\nindicates which layer to apply the QD. QD method indicates the downsampling method for QD\nTable 8 Ablation results of different decoder methods with their corresponding feature merge types and loss types\nMulti-level Features Loss Types\nDecoder FPN Token Merge Pixel level Dot product Attention Mask mIoU (ss)\nSETR-MLA (Zheng et al., 2021) ✓✓ 48.2\nSegmenter (Strudel et al., 2021) ✓ 49.0\nMaskFormer (Cheng et al., 2021) ✓✓ 46.7\nOurs-Variant 1 ✓ 49.6\nOurs-Variant 2 ✓✓ 50.6\nOurs ✓✓ 51.2\nViT-Base is employed as the backbone for all the variants\nphase. Importantly, incorporating QU restores the perfor-\nmance. QU helps recover the discarded information from QD\nand reconstructs the high-resolution feature map, which is\ncrucial for dense prediction tasks. Jointly leveraging QU and\nQD, the Shrunk architecture achieves optimal performance\nwhile reducing computational costs by 16.15% in compari-\nson to the ‘Single’ structure.\nIn the proposed Shrunk++ structure, we analyze the per-\nformance of two main token-squeezing techniques: nearest\ndownsampling and edge-aware downsampling. It is impor-\ntant to note that token squeezing is directly applied to the\nﬁrst layer of the network for optimal computational efﬁ-\nciency. Applying naive nearest downsampling with a 3x3\nkernel reduces the GFLOPs of the Shrunk structure with-\nout token-squeezing by a factor of 2.97. However, reducing\nthe computational cost with 3x3 and 2x2 nearest downsam-\npling leads to a performance drop of 13%. In contrast, by\nincorporating an additional edge extractor into our Shrunk++\narchitecture, we signiﬁcantly improve the mIoU, achieving\nperformance on par with Shrunk, i.e., 49.9% mIoU, with\na minor increase in computational cost to 74.6 GFLOPs.\nThe edge-aware downsampling technique preserves the edge\ndetails, thereby preserving discriminative features for dense\npredictions. Among the different settings, the 2x2 + Naive\nMLP Edge setting achieves an optimal balance between per-\nformance and efﬁciency.\n4.4.5 Ablation Studies on Decoder Variances\nDifferent decoder methods are associated with speciﬁc fea-\nture merge types and loss types. In Table 8, we compare\nthe designs of various decoders on a plain ViT backbone.\nFor hierarchical base models like Swin, the resolution of\nthe feature maps in each stage is reduced. Consequently, the\nadoption of a Feature Pyramid Network (FPN) is necessary to\nobtain feature maps with larger resolutions and rich seman-\ntic information. However, in Table 8, we observe that the\nFPN structure does not perform well with plain vision trans-\nformers. With plain ViT base models, the resolution remains\nconstant, and the feature map of the ﬁnal layer encapsulates\nthe most comprehensive semantic information. Hence, our\nproposed method, which utilizes tokens to merge features\nfrom different levels, achieves superior performance. By sim-\nply replacing the FPN structure with the ATM-based token\nmerge, we improve the performance from 46.7% to 50.6%.\nRegarding the loss type, the pixel-level loss refers to the con-\nventional cross-entropy loss applied to the feature map. The\ndot product loss corresponds to the loss utilized in Carion et\nal. (2020) and Cheng et al. ( 2021). Attention mask loss indi-\ncates the direct application of mask supervision to the similar-\nity map generated by the ATM during attention calculation.\nIncorporating loss supervision on the attention mask, as in\nour method, leads to a performance improvement of 0.6%.\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1141\nTable 9 Ablation of the QD module in terms of the targets and methods\nto down-sample\nApplied to Methods mIoU (ss)\nQ Conv 44.5\nQ, K, V Nearest 52.6\nQ Nearest 53.9\nWe have utilized bold text in the tables to highlight the best or state-\nof-the-art (SOTA) benchmarks. The experiments are carried out on the\nViT-Large backbone of ADE20K dataset\n4.4.6 Ablation for the QD Module\nThe motivation behind using QD is to leverage the pre-\ntrained weights of the backbone. As shown in Table 9,u s i n ga\nstride-2 convolution with learnable parameters to downsam-\nple the query will disturb the pre-trained weights, leading to\na notable decline in performance. Applying down-sampling\nto both the query and the key-value pairs would inevitably\nlead to information loss during the down-sampling process,\nwhich is evident in the lower performance. Our results show\nthat applying 2 ×2 nearest down-sampling exclusively to the\nquery in the QD module yields better results. This approach\nallows us to preserve the pre-trained weights of the backbone\nwhile achieving the desired down-sampling effect.\n4.5 Application 1: A Better Indicator for Feature\nRepresentation Learning\n4.5.1 Background\nSemantic segmentation serves as a fundamental vision task\nthat has been extensively employed in previous research\nto assess the representation learning capabilities of weakly,\nfully, and self-supervised base models (He et al., 2022; Chen\net al., 2022; Touvron et al., 2022; Peng et al., 2022). In prior\nwork, the UPerNet decoder structure has been commonly\nused for semantic segmentation. However, the UPerNet\ndecoder may not be a suitable indicator for evaluating the\nfeature representation ability of the base model. This is pri-\nmarily due to its heavier computational requirements and\nslower convergence rate. Additionally, variations in feature\nrepresentation acquired by the base model can be substantial\ndue to diverse training strategies during the ﬁne-tuning pro-\ncess on semantic segmentation datasets Consequently, the\ntask of semantic segmentation may not adequately evaluate\nthe feature representation ability of pre-trained models.\n4.5.2 Experiment Settings\nIn this section, we extensively evaluate our proposed SegVit\nacross diverse weakly, fully, and self-supervised vision trans-\nformers, including those proposed by He et al. He et al.\n(2022), Chen et al. Chen et al. ( 2022), Touvron et al. Tou-\nvron et al. ( 2022), and the BEiT model Peng et al. ( 2022).\nWe demonstrate that our method outperforms UPerNet Xiao\net al. ( 2018) in both self-supervised and multi-modality base\nmodels, achieving state-of-the-art performance. Notably, our\napproach achieves superior performance to UPerNet while\nutilizing only 5% of the computational cost in terms of the\ndecoder head. Table 10 illustrates that our proposed SegViT\nhead consistently outperforms UPerNet across all base mod-\nels. For the ViT-Base, our method improves the performance\nof UPerNet on the CLIP model by 1.16% while signiﬁcantly\nreducing the computational cost. Similar ﬁndings are evi-\ndent for ViT-Large base models. Furthermore, compared to\nUPerNet, our proposed SegViT’s decoder head exhibits a\nbetter alignment between the growth trend of segmentation\naccuracy and the classiﬁcation accuracy on ImageNet. This\nclearly demonstrates the superior efﬁciency of our SegViT\nTable 10 Comparisons for various ViT pre-training schedules on the validation set of ADE20K\nBackbone SegViT mIoU Head FLOPs UPerNet mIoU Head FLOPs ImageNet Acc\nMAE Base (He et al., 2022) 49.22 ( ▲ 1.12) 6.89 ( ▼ 329.73) 48.1 336.62 83.66\nCLIP Base (Radford et al., 2021) 50.76 ( ▲ 1.16) 6.89 ( ▼ 329.73) 49.6 336.62 80.20\nCAE Base (Chen et al., 2022) 50.42 ( ▲ 0.22) 6.89 ( ▼ 329.73) 50.2 336.62 83.90\niBot Base (Zhou et al., 2022) 50.58 ( ▲ 0.58) 6.89 ( ▼ 329.73) 50.0 336.62 84.00\nAugreg Base*† (Steiner et al., 2021) 51.30 ( ▲ 2.66) 6.89 ( ▼ 329.73) 48.6 336.62 85.49\nDEiT v3 Base † (Touvron et al., 2022) 52.40 ( ▲ 0.60) 6.89 ( ▼ 329.73) 51.8 336.62 85.70\nBEiT v2 Base † (Peng et al., 2022) 53.97 ( ▲ 0.47) 6.89 ( ▼ 329.73) 53.5 336.62 86.50\nAugreg Large*† (Steiner et al., 2021) 54.60 ( ▲ 2.50) 16.36 ( ▼ 1,366.33) 52.1 1382.69 85.59\nDEiT v3 Large* † (Touvron et al., 2022) 55.81 ( ▲ 1.21) 16.36 ( ▼ 1,366.33) 54.6 1382.69 87.70\nBEiT v2 Large † (Peng et al., 2022) 58.00 ( ▲ 1.30) 16.36 ( ▼ 868.28) 56.7 884.64 87.30\nAll results are reported in single-scale inference. The default conﬁguration for these base models is pre-trained on ImageNet-1K with 224*224\nresolutions. ‘*’ means the models use the backbone weights pre-trained with 384 * 384 resolutions. ’ †’ means the base models pre-trained on\nimagenet-21K. The proposed SegVit head has a less computational cost and performs better than UPerNet among all pre-training variants\n123\n1142 International Journal of Computer Vision (2024) 132:1126–1147\nhead compared to UPerNet, making it a more suitable indi-\ncator for representation learning in base models.\n4.6 Application 2: Continual Semantic Segmentation\nThe decoupling of class prediction and mask segmentation in\nour proposed SegVit decoder makes it inherently well-suited\nfor continual learning settings. This characteristic allows us\nto learn new classes by solely ﬁne-tuning the class proxy (the\nclass token), leveraging the powerful representation ability of\nthe plain vision transformer while keeping the old parameters\nfrozen. To validate the effectiveness of this new approach\nto continual learning, we conducted experiments following\nstandard settings adopted by prior studies.\n4.6.1 Experiment Settings\nContinual Semantic Segmentation (CSS) has two settings\n(Cermelli et al., 2020; Douillard et al., 2021): disjoint and\noverlapped. In the disjoint setup, all pixels in the images\nat each step belong to the previous classes or the current\nclass. In the overlapped setting, the dataset of each step con-\ntains all the images that have pixels of at least one current\nclass, and all pixels from previous and future tasks are labeled\nas background. The overlapped setting is more realistic and\nchallenging, thus we evaluate the performance of the over-\nlapped setup on the ADE20k dataset.\nFollowing prior studies (Phan et al., 2022; Cermelli et al.,\n2020; Douillard et al., 2021), we perform three experiments:\nadding 50 classes after training with 100 classes (100–50\nsetting with 2 steps), adding 50 classes each time after train-\ning with 50 classes (50–50 setting with 3 steps), adding 10\nclasses each time sequentially after training with 100 classes\n(100–10 setting with 6 steps).\n4.6.2 Baselines\nWe conducted a comprehensive comparison of our pro-\nposed method against state-of-the-art Continual Semantic\nSegmentation (CSS) techniques, including RCIL (Zhang et\nal., 2022), PLOP (Douillard et al., 2021), REMINDER (Phan\net al., 2022), SDR (Michieli & Zanuttigh, 2021), and MiB\n(Cermelli et al., 2020). To ensure fair comparisons, exist-\ning methods were evaluated using DeepLabV3 (Chen et al.,\n2017) with ResNet101 and ViT-Base backbones that were\npre-trained on ImageNet-21k. The reported results for PLOP,\nRCIL, and REMINDER were obtained based on the code-\nbases provided by the respective authors. Furthermore, we\nincluded the performance of the Oracle model, which rep-\nresents the upper bound achieved by jointly training on all\navailable data, serving as a benchmark for each method.\n4.6.3 Metrics\nWe evaluate the model performance by ﬁve mIoU metrics.\nFirst, we compute mIoU for the base classes C\n0, which\nreﬂects model rigidity: the model’s resilience to catastrophic\nforgetting. Second, we compute mIoU for all incremented\nclasses C\n1:T , which measures plasticity: the model capacity in\nlearning new tasks. Third, we compute the mIoU of all classes\nin C0:T (all), which shows the overall performance of mod-\nels. Fourth, we report the average of mIoU ( avg) measured\nstep after step as proposed by Douillard et al. ( 2021), which\nevaluates performance over the entire continual learning pro-\ncess. To ensure fair comparisons, we evaluate the relative\nperformance of each CSS method in terms of relative mIoU\nreduction compared with its Oracle model, jointly trained on\nall data.\n5 Results and Discussion\nTable 11 shows the results of different CSS methods on\nADE20k. Our SegViT-CL consistently outperforms exist-\ning methods in all mIoU for both settings. In terms of\nmIoU reduction, the proposed SegViT-CL only decreases\nthe mIoU of the Oracle model by 2 .2% on the 100–50 set-\nting, which is two times better than the second-best method,\nRCIL with ResNet backbone with 4 .6% reduction. This sub-\nstantial enhancement over existing methods underlines the\neffectiveness of our proposed method in the continual seman-\ntic segmentation paradigm. On a long CL setting 100–10 with\n6 tasks, ours is almost forgetting-free with a marginal mIoU\nreduction of 0 .3%, while recent CSS methods signiﬁcantly\nsuffer from forgetting with at least 5 .4% mIoU reduction.\nUsing the ViT backbone, existing methods including MiB,\nREMINDER, and PLOP still suffer from high mIoU reduc-\ntions. Compared with the Oracle, MiB (Cermelli et al., 2020),\nPLOP (Douillard et al., 2021), and REMINDER (Phan et al.,\n2022) decrease the mIoU by 8.6%, 6.5% and 5.6% respec-\ntively on the 100–10 setting, demonstrating the sub-optimal\nperformance of current CSS methods for ViT architecture.\nThis highlights the need for developing a specialized ViT\narchitecture that is robust to forgetting.\nTo evaluate the forgetting of every task on the 100–10\nsetting, we compute the performance drop at the last step\ncompared with its initial mIoU when the model ﬁrst learns\nthe task. For example, the initial mIoU of task 2 is the mIoU of\nclass 101–110 evaluated at step 2. Similarly, that of task 3 is\nthe mIoU of class 111–120 reported at step 3. Table 12 shows\nthe performance drop at the last step compared with the initial\nmIoU of each task. Averaged across 5 tasks, the mIoU only\ndrops by 0.45%, which shows that SegViT is robust to for-\ngetting across all tasks on the 100–10 setting. Figure 8 shows\nthe mIoU on the base classes after incrementally training on\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1143\nTable 11 CSS results on ADE20k in mIoU (%) on 100–50 and 100–10 settings\n100–50 (2 tasks) 100–10 (6 tasks)\nMethod 0–100 101–150 all Avg 0–100 101–150 All Avg\nILT (Michieli & Zanuttigh, 2019) 18.29 ( ▼ 26.1) 14.40 ( ▼ 13.8) 17.00 ( ▼ 22.0) 29.42 0.11 ( ▼ 44.2) 3.06 ( ▼ 25.1) 1.09 ( ▼ 37.9) 12.56\nMiB (Cermelli et al., 2020) 40.52 ( ▼ 3.9) 17.17 ( ▼ 11.0) 32.79 ( ▼ 6.2) 37.31 38.21 ( ▼ 6.1) 11.12 ( ▼ 17.1) 29.24 ( ▼ 9.8) 35.12\nSDR (Michieli & Zanuttigh, 2021) 40.52 ( ▼ 3.8) 17.17 ( ▼ 11.0) 32.79 ( ▼ 6.2) 37.31 37.26 ( ▼ 7.1) 12.13 ( ▼ 16.1) 28.94 ( ▼ 10.1) 34.48\nPLOP (Douillard et al., 2021) 41.76 ( ▼ 2.6) 14.52 ( ▼ 13.7) 32.74 ( ▼ 6.3) 37.73 38.59 ( ▼ 5.8) 14.21 ( ▼ 14.0) 30.52 ( ▼ 8.5) 34.48\nREMINDER (Phan et al., 2022) 41.55 ( ▼ 2.8) 19.16 ( ▼ 9.0) 34.14 ( ▼ 4.9) 38.43 38.96 ( ▼ 5.4) 21.28 ( ▼ 6.9) 33.11 ( ▼ 5.9) 37.47\nRCIL (Zhang et al., 2022) 42.35 ( ▼ 2.0) 18.47 ( ▼ 9.7) 34.45 ( ▼ 4.6) 38.48 29.42 ( ▼ 15.0) 13.49 ( ▼ 14.0) 28.36 ( ▼ 10.0) 29.93\nOracle-ResNet backbone 44.34 28.21 39.00 – 44.34 28.21 39.00 –\nMiB (Cermelli et al., 2020) 43.43 ( ▼ 3.2) 30.63 ( ▼ 4.3) 39.19 ( ▼ 3.6) 38.66 39.15 ( ▼ 7.5) 20.37 (\n▼ 14.5) 34.17 ( ▼ 8.6) 39.53\nPLOP (Douillard et al., 2021) 43.82 ( ▼ 2.8) 26.23 ( ▼ 8.7) 37.99 ( ▼ 4.8) 38.06 43.25 ( ▼ 3.4) 24.13 ( ▼ 10.8) 36.25 ( ▼ 6.5) 40.28\nREMINDER (Phan et al., 2022) 44.66 ( ▼ 2.0) 26.76 ( ▼ 8.1) 38.73 ( ▼ 4.0) 38.43 43.28 ( ▼ 3.4) 24.33 ( ▼ 10.6) 37.10 ( ▼ 5.6) 41.76\nOracle-ViT backbone 46.63 34.90 42.75 - 46.63 34.90 42.75 –\nSegViT-CL (ours) 53.64 (▼ 0.5) 40.00 (▼ 5.6) 49.09 (▼ 2.2) 46.82 53.77 (▼ 0.3) 35.54 (▼ 10.0) 47.70 (▼ 3.6) 50.59\nOracle 54.11 45.60 51.28 – 54.11 45.60 51.28 –\nWe have utilized bold text in the tables to highlight the best or state-of-the-art (SOTA) benchmarks. The relative mIoU reduction compared with the jo int training for each method is reported\n123\n1144 International Journal of Computer Vision (2024) 132:1126–1147\nTable 12 Performance drop\n(degree of forgetting) of all\nclasses grouped by tasks on the\n100–10 setting\nTasks 101–110 111–120 121–130 131–140 141–150 Avg\nFirst time 34.93 39.78 41.10 36.22 27.95 35.99\nLast time 34.51 39.30 40.86 35.09 27.95 35.54\nForgetting ▼ 0.42 ▼ 0.48 ▼ 0.24 ▼ 1.12 ▼ 0 ▼ 0.45\nWe have utilized bold text in the tables to highlight the best or state-of-the-art (SOTA) benchmarks. We report\nthe class mIoU when the model ﬁrst learns the task, and the mIoU when the model last learns it\nFig. 8 mIoU of recent CSS methods on the ﬁrst 100 base classes after\nincrementally learning new tasks on 100–5 settings with 11 tasks\nmany tasks in 100–5, which is a long continual learning set-\nting with 11 tasks. Overall, our SegViT achieves nearly zero\nforgetting for almost all tasks at the last step. In contrast to\nprevious CSS methods which require partial ﬁne-tuning, the\nproposed SegViT supports completely freezing old parame-\nters, effectively eliminating any interference with previously\nacquired knowledge.\n6 Conclusion\nThis paper presents SegViTv2, a novel approach for semantic\nsegmentation using plain ViT transformer base models. The\nproposed method introduces a lightweight decoder head that\nincorporates the Attention-to-mask (ATM) module. Addi-\ntionally, a Shrunk++ structure is proposed to reduce the\ncomputational cost of the ViT encoder by 50% while main-\ntaining competitive segmentation accuracy. Moreover, this\nwork extends the SegViT framework to address the chal-\nlenge of continual semantic segmentation, aiming to achieve\nnearly zero forgetting. By protecting the parameters of old\ntasks, SegViT effectively mitigates the impact of catastrophic\nforgetting. Extensive experimental evaluations conducted on\nvarious benchmarks demonstrate the superiority of SegViT\nover UPerNet, while signiﬁcantly reducing computational\ncosts. The introduced decoder head provides a robust and\ncost-effective avenue for future research in the ﬁeld of ViT-\nbased semantic segmentation.\nAcknowledgements This work was in part supported by the National\nKey R&D Program of China (No. 2022ZD0118700). Y . Liu’s partici-\npation was in part supported by the start-up funding of The University\nof Adelaide. We express our gratitude to The University of Adelaide\nHigh-Performance Computing Services for providing the GPU Com-\npute Resources, and to Mr. Wang Hui and Dr. Fabien V oisin for their\nvaluable technical support for the training infrastructure.\nFunding Open Access funding enabled and organized by CAUL and\nits Member Institutions.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nBao, H., Dong, L., Piao, S., Wei, F. (2022). BEiT: BERT pre-training of\nimage transformers, in International conference on learning rep-\nresentations, [Online]. Available: https://openreview.net/forum?\nid=p-BhZSz59o4\nBousselham, W., Thibault, G., Pagano, L., Machireddy, A., Gray, J.,\nChang, Y . H., Song, X. (2021). Efﬁcient self-ensemble framework\nfor semantic segmentation, arXiv preprint arXiv:2111.13280\nCaesar, H., Uijlings, J., Ferrari, V . (2018). Coco-stuff: Thing and stuff\nclasses in context, in Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pp. 1209–1218.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A.,\nZagoruyko, S. (2020). End-to-end object detection with transform-\ners, in Proceedings European conference on computer vision (pp.\n213–229), Springer.\nCermelli, F., Mancini, M., Bulò, S. R., Ricci, E., Caputo, B. (2020).\nModeling the background for incremental learning in semantic\nsegmentation, in Proceedings of the IEEE conference on computer\nvision and pattern recognition , pp. 9230–9239.\nCha, S., Yoo, Y ., Moon, T., et al. (2021). Ssul: Semantic segmenta-\ntion with unknown label for exemplar-based class-incremental\nlearning, in Proceedings of the advances in neural information\nprocessing systems, vol. 34, pp. 10919–10930.\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1145\nChen, X., Ding, M., Wang, X., Xin, Y ., Mo, S., Wang, Y ., Han, S., Luo, P.,\nZeng, G., Wang, J. (2022). Context autoencoder for self-supervised\nrepresentation learning, arXiv preprint arXiv:2202.03026.\nChen, Z., Liu, B. (2016). Lifelong machine learning. Synthesis lectures\non artiﬁcial intelligence and machine learning.\nChen, L. -C., Papandreou, G., Schroff, F., Adam, H. (2017) Rethink-\ning atrous convolution for semantic image segmentation, arXiv\npreprint arXiv:1706.05587\nChen, L.-C., Zhu, Y ., Papandreou, G., Schroff, F., Adam, H. (2018).\nEncoder-decoder with atrous separable convolution for semantic\nimage segmentation, in Proceedings of the European conference\non computer vision , pp. 801–818.\nChen, W., Zhu, X., Sun, R., He, J., Li, R., Shen, X., Yu, B. (2020).\nTensor low-rank reconstruction for semantic segmentation, in Pro-\nceedings European conference on computer vision (pp. 52–69)\nSpringer.\nCheng, B., Misra, I., Schwing, A. G., Kirillov, A., Girdhar, R. (2022).\nMasked-attention mask transformer for universal image segmen-\ntation.\nCheng, B., Schwing, A., Kirillov, A. (2021). Per-pixel classiﬁcation is\nnot all you need for semantic segmentation, Proceedings of the\nadvances in neural information processing systems , vol. 34.\nCheng, B., Schwing, A. G., Kirillov, A. (2021). Per-pixel classiﬁcation\nis not all you need for semantic segmentation.\nC h u ,X . ,T i a n ,Z . ,W a n g ,Y . ,Z h a n g ,B . ,R e n ,H . ,W e i ,X . ,X i a ,H . ,S h e n ,\nC. (2021). Twins: Revisiting the design of spatial attention in vision\ntransformers, Proceedings of the advances in neural information\nprocessing systems, vol. 34.\nDing, H., Jiang, X., Shuai, B., Liu, A. Q., Wang, G. (2018). Context\ncontrasted feature and gated multi-scale aggregation for scene seg-\nmentation, in Proceedings of the IEEE conference on computer\nvision and pattern recognition , pp. 2393–2402.\nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D.,\nGuo, B. (2022). Cswin transformer: A general vision transformer\nbackbone with cross-shaped windows, in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 12124–\n12134.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,\nS., et al. (2021). An image is worth 16x16 words: Transformers for\nimage recognition at scale, Proceedings International Conference\non Learning Representations\nDouillard, A., Chen, Y ., Dapogny, A., Cord, M. (2021). Plop: Learning\nwithout forgetting for continual semantic segmentation, in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition\nDouillard, A., Cord, M., Ollion, C., Robert, T., Valle, E. (2020). Podnet:\nPooled outputs distillation for small-tasks incremental learning, in\nProceedings European conference on computer vision (pp. 86–\n102), Springer.\nDouillard, A., Ramé, A., Couairon, G., Cord, M. (2022). Dytox: Trans-\nformers for continual learning with dynamic token expansion, in\nProceedings of the IEEE conference on computer vision and pat-\ntern recognition, pp. 9285–9295.\nFrench, R. M. (1999). Catastrophic forgetting in connectionist net-\nworks. Trends in Cognitive Sciences, 3 (4), 128–135.\nFu, J., Liu, J., Tian, H., Li, Y ., Bao, Y ., Fang, Z., Lu, H. (2019) Dual\nattention network for scene segmentation, in Proceedings of the\nIEEE conference on computer vision and pattern recognition , pp.\n3146–3154.\nHe, K., Chen, X., Xie, S., Li, Y ., Dollár, P., Girshick, R. (2022). Masked\nautoencoders are scalable vision learners, in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition ,\npp. 16000–16009.\nJain, J., Li, J., Chiu, M., Hassani, A., Orlov, N., Shi, H. (2022). One-\nformer: One transformer to rule universal image segmentation,\narXiv preprint arXiv:2211.06220\nJin, Z., Liu, B., Chu, Q., Yu, N. (2021). Isnet: Integrate image-level\nand semantic-level context for semantic segmentation, in Proceed-\nings of the IEEE international conference on computer vision , pp.\n7189–7198.\nKang, M., Park, J., Han, B. (2022). Class-incremental learning by\nknowledge distillation with adaptive feature consolidation, in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 16071–16080.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G.,\nRusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska,\nA., et al. (2017). Overcoming catastrophic forgetting in neural net-\nworks. Proceedings of the National Academy of Sciences, 114 (13),\n3521–3526.\nKong, Z., Dong, P., Ma, X., Meng, X., Niu, W., Sun, M., Shen, X., Yuan,\nG., Ren, B., Tang, H. et al. (2022). Spvit: Enabling faster vision\ntransformers via latency-aware soft token pruning, in Proceedings\nEuropean conference on computer vision (pp. 620–640), Springer.\nLi, X., Yang, Y ., Zhao, Q., Shen, T., Lin, Z., Liu, H. (2020). Spatial\npyramid based graph reasoning for semantic segmentation, in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 8950–8959.\nLi, F., Zhang, H., Liu, S., Zhang, L., Ni, L. M., Shum, H. -Y .\net al. (2022). Mask dino: Towards a uniﬁed transformer-based\nframework for object detection and segmentation,” arXiv preprint\narXiv:2206.02777\nLi, X., Zhao, H., Han, L., Tong, Y ., Tan, S., Yang, K. (2020). Gated\nfully fusion for semantic segmentation, in Proceedings of the AAAI\nconference on artiﬁcial intelligence , vol. 34, no. 07, pp. 11418–\n11425.\nLi, X., Zhong, Z., Wu, J., Yang, Y ., Lin, Z., Liu, H. (2019). Expectation-\nmaximization attention networks for semantic segmentation, in\nProceedings of the IEEE international conference on computer\nvision, pp. 9167–9176.\nLi, X., Zhong, Z., Wu, J., Yang, Y ., Lin, Z., Liu, H. (2019). Expectation-\nmaximization attention networks for semantic segmentation, in\nProceedings of the IEEE international conference on computer\nvision, pp. 9167–9176.\nLi, Z., & Hoiem, D. (2018). Learning without forgetting. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 40 ,\n2935–2947.\nLin, T. -Y ., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie,\nS. (2017). Feature pyramid networks for object detection, in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2117–2125.\nLin, T. -Y ., Goyal, P., Girshick, R., He, K., Dollár, P. (2017). Focal loss\nfor dense object detection, in Proceedings of the IEEE interna-\ntional conference on computer vision , pp. 2980–2988.\nLin, F., Liang, Z., He, J., Zheng, M., Tian, S., Chen, K. (2022). Struct-\ntoken: Rethinking semantic segmentation with structural prior.\nLin, G., Milan, A., Shen, C., Reid, I. (2017). ReﬁneNet: Multi-path\nreﬁnement networks for high-resolution semantic segmentation,\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1925–1934.\nLiu, J., He, J., Zhang, J., Ren, J., Li, H. (2020). EfﬁcientFCN:\nHolistically-guided decoding for semantic segmentation, in Pro-\nceedings European conference on computer vision\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B.\n(2021). Swin transformer: Hierarchical vision transformer using\nshifted windows, in Proceedings of the IEEE international confer-\nence on computer vision , pp. 10012–10022.\nLong, J., Shelhamer, J., Darrell, T. (2015). Fully convolutional networks\nfor semantic segmentation, in Proceedings of the IEEE conference\non computer vision and pattern Recognition , pp. 3431–3440.\n123\n1146 International Journal of Computer Vision (2024) 132:1126–1147\nLu, H., Fei, N., Huo, Y ., Gao, Y ., Lu, Z., Wen, J.-R. (2022). Cots:\nCollaborative two-stream vision-language pre-training model for\ncross-modal retrieval, in Proceedings of the IEEE conference on\ncomputer vision and pattern Recognition , pp. 15692–15701.\nMaracani, A., Michieli, U., Toldo, M., Zanuttigh, P. (2021). Recall:\nReplay-based continual learning in semantic segmentation, in Pro-\nceedings of the IEEE international conference on computer vision .\nMichieli, U., Zanuttigh, P. (2019). Incremental learning techniques for\nsemantic segmentation, in Proceedings of the IEEE conference on\ncomputer vision workshops , pp. 3205–3212.\nMichieli, U., Zanuttigh, P. (2021). Continual semantic segmentation via\nrepulsion-attraction of sparse and disentangled latent representa-\ntions, in Proceedings of the IEEE conference on computer vision\nand pattern recognition , pp. 1114–1124.\nMilletari, F., Navab, N., Ahmadi, S.-A. (2016) V-net: Fully convolu-\ntional neural networks for volumetric medical image segmentation,\nin 3DV. IEEE, pp. 565–571.\nMMSegmentation, (2020). MMSegmentation: OpenMMLab semantic\nsegmentation toolbox and benchmark, https://github.com/open-\nmmlab/mmsegmentation\nMottaghi, R., Chen, X., Liu, X., Cho, N. -G., Lee, S. -W., Fidler, S.,\nUrtasun, R., Yuille, A. (2014). The role of context for object detec-\ntion and semantic segmentation in the wild, in Proceedings of the\nIEEE conference on computer vision and pattern recognition , pp.\n891–898.\nOstapenko, O., Lesort, T., Rodríguez, P., Areﬁn, M.R ., Douillard, A.,\nRish, I., Charlin, L. (2022). Continual learning with foundation\nmodels: An empirical study of latent replay, in Conference on\nlifelong learning agents . PMLR, pp. 60–91.\nPeng, Z., Dong, L., Bao, H., Ye, Q., Wei, F. (2022). BEiT v2: Masked\nimage modeling with vector-quantized visual tokenizers.\nPeng, Y ., Qi, J., Ye, Z., & Zhuo, Y . (2021). Hierarchical visual-textual\nknowledge distillation for life-long correlation learning. Interna-\ntional Journal of Computer Vision, 129 , 921–941.\nPhan, M. H., Phung, S. L., Tran-Thanh, L., Bouzerdoum, A. et al.\n(2022). Class similarity weighted knowledge distillation for con-\ntinual semantic segmentation, in Proceedings of the IEEE con-\nference on computer vision and pattern Recognition , pp. 16866–\n16875.\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,\nSastry, G., Askell, A., Mishkin, P., Clark, J. et al. (2021). Learning\ntransferable visual models from natural language supervision, in\nInternational conference on machine learning . PMLR, pp. 8748–\n8763.\nRamasesh, V . V ., Lewkowycz, A., Dyer, E. (2022). Effect of scale on\ncatastrophic forgetting in neural networks, in Proceedings of the\ninternational conference on learning representation .\nRanftl, R., Bochkovskiy, A., Koltun, V . (2021) Vision transformers for\ndense prediction, in Proceedings of the IEEE international con-\nference on computer vision , pp. 12179–12188.\nRao, Y ., Zhao, W., Liu, B., Lu, J., Zhou, J., Hsieh, C.-J. (2021).\nDynamicvit: Efﬁcient vision transformers with dynamic token\nsparsiﬁcation, in Proceedings of the advances in neural informa-\ntion processing systems , vol. 34, pp. 13937–13949.\nRonneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional\nnetworks for biomedical image segmentation. Medical image\ncomputing and computer-assisted intervention (pp. 234–241).\nSpringer.\nRyoo, M., Piergiovanni, A., Arnab, A., Dehghani, M., Angelova,\nA. (2021). Tokenlearner: Adaptive space-time tokenization for\nvideos, Proceedings of the advances in neural information pro-\ncessing systems , vol. 34, pp. 12786–12797.\nShao, C., Feng, Y . (2022) Overcoming catastrophic forgetting beyond\ncontinual learning: Balanced training for neural machine transla-\ntion, arXiv preprint arXiv:2203.03910\nSteiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J.,\nBeyer, L. (2021). How to train your vit? Data, augmentation, and\nregularization in vision transformers.\nStrudel, R., Garcia, R., Laptev, I., Schmid, C. (2021) Segmenter: Trans-\nformer for semantic segmentation, in Proceedings of the IEEE\ninternational conference on computer vision , pp. 7262–7272.\nSun, K., Zhao, Y ., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y ., Wang,\nX., Liu, W., Wang, J. (2019). High-resolution representations for\nlabeling pixels and regions.\nTouvron, H., Cord, M., Jégou, H. (2022). Deit iii: Revenge of the vit,\nin Computer Vision-ECCV . 17th European conference\n,T e lA v i v ,\nIsrael, October 23–27, 2022, Proceedings, Part XXIV (pp. 516–\n533) Springer.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, Ł., Polosukhin, I. (2017). Attention is all you need,”\nProceedings of the advances in neural information processing sys-\ntems, vol. 30.\nWang, Z., Liu, L., Duan, Y ., Kong, Y ., Tao, D. (2022). Continual learn-\ning with lifelong vision transformer, in Proceedings of the IEEE\nconference on computer vision and pattern recognition , pp. 171–\n181.\nWang, Z., Liu, L., Kong, Y ., Guo, J., Tao, D. (2022). Online contin-\nual learning with contrastive vision transformer, in Proceedings\nEuropean conference on computer vision (pp. 631–650), Springer.\nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,\nShao, L. (2021). Pyramid vision transformer: A versatile backbone\nfor dense prediction without convolutions, in Proceedings of the\nieee international conference on computer vision , pp. 568–578.\nWang, Z., Zhang, Z., Ebrahimi, S., Sun, R., Zhang, H., Lee, C.-Y ., Ren,\nX., Su, G., Perot, V ., Dy, J., et al. (2022). Dualprompt: Complemen-\ntary prompting for rehearsal-free continual learning, in Computer\nVision-ECCV , 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XXVI (pp. 631–648), Springer.\nWang, Z., Zhang, Z., Lee, C.-Y ., Zhang, H., Sun, R., Ren, X., Su, G.,\nPerot, V ., Dy, J., Pﬁster, T. (2022). Learning to prompt for continual\nlearning, in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , pp. 139–149.\nWang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y ., Liu, D.,\nMu, Y ., Tan, M., Wang, X., et al. (2020). Deep high-resolution\nrepresentation learning for visual recognition. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 43 (10), 3349–3364.\nWei, L., Xie, L., Zhou, W., Li, H., Tian, Q. (2022). Mvp: Multimodality-\nguided visual pre-training, in Proceedings European conference on\ncomputer vision (pp. 337–353), Springer.\nWu, T., Caccia, M., Li, Z., Li, Y .-F., Qi, G., Haffari, G. (2022). Pre-\ntrained language model in continual learning: A comparative study,\nin Proceedings of the international conference on learning repre-\nsentation.\nWu, Y . -H., Liu, Y ., Zhan, X., Cheng, M. -M. (2022). P2t: Pyramid\npooling transformer for scene understanding, IEEE transactions\non pattern analysis and machine intelligence .\nWu, T., Lu, Y ., Zhu, Y ., Zhang, C., Wu, M., Ma, Z., Guo, G. (2020).\nGinet: Graph interaction network for scene parsing, in Proceedings\nEuropean conference on computer vision (pp. 34–51), Springer.\nXiao, T., Liu, Y ., Zhou, B., Jiang, Y ., Sun, J. (2018). Uniﬁed perceptual\nparsing for scene understanding, in Proceedings European confer-\nence on computer vision , pp. 418–434.\nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.\n(2021) “Segformer: Simple and efﬁcient design for semantic seg-\nmentation with transformers,”Proc. Adv. Neural Inf. Process. Syst.,\nvol. 34.\nXu, Y ., Zhang, J., Zhang, Q., Tao, D. (2022). “Rethinking hierar-\nchicies in pre-trained plain vision transformer, arXiv preprint\narXiv:2211.01785\nYan, S., Xie, J., He, X. (2021). Der: Dynamically expandable represen-\ntation for class incremental learning, in Proceedings of the IEEE\n123\nInternational Journal of Computer Vision (2024) 132:1126–1147 1147\nconference on computer vision and pattern recognition , pp. 3014–\n3023.\nYuan, Y ., Chen, X., Chen, X., Wang, J. (2019). Segmentation trans-\nformer: Object-contextual representations for semantic segmenta-\ntion, arXiv preprint arXiv:1909.11065\nYuan, Y ., Chen, X., Wang, J. (2020). Object-contextual representations\nfor semantic segmentation, in Proceedings of the European Con-\nference on Computer Vision Springer, pp. 173–190.\nZhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal,\nA. (2018). Context encoding for semantic segmentation, in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 7151–7160.\nZhang, W., Pang, J., Chen, K., Loy, C. C. (2021). K-net: Towards uni-\nﬁed image segmentation, Proceedings of the advances in neural\ninformation processing systems , vol. 34.\nZhang, B., Tian, Z., Shen, C. et al. (2021). Dynamic neural representa-\ntional decoders for high-resolution semantic segmentation, vol. 34.\nZhang, B., Tian, Z., Tang, Q., Chu, X., Wei, X., Shen, C., Liu, Y . (2022).\nSegvit: Semantic segmentation with plain vision transformers, in\nProceedings of the advances in neural information processing sys-\ntems.\nZhang, C. -B., Xiao, J. -W., Liu, X., Chen, Y . -C., Cheng, M. -M. (2022).\n“Representation compensation networks for continual semantic\nsegmentation, in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2022, pp. 7053–7064.\nZhang, W., Pang, J., Chen, K., & Loy, C. C. (2021). K-net: Towards\nuniﬁed image segmentation. Advances in Neural Information Pro-\ncessing Systems, 34 , 10326–10338.\nZhao, H., Shi, J., Qi, X., Wang, X., Jia, J. (2017). Pyramid scene pars-\ning network, in Proceedings of the IEEE conference on computer\nvision and pattern recognition .\nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J.,\nXiang, T., Torr, P. H. (2021). et al., Rethinking semantic segmenta-\ntion from a sequence-to-sequence perspective with transformers,\nin Proceedings of the IEEE conference on computer vision and\npattern Recognition , pp. 6881–6890.\nZhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J. (2018). Unet++:\nA nested U-net architecture for medical image segmentation, in\nProceedings of the deep learning in medical image analysis work-\nshop, pp. 3–11.\nZhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong,\nT. (2022). ibot: Image bert pre-training with online tokenizer,\nProceedings of the international conference on learning repre-\nsentation.\nZhou, Z., Zhang, B., Lei, Y ., Liu, L., Liu, Y . (2022). Zegclip: Towards\nadapting clip for zero-shot semantic segmentation, arXiv preprint\narXiv:2212.03588\nZhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A. (2017)\nScene parsing through ade20k dataset, in Proceedings of the IEEE\nconference on computer vision and pattern recognition , pp. 633–\n641.\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}