{
  "title": "Toward the Adoption of Explainable Pre-Trained Large Language Models for Classifying Human-Written and AI-Generated Sentences",
  "url": "https://openalex.org/W4403605341",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2071956141",
      "name": "Luca Petrillo",
      "affiliations": [
        "IMT School for Advanced Studies Lucca",
        "Institute of Informatics and Telematics"
      ]
    },
    {
      "id": "https://openalex.org/A2107438709",
      "name": "Fabio Martinelli",
      "affiliations": [
        "Institute of Informatics and Telematics"
      ]
    },
    {
      "id": "https://openalex.org/A1150715340",
      "name": "Antonella Santone",
      "affiliations": [
        "University of Molise"
      ]
    },
    {
      "id": "https://openalex.org/A1080042580",
      "name": "Francesco Mercaldo",
      "affiliations": [
        "University of Molise",
        "Institute of Informatics and Telematics"
      ]
    },
    {
      "id": "https://openalex.org/A2071956141",
      "name": "Luca Petrillo",
      "affiliations": [
        "Institute of Informatics and Telematics",
        "IMT School for Advanced Studies Lucca"
      ]
    },
    {
      "id": "https://openalex.org/A2107438709",
      "name": "Fabio Martinelli",
      "affiliations": [
        "Institute of Informatics and Telematics"
      ]
    },
    {
      "id": "https://openalex.org/A1150715340",
      "name": "Antonella Santone",
      "affiliations": [
        "University of Molise"
      ]
    },
    {
      "id": "https://openalex.org/A1080042580",
      "name": "Francesco Mercaldo",
      "affiliations": [
        "Institute of Informatics and Telematics",
        "University of Molise"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4308623137",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4319773014",
    "https://openalex.org/W6862353035",
    "https://openalex.org/W4387432528",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W4389033247",
    "https://openalex.org/W4400834586",
    "https://openalex.org/W4315783512",
    "https://openalex.org/W3119467012",
    "https://openalex.org/W3088577908",
    "https://openalex.org/W4225139287",
    "https://openalex.org/W4385156842",
    "https://openalex.org/W4389519206",
    "https://openalex.org/W3139815689",
    "https://openalex.org/W4404835041",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W4386361581",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4392654282"
  ],
  "abstract": "Pre-trained large language models have demonstrated impressive text generation capabilities, including understanding, writing, and performing many tasks in natural language. Moreover, with time and improvements in training and text generation techniques, these models are proving efficient at generating increasingly human-like content. However, they can also be modified to generate persuasive, contextual content weaponized for malicious purposes, including disinformation and novel social engineering attacks. In this paper, we present a study on identifying human- and AI-generated content using different models. Precisely, we fine-tune different models belonging to the BERT family, an open-source version of the GPT model, ELECTRA, and XLNet, and then perform a text classification task using two different labeled datasets—the first one consisting of 25,000 sentences generated by both AI and humans and the second comprising 22,929 abstracts that are ChatGPT-generated and written by humans. Furthermore, we perform an additional phase where we submit 20 sentences generated by ChatGPT and 20 sentences randomly extracted from Wikipedia to our fine-tuned models to verify the efficiency and robustness of the latter. In order to understand the prediction of the models, we performed an explainability phase using two sentences: one generated by the AI and one written by a human. We leveraged the integrated gradients and token importance techniques, analyzing the words and subwords of the two sentences. As a result of the first experiment, we achieved an average accuracy of 99%, precision of 98%, recall of 99%, and F1-score of 99%. For the second experiment, we reached an average accuracy of 51%, precision of 50%, recall of 52%, and F1-score of 51%.",
  "full_text": null,
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.672684371471405
    },
    {
      "name": "Computer science",
      "score": 0.5777156949043274
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5523449182510376
    },
    {
      "name": "Language model",
      "score": 0.500741720199585
    },
    {
      "name": "Human–computer interaction",
      "score": 0.32559844851493835
    }
  ]
}