{
  "title": "Who Did They Respond to? Conversation Structure Modeling Using Masked Hierarchical Transformer",
  "url": "https://openalex.org/W2998672049",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2513492337",
      "name": "Henghui Zhu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2109305684",
      "name": "Feng Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097231855",
      "name": "Zhiguo Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A93460222",
      "name": "Ramesh Nallapati",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128398165",
      "name": "Bing Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2513492337",
      "name": "Henghui Zhu",
      "affiliations": [
        "Boston University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251483872",
    "https://openalex.org/W2950304420",
    "https://openalex.org/W2798416089",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2582886532",
    "https://openalex.org/W2115615127",
    "https://openalex.org/W108071511",
    "https://openalex.org/W2949600515",
    "https://openalex.org/W2962854379",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W6677106573",
    "https://openalex.org/W2613142859",
    "https://openalex.org/W2972916088",
    "https://openalex.org/W2113063049",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2967674528",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W648786980",
    "https://openalex.org/W836999996",
    "https://openalex.org/W4230872509",
    "https://openalex.org/W4288283245",
    "https://openalex.org/W4317471952",
    "https://openalex.org/W4288345685"
  ],
  "abstract": "Conversation structure is useful for both understanding the nature of conversation dynamics and for providing features for many downstream applications such as summarization of conversations. In this work, we define the problem of conversation structure modeling as identifying the parent utterance(s) to which each utterance in the conversation responds to. Previous work usually took a pair of utterances to decide whether one utterance is the parent of the other. We believe the entire ancestral history is a very important information source to make accurate prediction. Therefore, we design a novel masking mechanism to guide the ancestor flow, and leverage the transformer model to aggregate all ancestors to predict parent utterances. Our experiments are performed on the Reddit dataset (Zhang, Culbertson, and Paritosh 2017) and the Ubuntu IRC dataset (Kummerfeld et al. 2019). In addition, we also report experiments on a new larger corpus from the Reddit platform and release this dataset. We show that the proposed model, that takes into account the ancestral history of the conversation, significantly outperforms several strong baselines including the BERT model on all datasets.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nWho Did They Respond to? Conversation\nStructure Modeling Using Masked Hierarchical Transformer\nHenghui Zhu,1∗ Feng Nan,2 Zhiguo Wang,2 Ramesh Nallapati,2 Bing Xiang2\n1Boston University,2AWS AI Labs\nhenghuiz@bu.edu, {nanfen, zhiguow, rnallapa, bxiang}@amazon.com\nAbstract\nConversation structure is useful for both understanding the\nnature of conversation dynamics and for providing features\nfor many downstream applications such as summarization of\nconversations. In this work, we deﬁne the problem of con-\nversation structure modeling as identifying the parent utter-\nance(s) to which each utterance in the conversation responds\nto. Previous work usually took a pair of utterances to decide\nwhether one utterance is the parent of the other. We believe\nthe entire ancestral history is a very important information\nsource to make accurate prediction. Therefore, we design a\nnovel masking mechanism to guide the ancestor ﬂow, and\nleverage the transformer model to aggregate all ancestors to\npredict parent utterances. Our experiments are performed on\nthe Reddit dataset (Zhang, Culbertson, and Paritosh 2017)\nand the Ubuntu IRC dataset (Kummerfeld et al. 2019). In\naddition, we also report experiments on a new larger corpus\nfrom the Reddit platform and release this dataset. We show\nthat the proposed model, that takes into account the ancestral\nhistory of the conversation, signiﬁcantly outperforms several\nstrong baselines including the BERT model on all datasets.\nIntroduction\nWhen a group of people communicate with one another,\nthere exist inherent structures in the conversation. One of\nthe structures can be deﬁned as the ‘reply\nto’ relationship\nbetween a pair of utterances. The ‘replyto’ relationship may\nbe explicitly deﬁned in some platforms including Reddit,\nTwitter, and Facebook. In other platforms such as Internet\nRelay Chat (IRC), Slack, and most other forums, there is\nno explicit ‘reply\nto’ relationship in the conversation. The\nproblem also exists in Automatic Speech Recognition (ASR)\ntranscripts of recorded conversations where the output typ-\nically consists of a ﬂat list of utterances with no structure\nassigned to them. Identiﬁcation of the structure of such con-\nversations typically entails signiﬁcant human labeling effort\n(Kummerfeld et al. 2019).\nThe problem of discovering conversation structure is also\nreferred to as conversation disentanglement in the literature\n∗Work done during an internship at the AWS AI Labs.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(Elsner and Charniak 2008; Kummerfeld et al. 2019). Disen-\ntangling the conversation structure opens the door for many\ndownstream applications. For example, the topics discussed\nin a conversation are often non-contiguous and intertwined\nwith each other. Discovering the conversation structure al-\nlows us to segment the conversation by topics. It also permits\neasier analysis and summarization of the various threads and\ntopics discussed in the conversation. In addition, conversa-\ntion structure can also be used to improve discourse act clas-\nsiﬁcation, which is useful in dialogue modeling (Zhang, Cul-\nbertson, and Paritosh 2017).\nFigure 1: An example sequence of utterances. Curved lines\nwith arrows are the ground truth ‘reply\nto’ relationship in\nthe conversation. Without the context of the ﬁrst utterance,\nit is hard to tell whether the fourth utterance is responding to\nthe second utterance or the third utterance.\nPrevious work (Kummerfeld et al. 2019) only takes a pair\nof utterances to decide whether one utterance is the parent\nof the other without considering the context. Consider an\nexample conversation shown in Figure 1. User A poses a\nquestion in the ﬁrst line; user B asks a clariﬁcation ques-\ntion in the second line responding to the ﬁrst line; user C\nthen interjects with an unrelated question in the third line;\nﬁnally, user A makes a statement in the fourth line. Judging\nin a pairwise fashion, without considering context, the last\nline is equally likely to be replying to the questions in the\nsecond and third lines. However, by considering the context\nof the ﬁrst utterance, it is clear that user A is continuing a\nconversation with user B to address the internet connection\nquestion.\nThe goal of this work is to automatically discover the\nstructure of a conversation given all the utterances. The task\n9741\nis very challenging since (a) the ‘replyto’ relationship can\nexhibit long-distance behavior, i.e., an utterance may re-\nsponse to an utterance that occurred several turns earlier in\nthe conversation, and (b) various topics can be intertwined\nwith one another in the conversation.\nIn this work, we propose a new Masked Hierarchical\nTransformer model to learn conversation structures taking\ninto account the prior context of each utterance. Concretely,\na pre-trained transformer is used to produce a feature vector\nof each utterance. Then a second-stage transformer is ap-\nplied to compare and aggregate utterances and produce the\nprobability for each utterance being the parent of a target\nutterance. To utilize the previous conversation structure in-\nformation, we introduce a novel masking mechanism for the\nsecond-stage transformer. We evaluate our model on three\ndatasets using metrics for accurately identifying the ‘re-\nply\nto’ edges as well as metrics for reconstructing the whole\nconversation structure. We demonstrate that our method out-\nperforms several strong baseline models including a BERT-\nbased sentence-pair classiﬁcation model, and achieves state-\nof-art results on all three conversation structure modeling\ndatasets.\nRelated Work\nNext Utterance Prediction Next-utterance prediction is\nvery related to our task of conversation structure modeling.\nThere are two types of formulation of next-utterance predic-\ntion. The ﬁrst one is to generate the next utterance in a con-\nversation given the conversation history (Zhao and Kawa-\nhara 2019; Dziri et al. 2018; Hu et al. 2019) and the sec-\nond type is to retrieve the next utterance in the conversation\nfrom a large list of response utterances (Lowe et al. 2015;\nWhang et al. 2019). These tasks are useful for building a\nchatbot, aiming to generate or retrieve a good response ac-\ncording to the context of the conversation, while this work\naims to recover the structure of the entire conversation.\nConversation Disentanglement Conversation structure\nmodeling task is also related to the conversation disentan-\nglement task (Shen et al. 2006; Elsner and Charniak 2008;\nKummerfeld et al. 2019). In particular, (Kummerfeld et al.\n2019) proposed a large corpus for conversation disentangle-\nment and a feed-forward model using average word embed-\nding of sentences and some additional features. Conversa-\ntion disentanglement is one type of conversation structure\nmodeling task, where several conversation threads are inter-\nleaved and need to be disentangled. The ground truth ‘re-\nply\nto’ labels need to be annotated manually. In this paper,\nwe have included the conversation disentanglement task for\nUbuntu IRC in (Kummerfeld et al. 2019) into our analysis\nas well and compare with their model.\nDiscourse Parsing Conversational discourse parsing is\nalso related to this task (Joty, Carenini, and Ng 2012;\nAfantenos et al. 2015; Du, Poupart, and Xu 2017). Discourse\nparsing usually considers different kinds of relationship such\nas ‘question-answer’ and ’acknowledgment’. The focus of\nthese papers is mostly on the classiﬁcation of the type of\nrelationship, which is not the focus of this work.\nModel\nConsider the list of utterances in a conversation denoted\nas S\n1,S 2 ··· ,S N . We aim to ﬁnd out which of the ut-\nterance(s) in S1,S 2 ··· ,S L is the parent utterance of the\ntarget utterance SL for L ∈{ 2,··· N}. (In Ubuntu IRC\ndataset, the parent of an utterance can be itself.) We may ac-\ncess the structure of the conversation history during training\n(or its predicted structure at test time) i.e., all the parent(s) of\nS\ni are assumed to be known when we are predicting whether\nSi is the parent of the target utteranceSL, wherei<L and\nL ∈{ 2,··· N}.\nBaseline Models\nThe conversation structure modeling task can be viewed as\nan utterance pair classiﬁcation task for the ‘replyto’ rela-\ntionship. Therefore, several existing methods for sentence-\npair classiﬁcation can be applied to this problem. In this pa-\nper, we consider the following models: Decomposable At-\ntention Model (Parikh et al. 2016), Enhanced Sequential In-\nference Model (ESIM) (Chen et al. 2018) as well as the\nstate-of-the-art BERT model (Devlin et al. 2019) for sen-\ntence pair classiﬁcation. The decomposable attention model\nuses an attention mechanism that compares and aggregates\nthe token-level information in both sentences to produce a\nset of meaningful features for sentence pair classiﬁcation.\nESIM improves this model by adding two LSTMs before\nand after the attention for including more contextual in-\nformation in the feature vectors. BERT uses a transformer\nmodel (V aswani et al. 2017) to perform comparison and ag-\ngregation, and leverages the unsupervised pretraining on a\nlarge corpus. In this paper, we use Glove embedding (Pen-\nnington, Socher, and Manning 2014) in the decomposable\nattention model and ESIM. For Reddit-small dataset, we\nalso consider ELMo (Peters et al. 2018) as word embed-\nding for comparison. Also, since there are usually more non-\nparent-relation sentence pairs compared with parent-relation\npairs, this paper balances the dataset by downsampling the\nnon-parent sentence pairs when training.\nMasked Hierarchical Transformer Model\nThe baseline methods take into account only two utter-\nances in the conversation, not the context of the conver-\nsation. Also, the structure of the conversation history may\nhelp in identifying the parent utterance(s) for a target utter-\nance. Therefore, we consider a Masked Hierarchical Trans-\nformer model for the conversation modeling task, displayed\nin Figure 2. The input of the model is a sequence of his-\ntory utterances S\n1,S 2 ··· ,S L, where the target utterance\nis placed at the end of the sequence asSL. For each utter-\nance, a shared utterance encoder (the yellow boxes in Fig-\nure 2) is ﬁrst used to produce a feature vector. In particu-\nlar, we use a transformer model with the same conﬁguration\nas BERT-base, uncased (Devlin et al. 2019), initial-\nized with the pre-trained model. The output of the reserved\n9742\n[CLS] token in the BERT model is used as the feature vec-\ntor of each utterance, denoted asV1,V 2 ··· ,V L. Then we\napply a second-stage masked transformer (the orange boxes\nin Figure 2) to compare and aggregate information between\nthe target utterance and each of the parent utterance candi-\ndates. We use a 4-layer transformer, each layer is with hid-\nden size 300, intermediate states size 1024, and 4 attention\nheads. In order to use conversation structure, rather than the\nentire history, we design a masking mechanism to ﬁlter out\nredundant utterances, and only leverage ancestral utterances\nto predict the target utterance. The details of the masking\nwill be discussed in the next subsection. Then, we denote\nthe output of the second-stage transformer for each utter-\nance as ˜V\n1, ˜V2 ··· , ˜VL. Finally, a fully-connected layer is\nused to produce the logits for the classiﬁcation problem. For\nthe Reddit datasets, since one comment has only one par-\nent comment, we apply a softmax after the fully-connected\nlayer and use a ranking loss. In particular,\nt\ni = Wo ˜Vi +bo,i =1 ,··· ,L −1,\nˆYi = exp(ti)∑L−1\nj=1 exp(tj )\n,i =1 ,··· ,L −1,\nand the rank loss is\n−\nL−1∑\ni=1\nyi log(ˆYi), (1)\nwhere Wo ∈ R1×300 and bo ∈ R are the parameters in the\nlast fully-connected layer andyi is the binary indicator for\nif utterancei is the parent utterance of the target utterance.\nFor the Ubuntu IRC dataset, one utterance may have more\nthan one parent. Also, the parent of an utterance can be it-\nself. Therefore, we use a binary cross-entroy loss for each\ncomment. In particular, the loss is deﬁned as\n−\nL∑\ni=1\nyi log(σ(ti))+(1 −yi)log(1 −σ(ti)), (2)\nti = Wo ˜Vi +bo,i =1 ,··· ,L,\nwhere Wo ∈ R1×300 and bo ∈ R are again the parameters\nin the last fully-connected layer andyi is the binary indi-\ncator for if utterance i is the parent utterance of the target\nutterance.\nAncestor Masking The mask M in the second-stage\ntransformer (the orange boxes in Figure 2) is an asymmetric\nbinary matrix of sizeL×Lthat encodes whether or not utter-\nance iattends to utterancej(M\nij =1 ) or not (Mij =0 ). In-\nstead of leveraging all history, we design the masking mech-\nanism to aggregate information only from an utterance’s an-\ncestors. In particular, the following properties should hold\ntrue:\n1) All history utterances are able to attend to the target ut-\nterance, since all history utterances are parent candidates\nfor the target utterance. And the transformer aim to pro-\nduce features of the utterance being the parent of the tar-\nget utterance.\nFigure 2: Diagram of the masked hierarchical transformer\nfor conversation structure modeling. The colored blocks on\nthe right indicates one element in the mask matrix, which\nmeans the corresponding utterance is attendable. The white\nblock, on the other hand, indicates a zero element.\n2) Every utterance is able to attend to itself.\n3) Every non-target utterance is able to attend to its ances-\ntors in the conversation graph.\n4) All remaining utterances should not be attended to.\nConsider a conversation example on the left side of Figure\n3. The mask matrix is shown on the right, denoted asM.\nThe last column of the matrix is all one due to property (1)\nabove. The diagonal elements of the matrix are one due to\nproperty (2). Finally, for property (3), consider the following\nexample pairs. Utterance 1 is the ancestor for utterance 4.\nTherefore, utterance 1 can be attended to by utterance 4 and\nthe corresponding element in the mask matrixM\n41 =1 .O n\nthe other hand, utterance 3 is not the ancestor of utterance 4.\nTherefore, the information of utterance 3 is not accessible to\nutterance 4 andM\n43 =0 .\nFigure 3: Graphical representation of the mask matrix for\nthe second-stage transformer.\nFor each target utterance, we generate this asymmetric\nmask matrix that encodes the conversation structure until\nthat point in time and run the top transformer on the en-\ntire sequence of utterances seen thus far. At training time,\nthe mask is deﬁned based on the ground-truth conversation\nstructure, while at test time, we deﬁne the mask based on\nprevious predictions.\nTwo-Stage Training The shared utterance encoder in the\nmasked hierarchical transformer model is initialized with a\n9743\npre-trained BERT model, while there is no pre-training for\nthe second-stage transformer initially. This may cause issues\nduring model training: if one uses a large learning rate of the\noptimizer during training, the parameters in the shared utter-\nance encoder may be washed out, causing what is known as\ncatastrophic forgetting. But if one applies a small learning\nrate, it may take a very long time for the top transformer\nto converge. To address this problem, this paper proposes\na two-stage training approach. In the ﬁrst stage, we freeze\nthe parameters of the shared utterance encoder after initial-\nization with the pre-trained model during the training. In\nthis case, we are essentially training the second-stage trans-\nformer and a relatively large learning rate can be applied.\nAfter this stage, the parameters in the shared utterance en-\ncoder are unfrozen and all the parameters in the model are\njointly trained with a relatively small learning rate.\nDatasets\nReddit-Small The comments in Reddit are organized by\nthe ‘reply to’ relationship, and the relationship graph is a\ntree whose root node is the title comment. The ﬁrst dataset\nwe consider is the selection of the Reddit conversations\n1\nfrom (Zhang, Culbertson, and Paritosh 2017)2. We call this\ndataset Reddit-small in this paper. This dataset has 9,483\nconversations. The following rule is used for ﬁltering the\ndata: if a comment is deleted, the comment itself and all its\ndescendants are considered invalid. Then we split the con-\nversations into train/dev/test sets using an 80/10/10 ratio.\nAlso, we include only the ﬁrst 16 comments in a Reddit con-\nversation in the test set due to the capacity of the model. We\nﬁnd very few conversations have more than 16 comments in\nthis selection. The statistics of three sets are shown in Ta-\nble 1. The maximum path length of the conversation tree in\nthose Reddit conversations is less than 5, which indicates the\ntree structures of the conversations are not very deep.\nSplit # comments # conversations Average depth\nTrain 88,754 7,487 4.49\nDev 11,056 936 4.43\nTest 11,524 936 4.61\nTable 1: Statistics of the Reddit-small dataset.\nReddit-Large Since the conversation tree structure in\nReddit-small dataset is relatively ﬂat, In this paper, we cre-\nate a relatively challenging dataset from Reddit dump\n3.W e\nﬁlter the sub-reddits to remove those that areOver 18 or\non the Not Safe For Work (NFWS) list4. The comments that\n1We deﬁne a conversation as a submission in Reddit. Each com-\nment in Reddit is equivalent to an utterance in a conversation.\n2They only provide the comment IDs and crawling scripts, the\ndata generated in this paper is crawled on 05/24/2019. Also, the\ncomments and conversation in Reddit change every day, i.e., an\nexisting comment can be deleted. Therefore, there are many com-\nments missing when we collected the data.\n3https://ﬁles.pushshift.io/reddit/\n4https://www.reddit.com/r/NSFW411/wiki/index\nhave non-ASCII characters, deleted, or contain more than\n128 characters are dropped. If one comment is dropped, so\nwill be all of its decedents. Next, we remove the conversa-\ntions that have a maximum tree depth less than 6. Finally,\nwe randomly sample 10% of the conversations that satisfy\nthe above conditions. We call this dataset Reddit-large in\nthis paper.\n5 Since we observe the conversations in Reddit-\nlarge are longer compared with the Reddit-small, a same al-\ngorithm usually has a lower performance in this dataset. To\nmake the evaluation result comparable to the previous Red-\ndit dataset, we randomly remove the leaves in the conversa-\ntion tree till the number of the comments in the conversation\nis 16, if there are more than 16 comments in the conver-\nsation. The statistics of train/dev/test set is shown in Table\n2. Compared with the Reddit-small dataset, the data is 30x\nlarger and the conversation trees are deeper, which makes\nreconstruction of the conversation more challenging.\nSplit # comments # conversations Average depth\nTrain 2,872,524 57,196 7.12\nDev 315,058 6,356 7.16\nTest 202,226 12,638 6.82\nTable 2: Statistics of the Reddit-large dataset.\nUbuntu IRC Finally, we consider the Ubuntu IRC dataset\n(Kummerfeld et al. 2019). It consists of messages manually\nannotated with ‘reply\nto’ structures that disentangle conver-\nsations. This work only considers the training and develop-\nment portion of the data\n6, where we use the ofﬁcial dev set\nin (Kummerfeld et al. 2019) as the test set in our experi-\nments. We randomly select 140 conversations\n7 of the train\nset in their paper as our training set, and the remaining 13\nconversations are used as our development set. The statistics\nof the dataset used in this paper are shown in Table 3. Com-\npared with the Reddit dataset, an utterance in IRC may have\nmore than one parent from the conversation history, although\n97.40% of utterances have only one parent. Furthermore, for\nan utterance in Ubuntu IRC, the parent utterance may be it-\nself. Finally, there are occurrences of system-generated mes-\nsages in this corpus along with human-generated messages.\nExperimental Results\nModel Performance on Reddit datasets\nWe ﬁrst consider the Reddit-small dataset. In this dataset,\neach comment has a unique parent comment, which ex-\ncludes itself. Therefore, we use a rank loss deﬁned in Equa-\ntion (1) in the masked hierarchical model.\n5We release the dataset at https://github.com/henghuiz/\nMaskedHierarchicalTransformer\n6The test set annotation was not released when submitting this\npaper.\n7In Ubuntu IRC dataset, we deﬁne a conversation as a sample of\ncontinuous conversation history in the channel, usually consisting\nof 100-500 messages and 1000 additional context messages before\nthose messages considered.\n9744\nSplit # conversation # ann. messages Avg. # parent\nTrain 140 61561 1.03\nDev 13 5902 1.03\nTest 10 2500 1.04\nTable 3: Statistics of the Ubuntu IRC dataset. (The test set in\nthis paper is the dev set in (Kummerfeld et al. 2019))\nSince the number of comments in a Reddit thread is rela-\ntively small but the length of comments is long, we limit the\nlength of each comment to be 50 subwords after byte-pair-\nencoding (BPE) tokenization (51st word and on-wards are\nremoved). Also, we limit the length of conversation history\nto be 16 during training by iteratively removing leaf nodes\nin the conversation graph so that the target comment and\nthe title comment (root node) are in the conversation history.\nThe input to our model is the comment history sorted by the\ntimestamps of the utterances.\nWe consider two metrics to evaluate the performance. The\ngraph accuracyis deﬁned as the average agreement between\nthe ground truth and predicted parent for each utterance. The\nconversation accuracy is deﬁned as the agreement between\nthe entire conversation tree structure reconstructed by the\nmodel and the ground-truth structure.\nAll baseline models are trained with Adam optimizer\n(Kingma and Ba 2014). For the decomposable attention\nmodel and the ESIM, we use a learning rate10\n−4 with batch\nsize 32 for 50 epochs. For the BERT model, we use a base\nlearning rate 3 × 10\n−5 for 10 epochs with the same learn-\ning schedule describe in (Devlin et al. 2019). For training\nour model, we use a learning rate10\n−4 for pre-training the\nsecond-stage transformer with batch size 32 for 10 epochs.\nThen we use a learning rate10\n−5 and batch size 8 for train-\ning all the parameters of the model for 10 epochs again.\nThe smaller batch size for the second stage of training is\ndue to the larger model capacity during training. We apply\nearly stopping for both baseline models and the two training\nstages of our approaches. When training, the ground truth\nconversation tree structure is used to generate the mask. Dur-\ning evaluation phase, we use the predicted conversation tree\nstructure by the model to generate the mask.\nTable 4 shows the results of the baseline models and our\nmodel in the Reddit-small dataset. We also include a naive\nmethod calledPredict first as a baseline, that simply\nreturns the title comment in the Reddit conversation. Also,\nwe include ELMo\n8 as an alternative for the word embedding\nfor the two embedding-based baselines, namely the decom-\nposable attention model and the ESIM model. Finally, we\nalso use the BERT model that makes pairwise decisions, as\na state-of-the-art baseline. As shown in Table 4,Predict\nfirst is a strong baseline due to the fact the conversation\ntrees are relatively ﬂat in Reddit-small dataset (see Table 1).\nAdding ELMo embedding improve the model accuracy. We\nrun our masked hierarchical transformer model with 5 initial\nrandom seeds and report the average and the standard devia-\n8This paper uses ELMo embedding from Tensorﬂow Hubs\nhttps://tfhub.dev/google/elmo/2.\ntion of the score. As shown in the table, our novel approach\noutperforms the baseline models by a large margin in both\ngraph accuracy and conversation accuracy.\nMethod Graph Acc. Conv. Acc.\nPredict ﬁrst 45.60% 15.28%\nDecom. Atten. w/ Glove 43.08% 12.22%\nDecom. Atten. w/ ELMo 45.82% 12.92%\nESIM w/ Glove 42.92% 12.08%\nESIM w/ ELMo 48.43% 14.31%\nBERT 55.86% 17.78%\nOur Approach 60.53% 20.61%\n(0.34%) (0.47%)\nTable 4: Performance of the models in Reddit-small dataset.\nThe numbers in the parentheses are the standard deviation of\nthe score over 5 runs.\nShown in Table 5 is the performance comparison of all\nthe models on Reddit-large dataset. Again, we see that our\napproach outperforms the baselines by a large margin.\nMethod Graph Acc. Conv. Acc.\nPredict ﬁrst 33.60% 0.06%\nDecom. Atten. w/ Glove 24.86% 0.02%\nESIM w/ Glove 18.79% 0.00%\nBERT 39.48% 0.04%\nOur Approach 42.87% 0.13%\nTable 5: Performance of the models in Reddit-large dataset\nModel Performance on Ubuntu IRC dataset\nCompared with the Reddit datasets, an utterance in the\nUbuntu IRC dataset may have more than one parent. There-\nfore, we consider the binary cross-entropy loss deﬁned in\nEquation (2) for the Ubuntu IRC disentanglement problem.\nFor training the masked hierarchical transformer model,\nwe include the most 40 recent messages including the target\nmessage to determine the parent message(s). Since the mes-\nsages are usually short, we limit their length to 36 subwords\nafter tokenization. In this dataset, there are no parent mes-\nsage annotations for the ﬁrst 1000 messages of each part,\nknown as the context messages. Therefore, we deﬁned the\nmessages itself as its parent for all the context messages.\nSimilar to the Reddit datasets, we use Adam optimizer\nwith a learning rate10\n−5 for pre-training the top transformer\nwith batch size 32 for 10 epochs. Then we use a learn-\ning 10\n−7 and batch size 4 for training all the parameters of\nthe model for 10 epochs again. Compared with the training\nconﬁguration used for the Reddit datasets, it uses a smaller\nlearning rate and batch size due to larger model size dur-\ning computation and fewer samples. Again, early stopping\nis used for these two training stages. Also, since most of the\nutterances in this dataset have only one parent, we consider\n9745\nthe utterance in the conversation history that has the largest\nprobability to be the parent of a target utterance during test\ntime.\nAs shown in (Kummerfeld et al. 2019), feature-based\nmodels work well on this dataset. The features consist of\na few global-level features including year and frequency of\nthe conversation, utterance level features including types of\nmessage, targeted or not, time difference between the last\nmessage, etc., and utterance pair features including how far\napart in position and time between the messages, whether\none message targets another, etc. Since these features have\nproved to be very useful in predicting the parent relationship\nin the Ubuntu IRC dataset, we also consider concatenating\nthem with the utterance features vector before feeding to the\nsecond-stage transformer.\nGraph metrics for Ubunut IRC dataset are ﬁrst considered\nincluding the precision, recall and F1 scores for the parent\nrelationship prediction. We ﬁrst consider the performance of\nthe baseline models without any additional features. For the\ndecomposable attention model and ESIM, we use the Glove\nembedding by (Kummerfeld et al. 2019). Our approach,\nshown as the second group in Table 6, outperforms the base-\nline models signiﬁcantly. Besides, we consider models that\nuse those features in (Kummerfeld et al. 2019). Since our test\nset is the development set in their paper and no results have\nbeen reported, we train the Linear and FeedForward\nmodel in their paper under our train/dev split and evaluate\nthem on our test set. Also, we include those features into\nboth baseline models and our model in the results shown as\nthe third group in Table 6. Again, the performance of our\nmodel is the best among all those models.\nMethod P R F\nPrevious 30.8 29.5 30.2\nLinear * (Kummerfeld et al. 2019) 64.2 61.6 62.9\nFF * (Kummerfeld et al. 2019) 69.3 66.5 67.9\nDecom. Atten. w/ Glove 27.6 26.6 27.1\nESIM w/ Glove 28.7 27.8 28.2\nBERT 34.8 33.7 34.2\nOur Approach 53.9 51.7 52.8\nDecom. Atten. w/ Glove+F* 65.8 63.1 64.4\nESIM w/ Glove+F* 64.7 61.9 63.3\nB E R T+F* 67.6 64.9 66.2\nOur Approach + F * 73.2 69.2 70.6\nTable 6: Graph results on the Ubuntu IRC test set (Dev set in\n(Kummerfeld et al. 2019)). The methods with sufﬁx ‘*’ are\nusing the additional hand-crafted features in (Kummerfeld\net al. 2019). The baseline models with ‘+F’ sufﬁx are with\nthe additional features.\nTable 7 shows the conversation results of the baseline\nmethods and our proposed one using the same evaluation\nmetrics as (Kummerfeld et al. 2019). In particular, a cluster\nin a conversation is deﬁned as a connected component in the\nconversation graph. Three types of metrics are considered as\nthe modiﬁed V ariation of Information (VI) in (Kummerfeld\net al. 2019), One-to-One Overlap (1-1) of the cluster (El-\nsner and Charniak 2008), and the precision, recall, and F1\nscore between the cluster prediction and ground truth. Our\napproach combined with the additional features achieves the\nbest performance among all the metrics. This suggests our\nmodel is both good at ﬁnding the correct parent of an utter-\nance and reconstructing the whole conversation.\nMethod VI 1-1 P R F\nPrevious 66.2 23.7 0.0 0.0 0.0\nLinear * 87.5 66.8 17.8 26.0 21.1\nFF * 88.8 71.3 28.0 32.7 30.2\nDecom. Atten 70.3 39.8 0.6 0.9 0.7\nESIM 72.1 44.0 1.4 2.2 1.8\nBERT 74.7 45.4 2.2 3.6 2.7\nOur Approach 82.1 59.6 8.7 12.6 10.3\nDecom. Atten.+F* 87.4 66.6 18.2 25.1 21.1\nE S I M+F* 87.7 65.8 18.9 28.3 22.6\nB E R T+F* 89.5 71.7 21.4 30.0 25.0\nOur Approach+F* 89.8 75.4 35.8 32.7 34.2\nTable 7: Conversation results on the Ubuntu IRC test set (dev\nset in (Kummerfeld et al. 2019)). The methods with sufﬁx\n‘*’ are using the additional hand-crafted features in (Kum-\nmerfeld et al. 2019). The baseline models with ‘+F’ sufﬁx\nare with the additional features.\nAblation Studies\nImportance of the Mask The ablation study is performed\non Reddit-small dataset. We aim to show the importance of\nthe mask used in our model. First, we consider training an al-\nternative model of the hierarchical transformer model with-\nout masking. In this case, the utterance is able to attend to all\nthe utterances. Intuitively, it is hard for this model to come\nup with a good prediction, since attention may be distracted\nby all the utterances. The results in Table 8 show a huge drop\nof performance when the no mask is used, indicating that the\nancestor history plays a very important role in recovering the\nstructure of the conversation.\nMethod Graph Acc. Conv. Acc.\nOur Approach w/o mask 46.43% 15.14%\nOur Approach 60.53% 20.61%\nTable 8: Performance of the models in Reddit-small dataset\nfor ablation study.\nImportance of the Ancestor Depth Alternatively, an-\nother way of creating the mask is to use only the parent-child\nrelation between two utterances. In other words, an utterance\ncan only attend to its immediate parent besides the target ut-\nterance. However, since the second-stage transformer is not\nvery deep, it may take many steps for the information of\none utterance to pass to its distant descendant. To validate\n9746\nthis hypothesis, we perform another ablation study by deﬁn-\ning the mask in the following way.We start from the mask\ndescribed in the modeling section, then we reset some cell\nvalues in the mask to zero, if the distance between two utter-\nances in the conversation graph is greater thand. We calld\nthe ancestor depth of the mask in this ablation study. There\nare two special cases in this model. Whend =1 , the mask\nis deﬁned by the immediate parent-child ‘reply\nto’ relation.\nFor Reddit-small dataset, the maximum depth of the conver-\nsation tree is 12. Therefore, settingd =1 1coincides with\nthe mask in our model, where one non-target utterance is\nable to attend to all of its ancestors. We varyd and report\nthe performance of the model under differentd in the left\nplot of Figure 4. We see generally whendincreases, the per-\nformance for graph accuracy increases. This result suggests\nincluding more ancestor information helps in conversation\nstructure modeling.\nFigure 4: Performance of the model with respect to different\nancestor depth (d) and temporal depth (t).\nTemporal Mask Finally, we consider another variant of\nmask design. The mask is designed by including temporal\ninformation only. Again, all utterances are able to attend to\nthe target utterance. For a non-target utterance, it can attend\nto t most recent comments before itself. We callt the tem-\nporal depth in this ablation study. The right plot in Figure 4\nshows the model performances as a function of various val-\nues of t. There is a marginal increase whent increases, but\ngenerally, the model performance is far lower than our pro-\nposed ancestor based masking approach. This suggests that\nthe conversation structure information is more powerful than\ntemporal information.\nConclusion\nIn this paper, we consider a conversation structure modeling\ntask on Reddit and Ubuntu IRC datasets. We proposed sev-\neral baseline methods as well as a novel Masked Hierarchi-\ncal Transformer model, that explicitly utilizes the conversa-\ntion structure. Experiments on three datasets verify that our\nproposed model outperforms the baseline models. The re-\nsults show that taking into account the history and structure\nof the conversation helps in recovering the parent utterance.\nThere are some possible directions for future work. One\npotential improvement for the model is to reduce the gap of\nthe model during training and prediction since the gold con-\nversation structure is used during training and the predicted\nstructure is used during test time, known as ‘exposure bias’.\nSome techniques including schedule sampling (Bengio et al.\n2015) can be applied to alleviate this problem. Also, a beam\nsearch can be applied to decode a better conversation struc-\nture instead of using a greedy way to reconstruct the con-\nversation structure. Finally, for a better inference speed, it is\npossible to redesign the model so that the presentation vector\nof an utterance is built once. It can be done by excluding the\ntarget utterance in the mask transformer and replace the fully\nconnected layer with a siamese network-like component, de-\nciding if an utterance is the parent of the target utterance.\nReferences\nAfantenos, S.; Kow, E.; Asher, N.; and Perret, J. 2015. Dis-\ncourse parsing for multi-party chat dialogues. InProceed-\nings of the 2015 Conference on Empirical Methods in Nat-\nural Language Processing, 928–937. Lisbon, Portugal: As-\nsociation for Computational Linguistics.\nBengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015.\nScheduled sampling for sequence prediction with recurrent\nneural networks. In Cortes, C.; Lawrence, N. D.; Lee, D. D.;\nSugiyama, M.; and Garnett, R., eds.,Advances in Neural In-\nformation Processing Systems 28. Curran Associates, Inc.\n1171–1179.\nChen, Q.; Zhu, X.; Ling, Z.-H.; Inkpen, D.; and Wei, S.\n2018. Neural natural language inference models enhanced\nwith external knowledge. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational Linguis-\ntics (V olume 1: Long Papers), 2406–2417. Melbourne, Aus-\ntralia: Association for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. InProceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language T echnologies,\nV olume 1 (Long and Short Papers), 4171–4186. Minneapo-\nlis, Minnesota: Association for Computational Linguistics.\nDu, W.; Poupart, P .; and Xu, W. 2017. Discovering con-\nversational dependencies between messages in dialogs. In\nAAAI Conference on Artiﬁcial Intelligence.\nDziri, N.; Kamalloo, E.; Mathewson, K. W.; and Zaiane, O.\n2018. Augmenting neural response generation with context-\naware topical attention.arXiv preprint arXiv:1811.01063.\nElsner, M., and Charniak, E. 2008. Y ou talking to me? a\ncorpus and algorithm for conversation disentanglement. In\nProceedings of ACL-08: HLT, 834–842. Columbus, Ohio:\nAssociation for Computational Linguistics.\nHu, W.; Chan, Z.; Liu, B.; Zhao, D.; Ma, J.; and Yan, R.\n2019. Gsn: A graph-structured network for multi-party dia-\nlogues. arXiv preprint arXiv:1905.13637.\n9747\nJoty, S.; Carenini, G.; and Ng, R. T. 2012. A novel dis-\ncriminative framework for sentence-level discourse analy-\nsis. In Proceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and Compu-\ntational Natural Language Learning, EMNLP-CoNLL ’12,\n904–915. Stroudsburg, PA, USA: Association for Computa-\ntional Linguistics.\nKingma, D. P ., and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKummerfeld, J. K.; Gouravajhala, S. R.; Peper, J. J.;\nAthreya, V .; Gunasekara, C.; Ganhotra, J.; Patel, S. S.; Poly-\nmenakos, L. C.; and Lasecki, W. 2019. A large-scale cor-\npus for conversation disentanglement. In Proceedings of\nthe 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 3846–3856. Florence, Italy: Association\nfor Computational Linguistics.\nLowe, R.; Pow, N.; Serban, I.; and Pineau, J. 2015. The\nUbuntu dialogue corpus: A large dataset for research in un-\nstructured multi-turn dialogue systems. In Proceedings of\nthe 16th Annual Meeting of the Special Interest Group on\nDiscourse and Dialogue, 285–294. Prague, Czech Repub-\nlic: Association for Computational Linguistics.\nParikh, A.; T¨ackstr¨om, O.; Das, D.; and Uszkoreit, J. 2016.\nA decomposable attention model for natural language infer-\nence. In Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, 2249–2255.\nAustin, Texas: Association for Computational Linguistics.\nPennington, J.; Socher, R.; and Manning, C. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nPeters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextual-\nized word representations. InProceedings of the 2018 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language T echnolo-\ngies, V olume 1 (Long Papers), 2227–2237. New Orleans,\nLouisiana: Association for Computational Linguistics.\nShen, D.; Yang, Q.; Sun, J.-T.; and Chen, Z. 2006. Thread\ndetection in dynamic text message streams. InProceedings\nof the 29th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, SIGIR\n’06, 35–42. New Y ork, NY , USA: ACM.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. InAdvances in neural information\nprocessing systems, 5998–6008.\nWhang, T.; Lee, D.; Lee, C.; Yang, K.; Oh, D.; and Lim, H.\n2019. Domain adaptive training bert for response selection.\narXiv preprint arXiv:1908.04812.\nZhang, A.; Culbertson, B.; and Paritosh, P . 2017. Character-\nizing online discussion using coarse discourse sequences. In\nInternational AAAI Conference on W eb and Social Media.\nZhao, T., and Kawahara, T. 2019. Effective incorporation of\nspeaker information in utterance encoding in dialog.arXiv\npreprint arXiv:1907.05599.\n9748",
  "topic": "Conversation",
  "concepts": [
    {
      "name": "Conversation",
      "score": 0.9281963109970093
    },
    {
      "name": "Computer science",
      "score": 0.7479138374328613
    },
    {
      "name": "Utterance",
      "score": 0.7064540982246399
    },
    {
      "name": "Transformer",
      "score": 0.577263355255127
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5495440363883972
    },
    {
      "name": "Natural language processing",
      "score": 0.5216237306594849
    },
    {
      "name": "Automatic summarization",
      "score": 0.5160329341888428
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4603095054626465
    },
    {
      "name": "Speech recognition",
      "score": 0.32336676120758057
    },
    {
      "name": "Communication",
      "score": 0.1584516167640686
    },
    {
      "name": "Psychology",
      "score": 0.12405219674110413
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}