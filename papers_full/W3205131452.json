{
    "title": "CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation.",
    "url": "https://openalex.org/W3205131452",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4260542893",
            "name": "Nicolae-Catalin Ristea",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202717937",
            "name": "Andreea-Iuliana Miron",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3207391621",
            "name": "Olivian Savencu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3216626044",
            "name": "Mariana Iuliana Georgescu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1515313818",
            "name": "Nicolae Verga",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2110150181",
            "name": "Fahad Shahbaz Khan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1966268561",
            "name": "Radu Tudor Ionescu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963703618",
        "https://openalex.org/W1997259062",
        "https://openalex.org/W3107469378",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2753461941",
        "https://openalex.org/W1984909694",
        "https://openalex.org/W2123957845",
        "https://openalex.org/W2962722483",
        "https://openalex.org/W2808312419",
        "https://openalex.org/W3120218437",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3195874325",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3194757573",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3156621598",
        "https://openalex.org/W2921322750",
        "https://openalex.org/W2963373786",
        "https://openalex.org/W2910094941",
        "https://openalex.org/W3100018800",
        "https://openalex.org/W3026014384",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3161035636",
        "https://openalex.org/W3011200734",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3098269293",
        "https://openalex.org/W3024124733",
        "https://openalex.org/W3137561054",
        "https://openalex.org/W2962879692",
        "https://openalex.org/W2986571455",
        "https://openalex.org/W3204614423",
        "https://openalex.org/W3011645314",
        "https://openalex.org/W2966108228",
        "https://openalex.org/W3013034453",
        "https://openalex.org/W3199293888",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W3103645830",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2153431772",
        "https://openalex.org/W2617128058",
        "https://openalex.org/W2771491591",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2752785527",
        "https://openalex.org/W2995910740",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W3166448862",
        "https://openalex.org/W2963073614"
    ],
    "abstract": "We propose a novel approach to translate unpaired contrast computed tomography (CT) scans to non-contrast CT scans and the other way around. Solving this task has two important applications: (i) to automatically generate contrast CT scans for patients for whom injecting contrast substance is not an option, and (ii) to enhance alignment between contrast and non-contrast CT by reducing the differences induced by the contrast substance before registration. Our approach is based on cycle-consistent generative adversarial convolutional transformers, for short, CyTran. Our neural model can be trained on unpaired images, due to the integration of a cycle-consistency loss. To deal with high-resolution images, we design a hybrid architecture based on convolutional and multi-head attention layers. In addition, we introduce a novel data set, Coltea-Lung-CT-100W, containing 3D triphasic lung CT scans (with a total of 37,290 images) collected from 100 female patients. Each scan contains three phases (non-contrast, early portal venous, and late arterial), allowing us to perform experiments to compare our novel approach with state-of-the-art methods for image style transfer. Our empirical results show that CyTran outperforms all competing methods. Moreover, we show that CyTran can be employed as a preliminary step to improve a state-of-the-art medical image alignment method. We release our novel model and data set as open source at: https://github.com/ristea/cycle-transformer.",
    "full_text": "CyTran: A Cycle-Consistent Transformer with Multi-Level Consistency for\nNon-Contrast to Contrast CT Translation\nNicolae-C˘ at˘ alin Risteaa,b,c, Andreea-Iuliana Mirond,e, Olivian Savencud,e, Mariana-Iuliana Georgescua,\nNicolae Vergad,e, Fahad Shahbaz Khanc,f, Radu Tudor Ionescua,g,∗\naDepartment of Computer Science, University of Bucharest, Romania\nbDepartment of Telecommunications, University Politehnica of Bucharest, Romania\ncMohamed bin Zayed University of Artiﬁcial Intelligence, UAE\ndColt ¸ea Hospital, Romania\neCarol Davila University of Medicine and Pharmacy, Romania\nfCVL, Link¨ oping University, Sweden\ngRomanian Young Academy, University of Bucharest, Romania\nAbstract\nWe propose a novel approach to translate unpaired contrast computed tomography (CT) scans to non-contrast\nCT scans and the other way around. Solving this task has two important applications: (i) to automatically\ngenerate contrast CT scans for patients for whom injecting contrast substance is not an option, and (ii) to\nenhance the alignment between contrast and non-contrast CT by reducing the diﬀerences induced by the contrast\nsubstance before registration.\nOur approach is based on cycle-consistent generative adversarial convolutional transformers, for short, Cy-\nTran. Our neural model can be trained on unpaired images, due to the integration of a multi-level cycle-\nconsistency loss. Aside from the standard cycle-consistency loss applied at the image level, we propose to apply\nadditional cycle-consistency losses between intermediate feature representations, which enforces the model to\nbe cycle-consistent at multiple representations levels, leading to superior results. To deal with high-resolution\nimages, we design a hybrid architecture based on convolutional and multi-head attention layers. In addition,\nwe introduce a novel data set, Coltea-Lung-CT-100W, containing 100 3D triphasic lung CT scans (with a total\nof 37,290 images) collected from 100 female patients (there is one examination per patient). Each scan con-\ntains three phases (non-contrast, early portal venous, and late arterial), allowing us to perform experiments to\ncompare our novel approach with state-of-the-art methods for image style transfer.\nOur empirical results show that CyTran outperforms all competing methods. Moreover, we show that\nCyTran can be employed as a preliminary step to improve a state-of-the-art medical image alignment method.\nWe release our novel model and data set as open source at:https://github.com/ristea/cycle-transformer.\nOur qualitative and subjective human evaluations reveal that CyTran is the only approach that does not\nintroduce visual artifacts during the translation process. We believe this is a key advantage in our application\ndomain, where medical images need to precisely represent the scanned body parts.\nKeywords: Transformers, generative adversarial transformers, deep learning, cycle-consistency, image\ntranslation, image registration, computed tomography, triphasic lung CT.\n1. Introduction\nPatients undergoing computed tomography (CT) screening may not be able to get intravenous injections\nwith contrast agents due to allergies [1] or other medical conditions, e.g. muscular dystrophy, causing low blood\ncreatinine levels. However, the contrast substance plays a very important role in helping medical experts to\ndetect and delimit certain lesions, e.g. malignant tumors [2]. For instance, radiotherapy heavily relies on the\naccurate segmentation of tumors.\nCertainly, the majority of patients are healthy enough to be injected with contrast agents, enabling medical\nexperts to obtain triple-phase CT scans which provide a signiﬁcantly more clear picture of the malignant lesions.\n∗Corresponding author: E-mail: raducu.ionescu@gmail.com\nPreprint submitted to Neurocomputing April 6, 2023\narXiv:2110.06400v3  [eess.IV]  5 Apr 2023\nThe triple-phase CT includes a native phase (before the contrast is injected), a portal venous phase (right after\nthe contrast is injected), and a late arterial phase (when the contrast passes into the arteries). As the three\nphases are in successive temporal order, the corresponding CT scans are taken in diﬀerent moments in time,\ninherently leading to a misalignment between scans caused by slight movements of the patient, e.g. due to\nbreathing. In this scenario, some image registration method can be employed to align the CT scans. However,\ndue to Hounsﬁeld unit (HU) diﬀerences between contrast and non-contrast CT in certain types of tissue, the\nimage alignment task becomes problematic, especially for the regions of interest, where tumors are located.\nA method capable of translating between contrast and non-contrast CT scans, in both directions, is likely to\nsolve the two problems presented above. Indeed, when patients are unable to undergo contrast CT screening,\nthe medical experts could simply employ the automated translation technique to generate contrast CT scans\ncorresponding to the venous or arterial phases. When CT scans need to be aligned, the same translation\ntechnique can be applied prior to the alignment step to remove the eﬀect of the contrast substance, thus\nallowing for a better alignment. Once the displacement ﬁeld is generated, it can be applied on the unmodiﬁed\ncontrast CT scan to obtain the ﬁnal alignment result.\nIn this work, we present a novel deep learning model to translate between contrast and non-contrast CT scans.\nOur method relies on the success of generative adversarial networks (GANs) [3] in image-to-image translation [4],\na class of vision tasks where the goal is to learn the mapping between an input image and an output image using\na training set of paired or unpaired images from two diﬀerent domains. Although GANs have been previously\nemployed for medical image-to-image translation [5, 6, 7, 8], we underline that the task of translating between\ncontrast and non-contrast CT scans is very challenging due to the requirement of recognizing speciﬁc tissue\ntypes, anatomical structures, and even tumors, which exhibit signiﬁcant HU changes between contrast phases.\nFailing to recognize such anatomical structures raises the possibility of introducing unrealistic information.\nThis would evidently lead to unreliable synthetic images, which cannot be used for diagnosis or treatment\npurposes. Convolutional neural networks fail to recognize the global structure of objects in natural images [9],\nand, for the same reason, can fail to recognize anatomical structures in CT scans. To this end, we propose a\ncycle-consistent generative adversarial transformer, called CyTran, which has a higher capacity of recognizing\nglobal structure due to the incorporated transformer block. Since pure vision transformers cannot process high-\nresolution images due to the high number of learnable parameters involved, we design a hybrid architecture\ncomprising both convolutions and multi-head attention, such that it can generate full-resolution CT scans. To\navoid mode collapse, we apply a cycle-consistency loss between each input image and the image translated back\nto its original domain, following Zhu et al. [4]. Along with the standard cycle-consistency loss applied at the\nimage level, we introduce two additional cycle-consistency losses between intermediate feature representations,\nenforcing the model to be cycle-consistent at multiple representations levels.\nTo demonstrate the applicability of CyTran on the important cases exempliﬁed above, we introduce the\nColtea-Lung-CT-100W data set formed of 100 3D anonymized triphasic lung CT scans of female patients. For\neach patient, there are three CT scans corresponding to the native, venous and arterial phases, respectively.\nWe conduct a set of experiments to compare CyTran with other state-of-the-art style transfer methods, namely\nCycleGAN [4], pix2pix [10], U-GAT-IT [11], CWT-GAN [12] and AttentionGAN [13]. Both automatic and\nhuman evaluations indicate that our approach is consistently better than the competing methods. We perform\nanother set of experiments to evaluate the applicability of style transfer methods in contrast to non-contrast CT\nalignment with a state-of-the-art 3D image registration method [14]. While the empirical results indicate that\nall style transfer methods are helpful, the highest performance improvements are brought by CyTran, conﬁrming\nthe superiority of our approach.\nTo the best of our knowledge, our contribution is fourfold:\n• We design a cycle-consistent generative adversarial transformer in medical imaging, demonstrating state-\nof-the-art results in style transfer between contrast and non-contrast CT scans.\n• We propose to introduce cycle-consistency losses at diﬀerent feature representation levels, improving the\nquality of the generated images.\n• We publicly release a novel data set containing triphasic lung CT scans.\n• We employ style transfer methods to enhance the alignment between contrast and non-contrast CT scans.\n2\n2. Related Work\n2.1. Transformers\nArchitectures based on self-attention, in particular transformers [15], have become the model of choice in\nnatural language processing (NLP). Thanks to the computational power and scalability of transformers, it has\nbecome possible to train models of unprecedented size [16]. With the ever growing size of models and data sets,\nthe performance improvements are constantly increasing. Considering the success of transformers in NLP [16],\narchitectures based on multi-head self-attention have been adopted in the computer vision community, attaining\nsuperior results in various tasks [17, 18, 19, 20]. Wu et al. [19] proposed a convolutional transformer that improves\nthe Vision Transformer (ViT) [17] in terms of performance and eﬃciency by introducing convolutions into the\nmodel, in an attempt to take advantage of both designs. We further replace the multi-layer perceptron from\nthe transformer block proposed by Wu et al. [19] with pointwise convolutional layers, allowing us to incorporate\nthe resulting block into an eﬃcient generative model that can produce high-resolution images. Zhang et al. [20]\nproposed a cycle-consistent attention mechanism for semantic segmentation. In contrast, we introduce a novel\ngenerative model based on an eﬃcient convolutional transformer backbone, where the cycle consistency is\nimposed at multiple representation levels, not only at the latent feature level.\nIn medical imaging, the popularity of transformer architectures is rising [21, 22, 23, 24, 25], most likely be-\ncause such models bring state-of-the-art results. For instance, Gao et al. [22] proposed an eﬃcient self-attention\nmechanism which reduces computational complexity for cardiac magnetic resonance imaging (MRI) segmenta-\ntion. Korkmaz et al. [24] presented a zero-shot learning method employing a cross-attention transformer block\nto reconstruct MRI images. Luthra et al. [25] proposed an encoder-decoder network, which uses transformer\nblocks for CT image denoising. Diﬀerent from the medical imaging methods based on transformers, we introduce\na novel convolutional transformer architecture for style transfer between contrast and non-contrast CT images,\nwhich can be trained on unpaired data through the use of multiple cycle-consistency terms in the loss. To our\nknowledge, this is the ﬁrst work to propose cycle-consistent transformers in medical imaging.\n2.2. Image Translation\nSince the introduction of GANs [3] in 2014, a large body of research has focused on theoretical and architec-\ntural changes [26, 10, 27, 28, 4], giving rise to a wide adoption of GANs across various generative tasks, including\nimage translation [10, 11, 12, 13, 4]. In 2016, the pix2pix framework [10] became one of the ﬁrst GAN models\nto address the task of image-to-image translation from a source domain image, e.g. a springtime landscape, to a\ncorresponding target domain image, e.g. a winter landscape, provided that paired images from the two diﬀerent\ndomains are available for training. To overpass the lack of paired data sets for style transfer, researchers have\ndeveloped methods for unpaired image-to-image translation [4]. Zhu et al. [4] solved the problem by using two\ngenerators, one that translates a source image to the target domain, and the other to translate the translated\nimage back to the source domain. The two generators are optimized such that the image passing through the\ntwo generators is close to the original input image, ensuring the cycle-consistency of the framework. More\nrecently, Lai et al. [12] introduced a cross-model weight transfer mechanism to transfer a certain proportion of\nthe weights from the discriminator to the generator, after each training iteration. Tang et al. [13] observed that\nstate-of-the-art GANs for unpaired image translation still generate visual artifacts, proposing to alleviate this\nproblem by training attention-guided generators to produce attention masks that are fused with the generated\nimage, thus increasing image quality. They introduced attention in the discriminator as well, ensuring that it\nfocuses on attended regions.\nGANs have also been adopted in medical imagining, often being employed for medical image translation [29,\n30, 7, 31, 32, 33]. For example, Seo et al. [32] proposed a two-stage algorithm to address style transfer between\ncontrast and non-contrast CT images. The ﬁrst stage removes the poor alignment eﬀects, while the second\nstage relies on a GAN architecture to enhance the contrast of CT images. Other approaches used the pix2pix\nframework in applications where paired images are available, such as positron emission tomography (PET) to\nMRI translation [29], organ segmentation [34], MRI to CT translation [30], and low-dose CT denoising [33].\nMore recently, researchers started using CycleGANs [4] for various medical imaging tasks [7, 31]. For instance,\nKearney et al. [7] employed a CycleGAN [4] to translate between MRI and CT data. Modanwal et al. [31]\nproposed an algorithm that modiﬁes CycleGAN by introducing two discriminators to translate between diﬀerent\nMRI images. Closer to our task, Chandrashekar et al. [6] proposed an algorithm which relies on CycleGAN to\nenhance the contrast of CT images. To the best of our knowledge, none of the related methods are based on\ntransformer architectures. We provide empirical evidence showing that cycle-consistent transformers outperform\n3\narchitectures based on pix2pix or CycleGAN when it comes to contrast to non-contrast CT translation and back.\nFurthermore, to the best of our knowledge, the idea of optimizing features at multiple levels to become cycle-\nconsistent has not been explored so far in medical imaging.\n2.3. Image Registration\nMedical image registration is a fundamental problem that improves visual inspection, diagnosis, and treat-\nment planning. It refers to the task of establishing spatial correspondences between points in a pair of ﬁxed\nand moving images through a spatially varying deformation model. The state-of-the-art methods for medical\nimage alignment are based on deep neural networks [35, 36, 37, 38, 39, 40, 41]. A recent registration approach\nproposed in [41] is based on a recursive cascade algorithm which assumes that, at each step, the neural model\nlearns to perform a progressive deformation of the current warped image. Nevertheless, the trend of applying\ntransformers has recently been adopted in medical image registration as well. However, Chen et al. [37] claimed\nthat architectures based solely on transformers emphasize the low-resolution features because of the consecutive\ndownsampling operations, resulting in a lack of detailed localization information that aﬀects image registration\nperformance. To alleviate this problem, the authors combined transformers with convolutional layers into an\narchitecture called ViT-V-Net, which improves the recovery of localization information.\nDiﬀerent from other medical image registration approaches, we employ CyTran as a data augmentation\nmethod to improve alignment results. The augmentation consists of adding training examples of non-contrast\nCT scans that are synthetically generated by CyTran. As a secondary contribution, we extend ViT-V-Net [37]\nby employing multiple cascades at inference time, further improving the registration results by a considerable\nmargin.\n2.4. Data Sets\nIn recent years, the open-source access to large medical databases [42, 43, 44, 45, 46] has accelerated the\ndevelopment of deep learning methods in the medical imaging ﬁeld. The organizers of the CHAOS challenge\n[47] released a medical data set containing CT and MRI data. The CT data was acquired from the upper\nabdomen area of 40 patients during the portal venous phase, after contrast agent injection. Moen et al. [48]\ndeveloped a data set of CT scans from 299 patients for three types of clinical exams: non-contrast head CT\nscans, low-dose non-contrast chest scans and contrast-enhanced CT scans of the abdomen. Bilic et al. [49]\nreleased a data set which consists of 140 CT scans, each having ﬁve organs labeled: lung, bones, liver, kidneys\nand bladder. The data set blends examples from a wide variety of sources, including abdominal and full-body,\ncontrast and non-contrast, low-dose and high-dose CT scans. The data sets of Moen et al. [48] and Bilic et\nal. [49] contain both contrast and non-contrast CT scans, but these are taken for diﬀerent body sections. In\ncontrast, our data set contains contrast and non-contrast CT scans of the same body section, the chest. To the\nbest of our knowledge, Coltea-Lung-CT-100W is the ﬁrst public data set formed entirely of triphasic lung CT\nscans, meaning that there are three 3D scans for each patient, corresponding to the native, early portal venous,\nand late arterial phases, respectively.\n3. Image Translation Method\nWe propose a cycle-consistent generative adversarial transformer, which employs a generative visual trans-\nformer network to translate lung CT images between two diﬀerent contrasts, e.g. native, venous or arterial. Our\napproach is inspired by the success of cycle-consistent GANs [4] in image-to-image translation for style transfer.\nBased on the assumption that style is easier to transfer than other aspects, e.g. geometrical deformations, cycle-\nGANs can replace the style of an image with a diﬀerent style, while keeping its content. Our task involves style\ntransfer between lung CT images acquired in diﬀerent contrast phases. The contrast substance introduces HU\nchanges for speciﬁc anatomical structures, such as tumors or blood vessels. However, the structures themselves\nshould not exhibit geometrical changes between contrast phases, other than those caused by small movements of\nthe patient, such as movements generated by respiration. While the changes between diﬀerent contrast phases\ncan be assimilated to style changes, we underline that the changes apply only to speciﬁc anatomical structures.\nHence, to accurately mimic the contrast changes, the employed style transfer model should be capable of rec-\nognizing anatomical structures. We believe that models equipped with the power to extract and use global\ninformation have a higher capacity of recognizing anatomical structures. We thus conjecture that generative\ntransformers can outperform convolutional generators at the task of learning to reproduce or unwind the changes\ncaused by the contrast substance.\n4\nWe propose a cycle-consistent architecture based on generative adversarial transformers, termed CyTran,\nto transfer CT scans between diﬀerent contrast phases. Following the CycleGAN framework [4], CyTran is\nformed of two discriminators and two generators. The neural architecture of the discriminators is identical to\nthe architecture used by Zhu et al. [4]. In a preliminary evaluation stage, we tried to replace the convolutional\ndiscriminators with transformers, but we observed that this change makes the discriminators too powerful with\nrespect to the generative transformers. For this reason, we turned our attention to replacing the generative\nmodels only. We next describe in detail the proposed generative architecture as well as the entire optimization\nprocess.\n3.1. Generative Convolutional Transformer Architecture\nAs we aim to beneﬁt from the modeling power of transformers while being able to generate high-resolution\nCT images, we design a generative convolutional transformer with a manageable number of parameters. As illus-\ntrated in Figure 1, our generator is formed of a convolutional downsampling block, a convolutional transformer\nblock, and a deconvolutional upsampling block. We underline that, without the convolutional downsampling\nblock and the replacement of dense layers with convolutional layers inside the transformer block, the transformer\nwould not be able to learn to generate images larger than 64 ×64 pixels, due to memory overﬂow (measured on\nan Nvidia GeForce RTX 3090 GPU with 24GB of VRAM). In contrast, we need a model capable of generating\nCT slices of 512 ×512 pixels. At this input resolution, our design changes signiﬁcantly reduce the number of\nlearnable parameters from 258 millions to 3.5 millions.\n3.2. Downsampling Block\nThe downsampling block starts with a convolutional layer formed of 32 ﬁlters with a spatial support of 7×7,\nwhich are applied using a padding of 3 pixels to preserve the spatial dimension, while enriching the number of\nfeature maps to 32. Next, we apply three convolutional layers composed of 32, 64 and 128 ﬁlters, respectively.\nAll convolutional ﬁlters have a spatial support of 3 ×3 and are applied at a stride of 2, using a padding of 1.\nEach layer is followed by batch-norm [50] and Rectiﬁed Linear Units (ReLU) [51]. Let T ∈Rh×w×c denote the\noutput tensor of the downsampling block. For an input CT slice of 512 ×512 pixels, the dimensions of T are\n64 ×64 ×128.\n3.3. Convolutional Transformer Block\nThe downsampling block is followed by the convolutional transformer block, which provides an output tensor\nof the same size as the input tensor. Our convolutional transformer block is inspired by the block proposed by\nWu et al. [19].\nThe input tensor T is interpreted as a set of h·woverlapping visual tokens. In our implementation, we have\n64 ·64 = 4096 tokens, where each token is a tensor of 3 ×3 ×128 components. The spatial dimensions of the\nvisual tokens are determined by the receptive ﬁeld of the ﬁlters in the next convolutional layer.\nConvolution projection. In a vanilla transformer, a sequence of tokens is typically projected onto a set of\nweight matrices to obtain the queries Q, the keys K, and the values V . Following Wu et al. [19], the projections\ntypically implemented through matrix multiplication are replaced by depthwise separable convolution opera-\ntions, referred to as convolutional projection. The convolutional projection is formed of three nearly identical\nprojection blocks, with separate parameters. Each projection is a depthwise separable convolution block [52]\nformed of two convolutional layers and a batch-normalization layer in between.\nThe ﬁrst layer in a projection block is a depthwise convolution with 128 ﬁlters, each having a receptive ﬁeld\nof 3 ×3. The projection block producing the queries is conﬁgured with a stride of 1 (generating activation maps\nof 64 ×64), while the other projection blocks use a stride of 2 (generating activation maps of 32 ×32). The\npadding is 1 for all three blocks. The output passes through a batch-norm, before going into the third layer.\nThe third layer applies pointwise convolution with 64 ﬁlters. We hereby note that, in pointwise convolution,\nthe ﬁlters always have a spatial support of 1 ×1 and are applied at a stride of 1, without padding. Finally,\nthe output tensors are reshaped into matrices by ﬂattening the activation maps, while preserving the number\nof channels.\nLet WQ, WK and WV denote the learnable parameters of the three projection blocks. The query, key and\nvalue embeddings are computed as follows:\nQ = conv projection (T,WQ) ,\nV = conv projection (T,WV ) ,\nK = conv projection (T,WK) ,\n(1)\n5\nQ\nK\nV\n M\nulti-head attention\nNorm\n layer\nPointwise convolution\nConvolutional projection\nConvolutional transformer block Upsampling blockDownsampling block G’\nU*\nDownsampling block F’Upsampling block\nQ\nK\nV\n M\nulti-head attention\nNorm\n layer\nPointwise convolution\nConvolutional projection\nConvolutional transformer block\nU*\nX\nTG\nZG\nG(X)=Ŷ\nŶ\nT̂FẐF\nF(G(X))=X̂\n‖^X − X‖1\nGenerator G\nGenerator F\n‖^ZF −TG‖1 ‖^T F −ZG‖1\nDiscriminator DX\n0.1\n0.9\nLGAN 1\n0\nDense layer\nDiscriminator DY\n0\n1\n0.2\n0.8\nDense layer\nReal\nFake\nReal\nFake\nReal images from domain X\nReal images from domain Y\nx\nx\nx\n=Lcycle =Lcycle_r1 =Lcycle_r2\nLGAN\nG”\nF”\nFigure 1: Our convolutional transformer for CT image translation from domain X to domain Y, based on multi-level cycle con-\nsistency. Each generator is formed of a downsampling block comprising convolutional layers, a convolution transformer block\ncomprising a multi-head self-attention mechanism, and an upsampling block comprising transposed convolutions. The source CT\nimage X is translated using the generative transformer G into the target CT image ˆY . The target CT image ˆY is translated back\nto the original domain X through the generative transformer F. The generative transformer G and the discriminator DY are opti-\nmized in an adversarial fashion, just as in any other GAN. In addition, the model is optimized with respect to the cycle-consistency\nlosses between the original source CT image X and the fake CT image ˆX, as well as between the intermediate representations TG\nand ˆZF on the one hand, and ZG and ˆTF on the other. Analogous steps are carried out for translating from domain Y to domain\nX (not represented in the image to improve clarity and ease readability). Best viewed in color.\nwhere Q ∈Rnq×dq , K ∈Rnk×dk and V ∈Rnv×dv . For the subsequent operation involving matrix multipli-\ncations, we need dq = dk, and nk = nv. In our implementation, nq = 4096 (obtained by ﬂattening 64 ×64\nactivation maps) and nk = nv = 1024 (obtained by ﬂattening 32 ×32 activation maps). Due to the equal\nnumber of ﬁlters in the pointwise convolution in all three blocks, dq = dk = dv = 64.\nWe underline that the goal of adding the convolutional projection is to achieve additional modeling power\n6\nof the local spatial context from the output of the subsequent multi-head attention layer.\nMulti-head self-attention. The convolutional projection layer is followed by a multi-head self-attention\nmechanism. The goal of self-attention is to capture the interaction among all tokens by encoding each entity in\nterms of the global contextual information. Given a sequence of items, the self-attention mechanism estimates\nthe relevance of an item to other items, e.g. which visual token embeddings are likely to come together in a\ntensor. Basically, the self-attention layer updates each visual token by aggregating global information from the\ncomplete input tensor.\nThe output U ∈Rnq×dv of the self-attention layer is given by:\nU = softmax\n(Q · K⊤\n√dk\n)\n·V , (2)\nwhere K⊤ is the transpose of K. For a given token, the self-attention computes the dot products of the query\nwith all keys, which are then normalized using the softmax operator to get the attention scores. According\nto Vaswani et al. [15], the magnitudes of the dot products grow proportionally to dk, propelling softmax into\nregions where it has extremely small gradients. To restore the gradient magnitudes, the dot products are scaled\nwith respect to dk. Each entity then becomes the weighted sum of all entities in the sequence, where the weights\nare given by the attention scores. At this point, U is a matrix of 4096 ×64 components. Upon reshaping\nthe 4096-dimensional vectors back into activation maps, we obtain a tensor of 64 ×64 ×64 components. In\norder to encapsulate multiple complex relationships among diﬀerent tokens in the tensor T, we employ a multi-\nhead attention module [15]. Each head i ∈{1,...,n h}comprises a convolution projection and a self-attention\nmechanism, having a particular set of learnable parameters{WQi,WKi,WVi}, where nh is the number of heads.\nFollowing the conﬁguration for the CvT blocks used in stage 3 by Wu et al. [19], we set nh = 6 in our block.\nTo form the output of the entire multi-head attention module, we concatenate the output tensors in the\nchannel dimension, obtaining a tensor of 64 ×64 ×384 components. A pointwise convolution with 128 ﬁlters\nbrings the dimension of the output tensor down to 64 ×64 ×128 components. Let U∗denote the ﬁnal output\ntensor of the multi-head self-attention module. We underline that the dimension of U∗ coincides with the\ndimension of the input tensor T, i.e. U∗∈Rh×w×c.\nPointwise convolution.After the multi-head attention layer, the output is summed up with the input of the\nconvolutional projection and fed into a batch-normalization layer. Unlike the vast majority of transformers, we\nintroduce a pointwise convolutional block instead of a multi-layer perceptron as the last processing step of the\ntransformer block, further reducing the number of learnable parameters. Our convolutional block is formed of\ntwo consecutive pointwise convolutional layers, the ﬁrst one being formed of 512 ﬁlters and the second one being\nformed of 128 ﬁlters. We use Gaussian Error Linear Units (GELU) [53] after the ﬁrst pointwise convolutional\nlayer.\nNext, the input of the norm layer is added to the output of our pointwise convolutional block, resulting in\nthe ﬁnal output of our convolutional transformer block, denoted as Z.\n3.4. Upsampling Block\nThe last block of our convolutional transformer applies upsampling operations, being designed to revert the\ntransformation of the downsampling block. The upsampling block is formed of three transposed convolutional\nlayers comprising 128, 64 and 32 ﬁlters, respectively. All kernels have a spatial support of 3 ×3, being applied\nat a stride of 2, using a padding of 1. As for the downsampling block, we introduce batch-norm and ReLU\nactivations after each transposed convolutional layer. Finally, we employ a convolutional layer with one ﬁlter\nto reduce the number of channels from 32 back to 1. The size of the receptive ﬁeld of this ﬁnal ﬁlter is 7 ×7.\nWe use a padding of 3 to preserve the spatial dimensions, obtaining an output image of 512 ×512 pixels.\n3.5. Learning on Unpaired CT Slices\nWe use the following notations throughout the remainder of this work. Let ( X,Y) denote the pair of source\nand target domains. Since we are interested in translating contrast CT scans to non-contrast CT scans and vice\nversa, the pair (X,Y) can take one of the following values: (native, arterial), (native, venous), (venous, native),\n(arterial, native). For our application purposes, we are not interested in translating the (arterial, venous) and\n(venous, arterial) pairs. Let X denote a sample from domain Xand Y a sample from domain Y, respectively.\nIn Figure 1, we illustrate the generative process based on cycle-consistency for the CT image X. The source\nCT image X is translated using the generative transformer G into ˆY , to make it seem that ˆY belongs to the\n7\ntarget domain Y. The target CT image ˆY is translated back to the original domain Xthrough the generative\ntransformer F. The generative transformer Gis optimized to fool the discriminator DY , while the discriminator\nDY is optimized to separate synthesized CT images from real samples, in an adversarial fashion. In addition,\nthe network is optimized with respect to the reconstruction error computed between the original sample X and\nthe cycle-generated CT sample ˆX. Adding the reconstruction error to the overall loss function ensures the cycle-\nconsistency for domain X. Moreover, we introduce additional losses to ensure the cycle-consistency at multiple\nrepresentation levels. More precisely, we propose to add a cycle-consistency loss between the intermediate\nrepresentations TG and ˆZF , and another cycle-consistency loss between the intermediate representations ZG\nand ˆTF , respectively.\nAn analogous training process is carried out to ensure cycle-consistency for domain Y. The complete loss\nfunction to optimize CyTran for contrast translation in both directions is:\nLCyTran (G,F,D X,DY ,X,Y ) = LGAN (G,DY ,X,Y )\n+ LGAN (F,DX,X,Y ) + λ·Lcycle(G,F, X,Y )\n+ β·(Lcycle r1(G,F, X,Y ) + Lcycle r2(G,F, X,Y )) ,\n(3)\nwhere, G and F are generative transformers, DX and DY are convolutional discriminators, X is a CT slice\nfrom contrast phase X, and Y is a CT slice from contrast phase Y. The hyperparameters λ and β control the\nimportance of the cycle-consistency losses with respect to the two GAN losses. The ﬁrst GAN loss is the least\nsquares loss that corresponds to the translation from domain Xto domain Y:\nLGAN (G,DY ,X,Y ) = EY ∼Pdata(Y )\n[\n(DY (Y ))2]\n+ EX∼Pdata(X)\n[\n(1−DY (G(X)))2]\n,\n(4)\nwhere E[·] is the expected value and Pdata(·) is the probability distribution of data samples. Analogously, the\nsecond GAN loss is the least squares loss that corresponds to the translation from domain Yto domain X:\nLGAN (F,DX,X,Y ) = EX∼Pdata(X)\n[\n(DX(X))2]\n+ EY ∼Pdata(Y )\n[\n(1−DX(F(Y )))2]\n.\n(5)\nThe cycle-consistency loss applied in Equation (3) between the inputs and the outputs of the generators G\nand F is deﬁned as the sum of cycle-consistency losses for both translations:\nLcycle(G,F, X,Y ) = EX∼Pdata(X) [∥F(G(X)) −X∥1]\n+ EY ∼Pdata(Y ) [∥G(F(Y )) −Y ∥1] , (6)\nwhere ∥·∥1 is the l1 norm.\nIn Equation (3), we introduce two cycle-consistency losses for the intermediate representations of the gener-\nators, but before deﬁning the losses, we need to introduce the required notations. Let TG and ZG represent the\nfeatures before and after the transformer block in G, and TF and ZF represent the features before and after the\ntransformer block in F. Let G′and F′denote the downsampling blocks of the generators Gand F, respectively.\nWith these notations, TG = G′(X), TF = F′(Y ), ˆTG = G′( ˆX) = G′(F(Y)) and ˆTF = F′( ˆY ) = F′(G(X)).\nSimilarly, let G′′ and F′′ denote the sub-networks formed of the downsampling block and the convolutional\ntransformer of the generators G and F, respectively. With these notations, ZG = G′′(X), ZF = F′′(Y ),\nˆZG = G′′( ˆX) = G′′(F(Y )) and ˆZF = F′′( ˆY ) = F′′(G(X)). Now, we deﬁne the cycle-consistency loss for the\nintermediate representations TG and TF before the convolutional transformer block as follows:\nLcycle r1(G,F, X,Y ) = ETG∼Pdata(G′(X)) [∥F′′(G(X)) −G′(X)∥1]\n+ ETF ∼Pdata(F′(Y )) [∥G′′(F(Y )) −F′(Y )∥1]\n= ETG∼Pdata(G′(X))\n[ˆZF −TG\n\n1\n]\n+ ETF ∼Pdata(F′(Y ))\n[ˆZG −TF\n\n1\n]\n.\n(7)\nAnalogously, we deﬁne the cycle-consistency loss for the intermediate representations ZG and ZF after the\n8\n(1)\n...\nMoving image\nX\nWarped image\nX(1)\nWarped image\nX(n)\nFigure 2: Cascaded algorithm for volumetric image registration. The moving 3D CT scan X is registered in a recursive cascade\nbased on n steps. Thus, X(i) is the warped image at step i, ∀i ∈ {1, ..., n}.\nconvolutional transformer block as follows:\nLcycle r2(G,F, X,Y ) = EZG∼Pdata(G′′(X)) [∥F′(G(X)) −G′′(X)∥1]\n+ EZF ∼Pdata(F′′(Y )) [∥G′(F(Y )) −F′′(Y )∥1]\n= EZG∼Pdata(G′′(X))\n[ˆTF −ZG\n\n1\n]\n+ EZF ∼Pdata(F′(Y ))\n[ˆTG −ZF\n\n1\n]\n.\n(8)\n4. Image Registration Method\nUnsupervised medical image registration can fail to properly align contrast CT scans to non-contrast CT\nscans, especially in the regions highlighted by the contrast agent, which are of primary interest. To alleviate\nsuch failure cases, we propose to employ CyTran on the contrast CT scans to eliminate diﬀerences induced\nby the contrast agent. We underline that switching the roles of contrast and non-contrast CT scans in image\nregistration leads to an equivalent problem. For simplicity, we only study the alignment of contrast CT scans\nto non-contrast CT scans.\nLet X ∈X denote a source 3D CT scan belonging to the contrast phase X(either venous or arterial), and\nY ∈Y a target 3D CT scan belonging to the non-contrast phase Y(native). Prior to the alignment of voxels\nin X to Y , we translate all slices in the CT scan X to the distribution Y, obtaining ˆX. Then, we align ˆX to\nY , both belonging to the same distribution Y, and we obtain the displacement ﬁeld M ˆX. Finally, we apply the\ndisplacement ﬁeld M ˆX to X in order to obtain the ﬁnal alignment result.\nTo perform the alignment, we rely on the state-of-the-art ViT-V-Net model introduced by Chen et al. [37].\nViT-V-Net is a hybrid convolutional-transformer architecture for self-supervised volumetric medical image reg-\nistration. The architecture contains three convolutional blocks and two max-pooling operations, which form\nthe ﬁrst part of the model, producing a high-level feature representation. The resulting feature maps are\nsubsequently divided into patches, which are further projected into tokens by a projection layer. The tokens\nare appended with positional embeddings, then fed into a transformer-based encoder. The transformer-based\nencoder consists of 12 transformer blocks comprising multi-head self-attention and dense layers, just as ViT\n[17]. Next, the output of the transformer-based encoder is reshaped and passed to a convolutional decoder.\nViT-V-Net also contains long skip connections between the encoder and the decoder. At the end, the output\nis decoded into a dense displacement ﬁeld. The ViT-V-Net model is optimized towards minimizing the mean\nsquared error between the moving image and the reference image. To ensure the smoothness of the displacement\nﬁeld, a diﬀusion regularizer is added to the optimization objective.\nAs another contribution, we extend ViT-V-Net [37] to a cascaded registration algorithm, which is intuitively\nillustrated in Figure 2. Let Rbe the registration model, X ∈Rh×w×d the moving 3D image, andX(1) ∈Rh×w×d\nthe warped image, such that R(X) = X(1). We propose to apply multiple cascades at inference time, by passing\nthe output several times through the model:\nR\n(\nX(i−1)\n)\n= X(i),∀i∈{1,...,n }. (9)\nThe recursive processing progressively reduces the alignment diﬀerences, leading to a superior result. This\nstatement is supported by the experiments presented below.\n9\nTable 1: The number of triphasic CT scans and individual slices from the Coltea-Lung-CT-100W data set.\nTraining Validation Test Total\n#scans 70 15 15 100\n#images 25,311 5,937 6,042 37,290\nFigure 3: A set of randomly selected samples from the Coltea-Lung-CT-100W data set. For each of the ﬁve samples, we illustrate\nthe native, arterial and venous phases, from top to bottom.\nFigure 4: Age distribution of the 100 Romanian female patients who agreed to donate their anonymized CT scans for research\npurposes and inclusion in the Coltea-Lung-CT-100W data set.\n5. Data Set\nWe release a novel data set entitled Coltea-Lung-CT-100W, which consists of 100 triphasic lung CT scans.\nThe scans are collected from 100 female patients and represent the same body section. There is one triphasic CT\nscan per patient. The slices are selected having as anatomical landmarks the 7th cervical vertebra cranially and\nthe 12th thoracic vertebra caudally. A triphasic scan is formed of a native (non-contrast) scan, an early portal\n10\nFigure 5: Manufacturer distribution of the CT scanners used to collect the CT scans from the Coltea-Lung-CT-100W data set.\nvenous scan, and a late arterial scan. In our data set, the three CT scans forming a triphasic scan always have\nthe same number of slices. Hence, each triphasic scan comes roughly aligned, i.e. the slice at index i gathered\nduring the native phase should correspond to the slice at index i gathered during each contrast phase. This is\nillustrated in Figure 3. The number of slices may diﬀer from one patient to another. Indeed, the number of\nslices per scan ranges between 64 and 229, and the total number of slices is 37 ,290. The size of a CT slice is\n512 ×512 pixels and the slice thickness varies between 1 .25 and 3 mm. The resolution of a pixel is 1 ×1 mm2.\nThe female patients are citizens of diﬀerent parts of Romania and belong to diﬀerent age groups, as illustrated\nthrough the bar chart presented in Figure 4. We refrain from providing more details about the patients to pre-\nserve their anonymity. The CT scans are produced by diﬀerent CT scanners from three diﬀerent manufacturers.\nThe distribution of scans per manufacturer is shown in Figure 5.\nWe split our data set into three subsets, one for training (70 scans), one for validation (15 scans), and one for\ntesting (15 scans). We report the number of slices in each subset in Table 1. Our data set is stored as anonymized\nraw DICOM ﬁles and can be freely downloaded from https://github.com/ristea/cycle-transformer. To\nthe best of our knowledge, Coltea-Lung-CT-100W is the only publicly available data set of triphasic lung CT\nscans.\n6. Experiments\nWe study two tasks on Coltea-Lung-CT-100W:\n• style transfer between contrast and non-constrast CT slices, considering the following pairs of contrast\nphases: native →venous, venous→native, native→arterial, arterial→native.\n• volumetric image registration of contrast CT scans to non-contrast CT scans, considering the following\npairs: venous→native, arterial→native.\n6.1. Style Transfer Experiments\n6.1.1. Baselines\nWe compare CyTran with ﬁve state-of-the-art style transfer methods. Since our data set is suitable for paired\nstyle transfer, we consider pix2pix [10] as the ﬁrst baseline. As CyTran is an approach capable of learning from\nunpaired images, we select four more baselines from the same category of methods, namely CycleGAN [4],\nU-GAT-IT [11], CWT-GAN [12], and AttentionGAN [13]. We note that GAN-based style transfer methods can\nintroduce visual artifacts during the generation process, which could be problematic in medical practice. We\nthus consider important comparing the generative models with a no-transfer baseline, which simply outputs the\nunprocessed input image. If this baseline outperforms a generative model, it indicates the respective model is\nunreliable, introducing too many artifacts.\n11\nTable 2: Style transfer results for the native →venous, venous→native, native→arterial and arterial →native contrast pairs. Our\nmodel is compared with several state-of-the-art baselines on the test set, in terms of MAE, RMSE and SSIM. The symbol ↑\nmeans higher values are better, while ↓ means lower values are better. There are three ablated versions of CyTran added to the\ncomparison: no attention (the transformer block is removed), no pointwise (the pointwise convolutions are replaced with dense layers,\nas in standard transformer blocks), no MLCC (our multi-level cycle-consistency loss is replaced with a standard cycle-consistency\nloss). The best results are highlighted in bold.\nMethod native→venous venous→native native→arterial arterial→native\nMAE\n↓\nRMSE\n↓\nSSIM\n↑\nMAE\n↓\nRMSE\n↓\nSSIM\n↑\nMAE\n↓\nRMSE\n↓\nSSIM\n↑\nMAE\n↓\nRMSE\n↓\nSSIM\n↑\nno-transfer 0.072 0.160 0.656 0.072 0.160 0.656 0.072 0.163 0.664 0.072 0.163 0.664\npix2pix [10] 0.070 0.165 0.729 0.076 0.180 0.646 0.064 0.157 0.738 0.075 0.174 0.648\nU-GAT-IT [11] 0.066 0.150 0.720 0.074 0.162 0.642 0.066 0.152 0.734 0.073 0.160 0.651\nCycleGAN [4] 0.066 0.150 0.724 0.071 0.160 0.660 0.065 0.154 0.729 0.072 0.160 0.662\nCWT-GAN [12] 0.069 0.153 0.726 0.072 0.159 0.656 0.065 0.153 0.733 0.071 0.163 0.663\nAttentionGAN [13] 0.064 0.150 0.730 0.072 0.159 0.659 0.064 0.151 0.737 0.073 0.159 0.659\nCyTran (no attention) 0.067 0.156 0.725 0.073 0.166 0.658 0.066 0.156 0.739 0.074 0.163 0.655\nCyTran (no pointwise) 0.066 0.151 0.733 0.073 0.164 0.660 0.065 0.150 0.741 0.072 0.159 0.662\nCyTran (no MLCC) 0.063 0.147 0.739 0.070 0.157 0.664 0.063 0.149 0.742 0.070 0.156 0.668\nCyTran (proposed) 0.061 0.144 0.7450.070 0.157 0.6640.059 0.147 0.7580.069 0.156 0.668\n6.1.2. Performance evaluation\nWe report the mean absolute error (MAE), root mean square error (RMSE) and structure-similarity index\nmeasure (SSIM) between the i-th translated image in a source scan and the slice at the same index i in the\ncorresponding target scan. The SSIM measures the capability of keeping original structures unchanged, while\nthe MAE and RMSE measure the ability of transferring the desired style, e.g. the ability of raising HU levels\nof a tumor in native →venous style transfer.\nFurthermore, we asked three medical experts (two radiotherapists and one oncologist with experience in\nradiology) to independently and blindly vote for the best among six style transfer methods: pix2pix, CycleGAN,\nU-GAT-IT, CWT-GAN, AttentionGAN and CyTran. The experts evaluated a total of 200 cases (with 8 slices\nper case to be analyzed), randomly picked from the test set. We selected 50 cases for each of the four contrast\npairs. One case is composed of an input slice, 6 translated slices (produced by the generative models) and the\ncorresponding target slice. All slices are provided in DICOM format and viewed with a specialized software,\nsuch that the experts can visualize and compare the slices under multiple window and level settings. The\nannotators were instructed to analyze the translated images in order to observe structural deformations, the\ncorrectness of contrast change with respect to the target image, and the occurrence of visual artifacts with\nrespect to the input CT image. To obtain reliable annotations, we relied on the many years (between 2 and\n35) of working experience of the medical experts in analyzing CT scans. During the annotation, we did not\ndisclose the matching between models and translated images to the experts, i.e. the votes are blind. With each\npresented case, the translated images were randomly shuﬄed, so it would be impossible for the experts to know\nwhich translation method produced a certain image. As evaluation measures, we report the number of votes\nand the corresponding percentage for each method.\n6.1.3. Data preprocessing\nTo avoid working with high values and ensure the stable training of deep learning models, we shifted the\nraw voxel values to the HU scale by subtracting the intercept (stored in the DICOM metadata), and we divided\nthe resulting values by 1000. We apply this preprocessing for all the models, including the baselines.\n6.1.4. Hyperparameter tuning\nWe train all generative models from scratch using Adam [54] for 70 epochs, on mini-batches of two examples.\nFor any additional hyperparameters of the baseline methods, we consider the default values proposed by the\nauthors introducing the respective models [4, 10, 11, 12, 13], along with additional conﬁgurations (comprising\n12\nTable 3: Subjective human evaluation results based on 50 cases randomly selected from the test set for each of the following contrast\npairs: native →venous, venous→native, native→arterial and arterial →native (there are 200 cases in total, with 6 slices per case).\nThe reported numbers represent blind votes awarded by three medical experts for each generative model.\nContrast pair\nExpert #1 Expert #2 Expert #3\npix2pix\nU-GAT-IT\nCycleGAN\nCWT-GAN\nAttentionGAN\nCyTran (ours)\npix2pix\nU-GAT-IT\nCycleGAN\nCWT-GAN\nAttentionGAN\nCyTran (ours)\npix2pix\nU-GAT-IT\nCycleGAN\nCWT-GAN\nAttentionGAN\nCyTran (ours)\nnative→venous 4 6 4 0 3 33 0 0 0 1 4 45 0 0 0 0 2 48\nvenous→native 3 3 2 6 0 36 0 2 7 0 6 35 0 0 8 0 3 39\nnative→arterial 0 2 2 1 4 41 0 0 0 0 5 45 0 0 0 0 4 46\narterial→native 0 3 3 0 4 40 0 0 6 0 4 40 0 0 3 0 4 43\nOverall votes 7 14 11 7 11 150 0 2 13 1 19 165 0 0 11 0 13 176\nOverall (%) 3.5% 7% 5.5% 3.5% 5.5% 75% 0% 1% 6.5% 0.5% 9.5% 82.5% 0% 0% 5.5% 0% 6.5% 88%\nvarious learning rates and loss weights) obtained through grid search. As we did not observe signiﬁcant im-\nprovements when changing the hyperparameters for any of the baseline models, we decided to keep the default\nhyperparameter values as indicated by the corresponding authors [4, 10, 11, 12, 13]. For our approach, we set\nthe learning rate to 10 −4, keeping the default values for the other parameters of Adam. We set the weights\nthat control the importance of the cycle-consistency terms in Equation (3) to λ = 10 and β = 1. Following\nZhu et al. [4], the same value of λ is used for CycleGAN. We provide the code to reproduce our results at:\nhttps://github.com/ristea/cycle-transformer.\n6.1.5. Quantitative results\nWe conducted style transfer experiments on four contrast pairs, comparing our approach against ﬁve state-\nof-the-art methods and the no-transfer baseline. The corresponding results are shown in Table 2. First of all,\nwe observe that CyTran is the only approach that consistently surpasses the no-transfer baseline across all\ncontrast pairs and evaluation metrics. For instance, our approach is the only one able to surpass the no-transfer\nbaseline for the arterial→native transfer. Although pix2pix [10] can leverage the paired nature of the data set, it\nseems to produce the lowest performance levels, being surpassed by CycleGAN [4], U-GAT-IT [11], CWT-GAN,\nAttentionGAN [13] and CyTran. All in all, our method attains the highest performance levels in each and every\nexperiment, always outperforming the baselines. This conclusion supports our conjecture that CyTran is a more\nsuitable method for style transfer between contrast and non-contrast CT images.\n6.1.6. Ablation study\nWe perform an ablation study to determine the impact of each component proposed in this work. The\nablation results are reported in Table 2. In our ﬁrst ablation experiment, we remove the transformer block,\nessentially transforming CyTran into a convolutional network without attention. This ablated conﬁguration\nresults in suboptimal results, being surpassed by most of the baselines. Hence, the ﬁrst ablation experiment\nreveals the importance of adding the transformer block inside CyTran. In the second ablation experiment,\nwe replace the pointwise convolutions with dense layers, as in standard transformer blocks. The resulting\narchitecture obtains scores that are comparable to the baselines. In the third ablation experiment, we replace\nour multi-level cycle-consistency (MLCC) loss with a standard cycle-consistency loss. This ablated version of\nCyTran surpasses all state-of-the-art models. Moreover, when transferring images to the native domain, the\nperformance level without MLCC is comparable to that of the full CyTran framework. This seems to indicate\nthat the multi-level cycle-consistency loss is mostly useful when transferring non-contrast (native) slices to\ncontrast (arterial or venous) slices. In summary, all our novel components are beneﬁcial to the proposed model.\n6.1.7. Subjective human evaluation results\nWhile the MAE, RMSE and SSIM metrics show that our method is the clear winner, the performance\nimprovements with respect to the second-best model seem rather small. To better assess the performance\n13\nsource pix2pix U-GAT-IT CycleGAN CyTran (ours) target\narterial→native native→arterial venous→native native→venous\nCWT-GAN AttentionGAN\nFigure 6: Examples of images translated by pix2pix, U-GAT-IT, CycleGAN, CWT-GAN, AttentionGAN and CyTran, respectively.\nThe input (source) and target images are displayed for reference. Defects become more visible when zooming in into this ﬁgure.\ndiﬀerences among the generative models, we turn our attention to the subjective evaluation study based on the\nannotations (votes) provided by three independent experts. On average, the experts spent between 16 and 29\nhours casting the votes for each presented case. The corresponding results are presented in Table 3.\nThe study shows that our approach was voted as the winner by all three experts, in all style transfer\nexperiments. Remarkably, even if the experts did not know which method produced which image, CyTran\ngathered more than 75% of the votes from each expert. The lowest percentage recorded by CyTran is for\nExpert #1, who rated our solution as being the most convenient in 150 of 200 (75%) cases. Moreover, CyTran\noutperforms U-GAT-IT, the second-best model according to Expert #1, by a considerable margin of 68%.\nExpert #3 was most favorable towards our model, voting for CyTran in 176 of 200 (88%) cases. In contrast,\nAttentionGAN, the second-best model in opinion of Expert #3, obtained only 13 of 200 (6 .5%) votes. Aside\nfrom counting the votes casted by each expert, we also analyze the inter-rater agreement via the Cohen’s κ\ncoeﬃcient. The average inter-rater agreement is κ= 0.81, with values ranging from 0 .76 to 0 .88. We consider\nthis level of agreement as substantial. In conclusion, all experts agree that CyTran is signiﬁcantly better than\nthe other state-of-the-art translation models.\nAfter casting the votes, we revealed the identity of the methods to the medical experts. Upon analyzing the\ngenerated outputs once again, all three experts observed that CyTran is the only method that does not produce\nvisual artifacts and does not lose information. When a vote went towards a diﬀerent method, it was almost\nalways decided based on the fact that the voted competing method produced an image closer to the target one.\n6.1.8. Qualitative analysis\nIn Figure 6, we present a randomly sampled case for each of the four contrast pairs. We observe that pix2pix\nsuﬀers from visual artifacts such as discontinuity in the scapula or erasure of the cortex of the rib, as seen in the\nnative→arterial translation. Moreover, soft tissues have an increase in noise and there is no additional useful\ninformation regarding the vessel contrast. When converting from arterial to native using pix2pix, the structure\nappears severely altered: the rib is completely dissociated, muscle margins are modiﬁed, vessel diﬀerentiation\nis diﬃcult. Similar to the pix2pix method, U-GAT-IT creates a false warping aura around the bones. The\nquality of the pulmonary parenchyma is not signiﬁcantly altered and the appearance of the vessels and soft\ntissues are harder to diﬀerentiate. CycleGAN increases noise especially around high contrast areas, as seen in\nthe native→arterial translation. The soft tissues adjacent to bony structure, such as the scapula or the ribs,\nhave a halo of structural deformation. This method also creates a grid-like texture on top of the CT image,\n14\nTable 4: Volumetric registration results for the venous →native and arterial →native contrast pairs. Our model is compared with\nseveral state-of-the-art methods and ablated versions of itself on the test set, in terms of MAE and SSIM. The symbol ↑ means\nhigher values are better, while ↓ means lower values are better. The best results are highlighted in bold.\nMethod venous→native arterial→native\nMAE ↓ SSIM ↑ MAE ↓ SSIM ↑\n1-cascade VTN [41] 0.0098 0.6883 0.0094 0.6894\n3-cascade VTN [41] 0.0083 0.7010 0.0081 0.7041\nViT-V-Net [37] 0.0077 0.7488 0.0071 0.7601\nViT-V-Net+pix2pix [10] 0.0093 0.7210 0.0090 0.7312\nViT-V-Net+U-GAT-IT [11] 0.0074 0.7501 0.0070 0.7602\nViT-V-Net+CycleGAN [4] 0.0075 0.7498 0.0069 0.7641\nViT-V-Net+CWT-GAN [12] 0.0073 0.7499 0.0067 0.7644\nViT-V-Net+AttentionGAN [13] 0.0073 0.7501 0.0066 0.7667\nViT-V-Net+CyTran (no MLCC) 0.0071 0.7521 0.0066 0.7703\nViT-V-Net+CyTran (ours) 0.0069 0.7545 0.0066 0.7709\n3-cascade ViT-V-Net (ours) 0.0064 0.7578 0.0053 0.7772\n3-cascade ViT-V-Net+CyTran (no MLCC) 0.0063 0.7640 0.0052 0.7910\n3-cascade ViT-V-Net+CyTran (ours) 0.0061 0.7697 0.0052 0.7933\nwhich distorts both the structural integrity and increases the number of visual artifacts. CWT-GAN usually\nrenders a very bright image of the CT scan while losing some essential information, as seen in the aortic arch\nof the native→arterial case or the mediastinum in the venous→native case, respectively. Overall, CWT-GAN is\nsimilar or slightly better than U-GAT-IT and inferior to both AttentionGAN or CyTran. When comparing our\ntechnique with AttentionGAN in the various cases exempliﬁed in Figure 6, we observe that there are a multitude\nof similarities: high resolution, enriched information, accurate diﬀerentiation between diﬀerent organs or tissues.\nDespite this fact, CyTran has slightly lower brightness in arterial or venous targets, but higher brightness and\ncontrast in native targets, thus oﬀering a clearer image. The big vessels of the heart can be more accurately\ndescribed, especially in the native output images. Furthermore, CyTran does not suﬀer from loss of information\nregarding the structures, tissue consistency and margins, as seen in the venous →native translation of the right\nbreast tumor. The true beneﬁt of CyTran lies in the conversion from native CT scans into generated arterial\nor venous CT scans, because of the fair amount of information acquired with respect to the native image, even\nif the vessels do not get as bright as in the reference (target) image. In a nutshell, CyTran oﬀers the best\nexperience in generating artiﬁcial images, being closest to the target among all methods, both in native and\ncontrast images.\n6.2. Volumetric Registration Experiments\n6.2.1. Baselines\nAs baselines, we consider three state-of-the-art unsupervised medical image registration methods: a 1-cascade\nVolume Tweening Network (VTN) [41], a 3-cascade VTN [41], and ViT-V-Net [37].\n6.2.2. Performance measures\nSince our data set does not contain any labeled segmentation maps, we consider performance measures\nsuitable for evaluating unsupervised registration methods, which quantify the ability to align moving structures,\nwithout damaging the integrity of CT scans. Therefore, we report the MAE and SSIM between the warped\nscan (the alignment result) and the reference scan.\n6.2.3. Data preprocessing\nWe apply the same data preprocessing steps as for the style transfer experiments.\n15\n1 2 3 4\n6\n6.5\n7\n7.5\n8 10-3 venous  native\nViT-V-Net\nViT-V-Net+CyTran\n1 2 3 4\n5\n5.5\n6\n6.5\n7\n7.5 10-3 arterial  native\nViT-V-Net\nViT-V-Net+CyTran\n1 2 3 4\n0.745\n0.75\n0.755\n0.76\n0.765\n0.77 venous  native\nViT-V-Net\nViT-V-Net+CyTran\n1 2 3 4\n0.75\n0.76\n0.77\n0.78\n0.79\narterial  native\nViT-V-Net\nViT-V-Net+CyTran\nFigure 7: Results for cascaded ViT-V-Net [37] and ViT-V-Net+CyTran models with various cascade steps. Models without recursive\ncascade are equivalent to models with one cascade step.\n6.2.4. Hyperparameter tuning\nWe train all networks from scratch with the hyperparameters indicated by the authors of cascaded VTNs\n[41] and ViT-V-Net [37], except for the mini-batch size, which we reduce to two data samples such that we can\ntrain each model on a single GPU.\nFor our approach, we introduce two hyperparameters: the augmentation rate and the number of cascade\nsteps. The augmentation rate represents the percentage of training data processed with a style transfer method\nand added to the training set. We consider augmentation rates ranging from 10% to 100%, at a step of 10%.\nFor the number of cascade steps, we consider all values between 1 and 4. We tune these hyperparameters on\nthe validation set, reporting the test results for the optimal conﬁguration found on the validation set.\n6.2.5. Quantitative results\nIn Table 4, we present the comparative results between our 3-cascade ViT-V-Net framework trained on\nimages translated by CyTran and the three state-of-the-art methods, two based on cascaded VTNs [41] and one\nbased on ViT-V-Net [37]. In addition, we show ablation results obtained by removing the cascade, CyTran, or the\nmulti-level cycle-consistency (MLCC) from our model. We also report results for the ViT-V-Net model trained\non images translated by our competitors: pix2pix, U-GAT-IT, CycleGAN, CWT-GAN and AttentionGAN.\nFirst, we observe that training ViT-V-Net on images translated by pix2pix [10] damages performance,\nleading to worse results for both venous →native and arterial →native pairs, in comparison with the vanilla\nViT-V-Net [37]. In contrast, U-GAT-IT [11], CycleGAN [4], CWT-GAN [12], AttentionGAN [13] and CyTran\nbring performance gains, indicating that our idea of transferring the style of source CT scans to target CT scans\nbefore registration is useful. Among these three models, CyTran gives the highest performance gains, once again\nshowing its superiority over pix2pix, U-GAT-IT, CycleGAN, CWT-GAN and AttentionGAN.\nInterestingly, our idea of introducing ViT-V-Net into a recursive cascade is also useful. This observation\nis conﬁrmed by the fact that the 3-cascade ViT-V-Net outperforms ViT-V-Net, as well as the fact that the 3-\ncascade ViT-V-Net+CyTran outperforms ViT-V-Net+CyTran. To further conﬁrm our observation, we present\nresults for ViT-V-Net and ViT-V-Net+CyTran with various numbers of cascade steps in Figure 7. We observe\nthat having more than one cascade brings considerable improvements over the baselines. For both approaches,\n16\nmoving (contrast)registered\nsample #1 sample #2 sample #3 sample #4 sample #5\nreference (native)\nsample #6\nflow field\nFigure 8: Examples of images registered by our best model, namely the 3-cascade ViT-V-Net+CyTran. The ﬁrst three examples\nillustrate the registration of arterial to native slices, while the last three examples show the registration of venous to native slices.\nFor each moving image (arterial or venous), we present the displacement map (or ﬂow ﬁeld) and the registered images (obtained\nby applying the ﬂow ﬁeld to the moving images). In all cases, the reference image is native (without contrast).\nthe highest gains are obtained with 3 cascades for the arterial→native pair, and 2 cascades for the venous→native\npair.\nIn the style transfer experiments, we did not observe major diﬀerences between the multi-level cycle-\nconsistency (MLCC) loss and the standard cycle-consistency loss for the native target domain. However, even\nminor diﬀerences in style transfer could lead to better alignment. To put this statement to the test, we included\nan ablated version of ViT-V-Net+CyTran, where the MLCC loss is replaced with the standard cycle-consistency\nloss. The reported results (shown in Table 4) indicate that the MLCC loss leads to superior alignment perfor-\nmance, conﬁrming that our MLCC loss is a useful contribution, even for the native target domain.\nIn summary, the empirical results show that our recursive cascaded ViT-V-Net based on style transfer with\nCyTran is the best approach for non-contrast to contrast CT scan registration, surpassing all baselines and\nablated models.\n6.2.6. Qualitative analysis\nIn addition to our quantitative results, we illustrate six randomly chosen registration results of our best\nregistration model in Figure 8. In the ﬁrst three columns, the phase of the moving image is arterial, while in\nthe last three columns, the phase is venous. First, we observe that the moving images (arterial or venous) are\nnot far from being well aligned to the reference images. However, the contrast substance introduces signiﬁcant\ndiﬀerences between pixel intensities (HU levels) of certain organs, e.g. the heart ventricles in the second sample\nor the pulmonary arterial tree vessels in the sixth sample. This observation indicates that a standard registration\nmodel (without a style transfer model to help reduce the HU level diﬀerences between contrast and non-contrast\nscans) needs to learn to match pixel intensity diﬀerences, while performing the alignment. Moreover, the pixel\nintensity diﬀerences also interfere with the loss function, creating large loss values even if the slices are well\n17\naligned. We alleviate these issues by introducing CyTran into the registration pipeline, signiﬁcantly easing the\ntask for the ViT-V-Net registration model. Indeed, the registered (output) slices depicted in Figure 8 conﬁrm\nthat the proposed registration pipeline works well.\n7. Conclusions\nIn this paper, we introduced cycle-consistent convolutional transformers in medical imaging. We employed\nour approach to transfer the style between contrast and non-contrast CT scans, showing that it outperforms\nstate-of-the-art methods such as pix2pix, U-GAT-IT, CycleGAN, CWT-GAN and AttentionGAN. Our qual-\nitative and subjective human evaluations revealed that CyTran is the only approach that does not introduce\nvisual artifacts during the translation process. We believe this is a key advantage in our application domain,\nwhere medical images need to precisely represent the scanned body parts. Moreover, we showed that CyTran\nbrings signiﬁcant improvements for a state-of-the-art medical image registration method. Another important\ncontribution of our work is Coltea-Lung-CT-100W, a new data set of triphasic CT scans comprising a total of\n37,290 images. In future work, we aim to apply our registration results to improve multi-image super-resolution\nand lesion segmentation.\nAcknowledgements\nThe research leading to these results has received funding from the NO Grants 2014-2021, under project\nELO-Hyp contract no. 24/2020. This article has also beneﬁted from the support of the Romanian Young\nAcademy, which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period\n2020-2022.\nReferences\n[1] S. Namasivayam, M. K. Kalra, W. E. Torres, W. C. Small, Adverse reactions to intravenous iodinated\ncontrast media: a primer for radiologists, Emergency Radiology 12 (5) (2006) 210–215.\n[2] Y. Yan, X. Sun, B. Shen, Contrast agents in dynamic contrast-enhanced magnetic resonance imaging,\nOncotarget 8 (26) (2017) 43491–43505.\n[3] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio,\nGenerative adversarial nets, in: Proceedings of NIPS, Vol. 27, 2014, pp. 2672–2680.\n[4] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation using cycle-consistent\nadversarial networks, in: Proceedings of ICCV, 2017, pp. 2223–2232.\n[5] K. Armanious, C. Jiang, S. Abdulatif, T. K¨ ustner, S. Gatidis, B. Yang, Unsupervised medical image\ntranslation using Cycle-MedGAN, in: Proceedings of EUSIPCO, 2019, pp. 1–5.\n[6] A. Chandrashekar, N. Shivakumar, P. Lapolla, A. Handa, V. Grau, R. Lee, A deep learning approach to\ngenerate contrast-enhanced computerised tomography angiograms without the use of intravenous contrast\nagents, European Heart Journal 41 (Supplement 2) (2020) ehaa946–0156.\n[7] V. Kearney, B. P. Ziemer, A. Perry, T. Wang, J. W. Chan, L. Ma, O. Morin, S. S. Yom, T. D. Solberg,\nAttention-aware discrimination for MR-to-CT image translation using cycle-consistent generative adver-\nsarial networks, Radiology: Artiﬁcial Intelligence 2 (2) (2020) e190027.\n[8] Q. Pengjiang, K. Xu, T. Wang, Z. Qiankun, H. Yang, B. Atallah, Z. Junqing, T. Bryan, M. R. F Jr,\nEstimating CT from MR abdominal images using novel generative adversarial networks, Journal of Grid\nComputing 18 (2) (2020) 211–226.\n[9] S. Sabour, N. Frosst, G. E. Hinton, Dynamic Routing between Capsules, in: Proceedings of NIPS, 2017,\npp. 3859—-3869.\n[10] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-image translation with conditional adversarial networks,\nin: Proceedings of CVPR, 2017, pp. 1125–1134.\n18\n[11] J. Kim, M. Kim, H. Kang, K. H. Lee, U-GAT-IT: Unsupervised Generative Attentional Networks with\nAdaptive Layer-Instance Normalization for Image-to-Image Translation, in: Proceedings of ICLR, 2019.\n[12] X. Lai, X. Bai, Y. Hao, Unsupervised Generative Adversarial Networks With Cross-Model Weight Transfer\nMechanism for Image-to-Image Translation, in: Proceedings of ICCV Workshops, 2021, pp. 1814–1822.\n[13] H. Tang, H. Liu, D. Xu, P. H. Torr, N. Sebe, AttentionGAN: Unpaired Image-to-Image Translation using\nAttention-Guided Generative Adversarial Networks, IEEE Transactions on Neural Networks and Learning\nSystems.\n[14] L. Chen, X. Yang, G. Jeon, M. Anisetti, K. Liu, A trusted medical image super-resolution method based\non feedback adaptive weighted dense network, Artiﬁcial Intelligence in Medicine 106 (2020) 101857.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser, I. Polosukhin, Attention\nis all you need, in: Proceedings of NIPS, 2017, pp. 5998–6008.\n[16] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al., Language models are few-shot learners, in: Proceedings of NeurIPS, 2020, pp. 1877–1901.\n[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at\nscale, in: Proceedings of ICLR, 2021.\n[18] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, M. Shah, Transformers in Vision: A Survey,\narXiv preprint arXiv:2101.01169.\n[19] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang, CvT: Introducing Convolutions to Vision\nTransformers, arXiv preprint arXiv:2103.15808.\n[20] G. Zhang, G. Kang, Y. Wei, Y. Yang, Few-shot segmentation via cycle-consistent transformer, arXiv\npreprint arXiv:2106.02320.\n[21] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, Y. Zhou, TransUNet: Transformers\nMake Strong Encoders for Medical Image Segmentation, arXiv preprint arXiv:2102.04306.\n[22] Y. Gao, M. Zhou, D. Metaxas, UTNet: A Hybrid Transformer Architecture for Medical Image Segmenta-\ntion, in: Proceedings of MICCAI, 2021.\n[23] A. Hatamizadeh, D. Yang, H. Roth, D. Xu, UNETR: Transformers for 3D Medical Image Segmentation,\narXiv preprint arXiv:2103.10504.\n[24] Y. Korkmaz, S. U. Dar, M. Yurt, M. ¨Ozbey, T. C ¸ ukur, Unsupervised MRI Reconstruction via Zero-Shot\nLearned Adversarial Transformers, arXiv preprint arXiv:2105.08059.\n[25] A. Luthra, H. Sulakhe, T. Mittal, A. Iyer, S. Yadav, Eformer: Edge Enhancement based Transformer for\nMedical Image Denoising, in: Proceedings of ICCV, 2021.\n[26] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, A. Courville, Improved training of wasserstein gans,\nin: Proceedings of NIPS, 2017, pp. 5769–5779.\n[27] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen, Improved techniques for training\nGANs, Proceedings of NIPS 29 (2016) 2234–2242.\n[28] P. Soviany, C. Ardei, R. T. Ionescu, M. Leordeanu, Image diﬃculty curriculum for generative adversarial\nnetworks (CuGAN), in: Proceedings of WACV, 2020, pp. 3463–3472.\n[29] H. Choi, D. S. Lee, Generation of structural MR images from amyloid PET: application to MR-less quan-\ntiﬁcation, Journal of Nuclear Medicine 59 (7) (2018) 1111–1117.\n[30] H. Emami, M. Dong, S. P. Nejad-Davarani, C. K. Glide-Hurst, Generating synthetic cts from magnetic\nresonance images using generative adversarial networks, Medical Physics 45 (8) (2018) 3627–3636.\n19\n[31] G. Modanwal, A. Vellal, M. Buda, M. A. Mazurowski, Mri image harmonization using cycle-consistent\ngenerative adversarial network, in: Proceedings of SPIE, Vol. 11314, 2020, p. 1131413.\n[32] M. Seo, D. Kim, K. Lee, S. Hong, J. S. Bae, J. H. Kim, S. Kwak, Neural Contrast Enhancement of CT\nImage, in: Proceedings of WCACV, 2021, pp. 3973–3982.\n[33] J. M. Wolterink, T. Leiner, M. A. Viergever, I. Iˇ sgum, Generative adversarial networks for noise reduction\nin low-dose CT, IEEE Transactions on Medical Imaging 36 (12) (2017) 2536–2545.\n[34] Y. Huo, Z. Xu, S. Bao, C. Bermudez, A. J. Plassard, J. Liu, Y. Yao, A. Assad, R. G. Abramson, B. A. Land-\nman, Splenomegaly segmentation using global convolutional kernels and conditional generative adversarial\nnetworks, in: Proceedings of SPIE, Vol. 10574, 2018, p. 1057409.\n[35] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, A. V. Dalca, An unsupervised learning model for\ndeformable medical image registration, in: Proceedings of CVPR, 2018, pp. 9252–9260.\n[36] M. Burduja, R. T. Ionescu, Unsupervised Medical Image Alignment with Curriculum Learning, in: Pro-\nceedings of ICIP, 2021, pp. 3787–3791.\n[37] J. Chen, Y. He, E. C. Frey, Y. Li, Y. Du, ViT-V-Net: Vision Transformer for Unsupervised Volumetric\nMedical Image Registration, arXiv preprint arXiv:2104.06468.\n[38] J. Krebs, T. Mansi, H. Delingette, L. Zhang, F. C. Ghesu, S. Miao, A. K. Maier, N. Ayache, R. Liao,\nA. Kamen, Robust non-rigid registration through agent-based action learning, in: Proceedings of MICCAI,\n2017, pp. 344–352.\n[39] M. M. Roh´ e, M. Datar, T. Heimann, M. Sermesant, X. Pennec, SVF-Net: Learning deformable image\nregistration using shape matching, in: Proceedings of MICCAI, 2017, pp. 266–274.\n[40] S. Zhao, T. Lau, J. Luo, E. I. Chang, Y. Xu, Unsupervised 3D end-to-end medical image registration with\nvolume tweening network, IEEE Journal of Biomedical and Health Informatics 24 (5) (2019) 1394–1404.\n[41] S. Zhao, Y. Dong, E. I. Chang, Y. Xu, et al., Recursive cascaded networks for unsupervised medical image\nregistration, in: Proceedings of ICCV, 2019, pp. 10600–10610.\n[42] T. Heimann, B. Van Ginneken, M. A. Styner, Y. Arzhaeva, V. Aurich, C. Bauer, A. Beck, C. Becker,\nR. Beichel, G. Bekes, et al., Comparison and evaluation of methods for liver segmentation from ct datasets,\nIEEE Transactions on Medical Imaging 28 (8) (2009) 1251–1265.\n[43] N. Kiryati, Y. Landau, Dataset growth in medical image analysis research, Journal of Imaging 7 (8) (2021)\n155.\n[44] I. B. Malone, D. Cash, G. R. Ridgway, D. G. MacManus, S. Ourselin, N. C. Fox, J. M. Schott,\nMIRIAD—Public release of a multiple time point Alzheimer’s MR imaging dataset, NeuroImage 70 (2013)\n33–36.\n[45] L. Oakden-Rayner, Exploring large-scale public medical image datasets, Academic Radiology 27 (1) (2020)\n106–112.\n[46] J. Sivaswamy, S. Krishnadas, G. D. Joshi, M. Jain, A. U. S. Tabish, Drishti-GS: Retinal image dataset for\noptic nerve head (ONH) segmentation, in: Proceedings of ISBI, 2014, pp. 53–56.\n[47] A. E. Kavur, N. S. Gezer, M. Barı¸ s, S. Aslan, P.-H. Conze, V. Groza, D. D. Pham, S. Chatterjee, P. Ernst,\nS. ¨Ozkan, et al., CHAOS challenge-combined (CT-MR) healthy abdominal organ segmentation, Medical\nImage Analysis 69 (2021) 101950.\n[48] T. R. Moen, B. Chen, D. R. Holmes III, X. Duan, Z. Yu, L. Yu, S. Leng, J. G. Fletcher, C. H. McCollough,\nLow-dose CT image and projection dataset, Medical Physics 48 (2) (2021) 902–911.\n[49] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-A. Heng, J. Hesser,\net al., The Liver Tumor Segmentation Benchmark (LiTS), arXiv preprint arXiv:1901.04056.\n20\n[50] S. Ioﬀe, C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate\nshift, in: Proceedings of ICML, PMLR, 2015, pp. 448–456.\n[51] V. Nair, G. E. Hinton, Rectiﬁed Linear Units Improve Restricted Boltzmann Machines, in: Proceedings of\nICML, 2010, pp. 807–814.\n[52] F. Chollet, Xception: Deep learning with depthwise separable convolutions, in: Proceedings of CVPR,\n2017, pp. 1251–1258.\n[53] D. Hendrycks, K. Gimpel, Gaussian Error Linear Units (GELUs), arXiv preprint arXiv:1606.08415.\n[54] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: Proceedings of ICLR, 2015.\n21"
}