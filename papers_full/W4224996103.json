{
  "title": "Adaptive Long-Short Pattern Transformer for Stock Investment Selection",
  "url": "https://openalex.org/W4224996103",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2581234759",
      "name": "Heyuan Wang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2144373700",
      "name": "Tengjiao Wang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2099223443",
      "name": "Shun Li",
      "affiliations": [
        "University of International Relations"
      ]
    },
    {
      "id": "https://openalex.org/A2297144205",
      "name": "Jiayi Zheng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2097121907",
      "name": "Shijie Guan",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1964725655",
      "name": "Wei Chen",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3190587204",
    "https://openalex.org/W3170261818",
    "https://openalex.org/W2997874851",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W3035414307",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4300511110",
    "https://openalex.org/W2952042565",
    "https://openalex.org/W2896309423",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W2969677753",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2744043447",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3175177406",
    "https://openalex.org/W2964413206",
    "https://openalex.org/W2171315676",
    "https://openalex.org/W3123329971",
    "https://openalex.org/W3126046721",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3047659929",
    "https://openalex.org/W2771333147",
    "https://openalex.org/W3034345631",
    "https://openalex.org/W3034478396",
    "https://openalex.org/W2998454530",
    "https://openalex.org/W2734777338",
    "https://openalex.org/W2008348094",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2025454036"
  ],
  "abstract": "Stock investment selection is a hard issue in the Fintech field due to non-stationary dynamics and complex market interdependencies. Existing studies are mostly based on RNNs, which struggle to capture interactive information among fine granular volatility patterns. Besides, they either treat stocks as isolated, or presuppose a fixed graph structure heavily relying on prior domain knowledge. In this paper, we propose a novel Adaptive Long-Short Pattern Transformer (ALSP-TF) for stock ranking in terms of expected returns. Specifically, we overcome the limitations of canonical self-attention including context and position agnostic, with two additional capacities: (i) fine-grained pattern distiller to contextualize queries and keys based on localized feature scales, and (ii) time-adaptive modulator to let the dependency modeling among pattern pairs sensitive to different time intervals. Attention heads in stacked layers gradually harvest short- and long-term transition traits, spontaneously boosting the diversity of representations. Moreover, we devise a graph self-supervised regularization, which helps automatically assimilate the collective synergy of stocks and improve the generalization ability of overall model. Experiments on three exchange market datasets show ALSP-TF‚Äôs superiority over state-of-the-art stock forecast methods.",
  "full_text": "Adaptive Long-Short Pattern Transformer for Stock Investment Selection\nHeyuan Wang1;3 , Tengjiao Wang1;3 , Shun Li2\u0003 , Jiayi Zheng1;3 , Shijie Guan1;3 and Wei Chen1;3\n1School of Computer Science, National Engineering Laboratory for Big Data Analysis and Applications,\nPeking University, China\n2University of International Relations\n3Institute of Computational Social Science, Peking University (Qingdao)\n{wangheyuan, tjwang, jiayizheng, guanshijie, pekingchenwei}@pku.edu.cn, lishunmail@foxmail.com\nAbstract\nStock investment selection is a hard issue in the\nFintech Ô¨Åeld due to non-stationary dynamics and\ncomplex market interdependencies. Existing stud-\nies are mostly based on RNNs, which struggle to\ncapture interactive information among Ô¨Åne granular\nvolatility patterns. Besides, they either treat stocks\nas isolated, or presuppose a Ô¨Åxed graph structure\nheavily relying on prior domain knowledge. In this\npaper, we propose a novel Adaptive Long-Short\nPattern Transformer (ALSP-TF) for stock ranking\nin terms of expected returns. SpeciÔ¨Åcally, we over-\ncome the limitations of canonical self-attention in-\ncluding context and position agnostic, with two ad-\nditional capacities: (i) Ô¨Åne-grained pattern distiller\nto contextualize queries and keys based on local-\nized feature scales, and (ii) time-adaptive modula-\ntor to let the dependency modeling among pattern\npairs sensitive to different time intervals. Attention\nheads in stacked layers gradually harvest short- and\nlong-term transition traits, spontaneously boosting\nthe diversity of stock representations. Moreover,\nwe devise a graph self-supervised regularization,\nwhich helps automatically assimilate the collective\nsynergy of stocks and improve the generalization\nability of overall model. Experiments on three ex-\nchange market datasets show ALSP-TF‚Äôs superior-\nity over state-of-the-art stock forecast methods.\n1 Introduction\nAs an essential ingredient of modern Ô¨Ånancial ecosystem, the\nforecast of stock market has aroused extensive research in-\nterest due to scientiÔ¨Åc and investment merits [Feng et al.,\n2019b]. Different from the general time series modeling, it\nis inherently difÔ¨Åcult to assess stock‚Äôs evolving trend con-\nfronting with highly volatile and interrelated natures of the\nmarket. Traditional literatures leverage machine learning al-\ngorithms based on manual indicators [Nayak et al., 2015;\nKhaidem et al., 2016], where the hypothetical stochastic pro-\ncess may become stranded in catching non-stationary os-\ncillations. In recent studies, deep neural networks have\n\u0003Corresponding Author.\n*context=4, *interval=7\n*context=8, *interval=25\npattern A\npattern C\npattern D\npattern B\n‚ãØ\n4.9\n‚ãØ\n6.115.1\nTimeLine\nPrice\n *context=4, *horizon=7\npattern A\npattern D\npattern E\npattern C\nTimeLine\nPrice\npattern B\n*context=8, *horizon=24\n*context=4, *horizon=7\npattern A\npattern D pattern E\npattern C\nTimeLine\nPrice\npattern B\n*context=8, *horizon=24\n*context=4, *horizon=7\npattern A\npattern D pattern E\npattern C\nTimeLine\nPrice\npattern B\n*context=8, *horizon=24\nFigure 1: Interaction of patterns along stock (SMIC) dynamic series.\nfA;Bg and fC;Dg have varied context spans and time intervals.\nshown encouraging prospects in characterizing stock dynam-\nics, especially the RNNs are the dominant choice. For in-\nstance, Zhang et al. [2017] used state frequency memory in\nthe LSTM network to decompose stock transaction patterns.\nSawhney et al. [2021] equipped LSTM with temporal atten-\ntion to reward driving hidden signals. In view of the short of\nRNNs in capturing granular feature units, Wang et al. [2021]\nproposed to apply gated causal CNNs to convolve price sub-\nsequences. However, both of them are ineffective to learn the\ndependencies of long-range discontinuous temporal states.\nAs an alternative, the well-known Transformer[Vaswaniet\nal., 2017] preliminarily designed in the NLP Ô¨Åeld has gained\nhuge success in learning sequential data [Ding et al., 2020;\nZhou et al., 2021 ]. The core of Transformer is multi-\nhead self-attention, which explicitly performs information\nexchange between input tokens to Ô¨Åx the deÔ¨Åciencies of RNN\nand CNN structures. However, directly applying the canon-\nical encoder to modeling stock movements is problematic in\ntwo non-neglectable aspects: (1) The global self-attention fo-\ncuses on point-wise token similarities without contextual in-\nsights [Xu et al., 2020 ]. Since stock Ô¨Çuctuations are con-\nditioned on composite signals over manifold time spans, the\nlack of pattern-wise interaction hinders the adequate discrim-\nination of stock tendency and is susceptible to noise points.\n(2) The basic query-key matching paradigm is position ag-\nnostic. Though sinusoidal position embedding is inserted to\nthe sequential input, it may not be optimal due to the inability\nto reveal precise distances [Wu et al., 2021 ]. As an empiri-\ncal example, in Fig. 1, the subsequences {A;B}and {C;D}\nreÔ¨Çect stock wave patterns in the context of different long-\nand short-term spans; Intuitively, conducting multi-granular\nmatching between them instead of simple dot-point projec-\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3970\ntions is more conducive to characterizing the evolution status.\nAdditionally, the patterns C and Dare closer than Aand B,\nwhich means that their interaction is more responsible for ob-\nserving high-frequency regularities. This inspires us to factor\nin the elapsed time between stock change patterns so as to\nmodulate the self-attention on stock trading series.\nThe synergy effect is another prominent trait of stock mar-\nket, i.e, related stocks are apt to exhibit synchronous changes,\noffering a desiderative pointcut for trend predictions. Never-\ntheless, it is non-trivial to anchor full-scale stock interdepen-\ndencies given that relationship sources may originate from\nvarious aspects (industry rotation, common shareholder, sup-\nply chain, etc). Previous works mostly treat individual stocks\nas isolated [Wang et al., 2020; Ding et al., 2020 ]. Some\nstudies presuppose a graph structure by resorting to limited\ndomain knowledge [Chen et al., 2018; Feng et al., 2019b;\nSawhney et al., 2021 ], which may lead to information bias\nin revealing intricate market factors. More particularly, task-\nspeciÔ¨Åc graph building requires massive expertise, which pre-\nvents the model from being applied to extended scenarios.\nAlong these lines, in this paper, we presentAdaptive Long-\nShort Pattern Transformer (ALSP-TF) for stock investment\nselection. The model is structurally innovated for hierarchical\nrepresentation and interaction of stock price series at differ-\nent context scales. In addition, with the help of a learnable\nsigmoid function, we make the self-attention aware of the\nweighted time intervals between patterns, in order to adap-\ntively adjust their dependencies beyond similarity matching.\nFurther, to get rid of reliance on any prior knowledge, we\ndevise a graph self-supervised regularizationwhich automat-\nically learns stock topology through dynamic path alignment,\nand thereby boosting the generalization capacity of the over-\nall model. Our major contributions are as follows:\n‚Ä¢ We delve into the issues of basic Transformer in model-\ning stock price series, and present a reformed self-attention\nencoder to exploit adaptive pattern-wise interactions sup-\nported by temporal representations at different grain levels.\n‚Ä¢ We construct a data-driven adjacency graph to uncover the\nimplicit similarities in volatility across different stocks. It\nhelps reduce the stochasticity of stock input sequences and\nserves as a self-supervision signal to guide the model‚Äôs rep-\nresentation learning.\n‚Ä¢ All the components are seamlessly integrated and jointly\ntrained to predict stock expected returns. Experiments on\ndatasets from NYSE, NASDAQ and TSE markets show the\neffectiveness of proposed model for investment selection.\n2 Related Work\nTechnical Analysis. TA is at the heart of stock trend pre-\ndiction, which is developed on top of price-volume indica-\ntors from historical quote data. Traditional mathematical al-\ngorithms such as HMM, SVM [Kavitha et al., 2013; Nayak\net al., 2015; Khaidem et al., 2016 ] leverage manual fea-\nture engineering and hypothetical stochastic processes. Later\nstudies move to exploiting deep neural networks to model\nthe hidden dependency of stock dynamics, where RNNs are\ncommonly utilized [Qin et al., 2017; Zhang et al., 2017;\nWang et al., 2020 ]. To enhance the capacity of handling\nÔ¨Åne-granular transition signals, some efforts have explored\nother architectures such as hybrid SAE-LSTM units [Bao et\nal., 2017], adversarial training [Feng et al., 2019a], and gated\ncausal convolutions [Wang et al., 2021 ]. These approaches\nhave made progress in several stock prediction tasks, whereas\nthey are commonly framed for the regression of prices or\nclassiÔ¨Åcation of bucketing stock movements. The latest\nwork [Sawhney et al., 2021] claimed that the absence of opti-\nmization toward expected returns will harm practical invest-\nment choices. They reformulated stock forecasts as a learning\nto rank task and realized state-of-the-art proÔ¨Åtability.\nMarket Relation Modeling. A new line of studies revolve\naround employing the collective synergy among stocks based\non their metadata relevance. For instance, Lai et al. [2017]\nacquired stock relatedness by querying company collabora-\ntion and competition from the search engine, then made infer-\nences based on unary and binary potentials in Markov random\nÔ¨Åelds. Chen et al. [2018] built a graph of corporations based\non their shareholding properties, and transferred stock predic-\ntion to node classiÔ¨Åcation using graph convolutional network\n(GCN) [Kipf and Welling, 2017 ]. Feng et al. [2019b] clus-\ntered stocks from the same industries and supply chains to\nmake the temporal price encoder aware of inter-stock rela-\ntions. Sawhney et al. [2021] augmented the corporate rele-\nvance based on Wikidata and used hypergraph convolution to\npropagate higher-order neighbor‚Äôs information. Despite ad-\nvances in graph-based stock forecasts, the preset knowledge-\nbased stationary graph structure requires extensive domain\nexpertise and may strain model‚Äôs extensibility.\nTransformer. A powerful attention neural model that is\npreliminarily designed for machine translation [Vaswani et\nal., 2017] and now has attained huge success in various Ô¨Åelds\nsuch as computer vision, multimodal reasoning and video\nclassiÔ¨Åcation [Gao et al., 2019; Dosovitskiy et al., 2021 ].\nThere are several recent studies in applying Transformer to\ntime series modeling. Zhou et al. [2021] devoted to optimiz-\ning the efÔ¨Åciency of time complexity and memory usage of\nTransformer on extremely long time series. Dinget al. [2020]\nÔ¨Årst tried to exploit Transformer on trading sequences to clas-\nsify stock price movements. Differently, we delve and revamp\nthe deÔ¨Åciencies of basic Transformer in grasping important\ncontext and temporal information of stock volatility patterns.\n3 Methodology\n3.1 Problem Formulation\nTo avoid the gap between stock movement prediction and in-\nvestment proÔ¨Åt, we follow the setup of[Sawhney et al., 2021]\nand adopt a learning to rank formulation for stock selection.\nGiven the candidate set S= {s1;:::;s N}of N stocks, on\nany trading day t, each stock si entails a feature sequence\nXi = [xt‚àí\u0001T;:::; xt‚àí1] ‚ààR\u0001T√óF of historical \u0001T time\nsteps (where F is the feature dimension), an associated clos-\ning price pt\ni and a 1-day return ratio rt\ni = pt\ni‚àípt‚àí1\ni\npt‚àí1\ni\n. The\nmodel F\u0012(X[1:N]) aims to output an ordering of all stocks\nYt = {yt\n1 > yt\n2 ::: > yt\nN}, where the top-ranked ones are\nexpected to gain more investment revenues on day t.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3971\n‚ãØ!\" ‚ãØ!# ‚ãØ!$ ‚ãØ!! TimeLine\nStock PairsDay\nDayDay‚ãØ ‚ãØ\nDTW Path (,,-)\n)!\n)\"\nDynamic Time Warping (multiscale)Filtering cost (sparsity O)\nStock Graph P\n(a) Time Series Initialization\n*69:5+:5;8‚Ä¶\nFeature MatchingRescale Coefficient√óQ‚Ñé567\n‚Ä¶ ‚Ä¶\n)58*69:5+:5;8\nCompositive PatternContext Attn)58*69:5+:5;8)58*69:5+:5;8\nCompositive PatternContext AttnCompositive PatternContext Attn\nR(S;6%,'%) Graph-based Smoothing\n‚ë†‚ë°\n‚ë¢\n+\n + Multi-head Local-interactionMulti-head Global-interaction\nSum & Norm768==‚àí‚àÜ@\n768==‚àí1Residual ConnectionFeed Forward\nTemporal Aggregation\n(b) Hierarchical Temporal Representation\nPredicted\nGPRankLoss\n,DE:=U!,/#=15MJ577,DNG!#,/$=25MJ577,DNG!$,/&=3\n()*()*()*\nTime IntervalEmbedding)58\nx Weighted Sum\n√óQ‚Ñé567\n+,-./,01*23/4*)-1‚ë£Adaptive Long-Short Pattern Transformer\n√óVWXYZ[\\\n%52]\n@,M5‚àí676E=,'5MI7:96=I;\n‚Ä¶ ‚Ä¶\nFigure 2: Overview of the proposed ALSP-TF.\n3.2 Framework Overview\nFig. 2 illustrates the overview of ALSP-TF. It consists of\nthree major parts: (i) Graph-based Initialization to shift the\ndynamic path alignment of stock pairs into a topology struc-\nture, and apply a gated graph smoothing operation to enrich\nthe initial embeddings of stock time series; (ii) Reformed\nTransformer Encoder with hierarchical blocks of multi-head\nself-attention to exploit interdependencies between different\nstock volatility patterns. Each block jointly leverages Ô¨Åne-\ngrained pattern distiller and time-adaptive modulator to cal-\nculate the attention scores on the basis of contextualized fea-\nture units; (iii) At last, regularized graph self-supervision sig-\nnals are added to tune the entire stock ranking framework.\n3.3 Time Series Embedding Initialization\nStock Graph Construction\nThe future movement of a stock is conditioned on its histor-\nical dynamic patterns as well as its neighbors‚Äô synchronous\ninformation. To incorporate inter-stock dependencies, recent\nresearch relies to a large extent on prior domain knowledge,\nwhich can be labor-intensive, difÔ¨Åcult to adequately reveal\nintricate market factors, and may narrow the applicability to\nnew prediction tasks. In this regard, we propose to learn the\nimplicit collective synergy among all candidate stocks in a\ndata-driven manner. SpeciÔ¨Åcally, we utilize a proximity func-\ntion by applying multi-dimensional Dynamic Time Warping\n(DTW) [Jeong et al., 2011 ] on the input signals X[1:N]. It\nÔ¨Ånds the minimum alignment cost of each pair of stock se-\nquences through dynamic programming on a path matrix D:\ncost(si;sj) ‚áê DTW({Dpq}\u0001T√ó\u0001T) : (1)\nHere we set each pixel Dpq = ‚àëF\nf=1(Xp;f\ni ‚àíXq;f\nj )2. Then,\nwe attach an edge between the stock pairs whose cost value\nis less than a limit threshold (by sorting alignment costs and\ncontrolling the graph sparsity \u001a) to build the stock graph G.\nGraph-based Sequence Smoothing\nTo further reduce the stochasticity of stock sequences, we\ndevelop a graph-based smoothing operation which substanti-\nates stock proximity correlations using neighboring features.\nLet ÀÜA ‚ààRN√óN denote G‚Äôs normalized adjacency matrix\nwith self-loops and e0\n[1:N] ‚àà R(N√ó\u0001T)F the input signals\nof X[1:N], we utilize the aggregation layer of graph attention\n(GAT) network [Velickovic et al., 2017] to convolve the k-\norder neighbor messages of stock si into embedding of ak\ni.\nThen we engage a GRU-like gate manner to rectify the fusing\nproportion in node updating process (ek\ni = Gate(ek‚àí1\ni ;ak\ni)):\nÔ£±\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£¥Ô£≥\nzk\ni = \u001b(Wzak\ni + Uzek‚àí1\ni + bz) ;\nrk\ni = \u001b(Wrak\ni + Urek‚àí1\ni + br) ;\n~ek\ni = tanh(Whak\ni + Uh(rk\ni ‚äôek‚àí1\ni ) + bh) ;\nek\ni = ~ek\ni ‚äôzk\ni + ek‚àí1\ni ‚äô(1 ‚àízk\ni) ;\n(2)\nwhere ‚äôis Hadamard product, W‚àó, U‚àó, b‚àóare trainable\nweights and biases. rk\ni and zk\ni are reset and update gates,\nwhich are responsible for catching irrelevant information to\nforget and the part of past state to move forward, respectively.\n3.4 Transformer Encoder\nNext, we describe our reformed Transformer encoder to adap-\ntively capture the interactive information between short- and\nlong-term volatility patterns with different time intervals. The\nrepresentation hierarchy consists of L blocks of multi-head\nself-attention and feed-forward layers. Taken initialized stock\nembedding sequences E ‚ààRN√ó\u0001T√ó\u0016das input, the canonical\nself-attention in [Vaswani et al., 2017] performs information\nexchange between every pair of time points for each stock\nsi. SpeciÔ¨Åcally, it transforms Ei ‚ààR\u0001T√ó\u0016d (the series of\nsi) into query Qi;h = EiWQ\nh, key Ki;h = EiWK\nh and\nvalue matrices V i;h = EiWV\nh with distinct linear projec-\ntion parameters, where h = 1;:::;H is the head index, and\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3972\nWQ\nh;WK\nh ;WV\nh ‚ààR\u0016d√ódf . Then scaled dot-product atten-\ntion scores are computed to acquire a weighted sum of the\nsequential values. Afterwards, the Ô¨Ånal layer output is repre-\nsented by the concatenation of all attention heads:\nFi = Multihead(Qi;h;Ki;h;V i;h)\n= ||h=H\nh=1 Softmax(Qi;hK‚ä§\ni;h=\n‚àö\ndf)V i;h;\n(3)\nwhere ||is the concatenation operator anddf is the dimension\nof projected feature space. Thereby all stocks‚Äô representa-\ntions are formed as F = [F 1; F2; ::: ; FN] ‚ààRN√ó\u0001T√óHdf\nfollowed by feed-forward layers.\nEnhancing Locality with Fine-grained Pattern Distiller\nThe dot-product attention calculated on top of point-wise to-\nkens exhibits a powerful ability in extracting global depen-\ndencies for words in NLP and regions in CV . Whereas, the\ncompound patterns implied in different short- and long-term\nlocal time spans are both important to shed light on the intri-\ncate stock market dynamics, while cannot be well exploited\nin such scheme. To solve it, in each block, we inject a local\ninteraction (Lit) layer before global matching such that com-\npositive signals are shifted into new contextualized query-key\ntuples. To save computational consumptions, we borrow the\nconcept of dilated causal convolutions in WaveNet [Oord et\nal., 2016], and skip interval ‚Äúholes‚Äù on cascading Lit layers\ninstead of making projection over contiguous subsequences\nin Ei to obtain wider receptive Ô¨Åelds hierarchically.\nRegarding Lit at the 1st block, at any time-step \u001c we apply\nself-attention to process its surrounding w-length context, i.e,\n\u0016E\n1‚Üí\u001c\ni = [E\u001c‚àíw+1\ni ;:::; E\u001c\ni] ‚ààRw√ó\u0016d (we use padding when\n\u001c ‚â§w). Note that the context is a predecessor time period\nfrom \u001c, which is like a position mask to Ô¨Ålter out temporal at-\ntention to future information. Therefrom we represent trans-\nformed key-value tuples as \u0016K\n\u001c\ni;h = \u0016V\n\u001c\ni;h = \u0016E\n1‚Üí\u001c\ni \u0016W\nQ\nh ‚àà\nRw√ódf , and form the state of current time-step as a query\nmatrix \u0016Q\n\u001c\ni;h = E\u001c\ni \u0016W\nQ\nh ‚ààR1√ódf . After that, we exploit\nthe context dependency and attentively sum up localized ele-\nments as a speciÔ¨Åc compound pattern for the moment \u001c:\nP1‚Üí\u001c\ni = Multihead( \u0016Q\n\u001c\ni;h; \u0016K\n\u001c\ni;h; \u0016V\n\u001c\ni;h) : (4)\nBy concatenating all time-steps, the embedding output of\nthis layer is denoted as P1\ni ‚àà R\u0001T√óHdf . Further, in the\nhigher (l+ 1)th block, we pass Pl\ni as input to Lit and \u000el+1 a\nwider skipping distance to handle longer-term local contexts\n\u0016E\nl+1‚Üí\u001c\ni = ||\u0016=w‚àí1\n\u0016=0 Pl‚Üí(\u001c ‚àí\u0016\u000el+1)\ni ‚ààRw√óHdf along time-\nsteps. In this way, the local receptive Ô¨Åelds in the stacking\nLits are exponentially expanded, supported by linearly grow-\ning parameters (e.g, we can obtain hierarchical patterns over\n3‚Üí7‚Üí15 days by setting the draw-out size as w = 3 and\nskipping distances as 1‚Üí2‚Üí4). The (l+ 1)-level compound\npatterns Pl+1\ni are derived in the same manner of Eq. 4 with\ndifferent parameters \u0016W\nQ\nh. Starting with \u000e1 = 1 to ensure no\nloss of coverage on the sequence, we devise \u000e[2:L] according\nto the performance in validation to gradually distill Ldiffer-\nent granularities of transition patterns for all stock sequences.\nPattern Interaction with Time-adaptive Modulator\nBuilding on the Ô¨Åne-grained patterns distilled fromLit at each\nblock, we turn to capturing global intra dependencies across\nthe entire sequence. To this end, the position information of\ntime series plays a critical role in measuring the variant of\nelapsed time between patterns. The canonical Transformer\nadds sinusoidal relative position embeddings to the input,\nwhich is proved weak due to loss of precise distance infor-\nmation. Let Z\u001c;\u0016 = |\u001c‚àí\u0016|denote the distance (i.e, temporal\nintervals) between the \u001cth and \u0016th moments, we construct a\nmatrix Z ‚ààR\u0001T√ó\u0001T to indicate temporal distance signals\nbetween every pair of patterns in the embedding sequence\nPl\ni (i ‚àà{1;::;N };l ‚àà{1;::;L}). Motivated by [Wu et al.,\n2021], we use a learnable sigmoid functionf(¬∑)to rescale raw\ntemporal values into an appropriate range, which is bounded,\ntunable and monotone to guarantee the model training:\n^Zh = f(Z; ah;vh) = 1 + exp(vh)\n1 + exp(vh ‚àíahZ) ; (5)\nwhere ah is a weight parameter that determines whether pre-\nferring to capture long-distance interactions (with a positive\nvalue) or focusing on short-distance dependencies (with a\nnegative value). vh controls the curve‚Äôs upperbound and as-\ncending steepness, whose larger value means intenser effect\nof the time distance between patterns. Both ah and vh are\ntailored for the hth global attention head in a learnable way.\nAfterwards, we use the time-adaptive coefÔ¨Åcients to adjust\nthe attention operation inside Eq. 3. At each level of granular-\nity l, we linearly transform the local pattern sequence Pl\ni and\ncompute Multihead(Ql;i;h\ncross;Kl;i;h\ncross;V l;i;h\ncross) as follows:\nFl;i=||h=H\nh\n=1 softmax(ReLU(Ql;i;h\ncrossKl;i;h‚ä§\ncross )‚àó^Zh\n‚àö\ndf\n)V l;i;h\ncross; (6)\nwhere Ql;i;h\ncross = Pl\ni ~W\nQ\nh, Kl;i;h\ncr\noss = Pl\ni ~W\nK\nh , V l;i;h\ncross =\nPl\ni ~W\nV\nh, ‚àómeans element-wise product, ReLU activation is\napplied to keep non-negativity and sharpen the original atten-\ntion weights. Hence, the feature similarity and time distance\nare jointly measured to make the volatility patterns in the lth\nblock crossly attend to each other. We keep one feed-forward\nnetwork and residual connection of canonical Transformer to\nfurther process Fl;i. The transformation matrices are shared\nfor all stocks. Finally, by averaging all steps of each layer and\nthen concatenating multi-granular embeddings, we represent\nthe stock set Sas a compact tensor O ‚ààRN√ó(L√óHdf ).\n3.5 Prediction and Network Optimization\nRank Loss. For ranking optimization, we acquire stock\npredicted return ratios on day tby feeding stock representa-\ntions O to a dense layer with the activation of Leaky-ReLU.\nThen we jointly compute the point-wise regression and pair-\nwise ranking loss with a weighting coefÔ¨Åcient \u000b, to minimize\nthe discrepancy between predicted ^rt\n[1:N] and ground-truth\nrt\n[1:N] meanwhile maintaining the relative order of stocks:\nLR=\nN‚àë\ni=1\nÓµπÓµπ^rt\ni ‚àírt\ni\nÓµπ\nÓµπ2\n+ \u000b\nN‚àë\ni=1\nN‚àë\nj=1\nmax\n(\n0;‚àí\n(\n^rt\ni ‚àí^rt\nj\n)(\nrt\ni ‚àírt\nj\n))\n:\n(7)\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3973\nMethods NASDAQ\nNYSE TSE\nSR IRR SR IRR\nSR IRR\nCLF\nARIMA [Wang and Leu,\n1996] Recurrent neural network using features extracted from ARIMA analyses 0.55 0.10 0.33 0.10 0.47 0.13\nAdv-ALSTM [Feng et al., 2019a] Simulate the stochasticity of stock dynamics with adversarial training 0.97 0.23 0.81 0.14 1.10 0.43\nHGCluster [Luo et al., 2014] Translate stock trend prediction into clustering of the hypergraph 0.06 0.10 0.10 0.11 0.20 0.10\nHATS [Kim et al., 2019] Use hierarchical attention network to model stock multigraphs 0.80 0.15 0.73 0.12 0.96 0.31\nHMG-TF [Ding et al., 2020] Apply Gaussian Transformer on daily and weekly trading series 0.83 0.19 0.75 0.13 1.05 0.33\nLSTM-RGCN [Li et al., 2021] LSTM + GCN for modeling stock relational graph 0.75 0.13 0.70 0.10 0.90 0.28\nHATR [Wang et al., 2021] Temporal-relational modeling with gated convolution + diffusion GCN 0.92 0.31 0.76 0.14 0.98 0.36\nREG\nSFM [Zhang et al., 2017] RNNs\n+ DFT-based state frequency memory to extract volatility patterns 0.16 0.09 0.19 0.11 0.08 0.07\nDA-RNN [Qin et al., 2017] RNNs + Dual-stage attentions to reward driving input and hidden states 0.71 0.14 0.66 0.13 0.86 0.25\nRL\nDQN [Carta et al., 2021] Ensemble\nof deep Q-learning agents to maximize a return function 0.93 0.20 0.72 0.12 1.08 0.31\niRDPG [Liu et al., 2020] Imitate RDPG model to learn trading policies for reward of Sharpe Ratio 1.32 0.28 0.85 0.18 1.10 0.55\nRAT [Xu et al., 2020] Relation-aware Transformer under RL framework for portfolio selection 1.37 0.40 1.03 0.22 1.20 0.64\nRAN\nSAE-LSTM [Bao et al., 2017] Stack\ned autoencoders + LSTM to forecast stock price for ranking 0.95 0.22 0.79 0.12 0.73 0.21\nRSR-E [Feng et al., 2019b] Temporal GCN using similarity of feature vectors as relation strength 1.12 0.26 0.88 0.20 1.07 0.50\nRSR-I [Feng et al., 2019b] Temporal GCN using neural net to compute relation strength 1.34 0.39 0.95 0.21 1.08 0.53\nSTHAN-SR [Sawhney et al., 2021] Attentive LSTM + hypergraph attention on multiple relationships 1.42 0.44 1.12 0.33 1.19 0.62\nALSP-TF (Ours) Adaptive Long-Short\nPattern Transformer with self-supervised regularization 1.55? 0.53? 1.24? 0.41? 1.27? 0.71?\nTable 1: ProÔ¨Åtability comparison with ClassiÔ¨Åcation (CLF), Regression (REG), Reinforcement Learning (RL), andRanking (RAN) baselines.\nRelation-exploited studies leverage the knowledge of industry, Ô¨Årst- and second-order Wikidata corporate relationships deÔ¨Åned in STHAN-\nSR. Bold & underlines depict the best & second-best results. ?means the improvement over SOTA is statistically signiÔ¨Åcant (p< 0:01).\nGraph Proximity Loss. To ensure that the learned stock\nembeddings can effectively capture the correlation informa-\ntion stored in the alignment graph structure G, we further in-\ntroduce a graph reconstruction strategy, which regulates stock\nrepresentations by explicitly drawing closer the node neigh-\nbors (we denote Ni as the 1-hop neighbors of si including\nitself) and pushing farther the negative ones in feature space:\nLGP(i) = ‚àí ‚àë\nj‚ààN(i\n)\nlog(\u001b(oio‚ä§\nj )) ‚àí ‚àë\np‚ààS‚àíNi\nlog(\u001b(‚àíoio‚ä§\np)) ; (8)\nwhere oi, oj, op denote the embeddings of\nstock node iand\nits neighbor jin a pair, as well as one sampled negative node.\nCombining the supervisory ranking signals and the self-\nsupervised graph proximity loss, we reach the complete end-\nto-end loss function with a weighting coefÔ¨Åcient \u0011:\nL= LR+ \u0011LGP: (9)\n4 Experiments\n4.1 Experimental Setup\nDatasets. We examine ALSP-TF on three real-world\ndatasets from US and Japanese Exchange markets. Ta-\nble 2 shows the detailed statistics. The Ô¨Årst dataset [Feng\net al., 2019b ] comprises 1,026 shares from fairly volatile\nUS S&P 500 and NASDAQ Composite Indexes; The sec-\nond dataset [Feng et al., 2019b] targets at 1,737 stocks from\nNYSE, which is by far the world‚Äôs largest stock exchange\nw.r.t. market capitalization of listed companies and is rel-\natively stable compared to NASDAQ; The third dataset [Li\net al., 2021 ] corresponds to the popular TOPIX-100 Index,\nwhich includes 95 stocks with the largest market capitaliza-\ntion in Tokyo stock exchange.\nImplementation Details. Our model is implemented with\nPyTorch. We collect daily quote data of all stocks includ-\ning normalized opening-high-low-closing prices (OHLC) and\nNASDAQ NYSE TSE\n# Stocks 1,026 1,737 95\nTrain Period\n(Days) 01/13-12/15 (756) 01/13-12/15 (756) 11/15-08/18 (693)\nVal Period (Days) 01/16-12/16 (252) 01/16-12/16 (252) 08/18-07/19 (231)\nTest Period (Days) 01/17-12/17 (237) 01/17-12/17 (237) 07/19-08/20 (235)\nTable 2: Statistics of datasets.\ntrading volumes from professional Wind-Financial Termi-\nnal1. For fair comparison, we follow [Sawhney et al., 2021]\nand generate samples by moving a 16-day lookback window\nalong trading days. We keep \u001a = 0:85;0:85;0:90 for NAS-\nDAQ, NYSE and TSE respectively, and set the hop of graph\nconvolutional operation to 2. For temporal modeling, we test\nstacking 1-5 Lit layers with varied skipping rates. The re-\nported results utilize a 3-layers‚Äô hierarchy assigning \u000e[1:3] to\n1‚Üí2‚Üí3 and the number of attention heads Hto 6 according\nto scores on validation set. The dimension of hidden feature\nspace df is 16. The loss factors are set to \u000b= 4 and \u0011= 0:5.\nWe tune the model and ablation variants on a GeForce RTX\n3090 GPU by Adam optimizer [Kingma and Ba, 2015 ] for\n100 epochs, the learning rate is 1e-3 and batch size is 16.\nMetrics. Following previous studies [Feng et al., 2019b;\nSawhney et al., 2021 ], we adopt a daily buy-hold-sell trad-\ning strategy to assess the proÔ¨Åtability of ALSP-TF in terms\nof Sharpe ratio (SR) and cumulative investment return ra-\ntio (IRR). That is, the trader buys \u0014 stocks with the highest\nexpected revenues once the market is closed on day t, then\nsells off these shares on next day‚Äôs close market. Formally,\nIRRt = ‚àë\ni‚àà^St\npt+1\ni ‚àípt\ni\npt\ni\n, where ^St denotes the stocks in port-\nfolio on day t. SR= E[Rp]‚àíRf\nstd[Rp] is a measure of risk-adjusted\nreturn describing the additional earnings an investor receives\nfor per unit of increase in risk. We also compare the model‚Äôs\nranking ability adopting the widely used metric nDCG@\u0014.\nWe report the mean results of Ô¨Åve individual runs for \u0014= 5.\n1https://www.wind.com.cn/en/wft.html\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3974\nIRR0.3\n0.4\n0.5\n0.6\n0.502\n0.426\n0.459\n0.5130.529\n-Smooth -Lit -Time Adaptive -GPloss ALSP-TF\n1.3\n1.4\n1.5\n1.6\n1.538\n1.420\n1.463\n1.527\n1.552\nSR1.3\n1.4\n1.5\n1.6\n1.538\n1.420\n1.463\n1.527\n1.552\n0.3\n0.4\n0.5\n0.6\n0.502\n0.426\n0.459\n0.513 0.529\nIRR0.3\n0.4\n0.5\n0.6\n0.502\n0.426\n0.459\n0.513 0.529\n0.6\n0.7\n0.8\n0.9\n0.832\n0.756\n0.801\n0.838 0.844\nnDCG@50.6\n0.7\n0.8\n0.9\n0.832\n0.756\n0.801\n0.838 0.844\nSR IRR nDCG@5\nFigure 3: Ablation study over different components (Graph smooth-\ning operation, Local interaction (Lit) & Time-adaptive modulator in\nTransformer, Self-supervised graph proximity loss) on NASDAQ.\n.5 .6 .7 .8 .85 .9 .95 1.0\nSparsity\n.78\n.82\n.86\n.90\n.94\n nDCG@5\nTSE\nNYSE\nNASDAQ\n(a) Graph structure\n8 12 16 20 24\nLookback window length\n.74\n.78\n.82\n.86\n.90\n.94 nDCG@5 v.s. STHAN-SR\nTSE\nNYSE\nNASDAQ\nGains (b) Lookback window\n1 2 3 4 5 6 7 8 9 10\nSelected top-rank stocks\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6 SR v.s. STHAN-SR\nTSE\nNYSE\nNASDAQ\nGains (c) Trade stocks\nFigure 4: InÔ¨Çuence of hyper-parameters \u001a, \u0001T, and \u0014.\n4.2 Overall Performance\nWe consider four categories of baselines for comparison.\nThe results are shown in Table 1, from which we have\nseveral observations: 1) In general, RL and ranking ap-\nproaches (e.g., iRDPG, RSR) perform better in investment\nreturns than conventional price classiÔ¨Åcation and regression\nmethods (e.g., HATR, SFM), which justiÔ¨Åes the effective-\nness of learning to rank optimization toward stock selection.\n2) Transformer-based encoders (HMG-TF, RAT) appreciably\nhave better capability of modeling stock temporal dependen-\ncies, while it is hard for RNN-based models (e.g., SFM, DA-\nRNN, SAE-LSTM) to harvest Ô¨Åne-granular interactive infor-\nmation among discontinuous time steps. 3) Exploiting inter-\nstock relationships is demonstrated to be conducive to invest-\nment forecasting (e.g., RSR, STHAN-SR). This accentuates\nthe collective synergistic effect of stock dynamics. Whereas,\nthe requirement to predeÔ¨Åne graph topologies based on do-\nmain knowledge raises the difÔ¨Åculty of generalizing these\nmethods in new scenarios. 4) By revamping Transformer to\nhierarchically perform pattern-wise interactions being aware\nof time intervals and fusing self-supervision signals of stock\nproximity, our proposed ALSP-TF obtains the best results\nacross all datasets. SpeciÔ¨Åcally, it fetches an average relative\nperformance gain of 8.57% and 18.54% in regard of risk ad-\njusted returns and cumulative proÔ¨Åts (t-testp< 0:01) over the\nbest baselines. In addition, greater degrees of improvements\nare observed onNASDAQand NYSE datasets than TSE, which\nmay indicate the advantage of ALSP-TF in dealing with large\ncandidate pools for stock portfolio selection.\n4.3 In-depth Analysis\nNext we conduct further analysis to learn the inÔ¨Çuence of var-\nious components and key hyperparameters in ALSP-TF.\nAblation Study. We investigate the effect of ablated vari-\nants from perspectives of both temporal and relational embed-\nhigh\nlow\nùêø!, \tùëé\" = 0.13, ùë£\" = 0.02 ùêø!, ùëé\" = ‚àí0.19, ùë£\" = 0.15 ùêø#, ùëé\" = ‚àí0.10, ùë£\" = 0.05\nlayer 1 \nlayer 2 \nlayer 3 \n                       \n       \nFigure 5: The time-adaptive coefÔ¨Åcients and global self-attention\nweights learned by different attention heads on NASDAQ.\nding. Due to space limitation, we depict the results on NAS-\nDAQ in Fig. 3, similar regularities can be observed on other\ndatasets. As shown, different components jointly contribute\nto the performance. The main beneÔ¨Åts stem from the local-\nity and time-sensitive peculiarities inside Transformer blocks,\nwhich serve for extracting multi-grained dynamic patterns\nand endow different attention heads with adaptive ability to\ntime intervals. The variant only retaining the temporal mod-\nule can beat other Transformer baselines (HMG-TF, RAT) as\nwell as state-of-the-art STHAN-SR that integrates LSTM and\nhypergraph convolution of pre-Ô¨Åxed corporate relations. Be-\nsides, introducing the graph-based smoothing and regulariza-\ntion further helps generate more stable and proÔ¨Åtable stock\nselections. This inspires us that the data-driven relation em-\nbedding channel can furnish useful hidden dependency infor-\nmation when prior domain knowledge is unavailable.\nParameter Sensitivity. We next look closer to building\nstock graphs with different sparsities. It can be seen from\nFig. 4a that either free of the proximity guidance (i.e., spar-\nsity = 1:0) or pushing too many edges will cause degrada-\ntion of performance. This is intuitive because among the\nvast number of inter-stock connections, only a few are mean-\ningful enough to signiÔ¨Åcantly inÔ¨Çuence the dynamic of re-\nlated stocks. Fig. 4b elaborates the impact of varied lookback\nlengths. We Ô¨Ånd that the advantage of ALSP-TF compared to\nSTHAN-SR is greater in the case of longer input, revealing the\nmerit of mining elaborate interactions among sequential pat-\nterns. We also explore the change of proÔ¨Åtability regarding\nthe number of selected top-rank stocks. Fig. 4c demonstrates\nALSP-TF‚Äôs suitability to trading strategies accompanied by\ndifferent risk appetites.\nInterpretation of Time-adaptive Self-attention. We fur-\nther interpret what is of importance the time-adaptive modu-\nlator in self-attention learns. TakingNASDAQas example, the\nupper part of Fig. 5 shows the proportion of positive/negative\nah and vh parameters tuned for the learnable rescale func-\ntion f(¬∑)over all attention heads. It is intriguing that the\nnumber of positive ah gradually increases along with the\nstacking hierarchy. It means that lower-layer attention heads\nprefer to capture interactions around local contexts, while\nthe heads at higher layers are responsible for modeling both\nshort- and long-term dependencies. In addition, most values\nof vh are positive, which may indicate that time information\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3975\nhas relatively strong impact on weighing the pattern interac-\ntions. Moreover, the bottom half of Fig. 5 visualizes the at-\ntention heatmaps of a stock sequence produced by different\nheads. We Ô¨Ånd the attention scores are structurally sharper\nsupported by the time-adaptive modulator. SpeciÔ¨Åcally, the\nÔ¨Årst heatmap concerns more on long interval dependencies,\nthe matching between time-steps 5th;13th and global con-\ntexts is prominently highlighted. In contrast, the latter two\nheatmaps put more emphasis on the information exchange\ninside local contexts. The interactive areas upon L2 layer‚Äôs\nembedding are broader, probably because self-attention acts\non patterns of a higher-level granularity. These results show\nthat our model can Ô¨Çexibly catch dynamic time factors and\nadjust attention on multi-grained sequential signals.\n5 Conclusion\nIn this paper, we presentALSP-TF, a new temporal-relational\nembedding framework for stock selection. The temporal\nmodule performs hierarchical representation and interaction\nof stock dynamic patterns based on a modiÔ¨Åed Transformer\nencoder. Different from vanilla self-attention that is con-\ntext and position agnostic, our model can adaptively cap-\nture short- and long-term pattern matching signals taking ad-\nvantage of locality and time-aware peculiarities. For rela-\ntional view, we propose a graph self-supervised regulariza-\ntion, which integrates collective synergies of stocks while re-\nlieving the dependence on prior domain knowledge. Through\nquantitative and qualitative analyses on three global market\ndatasets, we probe the effectiveness ofALSP-TF and set forth\nits applicability in investment forecast and recommendation.\nAcknowledgements\nThis research is supported by the National Key Research\nand Development Program of China (No. 2021ZD0111202),\nthe National Natural Science Foundation of China\n(No.62176005, 91846303), PKU-Haier ‚ÄúSmart Home‚Äù\nQingdao Joint Innovation Lab, Project 2020BD002 sup-\nported by PKU-Baidu Fund, and Research Funds for NSD\nConstruction, University of International Relations.\nReferences\n[Bao et al., 2017] Wei Bao, Jun Yue, and Yulei Rao. A deep\nlearning framework for Ô¨Ånancial time series using stacked\nautoencoders and long-short term memory. PloS one,\n12(7):e0180944, 2017.\n[Carta et al., 2021] Salvatore Carta, Anselmo Ferreira,\nAlessandro Sebastian Podda, Diego Reforgiato Recupero,\nand Antonio Sanna. Multi-dqn: An ensemble of deep\nq-learning agents for stock market forecasting. Expert\nSyst. Appl., 164:113820, 2021.\n[Chen et al., 2018] Yingmei Chen, Zhongyu Wei, and Xu-\nanjing Huang. Incorporating corporation relationship via\ngraph convolutional neural networks for stock price pre-\ndiction. In CIKM, pages 1655‚Äì1658, 2018.\n[Ding et al., 2020] Qianggang Ding, Sifan Wu, Hao Sun, Ji-\nadong Guo, and Jian Guo. Hierarchical multi-scale gaus-\nsian transformer for stock movement prediction. In IJCAI,\npages 4640‚Äì4646, 2020.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In ICLR, 2021.\n[Feng et al., 2019a] Fuli Feng, Huimin Chen, Xiangnan He,\nJi Ding, Maosong Sun, and Tat-Seng Chua. Enhancing\nstock movement prediction with adversarial training. In\nIJCAI, pages 5843‚Äì5849, 2019.\n[Feng et al., 2019b] Fuli Feng, Xiangnan He, Xiang Wang,\nCheng Luo, Yiqun Liu, and Tat-Seng Chua. Temporal re-\nlational ranking for stock prediction. TOIS, 37(2):1‚Äì30,\n2019.\n[Gao et al., 2019] Peng Gao, Zhengkai Jiang, Haoxuan You,\nPan Lu, Steven CH Hoi, Xiaogang Wang, and Hongsheng\nLi. Dynamic fusion with intra-and inter-modality attention\nÔ¨Çow for visual question answering. In CVPR, pages 6639‚Äì\n6648, 2019.\n[Jeong et al., 2011] Young-Seon Jeong, Myong K Jeong, and\nOlufemi A Omitaomu. Weighted dynamic time warp-\ning for time series classiÔ¨Åcation. Pattern recognition,\n44(9):2231‚Äì2240, 2011.\n[Kavitha et al., 2013] G Kavitha, A Udhayakumar, and\nD Nagarajan. Stock market trend analysis using hidden\nmarkov models. arXiv preprint arXiv:1311.4771, 2013.\n[Khaidem et al., 2016] Luckyson Khaidem, Snehanshu\nSaha, and Sudeepa Roy Dey. Predicting the direction of\nstock market prices using random forest. arXiv preprint\narXiv:1605.00003, 2016.\n[Kim et al., 2019] Raehyun Kim, Chan Ho So, Minbyul\nJeong, Sanghoon Lee, Jinkyu Kim, and Jaewoo Kang.\nHats: A hierarchical graph attention network for stock\nmovement prediction. arXiv preprint arXiv:1908.07999,\n2019.\n[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In ICLR,\n2015.\n[Kipf and Welling, 2017] Thomas N. Kipf and Max Welling.\nSemi-supervised classiÔ¨Åcation with graph convolutional\nnetworks. In ICLR, 2017.\n[Lai et al., 2017] Lin Lai, Chang Li, and Wen Long. A\nnew method for stock price prediction based on mrfs and\nSSVM. In ICDM, pages 818‚Äì823, 2017.\n[Li et al., 2021] Wei Li, Ruihan Bao, Keiko Harimoto, Deli\nChen, Jingjing Xu, and Qi Su. Modeling the stock relation\nwith graph network for overnight stock movement predic-\ntion. In IJCAI, pages 4541‚Äì4547, 2021.\n[Liu et al., 2020] Yang Liu, Qi Liu, Hongke Zhao, Zhen Pan,\nand Chuanren Liu. Adaptive quantitative trading: An im-\nitative deep reinforcement learning approach. In AAAI,\npages 2128‚Äì2135, 2020.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3976\n[Luo et al., 2014] Yongen Luo, Jicheng Hu, Xiaofeng Wei,\nDongjian Fang, and Heng Shao. Stock trends prediction\nbased on hypergraph modeling clustering algorithm. In\nPIC, pages 27‚Äì31, 2014.\n[Nayak et al., 2015] Rudra Kalyan Nayak, Debahuti Mishra,\nand Amiya Kumar Rath. A na ¬®ƒ±ve svm-knn based stock\nmarket trend reversal analysis for indian benchmark in-\ndices. Applied Soft Computing, 35:670‚Äì680, 2015.\n[Oord et al., 2016] Aaron van den Oord, Sander Diele-\nman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. Wavenet: A generative model for raw au-\ndio. In ISCA, page 125, 2016.\n[Qin et al., 2017] Yao Qin, Dongjin Song, Haifeng Chen,\nWei Cheng, Guofei Jiang, and Garrison W. Cottrell. A\ndual-stage attention-based recurrent neural network for\ntime series prediction. In IJCAI, pages 2627‚Äì2633, 2017.\n[Sawhney et al., 2021] Ramit Sawhney, Shivam Agarwal,\nArnav Wadhwa, Tyler Derr, and Rajiv Ratn Shah. Stock\nselection via spatiotemporal hypergraph attention net-\nwork: A learning to rank approach. In AAAI, pages 497‚Äì\n504, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998‚Äì6008, 2017.\n[Velickovicet al., 2017] Petar Velickovic, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Li `o, and\nYoshua Bengio. Graph attention networks. In ICLR, 2017.\n[Wang and Leu, 1996] Jung-Hua Wang and Jia-Yann Leu.\nStock market trend prediction using arima-based neural\nnetworks. In ICNN, volume 4, pages 2160‚Äì2165, 1996.\n[Wang et al., 2020] Heyuan Wang, Tengjiao Wang, and\nYi Li. Incorporating expert-based investment opinion sig-\nnals in stock prediction: A deep learning framework. In\nAAAI, pages 971‚Äì978, 2020.\n[Wang et al., 2021] Heyuan Wang, Shun Li, Tengjiao Wang,\nand Jiayi Zheng. Hierarchical adaptive temporal-relational\nmodeling for stock trend prediction. InIJCAI, pages 3691‚Äì\n3698, 2021.\n[Wu et al., 2021] Chuhan Wu, Fangzhao Wu, and Yongfeng\nHuang. Da-transformer: Distance-aware transformer. In\nNAACL-HLT, pages 2059‚Äì2068, 2021.\n[Xu et al., 2020] Ke Xu, Yifan Zhang, Deheng Ye, Peilin\nZhao, and Mingkui Tan. Relation-aware transformer for\nportfolio policy learning. In IJCAI, pages 4647‚Äì4653,\n2020.\n[Zhang et al., 2017] Liheng Zhang, Charu Aggarwal, and\nGuo-Jun Qi. Stock price prediction via discovering multi-\nfrequency trading patterns. In KDD, pages 2141‚Äì2149,\n2017.\n[Zhou et al., 2021] Haoyi Zhou, Shanghang Zhang, Jieqi\nPeng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\nZhang. Informer: Beyond efÔ¨Åcient transformer for long\nsequence time-series forecasting. In AAAI, pages 11106‚Äì\n11115, 2021.\nProceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n3977",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6247564554214478
    },
    {
      "name": "Investment strategy",
      "score": 0.458459734916687
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4483432471752167
    },
    {
      "name": "Stock market",
      "score": 0.43690404295921326
    },
    {
      "name": "Skew",
      "score": 0.4256768226623535
    },
    {
      "name": "Stock exchange",
      "score": 0.42158812284469604
    },
    {
      "name": "Machine learning",
      "score": 0.40010714530944824
    },
    {
      "name": "Econometrics",
      "score": 0.32007119059562683
    },
    {
      "name": "Mathematics",
      "score": 0.15694227814674377
    },
    {
      "name": "Economics",
      "score": 0.15005606412887573
    },
    {
      "name": "Context (archaeology)",
      "score": 0.1375170648097992
    },
    {
      "name": "Finance",
      "score": 0.12846243381500244
    },
    {
      "name": "Market liquidity",
      "score": 0.12061712145805359
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I67636235",
      "name": "University of International Relations",
      "country": "CN"
    }
  ]
}