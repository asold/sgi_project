{
  "title": "Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content",
  "url": "https://openalex.org/W2250460709",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2128657275",
      "name": "Nemanja Djuric",
      "affiliations": [
        "Yahoo (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A845246572",
      "name": "Hao Wu",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2301621503",
      "name": "Vladan Radosavljevic",
      "affiliations": [
        "Yahoo (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2000240052",
      "name": "Mihajlo Grbovic",
      "affiliations": [
        "Yahoo (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1964376778",
      "name": "Narayan Bhamidipati",
      "affiliations": [
        "Yahoo (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2073601450",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2153635508",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2000257387",
    "https://openalex.org/W2271992213",
    "https://openalex.org/W4233135949",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2154851992",
    "https://openalex.org/W2604272474",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2165599843",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2127426251",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2950830772",
    "https://openalex.org/W2242422364",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W2171361956",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2949547296",
    "https://openalex.org/W2107743791",
    "https://openalex.org/W2596585349",
    "https://openalex.org/W2171086879",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1880262756"
  ],
  "abstract": "We consider the problem of learning distributed representations for documents in data streams. The documents are represented as low-dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models. In particular, we exploit the context of documents in streams and use one of the language models to model the document sequences, and the other to model word sequences within them. The models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space. We discuss extensions to our model, which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy, thus learning user-specific vectors to represent individual preferences. We validated the learned representations on a public movie rating data set from MovieLens, as well as on a large-scale Yahoo News data comprising three months of user activity logs collected on Yahoo servers. The results indicate that the proposed model can learn useful representations of both documents and word tokens, outperforming the current state-of-the-art by a large margin.",
  "full_text": "Hierarchical Neural Language Models for Joint\nRepresentation of Streaming Documents and their Content\nNemanja Djuric∗†, Hao Wu∗‡, Vladan Radosavljevic†, Mihajlo Grbovic†, Narayan Bhamidipati†\n†Y ahoo Labs, Sunnyvale, CA, USA, {nemanja, vladan, mihajlo, narayanb}@yahoo-inc.com\n‡University of Southern California, Los Angeles, CA, USA, hwu732@usc.edu\nABSTRACT\nWe consider the problem of learning distributed represen-\ntations for documents in data streams. The documents\nare represented as low-dimensional vectors and are jointly\nlearned with distributed vector representations of word to-\nkens using a hierarchical framework with two embedded neu-\nral language models. In particular, we exploit the context of\ndocuments in streams and use one of the language models to\nmodel the document sequences, and the other to model word\nsequences within them. The models learn continuous vector\nrepresentations for both word tokens and documents such\nthat semantically similar documents and words are close in\na common vector space. We discuss extensions to our model,\nwhich can be applied to personalized recommendation and\nsocial relationship mining by adding further user layers to\nthe hierarchy, thus learning user-speciﬁc vectors to represent\nindividual preferences. We validated the learned representa-\ntions on a public movie rating data set from MovieLens, as\nwell as on a large-scale Yahoo News data comprising three\nmonths of user activity logs collected on Yahoo servers. The\nresults indicate that the proposed model can learn useful\nrepresentations of both documents and word tokens, out-\nperforming the current state-of-the-art by a large margin.\nCategories and Subject Descriptors\nI.2.7 [Artiﬁcial Intelligence]: Natural Language Process-\ning—Language Models; I.5.4 [ Pattern Recognition]: Ap-\nplications—Text processing; I.7.m [ Document and Text\nProcessing]: Miscellaneous\nKeywords\nMachine learning; document modeling; distributed represen-\ntations; word embeddings; document embeddings.\n1. INTRODUCTION\nText documents coming in a sequence are common in real-\nworld applications and can arise in various contexts. For ex-\nample, consider Web pages surfed by users in random walks\n∗Authors contributed equally. This work was done while\nHao Wu was an intern at Yahoo Labs in Sunnyvale, USA.\nCopyright is held by the International World Wide Web Conference Com-\nmittee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the\nauthor’s site if the Material is used in electronic media.\nWWW 2015, May 18–22, 2015, Florence, Italy.\nACM 978-1-4503-3469-3/15/05.\nhttp://dx.doi.org/10.1145/2736277.2741643.\nalong the hyperlinks, streams of click-through URLs associ-\nated with a query in search engine, publications of an author\nin chronological order, threaded posts in online discussion\nforums, answers to a question in online knowledge-sharing\ncommunities, or e-mails in a common thread, to name a few.\nThe co-occurrences of documents in a temporal sequence\nmay reveal relatedness between them, such as their seman-\ntic and topical similarity. In addition, sequences of words\nwithin the documents introduce another rich and complex\nsource of the data, which can be leveraged together with the\ndocument stream information to learn useful and insightful\nrepresentations of both documents and words.\nIn this paper, we introduce algorithm that can simulta-\nneously model documents from a stream and their residing\nnatural language in one common lower-dimensional vector\nspace. Such algorithm goes beyond representations which\nconsider each document as a separate bag-of-words compo-\nsition, most notably Probabilistic Latent Semantic Analysis\n(PLSA) [10] and Latent Dirichlet Allocation (LDA) [3]. We\nfocus on learning continuous representations of documents\nin vector space jointly with distributed word representations\nusing statistical neural language models [2], whose success\nwas previously shown in a number of publications [6]. More\nprecisely, we propose hierarchical models where document\nvectors act as units in a context of document sequences, and\nalso as global contexts of word sequences contained within\nthem. The probability distribution of observing a word de-\npends not only on some ﬁxed number of surrounding words,\nbut is also conditioned on the speciﬁc document. Mean-\nwhile, the probability distribution of a document depends\non the surrounding documents in stream data. Our work is\nmost similar to [13] as it also considers document vectors as\na global context for words contained within, however the au-\nthors in [13] do not model the document relationships which\nbrings signiﬁcant modeling strength to our models and opens\na plethora of application areas for the neural language mod-\nels. In our work, the models are trained to predict words and\ndocuments in a sequence with maximum likelihood. We op-\ntimize the models using stochastic gradient learning, a ﬂex-\nible and powerful optimization framework suitable for the\nconsidered large-scale problems in an online setting where\nnew samples arrive sequentially.\nVector representations of documents and words learned\nby our model are useful for various applications of critical\nimportance to online businesses. For example, by means of\nsimple nearest-neighbor searches in the joint vector space be-\ntween document and word representations, we can address\na number of important tasks: 1) given a query keyword,\narXiv:1606.08689v1  [cs.CL]  28 Jun 2016\nsearch for similar keywords to expand the query (useful in\nthe search product); 2) given a keyword, search for rele-\nvant documents such as news stories (useful in document\nretrieval); 3) given a document, retrieve similar or related\ndocuments (useful for news stream personalization and doc-\nument recommendation); and 4) automatically generate re-\nlated words to tag or summarize a given document (useful\nin native advertising or document retrieval). All these tasks\nare essential elements of a number of online applications,\nincluding online search, advertising, and personalized rec-\nommendation. In addition, as we show in the experimental\nsection, learned document representations can be used to\nobtain state-of-the-art document classiﬁcation results.\nThe proposed approach represents a step towards auto-\nmatic organization, semantic analysis, and summarization\nof documents observed in sequences. We summarize our\nmain contributions below:\n•We propose hierarchical neural language model which\ntakes advantage of the context of document sequences\nand the context of each word within a document to\nlearn their low-dimensional vector representations. The\ndocument contexts can act as an empirical prior which\nhelps learn smoothed representations for documents.\nThis is useful for learning representations of short doc-\numents with a few words, for which [13] tends to learn\npoor document representations as separately learned\ndocument vectors may overﬁt to a few words within a\nspeciﬁc document. Conversely, the proposed model is\nnot dependent on the document content as much.\n•The proposed approach can capture inherent connec-\ntions between documents in the data stream. We tai-\nlor our models to analyze movie reviews where a user’s\npreferences may be biased towards particular genres,\nas well as Yahoo News articles for which we collect\nclick-through logs of a large number of users and learn\nuseful news article representations. The experimental\nresults on retrieval and categorization tasks demon-\nstrate eﬀectiveness of the proposed model.\n•Our learning framework is ﬂexible and it is straight-\nforward to add more layers in order to learn additional\nrepresentations for related concepts. We propose ex-\ntensions of the model and discuss how to learn explicit\ndistributed representations of users on top of the basic\nframework. The extensions can be applied to person-\nalized recommendation and social relationship mining.\n2. RELATED WORK\nThe related work is largely centered on the notion of neu-\nral language models [2], which improve generalization of the\nclassic n-gram language models by using continuous vari-\nables to represent words in a vector space. This idea of dis-\ntributed word representations has spurred many successful\napplications in natural language processing. More recently,\nthe concept of distributed representations has been extended\nbeyond pure language words to a number of applications, in-\ncluding modeling of phrases [15], sentences and paragraphs\n[13], relational entities [4, 20], general text-based attributes\n[12], descriptive text of images [11], online search sessions\n[7], smartphone apps [1], and nodes in a network [19].\n2.1 Neural language models\nNeural language models take advantage of a word order,\nand state the same assumption asn-gram models that words\ncloser in a sequence are statistically more dependent. Typi-\ncally, a neural language model learns the probability distri-\nbution of next word given a ﬁxed number of preceding words\nwhich act as the context. More formally, given a word se-\nquence (w1,w2,...,w T ) in a training data, the objective of\nthe model is to maximize log-likelihood of the sequence,\nL=\nT∑\nt=1\nlog P(wt|wt−n+1 : wt−1), (1)\nwhere wt is the tth word in the sequence, and wt−n+1 :\nwt−1 represents a sequence of successive preceding words\n(wt−n+1,...,w t−1) that act as the context to the word wt.\nA typical model architecture to approximate probability dis-\ntribution P(wt|wt−n+1 : wt−1) is to use a neural network\n[2]. The neural network is trained by projecting the con-\ncatenation of vectors for context words ( wt−n+1,...,w t−1)\ninto a latent representation with multiple non-linear hidden\nlayers and the output soft-max layer comprising W nodes,\nwhere W is a size of the vocabulary, where the network\nattempts to predict wt with high probability. However, a\nneural network of large size is challenging to train, and the\nword vectors are computationally expensive to learn from\nlarge-scale data sets comprising millions of words, which are\ncommonly found in practice. Recent approaches with diﬀer-\nent versions of log-bilinear models [16] or log-linear models\n[14] attempt to modify the model architecture in order to\nreduce the computational complexity. The use of hierarchi-\ncal soft-max [18] or noise contrastive estimation [17] can also\nhelp speed up the training complexity. In the following we\nreview two recent neural language models [14, 15] which di-\nrectly motivated our work, namely continuous bag-of-words\n(CBOW) and skip-gram (SG) model.\n2.2 Continuous bag-of-words\nThe continuous bag-of-words model is a simpliﬁed neural\nlanguage model without any non-linear hidden layers. A log-\nlinear classiﬁer is used to predict a current word based on its\npreceding and succeeding words, where their representations\nare averaged as the input. More precisely, the objective of\nthe CBOW model is to maximize the log-likelihood,\nL=\nT∑\nt=1\nlog P(wt|wt−c : wt+c), (2)\nwhere c is the context length, and wt−c : wt+c is the sub-\nsequence (wt−c,...,w t+c) excluding wt itself. The probabil-\nity P(wt|wt−c : wt+c) is deﬁned using the softmax,\nP(wt|wt−c : wt+c) = exp(¯v⊤v′\nwt)∑W\nw=1 exp(¯v⊤v′w)\n, (3)\nwhere v′\nwt is the output vector representation of wt, and ¯v\nis averaged vector representation of the context, found as\n¯v = 1\n2 c\n∑\n−c≤j≤c,j̸=0\nvwt+j . (4)\nHere vw is an input vector representation of word w. It is\nworth noting that CBOW takes only partial advantage of\nthe word order, since any ordering of a set of contextual\nwords will result in the same vector representation.\n… …\nProjec�on\n…\ndm-b\ndm wm,t-c\nwmt\nwm,t-1\t wm,t+1 wm,t+c\ndm-1 dm+1 dm+b\ndocument\tstream\t(context)\nwords\twithin\tdocument\t(content)\t\nProjec�on\t\n…\nFigure 1: The proposed hierarchical model architecture with two embedded neural languages models\n(orange/left - document vectors; green/right - word vectors)\n2.3 Skip-gram model\nInstead of predicting the current word based on the words\nbefore and after it, the skip-gram model tries to predict the\nsurrounding words within a certain distance based on the\ncurrent one. More formally, skip-gram deﬁnes the objective\nfunction as the exact counterpart to the continuous bag-of-\nwords model,\nL=\nT∑\nt=1\nlog P(wt−c : wt+c|wt). (5)\nFurthermore, the model simpliﬁes the probability distribu-\ntion, introducing an assumption that the contextual words\nwt−c : wt+c are independent given current word wt,\nP(wt−c : wt+c|wt) =\n∏\n−c≤j≤c,j̸=0\nP(wt+j|wt), (6)\nwith P(wt+j|wt) deﬁned as\nP(wt+j|wt) =\nexp(v⊤\nwtv′\nwt+j )\n∑W\nw=1 exp(v⊤wtv′w)\n, (7)\nwhere vw and v′\nw are the input and output vectors of w.\nIncreasing the range of context c would generally improve\nthe quality of learned word vectors, albeit at the expense\nof higher computation cost. Skip-gram assumes that the\nsurrounding words are equally important, and in this sense\nthe word order is again not fully exploited, similarly to the\nCBOW model. A wiser strategy to account for this issue\nwould be to sample training word pairs in (7) less from rel-\natively distant words that appear in the context [14].\n3. HIERARCHICAL LANGUAGE MODEL\nIn this section we present our algorithm for joint model-\ning of streaming documents and the words contained within,\nwhere we learn distributed representations for both the doc-\numents and the words in a shared, low-dimensional embed-\nding space. The approach is inspired by recently proposed\nmethods for learning vector representations of words which\ntake advantage of a word order observed in a sentence [14].\nHowever, and unlike similar work presented by the authors\nof [13], we also exploit temporal neighborhood of the stream-\ning documents. In particular, we model the probability of\nobserving a particular document by taking into account its\ntemporally-close documents, in addition to conditioning on\nthe content of the document.\n3.1 Model architecture\nIn the considered problem setting, we assume the train-\ning documents are given in a sequence. For example, if\nthe documents are news articles, a document sequence can\nbe a sequence of news articles sorted in a chronological or-\nder in which the user read them. More speciﬁcally, we as-\nsume that we are given a set Sof S document sequences,\nwith each sequence s ∈S consisting of Ns documents, s =\n(d1,d2,...,d Ns). Moreover, each document in a stream dm\nis a sequence of Tm words, dm = ( wm1,wm2,...,w m,Tm).\nWe aim to simultaneously learn low-dimensional representa-\ntions of streaming documents and language words in a com-\nmon vector space, and represent each document and word\nas a continuous feature vector of dimensionality D. If we\nassume that there are M unique documents in the training\ncorpus and W unique words in the vocabulary, then during\ntraining we aim to learn ( M + W) ·D model parameters.\nThe context of a document sequence and the natural lan-\nguage context are modeled using the proposed hierarchical\nneural language model, where document vectors act not only\nas the units to predict their surrounding documents, but also\nas the global context of word sequences contained within\nthem. The architecture consists of two embedded layers,\nshown in Figure 1. The upper layer learns the temporal\ncontext of a document sequence, based on the assumption\nthat temporally closer documents in the document stream\nare statistically more dependent. We note that temporally\ndoes not need to refer to the time of creation of the docu-\nment, but also to the time the document was read by the\nuser, which is the deﬁnition we use in our experiments. The\nbottom layer models the contextual information of word se-\nquences. Lastly, we connect these two layers by adopting the\nidea of paragraph vectors [13], and consider each document\ntoken as a global context for all words within the document.\nMore formally, given set S of sequences of documents,\nthe objective of the hierarchical model is to maximize log-\nlikelihood of the streaming data,\nL=\n∑\ns∈S\n(\nα\n∑\ndm∈s\n∑\nwmt∈dm\nlog P(wmt|wm,t−c : wm,t+c,dm)+\n∑\ndm∈s\n(\nαlog P(dm|wm1 : wmT ) +\n∑\n−b≤i≤b,i̸=0\nlog P(dm+i|dm)\n))\n,\n(8)\nwhere α is the weight that speciﬁes a trade-oﬀ between fo-\ncusing on minimization of the log-likelihood of document\nsequence and of the log-likelihood of word sequences (we\nset α = 1 in the experiments), b is the length of the train-\ning context for document sequences, and c is the length of\nthe training context for word sequences. In this particular\narchitecture, we are using the CBOW model in the lower,\nsentence layer, and the skip-gram model in the upper, docu-\nment layer. However, we note that either of the two models\ncan be used in any level of the hierarchy, and a speciﬁc choice\ndepends on the modalities of the problem at hand.\nGiven the architecture illustrated in Figure 1, probability\nof observing one of the surrounding documents based on the\ncurrent document P(dm+i|dm) is deﬁned using the soft-max\nfunction as given below,\nP(dm+i|dm) =\nexp(v⊤\ndmv′\ndm+i)\n∑M\nd=1 exp(v⊤\ndmv′\nd)\n, (9)\nwhere vd and v′\nd are the input and output vector representa-\ntions of document d. Furthermore, probability of observing\na word depends not only on its surrounding words, but also\non a speciﬁc document that the word belongs to. More pre-\ncisely, probability P(wmt|wm,t−c : wm,t+c,dm) is deﬁned as\nP(wmt|wm,t−c : wm,t+c,dm) = exp(¯v⊤v′\nwmt)∑W\nw=1 exp(¯v⊤v′w)\n, (10)\nwhere v′\nwmt is the output vector representation of wmt, and\n¯v is the averaged vector representation of the context (in-\ncluding the speciﬁc document dm), deﬁned as follows,\n¯v = 1\n2 c+ 1(vdm +\n∑\n−c≤j≤c,j̸=0\nvwm,t+j ). (11)\nLastly, we deﬁne probability of observing a document given\nthe words contained within P(dm|wm1 : wm,Tm) in a similar\nway, by reusing equations (10) and (11) and replacing the\nappropriate variables.\n3.2 Variants of the model\nIn the previous section we presented a typical architecture\nwhere we speciﬁed language models in each layer of the hi-\nerarchy. However, in real-world applications, we could vary\nthe language models for diﬀerent purposes in a straightfor-\nward manner. For example, a news website would be inter-\nested in predicting on-the-ﬂy which news article a user would\nread after a few clicks on some other news stories, in order\nto personalize the news feed. Then, it could be more rea-\nsonable to use directed, feed-forward models which estimate\nP(dm|dm−b : dm−1), probability of the mth document in the\nsequence given its b preceding documents. Or, equivalently,\nif we want to model which documents were read prior to\nthe currently observed sequence, we can use feed-backward\nmodels which estimate P(dm|dm+1 : dm+b), probability of\nthe mth document given its b succeeding documents.\nMoreover, it is straightforward to extend the described\nmodel for problems that naturally have more than two lay-\ners. Let us assume that we have a data set with streaming\ndocuments speciﬁc to a diﬀerent set of users (e.g., for each\ndocument we also know which user generated or read the\ndocument). Then, we may build more complex models to\nsimultaneously learn distributed representations of users by\nadding additional user layer on top of the document layer.\nIn this setup a user vector could serve as a global context of\nstreaming documents pertaining to that speciﬁc user, much\nlike a document vector serves as a global context to words\npertaining to that speciﬁc document. More speciﬁcally, we\nwould predict a document based on the surrounding docu-\nments and its content, while also conditioning on a speciﬁc\nuser. This variant models P(dm|dm−b : dm−1,u), where u is\nan indicator variable for a user. Learning vector representa-\ntions of users would open doors for further improvement of\npersonalized services, where personalization would reduce to\nsimple nearest-neighbor search in the joint embedding space.\n3.3 Model optimization\nThe model is optimized using stochastic gradient ascent,\nwhich is very suitable for large-scale problems. However,\ncomputation of gradient ∇log P(wmt|wm,t−c : wm,t+c,dm)\nin (8) is proportional to the vocabulary size W, and of gra-\ndients ∇log P(dm+i|dm) and ∇log P(dm|wm1 : wm,Tm) is\nproportional to the number of training documents M. This\nis computationally expensive in practice, since both W and\nM could easily reach millions in our applications.\nAn eﬃcient alternative that we use is hierarchical soft-max\n[18], which reduces the time complexity to O\n(\nRlog(W) +\nbMlog(M)\n)\nin our case, where R is the total number of\nwords in the document sequence. Instead of evaluating ev-\nery distinct word or document during each gradient step in\norder to compute the sums in equations (9) and (10), hi-\nerarchical softmax uses two binary trees, one with distinct\ndocuments as leaves and the other with distinct words as\nleaves. For each leaf node, there is a unique assigned path\nfrom the root which is encoded using binary digits. To con-\nstruct a tree structure the Huﬀman encoding is typically\nused, where more frequent documents (or words) in the data\nset have shorter codes to speed up the training. The inter-\nnal tree nodes are represented as real-valued vectors, of the\nsame dimensionality as word and document vectors. More\nprecisely, hierarchical soft-max expresses the probability of\nobserving the current document (or word) in the sequence\nas a product of probabilities of the binary decisions speciﬁed\nby the Huﬀman code of the document as\nP(dm+i|dm) =\n∏\nl\nP(hl|ql,dm), (12)\nwhere hl is the lth bit in the code with respect to ql, which\nis the lth node in the speciﬁed tree path of dm+i. The prob-\nability of each binary decision is deﬁned as follows,\nP(hl = 1|ql,dm) = σ(v⊤\ndmvql), (13)\nwhere σ(x) is the sigmoid function, andvql is the vector rep-\nresentation of nodeql. It can be veriﬁed that∑M\nd=1 P(dm+i =\nd|dm) = 1, and hence the property of probability distribu-\ntion is preserved. We can compute P(dm|wm1 : wm,Tm)\nin the same manner. Furthermore, we similarly express\nP(wmt|wm,t−c : wm,t+c,dm), but with construction of a sep-\narate Huﬀman tree pertaining to the words.\nTable 1: Movie classiﬁcation accuracy\nAlgorithm drama comedy thriller romance action crime adventure horror\nLDA 0.5544 0.5856 0.8158 0.8173 0.8745 0.8685 0.8765 0.9063\nparagraph2vec 0.6367 0.6767 0.7958 0.7919 0.8193 0.8537 0.8524 0.8699\nword2vec 0.7172 0.7449 0.8102 0.8204 0.8627 0.8692 0.8768 0.9231\nHDV 0.7274 0.7487 0.8201 0.8233 0.8814 0.8728 0.8854 0.9872\nTable 2: Nearest word neighbors of selected keywords\nboxing university movies batman woods hijack tennis messi\nwelterweight school characters superman tiger hijacked singles neymar\nknockouts college ﬁlms superhero masters whereabouts masters ronaldo\nﬁghts professor studio gotham holes transponders djokovic barca\nmiddleweight center audiences comics golf autopilot nadal iniesta\nufc students actors trilogy hole radars federer libel\nheavyweight national feature avenger pga hijackers celebration atletico\nbantamweight medical pictures sci classic turnback sharapova cristiano\ngreats american drama sequel par hijacking atp benzema\nwrestling institute comedy marvel doral decompression slam argentine\namateur california audience prequel mcilroy baﬄing roger barcelona\n4. EXPERIMENTS\nIn this section we describe experimental evaluation of the\nproposed approach, which we refer to as hierarchical docu-\nment vector (HDV) model. First, we validated the learned\nrepresentations on a public movie ratings data set, where\nthe task was to classify movies into diﬀerent genres. Then,\nwe used a large-scale data set of user click-through logs on a\nnews stream collected on Yahoo servers to showcase a wide\napplication potential of the HDV algorithm. In all experi-\nments we used cosine distance to measure the closeness of\ntwo vectors (either document or word vectors) in the com-\nmon low-dimensional embedding space.\n4.1 Movie genre classiﬁcation\nIn the ﬁrst set of experiments we validated quality of the\nobtained distributed document representations on a classi-\nﬁcation task using a publicly available data set. We note\nthat, although we had access to a proprietary data set dis-\ncussed in Section 4.2 which served as our initial motivation\nto develop the HDV model, obtaining a similar public data\nset proved to be much more diﬃcult. To this end, we gen-\nerated such data by combining a public movie ratings data\nset MovieLens 10M1, consisting of movie ratings for around\n10,000 movies generated by more than 71 ,000 users, with\na movie synopses data set from Internet Movie DataBase\n(IMDB) that was found online2. Each movie in the data set\nis tagged as belonging to one or more genres, such as “ac-\ntion”, “comedy”, or “horror”. Then, following terminology\nfrom the earlier sections, we viewed movies as “documents”\nand synopses as“document content”. The document streams\nwere obtained by taking for each user the movies that they\nrated 4 and above (on the scale from 1 to 5), and ordering\nthem in a sequence according to a timestamp of the rating.\nThe described preprocessing resulted in 69,702 document se-\nquences comprising 8 ,565 distinct movies, with an average\nsynopsis length of 190 .6 words.\n1grouplens.org/datasets/movielens/, accessed March 2015\n2ftp.fu-berlin.de/pub/misc/movies/database/, March 2015\nLet us discuss several explicit assumptions that we made\nwhile generating the movie data set. First, we retained only\nhigh-rated movies for each user in order to make the data\nless noisy, as the assumption is that the users are more likely\nto enjoy two movies that belonged to the same genre, than\ntwo movies coming from two diﬀerent genres. Thus, by re-\nmoving low-rated movies we aim to keep only similar movies\nin a single user’s sequence. As we show in the remainder of\nthe section, the experimental results indicate that the as-\nsumption holds true. In addition, we used the timestamp of\na rating as a proxy for a time when the movie was actually\nwatched. Although this might not always hold in reality, the\nempirical results suggest that the assumption was reasonable\nfor learning useful movie and word embeddings.\nWe learned movie vector representations for the described\ndata set using the following methods: 1) LDA [3], which\nlearns low-dimensional representations of documents (i.e.,\nmovies) as a topic distribution over their synopses; 2) para-\ngraph vector (paragraph2vec) [13], where an entire movie\nsynopsis is taken as a single paragraph; 3) word2vec [15],\nwhere movie sequences are used as “sentences” and movies\nare used as “words”; and 4) the proposed HDV method.\nWe used publicly available implementations of online LDA\ntraining [9]3 and word2vec4, and used our implementations\nof paragraph2vec and HDV algorithms. Note that LDA and\nparagraph2vec take into account only the content of the doc-\numents (i.e., movie synopses), word2vec only considers the\nmovie sequences and does not consider the movie synopses\nand contained natural language in any way, while HDV com-\nbines the two approaches and jointly considers and models\nboth the movie sequences and the content of movie synopses.\nDimensionality of the embedding space was set toD= 100\nfor all low-dimensional embedding methods (in the case of\nLDA this amounts to using 100 topics). The neighborhood\nof the neural language methods was set to 5 for both docu-\nment and word sequences, and the models were trained for\n3hunch.net/∼vw/, accessed March 2015\n4code.google.com/p/word2vec, accessed March 2015\nTable 3: Most related news stories retrieved for a given keyword\nmovies tennis\n’American Hustle,’ ’Wolf of Wall Street’ Lead Nominations Tennis-Venus through to third round, Li handed walkover\n3 Reasons ’Jurassic World’ Is Headed in the Right Direction Nadal rips Hewitt, Serena and Sharapova survive at Miami\nIrish Film and TV academy lines up stellar guest list for awards Williams battles on at Sony Open in front of empty seats\n10 things the Academy Awards won’t say Serena, Sharapova again on Miami collision course\nI saw Veronica Mars, thanks to $35 donation, 2 apps & $8 ticket Wawrinka survives bumpy start to Sony Open\nboxing hijack\nYushin Okami’s Debut for WSOF 9 in Las Vegas Thai radar might have tracked missing plane\nUFC Champ Jon Jones Denies Daniel Cormier Title Shot Request Criminal probe under way in Malaysia plane drama\nUFC contender Alex Gustafsson staring at a no-win situation Live: Malaysia asks India to join the expanding search\nAlvarez back as Molina, Santa Cruz defend boxing titles Malaysia dramatically expands search for missing jet\nAnthony Birchak Creates MFC Championship Ring Malaysia widening search for missing plane, says minister\nuniversity entertainment\nThe 20 Public Colleges With The Smartest Students ‘American Hustle,’ ‘Wolf of Wall Street’ Lead Nominations\nSpring storm brings blizzard warning for Cape Cod Madison Square Garden Company Buys 50% Stake in Tribeca\nU.S. News Releases 2015 Best Graduate Schools Rankings Renaissance Technologies sells its Walt Disney position\nNo Friday Night Lights at $60 Million Texas Stadium News From the Advertising Industry\nNew Orleans goes all in on charter schools 10 things the Academy Awards won’t say\nTable 4: Top related words for news stories\nNews articles Related keywords\nhardcourts biscayne sharapova nadal nishikoriSerena beats Li for seventh Miami crown aces unforced walkover angelique threeset\nisas pensioners savers oft annuityThis year’s best buy ISAs isa pots taxfree nomakeupselﬁe allowance\nmwc quadcore snapdragon oneplus ghzGalaxy S5 will get oﬀ to a painfully slow start in Samsung’s home market appleinsider samsung lumia handset android\nreboot mutants anthology sequels prequel‘Star Wars Episode VII’: Actors Battle for Lead Role (EXCLUSIVE) liv helmer vfx villains terminator\nlesbian primaries rowse beshear legislaturesWestern U.S. Republicans to urge appeals court to back gay marriage schuette heterosexual gubernatorial stockman lgbt\npostcold rulers aipac redrawn eilatRussian forces ‘part-seize’ Ukrainian missile units warheads lawlessness blockaded czechoslovakia ukranian\ncatholics pontiﬀ curia dignitaries papacyPope marks anniversary with prayer and a tweet xvi halal theology seminary bishops\nberkowitz moelis erbey gse ciosUncle Sam buying mortgages? Who knew? lode ocwen nationstar fairholme subsidizing\ncarbs coconut cornstarch bundt vegetarian5 Foods for Healthier Skin butter tablespoons tsp dieters salad\nbeverley bynum vogel spoelstra shootaroundDwyane Wade on pace to lead all guards in shooting dwyane nuggets westbrook bledsoe kobe\n5 iterations. Once we obtained the document vectors us-\ning the abovementioned methods, we used linear Support\nVector Machine (SVM) [5] to predict a movie genre. Note\nthat we chose a linear classiﬁer, instead of some more pow-\nerful non-linear one, in order to reduce the eﬀect of variance\nof non-linear methods on the classiﬁcation performance and\nhelp with the interpretation of the results.\nThe classiﬁcation results after 5-fold cross-validation are\nshown in Table 1, where we report results on eight binary\none-vs-rest classiﬁcation tasks for eight most frequent movie\ngenres in the data set. We can see that the neural language\nmodels on average obtained higher accuracy than LDA, al-\nthough LDA achieved very competitive results on the last\nsix tasks. It is interesting to observe that the word2vec algo-\nrithm obtained higher accuracy than paragraph2vec despite\nthe fact that word2vec only considered sequences in which\nthe movies were seen without using the movie synopses, and\nthat, unlike word2vec, the paragraph2vec model was speciﬁ-\ncally designed for document representation. This result indi-\ncates that the users have strong genre preferences that exist\nin the movie sequences which was utilized by word2vec, val-\nidating our assumption discussed earlier. Furthermore, we\nsee that the proposed approach achieved higher accuracy\nthan the competing state-of-the-art methods, obtaining on\naverage 5.62% better performance over the paragraph2vec\nand 1.52% over the word2vec model. This can be explained\nby the fact that the HDV method successfully exploited both\nthe document content and the relationships in a stream be-\ntween them, resulting in improved performance.\n4.2 Large-scale document representation\nIn this section we evaluate the proposed algorithm a large-\nscale data set collected on Yahoo servers. The data con-\nsists of nearly 200 ,000 distinct news stories, viewed by a\nsubset of company’s users from March to June, 2014. Af-\nter pre-processing where the stopwords were removed, we\ntrained the HDV model on more than 80 million document\nsequences generated by users, containing a total of 100 mil-\nlion words with a vocabulary size of 161 ,000. Considering\nthat the used data set is proprietary and that numerical re-\nsults may carry business-sensitive information, we ﬁrst illus-\ntrate a wide potential of the proposed method for numerous\nTable 5: Titles of retrieved news articles for given news examples\nRussian forces ‘part-seize’ Ukrainian missile units\nRussia says cannot order Crimean ‘self-defense’ units back to base\nRussia says Ukraine “hallucinating” in warning of nuclear risks\nLavrov snubs Ukrainian but says talks will continue\nKiev must have say in Crimea’s future: US\nCrisis in Ukraine: After day of Paris talks, a dramatic change in tone\nGalaxy S5 will get oﬀ to a slow start in Samsung’s home market\nNew specs revealed for one of 2014’s most intriguing Android phones\nLG G Pro 2 review: the evolutionary process\n[video] HTC wins smartphone of the year\nSamsung apparently still has a major role in Apple’s iPhone 6\nSamsung’s new launch, the Galaxy S5, lacks innovative features\nThis year’s best buy ISAs\nSavings rates ’could rise’ as NS&I launch new products\nHow to use an Isa to invest in property\nPensions: now you can have a blank canvas - not an annuity\nEd Balls’ Budget Response Long on Jokes, a Bit Short on Analysis\nHalf a million borrowers to be repaid interest and charges\nWestern U.S. Republicans to urge appeals court to back gay marriage\nKy. to use outside counsel in gay-marriage case\nDisputed study’s author testiﬁes on gay marriage\nTexas’ Primary Color Battle Begins\nEyes on GOP as Texas holds nation’s 1st primary\nMichigan stumbles in court defending same-sex marriage ban\nonline applications using qualitative experiments, followed\nby the document classiﬁcation task where we show relative\nperformance improvements over the baseline method.\n4.2.1 Keyword suggestion\nAn important task in many online applications is ﬁnding\nsimilar or related words given an input word as a query.\nThis is useful in the setting of, for example, search retar-\ngeting [8], where advertisers bid on search keywords related\nto their product or service and may use the proposed model\nto expand the list of targeted keywords with additional rel-\nevant keywords. Table 2 shows example keywords from the\nvocabulary, together with their nearest word neighbors in\nthe embedding space. Clearly, meaningful semantic rela-\ntionships and associations can be observed within the clos-\nest distance of the input keywords. For example, for the\nquery word “batman”, the model found that other super-\nheroes such as “superman” and “avengers” are related, and\nalso found keywords related to comics in general, such as\n“comics”, “marvel”, or “sequel”.\n4.2.2 Document retrieval\nGiven a query word, one may be interested in ﬁnding the\nmost relevant documents, which is a typical task of an online\nsearch engine or news webservice perform. We consider the\nkeywords used in the previous section, and ﬁnd the titles\nof the closest document vectors in the common embedding\nspace. As can be seen in Table 3, the retrieved documents\nare semantically related to the input keyword. Interestingly,\nin some cases it might seem that the document is irrelevant,\nas, for example, in the case of keyword“university”and head-\nlines “Spring storm brings blizzard warning for Cape Cod”\nand “No Friday Night Lights at $60 Million Texas Stadium”.\nHowever, after closer inspection and a search for the head-\nlines in a popular search engine, we can see that the snow\nstorm from the ﬁrst headline aﬀected school operations and\nthe article includes a comment by an aﬀected student. Sim-\nilar search also conﬁrmed that the second article discussed\nschool facilities and an education fund. Although the titles\nmay be misleading, we can see that the both articles are of\ninterest to users interested in the keyword “university”, as\nour model correctly learned from the actual user sessions.\nWe note that the proposed approach diﬀers from the tra-\nditional information retrieval methods due to the fact that\nretrieved documents do not necessarily need to contain the\nquery word, as exampliﬁed in Table 3 in a case of the key-\nword “boxing”. As we can see, the HDV method found\nthat the articles discussing UFC (Ultimate Fighting Cham-\npionship) and WSOF (World Series of Fighting) events are\nrelated to the sport, despite the fact that neither of them\nspeciﬁcally mentions the word “boxing”.\n4.2.3 Document tag recommendation\nAn important task in online news services is automatic\ndocument tagging, where, given a news article, we assign a\nlist of relevant keywords (calledtags) that explain the article\ncontent. Tagging is very useful in improving the document\nretrieval systems, document summarization, document rec-\nommendation, contextual advertising (tags can be used to\nmatch display ads shown alongside the article), and many\nother applications. Our model is suitable for such tasks due\nto the fact that document and word vectors reside in the\nsame feature space, which allows us to reduce complex task\nof document tagging to a trivial K-nearest-neighbor search\nin the joint embedding space.\nUsing the trained model, we retrieved the nearest words\ngiven a news story as an input. In Table 4 we give the results,\nwhere titles of the example news stories are shown together\nwith their lists of nearest words. We can see that the re-\ntrieved keywords often summarize and further explain the\ndocuments. For example, for the news article “This year’s\nbest buy ISAs”, related to Individual Savings Account (ISA),\nthe keywords include“pensioners”and“taxfree”, while in the\nmortgage-related example (“Uncle Sam buying mortgages?\nWho knew?”) keywords include several ﬁnancial companies\nand advisors (e.g., Nationstar, Moelis, Berkowitz).\nTable 6: Relative improvement over LDA\nAlgorithm Avg. accuracy improvement\nLDA 0.00%\nparagraph2vec 0.27%\nword2vec 2.26%\nHDV 4.39%\n4.2.4 Document recommendation\nIn the document recommendation task, we search for the\nnearest news articles given a news story. The retrieved arti-\ncles can be provided as a reading recommendations for users\nviewing the query news story. We give the examples in\nTable 5, where we can see that relevant and semantically\nrelated documents are located nearby in the latent vector\nspace. For example, we can see that the nearest neighbors\nfor article related to the 2014 Ukraine crisis are other news\nstories discussing the political situation in Ukraine, while\nfor the article focusing on Samsung Galaxy S5 all nearest\ndocuments are related to the smartphone industry.\n4.2.5 News topic classiﬁcation\nLastly, we used the learned representations to label news\ndocuments with 19 ﬁrst-level topic tags from the company’s\ninternal interest taxonomy (e.g., taxonomy includes“home &\ngarden” and “science” tags). We used linear SVM to predict\neach topic separately, and the average improvement over\nLDA after 5-fold cross-validation is given in Table 6. We see\nthat the proposed method outperformed the competition on\nthis large-scale problem, strongly conﬁrming the beneﬁts of\nHDV for representation of streaming documents.\n5. CONCLUSION\nWe described a general unsupervised learning framework\nto model the latent structure of streaming documents, that\nlearns low-dimensional vectors to represent documents and\nwords in the same latent space. Our model exploits the con-\ntext of documents in streams and learns representations that\ncan capture temporal co-occurrences of documents and sta-\ntistical patterns of the words contained within. The method\nwas veriﬁed on a movie classiﬁcation task, outperforming the\ncurrent state-of-the-art by a large margin. Moreover, exper-\niments on a large-scale data set comprising click-through\nlogs on Yahoo News demonstrated eﬀectiveness and wide\napplicability of the proposed neural language approach.\n6. REFERENCES\n[1] R. Baeza-Yates, D. Jiang, F. Silvestri, and\nB. Harrison. Predicting the next app that you are\ngoing to use. In Proceedings of the 8th ACM\ninternational conference on Web search and data\nmining. ACM, 2015.\n[2] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.\nA neural probabilistic language model. Journal of\nMachine Learning Research, 3:1137–1155, 2003.\n[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent\ndirichlet allocation. the Journal of machine Learning\nresearch, 3:993–1022, 2003.\n[4] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,\nand O. Yakhnenko. Translating embeddings for\nmodeling multi-relational data. In Advances in Neural\nInformation Processing Systems, pages 2787–2795,\n2013.\n[5] C.-C. Chang and C.-J. Lin. LIBSVM: A library for\nsupport vector machines. ACM Transactions on\nIntelligent Systems and Technology, 2:27:1–27:27,\n2011.\n[6] R. Collobert and J. Weston. A uniﬁed architecture for\nnatural language processing: Deep neural networks\nwith multitask learning. In Proceedings of the 25th\ninternational conference on Machine learning, pages\n160–167. ACM, 2008.\n[7] N. Djuric, V. Radosavljevic, M. Grbovic, and\nN. Bhamidipati. Hidden conditional random ﬁelds\nwith distributed user embeddings for ad targeting. In\nIEEE International Conference on Data Mining, 2014.\n[8] M. Grbovic and S. Vucetic. Generating ad targeting\nrules using sparse principal component analysis with\nconstraints. In International World Wide Web\nConference, pages 283–284, 2014.\n[9] M. Hoﬀman, F. R. Bach, and D. M. Blei. Online\nlearning for Latent Dirichlet Allocation. In Advances\nin Neural Information Processing Systems, pages\n856–864, 2010.\n[10] T. Hofmann. Probabilistic latent semantic indexing. In\nProceedings of the 22nd annual international ACM\nSIGIR conference on Research and development in\ninformation retrieval, pages 50–57. ACM, 1999.\n[11] R. Kiros, R. Zemel, and R. Salakhutdinov. Multimodal\nneural language models. In Proceedings of the 31th\nInternational Conference on Machine Learning, 2014.\n[12] R. Kiros, R. S. Zemel, and R. Salakhutdinov. A\nmultiplicative model for learning distributed\ntext-based attribute representations. arXiv preprint\narXiv:1406.2710, 2014.\n[13] Q. V. Le and T. Mikolov. Distributed representations\nof sentences and documents. arXiv preprint\narXiv:1405.4053, 2014.\n[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEﬃcient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781, 2013.\n[15] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. Distributed representations of words and\nphrases and their compositionality. In Advances in\nNeural Information Processing Systems, pages\n3111–3119, 2013.\n[16] A. Mnih and G. Hinton. Three new graphical models\nfor statistical language modelling. In Proceedings of\nthe 24th international conference on Machine learning,\npages 641–648. ACM, 2007.\n[17] A. Mnih and Y. W. Teh. A fast and simple algorithm\nfor training neural probabilistic language models.\narXiv preprint arXiv:1206.6426, 2012.\n[18] F. Morin and Y. Bengio. Hierarchical probabilistic\nneural network language model. In AISTATS,\nvolume 5, pages 246–252, 2005.\n[19] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk:\nOnline learning of social representations. arXiv\npreprint arXiv:1403.6652, 2014.\n[20] R. Socher, D. Chen, C. D. Manning, and A. Ng.\nReasoning with neural tensor networks for knowledge\nbase completion. In Advances in Neural Information\nProcessing Systems, pages 926–934, 2013.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8742183446884155
    },
    {
      "name": "MovieLens",
      "score": 0.6703903675079346
    },
    {
      "name": "Word (group theory)",
      "score": 0.5648220181465149
    },
    {
      "name": "Natural language processing",
      "score": 0.5607044100761414
    },
    {
      "name": "Language model",
      "score": 0.5546245574951172
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5461834073066711
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5212648510932922
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5144005417823792
    },
    {
      "name": "Vector space model",
      "score": 0.4986300468444824
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.48654094338417053
    },
    {
      "name": "Information retrieval",
      "score": 0.4747764766216278
    },
    {
      "name": "Hierarchy",
      "score": 0.434021532535553
    },
    {
      "name": "Exploit",
      "score": 0.42855262756347656
    },
    {
      "name": "Machine learning",
      "score": 0.2515351176261902
    },
    {
      "name": "Recommender system",
      "score": 0.15584200620651245
    },
    {
      "name": "Collaborative filtering",
      "score": 0.0977582037448883
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Market economy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210134091",
      "name": "Yahoo (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 74
}