{
  "title": "AI-QMIX: Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning",
  "url": "https://openalex.org/W3033100261",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2593842446",
      "name": "Shariq Iqbal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5038213642",
      "name": "Christian A. Schroeder de Witt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117403213",
      "name": "Bei Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100221316",
      "name": "Wendelin B√∂hmer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042571382",
      "name": "Shimon Whiteson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1855009544",
      "name": "Fei Sha",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951984055",
    "https://openalex.org/W2903181768",
    "https://openalex.org/W2155968351",
    "https://openalex.org/W2970129473",
    "https://openalex.org/W2141559645",
    "https://openalex.org/W2970214542",
    "https://openalex.org/W2962966033",
    "https://openalex.org/W2292533394",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2176412452",
    "https://openalex.org/W1606056663",
    "https://openalex.org/W2997536466",
    "https://openalex.org/W2139993574",
    "https://openalex.org/W3093287223",
    "https://openalex.org/W2962938178",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2970879379",
    "https://openalex.org/W3012934742"
  ],
  "abstract": "Real world multi-agent tasks often involve varying types and quantities of agents and non-agent entities. Agents frequently do not know a priori how many other agents and non-agent entities they will need to interact with in order to complete a given task, requiring agents to generalize across a combinatorial number of task configurations with each potentially requiring different strategies. In this work, we tackle the problem of multi-agent reinforcement learning (MARL) in such dynamic scenarios. We hypothesize that, while the optimal behaviors in these scenarios with varying quantities and types of agents/entities are diverse, they may share common patterns within sub-teams of agents that are combined to form team behavior. As such, we propose a method that can learn these sub-group relationships and how they can be combined, ultimately improving knowledge sharing and generalization across scenarios. This method, Attentive-Imaginative QMIX, extends QMIX for dynamic MARL in two ways: 1) an attention mechanism that enables model sharing across variable sized scenarios and 2) a training objective that improves learning across scenarios with varying combinations of agent/entity types by factoring the value function into imagined sub-scenarios. We validate our approach on both a novel grid-world task as well as a version of the StarCraft Multi-Agent Challenge minimally modified for the dynamic scenario setting. The results in these domains validate the effectiveness of the two new components in generalizing across dynamic configurations of agents and entities.",
  "full_text": "Randomized Entity-wise Factorization for\nMulti-Agent Reinforcement Learning\nShariq Iqbal1 Christian A. Schroeder de Witt2 Bei Peng2 Wendelin B¬®ohmer 3 Shimon Whiteson2 Fei Sha1 4\nAbstract\nMulti-agent settings in the real world often in-\nvolve tasks with varying types and quantities\nof agents and non-agent entities; however, com-\nmon patterns of behavior often emerge among\nthese agents/entities. Our method aims to lever-\nage these commonalities by asking the ques-\ntion: ‚ÄúWhat is the expected utility of each agent\nwhen only considering a randomly selected sub-\ngroup of its observed entities?‚Äù By posing this\ncounterfactual question, we can recognize state-\naction trajectories within sub-groups of entities\nthat we may have encountered in another task\nand use what we learned in that task to inform\nour prediction in the current one. We then re-\nconstruct a prediction of the full returns as a\ncombination of factors considering these disjoint\ngroups of entities and train this ‚Äúrandomly fac-\ntorized‚Äù value function as an auxiliary objective\nfor value-based multi-agent reinforcement learn-\ning. By doing so, our model can recognize and\nleverage similarities across tasks to improve learn-\ning efÔ¨Åciency in a multi-task setting. Our ap-\nproach, Randomized Entity-wise Factorization\nfor Imagined Learning (R E F I L), outperforms\nall strong baselines by a signiÔ¨Åcant margin in\nchallenging multi-task StarCraft micromanage-\nment settings.\n1. Introduction\nMulti-agent reinforcement learning techniques often focus\non learning in settings with Ô¨Åxed groups of agents and enti-\nties; however, many real-world multi-agent settings contain\ntasks across which an agent must deal with varying quan-\n1Department of Computer Science, University of Southern\nCalifornia 2Department of Computer Science, University of Ox-\nford 3Department of Software Technology, Delft University of\nTechnology 4Google Research. Correspondence to: Shariq Iqbal\n<shariqiq@usc.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nFigure 1: Breakaway sub-scenario in soccer. Agents in the yel-\nlow square can generalize this experience to similar subsequent\nexperiences, regardless of the state of agents outside the square.\ntities and types of cooperative agents, antagonists, or other\nentities. This variability in type and quantity of entities re-\nsults in a combinatorial growth in the number of possible\nconÔ¨Ågurations, aggravating the challenge of learning con-\ntrol policies that generalize. For example, the sport of soccer\nexists in many forms, from casual 5 vs. 5 to full scale 11 vs.\n11 matches, with varying formations within each consisting\nof different quantities of player types (defenders, midÔ¨Åeld-\ners, forwards, etc.). Within these varied tasks, however, ex-\nist common patterns. For instance, a ‚Äúbreakaway‚Äù occurs\nin soccer when an attacker with the ball passes the defense\nand only needs to beat the goalkeeper in order to score (Fig-\nure 1). The goalkeeper and attacker can apply what they\nhave learned in a breakaway to the next one, regardless of\nthe task (e.g., 5 vs. 5). If players can disentangle their under-\nstanding of common patterns from their surroundings, they\nshould be able to learn more efÔ¨Åciently as well as share\ntheir experiences across all forms of soccer. These repeated\npatterns within sub-groups of entities can, in fact, be found\nin a wide variety of multi-agent tasks (e.g., heterogeneous\nswarm control (Prorok et al., 2017) and StarCraft unit mi-\ncromanagement (Samvelyan et al., 2019)). Our work1 aims\nto develop a methodology for artiÔ¨Åcial agents to incorporate\nknowledge of these shared patterns to accelerate learning in\na multi-task setting.\nOne way to leverage structural independence among agents,\nas in our soccer example, is to represent value functions as\na combination of factors that depend on disjunct subsets of\nthe state and action spaces (Koller & Parr, 1999). These sub-\nsets are typically Ô¨Åxed in advance using domain knowledge\nand thus do not scale to complex domains where depen-\ndencies are unknown and may shift over time. Recent ap-\n1Code available at: https://github.com/shariqiqbal2810/REFIL\narXiv:2006.04222v3  [cs.LG]  11 Jun 2021\nRandomized Entity-wise Factorization for MARL\nproaches (e.g. VDN (Sunehag et al., 2018), QMIX (Rashid\net al., 2018)) in cooperative deep multi-agent reinforcement\nlearning (MARL) factor value functions into separate com-\nponents for each agent‚Äôs action and observation space in\norder to enable decentralized execution. These approaches\nlearn a utility function for each agent that depends on the\nagent‚Äôs own action and observations, resulting in a unique\nobservation space for each task and exacerbating the chal-\nlenge of learning in a multi-task setting.\nHow can we teach agents to be ‚Äúsituationally aware‚Äù of\ncommon patterns that are not pre-speciÔ¨Åed, such that they\ncan share knowledge across tasks? Our main idea is as\nfollows: Given observed trajectories in a real task, we ran-\ndomly partition entities into sub-groups to ‚Äúimagine‚Äù that\nagents only observe a (random) subset of the entities they\nactually observe. Then, in addition to estimating utility of\ntheir actions given the full observations, we use the same\nmodel to predict utilities in the imagined scenario, providing\nan opportunity to discover sub-group patterns that appear\nacross tasks. For example, we might sample a breakaway\n(or a superset of the breakaway entities) in both 5v5 soccer\nand 11v11, allowing our model to share value function fac-\ntors across tasks. We can then use these factors to construct\na prediction of the full returns.\nOf course, the possibility of sampling sub-groups that do\nnot contain independent behavior exists. Imagine a sub-\ngroup that looks like a breakaway, but in reality a defender\nis closing in on the attacker‚Äôs left. In such cases, we must\ninclude factors that account for the effect interactions out-\nside of sampled sub-groups on each agent‚Äôs utility. Cru-\ncially however, the estimated utility derived from imagining\na breakaway often provides at least some information as to\nthe agent‚Äôs utility given the full observation (i.e., the agent\nknows there is value in dribbling toward the goal). Imagined\nsub-group factors are combined with interaction factors to\nproduce an estimate of the value function that we train as\nan auxiliary objective on top of a standard value function\nloss. As such, our approach allows models to exploit shared\ninter-task patterns via factorization without losing any ex-\npressivity. We emphasize that this approach does not rely\non sampling an ‚Äúoptimal‚Äù sub-group. In other words, there\nis no requirement to sample sub-groups that are indepen-\ndent from one another (c.f. Section 4.1). In fact, it is useful\nto learn a utility function for any sub-group state that may\nappear in another task.\nOur approach: Randomized Entity-wise Factorization for\nImagined Learning (R E F I L) can be implemented easily\nin practice by using masks in attention-based models. We\nevaluate our approach on complex StarCraft Multi-Agent\nChallenge (SMAC) (Samvelyan et al., 2019) multi-task set-\ntings with varying agent teams, Ô¨Ånding R E F I Lattains\nimproved performance over state-of-the-art methods.\n2. Background and Preliminaries\nIn this work, we consider thedecentralized partially observ-\nable Markov decision process (Dec-POMDP) (Oliehoek\net al., 2016) with entities (Schroeder de Witt et al., 2019),\nwhich describes fully cooperative multi-agent tasks.\nDec-POMDPs with Entities are described as tuples:\n(S,U,O,P,r,E,A,Œ¶,¬µ). Eis the set of entities in the\nenvironment. Each entity e has a state representation se,\nand the global state is the set s = {se|e ‚ààE}‚àà S. Some\nentities can be agents a ‚ààA‚äÜE . Non-agent entities are\nparts of the environment that are not controlled by learn-\ning policies (e.g., landmarks, obstacles, agents with Ô¨Åxed\nbehavior). The state features of each entity consist of two\nparts: se = [fe,œÜe] where fe represents the description of\nan entity‚Äôs current state (e.g., position, orientation, velocity,\netc.) while œÜe ‚ààŒ¶ represents the entity‚Äôs type (e.g., out-\nÔ¨Åeld player, goalkeeper, etc.), of which there is a discrete\nset. An entity‚Äôs type affects the state dynamics as well as\nthe reward function and, importantly, it remains Ô¨Åxed for\nthe duration of the entity‚Äôs existence. Not all entities may\nbe visible to each agent, so we deÔ¨Åne a binary observability\nmask: ¬µ(sa,se) ‚àà{1,0}, where agents can always observe\nthemselves ¬µ(sa,sa) = 1,‚àÄa‚ààA. Thus, an agent‚Äôs obser-\nvation is deÔ¨Åned as oa = {se|¬µ(sa,se) = 1,e ‚ààE}‚àà O.\nEach agent acan execute actions ua, and the joint action of\nall agents is denoted u = {ua|a‚ààA}‚àà U. P is the state\ntransition function which deÔ¨Ånes the probability P(s‚Ä≤|s,u)\nand r(s,u) is the reward function that maps the global state\nand joint actions to a single scalar reward.\nEntities cannot be added during an episode, but they may be-\ncome inactive (e.g., a unit dying in StarCraft) and no longer\naffect transitions and rewards. Since s and u are sets, their\nordering does not matter, and our modeling construct should\naccount for this (e.g., by modeling with permutation invari-\nant/equivariant attention models (Lee et al., 2019)). In many\ndomains, the set of entity types present {œÜe|e‚ààE} is Ô¨Åxed.\nWe are particularly interested in a multi-task setting where\nthe quantity and types of entities are varied between tasks,\nas identifying patterns within sub-groups of entities is cru-\ncial to generalizing experience effectively in these cases.\nLearning We aim to learn a set of policies that maximize\nexpected discounted reward (returns). Q-learning is speciÔ¨Å-\ncally concerned with learning an accurate action-value func-\ntion Qtot (deÔ¨Åned below), and using this function to select\nthe actions that maximize expected returns. The optimal\nQ-function for our setting is deÔ¨Åned as:\nQtot(s,u) :=\nE\n[‚àë‚àû\nt=0Œ≥tr(st,ut)\n‚èê‚èê‚èê\ns0=s,u0=u,st+1‚àºP(¬∑|st,ut)\nut+1=arg maxQtot(st+1,¬∑)\n]\n(1)\n= r(s,u) + Œ≥E\n[\nmax Qtot(s‚Ä≤,¬∑) |s‚Ä≤‚àºP(¬∑|s,u)\n]\nRandomized Entity-wise Factorization for MARL\nPartial observability is typically handled by using the his-\ntory of actions and observations as a proxy for state, of-\nten processed by a recurrent neural network (Hausknecht\n& Stone, 2015): Qtot\nŒ∏ (œÑt,ut) ‚âàQtot(st,ut) where the tra-\njectory is œÑa\nt := (oa\n0,ua\n0,...,o a\nt) and œÑt := {œÑa\nt }a‚ààA. To\nlearn the Q-function, deep reinforcement learning uses neu-\nral networks as function approximators trained to minimize\nthe loss function:\nLQ(Œ∏) := E\n[(\nytot\nt ‚àíQtot\nŒ∏ (œÑt,ut)\n)2‚èê‚èê‚èê(œÑt,ut,rt,œÑt+1) ‚àºD\n]\nytot\nt := rt + Œ≥Qtot\n¬ØŒ∏\n(\nœÑt+1,arg maxQtot\nŒ∏ (œÑt+1, ¬∑)\n)\n(2)\nwhere ¬ØŒ∏are the parameters of a target network that is copied\nfrom Œ∏periodically to improve stability (Mnih et al., 2015)\nand Dis a replay buffer (Lin, 1992) that stores transitions\ncollected by an exploratory policy (typicallyœµ-greedy). Dou-\nble deep Q-learning (van Hasselt et al., 2016) mitigates\noverestimation of the learned values by using actions that\nmaximize Qtot\nŒ∏ as inputs for the target network Qtot\n¬ØŒ∏ .\nValue Function FactorizationCentralized training for de-\ncentralized execution (CTDE) has been a major focus in\nrecent efforts in deep multi-agent RL (Lowe et al., 2017; Fo-\nerster et al., 2018; Sunehag et al., 2018; Rashid et al., 2018;\nIqbal & Sha, 2019). Some methods achieve CTDE through\nfactoring Q-functions into monotonic combinations of per-\nagent utilities, with each depending only on a single agent‚Äôs\nhistory of actions and observations Qa(œÑa,ua). This fac-\ntorization allows agents to independently maximize their\nlocal utility functions in a decentralized manner with their\nselected actions combining to form the optimal joint action.\nWhile factored value functions can only represent a lim-\nited subset of all possible value functions (B ¬®ohmer et al.,\n2020), they tend to perform better empirically than those\nthat learn unfactored joint action value functions (Oliehoek\net al., 2008).\nQMIX (Rashid et al., 2018) improves over value decompo-\nsition networks (VDN) (Sunehag et al., 2018) by using a\nmore expressive factorization than a summation of factors:\nQtot = g\n(\nQ1(œÑ1,u1; Œ∏Q),...,Q |A|(œÑ|A|,u|A|; Œ∏Q); Œ∏g\n)\nThe parameters of the monotonic mixing function Œ∏g are\ngenerated by a hyper-network (Ha et al., 2017) condition-\ning on the global state s: Œ∏g = h(s; Œ∏h). Every state can\ntherefore have a different mixing function; however, the\nmixing functions‚Äôs monotonicity maintains decentralizabil-\nity, as agents can greedily maximize Qtot without commu-\nnication. All parameters Œ∏= {Œ∏Q,Œ∏h}are trained with the\nDQN loss of Equation 2.\nAttention Mechanisms for MARLAttention models have\nrecently generated intense interest due to their ability to in-\ncorporate information across large contexts, including in\nMARL (Jiang & Lu, 2018; Iqbal & Sha, 2019; Long et al.,\n2020). Importantly for our purposes, they can process vari-\nable sized sets of Ô¨Åxed length vectors (in our case entities).\nAt the core of these models is a parameterized transforma-\ntion known as multi-head attention (Vaswani et al., 2017)\nthat allows entities to selectively extract information from\nother entities based on their local context.\nWe deÔ¨Åne X as a matrix where each row corresponds to\nthe state representation (or its transformation) of an en-\ntity. The global state s is represented in matrix form as\nXE where Xe,‚àó = se. Our models consist of entity-wise\nfeedforward layers eFF(X), which apply an identical lin-\near transformation to all input entities and multi-head atten-\ntion layers MHA (A,X,M), which integrate information\nacross entities. The latter take three arguments: the set of\nagents Afor which to compute an output vector, the ma-\ntrix X‚ààR|E|√ód where dis the dimensionality of the input\nrepresentations, and a mask M ‚ààR|A|√ó|E|. The layer out-\nputs a matrix H ‚ààR|A|√óh where his the hidden dimen-\nsion of the layer. The row Ha,‚àócorresponds to a weighted\nsum of linearly transformed representations from all entities\nselected by agent a. Importantly, if the entry of the mask\nMa,e = 0, then entity e‚Äôs representation is not included in\nHa,‚àó. Masking enables 1) decentralized execution by pro-\nviding the mask M¬µ\na,e = ¬µ(sa,se), such that agents can\nonly see entities observable by them in the environment,\nand 2) ‚Äúimagination‚Äù of the returns among sub-groups of\nentities. We integrate entity-wise feedforward layers and\nmulti-head attention into QMIX in order to share a model\nacross tasks where the number of agents and entities is vari-\nable and build our approach from there. The exact process\nof computing attention layers, as well as the speciÔ¨Åcs of\nour attention-augmented version of QMIX are described in\ndetail in the Supplement.\n3. R E F I L\nWe now proposeRandomized Entity-wise Factorization for\nImagined Learning (R E F I L). We observe that common\npatterns often emerge in sub-groups of entities within com-\nplex multi-agent tasks (cf. soccer breakaway example in\n¬ß1) and hypothesize that learning to predict agents‚Äô utilities\nwithin sub-groups of entities is a strong inductive bias that\nallows models to share information more freely across tasks.\nWe instantiate our approach by constructing an estimate of\nthe value function from factors based on randomized sub-\ngroups, sharing parameters with the full value function, and\ntraining this factorized version of the value function as an\nauxiliary objective.\n3.1. Main Idea\nRandom PartitioningGiven an episode sampled from a re-\nplay buffer, we Ô¨Årst randomly partition all entities in Einto\ntwo disjunct groups, held Ô¨Åxed for the episode. We denote\nRandomized Entity-wise Factorization for MARL\neFF\nFF\nMHA\nGRU\nFF\nùúñ-greedy\nAgent Utility Network\neFF\neFF\nMHA\nHypernetwork\nMixing\nNetwork\nHypernet\nSoftmax\nHypernet\nSoftmax\nHypernet\n Hypernet\nRest of imagined mixing \nnet is identical\nRandomized Entity-wise Factorization\nFigure 2: Schematic for R E F I L. Values colored orange or blue are used for computingQtot and Qtot\naux respectively. (left) Agent-speciÔ¨Åc\nutility networks. These are decentralizable due to the use of an observability mask ( M¬µ). We include Gated Recurrent Units (Chung\net al., 2014) to retain information across timesteps in order to handle partial observability. (top center)Hypernetworks used to generate\nweights for the mixing network. We use a softmax function on the weights across the hidden dimension to enforce non-negativity, which\nwe Ô¨Ånd empirically to be more stable than the standard absolute value function. Hypernetworks are not restricted by partial observability\nsince they are only required during training and not execution. (top right)The mixing network used to calculate Qtot. (bottom right)\nProcedure for performing randomized entity-wise factorization. For masks MI and MO, colored spaces indicate a value of 1 (i.e., the\nagent designated by the row will be able to see the entity designated by the column), while white spaces indicate a value of 0. The color\nindicates which group the entity belongs to, so agents in the red group see red entities in MI and blue entities in MO. Agents are split\ninto sub-groups and their utilities are calculated for both interactions within their group, as well as to account for the interactions outside\nof their group, then monotonically mixed to predict Qtot\naux.\nthe partition by a random binary2 vector m‚àà{0,1}|E|. me\nindicates whether entity eis in the Ô¨Årst group. The negation\n¬¨me represents whether eis in the second group. The sub-\nset of all agents is denoted mA := [ ma]a‚ààA. With these\nvectors, we construct binary attention masksMI and MO:\nMI := mAm‚ä§‚à®¬¨mA¬¨m‚ä§,MO := ¬¨MI. (3)\nwhere M¬µ\nI [a,e] indicates whether agent aand entity eare\nin the same group, and M¬µ\nO[a,e] indicates the opposite.\nThey are further combined with a partial observability mask\nM¬µ, which is provided by the environment, to generate the\nÔ¨Ånal attention masks\nM¬µ\nI := M¬µ ‚àßMI ,M¬µ\nO := M¬µ ‚àßMO (4)\nThese matrices are of size |A|√ó|E| and will be used by the\nmulti-head attention layers to constrain which entities can\nbe observed by agents.\nCounterfactual Reasoning Given an imagined partition\nm, an agent acan examine its history of observations and\nactions and reasoncounterfactually what its utility would be\nhad it solely observed the entities in its group. We call this\nquantity in-group utility and denote it by Qa\nI(œÑa\nI,ua; Œ∏Q).\nIn order to account for the potential interactions with enti-\nties outside of the agents group, we calculate an out-group\n2We Ô¨Årst draw p‚àà (0,1) uniformly, followed by |E| indepen-\ndent draws from a Bernoulli(p) distribution. Partitioning into two\ngroups induces a uniform distribution over all possible sub-groups.\nutility: Qa\nO(œÑa\nO,ua; Œ∏Q). Note that the real and imagined\nutilities share the same parameters Œ∏Q, allowing us to lever-\nage imagined experience to improve utility prediction in\nreal scenarios and vice versa. Breaking the fully observed\nutilities Qa into these randomized sub-group factors is akin\nto breaking an image into cut-outs of the comprising enti-\nties. While the ‚Äúimages‚Äù (i.e. states) from each task are a\nunique set, it‚Äôs likely that the pieces comprising them share\nsimilarities.\nSince we do not know the returns within the imagined sub-\ngroups, we must ground our predictions in the observed\nreturns. Just as QMIX learns a value function withnfactors\n(Qa for each agent), we learn an imagined value function\nwith 2nfactors (Qa\nI and Qa\nO for each agent) that estimates\nthe same value:\nQtot = g\n(\nQ1,...,Q |A|; h(s; Œ∏h,M)\n)\n‚âàQtot\naux = g\n(\nQ1\nI,...,Q |A|\nI ,Q1\nO,...,Q |A|\nO ;\nh(s; Œ∏h,MI),h(s; Œ∏h,MO)\n)\n(5)\nWhere g(¬∑) are mixing networks whose parameters are gen-\nerated by hypernetworks h(s; Œ∏h,M). This network‚Äôs Ô¨Årst\nlayer typically takes n inputs, one for each agent. Since\nwe have 2nfactors, we simply concatenate two generated\nversions of the input layer (using MI and MO). We then\napply the network to the concatenated utilities Qa\nI(œÑa\nI,ua)\nand Qa\nO(œÑa\nO,ua) of all agents a, to compute the predicted\nvalue Qtot\naux. This procedure is visualized in Figure 2 and\ndescribed in more detail in the Supplement.\nRandomized Entity-wise Factorization for MARL\nRandom \nInitialization Goal\n(a) Game Visualization\n0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00\nSteps √ó106\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin Rate Name\nREFIL\nQMIX (Attention)\nREFIL (Fixed Oracle)\nREFIL (Randomized Oracle) (b) Win Rate over Time\nFigure 3: Group Matching Game. We use the valuesna = 8, nc = 6, and ng = 2in our experiments. Shaded region is a 95% conÔ¨Ådence\ninterval across 24 runs.\nImportantly, since the mixing network is generated by the\nfull state context, our model can weight factors contextually.\nFor example, if the agenta‚Äôs sampled sub-group contains all\nrelevant information to compute its utility such that Qa\nI ‚âà\nQa, then the mixing networks can weight Qa\nI more heavily\nthan Qa\nO. Otherwise, the networks learn to balance Qa\nI and\nQa\nO for each agent, in order to estimateQtot. In this way, we\ncan share knowledge in similar sub-group states across tasks\nwhile accounting for the differences in utility that result\nfrom the out-of-group context.\nLearning We now describe the overall learning objective of\nR E F I L. To enforce Equation 5, we replace Qtot in Equa-\ntion 2 with Qtot\naux, resulting a new loss Laux. We combine the\nstandard QMIX loss (Eq. 2) LQ with this auxiliary loss to\nform:\nL= (1 ‚àíŒª)LQ + ŒªEmLaux (6)\nwhere Œªcontrols the tradeoff between the two losses. Note\nthat we randomly partition in each episode, hence the ex-\npectation with respect to the partition vector m. We empha-\nsize that the sub-groups are imagined. While we compute\nQtot\naux and its related quantities, we do not use them to se-\nlect actions in Equation 2. Action selection is performed\nby each agent maximizing Qa given their local observa-\ntions. This greedy local action selection is guaranteed to\nmaximize Qtot due to the monotonic structure of the mix-\ning network (Rashid et al., 2018). Moreover, our auxiliary\nobjective is only used in training, and execution in the en-\nvironment does not use random factorization. Treating ran-\ndom factorization as an auxiliary task, rather than as a rep-\nresentational constraint, allows us to retain the expressivity\nof QMIX value functions (without sub-group factorization)\nwhile exploiting the existence of shared sub-group states\nacross tasks.\n3.2. Implementation Details\nThe model architecture is shown in Figure 2, with more\ndetails described in the supplement. ‚ÄúImagination‚Äù can be\nimplemented efÔ¨Åciently using attention masks. SpeciÔ¨Åcally\ntwo additional passes through the network are needed, with\nM¬µ\nO and M¬µ\nI as masks instead of M¬µ, per training step.\nThese additional passes can be parallelized by computing\nall necessary quantities in one batch on GPU. It is feasi-\nble to split entities into an arbitrary number i of random\nsub-groups without using more computation by sampling\nseveral disjunct vectors mi and combining them them in\nthe same way as we combine mand ¬¨min Equation 3 to\nform MI and MO. Doing so could potentially bias agents\ntowards considering patterns within smaller subsets of enti-\nties.\n4. Experimental Results\nIn our experiments, we aim to answer the following ques-\ntions: 1) Are randomized counterfactuals an efÔ¨Åcient means\nfor leveraging common patterns? 2) Does our approach im-\nprove generalization in a multi-task setting? 3) Is training as\nan auxiliary objective justiÔ¨Åed? We begin with experiments\nin a simple domain we construct such that agents‚Äô decisions\nrely only on a known subset of all entities, so we can com-\npare our approach to those that use this domain knowledge.\nThen, we move on to testing on complex StarCraft micro-\nmanagement tasks to demonstrate our method‚Äôs ability to\nscale to complex domains.\n4.1. Group Matching Game\nIn order to answer our Ô¨Årst question, we construct a group\nmatching game, pictured in Figure 3a, where each agent\nonly needs to consider a subset of other agents to act ef-\nfectively and we know that subset as ground truth (unlike\nin more complex domains such as StarCraft). Agents (of\nwhich there are na) are randomly placed in one of nc cells\nand assigned to one of ng groups (represented by the dif-\nferent colors) at the start of each episode. Each unique\ngroup assignment corresponds to a task. Agents can choose\nfrom three actions: move clockwise, stay, and move counter-\nclockwise. Their ultimate goal is to be located in the same\ncell as the rest of their group members, at which point an\nepisode ends. There is no restriction on which cell agents\nform a group in (e.g., both groups can form in the same\ncell). All agents share a reward of 2.5 when any group is\nRandomized Entity-wise Factorization for MARL\ncompleted (and an equivalent penalty for a formed group\nbreaking) as well as a penalty of -0.1 for each time step in\norder to encourage agents to solve the task as quickly as pos-\nsible. Agents‚Äô entity-state descriptions se include the cell\nthat the agent is currently occupying as well as the group\nit belongs to (both one-hot encoded), and the task is fully-\nobservable. Notably, agents can act optimally while only\nconsidering a subset of observed entities.\nGround-truth knowledge of relevant entities enables us to\ndisentangle two aspects of our approach: the use of entity-\nwise factorization in general and speciÔ¨Åcally using ran-\ndomly selected factors. We would like to answer the ques-\ntion: does our method rely on sampling the ‚Äúright‚Äù groups of\nentities (i.e., those with no interactions between them), or is\nthe randomness of our method a feature that promotes gener-\nalization? We construct two approaches that use this knowl-\nedge to build factoring masks MI and MO that are used\nin place of randomly sampled groups (otherwise the meth-\nods are identical to R E F I L). R E F I L(Fixed Oracle) di-\nrectly uses the ground truth group assignments (different for\neach task) to build masks. R E F I L(Randomized Oracle)\nrandomly samples sub-groups from the ground truth groups\nonly, rather than from all possible entities. We additionally\ntrain R E F I Land QMIX (Attention) (i.e., R E F I Lwith\nno auxiliary loss).\nFigure 3b shows that using domain knowledge does not sig-\nniÔ¨Åcantly improve performance in this domain (QMIX (At-\ntention) vs. R E F I L(Fixed Oracle)). In fact our random-\nized factorization approach outperforms the use of domain\nknowledge. The randomization in R E F I Ltherefore ap-\npears to be crucial. Our hypothesis is that randomization of\nsub-group factors enables better knowledge sharing across\ntasks. For example, the situation where two agents from the\nsame group are located in adjacent cells occurs within all\npossible group assignments. When sampling randomly, our\napproach occasionally samples these two agents alone in\ntheir own group. Even if the rest of the context in a given\nepisode has never been seen before, as long as this sub-\nscenario has been seen, the model has some indication of\nthe value associated with each action. Even when restrict-\ning the set of entities to form sub-groups with those that we\nknow can be relevant to each agent (R E F I L(Randomized\nOracle)) we Ô¨Ånd that performance does not signiÔ¨Åcantly\nimprove. These results suggest that randomized sub-group\nformation for R E F I Lis a viable strategy, and the main\nbeneÔ¨Åt of our approach is to promote generalization across\ntasks by breaking value function predictions into reusable\ncomponents, even when the sampled sub-groups are not\ncompletely independent.\n4.2. STA RC R A F T\nWe next test on the StarCraft multi-agent challenge (SMAC)\n(Samvelyan et al., 2019). The tasks in SMAC involve mi-\nTable 1: Comparison of tested methods.\nName Imagined Model Base\nLearning Algorithm\nR E F I L ‚úì MHA1 QMIX2\nQMIX (Attention) MHA QMIX\nR E F I L(VDN) ‚úì MHA VDN 3\nVDN (Attention) MHA VDN\nQMIX (Max Pooling) Max-Pool QMIX\nQMIX (EMP) EMP 4 QMIX\nROMA (Attention) MHA ROMA 5\nQatten (Attention) MHA Qatten 6\nQTRAN (Attention) MHA QTRAN 7\nR E F I L(UPDeT) ‚úì UPDeT8 QMIX\nQMIX (UPDeT) UPDeT QMIX\n1: Vaswani et al. (2017) 2: Rashid et al. (2018) 3: Sunehag et al. (2018)\n4: Agarwal et al. (2019) 5: Wang et al. (2020a) 6: Yang et al. (2020)\n7: Son et al. (2019) 8: Hu et al. (2021)\ncromanagement of units in order to defeat a set of enemy\nunits in battle. SpeciÔ¨Åcally, we consider a multi-task setting\nwhere we train our models simultaneously on tasks with\nvariable types and quantities of agents. We hypothesize that\nour approach is especially beneÔ¨Åcial in this setting, as it\nshould encourage models to learn utilities for common pat-\nterns and generalize to more diverse settings as a result. The\ndynamic setting involves minor modiÔ¨Åcations to SMAC but\nwe change the environment as little as possible to maintain\nthe challenging nature of the tasks. In the standard version\nof SMAC, both state and action spaces depend on a Ô¨Åxed\nnumber of agents and enemies, so our modiÔ¨Åcations, dis-\ncussed in detail in the supplement, alleviate these problems.\nIn our tests we evaluate on three settings we call 3-8sz, 3-\n8csz, and 3-8MMM. 3-8sz pits symmetrical teams of be-\ntween 3 and 8 agents against each other where the agents\nare a combination of Zealots and Stalkers (inspired by the\n2s3z and 3s5z tasks in the original SMAC), resulting in 39\nunique tasks. 3-8csz pits symmetrical teams of between 0\nand 2 Colossi and 3 to 6 Stalkers/Zealots against each other\n(inspired by 1c3s5z), resulting in 66 tasks. 3-8MMM pits\nsymmetrical teams of between 0 and 2 Medics and 3 to 6\nMarines/Marauders against each other (inspired by MMM\nand MMM2, again resulting in 66 tasks).\nAblations and BaselinesWe introduce several ablations of\nour method, as well as adaptations of existing methods to\nhandle variable sized inputs. These comparisons are sum-\nmarized in Table 1.QMIX (Attention) is our method without\nthe auxiliary loss. R E F I L(VDN) is our approach using\nsummation to combine all factors as in VDN rather than\na non-linear monotonic mixing network. VDN (Attention)\ndoes not include the auxiliary loss and uses summation for\nfactor mixing. QMIX (Mean Pooling) is QMIX (Attention)\nwith attention layers replaced by mean pooling. We also\ntest max pooling but Ô¨Ånd the performance to be marginally\nworse than mean pooling. Importantly, for pooling layers\nwe add entity-wise linear transformations prior to the pool-\ning operations such that the total number of parameters is\ncomparable to attention layers.\nRandomized Entity-wise Factorization for MARL\n0.0 0.2 0.4 0.6 0.8 1.0\n√ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin Rate\nName\nREFIL\nQMIX (Attention)\nREFIL (VDN)\nVDN (Attention)\nQMIX (Mean Pooling)\n0.0 0.2 0.4 0.6 0.8 1.0\n√ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n√ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0 0.2 0.4 0.6 0.8 1.0\nSteps √ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin Rate\nName\nREFIL\nQMIX (EMP)\nROMA (Attention)\nQatten (Attention)\nQTRAN (Attention)\nREFIL (UPDeT)\nQMIX (UPDeT)\n(a) 3-8sz\n0.0 0.2 0.4 0.6 0.8 1.0\nSteps √ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 (b) 3-8csz\n0.0 0.2 0.4 0.6 0.8 1.0\nSteps √ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 (c) 3-8MMM\nFigure 4: Test win rate over time on multi-task S TA R C R A F T environments. Tasks are sampled uniformly at each episode. Shaded\nregion is a 95% conÔ¨Ådence interval across 5 runs. (top row) Ablations of our method. (bottom row) Baseline methods.\nFor baselines we consider some follow-up works to\nQMIX that improve the mixing network‚Äôs expressivity:\nQTRAN (Son et al., 2019) and Qatten (Yang et al., 2020).\nWe also compare to a method that builds on QMIX by at-\ntempting to learn dynamic roles that depend on the context\neach agent observes: ROMA (Wang et al., 2020a). We addi-\ntionally consider an alternative mechanism for aggregating\ninformation across variable sets of entities, known as Entity\nMessage Passing (EMP) (Agarwal et al., 2019). We speciÔ¨Å-\ncally use the restricted communication setting where agents\ncan only communicate with agents they observe, and we set\nthe number of message passing steps to three. Finally, we\nconsider the UPDeT architecture (Hu et al., 2021), a recent\nwork that also targets the multi-task MARL setting. UPDeT\nutilizes domain knowledge of the environment to map enti-\nties to the speciÔ¨Åc actions that they correspond to. We train\nUPDeT with QMIX as well asR E F I L. For all approaches\ndesigned for the standard single-task SMAC setting, we ex-\ntend them with the same multi-head attention architecture\nthat our approach uses.\nAblation Results Our results on challenges in multi-task\nS TA R C R A F T settings can be found in Figure 4. Tasks\nare sampled uniformly at each episode, so the curves repre-\nsent average win rate across all tasks. We Ô¨Ånd thatR E F I L\noutperforms all ablations consistently in these settings.R E -\nF I L(VDN) performs much worse than our approach and\nVDN (Attention), highlighting the importance of the mixing\nnetwork handling contextual dependencies between entity\npartitions. Since the trajectory of a subset of entities can\nplay out differently based on the surrounding context, it is\nimportant for our factorization approach to recognize and\nadjust for these situations. The use of mean-pooling in place\nof attention also performs poorly, indicating that attention is\nvaluable for aggregating information from variable length\nsets of entities.\nBaseline Results We Ô¨Ånd that algorithms designed to im-\nprove on QMIX for the single task MARL setting (ROMA,\nQatten, QTRAN), naively applied to the multi-task setting,\ndo not see the same improvements. R E F I L, on the other\nhand, consistently outperforms other methods, highlighting\nthe unique challenge of learning in multi-task settings. In\nFig. 5 (top right) we investigate the performance of R E -\nF I Lcompared to the two next best methods in the 3-8sz\nsetting on a task-by-task basis. We evaluate each method on\neach task individually and rank the tasks by performance,\nplotting from left to right. We Ô¨Ånd that the performance gain\nof R E F I Lcomes from generalizing performance across\na wider range of tasks, hence the reduced rate of decay in\ntask performance from best to worst.\nThe entity aggregation method of EMP underperforms rel-\native to the MHA module that we use. UPDeT is a related\nwork that focuses on designing an architecture compatible\nwith multiple tasks and variable entities and action spaces\nby utilizing domain knowledge to map entities to their cor-\nresponding actions. Despite adding this domain knowledge,\nQMIX (UPDeT) surprisingly underperforms in 2 of 3 set-\ntings, while performing similarly toR E F I Lon 3-8MMM;\nhowever, since UPDeT is an attention-based architecture, it\nis amenable to our proposed auxiliary training scheme. We\nÔ¨Ånd applying random factorization to QMIX (UPDeT) im-\nRandomized Entity-wise Factorization for MARL\nEp 1\nt = 0 Key\nAgent Enemy\nZealot\nEp 3\nEp 2\nStalker\nColossus\nt = 5\n t = 15\nUnit Shield\nUnit Health\nIndicates an agent‚Äôs \nattack. Length \ncorresponds to time \nuntil next attack.\nTask Rank0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin Rate\nName\nREFIL\nQMIX (Attention)\nQatten (Attention)\n0.0 0.2 0.4 0.6 0.8 1.0\nSteps √ó107\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWin Rate\nName\nŒª = 0.01\nŒª = 0.1\nŒª = 0.5\nŒª = 0.9\nŒª = 0.99\nŒª = 1.0\nFigure 5: (left) SimpliÔ¨Åed rendering of a common pattern that emerges across tasks in the 3-8csz SMAC setting, highlighted at t= 15.\nR E F I Lenables learning from each task to inform behavior in the others. (top right) Task-by-task performance on 3-8sz. R E F I L\ngeneralizes better across a wider range of tasks. (bottom right) Varying Œªfor R E F I Lin 3-8sz.\nproves its performance further in 3-8MMM as well as in\n3-8sz. In the case of 3-8MMM, where the asmyptotic win\nrate of R E F I L(UPDeT) and QMIX (UPDeT) are similar,\nwe Ô¨Ånd that R E F I L(UPDeT) wins on average in 22%\nfewer time steps by targeting enemy Medivacs, a unit ca-\npable of healing its teammates. Targeting of Medivacs is\nan example of a common pattern that emerges across tasks\nwhich R E F I Lis able to leverage.\nRole of Auxiliary ObjectiveIn order to understand the\nrole of training as an auxiliary objective (rather than entirely\nreplacing the objective) we vary the value of Œªto interpo-\nlate between two modes: Œª= 0 is simply QMIX (Attention),\nwhile Œª = 1 trains exclusively with random factorization.\nOur results on 3-8sz (Figure 5 (bottom right)) show that,\nsimilar to regularization methods such as Dropout (Srivas-\ntava et al., 2014), there is a sweet spot where performance is\nmaximized before collapsing catastrophically. Training ex-\nclusively with random factorization does not learn anything\nsigniÔ¨Åcant. This failure is likely due to the fact that we use\nthe full context in our targets for learning with imagined\nscenarios as well as when executing our policies, so we still\nneed to learn with it in training.\nQualitative Example of Common PatternFinally, we vi-\nsualize an example of the sort of common patterns thatR E -\nF I Lis able to leverage (Fig. 5 (left)). Zealots (the only\nmelee unit present) are weak to Colossi, so they learn to\nhang back and let other units engage Ô¨Årst. Then, they jump\nin and intercept the enemy Zealots while all other enemy\nunits are preoccupied, leading to a common pattern of a\nZealot vs. Zealot skirmish (highlighted at t=15). R E F I L\nenables behaviors learned in these types of sub-groups to\nbe applied more effectively across all tasks. By sampling\ngroups from all entities randomly, we will occasionally end\nup with sub-groups that include only Zealots, and the value\nfunction predictions learned in these sub-groups can be ap-\nplied not only to the task at hand, but to any task where a\nsimilar pattern emerges.\n5. Related Work\nMulti-agent reinforcement learning (MARL) is a broad Ô¨Åeld\nencompassing cooperative (Foerster et al., 2018; Rashid\net al., 2018; Sunehag et al., 2018), competitive (Bansal et al.,\n2018; Lanctot et al., 2017), and mixed (Lowe et al., 2017;\nIqbal & Sha, 2019) settings. This paper focuses on coopera-\ntive MARL with centralized training and decentralized exe-\ncution (Oliehoek et al., 2016, CTDE). Our approach utilizes\nvalue function factorization, an approach aiming to simul-\ntaneously overcome limitations of both joint (Hausknecht,\n2016) and independent learning (Claus & Boutilier, 1998)\nparadigms. Early attempts at value function factorisation re-\nquire apriori knowledge of suitable per-agent team reward\ndecompositions or interaction dependencies. These include\noptimising over local compositions of individual Q-value\nfunctions learnt from individual reward functions (Schnei-\nder et al., 1999), as well as summing individualQ-functions\nwith individual rewards before greedy joint action selection\n(Russell & Zimdars, 2003). Recent approaches from coop-\nerative deep multi-agent RL learn value factorisations from\na single team reward function by treating all agents as in-\ndependent factors, requiring no domain knowledge and en-\nabling decentralized execution. Value-Decomposition Net-\nworks (VDN) (Sunehag et al., 2018) decompose the jointQ-\nvalue function into a sum of local utility functions used for\ngreedy action selection. QMIX (Rashid et al., 2018; 2020)\nextends such additive decompositions to general monotonic\nfunctions. Some works extend QMIX to improve the ex-\npressivity of mixing functions (Son et al., 2019; Yang et al.,\n2020), learn latent embeddings to help exploration (Maha-\njan et al., 2019) or learn dynamic roles (Wang et al., 2020a),\nand encode knowledge of action semantics into network\narchitectures (Wang et al., 2020b).\nSeveral recent works have addressed the topic of general-\nization and transfer across related tasks with varying agent\nquantities, though the learning paradigms considered and\nassumptions made differ from our approach. Carion et al.\n(2019) devise an approach for assigning agents to tasks, as-\nsuming the existence of low-level controllers to carry out the\nRandomized Entity-wise Factorization for MARL\ntasks, and show that it can scale to much larger tasks than\nthose seen in training. Burden (2020) propose a transfer\nlearning approach using convolutional neural networks and\ngrid-based state representations to scale to tasks of arbitrary\nsize. Wang et al. (2021) introduce a method to decompose\naction spaces into roles, which they show can transfer to\ntasks with larger numbers of agents by grouping new actions\ninto existing clusters. They do not propose a model to han-\ndle the larger observation sizes, instead using a euclidean\ndistance heuristic to observe a Ô¨Åxed number of agents. Sev-\neral approaches devise attention or graph-neural-network\nbased models for handling variable sized inputs and focus\non learning curricula to progress on increasingly large/chal-\nlenging settings (Long et al., 2020; Baker et al., 2019; Wang\net al., 2020c; Agarwal et al., 2019). Most recently, Hu et al.\n(2021) introduce a method for handling variable-size inputs\nand action spaces and evaluate their model on single-task\nto single-task transfer. In contrast to these curriculum and\ntransfer learning approaches, we focus on training simulta-\nneously on multiple tasks and speciÔ¨Åcally develop a training\nparadigm for improving knowledge sharing across tasks.\n6. Conclusion\nIn this paper we considered a multi-task MARL setting\nwhere we aim to learn control policies for variable-sized\nteams of agents. We proposed R E F I L, an approach that\nregularizes value functions to share factors comprised of\nsub-groups of entities, in turn promoting generalization and\nknowledge transfer within and across complex cooperative\nmulti-agent tasks. Our results showed that our contributions\nyield signiÔ¨Åcant average performance improvements across\nthese tasks when training on them concurrently, speciÔ¨Åcally\nthrough improving generalization across a wider variety of\ntasks.\nAcknowledgements\nThis work is partially supported by NSF Awards\nIIS-1513966/ 1632803/1833137, CCF-1139148, DARPA\nAward#: FA8750-18-2-0117, FA8750-19-1-0504, DARPA-\nD3M - Award UCB-00009528, Google Research Awards,\ngifts from Facebook and NetÔ¨Çix, and ARO# W911NF-12-\n1-0241 and W911NF-15-1-0484. This project has also re-\nceived funding from the European Research Council under\nthe European Union‚Äôs Horizon 2020 research and innova-\ntion programme (grant agreement number 637713). The ex-\nperiments were made possible by a generous equipment\ngrant from NVIDIA.\nWe thank Greg Farquhar for developing the initial version of\nthe StarCraft environment with dynamic team compositions.\nWe also thank Luisa Zintgraf, Natasha Jaques, and Robby\nCostales for their helpful feedback on the draft.\nReferences\nAgarwal, A., Kumar, S., and Sycara, K. Learning transfer-\nable cooperative behavior in multi-agent teams. arXiv\npreprint arXiv:1906.01202, 2019.\nBaker, B., Kanitscheider, I., Markov, T., Wu, Y ., Powell, G.,\nMcGrew, B., and Mordatch, I. Emergent tool use from\nmulti-agent autocurricula. In International Conference\non Learning Representations, 2019.\nBansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mor-\ndatch, I. Emergent complexity via multi-agent competi-\ntion. In International Conference on Learning Represen-\ntations, 2018.\nB¬®ohmer, W., Kurin, V ., and Whiteson, S. Deep coordination\ngraphs. In Proceedings of Machine Learning and Systems\n(ICML), pp. 2611‚Äì2622, 2020.\nBurden, N. Deep Multi-Agent Reinforcement Learning in\nStarcraft II. Master‚Äôs thesis, University of Oxford, 2020.\nCarion, N., Usunier, N., Synnaeve, G., and Lazaric, A. A\nstructured prediction approach for generalization in co-\noperative multi-agent reinforcement learning. In Ad-\nvances in Neural Information Processing Systems , pp.\n8128‚Äì8138, 2019.\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y . Empiri-\ncal evaluation of gated recurrent neural networks on se-\nquence modeling. In NIPS 2014 Workshop on Deep\nLearning, December 2014, 2014.\nClaus, C. and Boutilier, C. The dynamics of reinforcement\nlearning in cooperative multiagent systems. AAAI/IAAI,\n1998(746-752):2, 1998.\nClevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast and\naccurate deep network learning by exponential linear\nunits (elus). arXiv preprint arXiv:1511.07289, 2015.\nFoerster, J., Farquhar, G., Afouras, T., Nardelli, N., and\nWhiteson, S. Counterfactual multi-agent policy gradients.\nIn AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018.\nHa, D., Dai, A. M., and Le, Q. V . Hypernetworks. InInter-\nnational Conference on Learning Representations, 2017.\nHausknecht, M. J. Cooperation and Communication in Mul-\ntiagent Deep Reinforcement Learning. PhD thesis, The\nUniversity of Texas at Austin, 2016.\nHausknecht, M. J. and Stone, P. Deep recurrent q-learning\nfor partially observable mdps. In 2015 AAAI Fall Sym-\nposia, pp. 29‚Äì37, 2015.\nRandomized Entity-wise Factorization for MARL\nHu, S., Zhu, F., Chang, X., and Liang, X. {UPD}et: Uni-\nversal multi-agent {rl}via policy decoupling with trans-\nformers. In International Conference on Learning Rep-\nresentations, 2021.\nIqbal, S. and Sha, F. Actor-attention-critic for multi-agent\nreinforcement learning. In Chaudhuri, K. and Salakhutdi-\nnov, R. (eds.),Proceedings of the 36th International Con-\nference on Machine Learning, volume 97 of Proceedings\nof Machine Learning Research , pp. 2961‚Äì2970, Long\nBeach, California, USA, 09‚Äì15 Jun 2019. PMLR.\nJiang, J. and Lu, Z. Learning attentional communication\nfor multi-agent cooperation. In Bengio, S., Wallach, H.,\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., and Gar-\nnett, R. (eds.), Advances in Neural Information Process-\ning Systems, volume 31, pp. 7254‚Äì7264. Curran Asso-\nciates, Inc., 2018.\nKoller, D. and Parr, R. Computing factored value functions\nfor policies in structured mdps. In IJCAI, volume 99, pp.\n1332‚Äì1339, 1999.\nLanctot, M., Zambaldi, V ., Gruslys, A., Lazaridou, A., Tuyls,\nK., Perolat, J., Silver, D., and Graepel, T. A uniÔ¨Åed Game-\nTheoretic approach to multiagent reinforcement learning.\nIn Guyon, I., Luxburg, U. V ., Bengio, S., Wallach, H.,\nFergus, R., Vishwanathan, S., and Garnett, R. (eds.), Ad-\nvances in Neural Information Processing Systems 30, pp.\n4193‚Äì4206. Curran Associates, Inc., 2017.\nLee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and Teh,\nY . W. Set transformer: A framework for attention-based\npermutation-invariant neural networks. In Chaudhuri, K.\nand Salakhutdinov, R. (eds.), Proceedings of the 36th In-\nternational Conference on Machine Learning, volume 97\nof Proceedings of Machine Learning Research, pp. 3744‚Äì\n3753, Long Beach, California, USA, 09‚Äì15 Jun 2019.\nPMLR.\nLin, L.-J. Self-improving reactive agents based on reinforce-\nment learning, planning and teaching. Machine Learning,\n8(3):293‚Äì321, 1992.\nLong, Q., Zhou, Z., Gupta, A., Fang, F., Wu, Y ., and Wang,\nX. Evolutionary population curriculum for scaling multi-\nagent reinforcement learning. In International Confer-\nence on Learning Representations, 2020.\nLowe, R., Wu, Y ., Tamar, A., Harb, J., Abbeel, O. P.,\nand Mordatch, I. Multi-agent actor-critic for mixed\ncooperative-competitive environments. In Advances in\nNeural Information Processing Systems, pp. 6382‚Äì6393,\n2017.\nMahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S.\nMaven: Multi-agent variational exploration. In Advances\nin Neural Information Processing Systems , pp. 7613‚Äì\n7624, 2019.\nMnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529, 2015.\nOliehoek, F. A., Spaan, M. T., Vlassis, N., and Whiteson, S.\nExploiting locality of interaction in factored dec-pomdps.\nIn Int. Joint Conf. on Autonomous Agents and Multi-\nAgent Systems, pp. 517‚Äì524, 2008.\nOliehoek, F. A., Amato, C., et al. A concise introduction to\ndecentralized POMDPs, volume 1. Springer, 2016.\nProrok, A., Hsieh, M. A., and Kumar, V . The impact of\ndiversity on optimal control policies for heterogeneous\nrobot swarms. IEEE Transactions on Robotics , 33(2):\n346‚Äì358, 2017.\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Fo-\nerster, J., and Whiteson, S. QMIX: Monotonic value\nfunction factorisation for deep multi-agent reinforcement\nlearning. In Proceedings of the 35th International Con-\nference on Machine Learning, volume 80 of Proceedings\nof Machine Learning Research , pp. 4295‚Äì4304, Stock-\nholmsm¬®assan, Stockholm Sweden, 10‚Äì15 Jul 2018.\nRashid, T., Samvelyan, M., de Witt, C. S., Farquhar, G., Fo-\nerster, J., and Whiteson, S. Monotonic value function\nfactorisation for deep multi-agent reinforcement learning.\nJournal of Machine Learning Research , 21(178):1‚Äì51,\n2020.\nRussell, S. and Zimdars, A. L. Q-decomposition for re-\ninforcement learning agents. In Proceedings of the\nTwentieth International Conference on International Con-\nference on Machine Learning , ICML‚Äô03, pp. 656‚Äì663,\nWashington, DC, USA, August 2003. AAAI Press. ISBN\n978-1-57735-189-4.\nSamvelyan, M., Rashid, T., Schroeder de Witt, C., Farquhar,\nG., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H.,\nFoerster, J., and Whiteson, S. The starcraft multi-agent\nchallenge. In Proceedings of the 18th International\nConference on Autonomous Agents and MultiAgent Sys-\ntems, pp. 2186‚Äì2188. International Foundation for Au-\ntonomous Agents and Multiagent Systems, 2019.\nSchneider, J., Wong, W.-K., Moore, A., and Riedmiller, M.\nDistributed Value Functions. In In Proceedings of the\nSixteenth International Conference on Machine Learning,\npp. 371‚Äì378. Morgan Kaufmann, 1999.\nRandomized Entity-wise Factorization for MARL\nSchroeder de Witt, C., Foerster, J., Farquhar, G., Torr, P.,\nBoehmer, W., and Whiteson, S. Multi-agent common\nknowledge reinforcement learning. In Advances in Neu-\nral Information Processing Systems 32, pp. 9927‚Äì9939.\nCurran Associates, Inc., 2019.\nSon, K., Kim, D., Kang, W. J., Hostallero, D. E., and Yi, Y .\nQtran: Learning to factorize with transformation for co-\noperative multi-agent reinforcement learning. In Interna-\ntional Conference on Machine Learning, pp. 5887‚Äì5896,\n2019.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and\nSalakhutdinov, R. Dropout: a simple way to prevent neu-\nral networks from overÔ¨Åtting. The journal of machine\nlearning research, 15(1):1929‚Äì1958, 2014.\nSunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zam-\nbaldi, V ., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo,\nJ. Z., Tuyls, K., and Graepel, T. Value-decomposition net-\nworks for cooperative multi-agent learning based on team\nreward. In Proceedings of the 17th International Con-\nference on Autonomous Agents and MultiAgent Systems,\nAAMAS ‚Äô18, pp. 2085‚Äì2087, Richland, SC, 2018. Inter-\nnational Foundation for Autonomous Agents and Multia-\ngent Systems.\nTieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide\nthe gradient by a running average of its recent magnitude.\nCOURSERA: Neural networks for machine learning , 4\n(2):26‚Äì31, 2012.\nvan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In Proceedings of\nthe 13th AAAI Conference on ArtiÔ¨Åcial Intelligence , pp.\n2094‚Äì2100, 2016.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems, pp. 6000‚Äì6010, 2017.\nWang, T., Dong, H., Lesser, V ., and Zhang, C. Roma: Multi-\nagent reinforcement learning with emergent roles. In\nProceedings of the 37th International Conference on Ma-\nchine Learning, 2020a.\nWang, T., Gupta, T., Mahajan, A., Peng, B., Whiteson, S.,\nand Zhang, C. {RODE}: Learning roles to decompose\nmulti-agent tasks. In International Conference on Learn-\ning Representations, 2021.\nWang, W., Yang, T., Liu, Y ., Hao, J., Hao, X., Hu, Y ., Chen,\nY ., Fan, C., and Gao, Y . Action semantics network: Con-\nsidering the effects of actions in multiagent systems. In\nInternational Conference on Learning Representations ,\n2020b.\nWang, W., Yang, T., Liu, Y ., Hao, J., Hao, X., Hu, Y ., Chen,\nY ., Fan, C., and Gao, Y . From few to more: Large-scale\ndynamic multiagent curriculum learning. In AAAI Con-\nference on ArtiÔ¨Åcial Intelligence, 2020c.\nYang, Y ., Hao, J., Liao, B., Shao, K., Chen, G., Liu, W.,\nand Tang, H. Qatten: A general framework for cooper-\native multiagent reinforcement learning. arXiv preprint\narXiv:2002.03939, 2020.\nRandomized Entity-wise Factorization for MARL\nA. Attention Layers and Models\nAttention models have recently generated intense interest due to their ability to incorporate information across large contexts.\nImportantly for our purposes, they are able to process variable sized sets of inputs.\nWe now formally deÔ¨Åne the building blocks of our attention models. Given the inputX, a matrix where the rows correspond\nto entities, we deÔ¨Åne an entity-wise feedforward layer as a standard fully connected layer that operates independently and\nidentically over entities:\neFF(X; W,b) = XW + b‚ä§,X‚ààRnx√ód,W ‚ààRd√óh,b‚ààRh (7)\nNow, we specify the operation that deÔ¨Ånes an attention head, given the additional inputs of S‚äÜ Z[1,nx], a set of indices\nthat selects which rows of the input Xare used to compute queries such that XS,‚àó‚ààR|S|√ód, and M, a binary obserability\nmask specifying which entities each query entity can observe (i.e. Mi,j = 1 when i‚ààS can incorporate information from\nj ‚ààZ[1,nx] into its local context):\nAtten(S,X,M; WQ,WK,WV) = softmax\n(\nmask\n(QK‚ä§\n‚àö\nh\n,M\n))\nV ‚ààR|S|√óh (8)\nQ= XS,‚àóWQ,K= XWK,V = XWV , M ‚àà{0,1}|S|√ónx\n,WQ,WK,WV ‚ààRd√óh (9)\nThe mask(Y,M) operation takes two equal sized matrices and Ô¨Ålls the entries of Y with ‚àí‚àûin the indices where Mis\nequal to 0. After the softmax, these entries become zero, thus preventing the attention mechanism from attending to speciÔ¨Åc\nentities. This masking procedure is used in our case to uphold partial observability, as well as to enable ‚Äúimagining‚Äù the\nutility of actions within sub-groups of entities. Only one attention layer is permitted in the decentralized execution setting;\notherwise information from unseen agents can be propagated through agents that are seen. WQ, WK, and WV are all\nlearnable parameters of this layer. Queries, Q, can be thought of as vectors specifying the type of information that an entity\nwould like to select from others, while keys, K, can be thought of as specifying the type of information that an entity\npossesses, and Ô¨Ånally, values, V, hold the information that is actually shared with other entities.\nWe deÔ¨Åne multi-head-attention as the parallel computation of attention heads as such:\nMHA (S,X,M) = concat\n(\nAtten\n(\nS,X,M; WQ\nj ,WK\nj ,WV\nj\n)\n,j ‚àà\n(\n1 ...n h))\n(10)\nThe size of the parameters of an attention layer does not depend on the number of input entities. Furthermore, we receive\nan output vector for each query vector.\nB. Augmenting QMIX with Attention\nThe standard QMIX algorithm relies on a Ô¨Åxed number of entities in three places: inputs of the agent-speciÔ¨Åc utility\nfunctions Qa, inputs of the hypernetwork, and the number of utilities entering the mixing network, which must correspond\nthe output of the hypernetwork since it generates the parameters of the mixing network. QMIX uses multi-layer perceptrons\nfor which all these quantities have to be of Ô¨Åxed size. In order to adapt QMIX to the variable agent quantity setting, such\nthat we can apply a single model across all tasks, we require components that accept variable sized sets of entities as inputs.\nBy utilizing attention mechanisms, we can design components that are no longer dependent on a Ô¨Åxed number of entities\ntaken as input. We deÔ¨Åne the following inputs: XE\nei := se\ni,1 ‚â§i‚â§d,e ‚ààE; M¬µ\nae := ¬µ(sa,se),a ‚ààA,e ‚ààE. The matrix\nXEis the global state s reshaped into a matrix with a row for each entity, and M¬µ is a binary observability matrix which\nenables decentralized execution, determining which entities are visible to each agent.\nB.1. Utility Networks\nWhile the standard agent utility functions map a Ô¨Çat observation, whose size depends on the number of entities in the\nenvironment, to a utility for each action, our attention-utility functions can take in a variable sized set of entities and return a\nutility for each action. The attention layer output for agent ais computed as MHA ({a},X,M¬µ), where Xis an row-wise\ntransformation of XE (e.g., an entity-wise feedforward layer). If agents share parameters, the layer can be computed in\nparallel for all agents by providing Ainstead of {a}, which we do in practice.\nRandomized Entity-wise Factorization for MARL\nB.2. Generating Dynamic Sized Mixing Networks\nAnother challenge in devising a QMIX algorithm for variable agent quantities is to adapt the hypernetworks that generate\nweights for the mixing network. Since the mixing network takes in utilities from each agent, we must generate feedforward\nmixing network parameters that change in size depending on the number of agents present, while incorporating global state\ninformation. Conveniently, the number of output vectors of a MHA layer depends on the cardinality of input setSand we\ncan therefore generate mixing parameters of the correct size by usingS= Aand concatenating the vectors to form a matrix\nwith one dimension size depending on the number of agents and the other depending on the number of hidden dimensions.\nAttention-based QMIX (QMIX (Attention)) trains these models using the standard DQN loss in Equation 2 of the main text.\nOur two layer mixing network requires the following parameters to be generated: W1 ‚ààR+(|A|√óhm), b1 ‚ààRhm\n, w2 ‚àà\nR+(hm), b2 ‚ààR, where hm is the hidden dimension of the mixing network and |A|is the set of agents.\nNote from Eq. (8) that the output size of the layer is dependent on the size of the query set. As such, using attention layers,\nwe can generate a matrix of size |A|√óhm, by specifying the set of agents, A, as the set of queries Sfrom Eq. (8). We do\nnot need observability masking since hypernetworks are only used during training and can be fully centralized. For each\nof the four components of the mixing network (W1,b1,w2,b2), we introduce a hypernetwork that generates parameters of\nthe correct size. Thus, for the parameters that are vectors ( b1 and w2), we average the matrix generated by the attention\nlayer across the |A|sized dimension, and for b2, we average all elements. This procedure enables the dynamic generation\nof mixing networks whose input size varies with the number of agents. Assuming q= [Q1(œÑ1,u1),...,Q n(œÑn,un)], then\nQtot is computed as:\nQtot(s,œÑ, u) = œÉ((q‚ä§W1) + b‚ä§\n1 )w2 + b2 (11)\nwhere œÉis an ELU nonlinearity (Clevert et al., 2015).\nC. Environment Details\nC.1. STA RC R A F Twith Variable Agents and Enemies\nThe standard version of SMAC loads map Ô¨Åles with pre-deÔ¨Åned and Ô¨Åxed unit types, where the global state and observations\nare Ô¨Çat vectors with segments corresponding to each agent and enemy. Partial observability is implemented by zeroing\nout segments of the observations corresponding to unobserved agents. The size of these vectors changes depending on the\nnumber of agents placed in the map Ô¨Åle. Furthermore, the action space consists of movement actions as well as separate\nactions to attack each enemy unit. As such the action space also changes as the number of agents changes.\nOur version loads empty map Ô¨Åles and programmatically generates agents, allowing greater Ô¨Çexibility in terms of the units\npresent to begin each episode. The global state is split into a list of equal-sized entity descriptor vectors (for both agents\nand enemies), and partial observability is handled by generating a matrix that shows what entities are visible to each agent.\nThe variable-sized action space is handled by randomly assigning each enemy a tag at the beginning of each episode and\ndesignating an action to attack each possible tag, of which there are a maximum number (i.e. the maximum possible number\nof enemies across all tasks). Agents are able to see the tag of the enemies they observe and can select the appropriate action\nthat matches this tag in order to attack a speciÔ¨Åc enemy. Since UPDeT (Hu et al., 2021) naturally handles the variable-sized\naction space and the mapping of entities to actions, we ensure that the output action utilities for UPDeT are shufÔ¨Çed based\non the tags to maintain the entity-to-action mapping.\nD. Experimental Details\nOur experiments were performed on a desktop machine with a 6-core Intel Core i7-6800K CPU and 3 NVIDIA Titan Xp\nGPUs, and a server with 2 16-core Intel Xeon Gold 6154 CPUs and 10 NVIDIA Titan Xp GPUs. Each experiment is run\nwith 8 parallel environments for data collection and a single GPU. R E F I Ltakes about 24 hours to run for 10M steps on\nS TA R C R A F T. QMIX (Attention) takes about 16 hours for the same number of steps on S TA R C R A F T. Reported times\nare on the desktop machine and the server runs approximately 15% faster due to more cores being available for running the\nenvironments in parallel.\nRandomized Entity-wise Factorization for MARL\nE. Hyperparameters\nHyperparameters were based on the PyMARL (Samvelyan et al., 2019) implementation of QMIX and are listed in Table 2.\nAll hyperparameters are the same in all S TA R C R A F T settings. Since we train for 10 million timesteps (as opposed to\nthe typical 2 million in standard SMAC), we extend the epsilon annealing period (for epsilon-greedy exploration) from\n50,000 steps to 500,000 steps. For hyperparameters new to our approach (hidden dimensions of attention layers, number\nof attention heads, Œªweighting of imagined loss), the speciÔ¨Åed values in Table 2 were the Ô¨Årst values tried, and we found\nthem to work well. The robustness of our approach to hyperparameter settings, as well as the fact that we do not tune\nhyperparameters per environment, is a strong indicator of the general applicability of our method.\nTable 2: Hyperparameter settings across all runs and algorithms/baselines.\nName Description Value\nlr learning rate 0.0005\noptimizer type of optimizer RMSProp 1\noptim Œ± RMSProp param 0.99\noptim œµ RMSProp param 1e‚àí5\ntarget update interval copy live params to target params every episodes 200\nbs batch size (# of episodes per batch) 32\ngrad clip reduce global norm of gradients beyond this value 10\n|D| maximum size of replay buffer (in episodes) 5000\nŒ≥ discount factor 0.99\nstarting œµ starting value for exploraton rate annealing 1.0\nending œµ ending value for exploraton rate annealing 0.05\nanneal time number of steps to anneal exploration rate over 500000\nha hidden dimensions for attention layers 128\nhr hidden dimensions for RNN layers 64\nhm hidden dimensions for mixing network 32\n# attention heads Number of attention heads 4\nnonlinearity type of nonlinearity (outside of mixing net) ReLU\nŒª Weighting between standard QMIX loss and imagined loss 0.5\n1: Tieleman & Hinton (2012)",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.7062698006629944
    },
    {
      "name": "Computer science",
      "score": 0.6713045835494995
    },
    {
      "name": "Generalization",
      "score": 0.6500185132026672
    },
    {
      "name": "Task (project management)",
      "score": 0.5858075618743896
    },
    {
      "name": "Factoring",
      "score": 0.5230749249458313
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5183011889457703
    },
    {
      "name": "Function (biology)",
      "score": 0.5037760138511658
    },
    {
      "name": "Machine learning",
      "score": 0.3859960436820984
    },
    {
      "name": "Engineering",
      "score": 0.1296796202659607
    },
    {
      "name": "Mathematics",
      "score": 0.10080111026763916
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 16
}