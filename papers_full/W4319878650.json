{
  "title": "Smart Analysis of Economics Sentiment in Spanish Based on Linguistic Features and Transformers",
  "url": "https://openalex.org/W4319878650",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5007054465",
      "name": "José Antonio Garcí­a-Dí­az",
      "affiliations": [
        "Universidad de Murcia"
      ]
    },
    {
      "id": "https://openalex.org/A5012134876",
      "name": "Francisco García‐Sánchez",
      "affiliations": [
        "Universidad de Murcia"
      ]
    },
    {
      "id": "https://openalex.org/A5077299140",
      "name": "Rafael Valencia‐García",
      "affiliations": [
        "Universidad de Murcia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3133966466",
    "https://openalex.org/W3081987387",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W3037458976",
    "https://openalex.org/W3092146409",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4224291642",
    "https://openalex.org/W6965441720",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6729269041",
    "https://openalex.org/W3139376569",
    "https://openalex.org/W6622938743",
    "https://openalex.org/W6607799657",
    "https://openalex.org/W3128561444",
    "https://openalex.org/W3108759814",
    "https://openalex.org/W3036066542",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W2131776862",
    "https://openalex.org/W2610005984",
    "https://openalex.org/W3195990001",
    "https://openalex.org/W3209875211",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6767182473",
    "https://openalex.org/W3035101152",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3133048943",
    "https://openalex.org/W3184834965",
    "https://openalex.org/W6603405191",
    "https://openalex.org/W2049211729",
    "https://openalex.org/W2785939461",
    "https://openalex.org/W2949202718",
    "https://openalex.org/W3115467802",
    "https://openalex.org/W3155398915",
    "https://openalex.org/W3101323730",
    "https://openalex.org/W3137066771",
    "https://openalex.org/W1445021160",
    "https://openalex.org/W4386951003",
    "https://openalex.org/W3173213197",
    "https://openalex.org/W3043424630",
    "https://openalex.org/W3093460409",
    "https://openalex.org/W2791458026",
    "https://openalex.org/W2012378416",
    "https://openalex.org/W4205931544",
    "https://openalex.org/W3081483437",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6747248625",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2951732656",
    "https://openalex.org/W6748304040",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2789758093",
    "https://openalex.org/W6785011507",
    "https://openalex.org/W2114401841",
    "https://openalex.org/W2983877531",
    "https://openalex.org/W4394637965",
    "https://openalex.org/W4285594799",
    "https://openalex.org/W810042281",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W3035390927"
  ],
  "abstract": "Texts related to economics and finances are characterized by the use of words and expressions whose meaning (and the sentiments they convey) substantially depend on the context. This poses a major challenge to Natural Language Processing tasks in general, and Sentiment Analysis in particular. For low-resource languages such as Spanish, this situation becomes even more acute. Yet, the latest advancements in the field, including word embeddings and transformers, have allowed to boost the performance of Sentiment Analysis solutions. In this work we explore the impact of the combination of different feature sets in the accuracy of Sentiment Analysis in Spanish financial texts. For this, a corpus with 15,915 tweets has been compiled and manually annotated as either positive, negative, or neutral. Then, feature sets based on contextual and non-contextual embeddings along with linguistic features were evaluated both individually and combined. The best results, with a weighted F1-score of 73.15880&#x0025;, were obtained with a combination of feature sets by means of knowledge integration.",
  "full_text": "Received 5 January 2023, accepted 7 February 2023, date of publication 10 February 2023, date of current version 15 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3244065\nSmart Analysis of Economics Sentiment in\nSpanish Based on Linguistic Features\nand Transformers\nJOSÉ ANTONIO GARCÍA-DÍAZ\n , FRANCISCO GARCÍA-SÁNCHEZ\n ,\nAND RAFAEL VALENCIA-GARCÍA\nFacultad de Informática, Universidad de Murcia, Campus de Espinardo, 30100 Murcia, Spain\nCorresponding author: Rafael Valencia-García (valencia@um.es)\nThis work is part of the research projects AIInFunds (PDC2021-121112-I00) and LT-SWM (TED2021-131167B-I00) funded by\nMCIN/AEI/10.13039/501100011033 and by the European Union NextGenerationEU/PRTR. This work is also part of the research\nproject LaTe4PSP (PID2019-107652RB-I00/AEI/ 10.13039/501100011033) funded by MCIN/AEI/10.13039/501100011033. Besides,\nit was partially supported by the Seneca Foundation-the Regional Agency for Science and Technology of Murcia (Spain)-through project\n20963/PI/18. In addition, Jose Antonio Garcia-Diaz is supported by Banco Santander and the University of Murcia through the Doctorado\nIndustrial programme.\nABSTRACT Texts related to economics and finances are characterized by the use of words and expressions\nwhose meaning (and the sentiments they convey) substantially depend on the context. This poses a major\nchallenge to Natural Language Processing tasks in general, and Sentiment Analysis in particular. For low-\nresource languages such as Spanish, this situation becomes even more acute. Yet, the latest advancements in\nthe field, including word embeddings and transformers, have allowed to boost the performance of Sentiment\nAnalysis solutions. In this work we explore the impact of the combination of different feature sets in\nthe accuracy of Sentiment Analysis in Spanish financial texts. For this, a corpus with 15,915 tweets has\nbeen compiled and manually annotated as either positive, negative, or neutral. Then, feature sets based on\ncontextual and non-contextual embeddings along with linguistic features were evaluated both individually\nand combined. The best results, with a weighted F1-score of 73.15880%, were obtained with a combination\nof feature sets by means of knowledge integration.\nINDEX TERMS Sentiment analysis, financial, transformers, feature engineering, deep learning.\nI. INTRODUCTION\nRecent trends in Natural Language Processing (NLP) and\nthe development of pre-trained linguistic models based on\ntransformers and attention mechanisms with large unan-\nnotated corpora are allowing to improve the accuracy of\nseveral NLP tasks such as named entity recognition (NER),\nautomatic summarization, or sentiment analysis (SA) among\nothers [1], [2]. Moreover, the development of multilingual\nassets and the conception of language specific linguistic\nmodels are allowing to improve the performance in low-\nresource languages such as Spanish [3].\nAmong specific domains from which information can be\nextracted, the language employed in the financial domain\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Ali Shariq Imran\n.\nis particularly challenging. First, it contains words and\nexpressions that are very specific to this domain. Moreover,\nit may contain terms such as anglicisms and acronyms for\nwhich pre-trained models may have few examples to learn\nfrom. Second, in some cases terms and phases are used in\nthe financial domain with a meaning that differ from the\ntrue definition of the words. Expressions such as Activo1 and\nPasivo2 could be easily misinterpreted as the adjectives active\nand passive, respectively. Non-contextual word embeddings\nare non-practical for dealing with those terms and they do\nnot handle polysemy [4]. Third, the positive and negative\npolarity expressed in a document is not easily inferred using\nlexicon-based approaches. For example, as the increment of\n1In English: Financial asset.\n2In English: Financial liability.\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 14211\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nsomething could be seen as something positive in a general\nway, in the financial domain, the polarity could change with\nnegative effects when it is risk premium or unemployment\nwhat increases.\nIn preliminary works, we have explored the reliability\nof applying non-contextual embeddings from Spanish pre-\ntrained models for conducting SA to the financial domain [5].\nFor that purpose, we built a preliminary dataset with tweets\nwritten in Spanish concerned with the financial domain and\nmanually labeled them as positive, negative, or neutral. This\nwork represents a significant extension of our previous work\nin which we perform an in-depth evaluation of SA for the\nfinancial domain with the following major contributions: (1)\nwe have enlarged and improved the dataset, which now con-\ntains 15,915 tweets labeled as positive, negative and neutral\nwith tweets between 2017 and 2021, resulting in an increase\nof almost double; (2) we evaluate several multilingual and\nSpanish pre-trained models based on transformers, such as\nBETO [3], multilingual BERT [6], ALBETO [7], Distilled\nBETO [7], MarIA [8], BERTIN [9] and XLM [10]; (3) we\nevaluate the reliability of combining the feature sets by means\nof ensemble learning and knowledge integration; and, (4) we\nconduct an error analysis to understand the pros and cons of\neach feature set.\nAlthough Spanish is one of the most widely spoken lan-\nguages in the world3 NLP resources are scarce. Therefore, the\ngovernment of Spain, through the ‘Plan for the Advancement\nof Language Technology’ [11], is supporting the development\nof language models and annotated datasets to significantly\nimprove NLP in Spanish. The novelty of our work is twofold.\nFirst, a new, manually-labeled sentiment dataset in Spanish\nis provided in the financial domain. Second, we analyze the\nperformance of state-of-the-art models. Indeed, even though\ntransformer-based architectures such as BERT and RoBERTa\nhave been in the literature for some years (BERT was first\npublished in 2018 and RoBERTa in 2019), these models were\nfocused on English, and their multilingual counterparts do\nnot offer satisfactory results when dealing with some NLP-\ntasks for Spanish texts as shown in our study. Therefore, given\nsuch limitations, it is necessary to validate new language\nmodels based on these architectures but centered on Spanish\nincluding BETO, ALBETO, MarIA and BERTIN. These\nmodels have been made available just recently and are still\nbeing validated in the Spanish language in different domains.\nAs the financial domain is very specific, it is necessary\nto study whether these general language models provide\ngood enough results for detecting sentiments in financial\ntweets written in Spanish. Moreover, just like in state-\nof-the-art researches, we analyze the mixture of different\nlinguistic features with deep learning technologies based on\ntransformers both to boost the accuracy of the results and to\nimprove their interpretability.\nThe rest of the manuscript is organized as follows.\nSection II provides background information concerning SA,\n3https://www.ethnologue.com/guides/ethnologue200\nincluding attention mechanisms and transformer language\nmodels, and current approaches to SA in the financial\ndomain. A detailed description of the corpus and its compi-\nlation process is presented in Section III. Then, in Section IV\nthe system architecture is described along with a complete\noverview of all its components. The evaluation of the feature\nsets in isolation or combined and the error analysis conducted\nfor the evaluation of the overall system is shown in Section V.\nFinally, conclusions and future work are put forward in\nSection VI.\nII. STATE OF THE ART\nIn this work, different feature sets based on linguistic features\nand transformer mechanisms are evaluated for dealing\nwith SA in Spanish financial texts. In the last few years,\nresearchers in the NLP field have explored different means to\nboost the overall classification success in SA [1]. The use of\nmechanisms of attention such as transformers along with the\nadoption of deep learning methods are proving successful for\nhigh resource languages such as English [2], but their impact\non other, less resourced languages such as Spanish is yet to\nbe validated. In this section, SA is introduced, and several\nSpanish and multilingual pre-trained contextual embeddings\ncommonly used to improve the performance of NLP tools are\nenumerated. Then, various approaches to SA in the financial\ndomain are discussed.\nA. SENTIMENT ANALYSIS\nThe NLP task in which the subjective sentiment of a\ntext is obtained is often referred to as Sentiment Analysis\n(SA), also known as, Opinion Mining [12]. Three levels\nof analysis depending on the degree of specificity can be\ndistinguished, namely, document-level, sentence-level and\naspect-level [1]. In document-level SA each document is\nlabeled with a single sentiment, which is typically accurate on\nsmall documents providing general insights about the users’\nattitudes. However, when dealing with, for example, product\nreviews from online stores, different even contradictory\nopinions on the same product or service can be found. Under\nsuch circumstances, the sentence-level SA is more useful\nsince a sentiment is calculated for each sentence in the\ndocument. The drawback here is that a manual revision is\nrequired to find out the topic each sentence is about. Aspect-\nlevel SA specifically deals with this issue by dividing texts\ninto subtopics and assigning a sentiment to each one, thus\nbecoming the most sophisticated approach for conducting\nSA [13].\nThe approaches for extracting the polarity of a text can\nbe divided into the following three categories [14]: lexicon-\nbased methods, machine learning-based methods and hybrid\ntechniques that extend machine learning models with lexicon-\nbased knowledge. The lexicon-based approach relies on\nwords expressing positive or negative feelings to humans\npreviously gathered and documented in a lexicon such as\nSentiWordNet [15]. Then, lexicon-based methods calculate\nthe polarity scanning through the documents for these\n14212 VOLUME 11, 2023\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nkeywords. Some linguistic phenomena, such as polysemy\nor ambiguity, can hamper the performance of this approach\nbut their effects can be lessened with the use of domain\nspecific lexicons [16], [17]. On the other hand, the machine\nlearning-based approach consists of training a model to\ndiscriminate between positive, neutral, and negative texts.\nThe use of supervised learning, in which the model is trained\nwith labeled source data, often outperforms unsupervised\nand semi-supervised learning approaches, but depends on\npreviously annotated examples, which is a time-consuming\nand subjective manual labor. In the last few years, researchers\nare exploring the use of deep neural networks, namely,\ndeep learning, for SA [18]. Deep learning algorithms\nsuch as Long-Short Term Memory (LSTM), Convolutional\nNeural Networks (CNN), Gated Recurrent Units (GRU) and\nRecurrent Neural Networks (RNN) are some of the most\ncommon ones to accomplish this task [1].\nThe application of machine learning-based techniques\nin SA is contingent upon the extraction of meaningful\nfeatures from the documents, commonly referred to as feature\nengineering. Three types of features are commonly distin-\nguished, including statistical, linguistic, and contextual [19].\nTraditional approaches generally use Bag-Of-Words (BoW),\na statistical model that considers the frequency of the words\nwithin a vocabulary to represent a text in the form of a sparse\nvector. In line with this, the term frequency-inverse document\nfrequency (TF–IDF) method puts the focus on words often\noccurring in one document but scarcely in the corpus.\nDifferently, linguistic phenomena such as stylistic features\ncharacterize the linguistic approach. Part-Of-Speech (PoS)\ntagging is a popular process in which words are categorized\nwith a part of speech depending on their context. Similarly,\nthe Linguistic Inquiry and Word Count (LIWC) [20] tool is a\ntext analysis program that calculates the percentage of words\nin a text that belong to one or more linguistic, psychological,\nand topical categories. LIWC has been used in opinion\nmining [21] and complex classification tasks, such as satire\ndetection [22]. The main challenge of linguistic features is\nthat they are not easily shared between languages and cultures\nand, consequently, the resulting models are largely language\nand cultural dependent. Finally, contextual features refer to\ninformation not explicitly expressed in the text, but available\nin its context such as author gender or time/date in which the\ntext was written. The use of these contextual features is less\ndiscussed in the literature since their availability is not always\nensured.\nThe state-of-the-art concerning modern feature engineer-\ning relies on the use of word embeddings [23]. Unsupervised\ngeneric tasks are typically used to learn these embeddings,\nwhich represent words or sentences using dense vectors with\nreal numbers in which words with similar semantics are\nclustered together. However, polysemy constitutes a major\nchallenge for traditional (non-contextual) embeddings since\na word will be represented with the same vector regardless\nof its meaning in a sentence. Contextual word embeddings\novercome this issue by taking into account the context of a\nword to generate the embeddings. In particular, the words\nthat are next to a given word are considered when producing\nits representation. Word embeddings constitute a statistical\napproach typically used in deep learning models. One of the\nmany advantages of word embeddings is that they can be used\nwith different neural networks architectures such as CNN\nand RNN, that exploit the spatial and temporal dimension\nof the text, respectively. However, while CNN models have\nbeen successful to reduce the dimensionality of the feature\nspace and to extract meaningful features from texts in SA\napplications, attention mechanisms help tackle another major\nissue in this field, namely, the need for focusing on the\nimportant parts of the contextual information of texts [2].\nAttention mechanisms are used to focus on the important\nparts of the context by assigning different weights [24].\nTransformers constituted a major improvement boosting the\nspeed with which models that use attention can be trained.\nOriginally introduced in [25], the Transformer is based solely\non attention mechanisms without the need for RNN or CNN\nin the encoder-decoder configuration. BERT [26], a large\npre-trained Transformer network, has become one of the\nmost popular language models across various NLP tasks.\nRoBERTa [27] constitutes an optimization of BERT pre-\ntraining procedure outperforming BERT models in almost\nall NLP tasks. Then, the research focus now is on building\ndomain specific language models that better capture the\ndomain features. An example in the financial domain is\nFinBERT ([28] or [29]), a language model pre-trained on\nlarge-scale financial corpora.\nSimilarly, BERT and other transformer models are being\nadapted to specific languages other than English. BETO [3],\nfor example, is an initiative to allow the use of BERT pre-\ntrained models for Spanish NLP tasks. Another example is\nALBETO [7], which is a version of ALBERT [30] (which,\nin turn, is a lightweight version of BERT) that has been pre-\ntrained only with documents written in Spanish. Likewise,\nMarIA [8] is based on RoBERTa and has been trained with\ntext gathered from the National Library of Spain. Finally,\nBERTIN is also based on RoBERTa and has been trained with\nthe Spanish split of the mC4 dataset [31].\nThe Spanish Society for Natural Language Processing\n(SEPLN) organizes every year since 2012 the ‘Workshop on\nSemantic Analysis at SEPLN’ (TASS)4, which focuses on SA\nfor the Spanish language. The original task of TASS is the\nevaluation of polarity classification systems of tweets written\nin Spanish and different variants (Spain, Peru, Costa Rica,\nChile, Uruguay, and Mexico). In Section V-Dwe compare the\nperformance of the approach proposed in this work against\nthe proposal that achieved the best results in the last edition\nof the shared task, that took place in 2020.\nTo conclude this SA overview, in a recent review Osorio\nAngel et al. [32] studied the latest advances in SA for the\nSpanish language. While the pipeline and the techniques\nused at each step (information extraction, preprocessing,\n4http://tass.sepln.org/\nVOLUME 11, 2023 14213\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nfeature extraction, sentiment classification and evaluation)\nare comparable to those used for any other language, the\nauthors highlight the ever-growing number of linguistic\nresources (e.g., lexicon or corpus) explicitly developed for\nthe Spanish language. In terms of performance, deep learning\nmodels achieved the best results.\nB. SENTIMENT ANALYSIS IN THE FINANCIAL DOMAIN\nProduct reputation, customer experience, market research and\nstock price prediction are some the areas within the financial\ndomain in which SA has proven useful [33]. In this section,\nthe most recent results in this field and the main approaches\nare discussed.\nGoing back to 2012, the authors in [34] described an\nexperiment using SA for market analysis, in which a\nstrong correlation between market and public sentiments\nwas discovered. Similarly, in [35] the authors gathered\ntweets from big cap technological stocks and used SA to\ncheck volatility, trading volume and stock prices. Again,\na high correlation between the stock prices and the extracted\nsentiments was found. Later, in 2014 Uhr et al. [36]\nintroduced a method for sentiment analysis in financial\nmarkets combining word associations through the ‘Concept\nfor the Imitation of the Mental Ability of Word Associ-\nation’ (CIMAWA) and lexical resources. They evaluated\nthe evolution of stock prices as compared to the sentiment\nmeasures calculated from a news corpus with 918,427\nfinance-related German documents. Sentiment values were\nobtained at document, sentence, and window size levels. The\nauthors concluded that sentiment analysis in large time scales\ncan assist in financial market-related decision making and\nrisk management. In [37] the authors compare various deep\nlearning models (LSTM, doc2vec and CNN) in predicting\nthe semantic polarity of contributions to a financial social\nnetwork such as StockTwits. They used Chi-square, analysis\nof variance (ANOV A) and mutual information as feature\nselection methods and BoW-based logistic regression (LR)\nas a baseline. From StockTwits, the authors collected the 140-\ncharacters messages posted by users in the first six months of\n2015 and determined that CNN was the most effective model\nwith a 90.9% accuracy.\nMore recently, the authors in [38] used the sentiments\nextracted from news articles along with other key indicators\nto build a predictive model from a fundamental analysis\nperspective. The target in [39] is markedly different. It con-\nstitutes a study on error patterns of some sentiment analysis\nmethods commonly used in the financial domain. In their\nexperiments, they make use of two datasets for the finance\ndomain, namely, the Yelp dataset and the StockTwits Senti-\nment (StockSen) dataset and investigate eight representative\nmodels belonging to one out of the three explored approaches:\nlexicon-based (OpinionLex, SenticNet, and L&M), machine\nlearning-based (Support Vector Machines (SVM) and fast-\nText), and deep learning NLP models (BiLSTM, S-LSTM,\nand BERT). Six common causes for financial sentiment\nanalysis errors are identified, i.e., irrealis mood, rhetoric,\ndependent opinion, unspecified aspects, unrecognized words,\nand external reference. But the latest contributions in this\narea put the focus on the use of SA for stock forecasting\napplying a variety of deep learning-based approaches. For\nexample, in [40] the authors employ a CNN model for\nclassifying the investors’ sentiments from Chinese posts in a\nstock forum and then propose a LSTM-based system that take\ninto account the sentiment analysis results along with further\ntechnical indicators to predict stock prices. In particular, the\nCNN-based sentiment analysis model outperformed other\ncompared approaches (LR, SVM, RNN and LSTM) when\ndealing with the 880,000 posts gathered from Eastmoney.com\nreaching a F-measure value of 0.8482. In a very similar\nfashion, Shi et al. [41] present an individual stock movement\nprediction algorithm in which the sentiment polarity of\nChinese comments posted in a financial online community\nare used in conjunction with the trade values of the stock\nin the previous five days. While SVM and LR models are\nemployed in the stock movement predictor, CNN- and RNN-\nbased algorithms are utilized in the sentiment classifier (with\nLR as baseline). For word embedding, a 300-dimensional\nsize was found optimal using word2vec. The best sentiment\nclassification results are obtained with GRU, reaching a\nF-measure value of 0.83. In contrast, the approach suggested\nin [42] relies on a number of proxy variables (i.e., the number\nof newly opened A-share accounts, the market turnover rate,\nthe number of monthly IPO, discount of closed-end funds and\nfirst-day return of IPO) to build an investor sentiment index\nusing the PCA (Principal Component Analysis) method.\nThen, the dynamic relationship between stock market returns,\ninvestor sentiment, and volatility are studied, finding an\nasymmetric influence of investor sentiment on the stock\nmarket, with a lower impact in the bearish stock market than\nin the bullish stock market.\nThe number of works concerned with SA for Spanish in\nthe financial domain is scarcer. In [43], the authors claim\nthat sentiment indicators about a given entity are traditionally\ntreated as silos that cannot be combined and propose a Linked\nData-based approach in which the sentiment information\nfrom different communities can be integrated. The proposed\nsystem gathers tweets from Twitter and builds a corpus in\nwhich only tweets related to financial institutions are kept.\nThen, each tweet is represented in the form of triplets which\nare annotated using SentiWordNet; the arithmetic mean of all\nregistered values are used to quantify the tweet sentiment.\nThe authors in [44] present a domain specific lexicon called\nFSAL focused on the financial domain and experimentally\nprove that combining different machine learning techniques\nfor SA with this domain specific lexicon results in better\nclassification performance than using a generic lexicon.\nIn their experiment, they make use of three machine learning\nalgorithms, namely, Naive Bayes (NB), Random Forest (RF)\nand SVM, to process a corpus with 500 tweets in Spanish.\nIn a recent report published by the Banco de España[45],\na Spanish dictionary of words with positive, negative or\n14214 VOLUME 11, 2023\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nTABLE 1. Corpus statistics.\nneutral connotation in the context of financial stability is\ndescribed, which is then used to create sentiment indices from\nfinancial texts. To calculate the sentiment index, the number\nof words in the text with a negative connotation and the\nnumber of words with a positive connotation are considered.\nThe texts from both the Banco de España’s Financial\nStability Reports and press reports are used to evaluate the\nrobustness of the built dictionary to estimate the sentiment\nof texts in this domain. Finally, in a previous work [5],\nwe explored the reliability of applying non-contextual pre-\ntrained embeddings to the financial domain. Specifically, the\nembeddings were trained from two main sources, namely, the\nSpanish Unannotated Corpora (SUC) and the Spanish Billion\nWord Corpus and Embeddings (SBWC). The embeddings\nwere trained using different methods, such as Word2Vec,\nfastText and GloVe. These resources were used to train a\ndeep learning classifier to extract the polarity of Spanish\ntweets from the financial domain in an earlier version of the\ndatasets presented in this work. The best accuracy achieved\nwas 58.036% using a GRU with fastText and SUC.\nA comprehensive evaluation of the most significant\napproaches to SA in finance is provided in [46]. According\nto their results, NLP transformers present the highest\nperformance and even their distilled versions (e.g., Distilled-\nBERT) obtain promising results in text classification tasks.\nIII. CORPUS\nFor compiling the dataset, we relied on the UMUCorpus-\nClassifier tool [47], developed by our research group. This\ntool crawls data from Twitter, a social network in which\nusers can send and receive micro-blogging posts of less\nthan 280 characters. We extracted tweets from popular\nSpanish economists and news sites focused on the financial\ndomain. The tweets are between November 2017 and October\n2021 and they have been manually labeled in different stages\nand by different annotators. Preliminary versions of this\ndataset were published in [5], [48].\nIn a nutshell, the labeling process can be described\nas follows: each annotator should identify a tweet with\nthe following sentiments: very-positive, positive, neutral,\nnegative, very-negative, out-of-domain, and do-not-know-do-\nnot-answer. It is worth noting that a tweet can be labeled more\nthan once. The average number of ratings of a tweet is 2.5342.\nIn addition, the annotators achieve a inter-agreement score\nbased on the Krippendorff’s alpha [49] of 0.63058 of a total\nof 32,028 annotations. Table 1 shows the corpus statistics.\nFIGURE 1. An example of a Tweet of the corpus.\nAn example of a tweet from the corpus is shown in\nFigure 1.5\nThe next step of the corpus compilation consisted in\ndiscarding those tweets labeled as out-of-domain. Next,\ntweets labeled as positive were merged with those labeled as\nvery-positive, and so were negative tweets with very-negative\nones. In case that a tweet received contradictory labels from\ndifferent annotators, it was considered as neutral. At the end\nof this process, we obtained 15915 tweets: 4176 positive\ntweets, 7782 neutral tweets, and 3957 negative tweets. The\ncorpus is available at the following URL. 6 This file contains\nthe Twitter’s IDs of the tweets, the label, and the split\n(training, validation, and testing). We do not include the\ntweets due to Twitter guidelines, 7 which advises not to share\nthe text, so users preserve the rights to delete their content on\nthe Internet.\nIV. SYSTEM ARCHITECTURE\nTo validate the feature sets and the deep learning architectures\na system was built whose architecture is depicted Figure 2.\nIn brief, the system works as follows. First, to clean\nthe dataset a text preprocessing phase is applied (see\nSection IV-A). Second, the dataset is divided into training,\nvalidation, and testing subsets in a 60-20-20 ratio (see\nSection III). Third, a feature extraction stage is conducted\nto obtain the linguistic features and the embedding-based\nfeatures (see Section IV-B). Fourth, we evaluate three\nstrategies for evaluating the features: (1) single feature\nevaluation, (2) knowledge integration, and (2) ensemble\nlearning. During this stage, an hyper-parameter optimization\nphase is performed to evaluate different deep learning\narchitectures (see Section IV-D). In the end, the test dataset\nis used to evaluate the best deep learning models for each\nfeature integration strategy.\nA. TEXT PREPROCESSING\nThe Text Preprocessing module generates different versions\nof the texts: (1) original, (2) normalized, and (2) normalized\nwith lowercase. The original version is used to extract\n5In English: ‘The development of 5G could bring Spain benefits of\n14.6 billion euros in 2021’.\n6http://pln.inf.um.es/corpora/economics/economics-2021.rar\n7https://developer.twitter.com/en/developer-terms/more-on-restricted-\nuse-cases\nVOLUME 11, 2023 14215\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nFIGURE 2. System architecture.\ncertain linguistic features related to correction and style. The\nnormalized version is used as source to extract PoS features.\nTo do this, hyperlinks, hashtags, or mentions among other\nsocial media jargon are removed. In addition, all digits and\npercentages are replaced with a fixed token because we do\nnot want deep learning classifiers to learn specific quantities.\nWe also strip expressive lengthening, by removing repetitions\nof the same letter that occurs more than twice. In addition,\nwe fix misspellings with the ASPELL tool. 8 Finally, the\nnormalized version in lowercase form is used to extract the\ntokens of the embeddings based features.\nB. FEATURE EXTRACTION\nFollowing the spirit of our previous works [50], we evaluated\nseveral feature sets for conducting opinion mining in the\nfinancial domain. Specifically, linguistic features along with\ncontextual and non-contextual pre-trained embeddings were\nevaluated.\nFor the extraction of the linguistic features (LF), we rely\non UMUTextStats [19], [51], which was developed by our\nresearch group inspired by LIWC [20]. This tool captures\n365 linguistic features organized as follows:\n• Correction and style of the writing communication\n(COR). It distinguishes orthographic errors, including\nstatistics concerning misspelled words, stylistics and\nbad performance errors, such as sentences starting with\nnumbers or with the same word, or common errors and\nredundant expressions.\n• Phonetics (PHO). It captures expressive lengthening,\nthat is, the intentional elongation of some letters with\nan emphasizing purpose.\n• Morphosyntax (MOR). It captures how words are\ncomposed, including grammatical gender, number,\nand a great variety of affixes, including nominals,\n8http://aspell.net/\nadjectivizers, verbalizers, adverbializers, augmentative,\ndiminutives, or derogatory suffixes. It also captures\nand organizes features according to their PoS category\n(e.g., verbs, nouns, adjectives, etc.). For this, we mix\nStanza [52] with lexicons that capture fine grained\ncategories.\n• Semantics (SEM). It includes onomatopoeia,\neuphemism, dysphemism, and synecdoche.\n• Pragmatics (PRA). It captures figurative language\ndevices, including understatements, rhetorical ques-\ntions, hyperbole, idiomatic expressions, verbal irony,\nmetaphors, or similes among others.\n• Stylometry (STY). It captures punctuation symbols,\ncorpus statistics, and other metrics related to the number\nof words, syllables, or sentences.\n• Lexical (LEC). It captures the topics in the text\nanalyzing both abstract and general topics.\n• Psycho-linguistic processes (PLI). Emojis and lexicons\nrelated to emotions and sentiments are considered in this\ncategory.\n• Register (REG). It captures the presence of informal or\ncultured language along with topics related to offensive\nspeech.\n• Social media jargon (SOC). Features associated to the\nspeaker’s mastery of social media jargon are captured in\nthis category.\nAs for the embeddings, both contextual and non-\ncontextual word and sentence embeddings were explored as\nfollows:\n• Non-contextual word embeddings (WE). Pre-trained\nmodels based on word2vec [53], fastText [54], and\ngloVe [55] are evaluated. As mentioned above, word\nembeddings enable the exploration of specific types of\nneural network architectures, such as CNN and RNN,\nwhich can take advantage of the space and temporal\ndimensions of language, respectively. In particular, CNN\n14216 VOLUME 11, 2023\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\ncan generate high-order features by clustering multi-\nword terms whose meaning differs from the one it can\nbe obtained taking each word separately (e.g., New and\nYork). On the other hand, RNN leverage the temporal\ndimension by considering the order of the words. In this\nwork, two bidirectional RNN based on LSTM and GRU\nhave been evaluated, namely, BiLSTM and BiGRU.\n• Non-contextual sentence embeddings (SE). These are\nextracted from FastText [56], whose Spanish model is\ntrained from CommonCrawl and Wikipedia [57].\n• Contextual word embeddings (BF). Different pre-\ntrained transformer-based models are evaluated. These\nmodels can classified as BERT-based models and\nRoBERTa-based models. The key difference between\nthese two architectures is that in RoBERTa, the masking\nis performed during training time whereas in BERT the\nmasking is performed at the beginning of the training.\nThe architectures based on BERT are: (i) BETO, the\nSpanish version of BERT [3]; two novel lightweight\nversions: (ii) ALBETO and (iii) Distilled BETO [7],\nand (iv) multilingual BERT (mBERT). The architectures\nbased on RoBERTa are (i) MarIA [8], (ii) BERTIN [9]\nand (iii) XLM [10]. The HuggingFace transformers\nlibrary was used to fine tune the models with the corpus.\nIt is important to bear in mind that given that these\nkinds of embeddings are very time consuming, they are\ndifficult to combine with other feature sets. Therefore,\nthe fixed representation of the [CLS] token is extracted\nas suggested in [58]. Then, this representation is used\nto combine the contextual embeddings more easily with\nthe rest of the feature sets. Preliminary results indicate\nthat the precision, recall, and accuracy of both contextual\nword and sentence embeddings are similar with this and\nother datasets.\nOnce feature sets are extracted, a feature normalization and\nselection step is conducted. A MinMax scaler is applied first\nto the linguistic features as they contain features in different\nscales with raw counts and percentages. Then, Information\nGain is applied to select the best features, discarding those\nthat belong to the last quartile.\nC. FEATURE INTEGRATION: KNOWLEDGE INTEGRATION\nAND ENSEMBLE LEARNING\nThis part of the pipeline is responsible for the integration of\nthe feature sets to build more robust solutions. Particularly,\nthree strategies are evaluated as follows: single feature\nevaluation, knowledge integration and ensemble learning.\nThe first strategy, single feature evaluation, does not\ncombine any feature set. Several models are trained for each\nfeature set separately using hyper-parameter tuning and the\nbest model is selected using the custom validation split. In this\nsense, we use this strategy as baseline to compare the other\ntwo strategies.\nThe second strategy for combining the features is known\nas knowledge integration, which consists in the integration\nof different feature sets within the same neural network. For\nthis, a multi-input deep learning network is trained from\nscratch. The idea is that the network learns during training\nhow to combine the strengths of each feature set. The network\narchitecture design followed in this work is to connect each\nfeature set into a different stack of hidden layers. Then, the\noutput of each layer is concatenated and connected to a new\nstack of hidden layers that are connected to the final output\nlayer. As it is not clear which is the best network architecture\nfor this task, we conducted the same hyper-parameter tuning\nstage (see Section IV-D) to get the best model, so different\nnumber of neurons, hidden layers, and activation functions\nare evaluated.\nThe third strategy, ensemble learning, consists of com-\nbining the predictions of several estimators (that are run\nseparately) to build a more robust estimator [59]. In this\nwork we checked four averaging methods to combine the\npredictions of each feature set. These averaging methods are\nas follows: (1) mode, which outputs the label with a majority\nvote of each estimator; (2) weighted mode, which is similar\nto the hard voting strategy, but the contribution of each model\nis weighted according to the performance of each model with\nthe validation set; (3) highest probability, which consists of\nobserving the probability of each label and model and select\nthe higher; and (4) average probabilities, which averages the\nprobabilities output by each model.\nD. HYPER-PARAMETER EVALUATION\nThe next step in our pipeline is to conduct the hyper-\noptimization stage. The main objective of this phase is to\nfind out what are the best parameters for the neural networks.\nIr order to do this, for each feature set (in isolation and in\ncombination) we trained several models and ranked them\nby weighted F1-score, considering the label distribution.\nFor each feature set, we tested different neural network\narchitectures. For LF, SE, and BF we relied on multilayer\nperceptron (MLP) as these features do not contain sequence\ninformation such as text. In case of word embeddings,\nwe evaluated CNN and bidirectional RNN based on LSTM\n(BiLSTM) and GRU (BiGRU). Moreover, we evaluated\nrandomly its shape, composed by the number of hidden layers\nand the number of neurons in each layer. The communication\nbetween the layers is made by several activation functions.\nWe also included a dropout mechanism to avoid overfitting.\nThe dropout is configured in a ratio of 10%, 20%, 30% or\nnot using dropout at all. In addition, two more parameters\nwere evaluated, namely, the batch size and the learning rate.\nAll neural networks made use of a time-based learning rate\nscheduler. The best hyper-parameters for each feature set and\ntheir combinations can be seen in Table 2.\nFrom Table 2, it can be observed that the majority of\nmodels that achieve the best results make use of shallow\nneural networks composed of one or two hidden layers.\nThe number of neurons per layer is large in case of\nsentence embeddings (256 for SE, 512 for BF) but smaller\nfor LF (8) and WE (3). Concerning the dropout, all the\nVOLUME 11, 2023 14217\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nTABLE 2. Hyper-parameters for each feature set and their combinations.\nfeatures in isolation achieved better results with smaller\ndropout (10% for LF, SE, WE; 30% with BF). When the\nfeatures are combined in the same neural network, some\ncombinations achieved better results without dropout, as is\nthe case of LF-WE, SE-WE, LF-WE-BF, SE-WE-BF, and the\ncombination of all feature sets (LF-SE-WE-BF).\nV. RESULTS AND DISCUSSION\nTo compare the results achieved by the rest of the feature\nsets, we set a baseline based on a BoW. This statistical model\nanalyzes the frequency of the words in the documents. The\ndrawbacks of BoW models are various. First, they consider\nwords without context, so they do not take into account\nlinguistic phenomena such as polysemy. Second, they create\nover-fitted models due to the vocabulary size, thus suffering\nthe curse of dimensionality. Moreover, in agglutinative\nlanguages this problem is aggravated, since there are a large\nnumber of words that can be obtained from the same root.\nThese problems are partially solved with the usage of bigrams\nand trigrams or the use of char-grams, as they are able to\ncapture lexical information, including the use of punctuation\nsymbols and morphological information, such as prefixes\nor suffixes. In addition, char-grams are more robust against\ngrammatical errors, as misspelled words and their correct\nversions share common char-grams.\nSpecifically, we extracted the TF–IDF of unigrams,\nbigrams, and trigrams as well as character n-grams of\nlength 3, 4, and 5, applying sub-linear scaling, from the\nlowercase version of the tweets. Next, we reduced the length\nof the features by applying Latent Semantic Analysis (LSA)\nto 100 components. We have selected this method because\nthese features are easy to extract, and they provide good\nresults in several classification tasks. However, these features\nhave some shortcomings. First, they are features that have\nlost the ordering and the meaning of the words. Second, they\nsuffer from the curse of dimensionality, as its size depends\non the vocabulary size. This handicap has been partially\novercome by applying LSA.\nSince we address an imbalanced classification problem,\nwe evaluated the performance of the deep learning models\nwith the weighted-average F1-score (see Equation 1), that is\nthe harmonic mean between Precision (see Equation 2), and\nRecall (see Equation 3) of each class but weighted by the\nnumber of instances in each class. In addition, the Accuracy\nmeasure (see Equation 4) has been also considered. In the\nequations, TP stands for true positive, TN stands for true\nnegative, FP stands for false positiveand FN stands for false\nnegative.\nF1 = 2 ∗ (Precision ∗ Recall)/(Precision + Recall)\n(1)\nPrecision = TP/(TP + FP) (2)\nRecall = TP/(TP + FN) (3)\nAccuracy = TP + TN/(TP + TN + FP + FN) (4)\nA. RESULTS OF THE FEATURE SETS IN ISOLATION\nPrior to the experimentation stage, we evaluated different\nmultilingual and Spanish pre-trained models based on\nTransformers. The idea was to determine which one is the\nmost accurate in order to use the best pre-trained model\nto combine the features with the rest. Accordingly, Table 3\nshows the results achieved by the pre-trained models based\non Transformers. The evaluated models are: (i) BETO [3],\na Spanish BERT trained with the Spanish Unannotated\nCorpora; (ii) ALBETO [7], a version of ALBERT, which\nis a lightweight version of BERT, pre-trained only with\nSpanish documents; (iii) Distilled BETO [7], trained using\ndistillation techniques to transfer the weights of BETO to a\nnew model with less layers and complexity; (iv) MarIA [8],\nbased on RoBERTa and trained with web crawlings from\nthe National Library of Spain; (v) BERTIN [9], which is\nanother model based on RoBERTa, trained with the Spanish\nsplit of the mC4 dataset; (vi) multilingual BERT [6], a BERT\n14218 VOLUME 11, 2023\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nTABLE 3. Pre-trained Transformer-based models.\nmodel trained with documents written in 104 languages; and\n(vii) XLM [10], a multilingual version of RoBERTa, trained\nwith data filtered from CommonCrawl from 100 different\nlanguages. It can be observed that the RoBERTa architecture\nachieves better results than BERT. The two best scores are\nachieved by MarIA (72.188%) and BERTIN (71.493%).\nBesides, XLM outperforms multilingual BERT (70.356%\nvs 69.957%). It can also be observed that the lightweight\nversions of BETO, ALBETO and Distilled BETO, reach\nsimilar results to this first one. In fact, ALBETO achieves\na better accuracy than BETO. When comparing the results\nachieved with transformers trained from single language\ndatasets versus those trained from multilingual datasets, it can\nbe seen that there is a slight advantage of the models trained\nonly with Spanish, which suggest that it is preferable to obtain\nspecific pre-trained models for the target language rather than\nto use multilingual variants.\nIn Table 4 the results of the feature sets in isolation\nare presented. As it can be observed, the baseline of\nTF–IDF of word and character n-grams with LSA achieved\na weighted F1-score of 55.97398%. The rest of the feature\nsets improve this baseline. The best result is achieved with\nBF, reaching a weighted F1-score of 68.554336%. This\nresult outperforms the weighted F1-score of LF (58.46046%),\nSE (65.18025%), and WE (65.14914%). It can be highlighted\nthat the embeddings (i.e., SE, WE, BF) are more effective\nthan LF for sentiment classification in the financial domain,\nboth in terms of precision and recall. This fact suggests that\nthe lexical features and what is said is more important than\nthe linguistic features that better capture the tone and style\nof the authors.\nWe can observe that all feature sets achieve similar results\nregarding the overall precision and recall of the system.\nHowever, considering the precision and recall of the features\nindividually (not shown in the table but available with the\nsource code 9), one can see that BF is the feature set in\nwhich these measures are more similar regardless the label\n(i.e., either positive, negative, or neutral). The precision and\nrecall among all classes achieved with SE are also similar.\nYet, WE achieve more precision with the positive class but\nless recall (precision of 64.03162%, recall of 58.13397%).\nThe same is observed with the neutral class (precision of\n74.44134%, recall of 68.46500%) but the opposite behavior\n9https://github.com/NLP-UMUTeam/Smart-Analysis-of-Economics-in-\nSpanish\nwith the negative class (precision of 52.61570%, recall of\n66.035353%). In LF, however, the system achieves better\nrecall than precision with the positive class (precision of\n50.40984%, recall of 58.85167%) but better precision than\nrecall with the neutral class (precision of 69.12568%, recall of\n64.99680%) and the negative class (precision of 47.51678%,\nrecall of 44.69697%).\nB. RESULTS OF KNOWLEDGE INTEGRATION\nThe results of the knowledge integration experiment are\nprovided in Table 5. The table is organized in combinations\nof two, three and four feature sets.\nConcerning the feature sets combined in pairs, we can\nobserve that the results are generally superior to the ones\nachieved individually. These results suggests that the feature\nsets are complementary. However, the results do not always\nimprove the ones achieved by the best individual model. For\nexample, the combination of LF and SE achieved a weighted\nF1-score of 64.17839%. This result improves largely the\nresults achieved by LF (58.46046%) but it is worse than SE\n(65.18025%). The best overall result is obtained with the\ncombination of three feature sets: LF, WE, and BF, achieving\na weighted F1-score of 73.15880%. These results improve the\ncombination of all features (72.98100%).\nThe degradation of the results when adding non-contextual\nsentence embeddings (SE) to the LF-WE-BF combination\nmight be due to merging embeddings that cannot handle\npolysemy in the same way.\nC. RESULTS OF ENSEMBLE LEARNING\nThe results of the ensemble learning experiment are shown\nin Table 6. The best weighted F1-score is achieved by\napplying the average probabilities strategy with an F1-score\nof 72.48612%. This result is slightly worse than the best result\nachieved by the knowledge integration strategy (73.15880%\nof F1-score with the combination LF-WE-BF). The result\nachieved by the highest probability strategy is similar\n(72.35689%) to the one obtained with average probabilities.\nIt draws our attention the good results achieved by the highest\nprobability strategy since other experiments conducted by\nour research group showed that this strategy usually achieves\ngood precision but limited recall in binary classification\nexperiments. Concerning the mode and the weighted mode,\nthe results are even lower with an F1-score of 69.42306%\nand 71.53359%, respectively. These results indicate that\nthe weighted mode is more accurate as it considers the\nperformance of each model.\nSumming up, the best result in terms of F1-score is\nobtained with the knowledge integration strategy, combining\nLF, WE and BF feature sets. The next best result (without con-\nsidering other knowledge integration-based combinations)\nis achieved by using ensemble learning with the average\nprobabilities strategy followed by the RoBERTa-based model\nand the BF isolated model. These results can be partially\ndue to the fact that knowledge integration strategies can learn\npatterns that occur when certain linguistic features and certain\nVOLUME 11, 2023 14219\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nTABLE 4. Results of the feature sets in isolation.\nTABLE 5. Results of the knowledge integration experiment.\nTABLE 6. Results of the ensemble learning experiment of LF, SE, WE, and\nBF.\nembeddings take place at the same time. At this point, it is\nalso worth highlighting that it is easier to adopt the ensemble\nlearning strategy than the knowledge integration strategy as\nit is not necessary to train and perform a hyper-parameter\nselector over a new model with a large number of parameters.\nD. COMPARISON WITH RELATED WORK\nThe TASS 2020 dataset 10 has been selected to compare the\nreliability of our methods with existing datasets. This shared\ntask proposed a sentiment classification task at three levels\nwith tweets written in different variants of Spanish. The\nvariants are Spain (ES), Costa Rica (CR), Peru (PE), Uruguay\n(UR), and Mexico (MX). The dataset is evaluated from both\na monolingual and a multi-variant perspectives.\nTable 7 reports the results of applying our best model (i.e.,\na combination of Linguistic features (LF), contextual sen-\ntence embeddings from RoBERTa (BF), and non-contextual\nword embeddings (WE) using knowledge integration) to the\ndataset provided by the TASS 2020 shared task. As reported\nin [60], the teams ELiRF-UPV , for the ES, CR PE and MX\nvariants, and Palomino-Ochoa, for the UR variant and the\nmulti-variant challenge, achieved the best results.\nWe can observe that our proposal outperforms the best\nmacro F1-score in ES (69.2% vs 67.1%) and MX (64.7%\n10http://tass.sepln.org/2020/?page_id=74\nvs 63.4%) but it is more limited for CR (63.8% vs 64.6%),\nPE (61.7% vs 63.6%) and UR (65.0% vs 66.9%). In the case\nof PE, our system outperforms the macro precision (69.1% vs\n67.2%) and macro recall (60.7% vs 60.3%).\nIt is worth mentioning that for this TASS 2020 dataset\ndifferent ensemble learning strategies outperform the results\nachieved with knowledge integration. We obtain a macro\nF1-score of 71% in ES with the highest probability strategy,\na macro F1-score of 65.8% in CR with the weighted mode\nstrategy, a macro F1-score of 63.9% in PE with the highest\nprobability strategy, a macro F1-score of 67.1% in UR with\nhighest probability strategy, and a macro F1-score of 65.77%\nin MX with the average probabilities strategy. However, none\nof the ensemble learning strategies outperform the knowledge\nintegration result with the multi-variant dataset, getting a\nbest macro F1-score of 46% with the average probabilities\nstrategy as compared with the 46.3% achieved with the\nLF-WE-BF knowledge integration-based combination.\nFrom these results we can conclude that our model is\ncompetitive in terms of macro precision, recall, and F1-\nscore in different Spanish dialects and that the knowledge\nintegration strategy achieves better performance with texts\nwritten in different Spanish variants whereas ensemble\nlearning strategies are better suited for one specific Spanish\nvariant.\nE. INTERPRETABILITY OF THE FEATURES\nTo discover the contribution of each linguistic feature to\nthe subjective polarity of the document, we calculated the\nInformation Gain [61] of each feature. The top-10 metrics\nand how they contribute to each label are shown in Figure 3.\nWe can observe that there are several features related to\nmorphology, such as the percentage of nouns, adverbs, and\n14220 VOLUME 11, 2023\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nTABLE 7. Comparison with TASS-2020. In the left, we report the macro averaged precision, recall and F1-score of our model based on LF, WE, and BF\ntrained with knowledge integration. In the right, we report the same metrics for the winner of the TASS-2020 best result.\nFIGURE 3. Information gain.\nwords in plural. Out of these, only the percentage of adverbs\nhas more relevance to neutral documents. The most relevant\nstylometric linguistic feature is the usage of the percentage\nsymbol, which appears very frequently in positive and\nnegative texts, but rarely seen in neutral ones. This finding\nsuggests that it is easy to infer the subjective polarity of tweets\nthat report objective facts and statistics. Another linguistic\ncategory that constitutes good LF is Lexical, with words\nand expressions related to death, that are more common in\nnegative documents, and negative processes from the psycho-\nlinguistic processes category.\nF. ERROR ANALYSIS\nFor conducting the error analysis, we made use of the neural\nmodel that provided best weighted F1-score over the test\nsplit, that consisted in the combination of LF, WE, and BF in\nthe same neural network. Prior to error analysis, to check in\nwhat cases the model gives wrong predictions the confusion\nmatrix has been plotted (see Figure 4). Particularly, taking\ninto account this confusion matrix it can be observed that\nthe model does not make many relevant wrong classifications\n(i.e., mismatching positive and negative documents), and\nthe focus can be set on the relationship between neutral\ndocuments and their prediction as either positive or negative,\nand vice versa. We observe that the model labeled wrongly\na 23% of negative and a 24% of positive documents as\nneutral. In addition, the ratio of wrong classifications of\nFIGURE 4. Confusion matrix of the combination of LF, WE and BF in the\nsame neural network.\nneutral documents is not skewed for negative nor positive\nlabels, being the percentage of wrong classifications of 12%\nand 11%, respectively.\nNext, to assess the overall performance of our best model,\nwe compared the predictions with the ones obtained by\nthe baseline model based on character and word n-grams.\nWe observed that there are no instances that are correctly\nclassified by the baseline model but not by our best model.\nThis finding suggests that our best model completely outper-\nforms the baseline. It is worth noting that the instructions for\nreplicating these results can be found in the code repository.\nFinally, we selected all the cases that were wrongly\nclassified by our best model and, specifically, we focused\non those predictions in which the neural model outputs a\nprobability of the opposite label with a chance larger than\n50% (i.e., either the ground truth is positive or negative but\nwith a prediction probability of the opposite label equal or\nlarger than 50%). However, under these specifications it is\nnot possible to find any wrong prediction. Consequently,\nwe changed the threshold from 50% to 45% and found three\ninstances, which are listed in Table 8.\nWe can observe that there were only three wrong\npredictions in all the test dataset. Out of these, there is only an\ninstance of a positive document that is wrongly classified as\nneutral by a slightly superior probability (0.47847% chance\nof being neutral vs 0.45145% probability of being positive).\nVOLUME 11, 2023 14221\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\nTABLE 8. Error Analysis. We include the text, the ground truth (label), and the probabilities that the case belong to the negative, neutral and positive\nclass as produced by the model.\nThis document informs that a company is committed to\nEuropean and emerging stocks, ruling out recession. There\nare also two negative documents wrongly classified, one as\npositive and the other as neutral. The first document deals\nwith the raising of the price of second-hand housing. This\ndocument is assigned a probability of 0.45024% of being\nnegative and a 0.49767% of being positive, with a negligible\nprobability of being neutral. In this sense, we highlight one of\nthe problems with the financial domain in which a document\ncould be positive to some actors in the financial market, but\nalso at the same time negative for others. Finally, the last\ndocument is about how minority shareholders on the IBEX\n3511 stock exchange have encountered difficulties to be able\nto exercise their rights due to online meetings.\nVI. CONCLUSION AND FURTHER WORK\nIn this paper we have explored the reliability of applying\ndifferent feature sets based on contextual and non-contextual\nembeddings with linguistic features to improve Sentiment\nAnalysis in Spanish for a challenging context such as the\nfinancial domain. Our results indicate that the combination\nof feature sets by means of knowledge integration provides\nthe best results with a weighted F1-score of 73.15880%.\nAdditionally, the results obtained by other strategies,\nsuch as ensemble learning, are quite similar in terms of\nperformance with the added benefits of (i) facilitating\nthe combination of the feature sets and (ii) being more\neasily trainable. From our experiment it can be also noted\nthat the usage of contextual word embeddings based on\nattention mechanisms represents a qualitative leap when it\ncomes to improving the accuracy of the models. Further,\nthe usage of general linguistic features provides limited\nresults regarding sentiment analysis although they improve a\nbaseline based on n-grams with LSA. Then again, the usage\nof linguistic features improves the accuracy when combined\nwith transformers.\nThe benefits of knowledge integration over ensemble\nlearning is that the neural network can learn from different\nfeature sets at once. So, with knowledge integration the\nnetwork can learn what features are more relevant for each\ndocument and how to combine them resulting in more\ngeneral solutions. In ensemble learning, however, the fact\nthat certain feature sets achieve, generally, better performance\nthan others is ignored, except in some texts. For instance,\nlinguistic features can make a significant difference with\ntransformers in cases when some linguistic clues that are\n11The Iberian Index (IBEX 35) is the Spain’s principal stock exchange\nindex.\nhard to obtain with transformers can be guessed. This is the\ncase of expressive lengthening, for instance. Therefore, the\ndisparate performance of each of the used feature set limits\nthe performance of ensemble learning. This behavior is not\nobserved in all the strategies, because it is more sensitive\nin strategies such as average probabilities or the mode of\npredictions than in weighted mode or highest probability.\nIn this sense, we argue that ensemble learning is more\neffective when all feature sets have similar performance or\nwhen several models are trained with the same feature sets\nbut varying hyper-parameters, such as the seed or the learning\nrate.\nAs future work, we are compiling a large corpus from\nSpanish financial newspapers to retrain BERT and RoBERTa\nbased models, applying masked language modeling, and\nthen fine-tune the model for sequence classification and\nperform an error analysis to check whether the accuracy of\nthe system is improved or not. Another promising research\nline is the application of Semantic Web and ontologies\nto guide an Aspect-based Sentiment Analysis [19]. In this\nsense, we can explore how sentiments are transmitted through\nentities and their relationships and discover new types of\nproducts in which to invest in the short or medium term.\nMoreover, it would be possible to model different types of\ncustomers and their preferences by extracting demographic\nand psychographic information and build recommender\nsystems [62] that can assist companies and individuals to\ncarefully choose their investments based on their preferences.\nREFERENCES\n[1] A. Ligthart, C. Catal, and B. Tekinerdogan, ‘‘Systematic reviews in\nsentiment analysis: A tertiary study,’’ Artif. Intell. Rev., vol. 54, no. 7,\npp. 4997–5053, Oct. 2021.\n[2] M. E. Basiri, S. Nemati, M. Abdar, E. Cambria, and U. R. Acharya,\n‘‘ABCDM: An attention-based bidirectional CNN-RNN deep model for\nsentiment analysis,’’ Future Gener. Comput. Syst., vol. 115, pp. 279–294,\nFeb. 2021.\n[3] J. Cañete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, and J. Pérez,\n‘‘Spanish pre-trained BERT model and evaluation data,’’ in Proc. ICLR,\n2020, pp. 1–10.\n[4] A. Miaschi and F. Dell’Orletta, ‘‘Contextual and non-contextual word\nembeddings: An in-depth linguistic investigation,’’ in Proc. 5th Workshop\nRepresent. Learn. (NLP), 2020, pp. 110–119.\n[5] J. A. Garcia-Diaz, O. Apolinario-Arzube, and R. Valencia-Garcia, ‘‘Eval-\nuating pre-trained word embeddings and neural network architectures for\nsentiment analysis in Spanish financial tweets,’’ in Proc. Mex. Int. Conf.\nArtif. Intell.Cham, Switzerland: Springer, 2020, pp. 167–178.\n[6] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[7] J. Cañete, S. Donoso, F. Bravo-Marquez, A. Carvallo, and V . Araujo,\n‘‘ALBETO and DistilBETO: Lightweight Spanish language models,’’\n2022, arXiv:2204.09145.\n14222 VOLUME 11, 2023\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\n[8] A. Gutiérrez-Fandiño, J. Armengol-Estapé, M. Pamies, J. Llop-Palao,\nJ. Silveira-Ocampo, C. P. Carrino, C. Armentano-Oller,\nC. Rodriguez-Penagos, A. Gonzalez-Agirre, and M. Villegas, ‘‘Maria:\nSpanish language models,’’ Procesamiento Del Lenguaje Natural, vol. 68,\npp. 39–60, Jan. 2022.\n[9] J. D. L. Rosa, E. G. Ponferrada, P. Villegas, P. G. D. P. Salas, M. Romero,\nand M. Grandury, ‘‘BERTin: Efficient pre-training of a Spanish language\nmodel using perplexity sampling,’’ Procesamiento del Lenguaje Natural,\nvol. 68, pp. 13–23, Jul. 2022.\n[10] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzman, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n‘‘Unsupervised cross-lingual representation learning at scale,’’ 2019,\narXiv:1911.02116.\n[11] M. O. E. Affairs, D. Transformation, and G. O. Spain. (Oct. 2015).\nPlan for the Advancement of Language Technology. Accessed: Nov. 7,\n2022. [Online]. Available: https://plantl.mineco.gob.es/tecnologias-\nlenguaje/PTL/Bibliotecaimpulsotecnologiaslenguaje/\nDetalle%20del%20Plan/Plan-Advancement-Language-Technology.pdf\n[12] R. K. Bakshi, N. Kaur, R. Kaur, and G. Kaur, ‘‘Opinion mining and\nsentiment analysis,’’ in Proc. 3rd Int. Conf. Comput. Sustain. Global\nDevelop. (INDIACom), 2016, pp. 452–455.\n[13] Y . Tian, L. Yang, Y . Sun, and D. Liu, ‘‘Cross-domain end-to-end\naspect-based sentiment analysis with domain-dependent embeddings,’’\nComplexity, vol. 2021, pp. 1–11, Mar. 2021.\n[14] O. Kolchyna, T. P. T. Souza, C. P. Treleaven, and T. Aste, ‘‘Twitter\nsentiment analysis: Lexicon method, machine learning method and their\ncombination,’’ 2015, arXiv:1507.00955.\n[15] S. Baccianella, A. Esuli, and F. Sebastiani, ‘‘SentiWordNet 3.0:\nAn enhanced lexical resource for sentiment analysis and opinion\nmining,’’ in Proc. Int. Conf. Lang. Resour. Eval. (LREC), vol. 10, 2010,\npp. 2200–2204.\n[16] J. M. Ruiz-Martinez, R. Valencia-Garcia, and F. Garcia-Sanchez,\n‘‘Semantic-based sentiment analysis in financial news,’’ in Proc. 1st Int.\nWorkshop Finance Econ. Semantic Web, 2012, pp. 38–51.\n[17] M. Du, X. Li, and L. Luo, ‘‘A training-optimization-based method for\nconstructing domain-specific sentiment lexicon,’’ Complexity, vol. 2021,\npp. 1–11, Feb. 2021.\n[18] M. Vicari and M. Gaspari, ‘‘Analysis of news sentiments using natural\nlanguage processing and deep learning,’’ AI Soc., vol. 36, no. 3,\npp. 931–937, Sep. 2021.\n[19] J. A. García-Díaz, M. Cánovas-García, and R. Valencia-García,\n‘‘Ontology-driven aspect-based sentiment analysis classification:\nAn infodemiological case study regarding infectious diseases in Latin\nAmerica,’’Future Gener. Comput. Syst., vol. 112, pp. 641–657, Nov. 2020.\n[20] Y . R. Tausczik and J. W. Pennebaker, ‘‘The psychological meaning of\nwords: LIWC and computerized text analysis methods,’’ J. Lang. Social\nPsychol., vol. 29, no. 1, pp. 24–54, Mar. 2010.\n[21] M. del Pilar Salas-Zárate, E. López-López, R. Valencia-García,\nN. Aussenac-Gilles, Á. Almela, and G. Alor-Hernández, ‘‘A study\non LIWC categories for opinion mining in Spanish reviews,’’ J. Inf. Sci.,\nvol. 40, no. 6, pp. 749–760, Dec. 2014.\n[22] M. D. P. Salas-Zárate, M. A. Paredes-Valverde, M. Á. Rodriguez-García,\nR. Valencia-García, and G. Alor-Hernández, ‘‘Automatic detection of\nsatire in Twitter: A psycholinguistic-based approach,’’ Knowl.-Based Syst.,\nvol. 128, pp. 20–33, Jul. 2017.\n[23] C. Periñán-Pascual, ‘‘Measuring associational thinking through word\nembeddings,’’Artif. Intell. Rev., vol. 55, no. 3, pp. 2065–2102, Mar. 2022.\n[24] S. Kardakis, I. Perikos, F. Grivokostopoulou, and I. Hatzilygeroudis,\n‘‘Examining attention mechanisms in deep learning models for sentiment\nanalysis,’’Appl. Sci., vol. 11, no. 9, p. 3883, Apr. 2021.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, N. A. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc.\nAdv. Neural Inf. Process. Syst., I. Guyon, U. V . Luxburg, S. Bengio,\nH. M. Wallach, R. Fergus, S. V . N. Vishwanathan, and R. Garnett, Eds.\nLong Beach, CA, USA, 2017, pp. 5998–6008.\n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang.\nTechnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[27] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[28] D. Araci, ‘‘FinBERT: Financial sentiment analysis with pre-trained\nlanguage models,’’ 2019, arXiv:1908.10063.\n[29] Z. Liu, D. Huang, K. Huang, Z. Li, and J. Zhao, ‘‘FinBERT: A\npre-trained financial language representation model for financial text\nmining,’’ in Proc. 29th Int. Joint Conf. Artif. Intell., Jul. 2020,\npp. 4513–4519.\n[30] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n‘‘ALBERT: A lite BERT for self-supervised learning of language\nrepresentations,’’ in Proc. 8th Int. Conf. Learn. Represent., Addis Ababa,\nEthiopia, Apr. 2020, pp. 1–17.\n[31] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nA. Barua, and C. Raffel, ‘‘MT5: A massively multilingual pre-trained text-\nto-text transformer,’’ in Proc. Conf. North Amer. Chapter Assoc. Comput.\nLinguistics, Human Lang. Technol., 2021, pp. 483–498.\n[32] S. O. Angel, A. P. P. Negrón, and A. Espinoza-Valdez, ‘‘Systematic\nliterature review of sentiment analysis in the Spanish language,’’ Data\nTechnol. Appl., vol. 55, no. 4, pp. 461–479, Aug. 2021.\n[33] L. J. Gandía and D. Huguet, ‘‘Análisis textual y del sentimiento en\ncontabilidad: Textual analysis and sentiment analysis in accounting,’’\nRevista de Contabilidad-Spanish Accounting Rev., vol. 24, no. 2,\npp. 168–183, Jul. 2021.\n[34] A. Mittal and A. Goel, ‘‘Stock prediction using Twitter sentiment\nanalysis,’’ Dept. Manage. Sci. Eng., Standford Univ., Standford, CA, USA,\nTech. Rep., CS229, 15, 2012.\n[35] T. Rao and S. Srivastava, ‘‘Analyzing stock market movements using\nTwitter sentiment analysis,’’ in Proc. ASONAM, 2012, pp. 119–123.\n[36] P. Uhr, J. Zenkert, and M. Fathi, ‘‘Sentiment analysis in financial markets\na framework to utilize the human ability of word association for analyzing\nstock market news reports,’’ in Proc. IEEE Int. Conf. Syst., Man, Cybern.\n(SMC), Oct. 2014, pp. 912–917.\n[37] S. Sohangir, D. Wang, A. Pomeranets, and T. M. Khoshgoftaar, ‘‘Big data:\nDeep learning for financial sentiment analysis,’’ J. Big Data, vol. 5, no. 1,\npp. 1–25, Jan. 2018.\n[38] A. Picasso, S. Merello, Y . K. Ma, L. Oneto, and E. Cambria, ‘‘Technical\nanalysis and sentiment embeddings for market trend prediction,’’ Exp. Syst.\nAppl., vol. 135, pp. 60–70, Nov. 2019.\n[39] F. Xing, L. Malandri, Y . Zhang, and E. Cambria, ‘‘Financial sentiment\nanalysis: An investigation into common mistakes and silver bullets,’’\nin Proc. 28th Int. Conf. Comput. Linguistics, Barcelona, Spain, 2020,\npp. 978–987.\n[40] N. Jing, Z. Wu, and H. Wang, ‘‘A hybrid model integrating deep learning\nwith investor sentiment analysis for stock price prediction,’’ Exp. Syst.\nAppl., vol. 178, Sep. 2021, Art. no. 115019.\n[41] Y . Shi, Y . Zheng, K. Guo, and X. Ren, ‘‘Stock movement prediction\nwith sentiment analysis based on deep learning networks,’’ Concurrency\nComput., Pract. Exper., vol. 33, no. 6, p. e6076, Mar. 2021.\n[42] J. Hu, Y . Sui, and F. Ma, ‘‘The measurement method of investor sentiment\nand its relationship with stock market,’’ Comput. Intell. Neurosci.,\nvol. 2021, pp. 1–11, Mar. 2021.\n[43] J. Fernando Sánchez-Rada, M. Torres, C. A. Iglesias, R. Maestre,\nand E. Peinado, ‘‘A linked data approach to sentiment and emotion\nanalysis of Twitter in the financial domain,’’ in Proc. 2nd Int.\nWorkshop Semantic Web Enterprise Adoption Best Practice 2nd\nInt. Workshop Finance Econ. Semantic Web Co-Located 11th Eur.\nSemantic Web Conf., vol. 1240, A. García-Crespo, J. M. Gómez-Berbís,\nM. Radzimski, J. L. Sánchez-Cervantes, S. Coppens, K. Hammar,\nM. Knuth, M. Neumann, D. Ritze, and M. V . Sande, Eds. Anissaras,\nGreece, 2014, pp. 1–12.\n[44] J. P. Braña, M. J. A. Litterio, and A. Fernández, ‘‘Fsal: Lexicón financiero\nde sentimiento en español rioplatense diseñado para ‘bolsas y mercados\nargentinos’ (BYMA),’’ Revista Abierta de Informática Aplicada, vol. 2,\npp. 5–22, Sep. 2021.\n[45] I. M. Bernal and C. G. Pedraz, ‘‘Sentiment analysis of the Span-\nish financial stability report,’’ Banco de España, Madrid, España,\nTech. Rep. 2011, 2020.\n[46] K. Mishev, A. Gjorgjevikj, I. V odenska, L. T. Chitkushev, and D. Trajanov,\n‘‘Evaluation of sentiment analysis in finance: From lexicons to transform-\ners,’’IEEE Access, vol. 8, pp. 131662–131682, 2020.\n[47] J. A. García-Diaz, A. Lmela, G. Alcaraz-Mármol, and R. Valencia-García,\n‘‘Umucorpusclassifier: Compilation and evaluation of linguistic corpus for\nnatural language processing tasks,’’ Procesamiento del Lenguaje Natural,\nvol. 65, pp. 139–142, Jan. 2020.\nVOLUME 11, 2023 14223\nJ. A. García-Díaz et al.: Smart Analysis of Economics Sentiment in Spanish Based on LFs and Transformers\n[48] J. A. García-Díaz, M. P. Salas-Zárate, M. L. Hernández-Alcaraz,\nR. Valencia-García, and J. M. Gomez-Berbís, ‘‘Machine learning based\nsentiment analysis on Spanish financial tweets,’’ in Proc. World Conf. Inf.\nSyst. Technol.Cham, Switzerland: Springer, 2018, pp. 305–311.\n[49] K. Krippendorff, ‘‘Reliability in content analysis: Some common mis-\nconceptions and recommendations,’’ Hum. Commun. Res., vol. 30, no. 3,\npp. 411–433, Jul. 2004.\n[50] J. A. García-Díaz and R. Valencia-García, ‘‘Compilation and evaluation of\nthe Spanish SatiCorpus 2021 for satire identification using linguistic fea-\ntures and transformers,’’Complex Intell. Syst., vol. 8, no. 2, pp. 1723–1736,\nApr. 2022.\n[51] J. A. García-Díaz, M. Cánovas-García, R. Colomo-Palacios, and\nR. Valencia-García, ‘‘Detecting misogyny in Spanish tweets. An approach\nbased on linguistics features and word embeddings,’’ Future Gener.\nComput. Syst., vol. 114, pp. 506–518, Jan. 2021.\n[52] P. Qi, Y . Zhang, Y . Zhang, J. Bolton, and C. D. Manning, ‘‘Stanza: A Python\nnatural language processing toolkit for many human languages,’’ in Proc.\n58th Annu. Meeting Assoc. Comput. Linguistics, Syst. Demonstrations,\n2020, pp. 101–108.\n[53] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efficient estimation\nof word representations in vector space,’’ in Proc. 1st Int. Conf. Learn.\nRepresent. (ICLR), Y . Bengio and Y . LeCun, Eds. Scottsdale, AZ, USA,\n2013, pp. 1–12.\n[54] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, ‘‘Advances\nin pre-training distributed word representations,’’ in Proc. Int. Conf. Lang.\nResour. Eval. (LREC), 2018, pp. 1–4.\n[55] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[56] K. Krasnowska-Kieraś and A. Wróblewska, ‘‘Empirical linguistic study\nof sentence embeddings,’’ in Proc. 57th Annu. Meeting Assoc. Comput.\nLinguistics, Florence, Italy, 2019, pp. 5729–5739.\n[57] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov, ‘‘Learning\nword vectors for 157 languages,’’ in Proc. 11th Int. Conf. Lang. Resour.\nEval. (LREC), Miyazaki, Japan, May 2018, pp. 1–5.\n[58] N. Reimers and I. Gurevych, ‘‘Sentence-BERT: Sentence embeddings\nusing Siamese BERT-networks,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process.\n(EMNLP-IJCNLP), 2019, pp. 1–11.\n[59] O. Sagi and L. Rokach, ‘‘Ensemble learning: A survey,’’ WIREs Data\nMining Knowl. Discovery, vol. 8, no. 4, p. e1249, Jul. 2018.\n[60] M. G. Vega, M. C. Díaz-Galiano, M. G. Cumbreras, F. M. P. D. Arco,\nA. Montejo-Ráez, S. M. J. Zafra, E. M. Cámara, C. A. Aguilar,\nM. A. S. Cabezudo, L. Chiruzzo, and D. Moctezuma, ‘‘Overview of\nTASS 2020: Introducing emotion detection,’’ in Proc. Iberian Lang.\nEval. Forum Co-Located 36th Conf. Spanish Soc. Natural Lang. Process.\n(SEPLN), vol. 2664, M. A. G. Cumbreras, J. Gonzalo, E. M. Cámara,\nR. Martínez-Unanue, P. Rosso, S. M. J. Zafra, J. A. O. Zambrano,\nA. Miranda, J. P. Zamorano, Y . Gutiérrez, A. Rosá, M. M. Gómez, and\nM. G. Vega, Eds. Malaga, Spain, 2020, pp. 163–170.\n[61] N. Patel and S. Upadhyay, ‘‘Study of various decision tree pruning methods\nwith their empirical comparison in WEKA,’’ Int. J. Comput. Appl., vol. 60,\nno. 12, pp. 20–25, Dec. 2012.\n[62] F. García-Sánchez, R. Colomo-Palacios, and R. Valencia-García,\n‘‘A social-semantic recommender system for advertisements,’’ Inf.\nProcess. Manag., vol. 57, no. 2, Mar. 2020, Art. no. 102153.\nJOSÉ ANTONIO GARCÍA-DÍAZ received the\nB.E., M.Sc., and Ph.D. degrees in computer\nscience from the University of Murcia, Espinardo,\nSpain. He is currently a Post-Doctoral with the\nDepartment of Informatics and Systems, Univer-\nsity of Murcia. He has participated in more than ten\nresearch projects. He has published over 40 articles\nin journals, conferences, and book chapters. His\nresearch interests include natural language pro-\ncessing technologies focused on automatic text\nclassification, sentiment and emotion analysis and hate speech detection. In\naddition, he has been part of the organizing committee for shared-tasks at\ninternational conferences such as IberLEF or EvalITA.\nFRANCISCO GARCÍA-SÁNCHEZ received the\nB.E., M.Sc., and Ph.D. degrees in computer\nscience from the University of Murcia, Murcia,\nSpain, in 2003, 2005, and 2007, respectively.\nFrom May 2012 to January 2017, he worked as\nthe Vice Dean of External Relations with the\nFaculty of Computer Science. Formerly, he was\na Ph.D. Assistant Professor with the Escola\nSuperior Tècnica d’Enginyeria (ETSE), University\nof Valencia. He is currently an Associate Professor\nwith the Department of Informatics and Systems, University of Murcia.\nHe has taken part both as a principal investigator and a researcher in several\nresearch projects related to the application of semantic web technologies\nto real world challenges. He has published over 70 articles in journals,\nconferences, and book chapters. He has conducted a number of research\nstays in some of the most prestigious, semantic web- and AI-concerned\nresearch institutes around the world, such as the Semantic Technology\nInstitute (STI, formerly the Digital Enterprise Research Institute, DERI),\nthe Centre for Information Technology Research (CITR), and the Stanford\nResearch Institute (SRI). His research interests include semantic web-\nbased applications, including ontologies, linked data and knowledge graphs,\nnatural language processing, semantic service-oriented architectures, social\nsemantic web, and the application of AI technologies (including machine and\ndeep learning).\nRAFAEL VALENCIA-GARCÍAreceived the B.E.,\nM.Sc., and Ph.D. degrees in computer science\nfrom the University of Murcia, Espinardo, Spain.\nHe is currently a Full Professor with the Depart-\nment of Informatics and Systems, University\nof Murcia. He has participated in more than\n35 research projects. He has published over\n150 articles in journals, conferences, and book\nchapters, 50 of them in JCR-indexed journals.\nHe is the author or coauthor of several books. His\nresearch interests include natural language processing, semantic web, and\nrecommender systems. He has been a Guest Editor of five JCR-indexed\njournals, such as CSI, IJSEKE, JRPIT, JUCS, and SCP.\n14224 VOLUME 11, 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7991542816162109
    },
    {
      "name": "Sentiment analysis",
      "score": 0.759224534034729
    },
    {
      "name": "Natural language processing",
      "score": 0.6753191351890564
    },
    {
      "name": "Transformer",
      "score": 0.6733621954917908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6077668070793152
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5575602054595947
    },
    {
      "name": "Natural language",
      "score": 0.4319199323654175
    },
    {
      "name": "Linguistics",
      "score": 0.38966378569602966
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I80180929",
      "name": "Universidad de Murcia",
      "country": "ES"
    }
  ]
}