{
  "title": "Improving the Adversarial Transferability of Vision Transformers with Virtual Dense Connection",
  "url": "https://openalex.org/W4393159840",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100390082",
      "name": "Jianping Zhang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5076825405",
      "name": "Yizhan Huang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5056204625",
      "name": "Zhuoer Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100704704",
      "name": "Weibin Wu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5069596903",
      "name": "Michael R. Lyu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2991240978",
    "https://openalex.org/W3089060762",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W6794449061",
    "https://openalex.org/W6747220948",
    "https://openalex.org/W2926400157",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6698183232",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W2274287116",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6792240715",
    "https://openalex.org/W3196621661",
    "https://openalex.org/W3034176567",
    "https://openalex.org/W3035520720",
    "https://openalex.org/W3177184533",
    "https://openalex.org/W2969772318",
    "https://openalex.org/W4384345715",
    "https://openalex.org/W4385890075",
    "https://openalex.org/W4361230822",
    "https://openalex.org/W4385764993",
    "https://openalex.org/W6803497561",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3211328899",
    "https://openalex.org/W2969542116",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W3109877713",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W4386076570",
    "https://openalex.org/W2963744840",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4287122830",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2976752987",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W4386072305",
    "https://openalex.org/W4312790346",
    "https://openalex.org/W3107235539",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4308245819",
    "https://openalex.org/W4310698795",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3006076803",
    "https://openalex.org/W4380993255",
    "https://openalex.org/W2774644650",
    "https://openalex.org/W4214669216",
    "https://openalex.org/W2964350391"
  ],
  "abstract": "With the great achievement of vision transformers (ViTs), transformer-based approaches have become the new paradigm for solving various computer vision tasks. However, recent research shows that similar to convolutional neural networks (CNNs), ViTs are still vulnerable to adversarial attacks. To explore the shared deficiency of models with different structures, researchers begin to analyze the cross-structure adversarial transferability, which is still under-explored. Therefore, in this work, we focus on the ViT attacks to improve the cross-structure transferability between the transformer-based and convolution-based models. Previous studies fail to thoroughly investigate the influence of the components inside the ViT models on adversarial transferability, leading to inferior performance. To overcome the drawback, we launch a motivating study by linearly down-scaling the gradients of components inside the ViT models to analyze their influence on adversarial transferability. Based on the motivating study, we find that the gradient of the skip connection most influences transferability and believe that back-propagating gradients from deeper blocks can enhance transferability. Therefore, we propose the Virtual Dense Connection method (VDC). Specifically, without changing the forward pass, we first recompose the original network to add virtual dense connections. Then we back-propagate gradients of deeper Attention maps and Multi-layer Perceptron (MLP) blocks via virtual dense connections when generating adversarial samples. Extensive experiments confirm the superiority of our proposed method over the state-of-the-art baselines, with an 8.2% improvement in transferability between ViT models and a 7.2% improvement in cross-structure transferability from ViTs to CNNs.",
  "full_text": "Improving the Adversarial Transferability of Vision Transformers with Virtual\nDense Connection\nJianping Zhang1, Yizhan Huang1, Zhuoer Xu2, Weibin Wu3*, Michael R. Lyu1\n1Department of Computer Science and Engineering, The Chinese University of Hong Kong\n2Tiansuan Lab, Antgroup\n3School of Software Engineering, Sun Yat-sen University\n{jpzhang, yzhuang22, lyu}@cse.cuhk.edu.hk, xuzhuoer.xze@antgroup.com, wuwb36@mail.sysu.edu.cn\nAbstract\nWith the great achievement of vision transformers (ViTs),\ntransformer-based approaches have become the new\nparadigm for solving various computer vision tasks. How-\never, recent research shows that similar to convolutional\nneural networks (CNNs), ViTs are still vulnerable to adver-\nsarial attacks. To explore the shared deficiency of models\nwith different structures, researchers begin to analyze the\ncross-structure adversarial transferability, which is still\nunder-explored. Therefore, in this work, we focus on the\nViT attacks to improve the cross-structure transferability\nbetween the transformer-based and convolution-based\nmodels. Previous studies fail to thoroughly investigate the\ninfluence of the components inside the ViT models on\nadversarial transferability, leading to inferior performance.\nTo overcome the drawback, we launch a motivating study\nby linearly down-scaling the gradients of components inside\nthe ViT models to analyze their influence on adversarial\ntransferability. Based on the motivating study, we find\nthat the gradient of the skip connection most influences\ntransferability and believe that back-propagating gradients\nfrom deeper blocks can enhance transferability. There-\nfore, we propose the Virtual Dense Connection method\n(VDC). Specifically, without changing the forward pass, we\nfirst recompose the original network to add virtual dense\nconnections. Then we back-propagate gradients of deeper\nAttention maps and Multi-layer Perceptron (MLP) blocks\nvia virtual dense connections when generating adversarial\nsamples. Extensive experiments confirm the superiority of\nour proposed method over the state-of-the-art baselines, with\nan 8.2% improvement in transferability between ViT models\nand a 7.2% improvement in cross-structure transferability\nfrom ViTs to CNNs.\nIntroduction\nTransformers have become the dominant solutions in the\nnatural language processing field with the state-of-the-art\nperformance on various downstream tasks. Vision trans-\nformers (ViTs) (Dosovitskiy et al. 2020) first adapt the self-\nattention mechanism of the transformers (Vaswani et al.\n2017) to the computer vision field for image recognition.\nSubsequently, diverse transformer-based approaches (Tou-\nvron et al. 2021a; Heo et al. 2021) have been proposed to\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nbetter adapt the transformer structure to the computer vision\nfield. Nowadays, ViTs have become the new paradigm for\nsolving various vision tasks such as object detection (Zhang\net al. 2021) and semantic segmentation (Zheng et al. 2021),\nwith competitive performance compared with convolutional\nneural networks (CNNs).\nRecent research reveals that both convolution-based and\ntransformer-based models are vulnerable to adversarial at-\ntacks (Wu et al. 2020c; Zhang et al. 2023b). Adversarial at-\ntacks inject human-imperceptible noise into the original im-\nage to mislead the deep neural network (DNN) models with\nhigh confidence. This phenomenon raises security concerns\nwith the wide application of deep neural networks (Zhang\net al. 2023c,d; Wu et al. 2019). Furthermore, the adversar-\nial examples crafted by the attacking algorithms manifest\nadversarial transferability. That is, the adversarial examples\ngenerated from a local surrogate model have the ability to\nmislead the target victim model (Wu et al. 2020b; Zhang\net al. 2022). Therefore, adversarial transferability provides\nan efficient way to craft adversarial examples for testing the\nvictim models without any access to the victim model un-\nder the black-box setting. Since victim models are usually\ndeployed in the black-box setting, it is imperative to devise\ntransferable attacking algorithms to assess their robustness\nbefore their deployment in real-world applications.\nThe transfer-based attacks have achieved high attack\nsuccess rates against convolution-based models. Never-\ntheless, recent studies have discovered the robustness of\nthe transformer-based models and the low cross-structure\ntransferability, when we transfer the adversarial examples\ngenerated by attacking transformer-based models to mis-\nlead convolution-based models or vice versa (Zhang et al.\n2023b). Some research believes that the low cross-structure\ntransferability is due to the model structure difference\nbetween transformer-based models and convolution-based\nones (Naseer et al. 2021). Convolution-based models utilize\nthe convolutional layers to capture the local information of\nthe input features in a small receptive field (Luo et al. 2016).\nTransformer-based models divide the input image into small\npatches and feed a sequence of small patches into the net-\nwork. With the help of the self-attention mechanism, ViT\nmodels can learn the global features at each stage of the\nnetwork, which shows distinct properties to CNN models.\nTherefore, enhancing the adversarial transferability from\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7133\ntransformer-based models to other transformer-based and\nconvolution-based models is of great significance, which\nfacilitates finding the common defects inside transformer-\nbased and convolution-based models in practice.\nHowever, the adversarial transferability of transformer-\nbased models is still under-explored. Although some works\nhave been proposed to improve the adversarial transferabil-\nity based on the special design of the transformer-based\nmodels, they still fail to thoroughly investigate the influ-\nence of the components inside the ViT models on adversar-\nial transferability, leading to inferior performance. To over-\ncome the drawback, we launch a motivating study by lin-\nearly down-scaling the gradient of selected components in-\nside the transformer-based models to find their influences on\nadversarial transferability. Based on the motivating study,\nwe find that the skip connections influence transferability\nthe most and believe that back-propagating deeper gradi-\nents to generate adversarial samples can boost their transfer-\nability. Therefore, we propose the Virtual Dense Connection\nmethod (VDC). Specifically, without changing the forward\npass, we first recompose the original model to add virtual\ndense connections. We then densely back-propagate gradi-\nents of Attention maps and Multi-layer Perceptron (MLP)\nblocks via virtual dense connections to generate adversar-\nial samples. Extensive experiments show that our proposed\napproach significantly outperforms the state-of-the-art base-\nlines by an 8.2% improvement in the transferability between\ntransformer-based models and a 7.2% improvement in the\ncross-structure transferability.\nIn summary, the contributions of this paper are:\n• We launch a motivating study to analyze the influence\nof each component inside the transformer-based mod-\nels on adversarial transferability. To this end, we linearly\ndown-scale the gradient of each component to observe\nthe transferability changes. We find that the gradient of\nthe skip connection most influences the adversarial trans-\nferability.\n• Based on the motivating study, we believe that back-\npropagating the gradient from deeper blocks to gener-\nate adversarial samples can improve their transferabil-\nity. Therefore, we propose the Virtual Dense Connection\nmethod (VDC). VDC recomposes the original network to\nadd virtual dense connections and then back-propagates\ngradients via virtual dense connections to generate trans-\nferable adversarial samples.\n• Extensive experiments confirm that our method can out-\nperform the state-of-the-art attacking approaches by a\nmargin of 8.2% on the transferability between ViT mod-\nels, and 7.2% on the cross-structure transferability from\nViT models to CNN models.\nRelated Work\nTransfer-based Adversarial Attacks\nThe transfer-based adversarial attack is one category of ad-\nversarial attacks under the black-box setting, which is built\non the transferability of adversarial examples. Transferabil-\nity is the phenomenon that the adversarial examples crafted\nby a local surrogate model can also mislead the target vic-\ntim model. Therefore, black-box attackers can generate ad-\nversarial examples of a fully accessible surrogate model by\nwhite-box attacking algorithms and directly transfer the ex-\namples to the target victim model. Representative white-box\nattacks include Fast Gradient Sign Method (FGSM) (Good-\nfellow, Shlens, and Szegedy 2014) and Project Gradient De-\nscent (PGD) (Madry et al. 2017). However, those white-box\napproaches reveal limited transferability, because the adver-\nsarial examples are model-specific and fail to mislead other\nmodels.\nTherefore, researchers begin to boost the transferability of\nadversarial examples. The current state-of-the-art transfer-\nbased attacks can be roughly classified into two trends:\ngradient-based approaches, and input transformation-based\napproaches. The gradient-based approaches utilize advanced\noptimizers (Dong et al. 2018), or model structures (Wu et al.\n2020a; Xu et al. 2023; Deng et al. 2023) to modify the gra-\ndient to escape from the local optima and stabilize the up-\ndate gradient. Momentum Iterative Method (MIM) (Dong\net al. 2018) combines the momentum optimizer with the\nBIM to improve the adversarial transferability. Skip Gra-\ndient Method (SGM) (Wu et al. 2020a) utilizes the skip\nconnection in the model structure to improve the trans-\nferability. SGM uses a decay factor to reduce the gradi-\nent from the residual module and focuses on the transfer-\nable low-level information to regularize the gradient. Input\ntransformation-based approaches combine the gradients of\nthe transformed images for generating transferable perturba-\ntion (Wu et al. 2021; Dong et al. 2019; Lin et al. 2019; Zhang\net al. 2023a). Although those approaches have achieved\nstate-of-the-art performance on boosting the transferability\nof convolution-based models, their performance drops dra-\nmatically on increasing the transferability of transformer-\nbased models, because of the model structure difference\nbetween convolution-based models and transformer-based\nmodels.\nAnother category of black-box attacks is query-based at-\ntacks (Andriushchenko et al. 2020; Bai et al. 2020; Wu\net al. 2023). However, query-based attacks require additional\nqueries to the victim model, which lacks in efficiency in the\nreal-world scenarios. Therefore, we focus on transfer-based\nattacks in this paper.\nTransformer-based Models\nThe transformer is a neural network architecture utilizing the\nself-attention mechanism originating from the natural lan-\nguage processing field. Recently, the transformer design has\nbeen adapted into the computer vision field. Vision trans-\nformers (ViTs) (Dosovitskiy et al. 2020) divide the input\nimage into a sequence of small image patches similar to a\nsequence of tokens for the language model. ViTs capture the\nrelationship between image patches based on the multi-head\nself-attention mechanism. Besides the basic version of the\nViT, advanced ViTs have been proposed to enhance the per-\nformance of ViTs on computer vision tasks. The pooling-\nbased vision transformer (PiT) (Heo et al. 2021) decreases\nthe spatial dimension and increases the channel dimension\nwith pooling to improve the model capability. The data-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7134\nFigure 1: Illustration of the down-scaled gradients in the\ntransformer block. All the dashed lines (red & black) de-\npict the normally back-propagated gradients. The red dashed\nlines represent the selected gradient to analyze their influ-\nence on adversarial transferability.\nefficient Vision Transformer (DeiT) (Touvron et al. 2021a)\ndeploys a distillation token to learn knowledge from CNNs.\nThe vision-friendly transformer (Visformer) (Chen et al.\n2021) transits a transformer-based model to a convolution-\nbased model.\nWith the development of transformer-based models, some\nresearchers (Bhojanapalli et al. 2021; Shao et al. 2021) as-\nsess the robustness of ViTs based on white-box and black-\nbox attacks. Other research (Mahmood, Mahmood, and\nVan Dijk 2021) also finds that the cross-structure transfer-\nability from transformer-based to convolution-based models\nis low. To understand the influence of the components in the\nnetwork on adversarial transferability, we launch a motivat-\ning study to explore the influence of the gradient from each\nnetwork component.\nAttacks on Transformer-based Models\nResearchers aim to improve transferability by exploring the\nunique structure of transformer-based models. Naseer et al.\nproposed Self-Ensemble (SE) to utilize the class token on\neach layer of ViTs with a shared classification head for the\ngradient ensemble and Token Refinement module (TR) to re-\nfine the class token with fine-tuning (Naseer et al. 2021). The\nPay No Attention (PNA) (Wei et al. 2022) method explores\nthe attention mechanism and skips the gradient of the at-\ntention during back-propagation to improve the transferabil-\nity of adversarial examples. Although previous approaches\nutilize the special architecture of transformer-based models\nfor transferable adversarial attacks, they fail to thoroughly\nexplore the influence of each component on adversarial\ntransferability, leading to limited improvement of transfer-\nability. Unlike previous methods, we analyze the influence\nof the gradient from each component in the transformer-\nbased models on adversarial transferability with a motivat-\ning study, and then design an effective attacking method ac-\ncordingly.\nMotivating Study\nIn this motivating study, we analyze the influence of the gra-\ndient from each component in the transformer-based mod-\nels on adversarial transferability. We select a representa-\ntive transformer-based model, ViT-B/16 (Dosovitskiy et al.\n2020), as the source model to craft adversarial examples.\nBlock Component ViT CNN Adv-CNN\nAttention\nQKV 45.4 24.1 16.6\nAttention Map 64.9 35.8 24.5\nskip Connection 19.0 13.3 8.8\nMLP MLP Layer 44.6 24.5 17.3\nSkip Connection 17.5 11.7 7.1\nTable 1: The average adversarial transferability (%) against\nViTs, CNNs, and adversarially-trained CNNs by scaling the\ngradients of different components in ViT-B/16.\nWe then measure the average transferability of the gener-\nated adversarial examples to multiple transformer-based and\nconvolution-based models. The details of the target models\nare in Section . In order to reflect the influence of each com-\nponent’s gradient on transferability, we follow the idea of at-\ntribution (Sundararajan, Taly, and Yan 2017). Therefore, we\ngradually down-scale the gradient and compute the average\nadversarial transferability during the down-scaling process.\nSpecifically, we down-scale the gradient from each com-\nponent using a linear sampling strategy, where we down-\nscale the gradient from 1 to 0 with a step size of 0.25.\nThe transformer-based models consist of several transformer\nblocks. Each transformer block contains an Attention block\nand a MLP block. The Attention block first computes the\nQKV values and the attention map by the product of the\nquery and key. Then the Attention block outputs the multi-\nplication of the attention map and the QKV value. The MLP\nblock passes the input through fully connected layers. Both\nthe input and the output of the Attention block and MLP\nblock are connected with a skip connection. Therefore, the\ncomponents we select are QKV , attention map, the skip con-\nnection from the Attention block, MLP layers, and the skip\nconnection from the MLP block, as shown in Figure 1.\nWe gradually down-scale the gradient of a selected com-\nponent, fixing the other back-propagated gradients and com-\nputing the average transferability during the down-scaling\nprocess. As we can see from Table 1, the adversarial trans-\nferability drops dramatically with the reduction of the gra-\ndient from skip connections in the Attention block or the\nMLP block. Thus, we believe that the skip connection in-\nside the transformer-based models influences the adversarial\ntransferability the most. This phenomenon implies that the\ngradient from the deeper block through the skip connection\nenhances the adversarial transferability, which motivates us\nto back-propagate more gradients from deeper blocks to im-\nprove the adversarial transferability.\nMethod\nPreliminary\nWe first set up some notations adopted in this paper. We re-\ngard a DNN image classifier as f(·). Given a sequence of\nimage patches xp = {x1\np, x1\np, ··· , xN\np } divided from the\noriginal image x with a shape of H × W × C, f(x) is the\noutput of the image classifier. H, W, and C are the origi-\nnal image’s height, width, and channel number, respectively.\nxi\np denotes the i-th patch of the original image. The patch\nshape is P × P × C, where P is the predefined patch size.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7135\nFigure 2: Illustration of model reparametrization by adding\nvirtual dense connections. The inputs of the current blocks\nare propagated to all later blocks via virtual dense connec-\ntions. We also modify the weights of the original block to\nguarantee that the input to the next block remains the same,\nkeeping the forward pass of the original model.\nMoreover, the total patch number N of an image is H·W\nP2 .\nWe set xadv as the adversarial example of image x with\ntrue label y. Adversarial examples satisfy two conditions:\nf(xadv) ̸= f(x) and\n\r\rx − xadv\r\n\r\np < ϵ.\nThe first condition implies that adversarial examples can\nmislead the image classifier with a wrong prediction. The\nsecond condition guarantees the difference between the ad-\nversarial example and the original image is smaller than a\nbudget ϵ, so it is hard for a human to detect the distortion.\n∥·∥p represents the Lp norm, and we measure the distortion\nby L∞ norm in this paper, which is widely adopted in the\nliterature (Dong et al. 2018).\nModel Recomposition\nIn order to utilize the gradient from deeper blocks, one intu-\nitive idea is to directly back-propagate the gradient through\nskip connections. However, there are no long-range connec-\ntions in transformer-based models. Thus, we propose to re-\ncompose the original model to add additional virtual con-\nnections.\nAs shown in the upper part of Figure 2, we suppose\nthere are n blocks in the network, and the output of blocki\nis zi = fi(zi−1) with input zi−1. Therefore, the out-\nput of the original network is zn = fn(zn−1) = ··· =\nfn(fn−1(··· f2(f1(x)) ··· )).\nThen, without changing the forward pass of the network,\nwe aim to recompose the original model to add virtual dense\nconnections. As shown in the lower part of Figure 2, we add\nvirtual dense connections so that the output of each block is\ndensely connected to the input of all the later blocks. There-\nfore, the additional input to blocki+1 through virtual dense\nconnections is x+z′\n1 +··· +z′\ni−1, because we densely prop-\nagate all the outputs of previous blocks (block 1 - blocki−1)\nand input x to the input of blocki+1. Since we should keep\nthe original forward pass of the model, we need to guarantee\nthe input to each block of the recomposed model remains\nthe same. Therefore, the function of blocki is changed from\nfi(zi−1) to f′\ni(zi−1) =fi(zi−1) − x − Pi−1\nk=1 z′\nk.\nAs a result, we recompose the original model to add vir-\ntual dense connections without changing the functionality\nof the original model. The transformation facilitates back-\npropagating more gradients from deep blocks to shallow\nblocks.\nVirtual Dense Connection Method\nBased on the observation in the motivating study, we think\nthat back-propagating more gradients from deeper blocks in\nthe network can enhance adversarial transferability. There-\nfore, our Virtual Dense Connection method (VDC) back-\npropagates more gradients through virtual dense connec-\ntions after model recomposition.\nFirst, we denote the gradient of blocki as gi = ∂fi(zi−1)\n∂zi−1\n.\nThen the gradient of the recomposed blocki is:\ng′\ni = ∂f ′\ni(zi−1)\n∂zi−1\n= ∂(fi(zi−1) − x − Pi−1\nk=1 z′\nk)\n∂zi−1\n(1)\nz′\ni−1 is the output of the recomposed blocki−1, and zi−1\nis the input to blocki. In the recomposed model with virtual\ndense connections, we have:\nzi−1 = x +\ni−1X\nk=1\nz′\nk. (2)\nTherefore, the gradient g′\ni = ∂(fi(zi−1)−zi−1)\n∂zi−1\n= gi − 1,\nwhere 1 is the identity matrix.\nTo craft adversarial perturbation, we compute the gradient\nof the loss to the input x of the recomposed model:\n∂loss\n∂x = ∂loss\n∂zn\n∂zn\n∂x = ∂loss\n∂zn\n∂(z′\nn + x + Pn−1\nk=1 z′\nk)\n∂x\n= ∂loss\n∂zn\n∂(f′\nn(zn−1) +x + Pn−1\nk=1 z′\nk)\n∂x\n= ∂loss\n∂zn\n∂(f′\nn(zn−1) +zn−1)\n∂x\n= ∂loss\n∂zn\n∂(f′\nn(zn−1) +zn−1)\n∂zn−1\n∂zn−1\n∂x\n= ∂loss\n∂zn\n(g′\ni + 1)∂zn−1\n∂x = ··· = ∂loss\n∂zn\nnY\nk=1\n(g′\nk + 1).\n(3)\nTo back-propagate more gradients from deeper blocks,\nVDC reduces the gradient inside recomposed blocks to\nback-propagate more gradients from deeper blocks via vir-\ntual dense connections. We utilize a factor 0 < λ <1 to\nreduce the gradient of each recomposed block. Therefore,\nthe updated gradient on the input is:\nGrad = ∂loss\n∂zn\nnY\nk=1\n(λg′\nk + 1) =∂loss\n∂zn\nnY\nk=1\n(λ(gk − 1) +1)\n= ∂loss\n∂zn\nnY\nk=1\n(λgk + (1− λ)1).\n(4)\nWe divide Grad by λn for simplicity and denote γ = 1−λ\nλ .\nThen the gradient is simplified to:\nGrad = ∂loss\n∂zn\nnY\nk=1\n(gk + γ1). (5)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7136\nNevertheless, it is hard to computeGrad because we cannot\ndirectly obtain the gradient gk inside each block. The com-\nputation of gk is expensive, which requiresO(H ×W ×C).\nInstead, we could acquire the gradient of the loss to the input\nof each block in O(1), which we denote as Gradi = ∂loss\n∂zi−1\n.\nWe expand the terms in Grad and denote their patterns\nby the expansion of gk or 1. For example, we denote the\nterm ∂loss\n∂zn\n(gk)(γ1) ··· (γ1) by the pattern [gk, 1, ··· , 1].\nFor the purpose of computing Grad in O(1), we only con-\nsider the terms in Grad with one consecutively skip, which\nmeans that there is only one consecutive substring of1 in the\npattern, and the previous example is one consecutively skip\nterm. Under one consecutive skip assumption, Grad can be\napproximated by fusing the Gradi with all Gradj, when\ni < j≤ n. Therefore, the combined gradient ConGradi of\nblocki can be expressed as follows:\nConGradi = Gradi + s ·\nnX\nj=i+1\nGradj · γn−j+1, (6)\nwhere we set a scaling factor 0 < s < 1 to control the\nratio of the back-propagated gradients from virtually con-\nnected deeper blocks. As a result, under the approximation\nassumption, the gradient from deeper blocks can be easily\nback-propagated in the backward pass and the computation\nof Grad in O(1). Finally, VDC updates the target image\nwith the sign of Grad by a small step size ϵ′ = ϵ\nT in each\niteration, where T is the iteration number. The update rule is\nformulated as:\nxadv\nt+1 = xadv\nt + ϵ′ · sgn{Grad}. (7)\nImplementation\nWe aim to implement our proposed VDC on transformer-\nbased models, taking the special design of ViTs into con-\nsideration. We demonstrate the components we select for\nutilizing VDC. The illustration of implementing VDC on\ntransformer-based models is shown in Figure 3.\nAttention Map.The Attention map is the core function-\nality of the transformer-based models, which computes the\nrelationship between image patches. Although the receptive\nfield of the transformer-based model is the whole image, the\ndeep blocks capture more high-level semantics compared\nwith shallow blocks (Dosovitskiy et al. 2020). The gradi-\nents of the attention map from deep blocks are meaningful\nbecause the gradients from deep blocks avoid overfitting to\nthe model. Therefore, we deploy VDC on Attention block to\ndensely connect the attention map in the Attention block.\nMLP Block. The MLP block is another indispensable\ncomponent in transformer-based models. Unlike the Atten-\ntion block, the MLP block aggregates the channel-wise in-\nformation of each patch. The skip connection of the MLP\nblock also shows the most influence on adversarial trans-\nferability in the motivating study. Therefore, we also apply\nVDC to the MLP block.\nFigure 3: Illustration of Virtual Dense Connection method.\nThe dark dash lines are the backward gradient through re-\ncomposed virtual connections on Attention maps and MLP\nblocks, which are in red and green dash lines to back-\npropagate more gradient from the deeper blocks.\nComparison with Previous Approaches\nWe recompose the original model without changing the for-\nward functionality of the original transformer-based mod-\nels and only modify the backward path through virtual\ndense connections to the Attention maps and MLP blocks.\nPrevious ViT attacking methods explore the structure of a\ntransformer-based model for boosting adversarial transfer-\nability (Wei et al. 2022). Nevertheless, they fail to investi-\ngate the advantages of each component in transformer-based\nmodels thoroughly. We do a motivating study to explore the\nbenefit of the skip connection and the gradient from deeper\nblocks.\nSGM (Wu et al. 2020a) assigns a decay factor on the\ngradients of residual modules to use more gradients from\nexisting skip connections. In contrast, VDC utilizes model\nrecomposition to construct virtual dense connections with-\nout changing the forward pass. Therefore, SGM can only\nbe applied to models with skip connections, while VDC\ndoes not rely on such specific model structures. Moreover,\nVDC can back-propagate more gradients from deeper blocks\nthrough virtual dense connections. For efficiency, we imple-\nment VDC under the one consecutive skip approximation to\ncompute the update gradient in O(1).\nExperiments\nIn this section, we first explain our experimental setup.\nThen we compare our approach with state-of-the-art ad-\nversarial attacks against transformer-based models and\nconvolution-based models to demonstrate the effectiveness\nof our approach on improving the transferability between\ntransformer-based models and the cross-structure transfer-\nability. Finally, we do an ablation study on the two compo-\nnents in our VDC as well as the hyper-parameters to under-\nstand our proposed approach better.\nExperimental Setup\nOur experiments mainly focus on the ImageNet dataset\n(Russakovsky et al. 2015) to attack image classification\nmodels, including transformer-based and convolution-based\nmodels. For fair comparisons, we follow the protocol (Wei\net al. 2022) in the literature for the model and dataset.\nDataset. To align with the previous work, we follow the\nbaseline method (Wei et al. 2022) to randomly sample 1000\nimages of different classes from the ILSVRC 2012 valida-\ntion set (Russakovsky et al. 2015). We ensure that almost\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7137\nModel Attack ViT-B/16 PiT-B DeiT-B Visformer-S CaiT-S/24 TNT-S LeViT-256 ConViT-B\nViT-B/16\nMIM 100.0 34.5 64.3 36.5 64.1 50.2 33.8 66.0\nSE 99.9 31.8 68.3 40.5 67.4 59.3 43.8 63.7\nSGM 100.0 34.3 72.8 38.3 72.2 59.4 39.8 75.0\nPNA 100.0 45.2 78.6 47.7 78.6 62.8 47.1 79.5\nVDC 100.0 54.8 85.8 57.4 84.1 74.8 58.1 85.9\nPiT-B\nMIM 24.7 100.0 33.9 44.5 34.7 43.0 38.3 37.8\nSE 31.7 99.8 40.9 52.1 42.2 52.6 47.3 44.9\nSGM 30.3 100.0 44.3 62.3 47.7 62.6 56.4 47.1\nPNA 47.9 100.0 62.4 74.6 62.6 70.6 67.3 61.7\nVDC 57.7 100.0 74.4 83.1 72.8 83.4 79.4 75.1\nDeiT-B\nMIM 86.3 68.4 100.0 71.9 97.7 89.8 68.3 98.3\nSE 91.6 93.7 99.9 82.7 98.4 94.6 80.7 97.8\nSGM 88.3 65.7 100.0 73.1 97.7 92.3 74.3 97.4\nPNA 91.0 74.2 100.0 82.5 98.1 94.4 80.1 98.4\nVDC 91.8 79.9 100.0 84.9 98.6 95.5 85.5 98.8\nVisformer-S\nMIM 28.1 50.3 36.9 99.9 41.0 51.9 49.4 39.6\nSE 35.2 57.0 46.2 99.6 49.4 59.1 56.4 45.4\nSGM 15.5 39.6 25.9 100.0 29.5 45.4 41.3 26.3\nPNA 35.4 61.5 51.0 100.0 54.7 66.3 64.6 50.7\nVDC 43.2 72.7 63.9 100.0 65.6 76.9 77.1 58.3\nTable 2: The attack success rates (%) against eight models by various transfer-based attacks. The best results are in bold.\nall of the selected images can be correctly classified by the\ntarget models.\nModels. We evaluate the transferability of adversarial ex-\namples of ViTs from two perspectives. We first assess the\ntransferability between transformer-based models. We select\nfour different transformer-based models as the local surro-\ngate models to attack eight target transformer-based mod-\nels, including the four surrogate models. The four surrogate\nmodels are ViT-B/16 (Dosovitskiy et al. 2020), PiT-B (Heo\net al. 2021), DeiT-B (Touvron et al. 2021a) , and Visformer-\nS (Chen et al. 2021). The additional four target models are\nCaiT-S/24 (Touvron et al. 2021b), TNT-S (Han et al. 2021),\nLeViT-256 (Graham et al. 2021), and ConViT-B (d’Ascoli\net al. 2021). We then evaluate the cross-structure trans-\nferability between transformer-based and convolution-based\nmodels. We choose two kinds of convolution-based models\nas the target models: normally trained undefended models\nand adversarially trained defended models. We select four\nundefended convolution-based models, including Inception-\nv3 (Inc-v3) (Szegedy et al. 2016), Inception-v4 (Inc-v4)\n(Szegedy et al. 2017), Inception-Resnet-v2 (IncRes-v2)\n(Szegedy et al. 2017), and Resnet-v2-152 (Res-v2) (He et al.\n2016a,b). We test three adversarially trained models (Tram`er\net al. 2017), including Inc-v3 ens3, Inc-v3 ens4, and IncRes-\nv2adv. Besides, we evaluate the cross-transferability from\nconvolution-based models to transformer-based models. We\nselect Resnet-v2, Densenet121 (Dense-121) (Huang et al.\n2017), and Mobilenetv3-small-075 (Mobile-v3) (Howard\net al. 2019) as the surrogate models and test the attack suc-\ncess rate on the eight transformer-based models.\nBaseline Methods. We choose MIM as our baseline ap-\nproach, because all the baseline methods utilize the momen-\ntum optimizer to enhance the transferability (Dong et al.\n2018). In order to show the advantages of our proposed\nVDC, we select SGM (Wu et al. 2020a) as our competi-\ntive baseline, which utilizes the skip connection structure in-\nside the network with a decay factor to reduce the gradient\nfrom the residual module. To show the state-of-the-art per-\nformance of our method, we compare our method with two\nstate-of-the-art attacking algorithms against transformer-\nbased models: PNA (Wei et al. 2022) and SE (Naseer et al.\n2021). PNA leverages the attention structure in the trans-\nformer block to craft transferable adversarial examples, and\nSE deploys the self-ensemble mechanism to augment the\ngradient. We do not compare VDC with TR (Naseer et al.\n2021), because TR requires more training data and compu-\ntation resources for fine-tuning on the Imagenet, which is\nunfair for performance comparison.\nEvaluation Metric. We measure the adversarial transfer-\nability based on the attack success rate. We compute the ra-\ntio of the adversarial examples that successfully mislead the\ntarget model among all the generated adversarial examples.\nHyper-parameters. We follow the hyper-parameter set-\nting of the baseline approaches in their implementations for\na fair comparison. Following the previous setting in the lit-\nerature (Wei et al. 2022), we set the budget ϵ = 16, with the\nimage pixel value ranging from 0 to 255. We pick the num-\nber of the iteration T = 10, so the step length α = ϵ\nT = 1.6.\nSince all the baselines utilize the momentum optimizer, we\nset the decay factor µ = 1.0. We resize all images to\n224×224 as the input and pick the patch size to be16 for the\ninputs of transformer-based models. For our proposed VDC,\nwe set the scaling factor and the decay factor to be 0.1 and\n0.5, respectively. Some transformer-based models have the\nsame resolution in the whole network, while the others have\ndifferent resolutions in different stages. Therefore, for the\nnetworks keeping the same resolution, we virtually connect\nall the blocks during the back-propagation. Otherwise, we\nonly virtually connect the blocks with the same resolution.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7138\nModel Attack Inc-v3 Inc-v4 IncRes-v2 Res-v2 Inc-v3 ens3 Inc-v3ens4 IncRes-v2adv\nViT-B/16\nMIM 31.7 28.6 26.1 29.4 22.3 19.8 16.5\nSE 40.8 40.0 31.5 38.8 31.0 30.5 23.8\nSGM 29.5 25.9 21.6 26.0 17.6 17.0 13.9\nPNA 42.7 37.5 35.3 39.5 29.0 27.3 22.6\nVDC 49.3 44.4 39.3 44.8 33.8 33.8 27.8\nPiT-B\nMIM 36.3 34.8 27.4 29.6 19.0 18.3 14.1\nSE 46.4 41.2 35.0 39.4 25.3 22.3 19.5\nSGM 39.8 35.4 29.8 30.8 18.1 16.4 11.5\nPNA 59.3 56.3 49.8 53.0 33.3 32.0 25.5\nVDC 68.2 60.3 57.0 59.5 42.2 39.8 32.3\nDeiT-B\nMIM 56.1 50.9 47.9 52.9 40.8 38.7 32.6\nSE 63.2 57.6 59.7 63.1 48.5 44.3 38.6\nSGM 52.1 45.8 43.2 46.9 31.8 31.5 27.2\nPNA 66.5 60.7 60.9 64.0 49.3 46.1 40.8\nVDC 69.9 63.0 63.8 65.8 53.3 52.0 45.3\nVisformer-S\nMIM 44.5 42.5 36.6 39.6 24.4 20.5 16.6\nSE 55.5 55.0 44.9 48.3 30.9 26.6 24.4\nSGM 33.1 32.7 24.6 26.2 11.7 9.4 6.9\nPNA 55.9 54.6 46.0 51.7 29.3 26.2 21.1\nVDC 71.9 69.8 60.9 65.0 40.8 34.8 28.3\nTable 3: The attack success rates (%) against seven models by various transfer-based attacks. The best results are in bold.\nExperimental Results\nWe present the experimental results of the adversarial trans-\nferability of our approach compared with baselines on dif-\nferent attacking settings. We craft adversarial examples by\nour approach and other baselines on the surrogate models\nand transfer the adversarial examples to target models. We\nmeasure the transferability between transformer-based mod-\nels and the cross-structure transferability from transformer-\nbased models to convolution-based models.\nWe first assess the transferability between transformer-\nbased models. We observe from Table 2 that, our proposed\nVDC achieves a 100% white-box attack success rate and\noutperforms all the baselines with a large margin of 8.2%\non average under the black-box setting. Compared with\nSGM, which utilizes the skip connection structure in the\nnetwork, our approach deploys virtual dense connections to\nthe deeper blocks exerting significant improvement on the\ntransferability. This result validates our assumption in the\nmotivating study that back-propagating more gradients from\ndeeper blocks can boost transferability and shows the ad-\nvantages of adding virtual dense connections in the back-\nward path. Compared with PNA and SE, which use differ-\nent architectures of the transformer-based models to enhance\ntransferability, our approach adds more connections virtu-\nally based on model recomposition. The remarkable perfor-\nmance also confirms the effectiveness of our selected archi-\ntecture components.\nMoreover, we evaluate the cross-structure transferability\nfrom transformer-based to convolution-based models. As\nshown in Table 3, the cross-structure transferability drops\ncompared with the transferability between transformer-\nbased models, due to the structure difference of models.\nCompared with baselines, our proposed VDC still enhances\nthe cross-structure transferability by over 7.2% on average,\nvalidating the superiority of the proposed VDC.\nComponent ViT CNN Adv-CNN\nNone 56.2 29.0 19.5\nMLP 66.0 34.4 24.1\nAttention 66.8 37.0 24.6\nAttention + MLP (VDC) 75.1 44.5 31.8\nTable 4: The average adversarial transferability (%) against\nViTs, CNNs, and adversarially trained CNNs by using dense\nconnection on different components in ViT-B/16.\nAblation Study\nWe do an ablation study to explore the contribution of the\ncomponents in VDC by attacking the ViT-B/16 model. We\ngenerate adversarial examples by different combinations of\nthe components in VDC and measure the transferability. The\nexperimental result is shown in Table 4. We can see that both\ndensely connecting the MLP blocks and the Attention maps\ncan enhance adversarial transferability. The transferability\nimprovement by densely connecting the MLP block is a little\nbit inferior than the Attention map, because the Attention\nmap is the core functionality in transformer-based models.\nConclusion\nIn this paper, we start with a motivating study to conclude\nthat back-propagating gradients from deeper blocks can en-\nhance transferability. We propose the Virtual Dense Con-\nnection method (VDC) to back-propagate more gradients\nfrom deeper blocks. Specifically, we recompose the origi-\nnal model to add virtual dense connections without chang-\ning the forward pass. Then we back-propagate gradients of\ndeeper Attention maps and MLP blocks via virtual dense\nconnections when generating adversarial samples. Extensive\nexperiments validate the superiority of our approach over the\nstate-of-the-art approaches.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7139\nAcknowledgments\nThe work described in this paper was supported by the\nNational Natural Science Foundation of China (Grant\nNo. 62206318) and the Research Grants Council of the\nHong Kong Special Administrative Region, China (CUHK\n14206921 of the General Research Fund).\nReferences\nAndriushchenko, M.; Croce, F.; Flammarion, N.; and Hein,\nM. 2020. Square attack: a query-efficient black-box adver-\nsarial attack via random search. In European Conference on\nComputer Vision, 484–501. Springer.\nBai, Y .; Zeng, Y .; Jiang, Y .; Wang, Y .; Xia, S.-T.; and Guo,\nW. 2020. Improving query efficiency of black-box adversar-\nial attack. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XXV 16, 101–116. Springer.\nBhojanapalli, S.; Chakrabarti, A.; Glasner, D.; Li, D.; Un-\nterthiner, T.; and Veit, A. 2021. Understanding robustness\nof transformers for image classification. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10231–10241.\nChen, Z.; Xie, L.; Niu, J.; Liu, X.; Wei, L.; and Tian,\nQ. 2021. Visformer: The vision-friendly transformer. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 589–598.\nDeng, Y .; Wu, W.; Zhang, J.; and Zheng, Z. 2023. Blurred-\nDilated Method for Adversarial Attacks. In Thirty-seventh\nConference on Neural Information Processing Systems.\nDong, Y .; Liao, F.; Pang, T.; Su, H.; Zhu, J.; Hu, X.; and Li,\nJ. 2018. Boosting adversarial attacks with momentum. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 9185–9193.\nDong, Y .; Pang, T.; Su, H.; and Zhu, J. 2019. Evading de-\nfenses to transferable adversarial examples by translation-\ninvariant attacks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 4312–\n4321.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nd’Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.;\nBiroli, G.; and Sagun, L. 2021. Convit: Improving vision\ntransformers with soft convolutional inductive biases. In In-\nternational Conference on Machine Learning, 2286–2296.\nPMLR.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J ´egou, H.; and Douze, M. 2021. Levit: a vision trans-\nformer in convnet’s clothing for faster inference. InProceed-\nings of the IEEE/CVF international conference on computer\nvision, 12259–12269.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y .\n2021. Transformer in transformer. Advances in Neural In-\nformation Processing Systems, 34: 15908–15919.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b. Identity map-\npings in deep residual networks. In European conference on\ncomputer vision, 630–645. Springer.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking spatial dimensions of vision transformers.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 11936–11945.\nHoward, A.; Sandler, M.; Chu, G.; Chen, L.-C.; Chen, B.;\nTan, M.; Wang, W.; Zhu, Y .; Pang, R.; Vasudevan, V .; et al.\n2019. Searching for mobilenetv3. In Proceedings of the\nIEEE/CVF international conference on computer vision ,\n1314–1324.\nHuang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger,\nK. Q. 2017. Densely connected convolutional networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 4700–4708.\nLin, J.; Song, C.; He, K.; Wang, L.; and Hopcroft, J. E. 2019.\nNesterov accelerated gradient and scale invariance for adver-\nsarial attacks. arXiv preprint arXiv:1908.06281.\nLuo, W.; Li, Y .; Urtasun, R.; and Zemel, R. 2016. Under-\nstanding the effective receptive field in deep convolutional\nneural networks. Advances in neural information process-\ning systems, 29.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and\nVladu, A. 2017. Towards deep learning models resistant to\nadversarial attacks. arXiv preprint arXiv:1706.06083.\nMahmood, K.; Mahmood, R.; and Van Dijk, M. 2021. On\nthe robustness of vision transformers to adversarial exam-\nples. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 7838–7847.\nNaseer, M.; Ranasinghe, K.; Khan, S.; Khan, F. S.; and\nPorikli, F. 2021. On improving adversarial transferability\nof vision transformers. arXiv preprint arXiv:2106.04169.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. International journal of computer vision, 115(3):\n211–252.\nShao, R.; Shi, Z.; Yi, J.; Chen, P.-Y .; and Hsieh, C.-J. 2021.\nOn the adversarial robustness of vision transformers. arXiv\npreprint arXiv:2103.15670.\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic\nattribution for deep networks. In International conference\non machine learning, 3319–3328. PMLR.\nSzegedy, C.; Ioffe, S.; Vanhoucke, V .; and Alemi, A. A.\n2017. Inception-v4, inception-resnet and the impact of resid-\nual connections on learning. In Thirty-first AAAI conference\non artificial intelligence.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7140\nSzegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna,\nZ. 2016. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2818–2826.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021a. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJ´egou, H. 2021b. Going deeper with image transformers. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 32–42.\nTram`er, F.; Kurakin, A.; Papernot, N.; Goodfellow, I.;\nBoneh, D.; and McDaniel, P. 2017. Ensemble adver-\nsarial training: Attacks and defenses. arXiv preprint\narXiv:1705.07204.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Advances in Neural Information\nProcessing Systems.\nWei, Z.; Chen, J.; Goldblum, M.; Wu, Z.; Goldstein, T.; and\nJiang, Y .-G. 2022. Towards transferable adversarial attacks\non vision transformers. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 36, 2668–2676.\nWu, D.; Wang, Y .; Xia, S.-T.; Bailey, J.; and Ma, X. 2020a.\nSkip connections matter: On the transferability of adver-\nsarial examples generated with resnets. arXiv preprint\narXiv:2002.05990.\nWu, W.; Su, Y .; Chen, X.; Zhao, S.; King, I.; Lyu, M. R.; and\nTai, Y .-W. 2020b. Boosting the transferability of adversar-\nial samples via attention. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n1161–1170.\nWu, W.; Su, Y .; Chen, X.; Zhao, S.; King, I.; Lyu, M. R.; and\nTai, Y .-W. 2020c. Towards global explanations of convolu-\ntional neural networks with concept attribution. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 8652–8661.\nWu, W.; Su, Y .; Lyu, M. R.; and King, I. 2021. Improving the\ntransferability of adversarial samples with adversarial trans-\nformations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9024–9033.\nWu, W.; Xu, H.; Zhong, S.; Lyu, M. R.; and King, I. 2019.\nDeep Validation: Toward detecting real-world corner cases\nfor deep neural networks. In IEEE/IFIP International Con-\nference on Dependable Systems and Networks (DSN), 125–\n137. IEEE.\nWu, W.; Zhang, J.; Wei, V . J.; Chen, X.; Zheng, Z.; King,\nI.; and Lyu, M. R. 2023. Practical and Efficient Model Ex-\ntraction of Sentiment Analysis APIs. In IEEE/ACM 45th\nInternational Conference on Software Engineering (ICSE) ,\n524–536. IEEE.\nXu, Z.; Gu, Z.; Zhang, J.; Cui, S.; Meng, C.; and Wang, W.\n2023. Backpropagation Path Search On Adversarial Trans-\nferability. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 4663–4673.\nZhang, J.; Huang, J.-t.; Wang, W.; Li, Y .; Wu, W.; Wang,\nX.; Su, Y .; and Lyu, M. R. 2023a. Improving the Transfer-\nability of Adversarial Samples by Path-Augmented Method.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 8173–8182.\nZhang, J.; Huang, Y .; Wu, W.; and Lyu, M. R. 2023b.\nTransferable Adversarial Attacks on Vision Transformers\nwith Token Gradient Regularization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 16415–16424.\nZhang, J.; Huang, Y .-C.; Wu, W.; and Lyu, M. R. 2023c. To-\nwards semantics-and domain-aware adversarial attacks. In\nProceedings of the Thirty-Second International Joint Con-\nference on Artificial Intelligence, 536–544.\nZhang, J.; Wu, W.; Huang, J.-t.; Huang, Y .; Wang, W.; Su, Y .;\nand Lyu, M. R. 2022. Improving Adversarial Transferability\nvia Neuron Attribution-Based Attacks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 14993–15002.\nZhang, J.; Xu, Z.; Cui, S.; Meng, C.; Wu, W.; and Lyu,\nM. R. 2023d. On the Robustness of Latent Diffusion Mod-\nels. arXiv preprint arXiv:2306.08257.\nZhang, Z.; Lu, X.; Cao, G.; Yang, Y .; Jiao, L.; and Liu, F.\n2021. ViT-YOLO: Transformer-based YOLO for object de-\ntection. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 2799–2808.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n6881–6890.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7141",
  "topic": "Transferability",
  "concepts": [
    {
      "name": "Transferability",
      "score": 0.9540047645568848
    },
    {
      "name": "Adversarial system",
      "score": 0.7309933304786682
    },
    {
      "name": "Connection (principal bundle)",
      "score": 0.5710105895996094
    },
    {
      "name": "Computer science",
      "score": 0.5128681659698486
    },
    {
      "name": "Transformer",
      "score": 0.46937063336372375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4623855650424957
    },
    {
      "name": "Machine learning",
      "score": 0.23489806056022644
    },
    {
      "name": "Engineering",
      "score": 0.22592422366142273
    },
    {
      "name": "Electrical engineering",
      "score": 0.11617651581764221
    },
    {
      "name": "Mechanical engineering",
      "score": 0.06997978687286377
    },
    {
      "name": "Voltage",
      "score": 0.06076809763908386
    },
    {
      "name": "Logit",
      "score": 0.0
    }
  ]
}