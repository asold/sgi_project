{
  "title": "An empirical evaluation of a novel domain-specific language – modelling vehicle routing problems with Athos",
  "url": "https://openalex.org/W4296828457",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Hoffmann, Benjamin",
      "affiliations": [
        "Technische Hochschule Mittelhessen"
      ]
    },
    {
      "id": null,
      "name": "Urquhart, Neil",
      "affiliations": [
        "Edinburgh Napier University"
      ]
    },
    {
      "id": "https://openalex.org/A5098199581",
      "name": "Chalmers Kevin",
      "affiliations": [
        "University of Roehampton"
      ]
    },
    {
      "id": null,
      "name": "Guckert, Michael",
      "affiliations": [
        "Technische Hochschule Mittelhessen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W143070698",
    "https://openalex.org/W2067400512",
    "https://openalex.org/W6960723595",
    "https://openalex.org/W2481675069",
    "https://openalex.org/W1729471791",
    "https://openalex.org/W3021105696",
    "https://openalex.org/W2406584862",
    "https://openalex.org/W2149586001",
    "https://openalex.org/W2809564239",
    "https://openalex.org/W2978273681",
    "https://openalex.org/W3082371875",
    "https://openalex.org/W1746353556",
    "https://openalex.org/W2556089607",
    "https://openalex.org/W2140095591",
    "https://openalex.org/W2052822586",
    "https://openalex.org/W1990341655",
    "https://openalex.org/W2202015452",
    "https://openalex.org/W2786980979",
    "https://openalex.org/W2565126387",
    "https://openalex.org/W2014596857",
    "https://openalex.org/W2070637511",
    "https://openalex.org/W2031236641",
    "https://openalex.org/W2811395712",
    "https://openalex.org/W3185884410",
    "https://openalex.org/W1515783630",
    "https://openalex.org/W2111199933",
    "https://openalex.org/W2788697254",
    "https://openalex.org/W4255888652",
    "https://openalex.org/W2905345048",
    "https://openalex.org/W2108103981",
    "https://openalex.org/W2295447624",
    "https://openalex.org/W2783016039",
    "https://openalex.org/W6903731114",
    "https://openalex.org/W4291213652",
    "https://openalex.org/W2002939956",
    "https://openalex.org/W3134208756"
  ],
  "abstract": "Abstract Domain-specific languages (DSLs) are a popular approach among software engineers who demand for a tailored development interface. A DSL-based approach allows to encapsulate the intricacies of the target platform in transformations that turn DSL models into executable software code. Often, DSLs are even claimed to reduce development complexity to a level that allows them to be successfully applied by domain-experts with limited programming knowledge. Recent research has produced some scientifically backed insights on the benefits and limitations of DSLs. Further empirical studies are required to build a sufficient body of knowledge from which support for different claims related to DSLs can be derived. In this research study, we adopt current DSL evaluation approaches to investigate potential gains in terms of effectiveness and efficiency, through the application of our DSL Athos , a language developed for the domain of traffic and transportation simulation and optimisation. We compare Athos to the alternative of using an application library defined within a general-purpose language (GPL). We specified two sets of structurally identical tasks from the domain of vehicle routing problems and asked study groups with differing levels of programming knowledge to solve the tasks with the two approaches. The results show that inexperienced participants achieved considerable gains in effectiveness and efficiency with the usage of Athos DSL. Though hinting at Athos being the more efficient approach, the results were less distinct for more experienced programmers. The vast majority of participants stated to prefer working with Athos over the usage of the presented GPL’s API.",
  "full_text": "https://doi.org/10.1007/s10664-022-10210-w\nAnempiricalevaluationofanoveldomain-speciﬁc\nlanguage–modellingvehiclerouting\nproblemswithAthos\nBenjaminHoﬀmann1 ·NeilUrquhart2 ·KevinChalmers3 ·MichaelGuckert1\nAccepted:13July2022 /\n© TheAuthor(s)2022\nAbstract\nDomain-specific languages (DSLs) are a popular approach among software engineers who\ndemand for a tailored development interface. A DSL-based approach allows to encapsulate\nthe intricacies of the target platform in transformations that turn DSL models into executable\nsoftware code. Often, DSLs are even claimed to reduce development complexity to a level\nthat allows them to be successfully applied by domain-experts with limited programming\nknowledge. Recent research has produced some scientifically backed insights on the ben-\nefits and limitations of DSLs. Further empirical studies are required to build a sufficient\nbody of knowledge from which support for different claims related to DSLs can be derived.\nIn this research study, we adopt current DSL evaluation approaches to investigate potential\ngains in terms of effectiveness and efficiency, through the application of our DSL Athos,\na language developed for the domain of traffic and transportation simulation and optimi-\nsation. We compare Athos to the alternative of using an application library defined within\na general-purpose language (GPL). We specified two sets of structurally identical tasks\nfrom the domain of vehicle routing problems and asked study groups with differing levels\nof programming knowledge to solve the tasks with the two approaches. The results show\nthat inexperienced participants achieved considerable gains in effectiveness and efficiency\nwith the usage of Athos DSL. Though hinting at Athos being the more efficient approach,\nthe results were less distinct for more experienced programmers. The vast majority of\nparticipants stated to prefer working with Athos over the usage of the presented GPL’s API.\nKeywords Domain-specific languages · General-purpose language · Empirical\nevaluation · Vehicle routing problem\nCommunicated by: Jeffrey C. Carver\n/envelopebackBenjamin Hoffmann\nbenjamin.hoffmann@mnd.thm.de\nNeil Urquhart\nN.Urquhart@napier.ac.uk\nKevin Chalmers\nKevin.Chalmers@roehampton.ac.uk\nExtended author information available on the last page of the article.\nPublished online: 23 September 2022\nEmpirical Software Engineering (2022) 27:180\n1 Introduction\n1.1 ProblemStatement\nA domain-specific language (DSL) is a programming language that is tailored towards the\nrequirements of a specific problem domain (Mernik et al. 2005). The potential advantages\nof an appropriately designed DSL include the abstraction of technical platform details (Iung\net al. 2020), facilitated program comprehension (do Nascimento et al. 2012), and the elimi-\nnation of error-prone, repetitive tasks (Hermans et al. 2009). These advantages are claimed\nto result in an increased productivity (Bari ˇsi´ce ta l . 2012a). A model written in a DSL is\nusually processed by a generator that relies on templates created by software engineering\nexperts. This way, the generated code follows best practice implementations and reflects\ndeep software engineering expertise.\nThe reduction in the level of required software development skills potentially allows\ndomain experts to develop applications for their domain. Using the DSL means to create a\nmodel and not to write code. This raises the questions whether modelling through a DSL\nis more effective than coding and whether the results are of comparable quality. Efficiency\nand ease of use are crucial quality criteria of a DSL. In this paper, we address this ques-\ntion for the domain of vehicle routing problems (VRPs) and their solution. Our DSL Athos\nprovides a declarative language in which problems together with algorithmic solutions can\nbe described and solved. The Athos generator transforms models into executable code for\nmulti-agent platforms, e.g. NetLogo (Tisue and Wilensky 2004).\nUntil recently, the aforementioned claims on the benefits brought about by DSLs have\noften received only anecdotal affirmation (Kosar et al. 2012;B a r iˇsi´ce ta l .2012b). Quanti-\ntative language evaluations have mostly been neglected (Bariˇsi´ce ta l .2012a) or conducted\nusing controvertible approaches (Challenger et al.2016). In their systematic mapping study,\nK o s a re ta l .(2016) found that software language engineers (SLEs) only very scarcely report\non language evaluation. This is all the more disconcerting given that the development of\nDSLs requires a considerable initial effort. To justify this investment, it is crucial that the\nfinal product is not unthinkingly put to use. Instead, the added value of a DSL as well as\nits potential for further improvement should be brought to light via empirical evaluation\n(Bariˇsi´ce ta l .2014).\nAlthough recently there has been an increased interest in the field of scientific DSL\nevaluation (Kosar et al. 2018), it is still in its early stages. Additional holistic evaluations\nfrom various fields are required in order to support researchers and developers likewise in\nthe adoption of approaches based on DSLs (Challenger et al. 2016) .K o s a re ta l .(2018)\nstress the importance of researchers publishing their results on DSL evaluations in order\nto create a substantial body of literature on the subject. Bari ˇsi´ce ta l .( 2014) point out that\na single evaluation study is only of limited informative value whereas real confidence can\nonly be gained from several evaluation studies with consistent findings. The authors thus\nemphasise the importance of a solid body of knowledge from which sound and valuable\nconclusions can be drawn.\n1.2 ResearchObjectives\nIn order to facilitate the integration of software engineering studies that employ controlled\nexperiments into a coherent body of knowledge, Jedlitschka et al. (2008) propose the usage\nof the goal template developed as part of the goal/question/metric method (Basili1994;v a n\nSolingen et al. 1999). Usage of this template is supposed to ensure a concise provision of\nEmpir Software Eng (2022) 27:180180   Page 2 of 52\noverview on the studies objects, purpose, quality focus as well as its perspective and context.\nThe research objectives of this study can thus be summarised as follows:\nAnalyze Athos and JSprit\nfor the purpose of their evaluation and comparison\nwith respect to their respective effectiveness, efficiency and user satisfaction\nfrom the viewpoint of experts from the domain of vehicle-routing and software developers\nin the context of the Operations Management (OM) lecture at the Technische Hochschule\nMittelhessen (THM) campus in Friedberg and the Model-Driven Software Development\nlecture at the Technische Hochschule Mittelhessen campus in Wetzlar.\nThe study presented in this paper compared two languages for modelling vehicle routing\nproblems with time windows (VRPTWs) in terms of how effectively and efficiently users\ncould comprehend and create VRPTW models and how satisfied they were with the respec-\ntive approach. According to the quality-in-use model defined in ISO/IEC 25010 ( 2011),\neffectiveness, efficiency and satisfaction are those characteristics that define the usability\nof a software component. In other words, the presented study evaluates and compares the\nusability of two VRPTW modelling approaches. For this, students from two different study\ncourses were selected to form the study’s sample population representing domain experts\nand software developers as potential target users.\n1.3 Context\nTwo studies were conducted among students of two different study courses offered at two\ndifferent campuses of Technische Hochschule Hessen (THM). The first study mainly fea-\ntured participants who were at the beginning of their programming careers. These students\nwere enrolled in a study course on information systems offered at the Friedberg campus.\nMost participants were in their second semester of study. The second study was conducted\namong participants with more advanced programming experience gained in real-world\nindustrial projects. These participants were enrolled in a co-operative course on software\nengineering offered at the campus for co-operative study courses located in Wetzlar. These\nparticipants were in their fourth semester and actively taking part in software development\nprojects of their employing company. Participants were not directly rewarded for partici-\npation in the study. An indirect reward was announced as a task on both of the compared\nlanguages was agreed to be part of the final exam and participation in the study was to serve\nas a training session towards that task.\nIn the next section, we discuss related DSL evaluation studies. After providing a general\nintroduction to Athos and its tooling in Section 3, we present the details of our evaluation\nstudy in Section4. In Section5, we analyse the results of the study and consider the potential\nthreats to validity in Section 6. Finally, in Section 7, we summarise the main findings and\nconclude this paper.\n2 RelatedWork\n2.1 StudiesComparingDSLsandApplicationLibraries\nKosar et al. (2010) showed that DSLs can significantly improve program/model understand-\ning compared to the adoption of application libraries used within general purpose languages\n(GPLs). The authors designed two questionnaires with tasks of comparable complexity: the\nEmpir Software Eng (2022) 27:180 Page 3 of 52    180\ntasks of the first questionnaire had to be solved using a DSL (XAML), and for the tasks of\nthe second questionnaire an application library of a GPL (C# forms) had to be used. The\ntasks of both questionnaires stemmed from the domain of graphical user interface develop-\nment. They were designed in a way that tested how efficiently study participants could a)\nlearn the notation, b) perceive the programs, and c) evolve given programs/models of the\nrespective approach. The authors conducted their study with 36 participants, all of whom\nreceived an introduction to the problem domain and training in both approaches. After par-\nticipants answered the questionnaires, the authors calculated the participants’ success rate\nfor all tasks. The authors then were able to statistically prove that the application of a\nDSL for GUI development has a beneficial effect on the success rate. The authors hence\nconcluded that DSLs have the potential to facilitate program understanding.\nKosar et al. ( 2012) then expanded their original experiment to a family of structurally\nsimilar experiments. The family consisted of three experiments each situated in a distinct\ndomain (feature diagrams, graph description and GUIs). In each experiment a DSL was\ncompared to either a GPL application library or an application programming interface (API)\nfor a GPL. In their study, the authors researched whether DSLs positively affected program\ncomprehension among participants.\nIn this study the authors took a within-subjects approach so that for each experiment two\ndistinct groups were formed. The first group started with the DSL and then the GPL; the\nsecond group used the inverse order. The authors aggregated the results obtained from both\ngroup and applied a parameterless test for dependent samples (Wilcoxon signed-rank test).\nThe obtained results show that in each of experiment usage of the DSL led to significantly\nimproved results. In all three explored domains, the results give proof to the claim that\nDSLs possess the potential to enhance the performance of developers. It is to be noted\nthat a generalisation of these findings to other domains is difficult if not impossible. In\nother domains the observed outcome might be markedly different. Therefore, it is of high\nimportance to conduct further empirical evaluation studies.\nK o s a re ta l .(2018) performed a modified replication of their family of experiments. The\nmost important modification of the original study was the replacement of pen and paper by\nintegrated development environment (IDE). Instead of having to manually write down their\nanswers, participants were allowed to use IDEs to solve the study tasks. Another modifica-\ntion was that the study design was changed from within-subjects to between-subjects,i . e .\nparticipants no longer solved both the DSLand GPL tasks but either the one or the other. To\ngain some further insight into how participants used the IDEs, they also asked participants\nto state which category of tools they used for each task.\nThe replication study evaluated languages from the same three domains as the original\n(feature diagrams, graph description, and GUIs). For each domain participants had to solve\na set of tasks that involved the respective DSL or the GPL. As was already mentioned, in\nthe replication study, the design of the study was modified so that no participant answered\nthe questions of the same domain with both the DSL and the GPL. The dependent variables\nof the study again were effectiveness and efficiency.\nThe results suggest that IDE support positively affected the effectiveness and efficiency\nof both DSL and GPL users. Other than that, the results are consistent with those of the\noriginal study. In all three domains DSL users achieved significantly better effectiveness\nresults than GPL users. At the same time, DSL users were also significantly more efficient.\nAn in-depth analysis, however, reveals that questions from the evolve category are a major\nreason for the superior results of DSL users. In the other two question categories, results\nwere less distinct. This is where the replication differs from the original study (where DSL\nEmpir Software Eng (2022) 27:180180   Page 4 of 52\nusers achieved significantly better results throughout all question categories in all three\ndomains). The authors deem the IDE support responsible for this. They consider it likely\nthat IDEs benefit GPLs more than DSLs.\nAll three experimental studies of Kosar et al. provide evidence that DSLs are a suitable\napproach to support software developers in minimising programming / modelling errors\nwhile at the same time helping them to become more efficient. Moreover, the replication\nstudy also provides some interesting findings related to the deployment of IDEs. Though we\nagree that IDE support of DSLs is a topic that merits further investigation, for our study, we\ndecided to not allow IDE usage. It is to be noted that this decision poses a possible threat to\nthe external validity of this study (see Section6.4). We still went with this decision, in order\nto be able to focus on Athos’abstract and concrete syntax and on how well these compare to\nthose of a selected baseline approach. Allowing the application of an IDE would introduce\nthe level of IDE support as a confounding variable.\nJohanson and Hasselbring ( 2017) present a similar study in which they compare their\nSprat Ecosystem DSL to an application library. The authors tailored their DSL for the\ndomain of marine ecosystems.Two important use cases covered by the DSL are parametriza-\ntion and data recording. Parametrization refers to the specification of simulation scenarios.\nData recording refers to the specification of the data to be produced in the course of a sim-\nulation run (data recording). For both use cases a task was defined. Participants approached\nboth tasks with both languages in a randomised order (within-subjects design).\nThe population of the presented study consisted of experts from the domain of marine\necosystems from different research groups located in various countries. The study focused\non how effective and efficient participants solved two different tasks with the compared\napproaches. The authors also investigated how satisfied users were with both approaches in\nterms of various characteristics (e.g., level of abstraction). Finally, the authors investigated\nwhether participants were able to modify (maintain) the language itself via additions to the\nlanguages main configuration file.\nThe study shows that with the DSL participants produced substantially more correct\nresults (on average 61% correctness increase) in a reduced period of time (on average the\nrequired time per task was reduced by 31%). The study also provides data that clearly show\nthat participants preferred usage of the DSL to the application of the baseline GPL. The\nauthors also provide evidence which shows that even though participants had only scarce\nknowledge of the Java language used in the configuration file, an overwhelming portion was\ncapable of performing the required language modification.\nThere are some important differences between the Sprat DSL evaluation study and the\none presented in this article. Most obviously, the evaluated languages target different appli-\ncation domains: Sprat is a DSL for the domain of marine ecosystem simulation whereas\nAthos is a language for the domain of traffic and vehicle routing problems. However, both\nlanguages are similar in that they enable users to specify the entities and parameters of a\ncomplex system in order to generate an executable computer simulation. Another important\ndifference is in the studies’ population: In the Sprat evaluation, the population consisted\nentirely of actual domain experts. In our study, on the other hand, students were invited to\nparticipate. Therefore, the results of the Sprat evaluation study are more generalisable to the\nintended population of language users. However, we believe that our selection of students\nthat we invited to participate ensures that our study’s population represents the targeted\nusers of our DSL reasonably well.\nThe advantage of asking students for participation shows in the respective population\nsize: in the Sprat evaluation 36 samples were obtained whereas our study produced 101\nEmpir Software Eng (2022) 27:180 Page 5 of 52    180\nsamples. In terms of quantity, there is also an important difference in the effort participants\nhad to take to solve all study tasks correctly. The Sprat evaluation consisted of two open-\nended tasks that could be answered in around 13 minutes. Our study features 15 different\ntasks – both open ended and multiple choice – that took most participants more than 70\nminutes (when facing the tasks for the first time). It thus could be argued that our study is\nmore realistic in terms of including the effects of mental exhaustion which occur during the\nspecification of real-world simulations.\nWe decided to use the studies of Kosar et al. as a template for our study. The approach\nof using an appropriate application library as a benchmark to evaluate the benefit of a given\nDSL holds merit. Especially in cases where no alternative DSL to the one to be evalu-\nated exists, the next best alternative is to compare against an application library of a GPL.\nWe firmly believe that the presented approach should be applied in several other domains\nin order to evaluate DSLs from these domains as existing study results from different\ndomains obtained with different languages are difficult to generalise. We therefore decided\nto adopt the approach of Kosar et al. for an evaluation of our DSL that targets the domain of\nVRPTWs. At this point, it is important to highlight the differences in our approach and the\none presented by Kosar et al.: First of all, it is highly likely that the order in which partic-\nipants answer two questionnaires consisting of similar tasks has an effect on the outcome.\nIt must be assumed that participants learn from the first questionnaire and thus understand\nand solve the tasks for the second approach more easily. Kosar et al. addressed this issue\nby conducting two surveys with two different study groups that answered the two question-\nnaires in reverse order. However, in their presented results, they merged the results from\nboth groups. Without differentiating the results of both groups, it is not possible to pre-\nvent learning effects from affecting the results. For this reason, we decided to compare the\nresults of our DSL and the GPL when both are used as a first approach to solve the tasks,\nand also when both are used as a second approach (more details on this are provided in\nSection 4). Another important aspect in which this study extends the work of Kosar et al.\nis through the investigation as to whether the effect of using a DSL is different for partici-\npants who have only scarce GPL experience from participants with a deeper understanding\nof GPL programming. Hence, even though we consider the study of Kosar et al. an impor-\ntant and valuable contribution to DSL evaluation, we believe that our study allows for an\neven deeper insight into how to interpret the obtained results.\n2.2 SimilarDSLEvaluationStudies\nDe Sousa and da Silva (2018) conducted a usability evaluation on DSL3S, a DSL that seeks\nto facilitate the development of spatial simulations by application of model-driven develop-\nment (MDD) techniques. The language was implemented as a unified modelling language\n(UML) profile that introduces stereotypes for core constructs from the domain of spatial\nsimulations such as spatial variables, animats and operations. The language is supported by\na tool stack that supports the creation, transformation and execution of models. For their\nlanguage evaluation, the authors created two artefacts: The first artefact was a step-by-step\nguide that comprised all the steps beginning at the setup of the necessary tools and a first\nproject to the definition, transformation and execution of a predator-prey model. The sec-\nond artefact was a questionnaire that recorded participants impressions on the DSL itself,\nthe tools supporting the DSL, and on the MDD approach in general. For each of these\nthree areas the questionnaire featured four questions. Each question asked to rate a specific\naspect within the respective area by means of a five-point Likert scale. In addition to these\nquestionnaires, the authors also recorded the profiles of the participants (e.g. profession,\nEmpir Software Eng (2022) 27:180180   Page 6 of 52\narea of expertise, or experience with spatial simulations) and how far participants went in\nthe step-by-step guide within a time frame of 50 minutes. The authors obtained somewhat\nmixed results with a slightly positive tendency in the language area. The ease of learning\nof the DSL was highly appreciated by participants and the supporting tools of the lan-\nguage were also highly marked. Especially the code generation aspect received almost only\nprime ratings. Participants appeared to take a rather guarded stance toward MDD in general.\nOnly in the question of whether approaches like DSL3S possess the potential to improve\ncommunication to stakeholders, participants clearly gave mostly positive feedback.\nDe Sousa and da Silva also found significant evidence for biased answers to two of the\nquestions: The ratings for the question on whether DSL3S or MDD in general could serve\nas a basis for a new standard language for simulation specification showed significant diver-\ngence between participants who claimed to be trained in computer science and those who\ndid not. Trained participants seemed to be more appreciative of DSLs as a new standard\nthan participants without training in computer science. This question also showed signifi-\ncant divergence when dividing participants depending on whether they had prior experience\nin simulations or not. Between these two sub-groups the ratings on the general development\nprocess also showed significant deviations. Participants with simulation experience consis-\ntently gave positive ratings while those without prior experience gave more diverse ratings.\nFrom their findings, the authors conclude that participants appreciated DSL3S, especially\nthe ease with which it can be learned. On the other hand, participants remained cautious\ntowards the general MDD approach. Here, especially two types of participants appeared to\nbe negatively biased towards the MDD approach: participants without training in computer\nscience as well as participants experienced in simulation. On the other hand, it was the group\nof participants with simulation experience who uniformly appreciated the general develop-\nment process. The authors thus assume that the scepticism of these participants might be\nmore based on a subjective feeling than on actual problems encountered in the development\nprocess.\nThere are several differences between the evaluation methodology applied by de Sousa\nand da Silva and the one applied in the study presented in this paper. Firstly, in their study,\nparticipants do only get to work with one language. Thus, especially participants who do\nnot have prior experience in the creation of computer simulations have no means to place\nthe evaluated approach into context. This does not necessarily render the ratings of this\nsub-group of participants less valuable, but it must be kept in mind upon evaluation of\nthe results. In their study, the group of participants without prior knowledge in simulation\nwere less appreciative of the adopted MDD approach than those participants experienced\nin this field. The diversity in the ratings of this subgroup might be due to the fact that\nthese participants lack the knowledge of an alternative approach and thus had to base their\nratings on mere gut instinct. Secondly, their approach does not require participants to use\nthe evaluated language in order to develop a solution for a given task. Instead, participants\nare only supposed to follow instructions of a step-by-step guide. This is problematic for\ntwo reasons: Firstly, the outcome of the language evaluation is largely dependent on the\nquality of the guide handed to participants. Secondly, most developers have made the first-\nhand experience with technologies that appeared promising when following an introductory\ntutorial but turned out to be less helpful when applied without direct guidance. Finally,\nanother important difference between their study and the study presented in this paper is\nthe fact that the evaluation of DSL3S is by and large of qualitative nature. Except for one\nmetric that tracked the progress of participants made within 50 minutes, all results were\nbased on subjective impressions of participants. This might pose a threat to validity since\nEmpir Software Eng (2022) 27:180 Page 7 of 52    180\nstudy participants could feel inclined to support study conductors by generally giving more\nfavourable results.\nEwais and de Troyer ( 2014) conducted a pilot evaluation study on three related graph-\nical DSLs from the domain of adaptive three-dimensional virtual learning environments\n(VLEs): The first language, named pedagogical model language (PML) is concerned with\nthe pedagogical structure of the VLE. Its main syntactical elements are pedagogical rela-\ntionship types (PRT) and pedagogical update rules (PURs) . PRTs are applied to define\nrelations between different learning concepts (e.g. one learning concept has to be mastered\nbefore another can be accessed). PURs are rules that when triggered update the learner’s\nprofile given that pre-defined conditions hold. The second language of the evaluation was\nthe Adaptive StoyLine Language (ASLL) . Its purpose is the definition of a narrative arc (or\nlearning path) for VLEs. For this, it allows to structure learning concepts by the definition\nof topics. Topics can be connected via so-called storyline adaptation rules that are used to\ndefine the circumstances under which a learner can advance to a new topic. Adaptive topic\nlanguage (ATL) is the third language evaluated in the study. In contrast to ASLL, it is used\ninside a single topic in order to define how the content of the given topic adapts to a learn-\ners behaviour or status. For this, it allows to define adaptation rules. These rules define\nevents of a source learning concept as triggers for the evaluation of conditions. They also\ndefine what adaptation is to be performed on a target learning concept in case the checked\ncondition holds.\nThe study was conducted among 14 participants working at the same computer science\ndepartment as the authors. The study was composed of three main steps: In the first step,\nthey introduced participants to the three DSLs. In step two, participants were supposed to\nuse the DSLs to author an adaptive 3D VLE on the solar system. It is to be noted that\nparticipants had to perform this task using pen and paper while receiving support on the\nlanguages from an instructor. The time for completing the definition of the VLE was taken.\nIn the third step, participants answered a questionnaire that was divided into six categories\n(demographics, specification of VLEs, and four more questionnaires/feedback instruments\nfrom the literature).\nAnalysing the usability of their language, the authors claim that they received good rat-\nings for all questions on five out of seven general ergonomic principles defined in ISO\n9241/10 (five questions for each of these principles were defined by Pr¨umper (1999), though\nthe paper does not state whether all of these questions were used). Questions regarding the\nprinciple of “suitability for learning” are reported to be neutrally rated. For the analysis of\nthe acceptability of the three DSLs, the authors subsumed 11 questions under this term.\nThey report that 7 of these questions received good ratings, whereas 4 were rated neutrally.\nThe questionnaire also posed open questions that allowed participants to give feedback on\nwhat they liked, what they disliked and suggestions on how to improve the DSLs. Inter-\nestingly, the majority of the participants appreciated that three DSLs had to be applied for\nthe definition of a VLE. Users disliked that the approach distinguished between adaptive\nstoryline and adaptive topic . Most recommendations concerned the need for an editing tool.\nOther than the time it took participants to complete the task, the study does not feature\nany metrics that give insight on how well participants performed in the second step of the\nstudy. Instead, the study is almost entirely made up of questions that ask for participants’\nopinion. This is a major difference when comparing their study to the study presented in this\narticle. While our study also contains a number of questions that investigate the impression\nAthos left with participants, most of the questions of our study are tasks designed to gain\ninsight on the results participants create by application of the DSL. Another difference is the\nfact that Ewais and de Troyer have not presented their participants an alternative approach\nEmpir Software Eng (2022) 27:180180   Page 8 of 52\nto solve the task at hand which might make it harder for participants to correctly assess the\nquality of the evaluated DSL. It also must be noted that the authors do neither provide an\noverview of all the questions asked in their questionnaire, nor do they elaborate on how they\ncalculated whether a complete question was rated as “poor”, “neutral”, or “good”.\n2.3 AdditionalDSLEvaluationStudies\nTekinerdogan and Arkin (2019) present ParDSL - a framework of four DSLs that support\nlanguage users in mapping parallel algorithms to target parallel computing platforms. The\nauthors designed each of the four languages for one specific activity involved in this map-\nping process. The framework thus comprises each a DSL that allows to model a) the physical\ntarget platform, b) the decomposition of the parallel algorithm to be mapped, c) the logical\nstructure of the target platform, and d) the transformations into executable code. The authors\nevaluate their ParDSL framework by discussing it in terms of several quality characteristics\ndefined in the FQAD DSL evaluation framework (Kahraman and Bilgen 2015).\nThe presented evaluation is, however, not conducted based on a controlled experiment.\nThe authors explain that the number of trained language users would not suffice to conduct\nan empirical evaluation study from which a statistically valid result could be obtained. The\nauthors therefore provide a comprehensive discussion of their DSL framework in terms\nof several FQAD quality characteristics. For example, the authors argue that ParDSL is\na functionally suitable language because it fulfills the functionality that was required in\ntheir presented examples. The authors base the usability evaluation of their framework on a\nset of informal interviews that were conducted among senior engineers and senior faculty\nmembers. In these interviews, the framework was lauded for its expressiveness in relation\nto its simplicity. Especially the separation of the four main activities of the mapping process\nwas positively recognised.\nCordasco et al. ( 2021) conducted a performance evaluation of their Fly DSL. Fly was\ndeveloped in order to enable experts from different domains to develop parallelised applica-\ntions without being experts in distributed cloud systems. The language thus allows users to\nmodel parallel data processing functions (or work-flows) on several different cloud services\nwhile making the necessary technical intricacies transparent. In the presented performance\nevaluation, the authors apply their language to several instances of the word count problem,\na well-known benchmark problem in the parallel computing domain. The authors create a\nFly program for the benchmark problem and vary several parameters (file size, number of\nfiles) to gain insight into how performance and cost aspects compare when using differ-\nent sequential and parallel execution settings. In addition to this benchmark problem, the\nauthors also apply their language to model and execute two algorithms from two distinct sci-\nentific use cases. As a baseline approach, the authors used a single-threaded Java execution\nand analysed the differences in performance and cost.\nWhile Fly is similar to Athos in that it is tailored for domain experts with little to no\nknowledge of computational technicalities and while both Athos and Fly were developed\nwith the Xtext1 framework, the presented study is very different from the one presented in\nthis article. First of all, Fly is a DSL targeted at a specific technical domain (parallelised,\ncloud-based computing) whereas Athos targets a specific application domain (vehicle rout-\ning and traffic). Another very important difference is that the presented study seeks to\nevaluate how well the generated code performs on different problem instances and on\n1https://www.eclipse.org/Xtext/\nEmpir Software Eng (2022) 27:180 Page 9 of 52    180\ndifferent target architectures. By contrast, in our study execution performance is not rele-\nvant. The controlled experiment presented here is to provide insight if and to what extend\nAthos enhances domain experts’ effectiveness, efficiency as well as their tool satisfaction.\n2.4 FrameworksforDSLEvaluationandExampleStudies\nChallenger et al. ( 2016) introduce a multi-agent systems (MASs) DSL evaluation frame-\nwork. The framework consists of a language dimension ,a n execution dimension and a\nquality dimension . The first two dimensions evaluate the DSLs by application of quantita-\ntive criteria while the latter takes a qualitative approach. Each dimension is further split into\ntwo sub-dimensions that define evaluation criteria for the respective aspect of the DSL. As\nan example, the language dimension is divided into thelanguage elements and model trans-\nformations sub-dimensions. Evaluation criteria for the language elements sub-dimension\nare the number of abstract and concrete syntax elements and the number of constraints\n(static semantics) . Criteria of the model transformation sub-dimension is the number of\nmodel-to-model (M2M) and model-to-text (M2T) transformations.\nTo demonstrate the application of the framework, the framework is used to evaluate a\nlanguage called SEA ML, a language developed by the authors. Applying the criteria of the\nlanguage dimension , the authors state the number of meta-model elements, attributes, etc.\nand note that these numbers can also be determined for any other MAS related DSL in order\nto compare it to SEA ML. They argue that the language with more elements is likely to be\nmore expressive and allows for more detailed modelling.\nFor the evaluation of the other two dimensions whose criteria rely on an actual appli-\ncation of the language, the authors conducted a multi-case study. The study featured four\nscenarios from different application domains. Each scenario required the development of a\nMAS for a particular domain. Members of the study group were asked to apply SEA ML\nfor domain analysis and domain modelling as well as implementation, testing and mainte-\nnance of the respective MAS. Members of the control group were asked to perform these\nsteps with a combination of general-purpose modelling notations (e.g. UML) and agent-\ndevelopment methodologies (e.g. Tropos) together with manual code development for a\nmulti-agent platform (JADEX or JACK).\nAs a quantitative result, the authors state that on average around 80% of the final product\ncould be generated while the remaining 20% required manual implementation. They claim\nthat this substantially reduces the complexity of MAS development. Another quantitative\nresult is that SEA ML considerably accelerates the development process: study group mem-\nbers required only an average of 3.5 hours per case study – opposed to an average of 6.25\nhours required by control group members. The most notable efficiency gain occurred in\nthe implementation phase: study group members required an average of 37 minutes for this\nphase. For control group members, the average time spent for implementation was nearly\nfour times as much. The study group also outperformed the control group in the testing and\nthe maintenance phases. In both phases they required only around half the time (testing: 17\nvs. 34 minutes, maintenance: 26 vs. 50 minutes).\nWith their framework and the presented study, Challenger et al. have made important\ncontributions to the field of DSL development. They provide criteria for the assessment of\na wide range of DSL components and present an example of how to obtain the data for\nthese criteria. The main purpose of these criteria is an assessment of the expressiveness of\na DSL as well as an assessment of the added value in terms of efficiency. Although the\npresented approach is to be considered one of the most sophisticated approaches in the field\nof DSL evaluation, it is important to note that it was mainly designed for graphical DSLs.\nEmpir Software Eng (2022) 27:180180   Page 10 of 52\nThe framework lacks some criteria, that might be especially important when dealing with\ntextual DSLs. For example, the framework seems to neglect the fact that language users\noften introduce syntactical and semantic mistakes when creating DSL models. Although it\ncan be argued that graphical DSLs are less prone to these types of mistakes, the framework\nstill might benefit from the introduction of additional criteria that allow further insight on\nthe number or percentage of mistakes made by new language users. The framework also\nseems to overlook the importance of the learnability and perceivability of the evaluated\nlanguage. A deeper insight on both aspects might be obtainable by an analysis of the number\nof modelling mistakes made by language users.\nAnother framework designed to guide DSL evaluation studies is the Usa-DSL evalu-\nation framework2 (Poltronieri et al. 2018). Following the works of Bari ˇsi´ce ta l .( 2012b,\n2014), the framework treats DSLs as interfaces enabling human-computer interaction. Usa-\nDSL focuses on the usability aspect of a DSL. Usability is defined within the context of\nthe quality in use model (ISO/IEC 25010 2011) as being composed of three characteristics:\neffectiveness, efficiency and satisfaction. Usa-DSL defines abstract guidelines that define\nthe flow of activities and artefacts in the evaluation process (Poltronieri et al. 2021). The\nframework does not prescribe the exact implementation of specific activities in the eval-\nuation process. Instead, the framework provides adequate suggestions on the order and\nembodiment of evaluation activities so that it is left to the experiment conductor, to com-\npose a concrete evaluation process from the various possible paths. In addition to the work\nof Kosar et al., we also considered the Usa-DSL framework in the planning, execution,\nanalysis and reporting of this study.\n3 Athos–aDSLforVehicleRoutingandTraﬃcSimulations\n3.1 ToolingandtheUnderlyingArchitectureoftheLanguage\nAthos was implemented using the Xtext language generation framework3 (version 2.12.0).\nXtext allows to create language-supporting tools like editors, compilers, validators and gen-\nerators as Eclipse plug-ins. Hence, Athos can be installed into current Eclipse IDEs as a\nplug-in from our update site. When Athos is installed, it can be used from within the Eclipse\nIDE to define traffic and transport simulations and optimisation models.\nFigure 1 provides an overview of the general workflow and the internal processes inside\nthe Athos framework: users can create textual models using the sophisticated Athos editor\nwhich (due to the underlying Xtext framework in combination with appropriate modifica-\ntions and extensions of the framework-generated code) offers a series of helpful features like\nsyntax-highlighting, code completion and validation and quick-fix mechanisms. Provided\nthat the current state of the Athos model in the editor is valid (i.e. there are no syntactical\nerrors in the code and all validation checks are passed), saving the model triggers the gen-\nerator. The generator was implemented with the Xtend4 language. It produces a model file\nfor the NetLogo5 platform from the Athos model. The generated NetLogo model can then\nbe loaded and run by the NetLogo platform. Depending on the Athos model, the NetLogo\n2https://github.com/Ildevana/Usa-DSL/wiki\n3https://www.eclipse.org/Xtext/\n4https://www.eclipse.org/xtend/\n5https://ccl.northwestern.edu/netlogo/\nEmpir Software Eng (2022) 27:180 Page 11 of 52    180\nFig.1 Interaction of entities in using Athos\nplatform may need to solve various VRPs. For these problems, a NetLogo extension provid-\ning heuristic optimisation algorithms was developed. If necessary, the platform can leverage\nthis extension to obtain near-optimal solutions which then are transformed into agent-\nbehaviour within the simulation. Athos also allows to specify metrics, i.e. mathematical\nexpressions that include events or data of interest. These metrics are also transformed appro-\npriately to NetLogo code so that the NetLogo platform tracks the defined metrics during the\nsimulation run and saves them to a result file (Hoffmann et al. 2019).\nWith the presented process and its components it is possible to create complex NetLogo\nsimulations from Athos models that in general only need a fraction of the lines of code\n(Hoffmann et al. 2018). Though currently, we have chosen NetLogo as our main target\nplatform, it is also possible to change the target-platform to different multi-agent simulation\nframeworks (Hoffmann et al. 2018).\n3.2 AthosbyExample\nIn this section, we provide a brief overview on the structure and syntax of Athos programs\nfor VRPTWs (c.f., e.g. Ombuki et al.2006) by presenting and discussing a simple example.\nThe presented example is the one that was used in the Athos and JSprit learning material\nthat was presented to study participants (see Section 4.5).\nListing 1 is a complete Athos program that models a simple VRPTW instance. Line 1\nis the preamble where a name for the model (and optionally its spatial dimensions) are\nspecified. Lines 2–3 constitute the product section . For classic academic problems like the\nexample at hand, the product often is not of interest. In these cases, one product with an\narbitrary name (here uBox) and a weight of 1.0 should be modelled. The weight capacity\nEmpir Software Eng (2022) 27:180180   Page 12 of 52\nListing1 General structure of an Athos Model (alternating colors indicate the different program sections)\nmodelled for a vehicle (line 26) then determines how many units of the product the vehicle\ncan transport. It is worth noting that for real-world problems, the possibility of defining\nseveral different programs with different weights (and volumes) can facilitate the modelling\nprocess for domain experts.\nLines 4–7 represent the function section . In this section, durationFunctions can be\ndefined. These functions feature a mathematical expression that determine the amount of\ntime it takes a vehicle to traverse an edge to which the respective function is associated. As\nan example, the definition of thefastW ayFunctionin line 7 says that an edge associated with\nthis function can be traversed in an amount of time6 equal to half of the length of the edge.7\nThough not of interest in this particular scenario, it should be noted that functions can be\nused to model congestion effects in a network as we have shown in Hoffmann et al. (2018).\nThe section spanning lines 8–24 is referred to as the network section . It consists of two\nsubsections referred to as the nodes and the edges section. The nodes section (lines 9–14)\ndefines all places of the network a vehicle can visit or emerge from: In line 10, a depot\nnamed n0 located at x = 3a n d y =− 4 is defined. The depot stores and delivers the\nuBox product and hosts a fleet of vehicles of type vehicles. The depot serves customers\nlocated at nodes n1,n2, and n3. Vehicles begin their delivery service at time 0 and have to\nreturn to the depot by time 300. Lines 11–13 define the customer nodes. For each customer\nthe demanded product together with the demanded quantity is defined. 8 Within a VRPTW\ninstance each customer has a time window within which the servicing has to commence and\n6The unit of time is to be defined by the modeller. In the generated NetLogo simulations the amount of time\ncorresponds to simulation ticks.\n7Again, the unit of length is to be defined by the modeller.\n8In this example, there is only one product. In Hoffmann et al. ( 2020), we give an example where multiple\nproducts and demand for multiple products are defined within one program.\nEmpir Software Eng (2022) 27:180 Page 13 of 52    180\na time it takes the vehicle to service the customer. Line 11, for example, states that customer\nn1 has to be serviced in the time interval [30, 45] and once the vehicles starts servicing the\ncustomer, it takes 3 units of time to complete the servicing procedure. Nodes without any\ndemands serve as navigational nodes of a network.\nIn the edges section from line 15 to 24 the different roads of the network are defined.\nIn this section, the linking between an edge and a function from the function section takes\nplace. The program also shows two approaches of defining edges in an Athos model: In line\n16 a single edge between nodesn0a n dn1 is defined linked with theslowW ayFunction.T h e\nalternative approach is to group edges together which is done twice in the example program.\nBy grouping edges, the associated function has to be stated only once which results in less\ntyping and faster reading of programs. Please note that it is also possible to associate other\nattributes to edges via this mechanism.\nThe final part of the presented Athos program is referred to as theagent section . Here, the\nvehicles or more generally agents, i.e. acting entities, are defined. Lines 25–28 define the\nvehicles type with a carrying capacity of 200 weight units. Line 27 states via the awaitT our\nbehaviour that the vehicle idles at its depot until it is given a tour of customers to service\nand vanish from the simulation when the tour is completed. In (Hoffmann et al. 2020),\nwe provide further insight on the different behaviours of vehicles (agents) and how these\nbehaviours are implemented by modelling vehicles as state machines.\nFigure 2 visualises the code of the Athos program. Customers are annotated with their\ndemands, time windows and service times, the depot is annotated with its time window and\nthe capacity of the vehicles stationed at the depot. A similar network representation is cre-\nated by the NetLogo platform from the generated NetLogo code. Since this visualisation\ncannot be seen by the modeler until the generated NetLogo simulation is run, we are cur-\nrently working on an additional plug-in that synchronises with the editor and visualises the\nFig.2 Pictures from the corresponding study questions Q09ATNW and Q09JSNW: The nodes (depot),\n(customer), (navigation) and edges (roads), (highways) represent elements for which pre-\ndefined code was given in the task. Elements emphasized by bold black colour had to be added to the program\nby customers\nEmpir Software Eng (2022) 27:180180   Page 14 of 52\ncode of Athos models and also allows to modify Athos programs by pointing and clicking\non the visualised elements.\n4 MaterialsandMethods\n4.1 GeneralEvaluationFrameworkandSelectionofStudyMethod\nFor this study we considered thephases and steps defined in the Usa-DSL evaluation frame-\nwork (Poltronieri et al. 2018) and performed the activities defined within the framework.\nThe Usa-DSL framework does not imperatively define the exact activity to perform. It\nguides researchers by providing reasonable suggestions. The exact embodiment of most\nactivities is to be defined by the researcher. For example, step 4 empirical study method\n(SE) of the planning phase demands the researcher to define the empirical study method but\ndoes not demand a specific study method to be embodied.\nDuring the planning phase of this study, in which we decided on the exact activities to\nperform, it was determined that the most promising and objective approach to gain insight\non the qualities of our DSL was to compare it to a baseline DSL. However, to the best of\nour knowledge, no DSL that would allow for a sensible comparison exists. We then decided\nto follow the approach taken by Kosar et al. (2010, 2012, 2018) and modify some important\naspects.\n4.2 ResearchQuestionsandHypothesis\nOur experimental study was in large parts inspired by the work of Kosar et al. (2010, 2012,\n2018). As a logical consequence, the formulation of the research questions and hypothesis\nis also based on their work.\nRQ 1: Does Athos enhance the effectiveness in model comprehension and creation?\nRQ 2: Does Athos enhance the efficiency in model comprehension and creation?\nRQ 3: Does Athos enhance the satisfaction of language users?\nFrom these research questions the following three hypothesis were derived:\nH1: The use of Athos has a positive effect on participants’ effectiveness.\nH2: The use of Athos has a positive effect on participants’ efficiency.\nH3: The use of Athos has a positive effect on participants satisfaction.\n4.3 DemographicInformationandEthicalConsiderations\nAs Athos was designed for experts from the domain of routing optimisation and traffic\nplanning, it was originally intended to have these experts participate in the study. However,\nit soon became clear that this was impractical for a pilot study since every participant would\nhave had to be introduced to Athos and the application library and it would have been hard to\nfind a date suiting all domain experts so that individual training would have been necessary.\nIn addition, it was noted that most domain experts would not possess enough knowledge of\na GPL and hence could not be introduced to an application library in a reasonable amount of\ntime. Therefore, focusing on domain experts as study participants would have led to a very\nsmall study population that would then reduce the statistical significance of the discovered\nresults.\nEmpir Software Eng (2022) 27:180 Page 15 of 52    180\nFor this reason we opted for students from T echnische Hochschule Mittelessen (THM)\nenrolled in courses that would ensure a certain level of knowledge in a given GPL. This\nway, we could focus on introducing participants to the DSL and the baseline application\nlibrary (and not the underlying GPL). It was decided to perform the study among two dif-\nferent student groups at two different campuses of THM. The first study was to take place\nat the campus in Friedberg, Germany, among students studying information systems. The\nstudy was integrated into a module on operations management which is taken by students\nin their second semester. The second study was to be conducted among students enrolled in\nsoftware technology at THM’s co-op campus (StudiumPlus) in Wetzlar, Germany. There,\nthe study was to be integrated in a module on model-driven software development. Please\nnote, that in both study groups students were taught aspects of the study problem relevant\nto the respective module.\nAn informed consent form was presented to each participant, along with an explanation\nof the study. Participants were informed that participation was on a voluntary basis and\nthat all data would be anonymised and thus not be attributable to an individual participant.\nEthical clearance was granted by both Edinburgh Napier University and THM.\nInitially it was intended to perform the study on the premises of THM, but due to the\nCovid-19 pandemic it was necessary to perform the study via an online meeting software.\nThe study was designed and carried out using the NoviSurvey online survey construction\nsoftware. During execution of the study, participants were granted help if they had problems\nto understand the tasks. Participants were allowed to use notepad and screenshot software\nin order to solve the tasks.\n4.4 SelectionofBaselineApplicationLibraryandDeﬁnitionofProblemDomain\nIn accordance with the study method described by Kosar et al. ( 2010), it was necessary to\nfind an application library against which Athos was to be compared. With JSprit,9 we found\nan application library for the domain of vehicle-routing optimisation. Before the tasks of the\ncontrolled experiment could be defined, it was necessary to look at the capabilities of the\ntwo approaches about to be compared. Even though Athos and JSprit have a common core\nof capabilities, each approach also possesses some features that the other does not offer. In\norder to devise a set of comparable tasks for each approach, the tasks had to be restricted to\nfeatures offered by both approaches.\nFigure 3 illustrates the capabilities/features of both approaches by visualising the set of\ncapabilities of each approach as a Venn-Diagram. Athos models, on the one hand, are auto-\nmatically transformed into executable simulations in which all actors can mutually influence\ntheir respective behaviour (e.g. by slowing down movement speed on a given road so that\nanother actor opts for a longer road which allows faster movement), it can be used to define\nboth static and dynamic vehicle routing problems (cf., e.g., Pillac et al. 2013).\nJSprit, on the other hand, was developed forstatic vehicle routing problems (Mayer et al.\n2016; Welch 2017); hence it does not support the specification of problems, in which new\nactors dynamically enter and alter the original scenario. In contrast to Athos, however, it\nallows to specify and solve a larger number of static vehicle routing problems, e.g. vehicle\nrouting problems with multiple time windows (VRPMTW), multiple depots (MDVRPTW),\nor pick-up and delivery problems (PDP), in which a product has to be fetched from one\nlocation to be delivered to a different one. The tasks in the controlled experiment were\n9https://github.com/graphhopper/jsprit\nEmpir Software Eng (2022) 27:180180   Page 16 of 52\nFig.3 Comparison of the capabilities of Athos and JSprit\ndesigned so that only functionality in the intersection of JSprit and Athos capabilities were\nto be solved.\n4.5 TheStudyProtocol\nIn this section, we describe how we conducted the two studies in Friedberg and Wetzlar.\nFigure 4 illustrates the performed steps: The rectangles in the upper part of the illustra-\ntion represent the performed steps in a chronological order. Below these steps, the number\nof participants in the respective study (Friedberg, and Wetzlar) as well as its groups (e.g.\nFriedberg, Athos first) is given. In addition, below the information on each study, a time-\nline shows how much time the respective study group took for the steps and the breaks in\nbetween.\nIn the first step, participants were introduced to the domain of VRPTW. For this, a\nfictitious case study was developed and presented to the participants. The case study demon-\nstrated the objective function and the constraints of VRPTWs using a scenario in which a\ngrocery retailer chose to introduce a food delivery service. This way, participants learned\nabout the constituting entities of VRPTWs and their relations. In the next step, partici-\npants learned about various mathematical aspects like deriving a graph from an incomplete\nnetwork. In addition, participants were presented a formal mathematical definition of the\nVRPTW.\nHaving imparted substantiated domain knowledge to participants, they were introduced\nto the two approaches compared in the study. Slides were prepared for both approaches\nthat explained how the VRPTW from the case study could be modelled. The first approach\npresented to participants was the JSprit application followed by Athos as an alternative\nFig.4 Study conduction protocol\nEmpir Software Eng (2022) 27:180 Page 17 of 52    180\napproach. Extreme care was taken to introduce both approaches with the same due diligence\nas to not affect the results by inferior learning material or inferior explanations of one of the\napproaches (also see Section 6).\nIn both Friedberg and Wetzlar, participants were randomly assigned to the Athos first and\nJSprit first groups (see next section). This was important since the tasks for both approaches\nwere very similar (see Section4.7) and thus it was highly likely that the tasks for the second\napproach might have been easier for participants due to learning effects from the tasks of the\nfirst approach. On the other hand, by the time a participant started to solve the tasks of the\nsecond approach, exhaustion effects might have started to negatively affect the performance\nof the participant. In order to be able to compare results free of any learning and exhaustion\neffects, we decided to split each study into (sub-)groups that applied the approaches in\nreverse order (see next section). Participants were then given the survey URL from which\nthey could access the survey with an arbitrary browser software. After some final words of\nexplanation regarding the voluntary basis of participation, data protection and the maximum\ntime limit for either approach participants were asked to begin the study. Participants had\ntwo example programs at their disposal. Since we had no means of prohibiting usage of\nadditional tools or help, participants were also allowed to use any means they deemed fit to\nhelp them in answering the questions.\nAs depicted by Fig. 4, the Friedberg study consisted of 118 participants. 63 of these\nparticipants were in the group that started with the Athos tasks with JSprit as the second\napproach (group FbAf); 55 participants were in the group that started with the JSprit tasks.\nThe Wetzlar study comprised 41 participants, 19 of which starting with the Athos tasks and\n22 beginning with the JSprit questions.\nFigure 4 also provides some insight on temporal aspects of the study protocol. In the\nFriedberg study, the study took place throughout the course of several days with one to sev-\neral days between the events / steps. The Wetzlar study, due to organisational reasons, had\nto be conducted within one single day and we were forced to reduce the time for mathemat-\nical explanations. It is important to note that this might have an impact on the results of the\nrespective approaches but does not threat the validity of comparisons between the groups of\none study since both groups followed the same study protocol.\n4.6 DeﬁnitionoftheSurveyStructure\nFigure 5 depicts the five sections of the survey together with the sequence in which partic-\nipants entered them. Before participants could enter any other section, they had to consent\nto the terms of the survey given in the informed consent form .I nt h e programming back-\nground section, we asked participants for details on their prior programming knowledge,\ne.g. the time they had been programming, a self-assessment of their skills or the number of\nprogramming languages they had already done some work with.\nBefore the beginning of the study, participants in both studies were randomly assigned\nto groups. Participants in the first group started with the section that comprised the Athos\nquestions and then went on to the section with the JSprit questions. For Friedberg, this\ngroup is referred to as FbAf (Friedberg, Athos first) , for Wetzlar the group is referred to as\nWzAf, (W etzlar , Athos first) .10 The other group began with the section containing the JSprit\n10Since a group that used Athos first must have used JSprit second, we sometimes use the labels FbJs\n(Friedberg, JSprit second) or WzJs (W etzlar , JSprit second) for these groups.\nEmpir Software Eng (2022) 27:180180   Page 18 of 52\nFig.5 Sequence in which participants answered the five sections of the survey; both studies were split into\ngroups that entered the taks sections for Athos and JSprit in opposite order\nquestions and then continued with the section made up of Athos questions. These groups\nare referred to as FbJf (Friedberg, JSpirt first) and WzJf (W etzlar , JSprit first) .11\nFor both the Athos and the JSprit section, participants had exactly 90 minutes to answer\nall or as many questions as they were able to. After that, the given answers were saved\nbut the survey tool did neither allow for further answers nor modification of any answers\ngiven until that point. In the last section, we asked participants for their opinion on several\naspects of the study, e.g. which approach they preferred or whether they felt that the learning\nmaterial for both approaches was of comparable quality.\nWith the additional questions in the programming background and opinion section, we\naddressed several threats to validity. A lopsided group assignment which would have posed\na threat to the conclusion validity (see Section 6.1), was already addressed by conducting\none controlled experiment on each campus and the randomisation of the assignment process.\nHowever, it might still have been possible that the majority of highly skilled and passionate\nprogrammers was assigned to one group, while the other group was mainly assigned those\nparticipants who only possessed little programming skills. The questions on prior program-\nming knowledge allowed us to get an impression whether participants of both groups were\nequally competent (see Section 5.2).\nEven though we spent considerable effort to create high-quality learning material for\nboth approaches (see Section 4.5), inferior training material for either approach might pose\nan instrumentation threat to the internal validity of the study (see Section6.2). We therefore\nasked participants whether they deemed the material for both approaches as being similarly\nhelpful or whether one approach was supported by better material. In the same way, we\naddressed another instrumentation threat posed by the possibility of unequal time for intro-\nduction of the approaches (also see Section4.5). Though granting the exact same amount of\ntime for the introduction of each approach would have been the most objective and efficient\nmeasure to address this threat, it quickly turned out to be impractical since explaining the\nintricacies of the approaches required a slightly different amount of time for each approach.\nMoreover, it was important for us to allow participants to ask questions at any given point\nduring the introduction and thus we could not completely control the exact duration of the\nintroduction. For these reasons, we asked participants whether they felt that a similar amount\n11Depending on context, sometimes these groups are referred to asFbAs (Friedberg, Athos second) and WzAs\n(W etzlar , Athos second).\nEmpir Software Eng (2022) 27:180 Page 19 of 52    180\nof time was spent for introducing both approaches or whether one approach was granted\nmore time than the other.\n4.7 DeﬁnitionoftheQuestionsandTheirTasks\nFor the tasks of the survey, it was decided to focus on a problem type natively supported\nby both approaches. Hence, the tasks of the survey are centred around the VRPTW, a\ngeneralisation of the travelling salesman problem (TSP). In addition to the informal descrip-\ntion of the VRPTW given in Section 3.2, a formal definition can be found in various\nworks throughout the literature, e.g. in Ombuki et al. ( 2006) and Baldacci et al. ( 2008).\nAll questions are publicly available as part of the lab package at github.com/benjaminh20/\nAthosEvaluation2020.\nAccording to the work presented by Kosar et al. ( 2010), we also defined three groups\nof questions, i.e. questions that aimed at testing participants’ ability to learn, perceive and\nevolve programs of the respective approach. Table 1 provides an overview on the questions\nwe defined for the survey. The first column shows the tag of the question: the first three\nsymbols (QXX) represent the number of the question and the following two letters indi-\ncate whether the questions aimed at the network or the agent / vehicle (behavior) part of the\nrespective language. The second column of the table shows the number of (sub) tasks par-\nticipants had to perform to completely answer the question. The third column informs about\nthe type of the question(s)/tasks. For example, question Q05NW was related to the network\npart of the language and was made up of one multiple-choice task. The fourth column gives\na brief description of what participants were expected to do to answer the question. Each\nquestion and its respective (sub) tasks were defined for both Athos and JSprit. As an exam-\nple, question 9 was implemented as Q09ATNW for Athos and as Q09JSNW for JSprit (‘AT’\nand ‘JS’ indicating the respective approach).\nThe main reason for splitting some questions into several subtasks was that some single-\ntask questions were self-contained whereas others appeared rather incomplete without an\nadditional task. For example, in Q11ATALL, participants were supposed to modify a pro-\ngram in which one single product was distributed to various customers from one single\ndepot. The task was to change the program in a way so that two different products were dis-\ntributed to the customers from two different depots. Each customer demanded either of the\ntwo products (not both). To correctly answer the question (complete the task) participants\nhad to perform two sub-tasks: first, they had to change the network definition (change the\ndemand of customers who now wanted the new product) and introduce the depot. Second,\nthey had to introduce a new agent type for a fleet that was to be stationed at the new depot.\nThe task was thus split into two subtasks. In contrast to that, in Q06ATNW/Q06JSNW\nparticipants were presented a network definition defined with the respective approach.\nTheir task then was to pick semantically equivalent network definitions from a set of four\nprograms. This task did not require to be split into subtasks.\nTo allow for a scientifically sound comparison, the questions for both approaches had to\nbe of comparable complexity. In fact,deviation in the complexity of questions poses a major\nthreat to validity (see Section 6). Hence, we took utmost care to define the questions so\nthat two corresponding questions had the same structural complexity. An example for this is\nillustrated in Fig.6, which is a juxtaposition of the task that was to be performed in the Athos\nquestion Q09ATNW (Fig. 6a) and the corresponding JSprit question Q09JSNW (Fig. 6b).\nIn both tasks, participants were presented pre-defined programs that consisted of a number\nof nodes (which either represented customers with demands, time windows and service\ntimes or simple network navigation nodes) and edges (which either represented highways\nEmpir Software Eng (2022) 27:180180   Page 20 of 52\nTable1 Questions and tasks to be solved by study participants (MC = multiple choice, SC = single choice,\nOT = open text)\nQuestion Tasks Type Description\nLearning\nQ01NW 1 MC Selection of syntactically incorrect statements: Participants had to spot\nsyntactical errors in a network specification\nQ01AG 1 MC Selection of syntactically incorrect statements: Syntactical errors in an\nagent behaviour specification had to be spotted\nQ02AG 1 MC Nonsense statements: From four given code snippets participants have\nto select those that do not make sense\nQ03ALL 1 SC Program with given result: From three programs one that represented a\ngiven tour and network had to be selected\nPerceiving\nQ04NW 1 SC Sensible result: Participants had to select a graph that corresponded the\nnetwork of a given program\nQ04AG 1 SC A tour on a graph that corresponded to a given program had to be\nselected\nQ05NW 1 MC Identify language constructs: Participants had to tick the lines to be\nchanged in a given program to transform the modelled network into a\ngiven target network\nQ05AG 1 MC Lines to change the customers for a tour had to be ticked\nQ06NW 1 MC Programs with equal result: From four syntactically different programs\nthose that semantically corresponded to a given program had to be\nselected\nQ07ALL 2 SC Associate semantic to element: From four options, the correct purpose\nof two program elements was to be selected\nQ08ALL 3 SC, MC Recognise based on comments: From descriptive comments embed-\nded in a given program, the actual program elements referred to in the\ncomment had to be selected\nEvolving\nQ09NW A3, J4 OT Add functionality: A program with gaps had to be completed according\nto given specifications\nQ09AG 1 OT Missing customer information had to be added\nQ10NW 1 OT Remove functionality: From a given program code was to be removed\nto achieve a given target state\nQ11ALL 2 OT Change functionality: A given program had to be changed according to\na graphical specification\nor roads). Elements already present in the respective pre-defined program are depicted in\ngrey colour. The elements printed in bold black colour where elements that participants\nwere supposed to add to the program. As can be seen, the number of pre-defined elements\n(five nodes, eight edges) and the number of elements to be added (one node, three edges)\nwere the same in both questions so that it is reasonable to assume that both questions were\nof the same structural complexity. In order to extend the code correctly, participants were\nsupposed to copy marked parts of the code into a text area and add the necessary elements.\nAs can be seen from Table 1, this question is the only one where the number of sub tasks\nin the two corresponding questions deviated. This is due to the different structure of the\ntwo approaches. In order to add the elements, it was sufficient to modify three program\nparts in Athos, whereas the JSprit code had to be modified at four different places. It is\nEmpir Software Eng (2022) 27:180 Page 21 of 52    180\nFig.6 Pictures from the corresponding survey questions Q09ATNW and Q09JSNW: The nodes (depot),\n(customer), (navigation) and edges — (roads), (highways) represent elements for which pre-\ndefined code was given in the task. Elements emphasized by bold black color had to be added to the program\nby customers\nimportant to note two things here: firstly, this is the only question where we had to have a\ndifferent number of subtasks; secondly, the higher number of subtasks did not result from\na more difficult task that participants had to perform but from the structure of the JSprit\ncode that did not allow to perform the task by only modifying three places of the program.\nThus, it remains a valid claim that the questions themselves were of identical structural\ncomplexity.\n4.8 PointAttributionandOpt-OutOption\nIn order to be able to statistically analyse the results of the studies, the answers had to be\nevaluated in order to derive thescore as a measure for participants’ effectiveness. For this, a\nscoring scheme had to be developed. We decided for a scheme in which each of the fifteen\nquestions was worth 10 points so that a maximum of 150 points could be achieved. Depend-\ning on the structure of the respective question, points were deduced for wrong answers,\nsyntactical and semantic mistakes. We took care to define objective and clear rules that\ndefined how many points for which exact mistake were deduced and we put special effort\ninto defining these rules in a way that no approach was favoured by these rules. This way\nwe sought to avoid biased marking which would have threatened thereliability of measures\nand would thus be a major threat to the validity of this study (see Section 6.1).\nThe study was conducted throughout the course of regular lectures (see Section 5.2)a n d\nit was emphasized both in the preparation and in the informed consent form of the study\nthat participation was completely voluntary. Not only were students allowed to refuse to\nparticipate, they were also given the chance to op-out of the questions and tasks of the\nsurvey. Generally, the survey system ensured that participants provided an answer for a\nquestion (or all of its tasks) before allowing to proceed to the next question. However, each\nquestion featured an op-out checkbox that participants could check to move on to the next\nquestion without the provision of an answer.\nEmpir Software Eng (2022) 27:180180   Page 22 of 52\nIn addition to questions that participants chose to op-out of, questions could also be left\nunanswered when students exceeded the 90 minute time limit. Finally, questions with text\nareas in which participants were supposed to enter code using the respective approach could\nbe left blank. It was also possible for participants to enter text that was clearly not intended\nto solve the respective task, e.g. apologetic statements for not being able to answer the\nquestion or complaints on the high difficulty of the task.\n4.9 In-andExclusionCriteria\nTo ensure that study results were not compromised by participants who did not seriously\nparticipate in the study, we applied a set of rules that defined what cases were to be in- and\nexcluded from the study. Firstly, if a data set contained more than seven non-attempts for one\nof the two approaches, the data set was removed from the results of the study (for example,\na data set that contained no non-attempts for Athos but eight non-attempts for JSprit would\nhave been excluded from the study). An answer was deemed as a non-attempt in cases were:\n– the opt-out box was checked,\n– no answer was given,\n– a text area was left empty,\n– a text area contained text that was clearly not intended to answer the task.\nIn a second step, for each group of both studies (i.e. FbAf, FbJf, WzAf, and WzJf),\nwe calculated the difference in the time spent with each approach. For each of the groups\nwe then analysed the resulting distribution by means of a stem and leaf plot and removed\noutliers. We repeated the same process for every group with the achieved scores. With this\napproach, we removed those cases in which a participant spent unusually more time with\none of the approaches or was unusually more successful with one of the approaches then\nwith the other. Our intention for the removal of these data sets was our conviction that such\nextraordinary differences in the time spent or the score achieved were most likely not due\nto differences in the approaches but the result of a loss of motivation or the occurrence of\na contingent event not related to the compared languages. These cases would have skewed\nthe within-subjects comparisons presented in Section 5.\nIn a final step, we only included those cases in which a participant either spent at least\n15 minutes or12 scored at least 25% of the achievable points with both Athos and JSprit. In\nother words, we removed all cases in which a participant spent less than 15 minutes without\nachieving at least 38 points for one or both approaches.\n4.10 TransformationofData\nAfter the evaluation of all obtained datasets and removal of non-attempting participants,\nsome of the data was transformed, i.e. new variables were derived from the existing ones. A\nsimple example is the variable numberOfLanguages from the number of ticked checkboxes\nand the number of additional languages stated in the survey. In Section 5.5,s c o r e sa n d\nthe time required to achieve the respective score will be presented. In order to be able to\nmathematically prove a connection between the used approach, the achieved score and the\ntime spent answering the questions, a metric that combined both the score and the time spent\n12This is a logical and not an exclusive or.\nEmpir Software Eng (2022) 27:180 Page 23 of 52    180\nwas required. From the field of behavioural research the rate correct score (RCS) (Woltz\nand Was 2006) was deemed well-suited for the data of the study. The RCS is defined as:\nRCS = c∑ RT (1)\nwith c denoting the number of correct responses (in a condition) and RT being the reac-\ntion time for a trial (Vandierendonck 2017). This definition was applied to the obtained\ndata by interpreting the c as the achieved score and ∑ RT as the time spent completing\nthe tasks. While the RCS is interpreted as “the number of correct responses per second of\nactivity” (Vandierendonck 2017, p. 654), a simple multiplication by 60 together with the\npresented interpretation yields the number of points per minute (PPM) spent in answering\nthe questions:\nPPM = 60 · c∑ RT (2)\nRCS is equal to another measure (Vandierendonck 2018) known as throughput (Thorne\n2006). Thorne (2006) notes that throughput is functionally comparable to number correct\nper qualified time unit , a measure they claim to be often applied in tests using pen and paper.\nThe same measure is also applied by Kosar et al. ( 2012, 2018) where it is referred to\nas participants’ efficiency. This is because a high value for either RCS, PPM and through-\nput can be regarded as an indicator for a participant working efficiently with the respective\napproach: A participant who achieved 120 points in 45 minutes (RCS: .044, PPM: 2.667)\ncertainly solved the tasks more efficiently than a participant who spent the full 90 minutes\non the tasks and also scored 120 points (RCS: .022, PPM: 1.333). However, it is debatable\nwhether a result of 20 points scored in 4 minutes (RCS: .083, PPM: 5), though more effi-\ncient, is more desirable than the aforementioned 120 points scored in 45 minutes. For this\nreason, the presentation of the study results in Section 5 gives insight on the scores alone\nand in relation to the time spent solving the tasks.\n4.11 StatisticalComparisonoftheApproaches\nThe design of our study allows for two directions in which the results of both approaches\ncan be compared. As is illustrated in Fig. 7, it is possible to compare dependent results\nFig.7 Different comparisons of the obtained results: Results obtained from two independent samples (Athos\nfirst vs JSprit first and Athos second vs. JSprit second) as well as two dependant samples (Athos first vs\nJSprit second and JSprit first vs Athos second) were compared\nEmpir Software Eng (2022) 27:180180   Page 24 of 52\nwithin the respective groups (within-subjects comparison), i.e. calculate the delta of points\nachieved with the first and the second approach. This can be done on the basis of each single\ndata set as well as in an aggregated way, e.g. by calculating the mean score for the first and\nthe second approach and then the difference of the two. The difference can then be tested\nfor statistical significance by application of the Wilcoxon signed-ranks test.\nThe other direction in which the results for two approaches can be compared is to com-\npare independent results across the two groups of one study (between subjects comparison).\nHere the results of both approaches used as a first approach (and also as a second approach)\nare compared. Statements on a statistically significant difference in the results can be made\nby application of the Mann-Whitney U test.\nFigure 7 also depicts the study results that were compared: Not only did we compare the\nresults in terms of the score achieved by participants, but also in terms of points in relation\nto time by testing the achieved PPM.\n5 Results\n5.1 ApplicationofExclusionCriteria\nIn Section 4.9, we presented the in- and exclusion criteria of the study. Table 2 summarises\nthe results of the application of these criteria to all submitted cases. The first column shows\nthe group to which the respective filter/measure was applied and the second column presents\nthe original number of cases for each group.\nColumns three to six show the number of participants who opted out of more than seven\nquestions for Athos, for JSprit, for either of the two, and for both. As was explained, a case\nwas excluded if a participants opted out of more than seven questions for either approach\nso that the fifth column is the one that gives the actual number of cases excluded due to\nthe number of unanswered questions. The sixth column is included in the table to show that\nmost cases that were excluded from the study exceeded the number of seven non-attempts\nfor both approaches.\nThe seventh and eighth column show the number of cases that were excluded due to an\nunusually high deviation (in relation to the observed deviation of the sample population)\nbetween the times spent and the scores achieved with the approaches, respectively. The ninth\ncolumn shows the number of cases that were removed because participants spent less than\nTable2 Number of cases excluded and included in the study\nGroup ORG Non-attempt filter I Deviation II INC\nNAAt NAJs OR* AND TME SCR MEF\nFbAf 63 16 23 24 15 4 0 0 35\nFbJf 55 16 14 17 13 0 0 4 34\nWzAf 19 1 2 2 1 0 2 1 14\nWzJf 22 1 0 1 0 1 2 0 18\nFor each group the table shows: the original cases (ORG); the number of cases with more than seven unan-\nswered questions for Athos (NAAt ), JSprit (NAJs ), either approach (OR*), and both approaches (AND); the\nnumber of cases with skewed times spent (TME), skewed scores (SCR), the number of cases without minimal\neffort (MEF), and the number of included cases (INC)\nEmpir Software Eng (2022) 27:180 Page 25 of 52    180\n15 minutes on one (or both) of the approaches without achieving at least 38 points. Finally,\nthe last column shows the number of included cases for the respective group.\nOverall, the data in the table show two important facts: firstly, the limit to allowed non-\nattempts was the one on which most exclusions were based. This means, that the majority\nof participants either just skimmed through the questions without answering them, or they\nput in the required effort for both approaches. Secondly, it can also be clearly seen that in\nthe Friedberg study there were substantially more cases excluded than in the Wetzlar study\ncourse. Given that the only compensation was that participation was announced to be a\nperfect training towards the final exam of the respective module, this was to be expected.\nStudents in Friedberg have more freedom in defining their individual curriculum than stu-\ndents enrolled in the co-operative study-program offered in Wetzlar. Thus, only a certain\nproportion of students attending a module actually sit the final exam. This fact is reflected\nin the data presented in the table and we do not consider the observed number of drop-outs\na threat to the validity of our study.\n5.2 PriorProgrammingKnowledge\nIn order to be able to put the language evaluation results into context, we presented par-\nticipants with questions on their programming background. Figure 8 compares the answers\ngiven by the two groups of the Friedberg participants, Fig. 9 does the same for the two\ngroups from the Wetzlar study. Overall, the results show that a) participants from the respec-\ntive groups of both studies (Athos first and JSprit first) possessed comparable prior pro-\ngramming skills and knowledge, and b) participants from Friedberg had less programming\nexperience than those from Wetzlar.\nIn terms of years of programming experience, Fig. 8 shows that in Friedberg the JSprit\nfirst group (FbJf) was slightly more experienced than the Athos first group (FbAf). Only\nseven members of the JSprit first group ( ≈ 21% of all group members) stated to have less\nthan a year of programming experience while twice as many members of the Athos first\ngroup (40%) claimed to have less than 12 months of programming experience. However, in\nboth groups the vast majority of participants did not have more than two years of program-\nming experience (more than 70% in both groups). Programmers with more than two years\nof experience were rare in both groups: only eight members of the Athos first group and ten\nmembers of the JSprit-first group stated to have more than two years of experience in the\nusage of computer languages. Based on these data, it can be concluded that most participants\nin the study conducted in Friedberg had only just started their programming careers.\nRegarding the other questions, results for both groups were generally very similar. In\nboth groups, most members had experience with two or three computer languages and there\nwere exactly seven participants in each group who had only worked with one language.\nThe two groups also had a similar number of participants that had used four or even more\nlanguages, though the Athos first group had slightly more participants in that category.\nAsked for a brief self assessment, in both groups the majority stated to consider themselves\nfair programmers. In both groups there was a strikingly high number of participants who\nexpressed doubt in their own programming skills: As many as 13 participants from the FbAf\nand 16 from the FbJf group thought of themselves as poor or even very poor programmers.\nThe majority of both groups was either interested or highly interested. While both groups\nalso had a surprisingly high number of participants with only moderate interest, only a\nnegligible number of participants stated their disinterest in the subject.\nFigure 9 illustrates the results of the groups from the Wetzlar study. As regards the years\nof experience, neither group appears to have had a definitive advantage over the other.\nEmpir Software Eng (2022) 27:180180   Page 26 of 52\nFig.8 Comparison of participants’ prior programming experience, number of programming languages used,\nself-assessed programming skills and general programming interest between the Friedberg group using Athos\nfirst ( ) and the Friedberg group using JSprit first ( )\nAlthough the JSprit first group consisted of more participants who had been programmers\nfor more than five years, the Athos first group on the other hand had more members with\nfour to five years of experience. While there were a few more members with 2–3 years of\nexperience in group WzJf, it is also important to consider that the group consisted of four\nmore members than the WzAf group.\nLooking at the answers to the other questions, the results for both groups are also rather\nsimilar. The distinct difference in the number of group members who had used 4–5 pro-\ngramming languages can be assumed to result from the different group sizes. In any case,\nboth groups nearly completely consisted of members who had experience with at least four\nlanguages. Only a minority of two participants in each group stated to have used only 2–3\ndifferent programming languages.\nEmpir Software Eng (2022) 27:180 Page 27 of 52    180\nFig.9 Comparison of participants’ prior programming experience, number of programming languages used,\nself-assessed programming skills and general programming interest between the Wetzlar study group using\nAthos first ( ) and the Friedberg study group using JSprit first ( )\nMembers of both groups expressed similar confidence in their programming skills. The\nmajority of both groups stated to be good programmers. However, in both groups there\nwere five participants who only considered themselves to be fair programmers. In both\ngroups, nearly all participants were either interested or even highly interest in the subject of\nprogramming.\nA comparison of the groups from Friedberg and Wetzlar clearly shows that participants\nfrom Wetzlar were more experienced, active and confident in the usage of programming lan-\nguages. The mode and median value for three of the prior knowledge questions are reported\nin Table 3. Overall, for each of the three questions, both the mode and mean value of the\ncombined Wetzlar study group were one level above the mode and mean of the combined\nFriedberg study group. Not reported in the table are the mode and median of the question\non participants’ interest in programming. For both the Friedberg and the Wetzlar study, the\nEmpir Software Eng (2022) 27:180180   Page 28 of 52\nTable3 Comparison of study groups\nExperience Languages Skills\nMode Mdn Mode Mdn Mode Mdn\nFriedberga\nOverall 1 – 2 1 – 2 2 3 Fair Fair\nAthos first < 1 1 – 2 2 2 Fair Fair\nJSprit first 1 – 2 1 – 2 3 3 Fair Fair\nWetzlarb\nOverall 2 – 3 2 – 3 4 5 Good Good\nAthos first 4 – 5 4 – 5 6 5.5 Good Good\nJSprit first 2 – 3 2 – 3 4 4.5 Good Good\naN = 69, nFbAf = 35, nFbJf = 34\nbN = 32, nWzAf = 14, nWzJf = 18\nmost frequent answer in both groups was that participants were “interested” in the subject\nof programming, which was also the median value.\nThese results allow two important conclusions: firstly, the validity of abetween-subjects\ncomparison, i.e. a comparison between the results of two groups of the same study, is not\nthreatened by a lopsided group assignment (see Section 6) since members of the two respec-\ntive groups in both studies had comparable programming knowledge. Secondly, the different\nknowledge levels observed for participants of the two studies from Friedberg and Wetzlar\nallow the assumption that participants from Friedberg can be considered to represent domain\nexperts with only limited programming experience, whereas participants from Wetzlar are\nmore suitable to represent junior software developers who possess a well-grounded back-\nground in programming. This provides some more context for the results presented in the\nfollowing sections.\n5.3 ScoreDistribution\nAs was mentioned in Section 4.2, we intended to find out whether Athos has the potential\nto facilitate comprehension of VRP models. For this reason, we formulated the hypothesis\nthat the application of Athos had no effect on the test results (scores) achieved by study par-\nticipants. In this section, we report the scores achieved by the groups of the two conducted\nstudies. In addition, we will present the outcome of various statistical tests performed on\nthe obtained data and discuss their implications on our hypothesis.\nFigure 10 presents an overview of the results obtained in the Friedberg study. What is\nparticularly noticeable is the marked difference in the scores for both languages used as\na first approach: the median value for Athos as a first approach surpassed the median of\nJSprit by more than 24 points. Even the lower quartile for Athos exceeds the median of\nJSprit by 7.5 points. It thus appears that for users with little programming experience, Athos\nhad a substantially positive effect on the observed scores. A comparison of the results for\nboth languages used as a second approach also shows that on average users scored higher\nwhen using Athos. However, the differences in the achieved scores do not appear to be as\ncompelling as those observed when both languages were used as first approaches.\nEmpir Software Eng (2022) 27:180 Page 29 of 52    180\nFig. 10 Boxplot showing the distribution of the scores achieved by participants of the Friedberg study\n(nFbAf = 35, nFbJf = 34)\nThe group that started with Athos exhibited declining scores when JSprit was used as a\nsecond approach. The median, for example, dropped from 92 points to a mere 74 points.\nEven the lower quartile with Athos first is above the median for JSprit as a second approach.\nThe top result with Athos first was a remarkable 146 points – with JSprit second the best\nresult dropped by 12 points to a score of 136. Conversely, when participants started with\nJSprit and used Athos as a second approach, there was a remarkable increase in points. The\nmedian drastically rose from 67.5 points to 94 points and the upper quartile increased from\n94.25 points to an impressive 121.25 points. With Athos second the top result was consid-\nerably higher than the best result achieved with JSprit first: with JSprit, the best participant\nscored 117 points which was 23 points less than the best score achieved with Athos second.\nFigure 11 gives an overview on the scores achieved in the Wetzlar study. What is\nimmediately obvious is the fact that participants in this study achieved higher scores than\nparticipants from the Friedberg study. This was to be expected due to the fact that par-\nticipants from Wetzlar had distinctly more experience in the field of programming (see\nSection 5.2). Looking at the respective interquartile ranges, it is also apparent that the\nFig. 11 Boxplot showing the distribution of the scores achieved by participants of the Wetzlar study\n(nWzAf = 14, nWzJf = 18))\nEmpir Software Eng (2022) 27:180180   Page 30 of 52\nresults were less dispersed than those in the Friedberg study. While there were exception-\nally high scoring and exceptionally low scoring participants in both groups, the majority of\nparticipants scored in the range of 100–140 points.\nJuxtaposing the results for both languages as the first approach, the results for JSprit were\nsomewhat better than those for Athos. With both approaches the best score was 148 points\nand with Athos no participant scored less than 100 points which is 26 points above the\nlowest score achieved with JSprit. However, the median score with JSprit is 10 points above\nthe median score achieved with Athos and both the upper and lower quartile comparison are\nin favour of JSprit.\nOn the other hand, if the languages were used as the second approach, the results for\nAthos were more favourable than those observed for JSprit. With Athos as the second\napproach, one participant even scored a perfect 150 points whereas the highest score with\nJSprit as the second approach was 139 points. The median value with Athos as the second\napproach was slightly above the median value for JSprit.\nA within-subjects comparison of both approaches shows that participants who started\nwith Athos were able to moderately improve their results with JSprit as a second approach.\nThe median score in that group rose by 11.5 points from 116 to 127.5 points. However,\nthe top result dropped from 148.0 to 139.0 points and the lower quartile also dropped by\n2.5 points. On the other hand, a similar improvement with the second approach could also\nbe observed when Athos was used second. Here, the median improved by 8 points and the\nother quartiles as well as both whiskers are also higher than those observed for JSprit as the\nfirst approach.\n5.4 StatisticalSigniﬁcanceTestsoftheScores\nFor the within-subjects comparison , we used the Wilcoxon signed-rank test. Table4 reports\non the results obtained. In the Friedberg study, both groups achieved significantly bet-\nter scores with Athos than with JSprit. The p-values of the tests for both groups are well\nbelow the 5% significance level. In Wetzlar, the group that started with Athos performed\nbetter when using JSprit, however, the result was not statistically significant. By contrast,\nthe group that started with JSprit performed significantly better with Athos as the second\napproach with a p-value of only 0.007 and thus high statistical certainty.\nTo statistically test the results of the between-subjects score comparison ,w eu s e dt h e\nMann-Whitney U test for which the results are displayed in Table 5. In Friedberg, Athos as\na first approach resulted in significantly higher scores than JSprit as a first approach. As a\nsecond approach, Athos was also the approach for which – on average – higher scores were\nobserved, though not at a statistically significant level. In Wetzlar, notwithstanding whether\nTable4 Within-subjects comparison of the achieved score using the Wilcoxon signed-rank test\nn Ties MdnScore WZ p\nAthos JSprit\nFb Athos first 35 0 92.0 74.0 84.5 − 3.776 0.000*\nFb JSpirt first 34 0 94.0 67.5 84.0 − 3.651 0.000*\nWz Athos first 14 0 116.0 127.5 38.0 − 0.912 0.362\nWz JSprit first 18 1 134.0 126.0 19.5 − 2.701 0.007*\nThe significance threshold (alpha) was defined to be 0.05\nEmpir Software Eng (2022) 27:180 Page 31 of 52    180\nTable5 Between subjects comparison of the achieved score using the Mann-Whitney U test\nN MdnScore MdnRnk UZ p\nAthos JSprit Athos JSprit Athos JSprit\nFb as first 35 34 92.0 67.5 42.30 27.49 339.5 −3.067 0.002*\nFb as second 34 35 94.0 74.0 37.97 32.11 494.0 −1.212 0.225\nWz as first 14 18 116.00 126.0 14.25 18.25 94.5 −1.200 0.230\nWz as second 18 14 134.00 127.5 18.22 14.29 95.0 −1.179 0.251\nThe significance threshold (alpha) was defined to be 0.05\nthey were used as a first or second approach, neither the use of Athos nor the use of JSprit\nresulted in significantly superior scores.\nThese results show that Athos has the potential to facilitate users’ effectiveness in pro-\ngram comprehension. Three out of four within-subjects comparisons show statistically\nsignificant improvements in the achieved scores for the Athos approach (FbAf, FbJf, WzJf).\nOnly one comparison was moderately in favour of JSprit (WzAf), i.e. not at a statisti-\ncally significant level. Of the between-subject comparisons, the only one with a statistically\nsignificant result (Friedberg, first approach) clearly favoured Athos as the more effective\napproach. Two more comparisons also were in favour of Athos (Friedberg, second approach;\nWetzlar, second approach), though not at a statistically significant level. Only one compar-\nison (Wetzlar, first approach) was in favour of Athos but also not at a level of statistical\nsignificance.\nIn three out of four tests for the Friedberg study group, the application of Athos elicited\na significant increase of achieved scores. Though there is no definite explanation as to\nwhy the positive effects on users’ effectiveness somewhat diminished when both Athos and\nJSprit were compared as second approaches, a reasonable assumption is that learning and\nexhaustion effects may have had a strong influence on the outcome. This is especially plau-\nsible given that participation in the study was completely voluntary and there was no special\nincentive for participants to keep the focus on the highest level for the entire study duration\nof up to three hours.\nThe results of the Wetzlar study group show a significant increase in scores when JSprit\nwas the first and Athos the second approach. This provides reliable evidence for the claim\nthat Athos has the potential to improve the effectiveness of more experienced program-\nmers in comparison to traditional approaches. When both languages were used as a second\napproach by the more experienced programmers from Wetzlar, the observed scores with\nAthos were also moderately better than those achieved with JSprit, but the improvement\nwas not enough to be statistically significant.\nIn case both languages were used as a first approach, the results for Athos were\nto some extent inferior compared to those achieved with JSprit. The same holds\ntrue for a within-subjects comparison that favours JSprit as the second approach\nover Athos as the first approach. These results show that despite its general poten-\ntial to enhance users’ benefits, there are certain situations in which more experienced\nusers might not benefit from the usage of Athos. But here it is important to note\nthat it was to be expected that the positive effects of Athos might be less pro-\nnounced among a group of more experienced programmers. As Bari ˇsi´ce ta l .( 2014)\nnoted, the positive effects of a DSL application are generally less distinct among\nparticipants with sophisticated programming knowledge. Two additional factors that\nEmpir Software Eng (2022) 27:180180   Page 32 of 52\nmight have contributed to these results are the fact that participants in the Wetzlar\nstudy had to learn and apply Athos at the very same day (see Section4.5) which might have\nbeen too short of a time to catch up with mostly 2–3 years of GPL experience. Secondly,\nit is possible that the presented questions were not complex enough for this group, since\nwe used the same questions for the comparably inexperienced participants from Friedberg.\nAs is reported by Kosar et al. (2018), improvement of results related to the application of a\nDSL are more distinguishable for tasks of high complexity.\n5.5 OverviewonResultsRelatedtoParticipants’Efﬁciency\nThe second research question asked whether Athos enhanced modellers’ efficiency in model\ncomprehension and creation. In Section 4.10, we defined the achieved PPM as a measure\nfor model comprehension efficiency. The next sections will present the observed efficiency\nresults of both studies.\n5.5.1 OverviewonEﬃciencyResultsfromFriedberg\nFigure 12 illustrates the amount of time participants from the two groups of the Friedberg\nstudy spent at the given language section as well as the final score they achieved. Each of\nFig. 12 Scatterplots of the scores achieved in relation to required time for Athos and JSprit used as a first\nand second method (data from the Friedberg study with less experienced programmers)\nEmpir Software Eng (2022) 27:180 Page 33 of 52    180\nthe four plots is divided into six parts. The part on the bottom left, for example, contains\ndata points representing those participants, that spent between 0 and 30 minutes trying to\nsolve the tasks with the respective approach and achieved less than 75 points. Data points\nin the upper right part represent those participants who scored more than 75 points within a\ntime frame of 60 to 90 minutes. Hence, the ideal approach would have the majority of data\npoints located in the upper left section, i.e. participants would have been awarded more than\n75 points in less than 30 minutes.\n5.5.2 Within-SubjectsComparison\nIn the group that started with Athos as a first approach, efficiency dropped to some extent\nwhen JSprit was used as a second approach: with Athos, 24 participants scored above 75\npoints within 60 to 90 minutes. With JSprit, the number of participants who scored over\nhalf the points in 60 to 90 minutes reduced to only 15. However, with JSprit as their second\napproach, there were six participants who missed the 75 points mark by only six or less\npoints.\nConversely, the group that started with JSprit as their first approach exhibited a massive\nefficiency gain with Athos as the second approach: with JSprit, only one single participant\nrecorded 75 points in less than 60 minutes. In stark contrast, with Athos as the second\napproach, as many as 17 participants scored 75 points or above in less than 60 minutes.\n5.5.3 BetweenSubjectsComparison\nIn order to compare how participants performed using either Athos or JSprit as a first\napproach, the two upper plots have to be analysed. A contrasting juxtaposition of the two\nplots suggests that participants scored higher in less time when using Athos as the first\napproach: with Athos, as many as 27 of the 35 participants (i.e. upwards of 75% of all par-\nticipants from that group) scored more than 75 points. Three participants who started with\nAthos managed to do so in a time window of 30 to 60 minutes. With JSprit first, only 15 of\nall 34 participants (a mere 44.1% of that group) achieved more than half of all points and\nonly one single participant achieved the score in a 30 to 60 minutes time window. Looking\nat the numbers of those who did not score at least half of all available points, with Athos\nonly 8 (22.9%) participants fell short of achieving 75 points. With JSprit, on the other hand,\nas many as 19 participants (55.9%) lost more than half of all points. With Athos as the first\nmethod only six participants (17.1%) spent more than an hour on the tasks without achiev-\ning at least 75 points. By contrast, with JSprit first 14 participants (41.2%) spent over 60\nminutes at the tasks and achieved less than 75 points. Used as a first approach, neither Athos\nnor JSprit saw a participant scoring close to 75 points in less than 30 minutes.\nA comparison of the results for both approaches used as a second approach also leads\nto the conclusion that participants of the Friedberg study were more efficient when using\nAthos: with JSprit, only 17 out of 35 participants scored higher than 75 points (48.6%);\nwith Athos, 21 out of 34 participants achieved more than 75 points (61.8%). From these\nparticipants, merely two JSprit users achieved their score within 60 minutes compared to\n17 Athos users who achieved more than half of the points within a time frame of up to 60\nminutes. With Athos as the second approach, there was even one participant who scored\nmore than 75 points in less than 30 minutes (78 points in 19 minutes and 21 seconds). With\nJSprit second, the closest a participant came to achieving 75 points in under 30 minutes was\na participant who scored 54 points in 15 minutes and 44 seconds.\nEmpir Software Eng (2022) 27:180180   Page 34 of 52\n5.5.4 EﬃciencyResultsfromWetzlar\nHaving compared the efficiency results for Athos and JSprit when used by participants with\ncomparably little programming experience, it is also interesting to see the results for both\napproaches applied by more experienced participants. Figure 13 illustrates the score and\nsurvey time of participants who took part in the Wetzlar study.\n5.5.5 Within-SubjectsComparison\nThe two diagrams on the left-hand side of Fig. 13 illustrate that the group which started\nwith Athos was noticeably more efficient with JSprit as their second approach. While the\nrecorded scores seemed somewhat similar, the required time to achieve these scores was\nmarkedly reduced with JSprit. With Athos, all 14 participants scored more than 75 points,\nbut only 9 (64.3%) did so in less than an hour. With JSprit, still all 14 participants scored\nmore than half of the achievable points; with JSprit as the second approach, however, 12\nparticipants (85.7%) finished in under 60 minutes.\nConsidering the right-hand side of Fig. 13, it is obvious that the group that used JSprit\nfirst and Athos second also exhibited a substantial increase of efficiency from the first to\nthe second approach. Here, the increase was even more distinct: with JSprit all but one\nFig. 13 Scatterplots of the scores achieved in relation to required time for Athos and JSprit used as a first\nand second method (data from the Wetzlar study with more experienced programmers)\nEmpir Software Eng (2022) 27:180 Page 35 of 52    180\nparticipant, i.e. 17 out of 18 (94.4%), scored above 75 points but not a single participant\nmanaged to do so in under 60 minutes. With Athos as the second approach, all 18 partici-\npants passed the 75 points mark and as many as 16 of these participants did so in under an\nhour. One participant even recorded 127 points in less than 30 minutes.\n5.5.6 Between-SubjectsComparison\nThe plots in the upper half of Fig.13 clearly show that a considerable number of participants\nsolved the Athos tasks in less time than the respective JSprit tasks and still achieved a score\nof 75 or higher: With Athos, all 14 participants were awarded more than 75 points and 9\nof these participants (64.3% of the entire sample) even managed to achieve their respective\nscore in 30 to 60 minutes. With JSprit, 17 of 18 participants (94.4%) passed the 75 points\nmark but none of these participants finished in less than 60 minutes.\nWhen used as a second approach, the time participants spent solving the tasks with the\nrespective approach appear to be rather similar: With Athos second, 16 out of 18 participants\n(88.9%) scored more than 75 points in less than 60 minutes; with JSprit, 12 out of 14\nparticipants (85.7%) collected more than 75 points in under 60 minutes. Though the survey\ntimes for the two approaches used as a second approach are not as far apart as when used\nas a first approach, participants who used Athos still spent less time than those who used\nJSprit. One of the participants who used Athos second even achieved more than 75 points\nin less than 30 minutes (127 points in 29 minutes and 56 seconds). With JSprit second, the\nclosest a participant with over 75 points came to the half-hour mark was a participant who\nscored 128 points in 38 minutes and 18 seconds. Looking at the scores, it seems that Athos\ncaught up with JSprit, especially in the number of high-scoring participants: With Athos\nsecond, 8 participants (44.4%) scored 135 or more points; With JSprit only 4 participants\n(28.6%) accomplished the same feat.\n5.6 TestResultsforParticipants’Efﬁciency\nThe results of the conducted significance tests confirm the observations made in the pre-\nvious section. Table 6 summarises the results of the within-subjects significance tests for\nwhich we applied the Wilcoxon signed-rank test. In Friedberg, there was a significant reduc-\ntion in achieved PPM when participants used Athos first and JSprit second. The other\nFriedberg group that had JSprit as the first approach displayed a significant increase in\nachieved PPM with Athos. In the Wetzlar study, there was a significant growth for either\nlanguage when used as a second approach. There is however, a difference in the observed p-\nvalues for both within-subjects comparisons: the growth observed with JSprit as the second\napproach barely remains below the 0.05 threshold for statistical significance; in the other\ngroup where Athos was used as the second approach, the observed p-value of 0.000 leaves\nno doubt that Athos as a second approach substantially enhanced users efficiency in model\ncomprehension and creation.\nThe results of the between-subjects significance tests are provided in Table 7.I nt h e\nFriedberg study, both groups were significantly more efficient when using Athos: with our\nDSL – both as a first and as a second approach – the achieved PPM proved to be significantly\nhigher than those obtained with JSprit.\nIn the Wetzlar study, results also hint at an increased efficiency of participants’ program\ncomprehension and creation when using Athos. While the previous section showed, that if\nonly the achieved scores were considered, the results of the group that used JSprit first were\nmoderately superior to those who used Athos as a first approach. Putting these scores in\nEmpir Software Eng (2022) 27:180180   Page 36 of 52\nTable6 Within-subjects comparison of the achieved PPM using the Wilcoxon signed-rank test\nn Ties MdnPPM WZ p\nAthos JSprit\nFb Athos first 35 0 1.2222 1.1218 152.0 − 2.670 0.008*\nFb JSpirt first 34 0 1.8913 0.8277 17.0 − 4.796 0.000*\nWz Athos first 14 0 2.0625 2.3719 21.0 − 1.977 0.048*\nWz JSprit first 18 0 3.0417 1.4412 0.0 − 3.724 0.000*\nThe significance threshold (alpha) was defined to be 0.05\nrelation to the time required to achieve them, however, shows that Athos was the sig-\nnificantly more efficient approach. Comparing both languages when applied as a second\napproach, also shows significantly superior efficiency results for Athos.\nThe presented results of participants’ efficiency (PPM) show that Athos substantially\nenhances the efficiency of end users. This is especially true for end users with limited\nknowledge in software development as the results from the Friedberg study group suggest.\nIn all four conducted tests participants exhibited significantly increased efficiency results\nwhen using Athos. This clearly supports the hypothesis that Athos can positively affect\nprogrammers’ efficiency in model comprehension and creation.\nThe results of the Wetzlar study provide evidence in support of the claim that Athos has\nthe potential to increase the efficiency of programmers who are experienced in the usage of\nGPLs and application libraries. Even though the group that started out with Athos and then\nused JSprit as a second approach exhibited significantly improved results with JSprit second,\nthe three remaining tests were clearly in favour of Athos as the more efficient approach.\nThe result from the within-subjects comparison suggest that learning effects seem to be\nan important factor in the achieved efficiency. In both groups the second approach yielded\nsuperior results in terms of PPM. Performing a between-subjects comparison ,h o w e v e r ,\nshows that Athos significantly increases participants efficiency.\n5.7 ResultsonObservedUserSatisfaction\nEnd user and/or domain-expert appreciation is a crucial factor that determines the success\nor failure of any DSL (Kahlaoui et al.2008). Hence, we wanted to gain some insight on how\nparticipants perceived the usage of both languages. In order to be able to answer the question\nof whether participants felt more satisfied with Athos than with a traditional approach for\nTable7 Between subjects comparison of the achieved PPM using the Mann-Whitney U test\nN MdnPPM MdnRnk UZ p\nAthos JSprit Athos JSprit Athos JSprit\nFb as first 35 34 1.2222 0.8277 42.77 27.00 323.0 −3.265 0.001*\nFb as second 34 35 1.8913 1.1218 42.79 27.43 330.0 −3.181 0.001*\nWz as first 14 18 2.0625 1.4412 20.43 13.44 71.0 −2.089 0.037*\nWz as second 18 14 3.0417 2.3719 19.89 12.14 65.0 −2.317 0.020*\nThe significance threshold (alpha) was defined to be 0.05\nEmpir Software Eng (2022) 27:180 Page 37 of 52    180\nmodelling VRPTWs, we designed the final part of the survey so that they could express their\nopinion on both languages. More precisely, for both languages participants were presented\nfive statements and asked to express their degree of agreement to the respective statement\nusing a five-point Likert scale.\nThe answers obtained from both studies leave no doubt that participants preferred work-\ning with Athos over using JSprit to model/program VRPTWs. In both studies, participants\nexpressed a similarly high appreciation for our DSL. By comparison, participants expressed\ndistinctly less satisfaction with the JSprit approach. As was to be expected, the participants\nfrom Friedberg with less Java experience were even more skeptical towards JSprit than\nparticipants from the Wetzlar study.\nFigure 14 illustrates the results obtained from the Friedberg study group. From 68 par-\nticipants who answered the question whether they agreed that Athos was easy to learn,\n81% gave their consent to this statement. For JSprit the same question was answered by\n67 participants but only 22% felt that JSprit was easy to learn. A considerable 82% stated\nthat Athos models were easy to read and understand whereas only 16% stated the same for\nJSprit. Nearly two out of three participants would affirm that modelling with Athos caused\nthem little effort. With JSprit, however, not even one out of five participants was under the\nimpression that model creation with JSprit was easy. A similarly high percentage was pos-\nitive towards Athos’ support of fast and efficient modelling (79%) as well as its potential\nin helping to avoid mistakes (69%). With JSprit participants appeared distinctively more\nskeptical to support these claims with only around 19% attesting to a potential for efficient\nmodelling and an even lower percentage (13%) feeling that the usage of JSprit could help\nto avoid mistakes.\nFig.14 Overview how participants from Frieberg perceived working with Athos and JSprit\nEmpir Software Eng (2022) 27:180180   Page 38 of 52\nThe results from the study group in Wetzlar are illustrated in Fig. 15. As can be seen,\nthe results obtained in this study are similarly positive towards Athos with agreement per-\ncentages ranging from 78% (avoidance of mistakes) to 97% (easy program/model creation).\nCompared to participants from Friedberg, participants from the Wetzlar study appeared to\nbe slightly more positive towards JSprit, though agreement percentages ranging from 16%\n(easy program/model creation) to 38% (easy to learn) remained low in comparison to the\nresults obtained for Athos.\n5.8 StatisticalTestsonObservedUserSatisfaction\nTo test the obtained results for statistical significance, we applied the Wilcoxon signed-rank\ntest on the answers for each statement. The test results are summarised in Table 8.T h et e s t\nresults unequivocally prove that users were more satisfied with Athos than they were with\nJSprit. For both the Friedberg and the Wetzlar study, Athos received a significantly higher\nlevel of agreement for every single positive statement on a given language characteristic.\nThese results provide substantial evidence that Athos increases satisfaction levels of both\nlanguage users with little programming experience as well as more experienced users at a\njunior developer level. Both type of users recognise and appreciate the effects that Athos\nhas on the comprehension and creation processes required to understand and write models\nfor VRPTWs.\nNot only did participants from both studies express their perception that Athos was easier\nto be learned, they also left no doubt that they felt that Athos models were easier to be read\nand understood. What is more, participants did not only prefer Athos in order to read and\nunderstand already existing models, they also clearly expressed their preference for Athos\nFig.15 Overview how participants from Wetzlar perceived working with Athos and JSprit\nEmpir Software Eng (2022) 27:180 Page 39 of 52    180\nTable8 Summary of the Wilcoxon signed-rank test applied to the obtained agreement levels to statements\non several language characteristics\nn Ties MdnLikert WZ p\nAthos JSprit\nFriedberg\nEasy to learn 67 11 4.0 3.0 145.0 − 5.416 0.000*\nEasy to read 66 10 4.0 2.0 109.0 − 5.686 0.000*\nEasy to create 64 12 4.0 2.0 132.0 − 5.125 0.000*\nFast and efficient 63 12 4.0 2.0 82.0 − 5.505 0.000*\nAvoidance of mistakes 62 13 4.0 2.0 61.0 − 5.537 0.000*\nWetzlar\nEasy to learn 32 4 5.0 3.0 28.5 − 4.071 0.000*\nEasy to read 32 1 5.0 2.5 34.0 − 4.234 0.000*\nEasy to create 32 2 4.5 2.0 26.0 − 4.343 0.000*\nFast and efficient 30 1 5.0 3.0 25.5 − 4.233 0.000*\nAvoidance of mistakes 30 5 4.0 3.0 43.0 − 3.254 0.000*\nLevels of agreement were expressed with a five-point Likert scale that ranged from 1 (Strong disagreement)\nto 5 (strong agreement)\nThe significance threshold (alpha) was defined to be 0.05\nwhen it comes to active creation of VRPTW models: participants stated that they considered\nmodel creation with Athos comparatively easy and effortless as well as fast and efficient.\nFinally, participants of both studies also stated their trust in the concise language syntax in\nsupporting them in the creation of correct models.\nAs was already mentioned, a high level of user acceptance is a necessary requirement\nfor the success of any DSL. Even though the presented levels of agreement are subjective\nperceptions of several language aspects rather than objective measurements of the respec-\ntive language characteristics, they are no less important. If users experience the usage of a\nlanguage in a negative way, it is likely that this will affect the results they produce with the\nrepudiated language. On the other hand, if users feel that a given language is a helpful tool\nfor a certain task, it is likely to positively affect the produced outcomes. This might even be\nreinforced by an easier and faster learning process that is not hindered by an inner distaste.\n5.9 QualityofTeaching\nIn order to address deviations in the quality of the training material and the time granted for\nlearning the approaches as possible threats to the internal validity of the study, we took care\nto not bias the results through either of the two means (see Section 6.2). To further ensure\nthat our efforts were successful, we wanted to gain insight on whether they were recognised\nby participants of the two controlled experiments. To this end, we asked participants whether\nthey considered the learning material provided for both approaches to be of similar quality\nand whether they had the impression that they were granted a comparable amount of time\nto learn both approaches. The results are presented in Table 9.\nAsked whether they would agree that the learning material was of nearly identical qual-\nity, 60 of 69 participants (87.0%) from Friedberg stated their approval of this claim. Four\nEmpir Software Eng (2022) 27:180180   Page 40 of 52\nTable9 Summary of the Wilcoxon signed-rank test applied to the obtained agreement levels to statements\non several language characteristics\nQuestion Athos Identical JSprit No answer\nFrq % Frq % Frq % Frq %\nFriedberg\nBetter learning material 4 5.8 60 87.0 0 0.0 5 7.2\nMore time to learn 2 2.9 58 84.1 6 8.7 3 4.3\nWetzlar\nBetter learning material 1 3.1 31 96.9 0 0.0 0 0.0\nEasy to read 0 0.0 30 93.8 2 6.3 0 0.0\nLevels of agreement were expressed with a five-point Likert scale that ranged from 1 (Strong disagreement)\nto 5 (strong agreement)\nparticipants (5.8%) deemed the Athos material superior and no participant considered the\nJSprit material superior. Five participants (7.2%) refused to answer the question. In the Wet-\nzlar study group, 31 of 32 participants (96.9%) agreed that the learning material was of\nnearly identical quality. One participant (3.1%) preferred the Athos learning material. No\nparticipant opted out of the question.\nRegarding the time granted to learn each approach, 58 out of 69 participants (84.1%)\nfrom the Friedberg study group would agree that the time for learning the approaches was\nnearly identical. Six participants (8.7%) felt that they were granted more time for learning\nJSprit, two participants had the impression that they were granted more time for Athos\n(2.9%). Three participants (4.3%) did not answer the question. In the Wetzlar study group,\n30 out of 32 participants (93.8%) stated, that the time they had for learning both approaches\nwas nearly identical. Two participants (6.3%) felt that they had more time to learn JSprit\nthan they had for Athos. No participant felt that more time for learning Athos was granted\nand no participant chose not to answer the question.\n6 ThreatstoValidity\nIn this section, we provide a summary of all aspects that we recognized as potential\nendangerments to the validity of this study. In order to overcome the problem that many\nexperimental studies in software engineering do not use the existing terminology for valid-\nity threats (Feldt and Magazinius 2010), we use the terminology presented in Wohlin et al.\n(2012) for this discussion. A brief overview on the identified threats is presented in Table10.\nThis table follows the structure used by Kosar et al. ( 2018). The next sections will provide\na detailed discussion of the various threats to the conclusion, internal, construct and exter-\nnal validity of our study. For each identified issue we briefly discuss why it might put the\nvalidity of this study at risk and explicate if and how we mitigated the respective threat.\n6.1 ThreatstotheConclusionValidity\nViolated assumption is a threat to the conclusion validity that stems from the selection of\ninappropriate significance tests, i.e. tests that come with requirements that the obtained data\nare not guaranteed to meet. In our study, there was no way to be absolutely sure that the\nEmpir Software Eng (2022) 27:180 Page 41 of 52    180\nTable10 Summary of the identified threats to the validity of the studies\nValidity Issue Cause Addressed\nConclusion Violated assumption Unknown distribution of results Yes\nReliability Subjective awarding of points Yes\nof measures Secret communication Yes\nRandom heterogeneity Participants with different prior\nknowledge and skills\nYes\nRandom irrelevancies Online with no control over physi-\ncal study environment\nNo\nInternal Maturation Occurrence of learning effects In part\nOccurrence of exhaustion effects In part\nInstrumentation Deviating task complexity Yes\nDeviating training material Yes\nDeviating time for teaching Yes\nFormulation of tasks (language) Yes\nMistakes in study tasks Yes\nBiased instruction in the study No\nCompensation Unequal incentives Yes\nConstruct Mono-method bias Effectiveness depends solely on the\nawarded scores\nIn part\nHypothesis guessing Participants may have worked to\nthe (dis)advantage of Athos\nYes\nInteraction of testing\nand treatment\nParticipants may have worked extra\ncarefully to reduce errors\nNo\nExternal Interaction of selection\nand treatment\nSubject population consists of students Yes\nInteraction of setting\nand treatment\nTasks were presented and to be\nsolved in a survey tool\nNo\nUsage of simple tasks No\nscores, the required times (and by extension the PPM) followed a specific distribution (e.g.\nnormal distribution). We addressed this with the application of parameterless tests that do\nnot make any assumptions on the underlying distribution of the obtained samples. However,\nit is to be noted that this choice bears the potential of introducing low statistical power as\nanother threat to conclusion validity due to the fact that paramterless tests are less powerful\nthan their parameterized counterparts in case that the tested data actually follow the assumed\ndistribution.\nReliability of measures potentially threatens the validity of the presented study since the\nresults on the effectiveness and efficiency are based on two measures: the score participants\nwere awarded for their answers and the time they spent answering the questions. The latter\nmeasure can be objectively taken (but may suffer from random irrelevancies discussed in\nthe next paragraph). The score, however, was awarded by the authors of this study and is\nthus the result of a process that is to some extent affected by subjective human interpretation\nwhich may be biased in one way or the other. The attribution of scores could be biased by\nthe researchers towards Athos to achieve the favoured results. Similarly, an overreaching\nattempt not to favour Athos in the awarded scores might bias the results in favour of JSprit.\nIn order to reduce the influence of subjective marking to a minimum, a fair and objective\nEmpir Software Eng (2022) 27:180180   Page 42 of 52\nscoring schema was developed and applied. The scoring schema was developed in a way that\nthe corresponding tasks and the respective steps needed to complete them, were awarded\nan identical (or at least comparable) amount of points. The scoring schema was especially\nimportant for the evolution tasks of the study, in which participants were not limited to a\npredefined set of option or radio buttons but could enter any model text they deemed fit\nto solve the task. The schema objectively defined which program parts participants had to\nadd for the solution to be considered correct. It also defined the partial points that each part\nof the expected code was awarded. While this approach does not completely eliminate the\nthreat to the reliability of the scores, it reduces the risk of bias towards either approach to a\nminimum.\nAt this point it is to be mentioned that another threat to the reliability of measures may\nbe posed by the fact that the study was conducted online and the environment thus not\nentirely controlled by the researchers. It is therefore possible that participants infringed the\nrule of non-communication between participants. However, we believe that this threat was\naddressed through the compensation for taking part in the study: participants were only\nindirectly rewarded for participation (also see below). Participants were explained that due\nto strict anonymisation there would be no individual reward based on the achieved score. As\nan incentive for participation it was announced that the tasks of the controlled experiment\nwould serve as an ideal preparation for similar questions that might be part of the final\nexamination (though all training material was provided to all students, i.e. independently\nof participation). This way, participants had only little reason to break the rules by secretly\nanswering the questions of the study together with a partner. Though we cannot be sure\nthat there was no rule infringement, we are confident that the taken approach served as a\nmitigation to this threat.\nRandom heterogeneity of subjects can threaten the conclusion validity of the study if\nparticipants of a study possess a heterogeneous set of characteristics that have an effect on\nthe observed result. In such groups, there is a chance that the different characteristics of\nthe participants are the actual source for observed differences in the results rather than the\napplied treatment. One way in which this threat could have occurred in our study would\nhave been through lopsided group assignment. By this we mean the assignment of skilled\nprogrammers to the groups in a way so that our preferred outcome was more likely to occur.\nFor example, if we intended to show that Athos was the more effective approach when used\nas a first approach, results supporting that claim would have been more likely if we assigned\nexperienced and skilled programmers to the Athos first group. This threat, however, was\neasily eliminated by randomising the group assignment. Even in the face of a randomised\ngroup assignment, there would have been a threat from the heterogeneity of subjects if we\nhad mixed participants from Friedberg and Wetzlar and formed groups (Athos first, JSprit\nfirst) from the unified pool of participants. In the worst case, all Wetzlar participants together\nwith the most highly skilled participants from Friedberg would have been assigned to one\ngroup and all remaining participants to the other. This would certainly have had a significant\neffect on the between-subjects comparison of both groups. By splitting our study into two\nstudies for either venue, this threat was mitigated. To show that in both studies the groups\nconsisted of similar skilled participants, we asked participants for a self-assessment of their\nskills as well as their prior knowledge. As was shown in Section 5.2, in both the Friedberg\nand Wetzlar study, the overall skill and knowledge level in both groups was not skewed in\nany direction.\nRandom irrelevancies in experimental setting is a threat to the conclusion validity of\nthis study that was not addressed. Since the study was conducted online (see Section 4.3),\nEmpir Software Eng (2022) 27:180 Page 43 of 52    180\nwe had no control over participants’ physical study environment and thus had no way to\nprevent or track any events that might have had an effect results participants produced. For\nexample, if a person entered the room and talked to a participant, this event is certain to have\nan effect on the time that a participant requires for the tasks as the disturbances will require\nthe participant to refocus. It may even have an effect on the produced score in case that\nthe unexpected moment of lost concentration causes a participant to provide an incorrect\nanswer. Though we did not explicitly take any measures to ensure that all participants took\nthe study in a controlled environment, there are still characteristics in our study design\nthat might serve as a mitigation to this threat. Firstly, with regard to the between-subjects\ncomparison, the homogeneity of the groups of both studies allows for the assumption that\nthe occurrence of disturbing events is equally distributed among participants of both groups.\nSecondly, as our study was embedded into the scope of a lecture, participants are likely to\nhave established a routine that made the occurrence of disturbing events during lectures less\nlikely. In addition, we urged participants to not get distracted during the study and focus on\nanswering the questions for both approaches.\n6.2 ThreatstotheInternalValidity\nMaturity which might result from learning and exhaustion effects poses a major threat to\nthe validity of this study. These effects must be expected to occur since participants first\nsolved a batch of tasks for one approach and after that solve a set of structurally similar\ntasks with the other approach. It is most likely that participants have learned the structure\nof a given problem to some extent when confronted with it for a second time. This is likely\nto result in a higher score for the approach that is applied as the second approach. On the\nother hand, when participants already answered all questions for a given approach, it is\nalso likely that they might feel mentally exhausted by this point. This most certainly has a\nnegative affect on their concentration and thus most likely negatively affects their score for\nthe second approach. For a pilot study for which no data exists, it is impossible to predict\nexactly whether (and to what extent) learning effects predominate exhaustion effects or in\nreverse, exhaustion has a greater effect on the result than knowledge gained from the first\ntime answering the respective question. To avoid these effects entirely, we followed the\napproach taken by Kosar et al. ( 2010) and formed two groups (in each study) that applied\nthe two compared approaches in opposite order. Contrary to Kosar et al. ( 2010), we did\nnot aggregate the results from both sub groups. Instead, we compared the results for the\napproaches when used as a first approach and also when used as a second approach (see\nSection 4.11).\nInstrumentation threats originate from inappropriate material and processes that may\nconfound the observed results. One possible cause that might have threatened the instru-\nmentation of the study would have been a deviation in the complexity of questions (and their\nrespective tasks) which could have seriously compromised the validity of this study. If the\nquestions for one approach had been less difficult than the questions for the other approach,\nthis certainly would have resulted in better scores and reduced the time required for the\napproach with the simpler questions. As a mitigation, we carefully designed the sections for\nboth approaches so that corresponding questions were of the same structural complexity.\nThis means that corresponding questions had the same number of nodes and edges so that\nneither was more difficult to comprehend (see Section4.7). We also ensured that the actions\nparticipants were supposed to perform were of comparable complexity, e.g. that they were\nsupposed to add or change the same number of elements in two corresponding questions.\nEmpir Software Eng (2022) 27:180180   Page 44 of 52\nInferior training material and unequal time for learning the approaches were two more\ninstrumentation threats that we recognised and mitigated: failure to produce training mate-\nrial of comparable quality would have posed a sincere threat to the validity of the study\nbecause it most likely would have had a major effect on the obtained results. For this rea-\nson, we took care of producing comparable learning aids. We achieved this by defining two\ngeneric example VRPTW problems (the one from the introductory case study and a slightly\nmodified version with an additional product and an additional depot). For each approach\nwe created a set of slides that illustrated how to use the respective approach to model the\ndefined example problems. As far as the approaches would allow, the slights showed how\nto model the problems using the exact same steps (at some points slight deviations were\ninevitable due to the different structures of Athos and JSprit). Participants also received four\npages with example programs (each page with the complete Athos or JSprit program for\none of the two example problems).\nIn the training session, the slides were presented to teach participants how to use the\napproaches. If one approach had received substantially more teaching time, this would have\nbeen another instrumentation threat to the validity of the study. However, we did not define\nan exact amount of time that was to be spend on Athos and JSprit since both approaches\nhad a different number of slides since JSprit required more lines of code for the example\nproblems. Instead, we defined an upper limit of 90 minutes for the presentation of either\napproach. Within this time frame, both approaches were presented and all questions that\noccurred in the scope of the presentation were answered. For both approaches participants\nwere asked whether any part of the presentation required additional discussion or whether\nthere were unanswered questions left. Neither Athos nor JSprit required the full 90 minutes\nbefore all questions were answered and participants signalled to have understood the rele-\nvant syntactical elements and associated semantics of both approaches. To gain information\non whether we were successful in our effort to introduce participants to both approaches\nin a comparable manner, we asked them in the last section for their feedback on this mat-\nter. In both study groups, the vast majority of participants acknowledged that both the time\nfor learning the approaches and the material provided to do so was nearly identical (see\nSection 5.9).\nAnother threat to validity concerns the design of the tasks participants were supposed to\nanswer in the study. An inappropriate language of the tasks poses a serious threat. Presenta-\ntion of the tasks in English only (as it is the language in which the study is communicated)\nin lecture modules for which the official language is German is likely to affect the results\nas there is a high chance that some participants lack the necessary skills in the language.\nWe thus provided the tasks in both German and English so that participants could switch\nbetween the languages at their own discretion.\nEven though we took utmost care in setting up the survey and formulating the questions\nwhich were also checked by members of the study group, there remained some flaws in\nthe formulation of some of the tasks. This led to participants demanding clarification dur-\ning the online study, which was provided. However, there is a chance that in some cases\nparticipants did not bother or dare to ask for an explanation of an ambiguously or falsely\nformulated task even though they might not have properly understood it. This may have led\nto some participants providing answers without fully understanding the question. However,\nenquiries on the tasks were evenly distributed which is why we consider it unlikely that\nflaws in the formulations of the tasks have biased the results in favour of any of the two\ncompared approaches.\nEmpir Software Eng (2022) 27:180 Page 45 of 52    180\nThere was a technical issue with one of the Athos questions 13 in which 18 checkboxes\nwere presented to participants. The correct answer would have demanded checking five\nof the boxes but the system would only accept a maximum of three checked boxes. As a\nremedy, we adapted the marking grid (rating scheme) for this and the corresponding JSprit\nquestion so that it was possible to gain the complete score even though not all answers were\nprovided. This way, the originally intended answer key of both the Athos and the corre-\nsponding JSprit tasks was modified in favour of both approaches. Though we are convinced\nthat it is unlikely that this approach biased the results towards Athos, it remains a (minor)\nthreat to the validity of the results (for this question). As a general remedy, we are currently\nconducting a replication of the study in order to validate and verify our results and to further\ncontribute reliable data to researchers in the field of DSL evaluation.\nAn unaddressed instrumentation threat is posed by the researcher who instructed par-\nticipants throughout the study: we decided that in all studies one and the same researcher\nwas to introduce participants to the approaches and direct the controlled experiment. This\ndecision addressed the threat that opting for different study instructors would have posed:\nwith different instructors, differences in the results of the groups could have been caused by\ndiffering teaching skills of the instructors rather than differences in the approaches or dif-\nferences in the participants of the study (important for a comparison between the Friedberg\nand the Wetzlar study). However, the fact that the instructor was involved in the design and\nimplementation of Athos poses a threat to the validity of the study. While every effort was\ntaken to present both approaches in a neutral and objective way, it cannot be guaranteed that\nthe presentation was completely free of any bias, even if only at a subconscious level. These\nbiases of the instructor may have influenced participants in their learning and their answers.\nIt is, however, important to emphasise, that the instructor took care to present both languages\nin a way that students approached them with an open mind. The threat of subconscious bias\nduring instruction could have been mitigated by having an independent person direct the\nstudy. However, this would have posed several other problems: most importantly, no inde-\npendent person who possessed the necessary knowledge in the domain and the approaches\nwas available as an instructor. An instructor with insufficient knowledge in one or both\napproaches would have caused an even more severe threat to the study validity.\nA last threat to the internal validity was posed by thecompensation of participants. Since\ndata collection was strictly anonymised, there was no way to directly compensate partici-\npants who solved the tasks to the best of their ability. Instead, a question on both approaches\nwas announced to be part of the final exam of both modules in which the study was embed-\nded. In the announcement, it was emphasised that both approaches were of equal importance\nfor the final exam. Neglecting this aspect would have posed a compensation threat as par-\nticipants might have been lead to believe that one of the approaches might be more relevant\nin the final exam. This would have skewed the effort invested in learning the two different\napproaches and by extension the achieved scores and required time.\n6.3 ThreatstotheConstructValidity\nMono-method bias is a threat to the construct validity of this study that was only partly\naddressed. In terms of the model activities that participants had to perform, we created a set\nof questions that covered a wide range of tasks necessary for the specification of VRPTWs\nso that there is no mono method bias in this regard. However, the fact that the result for\n13Question Q01ATNW\nEmpir Software Eng (2022) 27:180180   Page 46 of 52\nparticipants’ effectiveness is solely based on the score that participants achieved introduces\nunaddressed mono-method bias into the study. It might be worthwhile to define additional\nmeasures for effectiveness in future studies in order to be able to cross check the observed\nresults and thus increase future studies’ validity.\nHypothesis guessing threatens the validity of a study if participants conjecture on the\npurpose of the study and adapt their behaviour in support or opposition of this purpose.\nWe addressed this threat by openly communicating the purpose of the study and asking\nparticipants explicitly to not put in extra effort for one of the two languages. We explained,\nthat the most helpful behaviour for us would be to solve the tasks for both approaches\nwith an unprepossessed attitude so that we would be able to gain insight on the particular\nstrengths and weaknesses of our language. We believe, that the vast majority of participants\nwho did not drop out of the study understood the scientific relevance of the study and tried\nto use both approaches without any prejudice.\nInteraction of testing and treatment is an unaddressed threat that occurs whenever par-\nticipants increase their effort due to the knowledge that they are being tested. Even though\nwe asked participants to act as normally as possible, it is evident that there is no way that\nparticipants behave the exact same way as they would have behaved when not being in a test\nsituation. The fact that participants were assured that their results were strictly anonymised\nmight slightly mitigate this threat (but it may also lead to a less careful usage of both\napproaches) as participants had no personal gain from achieving a perfect result. Another\npossible mitigation to this threat might be that the study was conducted online and thus\nparticipants’ physical environment was familiar to them and it did not resemble the typical\nclass room surrounding (again, this might also negatively affect the achieved scores). Thus\nit is important to note that this threat could not be entirely eliminated.\n6.4 ThreatstotheExternalValidity\nIn our study, students enrolled in two different study courses from two different campuses\nwere selected for participation. Though this is a common practice found in the vast majority\nof controlled experiments in software engineering (Sjoeberg et al.2005), this type of conve-\nnience sampling poses the threat ofinteraction of selection and the treatment . This potential\nthreat occurs if the selected sample population does not appropriately represent the target\npopulation so that the generalisation from the sample to the target population is invalid. We\naddressed this threat by conducting two controlled experiments that represented the two dif-\nferent types of target users of our DSL, i.e. domain experts with little knowledge of software\nengineering and developers experienced in software development but with limited domain\nknowledge.\nEvidently, the ideal study group is a direct sample of the respective target population.\nHence, our sample populations have some inherent limitations with respect to generalisabil-\nity to our target users. For example, our Friedberg participants who are to be generalised to\ndomain experts, do not possess the same knowledge that domain experts (e.g. fleet dispatch-\ners) have acquired in their job. However, they likely do have a similar (or perhaps slightly\nsuperior) level of knowledge in terms of programming languages. Our participants from\nWetzlar, on the other hand, might have less programming knowledge than senior software\ndevelopers, but given that they are all employed by an industrial company that involves them\nin different software projects, they adequately represent junior software developers. By col-\nlection and reporting of data on participants’ programming background, skills and interest,\nwe provided quantitative evidence that allows for an assumption on the degree to which our\ndata can be generalised to the intended target population.\nEmpir Software Eng (2022) 27:180 Page 47 of 52    180\nFinally, an unaddressed threat to the external validity of our study can be found in the\ninteraction of setting and treatment . This is due to the fact that in our controlled experiment,\nparticipants were not allowed to use an IDE. This is a deviation from a real-world industrial\nworking environment were modellers are supported by IDEs that offer mechanisms such\nas syntax checking and highlighting and code completion features. Though we handed par-\nticipants two cheat sheets for each approach (in working environments modellers normally\nare free to use any material that facilitates their work), we explicitly banned the usage of\nIDEs. This most likely negatively affects the observed scores and times. However, it should\nnot invalidate our results as both approaches were identically affected by this restriction.\nAnother threat that stems from the setting is that the tasks were simplified academic tasks.\nWhile they were not trivial, they did not possess the high complexity that would be found\nin real-world industrial tasks. However, with the simplified tasks, we were able to focus on\nspecific aspects of both languages, so that we believe this design decision to be justified.\n7 Conclusion\nIn this study, we gave a brief introduction to Athos and presented a thorough evaluation\nof the DSL. We gave insight on how the evaluation study was conducted as a controlled\nexperiment in which participants tackled a set of structurally identical tasks with Athos\nand the Java application library JSprit as a baseline approach. The study was conducted\namong two study groups with distinctive prior knowledge in terms of software engineering.\nOne study group mostly consisted of students at the beginning of their career as software\ndevelopers whereas the other comprised only more experienced students enrolled in a co-op\nstudy program.\nIn terms of effectiveness of rather inexperienced users, a within-subjects comparison\nshows that both the Athos first and the JSprit first group exhibited significantly improved\nscores with Athos. A between-subjects comparison of the results obtained from inexperi-\nenced users shows that when participants used the compared languages as a first approach,\nAthos users achieved significantly better results than JSprit users. Comparing both lan-\nguages as a second approach also shows that scores achieved with Athos were higher,\nhowever this result is not statistically significant. With regard to efficiency, both within-\nsubjects comparisons (Athos first, JSprit first) show a statistically significant increase in\nPPM with Athos. The same goes for both between-subjects efficiency comparisons that both\nshow a significantly increased number of PPM with Athos.\nThese results provide very strong evidence for the claim that Athos is a DSL that bene-\nfits language users with only little knowledge in software development. With Athos, these\nlanguage users can model VRPTWs more effectively and more efficiently than they could\ndo with the GPL alternative. Since participants were second semester students enrolled in\na programming related course, special care must be taken when generalising these results.\nHowever, we showed that our participants from Friedberg had only little experience in the\nusage of programming languages. It can thus be assumed that experts from the domain of\nVRPTWs will similarly benefit from the usage of Athos as a language to model problems\nfrom their domain.\nFor more experienced users, a within-subjects comparison of the exhibited effectiveness\nreveals that when JSprit was used first and Athos second the scores achieved with Athos\nare significantly higher than those achieved with JSprit. However, in the other group that\nused Athos before JSprit, the JSprit results were better, though not at a level of statistical\nEmpir Software Eng (2022) 27:180180   Page 48 of 52\nsignificance. Neither of the two between-subjects comparisons (as first and as second\napproach) showed a statistically significant advantage for either approach. In terms of\nobserved efficiency among more experienced users, both between-subjects comparisons\n(as first and as second approach) and one within-subjects comparison (JSprit first) show a\nstatistically significant advantage for our Athos DSL.\nThese results show that the gains in effectiveness brought about by Athos are less distinc-\ntive among more experienced users than they are with programming beginners. This was to\nbe expected since these participants were less familiar with the (descriptive) Athos approach\nthan they were with the imperative Java language. For this reason it is all the more important\nto stress that one of the tests showed a statistically significant effectiveness increase with\nAthos. With regard to efficiency the results provide convincing evidence for the claim that\neven more experienced users become considerably more effective in modelling VRPTWs\nwhen using Athos as their modelling language. Since these more experienced participants\nwere software engineering students enrolled in a co-op program, their results can be gener-\nalised to professional software developers who support domain experts in the development\nof traffic and transport simulations.\nNot only did Athos enhance the effectiveness and efficiency of users with little pro-\ngramming knowledge and those with advanced experience in software development, it also\nachieved substantially higher satisfaction levels among both types of language users. The\npresented results leave no doubt that participants preferred the application of Athos to\nthe usage of the GPL. Though some care must be taken, it seems admissible to gener-\nalise these results to domain experts and professional software developers who are likely\nto also prefer to model VRPTW related problems with our DSL than with the alternative\nbaseline approach. This has important implications since a higher level of satisfaction is\nlikely to positively affect the quality of solutions produced by domain experts and software\ndevelopers.\nWith this paper we made the following contributions:\n– We provided substantial quantitative evidence in support of the claim that Athos has the\npotential to increase the effectiveness of program interpretation and creation.\n– Our results clearly suggest that especially end users with only limited programming\nknowledge can greatly benefit from the usage of our DSL. This claim holds merit in terms\nof achieved effectiveness (score) and efficiency (score in relation to time required).\n– Our results provide additional evidence to the claim that the positive effects ensuing\nfrom application of a DSL are less distinct for end users with sophisticated knowledge\nin software engineering.\n– The results with regard to the observed user satisfaction provide striking evidence that\nAthos has the potential to significantly increase the level of felt and expressed content-\nedness of users. This was shown to be a valid claim for both modellers with little and\ndevelopers with more advanced programming knowledge.\n– With the design of our study we have also expounded the possible effects of the study\ndesign on the outcome. Especially when used by experienced programmers, the actual\nlanguage was less important than possible learning effects. This suggests that when\nusing a within-subjects comparison the results should never be interpreted without\nproper consideration of these effects.\nWe consider the presented study another valuable contribution to the body of knowledge\nin the field of DSL evaluation. As our results are generally in line with findings of other\nresearchers in the field, our study adds additional authority to the respective results.\nEmpir Software Eng (2022) 27:180 Page 49 of 52    180\nSupplementary Information The online version contains supplementary material available at\nhttps://doi.org/10.1007/s10664-022-10210-w.\nFunding Open Access funding enabled and organized by Projekt DEAL. The research presented in this\narticle received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.\nA vailability of Data and Material Material used in the presented study is made available via GitHub at\ngithub.com/benjaminh20/AthosEvaluation2020/.\nCode A vailability The code for the Athos DSL is made available by the corresponding author upon request.\nDeclarations\nEthics Approval All procedures performed in the presented study that involve human participants were in\naccordance with the ethical standards of the institutional and/or national research committees.\nConsent to Participate Informed consent was obtained from all individual participants prior to their par-\nticipation in the presented study. Participants consented to their anonymised (and aggregated) data being\npublished.\nC o n f l i c to fI n t e r e s t sAll authors have no conflict of interest to report.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nBaldacci R, Battarra M, Vigo D (2008) Routing a heterogeneous fleet of vehicles. In: The vehicle routing\nproblem: latest advances and new challenges, operations research/computer science interfaces, vol 43.\nSpringer US, Boston, pp 3–27. https://doi.org/10.1007/978-0-387-77778-8 1\nBariˇsi´cA ,A m a r a lV ,G o u l˜ao M (2012a) Usability evaluation of domain-specific languages. In: 2012 eighth\ninternational conference on the quality of information and communications technology (QUATIC).\nIEEE, Piscataway, pp 342–347. https://doi.org/10.1109/QUATIC.2012.63\nBariˇsi´cA ,A m a r a lV ,G o u l˜ao M, Barroca B (2012b) How to reach a usable dsl? Moving toward a sys-\ntematic evaluation. Electronic Communication of The European Association of Software Science and\nTechnology.https://doi.org/10.14279/tuj.eceasst.50.741.750\nBariˇsi´cA ,A m a r a lV ,G o u l˜ao M, Barroca B (2014) Evaluating the usability of domain-specific languages.\nIn: Software design and development: concepts, methodologies, tools, and applications, IGI Global,\npp 2120–2141. https://doi.org/10.4018/978-1-4666-4301-7.ch098\nBasili VR (1994) Goal question metric paradigm. Encyclopedia of Software Engineering:528–532. https://\nci.nii.ac.jp/naid/10011278869/en/\nChallenger M, Kardas G, Tekinerdogan B (2016) A systematic approach to evaluating domain-specific\nmodeling language environments for multi-agent systems. Software Quality Journal 24:755–795.\nhttps://doi.org/10.1007/s11219-015-9291-5\nCordasco G, D’Auria M, Negro A, Scarano V, Spagnuolo C (2021) Toward a domain–specific language for\nscientific workflow–based applications on multicloud system. Concurrency and Computation: Practice\nand Experience 33(18):66. https://doi.org/10.1002/cpe.5802\nEmpir Software Eng (2022) 27:180180   Page 50 of 52\ndo Nascimento LM, Viana DL, Am Neto PS, Martins DAO, Garcia VC, Meira SRL (2012) A systematic\nmapping study on domain-specific languages. In: Proceedings of the 7th international conference on\nsoftware engineering advances (ICSEA’12), pp 179–187\nEwais A, de Troyer O (2014) A usability evaluation of graphical modelling languages for authoring adaptive\n3d virtual learning environments. In: Proceedings of the 6th international conference on computer sup-\nported education - V olume 1, SCITEPRESS - Science and Technology Publications, Lda, Setubal, PRT,\nCSEDU 2014, pp 459–466. https://doi.org/10.5220/0004947204590466\nFeldt R, Magazinius A (2010) Validity threats in empirical software engineering research - an initial sur-\nvey. In: Proceedings of the 22nd international conference on software engineering and knowledge\nengineering (SEKE), vol 16, Redwood City, California, pp 374–379\nHermans F, Pinzger M, van Deursen A (2009) Domain-specific languages in practice: a user study on\nthe success factors. In: Model driven engineering languages and systems: 12th international confer-\nence, MODELS 2009, Denver, CO, USA, October 4-9, 2009. Proceedings. Springer, pp 423–437.\nhttps://doi.org/10.1007/978-3-642-04425-0 33\nHoffmann B, Chalmers K, Urquhart N, Farrenkopf T, Guckert M (2018) Towards reducing complexity of\nmulti-agent simulations by applying model-driven techniques. In: Advances in practical applications of\nagents, multi-agent systems, and complexity: the PAAMS Collection. Springer International Publishing,\npp 187–199. https://doi.org/10.1007/978-3-319-94580-4 15\nHoffmann B, Guckert M, Chalmers K, Urquhart N (2019) Simulating dynamic vehicle routing problems with\nathos. In: ECMS 2019 proceedings, European council for modeling and simulation, vol 33, pp 296–302.\nhttps://doi.org/10.7148/2019-0296\nHoffmann B, Urquhart N, Chalmers K, Guckert M (2020) Athos: an extensible dsl for model driven traffic\nand transport simulation. In: Modellierung 2020, Gesellschaft f¨ur Informatik e.V , Bonn, pp 141–156\nIung A, Carbonell J, Marchezan L, Rodrigues E, Bernardino M, Basso FP, Medeiros B (2020) System-\natic mapping study on domain-specific language development tools. Empirical Software Engineering\n25:4205–4249. https://doi.org/10.1007/s10664-020-09872-1\nJedlitschka A, Ciolkowski M, Pfahl D (2008) Reporting experiments in software engineering. In: Guide to\nadvanced empirical software engineering. Springer, London, pp 201–228.https://doi.org/10.1007/978-1-\n84800-044-5 8\nJohanson AN, Hasselbring W (2017) Effectiveness and efficiency of a domain-specific language for high-\nperformance marine ecosystem simulation: a controlled experiment. Empirical Software Engineering\n22(4):2206–2236. https://doi.org/10.1007/s10664-016-9483-z\nKahlaoui A, Abran A, Lefebvre ´E (2008) Dsml success factors and their assessment criteria. Metrics News\n13(1):43–51\nKahraman G, Bilgen S (2015) A framework for qualitative assessment of domain-specific languages.\nSoftware & Systems Modeling 14(4):1505–1526. https://doi.org/10.1007/s10270-013-0387-8\nKosar T, Oliveira N, Mernik M, Pereira MJ, ˇCrepinˇsek M, da Cruz D, Henriques P (2010) Comparing\ngeneral-purpose and domain-specific languages: an empirical study. ComSIS–Computer Science an\nInformation Systems Journal 7:247–264. https://doi.org/10.2298/CSIS1002247K\nKosar T, Mernik M, Carver JC (2012) Program comprehension of domain-specific and general-purpose\nlanguages: comparison using a family of experiments. Empirical Software Engineering 17:276–304.\nhttps://doi.org/10.1007/s10664-011-9172-x\nKosar T, Bohra S, Mernik M (2016) Domain-specific languages: a systematic mapping study. Information\nand Software Technology 71:77–91. https://doi.org/10.1016/j.infsof.2015.11.001\nKosar T, Gaberc S, Carver JC, Mernik M (2018) Program comprehension of domain-specific and general-\npurpose languages: replication of a family of experiments using integrated development environments.\nEmpirical Software Engineering 23:2734–2763. https://doi.org/10.1007/s10664-017-9593-2\nMayer T, Uhlig T, Rose O (2016) An open-source discrete event simulator for rich vehicle routing prob-\nlems. In: 2016 IEEE 19th international conference on intelligent transportation systems (ITSC). IEEE,\npp 1305–1310. https://doi.org/10.1109/ITSC.2016.7795725\nMernik M, Heering J, Sloane AM (2005) When and how to develop domain-specific languages. ACM\nComput Surv 37(4):316–344. https://doi.org/10.1145/1118890.1118892\nOmbuki B, Ross BJ, Hanshar F (2006) Multi-objective genetic algorithms for vehicle routing problem with\ntime windows. Applied Intelligence 24:17–30. https://doi.org/10.1007/s10489-006-6926-z\nPillac V, Gendreau M, Gu ´eret C, Medaglia AL (2013) A review of dynamic vehicle routing problems.\nEuropean Journal of Operational Research 225(1):1–11. https://doi.org/10.1016/j.ejor.2012.08.015\nPoltronieri I, Zorzo AF, Bernardino M, de Borba Campos M (2018) Usa-dsl: usability evaluation\nframework for domain-specific languages. In: Proceedings of the 33rd annual ACM symposium on\napplied computing, association for computing machinery, New York, USA, SAC ’18, pp 2013–2021.\nhttps://doi.org/10.1145/3167132.3167348\nEmpir Software Eng (2022) 27:180 Page 51 of 52    180\nPoltronieri I, Pedroso AC, Zorzo AF, Bernardino M, de Borba Campos M (2021) Is usability evaluation of\ndsl still a trending topic? In: Kurosu M (ed) Human-computer interaction. Theory, methods and tools.\nSpringer International Publishing, Cham, pp 299–317\nPr¨umper J (1999) Test it: Isonorm 9241/10. In: Human-computer interaction – communication, cooperation,\nand application design. L. Erlbaum Associates Inc, New Jersey, USA, pp 1028–1032\nSjoeberg D, Hannay JE, Hansen O, Kampenes VB, Karahasanovic A, Liborg NK, Rekdal AC (2005) A\nsurvey of controlled experiments in software engineering. IEEE Transactions on Software Engineering\n31(9):733–753. https://doi.org/10.1109/TSE.2005.97\nde Sousa LM, da Silva AR (2018) Usability evaluation of the domain specific language for spatial simulation\nscenarios. Cogent Engineering 5(1). https://doi.org/10.1080/23311916.2018.1436889\nISO/IEC 25010 (2011) Systems and software engineering. Systems and software quality requirements and\nevaluation (square). System and software quality models. https://doi.org/10.3403/30215101\nTekinerdogan B, Arkin E (2019) Pardsl: a domain-specific language framework for supporting deployment\nof parallel algorithms. Software & Systems Modeling 18(5):2907–2935.https://doi.org/10.1007/s10270-\n018-00705-w\nThorne DR (2006) Throughput: a simple performance index with desirable characteristics. Behavior\nResearch Methods 38(4):569–573. https://doi.org/10.3758/BF03193886\nTisue S, Wilensky U (2004) Netlogo: design and implementation of a multi-agent modeling environment. In:\nProceedings of the Agent 2004 Conference, pp 161–184\nvan Solingen R, Berghout E, Berghout EW (1999) The goal/question/metric method: a practical guide for\nquality improvement of software development. The McGraw-Hill Companies, London\nVandierendonck A (2017) A comparison of methods to combine speed and accuracy measures of\nperformance: a rejoinder on the binning procedure. Behavior Research Methods 49(2):653–673.\nhttps://doi.org/10.3758/s13428-016-0721-5\nVandierendonck A (2018) Further tests of the utility of integrated speed-accuracy measures in task switching.\nJournal of Cognition 1(1). https://doi.org/10.5334/joc.6\nWelch P (2017) Developing a commercial dynamic vehicle routing system - a case study.\nhttps://doi.org/10.13140/RG.2.2.14915.30247\nWohlin C, Runeson P, H¨ost M, Ohlsson MC (2012) Experimentation in software engineering. Springer Berlin\nHeidelberg, Berlin\nWoltz DJ, Was CA (2006) Availability of related long-term memory during and after attention focus in\nworking memory. Memory & Cognition 34:668–684. https://doi.org/10.3758/BF03193587\nPublisher’snote Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional affiliations.\nAﬃliations\nBenjaminHoﬀmann1 ·NeilUrquhart2 ·KevinChalmers3 ·MichaelGuckert1\nMichael Guckert\nmichael.guckert@mnd.thm.de\n1 Kompetenzzentrum f¨ur Informationstechnologie, Technische Hochschule Mittelhessen,\nFriedberg, Germany\n2 School of Computing, Edinburgh Napier University, Edinburgh, Scotland\n3 School of Arts, University of Roehampton, London, England\nEmpir Software Eng (2022) 27:180180   Page 52 of 52",
  "topic": "Digital subscriber line",
  "concepts": [
    {
      "name": "Digital subscriber line",
      "score": 0.9055871963500977
    },
    {
      "name": "Executable",
      "score": 0.8999701142311096
    },
    {
      "name": "Computer science",
      "score": 0.7770209312438965
    },
    {
      "name": "Domain-specific language",
      "score": 0.7139486074447632
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6187025904655457
    },
    {
      "name": "Software engineering",
      "score": 0.6146246194839478
    },
    {
      "name": "Empirical research",
      "score": 0.5025131702423096
    },
    {
      "name": "Software",
      "score": 0.4944312572479248
    },
    {
      "name": "Programming language",
      "score": 0.48495033383369446
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45155027",
      "name": "Technische Hochschule Mittelhessen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I251738",
      "name": "Edinburgh Napier University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I877107187",
      "name": "University of Roehampton",
      "country": "GB"
    }
  ]
}