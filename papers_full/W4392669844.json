{
  "title": "Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models",
  "url": "https://openalex.org/W4392669844",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3206320674",
      "name": "Levon Haroutunian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098887313",
      "name": "Zhuang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A7122111",
      "name": "Lucian Galescu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2077550809",
      "name": "Philip Cohen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222967491",
      "name": "Raj Tumuluri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3176456866",
    "https://openalex.org/W3089025530",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4385571265",
    "https://openalex.org/W3214637961",
    "https://openalex.org/W3102018700",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385573257",
    "https://openalex.org/W2143349571",
    "https://openalex.org/W3173210704",
    "https://openalex.org/W4206710430",
    "https://openalex.org/W4385573324",
    "https://openalex.org/W3098495697",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3186412754",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4226226396",
    "https://openalex.org/W3115328016",
    "https://openalex.org/W4386566629",
    "https://openalex.org/W4288109580",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W4205646617",
    "https://openalex.org/W4385573770",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3018305985",
    "https://openalex.org/W3099942180",
    "https://openalex.org/W2971187756",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W3170014162",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2890585661",
    "https://openalex.org/W4307537763",
    "https://openalex.org/W2963374482",
    "https://openalex.org/W4298187912",
    "https://openalex.org/W3168664364",
    "https://openalex.org/W4385570948",
    "https://openalex.org/W3035565536",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W1493882948",
    "https://openalex.org/W2251957808",
    "https://openalex.org/W4385572167",
    "https://openalex.org/W3176957840",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4226389314",
    "https://openalex.org/W2970544186",
    "https://openalex.org/W2971087717"
  ],
  "abstract": "Levon Haroutunian, Zhuang Li, Lucian Galescu, Philip Cohen, Raj Tumuluri, Gholamreza Haffari. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of\nthe Asia-Paciﬁc Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1067–1082\nNovember 1–4, 2023. ©2023 Association for Computational Linguistics\n1067\nReranking for Natural Language Generation from Logical Forms:\nA Study based on Large Language Models\nLevon Haroutunian, Zhuang Li,\nLucian Galescu, Philip Cohen, Raj Tumuluri, Gholamreza Haffari\nOpenstream.ai\n{levon, zhuang.li, lucian, phil.cohen, raj, reza.haffari}@openstream.com\nAbstract\nLarge language models (LLMs) have demon-\nstrated impressive capabilities in natural lan-\nguage generation. However, their output qual-\nity can be inconsistent, posing challenges for\ngenerating natural language from logical forms\n(LFs). This task requires the generated outputs\nto embody the exact semantics of LFs, without\nmissing any LF semantics or creating any hal-\nlucinations. In this work, we tackle this issue\nby proposing a novel generate-and-rerank ap-\nproach. Our approach involves initially generat-\ning a set of candidate outputs by prompting an\nLLM and subsequently reranking them using a\ntask-specific reranker model. In addition, we\ncurate a manually collected dataset to evaluate\nthe alignment between different ranking met-\nrics and human judgements. The chosen rank-\ning metrics are utilized to enhance the training\nand evaluation of the reranker model. By con-\nducting extensive experiments on three diverse\ndatasets, we demonstrate that the candidates\nselected by our reranker outperform those se-\nlected by baseline methods in terms of semantic\nconsistency and fluency, as measured by three\ncomprehensive metrics. Our findings provide\nstrong evidence for the effectiveness of our ap-\nproach in improving the quality of generated\noutputs.\n1 Introduction\nWe consider the problem of natural language gener-\nation (NLG), which involves generating fluent and\nfaithful utterances from structured meaning repre-\nsentations such as LFs (Wang et al., 2021a; Chen\net al., 2020a). This task has gained significant im-\nportance, particularly for applications such as data\naugmentation for semantic parsing (Wang et al.,\n2021a) or question-answering systems (Ribeiro\net al., 2021b), as well as response generation for\ndialogue systems (Yu et al., 2019). This task plays\na crucial role in enhancing the performance and\ncapabilities of these systems by providing them\nwith diverse and high-quality natural language ut-\nterances aligned with their underlying logical rep-\nresentations.\nLLMs have shown impressive performance\nacross various NLG tasks (Chen et al., 2021;\nOuyang et al., 2022). However, the utterances gen-\nerated based on LFs sometimes suffer from various\ndeficiencies, such as hallucinations or missing parts\nof the input LF (Chen et al., 2020a). As depicted in\nFigure 1, only 1 out of 4 candidates generated by\nthe generator accurately and fluently reflects the se-\nmantic meaning of LF answer(density_1(m0)).\nThe remaining generated texts either introduce in-\naccuracies (#4) or are awkwardly phrased (#1 and\n#2).\nTo improve the quality and fidelity of natural lan-\nguage generated from LFs, we take a generate-and-\nrerank approach that combines a fixed LLM gener-\nator with a finetuned reranker that discriminatively\nscores candidates given several pre-determined\nmetrics (Suzgun et al., 2022). As in Figure 1, our\nreranker successfully assigns the sole accurate and\nfluent candidate (#3) generated by the generator\na higher score than the other candidates. Further-\nmore, this method is very flexible: it can be applied\nto any dataset that pairs LFs with natural language,\nregardless of the formalism employed, and can be\ntrained to align with any numeric metric.\nWhile implementing our method, it became ev-\nident that a reliable reference ranking metric was\nnecessary during both the training and evaluation\nphases of the reranker. However, determining the\nmost suitable text quality evaluation measure for\nour specific task remained unclear. To address this,\nwe manually curate an evaluation set, enabling us\nto thoroughly assess the alignment between vari-\nous evaluation metrics and human judgement. By\nmeasuring the extent to which evaluation metrics\naccurately reflect human judgement, we are able to\nidentify the most effective metrics for ranking the\nquality of generated texts and improve our generate-\n1068\n# exemplars\nLF: answer ( loc_1 ( m0 ) ) \nNL: where is m0\n...\n# candidates\n1. what density does m0 have\n2. what is the density of m0\n3. what is the population \ndensity of m0\n4. how many people are in m0\n# ranking\n1. what is the population \ndensity of m0\n2. what is the density of m0\n3. what density does m0 have\n4. how many people are in m0\n# logical form\nanswer ( density_1 ( m0 ) )\ngenerator\nreranker\nFigure 1: A high-level view of our approach. First, a\ngenerator model is given a set of exemplars and the LF\nof interest, from which it generates a set of candidates.\nThe reranker is given this output, along with the LF, to\nproduce a ranking of the candidates.\nand-rerank approach.\nOur contributions are:\n• We introduce a novel generate-and-rerank ap-\nproach for generating natural language text\nfrom LFs using LLMs. This approach lever-\nages the strengths of LLMs in initial text gen-\neration, followed by a reranking process to\nselect the most fluent and semantically faith-\nful candidates. The experiments show that our\nreranker significantly outperforms other can-\ndidate selection baselines across three datasets\nin terms of three evaluation metrics.\n• We conduct an in-depth analysis of various\npre-trained metrics by utilizing a carefully cu-\nrated dataset. This analysis allows us to iden-\ntify and select the metrics that effectively pro-\nduce rankings of natural language candidates,\nprioritizing fluency and semantic fidelity.\n• Through extensive experimentation, we pro-\nvide valuable insights and recommend strate-\ngies for developing the optimal training data\nfor a reranker, considering limitations on the\ngeneration budget. These strategies aim to\nmaximize the performance and effectiveness\nof the reranking process.\n2 Related Work\nNLG. There is a large body of work concern-\ning NLG from logical forms and/or structured data\n(Gardent et al., 2017; Chen et al., 2020a; Parikh\net al., 2020; Gehrmann et al., 2021; Shiri et al.,\n2022). Chen et al. (2020a) argues that NLG is\nbest formulated as the task of generating text from\nLFs, as opposed to generating directly from struc-\ntured data. This is the task of interest in our work,\nsimilar to others’ work in SparQL-to-text (Ngomo\net al., 2013), SQL-to-text (Xu et al., 2018; Ma et al.,\n2021), and AMR-to-text (Song et al., 2018; Zhu\net al., 2019; Ribeiro et al., 2021a, 2019).\nRecent work considers the use of LLMs for few-\nshot NLG (Chen et al., 2020b; Heidari et al., 2021)\nand semantic parsing (Drozdov et al., 2022; Shin\net al., 2021; Shin and Van Durme, 2022; Zhuo\net al., 2023) via in-context learning. Few-shot ap-\nproaches to these tasks generally involve construct-\ning a prompt containing a handful of training ex-\namples and sampling responses from an LLM with-\nout any training or fine-tuning beyond the LLM’s\npre-training. This method produces state-of-the-art\nresults despite in some cases using only a fraction\nof the data required by other methods. Follow-\ning these works, and specifically, the suggestion in\nShin and Van Durme (2022) that LLMs trained on\ncode are suited to the task of semantic parsing be-\ncause LFs are similar to code, we use Codex (Chen\net al., 2021) as our generation model.\nRe-ranking. This work is influenced by discrimi-\nnative reranking approaches in machine translation\n(Lee et al., 2021; Bhattacharyya et al., 2021), se-\nmantic parsing (Arcadinho et al., 2022), abstractive\nsummarization (Liu and Liu, 2021), text genera-\ntion (Langkilde-Geary, 2002; Deng et al., 2020; Li\net al., 2022), data-to-text (Harkous et al., 2020),\ntextual style transfer (Suzgun et al., 2022), and\nmathematical reasoning (Cobbe et al., 2021).\nLee et al. (2021) introduces a discriminative\nreranking approach (DrNMT) for neural machine\ntranslation, utilizing a pre-trained language model\nto predict the BLEU score of a candidate translation\ngiven the source sentence. Unlike our approach,\nwhich employs a margin ranking loss function, they\ntrain DrNMT by minimizing the Kullback–Leibler\ndivergence (Kullback and Leibler, 1951) of the can-\ndidate and target scores. Meanwhile, Arcadinho\net al. (2022) employ a similar reranking approach\nin semantic parsing. Their T5QL model incorpo-\nrates a ranking model (fine-tuned CodeBERT) to\npredict the correctness of a generated candidate\nparse from a given natural language question. In\ncontrast, our model uses a similar architecture but\nworks in reverse, generating text from LFs. Liu and\n1069\nLiu (2021) present a contrastive learning method,\nSimCLS, for ranking abstractive summarization\ncandidates. The authors finetune a RoBERTa en-\ncoder to measure the alignment of a summary with\nthe text it originates from: the embedding of a\nhigher quality summary will be more similar to the\nembedding of the original text than the embedding\nof a lower quality summary. Similar to our work,\nthey train their model by minimizing a ranking loss\nfunction.\n3 Reranking Approach\nIn this section, we present details of our methods,\nincluding our choice of generator model, reranker\narchitecture, and evaluation metric.\n3.1 Problem Formulation\nGiven a pool of LFs paired with their natural lan-\nguage utterances, our task is to generate a natural\nlanguage utterance y corresponding to an LF x. In\nthis work, we first generate a set of n-best candi-\ndates ˆYx := {ˆy1, ˆy2, ...,ˆyn}, and then rerank them\nusing a reranker based on a quality score. We as-\nsume that with access to one ground truth utterance\ny corresponding to the input LF x, we would be\nable to calculate the quality score for each candi-\ndate using a function Q(ˆyi|x, y). In our setting,\nQ is an automatic metric to score the quality of a\ngenerated candidate text against the ground-truth\ntext, such as BLEU (Papineni et al., 2002). These\nquality scores would determine the relative ranking\nof the n-best candidates, and would allow us to\nchoose the optimal text output.\nOur goal is to train a reranker model to predict\nthe relative order of the values assigned byQ given\nonly x; that is, without access to the gold reference\ny. This is achieved by training the parameters θ of\nthe scoring function Rθ(ˆyi|x).\n3.2 Generator\nWe prompt Codex (Chen et al., 2021) in a few-shot\nsetting to generate natural language candidates for\na given LF. Each prompt includes a number of\nexemplars1 randomly drawn from the training set,\npresented as simple input/output pairs. An example\nprompt is given in Appendix C.1.\nTo create training data for the reranker model,\nwe generate natural language candidates for LFs\n1It is 15 in our experiments.\nin the training set by repeatedly prompting Codex2\nuntil there are n unique candidates per logical form.\nAt inference time, we construct prompts for each\nLF in the test set in much the same manner. The\nscore G(ˆy|x) denotes the log-probability of ˆy given\nthe input x by Codex.\n3.3 Reranker\nOur reranker model is composed of CodeBERT\n(Feng et al., 2020) as the base model and a feed-\nforward regression head over the [CLS] token.\nFor each forward pass, the input to the reranker\nconsists of a LF concatenated with a natural lan-\nguage candidate, separated with an EOS token. The\noutput is a single real-valued number that repre-\nsents the relative quality of the candidate.\nWe finetune the model using the Hug-\ngingface library 3. We also use the pub-\nlicly available checkpoint for CodeBERT\n(microsoft/codebert-base)4, which has\napproximately 110M parameters.\nLoss Function The training objective for our\nreranker is to minimize a weighted margin ranking\nloss across pairs of natural language candidates.\nFor each set of candidates corresponding to one LF,\nthe loss is,\nL(θ) =\nPn\ni,j;i̸=j max[0, −zi,j(ˆzi,j + γ)]\nn(n − 1)\nwhere n is the number of candidates, and γ repre-\nsents a margin. The value of zi,j := Q(ˆyi|x, y) −\nQ(ˆyj|x, y) is the difference between the gold qual-\nity scores of candidates i and j. Its magnitude\nreflects the relative importance of obtaining the\ncorrect ranking for the pair. The score ˆzi,j :=\nRθ(ˆyi|x) − Rθ(ˆyj|x) represents the predicted dif-\nference between candidates i and j.\n3.4 Scoring of the Candidates\nAt test time, we use either the re-ranker score or its\ncombination with the generator probability score\nto select the winning candidate in the n-best list.\nThe combined score is\nλRθ(ˆyi|x) + (1− λ)G(ˆyi|x) (1)\n2We use the code-davinci-002 model of Codex, which\nhas around 175B parameters, with a temperature of 0.7 in our\nexperiments.\n3huggingface.co\n4github.com/microsoft/CodeBERT\n1070\nwhere λ is a hyperparameter that is tuned on the\ndevelopment set. In practice, we found that the\nvalue of λ did not generalize well across datasets\nor even across seeds; a separate λ value was thus\ntuned for each model run.\n4 Evaluation of Text Generation Metrics\nfor Reranking\nAutomatic evaluation of (generated) text quality is\nnot an easy task, and poses challenges for build-\ning the reference rankings for candidate sets in the\ntraining set and fairly evaluating the generation\nability of generators. Therefore, we curate an eval-\nuation set to evaluate the effectiveness of several\ntext generation metrics.\nGeneration Metrics. We consider the following\npre-existing metrics5:\n• BLEU6 (Papineni et al., 2002), which explic-\nitly measures the lexical overlap between ref-\nerence and hypothesis.\n• BERTScore (Zhang et al., 2019) and BLEURT\n(Sellam et al., 2020; Pu et al., 2021), which\nframe evaluation as a regression task.\n• PRISM (Thompson and Post, 2020) and\nBARTScore (Yuan et al., 2021), which frame\nevaluation as a generation task.\nWe also use probability scores from a semantic\nparser as an additional metric. For each dataset,\nwe train a semantic parser by finetuning CodeT5\n(Wang et al., 2021c) to generate LFs from natu-\nral language utterances. Then, we use the trained\nparser to calculate the probability that a generated\ncandidate is parsed to the original LF. This score\nmeasures the faithfulness of the candidate to the\noriginal semantics of the LF. Unlike other metrics\nabove, the parser probability is calculated based on\nthe generated candidate and the LF, as opposed to\nthe generated candidate and the reference text.\nIn addition to evaluating individual metrics, we\nalso evaluate their combinations. When combin-\ning metrics, we first normalize the scores for each\nmetric so that the mean score is 0 and the standard\ndeviation is 1, and then we sum the normalized\nscores across possible metrics. We normalize the\nscores in order to ensure that each metric is given\nthe same weight in relation to the others.\n5We do not finetune or otherwise modify these metrics.\n6We use the NLTK implementation of BLEU; nltk.org\nCuration of Evaluation Data. To determine the\nalignment of each metric with human preferences,\nwe constructed a small, manually-crafted evalua-\ntion set. We randomly selected 200 LFs from the\ntrain split of CFQ-MCD1 (Keysers et al., 2019),\neach with eight generated candidates. Each candi-\ndate is labelled either ‘correct’ or ‘incorrect’; rather\nthan producing a strict ranking in this evaluation\nset, we instead opted for binary classes to allow\nfor the fact that multiple candidates can be equally\nacceptable.\nWe developed a set of criteria to account for both\nsemantic accuracy and fluency, presented below:\n(i) If a candidate omits a piece of information\nthat appears in the reference, it is incorrect.\n(ii) If a candidate inserts or substitutes a piece\nof information that does not appear in the\nreference, it is incorrect.\n(iii) If a candidate is markedlyless fluent (e.g. con-\ntains unnatural constructions) compared to\nother candidates in the set, it is incorrect.\n(iv) If a candidate contains terms that appear in\nthe LF (e.g. ?x0) but should not appear in the\nutterance, it is incorrect.\n(v) Otherwise, the candidate is correct.\nUsing these criteria, a human annotator assigned\nbinary labels to the candidates in the evaluation set.\nEvaluation Measures. To assess the alignment\nbetween the chosen metrics and human judgements,\nwe calculated (i) top-1 accuracy, or the probability\nthat the highest-scoring candidate in a set belongs\nto the ‘correct’ class; and (ii) ranking accuracy,\nor the probability that any ‘correct’ candidate is\nranked above any ‘incorrect’ candidate. In the cal-\nculation of these values, the sets of candidates in\nthe evaluation set that are comprised of only one\nclass (i.e., either all are incorrect or all are correct)\nare excluded.\nResults. Table 1 provides the results of our evalu-\nation for each metric, as well as for the combination\nof all metrics and the best-performing combination\n(BLEURT + PRISM + Parser).\nOur findings support the conclusion by Freitag\net al. (2022) that trained metrics outperform BLEU,\nand the suggestion by Amrhein et al. (2022) and\nMoghe et al. (2022) that a combination of different\nfamilies of metrics is likely to be stronger than any\n1071\nMetric Top-1 Ranking\nBARTScore 70.62 75.57\nBERTScore 73.45 78.11\nBLEU 66.67 73.40\nBLEURT 71.75 81.96\nPRISM 76.27 81.19\nParser 76.27 79.41\nCombination of above metrics 79.10 82.59\nBLEURT + PRISM + Parser 81.36 84.22\nTable 1: Top-1 accuracies and ranking accuracies (rep-\nresented as percentages) across the evaluation dataset.\none metric alone. Each of the three metrics in our\nbest-performing combination work in very different\nways: BLEURT is an encoder-only model that is\ntrained to predict direct assessment scores assigned\nto machine translation outputs by human evalua-\ntors (Pu et al., 2021), PRISM is an encoder-decoder\nmodel trained for NMT and deployed as a zero-shot\nparaphraser (Thompson and Post, 2020), while our\nparser is an encoder-decoder model trained to con-\nvert text into LFs. We suspect that the differences\nbetween these models contribute to their strength\nin combination. Furthermore, we speculate that\nparser probability scores reflect the semantic con-\nsistency between a candidate and the reference LF,\nwhile BLEURT and PRISM scores more strongly\nreflect a candidate’s surface-level similarity to the\nreference text and its overall fluency.\nFollowing these results, we use the combination\nof scores assigned by BLEURT, PRISM, and the\ntask-specific semantic parser to determine refer-\nence rankings for training the reranker.\n5 Experiments\n5.1 Datasets\nWe conducted experiments using three datasets:\n• GeoQuery: This dataset consists of 880 En-\nglish questions focusing on the geography of\nthe United States (Zelle and Mooney, 1996).\nWe report results for both the standard split\nand the query split (Finegan-Dollak et al.,\n2018). The train and test sets in the query\nsplit contain distinct sets of LFs.\n• Jobs: The Jobs dataset comprises 640 En-\nglish queries that correspond to LFs in a jobs\ndatabase (Califf and Mooney, 1999). We\npresent results based on the standard split of\nthis database.\n• CFQ: CFQ contains approximately 239,000\nsynthetic English questions paired with\nSPARQL queries (Keysers et al., 2019), with\nthree different data splits designed to maxi-\nmize compound divergence between training\nand test sets. Our study focuses on the MCD1\nsplit, which consists of 96,000 training pairs\nand 12,000 test pairs.\nFor each dataset, we generate natural language\ncandidates for LFs in the training set as described\nin section 3.2, with n = 8natural language candi-\ndates per logical form. The candidate generation\nprocess requires repeated calls to Codex, which is\na significant bottleneck. Consequently, only 30k\ntraining pairs (240k total candidates) are used for\nexperiments on CFQ-MCD1. Following Drozdov\net al. (2022), we map Freebase identifiers to simpler\nkeywords. See Appendix D for the full mapping.\n5.2 Baselines\nWe compare the performance of our model against\nthree different baselines and one ORACLE method.\n• Random selection: A candidate is selected\nrandomly from the set of unique candidates\ngenerated for each LF. This method serves as\na lower bound.\n• Self-consistency: The most frequently ap-\npearing candidate is selected. Ties are broken\nrandomly. This method is proposed in Wang\net al. (2022) for use with chain-of-thought\nstyle prompting and is extended for use with\nsimpler prompting styles in Drozdov et al.\n(2022).\n• Highest generator probability: For each can-\ndidate, the token log probabilities given by the\ngenerator are averaged. The selected candi-\ndate is the one with the highest score.\n• Oracle: Scores for each of the three metrics\n(BLEURT, PRISM, and parser probability)\nare normalized and summed for each candi-\ndate. The candidate with the highest com-\nbined score is selected. The performance of\nthis method serves as an upper bound.\n5.3 Training Details\nAt the beginning of training, the base model of the\nreranker is frozen, and loss is only backpropagated\nthrough the regression head. After 10 epochs, the\nfinal layer of the base model is unfrozen, and loss\nis backpropagated through both the regression head\n1072\nMethod Jobs GeoQuery (standard) GeoQuery (query) CFQ-MCD1\nbleurt parser prism bleurt parser prism bleurt parser prism bleurt parser prism\nOracle 70.7 95 .0 6 .9 86.0 93 .6 40 .3 86.1 89 .3 40 .3 71.5 64 .3 22 .2\nRandom 59.4 84 .7 3 .0 74.2 77 .6 23 .1 74.4 85 .2 22 .7 61.1 44 .7 13 .0\nSelf-cons. 60.7 89 .1 3 .4 77.5 79 .7 26 .5 78.3 85 .2 27 .2 62.7 47 .1 14 .3\nGenerator 61.3 93 .0 3 .7 77.8 81 .8 28 .1 79.0 85 .7 29 .0 64.6 49 .0 15 .8\nReranker 62.7 93.5 4.4 81.0 90.9 32.5 79.9 89.8 28.9 66.3 61.9 17.2\nCombined 62.7 93.7 4.3 81.2 91.1 32.9 80.0 89.7 29.1 66.5 61.9 17.4\nTable 2: BLEURT, parser, and PRISM scores for re-ranker and baselines. The oracle method selects the candidate\nwith the highest combined BLEURT, PRISM, and parser score; the random method selects a candidate at random;\nthe self-consistency method selects the most frequently generated candidate; the generator method selects the\ncandidate with the highest probability from the generator (conditioned on the prompt); the reranker method selects\nthe candidate with the highest score from the trained reranker; and the combined method selects candidates based\non a linear combination of generator and reranker scores. Parser and PRISM scores are probabilities represented as\npercentages. Bold values represent the highest (non-oracle) score in the column, and underlined scores represent a\nstatistically significant improvement from generator scores (p < 0.01).\nand this final layer of CodeBERT for the remainder\nof the training. The optimization details are given\nin Appendix A.\n5.4 Main Results\nOur experiment results can be seen in Table 2,\ndepicting the performance of our reranker model.\nTraining of the reranker is performed five times\nwith different seeds, and we report the mean score.\nImportantly, the reranker significantly outper-\nforms the generator baseline regarding parser prob-\nability. Specifically, there is an impressive abso-\nlute difference of up to 12.9 percentage points (for\nCFQ-MCD1). The reranker also shows modest\ngains over the generator baseline in PRISM and\nBLEURT scores. These metrics suggest that the\nreranker’s selected candidates have both greater se-\nmantic consistency and slightly enhanced fluency\nthan those chosen without reranking.\nIt is worth noting that the performance gap be-\ntween the reranker and the highest probability base-\nline is most prominent in the GeoQuery standard\nsplit. In contrast, the other three datasets were\ndeliberately designed to assess models’ compo-\nsitional generalization capabilities. For instance,\nboth the query splits of GeoQuery and the Jobs\ndatasets have no LF overlap among their train and\ntest sets, and CFQ-MCD1 was split to maximize the\ncompound divergence between the two sets. The\nsmaller performance gap between the generator and\nthe reranker on these three datasets suggests that\nthe generator model, Codex, has stronger compo-\nsitional generalization abilities than the finetuned\nreranker.\nAdditionally, we observed that the self-\nconsistency method performs poorly compared to\nother baselines, such as candidate selection based\non generator probability. This finding indicates that\nself-consistency is not a helpful selection method\nfor this particular task.\n5.5 Influence of the Size of the n-Best List\nIn this experiment, we examine the effect of differ-\nent sizes of candidate lists seen during train and test\ntime. We use a subset of the CFQ-MCD1 dataset in\nthese experiments due to time constraints; specifi-\ncally, we randomly select 3,000 data pairs from the\ntrain split (further divided into 2,700 train pairs and\n300 dev pairs) and report our results on 1,200 ran-\ndomly selected data pairs from the test split. The\nresults of this experiment are shown in Figure 2.\nWhile scores for all metrics improve as the train-\ntime n-best list grows, the most significant gains\nare observed in parser probability. This suggests\nthat increasing the number of candidates per LF\nthat the reranker sees at training time is an effective\nway to increase the semantic consistency of candi-\ndates chosen by the reranker. The performance on\nBLEURT and PRISM increases more slowly as the\nnumber of candidates seen at train time increases,\nwith the largest increase happening in the jump\nfrom 16 candidates to 32, suggesting that it may be\nnecessary to generate many more candidates per\nLF in order to substantially improve the fluency\nof selected candidates. Additionally, increasing\nthe number of candidates seen at test time appears\nto have a negligible effect on the semantic consis-\ntency of candidates selected, but a notable effect\non the BLEURT and PRISM scores. Scores for\nthese metrics increase steadily for the first three\nsizes of candidate lists at test time, regardless of\nthe number of candidates seen at train time.\n1073\nFigure 2: BLEURT, parser probability, and PRISM\nscores across different sizes of candidate lists seen at\ntraining and test time.\nHowever, performance on these metrics drops\nwhen the test size increases to 32 candidates for\nrerankers trained with a candidate list size of 16 and\nbelow. We hypothesize that these observations are\ndue to changes in the quality and diversity of the\ntest candidates. As the number of candidates per LF\nincreases, it is more likely that any given candidate\nset will contain high-quality candidates. Increasing\nthe candidate set size also increases the diversity of\ncandidate sets. Improvements in test candidate set\nquality appear to be helpful for sizes up to n = 16,\nbut models trained on smaller candidate sets may\nnot be able to generalize well to candidate set sizes\nof n = 32 due to the larger degree of diversity.\nHowever, the reranker trained with 32 candidates\nper LF is able to take advantage of further quality\nimprovements in the largest test candidate list size\ndue to its exposure to diverse candidate sets.\n5.6 Fixed Generation Budget\nAs generating large-sized n-best lists from Codex\nis time-consuming, we consider a scenario in which\nthe time budget for training data generation is fixed.\nWhen given limited time to generate training data,\nis it better to prioritize coverage of as many LFs\nas possible by considering small n-best lists, or is\nit better to ensure that there is a large number of\ncandidates for each LF in the training set?\nWe generate natural language candidates for LFs\nin the training set of CFQ-MCD1 over one 24-hour\nperiod. We use Codex to generate 4, 8, 16, or 32\ncandidates per LF for 24 hours. We also generate a\ndataset containing a variable number of candidates\nper LF. To do this, we prompt Codex for 10 can-\ndidates per LF, then discard duplicates. Any LFs\nwith candidate sets of length 1 are also discarded.\nThis results in a dataset that pairs LFs with sets\nof candidates with a minimum length of 2 and a\nmaximum length of 10. The average number of\ncandidates per LF in this dataset is 7.6 in our trial;\nsee Table 3.\nA reranker is then trained for each dataset us-\ning the method described in Section 3.3. 7 Each\nreranker is evaluated on the full test split of CFQ-\nMCD1, with eight candidates per LF.\nThe results are shown in Table 4. The reranker\ntrained on the dataset with a variable number of\ncandidates per LF has the best performance as mea-\nsured by BLEURT and PRISM, and the second\nbest performance as measured by parser probabil-\nity. Its strong performance is likely due to the fact\nthat it is trained on the largest dataset (at 93k total\ncandidates) that also covers the largest number of\nLFs (12k). This suggests that the best way to use a\nlimited budget for generating reranker training data\nis to maximize the total quantity of generated candi-\ndates; ensuring a large (or even consistent) number\nof candidates per LF is less important. Using a\nvariable candidate set size is also more efficient in\na pay-per-token setting, as fewer duplicated candi-\ndates will be discarded than there would be with a\nfixed candidate set size.\n5.7 Using Instruction-Following LLMs\nWe perform the next experiment in order to deter-\nmine the effectiveness of a general-purpose lan-\nguage model in the role of generator in place of\na language model optimized for code, or in the\nrole of reranker in place of a discriminative model.\nWe generate eight candidates per LF by prompt-\ning either Codex (as in previous experiments) or\n7For the dataset with a variable number of candidates per\nLF, the loss for each candidate set is multiplied by a weight\nterm, which is calculated as the size of the candidate set di-\nvided by the average candidate set size. This is done in order\nto normalize the magnitude of gradient updates across the\ntraining set.\n1074\nCands. per LF Total LFs Total cands.\n4 11,395 45,580\n8 8,147 65,156\n16 4,076 65,216\n32 1,935 61,920\nVariable 12,253 93,226\nTable 3: Resulting dataset sizes after generating a speci-\nfied number of natural language candidates per LF.\nSet size bleurt parser (%) prism (%)\n4 66.1±0.4 57 .5±1.1 17 .0±0.4\n8 65.8±0.2 60 .4±0.9 16 .8±0.1\n16 65.7±0.2 60.8±1.1 16.8±0.2\n32 66.0±0.3 59 .7±0.3 16 .9±0.2\nVariable 66.2±0.2 60.4±0.6 17.1±0.1\nOracle 71.5 64 .3 22 .2\nGenerator 64.6 49 .0 15 .8\nTable 4: Scores for different candidate generation con-\nfigurations, given 24 hours of generation time with\nCodex. Set size refers to the number of candidates\nper LF in the training set. The oracle method selects the\ncandidate with the highest sum of normalized BLEURT,\nparser, and PRISM scores. The generator method se-\nlects the candidate with the highest probability assigned\nby Codex.\nChatGPT. Then, we rerank the candidates either\nusing a finetuned reranker (as in previous exper-\niments), or by prompting ChatGPT. We replicate\nour experiments on the GeoQuery dataset using\nGPT-3.5-turbo (ChatGPT8) (Ouyang et al., 2022).\nThe details of generation from ChatGPT are given\nin Appendix B.\nThe results9 are presented in Table 5. The best-\nperforming combination by a wide margin is Codex\nas the generator, and fine-tuned CodeBERT as the\nreranker. Using ChatGPT does not appear to add\nbenefits for either the candidate generation step\nor the reranking step. We performed manual error\nanalysis to determine why the performance gap was\nso wide, the results of which are presented below.\nProblems with Generation. The style of candi-\ndates generated by Codex tended to more closely\nmatch the style of the gold natural language utter-\nances than did the candidates generated by Chat-\nGPT. Namely, candidates generated by ChatGPT\ntended to use sentence casing and end punctuation,\nwhile Codex candidates tended to be all lowercase\n8platform.openai.com/docs/model-index-for-researchers\n9The scores reported for the two configurations using Code-\nBERT as the reranker represent the mean score over five trials,\nwhile the trials using GPT-3.5 report the score from one trial.\nGenerator Reranker bleurt parser prism\nCodex CodeBERT 79.9 89 .8 28 .9\nCodex GPT-3.5 76.9 86 .9 25 .3\nGPT-3.5 CodeBERT 71.0 71 .9 8 .2\nGPT-3.5 GPT-3.5 71.9 75 .6 8 .5\nTable 5: Scores for different combinations of models\nused as generators and rerankers. GPT-3.5 here refers\nto OpenAI’s ChatGPT.\nCodex Candidates\nwhat states are in the usa\nwhat are the names of the states\nwhat state is this\nwhat are all the states\nwhat states are there\nname all the states\nwhat states exist\nwhat are the states\nChatGPT Candidates\nWhich states are in the United States?\nWhat are the names of all the states?\nHow many states are there in the country?\nWhich states are located in the Midwest?\nWhat states make up New England?\nWhat is the largest state in terms of land area?\nWhich states have coastline?\nWhat is the capital of each state?\nLogical Form: answer ( state )\nGold Utterance: list the states\nTable 6: A comparison of candidates generated by\nCodex and ChatGPT. While the candidates generated\nby Codex are faithful to the style of the gold question\nand are mostly semantically consistent with the given\nLF, the candidates generated by ChatGPT include sub-\nstantial hallucinations.\nwith no punctuation, as are the GeoQuery ques-\ntions. Additionally, the ChatGPT candidates used\nmore varied language than the Codex candidates\ndid. While these surface-level differences may not\nreflect a difference in candidate quality, it is pos-\nsible that they are penalized by automatic metrics\nsuch as the ones we use here. A more concerning\nfinding is that the candidates generated by Chat-\nGPT tended to include more frequent and more se-\nvere hallucinations than those generated by Codex;\nsee Table 6 for examples.\nWe speculate that these differences in generated\ncandidates are due to the fact that Codex is directly\noptimized for tasks that involve code, which makes\nit a better fit for the task of generating text from\nstructured meaning representations. While Chat-\nGPT’s training does include tasks involving code,\nmany of its training tasks do not concern code.\nProblems with Reranking. When using Chat-\nGPT as a reranker, we found that it returned a natu-\n1075\nral language sequence that was not one of the given\ncandidates approximately 14% of the time. Most\ncommonly, these hallucinated candidates were in\nthe form of single noun phrases that were similar\nto segments of one or more of the given candidates.\nThe reason for this is likely a task mismatch.\nDecoder-only models such as ChatGPT are in-\ntended to generate sequences of text, which does\nnot align well with the task of reranking.\n6 Conclusion\nWe have introduced a novel generate-and-rerank ap-\nproach for generating high-quality natural language\nutterances from LFs using LLMs. Our approach is\nflexible and can be easily applied to diverse datasets\nand tasks. In addition, we have performed an anal-\nysis of the current popular evaluation metrics for\nNLG and selected the best metrics for the train-\ning and evaluation of our reranker. Our extensive\nexperiments show that our reranker, which uses a\nloss function that compares individual candidates\nagainst one another, improves the quality of gener-\nated natural language in both fluency and semantic\nfaithfulness in terms of the selected metrics on dif-\nferent evaluation datasets.\nLimitations\nThe results presented in Section 5.4 demonstrate\nthat our reranker improves the quality of natural\nlanguage text generated from LFs. However, the\napplicability of our method is somewhat limited by\nthe choice of Codex as the generator model.\nFirstly, Codex requires a lot of computation re-\nsources due to its size of 175 billion parameters\n(Chen et al., 2021), and a lot of time to gener-\nate candidates. A smaller model would be able\nto generate candidates much more efficiently, al-\nthough those candidates would likely be lower qual-\nity. Further experimentation is required to deter-\nmine whether the reranker’s performance can make\nup for a weaker generator.\nSecondly, it seems likely that the majority of the\nnatural language data that appears in Codex’s pre-\ntraining is in English, so our approach probably\ndoes not transfer well to other languages without\nmodification. It may be beneficial to further ex-\nplore this problem using a generator model with\nmultilingual pre-training.\nAnother issue is that the reranker we introduce\nin this work, as we have formulated, may suffer\nfrom a lack of composition generalization abilities,\nas we note in Section 5.4. The performance of a\nreranker in this setting may benefit from techniques\nused to improve compositional generalization in se-\nmantic parsers, such as the application of synthetic\ndata (Wang et al., 2015; Herzig and Berant, 2019;\nYu et al., 2021; Wang et al., 2021b; Akyurek and\nAndreas, 2023; Li et al., 2023, inter alia) or the use\nof supervised attention (Yin et al., 2021).\nThis approach could further be improved with\nthe use of more reliable automated metrics. Our\nevaluation in Section 4 found that the best perform-\ning combination of metrics had a top-1 accuracy of\n81.4% and a ranking accuracy of 84.22%, which\nindicates that a fair number of the ranking deci-\nsions made by this combined metric were incorrect.\nHowever, due to time constraints, this study in-\ncludes only one human annotator for our metric\nevaluation set, which hampers the reliability of our\nanalysis of automatic metrics. Further exploration\nis needed to assess the alignment between different\n(combinations of) automatic metrics and human\njudgement of semantic consistency and fluency in\nthis task. Additionally, there is much ongoing re-\nsearch in the creation and evaluation of automated\nmetrics, and advances in this work would likely to\ntranslate to stronger performance of the method we\nhave presented here.\nAcknowledgements\nWe express our deepest gratitude to David McGee,\nwhose feedback has been critical in the develop-\nment of this work. We are also grateful to the\nanonymous reviewers for their thoughtful com-\nments.\nReferences\nEkin Akyurek and Jacob Andreas. 2023. LexSym: Com-\npositionality as lexical symmetry. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 639–657, Toronto, Canada. Association for\nComputational Linguistics.\nChantal Amrhein, Nikita Moghe, and Liane Guillou.\n2022. Aces: Translation accuracy challenge sets\nfor evaluating machine translation metrics. ArXiv,\nabs/2210.15615.\nSamuel Arcadinho, David Oliveira Aparício, Hugo\nVeiga, and António Alegria. 2022. T5ql: Tam-\ning language models for sql generation. ArXiv,\nabs/2209.10254.\n1076\nSumanta Bhattacharyya, Amirmohammad Rooshenas,\nSubhajit Naskar, Simeng Sun, Mohit Iyyer, and An-\ndrew McCallum. 2021. Energy-based reranking:\nImproving neural machine translation using energy-\nbased models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4528–4537, Online. Association for\nComputational Linguistics.\nMary Elaine Califf and Raymond J. Mooney. 1999. Re-\nlational learning of pattern-match rules for informa-\ntion extraction. In Conference on Computational\nNatural Language Learning.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde, Jared Kaplan, Harrison Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Fe-\nlipe Petroski Such, David W. Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-V oss, William H. Guss, Alex Nichol, Igor\nBabuschkin, S. Arun Balaji, Shantanu Jain, Andrew\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew M. Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv,\nabs/2107.03374.\nZhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou,\nYunkai Zhang, Sairam Sundaresan, and William Yang\nWang. 2020a. Logic2Text: High-fidelity natural lan-\nguage generation from logical forms. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 2096–2111, Online. Association\nfor Computational Linguistics.\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\nand William Yang Wang. 2020b. Few-shot NLG\nwith pre-trained language model. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 183–190, Online.\nAssociation for Computational Linguistics.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nYuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam,\nand Marc’Aurelio Ranzato. 2020. Residual energy-\nbased models for text generation. In International\nConference on Learning Representations.\nAndrew Drozdov, Nathanael Scharli, Ekin Akyuurek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. ArXiv,\nabs/2209.15003.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nCatherine Finegan-Dollak, Jonathan K Kummerfeld,\nLi Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui\nZhang, and Dragomir Radev. 2018. Improving text-\nto-sql evaluation methodology. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n351–360.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André Martins. 2022.\nResults of wmt22 metrics shared task: Stop using\nbleu - neural metrics are better and more robust. In\nProceedings of the Seventh Conference on Machine\nTranslation, pages 46–68, Abu Dhabi.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124–133, San-\ntiago de Compostela, Spain. Association for Compu-\ntational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin\nDurmus, Ond ˇrej Dušek, Chris Chinenye Emezue,\nVarun Gangal, Cristina Garbacea, Tatsunori\nHashimoto, Yufang Hou, Yacine Jernite, Harsh Jham-\ntani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bod-\nhisattwa Prasad Majumder, Pedro Henrique Martins,\nAngelina McMillan-Major, Simon Mille, Emiel van\nMiltenburg, Moin Nadeem, Shashi Narayan, Vitaly\nNikolaev, Andre Niyongabo Rubungo, Salomey\nOsei, Ankur Parikh, Laura Perez-Beltrachini,\nNiranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez, Sashank Santhanam, João Sedoc,\nThibault Sellam, Samira Shaikh, Anastasia Shimo-\nrina, Marco Antonio Sobrevilla Cabezudo, Hendrik\nStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021. The\nGEM benchmark: Natural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120,\nOnline. Association for Computational Linguistics.\n1077\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\nHave your text and use it too! end-to-end neural data-\nto-text generation with semantic fidelity. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 2410–2424, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nPeyman Heidari, Arash Einolghozati, Shashank Jain,\nSoumya Batra, Lee Callender, Ankit Arun, Shawn\nMei, Sonal Gupta, Pinar Donmez, Vikas Bhardwaj,\nAnuj Kumar, and Michael White. 2021. Getting to\nproduction with few-shot natural language genera-\ntion models. In Proceedings of the 22nd Annual\nMeeting of the Special Interest Group on Discourse\nand Dialogue, pages 66–76, Singapore and Online.\nAssociation for Computational Linguistics.\nJonathan Herzig and Jonathan Berant. 2019. Don’t para-\nphrase, detect! rapid and effective data collection for\nsemantic parsing. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 3810–3820, Hong Kong, China. Association\nfor Computational Linguistics.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,\nMarc van Zee, and Olivier Bousquet. 2019. Measur-\ning compositional generalization: A comprehensive\nmethod on realistic data. ArXiv, abs/1912.09713.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufficiency. The annals of mathe-\nmatical statistics, 22(1):79–86.\nIrene Langkilde-Geary. 2002. An empirical verification\nof coverage and correctness for a general-purpose\nsentence generator. In Proceedings of the Inter-\nnational Natural Language Generation Conference,\npages 17–24, Harriman, New York, USA. Associa-\ntion for Computational Linguistics.\nAnn Lee, Michael Auli, and Marc’Aurelio Ranzato.\n2021. Discriminative reranking for neural machine\ntranslation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 7250–7264, Online. Association for Computa-\ntional Linguistics.\nZhaoyi Li, Ying Wei, and Defu Lian. 2023. Learning\nto substitute spans towards improving compositional\ngeneralization. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2791–2811,\nToronto, Canada. Association for Computational Lin-\nguistics.\nZhuang Li, Lizhen Qu, Qiongkai Xu, Tongtong Wu,\nTianyang Zhan, and Gholamreza Haffari. 2022. Vari-\national autoencoder with disentanglement priors for\nlow-resource task-specific natural language gener-\nation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 10335–10356.\nYixin Liu and Pengfei Liu. 2021. SimCLS: A sim-\nple framework for contrastive learning of abstractive\nsummarization. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 2: Short\nPapers), pages 1065–1072, Online. Association for\nComputational Linguistics.\nDa Ma, Xingyu Chen, Ruisheng Cao, Zhi Chen,\nLu Chen, and Kai Yu. 2021. Relation-aware graph\ntransformer for sql-to-text generation. Applied Sci-\nences.\nNikita Moghe, Tom Sherborne, Mark Steedman, and\nAlexandra Birch. 2022. Extrinsic evaluation of ma-\nchine translation metrics. ArXiv, abs/2212.10297.\nAxel-Cyrille Ngonga Ngomo, Lorenz Bühmann,\nChristina Unger, Jens Lehmann, and Daniel Gerber.\n2013. Sorry, i don’t speak sparql: translating sparql\nqueries into natural language. Proceedings of the\n22nd international conference on World Wide Web.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback. ArXiv,\nabs/2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nAmy Pu, Hyung Won Chung, Ankur Parikh, Sebastian\nGehrmann, and Thibault Sellam. 2021. Learning\ncompact metrics for MT. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 751–762, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\n1078\nLeonardo F. R. Ribeiro, Claire Gardent, and Iryna\nGurevych. 2019. Enhancing amr-to-text generation\nwith dual graph representations. In Conference on\nEmpirical Methods in Natural Language Processing.\nLeonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych.\n2021a. Structural adapters in pretrained language\nmodels for AMR-to-Text generation. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4269–4282, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schütze,\nand Iryna Gurevych. 2021b. Investigating pretrained\nlanguage models for graph-to-text generation. In Pro-\nceedings of the 3rd Workshop on Natural Language\nProcessing for Conversational AI, pages 211–227.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nRichard Shin and Benjamin Van Durme. 2022. Few-\nshot semantic parsing with language models trained\non code. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5417–5425, Seattle, United States.\nAssociation for Computational Linguistics.\nFatemeh Shiri, Terry Yue Zhuo, Zhuang Li, Shirui Pan,\nWeiqing Wang, Reza Haffari, Yuan-Fang Li, and Van\nNguyen. 2022. Paraphrasing techniques for maritime\nqa system. In 2022 25th International Conference on\nInformation Fusion (FUSION), pages 1–8. IEEE.\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018. A graph-to-sequence model for AMR-\nto-text generation. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1616–1626,\nMelbourne, Australia. Association for Computational\nLinguistics.\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\nsky. 2022. Prompt-and-rerank: A method for zero-\nshot and few-shot arbitrary textual style transfer with\nsmall language models. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2195–2222, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nBrian Thompson and Matt Post. 2020. Automatic ma-\nchine translation evaluation in many languages via\nzero-shot paraphrasing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 90–121, Online.\nAssociation for Computational Linguistics.\nBailin Wang, Wenpeng Yin, Xi Victoria Lin, and Caim-\ning Xiong. 2021a. Learning to synthesize data for\nsemantic parsing. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2760–2766.\nBailin Wang, Wenpeng Yin, Xi Victoria Lin, and Caim-\ning Xiong. 2021b. Learning to synthesize data for\nsemantic parsing. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2760–2766, Online. As-\nsociation for Computational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. ArXiv, abs/2203.11171.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021c. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYushi Wang, Jonathan Berant, and Percy Liang. 2015.\nBuilding a semantic parser overnight. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1332–1342, Beijing,\nChina. Association for Computational Linguistics.\nKun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng,\nand Vadim Sheinin. 2018. SQL-to-text generation\nwith graph-to-sequence model. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 931–936, Brussels,\nBelgium. Association for Computational Linguistics.\nPengcheng Yin, Hao Fang, Graham Neubig, Adam\nPauls, Emmanouil Antonios Platanios, Yu Su, Sam\nThomson, and Jacob Andreas. 2021. Compositional\ngeneralization for neural semantic parsing via span-\nlevel supervised attention. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2810–2823, Online.\nAssociation for Computational Linguistics.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\nRichard Socher, and Caiming Xiong. 2021. Grappa:\nGrammar-augmented pre-training for table semantic\nparsing.\n1079\nTao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, et al. 2019. Cosql: A conversational\ntext-to-sql challenge towards cross-domain natural\nlanguage interfaces to databases. arXiv preprint\narXiv:1909.05378.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. ArXiv, abs/2106.11520.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprogramming. In AAAI/IAAI, Vol. 2.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert. ArXiv,\nabs/1904.09675.\nJiehan Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min\nZhang, and Guodong Zhou. 2019. Modeling graph\nstructure in transformer for better amr-to-text genera-\ntion. In Conference on Empirical Methods in Natural\nLanguage Processing.\nTerry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh\nShiri, Weiqing Wang, Gholamreza Haffari, and Yuan-\nFang Li. 2023. On robustness of prompt-based se-\nmantic parsing with large pre-trained language model:\nAn empirical study on codex. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 1090–\n1102.\nA Reranker Training Details\nThe reranker model is trained for a maximum of\n100 epochs with early stopping if the loss on the\ndevelopment set does not decrease after 10 epochs.\nEach batch comprises all of the candidates corre-\nsponding to a single logical form, so the batch size\nis equal to the size of the candidate list. We uti-\nlized Adam (Kingma and Ba, 2014) to optimize the\nmodel, with a learning rate of 1 × 10−4. The best\nmodel is determined as the one that produces the\nsmallest loss on a held-out development set.\nHyperparameter tuning was conducted to deter-\nmine the learning rate and the optimal epoch count\nfor unfreezing the base model’s final layer. The\nlearning rates explored during this process were\n[1×10−5, 5×10−5, 1×10−4, 5×10−4, 1×10−3],\nwhile the numbers of epochs before unfreezing con-\nsidered were [1, 5, 10, 20]. The model’s training\nwas conducted on a single NVIDIA Tesla V100\nGPU. The duration of training varied significantly\ndepending on the size of the training dataset, rang-\ning from a minimum of approximately 20 minutes\nto a maximum of around 35 hours.\nB Generation from ChatGPT\nTo account for the fact that ChatGPT is optimized\nfor chat functionality while Codex is not, we mod-\nify our generation prompt slightly. We use the same\nnumber of in-context examples (15) for both gener-\nators, but for ChatGPT we incorporate more natural\nlanguage instruction to contextualize the examples\nand specifically prompt the model to generate eight\nunique candidates. The full prompt can be found\nin Appendix C.2. To complete the task of rerank-\ning using in-context learning, we use a prompt that\nprovides exemplars from the training set in order\nto condition the model on correct pairings of LFs\nand natural language. We present the prompt used\nfor the reranking task in Appendix C.3.\nC Sample Prompts\nC.1 Codex generation prompt\nBelow is an example that illustrates the format of\nour prompts to Codex.\n# geo_query Dataset:\nQuery: answer ( longest ( intersection (\nriver , traverse_2 ( intersection ( state\n, next_to_2 ( m0 ) ) ) ) ) )\n1080\nQuestion: what is the longest river that\nflows through a state that borders m0\nQuery: answer ( intersection ( state ,\nnext_to_2 ( largest_one ( population_1 ,\nstate ) ) ) )\nQuestion: what are the states that border\nthe state with the greatest population\nQuery: answer ( intersection ( river ,\ntraverse_2 ( m0 ) ) )\nQuestion: what rivers run through m0\nQuery: answer ( count ( intersection\n( state , low_point_2 ( lower_2 (\nlow_point_1 ( m0 ) ) ) ) ) )\nQuestion: count the states which have\nelevations lower than what m0 has\nQuery: answer ( highest ( intersection\n( place , loc_2 ( smallest_one (\npopulation_1 , state ) ) ) ) )\nQuestion: what is the highest point in\nthe state with the smallest population\nQuery: answer ( intersection ( state ,\nnext_to_2 ( m0 ) ) )\nQuestion: which states border m0\nQuery: answer ( density_1 ( intersection\n( state , traverse_1 ( longest (\nintersection ( river , loc_2 ( m0 ) )\n) ) ) ) )\nQuestion: which is the density of the\nstate that the largest river in the m0\nruns through\nQuery: answer ( elevation_1 ( highest (\nintersection ( place , loc_2 ( state ) )\n) ) )\nQuestion: how high are the highest points\nof all the states\nQuery: answer ( count ( intersection (\nstate , loc_2 ( m0 ) ) ) )\nQuestion: how many states are in the m0\nQuery: answer ( loc_1 ( m0 ) )\nQuestion: where is m0\nQuery: answer ( intersection ( state ,\nnext_to_2 ( m0 ) ) )\nQuestion: what state borders m0\nQuery: answer ( intersection ( state ,\nloc_1 ( highest ( place ) ) ) )\nQuestion: which state has the highest\nelevation\nQuery: answer ( intersection ( state ,\ncapital_2 ( m0 ) ) )\nQuestion: what states capital is m0\nQuery: answer ( intersection ( state ,\nnext_to_2 ( m0 ) ) )\nQuestion: what states surround m0\nQuery: answer ( count ( intersection (\nriver , loc_2 ( m0 ) ) ) )\nQuestion: how many rivers are found in m0\nQuery: answer ( largest ( intersection (\nstate , loc_2 ( m0 ) ) ) )\nQuestion:\nC.2 ChatGPT generation prompt\nBelow is an example that illustrates the format of\nour prompts to ChatGPT for generating natural\nlanguage candidates. Most of the exemplars are\nelided here for brevity. This prompt uses the same\nnumber of exemplars as the prompt in Appendix\nC.1, using the slightly modified form shown below.\nHere are some examples of query/question\npairs from the GeoQuery data set.\nlogical form: answer ( longest (\nintersection ( river , traverse_2 (\nintersection ( state , next_to_2 ( m0 ) )\n) ) ) )\nnatural language: what is the longest\nriver that flows through a state that\nborders m0\n[...]\nPlease generate 8 natural language\ncandidates for following logical form.\nPresent your answer as a numbered list.\nlogical form: answer ( largest (\nintersection ( state , loc_2 ( m0 ) )\n) )\n1081\nC.3 ChatGPT reranking prompt\nBelow is an example that illustrates the format of\nour prompts to ChatGPT for reranking natural lan-\nguage candidates. Most of the exemplars are elided\nhere for brevity.\nHere are some examples of query/question\npairs from the GeoQuery data set.\nlogical form: answer ( longest (\nintersection ( river , traverse_2 (\nintersection ( state , next_to_2 ( m0 ) )\n) ) ) )\nnatural language: what is the longest\nriver that flows through a state that\nborders m0\n[...]\nI would like for you to rank some natural\nlanguage candidates for the following\nlogical form.\nlogical form: answer ( largest (\nintersection ( state , loc_2 ( m0 ) )\n) )\nHere are the candidates:\nWhich state in m0 has the largest area?\nWhat is the largest state that lies within\nm0?\nWhich state in m0 has the largest\npopulation?\nWhat is the largest state found in m0 by\narea?\nWhich state in m0 is the largest in terms\nof land area?\nWhat state located in m0 has the largest\nlandmass?\nWhat is the largest state located in m0\nby size?\nWhich state in m0 has the highest number\nof inhabitants?\nWhich of these candidates is the best?\nPlease return the text of the best\ncandidate in quotation marks.\nD Freebase Identifier Mapping\nTable 7 shows the mapping we use to shorten Free-\nbase IDs into strings that are easier to interpret.\nMost of this mapping originates in Drozdov et al.\n(2022).\n1082\nFreebase Identifier Mapped String\nns:organization.organization.companies_acquired/ns:business. acquired\nns:organization.organization.acquired_by/ns:business.acquisi acquired_by\nns:film.actor.film/ns:film.performance.film starred_in\nns:film.film_art_director.films_art_directed art_directed\nns:film.film.film_art_direction_by art_direction_by\nns:film.film.cinematography cinematography_by\nns:film.film_costumer_designer.costume_design_for_film costume_designed\nns:film.film.costume_design_by costume_designed_by\nns:film.director.film directed\nns:film.film.directed_by directed_by\nns:film.film.distributors/ns:film.film_film_distributor_rela distributed_by\nns:film.film_distributor.films_distributed/ns:film.film_film distributed\nns:film.editor.film edited\nns:film.film.edited_by edited_by\nns:business.employer.employees/ns:business.employment_tenure employed\nns:people.person.employment_history/ns:business.employment_t employed_by\nns:film.producer.films_executive_produced executive_produced\nns:film.film.executive_produced_by executive_produced_by\nns:organization.organization_founder.organizations_founded founded\nns:organization.organization.founders founded_by\nns:people.person.gender gender_is\nˆns:people.person.gender same_gender_as\nns:film.actor.film/ns:film.performance.character portrayed\nns:people.person.nationality nationality_is\nns:film.film.prequel sequel_of\nns:film.film.sequel prequel_of\nns:influence.influence_node.influenced influenced\nns:influence.influence_node.influenced_by influenced_by\nns:people.person.spouse_s/ns:people.marriage.spouse|ns:ficti married_to\nˆns:people.person.nationality same_nationality_as\nns:people.person.children|ns:fictional_universe.fictional_ch parent_of\nns:people.person.parents|ns:fictional_universe.fictional_cha child_of\nns:film.producer.film|ns:film.production_company.films produced\nns:film.film.produced_by|ns:film.film.production_companies produced_by\nns:people.person.sibling_s/ns:people.sibling_relationship.si sibling_of\nns:film.film.starring/ns:film.performance.actor starred\nns:film.film.written_by written_by\nns:film.writer.film wrote\nns:film.actor actor\nns:film.film_art_director art_director\nns:film.cinematographer cinematographer\nns:film.cinematographer.film cinematographer_of\nns:film.film_costumer_designer costume_designer\nns:film.director film_director\nns:film.editor film_editor\nns:business.employer employer\nns:fictional_universe.fictional_character fictional_character\nns:film.film film\nns:film.film_distributor film_distributor\nns:people.person person\nns:film.producer film_producer\nns:film.production_company production_company\nns:film.writer writer\nns:m.05zppz male\nns:m.02zsn female\nns:m.0f8l9c French\nns:m.06mkj Spanish\nns:m.0b90_r Mexican\nns:m.03rjj Italian\nns:m.0d0vqn Swedish\nns:m.09c7w0 American\nns:m.0d060g Canadian\nns:m.0345h German\nns:m.03_3d Japanese\nns:m.07ssc British\nns:m.059j2 Dutch\nns:m.0d05w3 Chinese\nTable 7: Mapping from Freebase identifiers (truncated to first 60 characters) to shorter, more readable strings.",
  "topic": "Computational linguistics",
  "concepts": [
    {
      "name": "Computational linguistics",
      "score": 0.6527549624443054
    },
    {
      "name": "Computer science",
      "score": 0.6431224346160889
    },
    {
      "name": "Linguistics",
      "score": 0.5831177830696106
    },
    {
      "name": "Natural language processing",
      "score": 0.5239234566688538
    },
    {
      "name": "Natural language",
      "score": 0.5232395529747009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.419519305229187
    },
    {
      "name": "Philosophy",
      "score": 0.19810661673545837
    }
  ],
  "institutions": []
}