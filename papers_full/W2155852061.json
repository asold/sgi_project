{
    "title": "Multi-Class Composite N-gram language model for spoken language processing using multiple word clusters",
    "url": "https://openalex.org/W2155852061",
    "year": 2001,
    "authors": [
        {
            "id": "https://openalex.org/A5109832868",
            "name": "Hirofumi Yamamoto",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5056069616",
            "name": "Shuntaro Isogai",
            "affiliations": [
                "Waseda University"
            ]
        },
        {
            "id": "https://openalex.org/A5045034259",
            "name": "Yoshinori Sagisaka",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1585177184",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W1758669758",
        "https://openalex.org/W2131642018",
        "https://openalex.org/W2145826596",
        "https://openalex.org/W2165112885",
        "https://openalex.org/W2068970468",
        "https://openalex.org/W1607753411",
        "https://openalex.org/W2135854887"
    ],
    "abstract": "In this paper, a new language model, the Multi-Class Composite N-gram, is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data. The Multi-Class Composite N-gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters, called Multi-Classes. In the Multi-Class, the statistical connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent the positional attributes. Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams. In experiments, the Multi-Class Composite N-grams result in 9.5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3-grams.",
    "full_text": null
}