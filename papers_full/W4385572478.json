{
  "title": "The Diminishing Returns of Masked Language Models to Science",
  "url": "https://openalex.org/W4385572478",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2110610742",
      "name": "Zhi Hong",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2901282003",
      "name": "Aswathy Ajith",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": null,
      "name": "James Pauloski",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2206995868",
      "name": "Eamon Duede",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A1907500296",
      "name": "Kyle Chard",
      "affiliations": [
        "University of Chicago",
        "Argonne National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2017222965",
      "name": "Ian Foster",
      "affiliations": [
        "University of Chicago",
        "Argonne National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2962815673",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3197001178",
    "https://openalex.org/W4229443452",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3029384143",
    "https://openalex.org/W4224442790",
    "https://openalex.org/W4206622121",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2170155186",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2149369282"
  ],
  "abstract": "Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770Mparameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model size, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if any, in scientific information extraction tasks. We offer possible explanations for this surprising result.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1270–1283\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nThe Diminishing Returns of Masked Language Models to Science\nZhi Hong∗, Aswathy Ajith∗, J. Gregory Pauloski∗, Eamon Duede†,\nKyle Chard∗‡, Ian Foster∗‡\n∗Department of Computer Science, University of Chicago, Chicago, IL 60637, USA\n†Department of Philosophy and Committee on Conceptual and Historical Studies of Science,\nUniversity of Chicago, Chicago, IL 60637, USA\n‡Data Science and Learning Division, Argonne National Laboratory, Lemont, IL 60615, USA\nAbstract\nTransformer-based masked language models\nsuch as BERT, trained on general corpora, have\nshown impressive performance on downstream\ntasks. It has also been demonstrated that the\ndownstream task performance of such models\ncan be improved by pretraining larger mod-\nels for longer on more data. In this work,\nwe empirically evaluate the extent to which\nthese results extend to tasks in science. We\nuse 14 domain-specific transformer-based mod-\nels (including SCHOLAR BERT, a new 770M-\nparameter science-focused masked language\nmodel pretrained on up to 225B tokens) to eval-\nuate the impact of training data, model size,\npretraining and finetuning time on 12 down-\nstream scientific tasks. Interestingly, we find\nthat increasing model size, training data, or\ncompute time does not always lead to signifi-\ncant improvements (i.e., > 1% F1), if any, in\nscientific information extraction tasks. We offer\npossible explanations for this surprising result.\n1 Introduction\nMassive growth in the number of scientific publi-\ncations places considerable cognitive burden on\nresearchers (Teplitskiy et al., 2022). Language\nmodels can potentially alleviate this burden by au-\ntomating the scientific knowledge extraction pro-\ncess. BERT (Devlin et al., 2019) was pretrained\non a general corpus (BooksCorpus and Wikipedia)\nwhich differs from scientific literature in terms of\nthe context, terminology, and writing style (Ahmad,\n2012). Other masked language models have since\nbeen pretrained on domain-specific scientific cor-\npora (Huang and Cole, 2022; Gu et al., 2021; Gu-\nrurangan et al., 2020; Beltagy et al., 2019) with the\ngoal of improving downstream task performance.\n(We use the term domain to indicate a specific sci-\nentific discipline, such as biomedical science or\ncomputer science.) Other studies (Liu et al., 2019;\nKaplan et al., 2020) explored the impact of varying\nmodel size, training corpus size, and compute time\non downstream task performance. However, no pre-\nvious work has investigated how these parameters\naffect science-focused models.\nIn this study, we train a set of scientific lan-\nguage models of different sizes, collectively called\nSCHOLAR BERT, on a large, multidisciplinary sci-\nentific corpus of 225B tokens to understand the\neffects of model size, data size, and compute time\n(specifically, pretraining and finetuning epochs) on\ndownstream task performance. We find that for\ninformation extraction tasks, an important appli-\ncation for scientific language models, the perfor-\nmance gains by training a larger model for longer\nwith more data are not robust—they are highly task-\ndependent. We make the SCHOLAR BERT models\nand a sample of the training corpus publicly avail-\nable to encourage further studies.\n2 Related Work\nPrior research has explored the effects of varying\nmodel size, dataset size, and amount of compute\non language model performance.\nKaplan et al. (2020) demonstrated that cross-\nentropy training loss scales as a power law with\nmodel size, dataset size, and compute time for\nunidirectional decoder-only architectures. Brown\net al. (2020) showed that language model few-shot\nlearning abilities can be improved by using larger\nmodels. However, both studies explored only the\nGenerative Pretrained Transformer (GPT), an au-\ntoregressive generative model (Brown et al., 2020).\nComparing BERT-Base (110M parameters) and\nBERT-Large (340M parameters), Devlin et al.\n(2019) showed that masked language models can\nalso benefit from more parameters. Likewise, Liu\net al. (2019) demonstrate how BERT models can\nbenefit from training for longer periods, with bigger\nbatches, and with more data.\nModels such as BERT and RoBERTa were pre-\ntrained on general corpora. To boost performance\non scientific downstream tasks, SciBERT (Belt-\n1270\nagy et al., 2019), PubMedBERT (Gu et al., 2021),\nBioBERT (Lee et al., 2020), and MatBERT (Tre-\nwartha et al., 2022) were trained on domain-\nspecific text with the goal of enhancing perfor-\nmance on tasks requiring domain knowledge. Yet\nthere is no work on how that task performance\nvaries with pretraining parameters.\n3 Data and Methodology\nWe outline the pretraining dataset, related models\nto which we compare performance, and the archi-\ntecture and pretraining process used for creating\nthe SCHOLAR BERT models.\n3.1 The Public Resource Dataset\nWe pretrain the SCHOLAR BERT models on a\ndataset provided by Public.Resource.Org, Inc.\n(“Public Resource”), a nonprofit organization based\nin California. This dataset was constructed from\na corpus of 85M journal article PDF files, from\nwhich the Grobid tool, version 0.5.5, was used\nto extract text (GROBID). Not all extractions\nwere successful, because of corrupted or badly en-\ncoded PDF files. We work here with text from\n∼75M articles in this dataset, categorized as 45.3%\nbiomedicine, 23.1% technology, 20.0% physical\nsciences, 8.4% social sciences, and 3.1% arts &\nhumanities. (A sample of the extracted texts and\ncorresponding original PDFs is available in the\nData attachment for review purposes.)\n3.2 Models\nWe consider 14 BERT models: seven from exist-\ning literature (BERT-Base, BERT-Large, SciBERT,\nPubMedBERT, BioBERT v1.2, MatBERT, and Bat-\nteryBERT: Appendix A); and seven SCHOLAR -\nBERT variants pretrained on different subsets of\nthe Public Resource dataset (and, in some cases,\nalso the WikiBooks corpus). We distinguish these\nmodels along the four dimensions listed in Ta-\nble 1: architecture, pretraining method, pretrain-\ning corpus, and casing. SCHOLAR BERT and\nSCHOLAR BERT-XL , with 340M and 770M pa-\nrameters, respectively, are the largest science-\nspecific BERT models reported to date. Prior lit-\nerature demonstrates the efficacy of pretraining\nBERT models on domain-specific corpora (Sun\net al., 2019; Fabien et al., 2020). However, the\never-larger scientific literature makes pretraining\ndomain-specific language models prohibitively ex-\npensive. A promising alternative is to create\nlarger, multi-disciplinary BERT models, such as\nSCHOLAR BERT, that harness the increased avail-\nability of diverse pretraining text; researchers can\nthen adapt (i.e., finetune) these general-purpose\nscience models to meet their specific needs.\n3.3 S CHOLAR BERT Pretraining\nWe randomly sample 1%, 10%, and 100% of the\nPublic Resource dataset to create PRD_1, PRD_10,\nand PRD_100. We pretrain SCHOLAR BERT mod-\nels on these PRD subsets by using the RoBERTa\npretraining procedure, which has been shown to\nproduce better downstream task performance in\na variety of domains (Liu et al., 2019). See Ap-\npendix B.2 for details.\n4 Experimental Results\nWe first perform sensitivity analysis across Scholar-\nBERT pretraining dimensions to determine the\ntrade-off between time spent in pretraining versus\nfinetuning. We also compare the downstream task\nperformance of SCHOLAR BERT to that achieved\nwith other BERT models. Details of each evalua-\ntion task are in Appendix C.\n4.1 Sensitivity Analysis\nWe save checkpoints periodically while pretraining\neach SCHOLAR BERT(-XL) model. In this analy-\nsis, we checkpoint at ∼0.9k, 5k, 10k, 23k, and 33k\niterations based on the decrease of training loss be-\ntween iterations. We observe that pretraining loss\ndecreases rapidly until around 10 000 iterations,\nand that further training to convergence (roughly\n33 000 iterations) yields only small decreases of\ntraining loss: see Figure 1 in Appendix.\nTo measure how downstream task performance\nis impacted by pretraining and finetuning time, we\nfinetune each of the checkpointed models for 5\nand 75 epochs. We observe that: (1) The under-\ntrained 0.9k-iteration model sees the biggest boost\nin the F1 scores of downstream tasks (+8%) with\nmore finetuning, but even with 75 epochs of fine-\ntuning the 0.9k-iteration models’ average F1 score\nis still 19.9 percentage points less than that of the\n33k-iteration model with 5 epochs of finetuning.\n(2) For subsequent checkpoints, the performance\ngains from more finetuning decreases as the num-\nber of pretraining iterations increases. The average\ndownstream task performance of the 33k-iteration\nmodel is only 0.39 percentage points higher with\n75 epochs of finetuning than with 5 epochs. There-\n1271\nModel ArchitecturePretraining MethodCasing Pretraining Corpus Domain Tokens\nBERT_Base BERT-Base BERT Cased Wiki+Books Gen 3.3B\nSciBERT BERT-Base BERT Cased SemSchol Bio, CS 3.1B\nPubMedBERT BERT-Base BERT UncasedPubMedA +PMC Bio 16.8B\nBioBERT_1.2 BERT-Base BERT Cased PubMedB +Wiki+Books Bio, Gen7.8B\nMatBERT BERT-Base BERT Cased MatSci Mat 8.8B\nBatteryBERT BERT-Base BERT Cased Battery Mat 5.2B\nBERT_Large BERT-LargeBERT Cased Wiki+Books Gen 3.3B\nScholarBERT_1 BERT-LargeRoBERTa-like Cased PRD_1 Sci 2.2B\nScholarBERT_10 BERT-LargeRoBERTa-like Cased PRD_10 Sci 22B\nScholarBERT_100BERT-LargeRoBERTa-like Cased PRD_100 Sci 221B\nScholarBERT_10_WBBERT-LargeRoBERTa-like Cased PRD_10+Wiki+Books Sci, Gen 25.3B\nScholarBERT_100_WBBERT-LargeRoBERTa-like Cased PRD_100+Wiki+Books Sci, Gen 224.3B\nScholarBERT-XL_1BERT-XL RoBERTa-like Cased PRD_1 Sci 2.2B\nScholarBERT-XL_100BERT-XL RoBERTa-like Cased PRD_100 Sci 221B\nTable 1: Characteristics of the 14 BERT models considered in this study. The BERT-Base and -Large architectures\nare described in (Devlin et al., 2019); the BERT-XL architecture has 36 layers, hidden size of 1280, and 20 heads.\nDetails of the pretraining corpora are in Table 4 in the Appendix. The domains are Bio=biomedicine, CS=computer\nscience, Gen=general, Mat=materials science and engineering, and Sci=broad scientific.\nfore, in the remaining experiments, we use the\nSCHOLAR BERT(-XL) model that was pretrained\nfor 33k iterations and finetuned for 5 epochs.\n4.2 Finetuning\nWe finetuned the SCHOLAR BERT models and the\nstate-of-the-art scientific models listed in Table 1\non NER, relation extraction, and sentence classi-\nfication tasks. F1 scores for each model-task pair,\naveraged over five runs, are shown in Tables 2 and 3.\nFor NER tasks, we use the CoNLL NER evalua-\ntion Perl script (Sang and De Meulder, 2003) to\ncompute F1 scores for each test.\nTables 2 and 3 show the results, from which we\nobserve: (1) With the same training data, a larger\nmodel does not always achieve significant perfor-\nmance improvements. BERT-Base achieved F1\nscores within 1 percentage point of BERT-Large\non 6/12 tasks; SB_1 achieved F1 scores within\n1 percentage point of SB-XL_1 on 7/12 tasks;\nSB_100 achieved F1 scores within 1 percentage\npoint of SB-XL_100 on 6/12 tasks. (2) With the\nsame model size, a model pretrained on more data\ncannot guarantee significant performance improve-\nments. SB_1 achieved F1 scores within 1 percent-\nage point of SB_100 on 8/12 tasks; SB_10_WB\nachieved F1 scores within 1 percentage point of\nSB_100_WB on 7/12 tasks; SB-XL_1 achieved F1\nscores within 1 percentage point of SB-XL_100\non 10/12 tasks. (3) Domain-specific pretraining\ncannot guarantee significant performance improve-\nments. The Biomedical domain is the only domain\nwhere we see the on-domain model (i.e., pretrained\nfor the associated domain; marked with underlines;\nin this case, PubMedBERT) consistently outper-\nformed models pretrained on off-domain or more\ngeneral corpora by more than 1 percentage point\nF1. The same cannot be said for CS, Materials, or\nMulti-Domain tasks.\n4.3 Discussion\nHere we offer possible explanations for the three\nobservations above. (1) The nature of the task is\nmore indicative of task performance than the size\nof the model. In particular, with the same training\ndata, a larger model size impacts performance only\nfor relation extraction tasks, which consistently\nsaw F1 scores increase by more than 1 percentage\npoint when going from smaller models to larger\nmodels (i.e., BERT-Base to BERT-Large, SB_1 to\nSB-XL_1, SB_100 to SB-XL_100). In contrast,\nthe NER and sentence classification tasks did not\nsee such consistent significant improvements. (2)\nOur biggest model, SCHOLAR BERT-XL , is only\ntwice as large as the original BERT-Large, but its\npretraining corpus is 100X larger. The training loss\nof the SCHOLAR BERT-XL_100 model dropped\nrapidly only in the first ∼10k iterations (Fig. 1\nin Appendix), which covered the first 1/3 of the\nPRD corpus, thus it is possible that the PRD cor-\npus can saturate even our biggest model. (Kaplan\net al., 2020; Hoffmann et al., 2022). (3) Finetun-\ning can compensate for missing domain-specific\nknowledge in pretraining data. While pretraining\nlanguage models on a specific domain can help\nlearn domain-specific concepts, finetuning can also\nfill holes in the pretraining corpora’s domain knowl-\nedge, as long as the pretraining corpus incorporates\n1272\nDomain Biomedical CS Materials Multi-DomainSociology\nDataset BC5CDR JNLPBA NCBI-Disease ChemDNERSciERC MatSciNER ScienceExam Coleridge Mean\nBERT-Base 85.36 72.15 84.28 84.84 56.73 78.51 78.37 57.75 74.75\nBERT-Large 86.86 72.80 84.91 85.83 59.20 82.16 82.32 57.46 76.44\nSciBERT 88.43 73.24 86.95 85.76 59.36 82.64 78.83 54.07 76.16\nPubMedBERT 89.34 74.53 87.91 87.96 59.03 82.63 69.73 57.71 76.11\nBioBERT 88.01 73.09 87.84 85.53 58.24 81.76 78.60 57.04 76.26\nMatBERT 86.44 72.56 84.94 86.09 58.52 83.35 80.01 56.91 76.10\nBatteryBERT 87.42 72.78 87.04 86.49 59.00 82.94 78.14 59.87 76.71\nSB_1 87.27 73.06 85.49 85.25 58.62 80.87 82.75 55.34 76.08\nSB_10 87.69 73.03 85.65 85.80 58.39 80.61 83.24 53.41 75.98\nSB_100 87.84 73.47 85.92 85.90 58.37 82.09 83.12 54.93 76.46\nSB_10_WB 86.68 72.67 84.51 83.94 57.34 78.98 83.00 54.29 75.18\nSB_100_WB 86.89 73.16 84.88 84.31 58.43 80.84 82.43 54.00 75.62\nSB-XL_1 87.09 73.14 84.61 85.81 58.45 82.84 81.09 55.94 76.12\nSB-XL_100 87.46 73.25 84.73 85.73 57.26 81.75 80.72 54.54 75.68\nTable 2: NER F1 scores for each model. Models are finetuned five times for each dataset and the average result is\npresented. Underlined results represent the F1-scores of models trained on in-distribution data for the given task,\nand bolded results indicate the best performing model on that task. SB = SCHOLAR BERT.\nDomain CS BiomedicalMulti-DomainMaterials\nDataset SciERC ChemProt PaperField Battery Mean\nBERT-Base 74.95 83.70 72.83 96.31 81.95\nBERT-Large 80.14 88.06 73.12 96.90 84.56\nSciBERT 79.26 89.80 73.19 96.38 84.66\nPubMedBERT 77.45 91.78 73.93 96.58 84.94\nBioBERT 80.12 89.27 73.07 96.06 84.63\nMatBERT 79.85 88.15 71.50 96.33 83.96\nBatteryBERT 78.14 88.33 73.28 96.06 83.95\nSB_1 73.01 83.04 72.77 94.67 80.87\nSB_10 75.95 82.92 72.94 92.83 81.16\nSB_100 76.19 87.60 73.14 92.38 82.33\nSB_10_WB 73.17 81.48 72.37 93.15 80.04\nSB_100_WB 76.71 83.98 72.29 95.55 82.13\nSB-XL_1 74.85 90.60 73.22 88.75 81.86\nSB-XL_100 80.99 89.18 73.66 95.44 84.82\nTable 3: F1 scores for each model on Relation Extraction (SciERC, ChemProt) and Sentence Classification\n(PaperField, Battery) tasks. Models are finetuned five times for each dataset and the average result is presented.\nUnderlined results represent the F1-scores of models trained on in-distribution data for the given task, and bolded\nresults indicate the best performing model on that task. SB = SCHOLAR BERT.\nthe characteristics specific to the finetuning dataset.\n5 Conclusions\nWe have reported experiments that compare and\nevaluate the impact of various parameters (model\nsize, pretraining dataset size and breadth, and pre-\ntraining and finetuning lengths) on the performance\nof different language models pretrained on scien-\ntific literature. Our results encompass 14 existing\nand newly-developed BERT-based language mod-\nels across 12 scientific downstream tasks.\nWe find that model performance on downstream\nscientific information extraction tasks is not im-\nproved significantly or consistently by increasing\nany of the four parameters considered (model size,\namount of pretraining data, pretraining time, fine-\ntuning time). We attribute these results to both the\npower of finetuning and limitations in the evalua-\ntion datasets, as well as (for the SCHOLAR BERT\nmodels) small model sizes relative to the large pre-\ntraining corpus.\nWe make the ScholarBERT models available on\nHuggingFace ( https://huggingface.co/\nglobuslabs). While we cannot share the\nfull Public Resource dataset, we have pro-\nvided a sample of open-access articles from\nthe dataset ( https://github.com/tuhz/\nPublicResourceDatasetSample) in both\nthe original PDF and extracted txt formats to illus-\ntrate the quality of the PDF-to-text preprocessing.\nLimitations\nOur 12 labeled test datasets are from just five do-\nmains (plus two multi-disciplinary); five of the 12\nare from biomedicine. This imbalance, which re-\nflects the varied adoption of NLP methods across\ndomains, means that our evaluation dataset is nec-\nessarily limited. Our largest model, with 770M\nparameters, may not be sufficiently large to demon-\nstrate scaling laws for language models. We also\naim to extend our experiments to tasks other than\nNER, relation extraction, and text classification,\nsuch as question-answering and textual entailment\nin scientific domains.\n1273\nReferences\nJameel Ahmad. 2012. Stylistic features of scientific En-\nglish: A study of scientific research articles. English\nLanguage and Literature Studies, 2(1).\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Sci-\nBERT: A pretrained language model for scientific\ntext. In Conference on Empirical Methods in Natural\nLanguage Processing, pages 3615–3620. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nArman Cohan, Waleed Ammar, Madeleine Van Zuylen,\nand Field Cady. 2019. Structural scaffolds for ci-\ntation intent classification in scientific publications.\nIn Conference of the North American Chapter of\nthe Association for Computational Linguistics, pages\n3586–3596. Association for Computational Linguis-\ntics.\nColeridge Initiative. 2020. https:\n//www.kaggle.com/c/\ncoleridgeinitiative-show-us-the-data .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Conference of the North American Chap-\nter of the Association for Computational Linguistics,\npages 4171–4186. Association for Computational\nLinguistics.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: A resource for\ndisease name recognition and concept normalization.\nJournal of Biomedical Informatics, 47:1–10.\nMaël Fabien, Esaú Villatoro-Tello, Petr Motlicek, and\nShantipriya Parida. 2020. Bertaa: Bert fine-tuning\nfor authorship attribution. In 17th International Con-\nference on Natural Language Processing, pages 127–\n137. Association for Computational Linguistics.\nGROBID. 2008–2022. GROBID. https://\ngithub.com/kermitt2/grobid.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare, 3(1):1–23.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Oriol Vinyals Jack W. Rae, and\nLaurent Sifre. 2022. Training compute-optimal large\nlanguage models. arXiv preprint arXiv:2203.15556.\nShu Huang and Jacqueline M Cole. 2022. BatteryBERT:\nA pretrained language model for battery database\nenhancement. Journal of Chemical Information and\nModeling.\nHuggingFace. 2020. English wikipedia corpus.\nhttps://huggingface.co/datasets/\nwikipedia. [Online; accessed 08-January-2022].\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,\nYuka Tateisi, and Nigel Collier. 2004. Introduction to\nthe bio-entity recognition task at JNLPBA. In Inter-\nnational Joint Workshop on Natural Language Pro-\ncessing in Biomedicine and its Applications, pages\n70–75.\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu, Robert\nLeaman, Yanan Lu, Donghong Ji, Daniel M Lowe,\nRoger A Sayle, Riza Theresa Batista-Navarro, Rafal\nRak, Torsten Huber, Tim Rocktäschel, Sérgio Matos,\nDavid Campos, Buzhou Tang, Hua Xu, Tsendsuren\nMunkhdalai, Keun Ho Ryu, SV Ramanan, Senthil\nNathan, Slavko Žitnik, Marko Bajec, Lutz Weber,\nMatthias Irmer, Saber A Akhondi, Jan A Kors, Shuo\nXu, Xin An, Utpal Kumar Sikdar, Asif Ekbal, Masa-\nharu Yoshioka, Thaer M Dieb, Miji Choi, Karin Ver-\nspoor, Madian Khabsa, C Lee Giles, Hongfang Liu,\nKomandur Elayavilli Ravikumar, Andre Lamurias,\nFrancisco M Couto, Hong-Jie Dai, Richard Tzong-\nHan Tsai, Caglar Ata, Tolga Can, Anabel Usié,\nRui Alves, Isabel Segura-Bedmar, Paloma Martínez,\nJulen Oyarzabal, and Alfonso Valencia. 2015. The\nCHEMDNER corpus of chemicals and drugs and its\nannotation principles. Journal of Cheminformatics,\n7(1):1–17.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. BioBERT: A pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\n1274\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. BioCreative V CDR task corpus:\nA resource for chemical disease relation extraction.\nDatabase, 2016.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of enti-\nties, relations, and coreference for scientific knowl-\nedge graph construction. In Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3219–3232. Association for Computational Linguis-\ntics.\nNVIDIA. 2017. NVIDIA Apex (a PyTorch extension).\nhttps://github.com/NVIDIA/apex.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten bench-\nmarking datasets. In 18th BioNLP Workshop and\nShared Task, pages 58–65. Association for Computa-\ntional Linguistics.\nErik F Sang and Fien De Meulder. 2003. CoNLL eval\nscript. https://www.clips.uantwerpen.\nbe/conll2000/chunking/output.html.\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June Hsu, and Kuansan Wang. 2015.\nAn overview of Microsoft Academic Service (MAS)\nand applications. In 24th International Conference\non World Wide Web, pages 243–246.\nHannah Smith, Zeyu Zhang, John Culnan, and Peter\nJansen. 2019. ScienceExamCER: A high-density\nfine-grained science-domain corpus for common en-\ntity recognition. In 12th Language Resources and\nEvaluation Conference, pages 4529–4546. European\nLanguage Resources Association.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune BERT for text classification?\nIn China national conference on Chinese computa-\ntional linguistics, pages 194–206. Springer.\nMisha Teplitskiy, Eamon Duede, Michael Menietti, and\nKarim R Lakhani. 2022. How status of research pa-\npers affects the way they are read and cited.Research\nPolicy, 51(4):104484.\nAmalie Trewartha, Nicholas Walker, Haoyan Huo,\nSanghoon Lee, Kevin Cruse, John Dagdelen, Alexan-\nder Dunn, Kristin A. Persson, Gerbrand Ceder, and\nAnubhav Jain. 2022. Quantifying the advantage\nof domain-specific pre-training on named entity\nrecognition tasks in materials science. Patterns,\n3(4):100488.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In IEEE International Confer-\nence on Computer Vision, pages 19–27.\n1275\nA Extant BERT-based models\nDevlin et al. (2019) introduced BERT-Base and\nBERT-Large, with ∼110M and ∼340M parame-\nters, as transformer-based masked language mod-\nels conditioned on both the left and right contexts.\nBoth are pretrained on the English Wikipedia +\nBooksCorpus datasets.\nSciBERT (Beltagy et al., 2019) follows the\nBERT-Base architecture and is pretrained on data\nfrom two domains, namely, biomedical science and\ncomputer science. SciBERT outperforms BERT-\nBase on finetuning tasks by an average of 1.66%\nand 3.55% on biomedical tasks and computer sci-\nence tasks, respectively.\nBioBERT (Lee et al., 2020) is a BERT-Base\nmodel with a pretraining corpus from PubMed ab-\nstracts and full-text PubMedCentral articles. Com-\npared to BERT-Base, BioBERT achieves improve-\nments of 0.62%, 2.80%, and 12.24% on biomedical\nNER, biomedical relation extraction, and biomedi-\ncal question answering, respectively.\nPubMedBERT (Gu et al., 2021), another BERT-\nBase model targeting the biomedical domain, is\nalso pretrained on PubMed and PubMedCentral\ntext. However, unlike BioBERT, PubMedBERT\nis trained as a new BERT-Base model, using text\ndrawn exclusively from PubMed and PubMedCen-\ntral. As a result, the vocabulary used in Pub-\nMedBERT varies significantly from that used in\nBERT and BioBERT. Its pretraining corpus con-\ntains 3.1B words from PubMed abstracts and 13.7B\nwords from PubMedCentral articles. PubMed-\nBERT achieves state-of-the-art performance on\nthe Biomedical Language Understanding and Rea-\nsoning Benchmark, outperforming BERT-Base by\n1.16% (Gu et al., 2021).\nMatBERT (Trewartha et al., 2022) is a materials\nscience-specific model pretrained on 2M journal\narticles (8.8B tokens). It consistently outperforms\nBERT-Base and SciBERT in recognizing materials\nscience entities related to solid states, doped mate-\nrials, and gold nanoparticles, with ∼10% increase\nin F1 score compared to BERT-Base, and a 1% to\n2% improvement compared to SciBERT.\nBatteryBERT (Huang and Cole, 2022) is a model\npretrained on 400 366 battery-related publications\n(5.2B tokens). BatteryBERT has been shown to\noutperform BERT-Base by less than 1% on the\nSQuAD question answering task. For battery-\nspecific question-answering tasks, its F1 score is\naround 5% higher than that of BERT-base.\nB ScholarBERT Pretraining Details\nB.1 Tokenization\nThe vocabularies generated for PRD_1 and\nPRD_10 differed only in 1–2% of the tokens; how-\never, in an initial study, the PRD_100 vocabulary\ndiffered from that of PRD_10 by 15%. A manual in-\nspection of the PRD_100 vocabulary revealed that\nmany common English words such as “is,” “for,”\nand “the” were missing. We determined that these\nomissions were an artifact of PRD_100 being suffi-\nciently large to cause integer overflows in the un-\nsigned 32-bit-integer token frequency counts used\nby HuggingFace’s tokenizers library. For example,\n“the” was not in the final vocabulary because the\ntoken “th” overflowed. Because WordPiece itera-\ntively merges smaller tokens to create larger ones,\nthe absence of tokens like “th” or “##he” means\nthat “the” could not appear in the final vocabulary.\nWe modified the tokenizers library to use un-\nsigned 64-bit integers for all frequency counts, and\nrecreated a correct vocabulary for PRD_100. In-\nterestingly, models trained on the PRD_100 subset\nwith the incorrect and correct vocabularies exhib-\nited comparable performance on downstream tasks.\nB.2 RoBERTa Optimizations\nRoBERTa introduces many optimizations for im-\nproving BERT pretraining performance (Liu et al.,\n2019). 1) It uses a single phase training approach\nwhereby all training is performed with a maximum\nsequence length of 512. 2) Unlike BERT which ran-\ndomly introduces a small percentage of shortened\nsequence lengths into the training data, RoBERTa\ndoes not randomly use shortened sequences. 3)\nRoBERTa uses dynamic masking, meaning that\neach time a batch of training samples is selected\nat runtime, a new random set of masked tokens is\nselected; in contrast, BERT uses static masking,\npre-masking the training samples prior to train-\ning. BERT duplicates the training data 10 times\neach with a different random, static masking. 4)\nRoBERTa does not perform Next Sentence Predic-\ntion during training. 5) RoBERTa takes sentences\ncontiguously from one or more documents until the\nmaximum sequence length is met. 6) RoBERTa\nuses a larger batch size of 8192. 7) RoBERTa uses\nbyte-pair encoding (BPE) rather than WordPiece.\n8) RoBERTa uses an increased vocabulary size of\n50 000, 67% larger than BERT. 9) RoBERTa trains\nfor more iterations (up to500 000) than does BERT-\nBase (31 000).\n1276\nFigure 1: Pretraining loss plots for the SCHOLAR BERT models listed in Table 1. The vertical dashed lines indicate\nthe approximate locations of the iteration checkpoints selected for evaluation in Section 4.1.\nName Description Domain Tokens\nWiki English-language Wikipedia articles (HuggingFace, 2020) Gen 2.5B\nBooks BookCorpus (Zhu et al., 2015; HuggingFace, 2020): Full text of 11038 books Gen 0.8B\nSemSchol 1.14M papers from Semantic Scholar (Cohan et al., 2019), 18% in CS, 82% in Bio Bio, CS 3.1B\nPubMedA Biomedical abstracts sampled from PubMed (Gu et al., 2021) Bio 3.1B\nPubMedB Biomedical abstracts sampled from PubMed (Lee et al., 2020) Bio 4.5B\nPMC Full-text biomedical articles sampled from PubMedCentral (Gu et al., 2021) Bio 13.7B\nMatSci 2M peer-reviewed materials science journal articles (Trewartha et al., 2022) Materials 8.8B\nBattery 0.4M battery-related publications (Huang and Cole, 2022) Materials 5.2B\nPRD_1 1% of the English-language research articles from the Public Resource dataset Sci 2.2B\nPRD_10 10% of the English-language research articles from the Public Resource dataset Sci 22B\nPRD_100 100% of the English-language research articles from the Public Resource dataset Sci 221B\nTable 4: Pretraining corpora used by models in this study. The domains are Bio=biomedicine, CS=computer science,\nGen=general, Materials=materials science and engineering and Sci=broad scientific.\nWe adopt RoBERTa training methods, with three\nkey exceptions. 1) Unlike RoBERTa, we randomly\nintroduce smaller length samples because many of\nour downstream tasks use sequence lengths much\nsmaller than the maximum sequence length of 512\nthat we pretrain with. 2) We pack training sam-\nples with sentences drawn from a single document,\nas the RoBERTa authors note that this results in\nslightly better performance. 3) We use WordPiece\nencoding rather than BPE, as the RoBERTa authors\nnote that BPE can result in slightly worse down-\nstream performance.\nB.3 Hardware and Software Stack\nWe perform data-parallel pretraining on a clus-\nter with 24 nodes, each containing eight 40 GB\nNVIDIA A100 GPUs. In data-parallel distributed\ntraining, a copy of the model is replicated on each\nGPU, and, in each iteration, each GPU computes\non a unique local mini-batch. At the end of the iter-\nHyperparameter Value\nSteps 33 000\nOptimizer LAMB\nLR 0.0004\nLR Decay Linear\nLR Warmup Steps 0.06%\nBatch Size 32 768\nPrecision FP16\nWeight Decay 0.01\nAttention Dropout 10%\nHidden Dropout 10%\nHidden Activation GELU\nTable 5: Pretraining hyperparameters. All SCHOLAR -\nBERT variants use the same pretraining hyperparame-\nters.\nation, the local gradients of each model replica are\naveraged to keep each model replica in sync. We\nperform data-parallel training of SCHOLAR BERT\nmodels using PyTorch’s distributed data-parallel\nmodel wrapper and 16 A100 GPUs. For the larger\nSCHOLAR BERT-XL models, we use the Deep-\n1277\nSpeed data-parallel model wrapper and 32 A100\nGPUs. The DeepSpeed library incorporates a num-\nber of optimizations that improve training time and\nreduced memory usage, enabling us to train the\nlarger model in roughly the same amount of time\nas the smaller model.\nWe train in FP16 with a batch size of 32 768 for\n∼33 000 iterations (Table 5). To achieve training\nwith larger batch sizes, we employ NVIDIA Apex’s\nFusedLAMB (NVIDIA, 2017) optimizer, with an\ninitial learning rate of 0.0004. The learning rate is\nwarmed up for the first 6% of iterations and then\nlinearly decayed for the remaining iterations. We\nuse the same masked token percentages as are used\nfor BERT. Training each model requires roughly\n1000 node-hours, or 8000 GPU-hours.\nFigure 1 depicts the pretraining loss for each\nSCHOLAR BERT model. We train each model\npast the point of convergence and take checkpoints\nthroughout training to evaluate model performance\nas a function of training time.\nC Evaluation Tasks\nWe evaluate the models on eight NER tasks and\nfour sentence-level tasks. For the NER tasks, we\nuse eight annotated scientific NER datasets:\n1. BC5CDR (Li et al., 2016): An NER dataset\nidentifying diseases, chemicals, and their in-\nteractions, generated from the abstracts of\n1500 PubMed articles containing 4409 an-\nnotated chemicals, 5818 diseases, and 3116\nchemical-disease interactions, totaling 6283\nunique entities.\n2. JNLPBA (Kim et al., 2004): A bio-entity\nrecognition dataset of molecular biology con-\ncepts from 2404 MEDLINE abstracts, consist-\ning of 21 800 unique entities.\n3. SciERC (Luan et al., 2018): A dataset annotat-\ning entities, relations, and coreference clusters\nin 500 abstracts from 12 AI conference/work-\nshop proceedings. It contains 5714 distinct\nnamed entities.\n4. NCBI-Disease (Do˘gan et al., 2014): Annota-\ntions for 793 PubMed abstracts: 6893 disease\nmentions, of which 2134 are unique.\n5. ChemDNER (Krallinger et al., 2015): A\nchemical entity recognition dataset derived\nfrom 10 000 abstracts containing 19 980\nunique chemical entity mentions.\n6. MatSciNER (Trewartha et al., 2022): 800 an-\nnotated abstracts from solid state materials\npublications sourced via Elsevier’s Scopus/-\nScienceDirect, Springer-Nature, Royal Soci-\nety of Chemistry, and Electrochemical Soci-\nety. Seven types of entities are labeled: in-\norganic materials (MAT), symmetry/phase la-\nbels (SPL), sample descriptors (DSC), mate-\nrial properties (PRO), material applications\n(APL), synthesis methods (SMT), and charac-\nterization methods (CMT).\n7. ScienceExam (Smith et al., 2019): 133K en-\ntities from the Aristo Reasoning Challenge\nCorpus of 3rd to 9th grade science exam ques-\ntions.\n8. Coleridge (Coleridge Initiative, 2020): 13 588\nentities from sociology articles indexed by the\nInter-university Consortium for Political and\nSocial Research (ICPSR).\nThe sentence-level downstream tasks are relation\nextraction on the ChemProt (biology) and SciERC\n(computer science) datasets, and sentence classifi-\ncation on the Paper Field (multidisciplinary) and\nBattery (materials) dataset:\n1. ChemProt consists of 1820 PubMed abstracts\nwith chemical-protein interactions annotated\nby domain experts (Peng et al., 2019).\n2. SciERC, introduced above, provides 4716 re-\nlations (Luan et al., 2018).\n3. The Paper Field dataset (Beltagy et al.,\n2019), built from the Microsoft Academic\nGraph (Sinha et al., 2015), maps paper ti-\ntles to one of seven fields of study (geogra-\nphy, politics, economics, business, sociology,\nmedicine, and psychology), with each field of\nstudy having around 12K training examples.\n4. The Battery Document Classification\ndataset (Huang and Cole, 2022) includes\n46 663 paper abstracts, of which 29 472 are\nlabeled as battery and the other 17 191 as\nnon-battery. The labeling is performed in\na semi-automated manner. Abstracts are\nselected from 14 battery journals and 1044\nnon-battery journals, with the former labeled\n“battery” and the latter “non-battery.”\n1278\nD Extended Results\nTable 6 shows average F1 scores with standard de-\nviations for the NER tasks, each computed over five\nruns; Figure 2 presents the same data, with stan-\ndard deviations represented by error bars. Table 7\nand Figure 3 show the same for sentence classifica-\ntion tasks. The significant overlaps of error bars for\nNCBI-Disease, SciERC NER, Coleridge, SciERC\nSentence Classification, and ChemProt corrobo-\nrate our observation in Section 4 that on-domain\npretraining provides only marginal advantage for\ndownstream prediction over pretraining on a differ-\nent domain or a general corpus.\n1279\nBC5CDR JNLPBA NCBI-Disease SciERC\nBERT-Base 85.36 ± 0.189 72 .15 ± 0.118 84 .28 ± 0.388 56 .73 ± 0.716\nBERT-Large 86.86 ± 0.321 72 .80 ± 0.299 84 .91 ± 0.229 59 .20 ± 1.260\nSciBERT 88.43 ± 0.112 73 .24 ± 0.184 86 .95 ± 0.714 59 .36 ± 0.390\nPubMedBERT 89.34 ± 0.185 74 .53 ± 0.220 87 .91 ± 0.267 59 .03 ± 0.688\nBioBERT 88.01 ± 0.133 73 .09 ± 0.230 87 .84 ± 0.513 58 .24 ± 0.631\nMatBERT 86.44 ± 0.156 72 .56 ± 0.162 84 .94 ± 0.504 58 .52 ± 0.933\nBatteryBERT 87.42 ± 0.308 72 .78 ± 0.190 87 .04 ± 0.553 59 .00 ± 1.174\nSB_1 87.27 ± 0.189 73 .06 ± 0.265 85 .49 ± 0.998 58 .62 ± 0.602\nSB_10 87.69 ± 0.433 73 .03 ± 0.187 85 .65 ± 0.544 58 .39 ± 1.643\nSB_100 87.84 ± 0.329 73 .47 ± 0.210 85 .92 ± 1.040 58 .37 ± 1.845\nSB_10_WB 86.68 ± 0.397 72 .67 ± 0.329 84 .51 ± 0.838 57 .34 ± 1.199\nSB_100_WB 86.89 ± 0.543 73 .16 ± 0.211 84 .88 ± 0.729 58 .43 ± 0.881\nSB-XL_1 87.09 ± 0.179 73 .14 ± 0.352 84 .61 ± 0.730 58 .45 ± 1.614\nSB-XL_100 87.46 ± 0.142 73 .25 ± 0.300 84 .73 ± 0.817 57 .26 ± 2.146\nChemDNER MatSciNER ScienceExam Coleridge\nBERT-Base 84.84 ± 0.004 78 .51 ± 0.300 78 .37 ± 0.004 57 .75 ± 1.230\nBERT-Large 85.83 ± 0.022 82 .16 ± 0.040 82 .32 ± 0.072 57 .46 ± 0.818\nSciBERT 85.76 ± 0.089 82 .64 ± 0.054 78 .83 ± 0.004 54 .07 ± 0.930\nPubMedBERT 87.96 ± 0.094 82 .63 ± 0.045 69 .73 ± 0.872 57 .71 ± 0.107\nBioBERT 85.53 ± 0.130 81 .76 ± 0.094 78 .60 ± 0.072 57 .04 ± 0.868\nMatBERT 86.09 ± 0.170 83 .35 ± 0.085 80 .01 ± 0.027 56 .91 ± 0.434\nBatteryBERT 86.49 ± 0.085 82 .94 ± 0.309 78 .14 ± 0.103 59 .87 ± 0.398\nSB_1 85.25 ± 0.063 80 .87 ± 0.282 82 .75 ± 0.049 55 .34 ± 0.742\nSB_10 85.80 ± 0.094 80 .61 ± 0.747 83 .24 ± 0.063 53 .41 ± 0.380\nSB_100 85.90 ± 0.063 82 .09 ± 0.022 83 .12 ± 0.085 54 .93 ± 0.063\nSB_10_WB 83.94 ± 0.058 78 .98 ± 1.190 83 .00 ± 0.250 54 .29 ± 0.080\nSB_100_WB 84.31 ± 0.080 80 .84 ± 0.161 82 .43 ± 0.031 54 .00 ± 0.425\nSB-XL_1 85.81 ± 0.054 82 .84 ± 0.228 81 .09 ± 0.170 55 .94 ± 0.899\nSB-XL_100 85.73 ± 0.058 81 .75 ± 0.367 80 .72 ± 0.174 54 .54 ± 0.389\nTable 6: NER F1 scores for each of 14 models (rows), when the model is finetuned on eight different domain\ndatasets and the resulting finetuned model applied to that dataset’s associated NER task (columns). In each case, we\ngive the average value and its standard deviation over five runs.\nFigure 2: NER F1 scores from Table 6, with standard deviations represented by error bars.\n1280\nSciERC ChemProt PaperField Battery\nBERT-Base 74.95 ± 1.596 83 .70 ± 0.472 72 .83 ± 0.082 96 .31 ± 0.087\nBERT-Large 80.14 ± 2.266 88 .06 ± 0.353 73 .12 ± 0.125 96 .90 ± 0.156\nSciBERT 79.26 ± 0.498 89 .80 ± 0.263 73 .19 ± 0.046 96 .38 ± 0.153\nPubMedBERT 77.45 ± 0.964 91 .78 ± 0.096 73 .93 ± 0.099 96 .58 ± 0.148\nBioBERT 80.12 ± 0.179 89 .27 ± 0.281 73 .07 ± 0.074 96 .06 ± 0.200\nMatBERT 79.85 ± 0.121 88 .15 ± 0.026 71 .50 ± 0.135 96 .33 ± 0.106\nBatteryBERT 78.14 ± 0.550 88 .33 ± 0.939 73 .28 ± 0.022 96 .06 ± 0.437\nSB_1 73.01 ± 0.248 83 .04 ± 0.150 72 .77 ± 0.060 94 .67 ± 0.671\nSB_10 75.95 ± 0.203 82 .92 ± 0.792 72 .94 ± 0.182 92 .83 ± 3.758\nSB_100 76.19 ± 1.592 87 .60 ± 0.324 73 .14 ± 0.085 92 .38 ± 5.789\nSB_10_WB 73.17 ± 1.254 81 .48 ± 1.705 72 .37 ± 0.115 93 .15 ± 1.763\nSB_100_WB 76.71 ± 2.114 83 .98 ± 0.252 72 .29 ± 0.048 95 .55 ± 0.272\nSB-XL_1 74.85 ± 1.497 90 .60 ± 0.246 73 .22 ± 0.009 88 .75 ± 4.035\nSB-XL_100 80.99 ± 0.900 89 .18 ± 0.499 73 .66 ± 0.113 95 .44 ± 0.100\nTable 7: Sentence classification F1 scores for each of 14 models (rows), when the model is finetuned on one of four\ndifferent domain datasets and the finetuned model is applied to that dataset’s associated sentence classification task\n(columns). In each case, we give the average value and its standard deviation over five runs.\nFigure 3: Sentence classification F1 scores from Table 7, with standard deviations represented by error bars.\n1281\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection Limitations\n□\u0017 A2. Did you discuss any potential risks of your work?\nOur article reports ﬁndings on evaluating language models on scientiﬁc information extraction tasks,\nwhich we do not believe could pose any risk.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection Abstract and Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B.3 Hardware and Software Stack\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1282\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3 Table 1 and Appendix B Table 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4 and Appendix D\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix B.3 Hardware and Software Stack\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1283",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8149054646492004
    },
    {
      "name": "Computer science",
      "score": 0.7821468114852905
    },
    {
      "name": "Language model",
      "score": 0.7811038494110107
    },
    {
      "name": "Task (project management)",
      "score": 0.5687316060066223
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.5448006987571716
    },
    {
      "name": "Training set",
      "score": 0.5143638849258423
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48492223024368286
    },
    {
      "name": "Natural language processing",
      "score": 0.48126715421676636
    },
    {
      "name": "Labeled data",
      "score": 0.4551905691623688
    },
    {
      "name": "Information extraction",
      "score": 0.4393543004989624
    },
    {
      "name": "Machine learning",
      "score": 0.42076390981674194
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ]
}