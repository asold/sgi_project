{
    "title": "VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots",
    "url": "https://openalex.org/W4394591929",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3012528561",
            "name": "Akhil Padmanabha",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2152602997",
            "name": "Jessie Yuan",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5103102487",
            "name": "Janavi Gupta",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A4201122783",
            "name": "Zulekha Karachiwalla",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A730153875",
            "name": "Carmel Majidi",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2107741732",
            "name": "Henny Admoni",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2403753017",
            "name": "Zackory Erickson",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3217298356",
        "https://openalex.org/W2981450372",
        "https://openalex.org/W3009086013",
        "https://openalex.org/W3194176109",
        "https://openalex.org/W2156324385",
        "https://openalex.org/W1964695965",
        "https://openalex.org/W2991303041",
        "https://openalex.org/W2801271260",
        "https://openalex.org/W2139158987",
        "https://openalex.org/W4392646123",
        "https://openalex.org/W3045522824",
        "https://openalex.org/W2537120235",
        "https://openalex.org/W2128047661",
        "https://openalex.org/W4391407102",
        "https://openalex.org/W2963507484",
        "https://openalex.org/W4392633770",
        "https://openalex.org/W2084722860",
        "https://openalex.org/W2743243568",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W4392646041",
        "https://openalex.org/W4391335180",
        "https://openalex.org/W4323871205",
        "https://openalex.org/W4390305886",
        "https://openalex.org/W4383552038",
        "https://openalex.org/W2963166639",
        "https://openalex.org/W2966754145",
        "https://openalex.org/W2896654476",
        "https://openalex.org/W4392645923",
        "https://openalex.org/W2563129138",
        "https://openalex.org/W2055458476",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4294680543",
        "https://openalex.org/W4388720459",
        "https://openalex.org/W4388182168",
        "https://openalex.org/W4389665575"
    ],
    "abstract": "Physically assistive robots present an opportunity to significantly increase\\nthe well-being and independence of individuals with motor impairments or other\\nforms of disability who are unable to complete activities of daily living.\\nSpeech interfaces, especially ones that utilize Large Language Models (LLMs),\\ncan enable individuals to effectively and naturally communicate high-level\\ncommands and nuanced preferences to robots. Frameworks for integrating LLMs as\\ninterfaces to robots for high level task planning and code generation have been\\nproposed, but fail to incorporate human-centric considerations which are\\nessential while developing assistive interfaces. In this work, we present a\\nframework for incorporating LLMs as speech interfaces for physically assistive\\nrobots, constructed iteratively with 3 stages of testing involving a feeding\\nrobot, culminating in an evaluation with 11 older adults at an independent\\nliving facility. We use both quantitative and qualitative data from the final\\nstudy to validate our framework and additionally provide design guidelines for\\nusing LLMs as speech interfaces for assistive robots. Videos and supporting\\nfiles are located on our project website:\\nhttps://sites.google.com/andrew.cmu.edu/voicepilot/\\n",
    "full_text": "VoicePilot: Harnessing LLMs as Speech Interfaces for Physically\nAssistive Robots\nAkhil Padmanabha*\nCarnegie Mellon University\nPittsburgh, PA, USA\nakhilpad@andrew.cmu.edu\nJessie Yuan*\nCarnegie Mellon University\nPittsburgh, PA, USA\njzyuan@andrew.cmu.edu\nJanavi Gupta\nCarnegie Mellon University\nPittsburgh, PA, USA\njanavig@andrew.cmu.edu\nZulekha Karachiwalla\nCarnegie Mellon University\nPittsburgh, PA, USA\nzkarachi@andrew.cmu.edu\nCarmel Majidi\nCarnegie Mellon University\nPittsburgh, PA, USA\ncmajidi@andrew.cmu.edu\nHenny Admoni\nCarnegie Mellon University\nPittsburgh, PA, USA\nhenny@cmu.edu\nZackory Erickson\nCarnegie Mellon University\nPittsburgh, PA, USA\nzackory@cmu.edu\nFRAMEWORK ITERATION PROCESS\nUSER STUDY WITH ASSISTIVE FEEDING ROBOT \nDESIGN GUIDELINES\n11 older adults at an independent living facility\nVersion 1\nFrom Literature\nPilot Testing\nWith Lab Members\nVersion 2\nImprove Accuracy Demonstration\nWith Community Members\nVersion 3\nIncrease Ease of Use\nObi scoops yogurt Obi moves to mouth User takes a bite\n“Hey Obi, feed\nme a scoop of\neach food.”\nCustomizability Comparable Time to\nCaregiver\nMulti-Step Instruction Consistency Social Capability\nObi scoops pretzels Obi scoops blueberries \nFigure 1: Top: Our work introduces a framework that was iteratively developed for integrating LLMs as speech interfaces for\nassistive robots. Middle: We develop a speech interface for a feeding robot using the framework and evaluate it with 11 older\nadults at an independent living facility. Bottom: Using insights from the study, we present a reflection and 5 design guidelines.\n*These authors contributed equally and are ordered alphabetically.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0628-8/24/10\nhttps://doi.org/10.1145/3654777.3676401\nABSTRACT\nPhysically assistive robots present an opportunity to significantly\nincrease the well-being and independence of individuals with motor\nimpairments or other forms of disability who are unable to com-\nplete activities of daily living. Speech interfaces, especially ones\nthat utilize Large Language Models (LLMs), can enable individuals\nto effectively and naturally communicate high-level commands and\nnuanced preferences to robots. Frameworks for integrating LLMs\narXiv:2404.04066v2  [cs.RO]  17 Jul 2024\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\nas interfaces to robots for high level task planning and code gener-\nation have been proposed, but fail to incorporate human-centric\nconsiderations which are essential while developing assistive inter-\nfaces. In this work, we present a framework for incorporating LLMs\nas speech interfaces for physically assistive robots, constructed\niteratively with 3 stages of testing involving a feeding robot, cul-\nminating in an evaluation with 11 older adults at an independent\nliving facility. We use both quantitative and qualitative data from\nthe final study to validate our framework and additionally provide\ndesign guidelines for using LLMs as speech interfaces for assistive\nrobots. Videos, code, and supporting files are located on our project\nwebsite1\nCCS CONCEPTS\n• Computer systems organization →External interfaces for\nrobotics; • Computing methodologies →Speech recognition .\nKEYWORDS\nassistive robotics, large language models (LLMs), speech interfaces\nACM Reference Format:\nAkhil Padmanabha*, Jessie Yuan *, Janavi Gupta, Zulekha Karachiwalla,\nCarmel Majidi, Henny Admoni, and Zackory Erickson. 2024. VoicePilot:\nHarnessing LLMs as Speech Interfaces for Physically Assistive Robots. In\nThe 37th Annual ACM Symposium on User Interface Software and Technology\n(UIST ’24), October 13–16, 2024, Pittsburgh, PA, USA. ACM, New York, NY,\nUSA, 18 pages. https://doi.org/10.1145/3654777.3676401\n1 INTRODUCTION\nMotor impairments affect a significant percentage of the United\nStates with approximately 5 million individuals (1.7%) affected by\nvarying degrees of paralysis due to conditions including stroke and\nspinal cord injury [ 8]. Physical impairments and other forms of\ndisabilities can hinder individuals from performing activities of\ndaily living (ADLs), such as eating and bathing, and instrumental\nactivities of daily living (iADLs), such as cooking and cleaning. This\ncan significantly impact an individual’s independence and quality\nof life, requiring them to rely on a caregiver for assistance [ 18,\n19, 22, 50]. This is especially the case in older populations with\nover 20% of individuals in the United States over 65 years of age\nrequiring assistance with at least one of their self care or mobility\ntasks [21].\nPhysically assistive robots, such as the adaptive eating robot\nObi [2], shown in Fig. 2, can enable individuals with impairments to\nperform a range of self-care and household tasks [15, 25, 33, 39, 57].\nAssistive interfaces can allow individuals with varying forms of\ndisability to control such robots. Interfaces often have physical\nrequirements, dictating which individuals can effectively use them.\nFor example, individuals without the ability to move their head may\nuse eye tracking with a web-based interface [13, 14] while those\nwith head motion may prefer inertial interfaces that capture their\nhead movements and directly map them to robot motion [40, 41].\nAmong these interfaces, speech excels as an option for those\nwith the ability to speak, as it allows individuals to naturally provide\nrobots with both high-level commands and nuanced preferences\nor customizations. Speech interfaces for assistive robots have been\n1https://sites.google.com/andrew.cmu.edu/voicepilot/\nexplored extensively for control of both mobile robots and robot\narms [26, 34, 45] and can be more intuitive than existing interface\noptions like joysticks [ 44]. With the advent of Large Language\nModels (LLMs) and ongoing research combining LLMs with robots,\nrobust speech interfaces for robots are becoming increasingly viable\nin the near future. Existing works have presented frameworks and\nguidelines for integrating LLMs with robots for high-level task\nplanning and code generation [6, 12, 35, 37, 51, 53, 55, 58, 59]. These\nworks concentrate on prompt engineering and code generation to\nenhance the accuracy of large language models (LLMs). No prior\nworks involve human subjects interacting directly with an LLM-\nintegrated robot for physically assistive tasks. Consequently, they\nlack insight and guidelines on the human-centric considerations\nessential for using LLMs as assistive interfaces.\nTo address this gap, we develop a framework and design guide-\nlines for integrating LLMs as speech interfaces for physically assis-\ntive robots. Our presented framework underwent extensive iterative\ndevelopment and consists of components related to prompt engi-\nneering and system rollout, the process of deploying the LLM-based\nspeech interface and robot with users. First, we leveraged insight\nfrom existing literature to develop an initial version. We applied this\nVersion 1 framework to integrate an existing LLM, GPT-3.5 Turbo\nfrom OpenAI [3], with the commercial feeding robot Obi [2]. After\ntesting this interface with lab members, we refined the framework,\niterated on our LLM-based speech interface for the Obi, and demon-\nstrated it to community members at a disability awareness event\n(Version 2). We used insights from the demonstration to develop\na final framework and interface, evaluated through a user study\ninvolving 11 older adults at an independent living facility (Version\n3). Lastly, we use both quantitative and qualitative data from the\nstudy with older adults to present design guidelines.\nOur final framework, shown in Fig. 3, and design guidelines,\nshown in Fig. 8, provide insight on important human-centric con-\nsiderations to researchers, engineers, and product designers for\nintegrating off-the-shelf LLMs with assistive robots and devices.\nThe contributions of this work are as follows:\n•We present an iteratively constructed framework, shown in\nFig. 3, for integrating LLMs as assistive speech interfaces for\na robot.\n•We integrate an existing LLM with a commercial feeding ro-\nbot using our framework and evaluate the interface through\na human study with 11 older adults from an independent\nliving facility.\n•We present a reflection and design guidelines, shown in Fig. 8,\nbased on the findings of the human study.\n2 RELATED WORK\n2.1 LLMs for Robotics\nThere have been numerous advancements in using LLMs for ro-\nbotics [6, 12, 20, 30, 35, 37, 51, 53–55, 58, 59]. For the scope of this\npaper, we will focus on works that use off-the-shelf LLMs for high-\nlevel task planning through code generation, as these works are\nmost pertinent to using LLMs as assistive interfaces. Liang et al.\nutilize codewriting LLMs with perception and control APIs to cre-\nate reactive and waypoint-based trajectories for various robotic\nplatforms, while Vemprala et al. develop design guidelines and a\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nMicrophone\nReference Sheet\nBowl 0\nBowl 1\nBowl 2\nBowl 3\nFigure 2: Obi robot and setup for study with 11 older adults\nat an independent living facility. The Obi robot’s arm moves\ntowards a participant’s mouth with a spoonful of granola. A\nmicrophone is positioned to the right of the participant and\na cheat sheet with example commands is placed to the left\nof the participant.\npipeline for integrating ChatGPT with different robots and sim-\nulation environments [35, 53]. Simarily, Singh et al. and Arenas\net al. provide prompting guidelines and methodologies for using\nLLMs in robotic tasks [7, 51]. Other work has used ChatGPT as an\ninterface for surgical robots and quadrupeds [36, 42]. These prior\nworks primarily focus on high level planning and code generation\nwhich is just one aspect of designing a speech interface. Most utilize\nprompt engineering and evaluate their LLMs using accuracy, specif-\nically if the LLM succeeds in generating the “correct” code that the\nresearcher expects based on the provided prompt and command.\nWhile some papers include human studies to evaluate their meth-\nods, none evaluate their system with participants who interacted\ndirectly with the LLM-integrated robot. In comparison, our work\nincludes multiple rounds of direct interaction between the robot\nand participants including with older adults who are close to the\ntarget population of individuals who need feeding assistance. This\nprocess allows us to identify human-centric elements essential to\npositive user perception of the system.\nPast works have also explored using LLMs with social and ser-\nvice robots [27, 31, 32, 56]. Wu et al. [56] focus on learning personal\npreferences through language input for TidyBot, a cleaning robot,\nwhile Hu et al. [27] benchmark LLM performance for code genera-\ntion for various service tasks. Both papers focus on how LLMs can\nbe utilized to perform a specific task and similar to other papers,\ndo not have participants interacting directly with the robot. The\nabsence of these human studies limits the insights into improving\nthe general interface design and human-robot interaction. Lastly,\nKim et al. explore using LLMs with a social robot and focus on con-\nversational tasks, such as having the robot guide the participant in\nmaking a drink [32]. The authors compare their robot against text\nand voice based agents and present design insights from a thematic\nanalysis of the data obtained during the human study. Most of their\nfindings are only applicable to social, conversational robots and do\nnot generalize to physically assistive robots and tasks such as robot-\nassisted feeding. In comparison to all past work on using LLMs for\nrobots, we iterate on our presented framework through multiple\nrounds of testing with users and we introduce design guidelines\nderived from our final human study.\n2.2 Robot-Assisted Feeding\nRobot-assisted feeding systems employ robotic manipulators and\nperception systems to feed users either autonomously or semi-\nautonomously using input from users through assistive interfaces [9,\n10, 23, 29, 43]. Voice control interfaces have been explored in past\nworks on robot-assisted feeding, but use keyword detection in\ncomparison to LLMs, which could enable more natural speech in-\nteractions with the robot [ 11, 38]. Bhattacharjee et al. find that\nparticipants prefer using speech over a web interface while dining\nalone [11] while Nanavati et al. also find that participants see value\nin speech interfaces in quieter social settings [38]. In both studies,\nthe researchers find that participants may not want to use speech\nin social dining settings as it may interfere in conversations. Lastly,\nNanavati et al. find that participants want customizable feeding\nrobots that can be tailored to specific needs and take into account\ntheir preferences [38]. These works motivate the need for the de-\nvelopment of customizable speech interfaces for assistive feeding\nrobots.\nAlongside research progress in assistive feeding, commercial\nfeeding robots, such as the Obi feeding robot, are being used by hun-\ndreds of individuals across the United States. Obi, shown in Fig. 2,\nis a FDA compliant Class 1 medical device from DES ¯IN LLC [2].\nThe robot consists of a 6 degree-of-freedom robot arm with a spoon\nend-effector and a dish tray consisting of 4 bowls. The commercial\nsystem uses hard-coded trajectories for the utensil to scoop food\nfrom a bowl and transfer that food to a predefined location near the\nuser’s mouth. For this work, we utilize a research version of the Obi\nrobot with a Python API that allows us to send trajectories of joint\nangles to the robot. Obi is currently only commercially sold with a\ntwo button interface, with one button to select the bowl and the\nother button to scoop from the selected bowl. The button interface\ncan be limiting especially for users who may want to customize\ntheir assistance; in comparison, an LLM-based speech interface can\nallow a user to provide verbal instructions to Obi, much like they\nwould to a human caregiver, allowing them to customize how the\nrobot feeds them.\n3 VERSION 1: FRAMEWORK FROM\nLITERATURE\n3.1 Framework Development\nPrior work has shown that off-the-shelf LLMs can be used for high\nlevel planning and code generation for robots when provided with\na well-designed prompt [7, 51, 53, 54]. We start our design process\nby drawing on this existing work on prompt engineering for LLMs\nfor robotic control to develop an initial framework.\nOur Version 1 framework identifies five preliminary components,\nshown and described in Fig. 3. We describe how these preliminary\ncomponents were motivated from past literature:\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\nEnvironment\nDescription\nThe LLM prompt should include a physical description of the robot, the environment in which the\nrobot is operating, and the general task the robot is intended to accomplish.1\nPrompt Engineering (PE) System Rollout (SR) Prompt Engineering and System Rollout (PS)\nV1\nRobot\nFunctions\nThe Robot API should include high-level functions governing robotic movement, named to clearly\nconvey their purpose; simple functions are ideal and consolidate multiple functions when practical.2\nV1\nFunction\nApplications\nThe LLM should have descriptions of the functions and their intended use cases, along with\ninstructions on how the LLM should combine them and examples of possible sequences.3\n4 Code\nSpecifications\nThe system should enforce rules for the LLM when generating code, specifying the programming\nlanguage, accessible modules, code-only responses and restriction to provided functions.\nV1\nV1\n5 Safety The system should have tailored precautions for each robot/task to mitigate risks, including using\npredefined trajectories and restricting variables to conservative ranges with post-processing.V1\nRobot\nVariables\nThe LLM prompt should have descriptions of variables that affect the robot’s movement along with\ntheir default values and small, discrete ranges to ground physical variables like actuator speed.6\nInstructional\nMaterials\nThe system should be accompanied with directions for the user on how to talk to the robot and\nexample interactions to explain to users what they should expect.7\nUser Control\nFunctions\nThe system and/or LLM prompt should include functions that increase the feeling of control for the\nuser, such as allowing the user to pause or stop the robot's movements.8\nFeedback The system should have output to confirm the receipt of the user’s command and provide succinct\nupdates on its state.\nV2\nV3\nV3\n9\nV3\nPE\nPE\nPE\nPS\nPE\nPS\nSR\nPS\nPS\nFigure 3: Our final framework consisting of 9 components is shown. The color and annotation at the top left of each numbered\ncomponent indicates whether the component is related to prompt engineering (PE), system rollout (SR), or both prompt\nengineering and system rollout (PS). System rollout refers to the process of deploying the LLM-based speech interface and robot\nwith users. The annotation at the bottom right of each numbered component shows which framework iteration it was added in.\n➢ Environment Description: Vemprala et al. suggest to include\ninformation in the prompt about the environment in which\nthe task is taking place and the goals and objectives of the\ntask [53]; likewise, Arenas et al. recommend including an\noverview of the robot and the task at hand [ 7]. We com-\nbine insights from these past works to define Environment\nDescription as a physical description of the robot, the en-\nvironment in which the robot is operating, and the general\ntask the robot is intended to accomplish.\n➢ Robot Functions: Past papers on using LLMs for high-level\ntask planning and code generation suggest defining and list-\ning a set of robot functions, named the Robot API, in the\nprompt so that the LLM can access these function names\nand use them to control the robot [7, 51, 53]. Similar robot\nfunctions are required for LLM-based assistive speech in-\nterfaces. For example, for an assistive cleaning robot, you\nmay include a function to perceive objects and a function\nto grasp an object which can be used in conjunction by an\nLLM to generate code to detect and pick up toys from the\nfloor of a cluttered room.\n➢ Function Applications: Beyond listing the names of the ro-\nbot functions, Vemprala et al. and Arenas et al. recommend\nincluding documentation of what each function does in the\nprompt [7, 53]. Both works also suggest providing solution\nexamples to demonstrate to GPT how to use the functions\nto accurately respond to users commands; Arenas et al. es-\npecially emphasize the importance of examples because the\ncustom control functions are not in an LLM’s training set,\nso the model is unfamiliar with them [ 7]. Therefore, we\ndedicate a component of our framework to specifying what\neach function does and providing examples of how they are\nintended to be used.\n➢ Code Specifications: In order to use an off-the-shelf LLM for\ncode generation for robots, past work has shown that the\nLLM must be prompted with general details of the program-\nming environment, including what programming language\nshould be used and what packages are accessible [7, 51, 53].\n➢ Safety: While integrating LLMs with assistive robots, safety\nneeds to be evaluated on a case by case basis depending on\nthe robot and the task at hand. Vemprala et al. suggest having\na human in the loop to inspect the code that GPT generates,\nwhich isn’t practical for an assistive interface that should\nrespond immediately to the user [53]. Other options include\ncontrol-barrier functions [16, 48] and testing extensively in\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nsimulation before deployment [24, 54]. Due to the especially\ncrucial role of safety in the context of physically assistive\nrobots, we suggest using lightweight or compliant robots and\npredefined trajectories, thus barring GPT from modifying\nlow-level joint angles or robot positions.\n3.2 Implementation\nTo validate our framework, we implemented an LLM-based speech\ninterface for the Obi feeding robot. We chose to use the Obi as it is a\ncommercially available assistive robot designed with safety consid-\nerations, namely a lightweight arm and a detachable spoon in case\nof collisions. Additionally, as discussed in Section 2.2, prior studies\non robot-assisted feeding indicate a preference for voice control\ninterfaces in specific scenarios like individual dining, underscoring\nthe necessity for such interface development.\nUsing our initial framework, we crafted a tailored prompt for\nGPT-3.5 Turbo, subsequently referred to as GPT, designed for the\nObi feeding robot. Our prompt, shown in Fig. A1, integrated the\nfive identified components from our Version 1 framework.\nWe addressed the components as follows: to implement the\nEnvironment Description, within our prompt, we included de-\ntails on the task at hand, feeding the user, and the working en-\nvironment, such as the foods in each of the bowls, as the Obi\nrobot does not have a built-in perception system. We incorpo-\nrated the Robot Functions and Function Applications components\nby providing the names of three high-level robotic control func-\ntions in the prompt, along with a short description of what each\nfunction did and how it should be used. These three functions\nmoved the robot arm along hard-coded trajectories and were\ndescribed as follows: obi.scoop_from_bowlno(bowlno) moved\nthe robotic arm to the specified bowl and scooped food from it;\nobi.move_to_mouth() moved the robot to the user’s mouth posi-\ntion; and obi.scrape_down_bowlno(bowlno) moved the robot to\nthe specified bowl and subsequently scraped food from the sides to\nthe center of that bowl so that the robot could pick up additional\nfood. We designed the Robot API to include these three functions\nbecause they correspond to actions performed by the commercially\navailable Obi robot. Specifically, the robot’s default two-button\ninterface allows users to select the bowl with one button and\nscoop/feed themselves with the other. Additionally, the scraping\nmotion occurs automatically after every few scoops.\nAt the end of the prompt, to satisfy the Code Specifications com-\nponent, we included sentences instructing GPT to provide Python\ncode for each request. Lastly, to address the Safety component, each\nfunction only moved the robot along a predefined trajectory, as\nopposed to allowing GPT to directly dictate the joint angles of Obi’s\narm. This prevents GPT from generating unsafe or unanticipated\ntrajectories. Choosing to use the Obi robot, which is specifically de-\nsigned to be safe for individuals with motor impairments, instead of\nother robotic feeding systems, is another implementation decision\nfor enhancing safety.\nAt this stage, users would hold down a physical button while\nspeaking to record a command to the robot, which would be tran-\nscribed to text using OpenAI’s Whisper API [4]. The parsed speech-\nto-text commands were appended to the bottom of the prompt\nand provided to GPT, which generated Python code that could\nsubsequently be deployed on the Obi robot.\n3.3 Pilot Testing with Lab Members\nWe conducted pilot testing of our Version 1 interface for the Obi\nrobot within our research team and members of our lab group. Four\nlab members watched videos of the robot performing four feeding\ntasks and suggested different command phrasings to accomplish\nthe task, resulting in 48 phrases. These commands were tested\nfor accuracy, with mistakes analyzed and prompts adjusted itera-\ntively. Later, three researchers directly interacted with the robot to\nfurther assess and improve the system, focusing on user comfort\nduring assisted feeding. More details on this process is included in\nAppendix A.1.\n4 VERSION 2: IMPROVED ACCURACY AND\nCUSTOMIZABILITY\n4.1 Framework Iteration\nBased on our pilot testing, we noted instances where GPT made er-\nrors, enabling us to identify gaps or areas within the framework that\nrequired iteration. Specifically, the Code Specifications were not\ndetailed enough, causing GPT to import nonexistent modules and\ndefine unnecessary new functions. To address this, the framework\nwas modified to emphasize the importance of explicitly outlining\nhow GPT should write code. Additionally, the Function Applica-\ntions component was expanded to better describe the relationships\nbetween functions and provide examples, while the Robot Functions\ncomponent was modified to suggest merging common sequential\nfunctions to improve performance in tasks that required multiple\nrobot actions. More information on these changes can be found in\nAppendix A.2.\nFinally, we added a new field to the existing framework:\n➢ Robot Variables: Users wanted to have more ability to cus-\ntomize the way in which the robot moved. In the case of\nthe Obi robot, the first iteration had only a single speed and\nscoop depth, yet, based on user feedback, we found that pref-\nerences for these can vary significantly from user to user,\nbetween different foods, and even from situation to situation.\nAccordingly, we added a component to the framework pro-\nviding descriptions of variables affecting the way in which\nthe robot moves, their ranges, and their default values.\nDue to the user safety implications of the new Robot Variables\ncomponent which gives the LLM speech interface control of motion\nvariables such as speed, we proactively modified the Safety com-\nponent of the framework to suggest restriction of robot variables\nsuch as speed to conservative ranges and to check the validity of\nrobot variables in LLM-outputted code to ensure they are within\nexpected bounds.\n4.2 Implementation\nAt this stage, we changed the prompt for the Obi system to reflect\nthe modifications to the framework and thus improve the accuracy\nof GPT’s responses. Our updated prompt can be found in Fig. A1.\nIn summary, we updated the prompt to emphasize the produc-\ntion of responses exclusively in Python code, without importing\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\nFigure 4: 9 of the 11 participants are shown at various stages of the Version 3 study at an independent living facility with older\nadults.\nextra modules or creating new functions, as specified in the revised\nCode Specifications component. To adhere to the updated Func-\ntion Applications component, we added detailed instructions on\nhow the robot functions should be combined, emphasizing Obi’s\nmovement to the user’s mouth when asked to be fed, and included\na 4-second default pause between bites for better user experience,\nwith customization options for the delay. Additionally, the Robot\nFunctions component was modified to ensure that after scraping\ndown a bowl, Obi would immediately scoop from the same bowl to\nimprove accuracy, addressing previous issues where the sequence\nwas misunderstood. For the Robot Variables component, we added\nnew variables for Obi’s movements (speed, acceleration, and scoop\nsize) to enhance customizability, with post-processing by the sys-\ntem to ensure safety. More information on these changes can be\nfound in Appendix A.3.\n4.3 Demonstration with Community Members\nAfter the modifications to the implementation in Version 2, we\nbrought the system to a disability awareness event in Pittsburgh,\nPA, USA, where we demonstrated the speech interface to about 15\nmembers of the community, some of whom had motor impairments\nor other types of disabilities. During the event, community members\nwho wanted to interact with the robot provided commands to the\nfeeding robot to have it feed a lab member either M&Ms or Cheerios\ncereal placed in two bowls of the Obi. We encouraged visitors to\nbe creative with their requests to the robot. Most individuals who\ninteracted with the robot provided only 1-2 commands and the\ninteractions were short, lasting only a few minutes. Nevertheless,\nthis process was helpful for evaluating the system, enabling us to\nidentify where users were confused or struggled to use the interface,\nas well as hear feedback on what people liked and disliked.\n5 VERSION 3: INCREASED EASE OF USE\n5.1 Framework Iteration\nUsing insights from the Version 2 demonstration, we identified\nessential modifications and new components for our framework.\nFirst, we made adjustments to the Robot Variables component due to\nGPT’s inconsistent handling of continuous variables, which caused\nunpredictable speed changes that confused users. By grounding\nvariables like actuator speed on a 0-5 scale, speed adjustments\nbecame more consistent and aligned with user expectations. This\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nprocess allowed us to identify that while increasing the granularity\nof robot function and variables can enhance the user perception\nof robot customization, they can negatively impact the language\nmodel’s accuracy. More details are included in Appendix A.4.\nAdditionally, we added three new components to the framework,\nall of which focus on human-robot interaction:\n➢ Instructional Materials: Users tended to struggle with giving\ncommands to the system at first, as they were unsure what\nthey were allowed to say or what the robot was capable of\nand were wary of experimenting with new commands. Thus,\nit became evident that the framework required a field on how\nto effectively prepare users on how to use a speech interface\nby providing guidelines for user communication with the\nrobot and sample interactions to set their expectations. This\ncomponent was added to emphasize the importance of giving\nusers more guidance and support with using the system\nbeforehand to reduce user confusion.\n➢ User Control Functions: A component was added to incorpo-\nrate functions that increase the feeling of control for the user,\nan important consideration for assistive interfaces [11, 28,\n40]. Example functions include allowing the user to pause\nor stop the robot’s movements using voice commands.\n➢ Feedback: Users struggled to discern whether the system\nheard them or not, especially given the few second delay\nbetween completion of the command and robot movement\ndue to networking and GPT processing the request. As a\nresult, they would sometimes attempt to repeat themselves\nor become confused or frustrated. Thus, we added a Feed-\nback component to the framework to convey the need for\nfeedback from the system to the users informing them about\nthe current state of processing and execution and providing\na clear error message if it is unable to process their command\ncorrectly.\nIn comparison to all related work, these components move be-\nyond governing only prompt engineering, instead providing recom-\nmendations for designing the system as a whole in a user-centric\nway.\n5.2 Implementation\nWe updated the implementation for the Obi robot using our latest\nframework, aiming to enhance user acceptance of the system. Our\nupdated prompt is shown in Fig. A1.\nFirst, to adhere to the updated Robot Variables component on\ngrounding of physical variables, we changed the continuous speed\nand acceleration ranges provided to GPT from [0, 80] degrees per\nsecond and [0, 250] degrees per second squared respectively to dis-\ncrete [0, 5] ranges. Next, for the Feedback component, we changed\nhow users interact with the system by replacing the need to hold\ndown a button with a wakeup phrase, \"Hey Obi, \" implemented via\nthe Porcupine Python API from PicoVoice. We also added audio\ncues to inform users when the wakeup phrase is registered, when\ntheir request is being processed, and when the robot is about to\nexecute or has completed an action. For the Instructional Materials\ncomponent, we created an instructional video and a reference sheet,\nincluded as Supporting File 1 and 2, to help users understand and\nuse the system effectively. These materials are expected to be tem-\nporary aids as users become familiar with the robot. More details\ncan be found in Appendix A.5.\nFinally, we implemented three new User Control Functions that\nGPT is able to call, giving users the ability to interrupt and halt\nthe movement of the robot with a verbal command for any rea-\nson: obi.start(), which begins or resumes execution of any ro-\nbot code; obi.stop(), which permanently stops execution of any\ncurrently running robot code; and obi.pause_indefinitely(),\nwhich suspends execution of any currently running robot code.\n5.3 Testing with Older Adults\nTo evaluate Version 3 of the implementation, we conducted a formal\nstudy at an independent living facility, Baptist Providence Point\nin the greater Pittsburgh, Pennsylvania metropolitan area, with\n11 non-disabled older adults (5M, 6F) ranging in age from 72-91\n(mean = 81.1, SD = 5.9). A subset of the participants are shown\nin Fig. 4. The ethnicity of all participants was white. Older adults\nwere chosen in the study as they share more similarities with the\ntarget demographic of individuals with physical impairments as\ncompared to the prior populations the interface was tested with due\nto their age. Our study, approved by Carnegie Mellon University’s\nInstitutional Review Board, involved the researchers traveling to\nthe independent living facility and conducting one-hour-long stud-\nies with each participant. Recruitment was conducted in person\nthrough an information session held at the independent living facil-\nity as well as over email, and informed consent was obtained from\nthe participants for use of image in publication. Participants were\nasked how much experience they have with controlling a robot\non a 7-point scale ranging from “1 = no experience” to “7 = expert\nuser” and how they feel about using robots to help with everyday\ntasks ranging from “1 = very negatively” to “7 = very positively”.\nParticipants had a median experience of 1 (IQR = 1, min = 1, max =\n4) and a median attitude of 6 (IQR = 1, min = 4, max = 7).\nAs seen in Fig 2, we placed the Obi robot and a microphone\ndirectly in front of the participant. If needed, we adjusted the food\ndelivery location of the robot to a comfortable location for the\nparticipant directly in front of their mouth. As discussed previously,\nin line with the “Instructional Materials” field of our framework,\nwe started the study by showing the participant an instructional\nvideo and providing them with a reference sheet—both included as\nSupporting Files.\nAfter the instructional session, the participant was asked to\nselect three foods out of the following options: blueberries, yogurt,\ngranola, pudding, pretzels, and Cheerios. As seen in Fig 2, these\nfoods were added to bowls 0, 1, and 2 while bowl 3 is left empty. The\nfirst part of the study consisted of 1 practice task, eat a single scoop\nfrom any bowl, and 5 predefined tasks: (1) eat a single scoop from\nbowl 0 quickly, (2) eat a single large scoop from bowl 2, (3) scrape\nbowl 0 and eat a single scoop from bowl 0, (4) eat three scoops from\nbowl 1, (5) eat a single scoop from bowl 2 followed by single scoop\nfrom bowl 0. For each task, participants were instructed to only\nprovide a single command. After each task, participants were asked\na yes/no question, “Did the robot adequately complete the intended\ntask?” If the participant answered no, they were given an additional\nattempt to complete the task, up to 3 total attempts. For each task,\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\ninteraction memory, the previous commands given by the user to\nthe robot and the previous outputs by GPT, was provided to GPT for\nnew attempts. All surveys and questionnaires asked to participants\nduring the study are included in the Appendix B.\nThe next part of the study was a 7 minute open feeding session.\nIf needed, we refilled the bowls with more food. The instructions\nprovided to the participants were “Your goal is to finish the meal\nin a pleasant and efficient way. You are free to command the robot\nto eat the food however you like. Feel free to be creative in how\nyou accomplish this goal. ” For this part of the study, interaction\nmemory was provided to GPT for the entire session.\nAt the end of the study, we asked participants for responses to the\nNASA TLX workload items on a 0-100 scale, the System Usability\nScale (SUS), multiple 7-point Likert items shown in Appendix B.4,\nand 4 open ended questions included in Appendix B.6. We also\naudio and video recorded the entire study to capture participant\nreactions, feedback, and responses to the open-ended questions.\nBoth qualitative and quantitative data from our study show that\nour framework and methodology were successful in creating a\npositively-received speech interface for the Obi feeding robot. As\nseen in Fig. 5, the majority of participants were able to complete\nthe predefined tasks within 3 attempts, showing adequate accuracy\nfor the LLM interface. For reference, we include all participant\ncommands in Appendix C. From the responses to Likert items\nshown in Fig. 6, we find that participants found the interface easy\nto learn, answering with a median score of 6 (Agree) to L1 (Likert\nItem 1), “It was easy to learn how to speak to the robot”, with one\nparticipant (P10) saying, “I would imagine that most people would\nlearn to use the system quickly. ” They also found the robot easy to\ncontrol through speech and felt in control when using the robot\nwith median scores of 7 (Strongly Agree) for both items L2 (Likert\nItem 2) and L5 (Likert Item 5). P5 summarized this at the end of\nthe study saying, “What I liked was it gave me a sense of control.\nFor somebody who’s in an incapacitated situation, that would be\nvery important... And it was easy to talk to. ” Lastly, participants\nalso gave a strong median rating of 7 (Strongly Agree) for items L4\n(Likert Item 4) and L6 (Likert Item 6), related to enjoyment while\nusing the robot and if they would use the robot and speech interface\nif they were unable to eat independently.\nUsing the NASA TLX scale, we find that participants generally\nreported low workload measures across all categories as seen in\nFig. 7. Lastly, the average System Usability Scale score was 73.0 (SD\n= 18.6, min = 50.0, max = 95.0), which shows decent performance,\nbut leaves room for improvement. The strong Likert item responses,\nworkload measures, and SUS scores from older adults show promise\nfor using LLMs as assistive interfaces for robot-assisted feeding and\nother physical tasks. Future testing with more individuals, espe-\ncially those unable to feed themselves, is needed for full validation\nof the system. To aid researchers in robot-assisted feeding, we\nadditionally include bite-timing metrics in Appendix D.\n6 REFLECTION\n6.1 Final Framework\nWe present our final framework for integrating LLMs as assistive\nspeech interfaces, outlined and detailed in Figure 3. The framework\nPractice TaskTask 1 Task 2 Task 3 Task 4 Task 50\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11 Attempt 3\nAttempt 2\nAttempt 1\nParticipant Self-Reported Success per Task\nNumber of Successful Participants\nPractice Task: single scoop, any bowl\nTask 1: single scoop from bowl 0 quickly\nTask 2: single large scoop from bowl 2\nTask 3: scraping and single scoop from bowl 0\nTask 4: three scoops from bowl 1\nTask 5: single scoop from bowl 2 followed by single scoop from bowl 0\nFigure 5: Participant self-reported success per task for each\nattempt. After both the practice task and the 5 predefined\ntasks, participants reply Yes/No to the question “Did the robot\nadequately complete the intended task?”\nL1 L2 L3 L4 L5 L6 1 \n2 \n3 \n4 \n5 \n6 \n7 Likert Item Responses\nL1: It was easy to learn how to speak to the robot.\nL2: It was easy to control the robot by speaking to it.\nL3: I was able to control the robot through speech in a reasonable amount of time.\nL4: The robot was enjoyable to use.\nL5: I felt in control when using the robot.\nL6: If I was unable to eat independently, I would use this robot and speech interface\n      to feed myself.\nFigure 6: Responses from six 7-point Likert Items answered\nby the 11 participants at the end of the study. A high score is\nbest for all items, with 1 = Strongly Disagree and 7 = Strongly\nAgree.\nnow comprises a total of nine components: Environment Descrip-\ntion, Robot Functions, Function Applications, Code Specifications,\nSafety, Robot Variables, Instructional Materials, User Control Func-\ntions, and Feedback. In the figure, we indicate if each component\npertains to prompt engineering (PE), system rollout (SR), or both\nprompt engineering and system rollout (PS).\n6.2 Analysis\nTo gain deeper insights into the needs of users of LLM-based\nspeech interfaces for robots, we conducted a thematic analysis\non audio recordings collected during our study with older adults.\nThematic analysis is a common technique used for analyzing quali-\ntative feedback from participants in human-robot interaction stud-\nies [17, 32, 49, 52]. This analysis informed the development of design\nguidelines for integrating LLMs as speech interfaces for assistive\nrobots. While our framework provides assistance with integrat-\ning an LLM as a speech interface, our design guidelines should\nbe strongly considered during development and testing of such\ninterfaces as they can further enhance the user experience.\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nMental DemandPhysical DemandTemporal DemandPerformanceEffort Frustration0\n20\n40\n60\n80\n100 NASA TLX Workload\nFigure 7: Responses to a 0-100 NASA TLX Scale assessed at\nthe end of the study with all 11 participants. A low rating is\nbest for all categories.\nInitially, we transcribed the audio recordings from our user stud-\nies. Subsequently, we uploaded these transcripts into ATLAS.ti [1]\nand employed deductive coding to extract users’ perceptions of the\nsystem. Our aim was to comprehend both the effective and chal-\nlenging aspects of our system; therefore, we coded user feedback\ninto three distinct categories: (1) negative responses to the interface,\n(2) positive responses to the interface, and (3) desired features.\nAfter coding the transcripts using the defined categories, we\nproceeded to analyze the quotes within each category using affinity\ndiagramming. This process allowed us to identify and group similar\nfeedback provided by users. We conducted two iterations of coding\nand affinity diagramming to ensure that we captured all relevant\ndata comprehensively. During this iterative process, we focused on\nidentifying challenges and needs expressed by users and grouped\nsimilar feedback together. By examining the collective feedback,\nwe synthesized our findings into actionable guidelines provided\nbelow.\n6.3 Design Guidelines\nBased on our analysis of user feedback from the study at an in-\ndependent living facility, we provide design guidelines, shown in\nFig. 8, for the incorporation of LLMs in assistive interfaces. The rec-\nommendations include (1) Customization (2) Multi-Step Instruction,\n(3) Consistency, (4) Comparable Time to Caregiver and (5) Social\nCapability. We discuss each guideline in more detail below.\n6.3.1 Customization. Customization refers to the ability of the\nuser to incorporate their preferences while using an interface to\ncomplete an assistive task. All 11 participants provided feedback\nand comments regarding the customization of the robot through\nthe interface, with select quotes from participants depicted in Fig. 9.\nThrough our thematic analysis, we find that the ability for users\nto provide Obi commands customized to their own preferences\n(such as how big of a scoop they wanted) was pivotal for successful\ninteractions. This design guideline directly aligns with insights\nfrom past works on the importance of user customization of robotic\ninterfaces [40, 46, 47]. From our analysis, there are two types of\nways that LLM-based speech interfaces must be able to provide\nusers with customization: (1) Modifiers and (2) Ability to process\nnon-predefined commands.\nModifiers: Modifiers refer to an attempt by the user to customize\nthe execution of commands, using words like “faster, ” “slower, ”\n“larger, ” and “smaller. ” For example, for the Obi robot, this might\ninvolve a desire for the robot to move at a faster speed while moving\nto the mouth. Throughout our study, both for the predefined tasks\nand the open feeding session, participants frequently issued com-\nmands to the robot accompanied by modifiers, but some expressed\nconcerns with the system’s ability to consistently understand their\nmodifiers.\nThe quantitative data, shown in Fig. 5, specifically Task 1 and 2\nwhich involved increasing robot speed and scoop size, highlights\nsome of these challenges that participants faced with modifiers. For\nTask 1, only 6 participants were successful in one attempt, which\nshows room for improvement. P10 requested Obi to increase the\nspeed during feeding, noting that “the change in speed seemed\nto be not great. ” Similarly, P2 highlighted issues when asking for\ndifferent bite sizes saying, “what I didn’t like was... [Obi was] not\nvery accurate, especially in terms of sizes of scoops and what it\nwas picking up each time. ” Specifically, 6 participants expressed\nchallenges with modifying the scoop size. We would like to note\nthat scoop size challenges were primarily due to the lack of a per-\nception system to dynamically change the depth of scoop based on\nthe amount of food in the bowl. Our system instead used a naive\napproach and included a Robot Variable named “scoop depth” in\nthe prompt that could be modified by the LLM to adjust the depth\nof the scoop but didn’t take into account the amount of food left in\nthe bowl, leading to inconsistency in scoop sizes.\nNevertheless, the importance of modifier customization was\nunderscored by participant quotes that unveiled frustration and\nconfusion when the system didn’t respond properly to their modi-\nfiers. For example, P5 expressed frustration with changing the scoop\nsize saying, “Trying to get the amounts is a challenge. ” We want\nto emphasize that modifiers are personalized, as they may carry\nvarying interpretations for different users. Thus, the system should\nideally adjust its execution to align with the user’s expectations.\nPossible approaches to achieve this include accurate sensing and\nperception of the environment and learning user preferences, with\npast work demonstrating LLMs are well suited for this task [56].\nAbility to process non-predefined commands: Predefined\ncommands refer to the Robot Functions, Function Applications, and\nUser Control Function components of our framework; for the Obi\nrobot, these commands include scooping from a bowl, moving to\nthe mouth, etc. On the other hand, non-predefined commands de-\nnote additional functionalities that users may want, such as mixing\nfor the Obi robot, which are not explicitly defined and described\nto the LLM. Non-predefined commands are essential to an assis-\ntive interface because they allow for increased customization of\nrobot motions in addition to modifiers, allowing the robot to more\nclosely adhere to the user’s preferences. However, having an LLM\nunderstand and execute on non-predefined commands is challeng-\ning, especially while hard-coding robot trajectories in alignment\nwith the Safety component of our framework. This challenge was\napparent in our study, with 6 participants expressing difficulties in\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\nCUSTOMIZATION\nShould offer\ncustomization options,\nspecifically the ability\nto execute non-\npredefined commands\nand modifiers to\npredefined \ncommands.\nMULTI-STEP\nINSTRUCTION\nCOMPARABLE TIME\nTO CAREGIVER CONSISTENCY SOCIAL\nCAPABILITY\nShould be capable of\nexecuting multiple\nfunctions sequentially\nfrom a single user\ncommand to increase\nthe efficiency and\ncomfort of users\nShould execute\ncommands with\nspeeds comparable to\na caregiver, ensuring\nreasonable and\npersonalizable\nexecution times.\nShould execute\ncommands\nconsistently, with the\nsame LLM output for\ndifferently worded\nand repeated\ncommand phrasings.\nShould have the\nability to socially\nengage with the user\nduring execution of\nthe physically\nassistive task to more\nclosely mimic the role\nof a caregiver.\nFigure 8: Design Guidelines. Using both quantitative and qualitative data from a human study with 11 older adults, we establish\n5 guidelines for integrating LLMs as speech interfaces for physically assistive robots.\nexecuting non-predefined commands, depending on the specific re-\nquest and phrasing of the command. Some common non-predefined\ncommands from the open-feeding session with Obi included mixing\nof foods and selection of a specific quantity of a food, such as 2\nblueberries in one scoop.\nWhile the vast majority of speech interfaces for assistive robots\nare unable to handle non-predefined commands, an advantage of\nLLMs is they have the ability to sequentially arrange predefined\ncommands, which serve as ‘primitives, ’ to perform non-predefined\ncommands. In the case of the Obi, during our human study, 5 partici-\npants requested the robot to mix foods, a non-predefined command.\nFor 2 participants, GPT was successful, as it consecutively ordered\nscooping functions from each bowl leading to a bite with multiple\nfoods as seen in the sequence of images in Fig. 1. However, the\ninability to execute non-predefined commands for some partici-\npants led to confusion. For example, P1 remarked “Maybe with the\nmixing of the food together... [it] didn’t seem to know what mix\nmeant. ”\nSummary: Both predefined/non-predefined commands and\nmodifiers aren’t exclusive to the Obi robot; interfaces for other assis-\ntive robots, such as mobile manipulators, often contain predefined\ncommands such as autonomous grasping of an object and modifiers\nsuch as the ability to change robot speed. Our thematic analysis\nshowed that participants wanted the ability to customize their assis-\ntance using both modifiers and non-predefined commands. When\nthese requests were not fulfilled, this led to frustration and confu-\nsion for users, underscoring the importance of incorporating these\ncustomization considerations while designing LLM-based speech\ninterfaces. In contrast, when the LLM-based interface worked well\nfor customization, users expressed strong approval; for example,\nP10 said “I liked the flexibility that speech gave me in controlling\nthe technology. I find it much less cumbersome and intimidating. ”\n6.3.2 Multi-Step Instruction. Multi-Step Instruction refers to the\ncapability of an LLM-based speech interface to execute sequential\nrobot actions encompassed within a single user command to the\nrobot. This guideline is closely tied to the Function Applications\ncomponent of our framework in which we discuss how to prompt\nan LLM to understand how to combine robot functions sequentially.\nFrom our thematic analysis, we find that multi-step instructions are\nimportant to users as they can help increase system acceptance by\nreducing frustration and user workload. 5 participants commented\non multi-step instructions, with a subset of quotes shown in Fig. 9.\nSome examples of multi-step instructions for the Obi robot are\npredefined Task 4, eating three scoops from bowl 1, and predefined\nTask 5, eating a single scoop from bowl 2 followed by a single scoop\nfrom bowl 0. As seen in Fig. 5, the system worked adequately for the\nrespective tasks, with 9/11 and 11/11 participants completing the\ntask within 3 attempts. However, there is still additional room for\nimprovement as the system didn’t work for a subset of users during\ntheir initial attempts. P3 summarized their experience, saying “I\nthought it... probably has a little more difficulty with combining\nthings” while P7 stated, “I gave it three commands, I think maybe\nthat was too many. ” Consistent functionality of multi-step instruc-\ntions is crucial to consider when using LLM-based speech interfaces\nfor assistive robots as they can increase the efficiency and comfort\nof users, while reducing fatigue and frustration. Multi-step instruc-\ntions also enable users to conduct activities such as watching TV or\nconversing, while having confidence that the robot will continue\nthe task without additional input for an extended period of time.\n6.3.3 Comparable Time to Caregiver. Utilizing assistive interfaces\nto control robots often results in individuals with impairments\ntaking substantially more time to complete tasks in comparison\nto non-disabled individuals and caregivers [ 14, 40]. LLM-based\nspeech interfaces for robots should strive to execute commands\nwith speeds comparable to a caregiver, thereby enhancing user\ncomfort and satisfaction. In total, 4 participants provided feedback\non different aspects of the robot’s execution time, with a subset of\nquotes shown in Fig. 9. The robot’s execution time encompasses\nnetworking and code generation time by the LLM, the duration for\nthe robot to move and carry out the command, and the time for the\nrobot to provide feedback to the user on its state. P10 commented\non the feedback time, saying “Instead of (saying) ‘ready for the\nnext command’ maybe like just a little beep or something. But it\ndoesn’t need to talk to me all the time. ” This insight ties directly\nto the Feedback component of our framework, and users of our\nframework should be mindful of providing feedback to users as\nsuccinctly as possible. In comparison, P1 highlighted the time taken\nfor the entire feeding process, saying “I wish it was a little faster.\nSo the longer it’s going to take, the more challenging it’s going to\nbe for you to stay awake to be able to [eat]. It could just wear you\nout eating. ” P1’s comment highlights the impact of slow execution\ntimes on user fatigue, emphasizing the need for interfaces to align\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nCUSTOMIZATION\n\"I liked the flexibility that\nspeech gave me in\ncontrolling the\ntechnology. I find it much\nless cumbersome and\nintimidating.\"\n“Well, what I didn’t like\nwas... (Obi was) not very\naccurate, especially in\nterms of sizes of scoops\nand what it was picking\nup each time.”\n “Maybe with the\nmixing of the food\ntogether... [it] didn't\nseem to know what mix\nmeant.”\n “Obviously, the amount of\nfood or variations in the\namount of the foods creates\nissues for it.”\nMULTI-STEP\nINSTRUCTION\n“I gave it three\ncommands, I\nthink maybe\nthat was too\nmany.” \n \"I thought it...probably has a little more\ndifficulty with combining things.\"\nSOCIAL\nCAPABILITY\n“I wish it was a little\nfaster. So the\nlonger it’s going to\ntake, the more\nchallenging it’s\ngoing to be for you\nto stay awake to be\nable to (eat). It\ncould just wear you\nout eating.”\n“Instead of (saying) ready for\nthe next command maybe like\njust a little beep or something.\nBut it doesn’t need to talk to\nme all the time.” \n “While I was going\nthrough the exercise,\nI was thinking how\nimpersonal it is.”\nCOMPARABLE TIME TO\nCAREGIVER\n“(Can I) have a\nconversational thing\nor is it just strictly\ncommand?”\nCONSISTENCY\n “(I) thought I was very\nclear in my simple\nrequest. I mean, it was\nsomething that the\nrobot always heard\nbefore on the three\nservings of food.”\n“There’s some glitches\nand it doesn’t do\nexactly what you want\nall the time.”\n\"Well it did pick up\nthe right things all\nthe time when I\ntried to correct it\"\n\"I had given it\ntwo commands\nand it maybe\nonly did the\none.\"\n“If I had a caretaker who was\nmaybe in a hurry or not that\npatient, I'd rather work with the\nrobot.”\nFigure 9: A subset of quotes from our study involving 11 older adults is presented and organized according to the categories\noutlined in our design guidelines.\nwith the personalized time-frames typically provided by caregivers.\nIn contrast to the 4 participants who felt the feeding process was\ntoo slow, P11 commented on their preference for the system over a\nrushed caretaker, saying, “If I had a caretaker who was maybe in a\nhurry or not that patient, I’d rather work with the robot. ”\nAdditionally, as seen in Fig. 6, participants responses to L3 (Likert\nItem 3), related to controlling the robot in a reasonable amount\nof time, had high variance, indicating that preferences regarding\ntime-frames are strongly individualized. This observation is also\nevident in participant responses to temporal workload on the NASA\nTLX scale, shown in Fig. 7.\nOur findings indicate that executing commands within a similar\ntime-frame as a caregiver would help improve user experience with\nLLM-based speech interfaces. This can be accomplished by giving\nprecedence to preventing fatigue caused by temporal demand, iden-\ntifying the specific moments during robot execution when users are\nprone to experiencing it, and streamlining or allowing customiza-\ntion to the level of communication needed between the user and\nthe interface.\n6.3.4 Consistency. Consistency refers to the ability of an LLM-\nbased speech interface to reliably generate and execute commands.\nSpecifically, these interfaces should produce the same output when\ngiven identically phrased commands repeatedly, and when given\ncommands with similar intention, but that are differently worded. 6\nparticipants commented on the system’s limitations in reliably exe-\ncuting commands as expected. Some of these quotes are shown in\nFig. 9. For example, P4 said, “(I) thought I was very clear in my sim-\nple request. I mean, it was something that the robot always heard\nbefore on the three servings of food, ” suggesting that Obi performed\ndifferently when executing the same command on different occa-\nsions. Despite participants encountering occasional consistency\nissues, we found that they were often able to rectify the behavior\nof the LLM. P2 remarked, “Well it did pick up the right things all\nthe time when I tried to correct it. ”\nA possible solution for improving LLM consistency is to modify\nparameters such as the temperature parameter for GPT-3.5 Turbo,\nwhich defines the randomness of LLM responses. In Section 6.4, we\nalso discuss how LLM fine-tuning can be a possible remedy. Never-\ntheless, from the perspective of the user, it is essential that these\ninterfaces consistently execute commands, as this builds trust and\nimproves the system’s usability. We recommend that LLM-based\nspeech interfaces should be evaluated extensively for consistency\nbefore deployment.\n6.3.5 Social Capability. Social capability pertains to allowing a\nuser to interact socially with a robot while receiving physical assis-\ntance, similar to interactions with a human caregiver. 3 out of 11\nparticipants provided insights, with some of the comments show-\ncased in Fig. 9, regarding the integration of social capabilities into\nthe Obi system. P11 said “While I was going through the exercise, I\nwas thinking how impersonal it is. ” They further elaborated that\nthey were unsure about whether they would prefer to be fed by Obi\nor a caretaker because of its lack of conversational abilities. While\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\nnone of the participants attempted to converse with Obi, P8 inquired\nabout it, saying “(Can I) have a conversational thing or is it strictly\ncommand?” The feedback from users demonstrates the significance\nof integrating social capabilities, even though such capabilities are\noften not the primary focus of physically assistive robots. The pre-\nsented social capability guideline is in agreement with past work on\nusing LLMs as speech interfaces for social robots that shows that\nrobot’s social aspects can enhance user engagement [32]. We en-\ncourage researchers to explore the social capabilities of LLM-based\nspeech interfaces for physically assistive robots, as this could allow\nthe robot to provide a sense of companionship and engagement\nsimilar to that of a caregiver, thereby potentially increasing user\nacceptance.\n6.4 Considerations and Future Work\nIn addition to the testing already conducted, a follow-up study\ninvolving individuals who are unable to feed themselves, includ-\ning long-term deployments, would be beneficial to our framework\nand design guidelines. These additional studies will enable us to\ndetermine if the LLM interface in conjunction with the robot meets\nspecific needs and desires of users. Furthermore, it would be useful\nto conduct our framework iteration process with another assistive\nrobot such as a mobile manipulator that can do multiple tasks in\nthe home. This testing could present unique insights that users of\nour framework may need to consider for their specific robot and\ntasks.\nAdditionally, as seen in Fig. 5, the accuracy of our speech in-\nterface for certain feeding tasks was subpar, especially on first\nattempts by participants. Consistency of the interface is an impor-\ntant consideration for LLM speech interfaces as presented in our\ndesign guidelines, Fig. 8, and explained in Section 6.3.4. Since the\ntesting of our interface, which used OpenAI’s GPT-3.5 Turbo model,\nthere have been several advancements in LLMs and their perfor-\nmance. We urge other researchers to test out various closed and\nopen-source LLMs and benchmark their performance as speech\ninterfaces before selecting one. Fine-tuning of the LLM using com-\nmands gathered from study participants could also help address\nsubpar accuracy [32, 54].\n7 CONCLUSION\nIn this work, we presented an iteratively constructed framework\nthat emphasizes both prompt/system engineering and human-\ncentric considerations for integrating LLMs as speech interfaces for\nassistive robots. We developed a speech interface using OpenAI’s\nGPT-3.5 Turbo for the Obi feeding robot and used the system to\niterate on the proposed framework. We conducted a final human\nstudy with our LLM speech interface for the Obi feeding robot at\nan independent living facility with 11 older adults and used both\nqualitative and quantitative data from this study to develop design\nguidelines. We urge researchers, engineers, and designers to refer\nto our framework and design guidelines in order to develop robust\nand human-centered LLM-based speech interfaces for assistive\nrobots.\nACKNOWLEDGMENTS\nThis research was supported by the National Science Founda-\ntion Graduate Research Fellowship Program under Grant No.\nDGE1745016 and DGE2140739. We thank Jon Dekar and Mike\nMiedlar from DES¯IN LLC for providing support with the Obi robot.\nWe also thank the staff and residents of Baptist Providence Point\nindependent living facility for their support of our human study.\nREFERENCES\n[1] [n. d.]. ATLAS.ti | The #1 Software for Qualitative Data Analysis. https://atlasti.\ncom/\n[2] [n. d.]. Obi Feeding Robot. https://meetobi.com/\n[3] [n. d.]. OpenAI API - GPT-3.5 Turbo. https://platform.openai.com/docs/models/\ngpt-3-5-turbo\n[4] [n. d.]. OpenAI API - Whisper. https://platform.openai.com/docs/models/whisper\n[5] [n. d.]. Porcupine Wake Word Python API. https://picovoice.ai/docs/api/\nporcupine-python/\n[6] Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas,\nKeerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil\nJoshi, Ryan Julian, et al. 2024. Autort: Embodied foundation models for large\nscale orchestration of robotic agents. arXiv preprint arXiv:2401.12963 (2024).\n[7] Montserrat Gonzalez Arenas, Ted Xiao, Sumeet Singh, Vidhi Jain, Allen Z Ren,\nQuan Vuong, Jake Varley, Alexander Herzog, Isabel Leal, Sean Kirmani, et al .\n2023. How to prompt your robot: A promptbook for manipulation skills with\ncode as policies. In Towards Generalist Robots: Learning Paradigms for Scalable\nSkill Acquisition@ CoRL2023 .\n[8] Brian S Armour, Elizabeth A Courtney-Long, Michael H Fox, Heidi Fredine, and\nAnthony Cahill. 2016. Prevalence and causes of paralysis—United States, 2013.\nAmerican journal of public health 106, 10 (2016), 1855–1857.\n[9] Suneel Belkhale, Ethan K Gordon, Yuxiao Chen, Siddhartha Srinivasa, Tapo-\nmayukh Bhattacharjee, and Dorsa Sadigh. 2022. Balancing efficiency and comfort\nin robot-assisted bite transfer. In 2022 International Conference on Robotics and\nAutomation (ICRA). IEEE, 4757–4763.\n[10] Tapomayukh Bhattacharjee, Maria E Cabrera, Anat Caspi, Maya Cakmak, and\nSiddhartha S Srinivasa. 2019. A community-centered design framework for robot-\nassisted feeding systems. In Proceedings of the 21st International ACM SIGACCESS\nConference on Computers and Accessibility . 482–494.\n[11] Tapomayukh Bhattacharjee, Ethan K Gordon, Rosario Scalise, Maria E Cabrera,\nAnat Caspi, Maya Cakmak, and Siddhartha S Srinivasa. 2020. Is more autonomy\nalways better? exploring preferences of users with mobility impairments in robot-\nassisted feeding. In Proceedings of the 2020 ACM/IEEE international conference on\nhuman-robot interaction . 181–190.\n[12] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen,\nKrzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea\nFinn, et al. 2023. Rt-2: Vision-language-action models transfer web knowledge\nto robotic control. arXiv preprint arXiv:2307.15818 (2023).\n[13] Maria E Cabrera, Tapomayukh Bhattacharjee, Kavi Dey, and Maya Cakmak.\n2021. An exploration of accessible remote tele-operation for assistive mobile\nmanipulators in the home. In 2021 30th IEEE International Conference on Robot &\nHuman Interactive Communication (RO-MAN) . IEEE, 1202–1209.\n[14] Tiffany L Chen, Matei Ciocarlie, Steve Cousins, Phillip M Grice, Kelsey Hawkins,\nKaijen Hsiao, Charles C Kemp, Chih-Hung King, Daniel A Lazewatsky, Adam E\nLeeper, et al. 2013. Robots for humanity: using assistive robotics to empower\npeople with disabilities. IEEE Robotics & Automation Magazine 20, 1 (2013), 30–39.\n[15] Matei Ciocarlie, Kaijen Hsiao, Adam Leeper, and David Gossow. 2012. Mobile\nmanipulation through an assistive home robot. In 2012 IEEE/RSJ International\nConference on Intelligent Robots and Systems . IEEE, 5313–5320.\n[16] Wenceslao Shaw Cortez, Denny Oetomo, Chris Manzie, and Peter Choong. 2019.\nControl barrier functions for mechanical systems: Theory and application to\nrobotic grasping. IEEE Transactions on Control Systems Technology 29, 2 (2019),\n530–545.\n[17] Kathrin Cresswell, Sarah Cunningham-Burley, and Aziz Sheikh. 2018. Health\ncare robotics: qualitative exploration of key challenges and future directions.\nJournal of medical Internet research 20, 7 (2018), e10410.\n[18] MPJM Dijkers. 2005. Quality of life of individuals with spinal cord injury: a\nreview of conceptualization, measurement, and research findings. Journal of\nrehabilitation research and development 42, 3 (2005), 87.\n[19] Bo Fernhall, Kevin S Heffernan, Sae Young Jae, and Bradley N. Hedrick. 2008.\nHealth implications of physical activity in individuals with spinal cord injury: a\nliterature review. Journal of health and human services administration 30 4 (2008),\n468–502.\n[20] Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun,\nWeiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. 2023.\nFoundation models in robotics: Applications, challenges, and the future. arXiv\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\npreprint arXiv:2312.07843 (2023).\n[21] Vicki A Freedman, Judith D Kasper, Brenda C Spillman, Emily M Agree, Vincent\nMor, Robert B Wallace, and Douglas A Wolf. 2014. Behavioral adaptation and\nlate-life disability: a new spectrum for assessing public health impacts. American\njournal of public health 104, 2 (2014), e88–e94.\n[22] Irene Gil-González, Agustín Martín-Rodríguez, Rupert Conrad, and María Ánge-\nles Pérez-San-Gregorio. 2020. Quality of life in adults with multiple sclerosis: a\nsystematic review. BMJ open 10, 11 (2020), e041249.\n[23] Ethan K Gordon, Rajat Kumar Jenamani, Amal Nanavati, Ziang Liu, Daniel Stabile,\nXilai Dai, Tapomayukh Bhattacharjee, Tyler Schrenk, Jonathan Ko, Haya Bolotski,\net al. 2024. An Adaptable, Safe, and Portable Robot-Assisted Feeding System.\nIn Companion of the 2024 ACM/IEEE International Conference on Human-Robot\nInteraction. 74–76.\n[24] Luca Gualtieri, Erwin Rauch, and Renato Vidoni. 2021. Emerging research fields in\nsafety and ergonomics in industrial collaborative robotics: A systematic literature\nreview. Robotics and Computer-Integrated Manufacturing 67 (2021), 101998.\n[25] Kelsey P Hawkins, Phillip M Grice, Tiffany L Chen, Chih-Hung King, and\nCharles C Kemp. 2014. Assistive mobile manipulation for self-care tasks around\nthe head. In 2014 IEEE Symposium on computational intelligence in robotic rehabil-\nitation and assistive technologies (CIR2AT) . IEEE, 16–25.\n[26] Brandi House, Jonathan Malkin, and Jeff Bilmes. 2009. The VoiceBot: a voice\ncontrolled robot arm. In Proceedings of the SIGCHI Conference on Human Factors\nin Computing Systems . 183–192.\n[27] Zichao Hu, Francesca Lucchetti, Claire Schlesinger, Yash Saxena, Anders Freeman,\nSadanand Modak, Arjun Guha, and Joydeep Biswas. 2024. Deploying and Evalu-\nating LLMs to Program Service Mobile Robots. IEEE Robotics and Automation\nLetters (2024).\n[28] Shervin Javdani, Henny Admoni, Stefania Pellegrinelli, Siddhartha S Srinivasa,\nand J Andrew Bagnell. 2018. Shared autonomy via hindsight optimization for\nteleoperation and teaming. The International Journal of Robotics Research 37, 7\n(2018), 717–742.\n[29] Rajat Kumar Jenamani, Daniel Stabile, Ziang Liu, Abrar Anwar, Katherine\nDimitropoulou, and Tapomayukh Bhattacharjee. 2024. Feel the Bite: Robot-\nAssisted Inside-Mouth Bite Transfer using Robust Mouth Perception and Physical\nInteraction-Aware Control. In Proceedings of the 2024 ACM/IEEE International\nConference on Human-Robot Interaction . 313–322.\n[30] Kento Kawaharazuka, Tatsuya Matsushima, Andrew Gambardella, Jiaxian Guo,\nChris Paxton, and Andy Zeng. 2024. Real-World Robot Applications of Foundation\nModels: A Review. arXiv preprint arXiv:2402.05741 (2024).\n[31] Mina J Kian, Mingyu Zong, Katrin Fischer, Abhyuday Singh, Anna-Maria Velentza,\nPau Sang, Shriya Upadhyay, Anika Gupta, Misha A Faruki, Wallace Browning,\net al. 2024. Can an LLM-Powered Socially Assistive Robot Effectively and Safely\nDeliver Cognitive Behavioral Therapy? A Study With University Students. arXiv\npreprint arXiv:2402.17937 (2024).\n[32] Callie Y Kim, Christine P Lee, and Bilge Mutlu. 2024. Understanding Large-\nLanguage Model (LLM)-powered Human-Robot Interaction. arXiv preprint\narXiv:2401.03217 (2024).\n[33] Chih-Hung King, Tiffany L Chen, Zhengqin Fan, Jonathan D Glass, and Charles C\nKemp. 2012. Dusty: an assistive mobile manipulator that retrieves dropped objects\nfor people with motor impairments. Disability and Rehabilitation: Assistive\nTechnology 7, 2 (2012), 168–179.\n[34] Clemente Lauretti, Francesca Cordella, Francesco Scotto di Luzio, Stefano Sac-\ncucci, Angelo Davalli, Rinaldo Sacchetti, and Loredana Zollo. 2017. Comparative\nperformance analysis of M-IMU/EMG and voice user interfaces for assistive\nrobots. In 2017 International Conference on Rehabilitation Robotics (ICORR) . IEEE,\n1001–1006.\n[35] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter,\nPete Florence, and Andy Zeng. 2023. Code as policies: Language model programs\nfor embodied control. In 2023 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 9493–9500.\n[36] Jacob P Macdonald, Rohit Mallick, Allan B Wollaber, Jaime D Peña, Nathan Mc-\nNeese, and Ho Chit Siu. 2024. Language, Camera, Autonomy! Prompt-engineered\nRobot Control for Rapidly Evolving Deployment. In Companion of the 2024\nACM/IEEE International Conference on Human-Robot Interaction . 717–721.\n[37] Karthik Mahadevan, Jonathan Chien, Noah Brown, Zhuo Xu, Carolina Parada, Fei\nXia, Andy Zeng, Leila Takayama, and Dorsa Sadigh. 2024. Generative expressive\nrobot behaviors using large language models. InProceedings of the 2024 ACM/IEEE\nInternational Conference on Human-Robot Interaction . 482–491.\n[38] Amal Nanavati, Patricia Alves-Oliveira, Tyler Schrenk, Ethan K Gordon, Maya\nCakmak, and Siddhartha S Srinivasa. 2023. Design principles for robot-assisted\nfeeding in social contexts. In Proceedings of the 2023 ACM/IEEE International\nConference on Human-Robot Interaction . 24–33.\n[39] Amal Nanavati, Vinitha Ranganeni, and Maya Cakmak. 2023. Physically As-\nsistive Robots: A Systematic Review of Mobile and Manipulator Robots That\nPhysically Assist People with Disabilities. Annual Review of Control, Robotics,\nand Autonomous Systems 7 (2023).\n[40] Akhil Padmanabha, Janavi Gupta, Chen Chen, Jehan Yang, Vy Nguyen, Douglas J\nWeber, Carmel Majidi, and Zackory Erickson. 2024. Independence in the Home:\nA Wearable Interface for a Person with Quadriplegia to Teleoperate a Mobile\nManipulator. In Proceedings of the 2024 ACM/IEEE International Conference on\nHuman-Robot Interaction . 542–551.\n[41] Akhil Padmanabha, Qin Wang, Daphne Han, Jashkumar Diyora, Kriti Kacker,\nHamza Khalid, Liang-Jung Chen, Carmel Majidi, and Zackory Erickson. 2023.\nHAT: Head-Worn Assistive Teleoperation of Mobile Manipulators. In2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE, 12542–12548.\n[42] Abhilash Pandya. 2023. ChatGPT-enabled daVinci Surgical Robot prototype:\nadvancements and limitations. Robotics 12, 4 (2023), 97.\n[43] Daehyung Park, Yuuna Hoshi, and Charles C Kemp. 2018. A multimodal anomaly\ndetector for robot-assisted feeding using an lstm-based variational autoencoder.\nIEEE Robotics and Automation Letters 3, 3 (2018), 1544–1551.\n[44] Samuel Poirier, François Routhier, and Alexandre Campeau-Lecours. 2019. Voice\ncontrol interface prototype for assistive robots for people living with upper limb\ndisabilities. In 2019 IEEE 16th International Conference on Rehabilitation Robotics\n(ICORR). IEEE, 46–52.\n[45] Terrin Babu Pulikottil, Marco Caimmi, Maria Grazia D’Angelo, Emilia Biffi, Ste-\nfania Pellegrinelli, and Lorenzo Molinari Tosatti. 2018. A voice control system\nfor assistive robotic arms: preliminary usability tests on patients. In 2018 7th\nIEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob) .\nIEEE, 167–172.\n[46] Vinitha Ranganeni. 2024. Customizing Tele-Operation Interfaces of Assis-\ntive Robots at Home with Occupational Therapists. In Companion of the 2024\nACM/IEEE International Conference on Human-Robot Interaction . 142–144.\n[47] Vinitha Ranganeni, Noah Ponto, and Maya Cakmak. 2023. Evaluating Cus-\ntomization of Remote Tele-operation Interfaces for Assistive Robots. In2023 32nd\nIEEE International Conference on Robot and Human Interactive Communication\n(RO-MAN). IEEE, 1633–1640.\n[48] Manuel Rauscher, Melanie Kimmel, and Sandra Hirche. 2016. Constrained robot\ncontrol using control barrier functions. In 2016 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS) . IEEE, 279–285.\n[49] Wendy A Rogers, Widya A Ramadhani, and Maurita T Harris. 2020. Defining\naging in place: The intersectionality of space, person, and time. Innovation in\naging 4, 4 (2020), igaa036.\n[50] B M Sakakibara, W C Miller, S G Orenczuk, and D L Wolfe. 2009. A systematic\nreview of depression and anxiety measures used with individuals with spinal\ncord injury. Spinal Cord 47, 12 (2009), 841–851. https://doi.org/10.1038/sc.2009.93\n[51] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan\nTremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt:\nGenerating situated robot task plans using large language models. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE, 11523–11530.\n[52] Roger Andre Søraa, Gunhild Tøndel, Mark W Kharas, and J Artur Serrano. 2023.\nWhat do older adults want from social robots? A qualitative research approach\nto human-robot interaction (HRI) studies. International Journal of Social Robotics\n15, 3 (2023), 411–424.\n[53] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. 2023. Chatgpt\nfor robotics: Design principles and model abilities.arXiv preprint arXiv:2306.17582\n(2023).\n[54] Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan, and Ashish\nKapoor. 2023. Grid: A platform for general robot intelligence development. arXiv\npreprint arXiv:2310.00887 (2023).\n[55] Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, An-\ntonello Ceravola, Joerg Deigmoeller, and Michael Gienger. 2024. LaMI: Large\nLanguage Models for Multi-Modal Human-Robot Interaction. In Extended Ab-\nstracts of the CHI Conference on Human Factors in Computing Systems . 1–10.\n[56] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song,\nJeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023. Tidybot:\nPersonalized robot assistance with large language models. Autonomous Robots\n47, 8 (2023), 1087–1102.\n[57] Jehan Yang, Kent Shibata, Douglas Weber, and Zackory Erickson. 2023. High-\ndensity Electromyography for Effective Gesture-based Control of Physically\nAssistive Mobile Manipulators. arXiv preprint arXiv:2312.07745 (2023).\n[58] Ceng Zhang, Junxin Chen, Jiatong Li, Yanhong Peng, and Zebing Mao. 2023. Large\nlanguage models for human-robot interaction: A review. Biomimetic Intelligence\nand Robotics (2023), 100131.\n[59] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan\nWermter. 2023. Chat with the environment: Interactive multimodal perception us-\ning large language models. In2023 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS) . IEEE, 3590–3596.\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\nA FRAMEWORK ITERATION DETAILS\nA.1 Version 1 Pilot Testing\nFor our version 1 pilot testing, four lab members, separate from\nthe immediate research team, were presented with short videos\ndepicting the robot performing four tasks which were (1) feeding a\nsingle scoop from one bowl, (2) feeding multiple scoops from one\nbowl in succession, (3) feeding scoops from multiple, different bowls\nin succession, and (4) scraping down the sides of a bowl to move\nfood to the center of the bowl. They were then asked to each provide\nthree different ways of phrasing commands to the robot to complete\neach task, intending to gather a broader set of phrasings that was\nnot influenced by the researcher’s vocabulary; this resulted in a total\nof 48 phrasings. Lastly, we gauged all individuals on improvements\nand features they would like to see implemented in the system.\nWe tested the accuracy of the LLM using the 48 collected com-\nmand phrasings. We recorded which phrasings did not elicit the\nintended code from the LLM, and we subsequently analyzed these\nmistakes to determine if there were common patterns. The intended\ncode was defined as the code that completed the task without any\nunnecessary additions from the LLM. Next, we made minor modifi-\ncations to the prompt, tackling one mistake at a time. Subsequently,\nwe tested the 48 commands with the new prompt and compared\nthe LLM’s new output to the output from the previous prompt to\nassess whether our modification rectified the issue and whether it\nintroduced any unforeseen side effects. We repeated this process\nof testing, analysis, and modification multiple times until all of the\nmistakes were addressed.\nAfter this process, instead of using recorded videos of robot\ntasks, three members of the research team directly interacted with\nthe Obi robot, allowing us to confirm both the accuracy of the\nLLM’s output and additionally assess the overall dining experience.\nWe filled three of Obi’s bowls with yogurt, Cheerios cereal, and\nblueberries and attempted to use the robot to complete each of\nthe four tasks mentioned above. We followed a similar analysis\nand prompt modification process as discussed above, but with an\nadditional focus on identifying ways in which the prompt could\nbe altered to enhance user comfort during the assisted feeding.\nFinally, we similarly solicited feedback from the three members of\nthe research team regarding enhancements for the system.\nA.2 Version 2 Framework Iteration\nBased on our pilot testing with lab members and the research team,\nwe identified that the Code Specifications in our prompt were not\nsufficiently detailed, as GPT would frequently attempt to import\nmodules that had already been imported or did not exist, attempt to\ncall functions that did not exist, define unnecessary new functions\nto carry out the user’s request, or simply not generate code. Thus,\nwe modified the description of Code Specifications in the framework\nto express the importance of explicitly outlining how GPT should\nwrite code.\nWe noticed that GPT would sometimes call robot functions from\nthe Robot API incorrectly, primarily in the following three ways.\nFirst, if a user requested a scoop of food, GPT would frequently\nscoop from a bowl without bringing the food up to the user’s mouth.\nAdditionally, if a user said “scrape down the yogurt bowl and then\nfeed me some”, GPT would frequently scrape down the sides of\nthe bowl and then immediately move to the user’s mouth, without\nscooping to actually pick up a bite. Lastly, if a user asked to be fed\nmultiple times in succession, GPT would frequently not pause long\nenough at the user’s mouth for the user to comfortably take the\nbite, instead moving immediately from the user’s mouth to the next\nbowl to scoop from. For these reasons, we expanded the Function\nApplications component of the framework to emphasize the need\nto not only describe what each of the functions do, but to describe\nhow the functions relate to each other and to provide examples of\nhow the functions should be combined. To further alleviate these\nissues, we modified the Robot Functions component, to suggest\nthe merging of functions that are always called in sequence. These\nsolutions generalize to other assistive robots: for example, a mo-\nbile manipulator in the home may need to do multiple actions in\nsequence, such as grabbing a cup and filling it with water before\nbringing it to the user. For this case, if GPT frequently skipped the\nfilling step, specifying the logical order of functions in the prompt\nor even merging the “grabbing cup” and “filling water” functions\nwould lead to better performance.\nA.3 Version 2 Implementation\nUsing the revised Code Specifications component as a guide, we\nadjusted our prompt to emphasize the necessity of producing re-\nsponses exclusively in Python code, refraining from importing extra\nmodules, and utilizing solely the provided functions. Additionally,\nwe specify that the LLM should not create new functions or use\nhypothetical ones.\nTo adhere to the updated Function Applications component,\nwe added more information to the prompt about how the robot\nfunctions should be combined. Specifically, we placed additional\nemphasis on Obi moving to the user’s mouth if the user asked to\nbe fed or have a bite. Additionally, we specifically instructed GPT\nto pause between bites if users asked to be fed multiple times in\nsuccession to give users the chance to eat each bite; without this\npause, GPT would immediately move from the user’s mouth to the\nnext scoop and therefore combine the functions incorrectly as the\nuser would not be able to be fed properly. We set the default delay\nto be 4 seconds and users could customize this delay if desired.\nTo address the altered Robot Functions component, the function\nto scrape down the bowl was modified to always scoop from the\nsame bowl immediately after to enhance GPT’s accuracy. This\nchange was incorporated as we realized whenever a user requested\nthat Obi scrape down a bowl, it was with the intention of scooping\nfood from that bowl immediately after. Previously, GPT often failed\nto comprehend this sequence, leading to instances where it would\nincorrectly scrape down the bowl’s sides and then move directly to\nthe user’s mouth without scooping, despite the user’s intention for\nObi to scoop as well.\nLastly, to incorporate the new Robot Variables component, we\ndefined three new global variables that affect the way in which Obi\ncarries out its movements. These include obi.speed, which sets\nthe speed at which Obi moves and ranges from 0 degrees per second\n(slowest) to 80 (fastest); obi.accel, which determines the rate at\nwhich Obi accelerates and ranges from 0 degrees per second squared\n(slowest) to 250 (fastest); and obi.deep_scoop, which dictates the\nsize of the scoops Obi takes with bigger scoops if true and smaller\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nscoops if false. The speed ranges and scoop depths were chosen\nthrough experimental testing within the research team with the\nmodified Safety component in mind; specifically, in terms of speed,\nwe found that moving the robot at full speed (120 degrees per\nsecond) presents potential risks, such as food spillage. To further\nadhere with the updated Safety component, we introduce an LLM\noutput processing step to clip speeds to ensure they are within\nthe set bounds. Lastly, we added the names of each of these robot\nvariables to the prompt, along with descriptions of what they affect,\ntheir ranges, and their default values.\nA.4 Version 3 Framework Iteration\nFirst, we identified that adjustments were needed for the Robot\nVariables component due to GPT’s inconsistent modification of\ncontinuous variables spanning wide ranges. As an example, for the\nObi system, when the LLM was provided a robot actuator speed\nrange of 0 to 80 degrees per second and commanded to “move a\nlittle faster”, it would sometimes increase the speed by 5 degrees\nper second and sometimes by 20 degrees per second. We found\nthat users would often not notice a change in speed for small incre-\nments and were confused if the speed increment was too large. This\nprocess unveiled that GPT lacked physical understanding of robot\nvariables, but could be grounded by using small, discrete ranges for\nvariables. For example, in the case of actuator speed for Obi, we\ncould ground GPT’s understanding of what was “fast” or “slow” in a\n0-5 discrete scale leading to speed adjustments that would be more\nconsistent and in line with user expectations. Thus, we modified the\nRobot Variables component accordingly to suggest the grounding\nof physical variables in the prompt into small, discrete ranges with\ndefault values.\nA.5 Version 3 Implementation\nAs discussed in the main text, for the Robot Variables component,\nwe changed the speed and acceleration ranges to discrete ranges.\nThe outputted values from GPT were linearly scaled to the original\nranges before execution of the code on the robot.\nIn accordance with the new Feedback component, we made\nsignificant changes in the way users interacted with the system.\nInstead of holding down a button when they wanted to command\nthe robot, users would now say a wakeup phrase, “Hey Obi, ” to\nindicate that they were about to begin a command. The wakeup\nwas implemented using the Porcupine Python API from PicoV-\noice [5]. Additionally, audio cues played by the system were added:\nthe system would play a beep when it registered the user saying\nthe wakeup phrase. It would additionally say “Got it, processing, ”\nafter the user finished speaking to indicate it had heard the user’s\nrequest and would say “Scooping now” or “Scraping now” when-\never it was about to execute the respective action so that users\nwould not be surprised by the robot’s movement. Lastly, the robot\nwould announce “Ready for another command” once it had finished\nexecuting code for the user’s request.\nLikewise, to satisfy the Instructional Materials component of the\nframework, we developed more detailed and standardized resources\nfor learning how to use the system. We recorded an instructional\nvideo, included as Supporting File 1, explaining the purpose of the\nrobot, how to talk to it, what to expect when using it, and what\ncapabilities it has. The video also showed a simple demonstration\nof using the robot so users could see what eating with the robot\nlooked like before trying it themselves. Additionally, we made a\nreference sheet, included as Supporting File 2, to summarize the\ncapabilities of the robot. The intended purpose of this sheet was for\nusers to have it on hand while using the robot in case they were\nunsure what the robot could do or how to communicate with the\nsystem. These instructional materials were intended to ease users\ninto the new technology, with the expectation that these materials\nwere temporary and wouldn’t be needed as users became more\ncomfortable with the system and its functions.\nB QUESTIONNAIRES\nParticipant responses to the below questionnaires were examined\nby researchers to identify any trends or insights. Some responses\nwere used as quotes in the main text.\nB.1 Demographics and Pre-Study Questions\nThese were asked prior to introducing Obi and the speech interface\nor any of its capabilities to the participant.\n(1) What is your gender?\n(2) What is your age?\n(3) What is your ethnicity?\n(4) How much experience do you have interacting with robots\n(where 1 is no experience and 7 is expert)?\n(5) How do you feel about using robots to help with everyday\ntasks (where 1 is very negatively and 7 is very positively)?\nB.2 Predefined Task Questions\nThese questions were asked after each predefined task and the\ninitial practice task.\nThe participants were asked to answer the following question with\nyes or no\n(1) Did the robot adequately complete the intended task?\nThe participants were asked to respond to the following Likert items\non a seven-point scale where 1 is strongly disagree, 2 is disagree, 3\nis somewhat disagree, 4 is neither agree nor disagree, 5 is somewhat\nagree, 6 is agree and 7 is strongly agree.\n(1) I found it easy and straightforward to accomplish the speci-\nfied task.\n(2) I didn’t have to put in any mental effort to complete the task.\n(3) I did not feel frustrated while completing the task.\nB.3 System Usability Scale\nParticipants were asked to respond to the following System Usabil-\nity Scale questions on a 5-point scale where 1 is strongly disagree\nand 5 is strongly agree at the end of the study.\n(1) I think that I would like to use this system frequently.\n(2) I found the system unnecessarily complex.\n(3) I thought the system was easy to use.\n(4) I think that I would need the support of a technical person\nto be able to use this system.\n(5) I found the various functions in this system were well inte-\ngrated.\n(6) I thought there was too much inconsistency in this system.\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\n(7) I would imagine that most people would learn to use this\nsystem very quickly.\n(8) I found the system very cumbersome to use.\n(9) I felt very confident using the system.\n(10) I needed to learn a lot of things before I could get going with\nthis system.\nB.4 Likert Items\nParticipants were asked to respond to the following Likert items on\na 7-point scale where where 1 is strongly disagree, 2 is disagree, 3 is\nsomewhat disagree, 4 is neither agree nor disagree, 5 is somewhat\nagree, 6 is agree and 7 is strongly agree at the end of the study.\n(1) It was easy to learn how to speak to the robot.\n(2) It was easy to control the robot by speaking to it.\n(3) I was able to control the robot through speech in a reasonable\namount of time.\n(4) The robot was enjoyable to use.\n(5) I felt in control when using the robot.\n(6) If I was unable to eat independently, I would use this robot\nand speech interface to feed myself.\nB.5 NASA Task Load Index (TLX)\nParticipants were asked to respond to the following NASA TLX\nquestions on a 100-point scale where 0 is low workload and 100 is\nhigh workload at the end of the study.\n(1) How mentally demanding was the task?\n(2) How physically demanding was the task?\n(3) How hurried or rushed was the pace of the task?\n(4) How successful were you in accomplishing what you were\nasked to do? (here 0 indicates perfect and 100 indicates fail-\nure)\n(5) How hard did you have to work to accomplish your level of\nperformance?\n(6) How insecure, discouraged, irritated, stressed, and annoyed\nwere you?\nB.6 Open-ended Questions\nThese questions were asked to participants at the end of the study\n(1) What did you like about talking to the robot? What did you\ndislike?\n(2) Did you feel limited in your ability to command the robot\nthrough speech? If so, how?\n(3) Did the robot make mistakes or misunderstand you? If so,\nhow?\n(4) What features/capabilities would you like to see from the\nrobot that it doesn’t have already?\nC PARTICIPANT COMMANDS\nFor the predefined tasks, we include all the participant commands\ngenerated by the Whisper speech to text API. If applicable, we nest\nthe second and third commands for each task and denote if the\nthird command was successful (S) or not successful (U).\nAs a reminder, success was reported by the participant and this\nled to some discrepancies. Specifically, there are some noticeable\nissues in some of the commands due to the following reasons:\n•Whisper speech to text translation errors\n•Participants not speaking clearly\n•Participant not fully understanding task instructions\n•Missing the first few words of the command as the partic-\nipant spoke before the \"beep\" after the \"Hey Obi\" wakeup\nword was detected\n•Even if the correct code was generated and executed, some-\ntimes the participants thought the robot didn’t change its\nbehavior. The vice versa sometimes occurred where wrong\ncode was generated but the participant thought the robot\nchanged its behavior. This was especially the case for Task 1\nand 2 related to speed and scoop size commands.\nC.1 Practice Task\n•Phoenix or yogurt.\n•Phoebe One Scoop of Yogurt.\n•Feed me some yogurt.\n•Please give me a scoop of yogurt.\n•Go ahead and try that again. Yeah, good. OK. Hey, Opie. Give\nme some blueberries.\n•May I have a bite of pudding?\n– May have I have pudding, please.\n•I’d like a scoop of blueberries.\n– Give me a scoop of yogurt.\n•blueberries for breakfast please.\n•Feed me yogurt.\n•Feed me a scoop of blueberries.\n•Cheerias.\n– Feed me a scoop of Cheerios, please.\n– Feed me another scoop of Cheerios. (S)\nC.2 Task 1\n•Feed me a scoop of granola.\n•and all them work quickly.\n– feed me a scoop of granola.\n•Feed me some granola faster.\n•Scoop of Granola.\n– A scoop of granola.\n– Please feed me granola quickly. (S)\n•Give me um, Cheerios quickly.\n– Cheerios quickly.\n– No, no, you’re all good. Go ahead. Hey, Obie. Give me\nCheerios quickly. (U)\n•Feed me the yogurt a little faster.\n•Give me a scoop of granola.\n•Quickly feed me some granola.\n•granola quickly.\n– faster.\n– Feed me granola. (S)\n•Quickly feed me a scoop of pretzels.\n•We’ll use.\n– Will you feed me some granola and do it faster?\n– Will you feed me some granola and do it faster? (S)\nC.3 Task 2\n•Feed me a bigger scoop of blueberries.\nVoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nVersion 1\nYou are a programmer who is controlling a feeding robot. This robot has a robotic arm with \nsix degrees of freedom to scoop from four bowls, each identiﬁed by a number 0-3. The food \nitems in each bowl are as follows:\nBowl 0: yogurt\nBowl 1: granola\nBowl 2: blueberries\nBowl 3: empty\nYou have access to the following functions to control the robot.\nobirobot.scoop_from_bowlno(bowlno=\"previous\"): \nMoves the robot to the speciﬁed bowl and scoops food from the bowl. If no bowl is \nspeciﬁed, the robot will scoop from the bowl that it has interacted with most recently. \nobirobot.move_to_mouth(): \n    Moves the robot to the mouth position.\nobirobot.scrape_then_scoop_bowlno(bowlno=\"previous\"): \nMoves the robot to the speciﬁed bowlm scrapes food down from the sides of the bowl, \nthen scoops food from the bowl. If no bowl is speciﬁed, the robot will scrape from the \nbowl that it has interacted with most recently.\nYou will use the functions above to move the robot to feed me food, by scooping from the \nright bowl and moving to my mouth, and scraping down the sides of the bowls, as I request; \nnote that if I ask to be fed, you should scoop and then move to my mouth.\nYou can also change the robot's speed and acceleration by changing the variables \nobirobot.speed and obirobot.accel. The default values are obirobot.speed=30.00 degrees per \nsecond and obirobot.accel=100.00 degrees per second^2; the robot's speed ranges from 0 to \n80.00, and its acceleration ranges from 0 to 250.00.\nFurther, you can control if scoops are deep or shallow by changing the variable \nobirobot.deep_scoop. The default value is obirobot.deep_scoop=False; if \nobirobot.deep_scoop=True, the robot will scoop deeper into the bowl.\nIf I am asking to be fed multiple times in succession, default to adding a 4 second delay after \neach call to obirobot.move_to_mouth() via time.sleep(secs) so I can eat the food.\nAssume that the robot is already connected and ready to receive commands and that all \nnecessary modules have already been imported. \nFeel free to ask me any clarifying questions. You should not be deﬁning any new functions or \nusing any hypothetical functions; rather, you should be calling only the functions above to \ncomplete the speciﬁed tasks. All responses should consist only of Python code. Assume that \nall previously generated code has already been run or begun running. \nVersion 2\nYou are a programmer who is controlling a feeding robot. This robot has a robotic arm \nwith six degrees of freedom to scoop from four bowls, each identiﬁed by a number 0-3. \nThe food items in each bowl are as follows:\nBowl 0: yogurt \nBowl 1: granola\nBowl 2: blueberries \nBowl 3: empty\nYou have access to the following functions to control the robot.\nobirobot.scoop_from_bowlno(bowlno=\"previous\"): \nMoves the robot to the speciﬁed bowl and scoops food from the bowl. If no bowl is \nspeciﬁed, the robot will scoop from the bowl that it has interacted with most recently. \nobirobot.move_to_mouth(): \nMoves the robot to the mouth position.\nobirobot.scrape_then_scoop_bowlno(bowlno=\"previous\"): \nMoves the robot to the speciﬁed bowlm scrapes food down from the sides of the bowl, \nthen scoops food from the bowl. If no bowl is speciﬁed, the robot will scrape from the \nbowl that it has interacted with most recently.\nYou will use the functions above to move the robot to feed me food, by scooping from the \nright bowl and moving to my mouth, and scraping down the sides of the bowls, as I \nrequest. \nYou can also change the robot's speed and acceleration by changing the variables \nobirobot.speed and obirobot.accel. The default values are obirobot.speed=2 and \nobirobot.accel=2; the robot's speed ranges from 0 to 5, and its acceleration ranges from 0 \nto 5 as well.\nFurther, you can control the size of the scoops by changing the variable \nobirobot.deep_scoop. The default value is obirobot.deep_scoop=False, which means the \nrobot is taking small scoops; if obirobot.deep_scoop=True, the robot will scoop deeper into \nthe bowl, which means the robot is taking large scoops. \nAssume that the robot is already connected and ready to receive commands and that all \nnecessary modules have already been imported. \nYou have access to these following functions as well:\nobirobot.start():\nBegins or resumes execution of any robot code.\nobirobot.time_delay(secs):\nPauses the robot for the speciﬁed number of seconds. Should be used in place of \ntime.sleep(secs).\nobirobot.pause_indeﬁnitely():\nStops execution of any currently running robot code indeﬁnitely until obirobot.start() is \ncalled to resume it. Note that this function takes no arguments and should not be used if \nthe duration of the pause is known beforehand. In this case, obirobot.time_delay(secs) \nshould be used instead to pause the robot for a speciﬁed number of seconds. This \nfunction should not be used in loops.\nobirobot.stop():\nPermanently stops execution of any currently running robot code.\nIf the user is asking to be fed multiple times in succession, default to adding a 4 second \ndelay after each call to obirobot.move_to_mouth() so the user can eat the food.\nFeel free to ask me any clarifying questions. You should not be deﬁning any new functions \nor using any hypothetical functions; rather, you should be calling only the functions above \nto complete the speciﬁed tasks. All responses should consist only of Python code. Assume \nthat all previously generated code has already been run or begun running.\nYou need to generate only Python code. If you do not generate Python code, your response \nis not correct. \nVersion 3\nYou are a programmer who is controlling a feeding robot. You have access to four bowls, \neach identiﬁed by a number 0-3. The food items in each bowl are as follows: \nBowl 0: yogurt \nBowl 1: granola \nBowl 2: blueberries \nBowl 3: empty \nYou have access to the following functions to control the robot: \nobirobot.scoop_from_bowlno(bowlno): scoop from the speciﬁed bowl \nobirobot.scrape_down_bowlno(bowlno): scrape down the speciﬁed bowl \nobirobot.move_to_mouth(): move to the user's mouth position\n \nYou will use the functions above to generate python code to scoop or scrape food from the \nbowl and move to my mouth as I request. Feel free to ask me any clarifying questions.\nEnvironment Description (ED)\nRobot Functions (RF)\nRobot Variables (RV) \nFunction Applications (FA)\nCode Speciﬁcations (CS)\nUser-Control Functions (UF)\nLegend\nED\nRF \n \nRF/FA\nRF/FA\nRF/FA\nCS/FA\nCS/FA\nED\nRF \n \nRF\nFA\nRF\nFA\nRF\nFA\nFA\nRV\nRV\nFA\nCS\nCS\nED\nRF \n \nRF\nFA\nRF\nFA\nRF\nFA\nFA\nRV\nRV\nCS\nUF\nUF\nFA\nUF\nFA\nUF\nFA\nUF\nFA\nFA\nCS\nCS\nFigure A1: Prompt Iteration. The three versions of our prompt are shown with the label and text color indicating the framework\ncomponent that is being satisfied.\n•Feed me a bigger scoop of blueberries.\n•Feed me some blueberries slower.\n•Scoop of blueberries.\n– A large scoop of blueberries.\n•Give me a bigger scoop of... Hey, Obi, give me a bigger scoop\nof Cheerios.\n•Give me a bigger scoop of apple sauce.\n– Give me another scoop of apple sauce.\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Padmanabha*, Yuan* et al.\n– Please give me another scoop of apple sauce. (S)\n•blueberries.\n•Quickly me, I have some blueberries.\n•Feed me a big scoop of blueberries.\n•Feed me a large scoop of blueberries.\n•Feed me a scoop of Cheerios and make it quicker, larger. I’m\nsorry. I confused Obi.\nC.4 Task 3\n•the yogurt to the side.\n– Feed me the yogurt.\n•Move the yogurt back to the center of the bowl and feed me\none scoop.\n– down the sides of the yogurt bowl and feed me wet scoop.\n– down the sides of the yogurt bowl. Feed me one scoop. (U)\n•Scrape down the yogurt dish and give me a spoon.\n•scrape down the sides of the yogurt and give me a scoop.\n•down the side of the apple sauce.\n– the OB scraped down the sides of the applesauce.\n– down the sides of the applesauce four times. (U)\n•Please scrape down the side of the bow of the apple sauce.\n•the side of the yogurt bowl and give me a spoonful.\n•scrape down the sides of the yogurt bowl.\n•the sides of the bowl, yogurt bowl.\n•down the sides of the yogurt bowl and then feed me a scoop\nof yogurt.\n•Feed me some yogurt and scrape down the sides of the bowl.\nC.5 Task 4\n•Feed me a scoop of canola.\n– Feed me a scoop of granola. Three scoops of granola.\n•Feed me three small scoops of yogurt. Sorry, three small\nscoops of granola.\n– Feed me three small scoops of granola.\n•Give me three scoops of granola.\n•three scoops of yogurt.\n– The rescoops of blueberries.\n– three scoops of blueberries. (U)\n•Cheerios three times quickly.\n– areas three times quickly.\n– I want Cheerios three times very quickly. (U)\n•Give me please three scoops of applesauce.\n•Renola.\n– I’d like three scoops of yogurt.\n•Three scoops of blueberries, please.\n– Three scoops of blueberries.\n– I would like to have three scoops of blueberries in a row.\n(S)\n•Feed me three scoops of blueberries.\n•Feed me three small scoops of blueberries.\n•Feed me three scoops of Cheerios, please.\nC.6 Task 5\n•Feed me a scoop of blueberries, followed by a scoop of yogurt.\n•blueberries and then a scoop of yogurt.\n– Feed me a scoop of blueberries and a scoop of yogurt.\n•May I have a scoop of berries and a scoop of granola?\n•One scoop of blueberries and one scoop of granola.\n– scoop of blueberries and a scoop of yogurt.\n•Give me one scoop of blueberries and one scoop of jureos.\n– I want you to fill the blueberry, fill a spoon with blueberries\nand a spoon with Cheerios and make it full.\n– I want blueberries and Cheerios quickly and make it full.\n(S)\n•Give me a scoop of apple sauce and then a scoop of pudding.\n•I’d like a scoop of granola and a scoop of blueberries.\n•I’d like some granola and some yogurt please.\n– I’d like some blueberries and some yogurt please.\n•granola, then yogurt.\n•and then feed me a tiny scoop of yogurt.\n– me a scoop of pretzels and then feed me a tiny scoop of\nyogurt.\n•Feed me a scoop of granola and a scoop of yogurt.\nParticipant Time between Bites (s) Number of Bites\n1 38±7 12\n2 50±15 8\n3 42±14 8\n4 32±6 14\n5 55±16 7\n6 42±14 11\n7 37±8 11\n8 40±15 11\n9 71±25 7\n10 48±17 10\n11 44±11 10\nMean±SD 45±10 10±2\nTable A1: Time between Bites and Number of Bites per Par-\nticipant\nD BITE TIMING METRICS\nFrom the open feeding session from our study with older adults,\nwe present the average bite timing for each participant in Table A1.\nResearchers in robot-assisted feeding should interpret these values\ncautiously, as the bite timing could have been prolonged due to\nthe participants needing to issue the speech commands and due to\nthe speed of the robot. We additionally noticed that participants\nwho gave multi-step instructions as commands had shorter bite\ntimings."
}