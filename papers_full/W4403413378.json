{
  "title": "Automatic Library Migration Using Large Language Models: First Results",
  "url": "https://openalex.org/W4403413378",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Almeida, Aylton",
      "affiliations": [
        "Hospital das Clínicas da Universidade Federal de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A2756142893",
      "name": "Xavier Laerte",
      "affiliations": [
        "Pontifícia Universidade Católica de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A2748100588",
      "name": "Valente, Marco Tulio",
      "affiliations": [
        "Hospital das Clínicas da Universidade Federal de Minas Gerais"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4400484796",
    "https://openalex.org/W3184420437",
    "https://openalex.org/W2990868959",
    "https://openalex.org/W2773858744",
    "https://openalex.org/W2144827892",
    "https://openalex.org/W2129216008",
    "https://openalex.org/W4312763760",
    "https://openalex.org/W3121596715",
    "https://openalex.org/W3204578495",
    "https://openalex.org/W2953951611",
    "https://openalex.org/W2762481683",
    "https://openalex.org/W4389104713",
    "https://openalex.org/W4399668074",
    "https://openalex.org/W4284709233",
    "https://openalex.org/W3094949573"
  ],
  "abstract": "Despite being introduced only a few years ago, Large Language Models (LLMs)\\nare already widely used by developers for code generation. However, their\\napplication in automating other Software Engineering activities remains largely\\nunexplored. Thus, in this paper, we report the first results of a study in\\nwhich we are exploring the use of ChatGPT to support API migration tasks, an\\nimportant problem that demands manual effort and attention from developers.\\nSpecifically, in the paper, we share our initial results involving the use of\\nChatGPT to migrate a client application to use a newer version of SQLAlchemy,\\nan ORM (Object Relational Mapping) library widely used in Python. We evaluate\\nthe use of three types of prompts (Zero-Shot, One-Shot, and Chain Of Thoughts)\\nand show that the best results are achieved by the One-Shot prompt, followed by\\nthe Chain Of Thoughts. Particularly, with the One-Shot prompt we were able to\\nsuccessfully migrate all columns of our target application and upgrade its code\\nto use new functionalities enabled by SQLAlchemy's latest version, such as\\nPython's asyncio and typing modules, while preserving the original code\\nbehavior.\\n",
  "full_text": "arXiv:2408.16151v3  [cs.SE]  25 Sep 2024\nAutomatic Library Migration Using Large Language Models:\nFirst Results\nAylton Almeida\nayltonalmeida@dcc.ufmg.br\nFederal University of Minas Gerais\nBelo Horizonte, Minas Gerais, Brazil\nLaerte Xavier\nlaertexavier@pucminas.br\nPontiﬁcial University of Minas Gerais\nBelo Horizonte, Minas Gerais, Brazil\nMarco Tulio Valente\nmtov@dcc.ufmg.br\nFederal University of Minas Gerais\nBelo Horizonte, Minas Gerais, Brazil\nABSTRACT\nDespite being introduced only a few years ago, Large Languag e\nModels (LLMs) are already widely used by developers for code gen-\neration. However, their application in automating other So ftware\nEngineering activities remains largely unexplored. Thus, in this pa-\nper, we report the ﬁrst results of a study in which we are explo ring\nthe use of ChatGPT to support API migration tasks, an importa nt\nproblem that demands manual eﬀort and attention from develo p-\ners. Speciﬁcally, in the paper, we share our initial results involving\nthe use of ChatGPT to migrate a client application to use a new er\nversion of SQLAlchemy, an ORM (Object Relational Mapping) l i-\nbrary widely used in Python. We evaluate the use of three type s of\nprompts (Zero-Shot, One-Shot, and Chain Of Thoughts) and sh ow\nthat the best results are achieved by the One-Shot prompt, fo llowed\nby the Chain Of Thoughts. Particularly, with the One-Shot pr ompt\nwe were able to successfully migrate all columns of our targe t ap-\nplication and upgrade its code to use new functionalities en abled\nby SQLAlchemy’s latest version, such as Python’s asyncio and\ntyping modules, while preserving the original code behavior.\nKEYWORDS\nAPI Migration; Large Language Models; ChatGPT; Python; SQL -\nAlchemy\nACM Reference Format:\nAylton Almeida, Laerte Xavier, and Marco Tulio Valente. 2024. Aut omatic\nLibrary Migration Using Large Language Models: First Results. In Proceed-\nings of the 18th ACM / IEEE International Symposium on Empiri cal Software\nEngineering and Measurement (ESEM ’24), October 24–25, 202 4, Barcelona,\nSpain. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/36 74805.3690746\n1 INTRODUCTION\nLarge Language Models (LLMs) are being used to support sever al\nsoftware engineering tasks, including generating tests [2 , 15, 16],\nﬁxing bugs [17] and supporting code review and pair programm ing\nsessions [9, 18, 19]. However, to the best of our knowledge, t hey\nhave not yet been used to support API migration. This is usual ly a\nkey activity in modern software development, as the applica tions\nPermission to make digital or hard copies of all or part of thi s work for personal or\nclassroom use is granted without fee provided that copies ar e not made or distributed\nfor proﬁt or commercial advantage and that copies bear this n otice and the full cita-\ntion on the ﬁrst page. Copyrights for components of this work owned by others than\nthe author(s) must be honored. Abstracting with credit is pe rmitted. To copy other-\nwise, or republish, to post on servers or to redistribute to l ists, requires prior speciﬁc\npermission and/or a fee. Request permissions from permissi ons@acm.org.\nESEM ’24, October 24–25, 2024, Barcelona, Spain\n© 2024 Copyright held by the owner/author(s). Publication r ights licensed to ACM.\nACM ISBN 979-8-4007-1047-6/24/10\nhttps://doi.org/10.1145/3674805.3690746\nincreasingly depend on APIs [1]. On the one hand, these APIs t yp-\nically evolve rapidly to oﬀer new features and increase deve lop-\ners’ productivity [6, 8, 11, 13]. On the other hand, this evol ution\noften results in the introduction of breaking changes, whic h are\nchanges in the APIs that impact their clients [3–5, 22]. Thus , fre-\nquently, current software applications must be updated to u se a\nnew and improved API version [14]. Normally, this task is man ual\nsince we lack established and consolidated tools to support it, at\nleast in production software [7, 10, 20].\nThus, in this paper, we report the ﬁrst results of a study in wh ich\nwe are using language models, speciﬁcally GPT 4.0, to suppor t the\nmigration of client applications to use newer API versions. Specif-\nically, we chose the SQLAlchemy library as our object of stud y,\nwhich is a very popular Object Relational Mapper (ORM) in the\nPython ecosystem. This library faced a signiﬁcant update in its\nversion 2.0 to incorporate Python’s typing module. We then c hose\na real client application and carefully attempted to use lan guage\nmodels (particularly, GPT version 4.0) to migrate it to the n ew ver-\nsion of SQLAlchemy. For this, we explored and evaluate the re sults\nof three types of prompts: Zero-Shot (in which we simply desc ribe\nthe task we want the model to perform), One-Shot (where in add i-\ntion to describing the desired task, we also provide an examp le of\nits execution), and Chain Of Thoughts (where we brieﬂy descr ibe\nthe steps necessary for the model to perform the task).\nTo evaluate the correctness and the quality of the code migra ted\nby GPT we use a set of metrics including number of passing test s\nand SQLAlchemy-speciﬁc metrics, such as number of migrated columns\nand number of migrated methods. We also consider source code\nquality metrics, as provided by two popular static analysis tools\nfor Python (Pylint and Pyright). Moreover, we manually migr ated\nthe client application to use the newer SQLAlchemy version. Then,\nwe use this version as a ground-truth, particularly for anal ysing the\nquality scores generated by the mentioned static analysis t ools. As\nour last investigation, we evaluate and present the results of the\nmigration of the application’s tests.\nOur main contributions are twofold: (1) We describe the ﬁrst ver-\nsion of a framework for library migration using language mod els,\nspeciﬁcally OpenAI’s GPT model. The proposed framework con -\nsists of a set of prompts and a set of metrics for evaluating th e cor-\nrectness and quality of this task. Although subjected to imp rove-\nments, we argue this framework can serve as a starting point f or\ndevelopers interested in automating this task. (2) We also p resent\nand discuss the ﬁrst results of using this framework to suppo rt\nthe migration of a client of a popular library in the Python ec osys-\ntem (SQLAlchemy, version 1 to version 2). We discuss and anal yze\nthis migration using the proposed metrics. In addition to th e ap-\nplication code, we also perform and analyze the migration of the\napplication’s tests.\nESEM ’24, October 24–25, 2024, Barcelona, Spain Aylton Alme ida, Laerte Xavier, and Marco Tulio Valente\nThe remainder of this paper is organized as follows. In Secti on 2,\nwe describe the study design, detailing the methodology use d for\nthe migration process. Section 3 presents the results of the migra-\ntion, including a comparative analysis of the diﬀerent prom pting\nmethods. We organize this section in two parts. First, in Sec tion 3.1,\nwe present the migration of the application code. Then, in Se ction\n3.2, we assess the migration of the application tests. Secti on 4 dis-\ncusses threats to validity and in Section 5 we review related work.\nFinally, Section 6 concludes the paper and outlines directi ons for\nfuture work.\n2 STUDY DESIGN\nIn order to explore the eﬀectiveness of LLMs to support API mi gra-\ntion, we carefully attempt to use GPT 4.0 to upgrade the SQLAl chemy\nAPI in a client application. In this section, we detail the st eps taken\nto conduct this migration, describing the context of this ta sk and\nthe creation and evaluation of three types of prompts.\n2.1 Target API and Client Application\nSQLAlchemy1 is an Object-Relational Mapping (ORM) API to sup-\nport the integration of Python applications with relationa l databases,\nabstracting operations related to database connection and manip-\nulation. It supports the connection to various databases, s uch as\nPostgreSQL, MySQL, and Oracle, using a simple API that resem -\nbles pure SQL queries. The API is well-known and -adopted in t he\nopen source community, with more than 8,9K stars, 1,3K forks , and\n765K users among other GitHub repositories.\nIn this paper, we focus on migrating from version 1 to version\n2 of SQLAlchemy. Version 1 is widely used and provides all the\nfunctionalities expected from a robust ORM. However, as new fea-\ntures have been added to Python, SQLAlchemy was updated to\ntake advantage of such improvements. Among the major change s\nin version 2, the compatibility with Python’s static typing stands\nout, improving error detection during development and faci litating\nthe maintenance of large applications. While compatibilit y with\nPython’s asyncio was introduced in version 1.4, version 2.0 in-\ncludes several performance improvements and optimization s for\nasynchronous operations. Particularly, the asyncio module sup-\nports eﬃcient asynchronous tasks using the async/await syntax\nin Python. Finally, SQLAlchemy 2 has improved the query synt ax,\nmaking it more intuitive and easier to use.\nTo perform the migration of the API, we used a client applica-\ntion called BiteStreams/fastapi-template. It is a Python appli-\ncation that uses the FastAPI library, a popular web framewor k for\ncreating APIs, in conjunction with the SQLAlchemy ORM. The a p-\nplication implements a TODO list feature, containing REST r outes\nfor creating and listing tasks, which are stored in a Postgre SQL\ndatabase. It contains a single table called todo with four columns,\nresponsible for storing information about each todo item in the\nlist. Additionally, it has 18 methods, which allows the user to fetch\na single TODO item, list all of them, and insert new ones into t he\ndatabase. We selected this client application because it in cludes\nfour automated tests, including both integration and unit t ests, fa-\ncilitating the veriﬁcation of code behavior and functional ity after\n1https://github.com/sqlalchemy/sqlalchemy\nmigration. Although it is a simple project, we claim that it p rovides\nan interesting environment to perform our analysis.\n2.2 Migration Process\n1 @lru_cache ( maxsize = None )\n2 def get_engine ( db_string : str ):\n3 return create_async_engine( db_string , pool_pre_ping =\nTrue , poolclass = NullPool )\n4\n5\n6 class TodoInDB ( SQL_BASE ): # type: ignore\n7 __tablename__ = /quotedbl.Var todo/quotedbl.Var\n8\n9 id : Mapped [ int ] = mapped_column ( Integer , primary_key =\nTrue , autoincrement = True)\n10 key : Mapped [ str ] = mapped_column ( String ( length =128) ,\nnullable = False , unique = True )\n11 value : Mapped [ str ] = mapped_column ( String ( length =128)\n, nullable = False )\n12 done: Mapped [ bool ] = mapped_column ( Boolean , default =\nFalse )\nListing 1: Example of manually migrated code\nIn order to establish a baseline for the results, we started b y\nperforming a manual migration of the API in the client applic a-\ntion. Listing 1 presents an excerpt of the migrated code. Thi s man-\nual migration allowed us to identify particular changes req uired\nand the potential challenges that GPT could ﬁnd during the mi -\ngration process. Additionally, it helped us to understand s ome ad-\njustments needed to ensure that the automated migration pro cess\nwould work. For example, during this initial exploration, w e ob-\nserved the necessity of upgrading the library version in the depen-\ndencies ﬁle and adding both asyncpg and pytest-asyncio APIs\nto the project. Such APIs support the connection with the dat abase\nand the execution of tests when using Python’s asyncio.\nAfter the manual migration of the client application, we dec ided\nto break the automatic migration process in two steps. The ﬁr st\nstep consists on migrating only the application code with th e as-\nsistance of GPT. For this reason, we excluded the test ﬁles fr om the\nprompts. In other words, the tests were manually migrated to as-\nsess the code produced by GPT. In the second step, we tasked GP T\nto migrate only the test ﬁles. In this case, our intention is t o assess\nthe migrated tests by running them on the manually migrated a p-\nplication. In both steps, we evaluated three types of prompt s, as\nwe will describe in Section 2.3.\nFor the GPT migration, we implemented a simple script to tran -\nsition between prompting approaches. It uses OpenAI’s Pyth on\nAPI version 1.14 together with the Chat Completions API. We u sed\nmodel GPT-4 and a basic role was set up for the system: \"You are\na developer with expertise in Python\", along with a temperat ure\nof zero, which should result in more consistent outputs. For each\nprompt, the ﬁrst result was stored for analysis, since we not iced\nthat running the same prompt multiple times yields slightly diﬀer-\nent results, even though the temperature setting was set to z ero.\n2.3 Deﬁnition of Prompts\nA critical part of working with LLMs is deﬁning prompts that c learly\ncommunicate the intentions of the user. In this study, we exp lore\nAutomatic Library Migration Using Large Language Models:\nFirst Results ESEM ’24, October 24–25, 2024, Barcelona, Spain\nBase Prompt\nThe Python code bellow uses the library sqlalchemy with ver-\nsion 1. Migrate it so that it works with version 2 of sqlalchem y.\nMake the code compatible with python’s asyncio. Use python’s\ntyping module to add type hints to the code. Your answer must\nonly contain code. Do not explain it. Do not add markdown\nbackticks for code. Do not add extra functionality to the cod e.\nDo not remove code that is not being changed. If there’s no\nneed to change the code, answer only with the code itself. The\nﬁrst line of code must have a comment \"### START CODE ###\".\nThe last line of code must have a comment \"### END CODE ###\".\nHere is the code to migrate:\n### START CODE ###\n...\nFigure 1: Base command used for all prompts\nthree diﬀerent prompting methods. The ﬁrst is a Zero-Shot ap -\nproach, where the model receives a task description in the pr ompt\nbut no example is provided to illustrate the expected output [12].\nThe second prompt uses a One-Shot approach, providing the mo del\nwith an output example to help it understand the given task. L astly,\na Chain Of Thoughts approach was used. In this case, the promp t\ncontains a step-by-step guide that lead to the ﬁnal output [2 1]. The\nsame base command was used for the three prompts, as shown in\nFigure 1. In this command, we instruct the model to return onl y\ncode, with no explanations. This is relevant to better autom ate the\ntask. Additionally, we ask it to use both Python’s asyncio and typ-\ning features in order to ensure it adopts the newest features and\nsyntax for SQLAlchemy 2.\nThe ﬁrst prompting method used was the Zero-Shot approach.\nIn this method, no code examples are passed, giving the LLM co m-\nplete freedom on how to perform its task. It is composed only b y\nthe base command and the code to be migrated, as already pre-\nsented in Figure 1. The second method was a One-Shot approach ,\nin which a code example is included with the prompt to give the\nmodel a reference for the required migration. This can be see n\nin Figure 2. The provided example was retrieved from the exam -\nples folder in the SQLAlchemy repository, which deﬁnes some ta-\nbles and operations using the newest library version, toget her with\nasyncio and typing.\nOne-Shot Prompt\nUse the following code block as an example of migrated code,\nfollow the same patterns used in it:\n### START CODE ###\n...\nFigure 2: One-Shot prompt\nLastly, a Chain Of Thoughts prompt was used, where a step-by-\nstep guide is included along with the example to instruct the migra-\ntion process. The added step-by-step guide can be seen in Fig ure 3.\nIt was meant to provide GPT a guide on how to proceed with the\nmigration process, detailing actions such as using an async engine\nand applying the new column declaration methods.\nChain of Thoughts Prompt\nUse the steps bellow as a guide for the migration. You don’t\nneed to follow them exactly as described, but they should be\nable to help with the migration:\n1. Update the used database engine, if any, so that you’re usi ng\n‘create_async_engine‘ instead of ‘create_engine‘.\n2. If any tables and their columns are declared, update their\ndeclarations so that they use ‘mapped_columns‘ instead of\n‘schema.Column‘ and ensure they are correctly typed with th e\nMapped annotation, making sure to import the correct types\nfrom the library.\n3. Ensure that all queries, if any, are updated to use the new 2 .0\nstyle of querying, such as using ‘select()‘ instead of ‘quer y()‘.\n4. Update functions that use ‘sessionmaker‘ to use ‘session ‘ in-\nstead.\n5. Update the code to use async functions and await calls wher e\nnecessary.\n6. Implement type hinting for all functions and variables an d up-\ndate old type hinting to ensure they are correct.\n7. Ensure there are no missing import statements.\n8. Remove any unused imports or variable declarations.\n9. Make sure the code works.\n...\nFigure 3: Chain of Thoughts prompt\n2.4 Metrics\nTo evaluate the eﬀectiveness of the migration process and as sess\nwhether the migrated application works as intended, we used the\nfollowing metrics:\n• Runs successfully: Checks whether the code runs without\ncrashing after the migration.\n• Number of tests that pass: The number of tests that pass after\nthe migration process. This aims to check if the application\nstill works as expected after the migration. In total, 4 out o f\n4 tests should pass correctly.\n• Pylint score: This tool is a commonly used linter in Python.\nA base conﬁguration ﬁle was created, and after the code was\nmigrated, the linter was run in order to check for common\nerrors, such as missing imports or unused variables.\n• Pyright score: This tool is a type checker commonly used in\nPython. It was run to search for possible typing errors in the\ncode. It gives the number of errors and warnings found.\n• Number of migrated columns: The number of columns that\nwere correctly migrated to the new syntax. In total, there\nshould be 4 migrated columns.\nESEM ’24, October 24–25, 2024, Barcelona, Spain Aylton Alme ida, Laerte Xavier, and Marco Tulio Valente\nTable 1: Application Migration Results\nMetrics Runs Successfully Tests Pylint Pyright Migrated Columns Mi grated Methods\nBefore Migration Yes 4/4 7.68/10 2 errors - -\nManual Migration Yes 4/4 7.97/10 0 errors 4/4 18/18\nZero-Shot No 0/4 7.30/10 6 errors 0/4 16/18\nOne-Shot Yes 4/4 7.77/10 7 errors 4/4 13/18\nChain Of Thoughts No 0/4 7.33/10 5 errors 4/4 17/18\n• Number of migrated methods: This is the number of func-\ntions and methods that were migrated to use typing and\nasyncio in their signature. In total, 18 of them needed to\nbe updated in order to correctly use both features.\n• Number of migrated tests: This is the number of tests cor-\nrectly migrated. A test is considered migrated when it uses\nthe new features necessary for it to run correctly after the\nrest of the code has been migrated, such as Python’s async\nannotation. There are 4 tests that need migration.\n3 RESULTS\nIn this section, we present and analyze the results achieved by\nChatGPT using the three prompts deﬁned in the study. First, w e\npresent the results for the application migration (Section 3.1) and\nthen we also discuss the migration of the tests of this applic ation\n(Section 3.2).\n3.1 Application Migration\nTable 1 presents the results for migrating the application u sing\nZero-Shot, One-Shot, and Chain Of Thoughts prompts. To faci li-\ntate comparison and analysis, we also present the results fo r the\noriginal application (before migration) and for the baseli ne migra-\ntion, i.e., the one we performed manually. In this ﬁrst migra tion\nstep, we only migrate the application code, as mentioned in S ec-\ntion 2.1. Moreover, we use the manually migrated tests to eva luate\nthe functionality of the code produced by the LLM. 2\nZero-Shot Prompt: When analyzing the results for the Zero-Shot\napproach, we can see in Table 1 that it did not perform well. In\nfact, we could not run the application after the migration. T his\nwas mostly due to errors that prevented the application code from\nopening a connection with the database. This problem can be s een\nin more detail in Listing 2. First, in line 2 an attempt was mad e to\nimport the create_async_engine method from the wrong mod-\nule. This method is implemented in the module sqlalchemy.ext.\nasyncio, but the automatically migrated code attempts to import\nit from sqlalchemy. Additionally, in line 4, ChatGPT tried import-\ning a method called create_async_session, but this method does\nnot exist. Due to these errors, Pylint score was lower when co m-\npared to the original application (7.30 vs 7.86, respective ly).\nAnother factor that resulted in a poor result for the Zero-Sh ot\nprompting was its inability to use Python’s typing feature. This\ncan be seen in lines 4-7 in Listing 3, where the columns were no t\nupdated to use the new mapped_column method. Moreover, the\n2Just to clarity, the migration of the tests will be discussed in Section 3.2.\n1 from pydantic import BaseModel\n2 from sqlalchemy import Boolean , Column , Integer , String ,\ncreate_async_engine\n3 from sqlalchemy . exc import DatabaseError\n4 from sqlalchemy . ext . asyncio import AsyncSession ,\ncreate_async_session\n5 from sqlalchemy . orm import declarative_base , sessionmaker\n6 from sqlalchemy . pool import NullPool\nListing 2: Code generated by the zero-shot prompt with\nerrors in imports\ncolumns ( id, key, value, and done) were not declared using type\ninformation.\n1 class TodoInDB ( SQL_BASE ): # type: ignore\n2 __tablename__ = /quotedbl.Var todo/quotedbl.Var\n3\n4 id = Column ( Integer , primary_key = True , autoincrement =\nTrue )\n5 key = Column ( String ( length =128) , nullable = False ,\nunique = True )\n6 value = Column ( String ( length =128) , nullable = False )\n7 done = Column ( Boolean , default = False )\nListing 3: Code generated by the Zero-Shot prompt missing\ntypes\nFinally, in Listing 4, we can see other typing errors, such as us-\ning an Iterator instead of an AsyncIterator as the return type\nfor the create_todo_repository function (lines 2-3). Due to such\nproblems, the number of errors raised by Pyright was higher w hen\ncompared with the one of the original implementation (6 vs 2 t ype\nerrors, respectively).\nOne-Shot Prompt: In contrast with the Zero-Shot approach, the\nOne-Shot method yielded signiﬁcantly better results. The m igrated\ncode runs as expected, and all tests also passed. It is intere sting\nto see that when we provided an example to the GPT model, all\ncolumns and methods were migrated correctly, and there were no\nimport errors. A fragment of the migrated code can be seen in L ist-\ning 5. As we can observe, ChatGPT was able to correctly add typ es\nto the table columns and use the mapped_column method to map\nthem. For a better understanding, the reader can compare the code\nin Listing 5 with the one presented in Listing 3, which was gen er-\nated using Zero-Shot prompt.\nAs a negative note, both Pylint and Pyright scores decreased\nwhen the application was migrated using an One-Shot prompt. Two\nunused imports were generated, which resulted in a lower Pyl int\nAutomatic Library Migration Using Large Language Models:\nFirst Results ESEM ’24, October 24–25, 2024, Barcelona, Spa in\n1 @asynccontextmanager\n2 async def create_todo_repository() -> Iterator [\nTodoRepository ]:\n3 async with create_async_session( get_engine ( os . getenv (\n/quotedbl.Var DB_STRING /quotedbl.Var))) as session :\n4 todo_repository = SQLTodoRepository ( session )\n5\n6 try :\n7 yield todo_repository\n8 except Exception :\n9 await session . rollback ()\n10 raise\n11 finally :\n12 await session . close ()\nListing 4: Code generated by the Zero-Shot using incorrect\ntypes\n1 class TodoInDB ( SQL_BASE ): # type: ignore\n2 __tablename__ = /quotedbl.Var todo/quotedbl.Var\n3\n4 id = Column ( Integer , primary_key =True , autoincrement =\nTrue )\n5 key = Column ( String ( length =128) , nullable = False ,\nunique = True )\n6 value = Column ( String ( length =128) , nullable = False )\n7 done = Column ( Boolean , default = False )\nListing 5: Code generated by the One-Shot prompt with\ncorrect types\nscore. Additionally, a relevant typing error was detected i n the mi-\ngrated code, where instead of typing the session attribute as an\nAsyncSession, it was typed as a Session, resulting in multiple er-\nrors throughout the application.\nChain Of Thoughts Prompt: This prompt also performed well,\nresulting in the correct migration of both methods and colum ns,\nsimilar to the One-Shot prompt. It also achieved the second- best\nPylint results (after the One-Shot approach), and obtained the best\nPyright result, with ﬁve errors compared to 6 and 7 errors for the\nZero-Shot and One-Shot prompts, respectively. However, the create_async_engine\nwas imported from the wrong module, resulting in an error tha t\nprevented the application from running and the tests from pa ss-\ning. We claim this is a minor error that can be ﬁxed manually by a\ndeveloper with experience in Python, thus allowing the migr ated\napplication to execute and behave as expected.\nSummary: (1) One-Shot was the prompt with the best results: it\nwas able to generate a running application that passes the te sts;\nit also achieve the best Pylint score. (2) Chain Of Thoughts w as\nthe second best prompt and the one with the lowest number of\nPyright type errors (ﬁve errors); the code did not execute du e to\na minor import error. (3) Zero-Shot presented the worst resu lts\nand it was not able to correctly migrate any of the table colum ns.\n3.2 Tests Migration\nTable 2 presents the results for the three prompts when migra ting\nthe tests and running them against the manually migrated app li-\ncation. The column Migrated Tests shows the number of tests c or-\nrectly migrated. An example of a correctly migrated test can be\nseen in Listing 6. In this example, a test_todo was implemented\nusing an async function from Python’s asyncio module (line 2).\nAlso, throughout the test both await and async keywords were\ninserted when necessary to ensure a correct behavior in asyn chro-\nnous calls (lines 3, 4 and 6).\nTable 2: Tests Migration Results\nMetrics Migrated Tests Tests that Pass\nBefore Migration – –\nManual Migration 4/4 4/4\nZero-Shot 4/4 1/4\nOne-Shot 4/4 1/4\nChain Of Thoughts 4/4 1/4\n1 @pytest . mark . integration\n2 async def test_todo ( todo_repository : SQLTodoRepository ):\n3 async with todo_repository as r:\n4 await r. save ( Todo( key =/quotedbl.Var testkey /quotedbl.Var, value = /quotedbl.Var testvalue\n/quotedbl.Var))\n5\n6 todo = await r. get_by_key ( /quotedbl.Var testkey /quotedbl.Var)\n7 assert todo . value == /quotedbl.Var testvalue /quotedbl.Var\nListing 6: Correctly Migrated Test\nHowever, when looking at Table 2, we can see that although\nmigrated correctly only a single test has initially passed. This was\ndue to an issue in the way the tests are implemented, which add ed\na new layer of complexity to the migration process. Essentia lly, the\ntests are implemented in such a way that between their execut ion a\nﬁxture must run (in Python, a ﬁxture is a function that runs be fore\neach test). In our case, this function truncates the TODOs ta ble and\ninitializes a new session to ensure the database is empty bef ore\nrunning the next test. However, none of the approaches were a ble\nto correctly migrate this ﬁxture. Therefore, after the ﬁrst test ran\nand passed, the other three tests failed due to duplicate key errors\nwhen trying to insert the same row as the previous test in the t able.\nIt is interesting to point out that all three approaches mana ged\nto migrate the tests in the same way, including making the sam e\nmistakes while migrating the mentioned ﬁxture. This incorr ect im-\nplementation can be seen in Listing 7. In this code, we can see that\nan attempt to truncate the table occurs in lines 13 and 14. How -\never, there is no call to the commit function, which causes the\nsession to close without applying the truncate command. Before\nSQLAlchemy version 2, all sessions had an autocommit=True be-\nhavior, such that when they were closed, they automatically com-\nmitted the changes. Nevertheless, in version 2, this behavi or changed\nto autocommit=False, which resulted in the problem for all three\nprompts and explains why ChatGPT made this kind of mistake. A l-\nthough this error prevented the tests from passing, it can be easily\nESEM ’24, October 24–25, 2024, Barcelona, Spain Aylton Alme ida, Laerte Xavier, and Marco Tulio Valente\n1 @pytest . fixture\n2 async def todo_repository () -> SQLTodoRepository :\n3 time . sleep (1)\n4 alembicArgs = [ /quotedbl.Var --raiseerr /quotedbl.Var, /quotedbl.Var upgrade /quotedbl.Var, /quotedbl.Var head /quotedbl.Var]\n5 alembic . config . main ( argv = alembicArgs )\n6\n7 engine = create_async_engine( os . getenv ( /quotedbl.Var DB_STRING /quotedbl.Var))\n8 async_session = sessionmaker ( engine , class_ =\nAsyncSession )\n9\n10 async with async_session () as session :\n11 yield SQLTodoRepository ( session )\n12\n13 await session . execute (\n14 /quotedbl.Var;/quotedbl.Var. join ([f/quotedbl.Var TRUNCATE TABLE {t} CASCADE /quotedbl.Varfor t\nin SQL_BASE . metadata . tables . keys () ])\n15 )\nListing 7: Migrated Fixture\nﬁxed by an experienced developer. Indeed, we applied this ﬁx and\nthen all the four tests started to pass.\nSummary: An interesting issue occurred during the migration of\nthe tests. Initially, they were migrated correctly. Howeve r, a sub-\ntle change introduced in the new version of SQLAlchemy pre-\nvented the migrated tests from executing successfully. Spe ciﬁ-\ncally, in the new library version, autocommit is no longer true\nby default. Thus, once we manually restored the expected com -\nmit behavior, the tests migrated with the three prompts pass ing.\n4 THREATS TO VALIDITY\nThe ﬁrst threat is related to the selection of only one applic ation\nas target client for migration. We acknowledge that diﬀeren t appli-\ncations may demand diﬀerent approaches to migrate. This mea ns\nthat other applications may yield diﬀerent results when app lying\nthe same methodology. Besides, we also highlight that we use d our\nframework only with Python and the SQLAlchemy API. Changing\nboth the programming language and the target API may produce\ndiﬀerent observations.\nWe also acknowledge that the GPT model usually provides dif-\nferent answers in each interaction, even with the same promp t. To\nmitigate this threat, we have set the temperature parameter to\nzero, which makes the model more deterministic. We also reli ed\non the ﬁrst answer of each interaction, as reported in Sectio n 2.\nLastly, the way the prompts have been constructed may also in ﬂu-\nence the outputs. In this case, we carefully deﬁned our three types\nof prompts, getting inspired by the ones used in the literatu re.\n5 RELATED WORK\nUsing LLMs to support software development has been previou sly\nexplored in the literature. Several studies have been condu cted to\nunderstand how to take advantage of their potential to save d e-\nvelopment time and improve code quality. For example, Schäf er et\nal. [15] explored the usage of LLMs for the implementation of unit\ntests. In their study, ChatGPT was used to implement unit tes ts for\n25 diﬀerent npm packages. Their experiment suggests that pr ompt-\ning the LLM with a Few-Shot approach positively inﬂuences th e\nresults. This conclusion is aligned with the ﬁndings of our w ork.\nBy providing an example for the migration (as in the One-Shot and\nChain Of Thoughts approaches), GPT was able to support an im-\nproved migration compared to the Zero-Shot prompting.\nSobania et al. [17] assess ChatGPT’s bug-ﬁxing capabilitie s us-\ning a benchmark called QuixBugs. The authors ﬁnd that, even t hough\nChatGPT was not built speciﬁcally for code repair, it is extr emely\ncompetitive with other approaches created for this purpose , such\nas CoCoNut and Codex. Due to its interface, users can provide ex-\ntra information about the problems, such as error messages a nd\nexpected outputs, which further increase the chances of suc cess.\nIn this work, we focus on code migration instead of bug ﬁxing, but\nwe also achieve results that suggest that GPT can also be used to\nsupport code maintenance tasks.\n6 CONCLUSION AND FUTURE WORK\nIn this paper, we proposed a LLM-based framework for upgradi ng\nAPI versions using three diﬀerent prompting approaches and a set\nof metrics to asses the migrated code. We use this framework t o\nmigrate a client application to work with a newer version of t he\nSQLAlchemy library. We concluded that LLMs are able to corre ctly\nmigrate the project when provided with at least one example o f an\nalready migrated application, making only minor mistakes s uch as\ninserting unused imports or incorrectly typed variables, w hich are\nerrors than can be later ﬁxed by a developer.\nAlthough our ﬁrst results are promising, there is still futu re\nwork to be done in order to use LLMs to support library migra-\ntions. Thus, we plan to continue to work as follows:\n(1) We intend to evaluate other libraries, including librar ies for\nother programming languages, such as Java (an established\nstatically-typed programming language) and JavaScript (a\npopular language in the speciﬁc domain of Web apps).\n(2) We plan to deﬁne and evaluate other types of prompts (e.g. ,\nFew-Shots and Chain Of Symbols) and improve the current\nprompts. For example, in the case of Chain of Thoughts we\ncan add more details on the step-by-step guide or even pass\nthe oﬃcial migration documentation for the LLM to use as\na reference.\n(3) We plan to assess our framework with developers, for exam -\nple, by submitting pull requests in GitHub projects.\n(4) Finally, we also intend to evaluate other LLMs, such as Go ogle\nGemini3 and Amazon Q. 4\nReplication Data: The code of the application used in this re-\nsearch, along with the code of all migrated versions, is avai lable\nat: https://zenodo.org/records/11403035\nACKNOWLEDGMENTS\nThis research was supported by grants from CNPq and FAPEMIG.\nREFERENCES\n[1] Rabe Abdalkareem. 2017. Reasons and drawbacks of using t rivial npm packages:\nthe developers’ perspective. In Foundations of Software Engineering (FSE) . 1062–\n1064.\n[2] Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, B eliz Gokkaya, Mark\nHarman, Inna Harper, Alexandru Marginean, Shubho Sengupta , and Eddy Wang.\n3https://gemini.google.com\n4https://aws.amazon.com/pt/q\nAutomatic Library Migration Using Large Language Models:\nFirst Results ESEM ’24, October 24–25, 2024, Barcelona, Spa in\n2024. Automated Unit Test Improvement Using Large Language Models at Meta.\nIn Foundations of Software Engineering (FSE) .\n[3] Chris Bogart, Christian Kästner, James Herbsleb, and Fe rdian Thung. 2021.\nWhen and how to make breaking changes: Policies and practice s in 18 open\nsource software ecosystems. ACM Transactions on Software Engineering and\nMethodology (TOSEM) 30, 4 (2021), 1–56.\n[4] Aline Brito, Marco Tulio Valente, Laerte Xavier, and And re Hora. 2020. You\nBroke My Code: Understanding the Motivations for Breaking C hanges in APIs.\nEmpirical Software Engineering 25 (2020), 1458–1492.\n[5] Aline Brito, Laerte Xavier, Andre Hora, and Marco Tulio V alente. 2018. Why\nand How Java Developers Break APIs. In International Conference on Software\nAnalysis, Evolution and Reengineering (SANER) . 255–265.\n[6] Gleison Brito, Andre Hora, Marco Tulio Valente, and Roma in Robbes. 2018. On\nthe Use of Replacement Messages in API Deprecation: An Empir ical Study. Jour-\nnal of Systems and Software 137 (2018), 306–321.\n[7] André Hora, Romain Robbes, Nicolas Anquetil, Anne Etien , Stéphane Ducasse,\nand Marco Tulio Valente. 2015. How do developers react to API evolution? the\nPharo ecosystem case. In International Conference on Software Maintenance and\nEvolution (ICSME) . 251–260.\n[8] Daqing Hou and Xiaojia Yao. 2011. Exploring the intent be hind API evolution:\nA case study. In Working Conference on Reverse Engineering (WCRE) . 131–140.\n[9] Saki Imai. 2022. Is GitHub Copilot a Substitute for Human Pair-programming?\nAn Empirical Study. In International Conference on Software Engineering: Com-\npanion Proceedings (ICSE-Companion) . 319–321.\n[10] Raula Gaikovina Kula, Daniel M German, Ali Ouni, Takash i Ishio, and Katsuro\nInoue. 2018. Do developers update their library dependenci es? An empirical\nstudy on the impact of security advisories on library migrat ion. Empirical Soft-\nware Engineering 23 (2018), 384–417.\n[11] Maxime Lamothe, Yann-Gaël Guéhéneuc, and Weiyi Shang. 2021. A systematic\nreview of API evolution literature. Comput. Surveys 54, 8 (2021), 1–36.\n[12] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal,\nand Aman Chadha. 2024. A Systematic Survey of Prompt Enginee ring in Large\nLanguage Models: Techniques and Applications. arXiv:2402 .07927 [cs.AI]\n[13] Maria Salama, Rami Bahsoon, and Patricia Lago. 2019. St ability in software engi-\nneering: Survey of the state-of-the-art and research direc tions. IEEE Transactions\non Software Engineering 47, 7 (2019), 1468–1510.\n[14] Anand Ashok Sawant, Romain Robbes, and Alberto Bacchel li. 2018. On the reac-\ntion to deprecation of clients of 4+ 1 popular Java APIs and th e JDK. Empirical\nSoftware Engineering 23 (2018), 2158–2197.\n[15] Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2 024. An Empirical\nEvaluation of Using Large Language Models for Automated Uni t Test Genera-\ntion. IEEE Transactions on Software Engineering 50, 1 (2024), 85–105.\n[16] Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Ha san Tanvir, Noshin\nUlfat, Fahmid Al Rifat, and Vinicius Carvalho Lopes. 2024. U sing Large Language\nModels to generate JUnit Tests: An Empirical Study. In International Conference\non Evaluation and Assessment in Software Engineering (EASE 2024).\n[17] Dominik Sobania, Martin Briesch, Carol Hanna, and Just yna Petke. 2023. An\nAnalysis of the Automatic Bug Fixing Performance of ChatGPT . In IEEE/ACM\nInternational Workshop on Automated Program Repair (APR) . 23–30.\n[18] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys\nPoshyvanyk, and Gabriele Bavota. 2022. Using Pre-Trained M odels to Boost\nCode Review Automation. In International Conferenceon Software Engineering\n(ICSE). 2291–2302.\n[19] Rosalia Tufano, Luca Pascarella, Michele Tufano, Deny s Poshyvanyk, and\nGabriele Bavota. 2021. Towards Automating Code Review Acti vities. In Inter-\nnational Conference on Software Engineering (ICSE) . 163–174.\n[20] Ying Wang, Bihuan Chen, Kaifeng Huang, Bowen Shi, Congy ing Xu, Xin Peng,\nYijian Wu, and Yang Liu. 2020. An empirical study of usages, u pdates and risks\nof third-party libraries in Java projects. In International Conference on Software\nMaintenance and Evolution (ICSME) . 35–45.\n[21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma , Brian Ichter, Fei\nXia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. arXiv:2201.11 903 [cs.CL]\n[22] Laerte Xavier, Aline Brito, Andre Hora, and Marco Tulio Valente. 2017. Histori-\ncal and Impact Analysis of API Breaking Changes: A Large Scal e Study. In Inter-\nnational Conference on Software Analysis, Evolution and Re engineering (SANER) .\n138–147.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7274459600448608
    },
    {
      "name": "Natural language processing",
      "score": 0.4477458596229553
    },
    {
      "name": "Programming language",
      "score": 0.37771326303482056
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3706575632095337
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210134014",
      "name": "Hospital das Clínicas da Universidade Federal de Minas Gerais",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I170935008",
      "name": "Pontifícia Universidade Católica de Minas Gerais",
      "country": "BR"
    }
  ],
  "cited_by": 3
}