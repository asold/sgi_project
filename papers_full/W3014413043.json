{
    "title": "Listen and Fill in the Missing Letters: Non-Autoregressive Transformer for Speech Recognition",
    "url": "https://openalex.org/W3014413043",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2365667451",
            "name": "Chen, Nanxin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2575256866",
            "name": "Watanabe, Shinji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3043067048",
            "name": "Villalba, Jes√∫s",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2750406217",
            "name": "Dehak, Najim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2937808806",
        "https://openalex.org/W854541894",
        "https://openalex.org/W3034729383",
        "https://openalex.org/W2542835211",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W648786980",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W2913250058",
        "https://openalex.org/W2982095018",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2886769154",
        "https://openalex.org/W2963242190",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W1660460062",
        "https://openalex.org/W2991509857",
        "https://openalex.org/W2920538220",
        "https://openalex.org/W2912937082",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2945260553"
    ],
    "abstract": "Recently very deep transformers have outperformed conventional bi-directional long short-term memory networks by a large margin in speech recognition. However, to put it into production usage, inference computation cost is still a serious concern in real scenarios. In this paper, we study two different non-autoregressive transformer structure for automatic speech recognition (ASR): A-CMLM and A-FMLM. During training, for both frameworks, input tokens fed to the decoder are randomly replaced by special mask tokens. The network is required to predict the tokens corresponding to those mask tokens by taking both unmasked context and input speech into consideration. During inference, we start from all mask tokens and the network iteratively predicts missing tokens based on partial results. We show that this framework can support different decoding strategies, including traditional left-to-right. A new decoding strategy is proposed as an example, which starts from the easiest predictions to the most difficult ones. Results on Mandarin (Aishell) and Japanese (CSJ) ASR benchmarks show the possibility to train such a non-autoregressive network for ASR. Especially in Aishell, the proposed method outperformed the Kaldi ASR system and it matches the performance of the state-of-the-art autoregressive transformer with 7x speedup. Pretrained models and code will be made available after publication.",
    "full_text": null
}