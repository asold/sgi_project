{
  "title": "End-to-End Human Pose and Mesh Reconstruction with Transformers",
  "url": "https://openalex.org/W3112160422",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2915472489",
      "name": "Lin, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1578702133",
      "name": "Wang Li-juan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352283912",
      "name": "Liu, Zicheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3004162361",
    "https://openalex.org/W2143267104",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2888934629",
    "https://openalex.org/W2981637078",
    "https://openalex.org/W3108516375",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2956061722",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2573098616",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2797184202",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2768683308",
    "https://openalex.org/W2978956737",
    "https://openalex.org/W2965523038",
    "https://openalex.org/W3009795828",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035501466",
    "https://openalex.org/W2080873731",
    "https://openalex.org/W2032618685",
    "https://openalex.org/W2963592930",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2964194725",
    "https://openalex.org/W1967554269",
    "https://openalex.org/W2963995996",
    "https://openalex.org/W2883221003",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W3035551320",
    "https://openalex.org/W2883758202",
    "https://openalex.org/W2573854917",
    "https://openalex.org/W2798637590",
    "https://openalex.org/W2981920153",
    "https://openalex.org/W1987648924",
    "https://openalex.org/W3109877674",
    "https://openalex.org/W2895748257",
    "https://openalex.org/W2797515701",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2810307462",
    "https://openalex.org/W2545173102",
    "https://openalex.org/W3035291735",
    "https://openalex.org/W2994220412",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2962754033",
    "https://openalex.org/W2963907666",
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2981978060"
  ],
  "abstract": "We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D human pose and mesh vertices from a single image. Our method uses a transformer encoder to jointly model vertex-vertex and vertex-joint interactions, and outputs 3D joint coordinates and mesh vertices simultaneously. Compared to existing techniques that regress pose and shape parameters, METRO does not rely on any parametric mesh models like SMPL, thus it can be easily extended to other objects such as hands. We further relax the mesh topology and allow the transformer self-attention mechanism to freely attend between any two vertices, making it possible to learn non-local relationships among mesh vertices and joints. With the proposed masked vertex modeling, our method is more robust and effective in handling challenging situations like partial occlusions. METRO generates new state-of-the-art results for human mesh reconstruction on the public Human3.6M and 3DPW datasets. Moreover, we demonstrate the generalizability of METRO to 3D hand reconstruction in the wild, outperforming existing state-of-the-art methods on FreiHAND dataset. Code and pre-trained models are available at https://github.com/microsoft/MeshTransformer.",
  "full_text": "End-to-End Human Pose and Mesh Reconstruction with Transformers\nKevin Lin Lijuan Wang Zicheng Liu\nMicrosoft\n{keli, lijuanw, zliu}@microsoft.com\nAbstract\nWe present a new method, called MEsh TRansfOrmer\n(METRO), to reconstruct 3D human pose and mesh ver-\ntices from a single image. Our method uses a transformer\nencoder to jointly model vertex-vertex and vertex-joint in-\nteractions, and outputs 3D joint coordinates and mesh ver-\ntices simultaneously. Compared to existing techniques that\nregress pose and shape parameters, METRO does not rely\non any parametric mesh models like SMPL, thus it can be\neasily extended to other objects such as hands. We fur-\nther relax the mesh topology and allow the transformer\nself-attention mechanism to freely attend between any two\nvertices, making it possible to learn non-local relationships\namong mesh vertices and joints. With the proposed masked\nvertex modeling, our method is more robust and effective\nin handling challenging situations like partial occlusions.\nMETRO generates new state-of-the-art results for human\nmesh reconstruction on the public Human3.6M and 3DPW\ndatasets. Moreover, we demonstrate the generalizability of\nMETRO to 3D hand reconstruction in the wild, outperform-\ning existing state-of-the-art methods on FreiHAND dataset.\nCode and pre-trained models are available at https:\n//github.com/microsoft/MeshTransformer.\n1. Introduction\n3D human pose and mesh reconstruction from a single\nimage has attracted a lot of attention because it has many\napplications including virtual reality, sports motion analy-\nsis, neurodegenerative condition diagnosis, etc. It is a chal-\nlenging problem due to complex articulated motion and oc-\nclusions.\nRecent work in this area can be roughly divided into two\ncategories. Methods in the ﬁrst category use a parametric\nmodel like SMPL [29] and learn to predict shape and pose\ncoefﬁcients [14, 26, 39, 22, 24, 34, 44, 23]. Great success\nhas been achieved with this approach. The strong prior en-\ncoded in the parametric model increases its robustness to\nenvironment variations. A drawback of this approach is that\nthe pose and shape spaces are constrained by the limited ex-\n(a) (b) (c)\nFigure 1: METRO learns non-local interactions among\nbody joints and mesh vertices for human mesh reconstruc-\ntion. Given an input image in (a), METRO predicts hu-\nman mesh by taking non-local interactions into consider-\nation. (b) illustrates the attentions between the occluded\nwrist joint and the mesh vertices where brighter color indi-\ncates stronger attention. (c) is the reconstructed mesh.\nemplars that are used to construct the parametric model. To\novercome this limitation, methods in the second category do\nnot use any parametric models [25, 8, 32]. These methods\neither use a graph convolutional neural network to model\nneighborhood vertex-vertex interactions [25, 8], or use 1D\nheatmap to regress vertex coordinates [32]. One limitation\nwith these approaches is that they are not efﬁcient in mod-\neling non-local vertex-vertex interactions.\nResearchers have shown that there are strong correla-\ntions between non-local vertices which may belong to dif-\nferent parts of the body (e.g. hand and foot) [55]. In\ncomputer graphics and robotics, inverse kinematics tech-\nniques [2] have been developed to estimate the internal joint\npositions of an articulated ﬁgure given the position of an\nend effector such as a hand tip. We believe that learning\nthe correlations among body joints and mesh vertices in-\ncluding both short range and long range ones is valuable for\nhandling challenging poses and occlusions in body shape\nreconstruction. In this paper, we propose a simple yet effec-\ntive framework to model global vertex-vertex interactions.\nThe main ingredient of our framework is a transformer.\nRecent studies show that transformer [53] signiﬁcantly\nimproves the performance on various tasks in natural lan-\n1\narXiv:2012.09760v3  [cs.CV]  15 Jun 2021\nguage processing [4, 9, 40, 41]. The success is mainly at-\ntributed to the self-attention mechanism of a transformer,\nwhich is particularly effective in modeling the dependen-\ncies (or interactions) without regard to their distance in both\ninputs and outputs. Given the dependencies, transformer is\nable to soft-search the relevant tokens and performs predic-\ntion based on the important features [4, 53].\nIn this work, we propose METRO, a multi-layer Trans-\nformer encoder with progressive dimensionality reduction,\nto reconstruct 3D body joints and mesh vertices from a\ngiven input image, simultaneously. We design the Masked\nVertex Modeling objective with a transformer encoder ar-\nchitecture to enhance the interactions among joints and ver-\ntices. As shown in Figure 1, METRO learns to discover both\nshort- and long-range interactions among body joints and\nmesh vertices, which helps to better reconstruct the 3D hu-\nman body shape with large pose variations and occlusions.\nExperimental results on multiple public datasets demon-\nstrate that METRO is effective in learning vertex-vertex and\nvertex-joint interactions, and consequently outperforms the\nprior works on human mesh reconstruction by a large mar-\ngin. To the best of our knowledge, METRO is the ﬁrst ap-\nproach that leverages a transformer encoder architecture to\njointly learn 3D human pose and mesh reconstruction from\na single input image. Moreover, METRO is a general frame-\nwork which can be easily applied to predict a different 3D\nmesh, for example, to reconstruct a 3D hand from an input\nimage.\nIn summary, we make the following contributions.\n• We introduce a new transformer-based method, named\nMETRO, for 3D human pose and mesh reconstruction\nfrom a single image.\n• We design the Masked Vertex Modeling objective\nwith a multi-layer transformer encoder to model both\nvertex-vertex and vertex-joint interactions for better re-\nconstruction.\n• METRO achieves new state-of-the-art performance on\nthe large-scale benchmark Human3.6M and the chal-\nlenging 3DPW dataset.\n• METRO is a versatile framework that can be easily re-\nalized to predict a different type of 3D mesh, such as\n3D hand as demonstrated in the experiments. METRO\nachieves the ﬁrst place on FreiHAND leaderboard at\nthe time of paper submission.\n2. Related Works\nHuman Mesh Reconstruction (HMR):HMR is a task of\nreconstructing 3D human body shape, which is an active\nresearch topic in recent years. While pioneer works have\ndemonstrated impressive reconstruction using various sen-\nsors, such as depth sensors [33, 48] or inertial measurement\nunits [20, 54], researchers are exploring to use a monocular\ncamera setting that is more efﬁcient and convenient. How-\never, HMR from a single image is difﬁcult due to complex\npose variations, occlusions, and limited 3D training data.\nPrior studies propose to adopt the pre-trained parametric\nhuman models, i.e., SMPL [29], STAR [35], MANO [43],\nand estimate the pose and shape coefﬁcients of the para-\nmetric model for HMR. Since it is challenging to regress\nthe pose and shape coefﬁcients directly from an input im-\nage, recent works further propose to leverage various human\nbody priors such as human skeletons [26, 39] or segmenta-\ntion maps [34], and explore different optimization strate-\ngies [24, 22, 51, 14] and temporal information [23] to im-\nprove reconstruction.\nOn the other hand, instead of adopting a parametric hu-\nman model, researchers have also proposed approaches to\ndirectly regress 3D human body shape from an input image.\nFor example, researchers have explored to represent human\nbody using a 3D mesh [25, 8], a volumetric space [52], or an\noccupancy ﬁeld [46, 47]. Each of the prior works addresses\na speciﬁc output representation for their target application.\nAmong the literature, the relevant study is GraphCMR [25],\nwhich aims to regress 3D mesh vertices using graph convo-\nlutional neural networks (GCNNs). Moreover, recent pro-\nposed Pose2Mesh [8] is a cascaded model using GCNNs.\nPose2Mesh reconstructs human mesh based on the given\nhuman pose representations.\nWhile GCNN-based methods [8, 25] are designed to\nmodel neighborhood vertex-vertex interactions based on a\npre-speciﬁed mesh topology, it is less efﬁcient in model-\ning longer range interactions. In contrast, METRO models\nglobal interactions among joints and mesh vertices with-\nout being limited by any mesh topology. In addition, our\nmethod learns with self-attention mechanism, which is dif-\nferent from prior studies [8, 25].\nAttentions and Transformers:Recent studies [36, 28, 53]\nhave shown that attention mechanisms improve the per-\nformance on various language tasks. Their key insight is\nto learn the attentions to soft-search relevant inputs that\nare important for predicting an output [4]. Vaswani et\nal. [53] further propose a transformer architecture based\nsolely on attention mechanisms. Transformer is highly par-\nallelized using multi-head self-attention for efﬁcient train-\ning and inference, and leads to superior performance in\nlanguage modeling at scale, as explored in BERT [9] and\nGPT [40, 41, 5].\nInspired by the recent success in neural language ﬁeld,\nthere is a growing interest in exploring the use of trans-\nformer architecture for various vision tasks, such as learn-\ning the pixel distributions for image generation [7, 37] and\nclassiﬁcation [7, 10], or to simplify object detection as a set\n2\nFigure 2: Overview of the proposed framework.Given an input image, we extract an image feature vector using a convo-\nlutional neural network (CNN). We perform position encoding by adding a template human mesh to the image feature vector\nby concatenating the image feature with the 3D coordinates (xi,yi,zi) of every body joint i, and 3D coordinates (xj,yj,zj)\nof every vertex j. Given a set of joint queries and vertex queries, we perform self-attentions through multiple layers of a\ntransformer encoder, and regress the 3D coordinates of body joints and mesh vertices in parallel. We use a progressive di-\nmensionality reduction architecture (right) to gradually reduce the hidden embedding dimensions from layer to layer. Each\ntoken in the ﬁnal layer outputs 3D coordinates of a joint or mesh vertex. Each encoder block has 4 layers and 4 attention\nheads. H denotes the dimension of an image feature vector.\nprediction problem [6]. However, 3D human reconstruction\nhas not been explored along this direction.\nIn this study, we present a multi-layer transformer archi-\ntecture with progressive dimensionality reduction to regress\nthe 3D coordinates of the joints and vertices.\n3. Method\nFigure 2 is an overview of our proposed framework. It\ntakes an image of size 224 ×224 as input, and predicts a\nset of body joints J and mesh vertices V. The proposed\nframework consists of two modules: Convolutional Neural\nNetwork, and Multi-Layer Transformer Encoder. First, we\nuse a CNN to extract an image feature vector from an input\nimage. Next, Multi-Layer Transformer Encoder takes as\ninput the feature vector and outputs the 3D coordinates of\nthe body joint and mesh vertex in parallel. We describe each\nmodule in details as below.\n3.1. Convolutional Neural Network\nIn the ﬁrst module of our framework, we employ a\nConvolutional Neural Network (CNN) for feature extrac-\ntion. The CNN is pre-trained on ImageNet classiﬁcation\ntask [45]. Speciﬁcally, we extract a feature vector X from\nthe last hidden layer. The extracted feature vector X is typ-\nically of dimension 2048. We input the feature vector X to\nthe transformer for the regression task.\nWith this generic design, it allows an end-to-end train-\ning for human pose and mesh reconstruction. Moreover,\ntransformer can easily beneﬁt from large-scale pre-trained\nCNNs, such as HRNets [56]. In our experiments, we con-\nduct analysis on the input features, and discover that high-\nresolution image features are beneﬁcial for transformer to\nregress 3D coordinates of body joints and mesh vertices.\n3.2. Multi-Layer Transformer Encoder with Pro-\ngressive Dimensionality Reduction\nSince we need to output 3D coordinates, we cannot\ndirectly apply the existing transformer encoder architec-\nture [10, 6] because they use a constant dimensionality\nof the hidden embeddings for all the transformer layers.\nInspired by [18] which performs dimentionality reduction\ngradually with multiple blocks, we design a new architec-\nture with a progressive dimensionality reduction scheme.\nAs shown in Figure 2 right, we use linear projections to\nreduce the dimensionality of the hidden embedding after\neach encoder layer. By adding multiple encoder layers, the\nmodel is viewed as performing self-attentions and dimen-\nsionality reduction in an alternating manner. The ﬁnal out-\nput vectors of our transformer encoder are the 3D coordi-\nnates of the joints and mesh vertices.\nAs illustrated in Figure 2 left, the input to the transformer\nencoder are the body joint and mesh vertex queries. In the\nsame spirit as positional encoding [53, 25, 13], we use a\n3\ntemplate human mesh to preserve the positional information\nof each query in the input sequence. To be speciﬁc, we\nconcatenate the image feature vector X ∈ R2048×1 with\nthe 3D coordinates (xi,yi,zi) of every body joint i. This\nforms a set of joint queries QJ = {qJ\n1 ,qJ\n2 ,...,q J\nn}, where\nqJ\ni ∈R2051×1. Similarly, we conduct the same positional\nencoding for every mesh vertex j, and form a set of vertex\nqueries QV = {qV\n1 ,qV\n2 ,...,q V\nm}, where qV\nj ∈R2051×1.\n3.3. Masked Vertex Modeling\nPrior works [9, 49] use the Masked Language Model-\ning (MLM) to learn the linguistic properties of a training\ncorpus. However, MLM aims to recover the inputs, which\ncannot be directly applied to our regression task.\nTo fully activate the bi-directional attentions in our trans-\nformer encoder, we design a Masked Vertex Modeling\n(MVM) for our regression task. We mask some percentages\nof the input queries at random. Different from recovering\nthe masked inputs like MLM [9], we instead ask the trans-\nformer to regress all the joints and vertices.\nIn order to predict an output corresponding to a missing\nquery, the model will have to resort to other relevant queries.\nThis is in spirit similar to simulating occlusions where par-\ntial body parts are invisible. As a result, MVM enforces\ntransformer to regress 3D coordinates by taking other rel-\nevant vertices and joints into consideration, without regard\nto their distances and mesh topology. This facilitates both\nshort- and long-range interactions among joints and vertices\nfor better human body modeling.\n3.4. Training\nTo train the transformer encoder, we apply loss functions\non top of the transformer outputs, and minimize the errors\nbetween predictions and ground truths. Given a dataset\nD = {Ii,¯Vi\n3D, ¯Ji\n3D, ¯Ji\n2D}T\ni=1, where T is the total num-\nber of training images. I ∈Rw×h×3 denotes an RGB im-\nage. ¯V3D ∈ RM×3 denotes the ground truth 3D coordi-\nnates of the mesh vertices and M is the number of vertices.\n¯J3D ∈RK×3 denotes the ground truth 3D coordinates of\nthe body joints and K is the number of joints of a person.\nSimilarly, ¯J2D ∈RK×2 denotes the ground truth 2D coor-\ndinates of the body joints.\nLet V3D denote the output vertex locations, and J3D is\nthe output joint locations, we use L1 loss to minimize the\nerrors between predictions and ground truths:\nLV = 1\nM\nM∑\ni=1\n⏐⏐⏐⏐V3D −¯V3D\n⏐⏐⏐⏐\n1 , (1)\nLJ = 1\nK\nK∑\ni=1\n⏐⏐⏐⏐J3D −¯J3D\n⏐⏐⏐⏐\n1 . (2)\nIt is worth noting that, the 3D joints can also be cal-\nculated from the predicted mesh. Following the common\npractice in literature [8, 22, 25, 24], we use a pre-deﬁned\nregression matrix G∈RK×M , and obtain the regressed 3D\njoints by Jreg\n3D = GV3D. Similar to prior works, we use L1\nloss to optimize Jreg\n3D :\nLreg\nJ = 1\nK\nK∑\ni=1\n⏐⏐⏐⏐Jreg\n3D −¯J3D\n⏐⏐⏐⏐\n1 . (3)\n2D re-projection has been commonly used to enhance\nthe image-mesh alignment [22, 25, 24]. Also, it helps visu-\nalize the reconstruction in an image. Inspired by the prior\nworks, we project the 3D joints to 2D space using the esti-\nmated camera parameters, and minimize the errors between\nthe 2D projections and 2D ground truths:\nLproj\nJ = 1\nK\nK∑\ni=1\n⏐⏐⏐⏐J2D −¯J2D\n⏐⏐⏐⏐\n1 , (4)\nwhere the camera parameters are learned by using a linear\nlayer on top of the outputs of the transformer encoder.\nTo perform large-scale training, it is highly desirable to\nleverage both 2D and 3D training datasets for better gen-\neralization. As explored in literature [34, 22, 25, 24, 23,\n8, 32], we use a mix-training strategy that leverages differ-\nent training datasets, with or without the paired image-mesh\nannotations. Our overall objective is written as:\nL= α×(LV + LJ + Lreg\nJ ) + β×Lproj\nJ , (5)\nwhere α and β are binary ﬂags for each training sample,\nindicating the availability of 3D and 2D ground truths, re-\nspectively.\n3.5. Implementation Details\nOur method is able to process arbitrary sizes of mesh.\nHowever, due to memory constraints of current hardware,\nour transformer processes a coarse mesh: (1) We use a\ncoarse template mesh (431 vertices) for positional encod-\ning, and transformer outputs a coarse mesh; (2) We use\nlearnable Multi-Layer Perceptrons (MLPs) to upsample the\ncoarse mesh to the original mesh (6890 vertices for SMPL\nhuman mesh topology); (3) The transformer and MLPs are\ntrained end-to-end; Please note that the coarse mesh is ob-\ntained by sub-sampling twice to 431 vertices with a sam-\npling algorithm [42]. As discussed in the literature [25],\nthe implementation of learning a coarse mesh followed by\nupsampling is helpful to reduce computation. It also helps\navoid redundancy in original mesh (due to spatial locality\nof vertices), which makes training more efﬁcient.\n4. Experimental Results\nWe ﬁrst show that our method outperforms the previous\nstate-of-the-art human mesh reconstruction methods on Hu-\nman3.6M and 3DPW datasets. Then, we provide ablation\n4\n3DPW Human3.6M\nMethod MPVE ↓ MPJPE ↓ PA-MPJPE ↓ MPJPE ↓ PA-MPJPE ↓\nHMR [22] − − 81.3 88 .0 56 .8\nGraphCMR [25] − − 70.2 − 50.1\nSPIN [24] 116.4 − 59.2 − 41.1\nPose2Mesh [8] − 89.2 58 .9 64 .9 47 .0\nI2LMeshNet [32] − 93.2 57 .7 55 .7 41 .1\nVIBE [23] 99.1 82 .0 51 .9 65 .6 41 .4\nMETRO (Ours) 88.2 77.1 47.9 54.0 36.7\nTable 1: Performance comparison with the state-of-the-art methods on 3DPW and Human3.6M datasets.\nstudy and insights for the non-local interactions and model\ndesign. Finally, we demonstrate the generalizability of our\nmodel on hand reconstruction.\n4.1. Datasets\nFollowing the literature [34, 22, 25, 24, 23, 8, 32], we\nconduct mix-training using 3D and 2D training data. We\ndescribe each dataset below.\nHuman3.6M [21] is a large-scale dataset with 2D and 3D\nannotations. Each image has a subject performing a differ-\nent action. Due to the license issue, the groundtruth 3D\nmeshes are not available. Thus, we use the pseudo 3D\nmeshes provided in [8, 32] for training. The pseudo labels\nare created by model ﬁtting with SMPLify-X [38]. For eval-\nuation, we use the groundtruth 3D pose labels provided in\nHuman3.6M for fair comparison. Following the common\nsetting [50, 25, 22], we train our models using subjects S1,\nS5, S6, S7 and S8. We test the models using subjects S9\nand S11.\n3DPW [54] is an outdoor-image dataset with 2D and 3D\nannotations. The training set consists of 22K images, and\nthe test set has 35K images. Following the previous state-\nof-the-arts [23], we use 3DPW training data when conduct-\ning experiments on 3DPW.\nUP-3D [26] is an outdoor-image dataset. Their 3D annota-\ntions are created by model ﬁtting. The training set has 7K\nimages.\nMuCo-3DHP [31] is a synthesized dataset based on MPI-\nINF-3DHP dataset [30]. It composites the training data with\na variety of real-world background images. It has 200K\ntraining images.\nCOCO [27] is a large-scale dataset with 2D annotations.\nWe also use the pseudo 3D mesh labels provided in [24],\nwhich are ﬁtted with SMPLify-X [38].\nMPII [1] is an outdoor-image dataset with 2D pose labels.\nThe training set consists of 14Kimages.\nFreiHAND [61] is a 3D hand dataset. The training set\nconsists of 130K images, and the test set has 4K images.\nWe demonstrate the generalizability of our model on this\ndataset. We use the provided set for training, and conduct\nevaluation on their online server.\n4.2. Evaluation Metrics\nWe report results using three standard metrics as below.\nThe unit for the three metrics is millimetter (mm).\nMPJPE: Mean-Per-Joint-Position-Error (MPJPE) [21] is a\nmetric for evaluating human 3D pose [22, 24, 8]. MPJPE\nmeasures the Euclidean distances between the ground truth\njoints and the predicted joints.\nPA-MPJPE: PA-MPJPE, or Reconstruction Error [60], is\nanother metric for this task. It ﬁrst performs a 3D align-\nment using Procrustes analysis (PA) [12], and then com-\nputes MPJPE. PA-MPJPE is commonly used for evaluating\n3D reconstruction [60] as it measures the errors of the re-\nconstructed structure without regard to the scale and rigid\npose (i.e., translations and rotations).\nMPVE: Mean-Per-Vertex-Error (MPVE) [39] measures\nthe Euclidean distances between the ground truth vertices\nand the predicted vertices.\n4.3. Main Results\nWe compare METRO with the previous state-of-the-art\nmethods on 3DPW and Human3.6M datasets. Following\nthe literature [23, 24, 22, 25], we conduct mix-training us-\ning 3D and 2D training data. The results are shown in\nTable 1. Our method outperforms prior works on both\ndatasets.\nFirst of all, we are interested in how transformer works\nfor in-the-wild reconstruction of 3DPW. As shown in the\nleft three columns of Table 1, our method outperforms\nVIBE [23], which was the state-of-the-art method on this\ndataset. It is worth noting that, VIBE is a video-based ap-\nproach, whereas our method is an image-based approach.\nIn addition, we evaluate the performance on the in-door\nscenario of Human3.6M. We follow the setting in the prior\narts [24, 32], and train our model without using 3DPW data.\n5\nInput R-Wrist R-Elbow L-Knee L-Ankle Head Output\nFigure 3: Qualitative results of our method. Given an input image (left), METRO takes non-local interactions among joints\nand vertices into consideration for human mesh reconstruction (right). We visualize the self-attentions between a speciﬁed\njoint and all other vertices, where brighter color indicates stronger attention. We observe that METRO discovers rich, input-\ndependent interactions among the joints and vertices.\nThe results are shown in the right two columns of Table 1.\nOur method achieves better reconstruction performance, es-\npecially on PA-MPJPE metric.\nThe two datasets Human3.6M and 3DPW have different\nchallenges. The scenes in 3DPW have more severe occlu-\nsions. The scenes in Human3.6 are simpler and the chal-\nlenge is more on how to accurately estimate body shape.\nThe fact that METRO works well on both datasets demon-\nstrates that it is both robust to occlusions and capable of\naccurate body shape regression.\n4.4. Ablation Study\nEffectiveness of Masked Vertex Modeling:Since we de-\nsign a Masked Vertex Modeling objective for transformer,\none interesting question is whether the objective is useful.\nTable 2 shows the ablation study on Human3.6M. We ob-\nserve that Masked Vertex Modeling signiﬁcantly improves\nthe results. Moreover, we study how many percentage of\nquery tokens should be masked. We vary the maximum\nmasking percentage, and Table 3 shows the comparison. As\nwe increase the number of masked queries for training, it\nMPJPE ↓ PA-MPJPE ↓\nw/o MVM 61.0 39 .1\nw/ MVM 54.0 36.7\nTable 2: Ablation study of the Masked Vertex Modeling\n(MVM) objective, evaluated on Human3.6M.\nMax Percentage 0% 10% 20% 30% 40% 50%\nPA-MPJPE 39.1 37 .6 37 .5 36.7 38.2 37 .3\nTable 3: Ablation study of the Masked Vertex Modeling ob-\njective using different percentages of masked queries, eval-\nuated on Human3.6M. The variable n% indicates we mask\nrandomly from 0% to n% of input queries.\nimproves the performance. However, the impact becomes\nless prominent if we mask more than 30% of input queries.\nThis is because large numbers of missing queries would\nmake the training more difﬁcult.\n6\nMethod PA-MPVPE ↓ PA-MPJPE ↓ F@5 mm ↑ F@15 mm ↑\nHasson et al [22] 13.2 − 0.436 0 .908\nBoukhayma et al. [25] 13.0 − 0.435 0 .898\nFreiHAND [24] 10.7 − 0.529 0 .935\nPose2Mesh [8] 7.8 7 .7 0 .674 0 .969\nI2LMeshNet [32] 7.6 7 .4 0 .681 0 .973\nMETRO (Ours) 6.3 6.5 0.731 0.984\nTable 4: Performance comparison with the state-of-the-art methods, evaluated on FreiHAND online server. METRO outper-\nforms previous state-of-the-art approaches by a large margin.\nNon-local Interactions: To further understand the effect\nof METRO in learning interactions among joints and mesh\nvertices, we conduct analysis on the self-attentions in our\ntransformer.\nFigure 3 shows the visualization of the self-attentions\nand mesh reconstruction. For each row in Figure 3, we show\nthe input image, and the self-attentions between a speciﬁed\njoint and all the mesh vertices. The brighter color indicates\nstronger attention. At the ﬁrst row, the subject is severely\noccluded and the right body parts are invisible. As we\npredict the location of right wrist, METRO attends to rel-\nevant non-local vertices, especially those on the head and\nleft hand. At the bottom row, the subject is heavily bended.\nFor the head position prediction, METRO attends to the feet\nand hands (6th column at the bottom row). It makes sense\nintuitively since the hand and foot positions provide strong\ncues to the body pose and subsequently the head position.\nMoreover, we observe the model performs self-attentions\nin condition to the input image. As shown in the second\nrow of Figure 3, when predicting the location of right wrist,\nMETRO focuses more on the right foot which is different\nfrom the attentions in the other three rows.\nWe further conduct quantitative analysis on the non-\nlocal interactions. We randomly sample 5000 images from\n3DPW test set, and estimate an overall self-attention map.\nIt is the average attention weight of all attention heads at the\nlast transformer layer. We visualize the interactions among\n14 body joints and431 mesh vertices in Figure 4. Each pixel\nshows the intensity of self-attention, where darker color in-\ndicates stronger attention. Note that the ﬁrst 14 columns\nare the body joints, and the rest of them represent the mesh\nvertices. We observe that METRO pays strong attentions to\nthe vertices on the lower arms and the lower legs. This is\nconsistent with the inverse kinematics literature [2] where\nthe interior joints of a linked ﬁgure can be estimated from\nthe position of an end effector.\nInput Representations: We study the behaviour of our\ntransformer architecture by using different CNN backbones.\nWe use ResNet50 [16] and HRNet [56] variations for this\nexperiment. All backbones are pre-trained on the 1000-\nFigure 4: Visualization of self-attentions among body joints\nand mesh vertices. The x-axis and y-axis correspond to the\nqueries and the predicted outputs, respectively. The ﬁrst 14\ncolumns from the left correspond to the body joints. The\nrest of columns correspond to the mesh vertices. Each row\nshows the attention weightwi,j of the j-th query for thei-th\noutput. Darker color indicates stronger attention.\nBackbone MPJPE ↓ PA-MPJPE ↓\nResNet50 56.5 40 .6\nHRNet-W40 55.9 38 .5\nHRNet-W64 54.0 36.7\nTable 5: Analysis on different backbones, evaluated on Hu-\nman3.6M. All backbones are pre-trained on ImageNet. We\nobserve that increasing the number of ﬁlters in the high res-\nolution feature maps of HRNet is beneﬁcial to mesh regres-\nsion.\n7\nInput Wrist Thumb Index Middle Ring Pinky Output\nFigure 5: Qualitative results of our method on FreiHAND test set. We visualize the self-attentions between a speciﬁed joint\nand all the mesh vertices, where brighter color indicates stronger attention. METRO is a versatile framework that can be\neasily extended to 3D hand reconstruction.\nclass image classiﬁcation task of ImageNet [45]. For each\nbackbone, we extract a global image feature vector X ∈\nR2048×1, and feed it into the transformer. In Table 5, we\nobserve our transformer achieves competitive performance\nwhen using a ResNet50 backbone. As we increase the chan-\nnels of the high-resolution feature maps in HRNet, we ob-\nserve further improvement.\nGeneralization to 3D Hand in-the-wild:METRO is ca-\npable of predicting arbitrary joints and vertices, without the\ndependencies on adjacency matrix and parametric coefﬁ-\ncients. Thus, METRO is highly ﬂexible and general for\nmesh reconstruction of other objects. To demonstrate this\ncapability, we conduct experiment on FreiHAND [61]. We\ntrain our model on FreiHAND from scratch, and evaluate\nresults on FreiHAND online server. Table 4 shows the com-\nparison with the prior works. METRO outperforms previ-\nous state-of-the-art methods by a large margin. Without us-\ning any external training data, METRO achieved the ﬁrst\nplace on FreiHAND leaderboard at the time of paper sub-\nmission1.\nFigure 5 shows our qualitative results with non-local in-\nteractions. In the appendix, we provide further analysis on\nthe 3D hand joints, and show that the self-attentions learned\nin METRO are consistent with inverse kinematics [2].\n5. Conclusion\nWe present a simple yet effective mesh transformer\nframework to reconstruct human pose and mesh from a sin-\ngle input image. We propose the Masked Vertex Model-\ning objective to learn non-local interactions among body\njoints and mesh vertices. Experimental results show that,\nour method advances the state-of-the-art performance on\n3DPW, Human3.6M, and FreiHAND datasets.\nA detailed analysis reveals that the performance im-\nprovements are mainly attributed to the input-dependent\nnon-local interactions learned in METRO, which enables\n1According to the ofﬁcial FreiHAND leaderboard in November 2020:\nhttps://competitions.codalab.org/competitions/21238\n8\npredictions based on important joints and vertices, regard-\nless of the mesh topology. We further demonstrate the gen-\neralization capability of the proposed approach to 3D hand\nreconstruction.\nReferences\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2d human pose estimation: New benchmark\nand state of the art analysis. In CVPR, 2014. 5\n[2] Andreas Aristidou, Joan Lasenby, Yiorgos Chrysanthou, and\nAriel Shamir. Inverse kinematics techniques in computer\ngraphics: A survey. In Computer Graphics Forum, 2018.\n1, 7, 8, 12\n[3] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Ex-\nploiting temporal context for 3d human pose estimation in\nthe wild. In CVPR, 2019. 11\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In ICLR, 2015. 2\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 3\n[7] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Hee-\nwoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020. 2\n[8] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.\nPose2mesh: Graph convolutional network for 3d human pose\nand mesh recovery from a 2d human pose. In ECCV, 2020.\n1, 2, 4, 5, 7, 11, 17\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 2, 4,\n11\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[11] Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence\nChen, Jana Kosecka, and Ziyan Wu. Hierarchical kinematic\nhuman mesh recovery. ECCV, 2020. 11\n[12] John C Gower. Generalized procrustes analysis. Psychome-\ntrika, 1975. 5, 13\n[13] Thibault Groueix, Matthew Fisher, Vladimir G Kim,\nBryan C Russell, and Mathieu Aubry. 3d-coded: 3d cor-\nrespondences by deep deformation. In ECCV, 2018. 3\n[14] Peng Guan, Alexander Weiss, Alexandru O Balan, and\nMichael J Black. Estimating human shape and pose from\na single image. In ICCV, 2009. 1, 2\n[15] Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic\n3d human reconstruction in-the-wild. In CVPR, 2019. 11\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 7\n[17] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.\nEpipolar transformers. In CVPR, 2020. 11\n[18] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing\nthe dimensionality of data with neural networks. Science,\n2006. 3, 12\n[19] Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan. Hand-\ntransformer: Non-autoregressive structured modeling for 3d\nhand pose estimation. In ECCV, 2020. 11\n[20] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.\nBlack, Otmar Hilliges, and Gerard Pons-Moll. Deep inertial\nposer learning to reconstruct human pose from sparseinertial\nmeasurements in real time. ACM Transactions on Graphics,\n37(6):185:1–185:15, Nov. 2018. 2\n[21] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3. 6m: Large scale datasets and pre-\ndictive methods for 3d human sensing in natural environ-\nments. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 36(7):1325–1339, 2014. 5\n[22] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, 2018. 1, 2, 4, 5, 7, 11\n[23] Muhammed Kocabas, Nikos Athanasiou, and Michael J.\nBlack. Vibe: Video inference for human body pose and\nshape estimation. In CVPR, 2020. 1, 2, 4, 5, 11\n[24] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3d human pose\nand shape via model-ﬁtting in the loop. In ICCV, 2019. 1, 2,\n4, 5, 7, 11\n[25] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-\nilidis. Convolutional mesh regression for single-image hu-\nman shape reconstruction. In CVPR, 2019. 1, 2, 3, 4, 5, 7,\n11, 12, 14\n[26] Christoph Lassner, Javier Romero, Martin Kiefel, Federica\nBogo, Michael J Black, and Peter V Gehler. Unite the peo-\nple: Closing the loop between 3d and 2d human representa-\ntions. In CVPR, 2017. 1, 2, 5\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 5\n[28] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and\nMaosong Sun. Neural relation extraction with selective at-\ntention over instances. In ACL, 2016. 2\n[29] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J Black. Smpl: A skinned\nmulti-person linear model. ACM Transactions on Graphics,\n34(6):248, 2015. 1, 2\n[30] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3d human pose estimation in the wild\nusing improved cnn supervision. In 3DV, 2017. 5\n[31] Dushyant Mehta, Oleksandr Sotnychenko, Franziska\nMueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll,\nand Christian Theobalt. Single-shot multi-person 3d pose\nestimation from monocular rgb. In 3DV, 2018. 5\n9\n[32] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-\nto-lixel prediction network for accurate 3d human pose and\nmesh estimation from a single rgb image. InECCV, 2020. 1,\n4, 5, 7, 11, 12, 13, 14\n[33] Richard A Newcombe, Shahram Izadi, Otmar Hilliges,\nDavid Molyneaux, David Kim, Andrew J Davison, Push-\nmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew W\nFitzgibbon. Kinectfusion: Real-time dense surface mapping\nand tracking. In ISMAR, 2011. 2\n[34] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-\nter Gehler, and Bernt Schiele. Neural body ﬁtting: Unifying\ndeep learning and model based human pose and shape esti-\nmation. In 3DV, 2018. 1, 2, 4, 5\n[35] Ahmed A A Osman, Timo Bolkart, and Michael J. Black.\nSTAR: A spare trained articulated human body regressor. In\nECCV, 2020. 2\n[36] Ankur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and Jakob\nUszkoreit. A decomposable attention model for natural lan-\nguage inference. arXiv preprint arXiv:1606.01933, 2016. 2\n[37] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018. 2\n[38] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and\nMichael J. Black. Expressive body capture: 3d hands, face,\nand body from a single image. In CVPR, 2019. 5\n[39] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas\nDaniilidis. Learning to estimate 3d human pose and shape\nfrom a single color image. In CVPR, 2018. 1, 2, 5\n[40] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. Technical report, 2018. 2\n[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. Technical report, 2019. 2\n[42] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and\nMichael J Black. Generating 3d faces using convolutional\nmesh autoencoders. In ECCV, 2018. 4\n[43] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\nEmbodied hands: Modeling and capturing hands and bod-\nies together. SIGGRAPH Asia, 2017. 2\n[44] Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, and Chen\nChange Loy. Delving deep into hybrid annotations for 3d\nhuman recovery in the wild. In ICCV, 2019. 1\n[45] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International Journal of\nComputer Vision, 2015. 3, 8\n[46] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. arXiv preprint arXiv:1905.05172, 2019. 2\n[47] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In CVPR, 2020. 2\n[48] Daeyun Shin, Zhile Ren, Erik B Sudderth, and Charless C\nFowlkes. 3d scene reconstruction with multi-layer depth and\nepipolar transformers. In ICCV, 2019. 2\n[49] Wilson L Taylor. “cloze procedure”: A new tool for measur-\ning readability. Journalism quarterly, 1953. 4\n[50] Bugra Tekin, Artem Rozantsev, Vincent Lepetit, and Pascal\nFua. Direct prediction of 3d body poses from motion com-\npensated sequences. In CVPR, 2016. 5\n[51] Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, and Katerina\nFragkiadaki. Self-supervised learning of motion capture. In\nNeurIPS, 2017. 2\n[52] Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin\nYumer, Ivan Laptev, and Cordelia Schmid. Bodynet: V olu-\nmetric inference of 3d human body shapes. In ECCV, 2018.\n2\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 3, 12\n[54] Timo von Marcard, Roberto Henschel, Michael Black, Bodo\nRosenhahn, and Gerard Pons-Moll. Recovering accurate 3d\nhuman pose in the wild using imus and a moving camera. In\nECCV, 2018. 2, 5\n[55] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan.\nMining actionlet ensemble for action recognition with depth\ncameras. In CVPR, 2012. 1\n[56] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep\nhigh-resolution representation learning for visual recogni-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2019. 3, 7\n[57] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. Denserac: Joint\n3d pose and shape estimation by dense render-and-compare.\nIn ICCV, 2019. 11\n[58] Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, and Xi-\naogang Wang. 3d human mesh regression with dense corre-\nspondence. In CVPR, 2020. 11\n[59] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Object-\noccluded human shape and pose estimation from a single\ncolor image. In CVPR, 2020. 11\n[60] Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyri-\ndon Leonardos, Konstantinos G Derpanis, and Kostas Dani-\nilidis. Monocap: Monocular human motion capture using a\ncnn coupled with a geometric prior. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2018. 5\n[61] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan\nRussell, Max Argus, and Thomas Brox. Freihand: A dataset\nfor markerless capture of hand pose and shape from single\nrgb images. In ICCV, 2019. 5, 8\n10\nAPPENDICES\n3DPW Human3.6M\nMethod MPVE ↓ MPJPE ↓ PA-MPJPE ↓ MPJPE ↓ PA-MPJPE ↓\nHMR [22] − − 81.3 88 .0 56 .8\nGraphCMR [25] − − 70.2 − 50.1\nSPIN [24] 116.4 − 59.2 − 41.1\nPose2Mesh [8] − 89.2 58 .9 64 .9 47 .0\nI2LMeshNet [32] − 93.2 57 .7 55 .7 41 .1\nVIBE [23] 99.1 82 .0 51 .9 65 .6 41 .4\nHoloPose [15] − − − 60.2 46 .5\nArnab et al.[3] − − 72.2 77 .8 54 .3\nDenseRaC [57] − − − − 48.0\nZhang et al.[59] − − 72.2 − 41.7\nZeng et al.[58] − − − 60.6 39 .3\nHKMR [11] − − − 59.6 43 .2\nMETRO (Ours) 88.2 77.1 47.9 54.0 36.7\nFigure 6: Adding references (HKMR [11], Zenget al.[58], Zhang et al.[59], DenseRaC [57], Arnabet al.[3], HoloPose [15])\nto the comparisons on 3DPW and Human 3.6M datasets.\nA. Additional Reference\nWe would like to add additional references (HKMR [11],\nArnab et al. [3], Zeng et al. [58], Zhang et al. [59],\nDenseRaC [57], HoloPose [15]). Among the new refer-\nences, HKMR [11] regresses SMPL parameters by leverag-\ning a pre-speciﬁed hierarchical kinematic structure that con-\nsists of a root chain and ﬁve child chains corresponding to\n5 end effectors (head, left/right arms, left/right legs). Holo-\nPose [15] estimates rotation angles of body joints, and uses\nit as the prior to guide part-based human mesh reconstruc-\ntion. Zeng et al.[58] designs the continuous UV map to pre-\nserve neighboring relationships of the mesh vertices. Zhang\net al.[59] addresses the occlusion scenario by formulating\nthe task as a UV-map inpainting problem. Since 3DPW is\na relatively new benchmark, most literature reported results\non Human3.6M, but not 3DPW. We have added their Hu-\nman3.6M results in Table 6. As we can see, our method\noutperforms all of the prior works by a large margin.\nRecently, researchers are exploring the transformer mod-\nels for other 3D vision topics, such as multi-view human\npose estimation [17] and hand pose estimation based on\npoint could [19]. We encourage the readers to undertake\nthese studies for further explorations.\nB. Implementation Details and Computation\nResource\nWe develop METRO using PyTorch and Huggingface\ntransformer library. We conduct training on a machine\nequipped with 8 NVIDIA V100 GPUs (32GB RAM) and\nwe use batch size 32. Each epoch takes 32 minutes and we\ntrain for 200 epochs. Overall, our training takes 5 days. We\nuse the Adam optimizer and a step-wise learning rate de-\ncay. We set the initial learning rate as 1 ×10−4 for both\ntransformer and CNN backbone. The learning rate is de-\ncayed by a factor of 10 at the 100th epoch. Our multi-layer\ntransformer encoder is randomly initialized, and the CNN\nbackbone is initialized with ImageNet pre-trained weights.\nFollowing [25, 24], we apply standard data augmentation\nduring training.\nWe evaluate the runtime speed of our model using a ma-\nchine equipped with a single NVIDIA P100 GPU (16GB\nRAM). Our runtime speed is about 12 fps using batch size\n1. The runtime speed can be accelerated to around 24 fps\nusing batch size 32. Table 7 shows the details of each mod-\nule in METRO.\nFor our masked vertex modeling, following BERT [9],\nwe implement it by using a pre-deﬁned special [MASK] to-\nken (2051-D ﬂoating value vector in our case) to replace the\nrandomly selected input queries.\n11\nModel Dimensionality Reduction Scheme PA-MPJPE ↓\nTransformer [53] (H+ 3) →3 208.7\nMETRO (H+ 3) →H/2 →3 192.1\nMETRO (H+ 3) →H/2 →H/4 →3 43.8\nMETRO (H+ 3) →H/2 →H/4 →H/8 →3 36.7\nTable 6: Performance comparison of different dimentionality reduction schemes, evaluated on Human3.6M validation set.\nPlease note that all the transformer variants have the same total number of hidden layers (12 layers) for fair comparison.\nH=2048.\nCNN (HRNet-W64) Transformer\n# Parameters 128M 102M\nInference time 52.05 ms 28.22 ms\nTable 7: Number of parameters and inference time per im-\nage. The runtime speed is estimated by using batch size 1.\nPositional Encoding PA-MPJPE ↓\nSinusoidal [53] 37.5\nOurs 36.7\nTable 8: Comparison of different positional encoding\nschemes, evaluated on Human3.6M validation set.\nC. Progressive Dimensionality Reduction\nSince we gradually reduce the hidden sizes in the trans-\nformer architecture, one interesting question is whether\nsuch a progressive dimensionality reduction scheme is use-\nful. We have conducted an ablation study on different\nschemes, and Table 6 shows the comparison. In Table 6,\nthe row “(H+3) →3” corresponds to a baseline using one\nlinear projection H + 3 to 3. The result is poor. Row\n“(H+3)→H/2→3” is another baseline which keeps a smaller\ndimension throughout the network. The result is also bad.\nOur ﬁnding is that large-step (steep) dimension reduction\ndoes not work well for 3D mesh regression. Our progressive\nscheme is inspired by [18] which performed dimensionality\nreduction gradually with multiple blocks.\nD. Positional Encoding\nSince our positional encoding is different from the con-\nventional one, one may wonder what if we use sinusoidal\nfunctions [53] but not a template mesh. We have com-\npared our method with the conventional positional encod-\ning which uses sinusoidal functions, and Table 8 shows the\nresults. We see that using sinusoidal functions is slightly\nworse. This is probably because directly encoding coordi-\nnates makes it more efﬁcient to learn 3D coordinate regres-\nsion.\nE. Qualitative Results\nFigure 8 shows a qualitative comparison with the previ-\nous image-based state-of-the-art methods [32, 25] in chal-\nlenging scenarios. These methods only use a single frame as\ninput. In the ﬁrst row, the subject is heavily bending. Prior\nworks have difﬁculty in reconstructing a correct body shape\nfor the subject. In contrast, our method reconstructs a rea-\nsonable human mesh with correct pose. In the second row,\nthe subject is occluded by the vehicle. We see that prior\nworks are sensitive to the occlusions, and failed to gener-\nate correct human mesh. In contrast, our method performs\nmore robustly in this occlusion scenario. In the bottom row,\nthe subject is sitting on the chair. Our method reconstructed\na better human mesh compared to the previous state-of-the-\nart methods.\nFigure 9 shows the qualitative results of our method on\n3D hand reconstruction. Without making any modiﬁca-\ntions to the network architecture, our method works well\nfor hands and is robust to occlusions. It demonstrates our\nmethod’s advantage that it can be easily extended to other\ntypes of objects.\nF. Non-local Interactions of Hand Joints\nWe further conduct quantitative analysis on the non-local\ninteractions among hand joints learned by our model. We\nrandomly sample 1000 samples from FreiHAND test set,\nand estimate an overall self-attention map. Figure 10 shows\nthe interactions among 21 hand joints. There are 21 rows\nand 21 columns. Pixel ( i, j) represents the amount of at-\ntention that hand joint i attends to joint j. A darker color\nindicates stronger attention. We can see that the wrist joint\n(column 0) receives strong attentions from all the joints. In-\ntuitively wrist joint acts like a “root” of the hand’s kinemat-\nics tree. In addition, columns 4, 8, 12, and 16 receive strong\nattentions from many other joints. These columns corre-\nspond to the tips of thumb, index, middle, and ring ﬁngers,\nrespectively. These ﬁnger tips are end effectors [2] and they\ncan be used to estimate the interior joint positions in inverse\nkinematics. On the other hand, the tip of pinky only receives\n12\nMethod PA-MPVPE ↓ PA-MPJPE ↓ F@5 mm ↑ F@15 mm ↑\nI2LMeshNet [32] 7.6 7 .4 0 .681 0 .973\nMETRO 6.7 6 .8 0 .717 0 .981\nMETRO + Test time augmentation 6.3 6.5 0.731 0.984\nTable 9: Effectiveness of test-time augmentation on FreiHAND test set.\nattentions from the joints on the ring ﬁnger. This is proba-\nbly because pinky is not as active as the other ﬁngers and its\nmotion is more correlated to the ring ﬁnger compared to the\nother ﬁngers.\nG. Test-Time Augmentation for FreiHAND\nWe have explored test-time augmentation in our Frei-\nHAND experiments. We do not use test-time augmentation\nin Human3.6M and 3DPW experiments. Given a test image,\nwe apply different rotations and scaling to the test image.\nWe then feed these transformed images to our model, and\naverage the results to obtain the ﬁnal output mesh. In or-\nder to compute an average 3D mesh, we perform 3D align-\nment (i.e., Procrustes analysis [12]) to normalize the output\nmeshes. In Table 9, we empirically observed that such an\nimplementation is helpful to improve 0.4 PA-MPVPE on\nFreiHAND test set.\nH. Limitations\nAs METRO is a data-driven approach, it may not per-\nform well when the testing sample is very different from\nthe training data. We show some example failure cases in\nFigure 7 where the test images are downloaded from the In-\nternet. First, as shown in Figure 7(a), we observed that if the\ntarget body shape is very different from the existing training\ndata (i.e., SMPL style data), our method may not faithfully\nreconstruct the muscles of the subject. Secondly, as shown\nin Figure 7(b), our model fails to reconstruct a correct mesh\ndue to the fact that there is no glove data in the training set.\nFinally, the proposed method is a mesh-speciﬁc approach.\nIf we were to apply our pre-trained right-hand model to the\nleft-hand images, as can be seen in Figure 7(c), our model\nwill not work well. How to develop a uniﬁed model for\ndifferent 3D objects is an interesting future work.\n(a) Example1\nInput Output\n(b) Example2\nInput Output\n(c) Example3\nInput Output\nFigure 7: Failure cases. METRO may not perform well\nwhen the testing sample is very different from the training\ndata.\n13\nInput GraphCMR [25] I2L-M [32] Ours\nFigure 8: Qualitative comparison between our method and other single-frame-based approaches. Our method is more robust\nto challenging poses and occlusions.\n14\nFigure 9: Qualitative results of our method on FreiHAND test set. Our method can be easily extended to reconstruct 3D hand\nmesh.\n15\nFigure 10: Visualization of self-attentions among hand joints. There are 21 rows and 21 columns corresponding to 21 hand\njoints. Pixel (i, j) represents the amount of attention that joint iattends to joint j. A darker color indicates stronger attention.\nThe deﬁnition of the 21 joints is shown in Figure 11.\n16\nFigure 11: Deﬁnition of the hand joints. The illustration is adapted from [8].\n17",
  "topic": "Vertex (graph theory)",
  "concepts": [
    {
      "name": "Vertex (graph theory)",
      "score": 0.6905523538589478
    },
    {
      "name": "Computer science",
      "score": 0.6834421753883362
    },
    {
      "name": "Generalizability theory",
      "score": 0.633731484413147
    },
    {
      "name": "Triangle mesh",
      "score": 0.6255810260772705
    },
    {
      "name": "Transformer",
      "score": 0.5813149213790894
    },
    {
      "name": "Polygon mesh",
      "score": 0.5028910040855408
    },
    {
      "name": "Encoder",
      "score": 0.48051270842552185
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4593823552131653
    },
    {
      "name": "3d model",
      "score": 0.45896175503730774
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.4456018805503845
    },
    {
      "name": "Mesh networking",
      "score": 0.43917202949523926
    },
    {
      "name": "Parametric statistics",
      "score": 0.42965465784072876
    },
    {
      "name": "Algorithm",
      "score": 0.39799004793167114
    },
    {
      "name": "Theoretical computer science",
      "score": 0.359282910823822
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.21294915676116943
    },
    {
      "name": "Mathematics",
      "score": 0.16485491394996643
    },
    {
      "name": "Engineering",
      "score": 0.13033533096313477
    },
    {
      "name": "Graph",
      "score": 0.12135180830955505
    },
    {
      "name": "Combinatorics",
      "score": 0.10437843203544617
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Wireless",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": []
}