{
  "title": "Development of Deep Transformer-Based Models for Long-Term Prediction of Transient Production of Oil Wells",
  "url": "https://openalex.org/W3205128731",
  "year": 2021,
  "authors": [
    {
      "id": null,
      "name": "Abdrakhmanov, Ildar",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4317116262",
      "name": "Kanin, Evgenii",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4317116264",
      "name": "Boronin, Sergei",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2744882647",
      "name": "Burnaev, Evgeny",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4287177879",
      "name": "Osiptsov, Andrei",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3027568991",
    "https://openalex.org/W6683258052",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2898871780",
    "https://openalex.org/W2014713772",
    "https://openalex.org/W2025611629",
    "https://openalex.org/W2910850561",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6773071120",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W6767939250",
    "https://openalex.org/W6771603238"
  ],
  "abstract": "Abstract We propose a novel approach to data-driven modeling of a transient production of oil wells. We apply the transformer-based neural networks trained on the multivariate time series composed of various parameters of oil wells measured during their exploitation. By tuning the machine learning models for a single well (ignoring the effect of neighboring wells) on the open-source field datasets, we demonstrate that transformer outperforms recurrent neural networks with LSTM/GRU cells in the forecasting of the bottomhole pressure dynamics. We apply the transfer learning procedure to the transformer-based surrogate model, which includes the initial training on the dataset from a certain well and additional tuning of the model's weights on the dataset from a target well. Transfer learning approach helps to improve the prediction capability of the model. Next, we generalize the single-well model based on the transformer architecture for multiple wells to simulate complex transient oilfield-level patterns. In other words, we create the global model which deals with the dataset, comprised of the production history from multiple wells, and allows for capturing the well interference resulting in more accurate prediction of the bottomhole pressure or flow rate evolutions for each well under consideration. The developed instruments for a single-well and oilfield-scale modelling can be used to optimize the production process by selecting the operating regime and submersible equipment to increase the hydrocarbon recovery. In addition, the models can be helpful to perform well-testing avoiding costly shut-in operations.",
  "full_text": "DEVELOPMENT OF DEEP TRANSFORMER -BASED MODELS FOR\nLONG -TERM PREDICTION OF TRANSIENT PRODUCTION OF OIL\nWELLS\nIldar R. Abdrakhmanov\nIldar.Abdrakhmanov@skoltech.ru\nEvgenii A. Kanin\ne.kanin@skoltech.ru\nSergei A. Boronin\ns.boronin@skoltech.ru\nEvgeny V . Burnaev\ne.burnaev@skoltech.ru\nAndrei A. Osiptsov\na.osiptsov@skoltech.ru\nSkolkovo Institute of Science and Technology (Skoltech)\nBolshoy Boulevard 30, bld. 1, Moscow, Russia, 121205\nOctober 13, 2021\nABSTRACT\nWe propose a novel approach to data-driven modeling of a transient production of oil wells. We\napply the transformer-based neural networks trained on the multivariate time series composed of\nvarious parameters of oil wells measured during their exploitation. By tuning the machine learning\nmodels for a single well (ignoring the effect of neighboring wells) on the open-source ﬁeld datasets,\nwe demonstrate that transformer outperforms recurrent neural networks with LSTM/GRU cells in\nthe forecasting of the bottomhole pressure dynamics. We apply the transfer learning procedure to\nthe transformer-based surrogate model, which includes the initial training on the dataset from a\ncertain well and additional tuning of the model’s weights on the dataset from a target well. Transfer\nlearning approach helps to improve the prediction capability of the model. Next, we generalize\nthe single-well model based on the transformer architecture for multiple wells to simulate complex\ntransient oilﬁeld-level patterns. In other words, we create the global model which deals with the\ndataset, comprised of the production history from multiple wells, and allows for capturing the well\ninterference resulting in more accurate prediction of the bottomhole pressure or ﬂow rate evolutions\nfor each well under consideration. The developed instruments for a single-well and oilﬁeld-scale\nmodelling can be used to optimize the production process by selecting the operating regime and\nsubmersible equipment to increase the hydrocarbon recovery. In addition, the models can be helpful\nto perform well-testing avoiding costly shut-in operations.\nKeywords Deep learning ·Transformers ·Recurrent neural networks ·Bottomhole pressure ·Flow rate ·Production\nhistory data ·Forecast ·Planning ·Optimization\n1 Introduction\nLarge datasets are usually accumulated during an oilﬁeld life cycle. Typically, they are stored as time series and\ncomposed of various parameters measured at producing wells by pressure gauges, temperature sensors, and ﬂow meters.\nThis is a rich source of information that can be used to develop models to simulate well performance and predict the\nproduction process. Finding the relationship between bottomhole pressure and ﬂow rate at the surface is a primary task\nin the petroleum industry which has several technological applications. Its solution is usually carried out by constructing\nmathematical models based on conservation laws in ﬂuid mechanics supplemented by closure relations obtained from\nthe analysis of experimental data. The models can be either semi-analytical [ 1, 2] or fully numerical obtained by\narXiv:2110.06059v1  [cs.LG]  12 Oct 2021\nsimulators accounting for the multiphase ﬂow in porous media, PVT properties of the phases, and other physical\nphenomena accompanying hydrocarbon recovery (e.g., Eclipse, TNavigator). In both cases, additional information is\nrequired related to a reservoir, ﬂuids properties as well as geometries of the completion and formation, which can be\nextracted from the interpretation of well-testing, logging, and seismic survey data. Hydrodynamic simulators imply\nspeciﬁc competency to petroleum engineers, and the computation process is usually time-consuming, especially when\nmassive parametric studies are carried out.\nIn this work, we develop a novel data-driven model based on the deep learning algorithms, which allow fast prediction\nof time-dependent well parameters such as bottomhole pressure or ﬂow rate. It can be an alternative to relatively slow\nrunning numerical simulators based on mechanistic models. The developed instrument is not a replacement for the\ntraditional ﬂuid mechanics-based modeling, but it potentially expands the capability of petroleum engineers for planning\nthe development of the oil and gas ﬁelds.\nThere are many literature sources devoted to data-driven modeling of the transient production of oil wells. For example,\nin the paper [3], the authors used time series with a moderate duration to predict the bottomhole pressure dynamics. It\nwas obtained that long- and short-term time-series network (LSTNet) [4] outperforms the recurrent neural networks\n(RNNs) trained on the ﬁeld data for a single well in terms of prediction accuracy. The authors analyzed different feature\ncombinations and highlighted the optimal set providing the maximum quality of predictions. In the study [5], the authors\ncompared the predictive capabilities of convolutional (CNN) and recurrent (RNN) neural networks to simulate the\nreservoir behavior. The authors demonstrated that both networks could capture the interference between multiple wells\nat the same reservoir. RNN with the long-short-term memory (LSTM) cell [6] was applied in the paper [7] to predict the\nwellhead pressure during a fracturing treatment, and the input parameters of the network are the surface characteristics\nof the well. In the papers discussed above, the machine learning models utilize either recurrent or convolutional\nneural networks. However, RNNs have exploding and vanishing gradient problems, while the convolutional ﬁlters in\nCNNs have certain limitations. These issues restrict the applicability of RNNs and CNNs to model complex long-term\ndependencies in sequential data series. Auto-regression (AR) methods are another approach to sequence modeling and\ntime-series predictions. The commonly known autoregressive moving average (ARMA) and autoregressive integrated\nmoving average (ARIMA) are adapted to model trends, patterns, and seasonality. As a result, they have limitations in\nmodeling transient production processes with several extreme values in the well’s measurement data. The authors of the\npaper [8] reported another approach for oil well performance modeling. The researchers built the machine learning\nmodel based on linear regression to predict the bottomhole pressure using the nonlinear ﬂow-rate features. To reﬂect the\nphysical properties of the pressure response as a function of ﬂow rate and time, the authors extracted several features\nfrom the time-series data. The feature-based machine learning methods such as linear regression require manual feature\nengineering, leading to the loss of important information contained in raw data. In the literature, different authors also\napplied the widespread machine learning techniques such as the Gradient Boosting algorithm and/or Artiﬁcial Neural\nNetwork to predict the bottomhole pressure [9, 10] of a well and the ﬂow rate after the hydraulic fracturing treatment\n[11, 12, 13].\nIn the present paper, we apply a novel deep learning algorithm called transformer [14] to build surrogate models for\nsimulations of oil well performance. Transformer architecture was initially developed for natural language processing\n(NLP) problems. However, in recent years, researchers adapted transformers for time-series forecasting. In the paper\n[15], the transformer was used for inﬂuenza-like illness prediction. In the article [ 16], the authors introduced the\ntemporal fusion transformers for interpretable multi-horizon time-series forecasting. The main idea is to learn temporal\nrelations at different scales, recurrent layers are used for local processing and self-attention layers are responsible for\nlong-term processing. Several modiﬁcations of the original transformer were proposed in the work [ 17] to improve\ntime-series modeling. The authors enhanced the locality of the model with convolutional self-attention and addressed\nthe memory bottleneck problem with LogSparse self-attention layers. In contrast to recurrent neural networks, the\ntransformer does not process data sequentially. Instead, it handles the entire sequence via a multi-head self-attention\nmechanism. Self-attention enables the transformer to learn data representations efﬁciently relating to different positions\nin the input sequence. The transformer-based models have a large potential for more accurate predictions made on the\nbasis of unsteady and noisy data as compared to recurrent neural networks. Moreover, the transformer is much more\ncomputationally efﬁcient than RNN since its training can be performed using graphics processing units (GPU).\nTransformer network allows for applying the transfer learning technique relied on the following idea. First, we train\nthe model to predict the target parameter, for example, bottomhole pressure, using the production data from a certain\nwell. Next, we use the tuned weights to initialize the training procedure on the data from a target well. In other words,\nwe carry out the ﬁne-tuning based on the pre-trained model. The described technique allows the model to transfer\nknowledge from one time-series forecasting problem to another leading to acceleration of the training process and\nimprovement of the model prediction capability. The recurrent networks are not well suited to transfer learning since\nthe hidden state of recurrent cells is commonly not transferred.\n2\nThe main aim of the current work is to create an instrument based on transformer architecture for modeling the transient\nproduction of an oil well. To the best of our knowledge, the application of transformer networks for the considered\nproblem has never been reported in open literature. In the paper, we propose two models allowing one to simulate\nthe production process of either a single well or several wells (global model). The latter model is trained on data\nfrom multiple wells and allows for accounting for interference between wells leading to improvement of accuracy in\nestimations for each of the wells. We demonstrate that the transformer can learn complex variations in production data\nand dependencies between time-dependent characteristics of oil wells.\nThe paper is organized as follows. Section 2 outlines the problem formulation. Section 3 describes the modeling\napproach and the ﬁeld dataset utilized for the training of machine learning models. Section 4 revisits the deep learning\nalgorithms considered in the current work, recurrent neural networks and transformer, to model the transient well\nperformance; the single-well model is improved and expanded via applying the transfer learning and building the global\nmodel. Finally, Section 5 presents the obtained results and their discussion including the comparison of the predictive\ncapability of transformer and recurrent neural networks.\n2 Problem formulation\nWe consider a transient production of an oil well with arbitrary completion. In most cases, various parameters of the\nwell are measured during its exploitation including bottomhole/wellhead pressure and temperature, ﬂow rates for each\nphase, choke size, parameters of electric centrifugal pump (if available). One of the most important tasks of well\nperformance modeling is to determine the functional relationship between the bottomhole pressure and surface ﬂow\nrate. The problem can be solved using hydrodynamic simulators based on mathematical models formulated using\nconservation laws in ﬂuid mechanics. The simulations allow for taking into account different physical phenomena\naccompanying well production but require values of several additional reservoir and ﬂuids properties typically not\nmeasured in the ﬁeld. Also, the calculations are time-consuming so that they are intended to be run at high-performance\ncomputers.\nHere, we propose another approach to ﬁnd the dependence between bottomhole pressure and ﬂow rate. It is based\non deep learning algorithms trained on available ﬁeld data and allows for obtaining quick predictions of bottomhole\npressure or ﬂow rate. The data-driven coupled model of a well and reservoir can be used to plan and optimize the\nproduction process. Also, the developed model can be used to estimate well and formation properties using a constant\nﬂow rate response of a well, which is analog to well-test avoiding expensive shut-ins.\n3 Modeling approach and dataset\nIn the current section we describe the developed data-driven model to predict the bottomhole pressure of a single well.\nThe considered problem is an example of a supervised machine learning task. We assume that the ﬁeld dataset is\ncomposed of daily records of the dynamic parameters of a well (in general, the time intervals between the data points\nmay be heterogeneous). We train machine learning algorithms to predict the average value of the bottomhole pressure\nˆpt during the next day, and the input data is the sequence of vectors containing the values of governing parameters (ﬂow\nrates of oil, gas, and water; temperature at the bottomhole and wellhead, and others) at the current time moment t and at\nprevious days (xt−N+1,...,x t−1,xt). The surrogate model processes a matrix of dimension N ×M, where N −1 is\nthe number of days preceding the considered time moment, and M is the number of features. The forecast horizon\nof the models can be larger than one day, and it is required to use the sequence-to-sequence (Seq2Seq) models with\nencoder-decoder architecture. However, in the present work, both transformer and recurrent neural networks are trained\nto predict the one-day value of the target parameter. If the data-driven model to estimate of the ﬂow rate is required\n(liquid ˆQl\nt or for each phase ˆQo\nt , ˆQg\nt , ˆQw\nt ), it is formulated in a similar way with the exception that input parameters\ninclude the bottomhole pressure dynamics.\nFor test calculations we use the data from V olve oilﬁeld in North Sea published by Norwegian company Equinor for\nresearch purposes [18]. The data contains the daily measurements of various characteristics of an oil well, and in this\nwork, we build models for two wells operated from April 2014 to April 2016.\nFirst, we preprocess the data since measurements contain missing values, linear interpolation is applied to ﬁll the\ngaps. The resulting time series for two oil wells are shown in Appendix. Next, we perform the feature-wise min-max\nnormalization of the entire dataset using the maximum and minimum values of the features determined from the training\ndata as follows:\nxtj = xtj −min xtj\nmax xtj −min xtj\n, (1)\n3\nwhere j is a feature index. Data normalization is a standard technique in machine learning, especially when neural\nnetworks are applied. All input features have different scales, and, therefore, they are rescaled to the range from 0 to 1.\nTo produce the labeled samples for training and testing of the data-driven models based on deep learning algorithms,\nwe slice the original time series into samples of length N using the sliding window technique. The schematic picture of\nthis procedure is shown in Figure 1, where blue color denotes the days for which we predict the bottomhole pressure\nvalues, and red color represents the time intervals based on which the prediction is carried out. Several ﬁrst samples\nhave an incomplete history so that the corresponding segments are padded with zeros (pastel red in Figure 1).\nxt\nt\npt\nt\n1 day\n……\nFigure 1: Sliding window technique for cutting the initial time series into ﬁxed-length samples used afterward to train\nand test machine learning algorithms. Each data sample consists of the time interval (red color) based on which the\nvalue of the bottom-hole pressure is predicted for the next day (blue color). The light red color is used for the time\nintervals lying outside the original production period. The upper graph shows an example of the downhole pressure\nbehavior, while the lower one shows one of the time-dependent governing parameters, e.g., oil ﬂow rate.\nFinally, we divide the dataset into three parts: train (70%), validation (15%), and test (15%). One should mention that\nthe validation set follows the training set, while the test set follows the validation set.\n4 Deep learning models\n4.1 Recurrent neural network\nThe structure of the recurrent neural network (RNN) is shown in Figure 2.\nRNN converts the input matrix into the number, which corresponds to many-to-one architecture. RNN consists of\nidentical sequentially connected blocks (also known as RNN cells), and their number is equal to the length of each\ndata sample (in our case, N). Let us consider the simplest RNN cell. It processes the vector of governing parameters\nxi and previous hidden state hi−1. By applying a certain functional dependence, the RNN cell outputs a new hidden\nstate hi. The last hidden state vector ht is transformed into the bottomhole pressure value yt ≡pt by a linear layer. We\nsummarize the described transformations by following expressions:\nhi = σ(Wxxi + Whhi−1 + bh) ,\nyt = Wyht + by, (2)\nwhere Wx,Wh,bh,by are the matrices and columns with tuned weights. During the training of RNN with the simplest\ncell, one can observe the gradient vanishing and exploding problems, especially when the input sequence is sufﬁciently\nlong.\n4\nxt−N+1 xt−N+2 . . . xtRNN\nInput\nRNN \nCell\nLinear layer\nRNN \nCell\nRNN \nCell\nRNN \nCell\npt\nRNN\nOutput\nUniform\nFigure 2: The architecture of many-to-one RNN which converts the input matrix Xt = {xt−N+1,...,x t}with the\nvalues of the governing parameters into the bottomhole pressure value pt.\nLong-short term memory (LSTM) [ 6] and gated recurrent unit (GRU) [ 19] networks were designed to address the\nvanishing gradient problem via the gates mechanism. The following set of equations describes the LSTM network:\nfi = σ(Wf [hi−1,xi] +bf ) ,\nii = σ(Wi [hi−1,xi] +bi) ,\nˆci = tanh (Wc [hi−1,xi] +bc),\nci = fi ∗ci−1 + ii ∗ˆci,\nhi = ˆci ∗tanh (ci),\nyt = Wyht + by, (3)\nwhere fi forgets gate’s activation vector,ii updates gate’s activation vector,ˆci is the cell input activation vector,ci is the\ncell state activation vector, ∗is the element-wise product, Wf ,Wi,Wc,Wy,bf ,bi,bc,by are matrices and columns with\nweights learned during the training.\nGRU network has a smaller number of tuned parameters. Only two gates control the information ﬂow in the GRU cells.\nGRU network is faster and more lightweight than LSTM. The following equations describe the GRU cell:\nzi = σ(Wz [hi−1,xi] +bz) ,\nri = σ(Wr [hi−1,xi] +br) ,\nˆhi = tanh (Wh [ri ∗hi−1,xi] +bh),\nhi = (1−zi) ∗hi−1 + zi ∗ˆhi,\nyt = Wyht + by, (4)\nwhere zi updates gate’s activation vector, ri resets gate’s activation vector, ˆhi is the candidate activation vector,\nWz,Wr,Wh,bz,br,bh are the matrices and bias vectors with parameters learned during training.\nDespite both LSTM and GRU were developed to solve the vanishing gradient problem, the issue is not resolved\ncompletely. Exploding gradient problem is tackled with the gradient clipping technique used to rescale the gradient\nvectors whenever their norm exceeds a certain threshold. We average the gradient norms accumulated over several\ntraining epochs to determine the threshold value in our work.\n4.2 Transformer\nWe consider transformer architecture (Figure 3) proposed in the original paper [14] and introduce several modiﬁcations\nto adapt the model for predictions of time-series. The model consists of encoder and decoder parts. In the former part,\nthe input matrix of dimension [N ×M] is mapped to the tensor of shape [N ×dmodel] through a fully connected layer\n(input embedding in Figure 3). To encode the sequential information in the form of time series, we apply the ﬁxed\nsinusoidal positional encoding (positional encoding in Figure 3). The resulting tensor is passed to the encoder part\nof the network represented by several encoding layers. Each layer consists of a multi-head self-attention mechanism\nfollowed by a fully connected feed-forward network. Similar to the original architecture of the transformer, we use\n5\nxt−N+1 xt−N+2 . . . xt xt\nInput Embedding\nPositional\nEncoding\nMulti-Head \nAttention\nFeed  \nForward\nAdd & Normalise\nAdd & Normalise\nN x\nInput Embedding\nEncoder\nEncoder\nInput\nDecoder\nInput\nPositional\nEncoding\nMulti-Head \nAttention\nAdd & Normalise\nMulti-Head \nAttention\nAdd & Normalise\nx N\nFeed  \nForward\nAdd & Normalise\nLinear pt\nDecoder\nOutput\nDecoder\nFigure 3: Architecture of transformer-based model: encoder (on the left) and decoder (on the right) internal structures.\nresidual connections followed by layer normalization [14]. The decoder input is the last data point of the encoder input,\nthe vector with the values of the governing parameters corresponding to the time instant for which we perform the\nprediction. The model predicts the target value at the time instant corresponding to the last data point in the input\nsequence. The internal algorithm of the decoder resembles that of the encoder. The difference is in the transformation\nof the decoder output: it is mapped into the value of the target parameter through a linear layer. The number of samples\nin the dataset is not large compared to the typical NLP tasks. Using the same hyperparameters of the transformer as in\nthe original architecture [14], we obtain the model overﬁtting. To adapt the single-well pressure prediction model, we\nreduce the number of learned parameters by decreasing the model dimension, number of encoder and decoder layers,\nnumber of heads, and dimensions of the feed-forward networks. Note that these numbers in the global model describing\nmultiple wells (Section 5.2) should be increased due to using of a larger dataset for model training.\n4.3 Transfer learning approach\nTransfer learning is a technique in which the tuned weights of the surrogate model are transferred from one machine\nlearning problem to another. Instead of training the model from scratch, it is initialized with the pre-trained weights.\nTransfer learning facilitates training and almost always improves the predictive capability of the machine learning model.\nThe hidden state of the recurrent networks is commonly not transferred in stateless RNN models. We utilize stateless\nRNNs whose hidden state is uniformly initialized before each forward pass. Therefore, we cannot apply the transfer\nlearning technique for such RNN-based models. However, the transformer-based model is suitable for this procedure.\nWe can train the deep learning model on data from one well and then use the pre-trained model to ﬁne-tune it on the data\nfrom a different (target) well. First, we get rid of the linear output layer of the transformer and add two new linear layers\nwith ReLU non-linearity between them. The weights of the newly added linear layers are randomly initialized. Large\ngradients can propagate through the pre-trained transformer model and wreck the pre-trained weights. To avoid this, we\nrun a single warm-up epoch to let the Adam [20] optimizer accumulate the gradient statistics. Then, the whole model is\ntrained with a ten times smaller learning rate as in training from scratch. Correctly performed ﬁne-tuning enhances the\nquality of the prediction, and we conﬁrm this statement by the results of the numerical experiments shown below.\n4.4 Global model\nIt can be sufﬁcient to train the surrogate model on the measurements from the considered well to simulate the transient\nproduction process. However, if we consider a certain well surrounded by neighboring producing/injection wells, any\n6\nalteration of their operating mode leads to a signiﬁcant impact on time-dependent parameters of the well. We develop\na global model trained on the dataset composed of the measurements from multiple wells. The wells identiﬁers are\nencoded into the one-hot vectors, which we include into input features of the model. These new features allow the model\nto recognize the well, from which the data sample is processed. The data comes into the model in the mini-batches\nconsisting of the data samples belonging to multiple wells. Similar to the single-well model, we utilize the sliding\nwindow technique to create data samples (Figure 1). The window cuts the ﬁxed time interval from the time series\nfor each well. By incorporating the information from multiple wells, the created model allows capturing the patterns\nrunning at the ﬁeld development cell and/or the entire oilﬁeld scale (if the number of oil wells in the dataset is large)\ndue to an interference between neighboring wells. The global model predicts the values of a time-dependent target\nparameter (for example, bottomhole pressure) for each considered well more accurately as compared to the single-well\nmodel, and we support the statement by the numerical experiments.\n5 Results and discussion\nWe conducted a series of numerical experiments using the data for two wells (15/9-F-1C and 15/9-F-15D) from the\nﬁeld dataset described in Section 3.\n5.1 Single-well model\nFirst, we compared the predictive capabilities of the recurrent neural networks and transformer for prediction of the\nbottomhole pressure dynamics of a single well. We trained the deep learning models on the data from each well\nseparately. Input features include daily measurements of oil, gas, and water ﬂow rates, bottomhole temperature, and\nchoke size in percentage (see Appendix). The loss function is a mean squared error (MSE). The sequence length of the\ninput samples is fourteen days (N = 14). The best state of a surrogate model corresponds to the lowest MSE value on\nthe validation dataset. The example transformer-based model performance on train, validation, and test sets is shown in\nFigure 4.\nFigure 4: Prediction of the bottomhole pressure dynamics for well 15/9-F-1C by the transformer-based deep learning\nmodel.\nWe benchmark the transformer model against the recurrent neural networks on the test set for two wells, and Figure 5\npresents the obtained results.\nIn addition to the qualitative comparison of the surrogate models performance, we also carried out the quantitative one\nby computing the values of different metrics such as RMSE, RSE, RAE, and MAPE using the actual and estimated\n7\n(a)\n (b)\nFigure 5: Predictions of the bottomhole pressure dynamics for the test set corresponding to well 15/9-F-1C (a) and\n15/9-F-15D (b) by the deep learning models based on the recurrent neural networks (vanilla RNN, LSTM, GRU) and\ntransformer.\nbottomhole pressure values according to the formulas below:\nRMSE =\n√∑n\ni=1 (ˆyi −yi)2\nn ,\nRSE =\n∑n\ni=1 (ˆyi −yi)2\n∑n\ni=1 (y−yi)2 ,\nRAE =\n∑n\ni=1 |ˆyi −yi|∑n\ni=1 |y−yi|,\nMAPE =\nn∑\ni=1\n⏐⏐⏐⏐\nˆyi −yi\nyi\n⏐⏐⏐⏐, (5)\nwhere RMSE is the root mean square error, RSE is the relative square error, RAE is the relative absolute error, MAPE is\nmean absolute percentage error, yi is the actual value of the target parameter at the time moment i, ˆyi is the predicted\nvalue of the target parameter at the time moment i, yis the average value of the target parameter, n is the number of\ndatapoints. The obtained metrics are summarized in Table 1 and Table 2.\nTable 1: The values of metrics calculated for the test set corresponding to well 15/9-F-1C and based on the real and\npredicted values of the bottomhole pressure. The best values of each metric are highlighted in red.\nMetric/Model RNN LSTM GRU Transformer\nRMSE 8.39 7.76 8.12 7.66\nRSE 0.13 0.11 0.12 0.10\nRAE 0.34 0.33 0.34 0.34\nMAPE 2.59 % 2.61 % 2.73 % 2.62 %\nTable 2: The values of metrics calculated for the test set corresponding to well 15/9-F-15D and based on the real and\npredicted values of the bottomhole pressure. The best values of each metric are highlighted in red.\nMetric/Model RNN LSTM GRU Transformer\nRMSE 22.60 11.05 19.58 10.01\nRSE 0.62 0.15 0.46 0.12\nRAE 0.77 0.40 0.68 0.31\nMAPE 8.92 % 4.42 % 7.89 % 3.24 %\n8\nThe transformer model outperforms recurrent neural networks in bottomhole pressure prediction for both wells (Figure\n5, Table 1 and Table 2). The self-attention mechanism of the transformer network enables to capture of the complex\ntransient dynamics of ﬁeld data. The advantage of the transformer network over RNNs is more pronounced for well\n15/9-F-15D as compared to well 15/9-F-1C. The possible reason is that the test data corresponding to well 15/9-F-1C\nresembles the training data. Therefore, all models are close in terms of quality metrics. However, the test set related\nto well 15/9-F-15D is more variable. It suggests that the transformer model better reproduces true well performance\nbehavior and captures the relationship between pressure and ﬂow rates.\nThe ﬂow rate measurement requires the availability of the multiphase ﬂow meter, which is not always installed in a\nwell. Moreover, the dynamics of phase ﬂow rates can be highly oscillating and contain missing values, so that they can\nbe unreliable features for the machine learning model. Instead of relying on ﬂow rate data, we demonstrate that the\nsurrogate models trained solely on choke opening size and bottomhole temperature can predict the bottomhole pressure\nwith acceptable accuracy. Figure 6 shows the predictions of the models trained on the full and reduced feature sets for\nthe testing datasets corresponding to wells 15/9-F-1C and 15/9-F-15D.\n(a)\n (b)\nFigure 6: Predictions of the bottomhole pressure dynamics for the test set corresponding to well 15/9-F-1C (a) and\n15/9-F-15D (b) by the deep learning models based on transformer (a) and LSTM recurrent neural network (b) trained\non the full feature set and reduced feature set.\nWe applied the transformer-based deep learning model to the data related to well 15/9-F-1C, while LSTM was used for\nthat of well 15/9-F-15D. The transformer model trained on the reduced feature set (Figure 6a) provides almost the same\nprediction quality as the model trained on the full feature set. Table 3 summarizes the values of metrics computed with\nusing the actual and estimated bottomhole pressure dynamics corresponding to the test set of well 15/9-F-1C. In turn,\nFigure 6b shows the obtained results for well 15/9-F-15D, and Table 4 gives the numbers of the calculated metrics. We\nconclude that the transformer-based surrogate model trained on the reduced feature set has lower predictive capability\nthan the model tuned on the full feature set. However, the reversed situation is observed for the LSTM, which yields a\nbetter prediction quality after training on the reduced feature set.\nTable 3: The values of metrics calculated for the test set corresponding to well 15/9-F-1C and based on the real and\npredicted values of the bottomhole pressure, which are estimated by the transformer deep learning model trained on\nboth full and reduced feature sets.\nMetric/Model Transformer (full set) Transformer (reduced set)\nRMSE 7.66 10.36 (+35.24 %)\nRSE 0.10 0.20 (+100 %)\nRAE 0.34 0.43 (+26.47 %)\nMAPE 2.62 % 3.33 % (+27.10 %)\nIn addition to the model for predicting the bottomhole pressure, we also built a transformer-based model for forecasting\nthe dynamics of the liquid ﬂow rate. The bottomhole pressure and temperature and the choke opening size were taken\nas input features. We present the obtained results on the test dataset corresponding to well 15/9-F-1C in Figure 7, and\nthe computed quality metrics are written in Table 5.\n9\nTable 4: The values of metrics calculated for the test set corresponding to well 15/9-F-15D and based on the real and\npredicted values of the bottomhole pressure, which are estimated by the LSTM trained on both full and reduced feature\nsets.\nMetric/Model LSTM (full set) LSTM (reduced set)\nRMSE 11.05 10.24 (-7.33 %)\nRSE 0.15 0.13 (-13.33 %)\nRAE 0.40 0.37 (-7.5 %)\nMAPE 4.42 % 3.87 % (-12.44 %)\nFigure 7: Prediction of the liquid ﬂow rate dynamics for the test set corresponding to well 15/9-F-1C by the transformer-\nbased deep learning model.\n5.2 Transfer learning technique application\nThe next numerical experiment shows that the transfer learning technique that works with transformers can enable\nextra-quality predictions of the bottomhole pressure. We transferred weights from the model trained on the dataset\ncorresponding to well 15/9-F-1C. We added the feed-forward network on top of the transformer and ﬁne-tuned the\nwhole model on the production data corresponding to well 15/9-F-15D.The resulting predictions are shown in Figure 8.\nTable 6 compares the quality metrics of two transformer-based models: (i) trained from scratch and (ii) ﬁne-tuned\nwith pre-trained weights. The improvement is not signiﬁcant since we used the data from only a single well for the\npre-training stage.\n5.3 Global model\nThe ﬁnal numerical experiment is devoted to the global transformer model. We analyzed the data from the same\nwells (15/9-F-1C and 15/9-F-15D) and combined them into a single dataset. The predictions of the global model are\ndemonstrated in Figure 9 for each considered well. Moreover, we add the forecasts via the single-well model for\ncomparison purposes. The quality metrics values with the relative deviation from the numbers corresponding to the\nsingle-well model performance are summarized in Table 7.\nBy comparing the RMSE values, we conclude that the global model outperforms the single-well model for the well\n15/9-F-1C by 25% and the model for well 15/9-F-15D by 20%. If the data from several or even many wells working at\n10\nTable 5: The values of metrics calculated for the test set corresponding to well 15/9-F-1C and based on the real and\npredicted values of the liquid ﬂow rate, which are estimated by the transformer-based deep learning model.\nMetric Value\nRMSE 107.75\nRSE 0.04\nRAE 0.13\nFigure 8: Predictions of the bottomhole pressure dynamics for the test set corresponding to well 15/9-F-15D by the deep\nlearning models based on the transformer architecture trained from scratch and ﬁne-tuned starting from the pre-trained\nweights on the dataset corresponding to well 15/9-F-1C.\nthe same oil ﬁeld is available, it is possible to train the global model to forecast the bottomhole pressure or liquid ﬂow\nrate, accounting for the effect of the inﬂuence of wells on each other (interference) during the production process.\n6 Conclusions\nWe have developed a novel approach to simulate the transient production of an oil well based on the deep learning\nalgorithm called transformer. We have compared the predictive capability of the transformer with the performance\nof the RNN-based models in the prediction of the bottomhole pressure dynamics of a single well. The self-attention\nmechanism of the transformer model can learn the complex transient dependencies in ﬁeld data between the target\nparameter (for example, bottomhole pressure) and input features, which are the measured parameters of a well (for\nexample, ﬂow rates, bottomhole temperature, and choke opening size) improving the accuracy of predictions of the\ntarget parameter as compared to the RNNs. Moreover, the transformer can process the entire input sequences of various\nlengths; in contrast, the number of cells in recurrent networks is often limited. We have applied the transfer learning\ntechnique, which enables the transformer to enhance the forecasting ability to predict the target property for one well\nwith the initial tuning of the model weights on the data related to the other well. The transfer learning approach would\nbeneﬁt from the increase in the number of wells used during the pre-training stage. The global model trained on the\ndata from two wells allows for improving the quality of the predictions signiﬁcantly.\nThe possible continuation of the work will include the transfer learning application to the global transformer model,\nfurther investigation of the transformer-based model ability to predict ﬂow rate and temperature as well as to use the\ntransformer in well-testing interpretation to avoid expensive shut-ins.\n11\nTable 6: The values of metrics calculated for the test set corresponding to well 15/9-F-15D and based on the real and\npredicted values of the bottomhole pressure, which are estimated by the transformer deep learning model trained from\nscratch and pre-trained on the dataset from well 15/9-F-1C followed by the additional training on the dataset from the\nrequired well.\nMetric/Model Transformer Fine-tuned pre-trained\ntransformer\nRMSE 10.01 9.53 (-4.80 %)\nRSE 0.12 0.11 (-8.33 %)\nRAE 0.31 0.28 (-9.68 %)\nMAPE 3.24 % 3.00 % (-7.41 %)\n(a)\n (b)\nFigure 9: Predictions of the bottomhole pressure dynamics for the test set corresponding to well 15/9-F-1C (a) and\n15/9-F-15D (b) by the single-well and global deep learning models based on transformer.\nTable 7: The values of metrics calculated for the test sets for wells 15/9-F-1C and 15/9-F-15D and based on the real\nand predicted values of the bottomhole pressure, which are estimated by the global model relied on the transformer\narchitecture. In brackets, we write the relative difference between the values of metrics obtained via the global and\nsingle-well models.\nWell/Metric RMSE RSE RAE MAPE\n15/9-F-1C 5.76 (-24.80 %) 0.06 (-40 %) 0.24 (-29.41 %) 1.85 % (-29.39 %)\n15/9-F-15D 7.97 (-20.38 %) 0.08 (-33.33 %) 0.23 (-25.81 %) 2.39 % (-26.23 %)\n12\nAppendix\nField data – oil well 15/9-F-1C.\n13\nField data – oil well 15/9-F-15D.\n14\nReferences\n[1] Erdal Ozkan, Rajagopal Raghavan, and Others. New solutions for well-test-analysis problems: part 1-analytical\nconsiderations (includes associated papers 28666 and 29213). SPE Formation Evaluation, 6(03):359–368, 1991.\n[2] Erdal Ozkan and Rajagopal Raghavan. New solutions for well-test-analysis problems: Part 2 computational\nconsiderations and applications. SPE Formation Evaluation, 6(03):369–378, 1991.\n[3] Yuanjun Li, Ruixiao Sun, Roland Horne, and Others. Deep learning for well data history analysis. In SPE Annual\nTechnical Conference and Exhibition. Society of Petroleum Engineers, 2019.\n[4] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns\nwith deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in\nInformation Retrieval, pages 95–104, 2018.\n[5] Abdullah Alakeely, Roland N Horne, and Others. Simulating the Behavior of Reservoirs with Convolutional and\nRecurrent Neural Networks. SPE Reservoir Evaluation & Engineering, 2020.\n[6] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n[7] Srinath Madasu, Keshava P Rangarajan, and Others. Deep recurrent neural network DRNN model for real-time\nmultistage pumping data. In OTC Arctic Technology Conference. Offshore Technology Conference, 2018.\n[8] Chuan Tian and Roland N Horne. Applying machine-learning techniques to interpret ﬂow-rate, pressure, and\ntemperature data from permanent downhole gauges. SPE Reservoir Evaluation & Engineering, 22(02):386–401,\n2019.\n[9] EA Kanin, AA Osiptsov, AL Vainshtein, and EV Burnaev. A predictive model for steady-state multiphase pipe\nﬂow: Machine learning on lab data. Journal of Petroleum Science and Engineering, 180:727–746, 2019.\n[10] Egor Sergeevich Baryshnikov, Evgenii Aleskseevich Kanin, Andrei Aleksandrovich Osiptsov, Albert Vain-\nshtein, Evgeny Vladimirovich Burnaev, Grigoriy Vladimirovich Paderin, Alexander Sergeevich Prutsakov, and\nStanislav Olegovich Ternovenko. Adaptation of steady-state well ﬂow model on ﬁeld data for calculating the\nﬂowing bottomhole pressure. In SPE Russian Petroleum Technology Conference. OnePetro, 2020.\n[11] Anton D Morozov, Dmitry O Popkov, Victor M Duplyakov, Renata F Mutalova, Andrei A Osiptsov, Albert L\nVainshtein, Evgeny V Burnaev, Egor V Shel, and Grigory V Paderin. Data-driven model for hydraulic fracturing\ndesign optimization: Focus on building digital database and production forecast. Journal of Petroleum Science\nand Engineering, 194:107504, 2020.\n[12] AS Erofeev, DM Orlov, DS Perets, and DA Koroteev. Ai-based estimation of hydraulic fracturing effect. SPE\nJournal, pages 1–12, 2021.\n[13] VM Duplyakov, AD Morozov, DO Popkov, EV Shel, AL Vainshtein, EV Burnaev, AA Osiptsov, and GV Paderin.\nData-driven model for hydraulic fracturing design optimization. part ii: Inverse problem. Journal of Petroleum\nScience and Engineering, 208:109303, 2022.\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[15] Neo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep transformer models for time series forecasting:\nThe inﬂuenza prevalence case. arXiv preprint arXiv:2001.08317, 2020.\n[16] Bryan Lim, Sercan O Arik, Nicolas Loeff, and Tomas Pﬁster. Temporal fusion transformers for interpretable\nmulti-horizon time series forecasting. arXiv preprint arXiv:1912.09363, 2019.\n[17] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing\nthe locality and breaking the memory bottleneck of transformer on time series forecasting. arXiv preprint\narXiv:1907.00235, 2019.\n[18] Equinor Equinor. V olve Data Village. Equinor Data Portal Beta, 2018.\n[19] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078, 2014.\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n15",
  "topic": null,
  "concepts": []
}