{
  "title": "Could large language models estimate valence of words? A small ablation study",
  "url": "https://openalex.org/W4391366285",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Frederico Jandre",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3012121378",
      "name": "Gabriel Motta-Ribeiro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2601501710",
      "name": "Jo√£o Vitor Silva",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4317552966",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4322718260",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W2023736093",
    "https://openalex.org/W2126014798",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W137057924",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4210462189",
    "https://openalex.org/W4327657862",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4363624465"
  ],
  "abstract": "Large language models (LLMs) saw substantial development in recent years. Although trained with broad-range corpora, LLMs have been shown to display capabilities such as quantitative sentiment analysis without the need for further fine tuning. In this study, we performed a small ablation study to evaluate the performance of 3 off-the-shelf LLMs in the task of assigning ratings of hedonic valence to words: GPT-3.5 in chat mode, and GPT-3 and Bloom in completion mode. The models were operated via their public APIs, using prompts engineered to request emojis and ratings of valence in a 9-point scale to represent each of 140 words drawn from a large dataset rated by humans. Prompts were designed to demand the ratings from an adult, with modifiers average or overly positive employed to assess their effects on the results. All linear regressions between the LLM outputs and the human ratings had p-value. The 95% confidence intervals of the slopes include 1.0 for ‚Äúadult‚Äù and ‚Äúaverage adult‚Äù, except for the model Bloom. These simulacra responded, albeit with limitations, tovalenceofwords andtomodifiersintheprompt.",
  "full_text": "Couldlargelanguagemodelsestimatevalenceofwords?Asmallablationstudy\nFredericoC.JandreProgramadeEngenhariaBiom√©dica/COPPEUniversidadeFederaldoRiodeJaneiroRiodeJaneiro,Brasiljandre@peb.ufrj.br\nGabrielC.Motta-RibeiroBolsistadoProgramaDoutorEmpreendedorFunda√ß√£odeAmparo√†PesquisadoEstadodoRiodeJaneiroRiodeJaneiro,Brazilgabrielcasulari@peb.ufrj.br\nJo√£oVitorAssump√ß√£odaSilvaGradua√ß√£oemEngenhariadeControleeAutoma√ß√£o/POLIUniversidadeFederaldoRiodeJaneiroRiodeJaneiro,Braziljoaoxvitor18@poli.ufrj.br\nAbstract‚Äî Largelanguagemodels(LLMs)sawsubstantialdevelopmentinrecentyears.Althoughtrainedwithbroad-rangecorpora,LLMshavebeenshowntodisplaycapabilitiessuchasquantitativesentimentanalysiswithouttheneedforfurtherfinetuning.In thisstudy, weperformeda smallablationstudytoevaluatetheperformanceof3‚Äúoff-the-shelf‚ÄùLLMsinthetaskofassigningratingsofhedonicvalencetowords:GPT-3.5inchatmode,andGPT-3andBloomincompletionmode.ThemodelswereoperatedviatheirpublicAPIs,usingpromptsengineeredtorequestemojisandratingsof valencein a 9-pointscaletorepresenteachof140wordsdrawnfromalargedatasetratedbyhumans.Promptsweredesignedtodemandtheratingsfromanadult,withmodifiers‚Äúaverage‚Äùor‚Äúoverlypositive‚Äùemployedtoassesstheireffectsontheresults.AlllinearregressionsbetweentheLLMoutputsandthehumanratingshadp-value<0.001.The95%confidenceintervalsoftheslopesinclude1.0for‚Äúadult‚Äùand ‚Äúaverageadult‚Äù,exceptfor the modelBloom.Thesesimulacraresponded,albeitwithlimitations,tovalenceofwordsandtomodifiersintheprompt.\nKeywords‚Äîlargelanguagemodels,sentimentanalysis,hedonicvalence,ablationstudy,promptengineering\nI. INTRODUCTION\nLarge language models (LLMs) have had a substantialevolution in the last years, driven, among other things, byincreasing processing power, vast amounts of availablecorpora and new topologies for the underlying artificialneural networks (ANNs). Recent achievements haveendowed LLMs with stunning capabilities, as shown forinstance in the performance of GPT-3 in cognitivepsychology tasks [1]. These capabilities have inspired theconjecture that LLMs couldbe usedas simulacra of humanassessments, with some researchers proposing thesubstitutioninfields suchas experimentaleconomics[2],atleast during the process of gaining insights and testingmethods. One crucial aspect of current LLMs is that it isgenerallynot possible toestimate,fromfirstprinciples,howthe model will respondtotext inputs. Manyaspectsoftheirbehaviors are reported at length in the literature,prominently the propensity to generate text that isincompatible with verifiable facts of the world, usuallyregarded as a disadvantage due to the consequent lack oftrustworthiness of the outputs; also, that kindofmodelfailsinseeminglysimpletasks,suchasmatchingthefirstandlastsentences when composing a poem, but inverting thesequence of words [3]. On the other hand, LLMs haveshown, at least partially, intricate abilities of generalizationinverbal tasks, inthe sense of producingcorrectanswerstonovel challenges that woulddemandcognitive efforts from\nhumans [3]. It has beenproposed, forinstancein[3],thatatleast some of the knowledge about these modelscouldonlybegarneredwithextensiveexperimentalstudies.In this study, we investigate one of the applications ofLLMs alreadyfoundinthe literature. Automatedsentimentanalysis has been a topic of interest, for instance, forclassificationofsentencesaccordingtotheaffectivecontent,and has already been performed with LLMs under variousconditions. In a recent paper [4], for example, the authorsfine-tunedpretrainedLLMs withdatasetscomprisingwordsandshort texts ratedintwodimensionsofaffectivecontent,namely valence and arousal, andevaluatedthe correlationsbetween estimates of those ratings andthe values assignedby the human subjects. In another study [5] encompassingseveral languages, the authors showedthatunmodifiedGPTmodels performed better in English than some fine-tunedmodels when employed in the analysis of sentiment ofheadlinesina7-pointLikertscale.Thosefindingsimplythatthis kind of task may be accomplished by LLMs withrelatively short prompts and reduced necessityof handlingdatasets and training models. Their comparison betweenzero-shot approaches, inwhichthe LLMs werenotexposedtocomplete examples of the task,witha‚Äúfew-shot‚Äùversionwhichincludedrelevant examples, is of note, althoughtheyobtainedmixedresults.Inthis paper we raise andpartiallyanswerthefollowingquestions: could off-the-shelf LLMs, as provided to thegeneral public, returnestimatesofthevalenceofwordswiththe use of the ‚Äúsimulacrum‚Äù paradigm? Are such LLMsamenable to simulation of different human characteristicswhennaturallanguageisusedtointroducespecificsemanticelementsinthedescriptionsofthosesimulacra?The objectives of this study are to assess theperformance of simulacra of human subjects, built ondifferent LLMs as providedtovia public APIs andwithoutanyadditional tuning, inestimatingthe hedonic valencesofwords as ratedbyhumansubjects.Inaddition,weperformasmall ablation study, comparing different characterizationsofthesimulacra. II. METHODS\nA. ModelsThree LLMs were used in this study: GPT-3.5, GPT-3with text-davinci-003 model and Bloom. GPT-3.5 isoperated in chat mode (abbreviatedGPTChat), similarlytowhat is seen in [5]; the other models are operated incompletionmode.\nXVI Brazilian Conference on Computational Intelligence (CBIC 2023), Salvador, October 8th to 11th\n1\nB. PromptsThebasispromptswerewrittensoastoremainsimilartothat of Table 2, column 4 of the reference study [5],employed in chat mode. The prompts for completionmodehadtoundergosubstantialmodifications,duetothedifferentnature of the task. Twoother prompts were includedinthisstudy, withmodifierstothesimulatedsubject.Modifierscanbenone,‚Äúaverage‚Äùand‚Äúoverlypositive‚Äù.The above mentioned basis prompt for GPTChat wasmodified in order to command (a) a response to a word,instead of a sentence, (b) a number ina scale from1to9,similar to that used in the reference study with humansubjects ([6], see Datasets sec. C)and(c)afacialemoji,notpresentintheoriginalstudy.Thebasispromptasfollows:How negative or positive is this word on a 1-9 scale?Answer only withafacial emoji andanumber,with1being'very negative' and 9 'very positive'. Here is the word:<word>The basis prompt for both GPT-3 and Bloom wasadapted fromthe above with two goals: toallowfor usingcompletionmode inbothLLMs, andtotest if thereferenceto the Self-Assessment Manikin (SAM), pictorial verbalreport instrument mentionedin[6] andemployedinsimilarstudies, would also be accepted by the LLMs. Thecompletionbasispromptis,thus:As an<modifier>adult writingjustafacialemojiandanumber on the SAM scale from 1 to 9, where 1 is verynegative and 9 is very positive, I would represent thevalenceoftheword'<word>' withC. DatasetsFroma dataset of lemmas, reportedin[6] andavailableonline, two sets of words were usedinthe present study: apilot set of 30 words, with which prompts and dataacquisition code were designed, including tuning of themodels‚Äô input parameters; a final set of 140 words, drawnfromthedatasetasorderedbyvalence,pickedatintervalsof100 words plus a random jitter of +1 or -1, with whichmachine valence ratings were compared to human ratings.This sampling procedure sought to provide a set of wordswith valences following a histogramsimilar to that of thefull13,915wordsinthedataset.D. ExperimentalproceduresAll data was acquired with custom code written inPython3. Aprompt generationcodesampledthe140wordsfromthe whole dataset andsaveda comma separatedvalue(CSV) file whichincludedthe word, the meanandstandarddeviationofhumanvalenceratings,andthewholeprompttobe input in each LLM. These files were generated for thepilot set tointeractivelycreateandvalidatethecodefordataacquisition and the basis prompts. This code used HTTPSrequests based on the RESTAPIs available for eachLLM:chat completion and completion APIs from Open AI(https://api.openai.com/v1), respectively for GPTChat andGPT-3; Inference API from Huggingface for Bloom(https://api-inference.huggingface.co/models/bigscience/bloom).The process of adjusting the reference prompt for thecompletionmode involvedalsothe modificationofsomeofthe input parameters of the models aiming to have themodels output numbers within the specified range, with aminimal number oftokensforthepilotsetof30words.Tab.I shows the final set of parameters usedfor the requisitionswith the final set. During prompt design, machine valence\nratings were not comparedtohumanratingstoavoidtuningthepromptstospecificrelationships.\nTABLEI. PARAMETERS USEDFOR\nAPIREQUESTS\n.ALLVALUES DEFAULTEXCEPTTHOSEINBOLDFACE\n.BLANKCELLS AREPARAMETERS NOTAVAILABLE\n.\nParameter\nModelGPTChat GPT-3 Bloommax_tokens 5 9 5\ntemperature 1 1 1\ntop_p 1 1 1\nn 1 1\nstream 0 0\nlogprobs null null\necho 0 0\nstop null null\npresence_penalty 0 0\nfrequency_penalty 1 1\nbest_of 1 1\nlogit_bias null null\ntop_k 1\nmax_time 30\ndo_sample 1\nrepetition_penalty 1\nnum_return_sequences 1\nThe CSVfiles for the final set were generatedwiththechosen prompts and sets of parameters. The LLMs werecommanded with the prompts, sequentiallyandinorder ofvalence, eachresponse beingsavedtoa CSVfile includingall columns from the input file plus the outputs fromtheLLMs and a timestamp. A prompt was repeated withincreasing time intervals if an error was received. Eachrequest was made as a new interaction, seeking to avoidcarryover information fromprevious requests. Each runofrequests, consisting of the chat mode prompt andthe threecompletionmode prompts for all words, was repeatedthreeconsecutivetimes,withnointervalbetweenruns.E. StatisticalanalysisData fromthe CSVfileswereanalyzedandfiguresweregenerated in Python 3. The histogramof the valence wascomputedfor the whole dataset, aswellasforthe140-wordsample,withbinsspacedby0.1units.For data analysis we considered the LLMs asinstruments to measure the valence of words, comparingtheir measurements to a gold-standard instrument, i.e. thevolunteers from the original study. Considering only thefinal set, the medianof the validoutputs of the 3runswereused as the representative value for each prompt. Somevalues were missing in the respective outputs, see Resultsahead, and inthose cases the medians were computedwiththeavailable1or2values.Thesemedianswerecomparedtothe human valences to assess how well themachine-generated ratings represented such referenceratings, by fitting a linear regression, and calculating thePearson correlation coefficients and paired t-tests fordifferences of the means [7]. Regression parameters,\nXVI Brazilian Conference on Computational Intelligence (CBIC 2023), Salvador, October 8th to 11th\n2\np-values, Pearson correlation coefficients and standarderrors were calculatedwiththe linregress functionfromtheScipy library. Fromthat set of values, the 95%confidenceintervals for intercept and slope were calculated, whichshows if the slopes and intercepts lie in an interval thatoverlaps the relevant values of 1.0 and 0.0 respectively.P-values below0.05(0.007withBonferronicorrectionfor7comparisons) were consideredsignificant.Thevariabilityofoutputs among the 3 runs was calculated as the range(difference betweenthemaximumandminimumnumber)oftheoutputsforeachword,promptandmodel.Emoji outputs were qualitatively evaluated, particularlyto assess if the requestedconstraint tobe facial emojis wasrespected. III. RESULTS\nThe data presented in this studywas generatedonJune14\nth\n, 2023. All requisitions, including any necessaryrepetitions of prompts, were completed from around10:55 a.m. to 12:51 p.m. BRT, with a total cost of aroundUS$1.50fortheOpenAIAPI.The wordsamplingstrategyresultedina sample of 140words withdistributionsimilar tothe histogramofvalencesof the whole dataset (Fig. 1A). Note that the valences arerepresented by the average responses from severalvolunteers, resulting in non-integer numbers. For thevalences generatedfromtheLLMoutputs,theBloommodelreturned some non-integer ratings with a 0.5 resolution,while both GPT models always returned integer numbers.However, some GPT outputs had no numbers, hence therewere 36 missing values possibly resulting in non-integermedians. Atotal of 34 words hadat least 1missingvalue,with2words(‚Äòpool‚Äôand‚Äòstupid‚Äô)missing2values.Our settings for the prompt requests resulted invariability of the numerical outputs for the same promptrepeated3times, withlarger variabilityfortheGPTmodelsthan for Bloom (Fig. 1B). The ‚Äòaverage‚Äô modifier had asmall effect on the variability compared to no modifier,while the ‚Äòoverlypositive‚Äômodifierincreasedthevariabilityof the Bloom model and decreased the variability of theGPT-3 model, although with larger ranges. Interestingly,some of the ranges were above 5whichguarantees that thesame word was classified with a positive and a negativevalence in different rounds. This includedthe word‚Äòrapist‚Äôhavinga range equal 8for beingclassifiedinbothextremesofthescaleforthesamesimulatedpersona.The paired t-tests comparing the meanmachine-generated and human valence ratings resulted innon-significant differences for GPT3 with no modifier(mean difference=0.23, p=0.061) and Bloom with‚ÄúAverage‚Äù modifier (mean difference=0.19, p=0.054). Theother mean machine ratings, GPTChat (mean difference=0.58, p<0.001), GPT3 with ‚ÄúAverage‚Äù (meandifference=0.38, p=0.001) and ‚ÄúOverly Positive‚Äù (meandifference=2.78, p<0.001) modifiers, Bloom with nomodifier (meandifference=0.66, p<0.001)andwith‚ÄúOverlyPositive‚Äù(meandifference=0.56,p<0.001)modifier,showedsignificantstatisticaldifferences.The median machine generated valence ratings werepositively correlated to human ratings, showing againdifferences between modifiers and models. The linearregression of ratings from GPTChat and GPT-3 withoutmodifiers had slopes and intercepts that were notdistinguishable from the identity in an 95% confidenceinterval (Figs. 1C and 1D, and Tab. II). The ‚Äòaverage‚Äô\nmodifier changed the regression within that confidenceinterval, but the ‚Äòoverlypositive‚Äômodifierclearlybiasedtheratings towardhigher values (Figs. 1Eand1F). The Bloommodel output showed less variability between words withconcentrationtowards5,whichresultedinaslopebelow1.0andintercept above 0.0evenforthebestcorrelationwithoutmodifier (Fig. 1G). The ‚Äòaverage‚Äô modifier changed theregression parameters out of the confidence intervaldecreasing the slope and increasing the intercept, whichwere further altered by the ‚Äòoverly positive‚Äô modifiertendency to increase the ratings (Figs. 1Hand1I, andTab.II).For the emoji output, Bloomdid not return any emojiwhile the GPT LLM returned emojis for all words andrepetitions. Most of these emojis were facial as requested,but some of them were objects and others (example:‚Äúseashore‚Äù prompted an ‚òÄemoji, and ‚Äòprickly‚Äô promptedü¶î ). IV. DISCUSSION\nThis small ablation study using LLMs without finetuning showed: 1) some similarities, identified as aregression line with confidence intervals of slopes around1.0 and intercepts around 0.0, between average humanreports and LLMestimates of valences for a set of words,whenthe LLMs wereprogrammedtorespondasadultswithno further backstory; 2) the effect of a short preamble,characterizing the simulated personas as ‚Äúoverly positive‚Äù,in breaking those similarities for the two LLMs; 3)substantial differences inoutputs from2LLMswithsimilarnumber of parameters when operated with the sameobjective prompts; and 4) measurable differences ‚Äìvariability ‚Äì among outputs of each LLM repeatedlyoperated witha givenprompt withina relativelyshort timeinterval.With exception of the overly positive modifier withGPT-3, the mean differences between machine and humangeneratedvalencesweresmallerthan1pointinthespecifiedscale. Although, the t-test results indicatedthat these meandifferences were not significant onlyfor twoof sevenpairsof models andprompts, inwhichthe meandifference fromthe human valences were less than 5%. More importantly,we not onlyfoundstatisticallysignificantregressionmodelslinking machine estimates of valence andhumanratings ofsingle words, but also slopes around the unity. In ourcontemplation, these findings addtoother evidence thattheoriginal models, without further adjustments, already havesome representation of the affective content of isolatedwords. One simple explanation to that would be that themodels were trainedwithcorporathatexplicitlycontainthatinformation (for instance, text that reproduces the datasetherein studied or others). With that respect, in [5] theauthorsstatetheircautioninchoosing,fortheirexperiments,datasets published only after the models they tested weretrained. However, even if a scrutinization of the trainingcorpora showed that this explanation holds, the observedvariabilities in responses could mean that the ratings arefurther influenced, perhaps fromtext other thanthe dataset.On the other hand, the absence of explicit associationsbetween numbers and words in the training corpora mayrequire evenmore complexexplanations, ones that maynotbe available at this time since, to our knowledge,explainabilityisnotyetimplementedforthosemodels[3].\nXVI Brazilian Conference on Computational Intelligence (CBIC 2023), Salvador, October 8th to 11th\n3\nFig.1. Summary of experimental results. (A) Histogram of the human hedonic valence ratings of the original 13,915 words dataset (light blue)superimposed by the histogramof the sampled 140 words final set (yellow). (B) Box plots representing the variability in machinegeneratedvalenceratings measured as the difference between the maximumand minimumvalues output for 3 repetitions of the same prompts. For GPTChat and AvGPT3 one sample was excluded from this plot because two repetitions did not have numbers. Scatter plots of human valence ratings versus (C)GPTChat, (D-F) GPT3, and (G-I) Bloomgenerated ratings. Circles represent the experimental data(mediansofthe3repetitions),continuouslineisalinear regression; identity lines are dashed. (D, G) Prompt without modifiers, Ain the box plot.(E,H)Promptwith‚Äòaverage‚Äômodifier,Avintheboxplot.(F,I)Promptwith‚Äòoverlypositive‚Äômodifier,OPintheboxplot.\nTABLEII. COEFFICIENTS OF LINEARREGRESSIONBETWEENMACHINEGENERATEDANDHUMANVALENCERATINGS\n.VALUES AREESTIMATION\n(95%CI)\nModel Modifier Intercept Slope r\nGPTChat None\n0.55(-0.30,1.40)\n1.01(0.84,1.17)\n0.72\nGPT-3\nNone\n-0.06(-1.04,0.93)\n1.06(0.87,1.25)\n0.69\nAverage\n0.36(-0.52,1.25)\n1.00(0.83,1.17)\n0.70\nOverlypositive\n4.67(3.95,5.39)\n0.63(0.49,0.76)\n0.61\nBloom\nNone\n0.94(0.04,1.84)\n0.69(0.51,0.86)\n0.56\nAverage\n2.64(2.06,3.23)\n0.44(0.33,0.55)\n0.55\nOverlypositive\n3.14(2.18,4.10)\n0.49(0.31,0.68)\n0.41\nThe linear regression between machine estimates andhumanratings was clearlyaffectedbyasmallchangeintheprompt. For the GPTmodel,whiletheoutputnumbersseemclosely related when using the modifier ‚Äòaverage‚Äô or nomodifier, the ‚Äòoverly positive‚Äô modifier resulted in largerintercepts and smaller slopes. This illustrates how smallchanges in prompts can have significant effects on theoutput of such models. In the current example, the resultswere as expected possibly because ‚Äòaverage adult‚Äô and‚Äòadult‚Äô have relatedmeanings andthe correspondingtokenspropagate similarlywithinthe LLMs, whereas the modifier‚Äòoverly positive‚Äô mayhave propagatedalongthe ANNas akind of ‚Äúbiasing term‚Äù, resulting in larger numbers,semantically aligned with more ‚Äúpositive‚Äù valences, in thesense usually employed in such cases. The outputs of theBloommodel were alsoaffectedbythemodifiers.However,\nXVI Brazilian Conference on Computational Intelligence (CBIC 2023), Salvador, October 8th to 11th\n4\nwhile ‚Äòoverly positive‚Äô alsoresultedinthe largest interceptandsmallest slope, the effectof‚Äòaverage‚Äôwasmoremarkedthan with GPT, inthe same directionof the other modifier.The small sample of modifiers, with a single set ofparameters, does not allow to assess if Bloom is moresensitive tochanges inthe prompt thanthe GPTs, or if it isjust the case of the specific choice of prompt design orchoice of parameters. Also, we can not be sure that thisimpersonation observed with GPT will be consistent forothermodifiers,suchas‚Äúoverlynegative‚Äù.As discussed above, the GPT-3 and Bloom modelsshowedobservabledifferenceswithshort,objectivepromptsfor short,quantitativeoutputs.Bothmodelsareadvertisedtohave similar numbers of parameters (175 vs 176 Gigaparameters, respectively) [8, 9]. Somepossibleexplanationsfor the divergent behaviors could be linked, first of all, todifferences intraining, broadlyspeaking, includingnotonlydifferences between corpora but also in the objectivefunctions, algorithms for trainingandsoforth.Additionally,the API to the models have manydifferent parameters thatmay not be directly comparable, either for not having thesame names or functions, or havingdifferentweightsonthemodel output. Thus, the differences observed in this studycouldbe a consequence ofourchoicesforthoseparameters.Overall, these results seem to offer evidence that evensimilar LLMs may display quite divergent performanceseven in simple tasks, andthat cautionshouldbe exertedinchoosinganLLM, a corpus anda methodtofine-tuneitforagivenpurpose.The responses to the prompts, in the case of the GPTmodels, varied to the point that, with the constraintinvolving the number of tokens imposed in this study, insome of those responses the number to represent valencewas missing. This limitation may be due to the phrasingused in the prompts ‚Äì which were written so as toapproximately track the prompts in one of the referencestudies [5] ‚Äì, the chosen parameters, for instance thetemperature, and other factors. Further crafting of theprompts may help eliciting more adequate responses,perhaps with even less tokens. Interestingly, withtriplicaterequests of the prompts we observedvariations not onlyinthe structure of the output, but alsosubstantial variationinthe ratings. Researchers should consider this variabilitywhenusingLLMsforautomaticsentimentanalysis,becauseour observations indicate valences changing betweenpositive andnegative, evenwhenwords were not classifiedin the middle of the scale by humans. This limitationmayeven be present with a temperature of 0, given that it isunknownwhere the model‚Äôsmostprobableoutputfallsforagivenprompt. Some alternatives couldbetouseabootstrapstrategy, which wouldincrease the costs of experiments, orto move from our zero-shot approach to the ‚Äúfew-shot‚Äùlearning. However, this last strategyhadinconsistentresultsintasksrelatedtosentimentanalysis[5].In our prompts we includedthe request for the answersto include a face emoji. Such a pictorial form ofcommunication is increasingly popular in daily textconversationsandhasbeensubjectofresearchinthecontextof sentiment analysis [10]. Althoughwe have not analyzedthe emoji outputs to quantify their correspondence withnumerical ratings, as seenin[10], visual inspectionshowedthat not all generated emojis hadfaces as requested. Somespeculative hypothesis for this result could be that theinternal mechanisms of the LLMs did not give adequateweight tothe terms ‚Äòfacial‚Äô and‚Äòemoji‚Äô as connectedtotherequest for an emoji and the valence scale, or perhaps not\nenough emoji data was trained into the LLMs tomake thesame inferences it has done for numerical scales. This maybe true particularly for Bloom, since it did not output anyemoji at all. This warrants further research in order tounderstandLLM‚Äôs emoji output, for example, byusingdataofhumanclassificationsofemojiemotionalstates[10].There are a plethora of openissuesaroundthequestionsthis study tries to answer. For instance, what is the trueweight of usingthe expressionSAM, a householdacronymin the study of affective stimuli? Could it be that otherexpressions or instructions, cf. [6] by choosing aninstrument for verbal report differentfromtheSAM,ifusedtoprompt anLLM, wouldcause some kindofeffect?Note,concerningthesequestions,thattheGPTChatprompt,whichdid not use SAMor the word‚Äòvalence‚Äô, hadsimilar resultsto the GPT-3 without modifiers. We ran an additionalpost-experiment, repeating the procedures for GPT-3without modifier but changing ‚ÄòSAM‚Äô to ‚ÄòRPE‚Äô, an usualacronymfor Rating of PerceivedExertionscale commonlyused to measure intensity of exercise relatingnumbers andphrases indicating howlight/heavy is an activity[11]. Thenumbers returned had similar variability (mean range of2.12), smaller slope (0.78, CI 0.61‚Äì0.95) and higherintercept (1.59, CI 0.7‚Äì2.48). Althoughthese twoexamplescannot gauge the weight of the acronym‚ÄòSAM‚Äô, theyagainshow that our results are sensitive to small promptmodifications, andthat identifyingeffects of specificwordsmaydemandprobingalargenumberofdimensions.Another prominent matter is the choice of language, ofparticular interest to non-English speaking applications. Adetailed study of, say, responses to prompts in Portugueseand Spanish may be helpful for potential users in SouthAmerica, for instance.Astothezero-vsfew-shotstrategies,mentioned before, it may also be interesting to assess itseffect, whichhas a practical implicationas,atleastfornow,the more tokens in the input and the output, the moreexpensive the experiments. In the same economic vein, itwouldbe interestingtoknowwhether the LLMs areabletooutput reasonable answers tomore thanone wordatatime,for instance byusingplurals inthe prompt andpresentingaset of words, insteadof just one. The dual approachwouldbe to command responses frommany‚Äúsimulacra‚Äù at once,for instance adults of variousagesandbackgrounds,withorwithout characteristics in common. Combinations of bothapproachesmayproveevenmorecost-effective.As to the potential applications of LLMs with suchnative capabilities, both analysis and synthesis may provehelpful to researchers in areas that require gathering (orcreating) stimuli with given characteristics. One examplewould be the use of sentences with different affectivecontent to elicit emotions and physiological responses. IfLLMs couldbothsynthesize andthenanalyze sentences ondemand, and if the synthesis could be tailored to specificsubjects (as would be the case of ‚Äúoverly positive adults‚Äù,for instance), experimenters could be endowed with apowerful research tool. Nevertheless, there may be somepitfalls: as an example, there is the potential that certainbiases ‚Äì a preference for a given group of adjectives, orsubstantives, or something else ‚Äì spread across otherwisedifferent research groups if they all use the same tool toproduce textual stimuli, or between stimulus sets thatotherwisewouldbemoreindependentofoneanother.As anexercise onsome of the potential uses mentionedabove, a short, unstructuredpost-experimentwasperformedinChatGPTMay24version,forthesakeofexemplification.\nXVI Brazilian Conference on Computational Intelligence (CBIC 2023), Salvador, October 8th to 11th\n5\nThe prompt andcorrespondingoutputcanbeseeninTab.3.The prompt resulted in 3 different sets of words, withdiverse valences andemojisassociatedwiththem.Althoughsome limitations are clear,forinstanceonlyoneemojihadaface, it alsoshows for asingleprompt,aresponsewithbothsynthesis and analysis, and outputs from more than onesimulacra at once, illustratingthus some of the pointsmadeabove.A given set of ratings may represent what a singleperson, anexistinggroupof people, oranon-existentgroupwould assigntothe respective words. Inanycase, it wouldnot represent every group, much less every person. Now,supposing that these ratings are implicitly used when anLLMis prompted to synthesize a text with some requiredaffective value, it seems as thoughtheresultingtextswouldbe biased towards whatever the ratings of the words are.This argument, even if not directly applicable to therelationship between words and complex text, could beextended to ratings of short sentences, idioms andsoforth.Abroader questionthat arises wouldbe: what couldbe theimpact for societiesofthemassive,automatedproductionoftext by a handful of models that embed one specificaffective mapping? This studyshows that includingtraitsinthe descriptionof the simulacra couldalter this mappingtosome extent. Further work is needed inorder toassess themagnitude, breadthandlimitations ofthatkindoftechniqueto produce variability in contents of text synthesized byLLMs.\nTABLEIII. OUTPUT FROM\nCHAT\nGPTWHENPROMPTEDTOPRODUCEORIGINALNEOLOGISMS AS IF CREATEDBY\n3ADULTS\n,WITHASSOCIATEDVALENCERATINGS ANDEMOJIS\n,TOREPRESENTTHREEDIFFERENTCONCEPTS\n.\nPrompt\nNowpleaseforgeteverythingyouhaveread.Ignoresearchresults.Nowbegins a new subject. Answering as a human adult, please write theresponse of three adults of various ages and backgrounds to thefollowing demand: \"Pleasecreatethreeoriginalneologismstorepresentthe concepts of 'crossing muddywaters','flyinginaclearbluesky',and'same old, same old', and use one facial emoji and one number on thescale from 1 to 9, where 1 is very negative and 9 is very positive, torepresent the valence of each of the neologisms. Write onlythewords,emojisandratings,withoutfurthercomments.\"\nOutput[11]\nAdult1:\nNeologism1:Muddlance (6)\nNeologism2:Skycruise (8)\nNeologism3:Rutrepeat (3)\nAdult2:\nNeologism1:Slogstream (4)\nNeologism2:Claritysoar (9)Neologism3:Monotale:livros:*(2)\nAdult3:\nNeologism1:Murktraverse (5)\nNeologism2:Celestialglide (7)\nNeologism3:Copycatdrone (1)*TheLLMreturnedanemoji,butit‚Äôsnotavailableonourtexteditor.\nAs main limitations, this study focuses on a relativelysmall number of samples, which may impact itsgeneralization. Inaddition,thepromptsandparametersweremanually and empirically adjusted with the outputs of the30-word pilot set. Thus, it is possible that there are choicesofpromptsandparametersthatreturnbetterresults.V. CONCLUSION\nThis small study answers the questions posed withevidence thatitmaybefeasibletouseLLMswithoutfurtherfine tuning, withshort andobjective prompts, inthetaskofestimating the hedonic valences of words. This work alsoshows that these estimatesmaybepurposefullymodifiedbysmall changes inthe prompts, soas tocommandthe LLMsto respond as a ‚Äúsimulacrum‚Äù of a human with certaincharacteristics. Finally, this study alsoshows that there aremany known, and probably many more unknown, factorsthat affect these estimates. Thus, future work mayrefer tostudy which parameters of the used LLMs influence theobserved variability in ratings, how well these modelswould performwithfine tuningor simulatingother axes ofhumanemotionssuchasarousalanddominance.ACKNOWLEDGMENT\nAuthors acknowledge the participationofHenriqueSerdeira(NCE/UFRJ) as an observer along the development of thestudy. F.C.J. thanks EduardoCardosoLohmannforprolificconversationsonprobingcapabilitiesandlimitsofLLMs.\nREFERENCES\n[1] M. Binz and E. Schulz, ‚ÄúUsing cognitive psychology to understandGPT-3,‚Äù Proc. Natl. Acad. Sci. USA, vol. 120, n. 6, p.e2218523120,2023.[2] J.J. Horton,‚ÄúLargeLanguageModelsasSimulatedEconomicAgents:What Can We Learn from Homo Silicus?‚Äù arXiv preprintarXiv:2301.07543,2023.[3] S Bubeck et al., ‚ÄúSparks of artificial general intelligence: EarlyexperimentswithGPT-4,‚ÄùarXivpreprintarXiv:2303.12712,2023.[4] G.A. Mendes and B. Martins. \"Quantifying Valence and Arousal inText with Multilingual Pre-trained Transformers.\" EuropeanConferenceon InformationRetrieval,pp84-100,2023.[5] S. Rathje et al., ‚ÄúGPT is an effective tool for multilingualpsychological text analysis,‚Äù PsyArXiv preprint PsyArXiv:sekf5,2023.[6] A.B. Warriner, V. Kuperman and M. Brysbaert, ‚ÄúNorms of valence,arousal, and dominance for 13,915 English lemmas,‚Äù Behav. Res.Meth.,v.45,p.1191-1207,2013.[7] Lasaitis C, et al, ‚ÄúBrazilian norms for the International AffectivePicture System (IAPS): comparison of the affective ratings for newstimuli between Brazilian and North-American subjects‚Äù J braspsiquiatr[Internet],2008.[8] T. B. Brown et al., ‚ÄúLanguage Models areFew-ShotLearners‚ÄùarXivpreprintarXiv:2005.14165,2020.[9] BigScience Workshop, ‚ÄúBLOOM: A 176B-Parameter Open-AccessMultilingual Language Model‚Äù arXiv preprint arXiv:2211.05100,2023.[10] G. Kutsuzawa, et al., \"Classification of 74 facial emoji‚Äôs emotionalstatesonthevalence-arousalaxes\"SciRepv.12,p.398,2022.[11] R. CWilson and P. W. Jones, ‚ÄúAcomparison of the visual analoguescale and modified Borg scale for the measurement of dyspnoeaduring exercise.‚Äù Clinical Science (London, England: 1979),v.76,n.3,p.277‚Äì282,1989.\nXVI Brazilian Conference on Computational Intelligence (CBIC 2023), Salvador, October 8th to 11th\n6",
  "topic": "Valence (chemistry)",
  "concepts": [
    {
      "name": "Valence (chemistry)",
      "score": 0.7764770984649658
    },
    {
      "name": "Computer science",
      "score": 0.5158438086509705
    },
    {
      "name": "Confidence interval",
      "score": 0.4266428053379059
    },
    {
      "name": "Mode (computer interface)",
      "score": 0.4179025888442993
    },
    {
      "name": "Psychology",
      "score": 0.3709750771522522
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35429954528808594
    },
    {
      "name": "Natural language processing",
      "score": 0.3401980400085449
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3214362859725952
    },
    {
      "name": "Statistics",
      "score": 0.2691473364830017
    },
    {
      "name": "Mathematics",
      "score": 0.17215648293495178
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.11490386724472046
    },
    {
      "name": "Chemistry",
      "score": 0.08690199255943298
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}