{
    "title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla",
    "url": "https://openalex.org/W4287888679",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1969962628",
            "name": "Abhik Bhattacharjee",
            "affiliations": [
                "University of Rochester"
            ]
        },
        {
            "id": "https://openalex.org/A2771091425",
            "name": "Tahmid Hasan",
            "affiliations": [
                "University of Rochester"
            ]
        },
        {
            "id": "https://openalex.org/A2184437607",
            "name": "Wasi Ahmad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287894590",
            "name": "Kazi Samin Mubasshir",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2398801712",
            "name": "Md. Saiful Islam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2309093015",
            "name": "Anindya Iqbal",
            "affiliations": [
                "University of Rochester"
            ]
        },
        {
            "id": "https://openalex.org/A2159793110",
            "name": "M. Sohel Rahman",
            "affiliations": [
                "University of Rochester"
            ]
        },
        {
            "id": "https://openalex.org/A26972591",
            "name": "Rifat Shahriyar",
            "affiliations": [
                "University of Rochester"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3162296828",
        "https://openalex.org/W3103187652",
        "https://openalex.org/W3104688854",
        "https://openalex.org/W2915977242",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3035032094",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W2061272101",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W4385681388",
        "https://openalex.org/W3198897702",
        "https://openalex.org/W3013836620",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4287116725",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W3099919888",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2971120622",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2903101678",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2902736485",
        "https://openalex.org/W4287760320",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3038047279",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2591259438",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W222053410",
        "https://openalex.org/W2973088264",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3170705121",
        "https://openalex.org/W4294294857",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2990188683",
        "https://openalex.org/W2902800387",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4299574851",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2995230342",
        "https://openalex.org/W3174269049",
        "https://openalex.org/W3212697960"
    ],
    "abstract": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad, Kazi Samin Mubasshir, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar. Findings of the Association for Computational Linguistics: NAACL 2022. 2022.",
    "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1318 - 1327\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nBanglaBERT: Language Model Pretraining and Benchmarks for\nLow-Resource Language Understanding Evaluation in Bangla\nAbhik Bhattacharjee1∗, Tahmid Hasan1∗, Wasi Uddin Ahmad2†, Kazi Samin1,\nMd Saiful Islam3, Anindya Iqbal1, M. Sohel Rahman1, Rifat Shahriyar1\nBangladesh University of Engineering and Technology (BUET)1,\nAWS AI Labs2, University of Rochester3\nabhik@ra.cse.buet.ac.bd, {tahmidhasan,rifat}@cse.buet.ac.bd\nAbstract\nIn this work, we introduce BanglaBERT, a\nBERT-based Natural Language Understand-\ning (NLU) model pretrained in Bangla, a\nwidely spoken yet low-resource language in\nthe NLP literature. To pretrain BanglaBERT,\nwe collect 27.5 GB of Bangla pretraining data\n(dubbed ‘Bangla2B+’) by crawling 110 pop-\nular Bangla sites. We introduce two down-\nstream task datasets on natural language in-\nference and question answering and bench-\nmark on four diverse NLU tasks covering\ntext classiﬁcation, sequence labeling, and span\nprediction. In the process, we bring them\nunder the ﬁrst-ever Bangla Language Under-\nstanding Benchmark (BLUB). BanglaBERT\nachieves state-of-the-art results outperforming\nmultilingual and monolingual models. We\nare making the models, datasets, and a leader-\nboard publicly available at https://github.\ncom/csebuetnlp/banglabert to advance\nBangla NLP.\n1 Introduction\nDespite being the sixth most spoken language in\nthe world with over 300 million native speakers\nconstituting 4% of the world’s total population,1\nBangla is considered a resource-scarce language.\nJoshi et al. (2020b) categorized Bangla in the lan-\nguage group that lacks efforts in labeled data col-\nlection and relies on self-supervised pretraining\n(Devlin et al., 2019; Radford et al., 2019; Liu et al.,\n2019) to boost the natural language understanding\n(NLU) task performances. To date, the Bangla lan-\nguage has been continuing to rely on ﬁne-tuning\nmultilingual pretrained language models (PLMs)\n(Ashraﬁ et al., 2020; Das et al., 2021; Islam et al.,\n2021). However, since multilingual PLMs cover\na wide range of languages (Conneau and Lample,\n2019; Conneau et al., 2020), they are large (have\n∗These authors contributed equally to this work.\n†Work done while at UCLA.\n1https://w.wiki/Psq\nhundreds of millions of parameters) and require\nsubstantial computational resources for ﬁne-tuning.\nThey also tend to show degraded performance for\nlow-resource languages (Wu and Dredze, 2020) on\ndownstream NLU tasks. Motivated by the triumph\nof language-speciﬁc models (Martin et al. (2020);\nPolignano et al. (2019); Canete et al. (2020); An-\ntoun et al. (2020), inter alia ) over multilingual\nmodels in many other languages, in this work,\nwe present BanglaBERT – a BERT-based (Devlin\net al., 2019) Bangla NLU model pretrained on 27.5\nGB data (which we name ‘Bangla2B+’) we metic-\nulously crawled 110 popular Bangla websites to fa-\ncilitate NLU applications in Bangla. Since most of\nthe downstream task datasets for NLP applications\nare in the English language, to facilitate zero-shot\ntransfer learning between English and Bangla, we\nadditionally pretrain a model in both languages; we\nname the model BanglishBERT.\nWe also introduce two datasets on Bangla Natu-\nral Language Inference (NLI) and Question An-\nswering (QA), tasks previously unexplored in\nBangla, and evaluate both pretrained models on\nfour diverse downstream tasks on sentiment clas-\nsiﬁcation, NLI, named entity recognition, and QA.\nWe bring these tasks together to establish the ﬁrst-\never Bangla Language Understanding Benchmark\n(BLUB). We compare widely used multilingual\nmodels to BanglaBERT using BLUB and ﬁnd that\nboth models excel on all the tasks.\nWe summarize our contributions as follows:\n1. We present two pretrained models in Bangla:\nBanglaBERT and BanglishBERT, and intro-\nduce new Bangla NLI and QA datasets.\n2. We introduce the Bangla Language Under-\nstanding Benchmark (BLUB) and show that,\nin the supervised setting, BanglaBERT outper-\nforms mBERT and XLM-R (base) by 6.8 and\n4.3 BLUB scores, while in zero-shot cross-\nlingual transfer, BanglishBERT outperforms\n1318\nthem by 15.8 and 10.8, respectively.\n3. We provide the code, models, and a leader-\nboard to spur future research on Bangla NLU.\n2 BanglaBERT\n2.1 Pretraining Data\nA high volume of good quality text data is a prereq-\nuisite for pretraining large language models. For\ninstance, BERT (Devlin et al., 2019) is pretrained\non the English Wikipedia and the Books corpus\n(Zhu et al., 2015) containing 3.3 billion tokens.\nSubsequent works like RoBERTa (Liu et al., 2019)\nand XLNet (Yang et al., 2019) used more extensive\nweb-crawled data with heavy ﬁltering and cleaning.\nBangla is a rather resource-constrained language\nin the web domain; for example, the Bangla\nWikipedia dump from July 2021 is only 650 MB,\ntwo orders of magnitudes smaller than the English\nWikipedia. As a result, we had to crawl the web\nextensively to collect our pretraining data. We se-\nlected 110 Bangla websites by their Amazon Alexa\nrankings2 and the volume and quality of extractable\ntexts by inspecting each website. The contents in-\ncluded encyclopedias, news, blogs, e-books, sto-\nries, social media/forums, etc.3 The amount of data\ntotaled around 35 GB.\nThere are noisy sources of Bangla data dumps, a\ncouple of prominent ones being OSCAR (Suárez\net al., 2019) and CCNet (Wenzek et al., 2020). They\ncontained many offensive texts; we found them\ninfeasible to clean thoroughly. Fearing their po-\ntentially harmful impacts (Luccioni and Viviano,\n2021), we opted not to use them. We further dis-\ncuss ethical considerations at the end of the paper.\n2.2 Pre-processing\nWe performed thorough deduplication on the pre-\ntraining data, removed non-textual contents (e.g.,\nHTML/JavaScript tags), and ﬁltered out non-\nBangla pages using a language classiﬁer (Joulin\net al., 2017). After the processing, the dataset was\nreduced to 27.5 GB in size containing 5.25M docu-\nments having 306.66 words on average.\nWe trained a Wordpiece (Wu et al., 2016) vo-\ncabulary of 32k subword tokens on the resulting\ncorpus with a 400 character alphabet, kept larger\nthan the native Bangla alphabet to capture code-\nswitching (Poplack, 1980) and allow romanized\n2www.alexa.com/topsites/countries/BD\n3The complete list can be found in the Appendix.\nBangla contents for better generalization. We lim-\nited the length of a training sample to 512 tokens\nand did not cross document boundaries (Liu et al.,\n2019) while creating a data point. After tokeniza-\ntion, we had 7.18M samples with an average length\nof 304.14 tokens and containing 2.18B tokens in\ntotal; hence we named the dataset ‘Bangla2B+’.\n2.3 Pretraining Objective\nSelf-supervised pretraining objectives leverage un-\nlabeled data. For example, BERT (Devlin et al.,\n2019) was pretrained with masked language mod-\neling (MLM) and next sentence prediction (NSP).\nSeveral works built on top of this, e.g., RoBERTa\n(Liu et al., 2019) removed NSP and pretrained with\nlonger sequences, SpanBERT (Joshi et al., 2020a)\nmasked contiguous spans of tokens, while works\nlike XLNet (Yang et al., 2019) introduced objec-\ntives like factorized language modeling.\nWe pretrained BanglaBERT using ELECTRA\n(Clark et al., 2020b), pretrained with the Replaced\nToken Detection (RTD) objective, where a gener-\nator and a discriminator model are trained jointly.\nThe generator is fed as input a sequence with a\nportion of the tokens masked (15% in our case)\nand is asked to predict them using the rest of the\ninput (i.e., standard MLM). The masked tokens\nare then replaced by tokens sampled from the gen-\nerator’s output distribution for the corresponding\nmasks, and the discriminator then has to predict\nwhether each token is from the original sequence\nor not. After pretraining, the discriminator is used\nfor ﬁne-tuning. Clark et al. (2020b) argued that\nRTD back-propagates loss from all tokens of a se-\nquence, in contrast to 15% tokens of the MLM ob-\njective, giving the model more signals to learn from.\nEvidently, ELECTRA achieves comparable down-\nstream performance to RoBERTa or XLNet with\nonly a quarter of their training time. This compu-\ntational efﬁciency motivated us to use ELECTRA\nfor our implementation of BanglaBERT.\n2.4 Model Architecture & Hyperparameters\nWe pretrained the base ELECTRA model (a 12-\nlayer Transformer encoder with 768 embedding\nsize, 768 hidden size, 12 attention heads, 3072\nfeed-forward size, generator-to-discriminator ratio\n1\n3 , 110M parameters) with 256 batch size for 2.5M\nsteps on a v3-8 TPU instance on GCP. We used\nthe Adam (Kingma and Ba, 2015) optimizer with a\n2e-4 learning rate and linear warmup of 10k steps.\n1319\nTask Corpus |Train| |Dev| |Test| Metric Domain\nSentiment Classiﬁcation SentNoB 12,575 1,567 1,567 Macro-F1 Social Media\nNatural Language Inference BNLI 381,449 2,419 4,895 Accuracy Miscellaneous\nNamed Entity Recognition MultiCoNER 14,500 800 800 Micro-F1 Miscellaneous\nQuestion Answering BQA, TyDiQA 127,771 2,502 2,504 EM/F1 Wikipedia\nTable 1: Statistics of the Bangla Language Understanding Evaluation (BLUB) benchmark.\n2.5 BanglishBERT\nOften labeled data in a low-resource language for a\ntask may not be available but be abundant in high-\nresource languages like English. In these scenar-\nios, zero-shot cross-lingual transfer (Artetxe and\nSchwenk, 2019) provides an effective way to be\nstill able to train a multilingual model on that task\nusing the high-resource languages and be able to\ntransfer to low-resource ones. To this end, we pre-\ntrained a bilingual model, named BanglishBERT,\non Bangla and English together using the same set\nof hyperparameters mentioned earlier. We used the\nBERT pretraining corpus as the English data and\ntrained a joint bilingual vocabulary (each language\nhaving ∼16k tokens). We upsampled the Bangla\ndata during training to equal the participation of\nboth languages.\n3 The Bangla Language Understanding\nBenchmark (BLUB)\nMany works have studied different Bangla NLU\ntasks in isolation, e.g., sentiment classiﬁcation\n(Das and Bandyopadhyay, 2010; Sharfuddin et al.,\n2018; Tripto and Ali, 2018), semantic textual simi-\nlarity (Shajalal and Aono, 2018), parts-of-speech\n(PoS) tagging (Alam et al., 2016), named entity\nrecognition (NER) (Ashraﬁ et al., 2020). How-\never, Bangla NLU has not yet had a comprehen-\nsive, uniﬁed study. Motivated by the surge of NLU\nresearch brought about by benchmarks in other lan-\nguages, e.g., English (Wang et al., 2018), French\n(Le et al., 2020), Korean (Park et al., 2021), we\nestablish the ﬁrst-ever Bangla Language Under-\nstanding Benchmark (BLUB). NLU generally com-\nprises three types of tasks: text classiﬁcation, se-\nquence labeling, and text span prediction. Text\nclassiﬁcation tasks can further be sub-divided into\nsingle-sequence and sequence-pair classiﬁcation.\nTherefore, we consider a total of four tasks for\nBLUB. For each task type, we carefully select one\ndownstream task dataset. We emphasize the quality\nand open availability of the datasets while making\nthe selection. We brieﬂy mention them below.\n1. Single-Sequence Classiﬁcation Sentiment\nclassiﬁcation is perhaps the most-studied Bangla\nNLU task, with some of the earlier works dat-\ning back over a decade (Das and Bandyopadhyay,\n2010). Hence, we chose this as the single-sequence\nclassiﬁcation task. However, most Bangla senti-\nment classiﬁcation datasets are not publicly avail-\nable. We could only ﬁnd two public datasets: BYSA\n(Tripto and Ali, 2018) and SentNoB (Islam et al.,\n2021). We found BYSA to have many duplications.\nEven worse, many duplicates had different labels.\nSentNoB had better quality and covered a broader\nset of domains, making the classiﬁcation task more\nchallenging. Hence, we opted to use the latter.\n2. Sequence-pair Classiﬁcation In contrast to\nsingle-sequence classiﬁcation, there has been a\ndearth of sequence-pair classiﬁcation works in\nBangla. We found work on semantic textual simi-\nlarity (Shajalal and Aono, 2018), but the dataset is\nnot publicly available. As such, we curated a new\nBangla Natural Language Inference (BNLI) dataset\nfor sequence-pair classiﬁcation. We chose NLI as\nthe representative task due to its fundamental im-\nportance in NLU. Given two sentences, a premise\nand a hypothesis as input, a model is tasked to pre-\ndict whether or not the hypothesis is entailment,\ncontradiction, or neutral to the premise. We used\nthe same curation procedure as the XNLI (Conneau\net al., 2018) dataset: we translated the MultiNLI\n(Williams et al., 2018) training data using the En-\nglish to Bangla translation model by Hasan et al.\n(2020) and had the evaluation sets translated by\nexpert human translators.4 Due to the possibility of\nthe incursion of errors during automatic translation,\nwe used the Language-Agnostic BERT Sentence\nEmbeddings (LaBSE) (Feng et al., 2020) of the\ntranslations and original sentences to compute their\nsimilarity and discarded all sentences below a simi-\nlarity threshold of 0.70. Moreover, to ensure good-\nquality human translation, we used similar quality\nassurance strategies as Guzmán et al. (2019).\n4More details are presented in the ethical considerations\nsection.\n1320\nModels |Params.| SC NLI NER QA BLUB Score\nZero-shot cross-lingual transfer\nmBERT 180M 27.05 62.22 39.27 59.01/64.18 50.35\nXLM-R (base) 270M 42.03 72.18 45.37 55.03/61.83 55.29\nXLM-R (large) 550M 49.49 78.13 56.48 71.13/77.70 66.59\nBanglishBERT 110M 48.39 75.26 55.56 72.87/78.63 66.14\nSupervised ﬁne-tuning\nmBERT 180M 67.59 75.13 68.97 67.12/72.64 70.29\nXLM-R (base) 270M 69.54 78.46 73.32 68.09/74.27 72.82\nXLM-R (large) 550M 70.97 82.40 78.39 73.15 /79.06 76.79\nIndicBERT 18M 68.41 77.11 54.13 50.84/57.47 61.59\nsahajBERT 18M 71.12 76.92 70.94 65.48/70.69 71.03\nBanglishBERT 110M 70.61 80.95 76.28 72.43/78.40 75.73\nBanglaBERT 110M 72.89 82.80 77.78 72.63 /79.34 77.09\nTable 2: Performance comparison of pretrained models on different downstream tasks. Scores in bold texts have\nstatistically signiﬁcant (p <0.05) difference from others with bootstrap sampling (Koehn, 2004).\n3. Sequence Labeling In this task, all words of\na text sequence have to be classiﬁed. Named En-\ntity Recognition (NER) and Parts-of-Speech (PoS)\ntagging are two of the most prominent sequence\nlabeling tasks. We chose the Bangla portion of Se-\nmEval 2022 MultiCoNER (Malmasi et al., 2022)\ndataset for BLUB.\n4. Span Prediction Extractive question answer-\ning is a standard choice for text span predic-\ntion. Similar to BNLI, we machine-translated the\nSQuAD 2.0 (Rajpurkar et al., 2018) dataset and\nused it as the training set (BQA). For validation\nand test, We used the Bangla portion of the Ty-\nDiQA5 (Clark et al., 2020a) dataset. We posed the\ntask analogous to SQuAD 2.0: presented with a\ntext passage and a question, a model has to pre-\ndict whether or not it is answerable. If answerable,\nthe model has to ﬁnd the minimal text span that\nanswers the question.\nWe present detailed statistics of the BLUB\nbenchmark in Table 1.\n4 Experiments & Results\nSetup We ﬁne-tuned BanglaBERT and Banglish-\nBERT on the four downstream tasks and compared\nthem with several multilingual models: mBERT\n(Devlin et al., 2019), XLM-R base and large (Con-\nneau et al., 2020), and IndicBERT (Kakwani et al.,\n2020), a multilingual model for Indian languages;\nand sahajBERT (Diskin et al., 2021), an ALBERT-\nbased (Lan et al., 2020) PLM for Bangla. All pre-\n5We removed the Yes/No questions from TyDiQA and sub-\nsampled the unanswerable questions to have equal proportion.\ntrained models were ﬁne-tuned for 3-20 epochs\nwith batch size 32, and the learning rate was tuned\nfrom {2e-5, 3e-5, 4e-5, 5e-5}. The ﬁnal models\nwere selected based on the validation performances\nafter each epoch. We performed ﬁne-tuning with\nthree random seeds and reported their average\nscores in Table 2. We reported the average per-\nformance of all tasks as the BLUB score.\nZero-shot Transfer We show the zero-shot\ncross-lingual transfer results of the multilingual\nmodels ﬁne-tuned on the English counterpart of\neach dataset (SentNob has no English equivalent;\nhence we used the Stanford Sentiment Treebank\n(Socher et al., 2013) for the sentiment classiﬁca-\ntion task) in Table 2. In zero-shot transfer set-\nting, BanglishBERT achieves strong cross-lingual\nperformance over similar-sized models and falls\nmarginally short of XLM-R (large). This is an ex-\npected outcome since cross-lingual effectiveness\ndepends explicitly on model size (K et al., 2020).\nSupervised Fine-tuning In the supervised ﬁne-\ntuning setup, BanglaBERT outperformed multilin-\ngual models and monolingual sahajBERT on all\nthe tasks, achieving a BLUB score of 77.09, even\ncoming head-to-head with XLM-R (large). On the\nother hand, BanglishBERT marginally lags behind\nBanglaBERT and XLM-R (large). BanglaBERT is\nnot only superior in performance but also substan-\ntially compute- and memory-efﬁcient. For instance,\nit may seem that sahajBERT is more efﬁcient than\nBanglaBERT due to its smaller size, but it takes\n2-3.5x time and 2.4-3.33x memory as BanglaBERT\n1321\nto ﬁne-tune. 6\n100 500 1000 5000 Full Data\n35\n45\n55\n65\n75\nBanglaBERT\nXLM-R(Large)\nSentiment Classification\nTraining Samples\nMacro-F1 (%)\n0.1k 1k 10k 100k Full Data30\n40\n50\n60\n70\n80\nBanglaBERT\nXLM-R(Large)\nNatural Language Inference\nTraining Samples\nAccuracy (%)\nFigure 1: Sample-efﬁciency tests with SC and NLI.\nSample efﬁciency It is often challenging to an-\nnotate training samples in real-world scenarios, es-\npecially for low-resource languages like Bangla.\nSo, in addition to compute- and memory-efﬁciency,\nsample-efﬁciency (Howard and Ruder, 2018) is an-\nother necessity of PLMs. To assess the sample\nefﬁciency of BanglaBERT, we limit the number of\ntraining samples and see how it fares against other\nmodels. We compare it with XLM-R (large) and\nplot their performances on the SC and NLI tasks7\nfor different sample size in Figure 1.\nResults show that when we have fewer number\nof samples (≤ 1k), BanglaBERT has substantially\nbetter performance (2-9% on SC and 6-10% on\nNLI with p <0.05) over XLM-R (large), making\nit more practically applicable for resource-scarce\ndownstream tasks.\n5 Conclusion & Future Works\nCreating language-speciﬁc models is often infea-\nsible for low-resource languages lacking ample\ndata. Hence, researchers are compelled to use mul-\ntilingual models for languages that do not have\n6We present a detailed comparison in the Appendix.\n7Results for the other tasks can be found in the Appendix.\nstrong pretrained models. To this end, we in-\ntroduced BanglaBERT and BanglishBERT, two\nNLU models in Bangla, a widely spoken yet low-\nresource language. We presented new downstream\ndatasets on NLI and QA, and established the BLUB\nbenchmark, setting new state-of-the-art results with\nBanglaBERT. In future, we will include other\nBangla NLU benchmarks (e.g., dependency pars-\ning (de Marneffe et al., 2021)) in BLUB and in-\nvestigate the beneﬁts of initializing Bangla NLG\nmodels from BanglaBERT.\nAcknowledgements\nWe would like to thank the Research and Innovation\nCentre for Science and Engineering (RISE), BUET,\nfor funding the project, and Intelligent Machines\nLimited and Google TensorFlow Research Cloud\n(TRC) Program for providing cloud support.\nEthical Considerations\nDataset and Model Release The Copy Right\nAct, 20008 of Bangladesh allows reproduction and\npublic release of copy-right materials for non-\ncommercial research purposes. As a transformative\nresearch work, we will release BanglaBERT un-\nder a non-commercial license. Furthermore, we\nwill release only the pretraining data for which we\nknow the distribution will not cause any copyright\ninfringement. The downstream task datasets can\nall be made publicly available under a similar non-\ncommercial license.\nQuality Control in Human Translation Trans-\nlations were done by expert translators who provide\ntranslation services for renowned Bangla newspa-\npers. Each translated sentence was further assessed\nfor quality by another expert. If found to be of low\nquality, it was again translated by the original trans-\nlator. The sample was then discarded altogether if\nfound to be of low quality again. Fewer than 100\nsamples were discarded in this process. Translators\nwere paid as per standard rates in local currencies.\nText Content We tried to minimize offensive\ntexts in the pretraining data by explicitly crawl-\ning the sites where such contents would be nom-\ninal. However, we cannot guarantee that there is\nabsolutely no objectionable content present and\ntherefore recommend using the model carefully,\nespecially for text generation purposes.\n8http://bdlaws.minlaw.gov.bd/\nact-details-846.html\n1322\nReferences\nFiroj Alam, Shammur Absar Chowdhury, and Sheak\nRashed Haider Noori. 2016. Bidirectional LSTMs\n- CRFs networks for bangla POS tagging. In 2016\n19th International Conference on Computer and\nInformation Technology (ICCIT) , pages 377–382.\nIEEE.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nI. Ashraﬁ, M. Mohammad, A. S. Mauree, G. M. A.\nNijhum, R. Karim, N. Mohammed, and S. Mo-\nmen. 2020. Banner: A cost-sensitive contextualized\nmodel for bangla named entity recognition. IEEE\nAccess, 8:58206–58226.\nJosé Canete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge Pérez. 2020. Spanish pre-trained BERT model\nand evaluation data. In Proceedings of the Practi-\ncal ML for Developing Countries Workshop at ICLR\n2020, PML4DC.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020a. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020b. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, April, 2020 ,\nOnline.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32,\npages 7059–7069. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAmitava Das and Sivaji Bandyopadhyay. 2010. Phrase-\nlevel polarity identiﬁcation for bangla. Int. J. Com-\nput. Linguist. Appl.(IJCLA), 1(1-2):169–182.\nAvishek Das, Omar Sharif, Mohammed Moshiul\nHoque, and Iqbal H. Sarker. 2021. Emotion clas-\nsiﬁcation in a resource constrained language using\ntransformer-based approach. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop , pages 150–158, Online.\nAssociation for Computational Linguistics.\nMarie-Catherine de Marneffe, Christopher D. Man-\nning, Joakim Nivre, and Daniel Zeman. 2021. Uni-\nversal Dependencies. Computational Linguistics ,\n47(2):255–308.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMichael Diskin, Alexey Bukhtiyarov, Max Ryabinin,\nLucile Saulnier, Quentin Lhoest, Anton Sinitsin,\nDmitry Popov, Dmitry Pyrkin, Maxim Kashirin,\nAlexander Borzunov, Albert Villanova del Moral,\nDenis Mazur, Ilia Kobelev, Yacine Jernite, Thomas\nWolf, and Gennady Pekhimenko. 2021. Dis-\ntributed deep learning in open collaborations.\narXiv:2106.10207.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer,\nNaveen Arivazhagan, and Wei Wang. 2020.\nLanguage-agnostic bert sentence embedding.\narXiv:2007.01852.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nFLORES evaluation datasets for low-resource ma-\nchine translation: Nepali–English and Sinhala–\nEnglish. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6098–6111, Hong Kong, China. Association for\nComputational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Ma-\nsum Hasan, Madhusudan Basak, M. Sohel Rahman,\n1323\nand Rifat Shahriyar. 2020. Not low-resource any-\nmore: Aligner ensembling, batch ﬁltering, and new\ndatasets for Bengali-English machine translation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2612–2623, Online. Association for Computa-\ntional Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nKhondoker Ittehadul Islam, Sudipta Kar, Md Saiful Is-\nlam, and Mohammad Ruhul Amin. 2021. SentNoB:\nA dataset for analysing sentiment on noisy Bangla\ntexts. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3265–3271,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020a.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020b. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient\ntext classiﬁcation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427–431, Valencia, Spain. Association\nfor Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multi-\nlingual bert: An empirical study. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, April, 2020, Online.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for Indian\nlanguages. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4948–\n4961, Online. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing , pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2020. Albert: A lite bert for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, April, 2020, Online.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoît Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. Flaubert: Unsupervised language\nmodel pre-training for french. In Proceedings of\nThe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nAlexandra Luccioni and Joseph Viviano. 2021. What’s\nin the box? an analysis of undesirable content in\nthe Common Crawl corpus. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers) , pages 182–189, Online. As-\nsociation for Computational Linguistics.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022. MultiCoNER:\na Large-scale Multilingual dataset for Complex\nNamed Entity Recognition.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nSungjoon Park, Jihyung Moon, Sungdong Kim,\nWon Ik Cho, Ji Yoon Han, Jangwon Park, Chisung\nSong, Junseong Kim, Youngsook Song, Taehwan\nOh, Joohong Lee, Juhyun Oh, Sungwon Lyu,\nYounghoon Jeong, Inkwon Lee, Sangwoo Seo,\nDongjun Lee, Hyunwoo Kim, Myeonghwa Lee,\nSeongbo Jang, Seungwon Do, Sunkyoung Kim,\nKyungtae Lim, Jongwon Lee, Kyumin Park, Jamin\nShin, Seonghyun Kim, Lucy Park, Alice Oh, Jung-\nWoo Ha, and Kyunghyun Cho. 2021. KLUE: Ko-\nrean language understanding evaluation. In Thirty-\nﬁfth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2).\n1324\nMarco Polignano, Pierpaolo Basile, Marco de Gemmis,\nGiovanni Semeraro, and Valerio Basile. 2019. Al-\nberto: Italian BERT language understanding model\nfor NLP challenging tasks based on tweets. In Pro-\nceedings of the Sixth Italian Conference on Com-\nputational Linguistics, Bari, Italy, November 13-15,\n2019, CEUR Workshop Proceedings.\nShana Poplack. 1980. Sometimes I’ll start a sentence\nin Spanish Y TERMINO EN ESPAÑOL: toward\na typology of code-switching. Linguistics, 18(7-\n8):581–618.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8).\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 784–\n789, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nMd Shajalal and Masaki Aono. 2018. Semantic tex-\ntual similarity in Bengali text. In 2018 International\nConference on Bangla Speech and Language Pro-\ncessing (ICBSLP), pages 1–5. IEEE.\nAbdullah Aziz Sharfuddin, Md Naﬁs Tihami, and\nMd Saiful Islam. 2018. A deep recurrent neural\nnetwork with bilstm model for sentiment classiﬁca-\ntion. In 2018 International Conference on Bangla\nSpeech and Language Processing (ICBSLP) , pages\n1–4. IEEE.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous Pipeline for Process-\ning Huge Corpora on Medium to Low Resource In-\nfrastructures. In 7th Workshop on the Challenges\nin the Management of Large Corpora (CMLC-\n7), Cardiff, United Kingdom. Leibniz-Institut für\nDeutsche Sprache.\nNaﬁs Irtiza Tripto and Mohammed Eunus Ali. 2018.\nDetecting multilabel sentiment and emotions from\nbangla youtube comments. In 2018 International\nConference on Bangla Speech and Language Pro-\ncessing (ICBSLP), pages 1–6. IEEE.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. arXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems , volume 32, pages\n5753–5763. Curran Associates, Inc.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the 2015\nIEEE International Conference on Computer Vision\n(ICCV), ICCV ’15, page 19–27, USA. IEEE Com-\nputer Society.\n1325\nAppendix\nPretraining Data Sources\nWe used the following sites for data collection. We\ncategorize the sites into six types:\nEncyclopedia:\n• bn.banglapedia.org\n• bn.wikipedia.org\n• songramernotebook.com\nNews:\n• anandabazar.com\n• arthoniteerkagoj.com\n• bangla.24livenewspaper.com\n• bangla.bdnews24.com\n• bangla.dhakatribune.com\n• bangla.hindustantimes.com\n• bangladesherkhela.com\n• banglanews24.com\n• banglatribune.com\n• bbc.com\n• bd-journal.com\n• bd-pratidin.com\n• bd24live.com\n• bengali.indianexpress.com\n• bigganprojukti.com\n• bonikbarta.net\n• chakarianews.com\n• channelionline.com\n• ctgtimes.com\n• ctn24.com\n• daily-bangladesh.com\n• dailyagnishikha.com\n• dainikazadi.net\n• dainikdinkal.net\n• dailyfulki.com\n• dailyinqilab.com\n• dailynayadiganta.com\n• dailysangram.com\n• dailysylhet.com\n• dainikamadershomoy.com\n• dainikshiksha.com\n• dhakardak-bd.com\n• dmpnews.org\n• dw.com\n• eisamay.indiatimes.com\n• ittefaq.com.bd\n• jagonews24.com\n• jugantor.com\n• kalerkantho.com\n• manobkantha.com.bd\n• mzamin.com\n• ntvbd.com\n• onnodristy.com\n• pavilion.com.bd\n• prothomalo.com\n• protidinersangbad.com\n• risingbd.com\n• rtvonline.com\n• samakal.com\n• sangbadpratidin.in\n• somoyerkonthosor.com\n• somoynews.tv\n• tbsnews.net\n• teknafnews.com\n• thedailystar.net\n• voabangla.com\n• zeenews.india.com\n• zoombangla.com\nBlogs:\n• amrabondhu.com\n• banglablog.in\n• bigganblog.org\n• biggani.org\n• bigyan.org.in\n• bishorgo.com\n• cadetcollegeblog.com\n• choturmatrik.com\n• horoppa.wordpress.com\n• muktangon.blog\n• roar.media/bangla\n• sachalayatan.com\n• shodalap.org\n• shopnobaz.net\n• somewhereinblog.net\n• subeen.com\n• tunerpage.com\n• tutobd.com\nE-books/Stories:\n• banglaepub.github.io\n• bengali.pratilipi.com\n• bn.wikisource.org\n• ebanglalibrary.com\n• eboipotro.github.io\n• golpokobita.com\n• kaliokalam.com\n• shirisherdalpala.net\n• tagoreweb.in\nSocial Media/Forums:\n• banglacricket.com\n• bn.globalvoices.org\n• helpfulhub.com\n• nirbik.com\n• pchelplinebd.com\n• techtunes.io\n1326\nMiscellaneous:\n• banglasonglyric.com\n• bdlaws.minlaw.gov.bd\n• bdup24.com\n• bengalisongslyrics.com\n• dakghar.org\n• gdn8.com\n• gunijan.org.bd\n• hrw.org\n• jakir.me\n• jhankarmahbub.com\n• jw.org\n• lyricsbangla.com\n• neonaloy.com\n• porjotonlipi.com\n• sasthabangla.com\n• tanzil.net\nWe wrote custom crawlers for each site above\n(except the Wikipedia dumps).\nAdditional Sample Efﬁciency Tests\nWe plot the the sample efﬁciency results of the\nNER and QA tasks in Figure 2.\n100 500 1000 5000 Full Data\n0\n20\n40\n60\n80\nBanglaBERT\nXLM-R(Large)\nNamed Entity Recognition\nTraining Samples\nMicro-F1 (%)\n100 500 1000 2000 Full Data\n50\n60\n70\n80\n90\nBanglaBERT\nXLM-R(Large)\nQuestion Answering\nTraining Samples\nF1 (%)\nFigure 2: Sample-efﬁciency tests with NER and QA.\nSimilar results are also observed here for the\nNER task, where BanglaBERT is more sample-\nefﬁcient when we have ≤ 1k training samples. In\nthe QA task however, both models have identical\nperformance for all sample counts.\nCompute and Memory Efﬁciency Tests\nTo validate that BanglaBERT is more efﬁcient in\nterms of memory and compute, we measured each\nmodel’s training time and memory usage during\nthe ﬁne-tuning of each task. All tests were done\non a desktop machine with an 8-core Intel Core-i7\n11700k CPU and NVIDIA RTX 3090 GPU. We\nused the same batch size, gradient accumulation\nsteps, and sequence length for all models and tasks\nfor a fair comparison. We use relative time and\nmemory (GPU VRAM) usage considering those\nof BanglaBERT as units. The results are shown in\nTable 3. (We mention the upper and lower values\nof the different tasks for each model)\nModel Time Memory Usage\nmBERT 1.14x-1.92x 1.12x-2.04x\nXLM-R (base) 1.29-1.81x 1.04-1.63x\nXLM-R (large) 3.81-4.49x 4.44-5.55x\nSahajBERT 2.40-3.33x 2.07-3.54x\nBanglaBERT 1.00x 1.00x\nTable 3: Compute and memory efﬁciency tests\n1327"
}