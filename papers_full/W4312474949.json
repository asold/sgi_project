{
  "title": "SIPFormer: Segmentation of Multiocular Biometric Traits With Transformers",
  "url": "https://openalex.org/W4312474949",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5001125873",
      "name": "Bilal Hassan",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5011636952",
      "name": "Taimur Hassan",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016211840",
      "name": "Ramsha Ahmed",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5059512412",
      "name": "Naoufel Werghi",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5012053457",
      "name": "Jorge Dias",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2019696607",
    "https://openalex.org/W2517824892",
    "https://openalex.org/W2340393436",
    "https://openalex.org/W2774581827",
    "https://openalex.org/W2766373199",
    "https://openalex.org/W2970577635",
    "https://openalex.org/W1977060095",
    "https://openalex.org/W2036227434",
    "https://openalex.org/W2966520344",
    "https://openalex.org/W2935803957",
    "https://openalex.org/W2787567796",
    "https://openalex.org/W2323896297",
    "https://openalex.org/W2962837960",
    "https://openalex.org/W2899458249",
    "https://openalex.org/W100859729",
    "https://openalex.org/W3159022280",
    "https://openalex.org/W4289821977",
    "https://openalex.org/W4293240934",
    "https://openalex.org/W3209489733",
    "https://openalex.org/W3170593638",
    "https://openalex.org/W3127023551",
    "https://openalex.org/W2969276928",
    "https://openalex.org/W2964290628",
    "https://openalex.org/W2195934234",
    "https://openalex.org/W2102525097",
    "https://openalex.org/W2099278510",
    "https://openalex.org/W2966523470",
    "https://openalex.org/W2776621604",
    "https://openalex.org/W2942211779",
    "https://openalex.org/W2949442988",
    "https://openalex.org/W2909406878",
    "https://openalex.org/W2889334143",
    "https://openalex.org/W3185605277",
    "https://openalex.org/W3178971028",
    "https://openalex.org/W3121640062",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2114268578",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3193898253",
    "https://openalex.org/W3189659869",
    "https://openalex.org/W3191565355",
    "https://openalex.org/W4210257375",
    "https://openalex.org/W3210024281",
    "https://openalex.org/W2964098128",
    "https://openalex.org/W3200195424",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W6793164127",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4285010063",
    "https://openalex.org/W2910281775",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2613332842",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1503398984"
  ],
  "abstract": "The advancements in machine vision have opened up new avenues for implementing multimodal biometric identification systems for real-world applications. These systems can address the shortcomings of unimodal biometric systems, which are susceptible to spoofing, noise, nonuniversality, and intraclass variations. Besides, ocular traits among various biometric traits are preferably used in these recognition systems due to their great uniqueness, permanence, and performance. However, segmenting visual biometric features under unconstrained situations remains challenging due to a variety of variables, such as Purkinje reflexes, specular reflections, eye gaze, off-angle pictures, poor resolution, and numerous occlusions. To overcome these challenges, this research presents a novel framework called SIPFormer, comprising the encoder, decoder, and transformer blocks to simultaneously segment three ocular traits (sclera, iris, and pupil) using its discriminative multihead self-attention mechanism. Besides, we used the large publicly available iris database reflecting different unconstrained acquisition settings, with inherent noise effects such as scanner artifacts, intensity and illumination variations, motion blur, and occultations caused by eyelashes, eyelids, and eyeglasses. Furthermore, the simulation results demonstrate the efficacy of the proposed SIPFormer model, where it achieved the mean Dice similarity coefficient scores of 0.9018, 0.9176, and 0.9229 for segmenting the sclera, iris, and pupil classes, respectively.",
  "full_text": "IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023 5001914\nSIPFormer: Segmentation of Multiocular Biometric\nTraits With Transformers\nBilal Hassan , Taimur Hassan , Member, IEEE, Ramsha Ahmed , Naoufel Werghi , Senior Member, IEEE,\nand Jorge Dias , Senior Member, IEEE\nAbstract— The advancements in machine vision have opened\nup new avenues for implementing multimodal biometric identi-\nﬁcation systems for real-world applications. These systems can\naddress the shortcomings of unimodal biometric systems, which\nare susceptible to spooﬁng, noise, nonuniversality, and intra-\nclass variations. Besides, ocular traits among various biometric\ntraits are preferably used in these recognition systems due to\ntheir great uniqueness, permanence, and performance. However,\nsegmenting visual biometric features under unconstrained situ-\nations remains challenging due to a variety of variables, such\nas Purkinje reﬂexes, specular r eﬂections, eye gaze, off-angle\npictures, poor resolution, and numerous occlusions. To overcome\nthese challenges, this research presents a novel framework called\nSIPFormer, comprising the encoder, decoder, and transformer\nblocks to simultaneously segment three ocular traits (sclera,\niris, and pupil) using its discriminative multihead self-attention\nmechanism. Besides, we used the large publicly available iris\ndatabase reﬂecting different unconstrained acquisition settings,\nwith inherent noise effects such as scanner artifacts, intensity and\nillumination variations, motion blur, and occultations caused by\neyelashes, eyelids, and eyeglasses. Furthermore, the simulation\nresults demonstrate the efﬁcacy of the proposed SIPFormer\nmodel, where it achieved the mean Dice similarity coefﬁcient\nscores of 0.9018, 0.9176, and 0.9229 for segmenting the sclera,\niris, and pupil classes, respectively.\nIndex Terms— Biometric traits, iris, pupil, sclera, segmenta-\ntion, transformers.\nI. I NTRODUCTION\nO\nVER the last decade, the need for reliable authentication\nsystems has grown in step with the meteoric rise of\nthe information technology industry and rapid technological\ndevelopment. As a result, researchers are constantly striving\nManuscript received 26 July 2022; revised 26 October 2022; accepted\n28 November 2022. Date of publication 26 December 2022; date of current\nversion 13 January 2023. This work was supported by Khalifa University Cen-\nter for Autonomous Robotic Systems (KUCARS). The Associate Editor coor-\ndinating the review process was Dr. Hongrui Wang.(Corresponding author:\nTaimur Hassan.)\nBilal Hassan and Jorge Dias are with the Khalifa University Center for\nAutonomous Robotic Systems (KUCARS), Department of Electrical Engi-\nneering and Computer Science, Khalifa University of Science and Tech-\nnology, Abu Dhabi, United Arab Emirates (e-mail: bilal.hassan@ku.ac.ae;\njorge.dias@ku.ac.ae).\nTaimur Hassan and Naoufel Werghi are with the Khalifa University Center\nfor Autonomous Robotic Systems (KUCARS) and the Center for Cyber-\nPhysical Systems (C2PS), Department of Electrical Engineering and Computer\nScience, Khalifa University of Science and Technology, Abu Dhabi, United\nArab Emirates (e-mail: taimur.hassan@ku.ac.ae; naoufel.werghi@ku.ac.ae).\nRamsha Ahmed is with the Healthcare Engineering Innovation Cen-\nter (HEIC), Department of Biomedi cal Engineering, Khalifa University\nof Science and Technology, Abu Dhab i, United Arab Emirates (e-mail:\nramsha.ahmed@ku.ac.ae).\nDigital Object Identiﬁer 10.1109/TIM.2022.3232162\nto perfect foolproof authentication methods. Biometric tech-\nnology, which refers to the authentication of a person based\non measurable physical or behavioral attributes, is becoming\nincreasingly popular in this area. Moreover, biometric systems\nhave a remarkable false match and false rejection rate of 2%,\nmaking them almost hard to exploit using standard decryption\napproaches [1]. To a large extent, biometric systems are now\nrequired in our daily lives. Unlike conventional methods, these\nalternatives do not need us to physically store or remem-\nber sensitive information, such as user names, passwords,\nor other authentication credentials. Biometrics are used in\nvarious crucial applications, from unlocking mobile phones\nto cash withdrawal, and consumer apps to law enforcement\nand restricted access control [2], [3], [4].\nSeveral biometric identiﬁers can be used to identify an indi-\nvidual positively. Among these, ocular features have proven\nsuperior to other biometric attributes for applications requir-\ning high reliability and accuracy due to their dependability,\nlongevity, and efﬁciency [5]. In contrast, systems that rely\non other characteristics, such as ﬁngerprints, can be easily\ncompromised, as they may be burned or affected by allergic\nskin reactions with time. Similarly, the performance of the\nvoice recognition system is unreliable since voices can be\nmanipulated [6], [7]. The primary ocular biometric traits are\nthe sclera, iris, and pupil, as shown in Fig. 1. Each ocular\ntrait has its own uniqueness and importance, as described in\nthe following.\n1) Sclera: A relatively new biometric trait for person\nidentiﬁcation has shown promising results [8], [9]. The\nvascular pattern in the sclera (see Fig. 1) is highly\nunique for each individual and even observed to be\ndifferent between the left and right eyes of a person\n[10]. In addition, it is tough to counterfeit the sclera,\nunlike the iris, which can be easily forged by wearing a\ncontact lens [10]. Moreover, segmenting the sclera can\nhelp achieve higher accuracy of iris recognition systems\nunder unconstrained lighting conditions [11].\n2) Iris: The most widely used ocular trait in biometric\nsystems possesses a high degree of distinctiveness and\nrandomness in terms of its pattern, size, shape, and color.\nThis complexity is primarily because of the rich and\nunique textures of the iris, such as furrows, rings, freck-\nles, crypts, zigzags, or ridges [12], as shown in Fig. 1.\nMoreover, the iris trait exhibits greater immutability\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\nFig. 1. Periocular and ocular components. The red and aqua boxes represent\nthe vascular pattern of sclera and iris texture, respectively.\nthroughout a person’s life, and some studies even con-\ncluded the usefulness of the iris in postmortem recogni-\ntion [13].\n3) Pupil: The least commonly used ocular trait in biometry\ndue to its homogenous structure. Generally, segmen-\ntation and detection of the pupil are considered the\nfundamental procedure in developing various computer\nvision applications [14]. However, there are limited\nstudies based on the pupil as a standalone trait for\nauthentication [15].\nThe characteristics of these ocular traits in conjunction\nvary extensively across the human population. Over time,\nuniqueness, and randomness, their immutability can provide a\nrobust and reliable multimodal biometric recognition system.\nDespite these advantages, joint segmentation of ocular traits is\na challenging task mainly due to three major factors: Purkinje\nreﬂexes, eye gaze, and occlusions due to eyelids and eyelashes.\nGiven the above, the motivation of this work is to present\na robust framework that can jointly segment the three ocular\ntraits (sclera, iris, and pupil), facilitating the development of\na biometric system based on multiocular traits in the future.\nA recent spurt in the expansion of deep learning applications\nmay be attributed to the proven effectiveness of various\nconvolutional neural network (CNN) architectures over other\ntraditional approaches [16], [17], [18], [19]. These advantages\nhave proliferated deep learning-based biometric systems, and\nthe domain has surged recently insecurity and authentication\napplications with a signiﬁcant emphasis on ocular traits [20].\nA. Related Works\n1) Sclera: The researchers have previously proposed dif-\nferent deep learning solutions to segment and recognize\nthe sclera for biometric applications [10], [11], [21], [22],\n[23]. Maheshan et al. [10] proposed a CNN sclera recogni-\ntion engine consisting of four convolutional units and one\nfully connected unit. They evaluated the framework on the\nsclera segmentation, and recognition benchmarking competi-\ntion dataset and received an accuracy of 87.65%. Besides, the\nmethod for semantic segmentation of sclera in [11] used the\ncombination of CNN and conditional random ﬁelds (CRFs)\nas a postprocessing technique. They validated the framework\non the sclera competition dataset and received an accuracy of\n83.2% in the correct classiﬁcation of sclera pixels.\nMoreover, Zhu et al. [21] designed a stem-and-leaf branches\nnetwork, called SLBNet, to identify persons. They ﬁrst used\nthe traditional image processing techniques to segment the\nscleral vasculature, which is then passed on to the SLBNet\nto identify the person. Similarly, different neural network\narchitectures are implement ed in [22] to segment the iris\nand sclera using two different datasets. Furthermore, in [23],\na CNN model called ScleraNET is presented to identify and\nrecognize a person using a sclera vasculature pattern.\n2) Iris: Most of the existing works related to ocular bio-\nmetry in the literature have been conducted using the iris\ntrait [20]. In the past, many researchers proposed different\niris recognition methods. The most common ones include\nfeature descriptor-based methods [24], [25], [26]. Recently,\nresearchers have implemented deep learning-based iris seg-\nmentation and recognition frameworks. Jha et al. [27] pro-\nposed an iris segmentation framework at the pixel level\n(PixlSegNet). Their framework is based on the convolutional\nencoder–decoder architecture, where a stacked hourglass net-\nwork is used between the encoder and decoder paths. Besides,\nNguyen et al. [28] evaluated the performance of six different\npretrained CNN architectures on iris recognition using two\npublicly available datasets. They showed that standard CNN\nfeatures, originally extracted and trained for classifying com-\nmon objects, can also be transferred and used to recognize iris.\nMoreover, the capsule network-based deep learning frame-\nwork for the recognition of iris is proposed in [29]. Their\nalgorithm adjusted the network structure detail to adapt for\niris recognition based on a modiﬁed dynamic routing algorithm\nwithin the capsule layers. They employed the transfer learning\napproach and divided the threepretrained CNN architectures\ninto subnetwork sequences to extract the features. Further-\nmore, the deep multimodal biometric system based on iris\nrecognition is proposed in [5]. They ﬁrst localized the iris\nregions in both the left and right eyes of the same person\nand then passed to the CNN for extraction of discriminative\nfeatures and classiﬁcation using the rank fusion technique.\nIn addition to that, a fully convolutional deep neural net-\nwork framework to segment iris using low-quality images\nis proposed in [4]. They merged four different CNNs using\nsemiparallel deep neural network techniques.\n3) Pupil: In the past, various deep learning solutions\nhave been proposed to detect, segment, and track pupil.\nYiu et al. [30] used a U-Net-based CNN architecture called\nDeepVOG to segment the pupil. They trained the network\non two local datasets containing video-oculography (VOG)\nimages. They validated the framework on different datasets\nand achieved the highest median value of the Dice coefﬁcient\nas 0.978. Moreover, the approach in [31] is validated on two\ndifferent datasets, consisting of a close-up view of the eye\nand the full facial image. In the case of the full image, the\nauthors ﬁrst extracted the eye region, which is then passed to\nthe pupil segmentation network. In [32], another deep CNN\ncalled DeepEye is proposed for pupil detection based on atrous\nconvolutions and spatial pyramids.\nFurthermore, Whang et al. [33] used a lightweight CNN\narchitecture to segment the pupil from the video sequences.\nThey predicted the size of the pupil using the major and\nHASSAN et al.: SIPFormer: SEGMENTATION OF MULTIOCULAR BIOMETRIC TRAITS WITH TRANSFORMERS 5001914\nminor axes of an ellipse. Besides, Shi et al. [34] proposed an\nend-to-end deep learning framework for pupil detection and\ntracking. They used a CNN approach to detect the pupil and\nthe long short-term memory (LSTM) model to predict pupil\nmotion. Similarly, Ou et al. [35]employed the pretrained deep\nlearning-based detector for pupil-center detection and tracking\nin the visible-light mode.\nB. Contributions\nThe existing research for ocular biometrics is typically\nbased on a single (mostly iris) or two ocular traits (mostly\niris and pupil). The authors have implemented different deep\nlearning-based algorithms to detect, segment, and recognize\nthese ocular traits. Compared to previous methods, the pro-\nposed convolutional transformer-based framework, SIPFormer,\ncan simultaneously segment all three ocular biometric features\n(sclera, iris, and pupil) and will also improve the accuracy of\nmultimodal biometric systems for unconstrained conditions.\nThe notable contributions of this research are twofold, as sum-\nmarized in the following.\n1) This article presents a novel end-to-end deep learn-\ning model called SIPFormer, comprising an encoder,\ndecoder, and transformer blocks to perform joint seg-\nmentation of multiple ocular traits (sclera, iris, and\npupil). Besides, to the best of our knowledge, the\nSIPFormer framework is the ﬁrst attempt to utilize\ntransformers with convolutional blocks to perform joint\nocular traits segmentation.\n2) Moreover, SIPFormer, due to its discriminative multi-\nhead self-attention, possesses the intrinsic capacity to\nsegment multiocular traits irrespective of the scanner\nartifacts and noisy shadows produced due to the presence\nof eyelashes, eyelids, and spectacles. The SIPFormer\nhas been rigorously tested on ﬁve diversiﬁed datasets,\ndemonstrating comparable segmentation performance\nwith 136.21% fewer parameters than state-of-the-art\ndeep learning-based segmentation methods.\nThe remaining of this article is organized as follows.\nSection II gives the description of the dataset used in\nthis research and the proposed framework in detail. Next,\nSection III presents the experimental setup. Furthermore, the\nsimulation results are presented in Section IV, followed by a\ndiscussion and conclusion in Section V.\nII. M\nATERIALS AND METHODS\nA. Dataset Details\nIn this study, we have opted for the Chinese Academy\nof Sciences, Institute of Automation (CASIA) database [36]\nmainly because it is one of the largest datasets with a lot\nmore subjects and intraclass variations and our prime focus is\ntoward the segmentation and extraction of ocular modalities\nfor which this database is well suited. In this study, we used\na total of 52 034 images of about 2800 subjects for training\nand validation purposes. These images are retrieved from ﬁve\nsubsets of the CASIA-IrisV4 database, which are CASIA-\nIris-Interval (CII), CASIA-Iris-Syn (CIS), CASIA-Iris-Lamp\nFig. 2. Sample images from CASIA -IrisV4 subsets and corresponding\nground-truth labels. (a) CII. (b) CIS. (c) CIL. (d) CITW. (e) CIT.\n(CIL), CASIA-Iris-Thousand (CIT), and CASIA-Iris-Twins\n(CITW). The statistics and features of each of these ﬁve\nsubsets are shown in Table I. Fig. 2 shows the sample images\nand the corresponding ground-truth labels.\nWe randomly divided each of the ﬁve subsets of the\nCASIA-IrisV4 database in the ratio of 60:20:20 for training,\nvalidation, and testing purposes, as shown in Table II. The\nproposed SIPFormer model is trained using 60% of the images\nin each subset, while for both validation and testing, we used\n20% of the images in each subset.\nB. Data Preprocessing\nIn our study, we have used ﬁve different subsets of the\nCASIA-IrisV4 database that vary extensively in terms of\nenvironment, illumination, and camera sensor, as shown in\nTable I. Hence, before feeding to the CNN architecture, data\nprocessing is required to scale, convert, and standardize these\nimages according to the dimension, activation shape, and\nsize speciﬁed by the network input speciﬁcations. Therefore,\npreprocessing is the ﬁrst stage of the proposed SIPFormer\nsystem, where we ﬁrst performed the intensity transformation\nto adjust and increase the intensity differences in the ocular\nregion, as shown in Fig. 3(b). For this purpose, we employed\ntwo intensity transformation functions, gamma transformation,\nand contrast stretching, to pick out the details, such as limbus\n(iris–sclera boundary) in the ocular region.\nNext, we enhanced the images using the local enhancement\ntechnique. First, we improved the dynamic range of the images\nusing histogram equalization to evenly distribute the pixel\nintensities across the entire range. The histogram equalization\nprocess improved the contrast level, especially in images\nwhere the intensities are clustered predominantly around the\nlower or middle range. We further enhanced these images\nusing the contrast-limited adaptive histogram equalization\ntechnique, as shown in Fig. 3(c).\nAfter enhancing the images, we removed two types of\nreﬂections from the images: reﬂection in the ocular region\nmainly due to cornea and aqueous humor and reﬂection in\nthe periocular region mainly due to ﬂashlights. We used the\nadaptive thresholding scheme to identify the bright white spots\nin the images and ﬁlled regions using the morphological recon-\nstruction technique. Compared to the conventional algorithms\nbased on the global threshold value, the adaptive thresholding\nscheme utilizes the dynamic threshold value for each pixel in\nthe image, computed using the local mean intensity in the pixel\nneighborhood. Finally, we resized the preprocessed images\nto a common resolution of 576 × 768. Fig. 3(d) shows the\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\nTABLE I\nSPECIFICATIONS OF CASIA-I RIS V4 DATABASE\nTABLE II\nDETAILS OF TRAINING ,V ALIDA TION, AND TEST SETS\nFig. 3. Data preprocessing. (a) Pristine images. (b) Intensity transformation.\n(c) Image enhancement. (d) Reﬂection removal.\npreprocessing stage results on randomly selected images from\nall ﬁve CASIA-IrisV4 subsets.\nC. Proposed SIPFormer Architecture\nThe proposed SIPFormer model is designed to perform\nthe joint segmentation of the sclera, iris, and pupil from\nthe periocular scans. The high-level overview of the pro-\nposed SIPFormer model is shown in 4. As evident from\nFig. 4, the SIPFormer architecture consists of three units\ndubbed: the SIPFormer encoder, SIPFormer decoder, and the\nSIPFormer transformer. When the input scan is preprocessed,\nit is passed through the SIPFormer encoder, which generates\nthe latent feature representations to distinguish the multiocular\ntraits. Moreover, the SIPFormer encoder also serves as a\nbackbone to generate latent projections from the nonover-\nlapping sequenced patches obtained from the candidate scan.\nAfterward, the latent projection and the ﬂattened projections\nof the positional embeddings (generated through normalized\ncross correlation) are added and passed to the three-layered\nSIPFormer transformer. The SIPFormer transformer computes\nFig. 4. High-level overview of the SIPFormer model.\nthe contextual multihead self-attention distribution from the\nscan projections, enabling the SIPFormer encoder to amplify\nthe discrimination of ocular traits through the fusion between\nthe SIPFormer encoder and transformer features via convolu-\ntion. The resultant latent space distribution is passed to the\nSIPFormer decoder that generates the segmented scan with\nmultiocular trait representations. Moreover, Fig. 5 shows the\ndetailed SIPFormer architecture with layerwise conﬁguration\nand connections. The detailed description of each unit within\nthe proposed SIPFormer architecture is presented in the fol-\nlowing.\n1) SIPFormer Encoder:The SIPFormer encoder is respon-\nsible for generating the distribution of the latent featuref\ne(x)\nto extract the multiocular traits from the periocular scansx ∈\nR(R×C×Ch ),w h e r eR denotes the rows,C denotes the columns,\nand Ch denotes the channels ofx. Unlike the conventional pre-\ntrained networks, the SIPFormer encoder consists of multiple\ntrait preservation (TP) and residual (Res) blocks, as shown\nin Fig. 5. These blocks enable the SIPFormer encoder to\nproduce an accurate contextual and semantic representation\nof the ocular traits during the scan decomposition to yield\ndistinct feature maps. In total, there are four TP blocks\nand 12 Res blocks within the SIPFormer encoder, where\nHASSAN et al.: SIPFormer: SEGMENTATION OF MULTIOCULAR BIOMETRIC TRAITS WITH TRANSFORMERS 5001914\nFig. 5. Architectural details of the proposed SIPFormer model.\neach TP block consists of four convolutions (C), four batch\nnormalizations (BNs), two rectiﬁed linear units (R), and one\nmax pooling (MP), whereas each Res block contains three C,\nthree BNs, and three R’s along with a skip connection. The\nlatent features f\ne produced by the SIPFormer encoder (after\ntuning its learnable weights) effectively discriminate the ocular\ntrait representations from the noisy eyelashes regions and the\nbackground skin. However, it also results in false positives\ntoward differentiating between sclera and iris regions as their\nfeatures are very well correlated. To overcome this, we boost\nthe separation of interclass distributions by convolvingf\ne with\nthe transformer projections pt , which yields the fused feature\nrepresentation fd = fe ∗ pt . These fused features are passed\nto the SIPFormer decoder to extract ocular traits.\n2) SIPFormer Transformer:As mentioned above, after gen-\nerating fe from the SIPFormer encoder, we fuse them with\nthe transformer projections pt , generated by the SIPFormer\ntransformer unit to increase the interclass separability between\nthe well-correlated trait distributions. The SIPFormer trans-\nformer consists of three encoders that are coupled together\nin a cascaded fashion to generate transformer projections\np\nt . The architectural depiction of the transformer encoder\nin the SIPFormer transformer is similar to the vision trans-\nformer (ViT) [37] in terms of hidden layer size and self-\nattention mechanism. However, unlike in ViT, the input to the\nSIPFormer transformer is not the standard positional embed-\ndings generated by the multilayer perceptron (MLP). Instead,\nwe generated the positional embeddings through normalized\ncross correlation. Moreover, in the ViT, the linear embeddings\nfrom the image patches are gen erated again using MLPs,\nwhereas in the SIPFormer transformer, the linear embeddings\nare generated using the SIPFormer encoder. The encodings\nfrom the positional embeddings and linear embeddings are\nthen concatenated to pass on to the transformer encoder,\na ss h o w ni nF i g .5 .\nIn the SIPFormer transformer, the periocular scanx is ﬁrst\ndivided into nonoverlapping squared patchesx\np ∈ R(P×P×Ch ),\nwhere P denotes the resolution of x p, such that P =\n((RC/n p))1/2 and n p denotes the number of patches. Also,\neach patch is cross-correlated with x to generate the posi-\ntional embeddings xe ∈ R(P×P×Ch ). Afterward, we obtain\nthe ﬂattened projections of the positional embeddingxe\ni (cor-\nresponding to the patch x p\ni ), i.e., fp(xe\ni ), and the latent\nprojection for the patchx p\ni , i.e.,lt (x p\ni ) through the SIPFormer\nencoder backbone, as shown in Figs. 4 and 5. Then, we resize\nfp(xe\ni ) and lt (x p\ni ) to k dimensions and compute the sequenced\nembeddings (for the patchx p\ni ) by summinglt (x p\ni ) with fp(xe\ni ),\ni.e., qi = lt (x p\ni ) + fp(xe\ni ). Moreover, repeating the same\nworkﬂow for all the n p patches yields combined projections\nqo, as expressed in the following:\nqo =\n[\nlt\n(\nx p\n0\n)\n;lt\n(\nx p\n1\n)\n; ... ;lt\n(\nx p\n(n p−1)\n)]\n+\n[\nfp\n(\nxe\n0\n)\n; f p\n(\nxe\n1\n)\n; ... ; fp\n(\nxe\n(n p−1)\n)]\n(1)\nor\nqo =\n[\nq0; q1; ... ; q(n p−1)\n]\n. (2)\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\nThe combined projections qo are passed to the ﬁrst trans-\nformer encoder, where head j, qo\nj , is normalized to produce\nq′o\nj . Afterward, q′o\nj is linearly decomposed into query (Q j ),\nkey (K j ),a n dv a l u e(Vj ) pairs via learnable weights such\nthat Q j = q′o\nj wq , K = q′o\nj wk ,a n dV = q′o\nj wv. To compute\nthe contextual self-attention at head j, i.e., A j , Q j ,a n dK j\nare combined via scaled dot product, their resultant scores are\nfused with Vj , as formulated in the following:\nA j\n(\nq′o\nj ; Q j , K j , Vj\n)\n= σ\n⎛\n⎝\n(\nQ j K T\nj\n)\n√\nk\n⎞\n⎠Vj (3)\nwhere σ(.) denotes the sigmoid function. Apart from this, the\nproposed contextual self-attention maps from multiple heads\nare concatenated together to produce contextual multihead\nself-attention distribution ∅CMSA(q′o), as expressed in the\nfollowing:\n∅CMSA\n(\nq′o)\n=\n[\nA0\n(\nq′0\nj ; Q0, K0, V0\n)\n; A1\n(\nq′1\nj ; Q1, K1, V1\n)\n; ...\nAh−1\n(\nq′h−1\nj ; Qh−1, Kh−1, Vh−1\n)]\n. (4)\nWe also want to highlight here that, in contrast to the\nconventional multihead attention mechanism proposed in [37]\nand [38], which uses softmax as an attention operator, the\nproposed contextual multihead self-attention scheme employs\nsigmoid as an operator to compute self-attention, which allows\nthe transformer encoders to generate more vibrant attention\nmaps without biasing itself to one particular segmentation\ncategory out of the rest within the similarly structured scans.\nThis results in the generation of better latent projections\nallowing the SIPFormer dec oder to accurately extract the\nmultitrait information. Moreover, the self-attention distribution\n∅\nCMSA(q′o) is added withqo, where the resultant embeddings\nare normalized and are passed to the normalized feedforward\nblock to produce the ﬁrst transformer’s latent projections (p\nT 1)\npT 1 = ϕ f\n((\n∅CMSA\n(\nq′o)\n+ qo)′)\n+\n(\n∅CMSA\n(\nq′o)\n+ qo)′\n. (5)\npT 1 is passed to second transformer encoder, which produces\npT 2 in a similar manner, andpT 2 is passed to the third trans-\nformer encoder, which producespT 3 projections. Since within\nthe SIPFormer architecture, we injected three transformer\nencoders, so pt = pT 3, and after computing fd ,t h e ya r e\npassed to the SIPFormer decoder to segment multiocular trait\nrepresentations. Table III shows the SIPFormer transformer\nparameters used in this study.\n3) SIPFormer Decoder: After convolving f\ne with pt ,\nwe obtained fused feature representationsfd that are passed to\nthe SIPFormer decoder to segment the multiocular traits. The\nSIPFormer decoder consists of three upsampling blocks, where\neach block includes the transposed C, BN, and R activations,\nand the skip connections (between the SIPFormer encoder and\ndecoder). Besides, at the head of the decoder lies the softmax\nlayer that classiﬁes each pixel into one of the four categories\n(representing the background periocular, sclera, iris, and pupil\nregions).\nMoreover, Fig. 6 shows the channel activation maps learned\nby the proposed SIPFormer model with and without integrating\nTABLE III\nSIPF ORMER TRANSFORMER PARAMETERS\nFig. 6. Visualization of channel activation maps. (a) Input images. Nine\nmost strongest activation channels extracted (b) without integrating SIPFormer\ntransformer and (c) with SIPFormer transformer. (d) Class activation maps at\nthe scorer layer of the SIPFormer decoder.\nthe SIPFormer transformer module. The proposed model,\nwhen combined with the SIPFormer transformer module,\nfocuses mainly on the ocular traits in the images by neglecting\nthe noise effects, such as eyelashes, eyelids, spectacles, and\nshadows, which are inherent in unconstrained acquisition\nsettings. In contrast, when the SIPFormer transformer module\nis not integrated, the model is seriously affected by these\nnoises, resulting in poor feature learning. Therefore, we can\nsay that our proposed SIPFormer model is intrinsically robust\nin segmenting the multiocular traits attributed to its multihead\nself-attention mechanism based on the sigmoid function and\nthe positional embeddings generated using the normalized\ncross correlation. Moreover, we have presented the class\nactivation maps at the scorer layer of the SIPFormer decoder,\nwhich inﬂuenced the model to classify pixels belonging to four\ndifferent classes in this study.\n4) Postprocessing: The segmented images by the deep\nlearning model are often noisy. As a result, we designed\nHASSAN et al.: SIPFormer: SEGMENTATION OF MULTIOCULAR BIOMETRIC TRAITS WITH TRANSFORMERS 5001914\nFig. 7. Postprocessing results. (a) Original image. (b) Segmented image\nby the SIPFormer model. (c) Undersegmented/oversegmented pixels in (b).\n(d)–(f) Extract ocular masks from (b). (g)–(i) Postprocessed ocular regions\nmask. (j) Reconstructed segmented image after postprocessing. (k) Ground-\ntruth image. (l) Undersegmented/oversegmented pixels in (j).\na postprocessing phase to remove stray pixels and smooth\nthe segmented labels. For this purpose, we ﬁrst extract the\nmasks of all ocular traits (sclera, iris, and pupil) separately to\nperform the erosion operations using disk-shaped structural\nelements. Following that, w e use nonlinear median ﬁlter-\ning and 2-D convolutional blurring to minimize the noisy\npixels while preserving the boundary of the ocular region.\nNext, we binarized these ocular masks using the thresholding\ntechnique and concatenated them to acquire a single image.\nThe resultant image in this way may also contain some\nunclassiﬁed areas represented by pixels with a value of 0.\nFinally, we determine the locations of these empty pixels and\nassign the closest label (having the least Euclidean distance)\nto them to achieve the postprocessing images. Fig. 7 shows the\nillustration of the postprocessing steps to clean the segmented\npixels by the proposed SIPFormer model.\n5) Training Loss Function:The cross-entropy (L\nc) [39] or\nDice loss (Ld ) [40] functions are often used to train the deep\nlearning models for the semantic segmentation tasks [41],\n[42]. L\nc has gained popularity due to its ability to generate\ndesirable gradients by subtracting the expected probability\nfrom the actual labels. Moreover, it greatly improves network\nconvergence and is a suitable option for datasets with uni-\nformly distributed classes and explicit mask annotations [43],\n[44], [45]. However, L\nd and Tversky loss (Lt ) [46] are the\npreferable option for the sparse or unbalanced segmentation\npixels [47]. Besides, Ld assists the model in achieving more\nprecise segmented regions with high overlap with the corre-\nsponding ground truths (especially for cases with unbalanced\nclasses or unclear annotations) [41], [42]. In addition,Lt offers\nhigh resilience to unbalanced classes, which further contributes\nto improving semantic segmentation performance [47].\nGiven the above, we hypothesize that synergizing two\nloss functions may attain the best segmentation performance.\nTherefore, we employed a multiobjective hybrid loss function\n(L\nh) in this research to improve the capacity of the proposed\nSIPFormer model for better recognizing the three ocular\nregions. We linearly combined two-tiered objective functions\n(Ld and Lt ) to calculate Lh, as expressed in the following:\nLh = 1\nN\nN∑\ni=1\n(\nα1 Ld,i + α2 Lt,i\n)\n(6)\nLd,i = 1 −\n2 ∑ C\nj=1 ti, j pi, j\n∑ C\nj=1 t2\ni, j + ∑ C\nj=1 p2\ni, j\n(7)\nLt,i = 1 −\n∑ C\nj=1 ti, j pi, j\n∑ C\nj=1\n(\nti, j pi, j + β1t′\ni, j pi, j + β2ti, j p′\ni, j\n) (8)\nwhere ti, j shows the ground-truth labels ofith example belong-\ning to the jth ocular class, whereaspi, j denotes the predicted\nlabels of the ith example for the jth ocular class. The terms\nt′\ni, j and p′\ni, j represent the false predicted labels, wheret′\ni, j are\nthe ground-truth labels of the ith example belonging to the\nnon- jth class andp′\ni, j are the predicted labels marking theith\nexample for the non-jth class. N is the training batch size,\nand C speciﬁes the total classes. The termsα and β represent\nthe experimentally established loss weights for achieving the\nbest performance of the model.\nIII. E\nXPERIMENTAL SETUP\nThe proposed SIPFormer framework has been implemented\nusing the MATLAB R2022a simulation platform installed on\na 64 bits Windows OS. The machine is conﬁgured as Intel\nCore i7-11700 @2.5 GHz, with 32-GB memory and Nvidia\nGeForce RTX 3090. The SIPFormer model is trained using\n31 220 images, randomly chosen from the ﬁve subsets of the\nCASIA-IrisV4 database, as mentioned in Table II. Moreover,\nwe augmented the data at each epoch to prevent overﬁtting and\nimprove the classiﬁer performance against the unseen data.\nWe adopted four types of transformations to augment the data\n(reﬂection, rotation, scaling, and translation) to enable better\ngeneralization characteristics in the model.\nFurthermore, the Adam optimizer [48] is employed in the\nproposed research to update the SIPFormer parameters during\nthe training phase. The batch size and epochs are set to\n32 and 120, respectively, allowing the network to train over\n117 120 iterations with 976 iterations per epoch. Furthermore,\n10 407 separate images are used for validation purposes of\nthe SIPFormer model. We speciﬁed the validation frequency\nevery ten epochs, enabling the SIPFormer model to validate the\nunseen data 12 times. In an attempt to minimize the training\nerror on the validation set, the hyperparameters for training the\nSIPFormer model are determined via Bayesian optimization on\n30 objective function evaluations.\nIV . S\nIMULATION RESULTS\nIn this section, we ﬁrst present different ablation studies\nrelevant to the proposed SIPFormer model. Next, we evaluated\nthe performance of the proposed model both subjectively and\nobjectively, as explained in the following.\nA. Ablation Study\nThe ablative aspects of this research comprise: 1) deter-\nmining the best weights for the α and β parameters in the\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\nTABLE IV\nPERFORMANCE COMPARISON WITH DIFFERENT LOSS FUNCTION\nWEIGHTS .T HE RESULTS ARE PRESENTED IN THE MEAN DSC\nSCORE .T HE BOLD FONT REPRESENTS THE COMBINATION OF THE\nOPTIMAL WEIGHTS\nloss function; 2) training the network with different optimizers\nand loss functions to ﬁnd the best combination by measuring\nthe segmentation performance on the validation set; and 3)\nidentifying the best backbone network for extracting features\nand getting the best segmentation results.\n1) Tuning the Loss Parameters: In this experiment,\nwe experimented with different values for loss weight para-\nmeters to determine the best combination for training the\nnetwork. The α\n1 and α2 terms in (6) reﬂect the contribution\nof each loss function component toward the total loss, where\nα1 and α2 regulate the contributions of Ld [see (7)] and\nLt [see (8)], respectively. Table IV shows the performance\nof the SIPFormer model for different combinations of the\nα1 and α2 parameters, revealing that setting α1 = 0.4a n d\nα2 = 0.6 yields the best segmentation performance on the\nvalidation set with the mean Dice similarity coefﬁcient (DSC)\nof 0.9023.\nSimilarly, the β factor in (8) deﬁnes the contributions of\nfalsely predicted labels in the Traverky loss, where β\n1 and\nβ2 control the contribution of false positives and false nega-\ntives. It can be observed from Table IV that the SIPFormer\nachieved the best segmentation results when considering\nthe equal contribution of false positives and false negatives\n(β1 = β2 = 0.5).\n2) Selection of Optimizer and Loss Function:In this experi-\nment, we trained the model using various optimizers [48], [49],\n[50] and loss functions [39], [40], [46] to evaluate the seg-\nmentation performance, as shown in Table V. The SIPFormer\nmodel fared best on the validation set with the ADAM+ Lh\nconﬁguration, achieving a mean DSC score of 0.9023. Besides,\nwith mean DSC scores of 0.8935 and 0.9257, the ADAM+Lh\narrangement achieved the best performance for segmenting the\niris and pupil regions. However, the ADAM+Lh conﬁguration\nobtained the second-best results for the sclera, following\nthe best setting (SGDM + Lh) by just 0.43%. Furthermore,\ncompared to the second-best results (SGDM+ Lh), the mean\nDSC score with the ADAM+ Lh conﬁguration improves by\n1.86% in ocular regions segmentation, rising from 0.8858 to\n0.9023. The RMSP+ Lc setup, on the other hand, exhibited\nthe least accurate performance, with a mean DSC score of\n0.7836 for segmenting the ocular regions.\n3) Selection of Backbone Network: In this experiment,\nwe employed different backbone networks to determine the\noptimal structure for extracting features and achieving the best\nsegmentation performance on the validation set. The results are\nreported in Table VI, where it can be seen that the proposed\nSIPFormer model achieves a mean DSC score of 0.9023 for\njoint segmentation of the three ocular regions and exceeds the\nTABLE V\nPERFORMANCE COMPARISON FOR OCULAR REGIONS SEGMENTATION\nUSING VARIOUS OPTIMIZERS AND LOSS FUNCTIONS .T HE MEAN DSC\nSCORE METRIC IS USED TO PRESENT THE RESULTS .T HE TOP PER -\nFORMANCE IS HIGHLIGHTED IN BOLD ,W HILE THE SECOND -\nBEST PERFORMANCE IS UNDERLINED\nTABLE VI\nPERFORMANCE COMPARISON FOR OCULAR REGIONS SEGMENTATION\nUSING DIFFERENT BACKBONES STRUCTURES .T HE MEAN DSC SCORE\nMETRIC IS USED TO PRESENT THE RESULTS .T HE TOP PERFOR -\nMANCE IS HIGHLIGHTED IN BOLD ,W HILE THE SECOND -BEST\nPERFORMANCE IS UNDERLINED\nsecond-best results by 3.21%. Moreover, the SIPFormer model\nproduced 2.35% and 2.90% better performance for segmenting\nthe iris and pupil regions, whereas for segmenting sclera, the\nproposed framework achieves the second-best results with a\nmean DSC score of 0.8878, lagging the best results by 0.75%.\n4) Effects of Image Resolution and Patch Size: In this\nexperiment, we studied the effect of image resolution and\npatch size (PS) for the proposed SIPFormer model on the val-\nidation set. We tested our model performance based on three\nimage resolutions (320 × 512, 576 × 768, and 640 × 832)\nand three PSs (8 × 8, 16 × 16, and 32 × 32), as shown\nin Fig. 8. Furthermore, the performance of the model is\nanalyzed in terms of two parameters: segmentation accuracy\nusing the mean Dice score and model inference time in terms\nof frames per second (FPS). From the results in Fig. 8,\nwe can see that increasing the image resolution and the num-\nber of patches improves the segmentation accuracy slightly\n[see Fig. 8(a)]. However, the SIPFormer model efﬁciency is\ninversely proportional to the image resolution and PS. Thus,\nvariants with smaller PSs and higher image resolution are\ncomputationally far more expensive [see Fig. 8(b)]. Therefore,\nHASSAN et al.: SIPFormer: SEGMENTATION OF MULTIOCULAR BIOMETRIC TRAITS WITH TRANSFORMERS 5001914\nTABLE VII\nCATEGORIES OF OCULAR OCCLUSIONS FOR SUBJECTIVE EVA L UAT I O N\nFig. 8. Effects of image resolution and PS on (a) segmentation accuracy and\n(b) model inference time.\nwe opted for moderate settings (576× 768 image resolution\nwith 16 × 16 PS) in the proposed research.\nB. Subjective Evaluation\nPeriocular components, such as eyelashes, eyelids, and spec-\ntacle reﬂection, often obstruct the ocular region. Therefore,\nwe analyzed the performance of our trained SIPFormer model\nsubjectively against the variousocclusion categories, as shown\nin Table VII. To conduct this experiment, we randomly\nselected 200 images for each occlusion category from the\ntesting dataset.\nFig. 9 compares the segmentation results of the SIPFormer\nwith other state-of-the-art methods [37], [54], [55], [56],\n[57], [58]. To demonstrate the results, we randomly selected\n200 images from each occlusion category, as mentioned in\nTable VII. Here, we can observe that the segmented pixels by\nthe proposed SIPFormer model [see Fig. 9(j)] are more precise\nin general compared to other methods [see Fig. 9(c)–(i)],\nproducing a lesser number of false positive and false negative\npixels, as highlighted with the green and red, respectively.\nMoreover, the ﬁrst two rows in Fig. 9 show clear images,\nwhich do not contain occlusion in the sclera, iris, and pupil\nregions. The SIPFormer model achieved the best results\nfor images in this category, where the segmented regions\nare nearly identical to the corresponding ground truths [see\nFig. 9(b)]. The higher accuracy in such cases is perhaps due\nto the discernible and clear contours of each ocular component.\nThe following two rows present the segmentation results\nfor images containing obstructions in the ocular regions due to\neyelashes. The proposed framework showed a good generaliza-\ntion for such images and classiﬁed the majority of ocular pix-\nels correctly, with some false negatives (undersegmentation).\nSimilarly, the proposed SIPFormer model produced promising\nresults for images predominated with eyelids occlusion, which\nmainly truncates the contour and symmetry of the iris region.\nThe SIPFormer model preserved the symmetry of the ocular\nregions as in the ground truths for these images by precluding\nthe obstructed sclera and iris regions, as evident from the\nsegmentation results.\nFurthermore, we evaluated the segmentation performance\nfor images in the heavily occluded category. The ocular region\nin this category is occluded through multiple obstructions\nsuch as eyelashes, eyelids, and reﬂections. Compared with the\nother categories, the segmentation performance of SIPFormer\nfor this set is not as accurate, producing some false positive\nand false negative pixels, as evident from Fig. 9. Moreover,\nwe validated the performance of the SIPFormer model for\nimages containing spectacle reﬂections, as shown in the ninth\nand tenth row of Fig. 9. The proposed model generally\nproduced good results for such images. However, it segmented\nsome pixels around the contours of ocular regions as false\npositives and negatives.\nFinally, we computed the segmentation accuracy of\nSIPFormer for poorly acquired images having truncated ocular\nregions, as shown in the ﬁnal two rows of Fig. 9. We cate-\ngorized images as poorly acquired if: 1) the ocular region is\nnot entirely visible due to spectacle frame or poorly aligned\nshooting angle and 2) the ocular region is partially or wholly\ndark due to shadow or illumination differences. Generally, the\nSIPFormer demonstrated superior segmentation accuracy for\nsuch cases with some false negative pixels for the iris and\nsclera regions. The prime reason for these false negatives is the\nincomplete and irreconcilable contours of the iris and sclera\nregions in this category. To summarize, the segmented pixels\nby the SIPFormer model for pupil class overlap well with the\ncorresponding ground-truth pixels. However, the segmented\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\nFig. 9. Performance comparison for ocular regions segmentation. (a) Original images. (b) Ground-truth labels. (c) AUnet. (d) ReﬁneNet. (e) Segnet.\n(f) SegFormer. (g) FCN-Resnet101. (h) ViT+ SIPFormer ED. (i) DeepLabv3+. (j) SIPFormer.\npixels near the sclera–iris boundary are slightly less accurate.\nNext, we measure the exact overlap per class for each of the\nabove-discussed occlusion categories.\nC. Objective Evaluation\nIn this section, we present the objective evaluation of the\nproposed SIPFormer framework for jointly segmenting the\nsclera, iris, and pupil. We objectively evaluated the perfor-\nmance of SIPFormer via three main experiments, as explained\nin the following.\n1) Evaluation on Test Datasets:In this experiment, we ﬁrst\npresent the performance of the SIPFormer model against all\nof the ﬁve CASIA-IrisV4 test sets using different metrics. The\nresults are reported in Table VIII, where it can be observed that\nour proposed framework generally performed well for all ﬁve\nCASIA-IrisV4 subsets, achieving the mean intersection over\nunion (IoU) and DSC scores of 0.8226 and 0.9025, respec-\ntively. Also, we can analyze that the SIPFormer performed\nslightly better on the CIS and CIL datasets compared to the\nother three datasets (CII, CITW, and CIT). This is perhaps\nbecause the CII dataset contains close-up eye images, while\nall the other datasets contain images captured from a distance.\nAlso, the ratio of CII images is low compared to other datasets.\nIn addition, the CIT dataset comprises the most complex\nimages with more intraclass variances, including spectacles\nand specular reﬂections. Moreover, the CIT dataset with 20 000\nTABLE VIII\nDATASETWISE SEGMENTATION PERFORMANCE OF THE SIPF ORMER\nMODEL USING VARIOUS METRICS .T HE BOLD AND UNDERLINED\nVALUES REPRESENT THE BEST AND SECOND -BEST RESULTS ,\nRESPECTIVEL Y\nimages is the biggest, and achieving the best precision for such\na huge dataset is challenging. Inaddition, the images in the\nCITW dataset were shot during the annual festival in Beijing,\nand it is the only dataset where images were captured in an\noutdoor setting. This dataset was distinct from other datasets\ndue to the variation in lighting, which is why our proposed\nmodel obtained the lowest accuracy for CITW. However, it is\nessential to note that despite these differences in the datasets,\nthe segmentation results for each dataset change by a small\nmargin, which afﬁrms the resilience and reliability of the\nSIPFormer framework regardless of the datasets.\nHASSAN et al.: SIPFormer: SEGMENTATION OF MULTIOCULAR BIOMETRIC TRAITS WITH TRANSFORMERS 5001914\nTABLE IX\nCLASSWISE SEGMENTATION PERFORMANCE OF THE SIPF ORMER MODEL\nUSING VARIOUS METRICS .T HE BOLD AND UNDERLINED VALUES\nREPRESENT THE BEST AND SECOND -BEST RESULTS ,\nRESPECTIVELY\nMoreover, we computed the classwise performance of the\nproposed model on the complete test data containing 10 407\nimages, as shown in Table IX. Considering the classwise\nperformance, we can see that the SIPFormer model performed\nbest for pupil class followed by iris, as shown in Table IX.\nThe higher IoU and DSC scores for both pupil and iris classes\nrepresent the higher overlapped region and similarity index\nbetween the ground-truth and predicted labels of these classes.\nIn contrast, the same metrics gave relatively low scores for\nthe periocular and sclera cl asses. This can be because the\nCASIA-IrisV4 datasets are in grayscale. The pixel intensities\nin the periocular and sclera region are relatively close and\nare not distinguishable from the other two classes (iris and\npupil). As a result, the network performance slightly declined\nin predicting the periocular and sclera labels. Similarly, the\nhigher values of Nice1 and Nice2metrics for both periocular\nand sclera classes reﬂect the higher ratio of false predicted\nlabels for these classes compared to iris and pupil.\nFurthermore, we analyzed the performance of the\nSIPFormer using the receiver operating characteristic (ROC)\nand precision–recall curves of each class, as shown in Fig. 10.\nWe generated the ROC and precision–recall curves for each\nclass by varying the pixel classiﬁcation threshold between\n0 and 1 with a step size of 0.005. It can be observed from\nthe ROC curves in Fig. 10(a) that our proposed SIPFormer\nmodel is trained skillfully in correctly predicting the positive\nlabels in each class. The higher area under the curve (AUC)\nvalue for each class also quantiﬁes the superior predictive per-\nformance of the SIPFormer model. Since, in our study, there\nis a considerable imbalance between the classes. Therefore,\nprecision–recall curves [Fig. 10(b)] show the advantages of\nthe SIPFormer algorithm more intuitively due to the absence\nof true negatives in precisionand recall equations. The higher\nmean average precision (mAP) values for each class show that\nthe SIPFormer is trained precisely, returns accurately (high\nprecision), and correctly predicts the most positive results\n(high recall).\n2) Evaluation Based on Ocular Occlusion Categories:\nIn this experiment, we evaluated the performance of the\nSIPFormer model for different kinds of occlusion using var-\nious metrics. We used 200 randomly selected images from\neach occlusion category, as deﬁned in Table VII. Fig. 9 and\nTable X show that the SIPFormer worked best with clear\nFig. 10. (a) ROC curve and (b) precision–recall curve for each class.\nTABLE X\nIMAGES CATEGORYWISE SEGMENTATION PERFORMANCE OF THE\nSIPF ORMER MODEL USING VARIOUS METRICS .T HE BOLD AND\nUNDERLINED VALUES REPRESENT THE BEST AND SECOND -BEST\nRESULTS ,R ESPECTIVEL Y\ncategory images, followed by those with eyelid and eyelash\nobstruction, whereas its performance relatively declined for\nimages in the truncated and reﬂection categories. Furthermore,\nthe proposed framework performed the least accurately with\nheavily obscured images. The latter three categories can be\ntermed challenging cases in our study. In addition, fewer\nimages belong to these categories in the whole dataset. Con-\nsequently, this lack of data may have resulted in insufﬁcient\nnetwork training to manage such exceptions properly.\n3) Comparison With State-of-the-Art Literature: In this\nexperiment, we evaluated the segmentation accuracy of the\nSIPFormer model to other state-of-the-art models, as shown\nin Table XI. With a mean DSC score of 0.9018, the proposed\nframework provides the best segmentation results for the sclera\nclass, exceeding the second-best results by 0.92%. In contrast,\nit demonstrated the second-best performance in segmenting the\niris and the third-best performance in segmenting the pupil,\ntrailing the best results by 0.97% and 1.12%, respectively.\nIn addition, the proposed SIPFormer framework outperforms\nthe second-best method for segmentation of all ocular classes\nby 0.26%.\nFurthermore, we have evaluated the computational com-\nplexity of the proposed framework with other state-of-the-\nart methods using the consistent simulation environment,\nas shown in Table XII. All the models in Table XII have been\ntrained and evaluated using the same system and hardware\nspeciﬁcations. The computational complexity of the models is\nevaluated using an Intel Core i7-11700 CPU @2.5 GHz, with\n32-GB memory and Nvidia GeForce RTX 3090. Moreover,\nthe training hyperparameters for all models in Table XII are\nadjusted using the Bayesian optimization on 30 objective\nfunction evaluations to ensure the best training performances.\nHere, it can be observed that with 37.2 million parameters,\nthe SIPFormer framework requires 92.13% fewer parameters\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\nTABLE XI\nSEGMENTATION PERFORMANCE COMPARISON WITH THE STATE OF THE\nART USING MEAN DSC SCORE .T HE BOLD AND UNDERLINED\nVALUES REPRESENT THE BEST AND SECOND -BEST\nRESULTS ,R ESPECTIVEL Y\nTABLE XII\nCOMPUTATIONAL COMPLEXITY ANALYSIS WITH THE STATE OF THE ART\nUSING NVIDIA GTX 3090 GPU. THE BOLD AND\nUNDERLINED VALUES REPRESENT THE BEST AND SECOND -BEST\nPERFORMANCES ,R ESPECTIVEL Y\nthan the second-best performing method [58]. Besides, it can\nprocess 37 FPS and requires 26.70 milliseconds only to\nprocess a single image, making it suitable for real-world\nbiometric applications.\nV. CONCLUSION AND DISCUSSION\nThe motivation of this work is to segment multiple ocular\ntraits simultaneously toward devising a multimodal ocular\nsegmentation framework. For this purpose, we employed a\nnovel framework called SIFormer, which fuses the transformer\nprojections with the deep features at the encoder side to\nboost the separation between interclass distributions. Besides,\nthe proposed SIPFormer model obviates the need for high\ncomputational resources due to its lesser number of parameters\n(around 30 million), unlike in other popular semantic segmen-\ntation architectures such as ReﬁneNet [55], SegNet [56], and\nDeepLabv3+ [58].\nMoreover, for several images in the datasets, we observed\nthe difference between the black intensities in the ocular region\nto be very small and clustered predominantly around the\nlower or middle range. Therefore, in this work, we enhanced\nthe ocular traits using preprocessing stage and subsequently\nsuppressed the information on the periocular components.\nIn addition, we removed various reﬂections in the pristine\nimages via an improved holes ﬁlling strategy to achieve the\npreprocessed scans, as shown in Fig. 3. Furthermore, the\nclasses in the training data are highly imbalanced, with the\nperiocular being the dominant class having over 75% of the\nlabels. On the contrary, the pupil class had the least labels\n(less than 5%). This kind of bias can lead the network to\nignore marginalized classes. Therefore, we used the inverse\nfrequency weighting approach to balance the classes by assign-\ning increased weights to the underrepresented classes (sclera,\niris, and pupil). Also, we used the hybrid loss function to\naccount for both the Dice and Tversky losses, enabling the\nmodel to train more precisely and converge quickly.\nFurthermore, we conducted several experiments to evaluate\nthe performance of the SIPFormer framework. We computed\nthe segmentation accuracy of each class for various occlusion\ncategories, as shown in Fig. 9 and Table X. Moreover, the pro-\nposed framework is tested over multiple challenging datasets\nusing various evaluation metrics, as shown in Table VIII.\nBesides, we present the comparative analysis between the\nproposed SIPFormer and existing state-of-the-art algorithms,\nas shown in Tables XI and XII. We have also compared\nthe performance of the proposed SIPFormer model with the\nstandard ViT [37]+ SIPFormer ED unit. We trained the ViT\n(with three transformer encoders) from scratch on the CASIA\ndatasets, and the segmentation results are reported in Table XI.\nHere, we can observe that the SIPFormer achieved higher\nsegmentation accuracy compar ed to the ViT variant. This\nis perhaps because the positional embeddings in SIPFormer,\nunlike in standard ViT, are generated through normalized cross\ncorrelation between the original image and image patches. Fur-\nthermore, the latent projections of the decomposed image are\nobtained through the SIPFormer encoder and combined with\nthe ﬂattened positional encodings to feed to the SIPFormer\ntransformer. It provides the SIPFormer model with better\nand more robust feature learning capability resulting in high\naccuracy in segmenting the multiocular traits in the proposed\nstudy.\nThe simulation results in this research demonstrate the opti-\nmal performance of the SIPFormer framework in segmenting\nthe multiocular biometric traits. The proposed model can be\nreﬁned in the future using more publicly available datasets\nwith more challenging and unique ocular images (both from\nnear and far). Also, the segmented results from the SIPFormer\nmodel can be used for implementing a multiocular biometric\nrecognition system.\nA\nCKNOWLEDGMENT\nThe authors are grateful to the Chinese Academy of\nSciences Institute of Automation for making their datasets\npublicly available for research purposes. They have used\nﬁve subsets of the CASIA-IrisV4 database to perform this\nstudy [36].\nR\nEFERENCES\n[1] J. M. Smereka, “A new method of pupil identiﬁcation,”IEEE Potentials,\nvol. 29, no. 2, pp. 15–20, Mar. 2010.\n[2] P. Corcoran and C. Costache, “Smartphones, biometrics, and a brave new\nworld,” IEEE Technol. Soc. Mag., vol. 35, no. 3, pp. 59–66, Sep. 2016.\n[3] S. Thavalengal and P. Corcoran, “User authentication on smartphones:\nFocusing on iris biometrics,” IEEE Consum. Electron. Mag.,v o l .5 ,\nno. 2, pp. 87–93, Apr. 2016.\nHASSAN et al.: SIPFormer: SEGMENTATION OF MULTIOCULAR BIOMETRIC TRAITS WITH TRANSFORMERS 5001914\n[4] S. Bazrafkan, S. Thavalengal, and P. Corcoran, “An end to end deep\nneural network for iris segmentation in unconstrained scenarios,”Neural\nNetw., vol. 106, pp. 79–95, Oct. 2018.\n[5] A. S. Al-Waisy, R. Qahwaji, S. Ipson, S. Al-Fahdawi, and\nT. A. M. Nagem, “A multi-biometric iris recognition system based\non a deep learning approach,” Pattern Anal. Appl., vol. 21, no. 3,\npp. 783–802, Aug. 2018.\n[6] B. Hassan, R. Ahmed, B. Li, O. Hassan, and T. Hassan, “Autonomous\nframework for person identiﬁcation by analyzing vocal sounds and\nspeech patterns,” inProc. 5th Int. Conf. Control, Autom. Robot. (ICCAR),\nApr. 2019, pp. 649–653.\n[ 7 ] F .N .S i b a i ,H .I .H o s a n i ,R .M .N aqbi, S. Dhanhani, andS. Shehhi, “Iris\nrecognition using artiﬁcial neural networks,”Expert Syst. Appl., vol. 38,\nno. 5, pp. 5940–5946, May 2011.\n[8] Z. Zhou, Y . Du, N. Thomas, and E. Delp, “A new human identiﬁcation\nmethod: Sclera recognition,”IEEE Trans. Syst., Man, Cybern., A, Syst.,\nHum., vol. 42, no. 3, pp. 571–583, May 2012.\n[9] S.-Y . He and C.-P. Fan, “SIFT features and SVM learning based\nsclera recognition method with efﬁcient sclera segmentation for identity\nidentiﬁcation,” in Proc. IEEE Int. Conf. Artif. Intell. Circuits Syst.\n(AICAS), Mar. 2019, pp. 297–298.\n[10] M. S. Maheshan, B. S. Harish, and N. Nagadarshan, “A convolution\nneural network engine for sclera recognition,”Int. J. Interact. Multime-\ndia Artif. Intell., vol. 6, no. 1, p. 78, 2020.\n[11] R. Mesbah, B. McCane, and S. Mills, “Conditional random ﬁelds\nincorporate convolutional neural networks for human eye sclera seman-\ntic segmentation,” in Proc. IEEE Int. Joint Conf. Biometrics (IJCB),\nOct. 2017, pp. 768–773.\n[12] R. Hentati, M. Hentati, and M. Abid, “Development a new algorithm\nfor iris biometric recognition,”Int. J. Comput. Commun. Eng.,v o l .1 ,\nno. 3, p. 283, 2012.\n[13] M. Trokielewicz, A. Czajka, and P. Maciejewicz, “Iris recognition\nafter death,” IEEE Trans. Inf. Forensics Security , vol. 14, no. 6,\npp. 1501–1514, Jun. 2019.\n[14] N. Susitha and R. Subban, “Reliable pupil detection and iris segmen-\ntation algorithm based on SPS,”Cognit. Syst. Res., vol. 57, pp. 78–84,\nOct. 2019.\n[15] N. Nugrahaningsih and M. Porta , “Pupil size as a biometric trait,”\nin Proc. Int. Workshop Biometric Authentication. Cham, Switzerland:\nSpringer, 2014, pp. 222–233.\n[16] B. Hassan, S. Qin, T. Hassan, R. Ahmed, and N. Werghi, “Joint\nsegmentation and quantiﬁcation of chorioretinal biomarkers in optical\ncoherence tomography scans: A deep learning approach,”IEEE Trans.\nInstrum. Meas., vol. 70, pp. 1–17, 2021.\n[17] R. Ahmed, Y . Chen, B. Hassan, L. Du, T. Hassan, and J. Dias, “Hybrid\nmachine-learning-based spectrum sensing and allocation with adaptive\ncongestion-aware modeling in CR-assisted IoV networks,”IEEE Internet\nThings J., vol. 9, no. 24, pp. 25100–25116, Dec. 2022.\n[18] A. Khan, B. Hassan, S. Khan, R. Ahmed, and A. Abuassba, “DeepFire:\nA novel dataset and deep transfer learning benchmark for forest ﬁre\ndetection,” Mobile Inf. Syst., vol. 2022, pp. 1–14, Apr. 2022.\n[19] R. Ahmed, Y . Chen, and B. Hassan, “Deep residual learning-based\ncognitive model for detection and classiﬁcation of transmitted signal\npatterns in 5G smart city networks,”Digit. Signal Process., vol. 120,\nJan. 2022, Art. no. 103290.\n[20] L. A. Zanlorensi, R. Laroca, E. Luz, A. S. Britto, L. S. Oliveira, and\nD. Menotti, “Ocular recognition databases and competitions: A survey,”\nArtif. Intell. Rev., vol. 55, no. 1, pp. 129–180, Jan. 2022.\n[21] D. Zhu, J. Li, H. Li, J. Peng, X. Wang, and X. Zhang, “A less-constrained\nsclera recognition method based on Stem-and-leaf branches network,”\nPattern Recognit. Lett., vol. 145, pp. 43–49, May 2021.\n[22] G. Sahin and O. Susuz, “Encoder–decoder convolutional neural network\nbased iris-sclera segmentation,” inProc. 27th Signal Process. Commun.\nAppl. Conf. (SIU), Apr. 2019, pp. 1–4.\n[23] R. A. Naqvi and W.-K. Loh, “Sclera-Net: Accurate sclera segmenta-\ntion in various sensor images based on residual encoder and decoder\nnetwork,” IEEE Access, vol. 7, pp. 98208–98227, 2019.\n[24] J. Daugman, “Information theory and the iriscode,” IEEE Trans. Inf.\nForensics Security, vol. 11, no. 2, pp. 400–409, Feb. 2016.\n[25] W. Dong, Z. Sun, and T. Tan, “Iris matching based on personalized\nweight map,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 9,\npp. 1744–1757, Sep. 2011.\n[26] J. K. Pillai, V . M. Patel, R. Chellappa, and N. K. Ratha, “Secure\nand robust iris recognition using random projections and sparse rep-\nresentations,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 9,\npp. 1877–1893, Sep. 2011.\n[27] R. R. Jha, G. Jaswal, D. Gupta, S. Saini, and A. Nigam,\n“PixISegNet: Pixel-level iris segmentation network using convolutional\nencoder–decoder with stacked hourglass bottleneck,” IET Biometrics,\nvol. 9, no. 1, pp. 11–24, Jan. 2020.\n[28] K. Nguyen, C. Fookes, A. Ross, and S. Sridharan, “Iris recognition with\noff-the-shelf CNN features: A deep learning perspective,”IEEE Access,\nvol. 6, pp. 18848–18855, 2017.\n[29] T. Zhao, Y . Liu, G. Huo, and X. Zhu, “A deep learning iris recognition\nmethod based on capsule network architecture,”IEEE Access,v o l .7 ,\npp. 49691–49701, 2019.\n[30] Y .-H. Yiu et al., “DeepVOG: Open-source pupil segmentation and gaze\nestimation in neuroscience using deep learning,”J. Neurosci. Methods,\nvol. 324, Aug. 2019, Art. no. 108307.\n[31] K. Kitazumi and A. Nakazawa, “Robust pupil segmentation and center\ndetection from visible light images using convolutional neural network,”\nin Proc. IEEE Int. Conf. Syst., Man, Cybern. (SMC) , Oct. 2018,\npp. 862–868.\n[32] F. Vera-Olmos, E. Pardo, H. Melero, and N. Malpica, “DeepEye: Deep\nconvolutional network for pupil detection in real environments,”Integr.\nComput.-Aided Eng., vol. 26, no. 1, pp. 85–95, 2019.\n[33] A. J.-W. Whang et al., “Pupil s ize prediction techniques based on\nconvolution neural network,”Sensors, vol. 21, no. 15, p. 4965, 2021.\n[34] L. Shi, C. Wang, F. Tian, and H. Jia, “An integrated neural network\nmodel for pupil detection and tracking,”Soft Comput., vol. 25, no. 15,\npp. 10117–10127, Aug. 2021.\n[35] W.-L. Ou, T.-L. Kuo, C.-C. Chang, and C.-P. Fan, “Deep-learning-based\npupil center detection and tracking technology for visible-light wearable\ngaze tracking devices,”Appl. Sci., vol. 11, no. 2, p. 851, Jan. 2021.\n[36] CASIA Iris Image Database V4.0. Accessed: Mar. 17, 2022. [Online].\nAvailable: http://biometrics.idealtest.org/\n[37] A. Dosovitskiy et al., “An image is worth 16×16 words: Transformers\nfor image recognition at scale,” 2020,arXiv:2010.11929.\n[38] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017, pp. 1–15.\n[39] V . Pihur, S. Datta, and S. Datta, “Weighted rank aggregation of cluster\nvalidation measures: A Monte Carlo cross-entropy approach,”Bioinfor-\nmatics, vol. 23, no. 13, pp. 1607–1615, Jul. 2007.\n[40] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-Net: Fully convolutional\nneural networks for volumetric medical image segmentation,” inProc.\n4th Int. Conf. 3D Vis. (3DV), Oct. 2016, pp. 565–571.\n[41] B. Hassan, S. Qin, T. Hassan, M.U. Akram, R. Ahmed, and N. Werghi,\n“CDC-Net: Cascaded decoupled convolutional network for lesion-\nassisted detection and grading of retinopathy using optical coherence\ntomography (OCT) scans,” Biomed. Signal Process. Control\n, vol. 70,\nSep. 2021, Art. no. 103030.\n[42] B. Hassan et al., “Deep learningbased joint segmentation and charac-\nterization of multi-class retinal ﬂuid lesions on OCT scans for clinical\nuse in anti-VEGF therapy,”Comput. Biol. Med., vol. 136, Sep. 2021,\nArt. no. 104727.\n[43] R. Ahmed, Y . Chen, and B. Hassan,“Deep learning-driven opportunistic\nspectrum access (OSA) framework for cognitive 5G and beyond 5G\n(B5G) networks,”Ad Hoc Netw., vol. 123, Dec. 2021, Art. no. 102632.\n[44] A. Khan, S. Khan, B. Hassan, and Z. Zheng, “CNN-based smoker\nclassiﬁcation and detection in smart city application,”Sensors, vol. 22,\nno. 3, p. 892, Jan. 2022.\n[45] T. Hassan, B. Hassan, M. U. Akram, S. Hashmi, A. H. Taguri, and\nN. Werghi, “Incremental cross-domain adaptation for robust retinopathy\nscreening via Bayesian deep learning,” IEEE Trans. Instrum. Meas.,\nvol. 70, pp. 1–14, 2021.\n[46] S. S. M. Salehi, D. Erdogmus, andA. Gholipour, “Tversky loss function\nfor image segmentation using 3D fully convolutional deep networks,”\nin Proc. Int. Workshop Mach. Learn. Med. Imag.Cham, Switzerland:\nSpringer, 2017, pp. 379–387.\n[47] T. Hassan, B. Hassan, A. ElBaz, and N. Werghi, “A dilated residual\nhierarchically fashioned segmentation framework for extracting Gleason\ntissues and grading prostate cancer from whole slide images,” inProc.\nIEEE Sensors Appl. Symp. (SAS), Aug. 2021, pp. 1–6.\n[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\n2014, arXiv:1412.6980.\n[49] G. Hinton, N. Srivastava, and K. Swersky, “Neural networks for machine\nlearning,” Coursera, Video Lectures, vol. 264, no. 1, pp. 2146–2153,\n2012.\n[50] K. P. Murphy, Machine Learning: A Probabilistic Perspective .\nCambridge, MA, USA: MIT Press, 2012.\n5001914 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 72, 2023\n[51] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollar,\n“Designing network design spaces,” inProc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2020, pp. 10428–10436.\n[52] M. Tan and Q. Le, “EfﬁcientNetV2:Smaller models and faster training,”\nin Proc. Int. Conf. Mach. Learn., 2021, pp. 10096–10106.\n[53] K. He, X. Zhang, S. Ren, and J . Sun, “Deep residual learning for\nimage recognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[54] S. Mazhar et al., “AUnet: A deep learning framework for surface water\nchannel mapping using large-coverage remote sensing images and sparse\nscribble annotations from OSM data,” Remote Sens., vol. 14, no. 14,\np. 3283, 2022.\n[55] G. Lin, F. Liu, A. Milan, C. Shen, and I. Reid, “ReﬁneNet: Multi-path\nreﬁnement networks for dense prediction,” IEEE Trans. Pattern Anal.\nMach. Intell., vol. 42, no. 5, pp. 1228–1242, May 2020.\n[56] V . Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A deep\nconvolutional encoder–decoder architecture for image segmentation,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 12, pp. 2481–2495,\nJan. 2017.\n[57] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“SegFormer: Simple and efﬁcient design for semantic segmentation with\ntransformers,” in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021,\npp. 12077–12090.\n[58] L.-C. Chen, Y . Zhu, G. Papa ndreou, F. Schroff, and H. Adam,\n“Encoder–decoder with atrous separable convolution for semantic image\nsegmentation,” in Proc. Eur. Conf. Comput. Vis. (ECCV) .C h a m ,\nSwitzerland: Springer, 2018, pp. 801–818.\nBilal Hassan received the Ph.D. degree in pattern\nrecognition and intelligent systems from Beihang\nUniversity, Beijing, China, in 2022.\nHe is currently a Post-Doctoral Fellow with\nthe Khalifa University of Science and Technology,\nAbu Dhabi, United Arab Emirates. His research\ninterests lie in the domain of computer vision, med-\nical imaging, wireless communication, and control\nsystems.\nTaimur Hassan (Member, IEEE) received the Ph.D.\ndegree in computer engineering from the National\nUniversity of Sciences and Technology, Islamabad,\nPakistan, in 2019.\nHe is currently working as a Post-Doctoral\nFellow with the Khalifa University Center for\nAutonomous Robotic Systems (KUCARS) and the\nCenter for Cyber-Physical Systems (C2PS), Depart-\nment of Electrical Engineering and Computer Sci-\nence, Khalifa University of Science and Technology,\nAbu Dhabi, United Arab Emirates. He has led many\nlocal and foreign research projects. His research interests lie in the ﬁelds of\nmedical imaging, computer vision, deep learning, the Internet of Things, and\nrobotics.\nDr. Hassan’s Ph.D. research won the Gold Award in the Research and\nDevelopment Category at the Pakistan Software Houses Association for IT\nand ITeS (P@SHA) ICT Awards in 2016, the Gold Award in the Research and\nDevelopment Category at the Asia Paciﬁc ICT Alliance (APICTA) Awards in\n2016, and the Gold Award in the Artiﬁcial Intelligence category at P@SHA\nICT Awards in 2018. He was a recipient of many national and international\nawards.\nRamsha Ahmed received the B.E. degree in\ntelecommunication engineering and the M.S. degree\nin information security from the National University\nof Sciences and Technology, Islamabad, Pakistan, in\n2013 and 2017, respectively, and the Ph.D. degree\nin information and communication engineering from\nthe University of Science and Technology Beijing,\nBeijing, China, in 2022.\nShe is currently a Post-Doctoral Fellow with\nthe Khalifa University of Science and Technology,\nAbu Dhabi, United Arab Emirates. Her research\ninterests include medical imaging, wireless communication, information secu-\nrity, the Internet of Things (IoT), and machine vision-related applications.\nNaoufel Werghi (Senior Member, IEEE) received\nthe Ph.D. degree in computer vision from the Uni-\nversity of Strasbourg, Strasbourg, France, in 1996.\nHe has been a Research Fellow with the Divi-\nsion of Informatics, The University of Edinburgh,\nEdinburgh, U.K., and a Lecturer with the Depart-\nment of Computer Sciences, University of Glasgow,\nGlasgow, U.K. He has also been a Visiting Professor\nwith the Department of Electrical and Computer\nEngineering, University of Louisville, Louisville,\nKY , USA. He is currently an Associate Professor\nwith the Department of Electrical and Computer Engineering, Khalifa Uni-\nversity of Science and Technology, AbuDhabi, United Arab Emirates. His\nmain area of research is image analysis and interpretation, where he has been\nleading several funded projects in theareas of biometrics, medical imaging,\nand intelligent systems.\nJorge Dias (Senior Member, IEEE) r eceived the\nPh.D. degree in electrical engineering from the Uni-\nversity of Coimbra, Coimbra, Portugal, in 1994.\nHe coordinated the Artiﬁcial Perception Group,\nInstitute of Systems and Robotics, University of\nCoimbra. He is currently a Full Professor with\nthe Khalifa University of Science and Technology,\nAbu Dhabi, United Arab Emirates, where he is also\nthe Deputy Director of the Center of Autonomous\nRobotic Systems. His expertise is in the area of\nartiﬁcial perception (computer vision and robotic\nvision) and has contributions on the ﬁeld since 1984. He has been a principal\ninvestigator and a consortia coordinator from several research international\nprojects and coordinates the researchgroup on computer vision and artiﬁcial\nperception from the Khalifa University Center for Autonomous Robotic\nSystems (KUCARS). He has published several articles in the area of computer\nvision and robotics that include more than 300 publications in international\njournals and conference proceedings and recently published book on prob-\nabilistic robot perception that addresses the use of statistical modeling and\nartiﬁcial intelligence for perception, planning, and decision in robots. He was\nthe Project Coordinator of two European Consortium for the Projects “Social\nRobot” and “GrowMeUP” that were developed to support the inclusivity and\nwellbeing for of the elderly generation.",
  "topic": "Biometrics",
  "concepts": [
    {
      "name": "Biometrics",
      "score": 0.7521681785583496
    },
    {
      "name": "Computer science",
      "score": 0.7453415393829346
    },
    {
      "name": "Artificial intelligence",
      "score": 0.737028956413269
    },
    {
      "name": "Computer vision",
      "score": 0.7207684516906738
    },
    {
      "name": "Iris recognition",
      "score": 0.6438396573066711
    },
    {
      "name": "Segmentation",
      "score": 0.5625855922698975
    },
    {
      "name": "IRIS (biosensor)",
      "score": 0.4459819197654724
    },
    {
      "name": "Discriminative model",
      "score": 0.42824554443359375
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34066641330718994
    }
  ]
}