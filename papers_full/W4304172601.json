{
  "title": "TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data using deep learning based on transformer",
  "url": "https://openalex.org/W4304172601",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1917214346",
      "name": "Tao Song",
      "affiliations": [
        "China University of Petroleum, East China",
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2151139954",
      "name": "Dai Huanhuan",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2099309572",
      "name": "Shuang Wang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2074001660",
      "name": "Gan Wang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2099723430",
      "name": "Xudong Zhang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A1985094872",
      "name": "Ying Zhang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A4304174716",
      "name": "Linfang Jiao",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A1917214346",
      "name": "Tao Song",
      "affiliations": [
        "China University of Petroleum, East China",
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2151139954",
      "name": "Dai Huanhuan",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2099309572",
      "name": "Shuang Wang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2074001660",
      "name": "Gan Wang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2099723430",
      "name": "Xudong Zhang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A1985094872",
      "name": "Ying Zhang",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A4304174716",
      "name": "Linfang Jiao",
      "affiliations": [
        "China University of Petroleum, East China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2907514116",
    "https://openalex.org/W2523620612",
    "https://openalex.org/W2794480084",
    "https://openalex.org/W2060455590",
    "https://openalex.org/W3208416788",
    "https://openalex.org/W3089552784",
    "https://openalex.org/W2193667971",
    "https://openalex.org/W3013151148",
    "https://openalex.org/W2746479894",
    "https://openalex.org/W2787894218",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W3092295864",
    "https://openalex.org/W2598326928",
    "https://openalex.org/W2796170779",
    "https://openalex.org/W3005127469",
    "https://openalex.org/W4283586679",
    "https://openalex.org/W3011537067",
    "https://openalex.org/W2964643116",
    "https://openalex.org/W2102212449",
    "https://openalex.org/W4205463297",
    "https://openalex.org/W2803194497",
    "https://openalex.org/W2730411900",
    "https://openalex.org/W2612519453",
    "https://openalex.org/W3004512212",
    "https://openalex.org/W3198628780",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W4206082836",
    "https://openalex.org/W2948079051",
    "https://openalex.org/W3154396742",
    "https://openalex.org/W2966569571",
    "https://openalex.org/W3080797970",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2892221324",
    "https://openalex.org/W2953251392",
    "https://openalex.org/W3212776961",
    "https://openalex.org/W4205454839",
    "https://openalex.org/W2800392236",
    "https://openalex.org/W3201260316",
    "https://openalex.org/W2109890799",
    "https://openalex.org/W3156849853",
    "https://openalex.org/W1979283544",
    "https://openalex.org/W2896714640",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4213108508",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4226462961"
  ],
  "abstract": "Recent advances in single-cell RNA sequencing (scRNA-seq) have accelerated the development of techniques to classify thousands of cells through transcriptome profiling. As more and more scRNA-seq data become available, supervised cell type classification methods using externally well-annotated source data become more popular than unsupervised clustering algorithms. However, accurate cellular annotation of single cell transcription data remains a significant challenge. Here, we propose a hybrid network structure called TransCluster, which uses linear discriminant analysis and a modified Transformer to enhance feature learning. It is a cell-type identification tool for single-cell transcriptomic maps. It shows high accuracy and robustness in many cell data sets of different human tissues. It is superior to other known methods in external test data set. To our knowledge, TransCluster is the first attempt to use Transformer for annotating cell types of scRNA-seq, which greatly improves the accuracy of cell-type identification.",
  "full_text": "TransCluster: A Cell-Type\nIdentiﬁcation Method for\nsingle-cell RNA-Seq data using\ndeep learning based on\ntransformer\nTao Song1,2*, Huanhuan Dai1, Shuang Wang1*, Gan Wang1,\nXudong Zhang1, Ying Zhang1 and Linfang Jiao1\n1College of Computer Science and Technology, China University of Petroleum (East China), Qingdao,\nChina, 2Department of Artiﬁcial Intelligence, Faculty of Computer Science, Campus de\nMontegancedo, Polytechnical University of Madrid, Boadilla Del Monte, Madrid, Spain\nRecent advances in single-cell RNA sequencing (scRNA-seq) have accelerated\nthe development of techniques to classify thousands of cells through\ntranscriptome pro ﬁling. As more and more scRNA-seq data become\navailable, supervised cell type classiﬁcation methods using externally well-\nannotated source data become more popular than unsupervised clustering\nalgorithms. However, accurate cellular annotation of single cell transcription\ndata remains a signiﬁcant challenge. Here, we propose a hybrid network\nstructure called TransCluster, which uses linear discriminant analysis and a\nmodiﬁed Transformer to enhance feature learning. It is a cell-type identiﬁcation\ntool for single-cell transcriptomic maps. It shows high accuracy and robustness\nin many cell data sets of different human tissues. It is superior to other known\nmethods in external test data set. To our knowledge, TransCluster is theﬁrst\nattempt to use Transformer for annotating cell types of scRNA-seq, which\ngreatly improves the accuracy of cell-type identiﬁcation.\nKEYWORDS\ncell-type identiﬁcation, single-cell sequencing data, transformer, neural network,\ndeep learning\n1 Introduction\nRecent advances in single-cell RNA sequencing (scRNA-seq) have furthered the\nunderstanding of cell compositions in complex tissues (Haque et al., 2017). Through the\ncharacterization of different cell types based on gene expression levels, facilitating our\nunderstanding on disease pathogen e s e s ,c e l l u l a rl i n e a g e so rd i f f erentiation trajectories and\ncell-cell communication (Macosko et al., 2015; Regev et al., 2017; Potter, 2018; Shao et al.,\n2020). In the data processing protocols of scRNA-seq experiments, cell type identiﬁcation is a\nkey step in the subsequent analysis. The current strategies are divided into two main types,\nunsupervised-based and supervised-based annotation strategies. Unsupervised-based strategy\napplies clustering to classify cells into different clusters (Su et al., 2021; Tian et al., 2021).\nOPEN ACCESS\nEDITED BY\nQuan Zou,\nUniversity of Electronic Science and\nTechnology of China, China\nREVIEWED BY\nJin-Xing Liu,\nQufu Normal University, China\nQifeng Bai,\nLanzhou University, China\nChang Lu,\nNortheast Normal University, China\n*CORRESPONDENCE\nTao Song,\nt.song@upm.es\nShuang Wang,\nwangshuang@upc.edu.cn\nSPECIALTY SECTION\nThis article was submitted to\nComputational Genomics,\na section of the journal\nFrontiers in Genetics\nRECEIVED 07 September 2022\nACCEPTED 23 September 2022\nPUBLISHED 11 October 2022\nCITATION\nSong T, Dai H, Wang S, Wang G, Zhang X,\nZhang Y and Jiao L (2022), TransCluster:\nA Cell-Type Identiﬁcation Method for\nsingle-cell RNA-Seq data using deep\nlearning based on transformer.\nFront. Genet. 13:1038919.\ndoi: 10.3389/fgene.2022.1038919\nCOPYRIGHT\n© 2022 Song, Dai, Wang, Wang, Zhang,\nZhang and Jiao. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License\n(CC BY). The use, distribution or\nreproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nFrontiers inGenetics frontiersin.org01\nTYPE Original Research\nPUBLISHED 11 October 2022\nDOI 10.3389/fgene.2022.1038919\nSeveral methods including Scanpy (Wolf et al., 2018), Seurat (Butler\net al., 2018), SINCERA (Guo et al., 2015), SC3 (Kiselev et al., 2017),\nSIMLR (Wang et al., 2017), SNN-Clip (Xu and Su, 2015), BackSPIN\n(Zeisel et al., 2015) belong to this category. This type of approach\nrequires ap r i o r iknowledge about known cellular markers.\nReplicability of this cell identiﬁcation protocol can be further\nreduced with increased number of clusters and multiple\nselections of cluster marker genes (Shao et al., 2020; Wang et al.,\n2021). Supervised-based strategy determines potential cell identity\nby comparing similarities between individual cells and reference\ndatabases of bulk or scRNA-seq proﬁles, such as scDeepSort (Shao\net al., 2021), SingleR (Aran et al., 2019), ACTINN (Ma and\nPellegrini, 2020), singleCellNet (Tan and Cahan, 2019), scMap-\ncell (Kiselev et al., 2018) .S t i l l ,t h ea c c u r a t ec e l lt y p ea n n o t a t i o nf o r\nsingle-cell transcriptomic data remains a great challenge\n(Lahnemann et al., 2020).\nFortunately, recent advances in deep learning have enabled\nsigniﬁcant progress in the ability of arti ﬁcial intelligence\ntechniques to integrate big data, incorporate existing\nknowledge and learn arbitrarily complex relationships\n(Gibney, 2015; Silver et al., 2016). Given the state-of-the-art\naccuracy deep learning has achieved in numerous prediction\ntasks, it has been increasingly used in biological research (Zhang\net al., 2019) and biomedical applications (Wainberg et al., 2018;\nLv et al., 2020). For example, Jian Hu et al. proposed the ItClust\nmethod, which uses deep neural networks to learn feature\nexpressions on the source data and then migrate to the target\ndata to cluster the unknown labeled cells (Hu et al., 2020). One of\nthe commonly used deep learning methods is convolutional\nneural networks (CNNs) ( Qian et al., 2018 ), a class of\nfeedforward neural networks. In CNNs, convolutional\noperations are good at extracting local features but have\ndifﬁculties in capturing global representations. Since the\nintroduction of Transformer ( Vaswani et al., 2017 ), it has\nshown breakthrough performance in many learning tasks, and\nits strength lies in its ability to capture global contextual\ninformation in an attentional manner to establish a long-\nrange dependence on the target ( Song et al., 2022 ). We\npropose a new deep learning-based method for single-cell\ncategory prediction by combining CNN with Transformer to\nextract more powerful features.\nIn this study, we designed a cell type identiﬁcation method\ncalled TransCluster, based on the Transformer framework. To the\nbest of our knowledge, this is theﬁrst time that Transformer is\napplied to theﬁeld of single cell identiﬁcation. Firstly, we prepared\nsingle-cell transcriptional proﬁles of different tissues in the human\nbody (Han et al., 2020), as the training set. Next, we used the\nimproved Transformer model for feature extraction of the gene\nexpression matrix. Then, features were further extracted using CNN.\nFinally, we compared the performance of TransCluster with other\nknown methods on an external dataset. In addition, we evaluated the\nperformance of TransCluster with eight additional human tissue\nscRNA-seq atlases. The results demonstrated that TransCluster is a\nrobust method that can help scientists achieve the accurate cell-type\nannotation of scRNA-seq data without additional prior knowledge.\n2 Materials and methods\n2.1 Datasets\nThe scRNA-seq data (Shao et al., 2021; Pang et al., 2022) were\nobtained from theShao et al. (2021)and Baron et al. (Hu et al.,\n2020). The Shao dataset includes primary tissues from human\nand mouse, which exclude unannotated cells. The Baron dataset\nis a large human pancreas dataset. Human gene symbols were\nmodiﬁed based on NCBI gene data, unmatched genes and\nduplicated genes were removed, and for all human datasets,\nthe raw data were normalized by the global-scaling normalization\nmethod LogNormalize. On the one hand, we selected human\ntissues to verify the applicability of TransCluster on different\ntissues, including pancreas, human peripheral blood, adipose,\nadrenal gland, liver, kidney, spleen and pleura, with a total of\n8 tissues and 51744 cells and the number of cell categories are 14,\n10, 7, 9, 11, 7, 9, and 5 respectively. For each cell type, cells\nnumbering at least more than 5‰ of the total cells in each tissue,\nthe ratio of training and testing cells was set to 8:2, randomly\ndivided into training and testing sets, andﬁve experiments were\nperformed, with the average value taken as theﬁnal result. On the\nother hand, to compare the accuracy of TransCluster with other\nmethods, all cells from a particular tissue were used to train the\nmodel for cell-type prediction on the test cells that originated\nfrom the same tissue. Firstly, genes with zero expression in both\ndatasets were removed to decrease the amount of data and to\nreduce the effect of irrelevant information (it was experimentally\nveriﬁed that different gene classes of cells in both datasets could\nstill make accurate predictions). Secondly, cell types that were\npresent in both datasets were selected to avoid unknown cell\ntypes. Finally, cells that were present in both datasets were\nremoved to ensure the objectivity of the prediction results.\nThe lung dataset has six categories of cells, including\nTransformed epithelium, AT2 cell, Macrophage, T cell,\nEndothelium and Fibroblast. In the kidney dataset, there are\nthree categories of Endothelial cell, Epithelial cell, and Proximal\ntubule epithelial cell. In the blood dataset, there are four\ncategories of B cell, Dendritic cell, Monocyte, and T cell. In\naddition, we have done a large number of stability experiments to\ndetermine the hyperparameters of the model and show the\nexperimental results in the paper.\n2.2 Model architecture\nTransCluster consists of three components: A dimensionality\nreduction part, a weighted feature extractor and a linear classiﬁer.\nThe dimensionality reduction component uses linear\nFrontiers inGenetics frontiersin.org02\nSong et al. 10.3389/fgene.2022.1038919\ndiscriminant analysis (LDA) (Hastie et al., 2009), a supervised\nmachine learning algorithm that reduces features to the\nappropriate dimension based on the labels of the data. The\nweighted feature extractor inductively learns the feature\ninformation of the cells and generates linear separable feature\nspace of the cells. In this layer, a modi ﬁed version of the\nTransformer (Vaswani et al., 2017) is used as the backbone,\ncombined with a one-dimensional CNN (Qian et al., 2018). The\nﬁnal linear classiﬁer classiﬁes the ﬁnal cell state representation\ngenerated from the weighted feature extractor into one of the\npredeﬁned cell type categories. The structure of the model is\nshown in Figure 1.\n2.2.1 Linear discriminant analysis\nAs shown in Figure 1, we use linear discriminant analysis\n(LDA) to reduce the dimensionality of the gene expression\nmatrix. In order to solve a multilabel classiﬁcation problem\nefﬁciently and effectively, we need not only to consider the\ncorrelation of class labels and features of each data item but\nalso to take into account the different cardinalities of the classes\n(Xu et al., 2021). The basic idea of LDA is to project the high-\ndimensional samples into the optimal discriminant vector space\nin order to extract the categorical information and compress the\nspatial dimensionality (Guo et al., 2020). At the same time, the\nprojection ensures that the samples have the maximum inter-\nclass distance and the minimum intra-class distance in the new\nsubspace, i.e., the samples have the best separability in this space\n(Xu et al., 2021). For the input single-cell data matrix (the\nnumber of genes is m, the number of cells is n, the number of\nclasses is k and the dimension after dimensionality reduction is\nd), it is experimentally veriﬁed that the best performance is\nachieved when d equals k-1. So, the dense representation with\nﬁxed size dimension k-1is extracted as the initial representation.\nThe matrix after LDA processing is transposed, each number in\nthe matrix is added with the same number so that all matrix\nnumbers are positive, and table headers are added to obtain the\nreduced-dimensional gene expression matrix.\nD′ /equals [LDA(D)]\nT + A (1)\nwhere D is the gene expression matrix input to the LDA module,\nA is a suitable positive matrix with each element being an\nidentical positive number, and D′ is the output matrix after\npartial processing by dimensionality reduction.\n2.2.2 Weighted feature extractor\nTransformer uses multi-head attention instead of recurrent\nlayer or convolutional layer to extract information, which\nimproves the performance of multiple tasks in natural\nlanguage processing (NLP) (Vaswani et al., 2017; Sun et al.,\n2021). Compared with convolutional neural network (CNN)\n(Qian et al., 2018 ) and recurrent neural network (RNN)\n(Hochreiter, 1998), Transformer shows superior ability to deal\nFIGURE 1\nStructure of TransCluster. The input of TransCluster is the gene expression matrix of cells and the category of cells.(A) The cellular gene\nexpression matrix was downscaled by the LDA method according to the cell category.(B) Feature extraction of the input processed gene expression\nmatrix by(A) using a modiﬁed transformer and a one-dimensional CNN.(C) Classiﬁcation of cells using linear classiﬁers, n is the number of\ncategories.\nFrontiers inGenetics frontiersin.org03\nSong et al. 10.3389/fgene.2022.1038919\nwith long-range dependencies (Guo et al., 2020). Multi-head\nattention mechanism enables Transformer to learn the features\nof different subsequences in the sequence (Wang et al., 2022).\nTransformer is capable of linking different positions of a\nsequence to obtain an embedding containing contextual\ninformation when processing sequence information ( Baron\net al., 2016).\nThe LDA-encoded sequence is input to Transformer to\ngenerate a feature vectorT\ntransformer, which contains sequence\nstructure information.\nTtransformer /equals Transformer(D′) (2)\nWhere D′ is the output matrix after partial processing by\ndimensionality reduction, Ttransformer is the gene expression\nmatrix after transformer processing.\nSelf-attention layer, Firstly, each gene expression in the LDA\nreduced matrix is considered as a vector, and Transformer\nmultiplies each vector of the input by three matrices to obtain\nthree new vectors Q, K, and V, thus adding more parameters and\nimproving the model effect. The attention score is calculated by\ncomputing the dot product of Q and the K vector of each gene.\nThe obtained scores are normalized with SoftMax. The V-vector\nof each gene is multiplied by the normalized value to the output\nof the self-attentive layer, with the following equation.\nAttention(Q, K, V) /equals SoftMax(\nQKT\n/radicaltpext/radicaltpext\ndk\n√ )V (3)\nWhere Q is the query vector, K denotes the vector of relevance of\nthe queried information to other information, V denotes the\nvector of queried information, andd\nk is the dimension of the key\nvector.\nMulti-Head Attention, Each head computes its own\nAttention, and then multiplies it to obtain the ﬁnal feature\nrepresentation after stitching.\nMultiHead(Q, K, V) /equals Concat(head1, ..., headh)Wo (4)\nWhere headi /equals Attention(Q, K, V), Concat is a bitwise sum\noperation, Wo\ni ∈ Rhdv×dmodel , which is the weight matrix. If\nh /equals 8, then dk /equals dv /equals dmodel/h /equals 64, Setting different h =\n{1,2,3,4,5,6,7,8,9,10} to do sensitivity tests, the results show\nthat the model works best with h = 5. Therefore, our model\ntakes head = 5.\nPosition-wise Feed-Forward Networks, The position fully\nconnected feed-forward network has two dense layers, theﬁrst\nlayer has a Relu activation function and the second layer is a\nlinear activation function. Position-wise means that the input\nand output dimensions are the same. The formulation is stated\nbelow:\nFFN(x) /equals max (0,x W\n1 + b1)W2 + b2 (5)\nWhere x denotes the multi-head output. xW1 + b1 denotes a\nlinear transformation, max represents the Relu activation\nfunction, and W2 and b2 are the weights of the second linear\ntransformation.\nPositional encodings, Cos and sin functions are used to\nencode the position and enhance the model ’s ability to\nperceive the position information. The formulas are as follows.\nPE(pos,2i+1) /equals sin ( pos\n100002i/dmodel\n) (6)\nPE(pos,2i+1) /equals cos ( pos\n100002i/dmodel\n) (7)\nWhere pos indicates the position of the gene,i indicates the\ndimension of the gene, dmodel denotes the dimension of\nembedding.\nThe Transformer-processed matrixTtransformer is fed into the\n1D-CNN network ( Han et al., 2020 ) for further feature\nextraction. Where T is the sequence after one-dimensional\nconvolutional processing.\nT /equals CNN(T\ntransformer) (8)\n2.2.3 Linear classiﬁer layer\nThe features extracted by the CNN are fed into a linear\nclassi ﬁer for category prediction (Baron et al., 2016)a n dt h e\nprobabilities of each categor y are output. The features go\nthrough nonlinear changes in the dense layer to extract the\nassociation between these features andﬁnally map them to the\noutput space. The activation function is softmax. The loss\nfunction is calculated as follows. Whereyi is the real label and\nˆyi is the predicted label.\nLoss /equals− ∑\noutput size\ni/equals 1 yi.log ˆyi (9)\n2.3 Baseline methods\nTo test the performance of our method with other methods\non annotating cell types of single-cell transcriptomics data, we\ncompare TransCluster with the following baseline methods.\nBecause our model is a deep learning method, which is a\nsupervised learning method, in order to have more convincing\nexperimental results, all the baseline methods we selected are\nsupervised learning methods.\n1. scDeepsort (Shao et al., 2021) is a graph-based method for\nsingle-cell category prediction. To construct the weighted cell-\ngene graph, cells and genes were both treated as graph nodes\nand the gene expression for each cell was regarded as the\nweighted edge between cells and genes.\n2. SingleR (Aran et al., 2019) is an R package for automated cell\ntype annotation of single cell RNA-seq sequencing (scRNA-\nseq) data, using a reference transcriptome dataset of pure cell\ntypes to independently infer the likely cell type of each cell.\nFrontiers inGenetics frontiersin.org04\nSong et al. 10.3389/fgene.2022.1038919\n3. ACTINN (Ma and Pellegrini, 2020) is a method for automatic\ncell class recognition using neural networks, which uses a\nneural network with three hidden layers trained on a dataset\nwith predeﬁned cell types and predicts the cell types of other\ndatasets based on the trained parameters.\n4. singleCellNet (Tan and Cahan, 2019) is able to classify cells\nacross species based on the processed gene expression matrix.\n5. scMap_cell (Kiselev et al., 2018) takes the cells in the query\ndataset as the nearest neighbors of the reference data, and the\nnearest neighbor cells in the reference dataset are most similar\nto the cells in the query dataset.\n2.4 Metrics\nWe chose ﬁve metrics to evaluate the performance of the\nmodel, including accuracy, f1−score , precision, recall and\nmatthews correlation coef ﬁcient (MCC). Since we are\nsolving a multi-classi ﬁcation problem with unbalanced\ndata for each category, we choose macro precision, macro\nrecall and macro f1−score . These metrics have different\nfocuses. Accuracy focuses on assessing the model ’sa b i l i t y\nto correctly classify samples, while macro f1−score focuses on\nassessing the sensitivity of the model. Macro precision\naddresses the question of how many of the samples that\nthe model predicts as positive classes are predicted correctly,\nmacro recall addresses the question of how many of the\nsamples that the model predicts out of all positive classes.\nMCC focuses on the prediction of model classi ﬁcation\nperformance in unbalanced datasets. We calculate\naccuracy, macro precision, macro recall, MCC and macro\nf\n1−score respectively by the following equations. Where TP,\nFP, FN and TN are short for the true positives, the false\npositives, the false negatives and the true negatives\nrespectively ( Shao et al., 2021 ). TP is a positive sample\npredicted by the model as a positive class. TN is the\nnegative sample predicted as the negative class. FP is the\nnegative sample predicted as positive class. FN is the positive\nsample predicted as the negative class.\nAccuracy /equals\nTP + TN\nTP + TN + FP + FN (10)\nMacro Precision/equals 1\nl ∑\nl\ni/equals 1\nTPi\nTPi + FPi\n(11)\nMacro Recall/equals 1\nl ∑\nl\ni/equals 1\nTPi\nTPi + FNi\n(12)\nMCC /equals TP*TN − FP*FN/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext\n(TP + FP)*(TP + FN)*(TN + FP)*(TN + FN)\n√\n(13)\nMacro F1 /equals 2× Macro Precision× Macro Recall\nMacro Precision+ Macro Recall (14)\n3 Results\n3.1 Performance comparison of\nTransCluster with other methods on\nexternal test datasets\nWe compared the results of TransCluster with ﬁve\nbaseline methods on tissues of lung, kidney and blood in\nShao dataset (Shao et al., 2021). The ﬁve baseline methods\nincluded scDeepSort (Shao et al., 2021), SingleR (Aran et al.,\n2019), ACTINN (Ma and Pellegrini, 2020), singleCellNet (Tan\nand Cahan, 2019) and scMap_cell (Kiselev et al., 2018). In the\ndatasets of different tissues, all cells from a speciﬁct i s s u ei n\nthe Shao dataset were selected as the training set, and test cells\nf r o mt h es a m et i s s u e sw e r eu s e df o rc e l lt y p ep r e d i c t i o n .T h e\nprocessing of the datasets is described in detail in the\nMaterials and Methods section. The experimental results of\nthe baseline approach (Shao et al., 2021) are taken from the\nreferences, and the trainin g and test sets used for all\nexperiments are identical. The ﬁnal results are shown in\nTable 1.\nGenerally, from Table 1, it can be seen that TransCluster\ncan predict cell classes in external test datasets after training in\nthe training set, and the accuracy (ACC), f\n1−score and\nmatthews correlation coef ﬁcient (MCC) are higher than\nother models. As shown in Table 1, in the lung, blood and\nkidney datasets, the best performance is found in the blood\ndataset with an ACC of 0.9429,f\n1−score of 0.8224, and MCC of\n0.9050. In comparison, the performance in the lung dataset is\npoorer with an ACC of 0.7637,f1−score of 0.5942, and MCC of\n0.6545, due to the fact that the lung dataset has more cell types\nand is more difﬁcult to perform cell class identiﬁcation. This is\nsufﬁcient to demonstrate that our proposed model trained on\na cellular dataset of a speciﬁct i s s u ea n dp r e d i c t e dc e l lt y p eo n\nanother dataset of the same tissue. Since the training and test\nsets belong to different datasets of the same tissue, the gene\nclasses of both are somewhat different and the results of\nfeature learning are poor, resulting in generally a slightly\nlower accuracy than that of experiments performed on the\nsame dataset.\n3.2 Performance of TransCluster on\ndifferent tissues\nTo demonstrate the universality of TransCluster, we used\ndifferent data of human tissues from the Shao dataset (Shao\net al., 2021) to measure the prediction results. We split each\ndataset into an 8:2 of the training set and test set, and take the\naverage of ﬁv ee x p e r i m e n t sa st h eﬁnal result. Part of the\nexperimental results are shown in Figure 2.I nw h i c h ,t r a i n\nFrontiers inGenetics frontiersin.org05\nSong et al. 10.3389/fgene.2022.1038919\ndenotes the amount of data in the training set and test denotes\nthe amount of data in the test set. As shown inFigure 2,w ec a n\neasily ﬁnd that the model has the highest ACC and MCC in the\nPleura dataset with 0.9782 a nd 0.9595, respectively. The\nlowest ACC and MCC were obtained in the fat dataset with\n0.8051 and 0.7684. The reason for this situation may be that\nt h ed a t av o l u m eo ft h eA d i p o s ed a t a s e ti st o os m a l l ,r e s u l t i n g\nin incomplete feature learning, while the Pleura dataset has a\nlarge enough data volume and a relatively small number of cell\nclasses. Overall, it seems that TransCluster achieves more than\n80% accuracy in several different human tissue datasets, which\ndemonstrates the applicability of our model. Meanwhile, the\nhigher values of ACC and MCC and the generally lower values\nof precision and recall are due to the very unequal distribution\nof the categories in the used dataset. For multi-categorization,\nACC and MCC are more convincing indicators, and the\nperformance of ACC and MCC is sufﬁcient to illustrate the\ngoodness of our model.\n3.3 Sensitivity analysis\n3.3.1 Ablation experiments\nFor choosing the hyperparameters that make our model own\nthe best performance, we did some sensitivity analysis\nexperiments on the Baron dataset (Shao et al., 2021). First, in\nTable 2 , we discuss the variation of various performance\nparameters of the model with or without decoders at different\nnumbers of attentional heads. Meanwhile, we discuss the\nperformance of the model under different dimensionality\nreduction, and the experimental results are shown inFigure 3.\nIt is found that the performance of our model has very small\nﬂuctuation as the number of heads increases inTable 2. This\nmeans that for our model, too many attention heads have not\nmade the model work better. The model performs well in both\ncases with or without the decoder part. And in both cases, the\nbest result is achieved when head equalsﬁve. The highest ACC,\nf\n1−score, precision, recall and MCC of TransCluster with decoder\nTABLE 1 Performance comparison of TransCluster with existing methods on different datasets. The bolded part is the best performance case.\nTissue\nmodel\nLung Blood Kidney\nACC F1−score MCC ACC F1−score MCC ACC F1−score MCC\nTransCluster 0.7637 0.5942 0.6545 0.9429 0.8224 0.9050 0.9274 0.6804 0.8512\nscDeepSort 0.6622 0.5921 0.5990 0.9283 0.7993 0.7855 0.9173 0.6044 0.5402\nSingleR 0.6150 0.5905 0.5923 0.6128 0.5135 0.4902 0.3155 0.3730 0.2934\nACTINN 0.7346 0.5763 0.5809 0.8327 0.6074 0.5911 0.7682 0.5536 0.4264\nsingleCellNet 0.7032 0.5115 0.4983 0.9152 0.8082 0.7812 0.7200 0.5203 0.3848\nscMap_cell 0.3428 0.0424 0.2448 0.6115 0.3323 0.2899 0.0093 0 0.0348\nFIGURE 2\nPerformance of TransCluster on different tissues.\nFrontiers inGenetics frontiersin.org06\nSong et al. 10.3389/fgene.2022.1038919\nare 0.9370, 0.7324, 0.7714, 0.7156 and 0.9223, respectively. The\nhighest ACC,f1−score, precision, recall and MCC of TransCluster\nwithout decoder are 0.9354, 0.7855, 0.8001, 0.7817 and 0.9201.\nTherefore, our model is chosen to have the decoder part and the\nnumber of attention heads is chosen to beﬁve.\nFigure 3shows the variation of ACC,f1−score, precision, recall\nand MCC of the model with different number of dimensionality\nreduction, and k in the ﬁgure indicates the number of\ndimensionality reduction. FromFigure 3, It can be found that\nthe accuracy of the model generally shows an increasing trend as\nthe number of dimensionality reduction increases. Taking the\nBaron dataset (Shao et al., 2021) as an example, the accuracy of\nthe model is highest when using LDA dimensionality reduction\nand keeping the number of features as 13, i.e., the number of cell\ncategories in the dataset minus 1. This pattern was also found by\nexperiments on other datasets, so the number of downscaled\nretained features was chosen by subtracting 1 from the number of\ncell categories.\n3.3.2 Availability of the main part of the\nTransCluster\nThe model includes a linear discriminant analysis (LDA)\ndimensionality reduction part, which is placed after dividing the\ndataset. In order to determine the location of the LDA, we\nexperimented with the method of dimensionality reduction\nbefore dividing the dataset. Since the dimensionality reduction\nbetter represents the features of different cell classes, as shown in\nFigure 4, taking the liver dataset as an example, the performance\nof the model is better to reduce the dimension of the whole\ndataset and then split it than to reverse the order of two\noperations. This is because the LDA dimensionality reduction\nTABLE 2 Performance of the model with or without the decoder section under different number of attention heads on Baron dataset. The bolded part\nis the best performance case.\nHead-num TransCluster with decoder TransCluster without decoder\nACC F1−score Precision Recall MCC ACC F1−score Precision Recall MCC\n1 0.9105 0.7110 0.7217 0.7058 0.9007 0.9292 0.6787 0.7260 0.6655 0.9157\n2 0.9152 0.6582 0.6658 0.6544 0.8975 0.9183 0.7172 0.8372 0.7037 0.8978\n3 0.9012 0.6328 0.6443 0.6351 0.8647 0.9354 0.7182 0.7408 0.7098 0.9189\n4 0.9144 0.6740 0.6920 0.6673 0.9012 0.9191 0.6168 0.6789 0.6138 0.8896\n5 0.9370 0.7324 0.7714 0.7156 0.9223 0.9354 0.7855 0.8001 0.7817 0.9201\n6 0.9268 0.7405 0.7661 0.7262 0.9056 0.9230 0.7345 0.7717 0.7146 0.8989\n7 0.9307 0.6519 0.6573 0.6565 0.9136 0.9331 0.6907 0.7305 0.6854 0.9028\n8 0.9160 0.6947 0.7218 0.6821 0.8549 0.9230 0.6734 0.6914 0.6637 0.8974\n9 0.9160 0.6694 0.6882 0.6632 0.8561 0.9245 0.7192 0.7864 0.6873 0.8987\n10 0.9160 0.6619 0.6888 0.6485 0.8354 0.9261 0.7307 0.7647 0.7252 0.9072\nFIGURE 3\nThe effect of dimensionality reduction on the model.\nFIGURE 4\nLDA’s position experiment.\nFrontiers inGenetics frontiersin.org07\nSong et al. 10.3389/fgene.2022.1038919\nprocess uses cell labels as a reference, which makes its selection of\nthe main features more accurate. However, the actual cells to be\npredicted have no prior knowledge of the labels, therefore, it is\nmore reasonable to choose the way of dividing the datasetﬁrst.\nThis experiment is sufﬁcient to justify the LDA location and also\nreveals the usability of the main part of the model.\n3.3.3 Visual analysis\nT-distributed stochastic neighbor embedding (t-SNE)\n(Maaten and Hinton, 2008) is a machine learning algorithm\nused for dimensionality reduction, which can visualize high-\ndimensional data, so that we have an intuitive understanding of\nthe distribution of data. As shown inFigure 5, we visualize the\nprediction results of the model by the t-SNE method in order to\ndiscover the testing effect of the model more intuitively.\nIn Figure 5, the visualization of real cell classes is shown on\nthe left, and the distribution of cell classes predicted by\nTransCluster is shown on the right (taking the optimal\nprediction result of Spleen dataset as an example). We can\nknow that the vast majority of cells are predicted accurately\nexcept for very few cells.\n3.3.4 Confusion matrix\nThe confusion matrix is a summary of the predictions for a\nclassiﬁcation problem. The number of correct and incorrect\npredictions is summarized using count values and broken\ndown by each category, which is the key to the confusion\nmatrix (Görtler et al., 2022 ). The confusion matrix shows\nwhich part of the classi ﬁcation model is confused when\nmaking predictions, providing insight not only into the errors\nmade by the classiﬁcation model, but more importantly, the types\nof errors that occur, overcoming the limitations associated with\nusing classiﬁcation accuracy alone (Li et al., 2022). As shown in\nFigure 6, we show the confusion matrix of the classiﬁcation\nresults for the kidney dataset.\nAs can be seen fromFigure 6,f o rt h ek i d n e yd a t a s e t ,o ft h e\n740 predicted data for the 7 cell categories, the category that\ncould all be accurately predicted is B cell, 10.71% of Dendritic\ncells and 2.19% of Endothelial cells are incorrectly\ncategorized as Loop of Henle. 94.96% of Epithelial cells are\nFIGURE 5\nVisualization results.\nFIGURE 6\nConfusion matrix for the classiﬁcation results of the Kidney\ndataset.\nFrontiers inGenetics frontiersin.org08\nSong et al. 10.3389/fgene.2022.1038919\naccurately predicted, the probability of being incorrectly\npredicted as Dendritic cells, Endothelial cells, Intercalated\ncells are all 0.84%, the probability of being predicted as Loop\nof Henle is 2.52%. The accuracy of prediction of Loop of\nHenle is 97.85%, and that of Endothelial cell is 96.97%. 3.30%\nof Intercalated cells are incorrectly classi ﬁed as Loop of\nHenle. thus, the prediction accuracy of B cell is the\nhighest, and Loop of Henle caused the most interference to\nthe model prediction.\n4 Discussion\nIn this study, we proposed a single-cell category prediction\nmodel, TransCluster, which adopts a unique dimensionality\nreduction approach and feature extraction method. Unlike\nother methods, TransCluster begins with gene expression\nmatrix processing by LDA’s dimensionality reduction method\nto ensure that the features being learned are more targeted. At the\nsame time, the number of parameters is greatly reduced, making\nthe model run much faster than other baseline methods. The\nmodiﬁed Transformer is used for feature information extraction,\nwhich makes the extracted features closer to the target data and\nmore effective than other methods.\nTo our knowledge, which is the ﬁrst application of the\nTransformer module to the ﬁeld of single-cell category\nprediction. Extensive experiments in the human scRNA-seq\ndataset have shown that our model is able to accurately\npredict the majority of cells in multiple human tissues.\nComparison with other models reveals that our model can\nachieve state-of-the-art prediction performance, which\ndemonstrates the feasibility of the Transformer module in cell\nclassiﬁcation tasks.\nThere are some aspects of our approach that could be\nimproved in the future. Due to the rapid development of\ngraph neural networks ( Wu et al., 2021 ), models with\nconstructed cellular relationship graphs are starting to emerge\nin theﬁeld of cell type prediction. We will use graphs to improve\nthe cell type identiﬁcation pipeline. We expect that over time,\nmore cell types from larger maps should be used to train more\ncomprehensive neural networks. In the future, we will apply\nsingle-cell datasets containing more data information to single-\ncell category prediction.\nData availability statement\nThe source code is available at https://github.com/\nDanica123/TransCluster.git and all datasets are publicly\navailable at https://github.com/Danica123/TransCluster/\nreleases/tag/Dataset.\nAuthor contributions\nConceptualization, TS and SW; methodology, HD; software,\nGW and HD; validation, XZ, YZ and LJ; investigation, HD;\nresources, HD; data curation, HD and LJ; writing— original draft\npreparation, HD; writing— review and editing, SW; visualization,\nYZ; supervision, TS and SW. All authors have read and agreed to\nthe published version of the manuscript.\nFunding\nThis work was supported by National Key Research and\nDevelopment Project of China (2021YFA1000102,\n2021YFA1000103), National Natural Science Foundation of\nChina (Grant Nos. 61873280, 61972416, 62272479, 62202498),\nTaishan Scholarship (tsqn201812029), Foundation of Science\nand Technology Development of Jinan (201907116),\nShandong Provincial Natural Science Foundation\n(ZR2021QF023), Fundamental Research Funds for the Central\nUniversities (21CX06018A), Spanish project PID 2019-\n106960GB-I00, Juan de la Cierva IJC 2018-038539-I.\nAcknowledgments\nWe thank our partners who provided all the help during the\nresearch process and the team for their great support.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nFrontiers inGenetics frontiersin.org09\nSong et al. 10.3389/fgene.2022.1038919\nReferences\nAran, D., Looney, A. P., Liu, L., Wu, E., Fong, V., Hsu, A., et al. (2019). Reference-\nbased analysis of lung single-cell sequencing reveals a transitional proﬁbrotic\nmacrophage. Nat. Immunol. 20 (2), 163–172. doi:10.1038/s41590-018-0276-y\nBaron, M., Veres, A., Wolock, S. L., Faust, A. L., Gaujoux, R., Vetere, A., et al.\n(2016). A single-cell transcriptomic map of the human and mouse pancreas reveals\ninter- and intra-cell population structure.Cell Syst. 3 (4), 346–360. doi:10.1016/j.\ncels.2016.08.011\nButler, A., Hoffman, P., Smibert, P., Papalexi, E., and Satija, R. (2018). Integrating\nsingle-cell transcriptomic data across different conditions, technologies, and\nspecies. Nat. Biotechnol. 36 (5), 411–420. doi:10.1038/nbt.4096\nGibney, E. (2015). DeepMind algorithm beats people at classic video games.\nNature 518 (7540), 465–466. doi:10.1038/518465a\nGörtler, J., Hohman, F., Moritz, D., Wongsuphasawat, K., Ren, D., Nair, R., et al.\n(2022). “Neo: Generalizing confusion matrix visualization to hierarchical and\nmulti-output labels,”in CHI conference on human factors in computing systems.\nGuo, J., Sun, Y., Gao, J., Hu, Y., and Yin, B. (2020). Robust adaptive linear\ndiscriminant analysis with bidirectional reconstruction constraint. ACM Trans.\nKnowl. Discov. Data14 (6), 1–20. doi:10.1145/3409478\nGuo, M., Wang, H., Potter, S. S., Whitsett, J. A., and Xu, Y. (2015). Sincera: A\npipeline for single-cell RNA-seq proﬁling analysis. PLoS Comput. Biol. 11 (11),\ne1004575. doi:10.1371/journal.pcbi.1004575\nHan, X., Zhou, Z., Fei, L., Sun, H., Wang, R., Chen, Y., et al. (2020). Construction\nof a human cell landscape at single-cell level.Nature 581 (7808), 303–309. doi:10.\n1038/s41586-020-2157-4\nHaque, A., Engel, J., Teichmann, S. A., and Lonnberg, T. (2017). A practical guide\nto single-cell RNA-sequencing for biomedical research and clinical applications.\nGenome Med. 9 (1), 75. doi:10.1186/s13073-017-0467-4\nHastie, T., Tibshirani, R., and Friedman, J. H. (2009).The elements of statistical\nlearning : Data mining, inference, and prediction. New York, NY: Springer.\nHochreiter, S. (1998). The vanishing gradient problem during learning recurrent\nneural nets and problem solutions.Int. J. Unc. Fuzz. Knowl. Based. Syst.6 (2),\n107–116. doi:10.1142/s0218488598000094\nH u ,J . ,L i ,X . ,H u ,G . ,L y u ,Y . ,S u s z t a k ,K . ,a n dL i ,M .( 2 0 2 0 ) .I t e r a t i v et r a n s f e rl e a r n i n g\nwith neural network for clustering and cell type classiﬁcation in single-cell RNA-seq\nanalysis.Nat. Mach. Intell.2 (10), 607–618. doi:10.1038/s42256-020-00233-7\nKiselev, V. Y., Kirschner, K., Schaub, M. T., Andrews, T., Yiu, A., Chandra, T.,\net al. (2017). SC3: Consensus clustering of single-cell RNA-seq data.Nat. Methods\n14 (5), 483–486. doi:10.1038/nmeth.4236\nK i s e l e v ,V .Y . ,Y i u ,A . ,a n dH e m b e r g ,M .( 2 0 1 8 ) .s c m a p :p r o j e c t i o no fs i n g l e - c e l lR N A -\nseq data across data sets.Nat. Methods15 (5), 359–362. doi:10.1038/nmeth.4644\nLahnemann, D., Koster, J., Szczurek, E., McCarthy, D. J., Hicks, S. C., Robinson,\nM. D., et al. (2020). Eleven grand challenges in single-cell data science.Genome Biol.\n21 (1), 31. doi:10.1186/s13059-020-1926-6\nLi, X., Han, P., Wang, G., Chen, W., Wang, S., and Song, T. (2022). SDNN-PPI:\nSelf-attention with deep neural network effect on protein-protein interaction\nprediction. BMC Genomics 23 (1), 474. doi:10.1186/s12864-022-08687-2\nLv, H., Dao, F. Y., Zhang, D., Guan, Z. X., Yang, H., Su, W., et al. (2020). iDNA-\nMS: An integrated computational tool for detecting DNA modiﬁcation sites in\nmultiple genomes. iScience 23 (4), 100991. doi:10.1016/j.isci.2020.100991\nMa, F., and Pellegrini, M. (2020). Actinn: Automated identiﬁcation of cell types in\nsingle cell RNA sequencing. Bioinformatics 36 (2), 533 –538. doi:10.1093/\nbioinformatics/btz592\nMaaten, L. v. d., and Hinton, G. (2008). Visualizing Data using t-SNE.J. Mach.\nLearn. Res. 9, 2579–2605.\nMacosko, E. Z., Basu, A., Satija, R., Nemesh, J., Shekhar, K., Goldman, M., et al.\n(2015). Highly parallel genome-wide expression proﬁling of individual cells using\nnanoliter droplets. Cell 161 (5), 1202–1214. doi:10.1016/j.cell.2015.05.002\nPang, S., Zhang, Y., Song, T., Zhang, X., Wang, X., and Rodriguez-Paton, A.\n(2022). Amde: A novel attention-mechanism-based multidimensional feature\nencoder for drug-drug interaction prediction.Brief. Bioinform. 23 (1), bbab545.\ndoi:10.1093/bib/bbab545\nPotter, S. S. (2018). Single-cell RNA sequencing for the study of development,\nphysiology and disease. Nat. Rev. Nephrol. 14 (8), 479–492. doi:10.1038/s41581-\n018-0021-7\nQian, S., Liu, H., Liu, C., Wu, S., and Wong, H. S. (2018). Adaptive activation\nfunctions in convolutional neural networks.Neurocomputing 272, 204–212. doi:10.\n1016/j.neucom.2017.06.070\nRegev, A., Teichmann, S. A., Lander, E. S., Amit, I., Benoist, C., Birney, E., et al.\n(2017). The human cell atlas.Elife 6, e27041. doi:10.7554/eLife.27041\nShao, X., Liao, J., Lu, X., Xue, R., Ai, N., and Fan, X. (2020). scCATCH: Automatic\nannotation on cell types of clusters from single-cell RNA sequencing data.iScience\n23 (3), 100882. doi:10.1016/j.isci.2020.100882\nShao, X., Yang, H., Zhuang, X., Liao, J., Yang, P., Cheng, J., et al. (2021).\nscDeepSort: a pre-trained cell-type annotation method for single-cell\ntranscriptomics using deep learning with a weighted graph neural network.\nNucleic Acids Res.49 (21), e122. doi:10.1093/nar/gkab775\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G.,\net al. (2016). Mastering the game of Go with deep neural networks and tree search.\nNature 529 (7587), 484–489. doi:10.1038/nature16961\nSong, T., Wang, G., Ding, M., Rodriguez-Paton, A., Wang, X., and Wang, S.\n(2022). Network-based approaches for drug repositioning. Mol. Inf. 41 (5),\ne2100200. doi:10.1002/minf.202100200\nSu, Y., Liu, C., Niu, Y., Cheng, F., and Zhang, X. (2021). A community structure\nenhancement-based community detection algorithm for complex networks.IEEE\nTrans. Syst. Man. Cybern. Syst.51 (5), 2833–2846. doi:10.1109/tsmc.2019.2917215\nSun, J., Xie, J., and Zhou, H. (2021).“EEG classiﬁcation with transformer-based\nmodels,” in 2021 IEEE 3rd global conference on life sciences and technologies\nLifeTech. 09-11 March 2021. Nara, Japan.\nTan, Y., and Cahan, P. (2019). SingleCellNet: A computational tool to classify\nsingle cell RNA-seq data across platforms and across species. Cell Syst. 9( 2 ) ,\n207–213. doi:10.1016/j.cels.2019.06.004\nTian, Y., Su, X., Su, Y., and Zhang, X. (2021). Emodmi: A multi-objective\noptimization based method to identify disease modules. IEEE Trans. Emerg.\nTop. Comput. Intell.5 (4), 570\n–582. doi:10.1109/tetci.2020.3014923\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need,– in Proceedings of the Advances in Neural\nInformation Processing Systems 30 (NIPS 2017), Long Beach, CA, 5999–6009.\nWainberg, M., Merico, D., Delong, A., and Frey, B. J. (2018). Deep learning in\nbiomedicine. Nat. Biotechnol. 36 (9), 829–838. doi:10.1038/nbt.4233\nWang, B., Zhu, J., Pierson, E., Ramazzotti, D., and Batzoglou, S. (2017).\nVisualization and analysis of single-cell RNA-seq data by kernel-based similarity\nlearning. Nat. Methods 14 (4), 414–416. doi:10.1038/nmeth.4207\nWang, H., Zhao, J., Su, Y., and Zheng, C. H. (2021). scCDG: A Method based on\nDAE and GCN for scRNA-seq data Analysis.IEEE/ACM Trans. Comput. Biol.\nBioinform., early access., 1. doi:10.1109/TCBB.2021.3126641\nWang, S., Song, T., Zhang, S., Jiang, M., Wei, Z., and Li, Z. (2022). Molecular\nsubstructure tree generative model for de novo drug design.Brief. Bioinform.23 (2),\nbbab592. doi:10.1093/bib/bbab592\nWolf, F. A., Angerer, P., and Theis, F. J. (2018). Scanpy: Large-scale single-cell gene\nexpression data analysis.Genome Biol.19 (1), 15. doi:10.1186/s13059-017-1382-0\nWu, Q. W., Cao, R. F., Xia, J., Ni, J. C., Zheng, C. H., and Su, Y. (2021). Extra trees\nmethod for predicting LncRNA-disease association based on multi-layer graph\nembedding aggregation.IEEE/ACM Trans. Comput. Biol. Bioinform., early access.\ndoi:10.1109/TCBB.2021.3113122\nXu, C., and Su, Z. (2015). Identi ﬁcation of cell types from single-cell\ntranscriptomes using a novel clustering method. Bioinformatics 31 (12),\n1974–1980. doi:10.1093/bioinformatics/btv088\nXu, L., Raitoharju, J., Iosiﬁdis, A., and Gabbouj, M. (2021). Saliency-based\nmultilabel linear discriminant analysis. IEEE Trans Cybern . 52. 10200-10213.\ndoi:10.1109/TCYB.2021.3069338\nZeisel, A., Munoz-Manchado, A. B., Codeluppi, S., Lonnerberg, P., La Manno, G.,\nJureus, A., et al. (2015). Brain structure. Cell types in the mouse cortex and\nhippocampus revealed by single-cell RNA-seq. Science 347 (6226), 1138–1142.\ndoi:10.1126/science.aaa1934\nZhang, X., Peng, X., Han, C., Zhu, W., Wei, L., Zhang, Y., et al. (2019). A uniﬁed\ndeep-learning network to accurately segment insulin granules of different animal\nmodels imaged under different electron microscopy methodologies.Protein Cell10\n(4), 306–311. doi:10.1007/s13238-018-0575-y\nFrontiers inGenetics frontiersin.org10\nSong et al. 10.3389/fgene.2022.1038919",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7171030044555664
    },
    {
      "name": "Cluster analysis",
      "score": 0.596254289150238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5720981359481812
    },
    {
      "name": "Profiling (computer programming)",
      "score": 0.5166993141174316
    },
    {
      "name": "Cell type",
      "score": 0.483561247587204
    },
    {
      "name": "Annotation",
      "score": 0.45685014128685
    },
    {
      "name": "RNA-Seq",
      "score": 0.4493669271469116
    },
    {
      "name": "Data mining",
      "score": 0.41373229026794434
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4127030670642853
    },
    {
      "name": "Transcriptome",
      "score": 0.38924723863601685
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3786129951477051
    },
    {
      "name": "Cell",
      "score": 0.3382101058959961
    },
    {
      "name": "Computational biology",
      "score": 0.32796943187713623
    },
    {
      "name": "Biology",
      "score": 0.15519484877586365
    },
    {
      "name": "Gene expression",
      "score": 0.11711400747299194
    },
    {
      "name": "Gene",
      "score": 0.10973694920539856
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210162190",
      "name": "China University of Petroleum, East China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I88060688",
      "name": "Universidad Politécnica de Madrid",
      "country": "ES"
    }
  ]
}