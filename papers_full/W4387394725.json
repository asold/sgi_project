{
  "title": "SceneCraft: Automating Interactive Narrative Scene Generation in Digital Games with Large Language Models",
  "url": "https://openalex.org/W4387394725",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2639121613",
      "name": "Vikram Kumaran",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A2104780850",
      "name": "Jonathan Rowe",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A4209218194",
      "name": "Bradford Mott",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A2128592643",
      "name": "James Lester",
      "affiliations": [
        "North Carolina State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093524609",
    "https://openalex.org/W3133006485",
    "https://openalex.org/W2972244901",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W3088515710",
    "https://openalex.org/W4306759153",
    "https://openalex.org/W3164562127",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3212560870",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2621430944",
    "https://openalex.org/W2538192737",
    "https://openalex.org/W4298187623",
    "https://openalex.org/W2984803355",
    "https://openalex.org/W6755316025",
    "https://openalex.org/W4312941580",
    "https://openalex.org/W2115451979",
    "https://openalex.org/W3021347125",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W1561304027",
    "https://openalex.org/W6679152129",
    "https://openalex.org/W6685125084",
    "https://openalex.org/W2565206647",
    "https://openalex.org/W2902539999",
    "https://openalex.org/W2135966470",
    "https://openalex.org/W6851039867",
    "https://openalex.org/W2982451789",
    "https://openalex.org/W2900557941",
    "https://openalex.org/W3170894870",
    "https://openalex.org/W4366547384",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W3103662468",
    "https://openalex.org/W4363651325",
    "https://openalex.org/W2896459572",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2170516265",
    "https://openalex.org/W2997764164",
    "https://openalex.org/W3049275545",
    "https://openalex.org/W2128167024",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4293401335",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3101652466"
  ],
  "abstract": "Creating engaging interactive story-based experiences dynamically responding to individual player choices poses significant challenges for narrative-centered games. Recent advances in pre-trained large language models (LLMs) have the potential to revolutionize procedural content generation for narrative-centered games. Historically, interactive narrative generation has specified pivotal events in the storyline, often utilizing planning-based approaches toward achieving narrative coherence and maintaining the story arc. However, manual authorship is typically used to create detail and variety in non-player character (NPC) interaction to specify and instantiate plot events. This paper proposes SCENECRAFT, a narrative scene generation framework that automates NPC interaction crucial to unfolding plot events. SCENECRAFT interprets natural language instructions about scene objectives, NPC traits, location, and narrative variations. It then employs large language models to generate game scenes aligned with authorial intent. It generates branching conversation paths that adapt to player choices while adhering to the author’s interaction goals. LLMs generate interaction scripts, semantically extract character emotions and gestures to align with the script, and convert dialogues into a game scripting language. The generated script can then be played utilizing an existing narrative-centered game framework. Through empirical evaluation using automated and human assessments, we demonstrate SCENECRAFT’s effectiveness in creating narrative experiences based on creativity, adaptability, and alignment with intended author instructions.",
  "full_text": "SCENE CRAFT : Automating Interactive Narrative Scene Generation\nin Digital Games with Large Language Models\nVikram Kumaran, Jonathan Rowe, Bradford Mott, James Lester\nNorth Carolina State University\n{vkumara, jprowe, bwmott, lester}@ncsu.edu\nAbstract\nCreating engaging interactive story-based experiences dy-\nnamically responding to individual player choices poses sig-\nnificant challenges for narrative-centered games. Recent ad-\nvances in pre-trained large language models (LLMs) have\nthe potential to revolutionize procedural content generation\nfor narrative-centered games. Historically, interactive narra-\ntive generation has specified pivotal events in the storyline,\noften utilizing planning-based approaches toward achieving\nnarrative coherence and maintaining the story arc. However,\nmanual authorship is typically used to create detail and vari-\nety in non-player character (NPC) interaction to specify and\ninstantiate plot events. This paper proposes S CENE CRAFT ,\na narrative scene generation framework that automates NPC\ninteraction crucial to unfolding plot events. SCENE CRAFT in-\nterprets natural language instructions about scene objectives,\nNPC traits, location, and narrative variations. It then employs\nlarge language models to generate game scenes aligned with\nauthorial intent. It generates branching conversation paths\nthat adapt to player choices while adhering to the author’s\ninteraction goals. LLMs generate interaction scripts, seman-\ntically extract character emotions and gestures to align with\nthe script, and convert dialogues into a game scripting lan-\nguage. The generated script can then be played utilizing an\nexisting narrative-centered game framework. Through empir-\nical evaluation using automated and human assessments, we\ndemonstrate SCENE CRAFT ’s effectiveness in creating narra-\ntive experiences based on creativity, adaptability, and align-\nment with intended author instructions.\nIntroduction\nThe challenge of generating compelling, and adaptable nar-\nratives has long been a crucial component of interactive nar-\nrative design (Riedl and Bulitko 2013). While early research\nin narrative generation primarily focused on creating coher-\nent sequences of events, this approach often fails to produce\ncaptivating stories (Martin et al. 2018; Kreminski, Wardrip-\nFruin, and Mateas 2020; Ramirez and Bulitko 2014). In edu-\ncational games, designers strive to support learning by high-\nlighting relevant facts during gameplay, and making learn-\ning an intrinsic part of the story (Naul and Liu 2020). Ad-\nditionally, as players become more familiar with a game,\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ngameplay can become repetitive. Dynamic creation of sto-\nries around specific events becomes important for main-\ntaining player interest. Given these challenges, we propose\nSCENE CRAFT , a framework that leverages large language\nmodels (LLMs) to transform high-level author descriptions\ninto playable episodes within a 3D virtual world, complete\nwith engaging non-player character dialogue, gesture, and\nemotion. This approach simplifies how we create interac-\ntive scenes featuring non-player characters (NPCs), thereby\nstreamlining scene authoring. S CENE CRAFT is driven by\nauthorial intent (Riedl and Bulitko 2013) as a story sum-\nmary that is transformed into a choose-your-own-adventure\nstyle interaction that can play out in a 3D game environ-\nment. We use LLMs to generate narrative scenes for the out-\nline provided by the author and semantically extract from the\ngenerated narrative scene emotions and gestures to display\nprogrammatically during NPC interactions.\nResearch in natural language processing (NLP) focus-\ning on story generation has surged in recent years, address-\ning various aspects, including content control, common-\nsense knowledge use, understanding character actions, and\ncreativity (Alabdulkarim, Li, and Peng 2021). Recent ad-\nvances in LLMs using transformer-based models have out-\nperformed many earlier models in generating short stories\nand dialogues based on human-provided narrative outlines\n(OpenAI 2023; Chowdhery et al. 2022; Bubeck et al. 2023).\nIn addition, LLMs have demonstrated remarkable profi-\nciency in tasks such as extracting semantic information, gen-\nerating code based on abstract guidance, and identifying se-\nmantic features like emotions when given directive prompts,\nall without requiring specialized training (Liu et al. 2023). In\nthis paper, we leverage these specific capabilities of LLMs\nto transform high-level outlines and instructions from an au-\nthor into engaging and dynamic scripts for NPC interactions,\nenriching the immersive experiences within digital games.\nIn a 3D narrative-centered game, the story progresses as\nthe player interacts with NPCs to advance the story arc.\nEach interaction must be engaging and convey the neces-\nsary information to the player to move the narrative for-\nward. We build an end-to-end framework that enables au-\nthors to specify the topic of conversation, key issues to be ad-\ndressed, NPCs’ background, NPCs’ appearance, scene loca-\ntion, and potential story branching variations. From this in-\nput, our framework generates the story and the correspond-\nProceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE2023)\n86\ning dialogues that align with the author’s vision using LLMs.\nThen, we extract emotions and gestures for NPCs corre-\nsponding to each exchange, translating them into a compre-\nhensive custom game script encompassing dialogue, emo-\ntions, gestures, and story branches. For our narrative rep-\nresentation, we use an Ink-like scripting language. The Ink\n(https://www.inklestudios.com/ink) dialogue script is read\nby the StoryLoom engine (Mott et al. 2019), which can read\nthe Ink script and render it as a playable Unity game episode.\nThis script leverages existing game assets to produce an in-\nteractive game episode that can be played in a 3D virtual\nenvironment. The key contributions of SCENE CRAFT narra-\ntive generation framework are the following:\n• Simplifies transforming author natural language instruc-\ntions into playable 3D virtual game episodes without\nmanual intervention.\n• Provides authors the ability to control the generated con-\ntent by allowing authors to specify various aspects such\nas scene objectives (i.e., topics and context), NPC back-\nground and appearance, scene location, and narrative\nvariations.\n• Extracts emotions and gestures from generated story acts\nto improve the realism of NPC interaction, enhancing\nplayer engagement.\nWe demonstrate the effectiveness and strengths of\nSCENE CRAFT through automated assessments and hu-\nman participant evaluations.\nRelated Work\nStory-driven games have long been a subject of interest in\nthe interactive narrative community. A substantial body of\nresearch is dedicated to creating captivating storylines and\nimmersive experiences for players in interactive narrative-\ncentered games with varying degrees of success (Riedl and\nBulitko 2013; Riedl and Young 2010; Kreminski et al. 2020;\nStefnisson and Thue 2018). Interactive narrative-centered\ngames are virtual environments that aim to make the player\nan integral part of an immersive story, where they have\nbounded control over how the narrative proceeds.\nOne way to balance player agency and narrative control\nis to have an experience manager (Riedl and Bulitko 2013)\nthat revises the narrative as events unfold to maintain the\nstoryline. Researchers have conducted multiple studies to\nexplore how experience managers can accommodate player\nactions while ensuring the achievement of authorial goals\nby framing it as an automated planning problem (Ramirez\nand Bulitko 2014; Riedl and Young 2010). By represent-\ning interactive narratives as story graphs (Riedl and Young\n2006), with nodes representing world states and edges repre-\nsenting causal transitions, experience managers can actively\ntrack the narrative’s progress in the graph toward the autho-\nrial goals. Narrative control is sometimes accomplished by\npruning (Ware et al. 2022) and other similar operations on\nthe graph. We use dialogue graphs in our system to track\nbranched interaction. In our generated script, each node of\nthe graph represents a dialogue utterance between the player\nand an NPC and edges link the nodes to the corresponding\nresponse utterance by another character in the scene.\nInstead of top-down control of the narrative, researchers\nhave also designed games where the virtual agents co-create\nby suggesting or executing actions in the environment. In-\nteractions between players and AI agents or NPCs move the\nplot forward. One approach involves sifting the generated\nstoryline to identify compelling event patterns (Kreminski\net al. 2020). Meanwhile, another strategy employs the player\nmodifying NPC goals as a part of gameplay (Oliver and\nMateas 2021). The use of AI as a co-author has typically\nbeen in the mode of making suggestions to assist authors by\npresenting possible actions or new goals based on the cur-\nrent narrative state (Stefnisson and Thue 2018; Akoury et al.\n2020; Kreminski et al. 2022; Martin, Harrison, and Riedl\n2016). Our framework takes a different approach by trans-\nposing the interaction model where the author provides sum-\nmarized intent, and by using LLMs, we generate the scene\ninteraction scripts. In their work, Calderwood et al. (2022)\nfine-tuned a large language model on a data set ofTwine sto-\nries to create a mixed initiative platform for authoring for a\ntext-based story game. In their work, Lin and Riedl (2021)\nutilized high-level author context using language models for\nautomated story generation, our work diverges by employ-\ning the high-level author-provided context to craft interac-\ntive dialogues for individual scenes.\nAdvances in deep neural networks and language models\nhave driven significant progress in the related area of auto-\nmated text-based story generation. In prior versions of these\nlanguage models, there was a significant challenge in ex-\nerting control over the produced content, ensuring the nar-\nrative coherence aligned with common sense, and develop-\ning compelling characters that exhibit consistent behaviors\n(Alabdulkarim, Li, and Peng 2021). Several systems have\nbeen built to provide the neural network with high-level plot\npoints or events. In these models, the neural network fills in\nthe narrative gaps between events to successfully generate\ncoherent and interesting stories (Rashkin et al. 2020; Yao\net al. 2019; Wang, Durrett, and Erk 2020). The problem of\nautomatically generating stories has also been broken down\ninto event generation, followed by event-to-sentence gener-\nation as a two-stage process to improve coherence and plau-\nsibility of generated stories (Ammanabrolu et al. 2020). Re-\ncent advances in pretrained LLMs have shown great promise\nfor use by professional writers as co-authors for screenplays\nand scripts (Mirowski et al. 2023).\nA key aspect of engaging interactive narrative experiences\nis the dialogue with virtual characters, which reinforces in-\ngame events and propels the narrative forward. AI has been\ntypically used as a planner and suggester for the next event\nor action in writing scripts for interactive narrative games\n(Stefnisson and Thue 2018; Akoury et al. 2020; Kreminski\net al. 2022; Martin, Harrison, and Riedl 2016). Martin et\nal. (2016) created an AI tool that aids storytelling in open\nworlds through interactive author feedback based on a story\ngraph. Stefnisson and Thule (2018) offer “Mimisbrunnur”,\nan AI-assisted tool where human authors maintain control\nover the story but receive AI suggestions. Kreminski et al.\n(2022) present “Loose Ends,” a game providing dynamic\nstorytelling prompts for authors. Despite the AI-driven high-\nlevel input, the intricate dialogue and interaction predomi-\n87\nFigure 1: The SCENE CRAFT interactive narrative scene generation framework.\nnantly originate from human authorship, as all tools focus\non supporting human-led story creation. Our research uti-\nlizes LLMs to generate stimulating branching dialogues with\nindividual non-player characters (NPCs) within a high-level\nscene context provided by authors, thereby flipping the usual\nmode of co-authorship and addressing the gap for automat-\ning detailed interaction scenes in interactive narrative gener-\nation systems.\nRecent LLMs, such as OpenAI’s GPT series, have show-\ncased substantial generative potential. These LLMs have\nalso proven their capacity to follow instructions in prompts,\nderive semantic understanding from natural language text,\nand translate instructions into code (OpenAI 2023; Chowd-\nhery et al. 2022; Liu et al. 2023; Bubeck et al. 2023), thus\nproviding a valuable tool for generating content. These ca-\npabilities of LLMs have been used for procedural content\ngeneration (PCG) to create game levels by fine-tuning pre-\ntrained GPT-2 models on the ASCII representation of a puz-\nzle game, Sokoban (Todd et al. 2023). However, these PCG\napproaches only leverage a limited amount of the language\nsemantic knowledge captured by the LLMs as part of pre-\ntraining.\nInteractive Scene Generation Framework\nThe SCENE CRAFT framework, shown in Figure 1, outlines\nkey components of a structured approach for creating en-\ngaging virtual game episodes with NPC interactions derived\nfrom author input through PCG and LLM. The framework\ntakes input from the authors on NPC traits, scene location,\nscene objectives, and possible narrative variations. This in-\nput is fed to an Interactive Dialogue Generator module,\nwhich generates a dialogue script using prompt base instruc-\ntions on an LLM (GPT 3.5). Emote and Gesture to Asset\nMapper takes this dialogue script and pairs the expressed\nemotions and gestures with corresponding entities from our\ngame asset database. The Branching Dialogue Graph Gen-\nerator module combines the dialogue script and any narra-\ntive variations generated to craft a story graph. The Script\nto Game Engine module translates the story graph, mapped\nemotions, and gesture assets into an Ink-like scripting lan-\nguage. The Ink dialogue script is read by the StoryLoom\nengine and rendered as a playable Unity game episode. The\nfollowing sections provide additional details about various\nframework modules.\nInteractive Dialogue Generator\nWe discern the author’s intent by capturing four elements by\npresenting an author intent workbook where the author en-\nters their choices. These elements encompass the character\n(including their background) with whom the player inter-\nacts, the setting or location of the interaction, the scenario,\nthe main topics under discussion, and the tone of the conver-\nsation. The scene location and character are selected from a\ngiven list based on the character and location assets we cur-\nrently support in the game. The author can choose various\ncharacters with different backgrounds. We currently sup-\nport ten characters, including Kim (a female medical doc-\ntor), Quentin (a male chef), and Sam (a young gentleman).\nWe currently support nine locations in a tropical island or\na hospital, including a beach, infirmary, diner, and more.\nWhile the character and scene location is explicitly selected,\nthe author implicitly defines the interaction scenario, cen-\ntral discussion topics, conversation tone, and desired out-\ncome through their natural language story prompts. Addi-\ntionally, the authors can provide alternate prompts that the\ngenerator uses to create alternate branched paths to the con-\nversation. Unlike earlier mixed initiative author-AI interac-\ntions (Stefnisson and Thue 2018; Akoury et al. 2020), where\nAI offers suggestions and authors finalize the narrative, our\nmethod inverts this dynamic by having authors provide sum-\nmary guidance and utilizing large language models (LLMs)\nto generate the dialogue.\nThe generator transforms authorial input into generated\ninteraction dialogues using a predefined prompt template to\nfacilitate the scene-generation process. This prompt instructs\nthe LLM to generate a one-act play set in the specified loca-\ntion featuring the indicated character and discussing the des-\nignated topics. The prompt also directs the LLM to include\n88\nFigure 2: Examples of emote and gesture game assets.\nNarrative Utterance Emote & Gesture Map\n{“dialog”: “We have the\nmeans to do it right here\nin this lab.”,\n{“emote”:“Happy”,\n“scale”:0.7, “gesture”:\n“HandHipEngaged” }\n“emotion”:“Determined”}\nTable 1: Emote and gesture mapping from narrative script to\ngame assets.\ndescriptive details about emotions or gestures. The authors\nare asked to give a story prompt and one or more alternate\nstory prompts. Using LLMs, the Interactive Dialogue Gener-\nator generates multiple dialogue variations for each prompt\nwith utterances, emotions, and gestures (Figure 6). These di-\nalogue scripts are used as input for subsequent modules, dis-\ncussed below.\nThe Interactive Dialogue Generator converts author inputs\ninto interactive dialogues and is a crucial component facili-\ntating authorial control. We evaluate its ability to accomplish\nthis translation through automated and human testing.\nEmote and Gesture to Asset Mapper\nEach NPC utterance is associated with a specific emotion\nand gesture from a library of Unity assets. This collection\nconsists of a set of emotions that game characters can ex-\npress. The intensity of the emotion can be controlled by a\nfactor on a scale from 0 to 1. The prefabricated assets also\nconsist of typical gestures that characters use to emphasize\ntheir statements. The concept of emotion and gesture build-\ning blocks has proven effective in creating realistic NPCs\nin previous research (Thiebaux et al. 2008). In our library\nof assets, we currently support six emotes (happy, sad, an-\ngry, disgusted, surprised, and fearful) and approximately 39\ngestures, such as HandsOnHips, ChinPondering, and Head-\nScratching. Figure 2 illustrates emotions and gestures in our\nasset library. The gestures involve hand and body move-\nments that loop with a pause between loops. The emotions\nare primarily expressed through facial expressions.\nWe employ LLMs as semantic encoders in a zero-shot\nclassification to map the emotes and gestures for each scene\nutterance to the most suitable option in our asset library. This\nmapping is based on the name and description of the asset.\nThe system defaults to a neutral emote, and no gesture is ap-\nplied when the LLM is uncertain about the mapping. Conse-\nquently, each utterance is assigned a corresponding emote\nand gesture asset, which is presented along with the dia-\nlogue allowing for a realistic rendering of the NPC within\nthe game. Table 1 shows an example of mapping from the\nscene script to assigned emotions and gestures. In this case,\nthe feeling of ‘determined’ is mapped to a gesture of Hand-\nHipEngaged and an emotion of ‘Happy’ with an intensity\nscale of 0.7. The third block of Figure 3 shows a character\nElise, displaying emote and gestures corresponding to being\nupset over farm run-off affecting fish health in the river.\nBranched Dialogue Graph Generator\nThe current generation of LLMs has been trained exten-\nsively on computer programs. When prompted appropri-\nately, they have effectively translated natural language de-\nscriptions into code in many prevalent programming lan-\nguages (Chen et al. 2021). We use this capability to represent\nthe scene dialogue generated in the interactive dialogue gen-\nerator as a Python object. The Python representation is a list\nof utterances, where each utterance is a Python dictionary\nwith character, emotion, gesture, and dialogue content. The\nemotion and gesture values in the story graph nodes are from\nthe interactive dialogue generator. We then map these values\nto corresponding game assets with the help of the“Emote\nand Gesture to Asset Mapper” discussed in the prior section.\nDialogue sequences as Python objects are produced individ-\nually for every story prompt and alternate prompt supplied\nby the author. These multiple Python lists are combined to\nform the story graph.\nA typical representation of a branching narrative uses a\nstory graph representation (Riedl and Young 2006). The\nscene is represented as a directed graph, with each dialog\nutterance represented by a node and each arc corresponding\nto the next utterance. A path through the graph represents\nan unfolding conversation between the player and the NPC.\nThe nodes with multiple edges coming out of them indicate\na choice to control the progression of the conversation. Fig-\nure 4 shows an example of a branched dialogue graph. We\ngenerate such a graph from the conversation variants gener-\nated by the Interactive Narrative Generator. All occurrences\nof a character speaking a specific sentence in multiple vari-\nants are represented in a single node. The story graph is used\nas the basis to generate the scene script for StoryLoom.\nScript to Game Engine\nWe employ a rapid prototyping tool based on the StoryLoom\nArchitecture (Mott et al. 2019). This tool utilizes a text-\nbased representation of the interactive narrative to script in-\ngame interactions. The script-to-game engine relies on two\ntypes of assets: game world assets and narrative script as-\nsets. The game world assets are the building blocks phys-\n89\nFigure 3: Screenshots from game episodes generated by SCENE CRAFT .\nically rendered in the game and correspond to entities the\nplayer interacts with. Examples of game world assets are lo-\ncations, characters, and props. Narrative assets consist of di-\nalogue scripts and beat sheets that capture the interactions\nbetween the player and the game world assets. Thedialogue\nscripts represent the conversations, narrations, and branch-\ning choices presented to the player. Thebeat sheet describes\nthe overall story and the key events and triggers when the\nplayer reaches a location, interacts with a prop, or starts a\nconversation with a character. A beat event or trigger rep-\nresents a moment in the game narrative where something\nchanges in the story. The idea is that the player is allowed the\nfreedom to move around in the game, and when they inter-\nact with one of the trigger points specified in the beat sheet\nand if the conditions are met, an interaction is triggered. In\nour StoryLoom, the beat sheet is a spreadsheet identifying\nthe character or prop that triggers a dialogue script, mapping\nto the dialogue script, and some rules regarding the trigger.\nFigure 5 represents a high-level architecture overview of the\nscript to the game engine. The narrative progression during\ngameplay relies on dialogue scripts and beat sheets, which\nserve as the core components of each narrative episode. In\nStoryLoom, the creation of these scripts and beat sheets is a\nproduct of two integrated elements: The first is the ‘Branch-\ning Dialogue Graph Generator’, which crafts a story graph to\nmap out potential dialogue paths. The second is the ‘Emote\nand Gesture to Asset Mapper’, which scans the narrative and\nidentifies the necessary game assets (like characters’ emo-\ntions or gestures) for each line of dialogue. Together, they\ngenerate the StoryLoom dialogue scripts and beat sheets.\nThe dialogue is scripted using a language that extends\nInk (https://www.inklestudios.com/ink), a narrative script-\ning language for games. We have extensively modified the\nInk scripting language. Our modified Ink version preserves\nits original capabilities for representing narrative dialogues\nand branching narratives. Additionally, it allows us to ini-\ntiate Unity game engine-specific actions such as spawn-\ning characters and props, controlling the camera, activat-\ning workbooks, adding conditional triggers on game objects,\nand setting and modifying the game state, among other fea-\ntures. The scene dialogue generated in the framework’s pre-\nvious stages is converted into our custom Ink-like script and\nbeat sheet that can be played in the game engine. During\nthe conversion process, the scene location is established, and\nall characters from the script are moved to their identified\nspawn points, including both author-selected and auxiliary\ncharacters introduced by the generated script. The location,\ncharacters, emotes, and gestures are built on prefabricated\ngame world assets. The conversion process establishes the\nbeat sheet, initiating the conversation when the player ap-\nproaches and interacts with the appropriate character. Fi-\nnally, Ink script choice points and dialog path flow are set\nup. The Ink script is set up such that when a player decides\non a conversation alternative, all the choices made by the\nplayer are remembered in the game state, and the dialogue\nresponds to the choice as the conversation proceeds. The cor-\nresponding emote and gesture are triggered in the Ink script\njust before an NPC renders a dialogue. Figure 3 shows a se-\nquence of screenshots from the generated game episode.\nSCENE CRAFT Implementation\nGiven the author’s prompts, S CENE CRAFT operates as fol-\nlows. Figure 6(a) shows the author’s initial input context ob-\ntained by the framework. It is based on the input provided by\nthe author through the author’s intent workbook. The cap-\ntured details include information about all the NPC (non-\nplayer) characters and the character played by the user, along\nwith information about the location, possible spawn points,\nstory prompt, and possible variants. The framework identi-\nfies the spawn points and scene location as assets available\nin the game. The details from the author’s intent are sub-\nsequently used in LLM prompts that are used to generate a\nscene script realized in the Unity game. Figure 6(b) shows\nthe prompts used in sequence to translate the author’s input\ninto the scene script.\nSCENE CRAFT translates the author’s input into a\ndialogue-driven scene, including dialogue flows for all vari-\nants. It then converts the generated scene script into a Python\nobject that the subsequent framework modules can easily\nconsume. It also uses the LLM to identify specific emo-\ntions and gestures corresponding to each dialogue. Figure\n6(b) shows the emotion prompt; the gestures prompt is simi-\nlar and not shown here due to space constraints. The LLM is\nalso used to address Python compilation with the generated\nPython script object. As the output from the LLM is stochas-\ntic, it may sometimes be necessary to run these requests to\nthe LLM multiple times to generate a valid Python script ob-\nject. The terms in curly brackets on the first prompt in Figure\n6(b): {location}, {scene}, {non\nplayer job} are elements\ncaptured as part of the author intent. In subsequent prompts,\nthe variable {script} is the script generated in the previ-\nous interaction with LLM, and variables like {emote\nlist}\n90\nFigure 4: Example of a generated branched narrative graph.\nare lists extracted from the Python objects created in earlier\nprompts. The prompts are chained together to create a script\nPython object.\nThe Python object is a list of dialogue elements. Each dia-\nlogue element in the script is a Python dictionary containing\nthe dialogue along with the identity of the character who de-\nlivers the dialogue, the emote representing the character’s\nemotional state, and any physical gestures to display during\nthe dialogue. A script can have multiple variants resulting\nin a collection of dialogue lists, one for each variant. The\nPython object is converted to a story graph. The final step is\nthe creation of the game script, which is exported as a script\nwritten for our extended Ink language. The initial sections\nof the Ink script specify the location, spawn points, charac-\nters, and other details to identify all the necessary game el-\nements/assets to initialize. Each NPC with dialogues has an\nInk script file illustrating how different conversation paths\ninteract and how the user’s choices can impact the conversa-\ntion unfolding. The custom extension to Ink is used to indi-\ncate the emotions and gestures of characters at the appropri-\nate time. We use state variables to track the user-chosen path,\nand these state variables determine how their conversation\nbranches and progresses in the scene. StoryLoom performs\nseveral functions: it reads the script, interprets various com-\nmands and narrative paths, and constructs the game scene.\nThe output is a playable game episode.\nEvaluation\nWe assess the generated game episodes using automated\nand human evaluation methods. There are inherent chal-\nlenges associated with automatic evaluation for language\ngenerative models for semantic understanding, coherence,\nand cohesion of long passages, which necessitates including\nhuman evaluation. For automated evaluation, we use three\nhigh-level prompts and generate ten scene scripts for each\nprompt using various paraphrasings. The automated evalua-\ntions are conducted based on this generated dataset.\nAutomated Evaluation\nWe measure the creativity of our system by calculating the\nROUGE-L score between two generated scripts (Lin 2004).\nROUGE-L is a measurement that evaluates the similarity of\nFigure 5: Architecture of script to game engine module.\nsentence structures by identifying the longest sequence of\nmatching n-grams that occur in the same order. We compare\nthe ROUGE-L metric distribution when scripts are generated\nfrom the same prompt versus those from different prompts.\nAlthough we expect higher ROUGE-L scores for scripts de-\nrived from the same prompt, we consider high variation in\nthe distribution of ROUGE-L values as a sign of novelty in\nthe generated script.\nTo assess alignment and adaptability to author input, we\nconvert the prompt and scripts into sentence embeddings us-\ning Sentence-BERT (Reimers and Gurevych 2019). We can\nuse the embedding to overcome the substantial size differ-\nences between prompts and generated scripts. Cosine simi-\nlarity is employed to measure the similarity between these\nembeddings. We then plot the distribution of distances for\nprompt-to-generated script and prompt-to-unrelated script.\nStatistically distinct distributions will indicate that the gen-\nerated scripts are aligned with their corresponding prompt.\nHuman Evaluation\nParticipants for this study were recruited through a purpo-\nsive sampling method, utilizing the researcher’s established\nprofessional and academic network. We recruited 9 partic-\nipants with a broad range of game development expertise\n(ages 25 to 53) to use SCENE CRAFT to generate scenes. The\nparticipants consisted of two graduate students and one re-\nsearch scientist. All three were familiar with video games.\nAdditionally, it consisted of one game software engineer\nand two digital artists familiar with game development. Fi-\nnally, we had one participant who was a math teacher, a soft-\nware engineer, and one who was a scientist in biological\nsciences, all three with minimal exposure to video games.\nEach participant engaged with S CENE CRAFT during a 45\nminute session. The session started with an introduction to\nthe framework along with an explanation of how to pro-\nvide author intent for scene generation. Following the in-\ntroduction, the participant used the author’s intent work-\nbook to record their intent as discussed in an earlier section.\nThe participants waited for a few minutes as S CENE CRAFT\ngenerated the game episode before playing. Once the game\nepisode was generated, the participants played the episode\nand then responded to five questions listed below on the\n91\nFigure 6: (a) Structured author’s input used for all subsequent generations. (b) LLM prompts to generate scene scripts.\nLikert scale from 1 to 7. The participants generate up to 2-3\ngame episodes with different prompts and record their scores\nfor the questions. Each participant’s score for each question\nis average across all their game plays. The data presented is\nthe average of all the participants’ scores.\nThese questions are based on the User Experience Ques-\ntionnaire (UEQ) (Schrepp, Hinderks, and Thomaschewski\n2017), a validated quantitative tool to measure psychome-\ntric properties of a product user experience. The question-\nnaire was created by usability experts in 2005. The UEQ has\nmultiple versions with different sets of questions. We have\nused a subset of questions that apply to our specific scenario\nand map to the six usability measures described by UEQ:\nAttractiveness, Perspicuity, Efficiency, Dependability, Stim-\nulation, and Novelty.\n• How would you rate the ease of generating new narra-\ntives using the tool (Ease of Use)? From ‘Complicated’\nto ‘Easy’. S CENE CRAFT ’s ultimate objective is to sim-\nplify the process of scene generation. By assessing ease\nof use, we can determine if S CENE CRAFT successfully\nlowers the barrier to entry and allows writers and de-\nsigners to generate narratives efficiently and intuitively.\nThis question addresses UEQ measure to gauge how easy\nthe framework is to learn and use (‘Perspicuity’) and if\nthe users can accomplish their tasks efficiently (‘Effi-\nciency’).\n• How engaging and unexpected did you find the gener-\nated game (Creativity)? From ‘Dull’ to ‘Creative’. This\nquestion evaluates SCENE CRAFT ’s ability for originality\nand unexpectedness of the exchanges it creates. Based\non these scores, one can judge the extent to which\nSCENE CRAFT can break from conventional storytelling,\nthereby creating unique player experiences. This ques-\ntion validates that the framework fulfills the ‘Novelty’\ncriterion. It also addresses the fun aspect of the frame-\nwork (‘Stimulation’).\n• To what extent did the interaction in the game accu-\nrately represent and respond to your intent (Adaptabil-\nity)? From ‘Unsatisfactory’ to ‘Satisfactory’. As the au-\nthors specify their intent at the beginning of the process\nto initiate the generation of the scene, this question aims\nto assess the control authors perceive when using the tool.\nThis question validates if the designer feels in control of\nthe interaction (‘Dependability’).\n• How would you evaluate the game’s characters and sto-\nryline in terms of their believability, coherence, and\nengagement (Dependability)? From ‘Not Interesting’\nto ‘Interesting’. S CENE CRAFT , as part of its genera-\ntion, writes dialogues and renders emotion and gestures\nto characters to match the feeling of what the NPC\nsays. This question aims to apprehend if this feature of\nSCENE CRAFT was successful. This question validates\nthat the effects of their suggestions are as reflected in the\ngeneration as expected (‘Dependability’).\n• Overall, how pleased are you with the game generated\nby the tool (Satisfying)? From “Not Satisfied” to “Sat-\nisfied”. This general metric offers a holistic view of\nSCENE CRAFT ’s performance. A satisfying experience\nsuggests SCENE CRAFT meets expectations across all as-\npects of scene generation. This question validates that the\noverall impression of the product is positive (‘Attractive-\nness’).\nResults and Discussion\nWe showcase the ability of our framework to generate novel\ngame interactions aligned with the author’s intent. In the fol-\nlowing sections, we validate these capabilities through auto-\nmated and human evaluation.\nCreativity\nAs described above, we use the ROUGE-L score to measure\nthe similarity between generated scripts. ROUGE-L is par-\nticularly useful as it accounts for the longest common sub-\nsequence between two texts. ROUGE-L is typically used to\ncompare a generated text to a reference text, but we use it to\ncompare two generated texts.\nFigure 7 displays the number of pairs corresponding to\neach ROUGE-L value, comparing scripts that were gener-\nated from identical prompts with those generated from un-\nrelated prompts. Our analysis reveals that, as anticipated,\nthe ROUGE-L scores for generated script pairs originating\n92\nFigure 7: ROUGE-L distribution for generated scripts from\nthe same and unrelated prompts.\nfrom the same prompt were higher than those for the base-\nline, which consisted of script pairs from different prompts.\nThis outcome confirms that scripts generated from the same\nprompts exhibit greater similarity than unrelated prompt\npairs. However, we observed an interesting phenomenon\nwhere several generated scripts from the same prompt were\nas dissimilar as unrelated prompt pairs, as evidenced by the\noverlap in the ROUGE-L score distributions.\nAdditionally, we noted a significantly higher variance in\nthe ROUGE-L distribution for same-prompt script pairs,\nwith the variance being nearly twice that of the baseline\nunrelated-script pair distribution. This finding suggests that\nwhile the generation process tends to produce more similar\nscripts when given the same prompt, it also generates a wide\nrange of scripts with varying degrees of similarity. This vari-\nability in similarity indicates that the script generation capa-\nbilities show significant variability and novelty. We use hu-\nman evaluation, discussed below, to confirm these findings.\nAlignment with Author Intent\nWe evaluate the alignment between the author’s intent and\nthe generated script by examining the semantic similarity\nbetween the prompt and its corresponding generated script.\nGiven the substantial word count discrepancy between the\nprompts and their generated scripts, we chose sentence em-\nbedding similarity over the ROUGE metric for evaluation.\nWe expect a prompt to be most aligned with the script it gen-\nerated and less aligned with unrelated scripts. Figure 8 illus-\ntrates the distribution of cosine similarity scores, indicating\na statistically significant difference (p-value = 2.6e-62) in\nsimilarity distribution between the prompt and its generated\nscript compared to an unrelated script.\nTo further investigate whether specific utterances within\nthe script matched the author’s intent, we evaluated the sim-\nilarity between the author’s input and individual sentences in\nthe script, selecting the maximally similar sentence among\nthem. Figure 9 displays the corresponding similarity distri-\nbutions, revealing a pattern of very high similarity (median\n>0.7) between sentences in the prompt and those in its gen-\nerated script. Interestingly, the baseline comparison with an\nunrelated script exhibits a bimodal distribution. This type\nFigure 8: Cosine similarity between prompt string and gen-\nerated script. Baseline (orange) corresponds to prompt string\ncompared with unrelated script.\nof distribution may arise from the nature of the prompts\nused to generate the evaluation scripts - specifically, two of\nthese prompts relate to sickness on the island (‘fishes falling\nsick’ and ‘pandemic on the island’). These similarities in\ntheme could lead to overlapping sentences in scripts gener-\nated from these disparate prompts.\nOur quantitative analysis clearly aligns the author’s intent\nand the generated script. We corroborate these findings with\nhuman evaluation.\nHuman Evaluation of SCENE CRAFT\nHuman evaluators evaluated the ScreenCraft framework on\nfive questions corresponding to the following metrics: ‘Ease\nof Use’, ‘Creative’, ‘Adaptable’, ‘Dependable’, and ‘Satisfy-\ning’. As described above, 9 participants assessed each met-\nric after generating game episodes based on 2-4 prompts.\nAs shown in Figure 10, S CENE CRAFT received an average\nscore of 6.44 out of a maximum of 7 for ‘Ease of Use,’\ndemonstrating that participants found the framework user-\nfriendly and straightforward for generating game episodes.\nThe standard deviation was 0.53, showing a small spread in\nthe scores ranging from 6 to 7. For the ‘Creative’ metric,\nSCENE CRAFT received a mean score of 5.76. Even though\nthe score is relatively high, there is still potential for im-\nprovement. The standard deviation was 0.80, implying a\nmore diverse range of opinions. Scores ranged from 5 to 7\nfor the S CENE CRAFT adaptability metric, with an average\nof 6.22, indicating the users’ confidence in the reliability\nand stability of the framework. On the ‘Dependable’ met-\nric, S CENE CRAFT scored a high average of 6.06, signify-\ning users found the characters believable, coherent, and en-\ngaging. The standard deviation was 0.77, again indicating a\nlarger spread, with scores ranging from 5 to 7. Lastly, when\nasked to evaluate if the overall experience was satisfactory\nSCENE CRAFT received a high mean score of 6.19, showing\nthat users were satisfied with their overall experience, except\nfor a single outlier of 4.5. The outlier score was for a case\nwhere the participant gave unrelated prompts for the main\nstory and alternates. Overall, it was noted that participants\nwith more extensive game development experience tended\n93\nFigure 9: Maximum cosine similarity between prompt string\nand sentences in the generated script. Baseline (orange) cor-\nresponds to prompt string compared with unrelated script.\nto give lower scores.\nUsers received the S CENE CRAFT framework favorably,\nparticularly concerning ease of use, satisfaction, and adapt-\nability. The high scores for these metrics suggest that\nSCENE CRAFT effectively aids authors in transforming their\nideas into game episodes with minimal effort. While\nSCENE CRAFT received positive feedback on creativity and\ncharacter engagement, there is still room for improvement.\nSingle-sentence story prompts provided by participants were\npossibly limiting the unique context required for LLMs to\nshowcase higher creativity. Future iterations should encour-\nage participants to provide more elaborate prompts.\nLimitations\nWe recognize that certain factors limit the complete realiza-\ntion of our framework’s potential, and there are specific as-\npects we need to address. Our framework is built on publicly\navailable LLM implementations. As with most LLM work, it\nwill be important to investigate how LLM output aligns and\nconforms to the ethical principles of the authors using the\nframework. In this light, future studies should identify ways\nto align these models’ output with standard ethical guide-\nlines. Second, our proposed framework primarily targets au-\ntomating individual scene creation within a larger narrative\nstructure. However, generating a coherent and compelling\nnarrative requires a high-level plot that binds these individ-\nual scenes together. Addressing this limitation will involve\nintegrating a narrative planner and experience manager to\ndrive the higher-level plot.\nConclusion\nAuthoring narrative-centered games that create engaging,\ninteractive story-based experiences that can respond to in-\ndividual player choices has long been a central challenge\nfor intelligent narrative technologies. To address this chal-\nlenge, we introduced S CENE CRAFT , a narrative scene gen-\neration framework that leverages the capabilities of LLMs\nto automate the generation of non-player character interac-\ntions integral to unfolding plot events. By using LLMs to\nextract semantic aspects of the generated script, the in-game\ninteraction reflects the author’s objective through dialogue\nFigure 10: Score distribution for each of the human evalua-\ntion questions.\nutterances, emotes, and gestures. SCENE CRAFT delivers en-\ngaging player experiences with branching interactions in a\n3D virtual environment based on simple author instructions\nwithin a few minutes.\nOur empirical evaluation has demonstrated\nSCENE CRAFT ’s effectiveness in creating engaging narrative\nexperiences that align with authorial intent. The evaluation\nincorporated a two-pronged approach integrating automated\nmetrics and human assessment to appraise S CENE CRAFT ’s\ncapabilities. The automated evaluation using ROUGE-L\nscore and sentence similarity shows that S CENE CRAFT\ncan generate diverse scripts while maintaining alignment\nbetween the author’s intent and narrative episode. The effec-\ntiveness of S CENE CRAFT was also demonstrated through\nhuman evaluation, where the system was favorably received,\nparticularly in terms of ease of use, adaptability, and user\nsatisfaction. These high scores indicate that S CENE CRAFT\neffectively aids authors in turning their ideas into playable\ngame episodes with minimal effort. The more moderate\nscores in creativity suggest room for improvement, which\nwe hope to address in our future work. The empirical\nresults strongly support the potential of S CENE CRAFT as\nan effective tool for automating the generation of interactive\nnarratives in games.\nIn future work, it will be important to build on these find-\nings by investigating extensions to S CENE CRAFT incorpo-\nrating embodied conversational agents, generation of narra-\ntive plots, and narrative events triggered by interactions with\nNPCs in the game. LLMs have shown promise as zero-shot\nplanners (Huang et al. 2022), taking a zero-shot planning\napproach to these problems holds considerable promise for\nthe future. Finally, it will be important to investigate integra-\ntions of S CENE CRAFT with narrative experience managers\nto further enrich the narrative game design process in order\nto create a broad range of player-adaptive narrative games.\nAcknowledgments\nThis work is supported by the National Science Founda-\ntion under award DRL-2112635. Any opinions, findings,\nand conclusions or recommendations expressed in this ma-\nterial are those of the authors and do not necessarily reflect\nthe views of the National Science Foundation.\n94\nReferences\nAkoury, N.; Wang, S.; Whiting, J.; Hood, S.; Peng, N.; and\nIyyer, M. 2020. STORIUM: A Dataset and Evaluation Plat-\nform for Machine-in-the-Loop Story Generation. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, (EMNLP), 6470–6484.\nAlabdulkarim, A.; Li, S.; and Peng, X. 2021. Automatic\nStory Generation: Challenges and Attempts. In Proceedings\nof the Third Workshop on Narrative Understanding, 72–83.\nVirtual: Association for Computational Linguistics.\nAmmanabrolu, P.; Tien, E.; Cheung, W.; Luo, Z.; Ma, W.;\nMartin, L. J.; and Riedl, M. O. 2020. Story realization:\nExpanding plot events into sentences. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34,\n7375–7382.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nCalderwood, A.; Wardrip-Fruin, N.; and Mateas, M. 2022.\nSpinning Coherent Interactive Fiction through Foundation\nModel Prompts. In Proceedings of the 13th International\nConference on Computational Creativity, Bozen-Bolzano,\nItaly, June 27 - July 1, 2022, 44–53. Association for Com-\nputational Creativity (ACC).\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In International Con-\nference on Machine Learning, 9118–9147. PMLR.\nKreminski, M.; Dickinson, M.; Mateas, M.; and Wardrip-\nFruin, N. 2020. Why Are We Like This?: The AI architec-\nture of a co-creative storytelling game. InProceedings of the\n15th International Conference on the Foundations of Digital\nGames, 1–4.\nKreminski, M.; Dickinson, M.; Wardrip-Fruin, N.; and\nMateas, M. 2022. Loose Ends: a mixed-initiative creative\ninterface for playful storytelling. InProceedings of the AAAI\nConference on Artificial Intelligence and Interactive Digital\nEntertainment, volume 18, 120–128.\nKreminski, M.; Wardrip-Fruin, N.; and Mateas, M. 2020.\nToward Example-Driven Program Synthesis of Story Sifting\nPatterns. In AIIDE Workshops.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain: Association for Computational\nLinguistics.\nLin, Z.; and Riedl, M. O. 2021. Plug-and-blend: a frame-\nwork for plug-and-play controllable story generation with\nsketches. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence and Interactive Digital Entertainment, vol-\nume 17, 58–65.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. ACM\nComputing Surveys, 55(9): 1–35.\nMartin, L.; Ammanabrolu, P.; Wang, X.; Hancock, W.;\nSingh, S.; Harrison, B.; and Riedl, M. 2018. Event repre-\nsentations for automated story generation with deep neural\nnets. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 32.\nMartin, L. J.; Harrison, B.; and Riedl, M. O. 2016. Im-\nprovisational computational storytelling in open worlds. In\nInteractive Storytelling: 9th International Conference on\nInteractive Digital Storytelling, ICIDS 2016, Los Angeles,\nCA, USA, November 15–18, 2016, Proceedings 9, 73–84.\nSpringer.\nMirowski, P.; Mathewson, K. W.; Pittman, J.; and Evans,\nR. 2023. Co-Writing Screenplays and Theatre Scripts with\nLanguage Models: Evaluation by Industry Professionals. In\nProceedings of the 2023 CHI Conference on Human Factors\nin Computing Systems, 1–34.\nMott, B. W.; Taylor, R. G.; Lee, S. Y .; Rowe, J. P.; Saleh,\nA.; Glazewski, K. D.; Hmelo-Silver, C. E.; and Lester, J. C.\n2019. Designing and developing interactive narratives for\ncollaborative problem-based learning. In Interactive Story-\ntelling: 12th International Conference on Interactive Dig-\nital Storytelling, ICIDS 2019, Little Cottonwood Canyon,\nUT, USA, November 19–22, 2019, Proceedings 12, 86–100.\nSpringer.\nNaul, E.; and Liu, M. 2020. Why story matters: A review of\nnarrative in serious games. Journal of Educational Comput-\ning Research, 58(3): 687–707.\nOliver, E.; and Mateas, M. 2021. Crosston tavern: modulat-\ning autonomous characters behaviour through player-NPC\nconversation. In Proceedings of the AAAI Conference on\nArtificial Intelligence and Interactive Digital Entertainment,\nvolume 17, 179–186.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nRamirez, A.; and Bulitko, V . 2014. Automated planning and\nplayer modeling for interactive storytelling. IEEE Transac-\ntions on Computational Intelligence and AI in Games, 7(4):\n375–386.\nRashkin, H.; Celikyilmaz, A.; Choi, Y .; and Gao, J. 2020.\nPlotMachines: Outline-Conditioned Generation with Dy-\nnamic Plot State Tracking. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 4274–4295.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT:\nSentence Embeddings using Siamese BERT-Networks. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3982–3992.\n95\nRiedl, M. O.; and Bulitko, V . 2013. Interactive narrative: An\nintelligent systems approach. Ai Magazine, 34(1): 67–67.\nRiedl, M. O.; and Young, R. M. 2006. From linear story gen-\neration to branching story graphs. IEEE Computer Graphics\nand Applications, 26(3): 23–31.\nRiedl, M. O.; and Young, R. M. 2010. Narrative planning:\nBalancing plot and character. Journal of Artificial Intelli-\ngence Research, 39: 217–268.\nSchrepp, M.; Hinderks, A.; and Thomaschewski, J. 2017.\nConstruction of a Benchmark for the User Experience Ques-\ntionnaire (UEQ). International Journal of Interactive Mul-\ntimedia & Artificial Intelligence, 4(4).\nStefnisson, I.; and Thue, D. 2018. Mimisbrunnur: AI-\nassisted authoring for interactive storytelling. In Proceed-\nings of the AAAI Conference on artificial Intelligence and\nInteractive Digital entertainment, volume 14, 236–242.\nThiebaux, M.; Marsella, S.; Marshall, A. N.; and Kallmann,\nM. 2008. Smartbody: Behavior realization for embodied\nconversational agents. In Proceedings of the 7th interna-\ntional joint conference on Autonomous agents and multia-\ngent systems-Volume 1, 151–158.\nTodd, G.; Earle, S.; Nasir, M. U.; Green, M. C.; and To-\ngelius, J. 2023. Level Generation Through Large Language\nModels. In Proceedings of the 18th International Confer-\nence on the Foundations of Digital Games, 1–8.\nWang, S.; Durrett, G.; and Erk, K. 2020. Narrative interpola-\ntion for generating and understanding stories.arXiv preprint\narXiv:2008.07466.\nWare, S. G.; Garcia, E.; Fisher, M.; Shirvani, A.; and Farrell,\nR. 2022. Multi-agent narrative experience management as\nstory graph pruning. IEEE Transactions on Games.\nYao, L.; Peng, N.; Weischedel, R.; Knight, K.; Zhao, D.; and\nYan, R. 2019. Plan-and-write: Towards better automatic sto-\nrytelling. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 33, 7378–7385.\n96",
  "topic": "Narrative",
  "concepts": [
    {
      "name": "Narrative",
      "score": 0.8251545429229736
    },
    {
      "name": "Scripting language",
      "score": 0.7500201463699341
    },
    {
      "name": "Computer science",
      "score": 0.7434587478637695
    },
    {
      "name": "Natural language generation",
      "score": 0.5741546154022217
    },
    {
      "name": "Gesture",
      "score": 0.5263651013374329
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5113622546195984
    },
    {
      "name": "Plot (graphics)",
      "score": 0.4904143512248993
    },
    {
      "name": "Conversation",
      "score": 0.46811503171920776
    },
    {
      "name": "Multimedia",
      "score": 0.4208362102508545
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3385940194129944
    },
    {
      "name": "Natural language",
      "score": 0.28351885080337524
    },
    {
      "name": "Linguistics",
      "score": 0.23073962330818176
    },
    {
      "name": "Communication",
      "score": 0.17744073271751404
    },
    {
      "name": "Psychology",
      "score": 0.1765330731868744
    },
    {
      "name": "Programming language",
      "score": 0.15268760919570923
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I137902535",
      "name": "North Carolina State University",
      "country": "US"
    }
  ]
}