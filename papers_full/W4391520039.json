{
    "title": "Survey of transformers and towards ensemble learning using transformers for natural language processing",
    "url": "https://openalex.org/W4391520039",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A1973987244",
            "name": "Zhang Hong-zhi",
            "affiliations": [
                "Carleton University"
            ]
        },
        {
            "id": "https://openalex.org/A2743107121",
            "name": "Shafiq M. Omair",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1973987244",
            "name": "Zhang Hong-zhi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743107121",
            "name": "Shafiq M. Omair",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2950813464",
        "https://openalex.org/W3043424630",
        "https://openalex.org/W3015851404",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W3096485810",
        "https://openalex.org/W3089573765",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W3105220303",
        "https://openalex.org/W3117559572",
        "https://openalex.org/W2956147019",
        "https://openalex.org/W2979860911",
        "https://openalex.org/W3030030185",
        "https://openalex.org/W3044423116",
        "https://openalex.org/W3014765019",
        "https://openalex.org/W6607658636",
        "https://openalex.org/W3106388706",
        "https://openalex.org/W3105645800",
        "https://openalex.org/W3099950029",
        "https://openalex.org/W2970872015",
        "https://openalex.org/W3037973456",
        "https://openalex.org/W3212246833",
        "https://openalex.org/W3119755710",
        "https://openalex.org/W3129790026",
        "https://openalex.org/W2991568321",
        "https://openalex.org/W3096266342",
        "https://openalex.org/W3162462834",
        "https://openalex.org/W3080175947",
        "https://openalex.org/W4286252375",
        "https://openalex.org/W3163832451",
        "https://openalex.org/W4296806433",
        "https://openalex.org/W2979950223",
        "https://openalex.org/W3036111623",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W3103593657",
        "https://openalex.org/W3098749165"
    ],
    "abstract": null,
    "full_text": "Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nSURVEY\nZhang and Shafiq  Journal of Big Data           (2024) 11:25  \nhttps://doi.org/10.1186/s40537-023-00842-0\nJournal of Big Data\nSurvey of transformers and towards \nensemble learning using transformers \nfor natural language processing\nHongzhi Zhang1 and M. Omair Shafiq1* \nAbstract \nThe transformer model is a famous natural language processing model proposed \nby Google in 2017. Now, with the extensive development of deep learning, many natu-\nral language processing tasks can be solved by deep learning methods. After the BERT \nmodel was proposed, many pre-trained models such as the XLNet model, the RoB-\nERTa model, and the ALBERT model were also proposed in the research community. \nThese models perform very well in various natural language processing tasks. In this \npaper, we describe and compare these well-known models. In addition, we also apply \nseveral types of existing and well-known models which are the BERT model, the XLNet \nmodel, the RoBERTa model, the GPT2 model, and the ALBERT model to different \nexisting and well-known natural language processing tasks, and analyze each model \nbased on their performance. There are a few papers that comprehensively compare \nvarious transformer models. In our paper, we use six types of well-known tasks, such \nas sentiment analysis, question answering, text generation, text summarization, name \nentity recognition, and topic modeling tasks to compare the performance of various \ntransformer models. In addition, using the existing models, we also propose ensemble \nlearning models for the different natural language processing tasks. The results show \nthat our ensemble learning models  perform better than a single classifier on specific \ntasks.\nKeywords: Transformer model, Natural language tasks, Transformer-based model, \nEnsemble learning\n*Correspondence:   \nomair.shafiq@carleton.ca\n1 School of Information \nTechnology, Carleton University, \nOttawa, ON, Canada\nPage 2 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nGraphical Abstract\nIntroduction\nIn this section, we introduce the motivation of this research, the main research ques -\ntions, and the structure of the paper.\nMotivation\nNatural language processing (NLP) stands as a technology that facilitates computer-\nhuman interaction through the medium of natural languages. It encompasses the arti -\nficial processing of human language, empowering computers to not only read but also \ncomprehend it. There are many applications in the field of NLP , covering machine trans-\nlation, speech recognition, grammar analysis, semantics, and pragmatics. The core of \nNLP is to segment text corpora for processing, employing tools like ontology dictionar -\nies, word frequency analytics, and contextual semantic scrutiny to isolate the smallest \nunits of meaning.\nThe key to NLP is to enable natural language communication between humans and \ncomputers, where computers not only grasp the meaning of textual language but also \nexpress intentions and thoughts in a similar manner. This duality is categorized into ’nat-\nural language understanding’ and ’natural language generation, ’ forming the two pillars \nof NLP . However, both domains present formidable challenges. Even in the current the-\noretical and technological environment, creating a high-quality NLP system remains a \nchallenging long-term goal. However, practical systems with substantial NLP capabilities \nhave emerged for specific applications, some even achieving commercial or industrial \nsuccess. Examples include multilingual database interfaces, expert systems, machine \ntranslation platforms, full-text information retrieval systems, and automatic abstract -\ning tools. The resolution of NLP tasks remains a significant global challenge in today’s \ncontext.\nPage 3 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nThe advent of the transformer model in 2017 proposed by Google ushered in a wave \nof transformer-based models, including BERT, XLNet, and RoBERTa. However, there \nhave not been many studies to comprehensively examine how these models perform \nin different NLP tasks. Consequently, this paper undertakes the task of comparing five \ntransformer-based models across six distinct NLP tasks through a series of rigorous \nexperiments. Subsequently, we employ the experimental findings to dissect the differ -\nent performances of these models and delve into the underlying factors contributing to \nthese outcomes. Beyond model comparisons, our objective extends to the integration \nof these models’ strengths through ensemble learning techniques to yield a more robust \nand high-performing collective model.\nResearch questions\nThe following are the research questions of this research: \n1. What is the role of transformer models towards advanced NLP and text analytics?\n2. In different NLP tasks, what are the differences in the performance of different trans-\nformer-based models? And why are there these differences?\n3. Is there a classification method that can combine the advantages of different models?\nContributions\nThis paper has the following main contributions: \n1. We have read and organized papers on NLP tasks in the past few years from the lit -\nerature. In addition, building upon the existing literature, we analyzed the advantages \nof these papers and what needs to be added.\n2. We  use five different  existing and  well-known transformer-based models from the \nliterature to experiment on six types of existing and well-known NLP tasks. We com-\npare the performance of the model based on the experimental results, and analyze \nthe results from the perspective of model structure and training methods.\n3. Through analyzing the advantages and disadvantages of different models on different \ntasks, two ensemble learning models based on the existing models are proposed to \nbe applied to three NLP tasks. Experiments demonstrate that our proposed ensem -\nble learning models outperform a single model based on the transformer model.\nStructure of the paper\nIn “Introduction” section, we study the research questions and motivations. In “Back -\nground” section, we introduced the research background, and described the research \ntask and the model used. In “Review of related works” section, we sorted out the related \nliterature. In “Experimental setup” section, we briefly describe the experimental process \nand experimental conditions. In “Results and discussion ” section, we analyze the results \nand discuss the reasons. In “Using the ensemble learning methods” section, we proposed \nan ensemble learning model and conducted experiments on three natural language \nPage 4 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nprocessing tasks. In “Gap analysis and next steps” section, we analyzed the defects of the \narticle and the next stage of work. In “Conclusions” section we summarize the full text.\nBackground\nWhen it comes to models for solving NLP tasks, many people may think of long \nshort-term memory (LSTM) and other recurrent neural networks (RNNs). But at pre -\nsent, LSTM is becoming less popular in the field of NLP because the parallel comput -\ning power of the LSTM model is poor. In addition to that, the transformer model [1 ] \nproposed by Google in 2017 has a stronger ability to extract features. In the Stanford \nReading Comprehension Dataset (SQuAD2.0) list, the machine’s performance has \nexceeded human performance, which is largely due to the proposal of the pre-training \nmodel BERT which is built based on the encoder of the transformer model. In this \npaper, we review and compare the different transformer models which are the BERT \nmodel, the XLNet model, the RoBERTa model, the GPT2 model, and the ALBERT \nmodel. Attention Is All You Need [1 ] is a paper proposed by Google that takes atten -\ntion to the extreme. This paper [1 ] proposed a new model called transformer. This \nmodel has the same structure as the seq2seq model and is also an encoder–decoder \narchitecture. Such a structure consists of 6 coding blocks and 6 decoding blocks, as \nFig. 1 Structure of the well-known transformer model [1]\nPage 5 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nshown in Fig.  1. The encoder maps the text to the middle layer, so the middle layer has \na vector form with text information. Then the decoder translates the text information \nin the middle layer, and many NLP problems can be solved through this process.\nKey natural language processing tasks\nIn this section, we briefly introduce some of the well-known  NLP tasks  in the \nliterature.\nSentiment analysis\nWith the growth of the internet, people have become more likely to express them -\nselves online. For example, product reviews on e-commerce sites and what people say \non social media about the products and quality of specific brands. These reviews have \na huge impact on the product. For instance, brand companies can promptly respond \nto shifts in public sentiment on social media, especially when negative feedback \nincreases. Sentiment analysis is a core application designed to measure positivity or \nnegativity in text evaluation.\nSentiment analysis is a common task in natural language processing, frequently \nfound in shopping platform reviews. It is the key to product improvement. Through \nthis analysis, businesses gain comprehensive insights into product attributes, enabling \nimprovements across various dimensions.\nQuestion answering\nQuestion Answering Systems (QAS)  [15] represent an advanced evolution of infor -\nmation retrieval systems. They possess the capability to provide accurate and concise \nresponses in natural language to users’ queries, also expressed in natural language.\nWithin the area of natural language processing, the QAS stands as a important \ntopic. Its purpose is to address inquiries posed by individuals in natural language \nform, encompassing a wide array of practical applications. These applications span \nfrom intelligent voice interactions and online customer support to knowledge acqui -\nsition and empathetic chat services. QAS can be categorized into generative and \nretrieval-based systems, single-round and multi-round QAS, and those designed for \nopen-ended or domain-specific contexts.\nTranslation\nIn today’s era of accelerated communication and internet advancements, the expo -\nnential growth of information and increasing global interconnectivity have accen -\ntuated the challenges of language barriers. Consequently, the demand for machine \ntranslation is on the rise. Within the ongoing wave of artificial intelligence, machine \ntranslation theory, technology, and future prospects have attracted high interest.\nMachine translation involves the transformation of grammatical structures to align \nwith the target language, followed by the translation of individual words from the \nsource language to the target language. This process ensures effective cross-lingual \ncommunication in an increasingly interconnected world.\nPage 6 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nText generation\nIn NLP , text generation is an important application area. Keyword extraction and text \nsummarization are all applications in the field of text generation. The main techniques \nof text generation are as follows: synonym-based rewriting method, template-based \nrewriting method, the rewriting method based on a statistical model and semantic \nanalysis generation model, and neural network-based method.\nText summarization\nText summarization tasks revolve around creating a concise and coherent summary that \nretains the essence and core meaning of key information.\nThe procedure of utilizing computers to process extensive textual content to gener -\nate refined and succinct summaries is what defines text summarization. Summaries \nserve as efficient means for individuals to grasp a text’s primary content, enhancing both \ntime savings and reading effectiveness. However, manual summarization is labor-inten -\nsive and time-consuming, making it inadequate to meet the ever-growing information \ndemands. Hence, the emergence of computer-assisted automatic summarization became \nimperative. Automatic summarization primarily employs two methods: Extraction and \nAbstraction. \n1. Extraction: Extraction, as an automatic summarization technique, generates summa-\nries by extracting existing keywords and phrases from the source document. These \nextracted elements form the basis of the summary.\n2. Abstraction: Abstraction, on the other hand, takes a generative approach to auto -\nmatic summarization. It creates summaries by constructing abstract semantic repre -\nsentations and utilizing natural language generation technology to produce coherent \nsummaries.\nNamed entity recognition\nNamed entity recognition (NER) [2] serves as a fundamental tool across various appli -\ncation domains, including information extraction, QAS, and machine translation. In \nessence, NER is tasked with identifying three primary categories of textual entities: enti -\nties, temporal expressions, and numerical values.\nThe NER system excels at extracting these entities from unstructured text. And it can \nbe extended to cover a wide range of entity types, such as product name, model number, \nprice, etc., based on specific business requirements. Therefore, the concept of \"entity\" is \nbroadly defined to include any text fragment that is relevant to a business need, and the \ngoal of NER is to extract these specific entities from the text. In order to achieve this, \nNER systems typically use both rule-based and model-based approaches. \n1. Rule-based methods: Rule-based approaches offer a straightforward means of entity \nextraction. They are particularly effective for entities with distinct contextual cues or \nentities with features defined in the text.\n2. Model-based methods: From a modeling perspective, named entity recognition con -\nstitutes a sequence labeling task. In this case, the input to the model is a sequence \nPage 7 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \ncontaining various elements, such as text and time expressions. The model’s output \nis also a sequence, with each unit in the input sequence assigned a specific label cor -\nresponding to its entity type.\nIn summary, NER plays a key role in the identification and extraction of relevant textual \nentities. It is a generalized approach adapted to specific business needs.\nTopic modeling\nTopic modeling is a key technique for identifying topics, and central concepts in a given \ntext.\nAt its core, topic modeling is a statistical approach that uncovers abstract topics by \nanalyzing a collection of documents. It operates under the premise that if an article \ntouches upon a specific topic, certain distinctive words related to that topic will recur \nfrequently within the text. Typically, an article encompasses multiple topics, each with \nvarying proportions.\nFrom a structural perspective, topic modeling offers a method to reveal underlying \nthemes in textual content. Each topic manifests as a probability distribution over words \nin the vocabulary. This framework, known as a topic model, assumes a generative role. \nIn other words, it assumes that each word within an article arises from a dual probability \nselection process: one for choosing a topic and another for selecting a word from the \ndistribution of that topic.\nIn summary, topic modeling empowers us to uncover the underlying structure of tex -\ntual data by identifying and characterizing distinct themes, providing valuable insights \ninto the content and main ideas contained within.\nMachine learning and deep learning approaches for NLP before transformer\nIn this section, we describe some of the well-known machine learning and deep learning \nmethods for NLP tasks in the literature.\nRule‑based methods\nEstablishing systems for vocabulary analysis, syntactic and semantic interpretation, \nquestion-answering, chatbots, and machine translation predominantly relies on rule-\nbased frameworks. This approach leverages human introspective knowledge, does not \nrequire heavy reliance on data, and facilitates rapid system deployment. However, it is \nnot without its drawbacks, notably in terms of limited robustness and generalization \ncapabilities.\nThe rule-based methodology in NLP often involves abstracting extensive sets of sen -\ntences, tailored for specific human-computer interactions, into rules via grammar pro -\nductions. These rules incorporate key information markers. Subsequently, the system \ncan employ finite state automata generated from these rule sets to convert linguistic \ninput into a parameter sequence. This sequence then guides the corresponding infor -\nmation processing methods. This approach not only enhances the efficiency of natural \nlanguage understanding but also underscores the rule set’s scalability.\nPage 8 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nA NLP system, rooted in grammar rule matching, mainly focuses on the transforma -\ntion of natural language input into machine-understandable parameter data. It achieves \nits functions primarily through three core modules: word segmentation, parameter labe-\nling, and grammar rule matching.\nMethods based on machine learning\nThe concept underlying machine learning-based approaches involves harness -\ning annotated data to construct a machine learning system, predicated on manually \ndefined features. This system employs learning techniques to determine its param -\neters, which are then utilized during runtime for data processing and output gen -\neration. Notable successes in employing statistical methods have been witnessed in \napplications such as machine translation and search engines.\nNLP tasks encompass a multitude of subtasks within its purview. Traditional \nmachine learning methodologies, including support vector machines (SVM), \nMarkov models, conditional random fields (CRF), and others, have been effectively \nemployed to address these subtasks. Nonetheless, practical applications reveal certain \nlimitations: \n1. Dependency on training set quality: Traditional machine learning models heavily rely \non the quality of their training data. Manual labeling of the training set is a requisite, \nwhich can undermine training efficiency.\n2. Field-specific variations: Traditional machine learning models may exhibit varying \nperformance across different domains, weakening their adaptability and underscor -\ning the limitations of a singular learning approach. Creating a training dataset that \nsuits multiple domains requires significant human resources for manual labeling.\n3. Complex language features: When faced with high-level, abstract natural language, \ntraditional machine learning struggles to manually label these intricate language \ncharacteristics. Consequently, it is limited to learning predefined rules, unable to \ncapture nuanced language features beyond established rules.\nMethods based on deep learning\nDeep learning models are increasingly applied to NLP tasks, utilizing architectures \nlike Convolutional Neural Networks (CNNs) and RNNs.\nWhen applying a fully connected network to NLP tasks, several challenges arise: \n1. Variable input length: Different input samples can have varying input and output \nlengths, making it impossible to fix the number of neurons in the input and output \nlayers.\n2. Inability to share features: Features learned from different positions in the input text \ncannot be shared, leading to inefficiencies.\n3. Model complexity: The model tends to have a large number of parameters and \nrequires extensive computations.\nPage 9 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nTo address these issues, RNNs come into play. RNNs scan input data, enabling param -\neter sharing across all time steps. At each time step, they not only receive input from \nthe current moment but also incorporate information from the previous step, allow -\ning past information to influence current decisions effectively.\nTraditional RNNs, however, face limitations. They tend to simply pass along all learned \nknowledge to the next time step without any processing. Consequently, early knowledge \nmay be overwritten by more recent information, and long-range dependencies are chal -\nlenging to capture.\nLSTM models introduce a gating mechanism to mitigate the vanishing gradient prob -\nlem in training with long sequences.\nBy learning word embeddings, deep learning enables the completion of natural lan -\nguage classification and understanding. Compared to traditional machine learning, deep \nlearning-based NLP offers several advantages: \n1. Continuous learning: Deep learning continuously learns language features based on \nword or sentence vectorization, grasping higher-level and more abstract language \nfeatures to accommodate a broad range of NLP tasks.\n2. Automatic feature learning: Deep learning eliminates the need for manual definition \nof training sets by automatically acquiring high-level features through neural net -\nworks.\nDifferent models based on transformer\nIn this section, we introduce several models based on the transformer model available in \nthe literature.\nBERT [3]\nGoogle AI’s introduction of the Bidirectional Encoder Representations from Trans -\nformers (BERT) model in 2018 sent shockwaves through the NLP industry, heralding a \nmilestone in the field’s evolution. Notably, BERT exhibited exceptional performance on \nreading comprehension datasets, but its impact extended far beyond. What set BERT \napart was its capacity to concurrently fine-tune contextual representations across differ -\nent layers, distinguishing it from contemporaneous language models. This unique fea -\nture rendered the pre-trained BERT model a versatile tool, well-suited for addressing \nintricate NLP tasks, often necessitating only minor structural adjustments for tasks like \nsentiment analysis and question answering.\nBERT’s training regimen comprises two main phases: pre-training and fine-tuning. \nDuring the pre-training phase, the model undergoes training on diverse unlabeled data, \nengaging in various pre-training tasks. The subsequent fine-tuning phase initializes \nthe pre-trained BERT model and updates its parameters using task-specific datasets. \nDespite their shared architectural foundation, fine-tuned BERT models exhibit distinct \nparameterizations, underscoring their individuality. This nuanced difference between \npre-trained and final models stands as a hallmark of the BERT framework.\nPage 10 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nGPT2 [4]\nOpenAI introduced the Generative Pre-trained Transformer (GPT) model in their paper \ntitled “Improving Language Understanding by Generative Pre-Training. ” Following this \nmilestone, OpenAI also presented the GPT-2 model in their paper titled “Language \nModels are Unsupervised Multitask Learners. ” These models have significantly contrib -\nuted to the field of natural language processing and have garnered substantial attention \nfor their capabilities in language understanding and generation. The structure of the \nGPT-2 and the GPT is not much different, but GPT-2 uses a larger dataset for experi -\nments.  GPT-2 has a very large amount of training data. BERT, which has attracted wide-\nspread attention, used 300 million parameters for training and refreshed 11 NLP records. \nThe GPT-2 launched by OpenAI has as many as 1.5 billion parameters. It is trained on \nan 8 million web page dataset and covers a wide variety of topics. In the deep learning \nmethod, The BERT and GPT-2 models both use transformer technology. The difference \nis that BERT uses a two-way language model for pre-training, while GPT2.0 uses an ear -\nlier one-way language model. Therefore, the types of architectures that GPT-2 can use in \npre-training are therefore restricted and cannot fully integrate context.\nXLNet [5]\nThe XLNet paper first put forward a point of view, dividing the current pre-training \nmodel into two types: Auto Regression (AR) and Auto Encoder (AE). GPT is an AR \nmethod that continuously uses the information currently obtained to predict the next \noutput. The BERT model is an AE method that masks some words in the input sen -\ntence and then restores the data through the BERT model. This process is similar to a \ndenoising autoencoder. XLNet combines the advantages of the AR and AE methods, and \npermutation language model (PLM) is used to achieve this goal. The XLNet model can \nshuffle the order of the tags in the model, and then AR is used for prediction. By using \nthis method, when predicting the token, the two-way information of the token can be \nused at the same time, and the dependence between tokens can be learned, as shown in \nFig. 2.\nIn order to realize PLM, XLNet proposed Two-Stream Self-Attention and Partial Pre -\ndiction. In addition, XLNet also uses the Segment Recurrence Mechanism and Relative \nPositional Encoding in Transformer-XL.\nFig. 2 Process of XLNet [5]\nPage 11 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nRoBERTa [6]\nAfter the XLNet model outperformed the BERT model in NLP tasks, Facebook pro -\nposed the a Robustly Optimized BERT Pretraining Approach model (RoBERTa). Com -\npared with the BERT model, the RoBERTa model does not have too many structural \ndifferences, but the methods in the pre-training phase have changed. Compared with \nthe BERT model, the RoBERTa model has more model parameters, a larger batch size, \nand more training data. In addition, the RoBERTa model also has different pre-train -\ning methods. First, it deletes the next sentence prediction (NSP) task. Second, it uses \ndynamic masks. The BERT model gets a static mask during data preprocessing. The \nRoBERTa model uses dynamic masks: different mask modes are applied to different data \nsequences. Through this method, different masking methods can be learned by the RoB -\nERTa model for different language representations after a large amount of data training.\nALBERT [7]\nThat the size of a model can have an impact on its performance is a lesson learned from \nthe ongoing advances in language representation learning. Surprisingly, experiments \nhave shown that merely augmenting the number of hidden layers in a model does not \nnecessarily improve performance. To address these challenges, Google researchers pro -\nposed a lightweight variant of BERT known as ALBERT, boasting significantly fewer \nparameters than the original BERT model.\nALBERT accomplishes parameter reduction through two distinctive strategies. Firstly, \nit undertakes factorization of the embedding parameterization. The model’s objective \nis to increase the number of hidden layers without expanding the parameter count for \nword embedding. To achieve this, ALBERT decomposes the extensive word embedding \nmatrix into two smaller matrices, effectively decoupling the hidden layer size from the \nword embedding size. Secondly, ALBERT introduces parameter sharing across differ -\nent layers, a technique that reduces parameter spreading as the depth of the network \nincreases. By applying these methods, the model achieves a reduction in parameters \nwhile exerting the least possible impact on performance.\nWord vectors in transformer models\nWord vectors, commonly referred to as word embeddings, are the foundation of NLP . \nThese embeddings provide a dense vector representation for words, capturing semantic \nrelationships and nuances in meaning. For instance, words with similar meanings tend \nto be closer in the vector space, enabling models to understand semantic similarities and \ndifferences between words.\nIn the context of transformer models, word vectors play a pivotal role. The initial input \nto transformer models is the word embeddings of the text. These embeddings are then \nprocessed through multiple self-attention mechanisms and feed-forward networks pre -\nsent in the transformer architecture [1]. As the information flows through the layers of \na transformer, the model refines these embeddings, capturing contextual information \nfrom surrounding words. This ability to understand context is a significant advancement \nover traditional word embeddings, which are static and lack contextual awareness.\nPage 12 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nThe application of word vectors in transformer models, such as BERT, has led to \nbreakthroughs in various NLP tasks [3]. For instance, models like BERT utilize these \nembeddings to understand the context around a word, enabling superior performance \nin tasks like question answering, sentiment analysis, and more. The dynamic nature of \ntransformers, combined with the foundational knowledge captured in word vectors, has \nmade them the state-of-the-art choice for many NLP applications.\nWhat makes transformer model better for NLP tasks\nIn this section, we introduced several special mechanisms in the transformer model.\nSelf‑attention [1]\nThere are many similarities between self-attention and attention, but the transformer \nmodel uses self-attention to understand and translate other related words into a \ntranslation method of the word we are dealing with. Let us look at an example: a wolf \ndoes not want to eat a rabbit because it is too thin. Does it represent a wolf or a rab -\nbit? It can be easily judged for us. But for the machine, it is difficult to judge. The Self-\nattention mechanism can make the machine associate it with the rabbit.\nFirst, the self-attention mechanism will calculate three new vectors, which are \nquery, key, and value. Each query key will make a dot product calculation process. \nThen use SoftMax to normalize them. Finally, it will be multiplied by value and used \nas an attention vector. The formula is taken from [1 ] and is shown in formula 1 .\nMulti‑head attention [1]\nMulti-Head Attention is equivalent to the fusion of several different self-attentions. \nThe transformer uses 8 self-attentions for integration. This can enhance the expres -\nsiveness of each layer of attention without changing the number of parameters. And \nin this way, parallel calculations can be realized, making the calculation more efficient.\nPositional encoding [1]\nPositional Embedding is a very important part in the transformer model. We found \nthat self-attention can extract the dependency relationship between words, but it \ncannot extract the absolute position or relative position relationship of words. If the \norder of key and value is disturbed, the result of attention obtained is still the same. In \nthe NLP task, the order between words is very important, so the transformer model \nuses Positional Embedding to retain word information. Each position in the sequence \nis assigned a unique numerical identifier, with each number corresponding to a spe -\ncific vector. These vectors are subsequently added to the word embeddings, thereby \nincorporating distinct positional information into each word representation.\n(1)Attention(Q, K , V ) = softmax\n(\nQK T\n√\ndk\n)\nV\nPage 13 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nMask [1]\nMask can mask some values when the parameters are updated, and the masked values \nwill not work during the update. In the transformer model, two types of masks play \npivotal roles: padding masks and sequence masks. Since input sequences in a batch \ncan have varying lengths, it’s essential to standardize their lengths. Padding masks \nserve this purpose by ensuring that input sequences share the same length.\nOn the other hand, sequence masks are designed to prevent the decoder from \naccessing future information during the decoding process. In a sequence, at any given \ntime step ’t, ’ the output should solely depend on the preceding information up to time \n’t. ’ Sequence masks are used to conceal information occurring after time ’t, ’ ensuring \nthat the decoder remains unaware of future context. This mechanism is integral to \nthe model’s autoregressive nature and its ability to generate output one step at a time, \nmaintaining coherence and adherence to the order of the sequence.\nReview of related works\nFinancial practitioners often pay attention to economic-related news because they can \nlearn stock trends from this news. For example, the stock price in the past will reflect the \npast information, and the latest news will participate in the changes in the stock price. \nTherefore, financial practitioners need to obtain positive or negative information from \nthe latest news on time to make decisions. And people can analyze the information in \nthe news through the sentiment analysis model. However, due to the unavailability of \ndomain-specific languages and large-scale label datasets, financial sentiment analysis \nis challenging. MISHEV [8] and his team conducted comprehensive research on NLP-\nbased financial sentiment analysis methods. This research covers multiple natural lan -\nguage processing methods, ranging from dictionary-based methods, word and sentence \nencoders, and transformer models. Compared with other evaluation methods, the trans-\nformer shows excellent performance. The text expression method is the main advance -\nment in the accuracy of sentiment analysis. This method inputs the semantics of words \nand sentences into the model.\nKaliyar et al. [9] studied the bi-directional model BERT. Compared with other word \nembedding models, BERT is based on the bi-directional idea. It uses a transformer \nencoder-based architecture to calculate word embedding. Although compared with the \nBiLSTM model on the transmission encoder, BERT is a powerful feature extractor. But \nin a larger corpus, BERT has longer training and inference time. It also contains large \nmemory requirements. By designing a fine-tuned BERT model for future research, these \npractical problems can be alleviated. For small datasets, the performance improvement \nof BERT will be more noticeable. It shows that the use of pre-trained networks like BERT \nmay be critical to achieving performance in such context-related tasks.\nAs mentioned earlier, the BERT model has a very good performance in natural lan -\nguage processing tasks. However, in actual tasks, the BERT model requires a lot of com -\nputing power. Sun et al. [10] proposed a patient knowledge distillation method, which \ncan compress a large-scale BERT model into a smaller-scale model. Insufficient prob -\nlems in the calculation of the scale model can be solved by such means. The Patient-KD \nmethod introduced in their work achieves multi-layer distillation, enabling the student \nmodel to comprehensively absorb the knowledge embedded within the teacher network \nPage 14 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nmodel. They substantiated the model’s efficacy by subjecting it to a battery of natural \nlanguage processing tasks, thereby validating its effectiveness and utility.\nIn the initial phase, pre-training models have gained substantial traction in the realm \nof natural language processing tasks. However, the extensive adoption of large-scale \nmodels has also brought about challenges related to real-time processing and computa -\ntional constraints. Addressing these concerns, Sanh et al. [11] introduced DistilBERT, an \nenhanced iteration of the BERT model. DistilBERT features reduced parameters, expe -\ndites training, and preserves model performance. Their work demonstrated the viabil -\nity of training a universal language model through distillation and conducted in-depth \nanalysis of different components via ablation studies.\nBi-directional attention learning can greatly help self-attention networks (SAN), \nsuch as the BERT and XLNet models. Song et al. [12] proposed a pre-training scheme \n“Speech-XLNet” similar to XLNet for unsupervised acoustic model pre-training to learn \nthe voice representation of SAN. The parameters of the pre-trained SAN model were \nadjusted and updated under the hybrid SAN/HMM framework. They speculate that \nby shuffling the sequence of speech frames, the permutation in Speech-XLNet can be \nused as a powerful regularization function to make the SAN model use its attention \nweight method to focus on the global structure. In addition, the Speech-XLNet model \ncan perform speech representation learning by exploring the context. Various experi -\nments show that Speech-XLNet is better than the XLNet model in training efficiency \nand performance.\nEffectively identifying trends in human emotions in social media can play an impor -\ntant role in maintaining personal emotional health and collective social health. Alshah -\nrani et al. [13] fine-tuned the XLNet model to predict the sentiment of Twitter messages \nat the personal message level and user level. Since the XLNet model can collectively cap-\nture the context and use multi-head attention to calculate the context representation, \ntheir method greatly improves the technical level of the benchmark dataset. In addition, \nusing deep consensus algorithms, they can significantly improve accuracy.\nCompared with static word embedding, the word embedding method represented by \ncontext performs better in many NLP tasks. For example, how is the contextual repre -\nsentation model generated by the BERT model generated? Through research, Ethayarajh \net  al. [14] learned how words will be represented in a natural context. Initially, their \ninvestigation revealed that the uppermost layer of the BERT model and analogous mod -\nels generated notably more context-specific representations compared to the lower \nlayers. This heightened level of contextual specificity consistently coincided with an \nincreased degree of anisotropy.\nKlein and Moin [15] proposed a simple and effective problem generation method. They \nbundle GPT-2 and BERT and then use an end-to-end trainable approach to promote \nsemi-supervised learning. The problems generated by this method are of high quality \nand have a higher degree of semantic similarity. In addition, the experiments performed \nshow that the proposed method allows problems to be generated and greatly reduces the \nburden of complete annotations. The word embedding in a two-way context makes the \npre-trained BERT perform better in question answering tasks. In addition, since BLEU \nand similar scores are weak metrics for evaluating generation ability, they recommend \nusing BertQA as an alternative metric for evaluating the quality of problem generation.\nPage 15 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nThe BERT model performs very well on many NLP tasks and it not only has an Eng -\nlish version but also many other voice versions. The study found that the BERT model \ntrained by a single voice is better than the BERT model trained by multiple languages. \nTherefore, training the BERT model of a specific speech has a great effect on the natu -\nral language processing task of a specific language. De Lobel et al. [16] proposed a new \nDutch model based on RoBERTa, called Robbert. And through different NLP tasks, it \nis proved that its performance is better than other language models based on the BERT \nmodel. And, they found that Robbert’s model performs better when dealing with smaller \ndatasets.\nChernyavskiy et al. [17] proposed a system specially developed for SemEval-2020 Task \n11 in a news article. The model they proposed is based on the architecture of RoBERTa, \nand then the final model is completed through integrated learning of the model after \nsubtask training.\nIn work [18], Polignano et  al. proposed an Italian language understanding model, \nALBERTo. This model is used for training with hundreds of millions of Italian tweets. \nAfter training, the model is fine-tuned on a specific Italian task, and the final result is \nbetter than other models.\nThe research of Moradshahi et al. [19] shows that different NLP tasks cannot transfer \nknowledge through the BERT model. So they proposed HUBERT, a modified version of \nthe BERT model. This model separates the symbols from the roles in the BERT repre -\nsentation by adding a decomposition layer on the BERT model. The HUBERT architec -\nture utilizes tensor product notation, where the notation of each word is constructed by \nbinding two separate attributes together. In extensive empirical research, HUBERT has \nshown continuous improvement in knowledge transfer across various language tasks.\nWu et al. [20] proposed two methods to identify offensive language behaviors in social \nmedia. First, they use supervised classification. Second, they use different pre-training \nmethods to generate different data. In addition, they did good preprocessing work, and \nthey translated the emoji into words. Then, they use the BERT model for identification.\nGao et al. [21] study the feature engineering model, based on the related work in the \nembedded neural network, and try to use the BERT model with deep neural networks. \nThen, they proposed TD-BERT models with different forms. In different NLP tasks, they \ncompared its performance with other methods. The results show that the TD-BERT \nmodel performs best. Experiments show that the complex neural network used to bring \ngood performance through embedding does not match the BERT and incorporating tar -\nget information into the BERT can stably improve performance.\nIn this work, González-Carvajal et al. [22] compared the BERT model with traditional \nmachine learning methods in many aspects. The traditional machine learning NLP \nmethod uses the TF-IDF algorithm to train the model. The article compares and ana -\nlyzes the text analysis experiments of the four methods. In all these classifications, we \nuse two different classifiers: BERT and traditional classifiers.\nBaruah et al. [23] use classifiers based on BERT, RoBERTa, and SVM to detect aggres -\nsiveness in English, Hindi, and Bengali texts. Their SVM classifier performed very well \non the test set, with 3 out of 6 tests ranking second in the official results and fourth in \nthe other. However, through more careful analysis, it can be seen that the SVM classi -\nfier performs better because the SVM model has a better classification effect. It is found \nPage 16 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nthat the BERT-based classifier can better predict minority groups. It was also discovered \nthat their classifier did not correctly handle spelling errors and deliberate spelling errors. \nFastText word embedding works better when dealing with orthographic changes.\nLee et  al. [24] trained the Korean version of the BERT model, KR-BERT, by using a \nsmall corpus. Due to the particularity of Korean and the lack of corpus, it is also very \nimportant to use the BERT model for language representation. For this reason, they \ncompared different tokenizers and gradually narrowed the minimum range of tokens \nto build a better vocabulary for their model. After these modifications, the KR-BERT \nmodel they proposed can achieve better results even with a small corpus.\nIn this paper [25], Li et al. compared the BERT and XLNet models, especially from the \ncomparison of the computational characteristics of the two. Through comparison, they \nfound two points. The first is that the two models have similar computational character -\nistics. The second is that the XLNet model has a relative position encoding function. On \nmodern CPUs, they have 1.2 times the arithmetic operation and 1.5 times the execution \ntime. At this cost, a better benchmark score was obtained.\nAs multiple geographic locations are involved, the data is inherently multilingual, lead-\ning to frequent code-mixing. Sentiment analysis of the code-mixed text can provide \ninsights into popular trends in different regions, but it is a challenge due to the non-\ntrivial nature of inferring the semantics of such data. In this paper [26], the author use \nthe XLNet framework to solve these challenges. They used the available data to fine-tune \nthe pre-trained XLNet model without any other pre-processing mechanisms.\nEkta et al. [27] proposes a method for studying machine reading comprehension. This \nmethod uses eye-tracking data for training and studies the connection between visual \nattention and neural attention. However they show that this connection does not apply \nto the XLNet model, although XLNet performs best in this difficult task. Their results \nshow that the neural attention strategies learned by different architectures seem to be \nvery different, and the similarity of neural and human attention does not guarantee opti-\nmal performance.\nNatural speech processing technology has been widely used in real life. But models \nsuch as BERT and RoBERTa need to consume a lot of computing resources. Iandola et al. \n[28] found that the grouped convolution method improves the efficiency of the com -\nputer vision network, so they used this technology in SqueezeBERT. Experiments show \nthat its training speed is 4.3 times faster than BERT.\nThe BERT model has a good performance in several NLP tasks. However, its perfor -\nmance in certain professional fields is limited. Therefore, Chalkidis et al. [29] found that \nthe application of the BERT model in the professional field requires the following steps: \nuse the additional pre-training of the specific domain corpus to adjust the BERT model \nor pre-train the BERT model from scratch on the specific domain corpus.\nLee et al. [30] uses the BERT model to implement word embedding. Then the text pro-\ncessing is performed by integrating the two-way LSTM model and the attention mecha -\nnism. The accuracy of such an integrated method can reach 0.84.\nBashmal et  al. [31] also used an ensemble learning method based on the BERT \nmodel. After preprocessing Arabic Tweets, they encode emoticons. Then through \nthe integration of the BERT model and an improved BERT model for processing sen -\ntences, a high accuracy rate was finally obtained.\nPage 17 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nThe transformer model has achieved great results in many NLP tasks. However, the trans-\nformer model has many parameters and requires a lot of space and computing resources. So \nhow to add a smaller and faster model has become a problem. Nagarajan et al. [32] proposed \na new method to reduce the size of the transformer model. The approximate calculations to \nuse simple computing resources and reduce the use of some unimportant weights. Doing so \nallows the model to gain faster speed with only a loss of accuracy.\nThere are generally two methods of normalization in neural networks, layer nor -\nmalization and batch normalization. Shen et al. [33] described why the transformer \nmodel uses layer normalization. Later, they proposed a power normalization method, \nwhich achieved better results.\nWhile the transformer model has demonstrated proficiency in addressing numerous \nnatural language processing challenges, fine-tuning the model remains an intricate \nendeavor. In their work, Li et al. [34] introduced a visualization framework aimed at \nproviding researchers with an intuitive means of obtaining feedback during parameter \nadjustments. This framework enhances clarity during the model’s fine-tuning phase \nby offering researchers a more transparent view of its behavior and performance.\nThe BERT model based on the transformer model is also applied in the medical \nfield. Electronic health records are often combined with deep learning models to pre -\ndict patient conditions. Inspired by this, Rasmy et al. [35] proposed the Med-BERT \nmodel, a pre-training model trained through patient electronic health record data. In \nthe experiment, it was found that the Med-BERT model has a higher accuracy rate in \npredicting patients’ clinical tasks.\nWith the development of the Internet, it has become easier for people to obtain news and \ninformation, and there are more and more false information and false news. As a conse -\nquence, Schütz et al. [36] harnessed multiple pre-trained transformer-based models for the \npurpose of identifying fake news. Their empirical findings underscore the robust capability of \ntransformer models in effectively discerning and detecting fake news.\nAs the Internet continues to evolve, the prevalence of social media platforms has \nsurged, with a substantial portion of content comprising satire. Identifying satirical lan -\nguage poses a unique challenge due to its distinctive nature. In response, Potamias et al. \n[37] introduced an approach that amalgamates a recurrent neural network and a trans -\nformer model to discern satirical language. Empirical results from their study highlight \nthe enhanced performance of their proposed model when applied to the dataset.\nThe BERT model released by Google is trained on the English corpus. If you want \nto apply the BERT model to other models, you need to use corpora of other languages \nto train the model. Souza et al. [38] used the Spanish corpus to train the BERT model \nand got good results in the test on downstream tasks. They called the trained model \nBERTimbau.\nThe BERT model based on the transformer model has a good performance on many \nNLP tasks. González-Carvajal et  al. [39] described why the BERT model performs \nbetter than traditional machine learning methods on natural language processing \ntasks. Describe the superiority of BERT through different experiments.\nThe BERT model is a pre-trained model based on the transformer model, while the \nALBERT model is a lightweight BERT model. Choi et al. [40] compared the BERT model \nand the ALBERT model, and then proposed an improved version of the BERT and \nPage 18 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nALBERT model, the Sentence-BERT model, and the Sentence-ALBERT. Through exper-\nimental reality, the proposed model has better performance than BERT and ALBERT.\nKoutsikakis et  al. [41] used Greek predictions to train the BERT model, and obtained a \nGREEK-BERT model suitable for Greek NLP tasks. And in the task test of natural language \nprocessing. They found that the single-language GREEK-BERT model they trained is bet-\nter than the M-BERT model and XLM-R model that are suitable for multiple languages. In \ntheir research, Hall et al. [42] conducted an extensive review of NLP models and their applica-\ntions in the context of COVID-19 research. Their focus was primarily on transformer-based \nbiomedical pretrained language models (T-BPLMs) and the sentiment analysis related to \nCOVID-19 vaccination. The comprehensive review encompassed an analysis of 27 papers, \nrevealing that T-BPLM BioLinkBERT exhibited strong performance on the BLURB bench-\nmark, which involves the integration of document link knowledge and hyperlinking into the \npretraining process. Furthermore, the study delved into sentiment analysis, leveraging vari-\nous Twitter API tools. These analyses consistently depicted a positive sentiment among the \ngeneral public regarding vaccination efforts against COVID-19. The paper also thoughtfully \noutlines the limitations encountered during the research and suggests potential avenues for \nfuture investigations aimed at enhancing the utilization of T-BPLMs in various NLP tasks \nrelated to the pandemic.\nCasola et al. [43] conducted an extensive study on the increasingly popular pre-trained \ntransformers within the NLP community. While these models have showcased remark -\nable performance across various NLP tasks, their fine-tuning process poses challenges \ndue to a multitude of hyper-parameters. This complexity often complicates model selec -\ntion and the accurate assessment of experimental outcomes. The authors commence by \nintroducing and detailing five prominent transformer models, along with their typical \napplications in prior literature, with a keen focus on issues related to reproducibility. One \nnoteworthy observation was the limited reporting of multiple runs, standard deviation, \nor statistical significance in recent NLP papers. This shortfall could potentially hinder \nthe replicability and reproducibility of research findings. To address these concerns, the \nauthors conducted an extensive array of NLP tasks, systematically comparing the perfor-\nmance of these models under controlled conditions. Their analysis brought to light the \nprofound impact of hyper-parameters and initial seeds on model results, highlighting \nthe models’ relative fragility. In sum, this study underscores the critical importance of \ntransparently reporting experimental details and advocates for more comprehensive and \nstandardized evaluations of pre-trained transformers in the NLP domain.\nIn a separate vein, Friedman et  al. [44] introduce a transformer-based NLP architecture \ndesigned to extract qualitative causal relationships from unstructured text. They underscore \nthe significance of capturing diverse causal relations for cognitive systems operating across \nvarious domains, ranging from scientific discovery to social science. Their paper presents \nan innovative joint extraction approach encompassing variables, qualitative causal relation-\nships, qualifiers, magnitudes, and word senses, all of which are instrumental in localizing each \nextracted node within a comprehensive ontology. The authors demonstrate their approach’s \neffectiveness by presenting promising outcomes in two distinct use cases involving textual \ninputs from academic publications, news articles, and social media.\nIn the realm of actuarial classification and regression tasks, Troxler et  al. [45] delve \ninto the utilization of transformer-based models to integrate text data effectively. They \nPage 19 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \noffer compelling case studies involving datasets comprising car accident descriptions \nand concise property insurance claims descriptions. These case studies underscore the \npotency of transfer learning and the advantages associated with domain-specific pre-\ntraining and task-specific fine-tuning. Moreover, the paper explores unsupervised tech -\nniques, including extractive question answering and zero-shot classification, shedding \nlight on their potential applications. Overall, the results eloquently demonstrate that \ntransformer models can seamlessly incorporate text features into actuarial tasks with \nminimal preprocessing and fine-tuning requirements.\nSingh and Mahmood [46] offer a comprehensive overview of the current landscape of state-\nof-the-art NLP models employed across various NLP tasks to achieve optimal performance \nand efficiency. While acknowledging the remarkable success of NLP models like BERT and \nGPT in linguistic and semantic tasks, the authors underscore the significant computational \ncosts associated with these models. To mitigate these computational challenges, recent NLP \narchitectures have strategically incorporated techniques such as transfer learning, pruning, \nquantization, and knowledge distillation. These approaches have enabled the development of \nmore compact model sizes, which, remarkably, yield nearly comparable performance to their \nlarger counterparts. Additionally, the paper delves into the emergence of Knowledge Retriev-\ners, a critical development for efficient knowledge extraction from vast corpora. The authors \nalso explore ongoing research efforts aimed at enhancing inference capabilities for longer \ninput sequences. In sum, this paper provides a comprehensive synthesis of current NLP \nresearch, encompassing diverse architectural approaches, a taxonomy of NLP designs, com-\nparative evaluations, and insightful glimpses into the future directions of the field.\nIn a separate domain, Khare et  al. [47] present an innovative application of transformer \nmodels for predicting the thermal stability of collagen triple helices directly from their pri-\nmary amino acid sequences. Their work involves a comparative analysis between a small \ntransformer model trained from scratch and a fine-tuned large pretrained transformer model, \nProtBERT. Interestingly, both models achieve comparable R2 values when tested on the data-\nset. However, the small transformer model stands out by requiring significantly fewer param-\neters. The authors also validate their models against recently published sequences, revealing \nthat ProtBERT surpasses the performance of the small transformer model. This study marks \na pioneering endeavor in demonstrating the utility of transformer models in handling small \ndatasets and predicting specific biophysical properties. It serves as a promising stepping stone \nfor the broader application of transformer models to address various biophysical challenges.\nDiscussion\nOne year after the transformer model was proposed, the BERT model of the encoder part \nusing the transformer model has gradually become familiar and applied to many NLP tasks. \nAlthough the BERT model performs well in various NLP tasks, it is computationally intensive \nand takes a long time. Therefore, paper [10, 11] proposed a method of knowledge distillation \nto compress the capacity of the model. Two methods are proposed in paper [10], one is the \nstudent model learning k layers from the teacher model, and another one is that learning from \nevery k layer from the teacher model. In the methodology described in [11], the approach lev-\nerages the shared dimensionality between teacher and student networks. It involves the ini-\ntialization of the student network from the teacher network by selectively taking one layer out \nof every two layers in the model.\nPage 20 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nNot only the encoder part of the transformer is widely used in NLP tasks, but the GPT \nmodel of the decoder part based on the transformer model also performs well in NLP \ntasks. In addition, the RoBERTa model based on the BERT model and the XLNet model, \nwhich improves the BERT model, also has good performance. Paper [8 , 14, 22, 23] com-\npares several models. Among them, paper [8 ] has two contributions. The first is the use \nof models for sentiment analysis of financial news. There has been very little such work \nbefore. The second point is to conduct a lot of comparison experiments, using a lot of dif-\nferent text representation methods and machine-learning classifiers for comparison. In \npaper [14], Geometry of BERT, ELMo, and GPT-2 embeddings are mainly compared. By \nanalyzing the vectors corresponding to words in different layers, we understand the dif -\nferent embedding representations of the three models. In the article [22], the author com-\npared the classification performance of traditional machine learning that uses vocabulary \nextracted from a TF-IDF model and the BERT model through several experiments. The \nempirical evidence of the BERT model’s superiority in average NLP problems classical \nmethodologies have been added through four experiments. In the article [23], the author \ncompared BERT, RoBERTa, and SVM models in three languages. Interestingly, the best \nperforming model in this article is SVM, which shows that the performance of traditional \nmachine learning methods can also surpass the transformer model. In this article, we also \ndiscovered the importance of data preprocessing, because the spelling of words will cause \nerrors in word embedding, which will lead to incorrect predictions.\nOnly using a single model such as BERT or XLNet cannot solve some problems, so \npaper [13, 15, 21] proposed some solutions combining transformer models with other \nmachine learning methods. In paper [13], they used the XLNet model combined with \ndeep consensus for sentiment analysis. This combination can better improve the accu -\nracy of the model. In paper [15], the article studies the generation and answering of \nquestions. They use the GPT-2 model to combine with the BERT model. This combina -\ntion makes better use of collaborative question generation and question answering. In \npaper [17], they use RoBERTa as the main model. But at the same time, additional CRF \nlayers were added, and training was performed on two tasks. The results show that this \ncombination is better than just using RoBERTa. In the article [21], the author proposes \nthe TD-BERT model, which is similar to the model in which a fully connected network \nis added to the BERT network for classification. The difference is that TD-BERT adds \na maximum pooling layer after BERT, which allows the classifier to make better use of \nlocation information.\nThe BERT model uses a lot of English corpora for training and has a very large Eng -\nlish corpus. But for NLP tasks in other languages, the BERT model is not competent. In \npaper [16, 18, 24, 26], other language models based on the BERT model have been estab-\nlished. Among them, paper [16] trained a large number of Dutch corpus to obtain a Rob-\nBERT model based on Dutch, and paper [18] established an ALBERTo model for Italian \nNLP tasks. These two models perform very well in the NLP task of the corresponding \nlanguage. Since Korean is one of the rich languages that use non-Latin alphabets and \nlack resources, it is also important to capture language-specific linguistic phenomena. \nIn the article [24], the author proposed a KR-BERT model for Korean NLP tasks. This \nmodel uses a smaller corpus for training, which makes the training time of the model \nshorter and more efficient. People use a lot of informal languages when using social \nPage 21 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nmedia. A lot of code-mixed languages will be produced, such as mixing two languages. \nSuch sentences will be a big obstacle to sentiment analysis. In the article [26], the XLNet \nmodel was used to solve such problems, but it did not perform well. I think that for such \ncode-mixed languages, the corpus is no longer working, so you can try to use tradi -\ntional machine learning methods or use other data preprocessing methods. The atten -\ntion mechanism is a very important part of the transformer model. Is such an attention \nmechanism similar to the human attention mechanism? The paper [27] gave the answer. \nThey found that the higher similarity of human attention and performance is signifi -\ncantly related to the LSTM and CNN models, but it is not true for the XLNet model. \nThe XLNet achieved the best performance, which shows that similar to human attention \ndoes not guarantee the best performance. It also shows that the machine can only think \nin a more advanced way.\nIandola et  al. [28] with their SqueezeBERT approach and Nagarajan et  al. [32] with \ntheir use of approximate computing and pruning. The fine-tuning and pre-training of \nthe BERT model for specific tasks or domains include papers such as Chalkidis et  al. \n[29] with their domain-specific pre-training approach and Rasmy et al. [35] with their \nwork on the medical text. The application of the BERT model in various tasks such as \nsentiment analysis, fake news detection, and satire detection. Lee et al. [30] use of bidi -\nrectional LSTM with attention, Bashmal et al. [31] work on Arabic sentiment analysis, \nSchütz et al. [36] use a transformer-based approach to fake news detection, and Pota -\nmias et  al. [37] use recurrent and convolutional neural networks for satire detection. \nThe following papers are focused on comparative analysis and model development, \nincluding papers such as Souza et al. [38] with their BERTimbau approach for Brazilian \nPortuguese, González-Carvajal et al. [39] with their comparison of BERT to traditional \nmachine learning models, Choi et al. [40] with their comparative study of BERT variants, \nand Koutsikakis et al. [41] with their work on GREEK-BERT for Greek NLP tasks.\nHall et  al. [42] and Casola et  al. [43] review the use of transformer-based biomedi -\ncal pretrained language models in COVID-19 research and the importance of reporting \nexperimental details and standardization for reproducibility. Friedman et al. [44] present \na joint extraction approach to extracting qualitative causal relationships from unstruc -\ntured text, which has important implications for cognitive systems and graph-based \nreasoning. Troxler et al. [45] explore the use of transformer-based models to incorpo -\nrate text data into actuarial classification and regression tasks. Singh and Mahmood [46] \nprovide a comprehensive overview of current NLP research, including different archi -\ntectures, a taxonomy of NLP designs, comparative evaluations, and future directions in \nthe field. Khare et al. [47] demonstrate the potential of transformer models in predict -\ning the thermal stability of collagen triple helices directly from their primary amino acid \nsequences.\nExperimental setup\nIn our experiment, we mainly use six tasks to compare five different models. In this sec -\ntion, we introduce the datasets and various parameters used by the six tasks. Then we \nprovide the results of the five models on these six tasks and explain the results.\nPage 22 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nBenchmark datasets\nIn this part, we briefly introduce the datasets available in the literature used in different tasks.\nCoronavirus tweets dataset [48] for sentiment analysis task\nFor the sentiment analysis task, we use the dataset on kaggle website [48]. These data are \nobtained from Twitter, and then manually labeled and classified. The dataset includes six \ncolumns, namely UserName, ScreenName, Location, Tweet At (time to tweet), Original \nTweet (content of Tweet), Label (emotional label). Among them, the content in Origi -\nnal Tweet is the unwashed original Tweet. And, Label contains five categories, namely \nNeutral, Positive, Extremely Positive, Negative, Extremely Negative. Table  1 shows the \ndataset we used.\nSQuAD 1.1 [49] for question answering task\nStanford Question Answering Dataset (SQuAD) [49] is a reading comprehension data -\nset. This data set consists of questions asked by researchers in a series of Wikipedia arti -\ncles. The answer to each question is a piece of text corresponding to the reading article \nor question, but the question may not be answered. SQuAD2.0 contains 100,000 ques -\ntions with answers and 50,000 questions without answers. Because our purpose is to \ncompare the performance of different models on question-answering tasks, we choose \nto use the SQuAD1.1 dataset. The SQuAD1.1 dataset contains more than 500 articles, \nincluding more than 100,000 answered questions.\nGroningen Meaning Bank corpus [50] for named‑entity recognition task\nNER, also known as named entity recognition, has many applications in real life and is \na very important NLP task. The entity recognition dataset we use has a total of 281,837 \nsentences and a total of 1,354,149 words, including eight entities. The dataset is shown \nin Table 2.\nTable 2 GMB dataset [50]\npos sentence_idx Word Tag\nNNS 1.0 Thousands O\nIN 1.0 of O\nNNS 1.0 demonstrators O\nVBP 1.0 have O\nTable 1 Coronavirus tweets dataset [48]\nUserName ScreenName Location TweetAt OriginalTweet Sentiment\n3799 48751 London 16-03-2020 @MeNyrbie @Phil_Gahan... Neutral\n3800 48752 UK 16-03-2020 advice Talk to your... Positive\n3801 48753 Vagabonds 16-03-2020 Coronavirus Australia:... Positive\n3802 48754 16-03-2020 My food stock is not... Positive\nPage 23 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nCNN daily mail dataset [51] for text summarization task\nIn today’s digital age, the internet produces an ever-increasing volume of information, \nleading to the challenge of managing vast amounts of textual data. To address this issue, \nthe task of text summarization comes to the forefront as a solution. Text summariza -\ntion can be categorized into two main types based on the input data: single-document \nsummarization and multi-document summarization. Single-document summarization \ninvolves generating concise summaries from individual documents, allowing users to \nextract key information from a single source. On the other hand, multi-document sum -\nmarization goes a step further by creating summaries from a collection of documents \nrelated to a specific topic. Text summarization provides users with a concise overview of \ninformation from multiple sources, and help them understand complex topics.\nIn the text summarization task, we use the cnn_dailymail dataset [51]. The dataset \nincludes two columns, article and highlights. The article is the main body of the news \narticle, and the highlight is the summary information.\nDisaster tweets dataset [52] for topic modeling task \nIn NLP tasks, we can extract meaning through a series of levels such as words, sen -\ntences, paragraphs, and documents. And topic modeling is the best way to understand \ntext in understanding documents. The process of acquiring, recognizing, and extracting \nthese topics from a collection of documents is commonly referred to as topic modeling.\nThe disaster tweets dataset includes more than 11,000 disaster-related tweets. The \ndataset contains five columns, namely id, keyword, location, text, and target. We are \ngoing to do topic modeling tasks, so we only keep the two columns of keyword and text. \nThe dataset is shown in Table 3.\nTrump 2020 election speech for text generation task [53]\nText generation has a wide range of applications in real life, which is also an important \ntopic in natural language processing tasks. The text generation task is important for \nsome explanatory texts, such as news and reports. Text generation tasks can also be sub-\ndivided into different tasks: text summarization and text expansion tasks and so on. We \nhave already introduced the text summarization task before. In this section, we mainly \nimplement the text expansion task.\nFor this task, we selected speeches delivered by US president Donald Trump in 2020. \nWe mask the last word of each sentence and then use different models to predict the \nmasked words. The dataset contains 494 sentences.\nTable 3 Disaster tweets dataset [52]\nid Keyword Location Text Target\n0 Ablaze Communal violence in... 1\n1 Ablaze Telangana: Section 144... 1\n2 Ablaze New York City Arsonist sets cars... 1\n3 Ablaze Morgantown, WV Arsonist sets cars... 1\nPage 24 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nTestbed\nIn this part, we introduce the experimental process and the parameters used in the \nexperiment. Our tasks are all running on google colab. The pre-trained models are listed \nin Table 4.\nSentiment analysis task\nFor the sentiment analysis task, we used the Twitter dataset, and the original tweets con-\ntained a lot of dirty data, so we cleaned the data first. We removed hyperlinks, html, \nnumbers, people mentioned, and punctuation in the tweets. Then we convert emotional \ntags, converting text tags into digital tags. Then we send the data into different pre-\ntrained models. The pre-trained models are listed in Table  4. The detailed hyperparam -\neters are listed in Table 5.\nQuestion answering task\nFor the question-and-answer task, we used the SQuAD1.1 dataset. For the BERT, \nALBERT, and RoBERTa methods, we used the question answering function in the trans-\nformer library. For different models, we load different pre-trained models. The detailed \nhyperparameters are listed in Table 5.\nNamed‑entity recognition task\nFor the NER task, we mainly use the xxForTokenClassification function in the trans -\nformers library, for example, the BertForTokenClassification function for the BERT \nmodel. The detailed hyperparameters are listed in Table 5.\nText summarization task\nThe detailed hyperparameters are listed in Table 5. In our study, we leveraged BERT and \nRoBERTa as encoder–decoder models for text summarization. This approach applies \na transformer-based model to map from the input space (encoder: source text) to the \noutput space (decoder: summary text). An instance of EncoderDecoderModel from the \ntransformers library is initialized with the pre-trained model as both the encoder and \ndecoder. Specific tokens and parameters are then set in the config for the model. Then \nTable 4 The pre-trained models\nModel BERT XLNet GPT2 RoBERTa ALBERT\nPretrained model Bert-base-uncased xlnet-base-cased gpt2 Roberta-base ALBERT-base-v2\nTable 5 hyperparameters used in different tasks\nOptimizer Learning rate Batch size Epochs\nSentiment analysis Adam 1e−5 32 5\nQuestion answering Adam 1e−5 32 5\nName entity recognition Adam 1e−5 64 3\next summarization Adam 1e−5 32 5\nPage 25 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nwe utilize the Seq2SeqTrainer with a specific set of TrainingArguments for training our \nmodel. The model is trained to generate the summary text given the input text.\nTopic modeling task\nIn the topic modeling task, we used Twitter with disease topics to conduct experiments. \nWe get a topic after topic modeling for each tweet and compare it with the actual topic \nlabel to get the result. In the topic modeling task, we use the BERTopic function to eval -\nuate different models, and the parameters used are listed in Table 6.\nText generation task\nFor text generation tasks we use Donald Trump’s campaign speech. For BERT, XLNet, \nAlbert and RoBERTa models, we mask the last word of each sentence, and then predict \nthe mask based on the previous text. For the GPT2 model, we delete the last word of \neach sentence so that the GPT2 model generates the last word based on the previous \ntext.\nResults and discussion\nIn this section, we show the experimental results. At the same time, we also analyze the \nreasons for this result from various angles.\nSentiment analysis task\nIn this section, we show the results of different models on sentiment analysis tasks and \ngive the reason.\nResults\nThe results of the sentiment analysis task are shown in Table 7.\nThe confusion matrix for the sentiment analysis task is shown in Figs. 3, 4, 5, 6,  7.\nFrom the accuracy table and the confusion matrix, we can see that in the sentiment \nanalysis problem, the ALBERT model performed the best. The accuracy of the ALBERT \nmodel reached 0.766 and the F1 score reached 0.776. The GPT2 model performed the \nworst, with an accuracy of only 0.647 and an F1 score of 0.660.\nTable 6 The detailed hyperparameters\nMaximum number of topics The number of keywords selected for each topic\n219 5\nTable 7 Results for sentiment analysis task\nModel BERT GPT2 XLNet RoBERTa ALBERT\nAcc 0.76434 0.64691 0.68825 0.71695 0.76645\nF1 0.77443 0.65966 0.70266 0.72899 0.77559\nP 0.77851 0.67552 0.70387 0.73279 0.78785\nR 0.77297 0.65249 0.70604 0.72802 0.76654\nPage 26 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nDiscussion on the results of sentiment analysis task\nThe GPT-2 model presents unique characteristics that contribute to its relatively subop -\ntimal performance. Our observation from the learning curve denotes that the loss curve \nof the GPT-2 model’s validation set intersects the training set at the fifth epoch. This sug-\ngests that GPT-2 may demand a prolonged training period, incorporating more epochs, \nto reach its peak performance. Moreover, GPT-2 employs a unidirectional language \nmodel in its pre-training stage, restricting it to absorb only prior information. While this \nis suitable for tasks such as text generation, it falls short for sentiment analysis tasks, \nFig. 3 Confusion matrix of BERT\nFig. 4 Confusion matrix of GPT2\nFig. 5 Confusion matrix XLNet\nPage 27 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nwhich require a comprehensive understanding of the contextual information. Further -\nmore, GPT-2 lacks a fine-tuning stage for downstream tasks, which, despite its extensive \npre-training on large corpora enabling decent performance in various NLP tasks, results \nin comparatively inferior performance to models incorporating a supervised learning \nfine-tuning stage.\nAs the BERT model stands as the second-best performer, we proceed to a compari -\nson between the ALBERT model and BERT. Notably, the ALBERT model diverges from \nBERT in its training approach. Instead of utilizing the next sentence prediction (NSP) \ntask employed by BERT, the ALBERT model employs the Sentence Order Prediction \n(SOP) task.\nThe NSP task in BERT comprises a binary classification task, where positive samples \nare generated by extracting two consecutive sentences from the same document, while \nnegative samples entail sentences from different documents. This task primarily aims \nto enhance downstream task performance. However, the NSP task, in essence, encap -\nsulates two subtasks: topic prediction and relationship consistency prediction. Notably, \ntopic prediction is relatively straightforward, focusing on discerning differences in top -\nics between two sentences. On the other hand, relational consistency prediction poses a \nmore complex challenge.\nThe SOP prediction task, adopted by the ALBERT model, offers unique advantages. \nBy selecting sentences from the same document, this task effectively eliminates any \nFig. 6 Confusion matrix RoBERTa\nFig. 7 Confusion matrix of ALBERT\nPage 28 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \ninfluence of sentence order on the subject matter. Furthermore, the ALBERT model \ndispenses with the dropout layer, a change that further contributes to its superior per -\nformance in downstream tasks. These modifications collectively underscore why the \nALBERT model excels, particularly in our sentiment analysis tasks.\nDiscussion on the results of question answering task\nIn this section, we show the results of different models on question-answering tasks and \ngive the reason.\nResults\nThe results of the question-answering task are shown in Table 8.\nThe metric EM(exact match) means the percentage of predictions that match any one \nof the ground truths answers exactly.\nAccording to the results that are shown in Table  8, the RoBERTa model has the best \nperformance on question-answering tasks and the BERT model is not that good com -\npared to other models.\nDiscussion of question answering result\nIn our question-answering task experiment, we observed a significant performance \ncontrast between the RoBERTa and BERT models. We analyze and focus on the sali -\nent factors that contribute to the RoBERTa model’s superior performance and the rela -\ntive shortcomings of the BERT model. Firstly, one notable aspect of the RoBERTa model \nis its expansive use of training data during the pre-training phase. An increased cor -\npus size has been widely recognized in the literature as a highly effective strategy for \nimproving a model’s performance, particularly in terms of generalizability and robust -\nness. Secondly, the RoBERTa model boasts a greater number of parameters compared \nto the BERT model. This larger parameter space could provide more capacity for RoB -\nERTa to learn intricate representations and patterns from the data, contributing to its \nsuperior performance. The third point of differentiation lies in the pre-training tasks. \nSpecifically, RoBERTa omits the NSP task, which has been found to be non-essential, \nor even potentially counterproductive, in downstream tasks such as question-answering \nsystems. This modification is likely to free up model’s capacity for learning more benefi -\ncial representations. The fourth distinction involves the masking strategy. Unlike BERT, \nwhich employs a static masking approach during data preprocessing, RoBERTa uses a \ndynamic mask, generating a fresh mask pattern for each input sequence. This flexibil -\nity allows RoBERTa to continuously adapt to different masking strategies as it absorbs \nlarge amounts of data, thereby learning varied language representations. For BERT, its \nstatic masking strategy and reliance on predicting masked tokens may limit its ability to \nlearn diverse and comprehensive language representations. This limitation may become \nTable 8 Results of question answering task\nModel BERT XLNet RoBERTa ALBERT\nEM 0.794 0.854 0.886 0.830\nF1 0.873 0.921 0.951 0.901\nPage 29 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nprominent in question-answering tasks, where understanding context and nuanced lan -\nguage is crucial.\nNamed‑entity recognition task\nIn this section, we show the results of different models on named-entity recognition task \nand show the explanation.\nResults\nThe results of the NER task are shown in Table 9.\nDiscussion of named‑entity recognition result\nThe first reason why the ALBERT model performs well is the use of Sentence Oder \nPrediction (SOP) in the pre-training phase. Compared with the NSP used in the BERT \nmodel, SOP performs better in downstream tasks. At the same time, the dropout is \nremoved in the mask stage, which will not only make the memory usage low but also \npromote the performance of downstream tasks.\nAlthough the XLNet model is an autoregressive language model, it can still introduce \ncontextual information at the same time. This is because the XLNet model uses a per -\nmutation language model. The XLNet model randomly arranges context so that context \ninformation can be introduced at the same time during the training process, which is not \npossible with general autoregressive language models. Secondly, the XLNet model is an \nautoregressive language model and does not have the disadvantages of the self-encoding \nlanguage model, that is, the error caused by the difference between the fine-tune stage \nand the pre-training stage. Finally, the XLNet model will perform better on long texts, \nand the dataset we use is also longer text. So, the XLNet model performs well in the NER \ntask.\nThe RoBERTa model’s general performance in our NER experiments can be attributed \nto a few key factors. While RoBERTa’s strengths lie in understanding broad language \ncontexts, NER tasks require more localized pattern recognition. RoBERTa’s extensive \npre-training and dynamic masking strategy may not align perfectly with this demand. \nFurthermore, its larger model capacity might introduce unnecessary complexity, poten -\ntially leading to overfitting or dilution of task-specific patterns in NER tasks. Lastly, NER \ntasks often benefit from architectures that capture sequential text information more \neffectively than what the transformer-based architecture of RoBERTa might provide. \nConsequently, RoBERTa’s performance in NER tasks appears to be moderate, suggesting \nthe need for further optimization or model adaptation specific to NER tasks.\nTable 9 Results for NER task\nModel BERT XLNet RoBERTa ALBERT\nAcc 0.935 0.954 0.884 0.952\nF1 0.986 0.988 0.918 0.990\nPage 30 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nText summarization task\nIn this section, we show the results of different models on text summarization tasks and \nshow the explanation.\nResults\nThe results of the text summarization task are shown in Table 10.\nDiscussion on the results of text summarization result\nIn our text summarization task experiment, we observed interesting performance \ndynamics between the BERT and RoBERTa models. Contrary to our expectations, the \nBERT model outperformed the RoBERTa model. This observation invites an in-depth \nanalysis of the salient features that may contribute to BERT’s superior performance \nand the relative shortcomings of the RoBERTa model in text summarization tasks. The \nRoBERTa model, which was expected to perform optimally due to its extensive training \nset and large parameter count, yielded only moderate results. RoBERTa’s strength in its \nextensive pre-training phase should theoretically confer a robust generalization ability. \nAdditionally, its larger parameter count should provide a greater capacity for learning \ncomplex patterns in downstream tasks. Moreover, RoBERTa’s dynamic mask allows it \nto learn diverse language representations, a feature that theoretically should boost its \nperformance in downstream tasks. Despite these advantages, RoBERTa’s performance \nwas not as good as BERT in our text summarization tasks. This could be attributed to \na few factors. Firstly, text summarization tasks require a balanced understanding of \nboth global (entire document level) and local (sentence or paragraph level) contextual \ninformation. BERT’s bidirectional transformers excel in understanding this balance by \nencoding each word in the context of its preceding and following words, which could be \na major contributing factor to its superior performance. Secondly, text summarization \ntasks often involve intricate rephrasing, condensation, and selection operations, which \nrequire a deep understanding of the source text and a strong semantic understanding \nability. BERT’s pre-training tasks, including Masked Language Model (MLM) and NSP , \nare designed to learn deep bidirectional representations and might be more aligned with \nthe requisites of text summarization tasks compared to RoBERTa’s single MLM pre-\ntraining task.\nTopic modeling task\nIn this section, we show the results of different models on topic modeling tasks and show \nthe explanation.\nResults\nThe results of the topic modeling task are shown in Table 11.\nTable 10 Results for text summarization task\nModel BERT RoBERTa\nrouge1 0.1406 0.2864\nrougeL 0.1097 0.2306\nPage 31 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nWe generate topics by using the BERTopic method. Each tweet generates five topics, \nand the prediction is correct if the labeled topic is included. From the results in the table, \nwe can see that the GPT2 model and the RoBERTa model perform very well. The accu -\nracy of the GPT2 model can reach around 0.37. But the accuracy of the XLNet model is \nrelatively low and only around 0.15.\nDiscussion on the results of topic modeling result\nFor the topic modeling tasks, we utilized BERTopic, a topic modeling technique that lev-\nerages class-based TF-IDF and word embeddings to generate dense explanatory clusters. \nThis process emphasizes keywords in the topic descriptions, thereby augmenting their \ncomprehensibility. As our approach primarily relied on the pre-trained models for word \nembeddings, the pre-training specifics of these models are particularly relevant.\nThe GPT-2 model, which demonstrated superior performance, is characterized by a \nvast corpus used in its pre-training phase, along with a larger parameter set. It employs \nunsupervised learning during this phase, covering a broad spectrum of topics and \nenhancing its generalizability across diverse downstream tasks. In our topic modeling \ntask, we bypassed the training phase and directly applied these pre-trained models. This \napproach inherently favors more versatile models like GPT-2, explaining its exemplary \nperformance.\nOn the other hand, the XLNet model’s performance was relatively moderate. While \nXLNet also boasts a large parameter count and employs a permutation-based training \nstrategy to learn a wide range of topics, it might not have been as effective as GPT-2’s \nleft-to-right language modeling approach for our specific task. XLNet’s permutation-\nbased learning could introduce complexity and possibly weaken the localized contextual \nunderstanding required for effective topic modeling.\nIn summary, the superior performance of the GPT-2 model in our topic modeling \ntasks can be attributed to its broad unsupervised pre-training and greater generalizabil -\nity. While XLNet is a powerful model, its unique pre-training strategy might not be as \nwell-suited to the specific requirements of topic modeling tasks.\nText generation task\nIn this section, we show the results of different models on text generation tasks and show \nthe explanation.\nResults\nFrom the above table, we can still find that the GPT2 and RoBERTa models have achieved \nvery good performance. The accuracy of the RoBERTa model is around 0.42 and the \naccuracy of the XLNet model is only about 0.11. The results are shown in Table 12.\nTable 11 Results for topic modeling task\nModel BERT XLNet GPT2 RoBERTa ALBERT\nAccuracy 0.19635 0.15068 0.36986 0.17351 0.36073\nPage 32 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nDiscussion on the results of text generation task\nFor the text generation tasks, we did not train the models on downstream tasks. \nRather, we utilized the existing pre-trained models, which provided an advantage to \nmodels like RoBERTa and GPT-2, both of which benefit from a substantial pre-train -\ning corpus and unsupervised learning during this phase.\nIn the case of RoBERTa, our methodology involved masking the last word of each \nsentence for prediction. RoBERTa, developed with a dynamic masking mechanism \nduring its pre-training phase, generates a unique masking pattern for each input \nsequence. This leads to enhanced randomness and allows the model to learn a wider \nvariety of patterns, consequently improving its performance in text generation tasks.\nIn contrast, GPT-2’s superior performance can be ascribed to its architectural \nalignment with the task’s requirements. Given that we used the first half of each sen -\ntence to predict the final word, GPT-2’s unidirectional language modeling, focusing \non left-to-right context, gives it a natural advantage over bidirectional models like \nBERT. Coupled with GPT-2’s extensive pre-training corpus, this structural advantage \ncontributed to its excellent performance.\nHowever, XLNet, despite its large pre-training corpus, displayed only moder -\nate performance. This can be traced back to our task structure. Given that we didn’t \nemploy downstream task tuning but rather direct text generation, GPT-2’s unsuper -\nvised learning approach was more suitable. XLNet’s permutation-based training may \nnot be as compatible with such task requirements, which could explain its relative \nunderperformance.\nIn conclusion, while RoBERTa’s dynamic masking and GPT-2’s unidirectional language \nmodeling and unsupervised learning strategy proved advantageous in our text genera -\ntion tasks, XLNet’s permutation-based learning might not be as well-suited to the spe -\ncific task structure. These findings highlight the importance of aligning model selection \nand task design in NLP experiments.\nStatistical significance of model comparisons\nWe employed the paired t-test, a statistical method suitable for comparing the means \nof two related groups. The paired t-test is a statistical test used to determine whether \nthere exists a statistically significant difference between the means of two groups, based \non the premise that the variations between paired observations conform to a normal \ndistribution.\nFor each task, we paired the results of the models being compared. The t-test results \nindicated that the differences in performance metrics (e.g., accuracy, F1 score) between \nmodels were statistically significant at a p-value threshold of 0.05.\nThe statistical tests reinforce our model comparisons, confirming that the observed dif-\nferences in performance are not due to random variations but are statistically significant. \nTable 12 Results for text generation task\nModel BERT XLNet GPT2 RoBERTa ALBERT\nText Generation 0.31578 0.11133 0.36234 0.41700 0.26518\nPage 33 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nThis lends greater confidence to our evaluations and the subsequent conclusions drawn \nfrom them.\nEfficiency analysis\nIn this section, we analyze the efficiency of each model. Table 13 shows the GPU usage of \neach model in sentiment analysis tasks.\nUsing the ensemble learning methods\nIn this section, we propose our ensemble learning models using the existing and well-\nknown ensemble learning [54, 55] and apply these to the natural language processing tasks. \nThen, we compare the results of the ensemble learning model with the single model and \nanalyze the reasons.\nUsing the ensemble learning method for transformers\nIn real life, when we use machine learning methods, we often only get a classifier that is \nmore accurate in a single category. People want to train to obtain a classifier that is more \naccurate in each category, which uses an ensemble learning method. Ensemble learning can \ncombine multiple weak classifiers obtained by training so that a classifier with better per-\nformance in many aspects can be obtained. Ensemble learning can be divided into three \ncategories as the following: \n1. Bagging (reduce variance) [54, 55]\n2. Boosting (reduce deviation) [54, 55]\n3. Stacking (improve prediction results) [54, 55]\nArchitecture\nIn our sentiment analysis task, we also integrate different classifiers through ensemble \nlearning to obtain a strong classifier with better performance. We first train several trans-\nformer-based language models separately and then integrate these models through neural \nnetworks.\nProcedure\n1. The five transformer models are trained separately, and then the model is evaluated \nthrough the accuracy of the model and the confusion matrix.\n2. The confusion matrix is used to select the best model in the five categories (BERT, \nXLNet, ALBERT, RoBERTa, GPT2).\n3. The outputs of the four models are cascaded and then put into a deep neural network \nfor training.\nTable 13 GPU usage of different models\nModel BERT XLNet GPT2 RoBERTa ALBERT\nGPU/GB 4.47 4.97 3.99 4.30 3.27\nPage 34 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \n4. Determine the structure of the deep neural network.\n5. Compare the training result of the ensemble model with the training result of the \nsingle model.\nUsing the ensemble learning method for sentiment analysis task\nIn this section, we discuss in detail our ensemble learning model for the sentiment anal -\nysis task.\nCustomized architecture\nThe system design for the ensemble method of the sentiment analysis task is shown in \nFig. 8. The structure of the deep neural network is shown in Table 14.\nFor the first layer of the network, we used 20 hidden units. In order to prevent the \nmodel from overfitting, we used another dropout layer with a scale of 0.5. Then we added \nanother fully connected layer of 10 hidden units. Then there is a dropout layer with a \nratio of 0.5. Finally, we use a fully connected layer of 5 hidden units for classification.\nFig. 8 The ensemble learning model for sentiment analysis\nTable 14 The structure of fully connected neural network\nLayer Number of \nneurons/\ndropout rate\nDense 20\nDropout 0.5\nDense 10\nDropout 0.5\nDense 5\nTable 15 The detailed hyperparameters\nOptimizer Activation Dropout ratio\nAdam Softmax 0.5\nPage 35 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nHyperparameter\nThe detailed hyperparameters are shown in Table 15.\nDataset\nBoth the training set and the test set used in our experiment are provided by [48].\nDesign discussion\nWe can see from the confusion matrix and ROC curve that each classifier has dif -\nferent classification effects in different categories. We hope to use ensemble learn -\ning methods to combine the advantages of the classifier to improve the accuracy of \nclassification. For multi-classification problems, the model is generally classified by \nthe Softmax function. In order to combine the advantages of different classifiers, we \nsend the output of the five classifiers into the deep neural network, and then for each \nclassifier, we use the same weight. The reason why the Softmax function is not used \nfor classification is that it cannot combine the outputs of different models. The use \nof deep neural networks can not only combine the outputs of different classifiers but \nFig. 9 ROC curve for the Albert model\nFig. 10 ROC curve for the BERT model\nPage 36 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nalso distribute the weight of each classifier through network calculations to obtain \nnew outputs. We found that the classifier after ensemble learning performed better \non the model.\nFig. 11 ROC curve for the XLNet model\nFig. 12 ROC curve for the RoBERTa model\nFig. 13 ROC curve for the GPT2 model\nPage 37 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nResults\nFirst of all, we can see the results from Figs.  9, 10, 11, 12, 13,  14 and Table  16. The \nALBERT model performs best in a single model, with an accuracy of 0.766 and an F1 \nscore of 0.775. But the ensemble learning model we proposed can make the accuracy \nreach 0.787, and the F1 score can reach 0.795.\nAfter careful inspection of the confusion matrices of the individual models, a clear pat-\ntern emerged, with each model exhibiting different performance in different categories. \nIn order to improve the performance of the model on each classification, we chose to use \nensemble learning techniques.\nOur chosen ensemble method involves channeling the output of trained models into \na deep neural network, leveraging the network’s intrinsic characteristics to classify the \ndata effectively. Within the neural network’s hidden layers, the initial layers tend to grasp \nlow-level, elementary features. As the neural network progresses, these basic features \nare combined to identify more complex patterns and features.\nFurthermore, this approach helps to fuse features derived from the output of a sin -\ngle model, enabling the learning of a wider range of features. Finally, such an ensemble \nlearning approach enriches the classification capabilities of our model, thereby improv -\ning the overall performance.\nUsing the ensemble learning method for NER task\nIn this section, we discuss in detail our ensemble learning model on the NER task.\nFig. 14 ROC curve for the ensemble learning model\nTable 16 Results for sentiment analysis task\nModel BERT GPT2 XLNet RoBERTa ALBERT Ensemble \nlearning\nAcc 0.764 0.647 0.688 0.717 0.766 0.787\nF1 0.774 0.660 0.703 0.729 0.775 0.795\nP 0.779 0.676 0.704 0.733 0.788 0.811\nR 0.773 0.652 0.706 0.728 0.767 0.786\nAUC 0.942 0.859 0.918 0.927 0.935 0.953\nPage 38 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nCustomized architecture\nIn the NER task, the system design for the ensemble method is shown in Fig. 15.\nHyperparameters\nThe Hyperparameters are shown in Table 17.\nDataset\nThe dataset we used in this experiment is provided by [50]. We randomly selected \n80% of them as the training set and 20% as the test set.\nResults\nThe results are shown in Table 18.\nDiscussion\nFirst, we use the dataset to train each single classifier. We then use the trained model \nto make predictions on the test set data. We have four single classifiers, so there are \nfour sets of prediction results. Then we evaluate the performance of the model and \nFig. 15 The ensemble learning model for NER task. ‘*’ Stands for multiplication\nTable 17 The weight of different classifiers\nModel w1 w2 w3 w4\nWeight 0.938 0.951 0.894 0.964\nTable 18 Results for NER task\nModel BERT XLNet RoBERTa ALBERT Ensemble learning\nAcc 0.938 0.951 0.886 0.894 0.964\nPage 39 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nget the accuracy of each classifier. Next, we use the accuracy rate as the weight of the \nclassification result of a single model. Finally, the results of our ensemble model are \nobtained through weight voting.\nUsing the ensemble learning method for text generation task\nIn this section, we discuss in detail our ensemble learning model for the text generation \ntask.\nCustomized architecture\nIn the text generation task, the ensemble learning method we adopted is the same as \nthat used in the NER task, and we still use the weighted voting method. First, we test \nthe individual classifiers and observe their performance as individual classifiers. Then \nwe use the accuracy of the classifier on the test set as the weight in the weighted voting \nmethod to vote on the result.\nHyperparameters\nThe detailed hyperparameters are shown in Table 19.\nDataset\nThe data we used to validate the model’s text generation capabilities came from [53].\nResults\nThrough the first row of Table 20, we find that the accuracy of using the weighted voting \nmethod to integrate the five classifiers is slightly lower than the best-performing single \nclassifier. So, we decided to remove the single classifier with the lowest accuracy from \nthe ensemble model each time to observe the performance of the ensemble model. The \nreason for this is to achieve a better classification effect through the integration of sev -\neral high-quality single classifiers.\nTable 19 The weight of different classifiers\nModel BERT XLNet GPT2 RoBERTa ALBERT\nWeight 0.316 0.111 0.362 0.265 0.417\nTable 20 Results for different combination\nEnsemble model Acc\nXLNet+Albert+BERT+GPT2+RoBERTa 0.407\nAlbert+BERT+GPT2+RoBERTa 0.409\nBERT+GPT2+RoBERTa 0.409\nGPT2+RoBERTa 0.417\nRoBERTa 0.417\nPage 40 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nDiscussion\nFrom the performance of the ensemble learning models combining different classifi -\ners, we can see that the performance of the ensemble learning method is slightly lower \nthan the best single classifier. The possible reasons for this result are as follows. First \nof all, RoBERTa, which has the best classification performance, has already covered all \nthe correct classification results during classification. Therefore, when using the ensem -\nble learning model of weight voting, the classification result is always equal to or lower \nthan the performance of the RoBERTa classifier. Secondly, the reason for the general \nperformance of the ensemble learning model may also be that the model only uses sim -\nple linear superposition, without more complicated operation logic. This resulted in the \nperformance of the ensemble learning model being slightly lower than the best single \nclassifier.\nThe comparison table is shown in Table  21. The comparison table lists the different \ntransformer-based models used in the study along with the downstream tasks we were \ntested on, the datasets used, and the advantages and limitations of each model. The table \nalso includes a column for comparing each model with others in the study.\nApplicability range of ensemble methods in NLP tasks\nIn NLP , the effectiveness of ensemble methods is not a one-size-fits-all proposition. It \nvaries across tasks due to a multitude of factors. Here, we highlight the situations where \nensemble learning may prove beneficial or potentially less advantageous.\nIn certain tasks like sentiment analysis or NER, ensemble methods often provide sig -\nnificant advantages. These tasks typically benefit from the integration of diverse models, \neach offering unique perspectives on the data. An ensemble method can aggregate the \nstrengths of the different models, thus improving overall performance. As observed in \nour research, the deep neural network ensemble method combined the outputs of vari -\nous models for sentiment analysis and NER tasks, leading to improved accuracy and F1 \nscores.\nHowever,  the utility of ensemble methods may be reduced in some cases. Text gen -\neration tasks, especially those not involving downstream task training, might exem -\nplify such a case. Here, a single well-performing model could potentially outperform an \nensemble of models. This is because the task might favor a model with a particular set \nof features or training methodology. For instance, a model that has been pre-trained on \na large corpus and employs an unsupervised learning approach might be more suitable.\nIn such a situation, if one model’s performance is significantly superior to the others, \nensemble methods might be limited by the best classifier’s performance. If the high-per -\nforming model is already capturing the majority of the correct predictions, combining it \nwith other less effective models might not result in a substantial performance increase.\nMoreover, ensemble methods might not offer added value if the participating models \nare very similar or highly correlated in their predictions. In these instances, combining \nthe models might increase the computational complexity without necessarily enhancing \nthe performance.\nA careful consideration of computational resources is also important when decid -\ning between ensemble methods and single models. Ensemble methods often require \nPage 41 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \nTable 21 Comparison table\nName of \nthe model/\nmethod\nDownstream tasks Downstream datasets Advantages and limitations of the \nmodel/method\nComparison with other models/methods\nBERT [3] Sentiment analysis, question answering, \nNER, text summarization, topic modeling, \ntext generation\nCoronavirus tweets dataset, SQuAD 1.1, \nGroningen Meaning Bank corpus, CNN daily \nmail dataset, Disaster tweets dataset, Trump \n2020 election speech dataset\nBidirectional approach for better contextual \nunderstanding.\nOutperforms traditional models like LSTM \nand Gated Recurrent Unit (GRU). Outper-\nforms GP2 in classification tasks.\nALBERT [7] Sentiment analysis, question answering, \nNER, topic modeling, text generation\nCoronavirus tweets dataset, SQuAD 1.1, \nGroningen Meaning Bank corpus, Disaster \ntweets dataset, Trump 2020 election speech \ndataset\nImproved efficiency and reduced param-\neter count. Can be trained on relatively \nsmall batch sizes.\nAchieves similar results to BERT with fewer \nparameters and smaller batch sizes.\nRoBERTa [6] Sentiment analysis, question answering, \nNER, text summarization, topic modeling, \ntext generation\nCoronavirus tweets dataset, SQuAD 1.1, \nGroningen Meaning Bank corpus, CNN daily \nmail dataset, Disaster tweets dataset, Trump \n2020 election speech dataset\nTraining with dynamic masking for better \ngeneralization. Computationally expensive\nOutperforms in many benchmarks\nXLNet [5] Sentiment analysis, question answering, \nNER, topic modeling, text generation\nCoronavirus tweets dataset, SQuAD 1.1, \nGroningen Meaning Bank corpus, Disaster \ntweets dataset, Trump 2020 election speech \ndataset\nOvercomes limitations of sequential pre-\ntraining.Requires longer training times than \nBERT\nOutperforms BERT and ALBERT in some \nbenchmarks\nGPT2 [4] Sentiment analysis, question answering, \ntopic modeling, text generation\nCoronavirus tweets dataset, SQuAD 1.1, Dis-\naster tweets dataset, Trump 2020 election \nspeech dataset\nPre-trained on large corpus of text for \ngeneralization\nAchieves high performance in language \ngeneration tasks\nOur proposed \nensemble \nlearning \nmodels\nSentiment analysis, NER, text generation Coronavirus tweets dataset, Groningen \nMeaning Bank corpus, Trump 2020 election \nspeech dataset\nCombines strengths of multiple models \nfor better accuracy. Requires additional \ncomputation for inference\nOutperforms individual models in sentiment \nanalysis and NER tasks\nPage 42 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nmore computational power and longer training times. If computational resources are \nrestricted, using a single well-tuned model might be a more practical choice.\nIn summary, the decision to utilize ensemble methods should be informed by a deep \nunderstanding of the task at hand, the specific strengths and limitations of the models \nunder consideration, and the available computational resources. Future research could \naim to explore more sophisticated ensemble strategies and their applicability to a wider \narray of NLP tasks.\nGap analysis and next steps\nIn this section, we conducted a gap analysis and decided on the tasks for the next stage.\nGap analysis\n1. While we evaluated several natural language processing models, we acknowledge \nthat there are many other models that could have been used for the tasks we per -\nformed. Future work should consider a more comprehensive evaluation of various \nmodels to determine the optimal approach for each task.\n2. Our experiments focused exclusively on methods based on the transformer model, \nand we did not compare our results to those obtained using other machine learning \nor deep learning methods. Future work should explore a wider range of approaches \nto determine the most effective method for each task.\n3. Although we proposed an ensemble method for text generation, our results indicate \nthat its performance was not particularly good. This may be due to the limitations of \nthe ensemble learning method we used. Our proposed method relied on the output \nof a single model rather than considering model structure. Future work should inves-\ntigate alternative ensemble learning methods that incorporate multiple models more \neffectively.\n4. We did not use ensemble learning methods for the tasks of question answering, text \nsummarization, and topic modeling. Future work should consider the use of ensem -\nble learning methods to improve the performance of these tasks.\nNext steps\n1. In the follow-up research, first we will expand the scope of natural language pro -\ncessing tasks and conduct research in more tasks. Secondly, we will expand the use \nof models and use more models in different tasks to make the direct comparison of \nmodels more comprehensive.\n2. Subsequently, we will also use machine learning-based methods and deep learn -\ning methods to complete natural language processing tasks. Moreover, when using \nensemble learning methods, we will not only consider models based on the trans -\nformer model but also integrate machine learning methods and some other deep \nlearning methods. This can effectively integrate the advantages of different models \nand obtain a better-performing classification method.\nPage 43 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \n3. For ensemble learning models, we will use more ensemble methods to integrate dif -\nferent classifiers. And we will consider integrating the structures of the models. After \nproposing a variety of ensemble models and conducting experimental comparisons, \nwe may potentially find the best solution for each of the natural language processing \ntasks.\n4. We  also want to use and  implement the ensemble learning methods on question \nanswering, text summarization, and topic modeling tasks through the integration of \nthe internal structures of the models. Through the integration of the internal struc -\nture, some of the above-mentioned challenges could be overcome to achieve better \nperformance.\nConclusions\nIn this research paper, we delve into the area of natural language processing, employ -\ning five distinct well-known transformer-based models to solve six different natural lan -\nguage processing tasks. Our primary focus revolves around analyzing the strengths and \nweaknesses of these models, and drawing insightful comparisons based on the results.\nOur main contribution is that we carefully study the performance of these models on \ndifferent natural language processing tasks. Given the variance  in the  performance of \nthe models, we perform an extenstive analysis of all the models’ structural nuances and \ntraining methods. A comprehensive analysis of different models on different tasks allows \nus to uncover the underlying factors that lead to performance differences between these \nmodels.\nAfter identifying the unique advantages offered by the  different models, we develop \nensemble learning models using the ensemble learning methods to leverage their \nstrengths in different aspects. The first ensemble learning model  involves pooling the \noutputs of the individual model classifiers into a deep neural network, thereby achieving \nan ensemble learning effect. The second ensemble learning model leverages the accuracy \nof the individual models as weighted votes in a voting model, following an alternative \nwell-known approach to ensemble learning. Through the analysis of the results of some \nspecific natural language processing  tasks, our ensemble learning models have  better \nperformance compared to the performance of a single classifier.\nAbbreviations\nNLP  Natural language processing\nLSTM  Long short-term memory\nNER  Named entity recognition\nSVM  Support vector machine\nCRF  Conditional random field\nRNN  Recurrent neural network\nBERT  Bidirectional Encoder Representations from Transformers\nALBERT  A Lite BERT for Self-supervised Learning of Language Representations\nRoBERTa  A Robustly Optimized BERT Pretraining Approach\nAcknowledgements\nNot applicable.\nPage 44 of 45Zhang and Shafiq  Journal of Big Data           (2024) 11:25 \nAuthor contributions\nHZ is the student who worked on this research and this paper under the supervision of MOS. HZ prepared the drafts \nof the paper, MOS reviewed the drafts and provided feedbacks, HZ iteratively implemented the feedbacks to revise the \npaper.\nFunding\nThis research is funded by Natural Sciences and Engineering Research Council of Canada (NSERC), and Carleton Univer-\nsity, Canada.\nAvailability of data and materials\nNot applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 22 August 2022   Accepted: 11 October 2023\nReferences\n 1. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Adv \nneural Inf Process Syst. 2017;30.\n 2. Vajjala S, Majumder B, Gupta A, Surana H. Practical natural language processing: a comprehensive guide to building \nreal-world NLP systems. O’Reilly Media; 2020.\n 3. Devlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language under-\nstanding. North American Chapter of the Association for Computational Linguistics; 2019.\n 4. Radford A, et al. Language models are unsupervised multitask learners. OpenAI blog. 2019;1(8):9.\n 5. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. Xlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Adv neural Inf Process Syst. 2019;32.\n 6. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V. RoBERTa: a robustly opti-\nmized BERT pretraining approach. CoRR; 2019. arXiv: 1907. 11692.\n 7. Lan Z et al. ALBERT: a lite bert for self-supervised learning of language representations; 2019. arXiv preprint arXiv: \n1909. 11942.\n 8. Mishev K, Gjorgjevikj A, Vodenska I, Chitkushev LT, Trajanov D. Evaluation of sentiment analysis in finance: from \nLexicons to transformers. IEEE Access. 2020;8:131662–82.\n 9. Kaliyar RK. A multi-layer bidirectional transformer encoder for pre-trained word embedding: a survey of BERT. In: \n2020 10th international conference on cloud computing, data science & engineering (confluence). IEEE; 2020.\n 10. Sun S, Cheng Y, Gan Z, Liu J. Patient knowledge distillation for BERT model compression. In: Proceedings of the 2019 \nconference on empirical methods in natural language processing and the 9th international joint conference on \nnatural language processing. 2019; pp. 4323–32.\n 11. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter; 2019. \nCoRR arXiv: 1910. 01108.\n 12. Song X, Wang G, Wu Z, Huang Y, Su D, Yu D, Meng H. Speech-XLNet: unsupervised acoustic model pretraining for \nself-attention networks; 2019. arXiv: 1910. 10387\n 13. Alshahrani A, Ghaffari M, Amirizirtol K, Liu X. Identifying optimism and pessimism in twitter messages using XLNet \nand deep consensus. In: 2020 international joint conference on neural networks; 2020. pp. 1–8.\n 14. Ethayarajh K. How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, \nand GPT-2 embeddings. In: Proceedings of the 2019 conference on empirical methods in natural language process-\ning and the 9th international joint conference on natural language processing; 2019. p. 55–65.\n 15. Klein T, Nabi M. Learning to answer by learning to ask: getting the best of GPT-2 and BERT worlds. CoRR; 2019. arXiv: \n1911. 02365.\n 16. Delobelle P , Winters T, Berendt B. RobBERT: a Dutch RoBERTa-based language model. In: Findings of the association \nfor computational linguistics: the 2020 conference on empirical methods in natural language processing; 2020. pp. \n3255–3265.\n 17. Chernyavskiy A, Ilvovsky D, Nakov P . Aschern at SemEval-2020 Task 11: It takes three to tango: RoBERTa, CRF, and \ntransfer learning. In: Proceedings of the fourteenth workshop on semantic evaluation; 2020. p. 1462–1468.\n 18. Polignano M, Basile P , De Gemmis M, Semeraro G, Basile V. Alberto: Italian BERT language understanding model for \nNLP challenging tasks based on tweets. In: CEUR workshop proceedings. Vol. 2481; 2019. p. 1–6.\n 19. Moradshahi M, Palangi H, Lam MS, Smolensky P , Gao J. HUBERT utangles BERT to improve transfer across NLP tasks. \nCoRR, 2019. arXiv: 1910. 12647.\nPage 45 of 45\nZhang and Shafiq  Journal of Big Data           (2024) 11:25 \n \n 20. Wu Z, Zheng H, Wang J, Su W, Fong J. Bnu-hkbu uic nlp team 2 at semeval-2019 task 6: detecting offensive \nlanguage using BERT model. In: Proceedings of the 13th international workshop on semantic evaluation; 2019. p. \n551–555.\n 21. Gao Z, Feng A, Song X, Xi W. Target-dependent sentiment classification with BERT. IEEE Access. 2019;7:154290–9.\n 22. González-Carvajal S, Garrido-Merchán EC. Comparing BERT against traditional machine learning text classification. \nCoRR; 2020. arXiv: 2005. 13012.\n 23. Baruah A, Das K, Barbhuiya F, Dey K. Aggression identification in English, Hindi and Bangla text using BERT, RoBERTa \nand SVM. In: Proceedings of the second workshop on trolling, aggression and cyberbullying; 2020. p. 76–82.\n 24. Lee S, Jang H, Baik Y, Park S, Shin H. KR-BERT: a small-scale Korean-specific language model. CoRR; 2020. arXiv: 2008. \n03979.\n 25. Li H et al. Comparing BERT and XLNet from the perspective of computational characteristics. In: 2020 international \nconference on electronics, information, and communication (ICEIC). IEEE; 2020.\n 26. Banerjee S, Jayapal A, Thavareesan S. NUIG-Shubhanker@ Dravidian-CodeMix-FIRE2020: sentiment analysis of code-\nmixed dravidian text using XLNet. arXiv preprint; 2020. arXiv: 2010. 07773.\n 27. Ekta S, Tannert S, Frassinelli D, Bulling A, Vu NT. Interpreting attention models with human visual attention in \nmachine reading comprehension. CoNLL; 2020. p. 12–25.\n 28. Iandola FN et al. SqueezeBERT: what can computer vision teach NLP about efficient neural networks?. arXiv preprint; \n2020. arXiv: 2006. 11316.\n 29. Chalkidis I et al. LEGAL-BERT: the muppets straight out of law school. arXiv preprint; 2020. arXiv: 2010. 02559.\n 30. Lee LH et al. NCUEE at MEDIQA 2019: medical text inference using ensemble BERT-BiLSTM-attention model. In: \nProceedings of the 18th BioNLP workshop and shared task; 2019.\n 31. Bashmal L, AlZeer D. ArSarcasm shared task: an ensemble BERT model for SarcasmDetection in Arabic Tweets. In: \nProceedings of the sixth Arabic natural language processing workshop; 2021.\n 32. Nagarajan A, Sen S, Stevens J R, et al. Optimizing transformers with approximate computing for faster, smaller and \nmore accurate NLP models. arXiv preprint; 2020. arXiv: 2010. 03688.\n 33. Shen S, Yao Z, Gholami A et al. Powernorm: Rethinking batch normalization in transformers. In: International confer-\nence on machine learning. PMLR; 2020. p. 8741–51.\n 34. Li R, Xiao W, Wang L, et al. T3-Vis: a visual analytic framework for training and fine-tuning transformers in NLP . arXiv \npreprint; 2021. arXiv: 2108. 13587.\n 35. Rasmy L, et al. Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records \nfor disease prediction. NPJ Dig Med. 2021;4(1):1–13.\n 36. Schütz M et al. Automatic fake news detection with pre-trained transformer models. In: International conference on \npattern recognition. Cham: Springer; 2021.\n 37. Potamias RA, Siolas G, Stafylopatis AG. A transformer-based approach to irony and sarcasm detection. Neural Com-\nput Appl. 2020;32(23):17309–20.\n 38. Souza F, Nogueira R, Lotufo R. BERTimbau: pretrained BERT models for Brazilian Portuguese. In: Brazilian conference \non intelligent systems. Cham: Springer; 2020.\n 39. González-Carvajal S, Garrido-Merchán EC. Comparing BERT against traditional machine learning text classification. \narXiv preprint; 2020. arXiv: 2005. 13012.\n 40. Choi H et al. Evaluation of BERT and ALBERT sentence embedding performance on downstream NLP tasks. In: 2020 \n25th international conference on pattern recognition (ICPR). IEEE; 2021.\n 41. Koutsikakis J et al. Greek-bert: the greeks visiting sesame street. In: 11th Hellenic conference on artificial intelligence; \n2020.\n 42. Hall K, Chang V, Jayne C. A review on natural language processing models for COVID-19 research. Healthc Anal. \n2022;2: 100078.\n 43. Casola S, Lauriola I, Lavelli A. Pre-trained transformers: an empirical comparison. Mach Learn Appl. 2022;9:100334.\n 44. Friedman S et al. From Unstructured Text to Causal Knowledge Graphs: A Transformer-Based Approach. arXiv pre-\nprint; 2022. arXiv: 2202. 11768.\n 45. Troxler A, Schelldorfer J. Actuarial applications of natural language processing using transformers: case studies for \nusing text features in an actuarial context. arXiv preprint; 2022. arXiv: 2206. 02014.\n 46. Singh S, Mahmood A. The NLP cookbook: modern recipes for transformer based deep learning architectures. IEEE \nAccess. 2021;9:68675–702.\n 47. Khare E, et al. CollagenTransformer: end-to-end transformer model to predict thermal stability of collagen triple \nhelices using an NLP approach. ACS Biomater Sci Eng. 2022;8(10):4301–10.\n 48. Dataset for sentiment analysis task. https:// www. kaggle. com/ datat attle/ covid- 19- nlp- text- class ifica tion\n 49. Dataset for question answering task. https:// rajpu rkar. github. io/ SQuAD- explo rer/.\n 50. Dataset for NER task. https:// www. kaggle. com/ shoum ikgos wami/ annot ated- gmb- corpus\n 51. Dataset for text summarization task. https:// www. tenso rflow. org/ datas ets/ catal og/ cnn_ daily mail\n 52. Dataset for topic modeling task. https:// www. kaggle. com/ vbmok in/ nlp- with- disas ter- tweets- clean ing- data\n 53. Dataset for text generation task. https:// www. kaggle. com/ risha bh6377/ trump- 2020- elect ion- speech\n 54. Ribeiro MH, dos Santos Coelho L. Ensemble approach based on bagging, boosting and stacking for short-term \nprediction in agribusiness time series. Appl Soft Comput. 2020;86:105837.\n 55. Kumar A, Mayank J. Ensemble learning for AI developers. BApress: Berkeley, CA, USA; 2020.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}