{
  "title": "Self-attention transformer unit-based deep learning framework for skin lesions classification in smart healthcare",
  "url": "https://openalex.org/W4392304829",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5074745984",
      "name": "Khosro Rezaee",
      "affiliations": [
        "Mofid University"
      ]
    },
    {
      "id": "https://openalex.org/A5044915944",
      "name": "Hossein Ghayoumi Zadeh",
      "affiliations": [
        "Vali Asr University of Rafsanjan"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971768684",
    "https://openalex.org/W2911188335",
    "https://openalex.org/W2109274792",
    "https://openalex.org/W2068748159",
    "https://openalex.org/W2167835377",
    "https://openalex.org/W4311692525",
    "https://openalex.org/W4316673314",
    "https://openalex.org/W4224308582",
    "https://openalex.org/W2808402517",
    "https://openalex.org/W2559090303",
    "https://openalex.org/W2581082771",
    "https://openalex.org/W2592160412",
    "https://openalex.org/W2914959431",
    "https://openalex.org/W3114688163",
    "https://openalex.org/W3045827876",
    "https://openalex.org/W2892053105",
    "https://openalex.org/W3156313549",
    "https://openalex.org/W3127371061",
    "https://openalex.org/W3118471509",
    "https://openalex.org/W2940790545",
    "https://openalex.org/W4205329909",
    "https://openalex.org/W3036543015",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4214577800",
    "https://openalex.org/W4200015473",
    "https://openalex.org/W3195960585",
    "https://openalex.org/W3190941935",
    "https://openalex.org/W2899425762",
    "https://openalex.org/W4210634901",
    "https://openalex.org/W3036365526",
    "https://openalex.org/W3010812223",
    "https://openalex.org/W3131296117",
    "https://openalex.org/W4220785415",
    "https://openalex.org/W4221078717",
    "https://openalex.org/W4310131915",
    "https://openalex.org/W4318263565",
    "https://openalex.org/W2061253660",
    "https://openalex.org/W6766394743"
  ],
  "abstract": "Abstract Rising mortality rates in recent years have elevated melanoma to the ranks of the world’s most lethal cancers. Dermoscopy images (DIs) have been used in smart healthcare applications to determine medical features using deep transfer learning (DTL). DI-related lesions are widespread, have local features, and are associated with uncertainty. There are three components to our bi-branch parallel model: (1) the Transformer module (TM), (2) the self-attention unit (SAU), and (3) a convolutional neural network (CNN). With CNN and TM able to extract local and global features, respectively, a novel model has been developed to fuse global and local features using cross-fusion to generate fine-grained features. Parallel systems between the branches are merged using a feature-fusion architecture, resulting in a pattern that identifies the characteristics of a variety of lesions. Moreover, this paper proposes an optimized and lightweight CNN architecture version (optResNet-18) that discriminates skin cancer lesions with high accuracy. To verify the proposed method, the procedure evaluated the accuracy for the ISIC-2019 and the PH2 datasets as 97.48 and 96.87%, respectively, a significant difference over traditional CNN networks (e.g., ResNet-50 and ResNet-101) and the TM. The proposed model outperforms state-of-the-art performance metrics such as AUC, F1-score, specificity, precision, and recall. The proposed method can also be used as a generalizable model to diagnose different lesions in DIs with smart healthcare applications by combining DTL and medical imaging. With the proposed e-Health platform, skin diseases can be detected in real-time, which is crucial to speedy and reliable diagnostics.",
  "full_text": "Vol.:(0123456789)\n Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\nDiscover Applied Sciences\nResearch\nSelf‑attention transformer unit‑based deep learning framework \nfor skin lesions classification in smart healthcare\nKhosro Rezaee1  · Hossein Ghayoumi Zadeh2\nReceived: 2 July 2023 / Accepted: 27 November 2023\n© The Author(s) 2024  OPEN\nAbstract\nRising mortality rates in recent years have elevated melanoma to the ranks of the world’s most lethal cancers. Dermos-\ncopy images (DIs) have been used in smart healthcare applications to determine medical features using deep transfer \nlearning (DTL). DI-related lesions are widespread, have local features, and are associated with uncertainty. There are three \ncomponents to our bi-branch parallel model: (1) the Transformer module (TM), (2) the self-attention unit (SAU), and (3) \na convolutional neural network (CNN). With CNN and TM able to extract local and global features, respectively, a novel \nmodel has been developed to fuse global and local features using cross-fusion to generate fine-grained features. Paral-\nlel systems between the branches are merged using a feature-fusion architecture, resulting in a pattern that identifies \nthe characteristics of a variety of lesions. Moreover, this paper proposes an optimized and lightweight CNN architecture \nversion (optResNet-18) that discriminates skin cancer lesions with high accuracy. To verify the proposed method, the \nprocedure evaluated the accuracy for the ISIC-2019 and the PH2 datasets as 97.48 and 96.87%, respectively, a significant \ndifference over traditional CNN networks (e.g., ResNet-50 and ResNet-101) and the TM. The proposed model outperforms \nstate-of-the-art performance metrics such as AUC, F1-score, specificity, precision, and recall. The proposed method can \nalso be used as a generalizable model to diagnose different lesions in DIs with smart healthcare applications by combin-\ning DTL and medical imaging. With the proposed e-Health platform, skin diseases can be detected in real-time, which is \ncrucial to speedy and reliable diagnostics.\nArticle Highlights\n• A novel approach to classifying multiple lesions in DIs is presented using a Transformer module (TM) with the self-\nattention unit (SAU).\n• A lightweight and optimized version of deep transfer learning is designed to alleviate overfitting.\n• Test multi-tissue classifying accuracy was 97.48% and 96.87% for two datasets, respectively.\nKeywords Melanoma cancer · Smart healthcare · Bi-directional feature fusion · Transformer module · Convolutional \nneural network\n * Khosro Rezaee, kh.rezaee@meybod.ac.ir | 1Department of Biomedical Engineering, Meybod University, Meybod, Iran. 2Department \nof Electrical Engineering, Vali-e-Asr University of Rafsanjan, Rafsanjan, Iran.\nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\n1 Introduction\nMelanoma cancer has risen in recent years to become one of the world’s most serious cancers [1 , 2]. One method of \ndiagnosing melanoma is dermoscopy imaging. By using this tool, pigment changes in skin lesions can be assessed \nin the disease [3 ]. Despite the difficulty of reducing cancer-related mortality, image processing technologies can \nimprove survival rates through early detection and prognosis of melanoma. Computer-aided diagnostic (CAD) tech-\nnologies save effort and time. Asymmetry, border, color, size (more than 6 mm), and change of lesions are features of \nABCDE [4 ] metrics used by many medical professionals in assessing lesions. Dermatologists can accurately identify \nskin lesions in approximately 80% of cases [5 ]. The degree of homogeneity between and within skin lesions makes \nit difficult to classify them accurately [6 ]. In ISIC, which has become the most widely used dataset for skin lesions, all \nstages of development are included.\nThere is a considerable amount of asymmetry in the distribution of skin lesion diagnoses in the datasets shown. \nThus, there are a number of reasons why it is difficult to classify skin using automated algorithms, including the com-\nplexity of the skin. It has been difficult to classify systems due to the difficulties in producing fewer complex models, \nlightweight network architectures, class imbalances, and fast inference times. Deep learning (DL) is widely accepted \nfor medical applications [7 ]. However, by employing the Transformer architecture alone, the structure parameters \nwould be significantly increased. Some research indicates that a network’s effectiveness can be enhanced by using \nCNN as the network’s start [8 ].\nTo classify the multi-lesions seen on DIs, Transformer modules (TMs) and self-attention units (SAUs) are imple -\nmented in an optimized CNN structure. The Transformer structure and CNN branches are incorporated into the \nbi-directional feature fusion architecture (it is also called cross-fusion), so that the procedure can extract more com-\nprehensive features from the structure. Current state-of-the-art models identify skin lesions using a binary approach, \nbut the proposed model has classified all skin lesions, besides melanoma, using a multi-class method. Some of the \nmain contributions to this article include:\n1. Using a deep CNN structure and the Transformer concept, this paper introduces a smart healthcare system. Moreover, \nthe proposed architecture is bi-directional and two-branch fusion of features or cross-fusion.\n2. The suggested approach has been assessed on a variety of DIs in order to assess its ability and comprehensiveness \nto overcome uncertainty.\n3. A feature visualization test verifies the model’s validity on its classification basis.\n4. To verify the validity of the suggested approach, the proposed method analyzed two DI sets.\nThe subsequent sections of the manuscript are structured in the following manner: Sect.  2 of the document pro -\nvides an in-depth analysis and examination of the existing body of research and literature relevant to the topic at \nhand. In Sect.  3, a suggested transformer module and optimized ResNet categorization method are introduced. Sec -\ntion 4 discusses outcomes and comparisons. In Sect.  5, the final section of the manuscript, the conclusion is presented.\n2  Related work\nSeveral classification methods rely extensively on handcrafted feature sets to generalize to dermoscopic skin images \n[9, 10]. Inception v3 was employed to diagnose 2032 different illnesses based on 129,450 clinical images by Esteva \net al. [11]. Using this network, skin cancer could be accurately diagnosed. Employing heat maps generated by Li et al. \n[12], the researchers established a lesion index calculation unit for the FCRN model, so coarse classification results \ncan be filtered. Using augmented regularized learning (ARL) blocks, classification layers, and pooling global averages, \nZhang et al. [13] introduced a CNN for skin categorization. It was created by Iqbal et al. Based on ISIC 2017 and ISIC \n2019 datasets, they classified DIs into several classes by relying on deep CNN models [14]. Hence, 68 convolutional \nlayers are applied to feed feature data down from the top of the structure. In addition to a dermatoscope, Jinnai et al. \n[15] used a rapid region-based CNN to categorize melanoma from 5846 clinical scans.\nAccording to Yap et al. [16], taking into account patient information can increase categorization reliability. In order \nto complete the classification, the ResNet50 network’s properties from dermoscopic and macroscopical images were \nVol.:(0123456789)\nDiscover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1 \n Research\ncombined. In addition, loss-balancing was developed to handle imbalanced datasets. Srinivasu et al. [17] used a deep \nCNN with MobileNetV2 and Long Short-Term Memory (LSTM) to classify tumors from HAM10000 images. Additionally, \nWang et al. [18] created a self-supervised topological structure that can detect unlabeled images without requiring \nthe user to have any prior knowledge of the class. Pre-trained models like AlexNet, ResNet, and VGGNet have all been \ncompared in transfer learning-based investigations [19, 20].\nBranch networks can comprise cascading branches [21]. Various fields can take advantage of this technology, which \nextracts attributes of different sizes and combines them in an understandable manner. Liu et al. [22] propose a pyrami-\ndal structure to deal with visual features at different scales. Lin et al. [23] suggest a feature combination approach with \nmultiple resolutions using Feature Pyramid Networks (FPN).\nWu et al. [24] discovered that the deep convolution vector can be derived by converting the CNN’s structure into a \nquery vector, value vector, and key vector. Because it uses fewer parameters than other transformers, the transformer \ncan use the convolution module in the center. It can achieve an accuracy of 87.75% for the ImageNet dataset.\nIn MT-TransUNet, an innovative multi-task structure was introduced by Chen et al. [25], multi-task tokens in Transform-\ners were employed for classification and segmentation of the skin collaboratively. In a study by Kumar et al. [26], the Vision \nTransformer (ViT) classification proved helpful in diagnosing melanonychia nail conditions. Wu et al. [27] built a flexible \ntransformer system, termed FAT-Net, but incorporated an additional transformer branch. Zhou et al. [28] introduced a \nvisual presentation for skin masses detection using the vision-linguistic integration paradigm in Visual Question and \nAnswer (VQA). According to Sayed et al. [29], a method for detecting benign or malignant skin lesions has been devel-\noped. Using ISIC 2020, a large publicly available data set, they evaluated their proposed method for melanoma prediction.\nIt has been shown recently in studies by Iqbal et al. [14], Mahbod et al. [30], Kaur et al. [31], Kassem et al. [32], El-Khatib \net al. [33], Ha et al. [34], Kumar et al. [35], Alfi et al. [36], Lu et al. [37], Alenezi et al. [38], and Maqsood and Damaševičius \n[39] that various methods have been proposed to classify melanoma lesions.\nFew studies have shown that their proposed models have been able to be used as an efficient method in smart \nhealthcare applications, and it has been found that some of them are not generalizable. During the melanoma diagno-\nsis process, DI provides the specialist with specific features including well-defined local lesions and extensive systemic \nmanifestations. But the interpretability of these indicators needs expertise. However, deep learning networks help to \nimprove interpretability by extracting separable features. To increase the accuracy of the classification outcomes by \nconsidering the global DI features, the proposed method uses the transformer unit to integrate these features.\n3  Proposed learning\nUtilizing the Transformer module component and the CNN design, our novel approach presents a categorization health-\ncare system for feature extraction from DIs. Our procedure is multi-stage:\n1. The global receptive field of Transformer’s branch unit allows it to extract global features from passing DIs. The DI is \nthen suggested employing a CNN branch design with convolution of local receptive field features.\n2. Bidirectional feature fusion enhances recognition rates by obtaining more comprehensive and richer features.\n3. In the final phase, the fuse procedure evaluates the categorization vectors from each split, computes the loss, and \nuses the inverse gradient methodology to fine-tune the model’s settings.\n3.1  Transformer module\nA transformer is constructed of two components, a decoder and encoder. Figure  1 illustrates that the encoder compo -\nnent of the transformer’s architecture is sufficient for this specific application. Global receptive fields greatly influence \nthe establishment of the transformation network. An alternative interpretation of Transformers can be likened to a CNN \ninternational special. The Transformer Self-Attention system employes a normalized dot product attention structure. \nThe feature vector ai of the input xi is propagated to the outputs qi, ki, and vi. The Self-Attentive model in Self-Attentive \nsupports matrix operations on variables k  and q.\nPrior to calculating the attention weight matrix, it is imperative to normalize each input mapping. The softmax function \nis employed to compute the weights when the dimensions of the query and key inputs are  dk and  dv, correspondingly. \nThe attention weight matrix is then computed by taking the dot product of the query and key inputs. Finally, the output \nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\nis obtained by taking the weighted sum of the values, using the attention weights. The point multiplication operation is \ninitially determined and subsequently divided by the variable  (dk)0.5. The attention (ATN) module is described as follows:\nIn the context of image processing applications, the matrices representing the values, keys, and query (referred to as \nV, K, and Q) as well as the resulting matrix output are utilized.\nThe two “Add and Norm” layers of the encoder are denoted by the Norm and Add blocks.\nFeed-forward (FF) networks and multi-head attention networks are respectively referred to as FeForNetwork and \nMHATN. In addition, X indicates the input to FeFor or MHATN. Output and input have the same dimensions, allowing \nthem to be added. MHATN(X) and FeFor(X) represent outputs. The FF layer is completely coupleed to two other layers. \nThere is no specification regarding the activation function (AF) employed in the second layer, while ReLU is used in the \nfirst layer. The following equation represents the first two layers:\nFurthermore, it is imperative to acknowledge that the input variable X is presumed to be present in all equations. The \nsize of the resulting matrix obtained through the Feed Forward process corresponds to X ’s dimension.\n3.2  Modified CNN\nThe CNN model was constructed utilizing the enhanced ResNet architecture, which incorporates over-parameterized \nconvolution in the depth dimension. The ResNet-18 architecture employs a tensor, denoted as P ∈  RM × Cin. M represents \nthe spatial dimension of the feature map (FM) and C in indicates the number of channels in the input FM. This architec -\nture is specifically designed to optimize both speed and detection accuracy. The kernel K  is a three-dimensional entity \ndenoted by K ∈  RCout × M × Cin. Its output is a feature with dimensions corresponding to the size of  Cout, which represents \nthe number of output channels.\n(1)ATN (Q ,Ki,Vi)=V ×\n/parenleft.s2\nsoftmax\n/bracketleft.s2\nQ T Ki × /parenleft.s1dk\n/parenright.s1−0.5/bracketright.s2/parenright.s2\n(2)ATN Gen (Q ,K,V ) = V ×\n/parenleft.s2\nsoftmax\n/bracketleft.s2\nQ T K × /parenleft.s1dk\n/parenright.s1−0.5/bracketright.s2/parenright.s2\n(3)block1 ∶⇒ Layer Norm⟨X + MH ATN (X)⟩\n(4)block2 ∶⇒ Layer Norm⟨X + FeForNetwork (X )⟩\n(5)AFReLU ∶⇒ maximum⌊ 0,XW 1 + b1 ⌋W 2 + b2\nFig. 1  The left and right sides show the encoder structure for the transformer unit and self-attention part, respectively\nVol.:(0123456789)\nDiscover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1 \n Research\nThe input channels of the P feature map are generated through depthwise convolution performed on the D \nchannel. During the process of depthwise convolution, as depicted in Fig.  2, every input channel undergoes a trans-\nformation resulting in D-dimensional features. The output is then evaluated utilizing the equation provided below:\nThe process of performing convolution with depthwise hyperparameterization involves the utilization of a depth-\nwise convolution kernel J  in conjunction with a regular convolution kernel K . The conventional convolutional kernel is \ncombined with the depthwise convolutional kernel, and subsequently, the resulting composite convolutional kernel \nis employed. The ultimate characteristic is obtained through the process of convolving the feature map.\nFigure 2 illustrates the utilization of variables K  and J in order to achieve the desired outcome of K ′ = JT·K. Moreover, \nthe K′ kernel is applied to each individual DI channel. This results in the extraction of texture characteristics from DI \nimages that are more distinct and of higher quality. Given that K′  is of equal magnitude to the classical convolution \nkernel, computational capacity remains unaltered. The presence of D ≥ M is a necessary condition for K′ to achieve the \nsame linear transformation as K in standard CNN. Through the use of hyper-parameterized depthwise convolution, \nit becomes evident that the network is over-parameterized, which further promotes the progression of learnable \nparameters and expedites training.\n(6)Output=\nM ×Cin/uni2211.s1\ni\nKCoutiPi\n(7)Output=\nM/uni2211.s1\ni\nKiDCoutiPiCin\n(8)Output = P × /bracketleft.s1JT ⋅ K /bracketright.s1\nFig. 2  illustrates three distinct types of convolutions: a Conventional, b Depthwise, and c Depthwise over-parameterized\nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\n3.3  Fusion unit with bi‑directional operation\nIn the fusion layer between the CNN branches and Transformer, there is a bi-directional fusion structure. Using bi-directional \nfusion (or cross-fusion) of the CNN module and the Transformer branch, the structure becomes more efficient and accurate. \nThe bi-directional fusion model of ResNet-18 is based on the block convolution pool block and encoder block. It was also \ndecided to connect these two components because the distance between them affects parallel computing efficiency. Inte-\ngrating the two strands is crucial for making full use of the global characteristics obtained in the first half of the transition. \nThis will ensure the receiver field expands and loses local information when the convolution layers overlap. Global features \ndisappear due to layer-by-layer overlap in the attention mechanism. Figure 3a depicts a transition from CNN to Transformer \nas well as their parallelism and connection to self-attention. CNN is employed for the purpose of extracting local features \nfrom X ∈ RH×W×3 DIs. The variables d and M can be utilized as input values for the transformation process, in conjunction with \nthe Y ∈ RM×d parameters. The original input picture undergoes convolution using the parameters d and M of the target fusion \nlayer, resulting in the generation of Y0. The bi-directional feature fusion (i.e., cross-fusion) process is employed to integrate \nlocal and global features in both CNN and Transformer models. The utilization of an attention mechanism in the CNN to \nTransformer unit is illustrated in Fig. 3a, where it facilitates the integration of local CNN attributes with global Transformer \nfeatures. The feature vectors are combined with a CNN layer and multiple channels. There are two kinds of graphs that are \npresent in this context, namely local feature graphs and global label graphs. In a multi-head self-attention model, the vari-\nable H represents the overall count of heads, encompassing X ∈  [Xh], Y ∈ [Yh], and 1 ≤ h ≤ H. The aforementioned description \nis applicable to both domestic and cross-border mergers.\nThe utilization of the projection matrix Wh\nQ is observed in (9), while the Attention function operating on Q, K, and V is \nrepresented as ATN (Q, K, V). The letters K and V represent specific regional input details, while the letter Q signifies a broader \noverarching element. The variables WO and Wh\nQ are employed to modify the value of Y. The following equations are employed \nfor the computation of the global-to-local feature fusion architecture:\n(9)Head self−Attention= ATN\n⎛\n⎜\n⎜⎝\nYhW Q\nh\nXh\nXh\n⎞\n⎟\n⎟⎠\n(10)Youtput = /bracketleft.s1Concatenation(Nodehead1\n,Nodehead2\n, ...,Nodeheadh\n)/bracketright.s1W O + Y\n(11)Head self−Attention= ATN\n⎛\n⎜\n⎜⎝\nXh\nYhW K\nh\nYhW V\nh\n⎞\n⎟\n⎟⎠\nFig. 3  The provided visual representation illustrates the bidirectional feature fusion architecture involving a ResNet-to-Transformer, and b \ntransformer-to-ResNet\nVol.:(0123456789)\nDiscover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1 \n Research\nBy using the matrices Wh\nV and Wh\nK, key values are projected. In general, queries are considered local features, while \nkey-value pairs are considered global features. The structure of feature fusion illustrated in Fig. 3b encompasses the CNN \nup to the Transformer module. It uses two inputs: (1) X ∈  Rhw×C, which represents a local feature graph with C channels in \nH, W space, and (2) Y ∈  RM×d, which is composed of M and d blocks containing input and output features.\nBesides the commonly used serial approach, CNN and Transformer can also be implemented using parallel approaches. \nThe fact that the input of the model is the vector that indicates the entire HI, as shown in Fig.  4, implies that the recep-\ntive part is also global. Parallelizing the transformer and roll integration branches of the convolutional neural network \ncreates a bi-branch parallel structure. Figure 4 illustrates the bi-branch parallel network that is the basis for bi-directional \ncross-fusion.\n4  Results\n4.1  Dataset\nA validation of the suggested model is carried out using the first available ISIC-2019 data set [40]. HAM_10000 and \nBCN_20000 are included in the ISIC-2019 database (the newest kind of ISIC-2018). The eight classes in ISIC-2019 are: \nMelanoma (MEL), Vascular Lesion (VASC), Actinic Keratosis (AKIEC), Squamous Cell Carcinoma (SCC), Dermatofibroma \n(DF), Basal Cell Carcinoma (BCC), Benign Keratosis (BKL), and Melanocytic Nevus (NV). In the second dataset of dermo -\nscopic images, a broad range of patients with varying degrees of skin disease, lesions, or malignancy were presented. \nImages are taken from the  PH2 database, which includes images of normal skin, nevi, and malignant tissues associated \nwith melanoma. A sample of DIs received from the ISIC-2019 and  PH2 datasets is shown in Fig. 5.\nPH2 contains dermoscopy data with clinical diagnosis and manual segmentation. Dermoscopy images of 200 normal \nnevi, 200 aberrant nevi, and 40 malignant melanomas (three classes) were captured with 8 bits of color depth. Clinical \n(12)Xoutput = /bracketleft.s1Concatenation(Node head1\n,Node head2\n, ...,Node headh\n)/bracketright.s1+ X\nFig. 4  This diagram demon-\nstrates a global receptive field \nas it takes the entire HI vector \nas its input\nSCC VASC DF BKLA KIEC BCC MelN V\nFig. 5  These DIs were taken from the ISIC-2019 and the  PH2 datasets\nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\ndiagnostic techniques and outcomes at Hispano Hospital include dermoscopic components, such as ABCD recommen-\ndations, 7-point checklists, and Menzies methodologies. Scholars in this field hold the aforementioned characteristics \nin high regard and have successfully incorporated them into several research studies [41].\nIn addition, the proposed model was validated using the second available ISIC-2019 data set [42]. Two versions of the \ndataset are available, one with a lower resolution (600 × 450) and one with a higher resolution (1024 × 1024). As a result \nof including an additional class in the experimental set, the test is larger than the training set. To identify any potential \nlimitations from the implementation of ISIC-2019, a comprehensive series of tests has been conducted in order to opti-\nmize our outcomes. ISIC-2019 categorizes skin malignancies into 8 distinct categories, including melanoma, vascular \nlesions, actinic keratosis, squamous cell carcinoma, dermal fibroma, basal cell carcinoma, benign keratosis, and mel -\nanocytic nevus. To verify the efficacy of the proposed model, we employed the second ISIC-2019 dataset [42]. There are \ntwo resolutions in the dataset: 600 by 450 pixels and 1024 by 1024 pixels. An unidentified class has been added to the \nexperimental set-in addition to the established training set. We perform several tests to determine whether ISIC-2019 \nmay restrict our ability to achieve optimal results.\n4.2  Setting\nOur goal is to develop the model’s classified superiority by setting them during the learning process. This leads to value \nchanges when the training strategy is based on an optimization technique. Some other parameters were adjusted using \nvariables used in similar methods as well, and initialization of these parameters was also taken into consideration, since \nthe values of these parameters were changed during the learning procedure by optimizing the convolutional set-in \norder to achieve a better response and monitoring the training process. The mini-batch size was set at 6, and the train-\ning epochs were limited to 100. Furthermore, the fine-tune was set to 0.90 momentum and 0.00001 learning rate. In \naddition, DIs are divided into three sections. Data from 80% of the set was employed for training, 10% for testing, and \n10% for validation. Furthermore, the fivefold cross validation (CV = 5) method was used in another experiment. Due to \nthe different DIs split in all experiments, the response dispersion was low. There are 25,331 DIs in ISIC 2019, which are \ndistributed in a variety of methods, as illustrated in Table 1.\n4.3  Evaluations\nAs a result of the low categorization accuracy of some classes in the ISIC-2019 dataset, the classification distribution \nhas been tested using the criteria described in three fine-tuned models, namely the first model (ResNet50), the second \nmodel (ResNet101), and the third model (optResNet).\nVASC and DF lesions are more unpredictable than other types of skin lesions. The data set contains a relatively small \nnumber of these lesions. Overall, however, the proposed method offers satisfactory outcomes for the diagnosis of skin \nlesions. From the ISIC-2019 images, Fig. 6 shows the confusion matrix (CM) for 8 various classes of DIs. Based on multiple \nexaminations, DIs can accurately detect 97.3–97.8% of skin lesions.\nAdditionally, the classification technique was applied to two datasets, shifting the number of rounds and incorporating \nvarious lesion features. The classification of both types was confirmed by testing and validation. Sections (a) and (b) of \nFig. 7 show accuracy and loss based on convergence. In sections (c) and (d), optResNet-18 was also compared to other \nsimilar methods, such as ResNet-50 and ResNet-101, for the same accuracy criteria and loss for optimal convergence. \nA second experiment used the fivefold cross validation method to divide the data and estimate performance criteria. \nThis work examines the efficiency of the algorithm when the data distribution is changed. The Micro F-measure metric \nextends multi-label evaluation by employing the F1-score, whereas the Macro F-score integrates precision and recall for \neach item within a label vector, regardless of the presence of separate labels.\nTable 1  The distribution of \nthe DIs in ISIC 2019 DIs NV AKIEC SCC BCC VASC Mel DF BKL\nSample size 12,875 867 628 3323 253 4522 239 2624\nTrain size 10,301 693 502 2579 203 3618 191 2100\nValidation size 1287 87 63 332 25 452 24 262\nTest size 1287 87 63 332 25 452 24 262\nVol.:(0123456789)\nDiscover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1 \n Research\nFig. 6  The CM at the left (validation DIs) and the right (test DIs), in this figure, indicates that the model is 97.48% accurate on average\nFig. 7  Shown in sections (a, b) are the accuracy and loss of the suggested structure including optResNet-18 with TM and SAU for train-\ning and validation DIs. Sections (c, d) compare the convergence of optResNet-18 with other similar CNN models, such as ResNet-50 and \nResNet-101, through the same accuracy criteria and loss function\nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\nThe macro and micro metrics for F1-score, precision, and recall have been calculated separately for both datasets, as \nrevealed in Table 2. The outcomes reported in Table 3 are the evaluation of the same experiments performed in Table 2, \nwhich include fivefold (1), fivefold (2), and fivefold (3). There is a possibility that the proposed attention mechanism and \nenhanced learning algorithms will provide sufficient reliability and generalization. Based on the findings, the proposed \napproach proves to be competitive in terms of skin cancer detection and classification.\n5  Discussion\nThe lack of generalizability and repeatability has limited previous approaches to diagnosing skin diseases. The plotted \nROCs illustrate how the model can distinguish between different types of skin diseases. Using four random categories of \nunseen DIs, various analyses were performed, and shown in Fig. 8 is the AUC value for each class. As a result of the clas-\nsification of DF (AUC ≈ 0.873) and VASC (AUC ≈ 0.932) lesions due to their similarity to other classes, they had a lower AUC.\n5.1  Comparison\nThe transformer module (TM) of the optResNet-18 is oriented around the Self-attention unit (SAU) as opposed to previ-\nous TL topologies, allowing for rapid network construction while minimizing training and testing errors. It is crucial to \ndistinguish between DL algorithms based on their convergence speed and dependability. The optResNet-18 strategy with \nTM and SAU was found to have more useful features and converge faster than the other two strategies (e.g., ResNet-50 \nand ResNet-101). As depicted in Fig. 9, the TM with SAU and bi-directional feature fusion-based melanoma classification \nmethod is both stable and reliable, even when the number of DI classes is high.\nTable 2  Based on sets of \nunseen images, macro and \nmicro metrics have been \ncalculated for F1-score, \nprecision, and recall\nA five-fold cross validation was used to partition the data and calculate the average\nDataset Cross validation Macro Micro\nF1-score Precision Recall F1-score Precision Recall\nISIC-2019 Fivefold (1) 0.8910 0.9756 0.8496 0.9751 0.9751 0.9751\nFivefold (2) 0.9105 0.9765 0.8730 0.9759 0.9759 0.9759\nFivefold (3) 0.9170 0.9773 0.8817 0.9787 0.9787 0.9787\nPH2 Fivefold (1) 0.9582 0.9630 0.9583 0.9500 0.9500 0.9500\nFivefold (2) 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000\nFivefold (3) 0.9153 0.9333 0.9167 0.9000 0.9000 0.9000\nTable 3  Based on the \nISIC-2019 and  PH2 images, \na comparison is made of \nquantitative methods and our \nproposed model\nMethod Dataset Accuracy (%) Precision (%) Recall (%) Specificity (%) F1-score (%)\nKaur et al. [31] ISIC-2017 88.23 78.56 87.87 88.87 78.21\nISIC-2020 90.42 90.46 90.39 90.39 90.41\nKassem et al. [32] ISIC-2019 94.92 – 79.8 97.00 –\nEl-Khatib et al. [33] ISIC-2019 88.33 – 88.46 88.24 –\nHa et al. [34] ISIC-2020 96.02 – – – –\nKumar et al. [35] ISIC-2018 96.70 – – – –\nAlfi et al. [36] ISIC 2018 92.00 91.00 92.00 – –\nLu et al. [37] HAM10000 100 – – – –\nAlenezi et al. [38] ISIC-2020 99.00 – – – –\nMaqsood et al. [21] HAM10000 98.57 – – – –\nISIC2018 98.62 – – – –\nISIC2019 93.47 – – – –\nPH2 98.98 – – – –\nProposed method ISIC-2019 97.48 97.27 87.53 99.49 91.37\nPH2 96.86 96.45 96.12 97.88 96.44\nVol.:(0123456789)\nDiscover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1 \n Research\nHere, the maximum, minimum, and mean accuracy is computed and shown so that the highest possible level of \naccuracy can be achieved for feature extraction and classification. The optResNet-18 and TM with SAU were found to \noutperform other similar models for categorizing skin lesions, as shown in Table 3. In the context of melanoma detection, \nbidirectional feature fusion is superior to alternative methods for melanoma classification. Results that surpass or meet \nexpectations using criteria such as accuracy, F1-score, specificity, recall, and precision are highlighted in bold. Due to the \nhigh computational complexity of some methods, dermatologists may not consider time delay to be highly desirable, \nsince the dermatologist’s decision is time-sensitive. Based on what the authors know, accuracy does not exceed 94% \nfor methods that rely on identifying skin diseases. It should be noted, however, that these studies are not considered \naccurate in comparison with the method proposed in this study. The suggested healthcare system uses a small number \nof learnable parameters, which makes the network simpler and more efficient. Because of the necessity of diagnosing \nskin cancers as soon as possible, rapid and accurate DI categorization is a crucial feature of early skin cancer identification.\nAccurate classification algorithms, which function even without annotated datasets, reduce dermatologists’ burden. \nThis research presented the hybrid approach as a potential way to deal with limited time and space for education. It \nprovided efficient deployment of DIs in healthcare settings and precise categorizations.\n5.2  Limitations\nThere are many limitations associated with DL-based methods for skin cancer categorization, including model robust -\nness, lack of labeled data, imbalance and limitation of classification data, domain compatibility, and efficiency. There are \nmore benign skin cancer cases in most datasets than cancerous cases due to data imbalance. Therefore, this challenge \nFig. 8  The AUC value for each class was computed based on four random categories of DIs\nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\nleads to the inaccuracy of classification models. The robustness and efficiency of a classification model refer to its ability \nto correctly classify with a minimum amount of overhead. A skin cancer classification system must take into account all \nof these factors. Furthermore, the system should be validated using real-world data to ensure accuracy and reliability. \nUsing clinical scans for skin cancer categorization is problematic because they contain limited morphological informa-\ntion. Various imaging settings, such as angle, light, brightness, contrast, etc., also introduce significant inaccuracies in \ndiagnostic results. This paper shows that the model’s generalization performance decreases when training data is insuf-\nficient. Furthermore, various dermoscopic imaging conditions, such as hand tremors, excessive changes in cell color, \nimage darkness, and pixel occlusion, can affect the model and thus reduce detection. In addition, if the limitations of the \nmodel consist the increase in the number of layers, the number of parameters, and the change in the initial fine-tuning \nconditions of the network, the operational efficiency and resource consumption of the model could limit its clinical \nimplementation in a variety of medical devices. Neither the model’s performance nor its detection power can be evalu-\nated based on images where the lesion was mixed with the skin surface. Furthermore, artifacts such as human hair can \naffect the generation of original images and, therefore, the classification of the images. This is because the model is only \nable to detect the lesion if it is clearly defined and separated from the rest of the image. Any artifacts, such as hair, can \nmake it difficult to distinguish the lesion from the rest of the image, leading to a misclassification.\n6  Conclusion\nIn this work, a hybrid architecture was introduced for the classification of multiple skin lesions from DIs that integrates \nthe TM and the SAU with an efficient CNN structure. A lightweight architecture was created by lowering the computa-\ntional complexity of the CNN structure-based feature generation process. Compared to existing comparable methods \nthat are not generalizable, the proposed procedure typically maintains accuracy when complex sections and textures \nare incorporated into DIs. An extensive analysis of the ISIC-2019 and  PH2 datasets demonstrates the generalizability of \nthe suggested approach. An experiment was conducted with a large number of previously unseen DI collections. With \nthe aim of achieving the optimal model, the authors will combine and permute attention processes using DL models. \nThis study employs the idea of cross-fusion in the analysis of dermoscopy photos, with the aim of presenting a concise \ncompilation of traits that are integrated to determine the advancement of a disease. In order to enhance the represen-\ntation and visualization of traits, the network may employ many photos of a single patient. The authors of this study \nexpress their intention to further explore this subject matter in further research endeavors. The potential for elevated \nFig. 9  There are a number of DI classes, but the TM-based melanoma classification method combining SAUs and bidirectional feature \nfusions is stable and reliable, despite the number of DI categories. The performance of the fusion system and the TM part was measured \nthrough maximum, mean, and minimum values (in the first and second rows, respectively)\nVol.:(0123456789)\nDiscover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1 \n Research\nmisclassification rates may arise from suboptimal picture quality in specific photographs; however, enhancing the qual-\nity of all images would need additional computational resources. To make a kind of compromise between accuracy and \ncomputational complexity, we plan to use more data sets in the future. Furthermore, we use ensembling methods to \nintegrate attentional mechanisms in the future.\nAuthor contributions HGZ and KR planned the experiments. KR carried out the implementations of the proposed model. KR and HGZ con-\ntributed to the interpretation of the results. KR and HGZ took the lead in writing the manuscript. KR and HGZ reviewed the manuscript and \nprovided critical feedback and helped shape the research, analysis, and manuscript editing.\nFunding No funding was received.\nAvailability of data and materials The implemented codes are all available from the corresponding authors. Additionally, dermoscopic images \nare available through the link below. https:// chall enge2 019. isic- archi ve. com/.\nDeclarations \nCompeting interests There is no competing interests.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Papachristou I, Bosanquet N. Improving the prevention and diagnosis of melanoma on a national scale: a comparative study of perfor -\nmance in the United Kingdom and Australia. J Public Health Policy. 2020;41:28–38. https:// doi. org/ 10. 1057/ s41271- 019- 00187-0.\n 2. Siegel RL, Miller KD, Jemal A. Cancer statistics. CA Cancer J Clin. 2019;69(1):7–34. https:// doi. org/ 10. 3322/ caac. 21551.\n 3. Andtbacka RH, Kaufman HL, Collichio F, Amatruda T, Senzer N, Chesney J, Delman KA, Spitler LE, Puzanov I, Agarwala SS, Milhem M. Tali-\nmogene laherparepvec improves durable response rate in patients with advanced melanoma. J Clin Oncol. 2015;33(25):2780–8. https:// \ndoi. org/ 10. 1200/ jco. 2014. 58. 3377.\n 4. Rigel DS, Friedman RJ, Kopf AW, Polsky D. ABCDE—an evolving concept in the early detection of melanoma. Arch Dermatol. \n2005;141(8):1032–4. https:// doi. org/ 10. 1001/ archd erm. 141.8. 1032.\n 5. Nikolaou V, Stratigos AJ. Emerging trends in the epidemiology of melanoma. Br J Dermatol. 2014;170(1):11–9. https:// doi. org/ 10. 1111/ \nbjd. 12492.\n 6. Yang Y, Xie F, Zhang H, Wang J, Liu J, Zhang Y, Ding H. Skin lesion classification based on two-modal images using a multi-scale fully-shared \nfusion network. Comput Methods Programs Biomed. 2023;1(229):107315. https:// doi. org/ 10. 1016/j. cmpb. 2022. 107315.\n 7. Mukadam SB, Patil HY. Skin cancer classification framework using enhanced super resolution generative adversarial network and custom \nconvolutional neural network. Appl Sci. 2023;13(2):1210. https:// doi. org/ 10. 3390/ app13 021210.\n 8. Dhamija T, Gupta A, Gupta S, Katarya R, Singh G. Semantic segmentation in medical images through transfused convolution and trans-\nformer networks. Appl Intell. 2023;53(1):1132–48. https:// doi. org/ 10. 1007/ s10489- 022- 03642-w.\n 9. Barata C, Celebi ME, Marques JS. A survey of feature extraction in dermoscopy image analysis of skin cancer. IEEE J Biomed Health Inform. \n2018;23(3):1096–109. https:// doi. org/ 10. 1109/ JBHI. 2018. 28459 39.\n 10. Xie F, Fan H, Li Y, Jiang Z, Meng R, Bovik A. Melanoma classification on dermoscopy images using a neural network ensemble model. IEEE \nTrans Med Imaging. 2016;36(3):849–58. https:// doi. org/ 10. 1109/ TMI. 2016. 26335 51.\n 11. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer with deep neural net-\nworks. Nature. 2017;542(7639):115–8. https:// doi. org/ 10. 1038/ natur e21056.\n 12. Li Y, Shen L. Skin lesion analysis towards melanoma detection using deep learning network. Sensors. 2018;18(2):556. https:// doi. org/ 10. \n3390/ s1802 0556.\n 13. Zhang J, Xie Y, Xia Y, Shen C. Attention residual learning for skin lesion classification. IEEE Trans Med Imaging. 2019;38(9):2092–103. https:// \ndoi. org/ 10. 1109/ TMI. 2019. 28939 44.\n 14. Iqbal I, Younus M, Walayat K, Kakar MU, Ma J. Automated multi-class classification of skin lesions through deep convolutional neural \nnetwork with dermoscopic images. Comput Med Imaging Graph. 2021;1(88):101843. https:// doi. org/ 10. 1016/j. compm edimag. 2020. \n101843.\n 15. Jinnai S, Yamazaki N, Hirano Y, Sugawara Y, Ohe Y, Hamamoto R. The development of a skin cancer classification system for pigmented \nskin lesions using deep learning. Biomolecules. 2020;10(8):1123. https:// doi. org/ 10. 3390/ biom1 00811 23.\n 16. Yap J, Yolland W, Tschandl P . Multimodal skin lesion classification using deep learning. Exp Dermatol. 2018;27(11):1261–7. https:// doi. org/ \n10. 1111/ exd. 13777.\nVol:.(1234567890)\nResearch Discover Applied Sciences             (2024) 6:3  | https://doi.org/10.1007/s42452-024-05655-1\n 17. Srinivasu PN, SivaSai JG, Ijaz MF, Bhoi AK, Kim W, Kang JJ. Classification of skin disease using deep learning neural networks with MobileNet \nV2 and LSTM. Sensors. 2021;21(8):2852. https:// doi. org/ 10. 3390/ s2108 2852.\n 18. Wang D, Pang N, Wang Y, Zhao H. Unlabeled skin lesion classification by self-supervised topology clustering network. Biomed Signal \nProcess Control. 2021;1(66):102428. https:// doi. org/ 10. 1016/j. bspc. 2021. 102428.\n 19. Jojoa Acosta MF, Caballero Tovar LY, Garcia-Zapirain MB, Percybrooks WS. Melanoma diagnosis using deep learning techniques on der-\nmatoscopic images. BMC Med Imaging. 2021;21(1):1–1. https:// doi. org/ 10. 1186/ s12880- 020- 00534-8.\n 20. Kassani SH, Kassani PH. A comparative study of deep learning architectures on melanoma detection. Tissue Cell. 2019;1(58):76–83. https:// \ndoi. org/ 10. 1016/j. tice. 2019. 04. 009.\n 21. Ma M, Xia H, Tan Y, Li H, Song S. HT-Net: hierarchical context-attention transformer network for medical CT image segmentation. Appl \nIntell. 2022;15:1–4. https:// doi. org/ 10. 1007/ s10489- 021- 03010-0.\n 22. Liu Z, Gao G, Sun L, Fang L. IPG-net: image pyramid guidance network for small object detection. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition workshops; 2020. p. 1026–7. https:// doi. org/ 10. 1109/ CVPRW 50498. 2020. 00521.\n 23. Lin TY, Dollár P , Girshick R, He K, Hariharan B, Belongie S. Feature pyramid networks for object detection. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition; 2017. p. 2117–25.\n 24. Wu H, Xiao B, Codella N, Liu M, Dai X, Yuan L, Zhang L. Cvt: introducing convolutions to vision transformers. In Proceedings of the IEEE/\nCVF international conference on computer vision; 2021. pp 22–31. https:// doi. org/ 10. 1109/ ICCV4 8922. 2021. 00009.\n 25. Chen J, Chen J, Zhou Z, Li B, Yuille A, Lu Y. MT-TransUNet: mediating multi-task tokens in transformers for skin lesion segmentation and \nclassification. Preprint arXiv: 2112. 01767. 2021 Dec 3.\n 26. Kumar K, Kumar C, Nijhawan R, Mittal A. The hybrid vision transformer approach for hyperpigmentation nail disease detection. In: Pro -\nceedings of second international conference on sustainable expert systems. Singapore: Springer; 2022. p. 31–42. https:// doi. org/ 10. 1007/ \n978- 981- 16- 7657-4_4.\n 27. Wu H, Chen S, Chen G, Wang W, Lei B, Wen Z. FAT-Net: Feature adaptive transformers for automated skin lesion segmentation. Med Image \nAnal. 2022;1(76):102327. https:// doi. org/ 10. 1016/j. media. 2021. 102327.\n 28. Zhou L, Luo Y. Deep features fusion with mutual attention transformer for skin lesion diagnosis. In 2021 IEEE international conference on \nimage processing (ICIP); 2021 Sept 19. p. 3797–801. https:// doi. org/ 10. 1109/ ICIP4 2928. 2021. 95062 11.\n 29. Sayed GI, Soliman MM, Hassanien AE. A novel melanoma prediction model for imbalanced data using optimized SqueezeNet by bald \neagle search optimization. Comput Biol Med. 2021;1(136):104712. https:// doi. org/ 10. 1016/j. compb iomed. 2021. 104712.\n 30. Mahbod A, Schaefer G, Ellinger I, Ecker R, Pitiot A, Wang C. Fusing fine-tuned deep features for skin lesion classification. Comput Med \nImaging Graph. 2019;1(71):19–29. https:// doi. org/ 10. 1016/j. compm edimag. 2018. 10. 007.\n 31. Kaur R, GholamHosseini H, Sinha R, Lindén M. Melanoma classification using a novel deep convolutional neural network with dermoscopic \nimages. Sensors. 2022;22(3):1134. https:// doi. org/ 10. 3390/ s2203 1134.\n 32. Kassem MA, Hosny KM, Fouad MM. Skin lesions classification into eight classes for ISIC 2019 using deep convolutional neural network \nand transfer learning. IEEE Access. 2020;19(8):114822–32. https:// doi. org/ 10. 1109/ ACCESS. 2020. 30038 90.\n 33. El-Khatib H, Popescu D, Ichim L. Deep learning-based methods for automatic diagnosis of skin lesions. Sensors. 2020;20(6):1753. https:// \ndoi. org/ 10. 3390/ s2006 1753.\n 34. Ha Q, Liu B, Liu F. Identifying melanoma images using efficientnet ensemble: winning solution to the siim-isic melanoma classification \nchallenge. Preprint arXiv: 2010. 05351. 2020 Oct 11.\n 35. Kumar NS, Hariprasath K, Tamilselvi S, Kavinya A, Kaviyavarshini N. Detection of stages of melanoma using deep learning. Multimed Tools \nAppl. 2021;80:18677–92. https:// doi. org/ 10. 1007/ s11042- 021- 10572-1.\n 36. Alfi IA, Rahman MM, Shorfuzzaman M, Nazir A. A non-invasive interpretable diagnosis of melanoma skin cancer using deep Learning and \nensemble stacking of machine learning models. Diagnostics. 2022;12(3):726. https:// doi. org/ 10. 3390/ diagn ostic s1203 0726.\n 37. Lu X, Firoozeh Abolhasani Zadeh YA. Deep learning-based classification for melanoma detection using XceptionNet. J Healthc Eng. 2022. \nhttps:// doi. org/ 10. 1155/ 2022/ 21960 96.\n 38. Alenezi F, Armghan A, Polat K. A multi-stage melanoma recognition framework with deep residual neural network and hyperparameter \noptimization-based decision support in dermoscopy images. Expert Syst Appl. 2023;215:119352. https:// doi. org/ 10. 1016/j. eswa. 2022. \n119352.\n 39. Maqsood S, Damaševičius R. Multiclass skin lesion localization and classification using deep learning based features fusion and selection \nframework for smart healthcare. Neural Netw. 2023;160:238–58. https:// doi. org/ 10. 1016/j. neunet. 2023. 01. 022.\n 40. Sloan Kettering Cancer Center. The international skin imaging collaboration. 2019. Available online: https:// www. isic- archi ve. com/# !/ \ntopWi thHea der/ wideC onten tTop/ main/. Accessed on 15 Dec 2020.\n 41. Mendonça T, Ferreira PM, Marques JS, Marcal AR, Rozeira J. PH 2-A dermoscopic image database for research and benchmarking. In 2013 \n35th annual international conference of the IEEE engineering in medicine and biology society (EMBC); 2013, July. p. 5437–40. https://  \ndoi. org/ 10. 1109/ EMBC. 2013. 66107 79.\n 42. Combalia M, Codella NC, Rotemberg V, Helba B, Vilaplana V, Reiter O, et al. Bcn20000: dermoscopic lesions in the wild. Preprint arXiv: 1908. \n02288; 2019.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5319773554801941
    },
    {
      "name": "Health care",
      "score": 0.5053516030311584
    },
    {
      "name": "Unit (ring theory)",
      "score": 0.427426815032959
    },
    {
      "name": "Computer science",
      "score": 0.40685001015663147
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3731387257575989
    },
    {
      "name": "Psychology",
      "score": 0.3713243007659912
    },
    {
      "name": "Engineering",
      "score": 0.3030775487422943
    },
    {
      "name": "Electrical engineering",
      "score": 0.17679592967033386
    },
    {
      "name": "Economics",
      "score": 0.11071372032165527
    },
    {
      "name": "Mathematics education",
      "score": 0.10111752152442932
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ]
}