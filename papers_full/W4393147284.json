{
  "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models",
  "url": "https://openalex.org/W4393147284",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2101170428",
      "name": "Chang-Hun Lee",
      "affiliations": [
        "Korea Post",
        "Pohang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2361659717",
      "name": "Jungyu Jin",
      "affiliations": [
        "Pohang University of Science and Technology",
        "Korea Post"
      ]
    },
    {
      "id": "https://openalex.org/A2124570618",
      "name": "Tae-Su Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109905644",
      "name": "Hyungjun Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2951030768",
      "name": "Eunhyeok Park",
      "affiliations": [
        "Korea Post",
        "Pohang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101170428",
      "name": "Chang-Hun Lee",
      "affiliations": [
        "Convergence"
      ]
    },
    {
      "id": "https://openalex.org/A2361659717",
      "name": "Jungyu Jin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2109905644",
      "name": "Hyungjun Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2951030768",
      "name": "Eunhyeok Park",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2989530497",
    "https://openalex.org/W2944508492",
    "https://openalex.org/W4293166090",
    "https://openalex.org/W6893640197",
    "https://openalex.org/W6609175491",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3122499249",
    "https://openalex.org/W3020212829",
    "https://openalex.org/W6770056184",
    "https://openalex.org/W2883920103",
    "https://openalex.org/W2798504536",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4221159612",
    "https://openalex.org/W4297948009",
    "https://openalex.org/W2981751377",
    "https://openalex.org/W3202442802",
    "https://openalex.org/W2916954108",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2962761403",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W1999085092",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4283313765",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288026258",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3166859509",
    "https://openalex.org/W4287812978",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W2982041622",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4298422451"
  ],
  "abstract": "Large language models (LLMs) with hundreds of billions of parameters require powerful server-grade GPUs for inference, limiting their practical deployment. To address this challenge, we introduce the outlier-aware weight quantization (OWQ) method, which aims to minimize LLM's footprint through low-precision representation. OWQ prioritizes a small subset of structured weights sensitive to quantization, storing them in high-precision, while applying highly tuned quantization to the remaining dense weights. This sensitivity-aware mixed-precision scheme reduces the quantization error notably, and extensive experiments demonstrate that 3.1-bit models using OWQ perform comparably to 4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a parameter-efficient fine-tuning for task-specific adaptation, called weak column tuning (WCT), enabling accurate task-specific LLM adaptation with minimal memory overhead in the optimized format. OWQ represents a notable advancement in the flexibility, efficiency, and practicality of LLM optimization literature. The source code is available at https://github.com/xvyaward/owq.",
  "full_text": "OWQ: Outlier-Aware Weight Quantization\nfor Efficient Fine-Tuning and Inference of Large Language Models\nChanghun Lee1*, Jungyu Jin2*, Taesu Kim3, Hyungjun Kim3, Eunhyeok Park2\n1Department of Convergence IT Engineering, POSTECH\n2Graduate School of Artificial Intelligence, POSTECH\n3SqueezeBits Inc.\n{changhun.lee, jgjin0317}@postech.ac.kr, {taesu.kim, hyungjun.kim}@squeezebits.com, eh.park@postech.ac.kr\nAbstract\nLarge language models (LLMs) with hundreds of billions\nof parameters require powerful server-grade GPUs for in-\nference, limiting their practical deployment. To address this\nchallenge, we introduce the outlier-aware weight quantiza-\ntion (OWQ) method, which aims to minimize LLM‚Äôs foot-\nprint through low-precision representation. OWQ prioritizes\na small subset of structured weights sensitive to quantiza-\ntion, storing them in high-precision, while applying highly\ntuned quantization to the remaining dense weights. This\nsensitivity-aware mixed-precision scheme reduces the quan-\ntization error notably, and extensive experiments demonstrate\nthat 3.1-bit models using OWQ perform comparably to 4-\nbit models optimized by OPTQ. Furthermore, OWQ incorpo-\nrates a parameter-efficient fine-tuning for task-specific adap-\ntation, called weak column tuning (WCT), enabling accurate\ntask-specific LLM adaptation with minimal memory over-\nhead in the optimized format. OWQ represents a notable ad-\nvancement in the flexibility, efficiency, and practicality of\nLLM optimization literature. The source code is available at\nhttps://github.com/xvyaward/owq.\nIntroduction\nLarge language models (LLMs) (Brown et al. 2020; Radford\net al. 2019; Scao et al. 2022; Touvron et al. 2023; Zhang\net al. 2022) demonstrate impressive generation performance\non a wide range of complex language tasks, triggering ex-\nplosive growth in LLM-based applications. However, the ex-\ntensive memory and computational demands are major ob-\nstacles to the widespread use of LLMs, not only for train-\ning but for inference as well. For instance, using fp16, the\nGPT3-175B model necessitates approximately 330 GB of\nspace merely to store model parameters, which eventually\ncosts hundreds of thousands of dollars to build the system\nwith multiple server-grade GPUs. For the widespread adop-\ntion of LLMs, it is crucial to minimize such serving costs.\nRecently, weight quantization has emerged as an attrac-\ntive optimization method for LLMs (Frantar et al. 2023). By\nstoring parameters into low-precision representation, stor-\nage space can be considerably saved, and this also introduces\nperformance benefits by addressing memory bottlenecks and\n*These authors contributed equally.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nreduced communication costs (Park et al. 2023). Advanced\nstudies (Frantar et al. 2023; Park et al. 2023) have shown that\nmatrix multiplication with 3-bit weight and fp16 activation\nexhibits remarkable performance improvements on a single\nGPU compared to the case with fp16 weight and activation\non multiple GPUs. Weight quantization of LLMs could re-\nsolve the memory and performance issues of LLMs jointly.\nPreviously, OPTQ (Frantar et al. 2023), also known as\nGPTQ (Frantar et al. 2022), introduced a layer-wise post-\ntraining quantization (PTQ) method based on the optimal\nbrain compression (OBC) algorithm (Frantar and Alistarh\n2022). Notably, the compressed 3-bit OPT-175B model out-\nperforms the fp16 OPT-30B model, even though they have\nsimilar memory footprints. The compressed model now oc-\ncupies around 63 GB, allowing deployment on a single A100\nGPU. However, there is room for improvement. OPT-175B\nstill experiences some degradation with 3-bit quantization,\nand this effect is more pronounced in smaller models. Given\nthat various model sizes are optimal for different scenarios,\nmaintaining their accuracy remains highly important.\nOn the other hand, weight quantization broadens its\napplication to task-specific fine-tuning. For example,\nQLoRA (Dettmers et al. 2023) enables the fine-tuning of the\nquantized LLMs for target tasks by incorporating low-rank\nhigh-precision tensors into the quantized dense matrix. Dur-\ning this process, only the added tensors are updated, while\nthe quantized path remains unchanged. This method is gain-\ning attention as it allows for efficient fine-tuning with re-\nduced memory consumption while mitigating the drawback,\nquantization errors, during fine-tuning. However, the quality\nof the low-precision dense matrix is often overlooked, even\nthough its suboptimal quality could diminish the benefits\nof fine-tuning. Even for the adaptation, high-quality weight\nquantization is essential.\nIn this paper, we introduce a new weight quantiza-\ntion technique, Outlier-aware Weight Quantization (OWQ).\nOWQ is specifically designed considering the unique\ncharacteristics of LLMs, the presence of activation out-\nliers (Dettmers et al. 2022; Wei et al. 2022b; Xiao et al.\n2022). Our analysis reveals that these outliers play a key role\nin the quality degradation of weight quantization. Based on\nthis finding, we design OWQ that applies a mixed-precision\nquantization considering the sensitivity of each weight col-\numn. Extensive analysis indicates that the 3.1-bit OWQ\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13355\nHessian  ùëØùë¨=ùüêùëøùëøùëª\n‚àó =ùê∂#$\nOutlier-aware Weight Quantization\nWeak Columns in fp16\nOutput   ùíÄ'=ùëæ)ùëø\n‚àó\nWeak Columns Column IndexLow-precisionWeight\n+Sensitive Channels\n=\nùëø ùëøùëª ùëØùë¨\nùíÄ'\nùëæ) ùëø\nFreeze\n+ Task ATask BTask C\nTask-Specific Weak Column Tuning\nFine-tuning\nùê∂%&'\nùê∂#$Weight ùëæInput   ùëøùëÅ()*+,-(ùê∂#$\nActivation Outliers\n‚àÜùëå=ùëå‚àíùëå.\t\t\t\t\t\t\t=ùëäùëã‚àíùëä'ùëã\t\t\nQuantization Error\nBy Weak ColumnsBy other weights\n‚àÜùíÄ\nFigure 1: The overview of the proposed weak columns concept, Outlier-aware Weight Quantization (OWQ) scheme, and Weak\nColumn Tuning (WCT) scheme for efficient task-specific fine-tuning.\nmodel has comparable quality to the 4-bit OPTQ model.\nMoreover, we introduce an effective fine-tuning method\nbased on OWQ, called Weak Column Tuning (WCT); it\nshares the benefit of OWQ during fine-tuning and inference,\nso the network can be updated to the target task with min-\nimal memory overhead and accelerated during inference.\nFurthermore, the WCT-based fine-tuning model adapts with\nfewer trainable parameters than existing methods and out-\nperforms them because of the superior representation qual-\nity of the weights quantized by OWQ. To our knowledge,\nthis is the first study to consider the existence of activation\noutliers in extremely low-precision weight quantization and\nclosely integrate it with fine-tuning.\nBackground and Related Works\nQuantization and LLMs\nQuantization is a widely used optimization technique aimed\nat exploiting the benefit of low-precision while maintain-\ning the quality of the network. While its primary benefit is\nsize reduction, quantization can also substantially accelerate\nperformance through support for low-precision operations.\nHowever, a trade-off exists: quantization can lead to qual-\nity degradation, which numerous studies have aimed to ad-\ndress. Early studies focused on quantization-aware training\n(QAT) (Esser et al. 2019; Zhou et al. 2016), which tried to\nrestore quality through additional training; however, as the\nunderstanding of quantization grew and various techniques\nemerged, post-training quantization (PTQ) (Li et al. 2021;\nNagel et al. 2019, 2020; Wei et al. 2022a) have been actively\nstudied, enabling quality preservation without training.\nDue to the LLM‚Äôs necessity of significant storage space\nand computational resources, it is crucial to apply optimiza-\ntion via quantization. In general, QAT has been favored for\nextremely low-bit precision to minimize quantization error.\nHowever, it is less favorable for LLM quantization because\nof the high cost of the training environment. Instead, PTQ\nhas emerged as an important topic for LLM quantization.\nThis field has two distinct approaches: one aims to quantize\nboth activations and weights to int8 (Dettmers et al. 2022;\nXiao et al. 2022), considering both capacity reduction and\nacceleration. In contrast, the second approach focuses solely\non weight quantization to sub-4-bit precision (Frantar et al.\n2023; Park et al. 2023). In this paper, we align our work with\nthe latter approach. While concentrating on weight quanti-\nzation, we devise a novel quantization scheme, drawing sig-\nnificant inspiration from int8-related research on activation\nquantization.\nInt8 Quantization for Activation and Weight\nInt8 multiplication can provide up to 2x performance im-\nprovements and more than 5x energy consumption reduc-\ntion compared to fp16 baselines (Horowitz 2014). Numer-\nous studies (Dettmers et al. 2022; Xiao et al. 2022) aim to\nquantize both activation and weight to int8 for matrix multi-\nplication operations in LLMs. However, those studies iden-\ntify a unique challenge of LLMs for activation quantization.\nLLMs exhibit a few outliers in intermediate activations, with\nvalues significantly larger than others, and these outliers are\nconcentrated in specific feature dimensions. Preserving the\nvalues of these outliers is known to be crucial for maintain-\ning accuracy after activation quantization.\nIn this study, while only weight quantization is applied,\nwe figure out that the presence of activation outliers still im-\npacts the sensitivity of weight quantization. We also demon-\nstrate that considering activation outliers is essential for ac-\ncurate weight quantization.\nOPTQ: Weight Quantization for LLMs\nOPTQ (Frantar et al. 2023) is the state-of-the-art research\nin the field of weight quantization for LLMs. It is based\non Optimal Brain Compression (OBC) (Frantar and Alistarh\n2022), which employs element-wise quantization (pruning)\nand compensation, using a Hessian-based metric of layer-\nwise quantization errors (Eq. (1) and Eq. (2)). This approach\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13356\ndiffers from previous studies, which applied quantization\nthrough gradient descent based on the straight-through es-\ntimator (Li et al. 2021; Wei et al. 2022a) or the rounding-to-\nnearest mechanism (Dettmers et al. 2022).\nwq = argminwq\n(quant(wq) ‚àí wq)2\n[H‚àí1\nF ]qq\n, (1)\nŒ¥F = ‚àíwq ‚àí quant(wq)\n[H‚àí1\nF ]qq\n¬∑ (H‚àí1\nF ):,q (2)\nOPTQ has optimized OBC to parallelize quantization for\neach element of the output channel dimension, enabling\nrapid quantization. While it showcases the potential of sub-\n4-bit quantization for LLMs, reducing the model size or in-\ncreasing the problem‚Äôs complexity results in decreased ac-\ncuracy compared to the fp16 baselines. In this paper, we\nsuggest selectively applying high-precision to weights that\nare vulnerable to quantization caused by activation outliers\nand applying the OPTQ to the remaining weights with the\nmodification based on quantization configuration tuning for\nadditional error reduction. These enhancements can signif-\nicantly reduce the quantization error while preserving the\nquantization speed of OPTQ.\nParameter-Efficient Fine-Tuning (PEFT)\nFine-tuning LLMs for specific tasks can improve perfor-\nmance on unseen or complex tasks, but the large number\nof parameters requires a hyperscale computation system,\nwhich is often impractical due to high costs. Therefore,\nParameter-Efficient Fine-Tuning (PEFT) schemes have been\nintroduced to address this issue. LoRA (Hu et al. 2022) ex-\nemplifies PEFT by freezing pre-trained weights but incorpo-\nrating a small fraction of learnable parameters through low-\nrank decomposition. As only the added parameters are up-\ndated, LoRA can be adapted using significantly less mem-\nory. QLoRA (Dettmers et al. 2023) further reduces mem-\nory consumption by replacing dense weights with quantized\nweights, making the fine-tuning process more lightweight.\nSince fine-tuning can mitigate quantization errors, QLoRA\nemerges as a desirable optimization for the practical, task-\nspecific deployment of LLMs. In this study, we introduce\nan OWQ-compatible PEFT scheme. The superior represen-\ntation quality of OWQ yields exceptional performance after\nPEFT with lower resource overhead than QLoRA.\nProblem Definition and Motivation\nIn this section, before introducing our idea, we first define\nthe problem and explain our findings clearly. The proposed\nOWQ is designed to apply layer-wise uniform quantization\nfor the weights of LLMs with minimal quality degradation.\nWhen an input feature X ‚àà RCin√óN is given, where Cin\nrepresents the number of input channels and N is the se-\nquence length of the input, the full-precision weight ma-\ntrix W ‚àà RCout√óCin for Cout output features are mapped\nto low-precision toward minimizing the difference of out-\nput activations before and after quantization. The objective\nfunction to find the quantized weight ÀÜW that minimizes the\nsquared error is defined as follows:\narg min\nÀÜW\nE = arg min\nÀÜW\n||W X‚àí ÀÜW X||2\n2 s.t. C( ÀÜW) < Ct,\n(3)\nwhere C(¬∑) represents the compression ratio and Ct is the\ntarget compression ratio. The layer-wise quantization pro-\ncess is applied sequentially from the model input to the out-\nput, ensuring the quantization of all weights in the model.\nWe keep embedding and head weights with full-precision.\nLayer-wise Quantization and Hessian of Weights\nIn this subsection, we explain our insight concerning the re-\nlationship between weight sensitivity and activation outliers\nin the context of quantization: weights linked to activation\noutliers are particularly susceptible to quantization. This un-\nderstanding serves as the core motivation of OWQ.\nInitially, we restructure the squared error term in Eq. (3)\nto represent the sum of squared errors for each output\nchannel within the weight matrix, resulting in the equation\nŒ£Cout\ni=1 ||Wi,:X ‚àí ÀÜWi,:X||2\n2. This decomposition distinctly\nshowcases that the overall error is divided into individual\nerrors for each output channel. With the modified equation,\nour focus shifts to two key aspects. Firstly, it is important\nto note that there is no Hessian interaction between out-\nput channels. Specifically, the individual Hessians with re-\nspect to the layer-wise quantization error, denoted asH(i) ‚àà\nRCin√óCin , have an identical value as:\nH(i) = H = ‚àÇ2Ei\n‚àÇW 2\ni,:\n= 2XX T . (4)\nSecondly, as observed in previous studies (Nagel et al.\n2020), the individual error term can be approximated using\nTaylor expansion. By setting‚àÜWi,: = Wi,: ‚àí ÀÜWi,:, the error\nfor the i-th output channel can be expressed as follows:\nEi = ||Wi,:X ‚àí ÀÜWi,:X||2\n2 ‚âà ‚àÜWi,:H‚àÜWT\ni,:. (5)\nThe equation shows that in the context of layer-wise quanti-\nzation, the output error can be directly related to the Hessian\nand the magnitude of weight perturbation.\nKeeping these observations in mind, we can derive an\ninteresting insight by acknowledging the presence of acti-\nvation outliers in LLMs. Previous studies (Dettmers et al.\n2022; Xiao et al. 2022) have reported that certain feature\ndimensions of LLM activation contain outliers with signifi-\ncantly larger values than others. As shown in Figure 1 Left,\nthese activation outliers make some elements of H have ex-\nceptionally high values. This abnormal surge in Hessian val-\nues increases the corresponding weight channels‚Äô sensitivity\nto quantization. In detail, as indicated in Eq. (5), even when\nthe same degree of weight perturbation is present, the ensu-\ning change in output can be considerably larger due to some\nlarge elements of H. We refer to the weights susceptible to\nquantization as weak column, specifically those associated\nwith the activation outliers in a specific input channel.\nTherefore, if we quantize all weights to the same bit-width\nduring the weight quantization process, the quantization er-\nror from the weak columns corresponding to the activation\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13357\nFigure 2: Cumulative error for each input channel. The x-\naxis channel indices are sorted in ascending order based on\ntheir influence on the output error.\noutliers can lead to substantial perturbation on the output,\nresulting in a notable quantization error. Figure 2 supports\nthis assertion, indicating that a large portion of the error\noriginates from a limited number of channels, which align\nwith the weak columns. To minimize the error from weak\ncolumns, it‚Äôs imperative to specially address these columns.\nOWQ: Outlier-aware Weight Quantization\nTo tackle this issue, we introduce a novel concept termed\nOutlier-aware Weight Quantization (OWQ). OWQ encom-\npasses two steps: initially, it identifies the weak columns and\nexcludes them from quantization. Subsequently, it quantizes\nthe remaining weights to extreme low-precision using metic-\nulously tuned quantization parameters. The overview of the\nproposed OWQ scheme is illustrated in Figure 1 Left. In this\nsection, we thoroughly discuss the details of the OWQ im-\nplementation.\nIn Eq. (4), we highlighted the relationship between the\nHessian matrix and sensitivity caused by activation outliers.\nWe also demonstrated that the final error is influenced by the\nquadratic terms of perturbations with the Hessian matrix in\nEq. (5). Building on these insights, we define the sensitivity\nof j-th weight column as follows:\nsensitivityj = Œªj||‚àÜW:,j||2\n2, (6)\nwhere Œªj is the j-th diagonal element of the Hessian ma-\ntrix. By analyzing the sensitivity of individual columns, we\ncan effectively identify the weak columns that are vulnera-\nble to quantization and require higher precision. When the\ngoal is to select a specific number (k ) of weak columns,\nthe proposed metric is utilized to choose the top-k sensitive\ncolumns based on their sensitivity values.\nIt is worth noting that hessian-based metrics have been of-\nten used in previous studies (Dong et al. 2019, 2020). While\nour work has a distinctive difference in the granularity of the\nquantization domain and detailed expression, our observa-\ntion is well aligned with the intuition of the existing studies.\nFollowing the selection of weak columns, the remaining\nweights are quantized into low-precision. Any low-precision\nquantization scheme is applicable, but we employ OPTQ.\nSince OPTQ also utilizes sequential column-wise quantiza-\ntion, the weights excluding the weak columns can be seam-\nlessly integrated into the OPTQ framework. Additionally,\nplease note that the weak columns can be used to further\nmitigate errors that occur during the OPTQ process. OPTQ\nFigure 3: Min-max range of the weights in each column\n(blue dots) and the selected weak columns (red dots).\nupdates the remaining unquantized weights to compensate\nfor the errors caused by quantizing the weight columns in\nthe current step, as shown in Eq. (2). By rearranging the\nhigh-precision weak columns to the end of the weight be-\nfore utilizing OPTQ, quantization errors from other columns\nduring the OPTQ process can be largely compensated for by\nthe weak columns. Since the weak columns are retained with\nfull-precision, these compensated values can be preserved\neven if all other columns are quantized.\nAfter identifying the weak columns and quantizing the re-\nmaining weights, we store the weak columns as fp16 and use\nan extra single integer per column, which represents the col-\numn index of the weak columns. In addition, we store a low-\nprecision matrix whose positions of weak columns are zero-\nfilled. Therefore, compared to OPTQ, the additional storage\noverhead is solely caused by the weak columns. This over-\nhead is negligible (‚âà 0.3%), while the accuracy is signifi-\ncantly improved. In addition, we also provide the specialized\nacceleration for OWQ format on real GPU. A comprehen-\nsive explanation of the implementation will be provided in\nthe ‚ÄúAcceleration on Real Device‚Äù section.\nAdditionally, as depicted in Figure 3, our approach is\ndistinctly different from existing outlier-aware quantization\nstudies (Park, Kim, and Yoo 2018; Park, Yoo, and Vajda\n2018). While the naming is similar, the outcome is com-\npletely different. In those studies, outlier weights are ex-\ncluded from quantization based solely on their magnitude\nto minimize the error of weights before and after the quanti-\nzation. However, our method minimizes the error of output\nactivation. As shown in the figure, the weights are selected\nbased on their sensitivity rather than their magnitude.\nQuantization Configuration Search\nWe mentioned that we use OPTQ for the quantization of\nthe dense weights except for the weak columns. However,\nwe made an important modification to OPTQ to further re-\nduce quantization error. While OPTQ originally relies on a\nstraightforward min-max quantization approach, we have in-\ncorporated the benefits of truncation. Many previous stud-\nies (Esser et al. 2019; Li et al. 2021; Nahshan et al. 2021;\nWei et al. 2022a) pointed out that the truncation reduces the\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13358\noverall quantization error significantly, improving the qual-\nity of output significantly. In OWQ, we searched the quanti-\nzation configurations, such as step size and zero point using\na simple 2D grid search, to narrow the quantization range.\nThe optimal values of the parameters that minimize the dif-\nference before and after quantization are searched via round-\ning to the nearest with truncation, and we apply OPTQ on\ntop of the searched values.\nThis modification substantially reduces the overall recon-\nstruction error, significantly improving quality. In OWQ, the\nWikiText-2 perplexity is lowered from 12.14 to 11.21 for\nOPT-6.7B. An interesting observation is that when trun-\ncation is applied to conventional OPTQ, output quality\ndegrades, with the WikiText-2 perplexity increasing from\n12.88 to 48.26 in OPT-6.7B. We have observed that the weak\ncolumns in the key and query layers of transformer blocks\nhave exceptionally large values. Truncating these leads to\nsubstantial error, resulting in a significant reduction in ac-\ncuracy. However, with our approach, this error is avoided\nbecause weak columns are retained as full-precision. We\nargue that this enhancement is a significant advantage of\nOWQ, notably improving the representation quality of the\nlow-precision matrix.\nPEFT with Weak Column Tuning\nWhen applied to pre-trained LLMs, OWQ results in minimal\nquality degradation, allowing the model to maintain its zero-\nshot generative performance. Nonetheless, to further boost\noutput quality, fine-tuning might be necessary for newly in-\ntroduced tasks. In this context, we present a Weak Column\nTuning (WCT) scheme, illustrated in Figure 1 Right. WCT\nfirst quantizes the base model with OWQ and then fine-tunes\nonly the weak columns that retained high-precision as a re-\nsult of OWQ. In OWQ, we identify weak columns using\nchannel-wise sensitivity and retain them in high-precision,\nmeaning slight alterations in these sensitive columns have\na substantial impact on the output. Moreover, because they\nare high-precision (fp16), the values can be freely modified.\nEmpirically, we‚Äôve found that updating these weak columns\nfacilitates the adaptation of the quantized network to the de-\nsired task. It‚Äôs worth noting that the low-precision dense ma-\ntrix remains static during this update.\nBecause learnable weak columns represent only a small\nfraction of the overall parameters, updates also result in\nnegligible memory overhead. Furthermore, the OWQ for-\nmat is preserved after fine-tuning, enabling us to leverage\nthe benefits of acceleration with the customized kernel for\nOWQ. Additionally, by allocating per-task weak columns,\nwe can serve multiple task-specific models with minimal\nmemory overhead through shared utilization of the dense\nlow-precision matrix, which constitutes the majority of the\nmemory footprint.\nThe versatile adaptability of OWQ broadens its range of\napplications. Furthermore, as will be shown in the ‚ÄúExperi-\nment‚Äù section, WCT surpasses leading fine-tuning methods\nin terms of both memory usage and output quality. This is\nbecause OWQ improves the quality of dense low-precision\nrepresentation, and sensitive columns are pre-picked and\nused as the update target. These factors enhance the qual-\nity of the fine-tuned network significantly. Please refer to\nthe ‚ÄúComparison of PTQ Methods used in WCT‚Äù section\nfor details.\nExperiments\nExperimental Setup\nTo validate the outstanding performance of our proposed\nmethod, we present quantization results for large-scale\nLLMs such as OPT (Zhang et al. 2022) and LLaMA (Tou-\nvron et al. 2023) families. Our primary baseline is OPTQ,\nso we apply identical experimental settings of it. For in-\nstance, our calibration dataset consists of 128 random 2048\ntoken segments from the C4 dataset (Raffel et al. 2020). Ex-\nperiments were conducted on a single NVIDIA A100 GPU\nwith 80 GB of main memory or RTX3090 GPU with 24 GB\nmemory. Like OPTQ, our method quantizes the target model\nwithout re-training. To measure the zero-shot or few-shot\nperformance, we utilize an open-source evaluation repos-\nitory, EleutherAI/lm-evaluation-harness (Gao et al. 2023).\nPlease note that we report the numbers with an error mar-\ngin based on 3 experiments with different seeds.\nCompared to the baselines of 3-bit and 4-bit OPTQ re-\nsults, we present two variants, with an extra 0.01 bit and 0.1\nbit overhead, respectively. For the 4-bit baseline, we found\nthat 0.01 bit overhead is sufficient. The additional storage\narea of extra bit is evenly distributed across the linear layers\nwithin the transformer block. For instance, in the OPT model\nwhich has six linear layers (key, query, value, out, fc1, and\nfc2), the weak columns of the key layers contribute an av-\nerage of 0.00167 bit in the 3.01-bit configuration (0.15%\ncolumns of the key weight matrix). This extra bit covers\nthe all overhead of the mixed-precision representation. If\nwe quantize OPT-175B model with an average of 3.01 bits,\nit will require approximately 260 MB of additional storage\ncompared to the 3-bit OPT-175B OPTQ model, which uti-\nlizes around 63.1 GB of storage space. All experiments were\nconducted using the PyTorch 2.0 (Paszke et al. 2019) frame-\nwork with HuggingFace integration (Wolf et al. 2019).\nWhile not discussed in OPTQ papers, the ‚Äúact-order‚Äù\n(AO) option was recently added to their official GitHub. This\noption quantizes columns in order according to the activa-\ntion magnitude and is distinct from the earlier OPTQ ap-\nproach that quantized columns sequentially. Similarly, the\n‚Äútrue-sequential‚Äù (TS) option was introduced, which applies\nquantization sequentially, taking into account the quantiza-\ntion error from previous layers within a block. Both meth-\nods boost the accuracy of OPTQ, and for a comprehensive\ncomparison, we provide OPTQ results with ‚ÄúTS + AO‚Äù con-\nfiguration by default. We don‚Äôt use these options for OWQ\nunless otherwise noted.\nResults of Perplexity Measure\nThe accuracy of the proposed model is assessed through the\nevaluation on multiple language tasks, including WikiText-\n2 (Merity et al. 2016), Penn Treebank (PTB) (Marcus et al.\n1994), and C4. Perplexity-based tasks are particularly sen-\nsitive to model quantization (Frantar et al. 2023), with per-\nplexity numbers serving as indicators of the generative per-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13359\nOPT Bits 125M 350M 1.3B 2.7B 6.7B 13B 30B 66B\nfull 16 27.65 22.00 14.63 12.47 10.86 10.13 9.56 9.34\nRTN 4 37.28 25.94 48.17 16.92 12.10 11.32 10.98 110\nOPTQ 4 32.05 23.87 15.47 12.83 11.14 10.29 9.57 9.34\nOWQ 4.01 29.47‚àó\n¬±.17 23.19‚àó\n¬±.16 15.01¬±.06 12.39¬±.02 10.87¬±.02 10.26¬±.02 9.50¬±.01 9.25¬±.04\nOPTQ 3 53.43 32.28 20.90 16.55 12.88 11.58 10.29 9.90\nOWQ 3.01 35.26‚àó\n¬±.61 26.59‚àó\n¬±.51 16.40¬±.15 13.21¬±.09 11.21¬±.05 11.48¬±.03 9.61¬±.02 9.28¬±.03\nOWQ +TS+AO\n3.01 35.19‚àó\n¬±.68 25.86‚àó\n¬±.25 15.96¬±.18 13.28¬±.06 11.20¬±.05 11.14¬±.02 9.66¬±.06 9.31¬±.01\nOWQ\n3.1 33.41¬±.25 26.00¬±.14 15.39¬±.06 12.98¬±.10 11.14¬±.03 10.38¬±.01 9.57¬±.02 9.30¬±.02\nTable 1: OPT WikiText-2 perplexity (‚Üì). For the results with *, we used an extra 0.05 bits instead of 0.01 bits; there are few or\nno weak columns in the budget of 0.01 bits due to the small model dimension. TS: True-sequential and AO: Act-order options.\nLLaMA Bits 7B 13B 30B 65B\nfull 16 5.68 5.09 4.10 3.53\nRTN 4 6.29 5.53 4.54 3.92\nOPTQ 4 6.10 5.36 4.45 4.10\nOWQ 4.01 5.94¬±.02 5.25¬±.00 4.25¬±.00 3.74¬±.02\nOPTQ 3 8.13 6.67 5.67 5.41\nOWQ 3.01 6.66¬±.04 5.66¬±.01 4.75¬±.02 4.25¬±.01\nOWQ 3.1 6.41¬±.01 5.56¬±.01 4.63¬±.01 4.09¬±.01\nTable 2: LLaMA WikiText-2 perplexity (lower is better).\nformance of the quantized model. The results for WikiText-2\ncan be found in Table 1 and Table 2.\nThe results clearly demonstrate that OWQ consistently\ndelivers substantial quality improvements across the LLM\nfamilies, irrespective of the model size. The 3.01-bit OWQ\nmodel effectively mitigates the quality degradation observed\nin the 3-bit OPTQ model, while the 3.1-bit model achieves\ncomparable performance to the 4-bit OPTQ model. Further-\nmore, OWQ 4.01-bit yields noteworthy improvements, high-\nlighting the significance of treating weak columns. These\nresults underscore the importance and effectiveness of our\napproach in preserving model quality after quantization.\nAn interesting finding is the significant improvement in\nmodel quality for OPT models with less than 13 billion pa-\nrameters when applying OWQ. Although previous studies\nhave highlighted the presence of activation outliers in mod-\nels with more than 6.7 billion parameters (Dettmers et al.\n2022), even smaller models with moderately large channels\ncan still benefit from mixed precision quantization. This sug-\ngests that the concept of weak columns remains valid and ef-\nfective in enhancing the quality of LLMs, regardless of the\nmodel size.\nIn addition, please note that OWQ consistently outper-\nforms OPTQ with TS + AO, but TS + AO doesn‚Äôt give\nperformance benefits to OWQ (Table 1). Although no the-\noretical interpretation for those options was proposed, our\nstudy suggests that the benefits of ‚Äúact-order‚Äù arise from\nsensitivity-aware quantization. This means that applying se-\nquential quantization beginning with sensitive columns im-\nproves performance for OPTQ. However, act-order alone\ncannot sufficiently mitigate the quality degradation caused\nby weak columns within a low-precision domain. Interest-\ningly, the benefit of TS is also mitigated as quantization error\nis already greatly reduced by using OWQ.\nResults of Various Few-shot Tasks\nWe conducted additional experiments on diverse few-shot\nlanguage tasks. Referring to the ‚ÄúOpen LLM Leaderboard‚Äù\n(Beeching et al. 2023) from the Huggingface H4 team as\na benchmark, we relied on the average scores from ARC-\nchallenge (25-shot) (Clark et al. 2018), Hellaswag (10-shot)\n(Zellers et al. 2019), and MMLU (5-shot) (Hendrycks et al.\n2021). In particular, MMLU is a collection of 57 tasks.\nThese benchmarks were chosen because they test a range\nof reasoning and general knowledge across diverse fields, in\nfew-shot contexts. The average scores of Table 3 and Table 4\nconvince us that the proposed OWQ is consistently superior\nto OPTQ in various model sizes, from 125m to 66B. In par-\nticular, we can see the considerable gap between OPTQ 3-bit\nand OWQ 3.01-bit for the LLaMA families in Table 4 de-\nspite using the TS and AO options for OPTQ. Our method‚Äôs\nstrength lies in its universality, consistently boosting the per-\nformance of generative models with minimal storage over-\nhead.\nAcceleration on Real Device\nTo demonstrate the advantages of low-precision accelera-\ntion, we developed a customized CUDA kernel for OWQ\nand assessed its latency overhead on an A100 GPU. First, we\ndecompressed the low-precision matrix into the fp16 format\nand performed dense GeMV multiplication. At this stage,\nthe overhead is identical to that of OPTQ‚Äôs customized ker-\nnel. Additionally, we select the activation input channels for\nthese weak columns on-the-fly and use another GeMV ker-\nnel specifically for them. This approach effectively avoids\nthe issues of irregular memory access. Table 5 displays the\nkernel overhead of OWQ 3.01-bit for various model sizes.\nThe mixed-process computation adds up to only 3.21% la-\ntency compared to the 3-bit acceleration of the OPTQ ker-\nnel on the LLaMA 7B model, and the overhead is generally\namortized for larger models.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13360\nOPT Bits 125M 350M 1.3B 2.7B 6.7B 13B 30B 66B\nfull 16 26.78 28.77 36.34 40.27 44.22 45.34 47.97 49.86\nOWQ 4.01 26.67‚àó\n¬±.29 28.85‚àó\n¬±.11 36.48¬±.19 39.75¬±.05 43.94¬±.03 45.42¬±.14 47.83¬±.22 49.57¬±.07\nOPTQ\n4 26.43 28.31 36.41 39.66 43.69 44.97 47.58 49.47\nOWQ 3.1 25.99¬±.28 27.77¬±.23 35.86¬±.13 38.93¬±.19 42.80¬±.07 44.50¬±.22 47.20¬±.28 49.02¬±.25\nOWQ 3.01 26.18‚àó\n¬±.40 27.91‚àó\n¬±.17 35.51¬±.24 38.79¬±.81 42.51¬±.26 44.62¬±.13 47.17¬±.20 48.84¬±.03\nOPTQ\n3 26.00 27.78 34.01 37.16 41.63 43.70 46.65 48.01\nTable 3: The average value of three few-shot scores (%): ARC-challenge, Hellaswag, and MMLU for OPT families (higher is\nbetter). For the results with *, we used an extra 0.05 bits instead of 0.01 bits due to the small model dimension.\nLLaMA Bits 7B 13B 30B 65B\nfull 16 54.81 61.56 68.21 71.17\nOWQ 4.01 54.01¬±.04 60.54¬±.52 68.12¬±.15 70.30¬±.06\nOPTQ 4 52.91 59.90 66.96 69.92\nOWQ 3.1 51.93¬±.48 58.84¬±.36 66.18¬±.11 69.26¬±.29\nOWQ 3.01 50.21¬±.45 57.69¬±.21 66.21¬±.25 68.82¬±.29\nOPTQ 3 47.06 52.87 61.15 65.75\nTable 4: The average value of three scores (%): ARC-\nchallenge, Hellaswag, and MMLU for LLaMA families.\nOPT 6.7B 13B 66B LLaMA 7B 13B 65B\nk/q/v/o 3.07 3.11 2.84 k/q/v/o 3.21 3.47 2.34\nfc1 2.31 1.53 1.65 up/gate 2.34 2.01 1.65\nfc2 2.15 2.04 2.23 down 2.03 2.21 2.03\nTable 5: Kernel overhead of OWQ 3.01-bit (vs OPTQ) (%).\nQuantization Speed\nFor LLM quantization, the quantization algorithm‚Äôs speed is\ncrucial. While OWQ adds operations for weak column selec-\ntion and hyperparameter tuning compared to OPTQ, sharing\nthe Hessian with OPTQ minimizes OWQ‚Äôs overhead. Fur-\nthermore, applying the true-sequential option to OPTQ adds\nextra runtime for OPTQ, further reducing the gap with OWQ\nquantization time. On A100 GPU, OWQ can quantize a 66B\nmodel in under 3 hours, presenting its practicality.\nResults of WCT-based Fine-tuning\nTo validate the superior performance of Weak Column Tun-\ning (WCT) for task-specific adaptation, we fine-tuned the\nLLaMA 7B model with 4-bit quantization using WCT and\nthen compared the results. To obtain the results of QLoRA\n(Dettmers et al. 2023), we used the official checkpoint. For\na fair comparison, we used the same subset of the OpenAs-\nsistant (K¬®opf et al. 2023) dataset that QLoRA employed. We\nevaluated performance by inputting the results generated by\nboth models for 80 questions from the Vicuna Benchmark\n(Chiang et al. 2023) into GPT-4 (OpenAI 2023) to determine\nwhich was better. As reported in a previous work (Dettmers\net al. 2023), we found a bias in which GPT-4 favors the sys-\ntem that appears first in a given pair of systems, giving it a\n68\n146\n71\n78\n72\n81\n26\n10\n25\n29\n30\n25\n66\n4\n64\n53\n58\n54\n0 40 80 120 160\nWCT r=8\nWCT r=8\nWCT r=8\nWCT r=16\nWCT r=32\nWCT r=64\nWCT Won Tied Lost\n*\nQLoRA\nQLoRA\nQLoRA\nQLoRA\nLLaMA 7B\nLoRA\nFigure 4: GPT-4 based analysis of WCT-based fine-tuning.\n* denotes the base model (without fine-tuning).\nhigher score. To eliminate this bias, we evaluate both order-\ning cases and report results for a total number of 160 evalu-\nations. LoRA (Hu et al. 2022) and QLoRA both use the 64\nrank of adapter modules. We used nucleus sampling with p\n= 0.9 and temperature 0.7 for all generations.\nIn the WCT experiments, r = k means the configu-\nration with r weak columns for each layer. As depicted\nin Figure 4, WCT with 64 weak columns ( r = 64 ) sur-\npasses the QLoRA. In other words, GPT-4 evaluated the tun-\ning results using WCT as better more often (81 vs 54 for\nr = 64 case). Remarkably, WCT with just 8 weak columns\n(takes only 6.8% of learnable parameters vs. QLoRA) out-\nperforms QLoRA and yields results comparable to full-\nprecision LoRA. As the quality of the compressed weight is\non par with the full-precision model, OWQ combined with\nWCT delivers performance matching that of full-precision\nLoRA only with 24.4% of overall memory usage during in-\nference. WCT only updates weak columns, which are highly\nsensitive to update. This feature makes WCT compensate\naccuracy with a smaller rank than the conventional LoRA.\nWith OWQ + WCT, we can enjoy the benefits of quantiza-\ntion not only in inference but also in task-specific adaptation.\nComparison of PTQ Methods used in WCT\nWe compare several quantization methods used for quantiz-\ning fixed dense weights in WCT and verify that the sophis-\nticated quantization method is important for performance\nafter fine-tuning. After quantizing the LLaMA-7B model\nwith different quantization methods, the models refined with\nWCT are asked to generate answers to questions from the\nVicuna Benchmark, and the quality of the answers is com-\npared using GPT-4-based evaluation. For all quantization\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13361\n76\n74\n71\n64\n68\n31\n28\n25\n27\n31\n53\n58\n64\n69\n61\n0 40 80 120 160\nWCT w/ OWQ\nWCT w/ OWQ\nWCT w/ OWQ\nWCT w/ minmax OPTQ\nWCT w/ minmax RTN\nLeft Won Tied Right Won\nQLoRA\nQLoRA\nQLoRA\nWCT w/ minmax RTN\nWCT w/ minmax OPTQ\nFigure 5: Comparison of fine-tuned performance of several\npost-training quantization methods used for WCT.\nMethod / Bits / Group OPT-6.7B\nWiki2 PTB C4 lamb. ‚Üë\nOPTQ / 3 12.76 19.35 14.55 60.15\nOPTQ / 3 / g1024 12.05 18.01 13.89 64.44\nOPTQ / 3 / g128 11.55 17.26 13.43 64.96\nOWQ / 3.01 11.22 16.32 13.23 69.94\nOWQ / 3.01 / g1024 11.18 16.32 13.19 68.91\nOWQ / 3.01 / g128 11.16 16.23 13.10 68.61\nMethod / Bits / Group LLaMA 7B\nWiki2 PTB C4 winog. ‚Üë\nOPTQ / 3 8.08 14.13 10.26 64.33\nOPTQ / 3 / g1024 7.15 12.75 9.12 66.26\nOPTQ / 3 / g128 6.56 12.48 8.36 67.60\nOWQ / 3.01 6.65 12.47 8.62 67.05\nOWQ / 3.01 / g1024 6.61 12.05 8.49 67.60\nOWQ / 3.01 / g128 6.40 11.72 8.18 69.57\nTable 6: The results of OWQ and OPTQ with group-wise\nquantization. lamb. = lambada, winog. = winogrande.\nmethods, we used r = 8 as the number of weak columns and\nlinear asymmetric quantization with the per-channel gran-\nularity. Although the performance loss due to quantization\ncan be compensated by fine-tuning, Figure 5 shows that the\nquality of the quantized model before fine-tuning affects the\nperformance after fine-tuning.\nComparison with Group-wise Quantization\nApplying uniform quantization at fine-grained granularity\nsignificantly reduces quantization error while introducing\nsome storage overhead for quantization hyperparameters.\nOPTQ utilizes this approach by dividing row vectors into\ngroups (e.g., group size of 128 or 1024) and applying uni-\nform quantization independently with different configura-\ntions. This expansion can be applied orthogonally to OWQ,\nso we can combine it with OWQ to assess any improve-\nments. Results in Table 6 show that the improvement from\nfine-grained quantization is negligible, as OWQ already\nsubstantially enhances the 3-bit model‚Äôs quality. Moreover,\ncompared to grouped OPTQ with 128 group size, 3.01-bit\nOWQ‚Äôs storage overhead is only about 10% of grouped\nOPTQ overhead while achieving comparable or better per-\nplexity and zero-shot accuracy. Thus, OWQ is a superior so-\nlution to the grouping technique.\nEquation Œªj||‚àÜW:,j||2\n2 Œªj ||‚àÜW:,j||2\n2 Œ£|W:,j|\nOPT-6.7B 11.23 11.25 13.08 14.77\nLLaMA 7B 6.64 6.76 11.57 10.12\nTable 7: LLaMA WikiText-2 perplexity (lower is better).\nEff. bit 3.005 3.01 3.05 3.10 3.20 3.30\nWiki-2 (‚Üì) 11.38 11.22 11.19 11.17 11.15 11.13\nLat. (%) 2.89 2.91 4.19 6.21 6.86 8.69\nMem. (%) 0.21 0.41 2.04 3.95 7.60 10.95\nTable 8: Effective bit-width sweep using OPT-6.7B model.\nLat. = Latency Overhead, Mem. = Memory Overhead.\nWeak Column Selection Metrics\nIn this paper, we propose to use both the Hessian ma-\ntrix and weight perturbations for weak column selection\n(Eq. (6)), to minimize layer output error. Table 7 addition-\nally presents perplexity results for various weak column\nselection metrics. It is obvious that the results for the 1st\n(Œªj||‚àÜW:,j||2\n2, our proposed selection metric) and 2nd (Œª j,\nHessian only) columns are considerably better than those for\nthe 3rd (|| ‚àÜW:,j||2\n2, weight error only). This indicates that\n(1) minimizing layer-wise error is a valid objective for fi-\nnal accuracy, and (2) it is necessary to account for all factors\ncontributing to output activation error, rather than simply fo-\ncusing on minimizing weight error.\nVarying Ratios of Weak Columns\nWe report the trade-offs in perplexity, latency, and memory\nusage for varying ratios in Table 8. These overhead results\nare compared to the OPTQ 3-bit baseline. With just 3.005\nbits, the model already surpasses the OPTQ 3-bit perfor-\nmance with negligible overhead. However, as the bit width\nincreases, the performance gain saturates while overhead\nkeeps increasing. As activation outliers are limited to only a\nfew dimensions, even a small number of weak columns can\nlead to noticeable performance improvements. In the main\ntables, we focused on two specific ratios, 3.01 and 3.1-bit,\nas they effectively illustrate the trend.\nConclusion\nThe presence of activation outliers has been identified as\na significant challenge in LLM activation quantization. We\nfound that even in weight quantization, activation outliers\ncan increase the sensitivity of certain weight columns, lead-\ning to significant quality degradation in a low-precision do-\nmain. To overcome this, we introduced a novel quantiza-\ntion scheme, OWQ. Compared to existing 3-bit quantization\nmethods, OWQ improves quality notably with only negligi-\nble storage and computation overhead, sustaining the ben-\nefit of low-precision compression. In addition, we intro-\nduce the WCT scheme, which enables task-specific adapta-\ntion with minimal memory overhead and shows outstanding\nperformance. We believe that our insights will promote the\nwidespread adoption of LLMs.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13362\nAcknowledgments\nThis work was supported by IITP grant (MSIT, No.2021-\n0-00310) and NRF grant (MSIT, RS-2023-00213611, RS-\n2023-00228970) funded by the Korea government.\nReferences\nBeeching, E.; Fourrier, C.; Habib, N.; Han, S.; Lambert,\nN.; Rajani, N.; Sanseviero, O.; Tunstall, L.; and Wolf, T.\n2023. Open LLM Leaderboard. https://huggingface.co/\nspaces/HuggingFaceH4/open\nllm leaderboard.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nNeurIPS, 33: 1877‚Äì1901.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;\nSchoenick, C.; and Tafjord, O. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv:1803.05457.\nDettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.\n2022. Llm. int8 (): 8-bit matrix multiplication for transform-\ners at scale. arXiv:2208.07339.\nDettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,\nL. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.\narXiv:2305.14314.\nDong, Z.; Yao, Z.; Arfeen, D.; Gholami, A.; Mahoney,\nM. W.; and Keutzer, K. 2020. Hawq-v2: Hessian aware\ntrace-weighted quantization of neural networks. NeurIPS,\n33: 18518‚Äì18529.\nDong, Z.; Yao, Z.; Gholami, A.; Mahoney, M. W.; and\nKeutzer, K. 2019. Hawq: Hessian aware quantization of neu-\nral networks with mixed-precision. In CVPR, 293‚Äì302.\nEsser, S. K.; McKinstry, J. L.; Bablani, D.; Appuswamy, R.;\nand Modha, D. S. 2019. Learned step size quantization.\narXiv:1902.08153.\nFrantar, E.; and Alistarh, D. 2022. Optimal Brain Compres-\nsion: A Framework for Accurate Post-Training Quantization\nand Pruning. In NeurIPS.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2022.\nGPTQ: Accurate Post-Training Quantization for Generative\nPre-trained Transformers. arXiv:2210.17323.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2023.\nOPTQ: Accurate Quantization for Generative Pre-trained\nTransformers. In ICLR.\nGao, L.; Tow, J.; Abbasi, B.; Biderman, S.; Black, S.; DiPofi,\nA.; Foster, C.; Golding, L.; Hsu, J.; Le Noac‚Äôh, A.; Li,\nH.; McDonell, K.; Muennighoff, N.; Ociepa, C.; Phang, J.;\nReynolds, L.; Schoelkopf, H.; Skowron, A.; Sutawika, L.;\nTang, E.; Thite, A.; Wang, B.; Wang, K.; and Zou, A. 2023.\nA framework for few-shot language model evaluation.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\ntitask Language Understanding. arXiv:2009.03300.\nHorowitz, M. 2014. 1.1 computing‚Äôs energy problem (and\nwhat we can do about it). In ISSCC, 10‚Äì14. IEEE.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adapta-\ntion of Large Language Models. In ICLR. OpenReview.net.\nK¬®opf, A.; Kilcher, Y .; von R¬®utte, D.; Anagnostidis, S.; Tam,\nZ.-R.; Stevens, K.; Barhoum, A.; Duc, N. M.; Stanley, O.;\nNagyfi, R.; et al. 2023. OpenAssistant Conversations‚Äì\nDemocratizing Large Language Model Alignment. arXiv\npreprint arXiv:2304.07327.\nLi, Y .; Gong, R.; Tan, X.; Yang, Y .; Hu, P.; Zhang, Q.; Yu,\nF.; Wang, W.; and Gu, S. 2021. BRECQ: Pushing the Limit\nof Post-Training Quantization by Block Reconstruction. In\nICLR.\nMarcus, M.; Kim, G.; Marcinkiewicz, M. A.; MacIntyre,\nR.; Bies, A.; Ferguson, M.; Katz, K.; and Schasberger, B.\n1994. The Penn treebank: Annotating predicate argument\nstructure. In Human Language Technology: Proceedings of\na Workshop held at Plainsboro, New Jersey, March 8-11,\n1994.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.\nPointer sentinel mixture models. arXiv:1609.07843.\nNagel, M.; Amjad, R. A.; Van Baalen, M.; Louizos, C.; and\nBlankevoort, T. 2020. Up or down? adaptive rounding for\npost-training quantization. In ICML, 7197‚Äì7206. PMLR.\nNagel, M.; Baalen, M. v.; Blankevoort, T.; and Welling, M.\n2019. Data-free quantization through weight equalization\nand bias correction. In ICCV, 1325‚Äì1334.\nNahshan, Y .; Chmiel, B.; Baskin, C.; Zheltonozhskii, E.;\nBanner, R.; Bronstein, A. M.; and Mendelson, A. 2021.\nLoss aware post-training quantization. Machine Learning,\n110(11-12): 3245‚Äì3262.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPark, E.; Kim, D.; and Yoo, S. 2018. Energy-efficient neu-\nral network accelerator based on outlier-aware low-precision\ncomputation. In ISCA, 688‚Äì698. IEEE.\nPark, E.; Yoo, S.; and Vajda, P. 2018. Value-aware quantiza-\ntion for training and inference of neural networks. InECCV,\n580‚Äì595.\nPark, G.; Park, B.; Kim, M.; Lee, S.; Kim, J.; Kwon, B.;\nKwon, S. J.; Kim, B.; Lee, Y .; and Lee, D. 2023. LUT-\nGEMM: Quantized Matrix Multiplication based on LUTs\nfor Efficient Inference in Large-Scale Generative Language\nModels. arXiv:2206.09557.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. NeurIPS, 32.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485‚Äì5551.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13363\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili¬¥c, S.; Hesslow,\nD.; Castagn¬¥e, R.; Luccioni, A. S.; Yvon, F.; Gall¬¥e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv:2211.05100.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv:2302.13971.\nWei, X.; Gong, R.; Li, Y .; Liu, X.; and Yu, F. 2022a. QDrop:\nRandomly Dropping Quantization for Extremely Low-bit\nPost-Training Quantization. In ICLR.\nWei, X.; Zhang, Y .; Zhang, X.; Gong, R.; Zhang, S.; Zhang,\nQ.; Yu, F.; and Liu, X. 2022b. Outlier Suppression: Push-\ning the Limit of Low-bit Transformer Language Models. In\nAdvances in Neural Information Processing Systems.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.\n2019. Huggingface‚Äôs transformers: State-of-the-art natural\nlanguage processing. arXiv:1910.03771.\nXiao, G.; Lin, J.; Seznec, M.; Demouth, J.; and Han, S. 2022.\nSmoothquant: Accurate and efficient post-training quantiza-\ntion for large language models. arXiv:2211.10438.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019. HellaSwag: Can a Machine Really Finish Your\nSentence? arXiv:1905.07830.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al.\n2022. Opt: Open pre-trained transformer language models.\narXiv:2205.01068.\nZhou, S.; Wu, Y .; Ni, Z.; Zhou, X.; Wen, H.; and\nZou, Y . 2016. Dorefa-net: Training low bitwidth con-\nvolutional neural networks with low bitwidth gradients.\narXiv:1606.06160.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n13364",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.6894485950469971
    },
    {
      "name": "Outlier",
      "score": 0.5855333209037781
    },
    {
      "name": "Computer science",
      "score": 0.5469245910644531
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.48727428913116455
    },
    {
      "name": "Language model",
      "score": 0.4681420922279358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40416890382766724
    },
    {
      "name": "Natural language processing",
      "score": 0.34086501598358154
    },
    {
      "name": "Algorithm",
      "score": 0.30299293994903564
    }
  ]
}