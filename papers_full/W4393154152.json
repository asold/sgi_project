{
    "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
    "url": "https://openalex.org/W4393154152",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5045879629",
            "name": "Gengze Zhou",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A3014687114",
            "name": "Yicong Hong",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2097447165",
            "name": "Qi Wu",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A5045879629",
            "name": "Gengze Zhou",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A3014687114",
            "name": "Yicong Hong",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2097447165",
            "name": "Qi Wu",
            "affiliations": [
                "University of Adelaide"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2770387316",
        "https://openalex.org/W3111566807",
        "https://openalex.org/W3208883793",
        "https://openalex.org/W6600704668",
        "https://openalex.org/W3040922163",
        "https://openalex.org/W3169884222",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W4221167437",
        "https://openalex.org/W3008604580",
        "https://openalex.org/W3092825347",
        "https://openalex.org/W3015180014",
        "https://openalex.org/W4221142655",
        "https://openalex.org/W4385261523",
        "https://openalex.org/W3014266552",
        "https://openalex.org/W4378945721",
        "https://openalex.org/W4221147783",
        "https://openalex.org/W6600617704",
        "https://openalex.org/W4310629067",
        "https://openalex.org/W3163303190",
        "https://openalex.org/W3205276578",
        "https://openalex.org/W4313591672",
        "https://openalex.org/W4386081133",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W2926977875",
        "https://openalex.org/W6602430550",
        "https://openalex.org/W4385681654",
        "https://openalex.org/W4288043965",
        "https://openalex.org/W2990021239",
        "https://openalex.org/W3152079230",
        "https://openalex.org/W4383108296",
        "https://openalex.org/W3105274166",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3107069568",
        "https://openalex.org/W4285393899",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3098358101",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W3035232877",
        "https://openalex.org/W2805984364",
        "https://openalex.org/W4302284439",
        "https://openalex.org/W4387968661",
        "https://openalex.org/W4322718421",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4312501707",
        "https://openalex.org/W4285069854",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4226052928",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3109085430",
        "https://openalex.org/W4312956471",
        "https://openalex.org/W4394828156",
        "https://openalex.org/W4293329554",
        "https://openalex.org/W3105746012",
        "https://openalex.org/W4226163860",
        "https://openalex.org/W4323717348",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2963800628",
        "https://openalex.org/W4312253995",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4304097971",
        "https://openalex.org/W4385438165",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W3034500398",
        "https://openalex.org/W4214700710",
        "https://openalex.org/W3034578524",
        "https://openalex.org/W3176974620",
        "https://openalex.org/W4225405251",
        "https://openalex.org/W4306676716",
        "https://openalex.org/W3100923070",
        "https://openalex.org/W4318719538",
        "https://openalex.org/W4312434279",
        "https://openalex.org/W2964935470",
        "https://openalex.org/W4221143400",
        "https://openalex.org/W3212555761",
        "https://openalex.org/W4386072289",
        "https://openalex.org/W4310557634",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4323650415",
        "https://openalex.org/W4385574005",
        "https://openalex.org/W3105521436",
        "https://openalex.org/W4283640928",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2909303996"
    ],
    "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.",
    "full_text": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation\nwith Large Language Models\nGengze Zhou1, Yicong Hong2, Qi Wu1*\n1The University of Adelaide\n2The Australian National University\n{gengze.zhou, qi.wu01}@adelaide.edu.au, mr.yiconghong@gmail.com\nAbstract\nTrained with an unprecedented scale of data, large language\nmodels (LLMs) like ChatGPT and GPT-4 exhibit the emer-\ngence of significant reasoning abilities from model scaling.\nSuch a trend underscored the potential of training LLMs\nwith unlimited language data, advancing the development of\na universal embodied agent. In this work, we introduce the\nNavGPT, a purely LLM-based instruction-following naviga-\ntion agent, to reveal the reasoning capability of GPT mod-\nels in complex embodied scenes by performing zero-shot\nsequential action prediction for vision-and-language naviga-\ntion (VLN). At each step, NavGPT takes the textual descrip-\ntions of visual observations, navigation history, and future ex-\nplorable directions as inputs to reason the agent’s current sta-\ntus, and makes the decision to approach the target. Through\ncomprehensive experiments, we demonstrate NavGPT can\nexplicitly perform high-level planning for navigation, includ-\ning decomposing instruction into sub-goals, integrating com-\nmonsense knowledge relevant to navigation task resolution,\nidentifying landmarks from observed scenes, tracking nav-\nigation progress, and adapting to exceptions with plan ad-\njustment. Furthermore, we show that LLMs is capable of\ngenerating high-quality navigational instructions from obser-\nvations and actions along a path, as well as drawing accu-\nrate top-down metric trajectory given the agent’s navigation\nhistory. Despite the performance of using NavGPT to zero-\nshot R2R tasks still falling short of trained models, we sug-\ngest adapting multi-modality inputs for LLMs to use as vi-\nsual navigation agents and applying the explicit reasoning of\nLLMs to benefit learning-based models. Code is available at:\nhttps://github.com/GengzeZhou/NavGPT.\nIntroduction\nAmid the remarkable advances in large language model\n(LLM) training (Touvron et al. 2023; Brown et al. 2020;\nChowdhery et al. 2022; Zhang et al. 2022; Wei et al. 2021;\nBubeck et al. 2023; OpenAI 2023), we note a shift towards\nintegrating LLMs into embodied robotics tasks such as Say-\nCan (Ahn et al. 2022) and PaLM-E (Driess et al. 2023). This\ntrend stems from two primary considerations: the scale of\ntraining data and the scale of models. First, the develop-\nment of techniques for processing textual information pro-\nvides an abundant source of natural language training data\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The architecture of NavGPT. NavGPT synergizes\nreasoning and actions in LLMs to perform zero-shot Vision-\nand-Language Navigation following navigation system prin-\nciples. It interacts with different visual foundation models\nto adapt multi-modality inputs, handle the length of his-\ntory with a history buffer and a summarizer, and aggregate\nvarious sources of information through a prompt manager.\nNavGPT parses the generated results from LLMs (LLM\nThoughts and LLM Action) to move to the next viewpoint.\nfor learning interdisciplinary and generalizable knowledge.\nFurthermore, by accessing unlimited language data, signifi-\ncant emergent abilities (Wei et al. 2022a) are observed when\nscaling up the model, resulting in a remarkable enhancement\nin the reasoning capabilities when solving problems across\nwide domains. Consequently, training an LLM with unlim-\nited language data is seen as a viable pathway toward realiz-\ning a universal embodied agent.\nThis insight has spurred the integration of LLMs into\nvision-and-language navigation (VLN) (Anderson et al.\n2018), an exploratory task toward achieving real-world\ninstruction-following embodied agents. The latest research\nattempt to leverage GPT models (OpenAI 2023; Brown et al.\n2020) to benefit navigation. For example, using LLMs as a\nparser for diverse language input (Shah et al. 2023) — ex-\ntracting landmarks from instruction to support visual match-\ning and planning, or leveraging LLMs’ commonsense rea-\nsoning abilities (Zhou et al. 2023; Dorbala, Mullen Jr, and\nManocha 2023) to incorporate prior knowledge of inter-\nobject correlations to extend agents’ perception and facili-\ntate the decision making. However, we notice that the rea-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7641\nsoning ability of LLMs in navigation is still under-explored,\ni.e. , can LLMs understand the interactive world, the actions,\nand consequences in text form, and use all the information\nto solve a navigation task?\nIn light of this, we introduce NavGPT, a fully auto-\nmatic LLM-based system designed for language-guided vi-\nsual navigation, with the capability to handle multi-modality\ninputs, unconstrained language guidance, interaction with an\nopen-world environment, and progress tracking with naviga-\ntion history. NavGPT perceives the visual world by reading\ndescriptions of observations generated by visual foundation\nmodels (VFMs), and synergizing Thoughts (reasoning) and\nActions (decision making) in an explicit text form. To an ex-\ntreme extent, we use NavGPT to perform zero-shot VLN1 to\nclearly reveal the reasoning process of LLMs during naviga-\ntion.\nThrough comprehensive experiments, we found that\nLLMs possess the capability to execute complex naviga-\ntional planning. This includes the deconstruction of instruc-\ntions into distinct sub-goals, assimilation of commonsense\nknowledge pertinent to navigational tasks, identification of\nlandmarks within the context of observed environments,\ncontinuous monitoring of navigational progression, and re-\nsponding to anomalies by modifying their initial plan. The\naforementioned phenomenon reflects an astonishing reason-\ning ability in understanding and solving navigation prob-\nlems. Furthermore, we show that LLMs have the ability to\ndraw navigation trajectories in a metric map and regenerate\nnavigation instruction based on navigation history, reveal-\ning the historical and spatial awareness of LLMs for naviga-\ntion tasks. However, there remains a significant gap between\nthe zero-shot performance of current open-sourced LLMs in\nVLN compared to the fine-tuned models, where the bottle-\nneck of NavGPT lies in the information loss while trans-\nlating visual signals into natural language and summarizing\nobservations into history. As a result, we suggest the future\ndirection of building general VLN agents to be LLMs with\nmulti-modality inputs or a navigation system making use of\nhigh-level navigation planning, historical and spatial aware-\nness from LLMs.\nOur contributions can be summarized as follow: (1) We\nintroduce a novel instruction-following LLMs agent for vi-\nsual navigation with a supportive system to interact with the\nenvironment and track navigation history. (2) We investigate\nthe capabilities and limitations of current LLMs’ reasoning\nfor making navigation decisions. (3) We reveal the capability\nof LLMs in high-level planning for navigation, by observing\nthe thoughts of LLMs, making the planning process of nav-\nigation agents accessible and explainable.\nRelated Work\nVision-and-Language Navigation Language-driven vi-\nsion navigation is demanded by widely applicable embod-\nied navigation agents. Previous study shows the essentials\n1Our NavGPT is solely powered by off-the-shelf LLMs, with-\nout any learnable module or any prior experience in solving inter-\nactive navigation. Hence, all navigation tasks defined in this paper\nare novel to NavGPT.\nof modules to achieve such a goal (Anderson et al. 2018;\nQi et al. 2020b; Krantz et al. 2020; Ku et al. 2020; He et al.\n2021; Gu et al. 2022; Zhu et al. 2022; Hong et al. 2020a,\n2022; Zhao, Qi, and Wu 2023; Qiao et al. 2023b), whereas\na large number of research reveal the crucial effect of train-\ning strategies (Wang et al. 2019; Tan, Yu, and Bansal 2019).\nImportantly, the main problem in VLN is the generalizability\nof agents in unseen environments. Data augmentation (Wang\net al. 2022; Li, Tan, and Bansal 2022; Tan, Yu, and Bansal\n2019; Parvaneh et al. 2020; Li and Bansal 2023), memory\nmechanism (Chen et al. 2021b; Pashevich, Schmid, and Sun\n2021; Hong et al. 2023), pre-training (Hao et al. 2020; Chen\net al. 2022a; Qiao et al. 2023a; Wang et al. 2023) have been\nadopted to alleviate data scarcity. However, those augmen-\ntations and pre-training are limited to the sampled data from\na fixed number of scenes, which is not enough to reflect\na realistic application scene where objects could be out of\nthe domains and language instructions are more diverse. In\nour work, we utilize the reasoning and knowledge storage of\nLLMs and perform VLN in a zero-shot manner as an initial\nattempt to reveal the potential usage of LLMs for VLN in\nthe wild. A number of studies (Chen et al. 2021a; Deng,\nNarasimhan, and Russakovsky 2020; Chen et al. 2022b)\nhave presented compelling methodologies that underscore\nthe significance of topological maps in facilitating long-term\nplanning, specifically in the aspect of backtracking to prior\nlocations. In addition, Dorbala et al. (Dorbala et al. 2022)\nuse CLIP (Radford et al. 2021) to perform zero-shot VLN by\nchunking instructions into keyphrases and completely rely\non the text-image matching capability from CLIP to navi-\ngate. However, the planning and decision-making processes\nof the agents above are implicit and not accessible. On the\ncontrary, benefiting from the intrinsic of LLMs, we are able\nto access the reasoning process of agents, making it explain-\nable and controllable.\nLarge Language Models With the massive success in\nlarge-scale language model training (Touvron et al. 2023;\nBrown et al. 2020; Chowdhery et al. 2022; Zhang et al. 2022;\nWei et al. 2021), a new cohort of Large Language Mod-\nels (LLMs) has shown evolutionary progress toward achiev-\ning Artificial General Intelligence (AGI) (Bubeck et al.\n2023; OpenAI 2023). This burgeoning class of LLMs, un-\nderpinned by increasingly sophisticated architectures and\ntraining methodologies (Scao et al. 2022), has the potential\nto revolutionize various domains by offering unprecedented\ncapabilities in natural language understanding and genera-\ntion. The main concern for LLMs is that their knowledge\nis limited and confined after training is finished. The lat-\nest works study how to utilize LLMs interacting with tools\nto expand their knowledge as a plugin, including extend-\ning LLM to process multimodality content (Wu et al. 2023;\nYongliang et al. 2023), teaching LLMs to access the inter-\nnet with correct API calls (Schick et al. 2023), and expand-\ning their knowledge with local databases to accomplish QA\ntasks (Peng et al. 2023). Another stream of works studies\nhow to prompt LLMs in a hierarchical system to facilitate\nthe alignment of reasoning and corresponding actions (Yao\net al. 2022; Karpas et al. 2022) beyond the Chain of Thought\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7642\n(CoT) (Wei et al. 2022b). These works set up the preliminar-\nies for building an embodied agent directly using LLMs.\nLLMs in Robotics NavigationThe employment of Large\nLanguage Models (LLMs) in the field of robotics remains\nin the primary stage (Vemprala et al. 2023; Bubeck et al.\n2023). A handful of contemporary studies, however, have\nbegun to explore the utilization of generative models for\nnavigation. Shah et al. (Shah et al. 2023) employs GPT-\n3 (Brown et al. 2020) in an attempt to identify \"landmarks\"\nor subgoals, while Huang et al. (Huang et al. 2022) con-\ncentrates its efforts on the application of an LLM for the\ngeneration of code. Zhou et al. (Zhou et al. 2023) use\nLLM to extract the commonsense knowledge of the rela-\ntions between targets and objects in observations to per-\nform zero-shot object navigation (ZSON) (Gadre et al. 2022;\nMajumdar et al. 2022). Despite these recent advancements,\nour study diverges in its concentration on converting visual\nscene semantics into input prompts for the LLM, directly\nperforming VLN based on the commonsense knowledge and\nreasoning ability of LLMs. The work closest to ours is LGX\n(Dorbala, Mullen Jr, and Manocha 2023), but they are doing\nobject navigation where agents are not required to follow\nthe instruction and in their method, they use the GLIP (Li\net al. 2022a) model to decide the stop probability and did\nnot consider memorization of navigation history, action, and\nreasoning between LLM.\nMethod\nVLN Problem Formulation\nWe formulate the VLN problem as follows: Given a natu-\nral language instruction W, composed of a series of words\ntw1, w2, w3, . . . , wnu, at every step st, the agent interprets\nthe current location via the simulator to obtain an obser-\nvation O. This observation comprises N alternative view-\npoints, representing the egocentric perspectives of agents in\nvarying orientations.\nEach unique view observation is denoted as oipi ď\nNq, with its associated angle direction represented as\naipi ď Nq. The observation can thus be defined as\nOt fi rxo1, a1y, xo2, a2y, . . . ,xoN , aN ys. Throughout the\nnavigation process, the agents’ action space is confined to\nthe navigation graph G. The agent must select from the\nM “ |Ct`1| navigable viewpoints, where Ct`1 indicates\nthe set of candidate viewpoints, by aligning the observa-\ntion OC\nt fi rxoC\n1 , aC\n1 y, xoC\n2 , aC\n2 y, . . . ,xoC\nM , aC\nM ys with the\noracle W. The agent prognosticates the subsequent action\nby selecting the relative angle aC\ni from OC\nt , then enacts\nthis action through interaction with the simulator to tran-\nsition from the current state st “ xvt, θt, ϕty to st`1 “\nxvt`1, θt`1, ϕt`1y, where v denotes the current viewpoint\nlocation, θ denotes the current heading angle, and ϕ de-\nnotes the current elevation angle of the agent. The agent\nalso maintains a record of the state history ht and ad-\njusts the conditional transition probability between states\nSt “ Tpst`1|aC\ni , st, htq, where function T denotes the con-\nditional transition probability distribution.\nIn summary, the policyπ parametrized by Θ that the agent\nis required to learn is based on the oracle W and the current\nobservation OC\nt , which is πpat|W, Ot, OC\nt , St; Θq. In this\nstudy, NavGPT conducts the VLN task in a zero-shot man-\nner, where the Θ is not learned from the VLN datasets, but\nfrom the language corpus that the LLMs are trained on.\nNavGPT\nNavGPT is a system that interacts with environ-\nments, language guidance, and navigation his-\ntory to perform action prediction. Let Hăt`1 fi\nrxO1, R1, A1y, xO2, R2, A2y, . . . ,xOt, Rt, Atys be the\nnavigation history of observation O, LLM reasoning R\nand action A triplets for the previous t steps. To obtain the\nnavigation decision At`1, NavGPT needs to synergize the\nvisual perception from VFMs F, language instruction W,\nhistory H and navigation system principle P with the help\nof prompt manager M, define as follow:\nxRt`1, At`1y\n“LLMpMpPq, MpWq, MpFpOtqq, MpHăt`1qq (1)\nNavigation System PrincipleP. The Navigation System\nPrinciple formulates the behavior of LLM as a VLN agent. It\nclearly defines the VLN task and the basic reasoning format\nand rules for NavGPT at each navigation step. For example,\nNavGPT should move among the static viewpoints (posi-\ntions) of a pre-defined graph of the environment by identify-\ning the unique viewpoint ID. NavGPT should not fabricate\nnonexistent IDs.\nVisual Foundation ModelsF. NavGPT as an LLM agent\nrequires visual perception and expression ability from VFMs\nto translate the current environment’s visual observation into\nnatural language description. The VFMs here play the role\nof translator, to translate visual observations using their own\nlanguage, e.g. natural language, objects’ bounding boxes,\nand objects’ depth. Through the process of prompt manage-\nment, the visual perception results will be reformated and\ntranslated into pure natural language for LLMs to under-\nstand.\nNavigation History Hăt`1. The navigation history is es-\nsential for NavGPT to evaluate the progress of the comple-\ntion of the instruction, to update the current state, and make\nthe following decisions. The history is composed of sum-\nmarized descriptions of previous observations Oăt`1 and\nactions Aăt`1, along with the reasoning thoughts Răt`1\nfrom LLM.\nPrompt Manager M. The key to using LLM as a VLN\nagent is to convert all the above content into a natural lan-\nguage that LLM can understand. This process is done by the\nprompt manager, which collects the results from different\ncomponents and parses them into a single prompt for LLM\nto make navigation decisions.\nVisual Perceptron for NavGPT\nIn this section, we introduce the visual perception process\nof NavGPT. We take visual signals as a foreign language\nand handle the visual input using different visual foundation\nmodels to translate them into natural language, shown in fig-\nure 2.\nFor an agent standing at any viewpoint in the environ-\nment, the observation is composed of egocentric views from\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7643\nFigure 2: The process of forming natural language description from visual input. We used 8 directions to represent a viewpoint\nand show the process of forming the descriptions for one of the directions.\ndifferent orientations. The number of total views is defined\nby the field of view of each view image and the relative an-\ngle of each view. In our work, we set the field of view of\neach view as 45˝, and turn the heading angle θ 45˝ per view\nfrom 0˝ to 360˝, 8 directions in total. Besides, we turn the\nelevation angle ϕ 30˝ per view from 30˝ above the horizon-\ntal level to 30˝ below, 3 levels in total. As a result, we obtain\n3 ˆ 8 “ 24 egocentric views for each viewpoint.\nTo translate visual observation into natural language, we\nfirst utilize the BLIP-2 (Li et al. 2023a) model as the trans-\nlator. With the strong text generation capability of LLMs,\nBLIP-2 can achieve stunning zero-shot image-to-text gen-\neration quality. By carefully setting the granularity of visual\nobservation (field of views and the total view number in each\nobservation), we prompt BILP-2 to generate a decent lan-\nguage description of each view with a detailed depiction of\nthe shapes and color of objects and the scenes they are in\nwhile avoiding useless caption of views from a smaller FoV ,\nfrom which partial observation is available and it is hard to\nrecognize even for humans. See appendix for details.\nNotice that for the heading direction, the rotation interval\nis equal to the field of view, therefore there is no overlap-\nping between each orientation. For the elevations, there is a\n15˝’s overlapping between the top, middle, and down views.\nIn NavGPT we mainly focus on the heading angle of agents\nduring navigation, therefore, we prompt GPT-3.5 to summa-\nrize the scenes from the top, middle, and down views for\neach orientation into a sentence of description.\nBesides natural language descriptions of the scene from\nBLIP-2, we also excavate the lower-level feature extracted\nby other vision models. These vision models serve as auxil-\niary translators, translating visual input into their own \"lan-\nguage\" like the class of objects and corresponding bound-\ning boxes. The detection results will be aggregated by the\nprompt manager into prompts for LLMs. In this work, we\nutilize Fast-RCNN (Girshick 2015) to extract the bound-\ning boxes of objects in each egocentric view. After locat-\ning the objects, we calculate the relative heading angle for\neach object and the agent. We also extract the depth infor-\nmation of the center pixel of the object provided by the Mat-\nterport3D simulator (Anderson et al. 2018). With the depth,\nobjects’ relative orientation, and class, we filter the detection\nresults by leaving the object within 3 meters from the current\nviewpoint. The results from VFMs will be processed by the\nprompt manager into observation for the current viewpoint\nin natural language.\nSynergizing Reasoning and Actions in LLMs\nIn the VLN task, the agent needs to learn the policy\nπpat|W, Ot, OC\nt , St; Θq, which is difficult because the im-\nplicit connection between actions and observations and de-\nmain intensive computation. In order to explicitly access\nand enhance the agent’s comprehension of the current state\nduring navigation, we follow the ReAct paper (Yao et al.\n2022) to expand the agent’s action space to ˜A “ A Y R,\nwhere R P L is in the entire language space L, denoting the\nthought or reasoning trace of the agent.\nThe reasoning traces R of the agent will not trigger any\ninteraction with the external environment, therefore no ob-\nservation will be returned when the agent is outputting the\nreasoning during each navigation step. We synergize the\nNavGPT’s actions and thoughts by prompting it to make\nnavigation decisions after outputting the reasoning trace at\neach step. Introducing the reasoning traces aims to bootstrap\nthe LLMs in two aspects:\nFirstly, prompting the LLMs to think before choosing an\naction, enables LLMs to perform complex reasoning in plan-\nning and creating strategies to follow the instructions under\nthe new observations. For example, as shown in figure 3,\nNavGPT can generate a long-term navigation plan by ana-\nlyzing the current observation and the instruction, perform-\ning higher-level planning such as decomposing instruction\nand planning to reach the sub-goal, which is never seen ex-\nplicitly in previous works.\nSecondly, including reasoning traces R in the naviga-\ntion history Hăt enhances the problem-solving ability of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7644\nNavGPT. By injecting reasoning traces into navigation his-\ntory, NavGPT inherits from the previous reasoning traces,\nto reach a sub-goal with high-level planning consistently\nthrough steps, and can track the navigation progress with\nexception-handling abilities like adjusting the plan.\nNavGPT Prompt Manager\nWith the Navigation System Principle P, translated results\nfrom VFMs, and the History of NavigationHăt, the prompt\nmanager parses and reformates them into prompts for LLMs.\nDetails of the prompt are presented in the appendix.\nSpecifically, for Navigation System PrincipleP, NavGPT\nprompt manager will create a prompt to convey LLMs with\nthe rules, declaring the VLN task definition, defining the\nsimulation environment for NavGPT, and restricting LLMs’\nbehavior in the given reasoning format.\nFor perception results from VFMs F, the prompt man-\nager gathers the results from each direction and orders the\nlanguage description by taking the current orientation of\nNavGPT as the front, shown in figure 2, arranging the de-\nscription from 8 directions into prompt by concatenating\nthem clockwise.\nFor navigation historyHăt`1, the observation, reasoning,\nand actions triplesxOi, Ri, Aiy are stored in a history buffer,\nshown in figure 1. Directly extracting all triples in the buffer\nwill create too long a prompt for LLMs to accept. To handle\nthe length of history, the prompt manager utilizes GPT-3.5\nto summarize the observations from viewpoints in the tra-\njectory, inserting the summarized observations into the ob-\nservation, reasoning, and actions triples in the prompt.\nExperiment\nImplementation Details. We evaluate NavGPT based on\nGPT-4 (OpenAI 2023) and GPT-3.5 on the R2R dataset (An-\nderson et al. 2018). The R2R dataset is composed of 7189\ntrajectories, each corresponding to three fine-grained in-\nstructions. The dataset is separated into the train, val seen,\nval unseen, and test unseen splits, with 61, 56, 11, and 18\nindoor scenes, respectively. We apply the 783 trajectories\nin the 11 val unseen environments in all our experiments\nand for comparison to previous supervised approaches. We\nutilize BLIP-2 ViT-G FlanT5XL (Li et al. 2023a) as images\ntranslator and Fast-RCNN (Girshick 2015) as object detec-\ntor. The depth information of objects is extracted from the\nMattport3D simulator (Anderson et al. 2018) by taking the\ndepth of the center pixel in the bounding box.\nEvaluation Metrics. The evaluation of NavGPT utilizes\nstandardized metrics from the R2R dataset. These include\nTrajectory Length (TL), denoting the average distance trav-\neled by the agent; Navigation Error (NE), representing the\nmean distance from the agent’s final location to the destina-\ntion; Success Rate (SR), indicating the proportion of naviga-\ntion episodes where the agent successfully reaches the target\nlocation within a 3-meter margin of error; Oracle Success\nRate (OSR), the success rate of agent stopped at the closest\npoint to the goal on its trajectory; and Success Rate weighted\nby the normalized inverse of Path Length (SPL), which bal-\nances navigation precision and efficiency by adjusting the\nsuccess rate based on the ratio of the optimal path length to\nthe agent’s predicted path length.\nQualitive Results\nWe elaborately study the qualitative results of the reason\ntrace from NavGPT. We reveal the potential high-level plan-\nning capability of GPT-4 under embodied navigation tasks.\nReasoning Capability of GPT-4 for Language-guide Nav-\nigation As shown in figure 3, with GPT-4, NavGPT can\nperform various types of reasoning and high-level planning\nduring navigation. For short instructions, NavGPT can track\nthe navigation progress through steps to accomplish a sin-\ngle action described in the instructions, similar to the self-\nmonitoring VLN agents (Ma et al. 2019; Zhu et al. 2020;\nGao et al. 2023). For long instructions, NavGPT can break\nit down with sub-goals, similar to previous works on fine-\ngraining R2R data (Hong et al. 2020b; He et al. 2021; Zhao\net al. 2022), and plan to reach the destination by effectively\nidentifying landmarks from observations, similar to works\non utilizing objects information to perform cross-modality\nmatching in VLN (Gao et al. 2021; Qi et al. 2020a, 2021).\nWhen navigating to a viewpoint with unexpected observa-\ntion, NavGPT can plan to explore the environment and use\ncommonsense knowledge to assist decision-making, similar\nto VLN methods incorporate external knowledge(Li et al.\n2022b; Gao et al. 2021; Li et al. 2023b).\nHistory and Spatial Relative Relation Awareness for\nLLMs During Navigation We examined NavGPT’s\nawareness of historical and spatial relations by employing\nGPT-4 to delineate the trajectory in navigational history and\nto construct a map of visited viewpoints utilizing pyplot. The\nprocess involved extracting exclusively the actions At`1,\nobservations Ot`1, and the entire navigation history Ht`1.\nThe specifics of the prompt are presented in the appendix.\nAs shown in figure 4, we observed that GPT-4 could effec-\ntively extract landmarks from the redundant observation de-\nscriptions and generate navigation history descriptions with\nactions. This could be a potential way of generating new tra-\njectory instructions for VLN. Besides, the result shows GPT-\n4 can comprehensively understand the history of navigation,\nand thus can perform the essential progress tracking during\nnavigation. Moreover, shown in figure 4, GPT-4 can success-\nfully catch the relative position relations between viewpoints\nand draw a top-down view of the trajectory for visited view-\npoints. By providing language descriptions of actions taken\nby the agents, including the turning angle and relative dis-\ntances between viewpoints, GPT-4 shows a stunning aware-\nness of spatial relations. Such impressive reasoning ability\nsupports NavGPT to perform high-level planning shown in\nfigure 3, underlines the significant potential LLMs hold for\nembodied navigation tasks.\nComparison with Supervised Methods\nWe compare the results of using NavGPT to zero-shot the\nsequential navigation tasks with previous models trained on\nthe R2R dataset. As shown in table 1, NavGPT surpasses\nsome supervised baselines and is compatible with some su-\npervised agents. However, a significant discrepancy can be\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7645\nInstruction: Go forward past the bed and the stairs. \nStop in the doorway straight ahead.\nInit observation: … A bedroom with hardwood floors …\nThought: … The rear viewpoint seems to be in the \ndirection of the bed, so I should move there first.\nAction: \naction_maker(\"b0a6cf6e9b9043248ed1c1ed8db08436”)\nObservation: … A hallway with hardwood floors leads \nto a bathroom with a mirror and light fixtures …\nThought: I have moved past the bed, and now I see a \nhallway leading to a bathroom, … continue forward to \nfind the stairs and the doorway straight ahead …\nAction\n:\n \naction_maker(\"c53934a9642942ae9b8d9766ee1e51ac”)\nObservation: … A view of a hallway with hardwood \nfloors and a staircase…\nThought: I have found the stairs, and now I need to \nfind the doorway straight ahead …\nAction: \naction_maker(\"bf10ccfbb089402d9f3e606803dadb0d\")\nObservation: … hardwood floors and a doorway …\nThought: I have reached a viewpoint where I can see \na doorway straight ahead. I should move to the front \nnavigable viewpoint to get closer to the doorway and \ncheck if I have reached the destination.\nAction: \naction_maker(\"df14f9089a6646f0bbf5054e18fb8951”)\nObservation: … A hallway with hardwood floors and \nwhite doors leads to a white doorway …\nThought\n:\n … the destination of the doorway …\nInstruction: Go up the stairs. Stop at the top of the \nstairs.\nObservation\n: …A wooden staircase leads up to a room …\nThought: I am at the bottom of the stairs. I should \ncontinue going up the stairs by moving to the viewpoint \n'5b2de972faea4ca381cd72140f77b0c0'.\nAction: action_maker(\"5b2de972faea4ca381cd72140f77b0c0\")\nObservation: … staircase leads down to a room with \nbeams in a large barn with a ladder …\nThought: I am now on the stairs. I should continue \ngoing up the stairs by moving to the viewpoint \n'930cbd6da8f14dfba4d883c55a069f96'.\nAction: action_maker(\"930cbd6da8f14dfba4d883c55a069f96\")\nObservation: …A wooden staircase with a railing leads \nto a room with wooden floors, …\nThought: I am almost at the top of the stairs. I should \ncontinue going up the stairs …\nInstruction: Keep walking forward passing all the \npicture frames on the wall. Turn left at the corner \nbefore where the chairs are placed. Walk forward \npassing the big pots placed on the floor on your left. \nTurn left and stop in front of the sofa.\nObservation\n: …A hallway with two bars, a fireplace, \narches, and lights...\nThought: I have passed some picture frames on the wall. \nI should continue walking forward …\nAction\n: action_maker(\"b882d05cc8d842879c647824f537a17c\")\nObservation: …A hallway with a large painting and vase \non travertine tile flooring is being cleaned while \nwooden...\nThought: I have passed more picture frames on the wall. \nNow, I need to look for a corner before where the \nchairs are placed and turn left …\nAction: action_maker(\"041abeb88f4d4a9cb085e359686aa9ed\")\nObservation: …and a large urn sitting on a tile floor...\nThought: I have reached the corner before where the \nchairs are placed. Now, I need to turn left and walk \nforward passing the big pots placed on the floor on my \nleft…\nInstruction: Walk into the house, and go through \nthe kitchen. Leave the kitchen through the door \non the right of the large black bookcase. Go down \nthe stairs. Stop on the third stair from the top.\nInit observation: …with hardwood floors and a \ndining table…\nThought: First, I need to go through the kitchen. \nThe kitchen is in the front left direction, so I \nshould move to the viewpoint \n'68aafa779b9c41eca16156cfddcedd2b'.\nAction: \naction_maker(\"68aafa779b9c41eca16156cfddcedd2b\")\nObservation: …A dining room and kitchen with \nhardwood floors.\nThought: I am now in the kitchen. Next, I need to \nfind the door on the right of the large black \nbookcase. I should move to the viewpoint \n'e685b29863794f00b09c23e1536412d8' to get a \nbetter view of the kitchen and find the door.\nAction: \naction_maker(\"e685b29863794f00b09c23e1536412d8\")\nObservation: …A kitchen with white cabinets, \nvarying flooring and counter top materials, …\nThought\n: I can see the kitchen clearly now, but I \nstill can't find the large black bookcase and the \ndoor on its right. I should move to the viewpoint \n'5b01ffae96874e60a9decd1fbac7e4ee' to explore the \nkitchen further.\nAction: \naction_maker(\"5b01ffae96874e60a9decd1fbac7e4ee\")\nObservation: …A kitchen with a marble top island, \ncounter top, and fireplace.\nThought: I found the large black bookcase in the \nrear left direction, and the door on its right. I \nshould move to the viewpoint \n'e62661ec94d04adda7fee9a81d429d2b' to get closer \nto the door.\nAction: \naction_maker(\"e62661ec94d04adda7fee9a81d429d2b\"\nObservation: ...A hallway with hardwood floors \nand a door leads to a room with a bookcase …\nThought: I am now closer to the door …\nThought: first I need to walk towards the sink. The \nsink is not visible in the current viewpoint, but \nthere is a bathroom to the left, which might lead \nme closer to the sink.\nThought: I am now in the living room area, and I can \nsee the fireplace. I should move closer to the \nfireplace and look for the stool and table. \nLong instruction with progress tracking\nShort instruction with sub-goal planning Long instruction with exceptions handlingShort instruction with progress tracking\nIntegrating commonsense knowledge\nFigure 3: The qualitative of NavGPT. NavGPT can explicitly perform high-level planning for sequential action prediction,\nincluding decomposing instruction into sub-goals, integrating commonsense knowledge, identifying landmarks from observed\nscenes, tracking navigation progress, exceptions handling with plan adjustment.\ndiscerned. We suggest the limitations inhibiting the perfor-\nmance of LLMs in solving VLN can be primarily attributed\nto two factors: the precision of language-based depiction of\nvisual scenes and the tracking capabilities regarding objects.\nNavGPT’s functionality is heavily reliant on the quality\nof captions generated from VFMs. If the target object de-\nlineated in the instruction is absent in the observation de-\nscription, NavGPT is compelled to explore the environment.\nThe ideal circumstance entails all target objects being visible\npursuant to the instruction. However, the inherent granular-\nity of language description inevitably incurs a loss of infor-\nmation. Moreover, NavGPT must manage the length of the\nnavigation history to prevent excessively verbose descrip-\ntions as the steps accrue. To this end, a summarizer is imple-\nmented, albeit at the cost of further information loss. This\ndiminishes NavGPT’s tracking ability, impeding the forma-\ntion of seamless perceptions of the entire environment as the\ntrajectory lengthens.\nEffect of Visual Components\nWe perform additional experiments to investigate the ef-\nfectiveness of visual components in NavGPT, we construct\na baseline with GPT-3.5 for its easier access and budget-\nfriendly costs. To evaluate the zero-shot ability in various\nenvironments, we construct a new validation split sampling\nboth from the original training set and the validation unseen\nset. The scenes from the training and validation unseen set\nare 61 and 11 respectively, 72 scenes in total. We randomly\npicked 1 trajectory from the 72 environment, each is associ-\nated with 3 instructions. In total, we sample 216 samples to\nconduct the ablation study.\nEffect of Granularity in Visual Observation Descrip-\ntions. The Field of View (FoV) of an image critically influ-\nences BILP-2’s captioning ability, with an overly large FoV\nleading to generalized room descriptions and an extremely\nsmall FoV hindering object recognition due to limited con-\ntent. As shown in table 2, we investigate 3 granularity of\nvisual representation from a viewpoint. Specifically, variant\n#1 utilizes an image with 60 FoV , turn heading angle 30 de-\ngrees clock-wise to obtain 12 views from a viewpoint, while\nvariant #2 and #3 utilize an image with 30, 45 FoV , turn el-\nevation angle 30 degrees from top to down, and turn heading\nangle 30, 45 degrees clockwise to form 36 views, 24 views\nrespectively. From the results, we found that using FoV 45 a\nviewpoint generates the most suitable natural language de-\nscription for navigation, surpassing variant #1 and #2 by\n6.48% and 2.78% respectively.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7646\nTop-down Trajectory Drew by GPT-4\nInstruction Generated by GPT-4\nTrajectory of NavGPT Ground Truth Instruction\nExit the sewing room. Turn right. Go toward the glass cabinet with the dolls in it. Turn \ninto the doorway on the left. Pass the bed and go through the next doorway on the left \ninto the bathroom. Wait by the sink.\nStart at the initial point, move to the hallway with the violin hanging from the ceiling, \nproceed to the building with chandeliers and wooden floors, then navigate to the room with \na statue of a horse on a shelf, next, move towards the rooms with rugs and paintings, then \nproceed to the room with bathroom fixtures and framed pictures on the walls.\nFigure 4: We evaluate GPT-4 on a case where NavGPT successfully follows the ground truth path, using only the historical\nactions Aăt`1 and observationsOăt`1 to generate an instruction (without reasoning traceRăt`1 to avoid information leaking),\nand using the entire navigation history Hăt`1 to draw a top-down trajectory.\nTraining\nSchema Method TL NEÓ OSRÒ SRÒ SPLÒ\nTrain\nOnly\nSeq2Seq (Anderson et al. 2018) 8.39 7.81 28 21 -\nSpeaker Follower (Fried et al. 2018) - 6.62 45 35 -\nEnvDrop (Tan, Yu, and Bansal 2019) 10.70 5.22 - 52 48\nPretrain +\nFinetune\nPREV ALENT (Hao et al. 2020) 10.19 4.71 - 58 53\nVLNœBERT\n(Hong et al. 2021) 12.01 3.93 69 63 57\nHAMT (Chen et al. 2021b) 11.46 2.29 73 66 61\nDuET (Chen et al. 2022b) 13.94 3.31 81 72 60\nNo T\nrain DuET (Init. LXMERT (Tan and Bansal 2019)) 22.03 9.74 7 1 0\nNavGPT (Ours) 11.45 6.46 42 34 29\nTable 1: Comparison with previous methods on R2R validation unseen split.\nGranularity #\nTL NEÓ OSRÒ SRÒ SPLÒ\nFoV@60\n1 12.38 9.07 14.35 10.19 6.52\nFoV@30 2 12.67 8.92 15.28 13.89 9.12\nFoV@45 3 12.18 8.02 26.39 16.67 13.00\nTable 2: The effect of granularity in visual observation.\nObservation\n# TL NEÓ OSRÒ SRÒ SPLÒ\nBaseline 1\n16.11 9.83 15.28 11.11 6.92\n+ Obj 2 11.07 8.88 23.34 15.97 11.71\n+ Obj + Dis 3 12.18 8.02 26.39 16.67 13.00\nTable 3: The effect of additional information.\nEffect of Semantic Scene Understanding and Depth Esti-\nmation. NavGPT also collaborates with other visual foun-\ndation models to enhance the perception of the environment.\nWe investigate the effectiveness of adding the object infor-\nmation and the relative distance between the agent and the\ndetected objects. We constructed a baseline method based on\nthe caption results from BILP-2 and powered by GPT-3.5.\nAs shown in table 3, by adding object information, the SR\nincreases by 4.86% compared with the baseline, for the ad-\nditional object information emphasizes the salient object in\nthe scenes. Moreover, we observed a phenomenon in which\nagents failed to reach the destination because they do not\nknow how close they are to the destination. Once the target\nviewpoint is visible in sight, they tend to stop immediately.\nTherefore by adding depth information, the agent has a bet-\nter understanding of the current position and further raises\nthe SR by 0.7% and SPL by 1.29%.\nConclusion\nIn this work, we explore the potential of utilizing\nLLMs in embodied navigation tasks. We present NavGPT,\nan autonomous LLM system specifically engineered for\nlanguage-guided navigation, possessing the ability to pro-\ncess multi-modal inputs and unrestricted language guid-\nance, engage with open-world environments, and main-\ntain the navigation history. Limited by the quality of lan-\nguage description of visual scenes and the tracking abili-\nties of objects, NavGPT’s zero-shot performance on VLN\nis still not compatible with trained methods. However, the\nreasoning trace of GPT-4 illuminates the latent potential\nof LLMs in embodied navigation planning. Interaction of\nLLMs with downstream specialized models or the develop-\nment of multi-modal LLMs for navigation, heralding the fu-\nture of versatile VLN agents.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7647\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; et al. 2022.\nDo As I Can and Not As I Say: Grounding Language in\nRobotic Affordances. In arXiv preprint arXiv:2204.01691.\nAnderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.;\nSünderhauf, N.; Reid, I.; Gould, S.; and Van Den Hen-\ngel, A. 2018. Vision-and-language navigation: Interpret-\ning visually-grounded navigation instructions in real envi-\nronments. In CVPR.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; et al. 2020.\nLanguage models are few-shot learners. In NeurIPS.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.; et al.\n2023. Sparks of artificial general intelligence: Early experi-\nments with gpt-4. arXiv preprint arXiv:2303.12712.\nChen, K.; Chen, J. K.; Chuang, J.; Vázquez, M.; and\nSavarese, S. 2021a. Topological planning with transform-\ners for vision-and-language navigation. In CVPR.\nChen, S.; Guhur, P.-L.; Schmid, C.; and Laptev, I. 2021b.\nHistory aware multimodal transformer for vision-and-\nlanguage navigation. In NeurIPS.\nChen, S.; Guhur, P.-L.; Tapaswi, M.; Schmid, C.; et al.\n2022a. Learning from unlabeled 3d environments for vision-\nand-language navigation. In ECCV.\nChen, S.; Guhur, P.-L.; Tapaswi, M.; Schmid, C.; et al.\n2022b. Think Global, Act Local: Dual-scale Graph Trans-\nformer for Vision-and-Language Navigation. In CVPR.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; et al.\n2022. PaLM: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311.\nDeng, Z.; Narasimhan, K.; and Russakovsky, O. 2020.\nEvolving graphical planner: Contextual global planning for\nvision-and-language navigation. In NeurIPS.\nDorbala, V . S.; Mullen Jr, J. F.; and Manocha, D. 2023.\nCan an Embodied Agent Find Your\" Cat-shaped Mug\"?\nLLM-Based Zero-Shot Object Navigation. arXiv preprint\narXiv:2303.03480.\nDorbala, V . S.; Sigurdsson, G.; Piramuthu, R.; et al. 2022.\nCLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language\nNavigation. arXiv preprint arXiv:2211.16649.\nDriess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; et al.\n2023. PaLM-E: An Embodied Multimodal Language\nModel. arXiv preprint arXiv:2303.03378.\nFried, D.; Hu, R.; Cirik, V .; Rohrbach, A.; et al. 2018.\nSpeaker-follower models for vision-and-language naviga-\ntion. In NeurIPS.\nGadre, S. Y .; Wortsman, M.; Ilharco, G.; Schmidt, L.; and\nSong, S. 2022. CLIP on Wheels: Open-V ocabulary Models\nare (Almost) Zero-Shot Object Navigators. arXiv preprint\narXiv:2203.10421v1.\nGao, C.; Chen, J.; Liu, S.; Wang, L.; Zhang, Q.; and Wu,\nQ. 2021. Room-and-object aware knowledge reasoning for\nremote embodied referring expression. In CVPR.\nGao, C.; Peng, X.; Yan, M.; Wang, H.; et al. 2023. Adap-\ntive Zone-Aware Hierarchical Planner for Vision-Language\nNavigation. In CVPR.\nGirshick, R. 2015. Fast R-CNN. In ICCV.\nGu, J.; Stefani, E.; Wu, Q.; Thomason, J.; and Wang, X.\n2022. Vision-and-Language Navigation: A Survey of Tasks,\nMethods, and Future Directions. In ACL.\nHao, W.; Li, C.; Li, X.; Carin, L.; and Gao, J. 2020. Towards\nlearning a generic agent for vision-and-language navigation\nvia pre-training. In CVPR.\nHe, K.; Huang, Y .; Wu, Q.; Yang, J.; et al. 2021. Landmark-\nRxR: Solving Vision-and-Language Navigation with Fine-\nGrained Alignment Supervision. In NeurIPS.\nHong, Y .; Rodriguez, C.; Qi, Y .; Wu, Q.; and Gould, S.\n2020a. Language and visual entity relationship graph for\nagent navigation. In NeurIPS.\nHong, Y .; Rodriguez-Opazo, C.; Wu, Q.; and Gould, S.\n2020b. Sub-Instruction Aware Vision-and-Language Nav-\nigation. In NeurIPS.\nHong, Y .; Wang, Z.; Wu, Q.; and Gould, S. 2022. Bridging\nthe Gap Between Learning in Discrete and Continuous En-\nvironments for Vision-and-Language Navigation. In CVPR.\nHong, Y .; Wu, Q.; Qi, Y .; Rodriguez-Opazo, C.; and Gould,\nS. 2021. VLN\nœBERT\n: A recurrent vision-and-language\nbert for navigation. In CVPR.\nHong, Y .; Zhou, Y .; Zhang, R.; Dernoncourt, F.; Bui, T.;\nGould, S.; and Tan, H. 2023. Learning navigational visual\nrepresentations with semantic map supervision. In ICCV.\nHuang, C.; Mees, O.; Zeng, A.; and Burgard, W. 2022. Vi-\nsual Language Maps for Robot Navigation. arXiv preprint\narXiv:2210.05714.\nKarpas, E.; Abend, O.; Belinkov, Y .; Lenz, B.; et al.\n2022. MRKL Systems: A modular, neuro-symbolic ar-\nchitecture that combines large language models, external\nknowledge sources and discrete reasoning. arXiv preprint\narXiv:2205.00445.\nKrantz, J.; Wijmans, E.; Majumdar, A.; Batra, D.; and Lee,\nS. 2020. Beyond the nav-graph: Vision-and-language navi-\ngation in continuous environments. In ECCV.\nKu, A.; Anderson, P.; Patel, R.; Ie, E.; et al. 2020. Room-\nAcross-Room: Multilingual Vision-and-Language Naviga-\ntion with Dense Spatiotemporal Grounding. In EMNLP.\nLi, J.; and Bansal, M. 2023. PanoGen: Text-Conditioned\nPanoramic Environment Generation for Vision-and-\nLanguage Navigation. In NeurIPS.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023a. Blip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597.\nLi, J.; Tan, H.; and Bansal, M. 2022. EnvEdit: Environment\nEditing for Vision-and-Language Navigation. In CVPR.\nLi, L. H.; Zhang, P.; Zhang, H.; Yang, J.; Li, C.; Zhong, Y .;\nWang, L.; Yuan, L.; Zhang, L.; Hwang, J.-N.; et al. 2022a.\nGrounded language-image pre-training. In CVPR.\nLi, M.; Wang, Z.; Tuytelaars, T.; and Moens, M.-F. 2023b.\nLayout-aware Dreamer for Embodied Referring Expression\nGrounding. In AAAI.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7648\nLi, X.; Zhang, Y .; Yuan, W.; et al. 2022b. Incorporating Ex-\nternal Knowledge Reasoning for Vision-and-Language Nav-\nigation with Assistant’s Help. Applied Sciences.\nMa, C.-Y .; Lu, J.; Wu, Z.; AlRegib, G.; et al. 2019. Self-\nmonitoring navigation agent via auxiliary progress estima-\ntion. arXiv preprint arXiv:1901.03035.\nMajumdar, A.; Aggarwal, G.; Devnani, B.; Hoffman, J.;\nand Batra, D. 2022. Zson: Zero-shot object-goal naviga-\ntion using multimodal goal embeddings. arXiv preprint\narXiv:2206.12403.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nParvaneh, A.; Abbasnejad, E.; Teney, D.; Shi, J. Q.; et al.\n2020. Counterfactual vision-and-language navigation: Un-\nravelling the unseen. In NeurIPS.\nPashevich, A.; Schmid, C.; and Sun, C. 2021. Episodic\ntransformer for vision-and-language navigation. In ICCV.\nPeng, B.; Galley, M.; He, P.; Cheng, H.; et al. 2023. Check\nyour facts and try again: Improving large language models\nwith external knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nQi, Y .; Pan, Z.; Hong, Y .; Yang, M.-H.; van den Hengel, A.;\nand Wu, Q. 2021. The road to know-where: An object-and-\nroom informed sequential bert for indoor vision-language\nnavigation. In ICCV.\nQi, Y .; Pan, Z.; Zhang, S.; Hengel, A. v. d.; and Wu, Q.\n2020a. Object-and-action aware model for visual language\nnavigation. In ECCV.\nQi, Y .; Wu, Q.; Anderson, P.; Wang, X.; et al. 2020b.\nReverie: Remote embodied visual referring expression in\nreal indoor environments. In CVPR.\nQiao, Y .; Qi, Y .; Hong, Y .; Yu, Z.; Wang, P.; and Wu, Q.\n2023a. HOP+: History-enhanced and Order-aware Pre-\ntraining for Vision-and-Language Navigation.IEEE TPAMI.\nQiao, Y .; Qi, Y .; Yu, Z.; Liu, J.; and Wu, Q. 2023b. March\nin chat: Interactive prompting for remote embodied referring\nexpression. In ICCV.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; et al.\n2021. Learning transferable visual models from natural lan-\nguage supervision. In ICML.\nScao, T. L.; Wang, T.; Hesslow, D.; Saulnier, L.; et al. 2022.\nWhat Language Model to Train if You Have One Million\nGPU Hours? arXiv preprint arXiv:2210.15424.\nSchick, T.; Dwivedi-Yu, J.; Dessì, R.; Raileanu, R.; et al.\n2023. Toolformer: Language models can teach themselves\nto use tools. arXiv preprint arXiv:2302.04761.\nShah, D.; Osi ´nski, B.; Levine, S.; et al. 2023. Lm-nav:\nRobotic navigation with large pre-trained models of lan-\nguage, vision, and action. In CoRL.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nEMNLP.\nTan, H.; Yu, L.; and Bansal, M. 2019. Learning to Navigate\nUnseen Environments: Back Translation with Environmen-\ntal Dropout. In NAACL.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; et al.\n2023. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971.\nVemprala, S.; Bonatti, R.; Bucker, A.; and Kapoor, A. 2023.\nChatgpt for robotics: Design principles and model abilities.\narXiv preprint arXiv:2306.17582.\nWang, S.; Montgomery, C.; Orbay, J.; Birodkar, V .; et al.\n2022. Less is More: Generating Grounded Navigation In-\nstructions from Landmarks. In CVPR.\nWang, X.; Huang, Q.; Celikyilmaz, A.; Gao, J.; et al. 2019.\nReinforced cross-modal matching and self-supervised imi-\ntation learning for vision-language navigation. In CVPR.\nWang, Z.; Li, J.; Hong, Y .; et al. 2023. Scaling data genera-\ntion in vision-and-language navigation. In ICCV.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned\nlanguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler,\nD.; et al. 2022a. Emergent abilities of large language mod-\nels. arXiv preprint arXiv:2206.07682.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022b. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint\narXiv:2201.11903.\nWu, C.; Yin, S.; Qi, W.; Wang, X.; Tang, Z.; and Duan, N.\n2023. Visual chatgpt: Talking, drawing and editing with vi-\nsual foundation models. arXiv preprint arXiv:2303.04671.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2022. React: Synergizing reasoning and act-\ning in language models. arXiv preprint arXiv:2210.03629.\nYongliang, S.; Kaitao, S.; Xu, T.; Dongsheng, L.; et al.\n2023. HuggingGPT: Solving AI Tasks with ChatGPT and\nits Friends in HuggingFace. arXiv:2303.17580.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhao, C.; Qi, Y .; and Wu, Q. 2023. Mind the Gap: Improving\nSuccess Rate of Vision-and-Language Navigation by Revis-\niting Oracle Success Routes. In ACM MM.\nZhao, Y .; Chen, J.; Gao, C.; Wang, W.; Yang, L.; Ren, H.;\nXia, H.; and Liu, S. 2022. Target-Driven Structured Trans-\nformer Planner for Vision-Language Navigation. In ACM\nMM.\nZhou, K.; Zheng, K.; Pryor, C.; Shen, Y .; Jin, H.; Getoor,\nL.; and Wang, X. E. 2023. ESC: Exploration with Soft\nCommonsense Constraints for Zero-shot Object Navigation.\narXiv preprint arXiv:2301.13166.\nZhu, F.; Zhu, Y .; Chang, X.; and Liang, X. 2020. Vision-\nlanguage navigation with self-supervised auxiliary reason-\ning tasks. In CVPR.\nZhu, W.; Qi, Y .; Narayana, P.; Sone, K.; Basu, S.; Wang,\nX. E.; Wu, Q.; Eckstein, M. P.; and Wang, W. Y . 2022. Diag-\nnosing Vision-and-Language Navigation: What Really Mat-\nters. In NAACL.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7649"
}