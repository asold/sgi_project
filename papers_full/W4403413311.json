{
  "title": "Debugging with Open-Source Large Language Models: An Evaluation",
  "url": "https://openalex.org/W4403413311",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Majdoub, Yacine",
      "affiliations": [
        "University of Gab√®s"
      ]
    },
    {
      "id": "https://openalex.org/A2744925813",
      "name": "Charrada Eya Ben",
      "affiliations": [
        "University of Gab√®s"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4307479317",
    "https://openalex.org/W4362679230",
    "https://openalex.org/W4402684113",
    "https://openalex.org/W4392270558",
    "https://openalex.org/W2914851262",
    "https://openalex.org/W2914825488"
  ],
  "abstract": "Large language models have shown good potential in supporting software\\ndevelopment tasks. This is why more and more developers turn to LLMs (e.g.\\nChatGPT) to support them in fixing their buggy code. While this can save time\\nand effort, many companies prohibit it due to strict code sharing policies. To\\naddress this, companies can run open-source LLMs locally. But until now there\\nis not much research evaluating the performance of open-source large language\\nmodels in debugging. This work is a preliminary evaluation of the capabilities\\nof open-source LLMs in fixing buggy code. The evaluation covers five\\nopen-source large language models and uses the benchmark DebugBench which\\nincludes more than 4000 buggy code instances written in Python, Java and C++.\\nOpen-source LLMs achieved scores ranging from 43.9% to 66.6% with\\nDeepSeek-Coder achieving the best score for all three programming languages.\\n",
  "full_text": "Debugging with Open-Source Large Language Models:\nAn Evaluation\nYacine Majdoub\nUniversity of Gabes, Tunisia\nyacinemajdoub@fsg.u-gabes.tn\nEya Ben Charrada\nUniversity of Gabes, Tunisia\neya.bencharrada@fsg.rnu.tn\nABSTRACT\nLarge language models have shown good potential in supporting\nsoftware development tasks. This is why more and more developers\nturn to LLMs (e.g. ChatGPT) to support them in fixing their buggy\ncode. While this can save time and effort, many companies prohibit\nit due to strict code sharing policies. To address this, companies\ncan run open-source LLMs locally. But until now there is not much\nresearch evaluating the performance of open-source large language\nmodels in debugging. This work is a preliminary evaluation of\nthe capabilities of open-source LLMs in fixing buggy code. The\nevaluation covers five open-source large language models and uses\nthe benchmark DebugBench which includes more than 4000 buggy\ncode instances written in Python, Java and C++. Open-source LLMs\nachieved scores ranging from 43.9% to 66.6% with DeepSeek-Coder\nachieving the best score for all three programming languages.\nKEYWORDS\nDebugging, Large Language Models, Open-Source LLMs\nACM Reference Format:\nYacine Majdoub and Eya Ben Charrada. 2024. Debugging with Open-Source\nLarge Language Models: An Evaluation. In Proceedings of the 18th ACM /\nIEEE International Symposium on Empirical Software Engineering and Mea-\nsurement (ESEM ‚Äô24), October 24‚Äì25, 2024, Barcelona, Spain. ACM, New York,\nNY, USA, 7 pages. https://doi.org/XXXXXX/XXXXXXXXXXXXX\n1 INTRODUCTION\n\"I‚Äôd spend an hour figuring out what exactly goes wrong, then five\nminutes writing the code to fix it, and then half an hour testing the\nwhole thing. That‚Äôs just over 5% coding vs. almost 95% non-coding. \" 1\nDebugging is known to be time consuming and frustrating.\nTherefore it is not surprising to find out that developers are turning\nto large language models to help them solve their problems. In\na study with practitioners, Khojah et al. [ 1] found that software\nengineers were found to turn often to chatGPT for assistance in\nvarious software engineering tasks.\n1text taken from an answer on stackoverflow regarding the time spent debugging\nhttps://softwareengineering.stackexchange.com/a/93323\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN XXXXXXXXXXXXXXXX\nhttps://doi.org/XXXXXX/XXXXXXXXXXXXX\nRecent research showed promising results in using LLMs for soft-\nware Engineering tasks in general and for debugging in particular.\nFor example, LLMs were able to perform well in bug reproduc-\ntion [2], fault localisation [3] and program repair [4]. Despite these\nadvantages, using current state of the art LLMs such as ChatGPT\ncan be inappropriate for practitioners due to code sharing policies.\nIn fact, most companies consider their code to be private and don‚Äôt\nwant it to be sent to LLMs run by third parties. A solution to this\nproblem would be to run an open source LLM locally. So far, there\nhas been very limited assessments of the debugging capabilities of\nopen-source large language models. In fact, earlier works mostly\nfocus on evaluating code generation capabilities, for which many\nbenchmarks exist such as the famous OpenAI‚Äôs HumanEval [ 5]\nand its descendants (e.g. HumanEval+ [ 6] and Multilingual Hu-\nmanEval [7]) or the Google‚Äôs MBPP [8].\nThe goal of this work is to evaluate and compare the capabilities\nof open-source large language models in performing debugging\ntasks. We would like to answer the following two research ques-\ntions:\n‚Ä¢RQ1: How do open source LLMs perform in debugging? To\nanswer this question, we use benchmarking to evaluate five\nopen-source LLMs. The benchmark we used includes more\nthan 4000 buggy code instances in Python, C++ and Java.\n‚Ä¢RQ2: How does the performance of open-source LLMs in\ncode generation impact their performance in debugging? We\ncompare the scores that the LLMs obtained for debugging\nwith the scores that they achieved for coding as evaluated\nby the HumanEval Benchmark.\nOur evaluation suggests that although less capable than the most\nadvanced closed source models (e.g. GPT-4), some open source mod-\nels were able to achieve decent results compared to their relatively\nsmall size. For instance, DeepSeek-coder-instruct, which has only\n34B parameters, achieved a score above 63% in all three program-\nming languages. We also found that except for DeepSeek-coder, all\nmodels that achieved a higher scores in HumanEval also got better\nscores in debugging.\nThe contributions of this work are:\n‚Ä¢We conduct an empirical study that evaluates the debugging\ncapabilities of open source Large Language Models using\na large benchmark that includes a few thousands of buggy\ncode instances\n‚Ä¢We compare the debugging capabilities of the open source\nLLMs to their coding capabilities as evaluated by the Hu-\nmanEval benchmark\n‚Ä¢We provide an extensive discussion of the strengths and\nlimitations of current debugging and coding benchmarks\narXiv:2409.03031v1  [cs.SE]  4 Sep 2024\nESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain ‚Äî\n2 OPEN SOURCE LARGE LANGUAGE MODELS\nThere are many open-source LLMs available in the market. Al-\nthough nearly2 all models use the transformer architecture, they\ndiffer in their capabilities due to various factors such as model size,\nquality and volume of training data, and fine-tuning methods.\nFor this evaluation, we selected five reputed models. Four of them\nare code models, while the last one is a general-purpose model.\n2.1 Code models\nWe selected the coding models that achieved the best results on\nthe HumanEval benchmark [5]. HumanEval is a code generation\nbenchmark released by OpenAI that includes 146 coding tasks.We\npresent each of the coding models in the following paragraphs.\n2.1.1 Code Llama. Code Llama [9] is a family of large language\nmodels that is specialised for code, based on Llama2. Code Llama\nmodels have been created by fine-tuning the general language\nmodel Llama2 using code specific datasets. The developers of Codel-\nlama found that for a given budget, fine-tuning the generic Llama2\nto generate code outperforms the same architecture trained on code\nonly. The training was done with publicly available code (mostly\nnear-deduplicated dataset), which includes 8% of natural language\ntext related to code such as discussions or questions and answers\nincluding code snippets. In addition to supporting several natural\nlanguages, the Code Llama models are trained to handle long con-\ntexts of up to 100K tokens. Meta AI released Codellama in three\nmain variants namely (1) Code Llama , which is the foundation\nmodel (2) Code Llama - Python , which is specialized for python\ncode generation and Code Llama - Instruct , which is fine-tuned to\nfollow human instructions. All models are available in four sizes:\n7B, 13B, 34B and 70B.\nFor this evaluation, we use the Code Llama - Instruct 70B variant.\nThis variant was trained using 1 trillion tokens and achieved the\nbest performance on HumanEval with a 67.8% pass@1.\n2.1.2 Phind-Codellama. Phind-Codellama [10] is a fine-tuned ver-\nsion of Code Llama 34B. The first version of Phind-Codellama was\nfine-tuned on a dataset of nearly 80,000 programming problems\nand their corresponding solutions. The second version is Phind-\nCodeLlama-34-v2, which was initialised from the first version, was\ntrained on 1.5B additional tokens. Although Phind-Codellama has\nsmaller number of parameters compared to the larger Code Llama\n70B, it was able to achive relatively high results on HumanEval.\nFor instance Phind-CodeLlama-34B-v2 achieved 73.8% pass@1 on\nHumanEval.\n2.1.3 WizardCoder. WizardCoder [11] is a family of LLMs that use\nthe Evol-Instruct method [12], an instruction fine tuning method\nthat makes the code instructions more complex and which enhances\nthe performance of coding models. Wizardcoder is available in five\ndifferent sizes ranging from 1B to 33B parameters. The 15B version\nof WizardCoder [ 11], the results of a collaboration between re-\nsearchers from Microsoft and researchers from Haong Kong Baptist\nUniversity, is a fine-tuned version of StarCoder [13] and it achieved\n57.3 % pass@1 on HumanEval. The 33B version is trained from the\n2All models we found used the transformer architecture.\nDeepSeek-Coder-base model and achieved 79.9% pass@1 on Hu-\nmanEval [5]. In this evaluation we use the WizardCoder-33B-V1.1.\n2.1.4 Deepseek-Coder. DeepSeek-Coder [14] is a series of code\nmodels trained on a dataset comprising 2 trillion tokens from 87\nprogramming languages. The dataset is composed of 87% code and\n13% natural language in English and Chinese. The model is available\nin various sizes, from 1.3B to 33B parameters. The DeepSeek-Coder-\nInstruct variant is an enhancement of the base model that was\nfine-tuned with an additional 2 billion tokens of instruction data.\nThis improved the model‚Äôs ability to execute coding tasks given\nusing human instructions. DeepSeek-Coder-Base 33B achieved\n50.3% pass@1 on HumanEval, while DeepSeek-Coder-Instruct-33B\nachieved 69.2% pass@1 on HumanEval. We used DeepSeek-Coder-\nInstruct-33B in our evaluation.\n2.2 General-Purpose model: Llama 3\nThe last model we chose is Llama3, a general purpose LLM. We\nselected it because it is the best open source LLM available for now,\nand we wanted to compare its capabilities to the code-specialized\nlarge language models.\nLLama3, which is developed by Meta AI, was released in two\nsizes: 8B and 70B each with a pre-trained and instruction finetuned\nversion. Data quality was a major focus for LLama 3, the model\nhas been pre-trained on over 15 trillion high-quality tokens from\npublicly available sources, seven times more than LLama 2. The\ntraining data incorporates four times more coding data to boost\ncapabilities in that domain and over 5% of the data covers 30+ lan-\nguages beyond English. The dataset was filtered using a serie of\nfiltering pipelines, heuristic filtering, NSFW detection, deduplica-\ntion, and quality classifiers. The model also utilizes a more efficient\ntokenizer compared to the previous models of Meta AI, and it uses\ngrouped query attention (GQA) to improve inference efficiency and\nto handle sequences of up to 8,192 tokens.\nLlama3 8B achieved 62.2% pass@1 on HumanEval while Llama3\n70B achieved 81.7% pass@1 [ 15]. For this evaluation, we used\nLlama3 70B.\n3 STUDY DESIGN\nThe evaluation is done with Benchmarking. Benchmarking can be\nused to efficiently compare different methods, techniques and tools\nin empirical software engineering [16][17] .\nWe have chosen the benchmark DebugBench [18], one of the\nlargest and most recent benchmarks for debugging. In this section,\nwe present the benchmark and the experimental setup used for the\nevaluation.\n3.1 Benchmark\nDebugBench [18] is a benchmark designed to evaluate the debug-\nging capabilities of Large Language Models. It consists of a dataset\nof 4,253 instances of buggy code, collected from code solutions\nin LeetCode. The goal of DebugBench is to provide a larger scale\nevaluation that covers fine-grained bug types, and mitigates data\nleakage risks.\nDebugBench has two main advantages: (1) it uses code problems\nthat are quite challenging not only for developers but also for large\nDebugging with Open-Source Large Language Models: An Evaluation ESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain\nTable 1: Number of buggy code instances per programming\nlanguage in DebugBench\nProgramming Language Buggy Instances\nC++ 1438\nJava 1401\nPython 1414\nTotal 4253\nlanguage models and (2) it provides a comprehensive test suit that\nallows verifying whether the bug was fixed or not.\n3.1.1 Data. Tian et al [18] created the dataset using problem de-\nscriptions and code solutions from LeetCode. The authors used\nGPT-4 to automatically introduce bugs to the code and then used\nhuman inspections to check the integrity of the benchmark.\nTo minimize the risk of leakage the authors used code that was\nreleased on LeetCode after June 2022 with the average release date\nbeing April 2023. To ensure that the extracted code is correct, the\nauthors selected only code that passes all the tests related to it.\nThe authors of the benchmark develop a bug taxonomy based on\nBarr‚Äôs classification criteria that covers four major bug categories\n(Syntax, Reference, Logic and Multiple) as well as 18 minor bug\ntypes. The bugs were introduced by instructing GPT-4 to add a cer-\ntain type of bugs to the code. Since GPT sometimes fails in including\nbugs in the code, the authors filtered out the code that does not fail\ncertain tests. The benchmark includes 761 instances with syntax\nerrors, 684 instances with reference errors, 590 instances with logic\nerrors and 2218 instances with multiple errors. A description of the\nnumber of instances for each programming language is provided\nin Table 1\n3.1.2 Metrics. DebugBench assesses whether a bug is fixed or not\nby using a set of tests that is provided by LeetCode. If all tests pass,\nthen the bug is considered to be fixed, otherwise, the bug is not\nfixed. The metric used in DebugBench is the Pass Rate , which is\nthe number of bugs for which all corresponding tests have passed\n(repaired bugs) divided by the total number of bugs. [ 18]. More\nformally, the Pass Rate is defined as follows: for each buggy code\nùúÉùëñ and its fixed version ùúÉ‚àó\nùëñ , there is a set of test cases: (ùë•0\nùëñ ,ùë¶0\nùëñ ),\n(ùë•1\nùëñ ,ùë¶1\nùëñ ), ..., (ùë•ùëö\nùëñ ,ùë¶ùëö\nùëñ )to test it, where ùë•ùëñ is the input and ùë¶ùëñ is the\ncorresponding desired output. Let ùëéùúÉ (ùë•)= ùë¶ denote a program\na, based on the script ùúÉ that maps input ùë• to output ùë¶ . A bug is\nconsidered to be repaired if all tests pass which can be referred to\nas\nùëö√õ\nùëó=0\n[ùëéùúÉ‚àó\nùëñ\n(ùë•ùëó\nùëñ )= ùë¶ùëó\nùëñ ]\nThis criteria allows for a conservative measure of the bug fixing\ncapabilities. In fact, if the instance contains multiple bugs, it is\nconsidered to be repaired only if all bugs within the instance are\nfixed. For ùëõbug instances, the pass rate would be:\nùëÉùëÖ =\nùëõ‚àëÔ∏Å\nùëñ=0\n√ìùëöùëó\nùëó=0 [ùëéùúÉ‚àó\nùëñ\n(ùë•ùëó\nùëñ )= ùë¶ùëó\nùëñ ]\nùëõ √ó100%\n3.2 Experimental setup\nTo run the open-source LLMs, we use the platform together.ai 3.\nThis platform offers an interface that we used to prompt each of the\nmodels and get their responses. To test whether the fix is correct\nor not we use the LeetCode test suite to test the proposed answer.\nThe tests were accessed using an interface to LeetCode. If all tests\npassed, the fix was considered as successful.\nPrompt design. We used a zero-shot prompting. The prompt\ngiven to the LLM contained a description of the problem to be\nsolved, the buggy code, as well as a description of the task to be\ndone, i.e. debugging the code. We tweaked the prompts used by the\noriginal DebugBench paper so that we only get the fixed code in\nthe output. The prompt we used is presented in Listing 1.\nListing 1: Prompt\nObserve the following << Programming language >>\nfaulty code which is complete with no extra context .\nYour task is to fix up the code , not change it.\nYou have to write the fixed code ( class Solution ).\nDo not write anything else beside the fixed code\nin your response .\nBuggy Code : << Buggy code >>\nGoal : << Description >>\nCode extraction. Some models returned additional explanatory\ntext with the code, as shown in the WizardCoder example in List-\ning 2. To avoid failing the tests because of the additional text, we\nwrote a script to extract the code (class Solution) from the answer\ngenerated by the LLM. The extracted code is then used to run the\nautomated tests.\nListing 2: An output example of WizardCoder\nSolution :\nclass Solution :\ndef findComplement (self , num : int ) -> int :\ncom = ''\nwhile num > 0 :\nif num % 2 == 1:\ncom = '0' + com\nelse :\ncom = '1' + com\nnum = num // 2\nreturn int (com ,2)\nExplanation :\nThe bug in the code is that the com string is not being\nupdated properly . In the if block , we are adding 0\ninstead of '0' and in the else block , we are adding 1\ninstead of '1'. We need to add the string representation\nof 0 and 1 to the com string . Also , we need to return the\ninteger value of the binary string , so we need to remove\nthe first character of the string before converting it to\ninteger .\nExperiment repetition. Since there is some randomness in the\nresponse of LLMs, we repeated the experiment twice for all five\nmodels on the C++ dataset. The goal of the repetition is to verify\nthe reliability of the results.\n3https://www.together.ai/\nESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain ‚Äî\nTable 2: Debugging performance & running costs of the evaluated open-source large language models\nModel Python Java C++ Final score Costs\nFixed ProblemsPass rate Fixed ProblemsPass rate Fixed ProblemsPass rate\nCodellama-instruct-70b 589/1414 41.65% 553/1401 39.47% 728/1438 50.62% 43.96% $4.20\nPhind-codellama-34b-v2 694/1414 49.08% 550/1401 39.25% 830/1438 57.71% 48.76% $3.80\nWizardCoder_instruct-33b 813/1414 57.49% 708/1401 50.53% 834/1438 58.98% 55.37% $3.70\nDeepseek-coder-instruct-33b893/1414 63.15% 971/1401 69.30% 971/1438 64.81% 66.65% $3.70\nLlama3-70b 880/1414 62.23% 755/1401 53.89% 859/1438 59.73% 58.61%} $4.10\n4 RESULTS\n4.1 RQ1: Performance of Open-Source LLMs\nWe report in Table 2, the number of fixed problems for each model\nas well as the achieved pass rate for each programming language.\nWe also report the average pass rate for all languages and the costs\nof running the evaluation in USD. The evaluated LLMs achieved\nresults ranging from 43.9% to 66.6%. The best score was achieved\nby the code model DeepSeek-Coder which achieved a pass rate\nabove 63% for each of the languages. Both Codellama-instruct and\nPhind-Codellama achieved a pass rate below 50%. Still, we notice\nthat Phind-Codellama which is a fine-tuning of the Codellama-\n34B achieved better results than the larger Codellama-70B. Llama3,\nwhich is a general purpose LLM achieved almost 60% pass rate,\nwhich is considerably better than Llama2-based code models. In\nFigure 1 We see that DeepSeek-Coder performed best on all three\nprogramming languages, while the Llama-2 based models (Codel-\nlama and Phind-Codellama) had the lowest scores for all three\nlanguages. This suggests that if a model is fluent in one program-\nming language, then it is also likely to be fluent in other languages.\nModel size was not a determining factor in the performance\nof the models. In fact, some medium sized models, such as Phind-\nCodellama and DeepSeek-Coder, were able to achieve better scores\nthan Codellama which had twice the size.\nRegarding the costs of running the models, smaller models (33B\nand 34B) costed an average of 0.08 cent per code instance while\nlarger models (70B) costed an average of 0.09 cent per code instance.\nSo the difference in price between the models is negligible.\nClosed source models, namely GPT-4 and GPT-3.5 from OpenAI\nachieved 75.0% and 62.1% respectively as reported by the Debug-\nBench paper [18]. None of the evaluated tools was able to get a\nscore that is comparable to GPT-4 and only DeepSeek-Coder was\nable to achieve results that are better than GPT-3.5\nRQ1 answer:There is a lot of variation between the scores of the\ndifferent models, but some open source models were able to achieve\ndecent results. Compared to top closed source models, only one\nopen source was able to achieve results that are better than the\nGPT-3.5 score.\n4.2 RQ2: Relation between coding and\ndebugging performance\nWe report the scores that the models achieved on HumanEval and\non DebugBench in Figure 2. All five models achieved higher scores\nin the HumanEval coding benchmark compared to the benchmark\nDebugBench. In fact, all models were able to successfully solve\nthe HumanEval coding problems with a pass rate ranging between\n67% and 81%. The only benchmark that showed similar capabili-\nties in coding and debugging is the DeepSeek-Coder model which\nachieved a pass rate of 69.2% on HumanEval and a pass rate of\n66.65% on DebugBench. When comparing the performance of the\nLLMs on both benchmarks, we see that except for DeepSeek-Coder,\nthe models who had better results on HumanEval also achieved\nbetter scores in DebugBench. From these observations, we see that\nalthough there is a hint that models that are better in coding might\nbe better in debugging, we see no definite connection between the\nperformance of LLMs in coding, as evaluated by HumanEval, and\ntheir performance in debugging as evaluated by DebugBench.\nRQ2 answer:For four out of the five evaluated LLMs we noticed\na relation between the performance of the models on HumanEval\nand their performance on DebugBench.\nFigure 1: A visualisation of the debugging performance of\nthe LLMs for code in Python, C++ and Java\nDebugging with Open-Source Large Language Models: An Evaluation ESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain\nTable 3: Scores achieved for the C++ dataset during two runs and the calculated Mean and Standard deviation for each model\nModel First test pass rate Second test pass rate Mean Standard deviation\nCodellama-Instruct-70b 50.62% 51.55% 51.08% 0.465\nPhind-Codellama-34b-v2 57.71% 55.94% 56.82% 0.885\nWizardCoder-Instruct-33b 58.98% 55.79% 57.38% 1.595\nDeepSeek-Coder-Instruct-33b 64.81% 66.33% 65.57% 0.76\nLlama3-70b 59.73% 60.32% 60.02% 0.295\nFigure 2: Pass rates achieved by the open-source LLMs when\nevaluated on HumanEval and on DebugBench\n5 DISCUSSION\n5.1 Comparing with previous results\nThere has been a few other works that evaluated the capabilities of\nLLMs for debugging. In this section, we compare our results with\nthe results of other researchers.\nFirst, we compare our results to the score reported by Tian et\nal [18], the authors of the DebugBench paper. The authors tested\nthree open source models ùêµùëôùëúùëúùëö, ùëêùëúùëëùëíùëôùëôùëéùëöùëé and ùëêùëúùëëùëíùëôùëôùëéùëöùëé ‚àíùëñùëõùë†ùë°,\nand got a pass rate of 0.0 for all three of them. This means that none\nof the open source tools could repair any of the bugs. In our case,\nthe models were able to achieve decent results. This is probably due\nto the fact that all code models returned answers that contained not\nonly code but also explanatory text. The only model that returned\ncode only was the general purpose llama3. The additional text\nmakes the tests fail on LeetCode and this explains the 0.0 score\nobtained in the evaluation of Tian et al [18]. In our experiment, we\nused a script to extract the code only from the answer and this lead\nto a positive performance of the models.\nLee et al. [ 19] compared the debugging performance of some\nclosed source and open source LLMs using benchmarks in C, Java\nand Python. The authors generated 3 patches for each bug and\nlooked for a plausible or correct patch among the generated re-\nsponses. Codellama generated correct patches for 25/40 bugs in\nJava and for 33/40 bugs in Python, while DeepSeek-Coder achieved\n30/40 and 25/40 correct patches for Java and Python respectively.\nThe authors found that both GPT-3.5-Turbo-0125 (175B) and GPT-4\ngenerated a higher number of correct patches than the open source\nmodels. In our experiment, DeepSeek-Coder achieved better output\nthan codellama in all programming languages. This difference in\nthe results might be due to the fact that Lee et al. [ 19] used the\nDeepSeek-Coder-Base, while in our experiment we use DeepSeek-\nCoder-Instruct. The instruct version seems to have better coding\ncapabilities as it is reported to achieve 69.2% pass@1 on HumanEval\ncompared to 50.3% pass@1 for the base version.\nIn a study about vulnerability detection, Steenhoek et al. [ 20]\nfound that LLMs were unable to differentiate between buggy and\nfixed code. They report that LLMs performed only sligthly better\nthan random guessing and that they performed far worse on com-\nplex debugging tasks from DBGBench. The evaluation was done\nusing 100 functions from the SVEN dataset which is in C/C++ as\nwell as the 27 bugs from DBGBench [21]. These low results could\nbe explained by the complexity of the tasks performed in [20]. Pre-\nvious research by Huang and Changs [22] has already shown that\nLLMs seem to be unable to manage complex tasks. The authors\nalso note that existing benchmarks might be too simple to assess\nreasoning ability correctly [22].\nIn another evaluation of code generation capabilities by Liu et\nal. [6], the authors used HumanEval+ which is an improvement of\nthe classic HumanEval. The authors found that the two open source\nmodels Phind-Codellama and WizardCoder achieved scores that\nare better than ChatGPT but worse than GPT-4. In our experiment,\nboth open source models achieved a score that is lower than GPT-\n4 and GPT-3.5. Since the authors did not specify the version of\nchatGPT that was used, this could be due to the fact that they used\na version of chatGPT that is less effective than GPT-3.5.\n5.2 Contamination\nOne of the challenges in the evaluation of LLMs is contamina-\ntion [23‚Äì25]. For example, Jain et al. [26] found that there was a\ndrop in the performance of DeepSeek-Coder-Instruct on LeetCode\nproblems that were released since September 2023, its release date.\nThe authors interpret this as an indication of a potential contam-\nination. Although some research is being conducted on how to\nevaluate and remove contamination [ 27], decontamination does\nseem to be an easy task. In a study of code LLMs, Cao et al. [ 28]\nfound that existing countermeasures for contamination such as\nusing more recent data, using curated datasets or syntactic refac-\ntoring may not be effective. Among the models we evaluated, only\ntwo mentioned using a decontamination strategy. OpenAI‚Äôs de-\ncontamination methodology seems to have been applied to the\nESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain ‚Äî\nPhind‚Äôs dataset, and the DeepSeek-Coder developers mention filter-\ning out data from benchmarks such as HumanEval, MBPP, GSM8K\nand MATH. All other models didn‚Äôt mention any decontamination\nstrategy. The DebugBench data has been published on GitHub on\nJanuary 9th, so all evaluated models had a knowledge cutoff date\nprior to the benchmark release. Although this might decrease the\ncontamination threat, it cannot fully eliminate it due to the fact\nthat the LeetCode problems and solutions might have been used\nfor pre-training the models.\n5.3 Result reliability\nLLMs are known to have randomness in their responses. So to check\nthe reliability of the results, we run the experiment twice on the\nC++ dataset. We report the scores for both experiments in Table 3.\nWe performed a statistical analysis of the results by calculating the\nmean and the standard deviation for each pair of measurements,\nthe results are reported in the same Table. The deviations for all\npair range between 0.29 and 1.59 which is relatively small. This\nindicates that the measurements are consistent and therefore likely\nreliable.\n6 THREATS TO VALIDITY\nInternal validity. Similarly to other LLM evaluations, we face\na major internal threat due to the possible overlap between the\ntraining data and the evaluation dataset. We have already discussed\npossible contamination in Section 5.2. DebugBench was published\nafter the knowledge cutoff date of the models. Although this might\nlimit the threat, it cannot be fully eliminated because the used prob-\nlems and their solutions existed on LeetCode before the benchmark\nrelease. The threat might also limited by the fact that the tests used\nto evaluate the debugging capabilities are not public and are only\naccessible for running via the LeetCode platform, so we know that\nthese tests were not included in the training data of the evaluated\nLLMs.\nThe randomness in LLMs can also constitute a threat to the inter-\nnal validity of the experiment. In fact, LLMs can produce different\nanswers for the same prompt. To limit this threat, we repeated the\nexperiment with C++ data twice. Our analysis in Section 5.3 shows\nthat the measurements are consistent and likely to be reliable.\nExternal validity. The main external validity threat lies in the\nbenchmark code not being generalizable to other types of code. We\nargue that this threat is limited since the LeetCode dataset covers\na variety of coding problems with different levels of difficulties. It\nalso covers code in three different programming languages. Never-\ntheless, the results might not be generalizable to coding problems\nthat are of different nature such as front-end developement, or code\nthat uses specific libraries. In the future, We will evaluate the LLMs\nwith more datasets.\n7 RELATED WORK\n7.1 Use of LLM for debugging\nThe promising results about the capabilities of LLMs in software\nengineering lead to a surge in approaches that use LLMs to support\ndebugging activities. Kang et al. [29] introduced AutoSD, a method\nthat leverages large language models and debuggers to automati-\ncally generate hypotheses and interact with buggy code, enabling\nconclusions to be drawn prior to patching. Feng et al. [30] presented\nAdbGPT, a lightweight approach that employs prompt engineering\nto reproduce bugs from reports automatically, without the need\nfor training or hard-coding. Zhong et al. [ 31] developed LDB, a\ndebugging framework designed to assist large language models in\nrefining generated programs by utilizing runtime execution data,\nsegmenting programs into basic blocks, and tracking intermediate\nvariable values after each block. Singh et al. [32] proposed Panda, a\nframework aimed at providing context grounding to pre-trained\nlarge language models, thereby generating more useful and con-\ntextually relevant troubleshooting recommendations. Bouzenia et\nal. [33] introduced RepairAgent, an autonomous program repair\nagent that relies on a large language model. RepairAgent interleaves\nthe processes of gathering information about the bug, collecting\nrepair ingredients, and validating fixes, while dynamically decid-\ning which tools to invoke based on the collected information and\nfeedback from previous fix attempts.\n7.2 Evaluation of LLMs in debugging\nSeveral evaluations have been conducted to evaluate of the perfor-\nmance of LLMs in debugging. For example, Wu et al. [3] investigated\nthe capabilities of ChatGPT-3.5 and ChatGPT-4 in fault localisation.\nSobania et al. [34] evaluated the bug fixing performance of chat-\nGPT using the QuixBugs benchmark. Tian at al. [18] evaluated the\nperformance of five closed and open-source large language models\nusing DebugBench. Lee et al. [19] compared their agent to other\nLLMs including two open-source LLMs, namely CodeLlama and\nDeepSeek-Coder.\nMost of these works focused on evaluating the performance of\nthe chatGPT, which is a closed source model. Only the works of\nTian et al. [18] and Lee et al. [19] cover some open source models.\nIn this work, our goal was to evaluate different open-source LLMs\nand compare their capabilities.\n8 CONCLUSION AND FUTURE WORK\nIn this work, we evaluated the debugging capabilities of five open-\nsource large language models. The evaluation was done using De-\nbugBench, a benchmark that includes a dataset of 4253 buggy code\ninstances in Python, C++ and Java. Our results show that the ca-\npabilities of all the evaluated open-source LLMs are lower than\nthe capabilities of the most recent closed-source model (GPT-4).\nStill, considering their relatively small size, some open-source LLMs\nwere able to achieve decent results. For instance, DeepSeek-Coder\nwhich is only 33B in size achieved a score above 66%.\nOne limitation of our evaluation, is that the used code instances\nare limited to one class only and are mostly solutions to algorithmic\nproblems. So In the future, we would like to evaluate open-source\nLLMs using a wider variety of code types. Also we would like to\nevaluate the usefulness of the LLMs for practitioners when per-\nforming debugging tasks, with a special focus on complex tasks.\nFinally, we intend to explore how the debugging performance of\nopen-source LLMs is impacted by prompt engineering and chain-\nof-thought prompting.\nDebugging with Open-Source Large Language Models: An Evaluation ESEM ‚Äô24, October 24‚Äì25, 2024, Barcelona, Spain\nREFERENCES\n[1] Khojah Ranim, Mohamad Mazen, Leitner Philipp, and Neto Francisco Gomes\nde Oliveira. Beyond code generation: An observational study of chatgpt usage in\nsoftware engineering practice. In Proc. ACM Softw. Eng. , 2024.\n[2] Kang Sungmin, Yoon Juyeon, and Yoo Shin. Large language models are few-shot\ntesters: Exploring llm-based general bug reproduction. In 2023 IEEE/ACM 45th\nInternational Conference on Software Engineering (ICSE) , pages 2312‚Äì2323. IEEE,\n2023.\n[3] Wu Yonghao, Li Zheng, Zhang Jie M, Mike Papadakis, Mark Harman, and Yong\nLiu. Large language models in fault localisation. arXiv preprint arXiv:2308.15276 ,\n2023.\n[4] Xia Chunqiu Steven, Wei Yuxiang, and Zhang Lingming. Automated program\nrepair in the era of large pre-trained language models. In 2023 IEEE/ACM 45th\nInternational Conference on Software Engineering (ICSE) , pages 1482‚Äì1494. IEEE,\n2023.\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan ..., and Wojciech Zaremba. Evaluating large\nlanguage models trained on code, 2021.\n[6] Liu Jiawei, Xia Chunqiu Steven, Wang Yuyao, and Zhang Lingming. Is your\ncode generated by chatgpt really correct? rigorous evaluation of large language\nmodels for code generation. Advances in Neural Information Processing Systems ,\n36, 2024.\n[7] Athiwaratkun Ben, Gouda Sanjay Krishna, Wang Zijian, Li Xiaopeng, Tian\nYuchen, Tan Ming, Ahmad Wasi Uddin, Wang Shiqi, Sun Qing, Shang Mingyue,\net al. Multi-lingual evaluation of code generation models. In The Eleventh\nInternational Conference on Learning Representations , 2022.\n[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\nMichalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,\nand Charles Sutton. Program synthesis with large language models, 2021.\n[9] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-\nqing Ellen Tan, ..., and Gabriel Synnaeve. Code llama: Open foundation models\nfor code, 2024.\n[10] Phind. Phind, phind/phind-codellama-34b-v2 - hugging face. URL\nhttps://huggingface.co/Phind/Phind-CodeLlama-34B-v2.\n[11] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,\nChongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Em-\npowering code large language models with evol-instruct, 2023.\n[12] Xu Can, Sun Qingfeng, Zheng Kai, Geng Xiubo, Zhao Pu, Feng Jiazhan, Tao\nChongyang, and Jiang Daxin. Wizardlm: Empowering large language models to\nfollow complex instructions. arXiv preprint arXiv:2304.12244 , 2023.\n[13] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,\nChenghao Mou, and Harm de Vries. Starcoder: may the source be with you!,\n2023.\n[14] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,\nGuanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng\nLiang. Deepseek-coder: When the large language model meets programming ‚Äì\nthe rise of code intelligence, 2024.\n[15] Meta AI. llama3 ¬∑ hugging face. URL https://huggingface.co/meta-llama/Meta-\nLlama-3-8B-Instruct.\n[16] Hasselbring Wilhelm. Benchmarking as empirical standard in software engineer-\ning research. In Proceedings of the 25th International Conference on Evaluation\nand Assessment in Software Engineering , pages 365‚Äì372, 2021.\n[17] Paul Ralph, Sebastian Baltes, Domenico Bianculli, Yvonne Dittrich, Michael\nFelderer, Robert Feldt, ..., and Sira Vegas. ACM SIGSOFT empirical standards.\nCoRR, abs/2010.03525, 2020.\n[18] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu,\nZhiyuan Liu, and Maosong Sun. Debugbench: Evaluating debugging capability\nof large language models, 2024.\n[19] Cheryl Lee, Chunqiu Steven Xia, Jen-tse Huang, Zhouruixin Zhu, Lingming\nZhang, and Michael R Lyu. A unified debugging approach via llm-based multi-\nagent synergy. arXiv preprint arXiv:2404.17153 , 2024.\n[20] Steenhoek Benjamin, Rahman Md Mahbubur, Roy Monoshi Kumar, Alam Mirza\nSanjida, Barr Earl T, and Le Wei. A comprehensive study of the capabilities of\nlarge language models for vulnerability detection.arXiv preprint arXiv:2403.17218 ,\n2024.\n[21] B√∂hme Marcel, Soremekun Ezekiel O, Chattopadhyay Sudipta, Ugherughe Ema-\nmurho, and Zeller Andreas. Where is the bug and how is it fixed? an experiment\nwith practitioners. In Proceedings of the 2017 11th joint meeting on foundations of\nsoftware engineering , pages 117‚Äì128, 2017.\n[22] Huang Jie and Chang Kevin Chen-Chuan. Towards reasoning in large language\nmodels: A survey. In61st Annual Meeting of the Association for Computational Lin-\nguistics, ACL 2023 , pages 1049‚Äì1065. Association for Computational Linguistics\n(ACL), 2023.\n[23] Oscar Sainz, Jon Ander Campos, Iker Garc√≠a-Ferrero, Julen Etxaniz, Oier Lopez\nde Lacalle, and Eneko Agirre. Nlp evaluation in trouble: On the need to measure\nllm data contamination for each benchmark. arXiv preprint arXiv:2310.18018 ,\n2023.\n[24] Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen\nZhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. How much are llms\ncontaminated? a comprehensive survey and the llmsanitize library.arXiv preprint\narXiv:2404.00699, 2024.\n[25] Simone Balloccu, Patr√≠cia Schmidtov√°, Mateusz Lango, and Ond≈ôej Du≈°ek. Leak,\ncheat, repeat: Data contamination and evaluation malpractices in closed-source\nllms. arXiv preprint arXiv:2402.03927 , 2024.\n[26] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida\nWang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench:\nHolistic and contamination free evaluation of large language models for code.\narXiv preprint arXiv:2403.07974 , 2024.\n[27] Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, and Ion Stoica.\nRethinking benchmark and contamination for language models with rephrased\nsamples. arXiv preprint arXiv:2311.04850 , 2023.\n[28] Jialun Cao, Wuqi Zhang, and Shing-Chi Cheung. Concerned with data con-\ntamination? assessing countermeasures in code language model. arXiv preprint\narXiv:2403.16898, 2024.\n[29] Kang Sungmin, Chen Bei, Yoo Shin, and Lou Jian-Guang. Explainable automated\ndebugging via large language model-driven scientific debugging. Proceedings of\nthe 45th International Conference on Software Engineering , 2023.\n[30] Feng, Sidong, Chen, and Chunyang. Prompting is all you need: Automated\nandroid bug replay with large language models. In Proceedings of the 46th\nIEEE/ACM International Conference on Software Engineering , pages 1‚Äì13, 2024.\n[31] Lily Zhong, Zilong Wang, and Jingbo Shang. Ldb: A large language model\ndebugger via verifying runtime execution step-by-step. CoRR, February 2024 ,\nFebruary 2024.\n[32] Singh Vikramank, Vaidya Kapil Eknath, Kumar Vinayshekhar Bannihatti, Khosla\nSopan, Narayanaswamy Murali, Gangadharaiah Rashmi, and Kraska Tim. Panda:\nPerformance debugging for databases using llm agents. Amazon Science, 2024.\n[33] Bouzenia Islem, Devanbu Premkumar, and Pradel Michael. Repairagent: An\nautonomous, llm-based agent for program repair. arXiv preprint arXiv:2403.17134 ,\n2024.\n[34] Sobania Dominik, Briesch Martin, Hanna Carol, and Petke Justyna. An analysis of\nthe automatic bug fixing performance of chatgpt. In 2023 IEEE/ACM International\nWorkshop on Automated Program Repair (APR) , pages 23‚Äì30. IEEE, 2023.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8029977679252625
    },
    {
      "name": "Debugging",
      "score": 0.8000152111053467
    },
    {
      "name": "Programming language",
      "score": 0.6822831630706787
    },
    {
      "name": "Open source",
      "score": 0.6726197004318237
    },
    {
      "name": "Software bug",
      "score": 0.43452268838882446
    },
    {
      "name": "Software engineering",
      "score": 0.4174426198005676
    },
    {
      "name": "Software",
      "score": 0.21999353170394897
    }
  ]
}