{
    "title": "The Geometry of Multilingual Language Model Representations",
    "url": "https://openalex.org/W4385573436",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2320927890",
            "name": "Tyler Chang",
            "affiliations": [
                "San Diego Biomedical Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2134014566",
            "name": "Zhuowen Tu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115683131",
            "name": "Benjamin Bergen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3102425047",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2005577322",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W3201624868",
        "https://openalex.org/W3186903869",
        "https://openalex.org/W3183430109",
        "https://openalex.org/W3038047279",
        "https://openalex.org/W4297730150",
        "https://openalex.org/W2970316683",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W3205727812",
        "https://openalex.org/W3035579820",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3125516434",
        "https://openalex.org/W2995118574",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3035137491",
        "https://openalex.org/W4226155321",
        "https://openalex.org/W3018647120",
        "https://openalex.org/W3102226577",
        "https://openalex.org/W2970820321",
        "https://openalex.org/W3087873698",
        "https://openalex.org/W3103490574",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W3115830533",
        "https://openalex.org/W3035547806",
        "https://openalex.org/W3102483398"
    ],
    "abstract": "We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 119–136\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nThe Geometry of Multilingual Language Model Representations\nTyler A. Chang1,2, Zhuowen Tu 1, Benjamin K. Bergen 1\n1Department of Cognitive Science\n2Halıcıo˘glu Data Science Institute\nUniversity of California San Diego\n{tachang, ztu, bkbergen}@ucsd.edu\nAbstract\nWe assess how multilingual language models\nmaintain a shared multilingual representation\nspace while still encoding language-sensitive\ninformation in each language. Using XLM-R\nas a case study, we show that languages occupy\nsimilar linear subspaces after mean-centering,\nevaluated based on causal effects on language\nmodeling performance and direct comparisons\nbetween subspaces for 88 languages. The sub-\nspace means differ along language-sensitive\naxes that are relatively stable throughout mid-\ndle layers, and these axes encode information\nsuch as token vocabularies. Shifting represen-\ntations by language means is sufficient to in-\nduce token predictions in different languages.\nHowever, we also identify stable language-\nneutral axes that encode information such as\ntoken positions and part-of-speech. We visu-\nalize representations projected onto language-\nsensitive and language-neutral axes, identify-\ning language family and part-of-speech clus-\nters, along with spirals, toruses, and curves\nrepresenting token position information. These\nresults demonstrate that multilingual language\nmodels encode information along orthogonal\nlanguage-sensitive and language-neutral axes,\nallowing the models to extract a variety of fea-\ntures for downstream tasks and cross-lingual\ntransfer learning.\n1 Introduction\nDespite state-of-the-art performance on a wide va-\nriety of multilingual NLP tasks (Conneau et al.,\n2020a; Hu et al., 2020; Liang et al., 2020; Lin et al.,\n2021; Xue et al., 2021), the internal structure of\nmultilingual language model representation spaces\nis not well understood. The success of these mod-\nels is often attributed to a shared multilingual space\nacross languages, with claims that the models “map\nrepresentations coming from different languages in\na single shared embedding space” (Conneau et al.,\n2020b) and that “different languages are close to a\nshared space” (Pires et al., 2019). While intuitive,\nFigure 1: XLM-R representations in layer six projected\nonto a linear subspace where two axes are language-\nneutral (horizontal axes), and one axis is language-\nsensitive (vertical axis). Projecting from the side visual-\nizes the language-sensitive axis (top right). Projecting\nfrom the top down visualizes the language-neutral axes\nencoding token position information, forming a nearly\nperfect torus in the representation space (bottom right).\nmany of these assumptions have not been examined\nin detail. For example, the models have been shown\nto retain language-sensitive information such as\nlinguistic typological features (language-sensitive\nrepresentations; Choenni and Shutova, 2020; Liang\net al., 2021; Rama et al., 2020) despite also ex-\nhibiting cross-lingual alignment of representations\n(language-neutral representations; Conneau et al.,\n2020b; Libovický et al., 2020; Pires et al., 2019).\nIt remains unclear how the underlying geometry of\nthe representation space facilitates this integration\nof language-sensitive and language-neutral infor-\nmation, and the structure of the space with respect\nto individual features and axes remains an open\nquestion.\nIn this work, we present an analysis of the geom-\netry of multilingual language model representations\nusing downstream language modeling predictions,\ndirect comparisons between language subspaces,\nand visualizations of representations projected onto\nlow-dimensional subspaces. Using the multilingual\nlanguage model XLM-R as a case study (Conneau\n119\net al., 2020a), we find that language subspaces are\nsimilar to one another after mean-centering, and dif-\nferences between subspace means encode language-\nsensitive information such as language vocabular-\nies. We identify language-sensitive axes that cluster\nrepresentations by language family, and we identify\nlanguage-neutral axes that encode token positions\nand part-of-speech. These axes remain relatively\nstable across layers, suggesting that multilingual\nlanguage models maintain stable substructures for\nindividual features during processing. Our results\nhighlight the importance of representational geom-\netry in understanding multilingual language model\nrepresentations, laying the groundwork for future\nresearch in multilingual subspace geometry and\ninterpretable multilingual learning.1\n2 Related work\nPrevious work has considered how multilingual\nlanguage models encode different types of informa-\ntion. For example, mean representation distances\nbetween languages correlate with phylogenetic dis-\ntances between languages (Rama et al., 2020), and\nindividual representations can be used to predict lin-\nguistic typological features (Choenni and Shutova,\n2020), particularly after projecting onto language-\nsensitive subspaces (Liang et al., 2021). The mod-\nels also maintain language-neutral subspaces that\nencode information that is shared across languages.\nSyntactic information is encoded largely within a\nshared syntactic subspace (Chi et al., 2020), to-\nken frequencies may be encoded similarly across\nlanguages (Rajaee and Pilehvar, 2022), and rep-\nresentations shifted according to language means\nfacilitate cross-lingual parallel sentence retrieval\n(Libovický et al., 2020; Pires et al., 2019). How-\never, these studies have primarily focused on spe-\ncific features or probing tasks, with less focus on\nhow the broader geometry of the original represen-\ntation space facilitates feature encodings and inter-\nactions. To better understand this broader context,\nwe consider overall language subspaces in multilin-\ngual language models, identifying axes that encode\nlanguage-sensitive and language-neutral features.\n3 Language subspaces\nAs an initial assessment of the geometry of multilin-\ngual representation spaces, we identified affine (i.e.\nmean-shifted linear) subspaces for 88 languages in\n1Code is available at https://github.com/\ntylerachang/multilingual-geometry.\nthe multilingual language model XLM-R (Conneau\net al., 2020a) using singular value decomposition\n(SVD) over contextualized token representations in\neach language. Previous work has considered SVD\nprimarily in the context of singular value canonical\ncorrelation analysis (SVCCA; Raghu et al., 2017)\nto quantify informational similarity between sets\nof representations (e.g. Kudugunta et al., 2019;\nSaphra and Lopez, 2019), without considering the\ngeometry of the un-transformed subspaces. Here,\nwe found that SVD identifies affine subspaces that\nsufficiently account for language modeling perfor-\nmance in individual languages, and the subspaces\nare similar to one another after subtracting lan-\nguage means, particularly in middle layers.\n3.1 Model and dataset\nIn all experiments, we used the pre-trained lan-\nguage model XLM-R, which has achieved state-\nof-the-art performance on a variety of multilingual\nNLP tasks (Conneau et al., 2020a). XLM-R fol-\nlows the Transformer architecture of BERT and\nRoBERTa (Devlin et al., 2019; Liu et al., 2019),\nbut the model is trained to predict masked tokens\nin 100 languages. To extract contextualized to-\nken representations from XLM-R, we inputted text\nsequences from the OSCAR corpus of cleaned\nweb text data (Abadji et al., 2021), concatenat-\ning consecutive sentences such that each sequence\ncontained 512 tokens. We used the vector repre-\nsentations outputted by each Transformer layer as\nour token representations in layers one through\ntwelve, and we used the uncontextualized token\nembeddings for layer zero. We considered the\n88 languages that appear in both the XLM-R pre-\ntraining corpus and the OSCAR corpus, excluding\nlanguages with fewer than 100 sequences in OS-\nCAR.2\n3.2 Affine language subspaces\nWe defined an affine subspace for each language A\nusing the language’s mean representationµA ∈Rd\nalong with k directions of maximal variance in\nthe language, defined by an orthonormal basis\nVA ∈Rd×k. To identify this subspace, we applied\nsingular value decomposition (SVD) centered at\nµA using 262K contextualized token representa-\ntions from language A(512 sequences in the OS-\nCAR corpus). We selected the subspace dimension-\nality ksuch that the subspace accounted for 90%\n2Experimental details are outlined in Appendix A.\n120\n0 2 4 6 8 10 12\nLayer\n21\n23\n25\n27\n29\n211\nPerplexity ratio\nNo projection\nProj_A\nProj_B\nProj_B_ A\nFigure 2: Language modeling perplexity scores in lan-\nguage Awhen projecting representations onto different\naffine language subspaces in each layer. We report the\nmean perplexity ratio (the projected perplexity score\ndivided by the original perplexity score) over all 88 eval-\nuation languages A. For projections onto language B\nsubspaces, we report the mean perplexity ratio for all\npairs where the evaluation languageAdid not match the\nsubspace language B. Higher values indicate worse lan-\nguage modeling performance. Due to the interpretation\nof perplexities as inverse probabilities, we computed\ngeometric rather than arithmetic means. Shaded regions\nindicate one (geometric) standard deviation from the\nmean.\nof the total variance in the language. 3 Across all\nlayers, the median subspace dimensionality was\n335, less than half of the original 768 dimensions.\nAffine subspaces accounted for language mod-\neling performance. To assess the extent to which\naffine subspaces encoded relevant information in\ntheir corresponding languages, we evaluated lan-\nguage modeling perplexity scores for each lan-\nguage Awhen projecting representations onto the\ncorresponding language subspace:\nProjA(x) =VAVT\nA (x−µA) +µA\nWe computed the ratio of the projected perplexity\nto the original perplexity in language A. As shown\nin Figure 2 (ProjA), there were generally only mi-\nnor increases in perplexity despite projecting onto\nsubspaces with less than half the dimensionality\nof the original space. This suggests that affine lan-\nguage subspaces encode much of the information\nrelevant to the language modeling task in their cor-\nresponding languages.\nLanguage subspaces differed from one an-\nother. To evaluate whether the model was sim-\nply using the same affine subspace for all lan-\nguages, we evaluated perplexities in each language\n3Results were qualitatively similar for subspaces account-\ning for variance proportions in [75%, 90%, 95%, 99%].\nAwhen projecting onto subspaces for different lan-\nguages B. As shown in Figure 2 (ProjB), perplex-\nities increased substantially when projecting onto\nthese other language subspaces, suggesting that the\nmodel maps text in different languages into distinct\nsubspaces. However, we note that these projections\nprojected onto subspaces passing through each µB,\nwhich may have been quite far from the original\nmean of the evaluation language µA. Thus, the\nhigh perplexities when projecting onto other lan-\nguage subspaces may simply have been a result\nof projecting onto subspaces passing through µB\nrather than µA.\nMean-shifted subspaces were similar to one\nanother. We again evaluated perplexities in each\nlanguage A projected onto languages B, but we\nshifted the language B subspaces such that they\npassed through µA.4 As shown in Figure 2\n(ProjB,µA), particularly for middle layers, perplexi-\nties under these mean-shifted projections were only\nmoderately higher than when projecting onto the\nlanguage Asubspace. In other words, projecting\nonto the affine subspace for language B shifted\nonto µA was similar to projecting onto the affine\nsubspace for language Aitself (by default passing\nthrough µA). This suggests that in middle lay-\ners, the affine subspaces for different languages are\nsimilar to one another when shifted according to\nlanguage means. In further support of this finding,\nwe note that the perplexity gap between ProjB and\nProjB,µA tended to widen in deeper layers. This\nimplies that the language modeling performance\ndegradation caused by language projection ProjB\ncan be accounted for largely by language mean in\ndeeper layers. In these deeper layers, it appears that\nthe differences between language subspaces com-\npress into differences between subspace means.\n3.3 Subspace distances\nAs a complementary metric, we directly quantified\ndistances between the subspaces themselves. To do\nthis, we first note that our SVD approach to comput-\ning affine language subspaces identifies principal\naxes with corresponding variances for each lan-\nguage. By interpreting these axes and variances\nas covariance matrices (instead of using them to\ndefine affine subspaces), we can adopt theoretically-\n4Note that ProjB,µA(x) =VBVT\nB (x−µA)+µA is equal\nto ProjB(x) shifted by the constant vector(I−VBVT\nB )(µA−\nµB). In other words, the projected representation is shifted\nby the component of µA − µB that is perpendicular to the\nlanguage Bsubspace.\n121\nmotivated distance metrics from mathematics that\nquantify distances between positive definite matri-\nces (Bonnabel and Sepulchre, 2009).5 Specifically,\nwe define the distance between two positive definite\nmatrices KA,KB ∈Rd×d as:\nDistance(KA,KB) =\n√∑\ni\nlog2(λi) (1)\nwhere λi are the d positive real eigenvalues of\nK−1\nA KB (Bonnabel and Sepulchre, 2009). This\ndistance metric is both symmetric and invariant to\nlinear transformations (e.g. rotations, reflections,\nand scaling). However, because the metric ignores\nsubspace means, it can only be considered as a\ndistance metric between mean-centered subspaces.\nWe computed pairwise distances between the\n88 language subspaces identified in the previous\nsection. To gain an intuitive understanding of the\nsubspace distances, we compared the true distances\nto the distances between subspaces rotated by θde-\ngrees or scaled by a multiplier γ along each axis.\nFor example, the distance between the Spanish\nand Chinese subspaces in layer eight was approxi-\nmately equal to the mean distance from a language\nsubspace to itself before and after rotation by two\ndegrees or scaling by 1.53x.6 This way, we were\nable to consider the distance between any two sub-\nspaces in terms of the analogous rotation or scaling.\nAgain, mean-shifted subspaces were similar\nto one another. The mean rotation degrees and\nscaling multipliers for language subspace distances\nin each layer are shown in Figure 3. In line with\nthe perplexity comparison results in Section 3.2,\nlanguage subspaces in middle layers were surpris-\ningly similar to one another after mean-centering.\nIn layers six through eleven, the mean subspace\ndistances were equivalent to subspace rotations by\nless than five degrees and subspace scalings by less\nthan 1.6x. This result aligns with previous work\nsuggesting that representations from middle layers\nin multilingual language models are often the most\ncross-linguistically aligned, particularly after ad-\njusting for language means (Libovický et al., 2020;\nPires et al., 2019). In these middle layers, the model\n5A positive definite matrix is a symmetric matrix with\npositive eigenvalues. Our selected distance metric is described\nin more detail in Appendix B.1 and in Bonnabel and Sepulchre\n(2009).\n6A rotation was defined as a θ-degree rotation in each of\nthe (randomly selected) d//2 independent planes of rotation.\nA scaling was defined as multiplication or division by γalong\neach of the daxes of variance. Details for the rotation and\nscaling comparisons can be found in Appendix B.2.\n0 2 4 6 8 10 12\nLayer\n0\n5\n10\n15\n20\n25Rotation degrees\nRotation\n1.50\n1.75\n2.00\n2.25\nScaling multiplierScaling\nFigure 3: Analogous rotations and scalings be-\ntween mean-centered language subspaces in each layer.\nShaded regions indicate one standard deviation from the\nmean. In middle and late-middle layers, mean-centered\nsubspaces were more similar to one another.\nrepresentations are further from both the original\ninput and the final language modeling prediction,\nboth of which are highly language-sensitive.\n4 Language-sensitive axes\nThus, it appears that languages occupy similar sub-\nspaces in multilingual language models after mean-\ncentering, based on converging evidence from\ndownstream language modeling performance and\ndirect comparisons between language subspaces.\nHowever, differences in subspace means demon-\nstrate that the language subspaces still differ along\nparticular axes. Intuitively, these axes should en-\ncode language-sensitive information, information\nthat has high mutual information with the input\nlanguage identity. For example, the word order\nor specific tokens present in a raw linguistic input\nare highly informative of the input language. In\nthis section, we consider whether axes connecting\nsubspace means encode language-sensitive features\nsuch as token vocabularies. Indeed, we found that\nshifting representations by language means was\nsufficient to induce language modeling predictions\nin arbitrary target languages. We then used lin-\near discriminant analysis (LDA) to explicitly iden-\ntify language-sensitive axes, finding that these axes\nwere surprisingly stable across middle layers.\n4.1 Inducing target language vocabulary\nWe assessed whether shifting and projecting repre-\nsentations according to language means and sub-\nspaces was sufficient to induce language modeling\npredictions in different languages. We defined a\nlanguage’s vocabulary as the set of tokens with fre-\nquency at least 1e-6 (one occurrence per million\n122\nLang A Lang B Common tokens\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T oken proportion\nNo projection\nProj_B\nA  B\nA  B, Proj_B\nFigure 4: The proportion of tokens in the original eval-\nuation language A, the target language B, and in a set\nof common tokens across languages (e.g. numerals and\npunctuation), after different types of mean-shifting and\nlanguage subspace projections in the language model.\nLayers were projected individually, and results were av-\neraged across layers. Results varied substantially based\non the evaluation and target languages (standard devia-\ntions in all conditions between 0.11 and 0.25), likely due\nto differences in vocabulary overlap between language\npairs, raw vocabulary sizes, and model biases towards\ncertain languages (Wu and Dredze, 2020). The dashed\nline indicates the average proportion of tokens (5.4%)\nthat would be in any given language if predicting based\non random chance.\ntokens) in up to one billion tokens from the OS-\nCAR corpus for that language. In XLM-R, these\nvocabularies ranged from 3K to 24K tokens from\nthe original 250K token vocabulary. From each vo-\ncabulary, we excluded a set of common tokens (945\ntokens) that appeared in at least 90% of the 88 lan-\nguages; these common tokens consisted primarily\nof punctuation, numbers, and one- to two-character\nsequences of Latin characters.\nWe collected language modeling predictions\nfor 512 sequences in each evaluation language A,\ncomputing the proportion of predicted tokens in\nthe language A vocabulary. For all languages,\nnearly all predicted tokens were either common\ntokens or tokens in the evaluation language (M =\n99.5%,SD = 0.2%).7 The mean proportions of\ncommon tokens and tokens in the evaluation lan-\nguage Aare shown in Figure 4.\nShifting by language means induced target\nlanguage vocabulary. We considered the same to-\nken proportions when representations in language\nA were shifted towards each possible target lan-\nguage Bby adding the shift vector µB −µA. As\n7Because nearly all predicted tokens in the unmodified\nlanguage model were either in the evaluation language Aor\ncommon tokens, the proportion of tokens in other languages\nBin this condition were essentially due to vocabulary overlap\nbetween languages Aand B.\nshown in Figure 4, shifting towards µB substan-\ntially increased the proportion of predicted tokens\nin language B, decreasing the proportion of pre-\ndicted tokens in language A. This suggests that the\nmodel encodes token vocabularies to some degree\nalong axes where language means differ.\nProjecting onto subspaces induced additional\ntarget language vocabulary. Next, we considered\nwhether projecting onto the language Bsubspace\ninduced additional predictions in language B. Pro-\njecting onto this subspace is equivalent to removing\ninformation along axes orthogonal to the language\nB subspace, by simply setting the representation\nequal to µB along these axes. As shown in Figure\n4, this projection alone induced a proportion of tar-\nget language tokens similar to mean-shifting. This\nsuggests that along some axes, the language mean\nalone captures relevant information about language\nBvocabularies.\nThen, we considered the combination of mean-\nshifting and subspace projection; geometrically,\nthis set axes orthogonal to the language B sub-\nspace equal to µB, and it shifted representations\naccording to µB −µA along axes within the lan-\nguage B subspace. Indeed, this transformation\nfurther increased the proportion of predicted to-\nkens in language B, beyond mean-shifting and\nprojection individually (Figure 4). Compared to\nthe unmodified language model, this transforma-\ntion induced 4.7x more predicted tokens in the tar-\nget language (10% →47%), and 3.5x fewer pre-\ndicted tokens in the original evaluation language\n(75% →21%). These results suggest that along\nsome axes, language-sensitive information can be\ncaptured simply by language means (setting values\nequal to µB); along other axes, representation dis-\ntributions from other languages Acan be shifted\ntowards language B(shifting values by µB −µA).\nA more detailed interpretation of types of language-\nsensitive axes is included in Appendix C.\n4.2 Linear discriminant analysis (LDA)\nIn the previous section, we showed that language\nsubspaces differ along language-sensitive axes (e.g.\naxes connecting language means), and these axes\nencode information such as token vocabularies.\nNext, as in Liang et al. (2021), we applied lin-\near discriminant analysis (LDA) to identify specific\naxes that separate language subspaces. Givennsets\nof representations (in this case, one set of 4K ran-\ndomly sampled representations for each language),\n123\nFigure 5: Representations in layers five, seven, and nine projected linearly onto the first two LDA axes that separate\nlanguages in layer eight. Representations remained relatively unchanged along these axes as they passed through\nmiddle layers of the model. Detailed plots are included in Appendix D.1.\nLDA computes n−1 axes that maximize separa-\ntion between the sets. Building upon Liang et al.\n(2021), we directly visualized representations pro-\njected onto the identified language-sensitive axes.\nLanguages clustered by family. When repre-\nsentations were projected onto the first axes iden-\ntified by LDA, they clustered loosely by language\nfamily (Figure 5). This aligns with the findings\nof Liang et al. (2021), who found that LDA axes\nencode linguistic typological features and language\nfamilies. In Appendix D.1, we include LDA plots\nfor each layer, including all 88 individual language\nmeans. In earlier layers and the final hidden layer,\nrepresentations appeared to cluster more by script;\nindeed, these layers are closer to either the original\ntoken inputs or the output token predictions.\nLanguage-sensitive axes were stable in mid-\ndle layers. Notably, the axes identified by LDA\nresulted in surprisingly similar projections for mid-\ndle layers (e.g. layers five through nine). In fact,\nfor these layers, we were able to project represen-\ntations onto the same language-sensitive axes re-\ngardless of layer. For example, all plots in Figure 5\nproject onto the language-sensitive axes computed\nspecifically for layer eight, but they result in nearly\nidentical projections for representations in layers\nfive through nine. Projections onto these axes for\nthe remaining layers are shown in Appendix D.1,\nremaining similar for layers three through eleven.\nWe observed qualitatively similar results for the\nfirst ten language-sensitive axes identified by LDA.\nThese results suggest that representations remain\nlargely unchanged along language-sensitive axes as\nthey pass through middle and late-middle layers. In\nthese layers, the language model may be processing\nmore semantic information (Jawahar et al., 2019;\nTenney et al., 2019), transforming representations\nalong more language-neutral axes.\nFigure 6: Representations in layer eight projected lin-\nearly onto the first three LDA axes that separate different\ntoken positions. As shown in the right image, token po-\nsition information was primarily encoded independently\nof input language.\n5 Language-neutral axes\nIt is then apparent that multilingual language mod-\nels encode language-sensitive information along\nlanguage-sensitive axes. However, we still do not\nhave a clear picture of how information is encoded\nalong presumed language-neutral axes. Motivated\nby the use of LDA to identify language-sensitive\naxes, we used LDA to identify axes that encode po-\ntentially more language-neutral information: token\npositions and part-of-speech (POS). We assessed\nwhether the identified axes encoded the correspond-\ning features in language-neutral ways.\n5.1 Token positions\nFirst, we identified axes that encode tokens’ posi-\ntions in input sequences. Notably, token positions\nare encoded language-neutrally as absolute posi-\ntion embeddings before the first Transformer layer\nin XLM-R. Unless the model transforms position\ninformation in a language-sensitive way, the in-\nformation will remain encoded language-neutrally.\nStill, identifying token position axes serves to ver-\nify our assumptions about the language neutrality\nof position information in the model and to better\nunderstand how this information is represented.\n124\nFigure 7: Representations in layer four projected lin-\nearly onto LDA axes that separate different token po-\nsitions. In some subspaces, curves encoded absolute\npositions (left), while in others, toruses encoded relative\npositions (right).\nPosition axes were language-neutral. We per-\nformed LDA on sets of representations correspond-\ning to every sixteen token positions, identifying\naxes that separated the different positions. We\nused 8K representations sampled uniformly from\nall languages for each position index. We pro-\njected representations from all token positions onto\nthe identified position axes to qualitatively deter-\nmine whether the axes encoded position informa-\ntion language-neutrally. Indeed, as shown in Figure\n6, along the position axes, we could qualitatively\nidentify a token’s position in the input sequence\nwithout knowing its source language, demonstrat-\ning that token position information remains largely\nlanguage-neutral as it passes through the model.\nPosition information was encoded along non-\nlinear structures. As shown in Figure 7, token\nposition information was encoded along toruses,\nspirals, and curves in the position subspaces. These\nstructures were similar to the “Swiss-Roll manifold”\nidentified along directions of maximal variance in\nunidirectional monolingual language models in Cai\net al. (2021).8 We hypothesize that this spiral struc-\nture may be due to the need for both relative and ab-\nsolute position information in the model. In some\ncases, it is useful to know a token’s relative position\nto nearby tokens; this information can be encoded\nthrough toruses, whose circular structure can en-\ncode token positions modulo some window size\n(e.g. Figure 7, right). Then, angles on the torus rep-\nresent relative position distances. In other cases, it\nmay be useful to know a token’s absolute position\nin the overall sequence; this can be encoded along\na single curve or linear dimension (e.g. Figure 7,\n8Indeed, we were able to identify spirals encoding token\nposition information in middle layers simply by projecting\nonto directions of maximal variance (within each language\nor across all languages) in the multilingual model. This sug-\ngests that token position information is often encoded along\ndirections of high variance in language model representations.\nFigure 8: Representations in layer two projected lin-\nearly onto LDA axes that separate nouns, verbs, and\nadjectives. As shown in the right image, representations\nclustered independently of input language along these\naxes. Additional plots are included in Appendix D.3.\nleft). Combining this relative and absolute position\ninformation can be attained through multidimen-\nsional spirals (e.g. the 3D spiral in Figure 6), which\nproject onto lines and curves along some axes and\ntoruses along others. Future work may investigate\nhow these nonlinear structures arise from and inter-\nact with the dot product self-attention mechanism\nin Transformer models.9\nPosition representations were stable across\nlayers. Similar to the projections onto language-\nsensitive axes in Section 4.2, we were able to\nproject representations onto token position axes\nidentified for other layers while still producing\nnearly identical curves to those in Figure 6 (see\nAppendix D.2 for plots). In fact, we found that\nthese axes were stable as early as the second hid-\nden layer, remaining largely unchanged until the\nlast hidden layer. This suggests that each layer\napplies only minimal transformations to the rep-\nresentations along language-neutral position axes,\nleaving internal position representations largely un-\nchanged as they pass through the model.\n5.2 Part-of-speech\nAs a stronger test of whether multilingual language\nmodels align language-neutral information along\nlanguage-neutral axes, we considered axes that\nencode tokens’ part-of-speech (POS). Unlike to-\nken positions, POS is not inputted directly into\nthe model; in order to encode POS in a language-\nneutral way, the model must align features (e.g.\nfeatures of nouns vs. verbs) cross-linguistically\nwithout supervision.\nWe performed LDA on sets of representations\ncorresponding to POS tags in the Universal Depen-\n9Dot products are proportional to angle cosines on toruses.\nIf angles encode relative token positions (e.g. Figure 7, right),\nthe self-attention mechanism has easy access to relative posi-\ntion information.\n125\nFigure 9: Representations from layer four projected onto\na linear subspace where two axes encode token positions\n(horizontal axes), and one axis encodes part-of-speech\n(vertical axis). Projecting from the side visualizes the\npart-of-speech axis (top right). Projecting from the top\ndown visualizes the token position axes (bottom right).\ndencies (UD) dataset (Nivre et al., 2020). Specif-\nically, we mapped language model tokens to the\nPOS tag(s) that they were annotated with anywhere\nin the UD corpus.10 Using this mapping from to-\nkens to POS tags, we extracted token representa-\ntions in each language for each POS tag. To iden-\ntify axes separating specific POS tags using LDA,\nwe used a set of 8K token representations for each\nPOS tag, sampled uniformly from all languages\nwith tokens appearing in the UD corpus. When\nprojecting onto ndimensions, we used LDA over\nn+ 1POS tags, resulting in naxes that separated\nrepresentations for the provided POS tags.11\nPOS axes were language-neutral and stable\nacross layers. As with token positions, POS in-\nformation was encoded largely language-neutrally\nalong POS axes. As shown in Figure 8, when pro-\njected onto POS subspaces, representations clus-\ntered roughly by POS independently of their in-\nput language. This result aligns with previous\nwork showing that syntactic information aligns\nin shared linear subspaces across languages (Chi\net al., 2020). Unlike Chi et al. (2020), we iden-\ntified low-dimensional subspaces that allow us to\nvisualize representations projected directly onto the\nsubspaces without additional distortion (e.g. unlike\nt-SNE visualizations). Furthermore, we found that\nthese POS axes were relatively stable across layers\n10The UD corpus contained 61 of the 88 languages in both\nOSCAR and XLM-R.\n11We omit visualizations applying LDA over all 17 POS\ntags in the UD dataset because this method resulted in 16 dif-\nferent axes, and the first naxes for visualization generally did\nnot separate all POS tags. In particular, the first three dimen-\nsions tended to separate punctuation, numbers, and symbols\n(e.g. percent signs and emojis), grouping all other POS tags\ntogether.\nFigure 10: Representations from layer six projected onto\na linear subspace where two axes are language-sensitive\n(horizontal axes), and one axis encodes token positions\n(vertical axis). Projecting from the side visualizes the\nlanguage-neutral token position axis (top right). Project-\ning from the top down visualizes the language-sensitive\naxes separating languages (bottom right).\none through ten, resulting in similar projections\nwhen projecting onto POS axes identified for other\nlayers (see Appendix D.3 for additional plots). This\nresult aligns with the hypothesis that middle layers\nprocess higher level information (Jawahar et al.,\n2019; Tenney et al., 2019), but it also suggests that\nlow-level information is still retained along stable\naxes in these layers.\n6 Multilingual structure\nFinally, we synthesize results from previous sec-\ntions to develop a clearer picture of how multi-\nlingual language model representation spaces are\nstructured. In Section 3, we showed that individual\nlanguages occupy affine subspaces that are roughly\nsimilar to one another after mean-shifting. These\nlanguage subspaces encode information such as\ntoken positions and part-of-speech along shared\nlanguage-neutral axes (Section 5). The subspaces\ndiffer primarily along language-sensitive axes (e.g.\naxes connecting language means) that encode infor-\nmation such as token vocabularies (Section 4). We\nnote that to the extent to which language subspaces\ndiffer along at least one language-sensitive axis,\nthey are esentially non-overlapping in the higher di-\nmensional space. To be precise, representations in\ndifferent languages do not directly occupy a shared\nmultilingual subspace; rather, representations must\nbe projected onto specific axes and subspaces to ex-\ntract language-sensitive and language-neutral repre-\nsentations. We describe different types of language-\nsensitive and language-neutral axes in Appendix\n126\nC, but we leave a rigorous quantification of the\n“language-sensitivity” of individual features and\naxes to future work.\nStill, as shown in Figures 1, 9, and 10, we found\nthat axes encoding different features were often or-\nthogonal to and independent from one another, en-\nabling projections that were minimally influenced\nby extraneous noise and features along other di-\nmensions. For example, in Figure 9, the distinction\nbetween nouns and verbs was encoded along an\naxis orthogonal to a token’s position in the input\nsequence, allowing either feature to be extracted\nby projecting onto its corresponding axes (see ad-\nditional examples in Appendix D.4). However, this\nobservation may be because low-dimensional sub-\nspaces are likely to be orthogonal to one another\nin high-dimensional spaces, and we selected fea-\ntures that were likely to be minimally correlated\nwith one another. Future work might assess how\nmultilingual language models encode more com-\nplex linguistic features geometrically, or the extent\nto which the models’ representation spaces can\nbe fully decomposed into orthogonal subspaces\nencoding different features. This work has impli-\ncations for more targeted subspace alignment be-\ntween languages for better cross-lingual transfer\nlearning; for example, our methods might be used\nto identify specific axes that should or should not\nbe aligned for specific tasks using existing repre-\nsentation alignment approaches (e.g. Cao et al.,\n2020; Kulshreshtha et al., 2020; Zhao et al., 2021).\nFinally, in Sections 4.2, 5.1, and 5.2, we pre-\nsented initial results suggesting that language fam-\nilies, token positions, and part-of-speech are en-\ncoded stably across middle and late-middle layers\nin multilingual language models. In other words,\nthe transformations in these layers may be primar-\nily language-neutral, and they may retain struc-\ntures encoding low-level features for later token\nreconstruction (V oita et al., 2019). In these layers,\nrepresentations might change primarily along axes\ncorresponding to higher level features (e.g. seman-\ntics and general reasoning; Jawahar et al., 2019;\nTenney et al., 2019), leaving other axes unchanged.\nA more detailed analysis of how specific represen-\ntational structures are retained across layers is a\npromising direction for future research.\n7 Conclusion\nIn this work, we identified language subspaces and\nindividual language-sensitive and language-neutral\naxes in the multilingual language model XLM-R.\nWe assessed these subspaces and axes using a va-\nriety of methodologies, including causal effects\non language modeling predictions, direct compar-\nisons between subspaces, and low-dimensional vi-\nsualizations. Our results suggest that multilingual\nlanguage models encode features by projecting rep-\nresentations onto orthogonal axes in the representa-\ntion space, enabling the efficient and simultaneous\nencoding of a wide variety of signals for down-\nstream tasks and multilingual learning.\nLimitations\nOf course, our work has several limitations. First,\nour results were limited by the languages avail-\nable in XLM-R and the OSCAR corpus (Abadji\net al., 2021; Conneau et al., 2020a). Our 88 consid-\nered languages were skewed substantially towards\nIndo-European languages; 52 of the 88 languages\nwere in some subfamily of the Indo-European\nlanguages (see individual languages in Appendix\nD.1; language families obtained from the Glottolog\ndatabase; Hammarström et al., 2021). The 88 lan-\nguages included only six Austronesian languages,\ntwo African languages, and zero Native American\nlanguages. Due to this limited linguistic diversity,\nour results may overestimate the similarity between\nsubspaces and representations across languages.\nSecond, even among the considered languages,\ncorpora varied substantially in both size and quality,\nboth in OSCAR and the XLM-R pre-training cor-\npus. Languages with smaller and less clean corpora\nare likely to have less-nuanced learned representa-\ntions and less-representative extracted subspaces\nin XLM-R. It is also likely that language-sensitive\naxes and subspaces encode topic distribution shifts\nacross languages, along with the language fami-\nlies and token vocabularies observed in Section\n4. From this perspective, investigating language-\nsensitive axes for topic information might allow\nresearchers to quantify how topics differ across\nlanguages.\nFinally, due to limited computation resources,\nour experiments were run only on one pre-trained\nlanguage model, XLM-R. Different architectures,\nhyperparameter settings, and parameter initializa-\ntions could produce different results. We hope that\nfuture work will continue to assess the geometry of\nmultilingual language model representations, cov-\nering a wider variety of models and languages.\n127\nAcknowledgements\nWe would like to thank Ndapa Nakashole, Leon\nBergen, and the UCSD Language and Cognition\nLab for helpful discussion and input. We would\nalso like to thank the anonymous reviewers for\nvaluable feedback. Tyler Chang is partially sup-\nported by the UCSD HDSI graduate fellowship,\nand Zhuowen Tu is funded by NSF IIS-2127544.\nReferences\nJulien Abadji, Pedro Javier Ortiz Suárez, Laurent Ro-\nmary, and Benoît Sagot. 2021. Ungoliant: An op-\ntimized pipeline for the generation of a very large-\nscale multilingual web corpus. In Proceedings of the\nWorkshop on Challenges in the Management of Large\nCorpora, pages 1–9.\nSilvére Bonnabel and Rodolphe Sepulchre. 2009. Rie-\nmannian metric and geometric mean for positive\nsemidefinite matrices of fixed rank. SIAM Journal\non Matrix Analysis and Applications, 31:1055–1070.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embedding\nspace: Clusters and manifolds. In International Con-\nference on Learning Representations.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Multi-\nlingual alignment of contextual word representations.\nIn International Conference on Learning Representa-\ntions.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding universal grammatical relations\nin multilingual BERT. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5564–5577, Online. Association\nfor Computational Linguistics.\nRochelle Choenni and Ekaterina Shutova. 2020. What\ndoes it mean to be language-agnostic? Probing multi-\nlingual sentence encoders for typological properties.\narXiv, abs/2009.12862.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6022–6034, Online. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nHarald Hammarström, Robert Forkel, Martin Haspel-\nmath, and Sebastian Bank. 2021. Glottolog 4.5.\nMax Planck Institute for Evolutionary Anthropology,\nLeipzig.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In International Conference on Machine\nLearning, pages 4411–4421.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nSneha Kudugunta, Ankur Bapna, Isaac Caswell, and\nOrhan Firat. 2019. Investigating multilingual NMT\nrepresentations at scale. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1565–1575, Hong Kong,\nChina. Association for Computational Linguistics.\nSaurabh Kulshreshtha, Jose Luis Redondo Garcia, and\nChing-Yun Chang. 2020. Cross-lingual alignment\nmethods for multilingual BERT: A comparative\nstudy. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 933–942,\nOnline. Association for Computational Linguistics.\nSheng Liang, Philipp Dufter, and Hinrich Schütze. 2021.\nLocating language-specific information in contextu-\nalized embeddings. arXiv, abs/2109.08040.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark datasetfor cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nJindˇrich Libovický, Rudolf Rosa, and Alexander Fraser.\n2020. On the language neutrality of pre-trained mul-\ntilingual representations. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\n128\npages 1663–1674, Online. Association for Computa-\ntional Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2021. Few-shot learning with mul-\ntilingual language models. arXiv, abs/2109.08040.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 4034–4043, Marseille,\nFrance. European Language Resources Association.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. SVCCA: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Conference on Neu-\nral Information Processing Systems.\nSara Rajaee and Mohammad Taher Pilehvar. 2022. An\nisotropy analysis in the multilingual BERT embed-\nding space. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 1309–1316,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nTaraka Rama, Lisa Beinborn, and Steffen Eger. 2020.\nProbing multilingual BERT for genetic and typo-\nlogical signals. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 1214–1228, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nNaomi Saphra and Adam Lopez. 2019. Understanding\nlearning dynamics of language models with SVCCA.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3257–3267,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nWei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle\nAugenstein. 2021. Inducing language-agnostic mul-\ntilingual representations. In Proceedings of *SEM\n2021: The Tenth Joint Conference on Lexical and\nComputational Semantics, pages 229–240, Online.\nAssociation for Computational Linguistics.\nA Experimental details\nAll of our experiments used the base size XLM-R\nlanguage model from Conneau et al. (2020a) with\n270M parameters, implemented in the Huggingface\nTransformers library (Wolf et al., 2020). When ex-\ntracting representations from XLM-R, we inputted\nunmodified text from the September 2021 release\nof the OSCAR corpus of cleaned web text data\n(Abadji et al., 2021), concatenating lines such that\neach input sequence contained 512 tokens. We ex-\ntracted representations from randomly sampled se-\nquences from the first (up to) 8M such sequences in\n129\neach language. Experiments were run on eight Ti-\ntan Xp GPUs. Across all reported experiments, to-\ntal computation time was approximately 17.5 days.\nThe vast majority of computation time was spent\ncomputing evaluation perplexities and in-language\ntoken proportions for each projection type, lan-\nguage pair, and layer in Sections 3.2 and 4.1.\nB Subspace distances\nB.1 Subspace distance metric\nFor each language A, we obtained a data matrix\nXA ∈Rn×d of ncontextualized token represen-\ntations in language A from the desired layer in\nXLM-R. We used n= 262K (512 sequences in the\nOSCAR corpus), which we found to produce rela-\ntively stable subspaces for each language, as quanti-\nfied by low distances between subspaces computed\nfrom different random subsets of representations\nfrom language A. Intuitively, the specific random\nsubset of tokens from language Ashould not im-\npact the computed subspace. Only two of the 88\nselected languages had fewer than 262K tokens\nin the OSCAR corpus: Javanese (n= 193K) and\nSundanese (n= 70K).\nTo represent each language subspace, we con-\nsidered the covariance matrix KA ∈Rd×d of the\noriginal representation dimensions. If we mean-\ncentered the data matrix XA according to µA ∈\nRd, then KA could be computed as XT\nAXA =\nVAΣ2\nAVT\nA divided by n−1, where VA ∈Rd×d\nwere the right singular vectors of XA, and ΣA ∈\nRd×d was the diagonal matrix of dsingular values\nof XA (computed using singular value decompo-\nsition). Geometrically, KA can be interpreted as\nan ellipsoid with axis directions defined by the (or-\nthonormal) columns of VA and axis scales defined\nby the corresponding variances in those directions\n(Σ2\nA scaled down by n−1). This ellipsoid is the\nimage of the unit sphere under the linear transfor-\nmation KA. In this way, KA captures the variance\nin different directions for representations from lan-\nguage A.\nTo compute the distance between two covariance\nmatrices KA and KB, we used the distance metric\nfrom Bonnabel and Sepulchre (2009) between pos-\nitive definite matrices, written out as Equation 1 in\nSection 3.3. Motivating this metric, Bonnabel and\nSepulchre (2009) describe a natural Riemannian\nmetric on the space of positive definite matrices. A\nRiemannian metric defines an inner product over\nthe tangent space at each point in the manifold (in\nthis case, at each positive definite matrix). These\ninner products can be used to define the lengths\nof paths (geodesics) in the manifold. Then, the\ndistance between any two positive definite matri-\nces can be defined by the length of the shortest\ngeodesic curve connecting them. The resulting\nmetric is both symmetric and invariant to linear\ntransformations. Further theoretical properties of\nthe metric are described in Bonnabel and Sepulchre\n(2009).\nB.2 Rotation and scaling comparisons\nIn Section 3.3, we computed analogous rotation de-\ngrees and scaling multipliers for language subspace\ndistances in each layer of XLM-R. To do this, for\neach layer of XLM-R, we first computed the mean\ndistance from each language subspace to itself be-\nfore and after rotation by θdegrees or scaling by\nmultiplier γ along each axis. We computed this\nmean over all languages and over 16 random rota-\ntions or scalings, considering θ∈[0,1,..., 90] and\nγ ∈[1.00,1.01,..., 4.00]. Random rotations were\ncomputed by sampling d//2 independent planes of\nrotation (by generating a random orthonormal basis\nand pairing together consecutive dimensions) and\nrotating the principal axes (columns of VA ∈Rd×d\nas computed in Appendix B.1) by ±θdegrees in\neach plane. Random scalings were computed by\nmultiplying or dividing the variance along each\nprincipal axis by γ2, because scaling representa-\ntions by γ along each axis would scale the corre-\nsponding variances by γ2. As expected, we found\nthat the mean distance between transformed sub-\nspaces increased monotonically with respect to\nboth rotation degree and scaling multiplier.\nThen, given a true distance value between two\nlanguage subspaces, we computed the equivalent\nrotation degree (and scaling multiplier) by iden-\ntifying the lowest degree (or scaling multiplier)\nthat would result in an equal or greater distance\nvalue. For example, given a true distance value\nof 30.0, we identified the lowest rotation degree\nthat resulted in a mean distance of at least 30.0\nwhen language subspaces were rotated individu-\nally. Essentially, we sought to identify the location\n(in rotation degrees or scaling multipliers) along\nthe monotonically increasing distance curve where\neach true distance value would fall. We computed\nthis rotation degree and scaling multiplier for each\nlanguage pair’s distance in each XLM-R layer, with\nresults described in Section 3.3.\n130\nC Language-sensitive vs.\nlanguage-neutral axes\nHere, we consider in more detail what it means\nfor an axis or subspace to be language-sensitive or\nlanguage-neutral. Broadly, as in Section 4, we call\na subspace language-sensitive if when a represen-\ntation is projected onto the subspace, it has high\nmutual information with its input language identity.\nFor example, if representations from two languages\nhave different distributions when projected onto\nsome subspace, then the projected representations\nimplicitly contain information about their input\nlanguages. Then, we call the subspace language-\nsensitive; conversely, if the mutual information is\nlow, we call the subspace language-neutral.\nNotably, when considering linear axes and sub-\nspaces, we can consider the means and variances of\nprojected representations from different languages.\nSpecifically, we can have the following language-\nneutral axes:\n(i) Equal means, low variance in each language:\ndue to low variance, these axes are not likely\nto contain a significant amount of relevant\ninformation within or across languages, al-\nthough variance is not a perfect indicator for\ndownstream importance. Rajaee and Pilehvar\n(2022) suggest that there may be relatively\nfew “degenerate” axes of this type in multilin-\ngual language models because languages tend\nto be “degenerate” along different axes, but\nthey only considered axes in the original basis\nof the representation space.\n(ii) Equal means, similarly high variance in each\nlanguage: if projected representations from\ndifferent languages have similar distributions\n(on top of similar means and variances) along\nthese axes, then the axes are indeed language-\nneutral. This can be observed qualitatively by\nviewing distributions of projected representa-\ntions in different languages (e.g. Figures 6\nand 8 in Section 5).\nAnd the following language-sensitive axes:\n(iii) High variance in A, low variance in B: these\naxes may represent features that vary in lan-\nguage Abut not in languageB. Previous work\nhas found evidence for these types of axes\nin multilingual language models (low cosine\nsimilarities along certain axes only in specific\nlanguages; Rajaee and Pilehvar, 2022), charac-\nterized as “degenerate” axes only in some lan-\nguages. In our work (Section 4.1), we found\nthat setting representations from language A\nequal to µB along axes with low variance in\nlanguage Binduced more token predictions in\nlanguage Bthan shifting by µB −µA alone.\nIn other words, there was variance in language\nA(centered around µA) that may have been\ninterpreted as noise (resulting in tokens out-\nside language B) if simply shifted to be cen-\ntered around µB; setting the representations\nequal to µB along these axes was sufficient to\nremove this noise. These axes may have been\nencoding specific tokens or token features in\nlanguage A, less relevant to language B(high\nvariance in A, low variance in B).\nIn these cases, µB may be close to or far from\nµA, depending on whether language B has\nsome fixed analogous feature value for the fea-\nture that varies in language A. Because there\nis little to no variance in language B along\nthese axes, µB essentially serves as a fixed\nbias term for language Brepresentations.\n(iv) Unequal means, similarly high variance in\neach language: visually, these axes might\nrepresent languages as similar distributions\nshifted from one another (e.g. Figure 5, de-\npicting language clusters). These shifts might\nreflect true differences between languages in\nthe distributions of particular features, imper-\nfect alignment between languages, or entirely\ndifferent features for different languages (see\nAppendix C.1 below).\n(v) Unequal means, low variance in each lan-\nguage: due to the low variance within each\nlanguage, these axes would represent differ-\nent languages roughly as points, potentially\nencoding any of the wide variety of features\nthat vary across languages (although with lit-\ntle variance within each language). How-\never, there is little evidence that these axes\nexist in multilingual language models. Even\naxes identified using LDA between languages\n(designed to maximize variance between lan-\nguages and minimize variance within lan-\nguages) appeared closer to type (iv) above.\nFuture work may consider more rigorous quantifi-\ncations of the language-sensitivity of individual\n131\naxes and subspaces in multilingual language mod-\nels, possibly quantifying the relative dimensionali-\nties of the language-neutral and language-sensitive\nsubspaces. Future work may also consider nonlin-\near subspaces in the models; as demonstrated by\nthe spirals, toruses, and curves in Section 5.1, some\nfeatures are represented nonlinearly in multilingual\nlanguage models.\nC.1 Extracting features\nHowever, the interpretations above primarily con-\nsider representation spaces as isolated units; they\ndo not consider how the models extract informa-\ntion from the spaces. For example, even if a sub-\nspace appears language-neutral, features might be\nextracted from the projected representations in\nlanguage-sensitive ways, and the subspace axes\nmight thus have different effects on downstream\nprocessing for different languages. For example,\nrepresentation distributions in languages Aand B\nmight look similar along some “language-neutral”\naxis, but the distributions might encode different\nfeatures for downstream processing in the two lan-\nguages. We attempt to avoid this confound in\nour work by explicitly hypothesizing interpretable\nlanguage-neutral features (e.g. token positions or\npart-of-speech in Section 5) that might be encoded\nalong particular language-neutral axes. We can\nthen assess whether individual axes encode specific\nfeatures across languages, although causal effects\non downstream model predictions need to be veri-\nfied independently (e.g. Section 4.1).\nMore broadly, given any language-sensitive or\nlanguage-neutral axis, we cannot distinguish solely\non the basis of representation distributions in each\nlanguage whether the axis encodes the same feature\nor the same type of information across languages.\nBy grounding our analyses in interpretable features\nand downstream consequences, we can begin to\nidentify how individual axes affect model process-\ning in language-sensitive or language-neutral ways.\nD Additional plots\nIn this section, we include additional visualizations\nprojecting representations onto language-sensitive\nand language-neutral axes. In all visualizations, we\nuse the global representation mean µglobal across\nlanguages as our origin along each axis. The choice\nof mean only shifts projected representations by a\nconstant vector, not affecting visualizations. We\northogonalize and normalize our axes prior to pro-\njection, ensuring that representations are projected\ndirectly onto the corresponding subspaces without\nstretching or distortion.\nD.1 Language-sensitive axes\nWe projected representations onto the first two\nLDA axes that separated languages in each layer\n(Figure 11), including all 88 language means. In\nFigure 12, we show representations from each layer\nprojected onto the LDA axes identified specifically\nfor layer eight, showing that these axes resulted in\nsimilar projections across all middle layers.\nD.2 Token positions\nTo demonstrate that token position information is\nencoded stably across layers, we projected rep-\nresentations from each layer onto the first three\nLDA axes that separated token positions in layer\neight, obtaining similar plots for layers two through\neleven (Figure 13).\nD.3 Part-of-speech\nAs shown in Figure 15, LDA was able to identify\naxes that separated parts-of-speech (POS). Further-\nmore, these axes were stable across the majority of\nlayers; we obtained similar plots when projecting\nrepresentations from layers one through ten onto\nPOS axes identified for layer eight (Figure 14).\nD.4 Multiple features\nFinally, we projected representations onto sub-\nspaces where different axes encoded different fea-\ntures (e.g. token positions, part-of-speech, or input\nlanguage). In Figures 16 and 17 (along with Fig-\nures 1, 9, and 10 in the main text), projecting onto\ndifferent axes visualizes different features.\n132\nFigure 11: Representations in each layer projected onto the first two LDA axes that separate languages in that layer.\nPoints indicate language means. Language families were obtained from the Glottolog database (Hammarström\net al., 2021), with varying levels of granularity depending on the number of XLM-R languages in each family (e.g.\nbreaking down the Indo-European languages). We grouped together the two Southeast Asian language families in\nXLM-R: Austroasiatic (Vietnamese and Khmer) and Tai-Kadai (Thai and Lao). We identified language isolates as\nlanguages that were the only XLM-R language in their family (Basque, Esperanto, Georgian, Japanese, Korean, and\nMongolian). Best viewed on a computer for magnification.\n133\nFigure 12: Representations from each layer projected onto the first two LDA axes that separate languages in layer\neight. The structure of representations along these axes was relatively stable across middle layers.\nFigure 13: Representations from each layer projected onto the first three LDA axes that separate token positions in\nlayer eight. Token position information was encoded in stable structures along stable axes throughout middle layers.\n134\nFigure 14: Representations from each layer projected onto the LDA axes that separate nouns, verbs, and adjectives\nin layer eight. The axes encoded this information stably throughout middle layers. The axes were computed only\nfrom LDA in layer eight, but they encoded the same information in other layers as well.\n135\nFigure 15: Representations from layer four projected\nonto the LDA axes that separate the parts-of-speech\nshown in each figure. Top left: nouns, pronouns, and\nproper nouns. Top right: adverbs, verbs, and auxiliary\nverbs. Bottom left: adpositions, particles, and coordi-\nnating conjunctions. Bottom right: symbols, numerals,\nand punctuation.\nFigure 16: Representations from layer two projected\nonto a linear subspace where two axes encode part-\nof-speech (horizontal axes), and one axis is language-\nsensitive (vertical axis). Projecting from the side visual-\nizes the language-sensitive axis (top right). Projecting\nfrom the top down visualizes the language-neutral part-\nof-speech axes (bottom right).\nFigure 17: Representations from layer eight projected\nonto a linear subspace where one axis encodes part-\nof-speech (first horizontal axis), one axis encodes to-\nken positions (second horizontal axis), and one axis is\nlanguage-sensitive (vertical axis). Projecting from the\nside visualizes the language-sensitive axis separating\nlanguages. Projecting from the top down visualizes the\nlanguage-neutral token position axis (middle right) and\npart-of-speech axis (bottom right).\n136"
}