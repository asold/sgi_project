{
  "title": "Finding the Dominant Winning Ticket in Pre-Trained Language Models",
  "url": "https://openalex.org/W4285242513",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4320549389",
      "name": "Zhuocheng Gong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2109309577",
      "name": "Di He",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2166559730",
      "name": "Yelong Shen",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3205478007",
      "name": "Tie-Yan Liu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Wei‐Zhu Chen",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2098869281",
      "name": "Dongyan Zhao",
      "affiliations": [
        "Peking University",
        "Convergence"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2109109241",
      "name": "Rui Yan",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3203309275",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W3176647794",
    "https://openalex.org/W4288480287",
    "https://openalex.org/W4206281850",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3005842225",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W4385565413",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2894740066"
  ],
  "abstract": "The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the “dominant winning ticket”). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1459 - 1472\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nFinding the Dominant Winning Ticket in Pre-Trained Language Models\nZhuocheng Gong1, Di He2, Yelong Shen3, Tie-yan Liu2,\nWeizhu Chen3, Dongyan Zhao1,4,5∗, Ji-rong Wen6 and Rui Yan6∗\n1Wangxuan Institute of Computer Technology, Peking University, China\n2Microsoft Research, Beijing, China, 3Microsoft Azure AI\n4Artificial Intelligence Institute of Peking University\n5State Key Laboratory of Media Convergence Production\n6Gaoling School of Artificial Intelligence, Renmin University of China\n{gzhch, zhaody}@pku.edu.cn, {jrwen, ruiyan}@ruc.edu.cn\n{dihe, yelong.shen, tyliu, wzchen}@microsoft.com\nAbstract\nThe Lottery Ticket Hypothesis suggests that for\nany over-parameterized model, a small subnet-\nwork exists to achieve competitive performance\ncompared to the backbone architecture. In this\npaper, we study whether there is a winning\nlottery ticket for pre-trained language models,\nwhich allow the practitioners to fine-tune the\nparameters in the ticket but achieve good down-\nstream performance. To achieve this, we regu-\nlarize the fine-tuning process with L1 distance\nand explore the subnetwork structure (what we\nrefer to as the \"dominant winning ticket\"). Em-\npirically, we show that (a) the dominant win-\nning ticket can achieve performance that is com-\nparable with that of the full-parameter model,\n(b) the dominant winning ticket is transferable\nacross different tasks, (c) and the dominant win-\nning ticket has a natural structure within each\nparameter matrix. Strikingly, we find that a\ndominant winning ticket that takes up 0.05%\nof the parameters can already achieve satisfac-\ntory performance, indicating that the PLM is\nsignificantly reducible during fine-tuning.\n1 Introduction\nPre-trained Language Models (PLMs) have shown\nsignificant performance on various natural lan-\nguage processing (NLP) tasks (Devlin et al., 2018;\nLiu et al., 2019). However, as the number of model\nparameters gets huge, fine-tuning such models be-\ncomes inefficient. Many previous works target\nparameter-efficient fine-tuning approaches by freez-\ning the PLM parameters. One can either freeze a\nsubset of the parameters (Zaken et al., 2021) and\n*corresponding authors: Dongyan Zhao\n(zhaody@pku.edu.cn) and Rui Yan (ruiyan@ruc.edu.cn)\nFigure 1: An illustration of the structure of the dominant\nwinning ticket.\nfine-tune the remainings or freeze all of them and\nplug in light modules with new learnable parame-\nters (Houlsby et al., 2019; Mahabadi et al., 2021;\nHu et al., 2021).\nIn parallel to this line of study, an emerging sub-\nfield has explored the possibility of training smaller\nsubnetworks in place of the full models without\nhurting performance (Lee et al., 2018; Wang et al.,\n2020). Among them, the lottery ticket hypothe-\nsis (LTH) (Frankle and Carbin, 2018) has attracted\nmuch attention. LTH demonstrates that an over-\nparameterized network contains \"winning tickets\"\n(small-scale subnetworks) that can 1) match the\nperformance of the full model; and 2) outper-\nform randomly sampled subnetworks of the same\nsize. Winning tickets have been verified to exist in\nPLMs (Prasanna et al., 2020).\nIn this paper, we provide an interesting result by\nshowing that a subnetwork, which we refer to as\nthe dominant winning ticket, exists in the PLM. It\ncan make us to freeze all other parameters but only\ntrain the parameters of the subnetwork and obtain\ncompetitive performance for any downstream tasks.\nTo achieve this, we fine-tune the PLM on one task\n1459\n(e.g., MNLI) and enforce the parameter weights\nclose to their initial weights (pre-trained weights)\nby using the L1-distance penalty. This allows us\nto identify which parts of the model parameters\nchange greater from the pre-trained weights during\nfine-tuning. We observe this ticket has some novel\nproperties:\n• The dominant winning ticket (i.e., the sub-\nnetwork) is extremely sparse, which only\ntakes up to 0.05% of the total parameters on\nRoBERTa-large. But fine-tuning the subnet-\nwork can achieve comparable performance\nwith fine-tuning the whole model.\n• Compared with randomly sampled subnet-\nworks of the same size, the dominant winning\nticket can achieve better performance with a\nfaster convergence rate.\n• The dominant winning ticket is insensitive to\nrandom seed and transferable across different\ndownstream tasks. In other words, it is intrin-\nsically determined by the pre-trained weights\nand can adapt to various downstream tasks.\n• The dominant winning ticket is highly struc-\ntured. It is like the \"skeleton\" of the network,\nwhich may give us some insights into the\nmechanism of PLMs.\nWe organize the paper as follows. After a brief\noverview of related works in section 2, we intro-\nduce how we are aware of the existence of the dom-\ninant winning ticket in PLMs and how we identify\nand extract it in section 3. Then we experiment\non the dominant winning ticket in section 4. In\nsection 5, we do further discussions about the dom-\ninant winning ticket. Section 6 is about some im-\nplications and future directions of the work.\n2 Related Work\n2.1 Pruning\nMultiple studies of BERT concluded that it is con-\nsiderably overparametrized (Kovaleva et al., 2019;\nMichel et al., 2019). In particular, it is possible\nto ablate elements of its architecture without loss\nin performance or even with slight gains (V oita\net al., 2019; Li et al., 2021). There has been much\nrecent work on compressing PLM. See overviews\nby Ganesh et al. (2021). Pruning is a promising line\nof work for model compression which involves ob-\ntaining smaller subnetworks with minimal perfor-\nmance loss (Gordon et al., 2020; Sajjad et al., 2020).\nA common approach is selecting the weights to be\npruned by magnitude (Han et al., 2015).\nPrevious work has found that there exist subnet-\nworks inside the neural network, which is called\nthe lottery ticket hypothesis (Frankle and Carbin,\n2018). Some of the recent findings are that the lot-\ntery ticket hypothesis holds for PLMs: inside large-\nscale pre-trained model there exist subnetworks\nthat can be retrained alone to reach the performance\nclose to that of the full model (Chen et al., 2020;\nPrasanna et al., 2020; Liang et al., 2021). Prasanna\net al. (2020) claimed that \"When BERT plays the\nlottery, all tickets are winning\". Liang et al. (2021)\nshows that there exist super tickets inside PLMs\nthat can improve generalization.\n2.2 Parameter-efficient Fine-tune\nParameter-efficient fine-tuning aims at reducing the\nnumber of trainable parameters when fine-tuning\nthe models across different downstream domains.\nVarious approaches are invented to achieve the goal.\nSome inserted and only trained adapters, which\nhave much lesser trainable parameters, between ex-\nisting layers (Houlsby et al., 2019; Mahabadi et al.,\n2021; Rebuffi et al., 2017). Another line of the\nstudy proposed to update only a subset of parame-\nters when fine-tuning. For example, Gordon et al.\n(2020) leveraged L0 regularization to limit the non-\nzero elements in the update vectors. Zaken et al.\n(2021) proposed that only tune bias terms can reach\na decent performance. Zhao et al. (2020) applied\nthe sparse binary mask to the pre-trained weights to\nreduce the trainable parameter size. Besides, some\nproposed that the PLM has a low intrinsic dimen-\nsionality (Aghajanyan et al., 2020). Hu et al. (2021)\nproposed a low-rank decomposition-based method\nthat can also significantly reduce the number of\ntrainable parameters. Chen et al. (2021) combined\nlow-rank decomposition and sparse mask during\nfine-tuning.\n3 The Dominant Winning Ticket in PLMs\nWe start with a question: which parts of the model\nparameters are more important when adapting\nPLMs to downstream tasks? To answer this ques-\ntion, we design the L1-regularized fine-tuning ap-\nproach and then analyze the L1-regularized weights\nand reveal the existence of the dominant winning\nticket.\n1460\nFigure 2: Visualization of out weighted parameters in RoBERTa-large. Each row is a model fine-tuned on different\ndownstream tasks. Each column is the parameter matrix of different layers. For example, the top left block represents\nthe value projection matrix of the 5th transformer layer from the model fine-tuned with the MNLI dataset. (\"K\",\n\"Q\", \"V\", \"O\" stand for the key, query, value, and output projection metrics of the self-attention module, \"FC1\" and\n\"FC2\" stand for the successive two fully-connected transformation metrics in each layer.)\n3.1 L1-regularized Fine-tuning\nTo identify the subnetwork, we apply L1 regulariza-\ntion to all the transformer parameters. Specifically,\nwe modify the original training objective, which\nresults in the following minimization problem,\nmin\nθ\nL(D, f,θ) + λ||θ − θ0||1, (1)\nwhere D represents for the task-specific data, θ\nis the model configuration(i.e., parameters), and\nθ0 is the pre-trained model, which is fixed. λ is a\nhyper-parameter to control the weight of the reg-\nularization term, i.e., the strength of the sparsity.\nThis training objective can make a large part of the\nparameters being close to their initial weights.\n3.2 Regularized Weights Analysis\nWe use L1-regularized fine-tuning to answer the\nprevious proposed question: which parts of the\nmodel parameters are more important when adapt-\ning PLMs to downstream tasks? We compute the\ndifference between the post-fine-tune weights and\nthe pre-trained weights to see how the weights\nchange. Specifically, we get ∆θ = θ − θ0. In-\nspired by magnitude weight pruning (Han et al.,\n2015), we hypothesize that the magnitude of ∆θ\ncan be an indicator. We observe that the magnitude\nof ∆θ is very small for most of the parameters\n(smaller than 1e-5 for 99% parameters), indicating\nthat the L1 regularization does take effect. How-\never, while most of the parameters remain close to\ntheir initial weights during fine-tuning, a very small\nfraction of parameters have much greater change.\nWe define these parameters as out weighted pa-\nrameters. Intuitively, we can choose a threshold σ\nto select out weighted parameters. Formally, we\ndefine σ-bounded out weighted parameters:\nθσ = mσ ⊙ θ, mσ ∈ {0, 1}|θ| (2)\nwhere mσ is a binary mask vector, mσ,i =\n1 {|∆θi| > σ}.\nWe try to understand the mechanism of fine-\ntuning by analyzing the distribution of σ-bounded\nout weighted parameters. We visualize the distri-\nbutions of the out weighted parameters on several\ndifferent tasks (MNLI, QNLI, SST-2, QQP) as seen\nin Figure 2, where the parameter matrics are\nWQ, WK, WV , WO ∈ R dmodel×dmodel\nWFC 1 ∈ R dmodel×dfc , WFC 2 ∈ R dfc ×dmodel\n(3)\nFor RoBERTa-large, dmodel is 1024 and dfc is\n4096.\nObservations 1. The locations of the out\nweighted parameters have strong correlations\namong different tasks. Each column in Figure 2\nshows the out weighted parameters of the same\nmatrix on different tasks, which has a very similar\nphenomenon. This high similarity indicates that\nthe location of the out weighted parameters may be\ndownstream task-agnostic. That is to say, the PLM\n1461\nTable 1: The structure of the dominated ticket. For each matrix, we select the top 3 dominated dimensions. We\nhighlight several \"popular\" dimensions across different layers with colors. Dimension 4096 in WFC 2 and 1024\nin WK, WQ, WV , WO, WFC 1 are the symbol for the bias term.\nLayer WQ WK WV WO WFC1 WFC2\n0 981 673 304 1019 781 108 673 981 472 647 766 986 673 981 1019 487 584 3291\n1 981 673 106 106 981 673 673 981 106 593 929 587 673 106 981 1559 3646 1995\n2 673 981 106 981 106 673 673 106 981 1024 579 237 673 106 981 3708 3058 2753\n3 673 106 981 981 106 673 673 106 981 810 819 784 673 106 981 1591 480 609\n4 673 106 981 981 673 106 106 673 981 101 1024 76 673 106 981 3445 1906 2828\n5 673 106 981 673 106 981 673 106 981 1024 579 189 673 106 981 682 2123 841\n6 673 106 981 673 106 981 673 106 981 1024 430 498 673 106 981 1267 2920 2686\n7 673 106 981 673 474 106 673 106 412 692 670 699 673 106 412 2058 3485 1660\n8 673 106 412 474 673 547 673 412 474 1024 534 69 673 106 412 2780 2387 1015\n9 673 412 2 474 673 256 673 474 412 11 1024 608 673 2 412 3892 448 616\n10 673 412 474 474 673 256 673 474 412 254 534 1024 673 412 2 1028 592 1462\n11 673 2 93 474 256 623 673 474 412 330 758 1024 673 474 547 1014 3730 2157\n12 673 547 93 474 256 673 673 474 547 151 909 1024 673 474 547 145 2436 2338\n13 673 547 2 474 256 673 673 474 547 113 1024 151 673 547 474 902 2503 1700\n14 673 547 631 474 256 673 673 474 547 141 1024 307 673 631 547 1219 1318 1535\n15 673 631 547 631 673 474 673 631 474 1024 523 724 673 631 547 2413 608 15\n16 673 631 547 673 631 256 631 673 547 1024 657 819 631 673 547 3454 4096 1288\n17 631 673 547 673 631 256 631 673 547 254 640 209 631 673 547 3433 2 3962\n18 631 673 547 673 631 256 631 673 547 986 999 966 631 673 547 628 2617 4096\n19 631 673 547 673 631 256 631 673 399 657 1 845 631 673 399 1221 827 4096\n20 631 673 547 673 631 256 631 673 399 975 453 1006 631 673 914 1442 4096 1669\n21 631 673 None 673 631 None 631 673 547 1024 975 400 631 673 914 3290 4096 2850\n22 631 None None None None None 631 673 547 559 1024 46 631 673 914 3341 3176 1078\n23 631 673 None 631 673 None 631 673 914 167 871 891 631 673 422 2498 2596 1565\nitself determines which parameters tend to be out\nweighted.\nObservations 2. If looking into each block in\nFigure 2, we can see that the out weighted parame-\nters are distributed along with the output dimension.\nThe out weighted parameters in the matrix tend to\nbe dominated by a few output dimensions, what\nwe refer to as dominated dimensions. We identify\ndominated dimensions by counting the number of\nout weighted parameters in each dimension and\nobserve that this phenomenon exists in most of the\nparameter matrices in the PLM. We list all the dom-\ninated dimensions of the model to further analyze.\nDetails can be found in Table 2. An interesting\nfinding is that some dimensions consistently domi-\nnate several successive layers. For example, dimen-\nsion 6731 and 631 dominate WQ, WK, WV , and\nWFC 1 for more than 10 successive transformer\nlayers. These dimensions are like the skeleton of\nthe PLM that exists from bottom to top.\nWith the above observations, now we can pro-\npose our hypothesis about the dominant winning\nticket of PLMs.\nHypothesis. There exists a dominant winning\nticket inside a PLM that is intrinsically determined\nby the pre-trained weights. When fine-tuned in\nisolation, we can only finetune the parameters of\nthis ticket which can match the performance of full-\n1All dimensions in the paper are zero-indexed.\nparameter fine-tuning while converging faster than\nother methods.\n3.3 Extracting the Dominant Winning Ticket\nWhen it comes to extracting the dominant winning\nticket, the first question is to decide the sparsity of\nthe subnetwork (the number of trainable parame-\nters). Generally, we extract the dominant winning\nticket with algorithm 1.\nAlgorithm 1 Extracting the dominant winning\nticket\n1: Fine-tune a PLM f(x; θ0) with L1 regulariza-\ntion on any dowmstream task dataset D, get\nf(x; θ).\n2: Calculate ∆θ = θ − θ0, then select out out\nweighted parameters θσ with threshold σ.\n3: Select the k most dominated dimensions each\nmatrix, which forms the dominant winning\nticket.\nWe use hyperparameter k to control the spar-\nsity of the subnetwork. We empirically find that\nthe scale of the subnetwork is extremely small. k\nequals 1 (at most one dominated dimension for\neach matrix) is enough to achieve comparable re-\nsults to full-parameter fine-tuning for most of the\ntasks.\nIt is worth noticing that our extracted dominant\nwinning ticket excludes embedding and layer nor-\n1462\nmalization. Besides, as bias terms are vital to some\nextend (Zaken et al., 2021), we include bias terms\nwhen identifying the dominated dimensions, so the\nbias term in each matrix also has a chance to be\nselected.\n4 Testing the Dominant Winning Ticket\nfor Fine-tuning\nThe previous section is about how we identify and\nextract the dominant winning ticket. In this section,\nwe discuss the properties of the dominant winning\nticket by conducting systematic experiments.\nTo extract the dominant winning tickets, we per-\nform L1-regularized fine-tuning on MNLI, QNLI,\nSST-2, and QQP respectively. We find that the\ndominant winning tickets corresponding to differ-\nent downstream tasks look very close to each other\n(detailed statistics can be found in Appendix A.1),\nwhich matches the observation in Section 3.2. So,\nfor simplicity, we regard the subnetwork extracted\nfrom MNLI as the standard dominant winning\nticket. Our evaluation experiments are conducted\nupon it.\nWe compare the dominant winning ticket with\nthe subnetwork that has the same size as the\ndominant winning ticket, denoted as Dominant-\nk and Random-k respectively. The dimensions\nof Random-k are chosen from uniform distribu-\ntion. As the scale of the extracted subnetwork is\nadjustable by choosing different k, we consider\ntwo different compression ratios, i.e., k = 1 (one\ndimension per matrix at most) and k = 3 (three\ndimension per matrix at most).\n4.1 Exprimental Setup\nDatasets and models. We conduct experiments\non the GLUE benchmark (Wang et al., 2018). The\nevaluation is performed on the GLUE dev sets. We\nuse the publicly available RoBERTa-large 2 (Liu\net al., 2019) as pre-trained language models in all\nour experiments.\nImplementation details. Our implementation\nis based on the fairseq toolkit 3 (Ott et al.,\n2019). We fine-tune on the GLUE tasks fol-\nlowing the standard procedures. We optimize\nusing AdamW (Loshchilov and Hutter, 2018),\nwith batch size of 16. For L1-regularized fine-\ntuning, we empirically set the weight of L1 reg-\n2The dominant winning ticket in the paper\nis extracted from this specific model checkpoint.\nhttps://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz\n3https://github.com/pytorch/fairseq\nularizer λ to 0.001, and the threshold σ to 5e-5.\nFor dominant winning ticket fine-tuning, we per-\nform a hyperparameter search over initial learning\nrate in {5e-5, 1e-4, 2e-4, 4e-4}. For full-parameter\nfine-tuning, we search initial learning rate in\n{1e-5, 2e-5, 3e-5, 5e-5}.\n4.2 Performance\nOur main results on the GLUE benchmark are\nshown in Table 2. Fine-tuning the dominant win-\nning ticket can match the performance of fine-\ntuning the whole model while only requiring less\nthan 0.2% trainable parameters per task. Perfor-\nmance of Dominant-3 and Dominant-1 has no sig-\nnificant difference for most of the tasks. For small\ndatasets like MRPC and RTE, the smaller subnet-\nwork even performs slightly better because less\ntrainable parameters means free from overfitting.\nThis phenomenon also indicates that the scale of\nthe dominant winning ticket inside the PLMs can\nbe extremely small. When comparing the domi-\nnant winning ticket with random tickets that has\nthe same size, we can see that Random-3 performs\npretty well. We think this benifits from the strong\nreducibility of PLMs. However, when the spar-\nsity grows, the performance of Random-1 is much\nworse than Dominant-1, indicating that randomly\nsampled subnetworks unavoidable deletes useful\ninformation and become less expressive at such a\nlevel of sparsity.\nFigure 3 shows the training and evaluation\ncurves of different methods. We can clearly see\nthat the dominant winning ticket has advantages\nover random subnetworks in terms of convergence\nrate. For large datasets like QNLI, Dominant-1 and\nDominant-3 reach the best validation accuracy in\nthe first several epochs, while random subnetworks\nrequire much more training steps to warm up. The\ndominant winning ticket also gets lower training\nlosses in all tasks, indicating it fits the data better.\nAnother observation is that, though Dominant-3\nand Random-3 have no significant performance\ngaps in most of the tasks as seen in Table 2, their\nlearning behaviors vary quite a lot. Random-3 gets\nsatisfactory results eventually, but it requires much\nmore efforts to train. Meanwhile, there is no obvi-\nous difference between Dominant-1 and Dominant-\n3 from Figure 3, which is a good sign. This phe-\nnomenon suggests that when shrinking the parame-\nter size from 0.19% to 0.05%, the capacity of the\nmodel almost keeps unchanged.\n1463\nFigure 3: (Left) The training loss curves. (Middle) The training accuracy curves. (Right) The validation accracy\ncurve. We visualize the training process for QNLI, MRPC, and RTE. For MPRC and RTE, early stopping is triggered\nif the validation accuracy does not increase for 8 successive epochs.\n1464\nTable 2: The dominant winning ticket vs random ticket. We report the overall (matched and mismatched) accuracy\nfor MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. All results\nare the average of 5 trials.\nSparsity QNLI SST-2 MNLI CoLA MRPC STS-B RTE QQP\n105k 67k 393k 8.6k 3.7k 5.8k 2.5k 364k\nFull-ft 100% 94.7 96.4 90.4 68 90.9 92.4 86.4 92.2\nRandom-3 0.19% 94.1 96.2 89.8 68.6 89.4 91.8 72.2 91.1\nDominant-3 0.19% 94.3 96.3 90.5 69 90.2 92.2 86.6 91.7\nBitFit (Zaken et al., 2021) 0.06% 94.5 96 86.7 66.3 89.7 92 86.3 88.9\nRandom-1 0.05% 91.7 94.4 85.4 51.4 74.8 88.7 68.5 88.3\nDominant-1 0.05% 94.6 96.1 90.4 69.7 90.9 92.2 87.7 91\nTable 3: Performance comparison of full-parameter fine-\ntuning and 5%-parameter fine-tuning.\nParams 100% 5%\nQNLI 94.7 94.8\nSST-2 96.4 96.6\nMNLI 90.4 90.6\nCoLA 68 69.7\nMRPC 90.9 91.7\nSTS-B 92.4 92.2\nRTE 86.4 88\nQQP 91.9 91.6\n4.3 Stability of the Dominant Winning Ticket\nWe expect that subnetworks extracted from differ-\nent L1-regularized fine-tuning runs (with differ-\nent random seeds and different tasks) have similar\nstructures. We use Jaccard similarity to measure\nthe similarity between different tickets. Specifi-\ncally, the Jaccard similarity between two sets is\ndefined as:\nJ(A, B) = |A ∩ B|\n|A ∪ B|. (4)\nWe test four random seeds and four tasks (MNLI,\nQNLI, QQP, and SST-2). The average Jaccard\nsimilarity is 0.69 across seeds and 0.67 across\ntasks while the similarity between random tickets\nis nearly 0. This means that the dominant ticket has\na stable structure that utilize a certain portion of\npre-trained weights. This structure is task-agnostic\nand irrelevant to random seeds as observed in Sec-\ntion 3.2. Details about the structures of subnet-\nworks can be found in Appendix A.1.\n5 Discussions\n5.1 When BERT Plays the Lottery, Are All\nTickets Winning?\nThe lottery ticket hypothesis (LTH) states that\ndense, randomly-initialized networks contain sub-\nnetworks (winning tickets) that–when trained in\nTable 4: Dominated dimensions of WO and WFC 2.\nBias terms are highlighted with blue color.\nLayer WO WFC 2\n0 647 766 986 487 584 3291\n1 593 929 587 1559 3646 1995\n2 1024 579 237 3708 3058 2753\n3 810 819 784 1591 480 609\n4 101 1024 76 3445 1906 2828\n5 1024 579 189 682 2123 841\n6 1024 430 498 1267 2920 2686\n7 692 670 699 2058 3485 1660\n8 1024 534 69 2780 2387 1015\n9 11 1024 608 3892 448 616\n10 254 534 1024 1028 592 1462\n11 330 758 1024 1014 3730 2157\n12 151 909 1024 145 2436 2338\n13 113 1024 151 902 2503 1700\n14 141 1024 307 1219 1318 1535\n15 1024 523 724 2413 608 15\n16 1024 657 819 3454 4096 1288\n17 254 640 209 3433 2 3962\n18 986 999 966 628 2617 4096\n19 657 1 845 1221 827 4096\n20 975 453 1006 1442 4096 1669\n21 1024 975 400 3290 4096 2850\n22 559 1024 46 3341 3176 1078\n23 167 871 891 2498 2596 1565\nisolation–reach performance comparable to the\noriginal network.\nInitialization is an important factor in LTH as\nthe winning ticket extracted from a specific initial-\nization generally behaviors poorly on other random\nreinitialized networks (Frankle and Carbin, 2018).\nThis phenomenon is often ignored when discussing\nLTH in PLMs as the initialization of PLMs is deter-\nministic. PLMs initialize via pre-training weights,\nwhich contain rich information and are potentially\nuseful. As a result, many subnetworks inside PLMs\nare potential winning tickets\nIndeed, Prasanna et al. (2020) claimed that\neven the \"bad\" subnetworks in BERT (“good” and\n“bad”) have acceptable performance. We further\nargue that any randomly sampled subnetwork with\nthe sparsity of 95% can serve as the winning ticket.\nWe randomly select 5% of the parameters in each\n1465\nmatrix and freeze the rest of the model, surprisingly\nfinding that the performance is comparable with\nfull-parameter fine-tuning. As seen in Table 3, a\nsimple random subnetwork requiring no sophisti-\ncated pruning techniques can already match with\nfine-tuning.\nHowever, we still argue that not all tickets are\nwinning. The smaller the ticket is, the rarer the\nwinning ticket is.When the sparsity of the subnet-\nwork increases, the performance of subnetworks\nbegins to vary. For RoBERTa-large, at the sparsity\nof 99.95%, most of the \"ticket\" fail to win while a\nwell-selected subnetwork (the dominant winning\nticket) can suffer little performance drops. In this\nsense, the dominant winning ticket we found can\nbe regarded as the smallest winning ticket.\n5.2 Structured Winning Ticket\nOne astonishing fact about the dominant winning\nticket is that it is naturally structured. When we\nperform L1-regularized fine-tuning, the L1 regu-\nlarization is equally applied to all parameters. But\nweights in some dimensions tend to deviate from\nthe pre-trained weights more than others. These\ndimensions, which we refer to as dominated dimen-\nsions, are shared among different layers (like the\n673, 631, 474, and 106 dimensions in Table 1).\nIt seems like there is a \"skeleton\" inside the PLM\nthat can serve as the dominant winning ticket.\nThe structure of the dominant winning ticket is\ndifferent from other structured pruning studies in\ntwo ways. First, while their structures usually re-\nfer to certain parts of the model (e.g., channels in\nconvolutional layers and attention heads in Trans-\nformers), the structure of the dominant winning\nticket is micro within each parameter matrix. Sec-\nond, while most of the structured subnetworks rely\non structured pruning methods (Liang et al., 2021),\nwe do not apply structure-aware regularization tech-\nniques. In other words, the structure of the domi-\nnant winning ticket is naturally formed, waiting to\nbe found.\n5.3 Connections with Bias-terms Fine-tuning\nThe dominant winning ticket has some connections\nwith bias-terms fine-tuning (BitFit) (Zaken et al.,\n2021). The idea of BitFit is to fine-tune only the\nbias terms in the transformer, which only requires\nupdating a very small subset of parameters. If we\ntreat the bias term of an extended dimension of\nthe weight matrix, then the subnetwork forms by\nthe bias terms have a similar structure with the\ndominant winning ticket when k equals one (one\ntrainable dimension per matrix). And the trainable\nparameter size is close too.\nAs can be seen in Table 2, BitFit is quite promis-\ning with small-to-medium training data. When\nthe size of the training data is large, it still has ac-\nceptable performance. The overall performance of\nBitFit is much better than Random-1, indicating\nthat the bias terms indeed catch some additional\nsemantics. We find that the subnetwork of bias\nterms overlaps the dominant winning ticket. As\nshown in Table 4, the bias term serves as one of\nthe dominated dimensions in WO and WFC 2 a lot.\nWe think this might be an explanation of why only\ntuning the bias terms works well.\n6 Implications and Future Work\nThe dominant winning ticket can be utilized for\nparameter-efficient fine-tuning. As the dominant\nwinning ticket is stable across different tasks, when\ndeploying PLMs on different scenarios, we only\nneed to record the same small group of parame-\nters. Besides, benefiting from the structure of the\nticket, we only need to store the optimizer states\nfor certain dimensions of each parameter matrix.\nWith decent code implementation, we can promis-\ningly reduce memory usage and speed up the train-\ning process. In future work, we would examine\nthe memory reduction and speedup abilities of the\ndominant winning ticket.\nBesides the practical value, the dominant win-\nning ticket raises some interesting questions about\nPLMs. Now that the dominant winning ticket is\nintrinsically determined by the pre-trained weights,\nthen how does the subnetwork emerge during pre-\ntraining? Another worth investigating point lies\nin the natural structure of the ticket. Why certain\ndimensions of parameter matrices behave so differ-\nently from others?We still know little about the\nmechanism behind these phenomenons. We aim to\nstudy these questions in future work.\n7 Conclusion\nIn this paper, we reveal the existence of the domi-\nnant winning ticket inside pre-trained models and\nintroduce the L1-regularized fine-tuning to extract\nit. The dominant ticket is an extremely sparse sub-\nnetwork that can reach comparable performance\nwith fine-tuning the whole model. We observe\nthat the ticket has some novel properties. First, it\nis stable across different random seeds and tasks,\n1466\nwhich means once identified on one task, it can be\ntransferred to other tasks with no performance loss.\nSecond, the ticket has a natural structure within\neach parameter matrix, and this structure is shared\nacross layers. Our study not only has practical\nvalues for parameter-efficient fine-tuning but also\nraises some questions about the pre-trained models.\nAcknowledgments\nThis work is supported in part by the\nNational Hi-Tech RD Program of China\n(No.2020AAA0106600).\nReferences\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\nGupta. 2020. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. arXiv\npreprint arXiv:2012.13255.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Si-\njia Liu, Yang Zhang, Zhangyang Wang, and\nMichael Carbin. 2020. The lottery ticket hypoth-\nesis for pre-trained bert networks. arXiv preprint\narXiv:2007.12223.\nXuxi Chen, Tianlong Chen, Yu Cheng, Weizhu Chen,\nZhangyang Wang, and Ahmed Hassan Awadallah.\n2021. Dsee: Dually sparsity-embedded efficient tun-\ning of pre-trained language models. arXiv preprint\narXiv:2111.00160.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJonathan Frankle and Michael Carbin. 2018. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In International Conference on Learning\nRepresentations.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali\nKhan, Yin Yang, Hassan Sajjad, Preslav Nakov, Dem-\ning Chen, and Marianne Winslett. 2021. Compress-\ning large-scale transformer-based models: A case\nstudy on bert. Transactions of the Association for\nComputational Linguistics, 9:1061–1080.\nMitchell Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 143–155.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. Advances in Neural Infor-\nmation Processing Systems, 28.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374.\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip\nTorr. 2018. Snip: Single-shot network pruning based\non connection sensitivity. In International Confer-\nence on Learning Representations.\nJiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021.\nDifferentiable subset pruning of transformer heads.\narXiv preprint arXiv:2108.04657.\nChen Liang, Simiao Zuo, Minshuo Chen, Haoming\nJiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and\nWeizhu Chen. 2021. Super tickets in pre-trained lan-\nguage models: From model compression to improv-\ning generalization. arXiv preprint arXiv:2105.12002.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in adam.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021. Compacter: Efficient low-\nrank hypercomplex adapter layers. arXiv preprint\narXiv:2106.04647.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin Neural Information Processing Systems, 32:14014–\n14024.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of NAACL-HLT\n2019: Demonstrations.\nSai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.\nWhen bert plays the lottery, all tickets are winning.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3208–3229.\n1467\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Proceedings of the 31st\nInternational Conference on Neural Information Pro-\ncessing Systems, pages 506–516.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav\nNakov. 2020. Poor man’s bert: Smaller and faster\ntransformer models. arXiv e-prints, pages arXiv–\n2004.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting,\nthe rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5797–5808.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nChaoqi Wang, Guodong Zhang, and Roger Grosse. 2020.\nPicking winning tickets before training by preserving\ngradient flow. arXiv preprint arXiv:2002.07376.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitfit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Schütze. 2020. Masking as an efficient alterna-\ntive to finetuning for pretrained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226–2241.\n1468\nA Appendix\nA.1 Structure Comparison of Dominant\nWinning Tickets\n1469\nTable 5: Structure comparison of WQ.\nLayer MNLI QNLI SST QQP\n0 304 673 981 673 981 1019 8 673 981 673 981 1019\n1 106 673 981 106 673 981 106 673 981 106 673 981\n2 106 673 981 106 673 981 106 673 981 106 673 981\n3 106 673 981 106 673 981 106 673 981 106 673 981\n4 106 673 981 106 673 981 106 673 981 106 673 981\n5 106 673 981 106 673 981 106 673 981 106 673 981\n6 106 673 981 106 673 981 106 673 981 106 673 981\n7 106 673 981 106 673 981 106 412 673 106 412 673\n8 106 412 673 106 412 673 106 412 673 106 412 673\n9 2 412 673 2 412 673 106 412 673 2 412 673\n10 412 474 673 2 412 673 2 412 673 2 412 673\n11 2 93 673 2 547 673 474 623 673 2 412 673\n12 93 547 673 2 547 673 474 547 673 2 547 673\n13 2 547 673 2 547 673 51 547 673 2 547 673\n14 547 631 673 2 547 673 2 547 673 547 631 673\n15 547 631 673 547 631 673 547 631 673 547 631 673\n16 547 631 673 547 631 673 547 631 673 2 631 673\n17 547 631 673 547 631 673 547 631 673 547 631 673\n18 547 631 673 547 631 673 547 631 673 547 631 673\n19 547 631 673 256 631 673 547 631 673 547 631 673\nTable 6: Structure comparison of WK.\nLayer MNLI QNLI SST QQP\n0 108 781 1019 673 981 1019 93 781 1019 328 981 1019\n1 106 673 981 106 673 981 106 673 981 106 673 981\n2 106 673 981 106 673 981 106 673 981 106 673 981\n3 106 673 981 106 673 981 106 673 981 106 673 981\n4 106 673 981 106 673 981 106 673 981 106 673 981\n5 106 673 981 106 673 981 106 673 981 106 673 981\n6 106 673 981 106 673 981 106 673 981 106 673 981\n7 106 474 673 106 673 981 106 547 673 106 547 673\n8 474 547 673 474 547 673 474 547 673 474 547 673\n9 256 474 673 2 474 673 256 474 673 2 474 673\n10 256 474 673 256 474 673 256 474 673 474 547 673\n11 256 474 623 256 474 673 2 474 673 2 256 474\n12 256 474 673 256 474 673 2 474 673 256 474 673\n13 256 474 673 256 474 673 2 474 673 256 474 673\n14 256 474 673 256 474 673 256 474 673 256 474 673\n15 474 631 673 256 631 673 474 631 673 474 631 673\n16 256 631 673 256 631 673 547 631 673 256 631 673\n17 256 631 673 256 631 673 256 631 673 256 631 673\n18 256 631 673 256 631 673 534 631 673 256 631 673\n19 256 631 673 256 631 673 631 673 842 631 673 914\n1470\nTable 7: Structure comparison of WV .\nLayer MNLI QNLI SST QQP\n0 472 673 981 8 673 981 328 673 981 8 673 981\n1 106 673 981 106 673 981 106 673 981 106 673 981\n2 106 673 981 106 673 981 106 673 981 106 673 981\n3 106 673 981 106 673 981 106 673 981 106 673 981\n4 106 673 981 106 673 981 106 673 981 106 673 981\n5 106 673 981 106 673 981 106 673 981 106 673 981\n6 106 673 981 106 673 981 106 673 981 106 412 673\n7 106 412 673 106 412 673 106 412 673 106 412 673\n8 412 474 673 412 474 673 106 474 673 412 474 673\n9 412 474 673 412 474 673 412 474 673 412 474 673\n10 412 474 673 412 474 673 412 474 673 412 474 673\n11 412 474 673 2 474 673 412 474 673 412 474 673\n12 474 547 673 2 474 673 140 474 673 474 547 673\n13 474 547 673 2 474 673 474 547 673 474 547 673\n14 474 547 673 474 547 673 474 547 673 474 547 673\n15 474 631 673 474 631 673 474 631 673 474 631 673\n16 547 631 673 547 631 673 547 631 673 547 631 673\n17 547 631 673 547 631 673 547 631 673 547 631 673\n18 547 631 673 547 631 673 547 631 673 547 631 673\n19 399 631 673 547 631 673 631 673 842 547 631 673\n20 399 631 673 547 631 673 547 631 673 547 631 673\n21 547 631 673 547 631 673 547 631 673 547 631 673\n22 547 631 673 547 631 673 547 631 673 547 631 673\n23 631 673 914 547 631 673 547 631 673 41 631 673\nTable 8: Structure comparison of WO.\nLayer MNLI QNLI SST QQP\n0 647 766 986 647 725 766 304 647 725 647 725 766\n1 587 593 929 593 622 929 586 616 622 587 593 929\n2 237 579 1024 237 579 612 448 579 1024 488 556 579\n3 784 810 819 784 824 1024 589 784 1024 784 814 1014\n4 76 101 1024 10 421 1024 367 494 730 648 696 1024\n5 189 579 1024 225 579 1024 133 632 1024 203 204 232\n6 430 498 1024 430 498 1024 430 498 1024 430 500 1024\n7 670 692 699 665 692 1024 386 399 422 133 692 1024\n8 69 534 1024 67 534 1024 442 512 574 67 464 1024\n9 11 608 1024 11 608 1024 608 953 1024 11 830 1024\n10 254 534 1024 254 534 1024 254 534 1024 254 534 1024\n11 330 758 1024 330 758 1024 330 758 1024 330 758 843\n12 151 909 1024 73 909 1024 73 669 1024 73 151 1024\n13 113 151 1024 113 151 1024 113 941 1024 113 941 1024\n14 141 307 1024 141 307 1024 141 307 1024 141 489 506\n15 523 724 1024 523 722 1024 523 643 1024 488 523 1024\n16 657 819 1024 132 183 519 111 519 1024 145 183 1024\n17 209 254 640 567 640 657 74 107 125 871 984 1024\n18 966 986 999 633 763 1024 763 889 891 207 986 1024\n19 1 657 845 1 4 57 1 657 1024 1 405 657\n20 453 975 1006 293 363 453 453 1007 1020 453 1002 1024\n21 400 975 1024 437 519 903 901 975 1024 320 597 986\n22 46 559 1024 331 559 790 483 559 1024 180 195 626\n23 167 871 891 16 660 886 87 130 147 101 238 606\n1471\nTable 9: Structure comparison of WFC 1.\nLayer MNLI QNLI SST QQP\n0 673 981 1019 106 673 981 106 673 981 106 673 981\n1 106 673 981 106 673 981 106 673 981 106 673 981\n2 106 673 981 106 673 981 106 673 981 106 673 981\n3 106 673 981 106 673 981 106 673 981 106 673 981\n4 106 673 981 106 673 981 106 673 981 106 673 981\n5 106 673 981 106 673 981 106 673 981 106 673 981\n6 106 673 981 106 673 981 106 412 673 106 673 981\n7 106 412 673 2 106 673 106 412 673 106 412 673\n8 106 412 673 2 106 673 106 412 673 2 412 673\n9 2 412 673 2 412 673 412 474 673 2 412 673\n10 2 412 673 2 474 673 412 474 673 2 412 673\n11 474 547 673 2 474 673 2 474 673 2 474 673\n12 474 547 673 474 547 673 474 547 673 474 547 673\n13 474 547 673 474 547 673 474 547 673 474 547 673\n14 547 631 673 547 631 673 547 631 673 2 631 673\n15 547 631 673 547 631 673 547 631 673 2 631 673\n16 547 631 673 547 631 673 547 631 673 547 631 673\n17 547 631 673 547 631 673 547 631 673 547 631 673\n18 547 631 673 547 631 673 547 631 673 547 631 673\n19 399 631 673 547 631 673 547 631 673 547 631 673\n20 631 673 914 256 631 673 547 631 673 547 631 673\n21 631 673 914 256 631 673 547 631 673 547 631 673\n22 631 673 914 256 631 673 547 631 673 41 631 673\n23 422 631 673 631 673 841 265 563 631 631 651 715\nTable 10: Structure comparison of WFC 2.\nLayer MNLI QNLI SST QQP\n0 487 584 3291 487 584 2390 487 584 3660 458 487 584\n1 1559 1995 3646 1114 1559 4060 1559 3646 3912 1114 1559 4060\n2 2753 3058 3708 445 3056 3708 870 3636 3708 445 3056 3708\n3 480 609 1591 609 1177 1591 44 1591 1846 609 1177 1591\n4 1906 2828 3445 1393 2828 3445 1773 3445 3940 1906 2828 3445\n5 682 841 2123 682 841 3582 1298 1427 3624 682 841 2475\n6 1267 2686 2920 425 1072 2200 909 1946 3027 425 2686 2920\n7 1660 2058 3485 1660 2058 3485 317 1660 2058 1660 2058 3485\n8 1015 2387 2780 1832 2780 2998 217 1247 2982 933 2387 2780\n9 448 616 3892 34 1692 3892 34 1910 3892 34 1966 3892\n10 592 1028 1462 592 1028 4096 592 1028 2063 592 1028 4096\n11 1014 2157 3730 1014 2877 3730 1014 2877 3730 1014 2877 3730\n12 145 2338 2436 145 2436 2754 145 1534 2338 145 2338 2436\n13 902 1700 2503 902 1034 2503 902 1700 2503 902 1034 2503\n14 1219 1318 1535 1219 1535 2787 1535 2963 3720 866 1535 3443\n15 15 608 2413 15 64 2413 15 1997 2413 608 2413 3212\n16 1288 3454 4096 1009 2827 3454 2639 2827 3454 1531 2827 4096\n17 2 3433 3962 703 895 3235 699 2834 3475 703 1070 2588\n18 628 2617 4096 1429 3091 4096 834 3130 3913 484 2982 4096\n19 827 1221 4096 879 2409 3438 34 1207 4096 2556 3333 4096\n20 1442 1669 4096 1299 2177 3491 1442 3329 4096 1880 2527 3894\n21 2850 3290 4096 49 159 1116 2698 3290 4096 1426 3123 3290\n22 1078 3176 3341 87 1669 3798 2365 2404 4096 1444 2576 3745\n23 1565 2498 2596 1695 2544 3289 607 2010 2142 267 704 2084\n1472",
  "topic": "Ticket",
  "concepts": [
    {
      "name": "Ticket",
      "score": 0.9670522212982178
    },
    {
      "name": "Subnetwork",
      "score": 0.832324743270874
    },
    {
      "name": "Computer science",
      "score": 0.7426894903182983
    },
    {
      "name": "Parameterized complexity",
      "score": 0.6147335171699524
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3302712142467499
    },
    {
      "name": "Computer network",
      "score": 0.2748897671699524
    },
    {
      "name": "Algorithm",
      "score": 0.17424872517585754
    }
  ]
}