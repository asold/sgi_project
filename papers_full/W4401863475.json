{
  "title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)",
  "url": "https://openalex.org/W4401863475",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2880458404",
      "name": "Kenthapadi, Krishnaram",
      "affiliations": [
        "Oracle (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4288983082",
      "name": "Sameki, Mehrnoosh",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4222736721",
      "name": "Taly, Ankur",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2473418344",
    "https://openalex.org/W4394951183",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4389519963",
    "https://openalex.org/W4404783040",
    "https://openalex.org/W2909212904",
    "https://openalex.org/W4283167699",
    "https://openalex.org/W4389520260",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W4378908626",
    "https://openalex.org/W4404782801",
    "https://openalex.org/W4393160204",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3185727840",
    "https://openalex.org/W3175638203",
    "https://openalex.org/W4224037546",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W4389519585",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W3185146124",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4389518764",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4381432831",
    "https://openalex.org/W4378976798",
    "https://openalex.org/W4389524379",
    "https://openalex.org/W4389524506",
    "https://openalex.org/W4401042808",
    "https://openalex.org/W4401042371",
    "https://openalex.org/W4402684046",
    "https://openalex.org/W3170572542",
    "https://openalex.org/W4226040778",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4377010286",
    "https://openalex.org/W4401042338",
    "https://openalex.org/W4394877201",
    "https://openalex.org/W2350778671",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4389518686",
    "https://openalex.org/W3212596026",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W4389520670",
    "https://openalex.org/W4386566840",
    "https://openalex.org/W4404534210"
  ],
  "abstract": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our KDD 2024 tutorial, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms.",
  "full_text": "arXiv:2407.12858v1  [cs.CL]  10 Jul 2024\nGrounding and Evaluation for Large Language Models: Practical\nChallenges and Lessons Learned (Survey)\nKrishnaram Kenthapadi\nOracle Health AI\nRedwood City, CA, USA\nkrishnaram.kenthapadi@oracle.com\nMehrnoosh Sameki\nMicrosoft Azure AI\nBoston, MA, USA\nmesameki@microsoft.com\nAnkur Taly\nGoogle Cloud AI\nMountain View, CA, USA\nataly@google.com\nAbstract\nWith the ongoing rapid adoption of Artiﬁcial Intelligence ( AI)-based\nsystems in high-stakes domains, ensuring the trustworthin ess, safety,\nand observability of these systems has become crucial. It is essen-\ntial to evaluate and monitor AI systems not only for accuracy and\nquality-related metrics but also for robustness, bias, sec urity, inter-\npretability, and other responsible AI dimensions. We focus on large\nlanguage models (LLMs) and other generative AI models, whic h\npresent additional challenges such as hallucinations, har mful and\nmanipulative content, and copyright infringement. In this survey\narticle accompanying our tutorial , we highlight a wide range of\nharms associated with generative AI systems, and survey sta te of\nthe art approaches (along with open challenges) to address t hese\nharms.\nCCS Concepts\n• Computing methodologies → Artiﬁcial intelligence ; Ma-\nchine learning .\nKeywords\nResponsible AI; Generative AI; Large Language Models; Grou nd-\ning; Evaluations; Truthfulness; Safety and Alignment; Bia s and Fair-\nness, Model Robustness and Security; Privacy; Model Disgor ge-\nment and Unlearning; Copyright Infringement; Calibration and Con-\nﬁdence; Transparency and Causal Interventions.\n1 Introduction\nConsidering the increasing adoption of Artiﬁcial Intellig ence (AI)\ntechnologies in our daily lives, it is crucial to develop and deploy\nthe underlying AI models and systems in a responsible manner and\nensure their trustworthiness, safety, and observability. Our focus is\non large language models (LLMs) and other generative AI mode ls\nand applications. Such models and applications need to be ev alu-\nated and monitored not only for accuracy and quality-relate d met-\nrics but also for robustness against adversarial attacks, r obustness\nunder distribution shifts, bias and discrimination agains t under-\nrepresented groups, security and privacy protection, inte rpretabil-\nity, hallucinations (and other ungrounded or low-quality o utputs),\nPermission to make digital or hard copies of all or part of thi s work for personal or\nclassroom use is granted without fee provided that copies ar e not made or distributed\nfor proﬁt or commercial advantage and that copies bear this n otice and the full cita-\ntion on the ﬁrst page. Copyrights for components of this work owned by others than\nthe author(s) must be honored. Abstracting with credit is pe rmitted. To copy other-\nwise, or republish, to post on servers or to redistribute to l ists, requires prior speciﬁc\npermission and/or a fee. Request permissions from permissi ons@acm.org.\nKDD ’24, August 25–29, 2024, Barcelona, Spain\n© 2024 Copyright held by the owner/author(s). Publication r ights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671467\nharmful content (such as sexual, racist, and hateful respon ses), jail-\nbreaks of safety and alignment mechanisms, prompt injectio n at-\ntacks, misinformation and disinformation, fake, misleadi ng, and\nmanipulative content, copyright infringement, and other r espon-\nsible AI dimensions.\nIn this tutorial, we ﬁrst highlight key harms associated wit h\ngenerative AI systems, focusing on ungrounded answers (hal luci-\nnations), jailbreaks and prompt injection attacks, harmfu l content,\nand copyright infringement. We then discuss how to eﬀective ly ad-\ndress potential risks and challenges, following the framew ork of\nidentiﬁcation, measurement, mitigation (with four mitiga tion lay-\ners at the model, safety system, application, and positioni ng lev-\nels), and operationalization. We present real-world LLM us e cases,\npractical challenges, best practices, lessons learned fro m deploy-\ning solution approaches in the industry, and key open proble ms.\nOur goal is to stimulate further research on grounding and ev alu-\nating LLMs and enable researchers and practitioners to buil d more\nrobust and trustworthy LLM applications.\nWe ﬁrst present a brief tutorial outline in §1.1, followed by an\nelaborate discussion of diﬀerent responsible AI dimension s in §2.\nWe devote §3 to the problem of grounding for LLM applications ,\nand §4 to the emerging area of “LLM operations”. For each dime n-\nsion (discussed in §2 to §4), we present key business problem s, tech-\nnical solution approaches, and open challenges.\n1.1 Tutorial Overview\nOur tutorial\nconsists of the following parts: 1\nIntroduction and Overview of LLM Applications. We give an\noverview of the generative AI landscape in industry and moti vate\nthe topic of the tutorial with the following questions. What con-\nstitutes generative AI? Why is generative AI an important to pic?\nWhat are key applications of generative AI that are being dep loyed\nacross diﬀerent industry verticals? Why is it crucial to dev elop and\ndeploy generative AI models and applications in a responsib le man-\nner?\nHolistic Evaluation of LLMs. We highlight key challenges that\narise when developing and deploying LLMs and other generati ve\nAI models in enterprise settings, and present an overview of solu-\ntion approaches and open problems. We discuss evaluation di men-\nsions such as truthfulness, safety and alignment, bias and f airness,\nrobustness and security, privacy, model disgorgement and u nlearn-\ning, copyright infringement, calibration and conﬁdence, a nd trans-\nparency and causal interventions.\n1https://sites.google.com/view/llm-evaluation-tutorial\nKDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Ken thapadi, Mehrnoosh Sameki, and Ankur Taly\nGrounding for LLMs. We then provide a deeper discussion of\ngrounding for LLMs, that is, ensuring that every claim in the re-\nsponse generated by an LLM can be attributed to a document in\nthe user-speciﬁed knowledge base. We highlight how groundi ng\ndiﬀers from factuality in the context of LLMs, and present te ch-\nnical solution approaches such as retrieval augmented gene ration,\nconstrained decoding, evaluation, guardrails, and revisi on, and cor-\npus tuning.\nLLM Operations and Observability. We present processes and\nbest practices for addressing grounding and evaluation rel ated chal-\nlenges in real-world LLM application settings. We discuss m echa-\nnisms for managing safety risks and vulnerabilities associ ated with\ndeployed LLM and generative AI applications as well as pract i-\ncal approaches for monitoring the underlying models and sys tems\nwith respect to quality and other responsible AI related met rics. Us-\ning real-world LLM case studies, we highlight practical cha llenges,\nbest practices, lessons learned from deploying solution ap proaches\nin the industry, and key open problems.\n2 Holistic Evaluation of LLMs\nThe overarching goal of evaluation is to determine whether a trained\nLLM is ﬁt for deployment in an enterprise setting . A commonly quoted\nmaxim is that LLMs must ensure helpful, truthful, and harmless\nresponses [6]. While this seems straightforward, each of th ese di-\nmensions has several nuances. For instance, lack of truthfu lness\ncan range from subtle misrepresentations to making blatant false\nstatements (colloquially known as “hallucinations”) [48] . Similarly,\nharmful responses can vary from racially biased responses, to vio-\nlent, hateful, and other inappropriate responses, to respo nses caus-\ning social harm (e.g., instruction on how to cheat in an exami nation\nwithout getting caught). Further, in the context of evaluat ing LLMs,\nit is important to be aware of shortcomings that have been hig h-\nlighted with human and automatic model evaluations and with\ncommonly used datasets for natural language generation [37 ].\nBesides evaluations of response quality, practitioners al so have\nto worry about training data privacy, model stealing, copyr ight vi-\nolations, and security risks such as jailbreaking [137] and prompt\ninjection [121]. In some settings, one may also seek calibra ted con-\nﬁdence scores for responses, interpretability, and robust ness to ad-\nversarial prompts.\nIn the rest of this section, we outline several evaluation di men-\nsions that arise in enterprise deployments. Evaluation of L LMs is\nan important topic and there have been a number of dedicated\nframeworks [33, 66, 83] describing evaluation datasets, me trics,\nand benchmarks for various dimensions. A growing collectio n of\ntools and resources have been proposed across diﬀerent phas es of\nLLM development [71]. Here, we focus on the key business con-\ncerns, leading solution approaches, and open challenges fo r each\nevaluation dimension.\n2.1 Truthfulness\nBusiness problems : How do we ensure that LLM responses are\ninformed, relevant, and trustworthy? How do we detect and re -\ncover from hallucinations?\nSolution approaches: There is extensive work on hallucinations\nin LLMs [48, 54], including, the causes and sources of halluc ina-\ntions [77], and measures for evaluating LLMs based on their v ul-\nnerability to producing hallucinations [94]. A variety of m ethods\nhave been proposed to detect hallucinations, ranging from s am-\npling based approaches [75] to approaches leveraging inter nal states\nof the LLM [104]. There is also early work on detecting and pre -\nventing hallucinations in large vision language models [41 ] and\nother multimodal foundation models [128].\nA number of methods have been proposed to fundamentally re-\nduce hallucinations by tuning models. One line of work invol ves\ntraining or ﬁne-tuning LLMs on highly curated textbook-lik e datasets\n[40, 134]. Another approach involves ﬁne-tuning LLMs on pre f-\nerence data for factuality, i.e., response pairs ranked by f actual-\nity [109]. A fundamental hypothesis here is that LLMs have sy s-\ntematic markers for when they are being untruthful [59, 110] . The\nﬁne-tuning process aims to train LLMs to tap into these marke rs\nand upweight factual responses. Related to this, it has been conjec-\ntured that LLMs internalize diﬀerent “personas” during pre train-\ning, and by training on truthful question-answer pairs, one can\nupweight the “truthful” persona (even on unseen domains) [5 8].\nReducing hallucination on a synthetic task has been explore d as a\nway to reduce hallucination on real-world downstream tasks [57].\nFinally, a recent work shows that ﬁne-tuning LLMs on new info r-\nmation that was not acquired during pretraining can encoura ge\nthe model to hallucinate [38]. Curating ﬁne-tuning sets to a void\nthis issue paves another path to reducing hallucinations.\nWhile truthful responses are table stakes for enterprise de ploy-\nments, we may want to go one step further and ensure that all\nresponses are aligned with a speciﬁc knowledge base (e.g., a set of\nenterprise documents). This is known as grounding. This is a vast\ntopic in itself, and therefore we dedicate §3 entirely to it.\nFinally we emphasize that not all hallucinations are equall y bad.\nFor instance, hallucinations in response to nonsensical pr ompts or\nprompts with false premises (see [115] for examples of quest ions\nwhose premises are factually incorrect and hence ideally ne ed to be\nrebutted) are relatively less concerning than hallucinati ons in re-\nsponse to well-meaning prompts. Furthermore, hallucinati ons in\nhigh stakes verticals like healthcare and life sciences may be far\nmore concerning than hallucinations in other verticals.\nOpen challenges : A key open challenge is detecting hallucina-\ntions in video, speech, and multimodal settings. Another op en chal-\nlenge is getting LLMs to generate citations when they answer from\nparametric knowledge. More speciﬁcally, can the LLM be made\naware of document identiﬁers during pre-training, similar to the\nwork on diﬀerential search indexes [108], so that it can gene rate\nthe appropriate markers as citations for various claims in i ts re-\nsponse? A broader challenge is to leverage ideas and lessons from\nsearch and information retrieval literature [80, 136] to im prove rel-\nevance, trustworthiness, and truthfulness of LLM response s. For\nexample, how can we incorporate valuable information such a s\ndocument authors, document quality, authoritativeness of the do-\nmain, timestamp, and other relevant metadata during pre-tr aining\nand subsequent stages of LLM development?\nGrounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29 , 2024, Barcelona, Spain\n2.2 Safety and Alignment\nBusiness problems: How do we prevent an LLM from generating\ntoxic, violent, oﬀensive, or otherwise unsafe output? How d o we\ndetect such content in cases where prevention fails to work? How\ndo we ensure that the responses from an LLM are aligned with hu -\nman intent even in settings where it is hard for human experts to\nverify such alignment?\nSolution approaches : The problem can be addressed during dif-\nferent stages of the LLM lifecycle. During data collection a nd cura-\ntion, we can apply mechanisms to detect unsafe content and ta ke\nremedial steps, such as excluding or modifying such content . Dur-\ning pretraining and ﬁne-tuning, we can incorporate constra ints or\npenalties to discourage the learning of unsafe sequences. I n the\nreinforcement learning from human feedback (RLHF) stage, w e\ncan include response pairs with preference labels on which o ne\nis more appropriate, and tune the model to “align” its respon ses\nwith the preferences [18]. As part of prompt engineering, we can\ninclude instructions to discourage the LLM from generating unde-\nsirable outputs. Finally, when prevention fails, we can app ly toxic-\nity classiﬁers to detect undesirable outputs (as well as und esirable\ninputs) and ﬂag such instances for appropriate treatment by the\nuser-facing AI applications.\nAnother direction in alignment research is leveraging more pow-\nerful LLMs to detect safety and alignment issues with a weake r\nLLM in a cost-eﬀective and latency-sensitive fashion. The p rob-\nlem can be framed as a constrained optimization problem: giv en\ncost or latency constraints, determine the subset of prompt s and\nresponses to be evaluated using a more powerful LLM (e.g., GP T-\n4). In certain settings, the task to be evaluated could be too hard for\neven human experts (e.g., comparing two diﬀerent summaries of\na very large collection of documents or judging the quality o f hy-\npotheses generated based on a large volume of medical litera ture),\nnecessitating the use of powerful LLMs in a manner that align s\nwith human intent. The converse problem of leveraging less p ow-\nerful LLMs to align more powerful LLMs with human intent has\nalso been explored in alignment research. A related challen ge is\nto ensure that AI systems with superhuman performance (whic h\ncould possibly be smarter than humans) are designed to follo w hu-\nman intent. While current approaches for AI alignment rely o n\nhuman ability to supervise AI (using approaches such as rein force-\nment learning from human feedback), these approaches would not\nbe feasible when AI systems become smarter than humans [13].\nOverall, alignment is an active area of research, with appro aches\nranging from data-eﬃcient alignment [55] to alternatives t o RLHF\n[25] to aligning cross-modal representations [84].\nOpen challenges : There has a been a bunch of recent work on\ngenerating adversarial prompts to bypass existing mechani sms for\nmitigating toxic content generation [119, 137]. A key open c hal-\nlenge is mitigating toxic content generation even under suc h adver-\nsarial prompts. Recent research has shown that LLM based gua rdrail\nmodels could themselves be attacked. For instance, a two-st ep preﬁx-\nbased attack procedure – that operates by (a) constructing a uni-\nversal adversarial preﬁx for the guardrail model, and (b) pr opagat-\ning this preﬁx to the response – has been shown to be eﬀective\nacross multiple threat models, including ones in which the a dver-\nsary has no access to the guardrail model at all [76]. How do we\ndevelop eﬀective LLM based guardrails that are robust to suc h at-\ntacks (and even better, have provable robustness/security guaran-\ntees)? Another challenge lies in balancing reduction of und esirable\noutputs with preservation of the model’s ability towards cr eative\ngeneration. Finally, as LLMs are increasingly deployed as p art of\nopen-ended applications, an important socio-technical ch allenge\nis to investigate the opinions reﬂected by the LLMs, determi ne\nwhether such opinions are aligned with the needs of diﬀerent appli-\ncation settings, and design mechanisms to incorporate pref erences\nand opinions of relevant stakeholders (including those imp acted by\nthe deployment of LLM based applications) [101].\n2.3 Bias and Fairness\nBusiness problems: How do we detect and mitigate bias in foun-\ndation models? How can we apply bias detection and mitigatio n\nthroughout the foundation model lifecycle?\nSolution approaches : There is extensive work on detecting and\nmitigating bias in NLP models [12, 14, 15, 22, 36, 98]. In addi tion\nto known categories of bias observed in predictive ML models ,\nnew types of bias arise in LLMs and other generative AI models ,\ne.g., gender stereotypes, exclusionary norms, undesirabl e biases to-\nwards mentions of disability, religious stereotypes, and s exual ob-\njectiﬁcation [10, 30, 106, 122]. Additionally, due to the sh eer size\nof datasets used, it is diﬃcult to audit and update the traini ng data\nor even anticipate diﬀerent kinds of biases that may be prese nt.\nMitigation approaches include counterfactual data augmen tation\n(or other types of data improvements), ﬁnetuning, incorpor ating\nfairness regularizers, in-context learning, and natural l anguage in-\nstructions. For a longer discussion, we direct the readers t o the\nsurvey by Gallegos et al. [30]. More broadly, we can view bias mea-\nsurement and mitigation as an important component of buildi ng\na reliable and robust application that works well across diﬀ erent\nsubgroups of interest (including but not necessarily limit ed to pro-\ntected groups). By performing ﬁne-grained evaluation and r obust-\nness testing across such groups, we can identify underperfo rming\ngroups, improve the performance for such groups, and thereb y po-\ntentially boost even the overall performance.\nOpen challenges: Bias and fairness mitigation is a relatively nascent\nspace, and a key open question is identifying and designing p rac-\ntical, scalable processes from the large class of bias measu rement\nand mitigation techniques proposed for LLMs. A related chal lenge\nis ensuring that the bias mitigation approach does not cause the\nmodel to inadvertently demonstrate disparate treatment, w hich\ncould be considered unlawful in a wide range of scenarios und er\nUS law [70]. Further, how do we audit LLMs and other genera-\ntive AI models for diﬀerent types of implicit or subtle biase s and\ndesign mechanisms to mitigate or recover from such biases, a l-\nthough the models may not show explicit bias on standard benc h-\nmarks [8, 45, 46]? It has recently been argued that harmful bi ases\nare an inevitable consequence arising from the design of LLM s as\nthey are currently formulated, and that the connection betw een\nbias and fundamental properties of language models needs to be\nKDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Ken thapadi, Mehrnoosh Sameki, and Ankur Taly\nprobed further [96]. How do we revisit the foundational assu mp-\ntions underlying LLMs and approach the development and depl oy-\nment of LLMs with the goal of preventing bias-related harms by\ndesign?\n2.4 Robustness and Security\nBusiness problems : How do we measure and improve the ro-\nbustness of LLMs and other generative AI models and applicat ions\nagainst minor prompt perturbations, natural distribution shifts, and\nother unseen or challenging scenarios? How do we safeguard L LMs\nagainst manipulative eﬀorts by bad actors to (jail-)break a lignment,\nreveal system prompts, and inject malicious instructions i nto prompts\n(also called prompt injection attacks [121])?\nSolution approaches : Many techniques proposed for measuring\nand improving robustness in NLP models can be adopted or ex-\ntended for LLMs. In particular, the following ideas and noti ons\ncould be relevant for LLMs: deﬁnitions, metrics, and assump tions\nregarding robustness (such as label-preserving vs. semant ic-preserving);\nconnections between robustness against adversarial attac ks and ro-\nbustness under distribution shifts; similarities and diﬀe rences in\nrobustness approaches between vision and text domains; mod el-\nbased vs. human-in-the-loop identiﬁcation of robustness f ailures.\nMitigation approaches involve learning invariant represe ntations,\nand ensuring models do not rely on spurious patterns using te ch-\nniques like data augmentation, reweighting, ensembling, i nductive-\nprior design, and causal intervention [117]. Open-source e valua-\ntion frameworks and benchmarks such as Stanford HELM [66],\nEleuther Harness [33], LangTest [83], and Fiddler Auditor [ 51] can\nbe utilized for benchmarking diﬀerent LLMs and evaluating r obust-\nness in application-speciﬁc settings.\nLLMs have been shown to be vulnerable to adversarial pertur-\nbations in prompts [135], prompt injection attacks [121], d ata poi-\nsoning attacks [116], and universal and transferable adver sarial at-\ntacks on alignment [137]. Several benchmarks have been prop osed\nfor red-teaming / testing LLMs against adversarial attacks and re-\nlated issues [31, 87, 135]. Metrics for quantifying LLM cybe rsecu-\nrity risks, tools to evaluate the frequency of insecure code sugges-\ntions, and tools to evaluate LLMs to make it harder to generat e\nmalicious code or aid in carrying out cyberattacks have also been\nproposed [11]. Additional discussion and approaches can be found\nin survey articles by Barrett et al. [9] and Yao et al. [127].\nOpen challenges : A key challenge is to ensure that robustness\nand security mechanisms are not intentionally or unintenti onally\nremoved in the process of ﬁnetuning an LLM [90]. Another chal -\nlenge lies in ensuring that the mechanisms work not just duri ng\nevaluation but also during deployment (e.g., not subject to decep-\ntive attacks [49]). A broader challenge is to investigate ro bustness,\nsecurity, and safety of systems that could be composed of mul tiple\nLLMs. For example, it has been shown that adversaries can mis -\nuse combinations of models by decomposing a malicious task i nto\nsubtasks, leveraging aligned frontier models to solve hard but be-\nnign subtasks, and leveraging weaker non-aligned models to solve\neasy but malicious subtasks [56]. As such attacks do not requ ire the\naligned frontier models to generate malicious outputs and h ence\ncan go undetected, there is a need to extend red-teaming eﬀor ts\nbeyond single models in isolation.\n2.5 Privacy, Unlearning, and Copyright\nImplications\nBusiness problems: How do we ensure that LLMs, diﬀusion mod-\nels, and other generative AI models do not memorize training data\ninstances (including personally identiﬁable information (PII)) and\nreproduce such data in their responses? How do we detect PII i n\nLLM prompts / responses? How do prevent copyright infringe-\nment by LLMs? How can we make an LLM / generative AI model\nforget speciﬁc parts, facts, or other aspects associated wi th the\ntraining data?\nSolution approaches : Recent studies have shown that training\ndata can be extracted from LLMs [17] and from diﬀusion models\n[16] (which could have copyright implications in case the mo del\nis perceived as a database from which the original images or o ther\ncopyrighted data can be approximately retrieved). Several approaches\nfor watermarking [28, 39, 62] (or otherwise identifying / de tecting\n[81]) AI generated content have been proposed. Detecting PI I in\nLLM prompts / responses can be done using oﬀ-the-shelf packa ges,\nbut may require domain-speciﬁc modiﬁcations since what is c on-\nsidered as PII could vary based on the application. Unlearni ng in\nLLMs [68], and more broadly, model disgorgement [2] (“the el im-\nination of not just the improperly used data, but also the eﬀe cts\nof improperly used data on any component of an ML model”) are\nlikely to become important for copyright and privacy safegu ards,\nensuring responsible usage of intellectual property, comp liance,\nand related requirements as well for reducing bias or toxici ty and\nincreasing ﬁdelity.\nOpen challenges : A key challenge would be designing practical\nand scalable techniques. For example, how can we develop diﬀ eren-\ntially private model training approaches (e.g., DPSGD [1], PATE [85])2\nthat are applicable for billions or trillions of parameters in gener-\native AI models? How can we ensure privacy of end users when\nleveraging inputs from end users as part of retraining of LLM s (us-\ning, say, PATE-like approaches)? Considering the importan ce of\nhigh quality datasets for evaluating LLMs for truthfulness , bias, ro-\nbustness, safety, and related dimensions, and the challeng es with\nobtaining such datasets in highly sensitive domains such as health-\ncare, how do we develop practical and feasible approaches fo r dif-\nferentially private synthetic data generation [7, 69, 107] , poten-\ntially leveraging a combination of sensitive datasets (e.g ., patient\nhealth records and clinical notes) and publicly available d atasets\nalong with the ability to generate data by querying powerful LLMs?\n2.6 Calibration and Conﬁdence\nBusiness problems: How can we deploy LLMs in a human-AI hy-\nbrid setting to quantify the uncertainty (conﬁdence score) associ-\nated with an LLM response and defer to humans when conﬁdence\n2Examples of diﬀerentially private model training include D PSGD [1] and PATE[85].\nWhile DPSGD operates by controlling the inﬂuence of trainin g data during gradi-\nent descent, PATE transfers to a “student” model the knowled ge of an ensemble of\n“teacher” models, with intuitive privacy provided by train ing teachers on disjoint data\nand strong privacy guaranteed by noisy aggregation of teach ers’ answers.\nGrounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29 , 2024, Barcelona, Spain\nis low? Speciﬁcally, how can we achieve this in high-stakes a nd\nlatency-sensitive domains such as AI models used in healthc are\nsettings?\nSolution approaches : Learning to defer in human-AI settings is\nan active area of research [61], necessitating uncertainty quantiﬁ-\ncation and conﬁdence estimation for the underlying AI model s. It\nalso involves understanding the conditions under which hum ans\ncan eﬀectively complement AI models [24]. In the context of L LMs,\nrecent approaches such as selective prediction, self-eval uation and\ncalibration, semantic uncertainty, and self-evaluation- based selec-\ntive prediction have been proposed [20] (see references the re-in).\nOpen challenges: A key challenge is to ensure that self-evaluation,\ncalibration, selective prediction, and other conﬁdence mo deling ap-\nproaches for LLMs are eﬀective in out-of-distribution sett ings. This\nis particularly important for adoption in high-stakes sett ings like\nhealthcare. Another challenge is ensuring robustness of co nﬁdence\nmodeling approaches against adversarial prompts.\n2.7 Transparency and Causal Interventions\nBusiness problems : How do we explain the inner workings and\nresponses of LLMs and other generative AI models, especiall y in\nscenarios requiring the development of end-user trust and m eeting\nregulatory requirements? How can we modify factual associa tions\nlinked to an LLM without retraining it?\nSolution approaches: Explainability methods for LLMs have been\nwell studied [131], including techniques such as Chain-of- Thought\nPrompting [120] and variants. However, there is work on unfa ith-\nful explanations in chain-of-thought prompting [113], wit h con-\nnections to language model alignment through externalized rea-\nsoning (getting models to do as much processing/reasoning t hrough\nnatural language as possible). Mechanistic interpretabil ity [93] is\nanother active area of research, which has the potential to b e fur-\nther accelerated by the availability of small language mode ls like\nphi-2. Causal tracing approaches have been proposed to loca te and\nedit factual associations in LLMs. This involves ﬁrst ident ifying\nneuron activations that are decisive in the model’s factual predic-\ntions, and then modifying these neuron activations to updat e spe-\nciﬁc factual associations [78].\nOpen challenges : Analogous to the use of simpler approximate\nmodels for explaining complex predictive ML models (e.g., L IME),\ncan we employ simpler approximate models to explain LLMs and\nother generative AI models (e.g., using approaches such as m odel\ndistillation) in a faithful manner? Additionally, can we de velop\nmore eﬃcient and practical causal intervention approaches ?\n3 Grounding for LLMs\nBusiness problem : How do we ensure that responses generated\nby an LLM are grounded in a user-speciﬁed knowledge base? Her e,\n“grounding” means that every claim in the response can be att rib-\nuted [92] to a document in the knowledge base. We distinguish\nbetween the terms “grounding” and “factuality”. While “gro und-\ning” seeks attribution to a user-speciﬁc knowledge base, “f actual-\nity” seeks attribution to commonly agreed world knowledge.\nIn the context of “grounding”, the knowledge base may be a set\nof public and/or private documents, one or more Web domains, or\nthe entire Web. For instance, a healthcare company may want i ts\nchatbot to always produce responses that are grounded in a se t of\nhealthcare articles it consider authoritative. In additio n to ground-\ning to the knowledge base, one may also want responses to cont ain\ncitations into the relevant documents in the knowledge base . This\nenables transparency and allows the end-user to corroborat e all\nclaims in the response.\n3.1 Solution Approaches\nIn §2.1, we laid out some key directions for detecting and pre vent-\ning hallucinations in LLM responses. As mentioned earlier, the re-\nquirement of grounding goes a step further from merely preve nt-\ning hallucinations. We seek responses that are fully aligne d with a\ngiven knowledge base. For instance, there may be a well-supp orted,\nnon-hallucinated claim that disagrees with the provided kn owl-\nedge base. Such a claim would still be considered ungrounded . There\nis a vast and growing literature on grounding for LLMs. Below , we\nsketch out the key directions in this space.\nRetrieval Augmented Generation. Grounding failures often oc-\ncur because not all information in the knowledge base is stor ed in\nthe LLM’s parametric memory. One popular approach to circum -\nventing this challenge is Retrieval Augmented Generation (RAG) [52,\n65], which leverages in-context learning to expose the mode l to rel-\nevant information from the knowledge base. Speciﬁcally, gi ven a\nprompt (user question), we retrieve relevant snippets (cal led con-\ntext) from the knowledge base, augment the prompt with this con-\ntext, and then generate a response with the augmented prompt .\nThe success of a RAG system relies on the success of the retrie val\nstep and the generation step. Consequently, RAG systems are eval-\nuated based on dimensions such as context relevance (that is, whether\nthe retrieved context is relevant to the given prompt), answer faith-\nfulness (that is, whether the response generated by the LLM is prop-\nerly grounded in the retrieved context), and answer relevance (that\nis, whether the response is relevant to the user question) [2 7, 100].\nThe retrieval step seeks to eﬃciently retrieval all relevan t infor-\nmation for a given prompt. This typically involves chunking and\nindexing the knowledge base into a vector database, and quer y-\ning it based on the prompt. The tremendous commercial intere st\nin RAG systems has led to a proliferation of enterprise-grad e vec-\ntor databases (e.g., Pinecone [88], FAISS [26]) that enable retrieval\nfrom arbitrary knowledge bases.\nTo sharpen the retrieval step, several recent works have bee n ex-\nploring various aspects of it, including, the appropriate g ranularity\nof retrieval (such as a paragraph or a sentence) [21, 125], st rate-\ngies for decomposing complex prompts into one or more retrie val\nqueries [19, 53, 89], supervising retrieval based on qualit y of down-\nstream generation systems [118], and leveraging LLMs as ret rieval\nindexes [108, 129].\nKDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Ken thapadi, Mehrnoosh Sameki, and Ankur Taly\nThe generation step seeks to ensure that the LLM’s response\nremains grounded in the provided context. This is not a given , es-\npecially when the context conﬂicts with the LLM’s own parame -\nteric knowledge [123]. To combat this, a common strategy is t o\nﬁne-tune the LLM on (prompt, context, response) triples [5, 72]. In\norder to improve robustness, it is important to include a var iety\nof contexts with varying levels of noise in the tuning set [72 ]. To\nfurther incentivize LLMs to respond based on the provided co n-\ntext, recent work [60, 63] proposes additional ﬁne-tuning o n coun-\nterfactual contexts and responses that contain claims that conﬂict\nwith the model’s parameter memory. Besides ﬁne-tuning, it i s also\npossible to use reinforcement learning (RL) to reward groun ded\nresponses [79]. The reward model may be trained on human feed -\nback on grounding, or may use automated models for performin g\nchecks (discussed below).\nAnother challenge for the generation step is comprehending\ncontexts with temporal information. For instance, conside r a con-\ntext specifying health records of a patient and the query: “H ow\ndoes the patient’s blood pressure from this week compare to l ast\nweek?” Producing a grounded response to this prompt require s\nknowing the current week, and identifying the blood pressur es\nfrom the current week and the prior week.\nFinally, it should be noted that no matter how eﬀective the re -\ntrieval system is, there will always be instances of out-of- domain,\nadversarial, or nonsensical prompts where the retrieved co ntext re-\nmains irrelevant. In such cases, it is crucial to train the mo del to\ngenerate an “I don’t know” response by including demonstrat ions\nof such scenarios in the tuning set [29, 130].\nRetrieval augmented generation is a vast area of research, a nd\nthe above description provides only a brief overview. We ref er in-\nterested readers to surveys dedicated to RAG frameworks [35 , 132,\n133].\nConstrained Decoding. Another direction for improving ground-\nedness is to use constrained decoding [82, 105, 112, 126]. He re, the\nkey idea is to modify the decoding strategy to optimize the gr ound-\nedness of decoded responses. A simple version of this is Best-of-N\nsampling, wherein, we sample N diﬀerent responses and selec t the\nones with the largest grounding reward [79, 112]. Other work s like\nFUDGE [126] propose mechanisms for altering next word proba -\nbilities based on the likelihood of the current (partial) se quence\ncompleting into one that satisﬁes a certain attribute. One c an lever-\nage these ideas to optimize for the grounding attribute [112 ]. An-\nother direction is context-aware decoding [103], which upw eights\ntoken probabilities to amplify the diﬀerence between gener ations\nwith and without the provided context. A common caveat for co n-\nstrained decoding approaches is balancing groundedness wi th other\ndesirable attributes like coherence, ﬂuency, and helpfuln ess.\nEvaluation, Guardrails, and Revision. While the above direc-\ntions make great strides towards improving grounding of LLM re-\nsponses, they are not perfect. For instance, multiple recen t evalua-\ntions ﬁnd that models struggle to generate grounded respons es for\nprompts seeking time-varying information (e.g., “Who won t he lat-\nest soccer match between Liverpool and Manchester United?” ) [115],\nand balancing grounding with other response attributes [34 , 67].\nIn light of this, it is important to have inference time guard rails\nfor verifying grounding. There is extensive work on guardra ils\nfor LLM responses, to mitigate unsafe and harmful responses [50,\n74] and copyright violations, and to protect against LLM vul nera-\nbilities like prompt injection and jailbreaking [23, 95]. B elow, we\nspeciﬁcally discuss guardrails for checking grounding of r esponses.\nFor responses generated by RAG frameworks, grounding veri-\nﬁcation is carried out by comparing the response to the conte xt\nretrieved as part of the RAG retrieval step. The most common w ay\nof doing this is to use a natural language inference (NLI) mod el\nto determine if the context entails the response [47]. Longe r re-\nsponses may be broken into individual sentences, and a separ ate\nNLI call may be made for each claim [32]. This also allows iden -\ntifying citations for each claim in the response. The key adv an-\ntage of this approach is that smaller T5-family [91] models c an\nbe trained to perform NLI checks, making this approach attra ctive\nfor inference-time grounding veriﬁcation. While NLI based checks\nare getting rapidly deployed as guardrails for grounding [3 , 4],\nthey often struggle with performing grounding checks that i nvolve\nreasoning. Examples include validating claims making temp oral\nstatements, e.g., “the patient’s latest blood pressure is 1 30/80”, or\nclaims involving negation, e.g., “none of the reviews menti on that\nthe breakfast was bad”, or claims involving quantiﬁers, e.g ., “many\nguests appreciated the free breakfast”.\nAn alternative approach is to use an LLM to perform the ground -\ning checks [5]. This allows leveraging the LLM’s superior wo rld\nknowledge and reasoning abilities in making entailment jud gements.\nIn general, LLM based approaches excel when the response is m ore\nabstract and does not quote directly from the context. Howev er,\nLLM based approaches are more computationally expensive ma k-\ning them less viable as inference time guardrails.\nFinally, there is an emerging line of work on automatically a nd\niteratively revising LLM responses in light of grounding fe edback [32,\n73, 86, 114]. Some approaches consider oﬀ-the-shelf LLMs to per-\nform the revision tasks, while others train smaller, dedica ted revi-\nsion models [114].\nCorpus Tuning. An orthogonal approach to retrieval augmented\ngeneration is to pretrain the LLM on documents from the knowl -\nedge base to allow it to learn representations tailored to th e knowl-\nedge base [44, 124]. This is particularly helpful when the kn owl-\nedge base falls in a niche domain and/or involves novel terms not in\nthe model’s vocabulary; this is commonly the case for medica l and\nhealthcare domains. Such domain-speciﬁc tuning is expecte d to\nbeneﬁt both closed book question-answering [97] as well as R AG\napproaches [44].\n3.2 Open Challenges\nGrounding for LLMs is a rapidly evolving area with several op en\nchallenges. A key practical challenge for RAG frameworks is grap-\npling with imperfect retrieval. For instance, how should th e model\nrespond when the retrieval includes multiple opinions that contra-\ndict with each other, when the retrieval is missing crucial i nfor-\nmation sought by the prompt, or when the retrieval is complet ely\nirrelevant? In some cases, even when the retrieval is missin g infor-\nmation, the model may still have the necessary information i n its\nGrounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29 , 2024, Barcelona, Spain\nparametric memory. How should models balance amongst answe r-\ning from the context versus answering from parametric memor y\nversus not answering at all (punting)?\nA key challenge in tuning LLMs towards generating grounded\nresponses is that the models may optimize for grounding at th e ex-\npense of losing creativity and helpfulness. For instance, t hey may\nquote verbatim from the provided context, which was recentl y ob-\nserved for a number of commercial generative AI search engin es [67].\nFinally, a large open area is extending RAG frameworks to mul ti-\nmodal settings – for instance, settings where the underlyin g knowl-\nedge base may consist of text, images, audio, and video, or th e\nquery may be a combination of text and audio. This is an emergi ng\narea, and we refer interested readers to a recent survey by Zh ao et\nal. [133].\n4 LLM Operations and Observability\nBusiness problems: What processes and mechanisms are impor-\ntant for addressing grounding and evaluation related chall enges in\nreal-world LLM application settings in a holistic manner? H ow can\nwe monitor LLMs and other generative AI applications deploy ed in\nproduction for metrics related to quality, safety, and othe r responsi-\nble AI dimensions? How can we anticipate and manage risks fro m\nfrontier AI systems?\n4.1 Solution Approaches\nThe emerging area of “LLM operations” deals with processes a nd\ntools for designing, developing, and deploying LLMs, as wel l as\nmonitoring LLM applications once they are deployed in produ c-\ntion. Frameworks such as the following have been proposed to\naddress potential harms and challenges pertaining to groun ding,\nrobustness, and evaluation in real-world LLM applications [10, 30,\n64].\n• Identiﬁcation [31, 49, 87]: Recognizing and prioritizing p o-\ntential harms through iterative red-teaming, stress-test ing,\nand thorough analysis of the AI system.\n• Measurement [42, 74, 100, 135]: Establishing clear metrics ,\ncreating measurement test sets, and conducting iterative,\nsystematic testing—both manual and automated—to quan-\ntify the frequency and severity of identiﬁed harms.\n• Mitigation [50, 80, 95, 106]: Implementing tools and strate -\ngies, such as prompt engineering and content ﬁlters, to re-\nduce or eliminate potential harms. Repeated measurements\nneed to be conducted to assess the eﬀectiveness of the imple-\nmented mitigations. We could consider four layers of miti-\ngation at model, safety system, application, and positioni ng\nlevels.\n• Operationalization [43, 111]: Deﬁning and executing a de-\nployment and operational readiness plan to ensure the re-\nsponsible and ethical use of AI systems.\nDepending on the domain requirements, an “AI safety layer” f or\ndetecting toxicity and other undesirable outputs in realti me can be\nincluded between the model and the application. Measuring s hifts\nin the distribution of LLM prompts or responses could be help ful to\nidentify potential degradation of the model quality over ti me, and\nfurther this information can be combined with any user feedb ack\nsignals to determine regions where the model may be underper -\nforming [43].\nFurther, we need to diﬀerentiate undesirable outcomes or fa il-\nures in LLM applications caused by adversarial attacks from fail-\nures due to the LLM’s behavior in an unexpected manner in cert ain\ncontexts. To address the latter class of “unknown unknown” f ail-\nures, we should not only perform extensive testing and red te am-\ning to preemptively identify and mitigate as many potential harms\nas possible but also incorporate processes and mechanisms t o react\nquickly to any unanticipated harms during deployment. As an ex-\nample, Microsoft introduced a new category of harms called “ Dis-\nparaging, Existential, and Argumentative” harms as part of the re-\nsponsible AI evaluation for conversational AI application s in re-\nsponse to the unexpected behavior of the Bing AI chatbot as re -\nported by a New York Times journalist [99].\nMore broadly, the risk proﬁle associated with frontier AI sy s-\ntems is expected to expand in light of extensions of existing LLMs,\ne.g., multimodality, tool use, deeper reasoning and planni ng, larger\nand more capable memory, and increased interaction between AI\nsystems [102, 111]. Of these, tool use is considered to creat e several\nnew risks and vulnerabilities.\n4.2 Open Challenges\nA key challenge is to classify potential risks associated wi th tool\nuse, AI agents, interaction between AI systems, etc., in ter ms of the\nlevel of attention and action needed now and at diﬀerent poin ts in\nthe future. This involves prioritizing investments to addr ess such\nrisks, especially in the following two areas: (1) Identifyi ng failure\nmodes and tendencies of LLM-based applications: We need to p in-\npoint how these applications can be led astray, and (2) Devel op-\ning new safety and monitoring practices: This involves leve raging\nmetrics like weight updates, activations, and robustness s tatistics,\nwhich are not currently available as part of LLM APIs.\n5 Conclusion\nGiven the increasing prevalence of AI technologies in our da ily\nlives, it is crucial to integrate responsible AI methodolog ies into the\ndevelopment and deployment of Large Language Models (LLMs)\nand other Generative AI applications. We must understand th e\npotential harms these models may introduce, and leverage st ate-\nof-the-art techniques for enhancing overall quality, fair ness, ro-\nbustness, and explainability. Addressing the responsible AI related\nharms and challenges not only reduces legal, regulatory, an d rep-\nutational risks, but also safeguards individuals, busines ses, and so-\nciety as a whole. Moreover, there is a pressing need to establ ish\nways to quantitatively assess the performance, quality, an d safety\nof such models. Without comprehensive evaluations, establ ishing\ntrust in LLM-based applications becomes exceedingly diﬃcu lt. The\ngoal of this tutorial is to establish a foundation for the dev elopment\nof safer and more reliable generative AI applications in the future.\nAcknowledgments\nThe authors would like to thank numerous researchers, pract ition-\ners, and industry leaders for insightful discussions which helped\nKDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Ken thapadi, Mehrnoosh Sameki, and Ankur Taly\nshape the business problems, solution approaches, and open chal-\nlenges discussed in this article, and Mark Johnson and Qinla n Shen\nfor thoughtful feedback.\nReferences\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMaha n, Ilya Mironov,\nKunal Talwar, and Li Zhang. 2016. Deep learning with diﬀeren tial privacy. In\nCCS.\n[2] Alessandro Achille, Michael Kearns, Carson Klingenber g, and Stefano Soatto.\n2024. AI model disgorgement: Methods and choices. Proceedings of the National\nAcademy of Sciences 121, 18 (2024).\n[3] Google Cloud AI. 2024. Check grounding | Vertex AI Agent B uilder.\nhttps://cloud.google.com/generative-ai-app-builder/ docs/check-grounding\n[4] Microsoft Azure AI. 2024. Groundedness detection.\nhttps://learn.microsoft.com/en-us/azure/ai-services /content-safety/quickstart-groundedness\n[5] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hanna neh Hajishirzi.\n2024. Self-RAG: Learning to Retrieve, Generate, and Critiq ue through Self-\nReﬂection. In The Twelfth International Conference on Learning Represen tations.\n[6] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ga nguli, Tom\nHenighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSa rma, et al. 2021.\nA general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861 (2021).\n[7] Sergul Aydore, William Brown, Michael Kearns, Krishnar am Kenthapadi, Luca\nMelis, Aaron Roth, and Ankit A Siva. 2021. Diﬀerentially pri vate query release\nthrough adaptive projection. In International Conference on Machine Learning .\nPMLR, 457–467.\n[8] Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Tho mas L Griﬃths. 2024.\nMeasuring implicit bias in explicitly unbiased large langu age models. arXiv\npreprint arXiv:2402.04105 (2024).\n[9] Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carl ini, Brad Chen, Jihye\nChoi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam D atta, Soheil\nFeizi, et al. 2023. Identifying and mitigating the security risks of generative AI.\nFoundations and Trends® in Privacy and Security 6, 1 (2023), 1–52.\n[10] Emily M Bender, Timnit Gebru, Angelina McMillan-Major , and Margaret\nMitchell. 2021. On the Dangers of Stochastic Parrots: Can La nguage Models\nBe Too Big?. In ACM Conference on Fairness, Accountability, and Transpare ncy.\n[11] Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis , Shengye Wan, Ivan\nEvtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Corneliu s Aschermann,\nLorenzo Fontana, et al. 2023. Purple Llama CyberSecEval: A s ecure coding\nbenchmark for language models. arXiv preprint arXiv:2312.04724 (2023).\n[12] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and\nAdam T Kalai. 2016. Man is to computer programmer as woman is t o home-\nmaker? Debiasing word embeddings. In NeurIPS.\n[13] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bo wen Baker, Leo Gao,\nLeopold Aschenbrenner, Yining Chen, Adrien Ecoﬀet, Manas J oglekar, Jan\nLeike, et al. 2023. Weak-to-strong generalization: Elicit ing strong capabilities\nwith weak supervision. arXiv preprint arXiv:2312.09390 (2023).\n[14] Aylin Caliskan. 2021. Detecting and mitigating bias in natural language pro-\ncessing. Brookings Institution (2021).\n[15] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics de-\nrived automatically from language corpora contain human-l ike biases. Science\n356, 6334 (2017).\n[16] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jag ielski, Vikash Sehwag,\nFlorian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wall ace. 2023. Extracting\ntraining data from diﬀusion models. arXiv preprint arXiv:2301.13188 (2023).\n[17] Nicholas Carlini, Florian Tramer, Eric Wallace, Matth ew Jagielski, Ariel\nHerbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, Alina Oprea, and Colin Raﬀel. 2021. Extracting Training Data from\nLarge Language Models. In USENIX Security Symposium , Vol. 6.\n[18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Kre ndl Gilbert, Jérémy\nScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, Da vid Lindner, Pe-\ndro Freire, et al. 2023. Open problems and fundamental limit ations of reinforce-\nment learning from human feedback. arXiv preprint arXiv:2307.15217 (2023).\n[19] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei X ue, Yike Guo,\nand Jie Fu. 2024. RQ-RAG: Learning to Reﬁne Queries for Retri eval Augmented\nGeneration. arXiv:2404.00610\n[20] Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Ari k, Tomas Pﬁster, and\nSomesh Jha. 2023. Adaptation with Self-Evaluation to Impro ve Selective Pre-\ndiction in LLMs. In Findings of the Association for Computational Linguistics :\nEMNLP 2023 .\n[21] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin M a, Xinran Zhao,\nHongming Zhang, and Dong Yu. 2023. Dense X Retrieval: What Re trieval Gran-\nularity Should We Use? arXiv:2312.06648\n[22] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jenn ifer Chayes, Chris-\ntian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnar am Kenthapadi,\nand Adam Tauman Kalai. 2019. Bias in bios: A case study of sema ntic rep-\nresentation bias in a high-stakes setting. In FAccT.\n[23] Leon Derczynski, Erick Galinkin, Jeﬀrey Martin, Subho Majumdar, and Nanna\nInie. 2024. garak: A Framework for Security Probing Large La nguage Models.\nhttps://garak.ai. (2024).\n[24] Kate Donahue, Alexandra Chouldechova, and Krishnaram Kenthapadi. 2022.\nHuman-algorithm collaboration: Achieving complementari ty and avoiding un-\nfairness. In Proceedings of the 2022 ACM Conference on Fairness, Accounta bility,\nand Transparency.\n[25] Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev.\n2023. SteerLM: Attribute Conditioned SFT as an (User-Steer able) Alternative\nto RLHF. In Findings of the Association for Computational Linguistics : EMNLP\n2023. 11275–11288.\n[26] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeﬀ Joh nson, Gergely Szil-\nvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hossein i, and Hervé Jé-\ngou. 2024. The Faiss library. (2024). arXiv:cs.LG/2401.08 281\n[27] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RA-\nGAs: Automated Evaluation of Retrieval Augmented Generati on. In Proceedings\nof the 18th Conference of the European Chapter of the Associa tion for Computa-\ntional Linguistics: System Demonstrations . 150–158.\n[28] Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahlouj ifar, Mohammad Mah-\nmoody, and Mingyuan Wang. 2023. Publicly detectable waterm arking for lan-\nguage models. arXiv preprint arXiv:2310.18491 (2023).\n[29] Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vid hisha Balachandran,\nand Yulia Tsvetkov. 2024. Don’t Hallucinate, Abstain: Iden tifying LLM Knowl-\nedge Gaps via Multi-LLM Collaboration. arXiv:2402.00367\n[30] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Ta njim, Sungchul Kim,\nFranck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahme d. 2024. Bias\nand fairness in large language models: A survey. Computational Linguistics\n(2024), 1–79.\n[31] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda As kell, Yuntao Bai,\nSaurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,\net al. 2022. Red teaming language models to reduce harms: Met hods, scaling\nbehaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).\n[32] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, A run Tejasvi Cha-\nganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Ch eng Juan, and\nKelvin Guu. 2023. RARR: Researching and Revising What Langu age Models\nSay, Using Language Models. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics . 16477–16508.\n[33] L Gao, J Tow, B Abbasi, S Biderman, S Black, A DiPoﬁ, C Fost er, L Golding,\nJ Hsu, A Le Noac’h, et al. 2023. A framework for few-shot langu age model\nevaluation. Zenodo (2023).\n[34] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023 . Enabling Large\nLanguage Models to Generate Text with Citations. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Process ing. 6465–6488.\n[35] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,\nJiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Aug mented Gen-\neration for Large Language Models: A Survey. arXiv:2312.10 997\n[36] Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and Jame s Zou. 2018. Word\nembeddings quantify 100 years of gender and ethnic stereoty pes. Proceedings\nof the National Academy of Sciences 115, 16 (2018), E3635–E3644.\n[37] Sebastian Gehrmann, Elizabeth Clark, and Thibault Sel lam. 2023. Repairing the\ncracked foundation: A survey of obstacles in evaluation pra ctices for generated\ntext. Journal of Artiﬁcial Intelligence Research 77 (2023), 103–166.\n[38] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart,\nand Jonathan Herzig. 2024. Does Fine-Tuning LLMs on New Know ledge En-\ncourage Hallucinations? arXiv:2405.05904\n[39] Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Ch ang, and Cho-Jui\nHsieh. 2022. Watermarking pre-trained language models wit h backdooring.\narXiv preprint arXiv:2210.07543 (2022).\n[40] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teo doro Mendes, Allie\nDel Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauﬀma nn, Gustavo de\nRosa, Olli Saarikivi, et al. 2023. Textbooks are all you need . arXiv preprint\narXiv:2306.11644 (2023).\n[41] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detectin g and preventing hal-\nlucinations in large vision language models. In AAAI.\n[42] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi , Linhao Yu, Yan Liu,\nJiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluatin g large language\nmodels: A comprehensive survey. arXiv preprint arXiv:2310.19736 (2023).\n[43] Gyandev Gupta, Bashir Rastegarpanah, Amalendu Iyer, J oshua Rubin, and Kr-\nishnaram Kenthapadi. 2023. Measuring Distributional Shif ts in Text: The Ad-\nvantage of Language Model-Based Embeddings. arXiv preprint arXiv:2312.02337\n(2023).\n[44] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta , Kyle Lo, Iz Beltagy,\nDoug Downey, and Noah A. Smith. 2020. Don’t Stop Pretraining : Adapt Lan-\nguage Models to Domains and Tasks. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics .\n[45] Amit Haim, Alejandro Salinas, and Julian Nyarko. 2024. What’s in a Name?\nAuditing Large Language Models for Race and Gender Bias. arXiv preprint\nGrounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29 , 2024, Barcelona, Spain\narXiv:2402.14875 (2024).\n[46] Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky , and Sharese King. 2024.\nDialect prejudice predicts AI decisions about people’s cha racter, employability,\nand criminality. arXiv preprint arXiv:2403.00742 (2024).\n[47] Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Tait elbaum, Doron Kuk-\nliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinat an Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating Factual Consisten cy Evaluation. In\nProceedings of the 2022 Conference of the North American Chap ter of the Associa-\ntion for Computational Linguistics: Human Language Techno logies. 3905–3920.\n[48] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhang yin Feng, Haotian\nWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin , et al. 2023. A\nsurvey on hallucination in large language models: Principl es, taxonomy, chal-\nlenges, and open questions. arXiv preprint arXiv:2311.05232 (2023).\n[49] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte\nMacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, N ewton Cheng,\net al. 2024. Sleeper agents: Training deceptive LLMs that pe rsist through safety\ntraining. arXiv preprint arXiv:2401.05566 (2024).\n[50] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Run gta, Krithika Iyer, Yun-\ning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Tes tuggine, et al.\n2023. Llama Guard: LLM-based input-output safeguard for hu man-AI conver-\nsations. arXiv preprint arXiv:2312.06674 (2023).\n[51] Amal Iyer and Krishnaram Kenthapadi. 2023. Introducin g Fiddler Auditor: Eval-\nuate the Robustness of LLMs and NLP Models. Fiddler AI Blog.\n[52] Gautier Izacard and Edouard Grave. 2021. Leveraging Pa ssage Retrieval with\nGenerative Models for Open Domain Question Answering. In Proceedings of the\n16th Conference of the European Chapter of the Association f or Computational\nLinguistics: Main Volume .\n[53] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park.\n2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmente d Large Language\nModels through Question Complexity. arXiv:2403.14403\n[54] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey o f hallucination\nin natural language generation. Comput. Surveys 55, 12 (2023), 1–38.\n[55] Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Pa dmakumar, Sungjin\nLee, Yang Liu, and Mahdi Namazifar. 2023. Data-Eﬃcient Alig nment of\nLarge Language Models with Human Feedback Through Natural L anguage. In\nNeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.\n[56] Erik Jones, Anca Dragan, and Jacob Steinhardt. 2024. Ad versaries Can Misuse\nCombinations of Safe Models. arXiv preprint arXiv:2406.14595 (2024).\n[57] Erik Jones, Hamid Palangi, Clarisse Simões Ribeiro, Va run Chandrasekaran,\nSubhabrata Mukherjee, Arindam Mitra, Ahmed Hassan Awadall ah, and Ece Ka-\nmar. 2024. Teaching Language Models to Hallucinate Less wit h Synthetic Tasks.\nIn The Twelfth International Conference on Learning Represen tations (ICLR).\n[58] Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, and He He. 2023.\nPersonas as a way to model truthfulness in language models. arXiv preprint\narXiv:2310.18168 (2023).\n[59] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henig han, Dawn Drain,\nEthan Perez, Nicholas Schiefer, Zac Hatﬁeld-Dodds, Nova Da sSarma, Eli Tran-\nJohnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelso n Elhage, Tris-\ntan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Gan-\nguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Sha una Kravec, Liane\nLovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom\nBrown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlis h, Chris Olah,\nand Jared Kaplan. 2022. Language Models (Mostly) Know What T hey Know.\narXiv:2207.05221\n[60] Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 202 0. Learning The Dif-\nference That Makes A Diﬀerence With Counterfactually-Augm ented Data. In\nInternational Conference on Learning Representations .\n[61] Vijay Keswani, Matthew Lease, and Krishnaram Kenthapa di. 2021. Towards\nUnbiased and Accurate Deferral to Multiple Experts. In Proceedings of the 2021\nAAAI/ACM Conference on AI, Ethics, and Society .\n[62] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and\nTom Goldstein. 2023. A watermark for large language models. In International\nConference on Machine Learning . PMLR, 17061–17084.\n[63] Abdullatif Köksal, Renat Aksitov, and Chung-Ching Cha ng. 2023. Hallucina-\ntion Augmented Recitations for Language Models. arXiv:231 1.07424\n[64] Microsoft Learn. 2023. Overview of Responsible AI prac tices for Azure OpenAI\nmodels. Azure AI Services Documentation.\n[65] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio P etroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen- tau Yih, Tim\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retr ieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural Infor-\nmation Processing Systems . 9459–9474.\n[66] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipr as, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ana nya Ku-\nmar, et al. 2022. Holistic evaluation of language models. arXiv preprint\narXiv:2211.09110 (2022).\n[67] Nelson F. Liu, Tianyi Zhang, and Percy Liang. 2023. Eval uating Veriﬁability\nin Generative Search Engines. In The 2023 Conference on Empirical Methods in\nNatural Language Processing .\n[68] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, N athalie Baracaldo, Pe-\nter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, e t al. 2024.\nRethinking Machine Unlearning for Large Language Models. arXiv preprint\narXiv:2402.08787 (2024).\n[69] Terrance Liu, Giuseppe Vietri, and Steven Z Wu. 2021. It erative methods for\nprivate synthetic data: Unifying framework and new methods . Advances in\nNeural Information Processing Systems 34 (2021), 690–702.\n[70] Michael Lohaus, Matthäus Kleindessner, Krishnaram Ke nthapadi, Francesco\nLocatello, and Chris Russell. 2022. Are two heads the same as one? Identifying\ndisparate treatment in fair neural networks. Advances in Neural Information\nProcessing Systems 35 (2022), 16548–16562.\n[71] Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel Mc-\nDuﬀ, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, et al.\n2024. The Responsible Foundation Model Development Cheats heet: A Review\nof Tools & Resources. arXiv preprint arXiv:2406.16746 (2024).\n[72] Hongyin Luo, Tianhua Zhang, Yung-Sung Chuang, Yuan Gon g, Yoon Kim,\nXixin Wu, Helen M. Meng, and James R. Glass. 2023. Search Augm ented In-\nstruction Learning. In The 2023 Conference on Empirical Methods in Natural\nLanguage Processing .\n[73] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Halli nan, Luyu Gao,\nSarah Wiegreﬀe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder, Katherine He rmann, Sean\nWelleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Re ﬁne: Iterative Re-\nﬁnement with Self-Feedback. Advances in Neural Information Processing Sys-\ntems (2023).\n[74] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sulliv an, Chad Atalla,\nEmily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman L utz, et al. 2023.\nA Framework for Automated Measurement of Responsible AI Har ms in Gener-\native AI Applications. arXiv preprint arXiv:2310.17750 (2023).\n[75] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. S elfCheckGPT: Zero-\nResource Black-Box Hallucination Detection for Generativ e Large Language\nModels. In Proceedings of the 2023 Conference on Empirical Methods in Na tural\nLanguage Processing . 9004–9017.\n[76] Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chan drashekaran, Kassem\nFawaz, Somesh Jha, and Atul Prakash. 2024. PRP: Propagating Universal\nPerturbations to Attack Large Language Model Guard-Rails. arXiv preprint\narXiv:2402.15911 (2024).\n[77] Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Hossein i, Mark Johnson,\nand Mark Steedman. 2023. Sources of Hallucination by Large L anguage Models\non Inference Tasks. In Findings of the Association for Computational Linguistics :\nEMNLP 2023 . 2758–2774.\n[78] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belin kov. 2022. Locat-\ning and editing factual associations in GPT. Advances in Neural Information\nProcessing Systems 35 (2022), 17359–17372.\n[79] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Asl anides, Francis Song,\nMartin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell -Gillingham,\nGeoﬀrey Irving, and Nat McAleese. 2022. Teaching language m odels to support\nanswers with veriﬁed quotes. arXiv:2203.11147\n[80] Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 202 1. Rethinking search:\nMaking domain experts out of dilettantes. In ACM SIGIR Forum , Vol. 55. 1–27.\n[81] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Chris topher D Manning, and\nChelsea Finn. 2023. DetectGPT: Zero-shot machine-generat ed text detection\nusing probability curvature. In International Conference on Machine Learning .\nPMLR, 24950–24962.\n[82] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang L i, Tao Wang, Yanping\nHuang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trev or Strohman,\nJilin Chen, Alex Beutel, and Ahmad Beirami. 2024. Controlle d Decoding from\nLanguage Models. arXiv:2310.17022\n[83] Arshaan Nazir, Thadaka Kalyan Chakravarthy, David Amo re Cecchini, Rakshit\nKhajuria, Prikshit Sharma, Ali Tarik Mirik, Veysel Kocaman , and David Talby.\n2024. LangTest: A comprehensive evaluation library for cus tom LLM and NLP\nmodels. Software Impacts (2024), 100619.\n[84] Artemis Panagopoulou, Le Xue, Ning Yu, Junnan Li, Dongx u Li, Shaﬁq Joty,\nRan Xu, Silvio Savarese, Caiming Xiong, and Juan Carlos Nieb les. 2023. X-\nInstructBLIP: A Framework for aligning X-Modal instructio n-aware repre-\nsentations to LLMs and Emergent Cross-modal Reasoning. arXiv preprint\narXiv:2311.18799 (2023).\n[85] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Ra ghunathan, Kunal Tal-\nwar, and Ulfar Erlingsson. 2018. Scalable Private Learning with PATE. In Inter-\nnational Conference on Learning Representations .\n[86] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yu jia Xie, Yu Hu, Qi-\nuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng G ao. 2023.\nCheck Your Facts and Try Again: Improving Large Language Mod els with Ex-\nternal Knowledge and Automated Feedback. arXiv:2302.1281 3\nKDD ’24, August 25–29, 2024, Barcelona, Spain Krishnaram Ken thapadi, Mehrnoosh Sameki, and Ankur Taly\n[87] Ethan Perez, Saﬀron Huang, Francis Song, Trevor Cai, Ro man Ring, John\nAslanides, Amelia Glaese, Nat McAleese, and Geoﬀrey Irving . 2022. Red Team-\ning Language Models with Language Models. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language Processing . 3419–3448.\n[88] Pinecone. [n. d.]. Pinecone Vector Database. http://p inecone.io\n[89] Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, and\nLifu Huang. 2023. The Art of SOCRATIC QUESTIONING: Recursiv e Thinking\nwith Large Language Models. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing . 4177–4199.\n[90] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia , Prateek Mittal, and\nPeter Henderson. 2024. Fine-tuning Aligned Language Model s Compromises\nSafety, Even When Users Do Not Intend To!. In The Twelfth International Con-\nference on Learning Representations .\n[91] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee , Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex ploring the\nLimits of Transfer Learning with a Uniﬁed Text-to-Text Tran sformer. J. Mach.\nLearn. Res. 21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20 -074.html\n[92] Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Ar oyo, Michael\nCollins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iul ia Turc, and David\nReitter. 2023. Measuring Attribution in Natural Language G eneration Models.\nComputational Linguistics 49, 4 (2023), 777–840.\n[93] Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadﬁ eld-Menell. 2023.\nToward transparent AI: A survey on interpreting the inner st ructures of deep\nneural networks. In 2023 IEEE Conference on Secure and Trustworthy Machine\nLearning (SaTML). IEEE, 464–483.\n[94] Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anu bhav Sarkar, SM Ton-\nmoy, Aman Chadha, Amit P Sheth, and Amitava Das. 2023. The Tro ubling\nEmergence of Hallucination in Large Language Models-An Ext ensive Deﬁni-\ntion, Quantiﬁcation, and Prescriptive Remediations. In Proceedings of the 2023\nConference on Empirical Methods in Natural Language Process ing. 2541–2573.\n[95] Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedha r, Christopher\nParisien, and Jonathan Cohen. 2023. NeMo Guardrails: A Tool kit for Control-\nlable and Safe LLM Applications with Programmable Rails. In Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Pro cessing: System\nDemonstrations.\n[96] Philip Resnik. 2024. Large Language Models are Biased B ecause They Are Large\nLanguage Models. arXiv preprint arXiv:2406.13138 (2024).\n[97] Adam Roberts, Colin Raﬀel, and Noam Shazeer. 2020. How M uch Knowledge\nCan You Pack Into the Parameters of a Language Model?. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Pro cessing (EMNLP).\n[98] Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jenn ifer Chayes, Chris-\ntian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnar am Kenthapadi,\nAnna Rumshisky, and Adam Kalai. 2019. What’s in a Name? Reduc ing Bias\nin Bios without Access to Protected Attributes. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long and S hort Papers). 4187–\n4195.\n[99] Kevin Roose. 2023. A Conversation With Bing’s Chatbot L eft Me Deeply Un-\nsettled. New York Times (2023).\n[100] Jon Saad-Falcon, Omar Khattab, Christopher Potts, an d Matei Zaharia. 2024.\nARES: An Automated Evaluation Framework for Retrieval-Aug mented Gener-\nation Systems. In Proceedings of the 2024 Conference of the North American Chap -\nter of the Association for Computational Linguistics: Huma n Language Technolo-\ngies (Volume 1: Long Papers) . 338–354.\n[101] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and\nTatsunori Hashimoto. 2023. Whose opinions do language mode ls reﬂect?. In\nInternational Conference on Machine Learning . PMLR, 29971–30004.\n[102] Toby Shevlane, Sebastian Farquhar, Ben Garﬁnkel, Mar y Phuong, Jess Whit-\ntlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Ma rkus Anderljung,\nNoam Kolt, et al. 2023. Model evaluation for extreme risks. arXiv preprint\narXiv:2305.15324 (2023).\n[103] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetko v, Luke Zettlemoyer,\nand Wen-tau Yih. 2024. Trusting Your Evidence: Hallucinate Less with Context-\naware Decoding. In Proceedings of the 2024 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguisti cs: Human Language\nTechnologies (Volume 2: Short Papers) , Kevin Duh, Helena Gomez, and Steven\nBethard (Eds.). Association for Computational Linguistic s, Mexico City, Mex-\nico, 783–791. https://aclanthology.org/2024.naacl-sho rt.69\n[104] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar . 2023. On Early\nDetection of Hallucinations in Factual Question Answering . arXiv preprint\narXiv:2312.14183 (2023).\n[105] Nisan Stiennon, Long Ouyang, Jeﬀrey Wu, Daniel Ziegle r, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 202 0. Learning to\nsummarize with human feedback. In Advances in Neural Information Processing\nSystems. 3008–3021.\n[106] Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus , Nicholas Joseph,\nShauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Gangul i. 2023. Evaluat-\ning and mitigating discrimination in language model decisi ons. arXiv preprint\narXiv:2312.03689 (2023).\n[107] Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machana vajjhala, and\nGerome Miklau. 2021. Benchmarking diﬀerentially private s ynthetic data gen-\neration algorithms. arXiv preprint arXiv:2112.09238 (2021).\n[108] Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta,\nZhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and\nDonald Metzler. 2022. Transformer Memory as a Diﬀerentiabl e Search Index.\nIn Advances in Neural Information Processing Systems .\n[109] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christoph er D Manning, and\nChelsea Finn. 2024. Fine-Tuning Language Models for Factua lity. In The Twelfth\nInternational Conference on Learning Representations .\n[110] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sha rma, Rafael Rafailov,\nHuaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Ju st Ask for Cal-\nibration: Strategies for Eliciting Calibrated Conﬁdence S cores from Language\nModels Fine-Tuned with Human Feedback. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Processing . 5433–5442.\n[111] Helen Toner, Jessica Ji, John Bansemer, Lucy Lim, Chri s Painter, Courtney Cor-\nley, Jess Whittlestone, Matt Botvinick, Mikel Rodriguez, a nd Ram Shankar Siva\nKumar. 2023. Skating to Where the Puck is Going: Anticipatin g and Managing\nRisks from Frontier AI Systems. Report from the July 2023 Rou ndtable hosted\nby the Center for Security and Emerging Technology (CSET) at Georgetown\nUniversity and Google DeepMind.\n[112] Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Cai ming Xiong, and\nYingbo Zhou. 2023. Unlocking Anticipatory Text Generation : A Constrained\nApproach for Faithful Decoding with Large Language Models. arXiv preprint\narXiv:2312.06149 (2023).\n[113] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Lan-\nguage models don’t always say what they think: Unfaithful ex planations in\nchain-of-thought prompting. Advances in Neural Information Processing Sys-\ntems 36 (2023).\n[114] Giorgos Vernikos, Arthur Brazinskas, Jakub Adamek, J onathan Mallinson, Ali-\naksei Severyn, and Eric Malmi. 2024. Small Language Models I mprove Giants\nby Rewriting Their Outputs. In Proceedings of the 18th Conference of the Euro-\npean Chapter of the Association for Computational Linguist ics (Volume 1: Long\nPapers). 2703–2718.\n[115] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W ei, Jason Wei, Chris\nTar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 20 23. Fresh-\nLLMs: Refreshing Large Language Models with Search Engine A ugmentation.\narXiv:2310.03214\n[116] Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 20 21. Concealed Data\nPoisoning Attacks on NLP Models. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computationa l Linguistics: Hu-\nman Language Technologies . 139–150.\n[117] Xuezhi Wang, Haohan Wang, and Diyi Yang. 2022. Measure and Improve Ro-\nbustness in NLP Models: A Survey. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computationa l Linguistics: Hu-\nman Language Technologies . 4569–4586.\n[118] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parv ez, and Graham\nNeubig. 2023. Learning to Filter Context for Retrieval-Aug mented Generation.\narXiv:2311.08377\n[119] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How\nDoes LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Infor-\nmation Processing Systems .\n[120] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosm a, Fei Xia, Ed Chi,\nQuoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural Information Processing\nSystems 35 (2022), 24824–24837.\n[121] Simon Willison. 2022. Prompt injection attacks again st GPT-3. Simon Willison’s\nWeblog (2022).\n[122] Robert Wolfe, Yiwei Yang, Bill Howe, and Aylin Caliska n. 2023. Contrastive\nlanguage-vision AI models pretrained on web-scraped multi modal data exhibit\nsexual objectiﬁcation bias. In Proceedings of the 2023 ACM Conference on Fair-\nness, Accountability, and Transparency . 1174–1185.\n[123] Kevin Wu, Eric Wu, and James Zou. 2024. ClashEval: Quan tifying the tug-of-\nwar between an LLM’s internal prior and external evidence. a rXiv:2404.10198\n[124] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Pe rcy Liang. 2023. Data\nSelection for Language Models via Importance Resampling. I n Thirty-seventh\nConference on Neural Information Processing Systems .\n[125] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Z hu, Zihan Liu,\nSandeep Subramanian, Evelina Bakhturina, Mohammad Shoeyb i, and Bryan\nCatanzaro. 2024. Retrieval meets Long Context Large Langua ge Models. In The\nTwelfth International Conference on Learning Representat ions.\n[126] Kevin Yang and Dan Klein. 2021. FUDGE: Controlled Text Generation With\nFuture Discriminators. In Proceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational Linguist ics: Human Language\nTechnologies.\n[127] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo S un, and Yue Zhang.\n2024. A survey on large language model (LLM) security and pri vacy: The good,\nGrounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey) KDD ’24, August 25–29 , 2024, Barcelona, Spain\nthe bad, and the ugly. High-Conﬁdence Computing (2024), 100211.\n[128] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yun-\nhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker : Hallu-\ncination correction for multimodal large language models. arXiv preprint\narXiv:2310.16045 (2023).\n[129] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxu an Ju, Soumya\nSanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. G enerate rather\nthan Retrieve: Large Language Models are Strong Context Gen erators. In The\nEleventh International Conference on Learning Representa tions.\n[130] Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lia n, Xingyao Wang,\nYangyi Chen, Heng Ji, and Tong Zhang. 2024. R-Tuning: Instru cting Large\nLanguage Models to Say ‘I Don’t Know’. In Proceedings of the 2024 Conference\nof the North American Chapter of the Association for Computa tional Linguistics:\nHuman Language Technologies (Volume 1: Long Papers) . 7106–7132.\n[131] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,\nShuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainab ility for large\nlanguage models: A survey. ACM Transactions on Intelligent Systems and Tech-\nnology 15, 2 (2024), 1–38.\n[132] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui . 2024. Retrieval-\nAugmented Generation for AI-Generated Content: A Survey. a rXiv:2402.19473\n[133] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Cheng-\nwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, a nd Shaﬁq Joty.\n2023. Retrieving Multimodal Information for Augmented Gen eration: A Sur-\nvey. In Findings of the Association for Computational Linguistics : EMNLP 2023 .\n4736–4756.\n[134] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe\nMa, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mi ke Lewis, Luke\nZettlemoyer, and Omer Levy. 2023. LIMA: Less Is More for Alig nment. In\nThirty-seventh Conference on Neural Information Processin g Systems.\n[135] Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xi ng Xie. 2023. Prompt-\nBench: A uniﬁed library for evaluation of large language mod els. arXiv preprint\narXiv:2312.07910 (2023).\n[136] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, W enhan Liu, Chen-\nlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large langua ge models for\ninformation retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).\n[137] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikso n. 2023. Universal\nand transferable adversarial attacks on aligned language m odels. arXiv preprint\narXiv:2307.15043 (2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6750028729438782
    },
    {
      "name": "Ground",
      "score": 0.512131929397583
    },
    {
      "name": "Data science",
      "score": 0.3354915976524353
    },
    {
      "name": "Systems engineering",
      "score": 0.32807666063308716
    },
    {
      "name": "Engineering",
      "score": 0.20697137713432312
    },
    {
      "name": "Electrical engineering",
      "score": 0.09268707036972046
    }
  ]
}