{
  "title": "Application of an emotional classification model in e-commerce text based on an improved transformer model",
  "url": "https://openalex.org/W3135636901",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100451339",
      "name": "Xuyang Wang",
      "affiliations": [
        "Lanzhou University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5091606553",
      "name": "Yixuan Tong",
      "affiliations": [
        "Lanzhou University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2377040858",
    "https://openalex.org/W2842138566",
    "https://openalex.org/W2544339223",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2001892351"
  ],
  "abstract": "With the rapid development of the mobile internet, people are becoming more dependent on the internet to express their comments on products or stores; meanwhile, text sentiment classification of these comments has become a research hotspot. In existing methods, it is fairly popular to apply a deep learning method to the text classification task. Aiming at solving information loss, weak context and other problems, this paper makes an improvement based on the transformer model to reduce the difficulty of model training and training time cost and achieve higher overall model recall and accuracy in text sentiment classification. The transformer model replaces the traditional convolutional neural network (CNN) and the recurrent neural network (RNN) and is fully based on the attention mechanism; therefore, the transformer model effectively improves the training speed and reduces training difficulty. This paper selects e-commerce reviews as research objects and applies deep learning theory. First, the text is preprocessed by word vectorization. Then the IN standardized method and the GELUs activation function are applied based on the original model to analyze the emotional tendencies of online users towards stores or products. The experimental results show that our method improves by 9.71%, 6.05%, 5.58% and 5.12% in terms of recall and approaches the peak level of the F1 value in the test model by comparing BiLSTM, Naive Bayesian Model, the serial BiLSTM_CNN model and BiLSTM with an attention mechanism model. Therefore, this finding proves that our method can be used to improve the text sentiment classification accuracy and effectively apply the method to text classification.",
  "full_text": "RESEA RCH ARTICL E\nApplication of an emotional classification\nmodel in e-commerce text based on an\nimproved transformer model\nXuyang Wang\n☯\n, Yixuan Tong\nID\n☯\n*\nSchool of Computer and Commun ication of the Lanzhou University of Technolo gy, Lanzhou City, Gansu\nProvince , China\n☯ These authors contribu ted equally to this work.\n* tongyixuan 2008@163. com\nAbstract\nWith the rapid development of the mobile internet, people are becoming more dependent on\nthe internet to express their comments on products or stores; meanwhile, text sentiment\nclassification of these comments has become a research hotspot. In existing methods, it is\nfairly popular to apply a deep learning method to the text classification task. Aiming at solv-\ning information loss, weak context and other problems, this paper makes an improvement\nbased on the transformer model to reduce the difficulty of model training and training time\ncost and achieve higher overall model recall and accuracy in text sentiment classification.\nThe transformer model replaces the traditional convolutional neural network (CNN) and the\nrecurrent neural network (RNN) and is fully based on the attention mechanism; therefore,\nthe transformer model effectively improves the training speed and reduces training difficulty.\nThis paper selects e-commerce reviews as research objects and applies deep learning the-\nory. First, the text is preprocessed by word vectorization. Then the IN standardized method\nand the GELUs activation function are applied based on the original model to analyze the\nemotional tendencies of online users towards stores or products. The experimental results\nshow that our method improves by 9.71%, 6.05%, 5.58% and 5.12% in terms of recall and\napproaches the peak level of the F1 value in the test model by comparing BiLSTM, Naive\nBayesian Model, the serial BiLSTM_CNN model and BiLSTM with an attention mechanism\nmodel. Therefore, this finding proves that our method can be used to improve the text senti-\nment classification accuracy and effectively apply the method to text classification.\nIntroduction\nWith the rapid development of technology and the rapid popularization of the mobile internet\nin the past few years, many emerging industries have been born, and an increasing number of\npeople are willing to comment on merchants and the products they buy on review platforms,\nincluding Dianping.com, Meituan, Eleme, etc. The comment information shows a certain\nemotional tendency, so analyzing these texts assists with product improvement, user selection,\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 1 / 16\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Wang X, Tong Y (2021) Application of an\nemotional classification model in e-comm erce text\nbased on an improved transformer model. PLoS\nONE 16(3): e0247984. https://doi.o rg/10.1371/\njournal.pone .0247984\nEditor: Yiming Tang, Hefei University of\nTechnology , CHINA\nReceived: August 12, 2020\nAccepted: February 17, 2021\nPublished: March 5, 2021\nCopyright: © 2021 Wang, Tong. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All training data files\nare available from https://githu b.com/Embeddi ng/\nChineseNlp Corpus/tree/ma ster/datase ts/yf_\ndianping.\nFunding: This work was supported by Gansu\nProvincial Key R & D Plan: Research on Key\nTechnologi es of Drones, 18YF1GA06 0.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nkeyword extraction, etc. As artificial intelligence has been rapidly developing in recent years,\ntraditional manual processing is gradually being eliminated, and the wide application of artifi-\ncial intelligence algorithms has greatly promoted the rapid development of text sentiment\nclassification. Currently, there are basically 3 methods commonly used in the field of text\nsentiment classification: the semantic-dictionary-based method, the machine-learning-based\nmethod and the deep-learning-based method.\nThrough learning from existing deep learning models, we found that the CNN model can\nonly obtain partial text information when processing text classification. If the distance between 2\nwords is too long but there is still a dependency between them, the CNN model is unable to\ndetect with good accuracy. The RNN model shows a certain memory ability and can handle\nlong-distance dependencies between words very well. However, RNN has a high cost to train the\nmodel, making it too expensive to train. Therefore, in text sentiment classification, existing mod-\nels still have shortcomings, including relatively low accuracy and recall, a long training time, etc.\nAddressing the above problems, we try to optimize the method based on the transformer\nmodel to improve the recall rate and reduce the model training time to a certain extent. The\naccuracy of the data is guaranteed by adding new variables in the multihead attention mecha-\nnism. In addition, a new normalization method, instance normalization, is applied to process\nthe data. Finally, the model is optimized with activation functions to improve the ability of the\nmodel to perform random regularization, to make it more consistent with the natural process\nof cognition and to effectively improve its overall performance.\nRelated work\nGoogle proposed the transformer model in 2017 [1]. In existing encoding and decoding frame-\nworks, most deep learning frameworks are achieved by CNN or RNN; meanwhile, the trans-\nformer model removes traditional CNN and RNN structures and only uses the attention\nmechanism to achieve certain goals. Therefore, the transformer model is used as an encoding\nand decoding model based entirely on the attention mechanism. Additionally, because the\nattention mechanism was introduced by the transformer model, the entire architecture con-\nsists of a stacked self-attention and fully connected layer. The context with distant words is\ncombined via an attention mechanism, all words are processed in parallel, and each word\nnotices other words in the sentence in multiple processing steps.\nTransformer model is shown in Fig 1 as follow.\nThe left half of Fig 1 is the encoder, and the right half is the decoder. The encoder is stacked\nin 6 layers that have the same structure, and each layer can be further divided into 2 sublayers,\ni.e., multihead attention and feed forward. The decoder is also formed by 6 layers that have the\nsame structure, but the contents of each layer are different from those in the encoder module.\nThe first layer uses multihead attention to calculate the input self-attention. The second layer\nperforms an attention calculation on the data input by the encoder module. The third layer is\nthe feed forward layer. It follows that when executing the decoder module, each layer of the\ndecoder will apply multihead attention on the result obtained by the encoder. By adopting 2\nattention mechanisms, scaled dot-product attention and multihead attention, the transformer\nmodel greatly improves task performance, parallel processing and trainability.\nScaled dot-product attention mechanism\nScaled dot-product attention treats the input sequential encode representation as a set of key-\nvalue pairs (K, V) and a query Q, where the relationship of K and V is one-to-one. By means of\nquerying every element in Q, each element in K is multiplied to find the inner product. Then,\nthe softmax method is used to calculate the similarity between elements in Q and elements in\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 2 / 16\nV. A weighted sum is then used to obtain a new vector. Among them, T represents the matrix\ntranspose,\n1\nffiffi ffi\nd\nk\np\nis the scaling factor, where dk represents the dimension of the key, and 64 is\nused by default. And its formula is shown below.\nAtte nti on ðQ; K; VÞ ¼ sof tmax\nQK\nT\nffiffiffiffi ffi\nd\nk\np\n !\nV ð1Þ\nScaled dot-product attention adds a dimension with a value of K on the basis of a similarity\ncalculation using a dot product operation, which plays a role in regulating the inner product in\ncase the value sought is too large.\nScaled dot-product attention is as shown in Fig 2.\nMultihead attention\nThe structure of multihead attention is as shown in Fig 3 below. Query, key and value first go\nthrough a linear transformation. Then the results are input into scaled dot-product attention h\ntimes, namely, multihead attention, where each input is treated as a head. Moreover, the\nparameters W for the linear transformation of each input Q, K and V are different. The results\nFig 1. Transformer model diagram. This picture shows the overall structure of the transforme r model.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 01\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 3 / 16\nfrom the h-time scaled dot-product attention are combined; the value is determined after per-\nforming a linear transformation as the result of multihead attention. As above, the model is\ncharacterized by an h-time calculation instead of merely learning the results of one calculation.\nThe advantage is that this strategy allows the model to learn relevant information in different\nrepresentation subspaces. The formula of the model is shown below.\nMult iHe ad ðQ; K ; V Þ ¼ Conc at ðhea d\n1\n; . . . ; hea d\nh\nÞW\no\nð2Þ\nhea d\ni\n¼ Att ent ion ðQW\nQ\ni\n; KW\nK\ni\n; VW\nV\ni\nÞ ð3Þ\nFig 2. Scaled dot-produc t attention. This picture introduces the structure of the Scaled dot-produc t attention\nmechan ism.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 02\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 4 / 16\nLayer normalization method\nLayer normalization (LN) [2] is applied to normalize the sample. The depth of the transformer\nmodel is not fixed, and many static features need to be saved in each training; therefore, if\nthere is a particular order in the calculation process, the subsequent model training will be dif-\nficult. The LN method is not required to perform batch training, and normalization operations\ncan be performed within a single piece of data. The LN method performs normalization opera-\ntions on the inputs to all neurons in a certain layer of deep learning according to the formula\nbelow.\nm\nl\n¼\n1\nH\nX\nH\ni¼1\na\nl\ni\nð4Þ\ns\nl\n¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi\n1\nH\nX\nH\ni¼1\nða\nl\ni\n\u0000 m\nl\nÞ\n2\ns\nð5Þ\nMoreover, in the LN method, the inputs into neurons of the same layer share the same\nmeans and variances, while a different input sample will have different means and variances,\nwhich avoids the effects of training caused by insufficient variance and other conditions. In\nFig 3. Multi-head attention . This picture introduces the structure of the Multi-Head Attention mechanism.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 03\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 5 / 16\naddition, the LN method processes on a single training sample and is independent of other\ndata. Therefore, the LN method can avoid the impact of different data distributions on model\ntraining.\nWord2vec\nWord2vec was proposed by Tomas in 2013 [3], which makes the original neural network lan-\nguage model with too many parameters and huge computational effort simple and easy. The\ncore of the algorithm is a neural network approach that uses the bag-of-words model CBOW\n(continuous bag-of-words) or Skip-Gram model to map words into the same coordinate sys-\ntem for learning lexical vectors in a corpus.\nThe CBOW model, which is a prediction of the current word vector W\nt\nwith a known con-\ntext W\n(t−2)\n, W\n(t−1)\n, W\n(t+1)\n, W\n(t+2)\n, learns the objective function of maximizing the log-likeli-\nhood function as shown in the following equation.\nL ¼\nX\nw¼C\nlog pðw j Cont extðwÞÞ\nð6Þ\nSkip-gram model, which is to predict the contextual word vectors with known current words,\nwith the function shown below.\nL ¼\nX\nw¼C\nlog pðCont ext ðwÞjwÞ\nð7Þ\nWord2vec is trained using the gradient ascent method, and in order to improve the training\nperformance, both Hierarchical Softmax and Negative Sampling methods are used for solving.\nModel design\nTo increase the recall of text sentiment classification and inspired by DeepMind, this paper\nproposes a modified transformer model. It is found that in the model based on transformer+-\nsoftmax, when processing the dataset used in this paper and when the padding is 0, overfitting\nof the softmax function will occur during optimization. Inspired by rawkey, our model adds a\nnew rawkeys variable to multihead attention and fills the data before they are applied to the\nsoftmax function to avoid the condition when the padding is 0. This process benefits the opti-\nmization of the softmax function, ensuring the accuracy of the result, as shown in Fig 4. Addi-\ntionally, this paper introduces a new normalization method, instance normalization (IN), to\nnormalize data. When processing data, it is not affected by channels or batchsize and also guar-\nantees the independence of each text instance, as shown in Fig 5.\nFinally, this paper also modifies the original activation function and introduces GELUs.\nThus, the model is more consistent with the cognition process in nature by adding random\nregularization.\nWord vector model\nThe word vector model uses the word2vec model [3] for training. Using the skip-gram model\nin word2vec, the vector dimension is set to 100, the number of iterations is 8 and the training\nresult is saved in the format of bin. In the word embedding layer, the position definition is\ntransmitted by means of a fixed one-hot method and stitched to a word vector [4]. As for the\ngeneration of the position vectors, this paper applies sine and cosine to obtain the embedding\npart of each position. Then parity is used and wrapped with sine and cosine. Subsequently, this\npaper utilizes batchsize, sequencelen and embeddingsize to construct a 3-dimensional matrix.\nThe skip-gram model is displayed in Fig 6.\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 6 / 16\nRawkey variable\nThis paper adds the rawkey variable to the multihead attention of the original transformer\nmodel to calculate the value of the mask. By means of adding rawkey, the variable of keys is\nadded with the value of position embedding, so it does not contain padding with a value of 0.\nThe function is covered with a very small number before application of the softmax function,\nleading to 0 points as the result. Similarly, if there is no value when filling the query, then it will\nbe covered by 0. The use of masking depends on whether the sum of the query result in the last\ndimension is equal to 0, so the filled part is added by position embedding and will never be 0.\nNormalization method\nIn terms of the choice of normalization method, this paper applies a new normalization\nmethod, instance normalization [5], to normalize the samples. Compared with the LN used in\nFig 4. Multi-head attention model with rawkeys variable. This picture shows the structure diagram of the multi-\nhead attention model after adding the rawkeys variab le.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 04\nFig 5. Using the IN method. This picture shows the model structure diagram using the IN standardi zation method in\nthe final data processin g stage.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 05\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 7 / 16\nthe original paper, the same mean and variance are shared when inputting into neurons of the\nsame layer, and different means and variances are provided for different input samples to\navoid the impact of insufficient variance and other conditions on training neural networks;\nhowever, its effect is only average. The underlying reason for its average effect is that, consider-\ning the dependency goal of mini-batch, the LN method regards the response values of all neu-\nrons in the same layer as the collection range to calculate means and variances. Therefore,\nwhen the range of the statistical value is narrowed, its strength is no longer obvious.\nHowever, the IN method picked in this paper is able to avoid this problem. The IN method\nwas first proposed in the field of image processing and was later applied to image pixel normal-\nization. This paper analyzes the theory of IN, utilizes it to normalize textual features, and\nmakes the theory accelerate model convergence while keeping the independence of each text\ninstance. In addition, the normalization of the IN method is not affected by the size of the\nchannel or batch. IN normalizes the data according to the formula below.\ny\ntij k\n¼\nx\ntij k\n\u0000 m\nti\nffiffiffiffiffiffiffiffiffiffiffiffi ffi\ns\n2\nti\nþ �\np\nð8Þ\nm\nti\n¼\n1\nHW\nX\nW\nl¼1\nX\nH\nm¼1\nx\ntilm\nð9Þ\ns\n2\nti\n¼\n1\nHW\nX\nW\nl¼1\nX\nH\nm¼1\nðx\ntilm\n\u0000 mu\nti\nÞ\n2\nð10Þ\nFig 6. skip-gram model. This picture is a schemati c diagram of the model of the skip-gram model.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 06\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 8 / 16\nGELUs activation function\nFor the choice of activation function, the selected GELUs (Gaussian Error Linear Units) is a\nnew activation function proposed by Dan and others in 2018. GELUs is a neural network acti-\nvation function of high performance, which has been applied successfully in the bert model\n[6]. Compared to existing activation functions, including ReLU, Sigmoid, etc., the sigmoid\nfunction is easily saturated, while the ReLU function lacks random factors [7]. However, in\nneural network modeling, a very important property of a model is its nonlinearity. Moreover,\nfor the generalization ability of a model, random regularization and other factors need to be\nintroduced. The two factors above are separated. GELUs introduces the idea of random regu-\nlarization in activation, which is a probabilistic description of the neuron input. From a visual\npoint of view, GELUs is more consistent with the natural cognitive process for which the\nmathematical expression is as follows:\nGEL Us ðxÞ ¼ xPðX � xÞ ¼ xFðxÞ ð11Þ\nwhere F(x) is the probability function of the normal distribution. If using a simple normal dis-\ntribution N(0,1), then the mathematical formula of the GELUs(x) for a hypothetic standard\nnormal distribution is calculated, as shown below.\nGE LUsðxÞ ¼ 0:5x 1 þ tanh\nffiffi ffi\n2\np\nr\nx þ 0:0447 15x\n3\nð Þ\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n !\nð12Þ\nor\nxsð1:702xÞ ð13Þ\nExperiment and analysis\nExperimental dataset\nIn this paper, the Dianping review dataset [8] from an assistant professor at Rutgers University\nis used to crawl and construct comments from a popular review website as well as a famous\nonline review website in China, and then, users’ business scores are divided. Dianping.com\ndivides the scores into 1, 2, 3, 4 and 5 levels. Through analysis of emotional tendency, this\npaper divides the emotional tendency into positive emotion and negative emotion. The com-\nprehensive scores of 1 and 2 are divided into negative emotion, which is recorded as—1; the\nscores of 4 and 5 are divided into positive emotion, which is recorded as 1; and the score of 3 is\nregarded as the medium evaluation, which is recorded as 0. Although neutral evaluation has\nlittle effect on affective analysis, it can be used as the corpus for the training corpus model.\nThis paper uses 250000 unlabeled comments and 300000 tagged comments, and the data dis-\ntribution is shown in Fig 7.\nIn the experiment, the data used jieba word segmentation for standardized word segmenta-\ntion, and the Baidu stop word list was used for preprocessing to remove stop words. However,\nthis article screens the stop word list according to the actual situation and keeps some emo-\ntional words, such as good, very good, bad, dissatisfied and other words with a certain emo-\ntional tendency, which is helpful for text training.\nThrough the analysis of the data, as shown in Fig 8, it is found that the sentence length of\n100 words can cover most of the data, so the sentence length input by the model is vectorized\ninto 100 words, and the data set is divided into a training set and a test set according to the\nratio of 80% and 20%, respectively. Examples of the data are shown in Table 1.\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 9 / 16\nFig 7. Data distribut ion. This picture shows the distribution of data in the dataset used in this article.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 07\nFig 8. Quantitativ e distribution of words. This picture presents the number of words containe d in each text in the\ndataset used in this paper.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 08\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 10 / 16\nExperiment result and analysis\nIn the experiment, the number of layers of transformer is set to 1, the number of multihead\nattention for each encoder is set to 8, and other parameters are set according to Table 2 as fol-\nlows. Three different evaluation indicators, including precision, recall and the F1 value, are\napplied to measure the quality of the model. Precision refers to the proportion of TP (True\nPositives) in all the elements labeled as belonging to the positive class (including TP and FP),\nrecall is the percentage of TP in all the elements that actually belong to the positive class (i.e.,\nthe sum of TP and FN), and the F1 value is the harmonic average of precision and recall, and\ntheir formulas are as follows.\nPrec isi on ¼\nTP\nTP þ FP\nð14Þ\nRec al l ¼\nTP\nTP þ FN\nð15Þ\nF\n1\n¼\n2 � Pre cisi on � Rec all\nPrec isi on þ Rec all\nð16Þ\nThis paper calculates the binary cross entropy loss to evaluate the value of model loss. In the\ntraining process, it is found that the loss value decreases as the number of iterations increases.\nAs shown in Fig 9, the convergence speed is very fast in the first 250 steps, as the loss value\nTable 1. Data sample.\nDataset Models\n1 Nanxin can be regarded as the famous dessert shop in Guangzhou.\nThere are many people passing by for several time periods.\nLooking at the dense and deliciou s menu, it is easy to choose difficult diseases.\nQuickly withdrew after ordering.\nI went to the checkout counter and ordered.\nThe waiter just served the food. The food tasted good.\n2 Ordered red bean taro and coconut juice papaya red bean soup,\nsweet and greasy, canned coconut juice with coconut juice,\na thick flavor of flavor, this shop can only be said to be overdone ,\nand there are desserts in front Women are disgusti ng with a stinking face.\nPart of the data in the data set.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.t00 1\nTable 2. Parameter setting.\nValuel Value\nEmbedd ing dimension 100.000\nLearning Rate 0.001\nDropout in Multi-Head Attention 0.9\nSentence Length 100.000\nBatchSize 128.000\nModel parameter settings.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.t00 2\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 11 / 16\nchanges significantly. However, in 250-1500 steps, the convergence slows gradually, and the\nchanging rate of the loss value declines. After 1500 steps, the loss value stabilizes.\nAs seen from Fig 10, this paper improves the original transformer model. When only add-\ning a rawkey variable, the recall rate is improved by 0.4% compared with the original model.\nWhen only changing the IN method, the recall rate is increased by 0.51%. When only using\nthe GELUs activation function, the recall increases by 0.38%. In general, this paper improves\nthe original model, and the recall increased by 0.56% compared with original model.\nTo verify the feasibility of this paper, this paper selects several relatively new text sentiment\nclassification models to conduct a comparative test on the same dataset. The experimental\nresults are shown in the following Fig 11 and Table 3.\nNaive Bayesian classifier(NBC) [9]. The training samples are obtained by pre-processing\nthe data, then estimating the probability of occurrence of different categories, and then the\nprobability of occurrence of each attribute value under the condition of each category without\nsharply, so as to calculate the probability of belonging to each category for each combination\nof attributes, and finally selecting the maximum probability value as the output of the inferred\nresult for that piece of data.\nBiLSTM [10] is an improved version of LSTM by combining the forward LSTM with the\nbackward LSTM to form the BiLSTM model. BiLSTM can better capture the semantic depen-\ndencies in both directions. For the sentiment classification task, the information obtained is\nmore comprehensive, including all the information in the forward and backward directions.\nSerial BiLSTM_CNN model [11]. BiLSTM is applied to extract context from text to resolve\nthe problems of gradient disappearance and long-term dependence. Additionally, a CNN\n(convolutional neural network) is added to extract local semantic features from text, and soft-\nmax is used for processing at the last stage.\nBidirectional long short-term memory network with attention mechanism (BiLSTM+-\nAttention) [12]. An attention mechanism based on the BiLSTM model is added; thus, self-\nattention weights are added to the text features of the current word. Then, the results are nor-\nmalized through the softmax layer. The fully connected layer is utilized to output the processed\nmatrix with attention weights.\nFig 9. Changing curve of loss. This picture shows the change curve of the loss value of the model during training .\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 09\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 12 / 16\nBy comparing the above 3 indicators, the modified transformer model proposed in this\npaper performs the best in text sentiment classification. A significant improvement is seen in\nboth recall and the F1 value in comparison with BiLSTM, Naive Bayes Classifier, the serial\nBiLSTM_CNN model and BiLSTM with an attention. Based on the formulas for calculating\nprecision and recall, only when these two values are both relatively large is the performance at\nthis time optimal. Moreover, the F1 value can be regarded as a harmonic mean of these two\nvalues, allowing a comprehensive assessment of the quality of the model. It is found that when\ncomparing the values of the 3 indicators, the precision and recall of the NBC model are both\nthe lowest. BiLSTM+Attention enhances the acquisition of text features in current words by\nintroducing the attention mechanism, so its precision is the highest among the five models.\nAnd the model used in this paper improves on the transformer model, which has the highest\nrecall rate and the best F1 value for its overall evaluation.\nIn terms of the time spent in model training, as shown Fig 12, the training time of the modi-\nfied model in this paper is far less than that consumed by NBC, BiLSTM, serial BiLSTM_CNN\nFig 10. Compar ative experimen t of transform er model and improved model. (a) Increase rawkey variable; (b) Use the IN method; (c) Using the GELUS\nfunction; (d) Compar ison of improved models.\nhttps://d oi.org/10.1371 /journal.pone. 0247984.g010\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 13 / 16\nand BiLSTM+Attention, decreasing the results by4.19 times, 4.91 times, 3.55 times and 3.38\ntimes, respectively.\nThe text sentiment classification model based on the modified transformer proposed by\nthis paper performs the best in terms of recall rate. In comparison with the original BiLSTM\n+Attention, its recall increases by 5.12%. Taken together, the F1 value of our model is the high-\nest among the 5 models due to the following reasons. 1. IN is applied as the normalization\nmethod in the model. The advantages of IN are used to reduce the impact of channels and\nbatchsize on the model. 2. A new activation function, GELUs, is used, and the random regular-\nization idea is utilized in the process of activation, which is intuitively more in line with natural\ncognition. 3. The variable rawkey is added in multihead attention, and the data are optimized\nand completed before the softmax application, which is beneficial to the optimization of the\nmodel. 4. As for the dataset, e-commerce reviews have a tight context, which highlights the\nadvantages of the transformer model. Through the above analysis of the experimental results,\nit is demonstrated that the modified text sentiment classification model based on transformer\nFig 11. Model recall ratio comparis on. This picture shows the change curve of the recall value compared with the two\ncomparison models.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 11\nTable 3. Experiment results.\nModell Precisionl recalll F\n1\nvalue\nNBC 0.8924 0.9019 0.9136\nBILSTM 0.9186 0.9189 0.9574\nSerial BiLSTM_CNN 0.9195 0.9224 0.9578\nBi_LSTM+A ttention 0.9709 0.9478 0.9589\nOur Model 0.9605 0.9989 0.9671\nComparis on of experimental results.\nhttps://do i.org/10.1371/j ournal.pone .0247984.t003\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 14 / 16\nproposed in this paper performs better than the serial BiLSTM_CNN and BiLSTM+Attention\nmodels, and the experimental results verify the effectiveness and feasibility of our method.\nConclusion\nThis paper proposes a text sentiment classification model using e-commerce reviews as the\nbasis of an advanced transformer. A new variable, rawkeys, is added to multihead attention\nto make the model more accurate when processing data. In addition, a new normalization\nmethod, IN, is utilized to process the data results. A new activation function, GELUs, is applied\nat the end, which enhances the generalization ability of the whole model, makes the process\nmore in line with the natural cognition process, and further shortens the time required for\nmodel training. The weaknesses of our model are that it takes too long to train data of a large\nvolume, it is not optimized well for preprocessing data, and it does not perform as well as the\nBiLSTM + Attention model in terms of precision. In future work, the model will be improved,\nthe data at the preprocessing phase will be optimized, and the advantages of other models\nwill be integrated to improve the classification performance of this model, guaranteeing stable\nrecall.\nAuthor Contributions\nConceptualization: Yixuan Tong.\nMethodology: Yixuan Tong.\nSoftware: Yixuan Tong.\nValidation: Yixuan Tong.\nWriting – original draft: Yixuan Tong.\nWriting – review & editing: Xuyang Wang.\nFig 12. Comparis on of model digestion time. This picture shows the time spent training the model compared to the\ntwo comparison models.\nhttps://d oi.org/10.1371/j ournal.pon e.0247984.g0 12\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 15 / 16\nReferences\n1. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkorei t, Llion Jones, Aidan N. Attention Is All\nYou Need. CoRR. 2017; abs/1706 .03762.\n2. Ba Jimmy Lei, Kiros Jamie Ryan, Hinton Geoffre y E. Layer Normalizat ion. CoRR. 2016; abs/\n1301.3781.\n3. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. Efficient Estimation of Word Representa tions in\nVector Space. CoRR. 2013; abs/1301 .3781.\n4. Hao Cheng. Research on Word2Vec Baesed Chinese Question Retrieval and System Implementati on\nHarbin Institute of Technology , 2016\n5. Dmitry Ulyanov , Andrea Vedald, Victor Lempitsky . Instance Normalizat ion: The Missing Ingredient for\nFast Stylization . CoRR. 2016; abs/1607 .08022.\n6. Dan Hendrycks , Kevin Gimpel. Bridging Nonlinea rities and Stochastic Regularizers with Gaussian Error\nLinear Units. CoRR. 2016; abs/1606 .08415.\n7. Feini Zhao. SAR Image Target Recognition Based on Deep Learning Networ k. XIDIAN UNIVE RSITY.\n2017 https://doi.o rg/10.1109/D SAA.2014.7 058124\n8. Zhang Yongfe ng, Zhang Haochen, Zhang Min, Liu Yiqun, Ma Shaoping. Do Users Rate or Review?:\nBoost Phrase -level Sentimen t Labeling with Review- level Sentimen t Classifica tion. SIGIR 2014. 2014\nhttps://doi.or g/10.114 5/2600428.26 09501\n9. Li Jingmei, Sun Lihua, Zhang Qiaorong , Zhang Chunsheng. Applicat ion of native Bayes classifier to text\nclassifica tion. Journal of Harbin Engineerin g University. 2003\n10. Mike Schuster, Kuldip K. Paliwal Bidirectiona l recurrent neural networks IEEE transaction s on Signal\nProcessin gs. 1997\n11. Hong Zhao and Le Wang and Weijie Wang. Text sentiment analysis based on serial bi-directiona l long\nshort term memory and convolutio nal neural network hybrid model. Journal of Compu ter Applica tions.\n2019\n12. Liang Zhang and Yuanfeng Yang, Jinxiang Li and Yi Jin and Yuanxia Liu. Design and implementa tion of\nBi-LSTM+A ttention sentime nt analysis model. Electro nic Technolo gy & Software Engineeri ng. 2019\nPLOS ONE\nApplicat ion of improved transfor mer model\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02479 84 March 5, 2021 16 / 16",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7591233849525452
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6272742748260498
    },
    {
      "name": "Transformer",
      "score": 0.5957311987876892
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5432785749435425
    },
    {
      "name": "Deep learning",
      "score": 0.5366356372833252
    },
    {
      "name": "Machine learning",
      "score": 0.48633843660354614
    },
    {
      "name": "Sentiment analysis",
      "score": 0.46065282821655273
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4427887201309204
    },
    {
      "name": "Recall",
      "score": 0.41850775480270386
    },
    {
      "name": "Word2vec",
      "score": 0.41234204173088074
    },
    {
      "name": "F1 score",
      "score": 0.4107096791267395
    },
    {
      "name": "Artificial neural network",
      "score": 0.4026902914047241
    },
    {
      "name": "Natural language processing",
      "score": 0.34923815727233887
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Embedding",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "cited_by": 10
}