{
  "title": "Language Models as Knowledge Embeddings",
  "url": "https://openalex.org/W4225632115",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2112373825",
      "name": "Xin-Tao Wang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103044703",
      "name": "Qianyu He",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161872403",
      "name": "Jiaqing Liang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131222654",
      "name": "Yanghua Xiao",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W3201503287",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2997897037",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3215677344",
    "https://openalex.org/W2127426251",
    "https://openalex.org/W2988237903",
    "https://openalex.org/W2913224127",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2759136286",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W3103296573",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3099206682",
    "https://openalex.org/W205829674",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W3101204082",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental results show that LMKE achieves state-of-the-art performance on KE benchmarks of link prediction and triple classification, especially for long-tail entities.",
  "full_text": "Language Models as Knowledge Embeddings\nXintao Wang1 , Qianyu He1 , Jiaqing Liang1∗ , Yanghua Xiao1,2∗\n1Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n2Fudan-Aishu Cognitive Intelligence Joint Research Center\n{xtwang21, qyhe21}@m.fudan.edu.cn, l.j.q.light@gmail.com, shawyh@fudan.edu.cn\nAbstract\nKnowledge embeddings (KE) represent a knowl-\nedge graph (KG) by embedding entities and re-\nlations into continuous vector spaces. Existing\nmethods are mainly structure-based or description-\nbased. Structure-based methods learn representa-\ntions that preserve the inherent structure of KGs.\nThey cannot well represent abundant long-tail enti-\nties in real-world KGs with limited structural in-\nformation. Description-based methods leverage\ntextual information and language models. Prior\napproaches in this direction barely outperform\nstructure-based ones, and suffer from problems\nlike expensive negative sampling and restrictive\ndescription demand. In this paper, we propose\nLMKE, which adopts Language Models to derive\nKnowledge Embeddings, aiming at both enrich-\ning representations of long-tail entities and solv-\ning problems of prior description-based methods.\nWe formulate description-based KE learning with\na contrastive learning framework to improve effi-\nciency in training and evaluation. Experimental re-\nsults show that LMKE achieves state-of-the-art per-\nformance on KE benchmarks of link prediction and\ntriple classification, especially for long-tail entities.\n1 Introduction\nKnowledge graphs (KGs) are multi-relational graphs com-\nposed of entities as nodes and relations as edges, such as\nWordNet [Miller, 1995]. They have been used to support a\nwide range of applications, including information retrieval,\nrecommender systems and question answering. For better ap-\nplications of symbolic knowledge in KGs in recent machine\nlearning models, many efforts have been devoted to embed-\nding KGs into low-dimension vector spaces, which are re-\nferred to as knowledge embeddings (KEs). The principal ap-\nplications of KEs are link prediction and triple classification\nin KGs, while there is also an increasing trend to use KEs in\nnatural language processing (NLP) tasks like text generation.\nThere are mainly two kinds of existing KE methods,\nnamely traditional structure-based methods and emerging\n∗Corresponding Authors\n1 500 1000 1500 2000\nEntity Rank by Degree\n0\n200\n400Entity Degree\n0-0 1-1 2-3 4-78-1516-3132-6364-127128-255256-511\nGroup of Entity Degrees\n0.0\n0.2\n0.4\n0.6\n0.8Hits@10\n0\n500\n1000\n1500\n2000\n#Triples\nHits@10\n#Triples\nFigure 1: Distribution of entity degrees and performance of Ro-\ntatE (a typical structure-based method) on WN18RR. Entities are\ngrouped by the logarithm of degrees (right). For each group, we re-\nport the number of relevant triples and their average performance on\nlink prediction. Please refer to Sec 4.3 for more details.\ndescription-based methods. Structure-based methods, such\nas TransE [Bordes et al., 2013] and RotatE [Sun et al., 2019],\nare trained to preserve the inherent structure of KGs. These\nmethods cannot well represent long-tail entities, as they de-\npend solely on the structure of KGs and thus favor entities\nrich in structure information (i.e., linked with plentiful enti-\nties). However, real-world KGs are widely observed to have a\nright-skewed degree distribution, i.e., degrees of entities ap-\nproximately follow the power-law distribution that forms a\nlong tail, as is illustrated in Fig.1. These KGs are populated\nwith massive unpopular entities of low degrees. For example,\non WN18RR, 14.1% entities are with degree 1, and 60.7%\nentities have no more than 3 neighbors. Their embeddings\nconsequently suffer from limited structural connectivity. This\nproblem is justified by the declined performance of structure-\nbased methods on long-tail entities shown in Fig.1, which\nsuggests that their embeddings are still unsatisfactory.\nDescription-based KE methods represent entities in KGs\nby encoding their descriptions with language models, such as\nDKRL [Xie et al., 2016] and KEPLER [Wang et al., 2019b].\nTextual descriptions provide rich information for numerous\nsemantic-related tasks, which brings an opportunity to learn\ninformative representations for long-tail entities. In an ex-\ntreme case, an emerging entity that is novel to an existing KG\n(in other words, having zero degrees in the KG) can still be\nwell represented with its textual information. This feature is\nreferred to as a capacity in the “inductive (zero-shot) setting”\nby prior approaches. Besides, lots of missing knowledge in\nKGs could be covered by rich textual information contained\nin entity descriptions or learned by pretrained language mod-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2291\nels (PLMs), given that current text corpora outstrip KGs in\nscale and contain much more information. However, existing\ndescription-based methods barely outperform structure-based\nmethods and suffer from the following problems:\n1. Expensive negative sampling. While negative sampling\nis vital to KE learning, existing description-based meth-\nods are only allowed to have limited negative samples\nbecause of the encoding expense of language models.\n2. Restrictive description demand. Existing methods gen-\nerally require descriptions of all entities in the KG, and\ndiscard entities with no or short descriptions. Although\ndelicate benchmarks can satisfy this demand, real-world\nKGs can hardly contain descriptions for all entities.\nIn this paper, we propose LMKE, which adopts Language\nModels to deriveKnowledge Embeddings, aiming at both en-\nhancing representations of long-tail entities and solving the\nabove issues in prior description-based KEs. LMKE lever-\nages the inductive capacity of description-based methods to\nenrich the representations of long-tail entities. In LMKE,\nentities and relations are regarded as special tokens whose\noutput embeddings are contextualized in and learned from\nrelated entities, relations, and their textual information, so\nLMKE is also suitable for entities with no description. A con-\ntrastive learning framework is further proposed where entity\nembeddings within a mini-batch serve as negative samples for\neach other to avoid the additional cost of encoding negative\nsamples. In summary, our contributions are listed as follows:1\n1. We identify the problem of structure-based KEs in rep-\nresenting long-tail entities, and generalize the inductive\ncapacity of description-based KEs to solve this problem.\nTo the best of our knowledge, we are the first to propose\nleveraging textual information and language models to\nenrich representations of long-tail entities.\n2. We propose a novel method LMKE that tackles two defi-\nciencies of existing description-based methods, namely\nexpensive negative sampling and restrictive description\ndemand. We are also the first to formulate description-\nbased KE learning as a contrastive learning problem.\n3. We conduct extensive experiments on widely-used\nKE benchmarks, and show that LMKE achieves the\nstate-of-the-art performance on both link prediction\nand triple classification, significantly surpassing exist-\ning structure-based and description-based methods, es-\npecially for long-tail entities.\n2 Related Work\nKnowledge embeddings represent entities and relations of\nKGs in low-dimension vector spaces. KGs are composed of\ntriples, where a triple (h, r, t)means there is a relationr ∈R\nbetween the head entity h ∈E and the tail entity t ∈E. E and\nR denote the entity set and the relation set respectively.\nStructure-Based Knowledge Embeddings. Existing KEs\nare mainly structure-based, deriving the embeddings by\n1Our codes are available at https://github.com/Neph0s/LMKE\nsolely structure information of KGs. These methods are fur-\nther distinguished into translation-based models and semantic\nmatching models in terms of their scoring functions [Wang\net al., 2017]. Translation-based models adopt distance-based\nscoring functions, which measure the plausibility of a triple\n(h, r, t)by the distance between entity embeddings h and t\nafter a translation specific to the relation. The most represen-\ntative model is TransE. It embeds entities and relations as h,\nr, t ∈Rd in a shared vector space with a dimensiond. Its loss\nfunction is defined as ∥h + r − t∥ so as to make h close to t\nafter a translation r. TransH [Wang et al., 2014] proposes to\nproject entity embeddings h and t to a relation-specific hyper-\nplane, and TransR [Lin et al., 2015] further proposes projec-\ntions into relation-specific spaces. RotatE defines a relation\nas a rotation from entity h to entity t in a complex vector\nspace, so their embeddings h, r, t ∈ Cd are expected to sat-\nisfy h ⊙ r ≈ t, where ⊙ stands for element-wise product. Se-\nmantic matching models adopt similarity-based scoring func-\ntions, which measure the plausibility of a triple (h, r, t)by\nmatching latent semantics of h, r, t. RESCAL[Nickel et al.,\n2011] represents the relation r as a matrix Mr and use a bi-\nlinear function h⊤Mrt to score (h, r, t). DistMult[Yang et\nal., 2014] makes Mr diagonal for simplicity and efficiency.\nCoKE [Wang et al., 2019a] employs Transformers [Vaswani\net al., 2017 ] to derive contextualized embeddings, where\ntriples or relation paths serve as the input token sequences.\nDescription-based Knowledge Embeddings. Recently,\ndescription-based KE methods are gaining increasing atten-\ntion for the importance of textual information and develop-\nment in NLP. DKRL [Xie et al., 2016] first introduces enti-\nties’ descriptions and encodes them via a convolutional neural\nnetwork for description-based embeddings. KEPLER [Wang\net al., 2019b] uses PLMs as the encoder to derive description-\nbased embeddings and is trained with the objectives of both\nKE and PLM. Pretrain-KGE [Zhang et al., 2020b] proposes\na general description-based KE framework, which initializes\nanother learnable KE with description-based embeddings and\ndiscards PLMs for efficiency after fine-tuning PLMs. KG-\nBERT [Yao et al., 2019] concatenates descriptions of h, r, t\nas an input sequence to PLMs, and scores the triple by the se-\nquence embedding. StAR [Wang et al., 2020] thus partitions\na triple into two asymmetric parts between which it performs\nsemantic matching.\n3 Methods\nIn this section, we introduce LMKE and its variants. We first\nprovide background on language models (Sec 3.1). After-\nward, we elaborate on how LMKE adopts language models\nto derive knowledge embeddings (Sec 3.2), and its contrastive\nvariants for both zero-cost negative sampling in training and\nefficient link prediction in evaluation (Sec 3.3).\n3.1 Language Models\nPretrained language models have been increasingly prevalent\nin NLP. They have been pretrained on large-scale corpora\nto store vast amounts of general knowledge. For example,\nBERT [Devlin et al., 2018] is a Transformer encoder pre-\ntrained to predict randomly masked tokens. Afterward, PLMs\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2292\nLanguage Model\n02wm6l Republic of … /Location\n/capital Location capital \n‘Republic of Venice’\nHead Entity Head Texts ‘Capital’\nRelation Relation Texts\n[CLS] [SEP]07_pf Venice is ...\n‘Venice’\nTail Entity Tail texts\ne(Triple) e(Head) e(Relation) e(Tail)\nPrediction over Triples\nTriple Classification\nPrediction over Entities / Relations\nLink Prediction, Relation Prediction, Entity Classification ...\nFigure 2: The architecture of LMKE.\ncan be readily used to achieve excellent performance in vari-\nous downstream tasks with fine-tuning.\nTo better understand such excellence, knowledge probing\nis proposed [Petroni et al., 2019], which questions PLMs with\nmasked cloze sentences. Researches in this direction have\ndemonstrated that PLMs contain abundant factual knowledge\nand have the potential to be used as knowledge bases. In-\nterestingly, PLMs are also shown to be capable of learning\nrelations satisfying one-hop rules like equivalence, symme-\ntry, inversion, and implication [Kassner et al., 2020], which\nis also the desiderata of knowledge embeddings.\n3.2 LMKE\nLMKE adopts language models as knowledge embeddings,\ni.e., deriving embeddings of entities and relations that can\nsupport prediction of the plausibility of given triples.\nLMKE learns embeddings of entities and relations in the\nsame space as word tokens. Given a specific triple u =\n(h, r, t), LMKE leverages descriptionssh, sr, st of h, r, tas\nadditional input, and outputs its probability p(u). LMKE\nmakes every entity and relation a token and gets their em-\nbeddings via pretrained language models. The triple is trans-\nformed into a sequence of tokens of h, r, tand the words\nin their descriptions, which serves as the input to PLMs.\nThe description of an entity (or relation) e is a sequence\nof tokens se = (x1, . . . , xne ) where ne denotes the length.\nThe input sequence is thus su = (h, sh, r, sr, t, st) =\n(h, xh\n1 , . . . , xh\nnh , r, xr\n1, . . . , xr\nnr , t, xt\n1, . . . , xt\nnt ). Then, two\nspecial tokens [CLS] and [SEP] are inserted in the front and\nback of su. Feeding forward su into the PLM, LMKE gen-\nerates the encoded embeddings h, r, t ∈ Rd of the three to-\nkens h, r, t. In this way, we embed entities, relations, and\nwords of their descriptions in a shared vector space. The em-\nbedding of one entity (or relation) is contextualized in and\nlearned from not only its own textual information but also\nthe other two components of the triple and their textual infor-\nmation. Therefore, long-tail entities can be well represented\nwith their descriptions, and entities without descriptions can\nalso learn representations from related entities’ descriptions.\nThe output embedding of [CLS] aggregates information of\nthe whole sequence, so we regard it as embedding u for the\nentire triple. To score plausibility p(u) of the triple, LMKE\ninputs u into a linear layer like KG-BERT:\np(u) =σ(wu + b) (1)\nwhere w ∈Rd and b are learnable weight and bias. We adopt\nbinary cross-entropy as the loss function for training. Be-\nsides, LMKE can also be used for prediction over entities or\nrelations with h, r, t. The architecture is shown in Fig. 2.\nThe principal applications of KEs are predicting missing\nlinks and classifying possible triples, which are formulated\nas two benchmark tasks for KE evaluation, namely link pre-\ndiction [Bordes et al., 2013] and triple classification [Socher\net al., 2013]. Triple classification is a binary classification\ntask that judges whether a triple u is true or not, which can\nbe directly performed with p(u). Link prediction is to predict\nthe missing entity in a corrupted triple (?, r, t)or (h, r,?)\nwhere ? denotes an entity removed for prediction. In this\ntask, models need to corrupt a triple by replacing its head\nor tail entity with every entity in the KG, score the replaced\ntriples, and rank the entities in descending order of the scores.\nHowever, it is hardly affordable for LMKE to score every re-\nplaced triple as an integral with PLMs, concerning the time\ncomplexity shown in Table 1. Even for a middle-sized dataset\nFB15k-237, there would be 254 million triples to be encoded.\nMethod Training Link Prediction\nKG-BERT O(NtrainNneg) O(∣E∣Neval)\nStAR O(NtrainNneg) O(∣E∣∣R∣)\nLMKE O(NtrainNneg) O(∣E∣Neval)\nC-LMKE O(Ntrain) O(Neval + ∣E∣)\nTable 1: The time complexity of training and link prediction evalu-\nation. ∣E∣ or ∣R∣ denotes the number of entities or relations in the\nKG. Ntrain or Neval is the number of triples in the training or evalua-\ntion split. Nneg is the negative sampling size. C-LMKE denotes con-\ntrastive LMKE, whose complexity is lower than prior approaches.\n3.3 Contrastive KE Learning\nTowards efficient link prediction with language models, one\nsolution is to encode the triples partially. Masked entity\nmodel (MEM-KGC) [Choi et al., 2021] replaces the removed\nentity and its description with a mask q, and predicts the\nmissing entity by feeding its output embedding q into a lin-\near layer. It can be viewed as a masked variant of LMKE\nthat trades off the exploitation of textual information for time\ncomplexity. Since only one masked incomplete triple is en-\ncoded, the complexity is downgraded. Nevertheless, it drops\ntextual information of the target entities to be predicted, thus\nhurting the utility of textual information.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2293\nQuery Encoder Key Encoder\n02wm6l Republic of … capital MASK_T 07_pf Venice is ...\n‘Republic of Venice’\nHead Entity Head Texts ‘Capital’\nRelation\nRelation Texts Masked Tail Entity ‘Venice’\nTail Entity Tail texts\nTriple 1\nTriple n\n. . .\nBatch Size\nMasked Tail Embedding Tail Embedding\nPositive Pairs\nKey Encoder\nMasked Tail Embedding Tail EmbeddingPositive Pairs\nQuery Encoder\n0f8l9c France, … contain MASK_T 04gdr Louvre is…\n‘France’\nHead Entity Head Texts ‘Contain’\nRelation\nRelation Texts Masked Tail Entity ‘The Louvre’\nTail Entity Tail texts\n/capital\n/contain\nFigure 3: The architecture of contrastive LMKE. The queries and keys are encoded separately before contrastive matching within the batch.\nWe propose a contrastive learning framework to better ex-\nploit description-based KEs for link prediction, where the\ngiven entity-relation pairand the target entity serve as the\nquery q and the key k to be matched in contrastive learning.\nFrom this point of view, the output embedding q of the\nmasked entity in MEM-KGC is the encoded query, and the\ni-th row in the weights We of the linear layer serves as the\nkey that corresponds to the i-th entity for each 1 ≤ i ≤ ∣E∣.\nTherefore, feeding q into the linear layer can be viewed as\nmatching the query q with the keys. The difference is that the\nkeys are directly learned representations instead of encodings\nof textual information like the queries.\nContrastive LMKE (C-LMKE) is an LMKE variant under\nthis framework that replaces the learned entity representa-\ntions (rows of We) with encodings of target entities’ descrip-\ntions, as is shown in Fig. 3. It features the contrastive match-\ning within the mini-batch, which allows efficient link predic-\ntion without loss of information exploitation while avoiding\nthe additional cost of encoding negative samples. The candi-\ndate keys of a query are defined as keys of the queries in the\nbatch. C-LMKE’s time complexity is analyzed in Table 1.\nThis practice solves the problem of expensive negative\nsampling and allows description-based KEs to learn from\nmuch more negative samples. While negative samples are\nvital to KE learning, most existing description-based meth-\nods are allowed to have only a few for each positive sample\n(usually ranging from 1 to 5) because of the cost of language\nmodels. C-LMKE currently binds the negative sampling size\nto the batch size, and with our contrastive framework, exist-\ning approaches in contrastive learning like memory bank can\nbe further introduced to achieve more improvement.\nWe match an encoded query q with an encoded key k by\na two-layer MLP (multi-layer perceptron), instead of cosine\nsimilarity that is commonly adopted in contrastive learning,\nbecause there may exist multiple keys matching q. If both k1\nand k2 match q and we maximize similarity between (q, k1)\nand (q, k2), (k1, k2) will also be enforced to be similar, which\nis not desirable. Thus, the probability that q matches k is:\np(q, k) =σ(MLP[q; k; q − k; q ⊙ k; d]) (2)\nwhere d = [log (dq + 1); log(dk + 1)] is the logarithm of\nentity degrees. dq and dk are the degrees of the given entity\nin q and the target entity k counted on the training set. If\nthe training set and the test set follow the same distribution,\nentities of higher degrees will also be more likely to appear\nin test triples, so degrees are important structural information\nto be concerned. While structure-based KEs learn degree in-\nformation as aggregation into clusters [Pei et al., 2019] and\nMEM-KGC learns it as the imbalance of entity labels, our\nmatching function cannot capture this information, so we ex-\nplicitly include it as additional features for prediction.\nSince there are usually multiple correct entities for a cor-\nrupted triple regarding relations that are not one-to-one, we\nadopt binary cross-entropy to judge whether each entity is\na correct key (multi-label classification), instead of multi-\nclass cross-entropy to find the most likely entity. Given that\nmost of the keys are negative, we average losses over posi-\ntive ones and negative ones respectively, and sum them up.\nThe loss of matching a query q with the keys K is thus\nformulated as L(q, K) = −∑k+∈K+ wq,k+ log(p(q, k+)) −\n∑k−∈K− wq,k− log(1 − p(q, k−)), where K+, K− denotes the\npositive and negative keys respectively andK =K+∪K−. We\nadopt self-adversarial negative sampling [Sun et al., 2019]\nfor efficient KE learning, which calculates the weights as\nwq,k+ = 1\n∣K+∣ and wq,k− = exp(p(q,k−))\n∑k∈K− exp(p(q,k)). In this case,\nloss of false-positive samples is amplified and loss of true-\nnegative samples is diminished.\n4 Experiments and Analyses\nIn this section, we evaluate the effectiveness of our methods.\n4.1 Experiment Setup\nDatasets. We experiment on four popular bench-\nmark datasets: FB13 [Socher et al., 2013 ], FB15k-\n237 [Toutanova, 2015], UMLS [Dettmers et al., 2018] and\nWN18RR [Dettmers et al., 2018], whose statistics are shown\nin Table 3. FB13 and FB15k-237 are derived from Freebase.\nWN18RR is derived from WordNet. UMLS is a medical\nontology describing relations between medical concepts.\nFB15k-237 and WN18RR are used for link prediction, where\nabundant inverse relations are removed in case they can serve\nas shortcuts. For triple classification, FB13 and UMLS are\nused. Only the test split of FB13 contains negative samples.\nIn other situations, negative samples are created by randomly\nreplacing head or tail entities, where the ground truth\n(training triples for the training split and all triples for the\ntest split) would be avoided. Following KG-BERT, the entity\ndescriptions we use are synsets definitions for WN18RR,\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2294\nDataset FB15k-237 WN18RR\nMetric MR MRR Hits@1 Hits@3 Hits@10 MR MRR Hits@1 Hits@3 Hits@10\nStructure-based Knowledge\nEmbeddings\nTransE [Bordes et al.,\n2013]♣ 323 0.279 0.198 0.376 0.441 2300 0.243 0.043 0.441 0.532\nDistMult [Yang et al.,\n2014]♠ 254 0.241 0.155 0.263 0.419 5110 0.430 0.390 0.440 0.490\nComplEx [Trouillon et al.,\n2016]♠ 339 0.247 0.158 0.275 0.428 5261 0.440 0.410 0.460 0.510\nRotatE [Sun et al., 2019]♠ 177 0.338 0.241 0.375 0.533 3340 0.476 0.428 0.492 0.571\nTuckER [Balaˇzevi´c et al.\n, 2019] - 0.358 0.266 0.394 0.544 - 0.470 0.443 0.482 0.526\nHAKE [Zhang et al., 2020a] - 0.346 0.250 0.381 0.542 - 0.497 0.452 0.516 0.582\nCoKE [Wang et al.,\n2019a] - 0.364 0.272 0.400 0.549 - 0.484 0.450 0.496 0.553\nDescription-based Knowledge Embeddings\nPretrain-KGETransE [Zhang et al.,\n2020b] 162 0.332 - - 0.529 1747 0.235 - - 0.557\nKG-BERT [Yao et al.\n, 2019]♣ 153 - - - 0.420 97 0.216 0.041 0.302 0.524\nStARBERT-base [Wang et\nal., 2020] 136 0.263 0.171 0.287 0.452 99 0.364 0.222 0.436 0.647\nMEM-KGCBERT-base(w/o EP) - 0.339 0.249 0.372 0.522 - 0.533 0.473 0.570 0.636\nMEM-KGCBERT-base(w/ EP) - 0.346 0.253 0.381 0.531 - 0.557 0.475 0.604 0.704\nC-LMKEBERT-tiny 132 0.406 0.319 0.445 0.571 148 0.545 0.467 0.587 0.692\nC-LMKEBERT-base 183 0.404 0.324 0.439 0.556 72 0.598 0.480 0.675 0.806\nTable 2: Results of link prediction on FB15k-237 and WN18RR. ♠ denotes results from [Sun et al., 2019]. ♣ denotes results from [Wang\net al., 2020]. We implement StAR on FB15k-237 with BERT-base as the base model. Other results are taken from their original papers. EP\ndenotes the entity prediction task of MEM-KGC. C-LMKE denotes contrastive LMKE.\nand descriptions from Wikipedia for FB13, from [Xie et al.,\n2016] for FB15k-237, and from [Yao et al., 2019] for UMLS.\nThe relation descriptions are their names for all datasets.\nDataset #Entity #Relation #T\nrain #Dev #Test Avg DL\nFB13 75,043 13 316,232\n5,908 23,733 110.7\nFB15k-237 14,541 237 272,115 17,535 20,466 141.7\nUMLS 135 46 5,216 652 661 161.7\nWN18RR 40,943 11 86,835 3,034 3,134 14.4\nTable 3: Statistics of the datasets. Avg DL means the average length\n(number of words) of descriptions.\nBaselines. We compare our methods with structure-based\nand description-based methods. The structure-based methods\ninclude TransE, TransH, TransR, DistMult, ComplEx, Ro-\ntatE. The description-based methods include Pretrain-KGE,\nKG-BERT, exBERT, and StAR. For a fair comparison, we\nreimplement StAR on FB15k-237 with BERT-base.\nMetrics. For triple classification, we report accuracy. For\nlink prediction, we report Mean Rank (MR), Mean Recipro-\ncal Rank (MRR), and Hits@{1, 3, 10} in the “filtered” set-\nting. Metrics for link prediction are based on the rank of the\ncorrect entity in a list of all entities ranked by their plausibil-\nity. The “filtered” setting is a common practice that removes\nother correct entities (which also constitute triples existing in\nthe KG) from the list. Hits@K measures the proportion of\ncorrect entities ranked in the top K. The results are averaged\nover test triples and over predicting missing head and tail en-\ntities. Generally, a good model is expected to achieve higher\nMRR, Hits@N, and lower MR.\nSettings. We evaluate LMKE on triple classification and\nC-LMKE on link prediction with BERT-base [Devlin et al.,\n2018] and BERT-tiny [Turc et al., 2019 ] as the language\nmodel. Larger models are not considered, in which case we\nhave to use a tiny batch size. We search these hyperparame-\nters with BERT-tiny: learning rate of PLMs among {10−4,\n5×10−5, 10 −5}, learning rate of other components among\n{10−3, 5×10−4, 10−4, 10−5}, batch size among {12, 16, 32,\n64} based on best Hits@10 on the dev set. With BERT-base,\nwe set the batch size as 16 for triple classification and 12 for\nlink prediction, which considers both results of BERT-tiny\nand limited memory. Our models are fine-tuned with Adam\nas the optimizer. For triple classification, we sample 1 nega-\ntive triple for each positive triple. For link prediction, we en-\ncode given entity-relation pair queries and target entity keys\nwith two language models. They are equally initialized and\nshare the same embeddings of words, entities, and relations.\n4.2 General Performance\nWe compare our methods with prior approaches on both link\nprediction and triple classification. Experimental results in\nTable 2 and Table 4 show that our methods achieve the state-\nof-the-art performance on both tasks.\nResults of link prediction on FB15k-237 and WN18RR are\ndemonstrated in Table 2, which show that our method sig-\nnificantly outperforms existing approaches. C-LMKE with\nBERT-base achieves surpassing performance on MR, MRR\nand Hits@{1, 3, 10 } on both datasets. The improvement is\nmore significant on WN18RR, where Hits@10 of our method\nreaches to 0.806 while the best result of previous methods is\n0.704 from MEM-KGC. However, its entity prediction task\nis compatible with our work, without which its Hits@10 de-\nclines to 0.636. Our method is also the first description-based\none to outperform state-of-the-art structure-based methods\non FB15k-237. Even with BERT-tiny, our method achieves\nbetter or comparable performance compared with prior ap-\nproaches built with larger models.\nThe results suggest that description-based methods largely\noutperform structure-based ones on WN18RR, but barely sur-\npass them on FB15k-237. We analyze the difference between\nthese datasets to explain this phenomenon. FB15k-237 differs\nfrom WN18RR mainly in two aspects: sparsity and descrip-\ntion. According to statistics shown in Table 3, the average\ndegree on FB15k-237 and WN18RR is 37.4 and 4.2 respec-\ntively. The former is about 8.9 times the latter, which indi-\ncates that entities in FB15k-237 generally have access to rich\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2295\nstructural information while entities in WN18RR are more\nlikely to be long-tail. Also, textual information better covers\nstructural information on WN18RR compared with FB15k-\n237. Entities on WN18RR are words, and descriptions are\nexactly their definitions from which structural relations are\nderived, so descriptions can ideally support the inference of\nstructural relations. However, entities on FB15k-237 are real-\nworld entities whose collected descriptions only partially sup-\nport the inference. For example, the fact (Albert Einstein,\nisA, peace activist) is not covered by collected descriptions of\nthese entities. As a result, the overreliance on textual informa-\ntion may hurt performance, and description-based methods\ndid not achieve surpassing performance. This also explains\nwhy replacing BERT-base in C-LMKE with BERT-tiny does\nnot decrease the performance.\nDataset FB13 UMLS\nTransE [Bordes et al.,\n2013] 81.5 78.1\nTransH [Wang et al.\n, 2014] 83.3 79.2\nTransR [Lin et al.,\n2015] 82.5 81.9\nDistMult [Yang et al.,\n2014] 86.2 86.8\nKG-BERT [Yao et al.\n, 2019] 90.4 89.7\nexBERT [Yaser\nJaradeh et al., 2021] - 90.3\nLMKEBERT-base 91.7 92.4\nTable 4: Accuracy of triple classification on FB13 and UMLS. Re-\nsults of existing baselines on FB13 and UMLS are taken from [Yao\net al., 2019] and [Yaser Jaradeh et al., 2021] respectively.\nResults of triple classification on FB13 and UMLS in Table\n4 show that LMKE also outperforms existing methods on this\ntask. Comparison between results of LMKE and KG-BERT\ndemonstrates the effectiveness of learning embeddings of en-\ntity and relation tokens in the same space as word tokens.\n4.3 Performance Grouped by Entity Degrees\nTo show the effectiveness of our methods on long-tail enti-\nties, we group entities by the logarithm of degrees, collect\nrelevant triples for each group, and study the average link pre-\ndiction performance of different methods on different groups.\nA triple ( h, r, t) is relevant to group i if h or t is in group\ni. The grouping rule is the same as in Fig.1. The results\non FB15k-237 in Fig.4 show that description-based methods\nsignificantly outperform structure-based ones on long-tail en-\ntities in group 0, 1 and 2 (whose degrees are lower than 4),\nand our C-LMKE significantly surpasses other description-\nbased ones. Comparison between results of C-LMKE with or\nwithout degree information indicates that introducing degree\ninformation generally improves its performance on entities\n0 1 2 3 4 5 6 7 8 9 10 11 12 13\nGroup of Entity Degrees\n0.0\n0.2\n0.4\n0.6\n0.8Hits@1\nTransE\nRotatE\nMEM-KGC\nStAR\nC-LMKE(w/o deg)\nC-LMKE\n0 1 2 3 4 5 6 7 8 9 10 11 12 13\nGroup of Entity Degrees\n0.0\n0.2\n0.4\n0.6\n0.8Hits@10\nTransE\nRotatE\nMEM-KGC\nStAR\nC-LMKE(w/o deg)\nC-LMKE\nFigure 4: Average Hits@1 and Hits@10 performance grouped by\nthe logarithm of entity degrees on FB15k-237.\nthat are not long-tail. On popular entities, however, structure-\nbased methods generally perform better. Although StAR is\nalso description-based, it achieves the best Hits@10 on group\n12 and 13, because it is trained with an additional objective\nthat follows structure-based methods.\n4.4 Importance of Negative Sampling Size\nWe study the performance of C-LMKE with different nega-\ntive sampling sizes Nneg to justify its importance. We set the\nbatch size as 32 and limit Nneg for each triple by only using\nseveral encoded target entities of other triples in the batch as\nnegative keys. We report Hits@10 of C-LMKE with BERT-\ntiny on FB15k-237 for 40 epochs. The results shown in Fig.\n5 indicate that larger Nneg continuously brings better perfor-\nmance until convergence. Increasing Nneg greatly accelerates\ntraining, and improves the eventual performance whenNneg is\nunder 8. However, prior description-based methods generally\nset Nneg as only 1 or 5, which limits their performance.\n0 10 20 30 40\nEpoch\n20\n30\n40\n50\n60Hits@10 Nneg=1\nNneg=2\nNneg=4\nNneg=8\nNneg=16\nNneg=31\nFigure 5: Hits@10 of C-LMKE with BERT-tiny on FB15k-237 with\ndifferent negative sampling size.\n5 Conclusion\nIn this paper, we propose LMKE, an effective and efficient\nmethod to adopt language models as knowledge embeddings.\nMotivated by the inability of structure-based KEs to well rep-\nresent long-tail entities, LMKE leverages textual descriptions\nand learns embeddings of entities and relations in the same\nspace as word tokens. It solves the restrictive demand of prior\ndescription-based approaches. A contrastive learning frame-\nwork is further proposed that allows zero-cost negative sam-\npling and significantly downgrades time complexity in both\ntraining and evaluation. Results of extensive experiments\nshow that our methods achieve state-of-the-art performance\non various benchmarks, especially for long-tail entities.\nIn the future, we plan to explore the effectiveness of more\nadvanced contrastive learning approaches in description-\nbased KEs. We are also interested in language models’ ca-\npacity of modeling composition patterns in KGs.\nAcknowledgements\nThis work was supported by National Key Research\nand Development Project (No.2020AAA0109302), Shang-\nhai Science and Technology Innovation Action Plan\n(No.19511120400), Shanghai Municipal Science and Tech-\nnology Major Project (No.2021SHZDZX0103), China Post-\ndoctoral Science Foundation (No. 2020M681173 and No.\n2021T140124), and National Natural Science Foundation of\nChina (Grant No. 62102095).\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2296\nReferences\n[Balaˇzevi´c et al., 2019] Ivana Bala ˇzevi´c, Carl Allen, and\nTimothy M Hospedales. Tucker: Tensor factoriza-\ntion for knowledge graph completion. arXiv preprint\narXiv:1901.09590, 2019.\n[Bordes et al., 2013] Antoine Bordes, Nicolas Usunier,\nAlberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko. Translating embeddings for modeling\nmulti-relational data. In Proc. of NeurIPS, 2013.\n[Choi et al., 2021] Bonggeun Choi, Daesik Jang, and\nYoungjoong Ko. Mem-kgc: Masked entity model for\nknowledge graph completion with pre-trained language\nmodel. IEEE Access, 2021.\n[Dettmers et al., 2018] Tim Dettmers, Pasquale Minervini,\nPontus Stenetorp, and Sebastian Riedel. Convolutional 2d\nknowledge graph embeddings. In Proc. of AAAI, 2018.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Kassner et al., 2020] Nora Kassner, Benno Krojer, and Hin-\nrich Sch ¨utze. Are pretrained language models symbolic\nreasoners over knowledge? In Proc. of CoNLL, 2020.\n[Lin et al., 2015] Yankai Lin, Zhiyuan Liu, Maosong Sun,\nYang Liu, and Xuan Zhu. Learning entity and relation\nembeddings for knowledge graph completion. In Proc. of\nAAAI, 2015.\n[Miller, 1995] George A Miller. Wordnet: a lexical database\nfor english. Communications of the ACM, 1995.\n[Nickel et al., 2011] Maximilian Nickel, V olker Tresp, and\nHans-Peter Kriegel. A three-way model for collective\nlearning on multi-relational data. In Icml, 2011.\n[Pei et al., 2019] Shichao Pei, Lu Yu, Robert Hoehndorf, and\nXiangliang Zhang. Semi-supervised entity alignment via\nknowledge graph embedding with awareness of degree dif-\nference. In Proc. of WWW, 2019.\n[Petroni et al., 2019] Fabio Petroni, Tim Rockt ¨aschel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. Language models as\nknowledge bases? arXiv preprint arXiv:1909.01066,\n2019.\n[Socher et al., 2013] Richard Socher, Danqi Chen, Christo-\npher D Manning, and Andrew Ng. Reasoning with neural\ntensor networks for knowledge base completion. In Ad-\nvances in neural information processing systems, 2013.\n[Sun et al., 2019] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun\nNie, and Jian Tang. Rotate: Knowledge graph embedding\nby relational rotation in complex space. arXiv preprint\narXiv:1902.10197, 2019.\n[Toutanova, 2015] Kristina Toutanova. Observed versus la-\ntent features for knowledge base and text inference. ACL-\nIJCNLP 2015, page 57, 2015.\n[Trouillon et al., 2016] Th´eo Trouillon, Johannes Welbl, Se-\nbastian Riedel, ´Eric Gaussier, and Guillaume Bouchard.\nComplex embeddings for simple link prediction. In Proc.\nof ICML, 2016.\n[Turc et al., 2019] Iulia Turc, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. Well-read students learn better:\nOn the importance of pre-training compact models. arXiv\npreprint arXiv:1908.08962, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, 2017.\n[Wang et al., 2014] Zhen Wang, Jianwen Zhang, Jianlin\nFeng, and Zheng Chen. Knowledge graph embedding by\ntranslating on hyperplanes. In Proc. of AAAI, 2014.\n[Wang et al., 2017] Quan Wang, Zhendong Mao, Bin Wang,\nand Li Guo. Knowledge graph embedding: A survey of ap-\nproaches and applications. IEEE Transactions on Knowl-\nedge and Data Engineering, 2017.\n[Wang et al., 2019a] Quan Wang, Pingping Huang, Haifeng\nWang, Songtai Dai, Wenbin Jiang, Jing Liu, Yajuan Lyu,\nYong Zhu, and Hua Wu. Coke: Contextualized knowl-\nedge graph embedding. arXiv preprint arXiv:1911.02168,\n2019.\n[Wang et al., 2019b] Xiaozhi Wang, Tianyu Gao, Zhaocheng\nZhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. Kepler: A uni-\nfied model for knowledge embedding and pre-trained lan-\nguage representation. arXiv preprint arXiv:1911.06136,\n2019.\n[Wang et al., 2020] Bo Wang, Tao Shen, Guodong Long,\nTianyi Zhou, and Yi Chang. Structure-augmented text rep-\nresentation learning for efficient knowledge graph comple-\ntion. arXiv preprint arXiv:2004.14781, 2020.\n[Xie et al., 2016] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo\nLuan, and Maosong Sun. Representation learning of\nknowledge graphs with entity descriptions. In Proc. of\nAAAI, 2016.\n[Yang et al., 2014] Bishan Yang, Wen-tau Yih, Xiaodong\nHe, Jianfeng Gao, and Li Deng. Embedding entities and\nrelations for learning and inference in knowledge bases.\narXiv preprint arXiv:1412.6575, 2014.\n[Yao et al., 2019] Liang Yao, Chengsheng Mao, and Yuan\nLuo. Kg-bert: Bert for knowledge graph completion.\narXiv preprint arXiv:1909.03193, 2019.\n[Yaser Jaradeh et al., 2021] Mohamad Yaser Jaradeh,\nKuldeep Singh, Markus Stocker, and S ¨oren Auer. Triple\nclassification for scholarly knowledge graph completion.\narXiv preprint arXiv:2111.11845, 2021.\n[Zhang et al., 2020a] Zhanqiu Zhang, Jianyu Cai, Yongdong\nZhang, and Jie Wang. Learning hierarchy-aware knowl-\nedge graph embeddings for link prediction. In Proc. of\nAAAI, 2020.\n[Zhang et al., 2020b] Zhiyuan Zhang, Xiaoqian Liu,\nYi Zhang, Qi Su, Xu Sun, and Bin He. Pretrain-kge:\nLearning knowledge representation from pretrained\nlanguage models. In Proc. of EMNLP, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n2297",
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ]
}