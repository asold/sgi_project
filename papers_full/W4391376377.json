{
  "title": "SCTransNet: Spatial-Channel Cross Transformer Network for Infrared Small Target Detection",
  "url": "https://openalex.org/W4391376377",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1909946215",
      "name": "Yuan Shuai",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2226379185",
      "name": "Qin Hanlin",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2007010312",
      "name": "Yan Xiang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2715660432",
      "name": "Akhtar Naveed",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2744430004",
      "name": "Mian, Ajmal",
      "affiliations": [
        "University of Western Australia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3088317060",
    "https://openalex.org/W4312505779",
    "https://openalex.org/W4318765496",
    "https://openalex.org/W4386076367",
    "https://openalex.org/W2006851788",
    "https://openalex.org/W2158479002",
    "https://openalex.org/W2041560658",
    "https://openalex.org/W2051185466",
    "https://openalex.org/W1978993121",
    "https://openalex.org/W2981068491",
    "https://openalex.org/W3010079414",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W4205138939",
    "https://openalex.org/W3118249006",
    "https://openalex.org/W3171950886",
    "https://openalex.org/W4313506322",
    "https://openalex.org/W4313855947",
    "https://openalex.org/W3118934234",
    "https://openalex.org/W4317794940",
    "https://openalex.org/W4377710514",
    "https://openalex.org/W4304092528",
    "https://openalex.org/W3124866053",
    "https://openalex.org/W4285137940",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W4386634626",
    "https://openalex.org/W4308193099",
    "https://openalex.org/W3197957534",
    "https://openalex.org/W4313827740",
    "https://openalex.org/W4226042736",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W4386075800",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W4285804308",
    "https://openalex.org/W4285259796",
    "https://openalex.org/W3194894013",
    "https://openalex.org/W4286634289",
    "https://openalex.org/W3154206785",
    "https://openalex.org/W4282972322",
    "https://openalex.org/W4313855945",
    "https://openalex.org/W4313065862",
    "https://openalex.org/W4304084055",
    "https://openalex.org/W4386158776",
    "https://openalex.org/W6801762145",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W4286630479",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312827004",
    "https://openalex.org/W4384519318",
    "https://openalex.org/W4311970716",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W4312820606",
    "https://openalex.org/W1992873714",
    "https://openalex.org/W3039443125",
    "https://openalex.org/W2991758591",
    "https://openalex.org/W2912919760",
    "https://openalex.org/W4226043641",
    "https://openalex.org/W6724804524",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W6861715728"
  ],
  "abstract": "Infrared small target detection (IRSTD) has recently benefitted greatly from\\nU-shaped neural models. However, largely overlooking effective global\\ninformation modeling, existing techniques struggle when the target has high\\nsimilarities with the background. We present a Spatial-channel Cross\\nTransformer Network (SCTransNet) that leverages spatial-channel cross\\ntransformer blocks (SCTBs) on top of long-range skip connections to address the\\naforementioned challenge. In the proposed SCTBs, the outputs of all encoders\\nare interacted with cross transformer to generate mixed features, which are\\nredistributed to all decoders to effectively reinforce semantic differences\\nbetween the target and clutter at full scales. Specifically, SCTB contains the\\nfollowing two key elements: (a) spatial-embedded single-head channel-cross\\nattention (SSCA) for exchanging local spatial features and full-level global\\nchannel information to eliminate ambiguity among the encoders and facilitate\\nhigh-level semantic associations of the images, and (b) a complementary\\nfeed-forward network (CFN) for enhancing the feature discriminability via a\\nmulti-scale strategy and cross-spatial-channel information interaction to\\npromote beneficial information transfer. Our SCTransNet effectively encodes the\\nsemantic differences between targets and backgrounds to boost its internal\\nrepresentation for detecting small infrared targets accurately. Extensive\\nexperiments on three public datasets, NUDT-SIRST, NUAA-SIRST, and IRSTD-1k,\\ndemonstrate that the proposed SCTransNet outperforms existing IRSTD methods.\\nOur code will be made public at https://github.com/xdFai.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nSCTransNet: Spatial-channel Cross Transformer\nNetwork for Infrared Small Target Detection\nShuai Yuan Student Member, IEEE, Hanlin Qin Member, IEEE, Xiang Yan Member, IEEE, Naveed Akhtar Mem-\nber, IEEE, Ajmal Mian Senior Member, IEEE\nAbstract—This is the pre-acceptance version, to read the final\nversion please go to IEEE TRANSACTION ON GEOSCIENCE\nAND REMOTE SENSING on IEEE Xplore. Infrared small\ntarget detection (IRSTD) has recently benefitted greatly from\nU-shaped neural models. However, largely overlooking e ffective\nglobal information modeling, existing techniques struggle when\nthe target has high similarities with the background. We present a\nSpatial-channel Cross Transformer Network (SCTransNet) that\nleverages spatial-channel cross transformer blocks (SCTBs) on\ntop of long-range skip connections to address the aforementioned\nchallenge. In the proposed SCTBs, the outputs of all encoders\nare interacted with cross transformer to generate mixed features,\nwhich are redistributed to all decoders to e ffectively reinforce\nsemantic differences between the target and clutter at full levels.\nSpecifically, SCTB contains the following two key elements: (a)\nspatial-embedded single-head channel-cross attention (SSCA) for\nexchanging local spatial features and full-level global channel\ninformation to eliminate ambiguity among the encoders and facil-\nitate high-level semantic associations of the images, and (b) a com-\nplementary feed-forward network (CFN) for enhancing the fea-\nture discriminability via a multi-scale strategy and cross-spatial-\nchannel information interaction to promote beneficial informa-\ntion transfer. Our SCTransNet e ffectively encodes the semantic\ndifferences between targets and backgrounds to boost its internal\nrepresentation for detecting small infrared targets accurately.\nExtensive experiments on three public datasets, NUDT-SIRST,\nNUAA-SIRST, and IRSTD-1K, demonstrate that the proposed\nSCTransNet outperforms existing IRSTD methods. Our code will\nbe made public at https: //github.com/xdFai/SCTransNet.\nIndex Terms—Infrared small target detection, transformer,\ncross attention, CNN, deep learning.\nI. I ntroduction\nThis work was supported in part by the Shaanxi Province Key Research\nand Development Plan Project under Grant 2022JBGS2-09, in part by the\n111 Project under Grant B17035, in part by the Shaanxi Province Science\nand Technology Plan Project under Grant 2023KXJ-170, in part by the Xian\nCity Science and Technology Plan Project under Grant 21JBGSZ-QCY9-\n0004, Grant 23ZDCYJSGG0011-2023, Grant 22JBGS-QCY4-0006, and Grant\n23GBGS0001, in part by the Aeronautical Science Foundation of China\nunder Grant 20230024081027, in part by the Natural Science Foundation\nExplore of Zhejiang province under Grant LTGG24F010001, in part by the\nNatural Science Foundation of Ningbo under Grant 2022J185, in part by the\nChina scholarship council 202306960052, in part by the Technology Area\nFoundation of China 2021-JJ-1244, 2021-JJ-0471, 2023-JJ-0148, and part\nby the Xidian Graduate Student Innovation fund under Grant YJSJ23010.\n(Corresponding authors: Hanlin Qin, Xiang Yan.)\nShuai Yuan, Hanlin Qin, and Xiang Yan are with the School of Op-\ntoelectronic Engineering, Xidian University, Xi’an 710071, China. (email:\nyuansy@stu.xidian.edu.cn; hlqin@mail.xidian.edu.cn; xyan@xidian.edu.cn)\nNaveed Akhtar is with the School of Computing and Information Systems,\nFaculty of Engineering and IT, The University of Melbourne, Parkville VIC\n3052, Australia (email: naveed.akhtar1@unimelb.edu.au).\nAjmal Mian is with the Department of Computer Science and Software\nEngineering, The University of Western Australia, Perth, 6009, Australia\n(email: ajmal.mian@uwa.edu.au).\nI\nNFRARED small target detection (IRSTD) plays an impor-\ntant role in tra ffic monitoring [1], maritime rescue [2], and\ntarget warning [3], where separating small targets in complex\nscene backgrounds is required. The challenges emerging from\nthe dynamic nature of scenes have attracted considerable\nresearch attention in single-frame IRSTD [4]. Early methods in\nthis direction employed image filtering [5], [6], human visual\nsystem (HVS) [7], [8], and low-rank approximation [9], [10]\ntechniques while relying on complex handcrafted feature\ndesigns, empirical observations, and model parameter fine-\ntuning. However, suffering from the absence of a reliable high-\nlevel understanding of the holistic scene, these methods exhibit\npoor robustness.\nRecently, learning-based methods have become more popu-\nlar due to their strong data-driven feature mining abilities [11].\nTo capture the target’s outlines and mitigate performance\ndegradation caused by its small size, these methods approach\nthe IRSTD problem as a semantic segmentation task instead\nof a traditional object detection issue. Unlike general object\nsegmentation in autonomous driving [12], imaging mechanism\nof the IR detection systems in remote sensing applications [13]\nleads to small targets in images exhibiting the following\ncharacteristics. 1) Dim and small: Due to remote imaging,\nIR targets are small and usually exhibit a low signal-to-clutter\nratio, making them susceptible to immersion in heavy noise\nand background clutter. 2) Characterless: Thermal images\nlack color and texture information in targets, and imprecise\ncamera focus can cause target blurring. These factors pose\npeculiar challenges in designing feature extraction techniques\nfor IRSTD. 3) Uncertain shapes: The scales and shapes of IR\ntargets vary significantly across di fferent scenes, which makes\nthe problem of detection considerably challenging.\nTo identify small IR targets in complex backgrounds, nu-\nmerous learning-based methods have been proposed, among\nwhich neural networks with U-shaped architectures have\ngained prominence. Benefiting from these frameworks of\nencoders, decoders, and long-range skip connections, asym-\nmetric contextual modulation (ACM) network [14] initially\ndemonstrated the e ffectiveness of cross-layer feature fusion\nfor retaining IR target features. This is achieved through\nbidirectional aggregation of high-level semantic information\nand low-level details using asymmetric top-down and bottom-\nup structures. Subsequently, feature fusion strategies have\nbeen widely adopted in IRSTD task [18], [19], [20], [21].\nA few recent methods facilitate the transfer of beneficial\nfeatures to the decoder component by improving the skip\nconnections [22], [23]. Inspired by the nested structure [24],\narXiv:2401.15583v3  [cs.CV]  30 Apr 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nFig. 1. The framework and visualization maps of representative IRSTD methods, with each method’s frame labeled according to the specific challenge it\naddresses. The visualization maps show that the CNN-based approaches (ACM [14], DNA-Net [15], and UIU-Net [16]) focus on modeling the local information\nof the target and less on establishing the global semantic information of the image; Mixer CNN and transformer methods (MTU-Net [17] and our SCTransNet)\npay more attention to the background global information and target semantics. Only our method meticulously models buildings and the sky separately in the\nhigh-level semantic map, e ffectively distinguishing the target from the background, reducing false alarms.\nDNA-Net [15] developed a densely nested interactive module\nto facilitate gradual interaction between high- and low-level\nfeatures and adaptively enhance features. Moreover, there\nare also approaches that focus on developing more e ffective\nencoders and decoders [25], [26]. For instance, UIU-Net [16]\nembeds smaller U-Nets in the U-Net to learn the local con-\ntrast information of the target and perform interactive-cross\nattention (IC-A) for feature fusion.\nDespite achieving satisfactory results, the aforementioned\nCNN-based approaches lack the ability to encode compre-\nhensive attributes of the target, missing their discriminative\nfeatures. To address that, MTU-Net [17] employs a multilevel\nVision Transformer (ViT)-CNN hybrid encoder to exploit the\nspatial correlation among all encoded features for contextual\ninformation aggregation. However, a simple spatial ViT-CNN\nhybrid module is insu fficient for understanding the global\nsemantics of images, which makes high false alarms. To\nfurther dissect the issue, we illustrate the frameworks of\nACM [14], DNA-Net [15], UIU-Net [16], and MTU-Net [17]\nseparately, along with visualizations of the attention maps\nfrom different decoder levels in Fig. 1(c)-(f). Given the input\nimage in Fig. 1(b), we observe that false alarms occur when\nexisting models direct their attention to localized regions of\nbackground clutters in high-level features. In other words,\nfalse alarms are often caused by discontinuity modeling of\nbackgrounds in the deeper layers. We identify this problem to\nthe following three main reasons:\n1) Semantic interaction across feature levels is not\nestablished well . As shown in Fig. 1(a) ①, IR small targets\nexhibit limited features owing to their diminutive size. Mul-\ntiple downsampling processes inevitably result in the loss of\nspatial information. This considerably affects the level-to-level\nfeature interactions in the network, eventually leading to poor\ncomprehensive global semantic information encoding.\n2) Feature enhancement fails to bridge the informa-\ntion gap between encoders and decoders . As shown in\nFig. 1(a) ②, there exists a semantic gap between the output\nfeatures of encoders and the input features of the decoders.\nSimple skip connections and dense nested modules are insuf-\nficient to enhance the advantageous responses of the features\nto the decoder, thereby making it challenging to establish a\nmapping relationship from the IR image to the segmentation\nspace.\n3) Inaccurate long-range contextual perception of targets\nand backgrounds in deeper layers . IR small targets can\nbe highly similar to the scene background. As shown in\nFig. 1(a)③, a powerful detector not only has to sense the local\nsaliency of the target but also needs to model the continuity of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nthe background. Convolutional Neural Networks (CNNs) and\nvanilla ViTs are not fully equipped to achieve this.\nInspired by the success of channel-wise cross fusion trans-\nformer in image segmentation [27], [28], [29] and local spatial\nembedding in image restoration [30], [31], [32], we propose\na spatial-channel cross transformer network (SCTransNet) for\nIRSTD to address the above challenges, aiming to distinguish\nthe small targets and background clutters in deeper layers. As\nillustrated in Fig. 1(g), our framework adds multiple spatial-\nchannel cross transformer blocks (SCTB) (Sec. III-B) on the\noriginal skip connections to establish an explicit association\nwith all encoders and decoders. Specifically, SCTB consists\nof two components: Spatial-embedded single-head channel-\ncross attention (SSCA) (Sec. III-B1) and complementary feed-\nforward network (CFN) (Sec. III-B2).\nThe SSCA applies channel cross-attention from the feature\ndimension at all levels to learn global information. Besides,\ndepth-wise convolutions are used for local spatial context\nmixing before feature covariance computation. This strategy\nprovides two advantages: Firstly, it highlights the context\nof local space with a small computational overhead using\nthe convolution’s local connectivity, thereby increasing the\nsaliency of IR small targets. Secondly, it makes sure that\ncontextualized global relationships among full-level feature\npixels are implicitly captured during the attention matrix com-\nputation, thereby reinforcing the continuity of the background.\nAfter the SSCA completes the cross-level information in-\nteraction, CFN performs feature enhancement at every level\nin two complementary stages. Initially, it utilizes multi-scale\ndepth-wise convolutions to enhance target neighborhood space\nresponse and pixel-wise aggregates the cross-channel nonlin-\near information. Subsequently, it estimates total spatial infor-\nmation on a channel-by-channel basis using global average\npooling and creates local cross-channel interactions between\ndistinct semantic patterns as an attention map. The above\nstrategy has two advantages. (1) Multi-scale spatial modeling\ncan emphasize semantic di fferences between the target and\nbackground. (2) Establishing the complementary correlation of\nthe local space global channel (LSGC) and the global space\nlocal channel (GSLC) can facilitate the interface between\ninfrared images and semantic maps.\nBenefiting from the above structure (Fig. 1(g)), our SC-\nTransNet can perceive the image semantics better than other\nmethods leading to reduced false alarms. Our main contribu-\ntions are as follows:\n• We propose SCTransNet, which leverages multiple\nspatial-channel cross transformer blocks (SCTB) connect-\ning all encoders and decoders to predict the context of\ntargets and backgrounds in the deeper network layers.\n• We propose a spatial-embedded single-head channel-cross\nattention (SSCA) module to foster semantic interactions\nacross all feature levels and learn the long-range context\ncorrelation of the image.\n• We devise a novel complementary feed-forward network\n(CFN) by crossing spatial-channel information to enhance\nthe semantic di fference between the target and back-\nground, bridging the semantic gap between encoders and\ndecoders.\nII. RELATED WORK\nWe first briefly review the CNN- and transformer-based\ntechniques in IRSTD. Following that, we discuss the appli-\ncation of channel-wise cross transformer in image processing.\nA. CNN-based IRSTD methods\nOwing to the local saliency of IR small targets coinciding\nwith the local connectivity of convolution neural networks\n(CNNs), CNNs have demonstrated remarkable performance\nin the IRSTD task. To e ffectively preserve the semantic\npatterns of small targets, diverse feature fusion strategies\nhave been proposed. One common strategy is cross-layer\nfeature fusion [33], [34], [35], which can address the loss\nof target information when fusing the encoded and decoded\nfeatures. Additionally, densely nested interactive feature fu-\nsion [15], [36] is used to repetitively fuse and enhance the\nfeatures of di fferent levels, maintaining the information of IR\nsmall targets in the deeper layers. Considering variations in\ntarget scales, multi-scale feature fusion [37], [38] has been\nproposed to enhance the low-resolution feature maps. Besides\nfeature fusion, incorporating prior information about the target\ninto CNNs is also an e ffective strategy. For instance, Sun\net al. [39] exploited the small-target gray gradient change\nproperty using a receptive-field and direction-induced attention\nnetwork (RDIAN), which solves the imbalance between the\ntarget and background classes. Zhang et al. [40] used Taylor’s\nfinite difference for complex edge feature extraction of a target\nto enhance the target and background gray scale di fference.\nAlthough satisfactory results are achieved by CNN-based\ntechniques, the inherent inductive bias of CNNs makes it\ndifficult to unambiguously establish long-range contextual\ninformation for the IRSTD task. Unlike the aforementioned\nmethods, we incorporate transformer blocks into the backbone\nof CNNs as a core unit to capture non-local information for\nthe entire image.\nB. Transformer-based IRSTD methods\nVision Transformer (ViT) [41] decomposes an im-\nage/features into a series of patches and computes their\ncorrelation. This computational paradigm can stably establish\nlong-distance dependence among di fferent patches, leading\nto its widespread usage in IRSTD tasks for global image\nmodeling [42], [43], [44]. Inspired by TransUnet [45], IRST-\nFormer [46] embedded the spatial transformer within mul-\ntiple encoder stages in a U-Net. Motivated by Swin trans-\nformer [47], FTC-Net [48] establishes a robust feature repre-\nsentation of the target using a two-branch structure combining\nthe local feature extraction of CNNs and the global feature\nextraction capability of the Swin transformer. Recently, Meng\net al. [49] modeled the local gradient information of the target\nusing central di fference convolution and employed criss-cross\nmulti-attention [50] to acquire contextual information. Note\nthat, the above methods use spatial self-attention (SA) to\ncalculate covariance-based attention maps, which have two\nproblems: 1) The computational complexity is proportional\nto the square of the number of tokens, which limits the\nmultiple nesting of the spatial transformer and its fine-grained\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nFig. 2. Overview of the proposed SCTransNet for infrared small object detection. Our SCTransNet adopts a U-shaped structure and adds four spatial-channel\ncross transformer blocks (SCTB) on the long-range skip connections, and the multi-scale deeply supervised fusion strategy is used to optimize our SCTransNet.\nrepresentation of high-resolution images [30]. 2) The SA only\nconstructs long-distance dependency for a single feature map,\nwhereas it is more critical to establish contextual connections\namong all levels.\nDifferent from previous works, we present the channel-wise\ncross transformer on the long-range skip connections for the\nfirst time in the IRSTD task. This allows establishing cross-\nchannel semantic patterns across all levels with an acceptable\ncomputational overhead.\nC. Channel-wise Cross Transformer on Image Processing\nUnlike spatial transformers, channel-wise transformers\n(CT) [30] treat each channel as a patch. Note that every\nchannel is a unique semantic pattern, CT essentially establishes\ncorrelations between multiple semantic patterns. Considering\nthat not every skip connection is e ffective, Wang et al. [27]\nproposed UCTransNet, utilizing channel-wise cross fusion\ntransformer (CCT) to address the semantic di fference for\nprecise medical image segmentation. The CCT’s powerful\nglobal semantic modeling capability facilitates its widespread\napplication in tasks such as metal surface defect detection [29],\nremote sensing image segmentation [51], and building edge\ndetection [28]. This inspires us to introduce this model to\nseparate IR targets and backgrounds in the deeper layers\neffectively. However, IR small targets differ significantly from\nthe usual large-size targets not only in size but also in terms\nof effective features and sample balance. The attention matrix\ncomputation, the positional encoding, and the pure channel\nmodeling in vanilla CCT are harmful to the limited-pixel\ntarget detection. Therefore, we propose a spatial-channel cross\ntransformer block. Its launching point is leveraging the target’s\nlocal spatial saliency and global background continuity to\nseparate the target in the deep layers.\nIII. METHOD\nThis section elaborates on the proposed Spatial-channel\nCross Transformer Network (SCTransNet) for infrared small\ntarget detection. We begin by presenting the overall structure\nof the proposed SCTransNet in Section III-A. Then, we present\nthe technical details of the spatial-channel cross transformer\nblock (SCTB) and its internal structure: Spatial-embedded\nsingle-head channel-cross attention (SSCA) and the comple-\nmentary feed-forward network (CFN) in Section III-B.\nA. Overall pipeline\nAs shown in Fig. 2, given an infrared image, SCTransNet\ninitially employs four groups of residual blocks (RBs) [52]\nand max-pooling layers, to acquire high-level features Ei ∈\nRCi×H\ni ×W\ni , ( i = 1,2,3,4). Ci are the channel dimensions,\nin which C1 = 32, C2 = 64, C3 = 128, C4 = 256. Next,\nwe perform patch embedding on Ei using convolution with\nkernel size and stride size of P, P/2, P/4, and P/8 to obtain\nembedded layers Ii ∈RCi×H\n16 ×W\n16 , ( i = 1,2,3,4) respectively.\nThese layers are then fed into the SCTB for full-level semantic\nfeature blending and obtaining the output Oi ∈ RCi×H\n16 ×W\n16 ,\n(i = 1,2,3,4), which have the same size of Ii. Details of SCTB\nare provided in the next Section. The Oi are recovered to the\nsize of the original encoder processing using feature mapping\n(FM), which consists of bilinear interpolation, convolution,\nbatch normalization, and ReLU activation. Meanwhile, we\nemploy a residual connection to merge the features between\nthe encoders and decoders. The process described above can\nbe expressed mathematically as\nOi = Ei + FMi(SCTB(I1,I2,I3,I4)) (i = 1,2,3,4). (1)\nFinally, the Channel-wise Cross Attention (CCA) [27] is\nemployed to fuse the high- and low-level features, followed\nby decoding using two CBL blocks.\nTo enhance the gradient propagation e fficiency and feature\nrepresentation, we utilize a multi-scale deeply supervised\nfusion strategy to optimize SCTransNet. Specifically, a 1 ×1\nconvolution and sigmoid function are used for each decoder\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nFig. 3. The proposed spatial-channel cross transformer block (SCTB), which consists of spatial-embedded single-head channel-cross attention (SSCA) and\ncomplementary feed-forward network (CFN). (a) SSCA establishes image full-scale information association by means of di fferent levels of semantic interaction.\n(b) CFN bridges the semantic gap between encoder and decoder through complementary feature enhancement.\noutputs Fi, acquiring the saliency map Mi which is denoted\nas\nMi = Sigmoid( f1×1(Fi)) (i = 1,2,3,4,5). (2)\nNext, we upsample the low-resolution salient maps Mi (i =\n2,3,4,5) to the original image size and fuse all the salient\nmaps to obtain MP as\nMP = Sigmoid( f1×1[M1,B(M2),B(M3),B(M4),B(M5)]),\n(3)\nwhere [ ·] is the channel-wise concatenation, B denotes the\nbilinear interpolation. Finally, we calculate the Binary Cross\nEntropy (BCE) [16] loss between the overall saliency maps\nand the ground truth (GT) Y as below, and combine the losses.\nl1 = LBCE (M1,Y), (4)\nli = LBCE (B(Mi),Y) (i = 2,3,4,5), (5)\nlP = LBCE (MP,Y), (6)\nL = λ1l1 + λ2l2 + λ3l3 + λ4l4 + λ5l5 + λPlP, (7)\nin which λi (i = 1,2,3,4,5) represents the weights correspond-\ning to di fferent loss functions. In this work, λi and λP are set\nto 1 empirically.\nB. Spatial-channel Cross Transformer Block\nRecently, successful architectures such as MLP-mixer [53]\nand Poolformer [54] have both considered the interaction be-\ntween spatial and channel information in constructing context\ninformation. However, vanilla CCT focuses excessively on\nestablishing channel information and overlooks the crucial role\nof spatial information in neighborhood modeling. To address\nthis, we develop a spatial-channel cross transformer block\n(SCTB) as a spatial-channel blending unit to mix full-level\nencoded features. As shown in Fig. 3, given the i-th level\nfeatures Ii ∈RCi×h×w,(i = 1,2,3,4), in which h = H\n16 ,w = W\n16 .\nthe procedure of SCTB can be defined as\nJP = LN([I1,I2,I3,I4]), (8)\nJi = LN(Ii), (9)\nPi = SSCA(J1,J2,J3,J4,JP) + Ii, (10)\nOi = CFNi(Pi), (11)\nwhere LN denotes the layer normalization, Ji ∈RCi×h×w,(i =\n1,2,3,4) and the concatenated tokens JP ∈RCP×h×w are the\nfive inputs of SSCA, Pi represent the outputs of SSCA, and Oi\nstands for the outputs of SCTB. The SSCA: Spatial-embedded\nsingle-head channel-cross attention; and CFN: Complementary\nfeed-forward network, are separately described below.\n1) Spatial-embedded single-head channel-cross attention:\nIn Fig. 3(a), given the five input tokens Ji and JP for\nwhich LN is performed, the launching point of SSCA is to\ncalculate the local-spatial channel similarity between single-\nlevel features and full-level concatenation features to establish\nglobal semantics. Therefore, our SSCA employs the four input\ntokens Ji as queries, one concatenated token JP as key and\nvalue. This is accomplished by utilizing 1 ×1 convolutions to\nconsolidate pixel-wise cross-channel context and then applying\n3 ×3 depth-wise convolutions to capture local spatial context.\nMathematically,\nQi = WQ\ndi WQ\npiJi, K = WK\nd WK\np JP, V = WV\nd WV\np JP, (12)\nwhere W(·)\npi ∈RCi×1×1 and W(·)\np ∈RCP×1×1 are the 1 ×1 point-\nwise convolution, W(·)\ndi ∈RCi×3×3 and W(·)\nd ∈RCP×3×3 are the\n3×3 depth-wise convolution. Next, we reshape Qi ∈RCi×h×w,\nK ∈RCP×h×w, and V ∈RCP×h×w to RCi×hw, RCP×hw and RCP×hw,\nseparately. Our SSCA process is defined as\nCAi = Wpi CrossAtt(Qi,K,V), (13)\nCrossAtt(Qi,K,V) = AiV = Softmax\n(\nI(Qi KT\nλ )\n)\nV, (14)\nwhere CAi ∈RCi×h×w are the output of SSCA, Ai ∈RCi×CP\nrepresent di fferent level covariance-based attention maps, I\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nTABLE I\nComparisons withSOTAmethods onNUAA-SIRST, NUDT-SIRSTand IRSTD-1K in IoU(%), nIoU (%), F-measure(%), Pd(%), Fa(10−6).\nMethod\nNUAA-SIRST [14] NUDT-SIRST [15] IRSTD-1K [40]\nmIoU nIoU F-measure Pd Fa mIoU nIoU F-measure Pd Fa mIoU nIoU F-measure Pd Fa\nTop-Hat [5] 7.143 18.27 14.63 79.84 1012 20.72 28.98 33.52 78.41 166.7 10.06 7.438 16.02 75.11 1432\nMax-Median [55] 4.172 12.31 10.67 69.20 55.33 4.197 3.674 7.635 58.41 36.89 6.998 3.051 8.152 65.21 59.73\nWSLCM [56] 1.158 6.835 4.812 77.95 5446 2.283 3.865 5.987 56.82 1309 3.452 0.678 2.125 72.44 6619\nTTLCM [57] 1.029 4.099 4.995 79.09 5899 2.176 4.315 7.225 62.01 1608 3.311 0.784 2.186 77.39 6738\nIPI [9] 25.67 50.17 43.65 84.63 16.67 17.76 15.42 26.94 74.49 41.23 27.92 20.46 35.68 81.37 16.18\nPSTNN [58] 30.30 33.67 39.16 72.80 48.99 14.85 23.57 35.63 66.13 44.17 24.57 17.93 37.18 71.99 35.26\nMSLSTIPT [59] 10.30 15.93 18.83 82.13 1131 8.342 10.06 18.26 47.40 888.1 11.43 5.932 12.23 79.03 1524\nACM [14] 68.93 69.18 80.87 91.63 15.23 61.12 64.40 75.87 93.12 55.22 59.23 57.03 74.38 93.27 65.28\nALCNet [18] 70.83 71.05 82.92 94.30 36.15 64.74 67.20 78.59 94.18 34.61 60.60 57.14 75.47 92.98 58.80\nRDIAN [39] 68.72 75.39 81.46 93.54 43.29 76.28 79.14 86.54 95.77 34.56 56.45 59.72 72.14 88.55 26.63\nISTDU [22] 75.52 79.73 86.06 96.58 14.54 89.55 90.48 94.49 97.67 13.44 66.36 63.86 79.58 93.60 53.10\nMTU-Net [17] 74.78 78.27 85.37 93.54 22.36 74.85 77.54 84.47 93.97 46.95 66.11 63.24 79.26 93.27 36.80\nIAANet [60] 74.22 75.58 85.02 93.53 22.70 90.22 92.04 94.88 97.26 8.32 66.25 65.77 78.34 93.15 14.20\nAGPCNet [19] 75.69 76.60 85.26 96.48 14.99 88.87 90.64 93.88 97.20 10.02 66.29 65.23 79.58 92.83 13.12\nDNA-Net [15] 75.80 79.20 86.24 95.82 8.78 88.19 88.58 93.73 98.83 9.00 65.90 66.38 79.44 90.91 12.24\nUIU-Net [16] 76.91 79.99 86.95 95.82 14.13 93.48 93.89 96.63 98.31 7.79 66.15 66.66 79.63 93.98 22.07\nSCTransNet 77.50 81.08 87.32 96.95 13.92 94.09 94.38 96.95 98.62 4.29 68.03 68.15 80.96 93.27 10.74\nFig. 4. Information enhancement from di fferent perspectives: (a) the local\nspatial and global channel (LSGC) paradigms; (b) the global spatial and local\nchannel (GSLC) paradigms. Our CFN integrates both of these information\nenhancement methods internally.\ndenotes the instance normalization operation [61], and λ is\nan optional temperature factor defined by λ= pCP. Notably,\nwe differ from the common channel-cross attention under two\nfurther aspects: Our patches are without positional encoding,\nand we use a single head to learn the attention matrix. These\nstrategies will be compared for their e fficacy in detail in the\nablation study IV-E2.\n2) Complementary Feed-forward Network: As shown in\nFig. 4(a), previous studies [41], [32], [30] always incorporate\nsingle-scale depth-wise convolutions into the standard feed-\nforward network to enhance local focus. More recently, state-\nof-the-art MSFN [31] incorporates two paths with depth-\nwise convolution using di fferent kernel sizes to enhance the\nmulti-scale representation. However, the above approaches\nare limited to a local spatial global channel paradigm of\nfeature representation. In fact, global spatial and local channel\ninformation (Fig. 4(b)) is equally important [62]. Hence, we\ndesign a CFN, which combines the advantages of both feature\nrepresentations.\nIn Fig. 3(b), given an input tensor Xi ∈RCi×h×w, CFN first\nmodels multi-scale LSGC information. Specifically, after the\nlayer normalization, CFN utilizes 1 ×1 convolution to increase\nthe channel dimension in the ratio of η and splits the feature\nTABLE II\nComprehensive evaluation metrics with competitive algorithms.\nModel Params (M) Flops (G) IoU nIoU F-measure\nDNA-Net [15] 4.697 14.26 80.23 82.59 88.60\nUIU-Net [16] 50.54 54.42 82.40 86.12 90.35\nSCTransNet 11.19 20.24 83.43 86.86 90.96\nmap equally into two branches. Subsequently, 3 ×3 and 5 ×5\ndepth-wise convolutions are employed to enhance the local\nspatial information. This is followed by channel concatenating\nthe multi-scale features and restoring them to their original\ndimensions. The above process can be defined as\nX3×3,X5×5 = Chunk( f c\n1×1(LN(Xi))), (15)\nXsc = f c\n1×1[δ( f dwc\n3×3 (X3×3)),δ( f dwc\n5×5 (X5×5))], (16)\nwhere f c\n1×1 denotes 1 ×1 convolution, f dwc\n3×3 and f dwc\n5×5 represent\n3×3 and 5×5 depth-wise convolutions. Here, Chunk(·) denotes\ndividing the feature vector into two equal parts along the\nchannel dimension.\nNext, CFN constructs the GSLC information. Because of\nthe varying resolution of the small target detection image\ninputs in the test stage, we first use the global average pooling\n(GAP) of spatial dimensions to approximate the total spatial\ninformation of the features instead of using computationally\nintensive spatial MLPs to precisely compute the global spatial\ninformation [63]. We then employ a one-dimensional convo-\nlution with a kernel size of 3 to capture the local channel\ninformation of the spatially compressed feature as follows\nXo = f 1D\n3 (GAP2D(Xsc)) ⊙Xsc + Xi, (17)\nwhere ⊙is the broadcasted Hadamard product. By incorpo-\nrating complementary spatial and channel information, CFN\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nFig. 5. ROC curves of di fferent methods on the NUAA-SIRST, NUDT-SIRST, and IRSTD-1K dataset. Our SCTransNet can achieve the highest Pd at very\nlow Fa.\nTABLE III\nThe Area Under Curve (AUC) with different thresholds of theSOTAmethods on theNUAA-SIRST, NUDT-SIRST,and IRSTD-1K datasets.\nDataset Index ACM ALCNet RDIAN ISTDU MTU-Net IAANet AGPCNet DNA-Net UIU-Net SCTransNet\nNUAA-SIRST [14] AUCFa=0.5 0.7223 0.8618 0.5461 0.7515 0.7457 0.8081 0.6953 0.6582 0.4854 0.9539\nAUCFa=1 0.8180 0.9025 0.7321 0.8579 0.8437 0.8614 0.8262 0.8098 0.7197 0.9589\nNUDT-SIRST [15] AUCFa=0.5 0.4392 0.6321 0.4630 0.8635 0.4640 0.7569 0.5038 0.6300 0.8275 0.9853\nAUCFa=1 0.5865 0.7716 0.6695 0.9211 0.6064 0.8463 0.7306 0.8072 0.9013 0.9863\nIRSTD-1K [40] AUCFa=0.5 0.5374 0.6606 0.4545 0.6014 0.5018 0.7862 0.6211 0.6162 0.4749 0.9107\nAUCFa=1 0.7366 0.8006 0.6480 0.7687 0.7198 0.8456 0.7752 0.7684 0.7099 0.9200\nenriches the representation of features in terms of the target’s\nlocalization and the background’s global continuity.\nIV . Experiments andAnalysis\nA. Evaluation metrics\nWe compare the proposed SCTransNet with the state-of-the-\nart (SOTA) methods using several standard metrics.\n1) Intersection over Union (IoU) : IoU is a pixel-level\nevaluation metric defined as\nIoU = Ai\nAu\n=\nPN\ni=1 T P[i]\nPN\ni=1(T[i] + P[i] −T P[i])\n, (18)\nwhere Ai and Au denote the size of the intersection region\nand union region, respectively. N is the number of samples,\nTP[·] denotes the number of true positive pixels, T[·] and P[·]\nrepresent the number of ground truth and predicted positive\npixels, respectively.\n2) Normalized Intersection over Union (nIoU) : nIoU is the\nnormalized version of IoU [14], given as\nnIoU = 1\nN\nNX\ni=1\nT P[i]\nT[i] + P[i] −T P[i]. (19)\n3) F-measure (F): It evaluates the miss detection and false\nalarms at pixel-level, given as\nF = 2 ×Prec ×Rec\nPrec + Rec , (20)\nwhere Prec and Rec denote the precision rate and recall rate\nrespectively.\n4) Probability of Detection (P d): Pd is the ratio of correctly\npredicted targets Npred and all targets Nall, given as\nPd = Npred\nNall\n. (21)\nFollowing [15], if the deviation of target centroid is less than\n3, we consider the target correctly predicted.\n5) False-Alarm Rate (F a): Fa is the ratio of false predicted\ntarget pixels Nf alse and all the pixels in the image Pall, given\nas\nFa = Nf alse\nPall\n. (22)\nIn addition to the fixed-threshold evaluation methods, we\nalso utilize Receiver Operation Characteristics (ROC) curves\nto comprehensively evaluate the models. ROC is used to\ndescribe the changing trends of Pd under varying Fa.\nB. Experiment settings\nDatasets: In our experiments, we utilized three public datasets,\nnamely; NUAA-SIRST [14], NUDT-SIRST [15], and IRSTD-\n1K [40], which consist of 427, 1327, and 1000 images,\nrespectively. We adopt the method used by [15] to partition the\ntraining and test sets of NUAA-SIRST and NUDT-SIRST, and\n[40] for splitting the IRSTD-1K. Hence, all splits are standard.\nImplementation Details: We employ U-Net with four RBs\nas our detection backbone [17], the number of downsampling\nlayers is 4, and the basic width is set to 32. The kernel size\nand stride size P for patch embedding is 16, the number of\nSCTB is 4, and the channel expansion factor η in CFN is\n2.66. Our SCTransNet does not use any pre-trained weights\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nFig. 6. Visual results obtained by di fferent IRSTD methods on the NUAA-SIRST, NUDT-SIRST, and IRSTD-1K datasets. Circles in blue, yellow, and red\nrepresent correctly detected targets, miss detection, and false alarms, respectively.\nfor training, every image undergoes normalization and ran-\ndom cropping into 256 ×256 patches. To avoid over-fitting,\nwe augment the training data through random flipping and\nrotation. We initialized the weights and bias of our model\nusing the Kaiming initialization method [64]. The model is\ntrained using the BCE loss function and optimized by the\nAdam optimizer with the initial learning rate of 0.001, and\nthe learning rate is gradually decreased to 1 ×10−5 using the\nCosine Annealing strategy. The batch size and epoch are set as\n16 and 1000, respectively. Following [14], [18], [15], the fixed\nthreshold to segment the salient map is set to 0.5. The proposed\nSCTransNet is implemented with PyTorch on a single Nvidia\nGeForce 3090 GPU, an Intel Core i7-12700KF CPU, and 32\nGB of memory. The training process took approximately 24\nhours.\nBaselines: To evaluate the performance of our method,\nwe compare SCTransNet to the SOTA IRSTD methods,\nspecifically, seven well-established traditional methods (Top-\nHat[5], Max-Median [55], WSLCM [56], TLLCM [57],\nIPI [9], MSLSTIPT [59]), and nine learning-based meth-\nods (ACM [14], ALCNet [18], RDIAN [39], ISTDU [22],\nIAANet [60], AGPCNet [19], DNA-Net [15], UIU-Net [16],\nand MTU-Net [17]) on the NUAA-SIRST, NUDT-SIRST\nand IRSTD-1K datasets. To guarantee an equitable compar-\nison, we retrained all the learning-based methods using the\nsame training datasets as our SCTransNet, and following\nthe original papers, adopted their fixed thresholds. Open-\nsource implementations of most techniques can be found at\nhttps://github.com/XinyiYing/BasicIRSTD and https: //github.\ncom/xdFai/SCTransNet.\nC. Quantitative Results\nQuantitative results are shown in Table I. In general, the\nlearning-based methods significantly outperform the conven-\ntional algorithms in terms of both target detection accuracy\nand contour prediction of targets. Meanwhile, our method\noutperforms all other algorithms. In the three metrics of\nIoU, nIoU and F-measure, SCTransNet stands considerably\nahead on all three public datasets. This indicates that our\nalgorithm possesses a strong ability to retain target contours\nand can discern pixel-level information di fferences between\nthe target and the background. We also note that even though\nSCTransNet does not obtain optimal Pd and Fa, e.g., DNA-\nNet’s Pd is higher than ours by only 0.2 in the NUDT-\nSIRST, whereas our target detection false alarms are over twice\nas low as DNA-Net’s. This demonstrates that our algorithm\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nFig. 7. 3D visualization of salient maps of di fferent methods on 6 test images.\nachieves a superior balance between false alarms and detection\naccuracy, as indicated by the remarkably high composite\nmetric, F-measure. Next, we comprehensively compare the\npresent algorithm with the most competitive deep learning\nmethods, DNA-Net and UIU-Net. Table II gives the average\nmetrics of the di fferent algorithms on the three data, and we\ncan observe that SCTransNet has acceptable parameters at the\nhighest performance and outperforms the powerful UIU-Net.\nFig. 5 displays the ROC curves of various competitive\nlearning-based algorithms. It is evident that the ROC curve of\nSCTransNet outperforms all other algorithms. For instance, by\nappropriately selecting a segmentation threshold, SCTransNet\nachieves the highest detection accuracy while maintaining the\nlowest false alarms in the NUAA-SIRST and NUDT-SIRST\ndatasets.\nTable III presents the Area Under Curve (AUC) of Fig. 5 in\ntwo di fferent thresholds: Fa = 0.5 ×10−6 and Fa = 1 ×10−6.\nIt can be seen that our method consistently achieves opti-\nmal detection performance across various false alarm rates.\nMeanwhile, while undergoing the same continuous threshold\nchange, the curve of our method is more continuous and\nrounded compared to other methods. This observation suggests\nthat SCTransNet showcases exceptional tunable adaptability.\nD. Visual Results\nThe qualitative results of the seven representative algorithms\nin the NUAA-SIRST, NUDT-SIRST, and IRSTD-1K datasets\nare given in Fig. 6 and Fig. 7. Among them, conventional algo-\nrithms such as Top-Hat and TTLCM frequently yield a high\nnumber of false alarms and missed detections. Furthermore,\neven in cases where the target is detected, its contour is often\nunclear, hindering further accurate identification of the target\ntype. In the learning-based algorithms, our method achieves\nprecise target detection and effective contour segmentation. As\nillustrated in Fig. 6(2), our method successfully distinguishes\nbetween two closely located targets, whereas other deep learn-\ning methods tend to merge them into a single target. This\nsuggests that our method discriminates each element in the\nimage accurately. In Fig. 6(4), only our method accurately\nseparates the shape of the unmanned aerial vehicle (UA V)\nfrom the mountain range. This is because our method not\nonly learns the target’s features but also constructs high-\nlevel semantic information about the backgrounds, thereby\naccurately capturing the overall continuity of the background.\nIn Fig. 6(6), except for the present method and DNA-Net,\nthe remaining methods produce false alarms on the stone in\nthe grass. This can be attributed to their limitation in only\nconstructing local contrast information and lack of establishing\nlong-distance dependence on the image.\nE. Ablation Study\nIn this section, we first employ two baselines to demonstrate\nthe effectiveness of SCTransNet.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE IV\nBased onU-Net, ablation study of the residual blocks(RBs), deep\nsupervision (DS), SSCA, CFN, and CCA module in averageIoU(%),\nnIoU (%), F-measure(%) on NUAA-SIRST, NUDT-SIRST,and IRSTD-1K.\nU-Net +RBs +DS +SSCA +CFN +CCA IoU nIoU F-measure\n✓ ✗ ✗ ✗ ✗ ✗ 75.29 78.60 86.36\n✓ ✓ ✗ ✗ ✗ ✗ 77.07 80.13 87.05\n✓ ✓ ✓ ✗ ✗ ✗ 77.73 80.78 87.47\n✓ ✓ ✓ ✓ ✗ ✗ 82.39 85.71 90.34\n✓ ✓ ✓ ✓ ✓ ✗ 82.89 86.28 90.66\n✓ ✓ ✓ ✓ ✓ ✓ 83.43 86.86 90.96\n• U-Net: We incrementally incorporate the residual blocks\n(RBs), deep supervised (DS), SSCA, CFN, and CCA into\nthe baseline U-Net to validate the e ffectiveness of the\nabove modules for infrared small target detection. The\nresults are presented in Table IV. We observe that the\nalgorithm’s performance improves consistently with the\ninclusion of the aforementioned modules. In particular,\nthe SSCA module significantly enhances the IoU, nIoU ,\nand F-measure value of the algorithm by 4.66%, 4.93%,\nand 2.87%, respectively. This effectively demonstrates the\neffectiveness of the full-level information modeling of the\nIR small target.\n• UCTransNet: We incrementally incorporate the RBs, DS,\nand skip connections (SKs), and use the proposed SCTB\nto replace CCT in the baseline UCTransNet to validate\nthe effectiveness of these modules. As shown in Table V,\nthese modules consistently enhance the algorithm’s per-\nformance. Particularly, the proposed SCTB improves the\nIoU, nIoU , and F-measure value of the algorithm by\n1.40%, 1.88%, and 1.12%, respectively, compared to the\nprimitive CCT. This demonstrates the proposed SCTB can\nmore effectively enhance the semantic difference between\nIR small targets and backgrounds than CCT.\nNext, we will delve into a detailed discussion of the\nproposed SCTB, SSCA and CFN, and compare the adopted\nCCA block with other feature fusion approaches implemented\nin IRSTD.\n1) The Spatial-channel Cross Transformer Block: In the\nproposed SCTransNet, a primary idea is utilizing SCTB to mix\nand redistribute the output features of the full-stage encoders\nto predict contextual information about the small target and\nbackgrounds. Since the network is encoded four times, the\nTABLE V\nBased onUCTransNet, ablation study of theRBs, DS, skip connections\n(SKs) and SCTB, reporting averageIoU(%), nIoU (%), F-measure(%) on\nNUAA-SIRST, NUDT-SIRST,and IRSTD-1K. Note that, we replace the\nCCT in UCTransNet using the proposedSCTB.\nUCTransNet +RBs +DS +SKs SCTB r / CCT IoU nIoU F-measure\n✓ ✗ ✗ ✗ ✗ 78.78 81.56 87.80\n✓ ✓ ✗ ✗ ✗ 79.95 82.97 88.45\n✓ ✓ ✓ ✗ ✗ 81.47 83.89 88.92\n✓ ✓ ✓ ✓ ✗ 82.03 84.98 89.54\n✓ ✓ ✓ ✓ ✓ 83.43 86.86 90.66\nFig. 8. Ablation on level of queries (Q) and composition of concatenated\nfeature (J) on NUAA-SIRST, SUDT-SIRST and IRSTD-1K.\nnumber of queries (Q) is set to 4, and both keys (K) and values\n(V) are formed by mapping the concatenated features (J) of\nthe complete 4-level features. In this section, we will discuss\ndifferent levels of Q and the composition of J to illustrate the\nimportance of full-level feature modeling.\nFig. 8 presents the ablation results for the level of Q and\ncomposition of J across three datasets. Note that when chang-\ning Q, J is composed of full-level features, and likewise, Q is\nthe full-level feature input when varying J. The experimental\nresults for Q indicate significant di fferences in the informa-\ntion learned by the neural network from di fferent levels of\nfeatures. Queries with higher and more comprehensive levels\n(Q123, Q234, Q34) encompass rich image semantics, thus\nachieving higher performance. The model performs best when\nfed with full-level Q inputs (SCTransNet), thus validating our\nmotivation. Similarly, the experimental results for J suggest\nthat selecting complete channel information allows queries\nto capture more accurate key features, thereby improving the\nperformance of IRSTD.\n2) The Spatial-embedded Single-head Channel-cross Atten-\ntion: To demonstrate the e fficacy of the proposed SSCA,\nwe present multi-head cross-attention [27] (MCA, a typical\nfull-level information interaction structure in UCTransNet)\nand three network structure variants: SSCA with positional\nencoding ( SSCA w PE ), SSCA with multi-head ( SSCA w\nMH), and SSCA without spatial-embedding ( SSCA w /o SE ),\nrespectively.\n• SSCA w PE : We incorporate positional encoding during\nthe patch embedding stage. To accommodate test images\nof di fferent sizes, we employ interpolation to scale the\nposition-coding matrix, ensuring the proper functioning\nof the algorithm.\n• SSCA w MH: We use a typical multi-head cross-attention\nmechanism to replace the single-head cross-attention\nmechanism in SSCA to verify the e ffectiveness of the\nsingle-head strategy for extracting limited features from\nthe IR small targets.\n• SSCA w /o SE : To validate the e ffectiveness of local\nspatial information coding, we eliminate the depth-wise\nconvolution in the QKV matrix generation process in\nSCTB.\nAs illustrated in Table VI, our SSCA has higher IoU, IoU,\nand F-measure values than the MCA and the variant SSCA w\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nTABLE VI\nIoU(%)/nIoU (%)/F-measure(%) values achieved by variants ofSSCA and\nMCA on NUAA-SIRST, NUDT-SIRSTand IRSTD-1K.\nModel Dataset\nNUAA-SIRST NUDT-SIRST IRSTD-1K\nMCA [27] 74.72 /78.35/85.53 93.07 /93.61/96.41 65.60 /66.57/79.22\nSSCA w PE 77.10 /79.88/87.07 94.03 /94.25/96.93 66.01 /65.29/79.52\nSSCA w MH 76.35 /79.56/86.59 93.72 /94.13/96.76 67.08 /67.55/80.30\nSSCA w/o SE 76.40 /79.19/86.62 93.23 /93.49/96.50 66.10 /65.48/79.59\nSSCA 77.50/81.08/87.32 94.09 /94.38/96.95 68.03 /68.15/80.96\nFig. 9. Visualization map of SCTransNet and SSCA w/o SE. The feature maps\nfrom the deep layers of SCTransNet have an accurate representation of the\nlocalized region of the target from the background, and accurate segmentation\nresults are obtained at the output layer.\nPE on three datasets. This suggests that SCTransNet can better\nperceive the information di fference between small targets and\ncomplex backgrounds than MCA through comprehensive in-\nformation interaction. It also illustrates that absolute positional\nencoding is not suitable for IRSTD tasks. This is due to\nthe scaling of the position-embedding matrix in variable-size\nimage inputs, which leads to inaccurate small-target position\ncoding information, consequently a ffecting the prediction of\ntarget pixels.\nCompared to our SSCA, SSCA w MH suffers decreases\nof 1.15%, 1.52%, and 0.73% in terms of IoU, IoU, and F-\nmeasure values on the SIRST-1K dataset. This is because the\nmulti-head strategy complicates the feature mapping space of\nIR small targets, which is rather unfavorable for extracting\ninformation from targets with limited features. Therefore, in\nSCTransNet, we utilize the single-head attention for IRSTD.\nComparing SSCA and the variant SSCA w /o SE , we find\nthat the local spatial embedding can significantly improve the\nperformance of infrared small target detection in the three\npublic datasets. Visualization maps displayed in Fig. 9 further\nillustrate the e ffectiveness of this strategy. This is due to the\nability of local spatial embedding to capture both specific\ndetails of the target and potential spatial correlations in the\nFig. 10. The structure of representative feed-forward networks and CFN w /o\nGSLC.\nTABLE VII\nIoU(%)/nIoU (%) values achieved by the representative feed-forward\nNetworks and the variants ofCFN on NUAA-SIRST and NUDT-SIRST.\nModel Params(M) Flops(G) Dataset\nNUAA-SIRST NUDT-SIRST\nFFN [41] 11.0292 20.1474 76.87/80.08 93.58 /93.85\nLeFF [32] 11.1312 20.1944 76.49 /80.21 93.92 /94.07\nGDFN [30] 10.1841 19.7210 75.48 /79.32 93.40 /93.64\nMSFN [31] 11.7107 20.5026 77.35 /79.89 93.88 /94.24\nCFN w/o GSLC 11.1905 20.2362 76.54 /80.56 93.95 /94.18\nCFN 11.1905 20.2372 77.50/81.08 94.09 /94.38\nbackground within the deep layers. As a result, this approach\nminimizes instances of missed detections and improves the\nconfidence of the detection process.\n3) The Complementary Feed-forward Network: Feed-\nforward networks (FFNs) are used to strengthen the informa-\ntion correlation within features and introduce nonlinear radi-\ncalization to enrich the feature representation. In this section,\nwe use five di fferent FFN models based on SCTransNet to\ncompare the proposed CFNs. As shown in Fig. 10, we used\ntypical FFN [41] (ViT for image classification), LeFF [32]\n(Uformer for image restoration) embedded in localized space,\nGDFN [30] (Restormer for image restoration) based on gated\nconvolution, MSFN [31] (Sparse transformer for image derain-\ning) based on multi-scale depth-wise convolution, the variant\nCFN without global spatial and local channel module ( CFN\nw/o GSLC), respectively.\nAs shown in Table VII, LeFF exhibits a slight improvement\nin metrics over FFN, which indicates that the local spatial\ninformation aggregation employed in feed-forward neural net-\nworks is effective for IRSTD. Because gated convolution tends\nto consider IR small targets as noise and filters them out,\nthis results in the GDFN having a low detection accuracy.\nWe also find that MSFN outperforms all methods except\nour CFN, illustrating the superior ability of multi-scale struc-\ntures to interact with spatial information compared to single-\nscale structures. Finally, we observe that the performance\nof the variant CFN w /o GSLC is inferior to that of MSFN.\nHowever, when we incorporate the GSLC module, our CFN\nachieves optimal values of IoU and nIoU on the NUAA\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nFig. 11. Visualization map of SCTransNet and CFN w /o GSLC. The feature\nmaps from the deep layer of CFN w /o GSLC mix targets in the background.\nIt finally results in missed detection in the output layer.\nTABLE VIII\nIoU(%)/nIoU (%) values achieved by the different cross-layer feature\nfusing modules onNUAA-SIRST and NUDT-SIRST.\nModel Params(M) Flops(G) Dataset\nNUAA-SIRST NUDT-SIRST\nC.ACM [14] 13.0627 30.9862 75.68 /79.52 93.92 /94.24\nC.AGPC [19] 11.7581 22.9647 77.39 /79.96 94.01 /94.22\nC.AFFPN [33] 11.7171 22.7291 76.12 /79.33 93.53 /93.69\nSCTransNet 11.1905 20.2372 77.50 /81.08 94.09 /94.38\nand NUDT datasets. Moreover, the network’s parameters and\ncomputational complexity remain almost unchanged, which\ndemonstrates the validity and utility of the complementary\nmechanism proposed in this paper for the IRSTD task. As\nillustrated in Fig. 11, with the help of the complementary\nmechanism, the network allows for more e ffective enhance-\nment of infrared small targets and suppression of clutter in\nbuilding and jungle backgrounds, leading to improved target\ndetection accuracy.\n4) The Impact of CCA Block: As mentioned in Sec. II-A,\ncross-layer feature fusion can facilitate the preservation of\nenhanced target information. In this section, we utilize three\ncross-layer feature fusion structures, namely ACM [14],\nAGPC [19], and AFFPN [33], derived from di fferent IRSTD\nmethods, to replace the CCA module employed in SC-\nTransNet. This substitution yields the variation structures,\nnamely C.ACM, C.AGPC, and C.AFFPN, respectively. As\nshown in Table VIII, the results illustrate that our SCTransNet\nobtains the highest IoU and nIoU values on the NUAA\nand NUDT datasets with the lowest model parameters and\ncomputational complexity. This illustrates the e ffectiveness of\nthe CCA we utilized.\nF . Core Hyper-parameter Analysis\nWe utilize the depth of the RBs, the number of SCTBs,\nthe channel expansion factor of CFNs, and the base width of\nTABLE IX\nHyper-parameter study of theRBs in averageIoU(%), nIoU (%),\nF-measure(%) on NUAA-SIRST, NUDT-SIRST,and IRSTD-1K.\n1 2 3 4 IoU nIoU F-measure Params(M) Flops(G)\n✓ ✗ ✗ ✗ 82.29 85.77 90.26 20.0212 11.1462\n✓ ✗ ✗ ✗ 82.33 85.89 90.31 20.0967 11.1484\n✓ ✓ ✗ ✗ 82.49 86.11 90.40 20.1680 11.1569\n✓ ✓ ✓ ✗ 82.95 86.27 90.68 20.2372 11.1905\n✓ ✓ ✓ ✓ 83.43 86.86 90.96 20.2372 11.1905\nthe model to validate the hyper-parameters of SCTransNet.\nAs shown in Table IX, the numbers “0”, “1”, “2”, and\n“3” indicate the embedding depth of the RBs. We observe\nthat as the residual block depth increases, there is a slight\nincrease in both the number of parameters and flops, and\nthe performance of IRSTD shows significant improvement.\nThis improvement can be attributed to the residual connection\nfacilitating gradient propagation and mitigating feature degra-\ndation. Therefore, our SCTransNet uses four residial blocks\nfor information encoding. Table X illustrates the results of the\nhyper-parameter study of the number of SCTBs, the channel\nexpansion factor of CFNs, and the basic width of the model.\nIt is evident that as the number of SCTB modules increases,\nthe model’s performance steadily improves, rea ffirming the\neffectiveness of the SCTB model. We observe that while the\nperformance with 6 SCTBs is slightly better than with 4\nSCTBs, it incurs excessive computational complexity. When\nthe channel expansion factor η = 2.66, the model can get the\nbest performance. Additionally, we also noticed that setting the\nbase width of the model W =48 results in a slight degradation\nin performance compared with W=32, which can be attributed\nto the excessive model parameters reducing the algorithm’s\ngeneralization ability. Therefore, in our proposed SCTransNet,\nthe number of SCTBs, the channel expansion factor of CFNs,\nand the base width of the model are set to 4, 2.66, and 32,\nrespectively.\nG. Robustness of SCTransNet\nIn an actual IR detection system, the non-uniform response\nof the focal plane array (FPN) can cause stripe noise in IR\nimages [65]. This presents a challenge to the noise immunity\nand generalization ability of the IRSTD methods. Fig. 12\ngives the visual e ffect of the IR image with real stripe noise\non various detection methods. It is evident that the noise\ndestroys the local neighborhood information of the targets.\nIn Fig. 12(1), only our SCTransNet accurately detects two\ntargets, while the other methods exhibit missed detections\nand false alarms. In Fig. 12(2), there is also a piece of\nblind element in the striped image, which interferes with the\nsemantics understanding of the building. As a result, the ACM,\nRDIAN, and MTU-Net generate false alarms around the blind\nelement. The ability to explicitly establish full-level contextual\ninformation about the target and the background is what makes\nour approach more robust.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nTABLE X\nHyper-parameter study of the number ofSCTBs, the channel expansion\nfactor ofCFN, and the basic width of the model in averageIoU(%),\nnIoU (%), F-measure(%) on NUAA-SIRST, NUDT-SIRST,and IRSTD-1K.\nHyper-param IoU nIoU F-measure Params(M) Flops(G)\nThe number of SCTBs\nN = 1 82.33 85.86 90.28 17.7408 6.3295\nN = 2 82.53 86.05 90.43 18.5729 7.9498\nN = 3 82.97 86.46 90.58 19.4051 9.5702\nN = 4 83.43 86.86 90.96 20.2372 11.1905\nN = 5 83.40 86.84 90.95 21.0694 12.8108\nN = 6 83.45 86.86 90.97 21.9015 14.4312\nThe channel expansion factor of CFNs\nη = 1.33 82.80 86.18 90.59 19.2457 9.2539\nη = 2.00 82.75 86.32 90.56 19.7474 10.2338\nη = 2.66 83.43 86.86 90.96 20.2372 11.1905\nη = 3.00 83.24 86.69 90.84 20.4938 11.6917\nη = 3.99 83.10 86.60 90.77 21.2306 13.1307\nThe basic width of the model\nW = 8 77.52 80.55 87.33 1.3321 0.7468\nW = 16 81.02 84.50 89.51 5.1488 2.8609\nW = 32 83.43 86.86 90.96 20.2372 11.1905\nW = 48 82.95 86.48 90.60 45.2687 24.994\nFig. 12. Visual results obtained by di fferent IRSTD methods on the real\nstripy IR images. Circles in blue, yellow, and red represent correctly detected\ntargets, miss detections, and false alarms, respectively.\nV . CONCLUSION\nIn this paper, we presented a Spatial-channel Cross Trans-\nformer Network (SCTransNet) for IR small target detection.\nOur SCTransNet utilizes spatial-channel cross transformer\nblocks to establish associations between encoder and de-\ncoder features to predict the context di fference of targets\nand backgrounds in deeper network layers. We introduced a\nspatial-embedded single-head channel-cross attention module,\nwhich establishes the semantic relevance between targets and\nbackgrounds by interacting local spatial features with global\nfull-level channel information. We also devised a comple-\nmentary feed-forward network, which employs a multi-scale\nstrategy and crosses spatial-channel information to enhance\nfeature differences between the target and background, thereby\nfacilitating effective mapping of IR images to the segmentation\nspace. Our comprehensive evaluation of the method on three\npublic datasets shows the e ffectiveness and superiority of the\nproposed technique.\nReferences\n[1] Y . Sun, J. Yang, and W. An, “Infrared dim and small target\ndetection via multiple subspace learning and spatial-temporal\npatch-tensor model,” IEEE Trans. Geosci. Remote Sens., vol. 59,\nno. 5, pp. 3737–3752, 2020.\n[2] P. Wu, H. Huang, H. Qian, S. Su, B. Sun, and Z. Zuo,\n“SRCANet: Stacked residual coordinate attention network for\ninfrared ship detection,” IEEE Trans. Geosci. Remote Sens. ,\nvol. 60, pp. 1–14, 2022.\n[3] P. Yan, R. Hou, X. Duan, C. Yue, X. Wang, and X. Cao,\n“STDMANet: Spatio-temporal di fferential multiscale attention\nnetwork for small moving infrared target detection,” IEEE\nTrans. Geosci. Remote Sens. , vol. 61, pp. 1–16, 2023.\n[4] X. Ying, L. Liu, Y . Wang, R. Li, N. Chen, Z. Lin, W. Sheng,\nand S. Zhou, “Mapping degeneration meets label evolution:\nLearning infrared small target detection with single point su-\npervision,” in Proc. IEEE /CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), 2023, pp. 15 528–15 538.\n[5] X. Bai and F. Zhou, “Analysis of new top-hat transformation and\nthe application for infrared dim small target detection,” Pattern\nRecognit., vol. 43, no. 6, pp. 2145–2156, 2010.\n[6] J.-F. Rivest and R. Fortin, “Detection of dim targets in digital\ninfrared imagery by morphological image processing,” Opt.\nEng., vol. 35, no. 7, pp. 1886–1893, 1996.\n[7] C. P. Chen, H. Li, Y . Wei, T. Xia, and Y . Y . Tang, “A local\ncontrast method for small infrared target detection,”IEEE Trans.\nGeosci. Remote Sens. , vol. 52, no. 1, pp. 574–581, 2013.\n[8] S. Kim and J. Lee, “Scale invariant small target detection by\noptimizing signal-to-clutter ratio in heterogeneous background\nfor infrared search and track,” Pattern Recognit., vol. 45, no. 1,\npp. 393–406, 2012.\n[9] C. Gao, D. Meng, Y . Yang, Y . Wang, X. Zhou, and A. G. Haupt-\nmann, “Infrared patch-image model for small target detection\nin a single image,” IEEE Trans. Image Process., vol. 22, no. 12,\npp. 4996–5009, 2013.\n[10] H. Zhu, S. Liu, L. Deng, Y . Li, and F. Xiao, “Infrared small\ntarget detection via low-rank tensor completion with top-hat\nregularization,” IEEE Trans. Geosci. Remote Sens. , vol. 58,\nno. 2, pp. 1004–1016, 2019.\n[11] H. Wang, L. Zhou, and L. Wang, “Miss detection vs. false alarm:\nAdversarial learning for small object segmentation in infrared\nimages,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , 2019,\npp. 8509–8518.\n[12] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu,\n“Ccnet: Criss-cross attention for semantic segmentation,” in\nProc. IEEE Int. Conf. Comput. Vis. (ICCV), 2019, pp. 603–612.\n[13] X. He, Y . Zhou, J. Zhao, D. Zhang, R. Yao, and Y . Xue, “Swin\ntransformer embedding unet for remote sens. image semantic\nsegmentation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp.\n1–15, 2022.\n[14] Y . Dai, Y . Wu, F. Zhou, and K. Barnard, “Asymmetric contextual\nmodulation for infrared small target detection,” in Proceedings\nof the IEEE /CVF Winter Conference on Applications of Com-\nputer Vision, 2021, pp. 950–959.\n[15] B. Li, C. Xiao, L. Wang, Y . Wang, Z. Lin, M. Li, W. An,\nand Y . Guo, “Dense nested attention network for infrared small\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\ntarget detection,” IEEE Trans. Image Process. , vol. 32, pp.\n1745–1758, 2022.\n[16] X. Wu, D. Hong, and J. Chanussot, “UIU-Net: U-net in u-net for\ninfrared small object detection,” IEEE Trans. Image Process. ,\nvol. 32, pp. 364–376, 2022.\n[17] T. Wu, B. Li, Y . Luo, Y . Wang, C. Xiao, T. Liu, J. Yang, W. An,\nand Y . Guo, “MTU-Net: Multilevel transunet for space-based\ninfrared tiny ship detection,” IEEE Trans. Geosci. Remote Sens.,\nvol. 61, pp. 1–15, 2023.\n[18] Y . Dai, Y . Wu, F. Zhou, and K. Barnard, “Attentional local\ncontrast networks for infrared small target detection,” IEEE\nTrans. Geosci. Remote Sens. , vol. 59, no. 11, pp. 9813–9824,\n2021.\n[19] T. Zhang, L. Li, S. Cao, T. Pu, and Z. Peng, “Attention-guided\npyramid context networks for detecting infrared small target\nunder complex background,” IEEE Trans. Aerosp. Electron.\nSyst., 2023.\n[20] X. Tong, S. Su, P. Wu, R. Guo, J. Wei, Z. Zuo, and B. Sun,\n“MSAFFNet: A multi-scale label-supervised attention feature\nfusion network for infrared small target detection,” IEEE Trans.\nGeosci. Remote Sens. , 2023.\n[21] M. Zhang, K. Yue, J. Zhang, Y . Li, and X. Gao, “Exploring fea-\nture compensation and cross-level correlation for infrared small\ntarget detection,” in Proceedings of the 30th ACM International\nConference on Multimedia , 2022, pp. 1857–1865.\n[22] Q. Hou, L. Zhang, F. Tan, Y . Xi, H. Zheng, and N. Li, “ISTDU-\nNet: Infrared small-target detection u-net,” IEEE Geosci. Re-\nmote Sens. Lett. , vol. 19, pp. 1–5, 2022.\n[23] X. He, Q. Ling, Y . Zhang, Z. Lin, and S. Zhou, “Detecting dim\nsmall target in infrared images via subpixel sampling cuneate\nnetwork,” IEEE Geosci. Remote Sens. Lett. , vol. 19, pp. 1–5,\n2022.\n[24] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang,\n“Unet++: A nested u-net architecture for medical image seg-\nmentation,” in Deep Learning in Medical Image Analysis\nand Multimodal Learning for Clinical Decision Support: 4th\nInternational Workshop, DLMIA 2018, and 8th International\nWorkshop, ML-CDS 2018, Held in Conjunction with MICCAI\n2018, Granada, Spain, September 20, 2018, Proceedings 4 .\nSpringer, 2018, pp. 3–11.\n[25] R. Kou, C. Wang, Y . Yu, Z. Peng, M. Yang, F. Huang, and Q. Fu,\n“LW-IRSTnet: Lightweight infrared small target segmentation\nnetwork and application deployment,” IEEE Trans. Geosci.\nRemote Sens., 2023.\n[26] J. Lin, K. Zhang, X. Yang, X. Cheng, and C. Li, “Infrared\ndim and small target detection based on u-transformer,” J. Vis.\nCommun. Image Represent. , vol. 89, p. 103684, 2022.\n[27] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “UCTransnet:\nrethinking the skip connections in u-net from a channel-wise\nperspective with transformer,” in Proceedings of the AAAI\nconference on artificial intelligence , vol. 36, no. 3, 2022, pp.\n2441–2449.\n[28] Y . Li, Z. Cheng, C. Wang, J. Zhao, and L. Huang, “RCCT-\nASPPNet: Dual-encoder remote image segmentation based on\ntransformer and ASPP,” Remote Sens. , vol. 15, no. 2, p. 379,\n2023.\n[29] Q. Luo, J. Su, C. Yang, W. Gui, O. Silven, and L. Liu, “CAT-\nEDNet: Cross-attention transformer-based encoder–decoder net-\nwork for salient defect detection of strip steel surface,” IEEE\nTrans. Instrum. Meas. , vol. 71, pp. 1–13, 2022.\n[30] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-\nH. Yang, “Restormer: E fficient transformer for high-resolution\nimage restoration,” in Proc. IEEE /CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), 2022, pp. 5728–5739.\n[31] X. Chen, H. Li, M. Li, and J. Pan, “Learning a sparse\ntransformer network for e ffective image deraining,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , 2023,\npp. 5896–5905.\n[32] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, and H. Li, “Uformer:\nA general u-shaped transformer for image restoration,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , 2022,\npp. 17 683–17 693.\n[33] Z. Zuo, X. Tong, J. Wei, S. Su, P. Wu, R. Guo, and B. Sun,\n“AFFPN: attention fusion feature pyramid network for small\ninfrared target detection,” Remote Sens., vol. 14, no. 14, p. 3412,\n2022.\n[34] C. Yu, Y . Liu, S. Wu, X. Xia, Z. Hu, D. Lan, and X. Liu, “Pay\nattention to local contrast learning networks for infrared small\ntarget detection,” IEEE Geosci. Remote Sens. Lett. , vol. 19, pp.\n1–5, 2022.\n[35] X. Tong, B. Sun, J. Wei, Z. Zuo, and S. Su, “EAAU-Net:\nEnhanced asymmetric attention u-net for infrared small target\ndetection,” Remote Sens., vol. 13, no. 16, p. 3200, 2021.\n[36] S. Liu, P. Chen, and M. Wo ´zniak, “Image enhancement-based\ndetection with small infrared targets,” Remote Sens. , vol. 14,\nno. 13, p. 3232, 2022.\n[37] L. Huang, S. Dai, T. Huang, X. Huang, and H. Wang, “Infrared\nsmall target segmentation with multiscale feature representa-\ntion,” Infr. Phys. Technol., vol. 116, p. 103755, 2021.\n[38] Y . Chen, L. Li, X. Liu, and X. Su, “A multi-task framework for\ninfrared small target detection and segmentation,” IEEE Trans.\nGeosci. Remote Sens. , vol. 60, pp. 1–9, 2022.\n[39] H. Sun, J. Bai, F. Yang, and X. Bai, “Receptive-field and\ndirection induced attention network for infrared dim small target\ndetection with a large-scale dataset irdst,” IEEE Trans. Geosci.\nRemote Sens., vol. 61, pp. 1–13, 2023.\n[40] M. Zhang, R. Zhang, Y . Yang, H. Bai, J. Zhang, and J. Guo,\n“ISNet: Shape matters for infrared small target detection,” in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\n2022, pp. 877–886.\n[41] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly et al. , “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” arXiv preprint\narXiv:2010.11929, 2020.\n[42] M. Zhang, H. Bai, J. Zhang, R. Zhang, C. Wang, J. Guo,\nand X. Gao, “Rkformer: Runge-kutta transformer with random-\nconnection attention for infrared small target detection,” in\nProceedings of the 30th ACM International Conference on\nMultimedia, 2022, pp. 1730–1738.\n[43] P. Pan, H. Wang, C. Wang, and C. Nie, “ABC: Attention with\nbilinear correlation for infrared small target detection,” arXiv\npreprint arXiv:2303.10321, 2023.\n[44] F. Liu, C. Gao, F. Chen, D. Meng, W. Zuo, and X. Gao,\n“Infrared small-dim target detection with transformer under\ncomplex backgrounds,” arXiv preprint arXiv:2109.14379, 2021.\n[45] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L.\nYuille, and Y . Zhou, “Transunet: Transformers make strong\nencoders for medical image segmentation,” arXiv preprint\narXiv:2102.04306, 2021.\n[46] G. Chen, W. Wang, and S. Tan, “Irstformer: A hierarchical\nvision transformer for infrared small target detection,” Remote\nSens., vol. 14, no. 14, p. 3258, 2022.\n[47] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin,\nand B. Guo, “Swin transformer: Hierarchical vision transformer\nusing shifted windows,” in Proc. IEEE Int. Conf. Comput. Vis.\n(ICCV), 2021, pp. 10 012–10 022.\n[48] M. Qi, L. Liu, S. Zhuang, Y . Liu, K. Li, Y . Yang, and X. Li,\n“FTC-net: fusion of transformer and cnn features for infrared\nsmall target detection,” IEEE J. Sel. Top. Appl. Earth Observ.\nRemote Sens., vol. 15, pp. 8613–8623, 2022.\n[49] S. Meng, C. Zhang, Q. Shi, Z. Chen, W. Hu, and F. Lu, “A\nrobust infrared small target detection method jointing multiple\ninformation and noise prediction: Algorithm and benchmark,”\nIEEE Trans. Geosci. Remote Sens. , 2023.\n[50] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu,\n“Ccnet: Criss-cross attention for semantic segmentation,” in\nProc. IEEE Int. Conf. Comput. Vis. (ICCV), 2019, pp. 603–612.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\n[51] C. Xu, Z. Ye, L. Mei, S. Shen, Q. Zhang, H. Sui, W. Yang,\nand S. Sun, “SCAD: A siamese cross-attention discrimination\nnetwork for bitemporal building change detection,” Remote\nSens., vol. 14, no. 24, p. 6213, 2022.\n[52] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), 2016, pp. 770–778.\n[53] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai,\nT. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit\net al. , “Mlp-mixer: An all-mlp architecture for vision,” Adv.\nNeural Inf. Process. Syst. (NeurIPS), vol. 34, pp. 24 261–24 272,\n2021.\n[54] W. Yu, M. Luo, P. Zhou, C. Si, Y . Zhou, X. Wang, J. Feng, and\nS. Yan, “Metaformer is actually what you need for vision,” in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\n2022, pp. 10 819–10 829.\n[55] S. D. Deshpande, M. H. Er, R. Venkateswarlu, and P. Chan,\n“Max-mean and max-median filters for detection of small\ntargets,” in Signal and Data Processing of Small Targets 1999 ,\nvol. 3809. SPIE, 1999, pp. 74–83.\n[56] J. Han, S. Moradi, I. Faramarzi, H. Zhang, Q. Zhao, X. Zhang,\nand N. Li, “Infrared small target detection based on the weighted\nstrengthened local contrast measure,” IEEE Geosci. Remote\nSens. Lett., vol. 18, no. 9, pp. 1670–1674, 2020.\n[57] J. Han, S. Moradi, I. Faramarzi, C. Liu, H. Zhang, and Q. Zhao,\n“A local contrast method for infrared small-target detection\nutilizing a tri-layer window,” IEEE Geosci. Remote Sens. Lett. ,\nvol. 17, no. 10, pp. 1822–1826, 2019.\n[58] L. Zhang and Z. Peng, “Infrared small target detection based on\npartial sum of the tensor nuclear norm,” Remote Sens., vol. 11,\nno. 4, p. 382, 2019.\n[59] Y . Sun, J. Yang, and W. An, “Infrared dim and small target\ndetection via multiple subspace learning and spatial-temporal\npatch-tensor model,” IEEE Trans. Geosci. Remote Sens., vol. 59,\nno. 5, pp. 3737–3752, 2020.\n[60] K. Wang, S. Du, C. Liu, and Z. Cao, “Interior attention-\naware network for infrared small target detection,” IEEE Trans.\nGeosci. Remote Sens. , vol. 60, pp. 1–13, 2022.\n[61] D. Ulyanov, A. Vedaldi, and V . Lempitsky, “Instance normaliza-\ntion: The missing ingredient for fast stylization,” arXiv preprint\narXiv:1607.08022, 2016.\n[62] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “ECA-\nNet: E fficient channel attention for deep convolutional neural\nnetworks,” in Proc. IEEE /CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), 2020, pp. 11 534–11 542.\n[63] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby,\nE. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek et al.,\n“Resmlp: Feedforward networks for image classification with\ndata-efficient training,” IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 45, no. 4, pp. 5314–5321, 2022.\n[64] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into\nrectifiers: Surpassing human-level performance on imagenet\nclassification,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,\n2015, pp. 1026–1034.\n[65] S. Yuan, H. Qin, X. Yan, N. Akhtar, S. Yang, and S. Yang,\n“ARCNet: An asymmetric residual wavelet column correc-\ntion network for infrared image destriping,” arXiv preprint\narXiv:2401.15578, 2024.\nShuai Yuan received the B.S. degree from Xi’an\nTechnological University, Xi’an, China, in 2019.\nHe is currently pursuing a Ph.D. degree at Xidian\nUniversity, Xi’an, China. He is currently studying\nat the University of Melbourne as a visiting stu-\ndent, working closely with Dr. Naveed Akhtar. His\nresearch interests include infrared image understand-\ning, remote sensing, and deep learning.\nHanlin Qin received the B.S and Ph.D. degrees from\nXidian University, Xi’an, China, in 2004 and 2010.\nHe is currently a full professor at the School of\nOptoelectronic Engineering, Xidian University. He\nauthored or co-authored more than 100 scientific\narticles. His research interests include electro-optical\ncognition, advanced intelligent computing, and au-\ntonomous collaboration.\nXiang Yan received the B.S and Ph.D. degrees from\nXidian University, Xi’an, China, in 2012 and 2018.\nHe was a visiting Ph.D. Student with the School of\nComputer Science and Software Engineering, Aus-\ntralia, from 2016 to 2018, working closely with Prof.\nAjmal Mian. He is currently an associate professor at\nXidian University, Xi’an, China. His current research\ninterests include image processing, computer vision\nand deep learning.\nNaveed Akhtar is a Senior Lecturer at the Univer-\nsity of Melbourne. He received his PhD in Computer\nScience from the University of Western Australia\nand Master degree from Hochschule Bonn-Rhein-\nSieg, Germany. He is a recipient of the Discovery\nEarly Career Researcher Award from the Australian\nResearch Council. He is a Universal Scientific Ed-\nucation and Research Network Laureate in Formal\nSciences. He was a finalist of the Western Australia’s\nEarly Career Scientist of the Year 2021. He is\nan ACM Distinguished Speaker and serves as an\nAssociate Editor of IEEE Trans. Neural Networks and Learning Systems.\nAjmal Mian is a Professor of Computer Science\nat The University of Western Australia. He is the\nrecipient of three esteemed national fellowships from\nthe Australian Research Council (ARC) including\nthe recent Future Fellowship Award 2022. He is a\nFellow of the International Association for Pattern\nRecognition and recipient of several awards includ-\ning the West Australian Early Career Scientist of the\nYear Award 2012, the HBF Mid-Career Scientist of\nthe Year Award 2022, Excellence in Research Super-\nvision Award, EH Thompson Award, ASPIRE Pro-\nfessional Development Award, Vice-chancellors Mid-career Research Award,\nOutstanding Young Investigator Award, and the Australasian Distinguished\nDoctoral Dissertation Award. Ajmal Mian has secured research funding from\nthe ARC, NHMRC, DARPA, and the Australian Department of Defence. He\nhas served as a Senior Editor for IEEE Transactions on Neural Networks\n& Learning Systems and Associate Editor for IEEE Transactions on Image\nProcessing and the Pattern Recognition journal. His research interests include\ncomputer vision, machine learning, remote sensing, and 3D point cloud\nanalysis.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5794147849082947
    },
    {
      "name": "Infrared",
      "score": 0.44687420129776
    },
    {
      "name": "Computer science",
      "score": 0.3710293769836426
    },
    {
      "name": "Electrical engineering",
      "score": 0.26849091053009033
    },
    {
      "name": "Physics",
      "score": 0.1801324188709259
    },
    {
      "name": "Engineering",
      "score": 0.15647989511489868
    },
    {
      "name": "Optics",
      "score": 0.13984781503677368
    },
    {
      "name": "Voltage",
      "score": 0.1142052412033081
    }
  ],
  "institutions": []
}