{
  "title": "Self-Supervised Graph Transformer on Large-Scale Molecular Data",
  "url": "https://openalex.org/W3095883070",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2096864261",
      "name": "Rong Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221439924",
      "name": "Bian, Yatao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221439925",
      "name": "Xu, Tingyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351947188",
      "name": "Xie Wei-yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099492898",
      "name": "Wei Ying",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225185943",
      "name": "Huang, Wenbing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381687061",
      "name": "Huang, Junzhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2145578524",
    "https://openalex.org/W2527189750",
    "https://openalex.org/W2749279690",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W3005552578",
    "https://openalex.org/W2945052571",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2008505552",
    "https://openalex.org/W2991425343",
    "https://openalex.org/W2787998955",
    "https://openalex.org/W2279490987",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2473190403",
    "https://openalex.org/W2965556524",
    "https://openalex.org/W2153693853",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2994710732",
    "https://openalex.org/W1738019091",
    "https://openalex.org/W3082217026",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2038702914",
    "https://openalex.org/W2996443485",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2735246657",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W2962767366",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W2947209427",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2999218478",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W2461470610",
    "https://openalex.org/W2962960500",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W2912078280",
    "https://openalex.org/W2895744665",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2963711743",
    "https://openalex.org/W2461620095",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2406943157",
    "https://openalex.org/W2038895391",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W2893944917",
    "https://openalex.org/W3102797483",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W2214665483",
    "https://openalex.org/W2964198013",
    "https://openalex.org/W2619042136",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W3012816161",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2076498053",
    "https://openalex.org/W2962876364",
    "https://openalex.org/W2996268457"
  ],
  "abstract": "How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for molecular representation learning. Nevertheless, two issues impede the usage of GNNs in real scenarios: (1) insufficient labeled molecules for supervised training; (2) poor generalization capability to new-synthesized molecules. To address them both, we propose a novel framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node-, edge- and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks into the Transformer-style architecture to deliver a class of more expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules -- the biggest GNN and the largest training dataset in molecular representation learning. We then leverage the pre-trained GROVER for molecular property prediction followed by task-specific fine-tuning, where we observe a huge improvement (more than 6% on average) from current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the significant potential on performance boosting.",
  "full_text": "Self-Supervised Graph Transformer on Large-Scale\nMolecular Data\nYu Rong1‚àó, Yatao Bian1‚àó, Tingyang Xu1, Weiyang Xie1, Ying Wei1,\nWenbing Huang2‚Ä†, Junzhou Huang1\n1Tencent AI Lab\n2 Beijing National Research Center for Information Science and Technology (BNRist),\nDepartment of Computer Science and Technology, Tsinghua University\nyu.rong@hotmail.com, yatao.bian@gmail.com, {tingyangxu, weiyangxie}@tencent.com\njudyweiying@gmail.com, hwenbing@126.com, jzhuang@uta.edu\nAbstract\nHow to obtain informative representations of molecules is a crucial prerequisite\nin AI-driven drug design and discovery. Recent researches abstract molecules as\ngraphs and employ Graph Neural Networks (GNNs) for molecular representation\nlearning. Nevertheless, two issues impede the usage of GNNs in real scenarios:\n(1) insufÔ¨Åcient labeled molecules for supervised training; (2) poor generalization\ncapability to new-synthesized molecules. To address them both, we propose a\nnovel framework, GROVER, which stands for Graph Representation frOm self-\nsuperVised mEssage passing tRansformer. With carefully designed self-supervised\ntasks in node-, edge- and graph-level,GROVER can learn rich structural and seman-\ntic information of molecules from enormous unlabelled molecular data. Rather, to\nencode such complex information, GROVER integrates Message Passing Networks\ninto the Transformer-style architecture to deliver a class of more expressive en-\ncoders of molecules. The Ô¨Çexibility of GROVER allows it to be trained efÔ¨Åciently\non large-scale molecular dataset without requiring any supervision, thus being\nimmunized to the two issues mentioned above. We pre-train GROVER with 100\nmillion parameters on 10 million unlabelled molecules‚Äîthe biggest GNN and the\nlargest training dataset in molecular representation learning. We then leverage the\npre-trained GROVER for molecular property prediction followed by task-speciÔ¨Åc\nÔ¨Åne-tuning, where we observe a huge improvement (more than 6% on average)\nfrom current state-of-the-art methods on 11 challenging benchmarks. The insights\nwe gained are that well-designed self-supervision losses and largely-expressive\npre-trained models enjoy the signiÔ¨Åcant potential on performance boosting.\n1 Introduction\nInspired by the remarkable achievements of deep learning in many scientiÔ¨Åc domains, such as com-\nputer vision [19, 57], natural language processing [51, 53], and social networks [3, 31], researchers\nare exploiting deep learning approaches to accelerate the process of drug discovery and reduce costs\nby facilitating the rapid identiÔ¨Åcation of molecules [5]. Molecules can be naturally represented by\nmolecular graphs which preserve rich structural information. Therefore, supervised deep learning of\ngraphs, especially with Graph Neural Networks(GNNs) [25, 46] have shown promising results in\nmany tasks, such as molecular property prediction [13, 23] and virtual screening [56, 64].\n‚àóEqual contribution.\n‚Ä†Wenbing Huang is the corresponding author.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2007.02835v2  [q-bio.BM]  29 Oct 2020\nDespite the fruitful progress, two issues still impede the usage of deep learning in real scenarios:\n(1) insufÔ¨Åcient labeled data for molecular tasks; (2) poor generalization capability of models in the\nenormous chemical space. Different from other domains (such as image classiÔ¨Åcation) that have\nrich-source labeled data, getting labels of molecular property requires wet-lab experiments which is\ntime-consuming and resource-costly. As a consequence, most public molecular benchmarks contain\nfar-from-adequate labels. Conducting deep learning on these benchmarks is prone to over-Ô¨Åtting and\nthe learned model can hardly cope with the out-of-distribution molecules.\nIndeed, it has been a long-standing goal in deep learning to improve the generalization power\nof neural networks. Towards this goal, certain progress has been made. For example, in Natural\nLanguage Processing (NLP), researchers can pre-train the model from large-scale unlabeled sentences\nvia a newly-proposed technique‚Äîthe self-supervised learning. Several successful self-supervised\npretraining strategies, such as BERT [9] and GPT [38] have been developed to tackle a variety of\nlanguage tasks. By contending that molecule can be transformed into sequential representation‚Äî\nSMILES [59], the work by [58] tries to adopt the BERT-style method to pretrain the model, and Liu\net.al. [ 29] also exploit the idea from N-gram approach in NLP and conducts vertices embedding\nby predicting the vertices attributes. Unfortunately, these approaches fail to explicitly encode the\nstructural information of molecules as using the SMILES representation is not topology-aware.\nWithout using SMILES, several works aim to establish a pre-trained model directly on the graph\nrepresentations of molecules. Hu et.al. [ 18] investigate the strategies to construct the three pre-\ntraining tasks, i.e., context prediction and node masking for node-level self-supervised learning and\ngraph property prediction for graph-level pre-training. We argue that the formulation of pre-training\nin this way is suboptimal. First, in the masking task, they treat the atom type as the label. Different\nfrom NLP tasks, the number of atom types in molecules is much smaller than the size of a language\nvocabulary. Therefore, it would suffer from serious representation ambiguity and the model is hard to\nencode meaningful information especially for the highly frequent atoms. Second, the graph-level\npre-training task in [18] is supervised. This limits the usage in practice since most of molecules are\ncompletely unlabelled, and it also introduces the risk of negative transfer for the downstream tasks if\nthey are inconsistent to the graph-level supervised loss.\nIn this paper, we improve the pre-training model for molecular graph by introducing a novel molecular\nrepresentation framework, GROVER, namely, Graph Representation frOm self-superVised mEssage\npassing tRansformer. GROVER constructs two types of self-supervised tasks. For the node/edge-level\ntasks, instead of predicting the node/edge type alone, GROVER randomly masks a local subgraph\nof the target node/edge and predicts this contextual property from node embeddings. In this way,\nGROVER can alleviate the ambiguity problem by considering both the target node/edge and its context\nbeing masked. For the graph-level tasks, by incorporating the domain knowledge, GROVER extracts\nthe semantic motifs existing in molecular graphs and predicts the occurrence of these motifs for a\nmolecule from graph embeddings. Since the semantic motifs can be obtained by a low-cost pattern\nmatching method, GROVER can make use of any molecular to optimize the graph-level embedding.\nWith self-supervised tasks in node-, edge- and graph-level, GROVER can learn rich structural and\nsemantic information of molecules from enormous unlabelled molecular data. Rather, to encode such\ncomplex information, GROVER integrates Message Passing Networks with the Transformer-style\narchitecture to deliver a class of highly expressive encoders of molecules. The Ô¨Çexibility ofGROVER\nallows it to be trained efÔ¨Åciently on large-scale molecular data without requiring any supervision.\nWe pre-train GROVER with 100 million parameters on 10 million of unlabelled molecules‚Äîthe\nbiggest GNN and the largest training dataset that have been applied. We then leverage the pre-trained\nGROVER models to downstream molecular property prediction tasks followed by task-speciÔ¨Åc\nÔ¨Åne-tuning. On the downstream tasks, GROVER achieve 22.4% relative improvement compared\nwith [29] and 7.4% relative improvement compared with [18] on classiÔ¨Åcation tasks. Furthermore,\neven compared with current state-of-the-art results for each data set, we observe a huge relative\nimprovement of GROVER (more than 6% on average) over 11 popular benchmarks.\n2 Related Work\nMolecular Representation Learning. To represent molecules in the vector space, the traditional\nchemical Ô¨Ångerprints, such as ECFP [42], try to encode the neighbors of atoms in the molecule into a\nÔ¨Åx-length vector. To improve the expressive power of chemical Ô¨Ångerprints, some studies [ 7, 10]\nintroduce convolutional layers to learn the neural Ô¨Ångerprints of molecules, and apply the neural\n2\nÔ¨Ångerprints to the downstream tasks, such as property prediction. Following these works, [21, 62]\ntake the SMILES representation [ 59] as input and use RNN-based models to produce molecular\nrepresentations. Recently, many works [ 23, 45, 46] explore the graph convolutional network to\nencode molecular graphs into neural Ô¨Ångerprints. A slot of work [ 44, 61] propose to learn the\naggregation weights by extending the Graph Attention Network (GAT) [54]. To better capture the\ninteractions among atoms, [13] proposes to use a message passing framework and [25, 63] extend\nthis framework to model bond interactions. Furthermore, [30] builds a hierarchical GNN to capture\nmultilevel interactions.\nSelf-supervised Learning on Graphs. Self-supervised learning has a long history in machine\nlearning and has achieved fruitful progresses in many areas, such as computer vision [ 35] and\nlanguage modeling [9]. The traditional graph embedding methods [ 14, 37] deÔ¨Åne different kinds\nof graph proximity, i.e., the vertex proximity relationship, as the self-supervised objective to learn\nvertex embeddings. GraphSAGE [15] proposes to use a random-walk based proximity objective to\ntrain GNN in an unsupervised fashion. [ 36, 50, 55] exploit the mutual information maximization\nscheme to construct objective for GNNs. Recently, two works are proposed to construct unsupervised\nrepresentations for molecular graphs. Liu et.al. [ 29] employ an N-gram model to extract the context\nof vertices and construct the graph representation by assembling the vertex embeddings in short walks\nin the graph. Hu et.al. [ 18] investigate various strategies to pre-train the GNNs and propose three\nself-supervised tasks to learn molecular representations. However, [18] isolates the highly correlated\ntasks of context prediction and node/edge type prediction, which makes it difÔ¨Åcult to preserve domain\nknowledge between the local structure and the node attributes. Besides, the graph-level task in [18] is\nconstructed by the supervised property labels, which is impeded by the limited number of supervised\nlabels of molecules and has demonstrated the negative transfer in the downstream tasks. Contrast\nwith [18], the molecular representations derived by our method are more appropriate in terms of\npersevering the domain knowledge, which has demonstrated remarkable effectiveness in downstream\ntasks without negative transfer.\n3 Preliminaries of Transformer-style Models and Graph Neural Networks\nWe brieÔ¨Çy introduce the concepts of supervised graph learning, Transformer [53], and GNNs in this\nsection.\nSupervised learning tasks of graphs. A molecule can be abstracted as a graph G= (V,E), where\n|V| = n refers to a set of n nodes (atoms) and |E| = m refers to a set of m edges (bonds) in\nthe molecule. Nv is used to denote the set of neighbors of node v. We use xv to represent the\ninitial features of node v, and euv as the initial features of edge (u,v). In graph learning, there\nare usually two categories of supervised tasks: i) Node classiÔ¨Åcation/regression, where each node\nv has a label/target yv, and the task is to learn to predict the labels of unseen nodes; ii) Graph\nclassiÔ¨Åcation/regression, where a set of graphs {G1,...,G N}and their labels/targets {y1,...,y N}are\ngiven, and the task is to predict the label/target of a new graph.\nAttention mechanism and the Transformer-style architectures. The attention mechanism is the\nmain building block of Transformer. We focus on multi-head attention, which stacks several scaled\ndot-product attention layers together and allows parallel running. One scaled dot-product attention\nlayer takes a set of queries, keys, values (q,k,v) as inputs. Then it computes the dot products of\nthe query with all keys, and applies a softmax function to obtain the weights on the values. By\nstacking the set of (q,k,v)s into matrices (Q,K,V), it admits highly optimized matrix multiplication\noperations. SpeciÔ¨Åcally, the outputs can be arranged as a matrix:\nAttention(Q,K,V) = softmax(QK‚ä§/\n‚àö\nd)V, (1)\nwhere dis the dimension of q and k. Suppose we arrange k attention layers into the multi-head\nattention, then its output matrix can be written as,\nMultiHead(Q,K,V) = Concat(head1,..., headk)WO,\nheadi = Attention(QWQ\ni ,KWK\ni ,VWV\ni ), (2)\nwhere WQ\ni ,WK\ni ,WV\ni are the projection matrices of head i.\nGraph Neural Networks (GNNs). Recently, GNNs have received a surge of interest in various\ndomains, such as knowledge graph, social networks and drug discovery. The key operation of\n3\nGNNs lies in a message passing process, which involves message passing (also called neighborhood\naggregation) between the nodes in the graph. The message passing operation iteratively updates a\nnode v‚Äôs hidden states, hv, by aggregating the hidden states of v‚Äôs neighboring nodes and edges.\nIn general, the message passing process involves several iterations, each iteration can be further\npartitioned into several hops. Suppose there are L iterations, and iteration l contains Kl hops.\nFormally, in iteration l, the k-th hop can be formulated as,\nm(l,k)\nv = AGGREGATE(l)({(h(l,k‚àí1)\nv ,h(l,k‚àí1)\nu ,euv) |u‚ààNv}), (3)\nh(l,k)\nv = œÉ(W(l)m(l,k)\nv + b(l)),\nwhere m(l,k)\nv is the aggregated message, andœÉ(¬∑) is some activation function. We make the convention\nthat h(l,0)\nv := h(l‚àí1,Kl‚àí1)\nv . There are several popular ways of choosing AGGREGATE(l)(¬∑), such as\nmean, max pooling and graph attention mechanism [15, 54]. For one iteration of message passing,\nthere are a layer of trainable parameters (i.e., parameters inside AGGREGATE(l)(¬∑), W(l) and b(l).\nThese parameters are shared across the Kl hops within the iteration l. After Literations of message\npassing, the hidden states of the last hop in the last iteration are used as the embeddings of the nodes,\ni.e., h(L,KL)\nv ,v ‚ààV. Lastly, a READOUT operation is applied to get the graph-level representation,\nhG = READOUT({h(0,K0)\nv ,..., h(L,KL)\nv |v‚ààV}). (4)\n4 The GROVER Pre-training Framework\nThis section contains details of our pre-training architecture together with the well-designed self-\nsupervision tasks. On a high level, the model is a Transformer-based neural network with tailored\nGNNs as the self-attention building blocks. The GNNs therein enable capturing structural information\nin the graph data and information Ô¨Çow on both the node and edge message passing paths. Furthermore,\nwe introduce a dynamic message passing scheme in the tailored GNN, which is proved to boost the\ngeneralization performance of GROVER models.\n4.1 Details of Model Architecture\nGROVER consists of two modules: the node GNN transformer and edge GNN transformer. In order\nto ease the exposition, we will only explain details of the node GNN transformer (abbreviated as node\nGTransformer) in the sequel, and ignore the edge GNN transformer since it has a similar structure.\nFigure 1 demonstrates the overall architecture of node GTransformer. More details of GROVER are\ndeferred to Appendix A.\nMulti-Head Attention\nLayerNorm\nFeed Forward\nNode Embed\nAggregate2Node\nConcat\nLayerNormFeed Forward\nEdge Embed\nAggregate2Edge\nConcat\nLayerNorm\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPNNodeDyMPNQ K VNodeDyMPNNodeDyMPN\nEdge-view GTransformer\nNode-view GTransformer\nLinear\nLong-range residual connection\nInput Graph\nFigure 1: Overview of GTransformer.\nGNN Transformer (GTransformer). The key component\nof the node GTransformer is our proposed graph multi-\nhead attention component, which is the attention blocks\ntailored to structural input data. A vanilla attention block,\nsuch as that in Equation (1), requires vectorized inputs.\nHowever, graph inputs are naturally structural data that are\nnot vectorized. So we design a tailored GNNs ( dyMPN,\nsee the following sections for details) to extract vectors\nas queries, keys and values from nodes of the graph, then\nfeed them into the attention block.\nThis strategy is simple yet powerful, because it enables\nutilizing the highly expressive GNN models, to better\nmodel the structural information in molecular data. The\nhigh expressiveness of GTransformer can be attributed to\nits bi-level information extraction framework. It is well-\nknown that the message passing process captures local\nstructural information of the graph, therefore using the\noutputs of the GNN model as queries, keys and values\nwould get the local subgraph structure involved, thus constituting the Ô¨Årst level of information\nextraction. Meanwhile, the Transformer encoder can be viewed as a variant of the GAT [22, 54] on a\nfully connected graph constructed by V. Hence, using Transformer encoder on top of these queries,\n4\nÔºü\nContextual property extraction Subgraph masking\nÔºü\nÔºü\nPrediction\n‚Ä¶\n‚Ä¶\nnode-based\nedge-based\nnode/edge\nrepresentation\nmasked part\n‚Ä¶\nSemantic motifs from\ndomain knowledge Graph-level Prediction\ngraph\nrepresentation\nInput molecule\nMolecular graph\nContextual property prediction (node/edge level task) Graph-level motif prediction\nùëò = 1\nùëò = 1\nFigure 2: Overview of the designed self-supervised tasks of GROVER.\nkeys and values makes it possible to extract global relations between nodes, which enables the second\nlevel of information extraction. This bi-level information extraction strategy largely enhances the\nrepresentational power of GROVER models.\nAdditionally, GTransformer applies a single long-range residual connection from the input feature to\nconvey the initial node/edge feature information directly to the last layers ofGTransformer instead of\nmultiple short-range residual connections in the original Transformer architecture. Two beneÔ¨Åts could\nbe obtained from this single long-range residual connection: i) like ordinary residual connections,\nit improves the training process by alleviating the vanishing gradient problem [17], ii) compared to\nthe various short-range residual connections in the Transformer encoder, our long-range residual\nconnection can alleviate the over-smoothing [20, 34] problem in the message passing process.\nDynamic Message Passing Network (dyMPN). The general message passing process (see Equa-\ntion (3)) has two hyperparameters: number of iterations/layers Land number of hops Kl,l = 1,...,L\nwithin each iteration. The number of hops is closely related to the size of the receptive Ô¨Åeld of the\ngraph convolution operation, which would affect generalizability of the message passing model.\nGiven a Ô¨Åxed number of layers L, we Ô¨Ånd out that the pre-speciÔ¨Åed number of hops might not\nwork well for different kinds of dataset. Instead of pre-speciÔ¨Åed Kl, we develop a randomized\nstrategy for choosing the number of message passing hops during training process: at each epoch, we\nchoose Kl from some random distribution for layer l. Two choices of randomization work well: i)\nKl ‚àºU(a,b), drawn from a uniform distribution; ii)Klis drawn from a truncated normal distribution\nœÜ(¬µ,œÉ,a,b ) , which is derived from that of a normally distributed random variable by bounding the\nrandom variable from both bellow and above. SpeciÔ¨Åcally, let its support be x ‚àà[a,b], then the\np.d.f. is f(x) =\n1‚àö\n2œÄ exp [‚àí1\n2 ( x‚àí¬µ\nœÉ )2]\nœÉ[Œ¶( b‚àí¬µ\nœÉ )‚àíŒ¶( a‚àí¬µ\nœÉ )] , where Œ¶(x) = 1\n2 (1 + erf( x‚àö\n2 )) is the cumulative distribution of\na standard normal distribution.\nThe above randomized message passing scheme enables random receptive Ô¨Åeld for each node in\ngraph convolution operation. We call the induced network Dynamic Message Passing networks\n(abbreviated as dyMPN). Extensive experimental veriÔ¨Åcation demonstrates that dyMPN enjoys\nbetter generalization performance than vanilla message passing networks without the randomization\nstrategy.\n4.2 Self-supervised Task Construction for Pre-training\nThe success of the pre-training model crucially depends on the design of self-supervision tasks.\nDifferent from Hu et.al. [ 18], to avoid negative transfer on downstream tasks, we do not use the\nsupervised labels in pre-training and propose new self-supervision tasks on both of these two levels:\ncontextual property prediction and graph-level motif prediction, which are sketched in Figure 2.\n3\nC\nN\nKey: C_N-DOUBLE1_O-SINGLE1\nKey: DOUBLE_C-SINGLE1_N-SINGLE1\nC CC\nON\nùëò = 1\nFigure 3: Illustration of contextual proper-\nties.\nContextual Property Prediction. A good self-\nsupervision task on the node level should satisfy the fol-\nlowing properties: 1) The prediction target is reliable and\neasy to get; 2) The prediction target should reÔ¨Çect con-\ntextual information of the node/edge. Guided by these\ncriteria, we present the tasks on both nodes and edges.\nThey both try to predict the context-aware properties of\nthe target node/edge within some local subgraph. What\nkinds of context-aware properties shall one use? We de-\nÔ¨Åne recurrent statistical properties of local subgraph in the\nfollowing two-step manner (let us take the node subgraph\n5\nin Figure 3 as the example): i) Given a target node (e.g., the Carbon atom in red color), we extract its\nlocal subgraph as its k-hop neighboring nodes and edges. When k=1, it involves the Nitrogen atom,\nOxygen atom, the double bond and single bond. ii) We extract statistical properties of this subgraph,\nspeciÔ¨Åcally, we count the number of occurrence of (node, edge) pairs around the center node, which\nmakes the term of node-edge-counts. Then we list all the node-edge counts terms in alphabetical\norder, which constitutes the Ô¨Ånal property: e.g., C_N-DOUBLE1_O-SINGLE1 in the example. This\nstep can be viewed as a clustering process: the subgraphs are clustered according to the extracted\nproperties, one property corresponds to a cluster of subgraphs with the same statistical property.\nWith the context-aware property deÔ¨Åned, the contextual property prediction task works as follows:\ngiven a molecular graph, after feeding it into the GROVER encoder, we obtain embeddings of its\natoms and bonds. Suppose randomly choose the atomvand its embedding is hv. Instead of predicting\nthe atom type of v, we would like hv to encode some contextual information around node v. The\nway to achieve this target is to feed hv into a very simple model (such as a fully connected layer),\nthen use the output to predict the contextual properties of node v. This prediction is a multi-class\nprediction problem (one class corresponds to one contextual property).\nGraph-level Motif Prediction. Graph-level self-supervision task also needs reliable and cheap\nlabels. Motifs are recurrent sub-graphs among the input graph data, which are prevalent in molecular\ngraph data. One important class of motifs in molecules are functional groups, which encodes the\nrich domain knowledge of molecules and can be easily detected by the professional software, such\nas RDKit [27]. Formally, the motif prediction task can be formulated as a multi-label classiÔ¨Åcation\nproblem, where each motif corresponds to one label. Suppose we are considering the presence of p\nmotifs {m1,..., mp}in the molecular data. For one speciÔ¨Åc molecule (abstracted as a graph G), we\nuse RDKit to detect whether each of the motif shows up in G, then use it as the target of the motif\nprediction task.\n4.3 Fine-tuning for Downstream Tasks\nAfter pre-training GROVER models on massive unlabelled data with the designed self-supervised\ntasks, one should obtain a high-quality molecular encoder which is able to output embeddings for\nboth nodes and edges. These embeddings can be used for downstream tasks through the Ô¨Åne-tuning\nprocess. Various downstream tasks could beneÔ¨Åt from the pre-trained GROVER models. They can be\nroughly divided into three categories: node level tasks, e.g., node classiÔ¨Åcation; edge level tasks, e.g.,\nlink prediction; and graph level tasks, such as the property prediction for molecules. Take the graph\nlevel task for instance. Given node/edge embeddings output by the GROVER encoder, we can apply\nsome READOUT function (Equation (4)) to get the graph embedding Ô¨Årstly, then use additional\nmultiple layer perceptron (MLP) to predict the property of the molecular graph. One would use part\nof the supervised data to Ô¨Åne-tune both the encoder and additional parameters (READOUT and MLP).\nAfter several epochs of Ô¨Åne-tuning, one can expect a well-performed model for property prediction.\n5 Experiments\nPre-training Data Collection. We collect 11 million (M) unlabelled molecules sampled from\nZINC15 [48] and Chembl [11] datasets to pre-train GROVER. We randomly split 10% of unlabelled\nmolecules as the validation sets for model selection.\nFine-tuning Tasks and Datasets. To thoroughly evaluate GROVER on downstream tasks, we\nconduct experiments on 11 benchmark datasets from the MoleculeNet [ 60] with various targets,\nsuch as quantum mechanics, physical chemistry, biophysics and physiology.3 Details are deferred to\nAppendix B.1. In machine learning tasks, random splitting is a common process to split the dataset.\nHowever, for molecular property prediction, scaffold splitting [ 2] offers a more challenging yet\nrealistic way of splitting. We adopt the scaffold splitting method with a ratio for train/validation/test\nas 8:1:1. For each dataset, as suggested by [60], we apply three independent runs on three random-\nseeded scaffold splitting and report the mean and standard deviations.\nBaselines. We comprehensively evaluateGROVER against 10 popular baselines from MoleculeNet\n[60] and several state-of-the-arts (STOAs) approaches. Among them, TF_Roubust [40] is a DNN-\nbased mulitask framework taking the molecular Ô¨Ångerprints as the input. GraphConv [24], Weave\n3All datasets can be downloaded from http://moleculenet.ai/datasets-1\n6\nTable 1: The performance comparison. The numbers in brackets are the standard deviation. The\nmethods in green are pre-trained methods.\nClassiÔ¨Åcation (Higher is better)\nDataset BBBP SIDER ClinTox BACE Tox21 ToxCast\n# Molecules 2039 1427 1478 1513 7831 8575\nTF_Robust [40] 0.860(0.087) 0.607(0.033) 0.765(0.085) 0.824(0.022) 0.698(0.012) 0.585(0.031)\nGraphConv [24] 0.877(0.036) 0.593(0.035) 0.845(0.051) 0.854(0.011) 0.772(0.041) 0.650(0.025)\nWeave [23] 0.837(0.065) 0.543(0.034) 0.823(0.023) 0.791(0.008) 0.741(0.044) 0.678(0.024)\nSchNet [45] 0.847(0.024) 0.545(0.038) 0.717(0.042) 0.750(0.033) 0.767(0.025) 0.679(0.021)\nMPNN [13] 0.913(0.041) 0.595(0.030) 0.879(0.054) 0.815(0.044) 0.808(0.024) 0.691(0.013)\nDMPNN [63] 0.919(0.030) 0.632(0.023) 0.897(0.040) 0.852(0.053) 0.826(0.023) 0.718(0.011)\nMGCN [30] 0.850(0.064) 0.552(0.018) 0.634(0.042) 0.734(0.030) 0.707(0.016) 0.663(0.009)\nAttentiveFP [61] 0.908(0.050) 0.605(0.060) 0.933(0.020) 0.863(0.015) 0.807(0.020) 0.579(0.001)\nN-GRAM [29] 0.912(0.013) 0.632(0.005) 0.855(0.037) 0.876(0.035) 0.769(0.027) -4\nHU. et.al[18] 0.915(0.040) 0.614(0.006) 0.762(0.058) 0.851(0.027) 0.811(0.015) 0.714(0.019)\nGROVERbase 0.936(0.008) 0.656(0.006) 0.925(0.013) 0.878(0.016) 0.819(0.020) 0.723(0.010)\nGROVERlarge 0.940(0.019) 0.658(0.023) 0.944(0.021) 0.894(0.028) 0.831(0.025) 0.737(0.010)\nRegression (Lower is better)\nDataset FreeSolv ESOL Lipo QM7 QM8\n# Molecules 642 1128 4200 6830 21786\nTF_Robust [40] 4.122(0.085) 1.722(0.038) 0.909(0.060) 120.6(9.6) 0.024(0.001)\nGraphConv [24] 2.900(0.135) 1.068(0.050) 0.712(0.049) 118.9(20.2) 0.021(0.001)\nWeave [23] 2.398(0.250) 1.158(0.055) 0.813(0.042) 94.7(2.7) 0.022(0.001)\nSchNet [45] 3.215(0.755) 1.045(0.064) 0.909(0.098) 74.2(6.0) 0.020(0.002)\nMPNN [13] 2.185(0.952) 1.167(0.430) 0.672(0.051) 113.0(17.2) 0.015(0.002)\nDMPNN [63] 2.177(0.914) 0.980(0.258) 0.653(0.046) 105.8(13.2) 0.0143(0.002)\nMGCN [30] 3.349(0.097) 1.266(0.147) 1.113(0.041) 77.6(4.7) 0.022(0.002)\nAttentiveFP [61] 2.030(0.420) 0.853(0.060) 0.650(0.030) 126.7(4.0) 0.0282(0.001)\nN-GRAM [29] 2.512(0.190) 1.100(0.160) 0.876(0.033) 125.6(1.5) 0.0320(0.003)\nGROVERbase 1.592(0.072) 0.888(0.116) 0.563(0.030) 72.5(5.9) 0.0172(0.002)\nGROVERlarge 1.544(0.397) 0.831(0.120) 0.560(0.035) 72.6(3.8) 0.0125(0.002)\n[23] and SchNet [45] are three graph convolutional models. MPNN [13] and its variants DMPNN\n[63] and MGCN [30] are models considering the edge features during message passing. AttentiveFP\n[61] is an extension of the graph attention network. SpeciÔ¨Åcally, to demonstrate the power of our\nself-supervised strategy, we also compare GROVER with two pre-trained models: N-Gram [29] and\nHu et.al [18]. We only report classiÔ¨Åcation results for [18] since the original implementation do not\nadmit regression task without non-trivial modiÔ¨Åcations.\nExperimental ConÔ¨Ågurations. We use Adam optimizer for both pre-train and Ô¨Åne-tuning. The\nNoam learning rate scheduler [ 9] is adopted to adjust the learning rate during training. SpeciÔ¨Åc\nconÔ¨Ågurations are:\nGROVER Pre-training. For the contextual property prediction task, we set the context radius\nk= 1 to extract the contextual property dictionary, and obtain 2518 and 2686 distinct node and edge\ncontextual properties as the node and edge label, respectively. For each molecular graph, we randomly\nmask 15% of node and edge labels for prediction. For the graph-level motif prediction task, we use\nRDKit [27] to extract 85 functional groups as the motifs of molecules. We represent the label of\nmotifs as the one-hot vector. To evaluate the effect of model size, we pre-train two GROVER models,\nGROVERbase and GROVERlarge with different hidden sizes, while keeping all other hyper-parameters\nthe same. SpeciÔ¨Åcally, GROVERbase contains ‚àº48M parameters and GROVERlarge contains ‚àº100M\nparameters. We use 250 Nvidia V100 GPUs to pre-trainGROVERbase and GROVERlarge. Pre-training\nGROVERbase and GROVERlarge took 2.5 days and 4 days respectively. For the models depicted in\nSection 5.2, we use 32 Nvidia V100 GPUs to pre-train the GROVER model and its variants.\nFine-tuning Procedure. We use the validation loss to select the best model. For each training\nprocess, we train models for 100 epochs. For hyper-parameters, we perform the random search on the\nvalidation set for each dataset and report the best results. More pre-training and Ô¨Åne-tuning details\nare deferred to Appendix C and Appendix D.\n4The result is not presented since N-Gram on ToxCast is too time consuming to be Ô¨Ånished in time.\n7\n5.1 Results on Downstream Tasks\nTable 1 documents the overall results of all models on all datasets, where the cells in gray indicate the\nprevious SOTAs, and the cells in blue indicates the best result achieved byGROVER. Table 1 offers\nthe following observations: (1) GROVER models consistently achieve the best performance on all\ndatasets with large margin on most of them. The overall relative improvement is 6.1% on all datasets\n(2.2% on classiÔ¨Åcation tasks and 10.8% on regression tasks).5. This remarkable boosting validates\nthe effectiveness of the pre-training model GROVER for molecular property prediction tasks. (2)\nSpeciÔ¨Åcally, GROVERbase outperforms the STOAs on 8/11 datasets, while GROVERlarge surpasses\nthe STOAs on all datasets. This improvement can be attributed to the high expressive power of the\nlarge model, which can encode more information from the self-supervised tasks. (3) In the small\ndataset FreeSolv with only 642 labeled molecules, GROVER gains a 23.9% relative improvement\nover existing SOTAs. This conÔ¨Årms the strength of GROVER since it can signiÔ¨Åcantly help with the\ntasks with very little label information.\n5.2 Ablation Studies on Design Choices of the GROVER Framework\n5.2.1 How Useful is the Self-supervised Pre-training?\nTo investigate the contribution of the self-supervision strategies, we compare the performances of\npre-trained GROVER and GROVER without pre-training on classiÔ¨Åcation datasets, both of which\nfollow the same hyper-parameter setting. We report the comparison of classiÔ¨Åcation task in Table 2,\nit is not supervising that the performance of GROVER becomes worse without pre-training. The self-\nsupervised pre-training leads to a performance boost with an average AUC increase of 3.8% over the\nmodel without pre-training. This conÔ¨Årms that the self-supervised pre-training strategy can learn the\nimplicit domain knowledge and enhance the prediction performance of downstream tasks. Notably,\nthe datasets with fewer samples, such as SIDER, ClinTox and BACE gain a larger improvement\nthrough the self-supervised pre-training. It re-conÔ¨Årms the effectiveness of the self-supervised\npre-training for the task with insufÔ¨Åcient labeled molecules.\n5.2.2 How Powerful is GTransformer Backbone?\n0 50 100 150 200 250\nEpoch\n10 1\n100\nLoss\nTraining Loss\nGROVER-GTransformer\nGROVER-GIN\nGROVER-MPNN\n0 50 100 150 200 250\nEpoch\n10 1\n100\nLoss\nValidation Loss\nGROVER-GTransformer\nGROVER-GIN\nGROVER-MPNN\nFigure 4: The training and validation losses on different backbones.\nTo verify the expressive power of\nGTransformer, we implement GIN\nand MPNN based on our frame-\nwork. We use a toy data set with\n600K unlabelled molecules to pre-\ntrain GROVER with different back-\nbones under the same training set-\nting with nearly the same number of\nparameters (38M parameters). As\nshown in Figure 4, GROVER with GTransformer backbone outperforms GIN and MPNN in both\ntraining and validation, which again veriÔ¨Åes the effectiveness of GTransformer.\n5.2.3 Effect of the Proposed dyMPN and GTransformer.\nTo justify the rationale behind the proposed GTransformer and dyMPN, we implement two vari-\nants: GROVER w/o dyMPN and GROVER w/o GTrans. GROVER w/o dyMPN Ô¨Åx the number of\nmessage passing hops Kl, while GROVER w/o GTrans replace the GTransformer with the original\nTransformer. We use the same toy data set to train GROVER w/o dyMPN and GROVER w/o GTrans\nunder the same settings in Section 5.2.2. Figure 5 displays the curve of training and validation loss for\nthree models. First, GROVER w/o GTrans is the worst one in both training and validation. It implies\nthat trivially combining the GNN and Transformer can not enhance the expressive power of GNN.\nSecond, dyMPN slightly harm the training loss by introducing randomness in the training process.\nHowever, the validation loss becomes better. Therefore, dyMPN brings a better generalization ability\nto GROVER by randomizing the receptive Ô¨Åeld for every message passing step. Overall, with new\nTransformer-style architecture and the dynamic message passing mechanism, GROVER enjoys high\n5We use relative improvement [52] to provide the uniÔ¨Åed descriptions.\n8\nexpressive power and can well capture the structural information in molecules, thus helping with\nvarious downstream molecular prediction tasks.\nGROVERNo Pretrain Abs. Imp.BBBP (2039) 0.940 0.911 +0.029SIDER (1427) 0.658 0.624 +0.034ClinTox (1478)0.944 0.884 +0.060BACE (1513) 0.894 0.858 +0.036Tox21 (7831) 0.831 0.803 +0.028ToxCast (8575)0.737 0.721 +0.016Average 0.834 0.803 +0.038\nTable 2: Comparison between GROVER\nwith and without pre-training.\n0 50 100 150 200 250\nEpoch\n10 1\n100\nLoss\nTraining Loss\nGROVER\nGROVER w/o DyMPN\nGROVER w/o GTrans\n0 50 100 150 200 250\nEpoch\n10 1\n100\nLoss\nValidation Loss\nGROVER\nGROVER w/o DyMPN\nGROVER w/o GTrans\nFigure 5: The training and validation loss of GROVER and\nits variants.\n6 Conclusion and Future Works\nWe explore the potential of the large-scale pre-trained GNN models in this work. With well-designed\nself-supervised tasks and largely-expressive architecture, our model GROVER can learn rich implicit\ninformation from the enormous unlabelled graphs. More importantly, by Ô¨Åne-tuning on GROVER,\nwe achieve huge improvements (more than 6% on average) over current STOAs on 11 challenging\nmolecular property prediction benchmarks, which Ô¨Årst veriÔ¨Åes the power of self-supervised pre-\ntrained approaches in the graph learning area.\nDespite the successes, there is still room to improve GNN pre-training in the following aspects:\nMore self-supervised tasks. Well designed self-supervision tasks are the key of success for GNN\npre-training. Except for the tasks presented in this paper, other meaningful tasks would also boost\nthe pre-training performance, such as distance-preserving tasks and tasks that getting 3D input\ninformation involved. More downstream tasks. It is desirable to explore a larger category of\ndownstream tasks, such as node prediction and link prediction tasks on different kinds of graphs.\nDifferent categories of downstream tasks might prefer different pre-training strategies/self-supervision\ntasks, which is worthwhile to study in the future. Wider and deeper models. Larger models are\ncapable of capturing richer semantic information for more complicated tasks, as veriÔ¨Åed by several\nstudies in the NLP area. It is also interesting to employ even larger models and data than GROVER.\nHowever, one might need to alleviate potential problems when training super large models of GNN,\nsuch as gradient vanishing and oversmoothing.\nBroader Impact\nIn this paper, we have developed a self-supervised pre-trained GNN model‚Äî GROVER to extract the\nuseful implicit information from massive unlabelled molecules and the downstream tasks can largely\nbeneÔ¨Åt from this pre-trained GNN models. Below is the broader impact of our research:\n- For machine learning community: This work demonstrates the success of pre-training\napproach on Graph Neural Networks. It is expected that our research will open up a new\nvenue on an in-depth exploration of pre-trained GNNs for broader potential applications,\nsuch as social networks and knowledge graphs.\n- For the drug discovery community: Researchers from drug discovery can beneÔ¨Åt from\nGROVER from two aspects. First, GROVER has encoded rich structural information of\nmolecules through the designing of self-supervision tasks. It can also produce feature vectors\nof atoms and molecule Ô¨Ångerprints, which can directly serve as inputs of downstream tasks.\nSecond, GROVER is designed based on Graph Neural Networks and all the parameters are\nfully differentiable. So it is easy to Ô¨Åne-tune GROVER in conjunction with speciÔ¨Åc drug\ndiscovery tasks, in order to achieve better performance. We hope that GROVER can help\nwith boosting the performance of various drug discovery applications, such as molecular\nproperty prediction and virtual screening.\n9\nAcknowledgements and Disclosure of Funding\nThis work is jointly supported by Tencent AI Lab Rhino-Bird Visiting Scholars Program (VS202006),\nChina Postdoctoral Science Foundation (Grant No.2020M670337), and the National Natural Science\nFoundation of China (Grant No. 62006137). The GPU resources and distributed training optimization\nare supported by Tencent Jizhi Team. We would thank the anonymous reviewers for their valuable\nsuggestions. Particularly, Yu Rong wants to thank his wife, Yunman Huang, for accepting his proposal\nfor her hand in marriage.\nReferences\n[1] Tox21 challenge, 2017. https://tripod.nih.gov/tox21/challenge/.\n[2] Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks.\nJournal of medicinal chemistry, 39(15):2887‚Äì2893, 1996.\n[3] Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang.\nRumor detection on social media with bi-directional graph convolutional networks. In AAAI\n2020, 2020.\n[4] L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in\nthe chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.\n[5] Hongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona, and Thomas Blaschke. The\nrise of deep learning in drug discovery. Drug discovery today, 23(6):1241‚Äì1250, 2018.\n[6] Zhengdao Chen, Xiang Li, and Joan Bruna. Supervised community detection with line graph\nneural networks. arXiv preprint arXiv:1705.08415, 2017.\n[7] Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola, and Klavs F Jensen.\nConvolutional embedding of attributed molecular graphs for physical property prediction.\nJournal of chemical information and modeling, 57(8):1757‚Äì1772, 2017.\n[8] John S Delaney. Esol: estimating aqueous solubility directly from molecular structure. Journal\nof chemical information and computer sciences, 44(3):1000‚Äì1005, 2004.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[10] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,\nAl√°n Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning\nmolecular Ô¨Ångerprints. In Advances in neural information processing systems, pages 2224‚Äì\n2232, 2015.\n[11] Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey,\nYvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: a\nlarge-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100‚Äì\nD1107, 2012.\n[12] Kaitlyn M Gayvert, Neel S Madhukar, and Olivier Elemento. A data-driven approach to\npredicting successes and failures of clinical trials. Cell chemical biology, 23(10):1294‚Äì1301,\n2016.\n[13] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. In ICML, pages 1263‚Äì1272. JMLR. org, 2017.\n[14] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In\nProceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and\ndata mining, pages 855‚Äì864, 2016.\n[15] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\ngraphs. In Advances in neural information processing systems, pages 1024‚Äì1034, 2017.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiÔ¨Åers:\nSurpassing human-level performance on imagenet classiÔ¨Åcation. In Proceedings of the IEEE\ninternational conference on computer vision, pages 1026‚Äì1034, 2015.\n10\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770‚Äì778, 2016.\n[18] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure\nLeskovec. Pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.\n[19] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4700‚Äì4708, 2017.\n[20] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-\nsmoothing for general graph convolutional networks. arXiv e-prints, pages arXiv‚Äì2008, 2020.\n[21] Stanis≈Çaw JastrzÀõ ebski, Damian Le¬¥sniak, and Wojciech Marian Czarnecki. Learning to smile (s).\narXiv preprint arXiv:1602.06289, 2016.\n[22] Chaitanya Joshi. Transformers are graph neural networks, 2020. https://\ngraphdeeplearning.github.io/post/transformers-are-gnns/.\n[23] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular\ngraph convolutions: moving beyond Ô¨Ångerprints. Journal of computer-aided molecular design,\n30(8):595‚Äì608, 2016.\n[24] Thomas N. Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional\nnetworks. In International Conference on Learning Representations (ICLR), 2017.\n[25] Johannes Klicpera, Janek Gro√ü, and Stephan G√ºnnemann. Directional message passing for\nmolecular graphs. In International Conference on Learning Representations (ICLR), 2020.\n[26] Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. The sider database of drugs and\nside effects. Nucleic acids research, 44(D1):D1075‚ÄìD1079, 2015.\n[27] Greg Landrum et al. Rdkit: Open-source cheminformatics. 2006.\n[28] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang. Semi-\nsupervised graph classiÔ¨Åcation: A hierarchical graph perspective. In The World Wide Web\nConference, pages 972‚Äì982. ACM, 2019.\n[29] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised\nrepresentation for graphs, with applications to molecules. In Advances in Neural Information\nProcessing Systems, pages 8464‚Äì8476, 2019.\n[30] Chengqiang Lu, Qi Liu, Chao Wang, Zhenya Huang, Peize Lin, and Lixin He. Molecular\nproperty prediction: A multilevel quantum interactions modeling perspective. In Proceedings of\nthe AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33, pages 1052‚Äì1060, 2019.\n[31] Jing Ma, Wei Gao, and Kam-Fai Wong. Detect rumors on twitter by promoting information\ncampaigns with generative adversarial learning. In The World Wide Web Conference, pages\n3049‚Äì3055, 2019.\n[32] Ines Filipa Martins, Ana L Teixeira, Luis Pinheiro, and Andre O Falcao. A bayesian approach\nto in silico blood-brain barrier penetration modeling. Journal of chemical information and\nmodeling, 52(6):1686‚Äì1697, 2012.\n[33] David L Mobley and J Peter Guthrie. Freesolv: a database of experimental and calculated\nhydration free energies, with input Ô¨Åles.Journal of computer-aided molecular design, 28(7):711‚Äì\n720, 2014.\n[34] Kenta Oono and Taiji Suzuki. On asymptotic behaviors of graph cnns from dynamical systems\nperspective. arXiv preprint arXiv:1905.10947, 2019.\n[35] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisen-\nsory features. In Proceedings of the European Conference on Computer Vision (ECCV), pages\n631‚Äì648, 2018.\n[36] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and\nJunzhou Huang. Graph representation learning via graphical mutual information maximization.\nIn Proceedings of The Web Conference 2020, pages 259‚Äì270, 2020.\n11\n[37] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pages 701‚Äì710, 2014.\n[38] Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever. Improving language\nunderstanding with unsupervised learning. Technical report, OpenAI, 2018.\n[39] Raghunathan Ramakrishnan, Mia Hartmann, Enrico Tapavicza, and O Anatole V on Lilienfeld.\nElectronic spectra from tddft and machine learning in chemical space. The Journal of chemical\nphysics, 143(8):084111, 2015.\n[40] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay\nPande. Massively multitask networks for drug discovery. arXiv preprint arXiv:1502.02072,\n2015.\n[41] Ann M Richard, Richard S Judson, Keith A Houck, Christopher M Grulke, Patra V olarath,\nInthirany Thillainadarajah, Chihae Yang, James Rathman, Matthew T Martin, John F Wambaugh,\net al. Toxcast chemical landscape: paving the road to 21st century toxicology.Chemical research\nin toxicology, 29(8):1225‚Äì1251, 2016.\n[42] David Rogers and Mathew Hahn. Extended-connectivity Ô¨Ångerprints. Journal of chemical\ninformation and modeling, 50(5):742‚Äì754, 2010.\n[43] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep\ngraph convolutional networks on node classiÔ¨Åcation. In International Conference on Learning\nRepresentations, 2020.\n[44] Seongok Ryu, Jaechang Lim, Seung Hwan Hong, and Woo Youn Kim. Deeply learning molec-\nular structure-property relationships using attention-and gate-augmented graph convolutional\nnetwork. arXiv preprint arXiv:1805.10988, 2018.\n[45] Kristof Sch√ºtt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre\nTkatchenko, and Klaus-Robert M√ºller. Schnet: A continuous-Ô¨Ålter convolutional neural network\nfor modeling quantum interactions. In Advances in neural information processing systems ,\npages 991‚Äì1001, 2017.\n[46] Kristof T Sch√ºtt, Farhad Arbabzadah, Stefan Chmiela, Klaus R M√ºller, and Alexandre\nTkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature com-\nmunications, 8(1):1‚Äì8, 2017.\n[47] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in\nTensorFlow. arXiv preprint arXiv:1802.05799, 2018.\n[48] Teague Sterling and John J Irwin. Zinc 15‚Äìligand discovery for everyone. Journal of chemical\ninformation and modeling, 55(11):2324‚Äì2337, 2015.\n[49] Govindan Subramanian, Bharath Ramsundar, Vijay Pande, and Rajiah Aldrin Denny. Computa-\ntional modeling of Œ≤-secretase 1 (bace-1) inhibitors using ligand based approaches. Journal of\nchemical information and modeling, 56(10):1936‚Äì1949, 2016.\n[50] Fan-Yun Sun, Jordan Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and\nsemi-supervised graph-level representation learning via mutual information maximization. In\nInternational Conference on Learning Representations, 2019.\n[51] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing systems, pages 3104‚Äì3112, 2014.\n[52] Leo T√∂rnqvist, Pentti Vartia, and Yrj√∂ O Vartia. How should relative changes be measured?\nThe American Statistician, 39(1):43‚Äì46, 1985.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998‚Äì6008, 2017.\n[54] Petar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n[55] Petar VeliÀáckovi¬¥c, William Fedus, William L Hamilton, Pietro Li√≤, Yoshua Bengio, and R Devon\nHjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.\n12\n[56] Izhar Wallach, Michael Dzamba, and Abraham Heifets. Atomnet: a deep convolutional\nneural network for bioactivity prediction in structure-based drug discovery. arXiv preprint\narXiv:1510.02855, 2015.\n[57] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang\nWang, and Xiaoou Tang. Residual attention network for image classiÔ¨Åcation. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 3156‚Äì3164, 2017.\n[58] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert:\nLarge scale unsupervised pre-training for molecular property prediction. In Proceedings of\nthe 10th ACM International Conference on Bioinformatics, Computational Biology and Health\nInformatics, pages 429‚Äì436, 2019.\n[59] David Weininger, Arthur Weininger, and Joseph L Weininger. Smiles. 2. algorithm for generation\nof unique smiles notation. Journal of chemical information and computer sciences, 29(2):97‚Äì\n101, 1989.\n[60] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S\nPappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine\nlearning. Chemical Science, 9(2):513‚Äì530, 2018.\n[61] Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li,\nZhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of\nmolecular representation for drug discovery with the graph attention mechanism. Journal of\nmedicinal chemistry, 2019.\n[62] Zheng Xu, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Seq2seq Ô¨Ångerprint: An unsupervised\ndeep molecular embedding for drug discovery. In BCB, 2017.\n[63] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel\nGuzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molec-\nular representations for property prediction. Journal of chemical information and modeling,\n59(8):3370‚Äì3388, 2019.\n[64] Liangzhen Zheng, Jingrong Fan, and Yuguang Mu. Onionnet: a multiple-layer intermolecular-\ncontact-based convolutional neural network for protein‚Äìligand binding afÔ¨Ånity prediction. ACS\nomega, 4(14):15956‚Äì15965, 2019.\n13\nAppendix\nA The Overall Architecture of GROVER Model\nInput Graph\nMulti-Head Attention\nLayerNorm\nFeed Forward\nNode Embed\nAggregate2Node\nConcat\nLayerNormFeed Forward\nEdge Embed\nAggregate2Edge\nConcat\nLayerNorm\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPNNodeDyMPNQ K VNodeDyMPNNodeDyMPN\nMulti-Head Attention\nLayerNorm\nFeed Forward\nNode Embed\nAggregate2Node\nConcat\nLayerNormFeed Forward\nEdge Embed\nAggregate2Edge\nConcat\nLayerNorm\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPNEdgeDyMPNQ K VEdgeDyMPNEdgeDyMPN\nEdge-view GTransformer\nNode-view GTransformer\nLinear Linear\nInput Graph\nMulti-Head Attention\nLayerNorm\nFeed Forward\nNode Embed\nAggregate2Node\nConcat\nLayerNormFeed Forward\nEdge Embed\nAggregate2Edge\nConcat\nLayerNorm\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPNNodeDyMPNQ K VNodeDyMPNNodeDyMPN\nMulti-Head Attention\nLayerNorm\nFeed Forward\nNode Embed\nAggregate2Node\nConcat\nLayerNormFeed Forward\nEdge Embed\nAggregate2Edge\nConcat\nLayerNorm\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPN\nNodeDyMPNEdgeDyMPNQ K VEdgeDyMPNEdgeDyMPN\nEdge-view GTransformer\nNode-view GTransformer\nLinear Linear\nFigure 6: Overview of the whole GROVER architecture with both node-view GTransformer (in pink\nbackground) and edge-view GTransformer (in green background)\nFigure 6 illustrates the complete architecture of GROVER models, which contains a node-view\nGTransformer (in pink background) and an edge-view GTransformer (in green background). Brief\npresentations of the node-view GTransformer have been introduced in the main text, and the edge-\nview GTransformer is in a similar structure. Here we elaborate more details of the GROVER model\nand its associated four sets of output embeddings.\nAs shown in Figure 6, node-view GTransformer contains node dyMPN, which maintains hidden\nstates of nodes hv,v ‚ààV and performs the message passing over nodes. Meanwhile, edge-view\nGTransformercontains edge dyMPN, that maintains hidden states of edgeshvw,hwv,(v,w) ‚ààE and\nconducts message passing over edges. The edge message passing is viewed as an ordinary message\npassing over the line graph of the original graph, where the line graph describes the neighboring of\nedges in the original graph and enables an appropriate way to deÔ¨Åne message passing over edges [6].\nNote that edge hidden states have directions, i.e., hvw is not identical to hwv in general.\nThen, after the multi-head attention, we denote the transformed node and edge hidden states by ¬Øhv\nand ¬Øhvw, respectively.\nGiven the above setup, we can explain whyGROVER will output four sets of embeddings in Figure 6.\nLet us focus on the information Ô¨Çow in the pink panel of Figure 6, Ô¨Årst. Here the node hidden\nstates ¬Øhv encounter the two components, Aggregate2Node and Aggregate2Edge, which are used to\naggregate the node hidden states to node messages and edge messages, respectively. SpeciÔ¨Åcally,\nthe Aggregate2Node and Aggregate2Edge components in node-view GTransformer is formulated\nas follows:\nmnode-embedding-from-node-states\nv =\n‚àë\nu‚ààNv\n¬Øhu (5)\nmedge-embedding-from-node-states\nvw =\n‚àë\nu‚ààNv\\w\n¬Øhu. (6)\nThen the node-view GTransformer transforms the node messages mnode-embedding-from-node-states\nv\nand edge messages medge-embedding-from-node-states\nvw through Pointwise Feed Forward layers [ 53] and\nAdd&LayerNorm to produce the Ô¨Ånal node embeddings and edge embeddings, respectively.\n14\n4\nKey:DOUBLE_C-SINGLE1_N-SINGLE1\nC\nC CO\nN C CCON\nùëò=1\nFigure 7: Examples of constructing contextual properties for edges\nSimilarly, for the information Ô¨Çow in the green panel, the edge hidden states ¬Øhvw encounter the\ntwo components Aggregate2Node and Aggregate2Edge as well. Their operations are formulated as\nfollows,\nmnode-embedding-from-edge-states\nv =\n‚àë\nu‚ààNv\n¬Øhuv, (7)\nmedge-embedding-from-edge-states\nvw =\n‚àë\nu‚ààNv\\w\n¬Øhuv. (8)\nThen, the edge-view GTransformer transforms the node messages and edge messages through\nPointwise Feed Forward layers and Add&LayerNorm to produce the Ô¨Ånal node embeddings and edge\nembeddings, respectively.\nIn summary, the GROVER model outputs four sets of embeddings from two information Ô¨Çows. The\nnode information Ô¨Çow (node GTransformer) maintains node hidden states and Ô¨Ånally transform\nthem into another node embeddings and edge embeddings, while the edge information Ô¨Çow (edge\nGTransformer) maintains edge hidden states and also transforms them into node and edge embeddings.\nThe four sets of embeddings reÔ¨Çect structural information extracted from the two distinct views, and\nthey are Ô¨Çexible to conduct downstream tasks, such as node-level prediction, edge-level prediction\nand graph-level prediction (via an extra READOUT component).\nA.1 Fine-tuning Model for Molecular Property Prediction\nAs explained above, given a molecular graph Gi and the corresponding label yi, GROVER produces\ntwo node embeddings, Hi,node-view and Hi,edge-view, from node-view GTransformer and edge-view\nGTransformer, respectively. We feed these two node embeddings into a shared self-attentive READ-\nOUT function to generate the graph-level embedding [28, 54]:\nS = softmax\n(\nW2 tanh\n(\nW1H‚ä§))\n,\ng = Flatten(SH), (9)\nwhere W1 ‚ààRdattn_hidden√ódhidden_size and W2 ‚ààRdattn_out√ódattn_hidden are two weight matrix and g is the Ô¨Ånal\ngraph embedding. After the READOUT, we employ two distinct MLPs to generate two predictions:\npi,node-view and pi,edge-view. Besides the supervised loss L(pi,node-view,yi) + L(pi,edge-view,yi), the\nÔ¨Ånal loss function also includes a disagreement loss [ 28] Ldiss = ||pi,node-view ‚àípi,edge-view||2 to\nretrain the consensus of two predictions.\nA.2 Constructing Contextual Properties for Edges\nIn Section 4.2 we describe an example of constructing contextual properties of nodes, here we present\nan instance of cooking edge contextual properties in order to complete the picture.\nSimilar to the process of node contextual property construction, we deÔ¨Åne recurrent statistical\nproperties of local subgraph in a two-step manner. Let us take the graphs in Figure 7 for instance and\nconsider the double chemical bond in red color in the left graph.\nStep I: We extract its local subgraph as itsk-hop neighboring nodes and edges. When k=1, it involves\nthe Nitrogen atom, Carbon atom and the two single bonds. Step II: We extract statistical properties\n15\nof this subgraph, speciÔ¨Åcally, we count the number of occurrence of (node, edge) pairs around the\ncenter edge, which makes the term of node-edge-counts. Then we list all the node-edge counts\nterms in alphabetical order, which makes the Ô¨Ånal property: e.g., DOUBLE_C_SINGLE1_N-SINGLE1\nin the example.\nNote that there are two graphs and two double bonds in red color in Figure 7, since their subgraphs\nhave the same statistical property, the resulted contextual properties of the two bonds would be the\nsame. For a different point of view, this step can be viewed as a clustering process: the subgraphs are\nclustered according to the extracted properties, one property corresponds to a cluster of subgraphs\nwith the same statistical property.\nB Details about Experimental Setup\nB.1 Dataset Description\nTable 3: Dataset information\nType Category Dataset # Tasks # Compounds Metric\nClassiÔ¨Åcation\nBiophysics BBBP 1 2039 ROC-AUC\nPhysiology\nSIDER 27 1427 ROC-AUC\nClinTox 2 1478 ROC-AUC\nBACE 1 1513 ROC-AUC\nTox21 12 7831 ROC-AUC\nToxCast 617 8575 ROC-AUC\nRegression\nPhysical chemistry\nFreeSolv 1 642 RMSE\nESOL 1 1128 RMSE\nLipophilicity 1 4200 RMSE\nQuantum mechanics\nQM7 1 6830 MAE\nQM8 12 21786 MAE\nTable 3 summaries information of benchmark datasets, including task type, dataset size, and evaluation\nmetrics. The details of each dataset are listed bellow [60]:\nMolecular ClassiÔ¨Åcation Datasets.\n- BBBP [32] involves records of whether a compound carries the permeability property of\npenetrating the blood-brain barrier.\n- SIDER [26] records marketed drugs along with its adverse drug reactions, also known as the\nSide Effect Resource .\n- ClinTox [12] compares drugs approved through FDA and drugs eliminated due to the\ntoxicity during clinical trials.\n- BACE [49] is collected for recording compounds which could act as the inhibitors of human\nŒ≤-secretase 1 (BACE-1) in the past few years.\n- Tox21 [1] is a public database measuring the toxicity of compounds, which has been used\nin the 2014 Tox21 Data Challenge.\n- ToxCast [41] contains multiple toxicity labels over thousands of compounds by running\nhigh-throughput screening tests on thousands of chemicals.\nMolecular Regression Datasets.\n- QM7 [4] is a subset of GDB-13, which records the computed atomization energies of stable\nand synthetically accessible organic molecules, such as HOMO/LUMO, atomization energy,\netc. It contains various molecular structures such as triple bonds, cycles, amide, epoxy, etc .\n- QM8 [39] contains computer-generated quantum mechanical properties, e.g., electronic\nspectra and excited state energy of small molecules.\n- ESOL is a small dataset documenting the solubility of compounds [8].\n16\n- Lipophilicity [11] is selected from the ChEMBL database, which is an important prop-\nerty that affects the molecular membrane permeability and solubility. The data is obtained\nvia octanol/water distribution coefÔ¨Åcient experiments .\n- FreeSolv [33] is selected from the Free Solvation Database, which contains the hydration\nfree energy of small molecules in water from both experiments and alchemical free energy\ncalculations .\nDataset Splitting. We apply the scaffold splitting [ 2] for all tasks on all datasets. It splits the\nmolecules with distinct two-dimensional structural frameworks into different subsets. It is a more\nchallenging but practical setting since the test molecular can be structurally different from training\nset. Here we apply the scaffold splitting to construct the train/validation/test sets.\nB.2 Feature Extraction Processes for Molecules\nThe feature extraction contains two parts: 1) Node / edge feature extraction. We use RDKit to\nextract the atom and bond features as the input of dyMPN. Table 4 and Tabel 5 show the atom\nand bond feature we used in GROVER. 2) Molecule-level feature extraction. Following the same\nprotocol of [60, 63], we extract additional 200 molecule-level features by RDKit for each molecule\nand concatenate these features to the output of self-attentive READOUT, to go through MLP for the\nÔ¨Ånal prediction.\nTable 4: Atom features.\nfeatures size description\natom type 100 type of atom (e.g., C, N, O), by atomic number\nformal charge 5 integer electronic charge assigned to atom\nnumber of bonds 6 number of bonds the atom is involved in\nchirality 5 number of bonded hydrogen atoms\nnumber of H 5 number of bonded hydrogen atoms\natomic mass 1 mass of the atom, divided by 100\naromaticity 1 whether this atom is part of an aromatic system\nhybridization 5 sp, sp2, sp3, sp3d, or sp3d2\nTable 5: Bond features.\nfeatures size description\nbond type 4 single, double, triple, or aromatic\nstereo 6 none, any, E/Z or cis/trans\nin ring 1 whether the bond is part of a ring\nconjugated 1 whether the bond is conjugated\nC Implementation and Pre-training Details\nWe use Pytorch to implement GROVER and horovod [47] for the distributed training. We use the\nAdam optimizer with learning rate 0.00015 and L2 weight decay for 10‚àí7. We train the model for\n500 epochs. The learning rate warmup over the Ô¨Årst two epochs and decreases exponentially from\n0.00015 to 0.00001. We use PReLU [16] as the activation function and the dropout rate is 0.1 for\nall layers. Both GROVERbase and GROVERlarge contain 4 heads. We set the iteration L = 1 and\nsample Kl ‚àºœÜ(¬µ= 6,œÉ = 1,a = 3,b = 9) for the embedded dyMPN in GROVER. œÜ(¬µ,œÉ,a,b ) is\na truncated normal distribution with a truncation range (a,b). The hidden size for GROVERbase and\nGROVERbase are 800 and 1200 respectively.\n17\nD Fine-tuning Details\nFor each task, we try 300 different hyper-parameter combinations via random search to Ô¨Ånd the best\nresults. Table 6 demonstrates all the hyper-parameters of Ô¨Åne-tuning model. All Ô¨Åne-tuning tasks are\nrun on a single P40 GPU.\nTable 6: The Ô¨Åne-tuning hyper-parameters\nhyper-parameterDescription Range\nbatch_size the input batch_size. 32init_lr initial learning rate ratio of Noam learning rate scheduler. The real initial learning rate is max_lr/init_lr.10max_lr maximum learning rate of Noam learning rate scheduler. 0.0001‚àº0.001Ô¨Ånal_lr Ô¨Ånal learning rate ratio of Noam learning rate scheduler. The real Ô¨Ånal learning rate is max_lr/Ô¨Ånal_lr. 2‚àº10dropout dropout ratio. 0, 0.05, 0.1,0.2attn_hidden hidden size for the self-attentive readout. 128attn_out the number of output heads for the self-attentive readout. 4,8dist_coff coefÔ¨Åcient of the disagreement loss 0.05, 0.1,0.15bond_drop_ratedrop edge ratio [43] 0, 0.2,0.4,0.6ffn_num_layerThe number of MLP layers. 2,3ffn_hidden_sizeThe hidden size of MLP layers. 5,7,13\nE Additional Experimental Results\nE.1 Effect of Self-supervised Pre-training on Regression Tasks\nTable 7 depicts the additional results of the comparison of the performance of pre-trained GROVER\nand GROVER without pre-training on regression tasks.\nTable 7: Comparison between GROVER with and without pre-training on regression tasks\nGROVER No Pre-training Absolute Improvement\nRMSE\nFreeSolv 1.544 1.987 0.443\nESOL 0.831 0.911 0.080\nLipo 0.560 0.643 0.083\nMAE\nQM7 72.600 89.408 16.808\nQM8 0.013 0.017 0.004\nE.2 GROVER Fine-tuning Tasks with Other Backbones\nIn order to verify the effectiveness of the proposed self-supervised tasks, we report the Ô¨Åne-tuning\nresults by Hu et al. with and without pre-training in Table 8. As a comparison, we also involve the\nperformance of GROVER with the backbone GIN and MPNN trained in Section 5.2. We Ô¨Ånd that\nwithout pre-training, our GROVER-GIN is consistent with Hu et al. on average, thus verifying the\nreliability of our implementations. However, after pre-training, GROVER-GIN achieves nearly 2%\nhigher number than Hu et al., which supports the advantage of our proposed self-supervised loss.\nTable 8: Comparison between different methods. The metric is AUC-ROC. The numbers in brackets\nare the standard deviation.\nHu. et al. GROVER-GIN GROVER-MPNN\nw pre-train w/o pre-train w pre-train w/o pre-train w pre-train w/o pre-train\nBBBP 0.915(0.040) 0.899(0.035) 0.925(0.036) 0.901(0.051) 0.929(0.029) 0.917(0.027)\nSIDER 0.614(0.006) 0.615(0.007) 0.648(0.015) 0.627(0.016) 0.650(0.003) 0.637(0.030)\nBACE 0.851(0.027) 0.837(0.028) 0.862(0.020) 0.823(0.050) 0.872(0.031) 0.852(0.034)\nAverage 0.793 0.784 0.812 0.784 0.817 0.802\n18",
  "topic": "Molecular graph",
  "concepts": [
    {
      "name": "Molecular graph",
      "score": 0.8215211629867554
    },
    {
      "name": "Computer science",
      "score": 0.7457211017608643
    },
    {
      "name": "ENCODE",
      "score": 0.6384186744689941
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5305778384208679
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5257402658462524
    },
    {
      "name": "Transformer",
      "score": 0.5255305171012878
    },
    {
      "name": "Graph",
      "score": 0.48920273780822754
    },
    {
      "name": "Machine learning",
      "score": 0.48196718096733093
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4813438355922699
    },
    {
      "name": "Encoder",
      "score": 0.43501368165016174
    },
    {
      "name": "Chemistry",
      "score": 0.10022240877151489
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}