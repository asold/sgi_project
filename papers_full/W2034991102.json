{
  "title": "Syntactic discriminative language model rerankers for statistical machine translation",
  "url": "https://openalex.org/W2034991102",
  "year": 2011,
  "authors": [
    {
      "id": "https://openalex.org/A5008032834",
      "name": "Simon Carter",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5109059955",
      "name": "Christof Monz",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2794026841",
    "https://openalex.org/W2056250865",
    "https://openalex.org/W2280403519",
    "https://openalex.org/W2066370905",
    "https://openalex.org/W2152263452",
    "https://openalex.org/W2437005631",
    "https://openalex.org/W2111142112",
    "https://openalex.org/W2140343992",
    "https://openalex.org/W2128201184",
    "https://openalex.org/W2113762394",
    "https://openalex.org/W2017708149",
    "https://openalex.org/W4249572517",
    "https://openalex.org/W2129727551",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2143564602",
    "https://openalex.org/W2142112143",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W2110417778",
    "https://openalex.org/W2146574666",
    "https://openalex.org/W1973923101",
    "https://openalex.org/W2124445791",
    "https://openalex.org/W2158148237",
    "https://openalex.org/W2158025800",
    "https://openalex.org/W2062323476",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2160697141",
    "https://openalex.org/W2126610017",
    "https://openalex.org/W2122228338",
    "https://openalex.org/W2152937686",
    "https://openalex.org/W2618735189",
    "https://openalex.org/W2113788796",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2131297983",
    "https://openalex.org/W179314280",
    "https://openalex.org/W1491975949",
    "https://openalex.org/W2171541062",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W1544826511",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W1986543644",
    "https://openalex.org/W2092654472",
    "https://openalex.org/W1559419789",
    "https://openalex.org/W2188741930",
    "https://openalex.org/W2107085272",
    "https://openalex.org/W91928571",
    "https://openalex.org/W3202207277",
    "https://openalex.org/W23077562",
    "https://openalex.org/W1982725271",
    "https://openalex.org/W2040870580",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2108301419",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2786062171",
    "https://openalex.org/W1979711143",
    "https://openalex.org/W3167803720"
  ],
  "abstract": null,
  "full_text": "Mach Translat (2011) 25:317–339\nDOI 10.1007/s10590-011-9108-7\nSyntactic discriminative language model rerankers\nfor statistical machine translation\nSimon Carter · Christof Monz\nReceived: 28 December 2010 / Accepted: 12 August 2011 / Published online: 1 September 2011\n© The Author(s) 2011. This article is published with open access at Springerlink.com\nAbstract This article describes a method that successfully exploits syntactic fea-\ntures for n-best translation candidate reranking using perceptrons. We motivate the\nutility of syntax by demonstrating the superior performance of parsers over n-gram\nlanguage models in differentiating between Statistical Machine Translation output and\nhuman translations. Our approach uses discriminative language modelling to rerank\nthe n-best translations generated by a statistical machine translation system. The per-\nformance is evaluated for Arabic-to-English translation using NIST’s MT-Eval bench-\nmarks. While deep features extracted from parse trees do not consistently help, we\nshow how features extracted from a shallow Part-of-Speech annotation layer outper-\nform a competitive baseline and a state-of-the-art comparative reranking approach,\nleading to significant BLEU improvements on three different test sets.\nKeywords Statistical machine translation · Discriminative language models ·\nSyntax\n1 Introduction\nLanguage models (LMs), alongside translation models, form the core of mod-\nern Statistical Machine Translation (SMT) systems, whether they be phrase-based\nThis work is a revised and substantially expanded version of ( Carter and Monz 2009 )a n d( Carter and\nMonz 2010 ).\nS. Carter ( B) · C. Monz\nISLA, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, The Netherlands\ne-mail: s.c.carter@uva.nl\nC. Monz\ne-mail: c.monz@uva.nl\n123\n318 S. Carter, C. Monz\n(Koehn et al. 2003 ) or hierarchical systems ( Chiang 2005, 2007). A language model’s\nprimary job in an SMT system is to check and enforce the ﬂuency of a translation can-\ndidate without any knowledge of the source sentence being translated. In so doing, the\nlanguage model impacts on word order decisions and translation selection decisions.\nThe most commonly used LMs are word n-gram models, which make the Markov-\nian assumption and base the probability of a word following a preﬁx string on a limited\ncontext of the previous n − 1 words. Given that n-gram LMs utilise a limited lexical\ncontext when deciding on the overall ﬂuency of a sentence, the use of syntax, which\ncontains deeper, and consequently long-range dependency information, is intuitively\nappealing.\nWe motivate the use of syntax by ﬁrst demonstrating that parsers are better able\nto discriminate between ﬂuent and disﬂuent English than a large four-gram language\nmodel. We then show that the probability output by a parser cannot differentiate\nbetween varying degrees of ﬂuent English, because if often tries to assign sensible\nstructure to disﬂuent word sequences. As a result of the initial experiments presented\nin this article, we propose a syntactic discriminative language model, which allows\nfor the use of ﬁner-grained syntactic features, to be used in a n-best reranking set-\nting. Together with this model, we suggest the extraction of features from a novel\nnon-context-aware Part-of-Speech (POS) tagger. Accordingly, we aim to integrate\nsyntactic features that represent the actual word sequence, leading to models that bet-\nter discriminate between disﬂuent, machine-generated language and human-produced\nsentences.\nThe remainder of this article is structured as follows: we discuss related work in\nSect. 2. We then examine the ability of a parser to discriminate between sentences\nof varying degrees of ﬂuency in Sect. 3. We introduce discriminative LMs in Sect. 4,\npresent the perceptron model used in Sect. 4.1, and describe the syntactic features in\nSect. 4.2. Experimental results are reported in Sect. 5. We discuss the ﬁndings in detail\nin Sect. 6, and conclude in Sect 7.\n2 Related work\nOne manner to overcome the data sparseness problem of n-gram LMs has been the\ngeneralisation of existing data to encapsulate greater information that go beyond the\nsurface form. Class-based models ( Brown et al. 1992 ) generalise from the word form\nto an abstract representation based on word clusters. These models are often used dur-\ning decoding via interpolation with a standard lexical model. The class-based models\nare further generalised via Factored language models (FLMs) ( Bilmes and Kirchhoff\n2003; Birch et al. 2007 ; Koehn and Hoang 2007 ). Here, each word is represented by\nas many different forms as are required, from morphological information to supertags\nand partial parses ( Birch et al. 2007 ). Both of these move beyond the surface word\nform, but stay within the n-gram framework.\nA study of the use of a range of generative syntactic models was undertaken by Och\net al. (2003, 2004), who performed n-best reranking as we have done. Syntactic features\nwere not successful. The approach of Post and Gildea (2008) integrates a parser as\nlanguage model into the decoding framework, but they are unable to outperform the\n123\nSyntactic discriminative language model rerankers 319\nbaseline. Unlike the two previous approaches which attempted to utilise full parse\ntrees for improving SMT output, the utility of shallow parse information has been\ndemonstrated by Hasan et al. (2006) for the translation of speech corpora. Supertag-\nging, lightweight dependency analysis, a link grammar parser and a chunk parser are\nused to rerank n-best lists within a log-linear framework.\nShen et al. (2004) were the ﬁrst to use a perceptron-like algorithm in a small-scale\napplication of reranking SMT n-best lists. They used the algorithm to optimise weights\nfor a small number of features (tens instead of millions). The use of perceptron-type\nalgorithms with millions of features for SMT has been explored by Arun and Koehn\n(2007). They examine the use of online algorithms for the discriminative training of a\nphrase-based SMT system. In this article we focus on the use of perceptrons for reran-\nking using only target-side syntactic information. Other researchers have attempted\nto examine the training of speciﬁc features types; from the training of reordering\nfeatures ( Tillmann and Zhang 2006 ; Chang and Toutanova 2007 ), translation model\nfeatures ( Blunsom et al. 2008 ), independent target- and source-side features ( Chiang\net al. 2009 ), and both translation and language model features combined ( Shen et al.\n2004; Liang et al. 2006 ; Watanabe et al. 2007 ; Chiang et al. 2008 ).\nThe work most closely related to ours is the discriminative syntactic LM proposed\nin ( Collins et al. 2005 ). The work presented in this article differs in two important\naspects. First, we focus on the use of syntactic features in SMT. Second, we propose\nthe use of a simple POS tagger, which gives significant improvements over a 1-best\nbaseline and competing state-of-the-art reranker. Li and Khudanpur (2008) apply the\nframework of Roark et al. (2007) to create the ﬁrst large-scale discriminative language\nmodel to SMT for reranking. Using a standard n-gram feature set, they outperformed\nthe 1-best output of their baseline SMT system. They focus on the application of\nn-gram-only models to SMT and the use of data ﬁltering thresholds.\nThis article extends our previous work in Carter and Monz (2009, 2010) in three\nways; ﬁrst by expanding upon the original experiments in Carter and Monz (2009) with\ntwo new additional test sets, and three higher order LMs, demonstrating the validity\nof the results and conclusions drawn. Second, we motivate the work ﬁrst presented\nin Carter and Monz (2010) by giving concrete examples of where parsers go wrong,\nmotivating the best-performing S-POS reranker. Finally, we present four new deep,\nsyntactic reranking, models, and provide a more detailed analysis of both deep and\nshallow syntactic features for reranking SMT output.\n3 Parsing ungrammatical English\nIn this section, we examine the ability of a parser to differentiate between ﬂuent and\ndisﬂuent English. Specifically, we look at two tasks. The ﬁrst, in Sect. 3.3, is differen-\ntiating between SMT output and human-produced English. If a state-of-the-art parser\nis unable to do this task as well as a standard LM, then it is not suitable for use within\nan SMT system, as suggested by Och et al. (2004). The second, harder task, reported\nin Sect. 3.4, is to differentiate between varying degrees of ﬂuent sentences produced\nby an SMT system. This second set of experiments is representative of a standard\nreranking task.\n123\n320 S. Carter, C. Monz\n3.1 Parser\nWe used an implementation of Collins Model 2 (CM2) by Bikel (2002) to provide\nparses and parse probabilities. CM2 uses four different probability distributions for\nassigning probabilities to (i) head labels, (ii) sibling labels and head tags, (iii) sibling\nhead words, and (iv) subcategorization frames. We chose Model 2 above Model 1\nbecause of the higher reported Labeled Recall and Precision scores ( Collins 1997 ).\nThe parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn\nTree Bank (PTB) ( Marcus et al. 1994 ), about 40,000 sentences, and tested on section\n23, about 2,500 sentences. Because we are applying the parser to lowercased SMT\noutput, we lowercase the parser training data.\nWe compare the parser against a large four-gram language model. The SRILM\ntoolkit ( Stolcke 2002 ) with modiﬁed Kneser-Ney smoothing was used to build the\ntarget four-gram language model using the AFP and Xinhua portions of the English\nGigaword corpus (LDC2003T05) and the English side of the bitext.\n3.2 Experimental set-up\nIn this section, we provide details of the experimental set-up for our experiments\nanalysing the ability of parsers to differentiate between ﬂuent and disﬂuent English.\nMoses was used as a state-of-the-art baseline SMT system for reporting experi-\nmental results ( Koehn et al. 2007 ). It is a phrase-based MT system using stacks to\norganise partial translation candidates. The parameters used for the experiments dis-\ncussed here are stack size of 100, distortion limit of 6, and phrase table limit of 20.\nWe utilise lexicalised reordering along with the standard Moses phrase tables scores\nand a language model.\nTo build the phrase table and language model, we used ﬁve corpora distrib-\nuted by the Linguistic Data Consortium (LDC), totaling 300K sentence pairs. The\nparallel text includes Arabic news LDC2004T18, automatically extracted parallel\ntext LDC2007T08, eTIRR news LDC2004E72 and translated Arabic treebank data\nLDC2005E46. Alignments were extracted using the GIZA ++ toolkit ( Och and Ney\n2000). Minimum Error Rate Training (MERT) ( Och 2003 ) was used for optimising\nthe parameters of the Moses baseline SMT system.\nFor Arabic-to-English translation, performance is evaluated using NIST’s MT-Eval\nbenchmarking sets from 2002 through to 2006 (henceforth referred to as MT02, MT03,\nMT04, MT05 and MT06). Statistics for each set (#source sentences/#refs): MT02\n(1043/4), MT03 (663/4), MT04 (1353/5), MT05 (1056/5), MT06(1796/4). Sets MT02\nand MT03 were used for development.\n3.3 Discriminating between SMT and human translations\nIn this section we present results on the discrimination of the SMT output and reference\ntranslation using a parser or LM probabilities. Formally, our experiment is composed\nof a set of SMT output sentences and their respective reference translations, which in\nour case is 4 each. In this work, a speciﬁc SMT output sentence with its respective\n123\nSyntactic discriminative language model rerankers 321\nTable 1 Percentage of references with higher probability than respective SMT output for the test sets\nMT04, MT05 and MT06\nModel No normalisation Normalised\nMT04 MT05 MT06 MT04 MT05 MT06\nLM4 13 .58 28 .05 15 .24 23 .08 30 .78 30 .68\nLM5 15 .63 34 .44 17 .22 6 .92 35 .72 34 .28\nLM6 16 .09 33 .62 17 .35 27 .27 36 .55 34 .58\nCM2 34.09 49 .13 3 .66 1 .09 54 .85 63 .37\nWe present normalised and unnormalised results\nWe highlight in bold the best scores\nreferences are referred to as a ‘sentence set’. Unlike in our previous work ( Carter and\nMonz 2009 ), no thresholding based on SMT output sentence length is applied. We\ndeﬁne the accuracy of a model to be the percentage of references which are assigned\nan equal or higher parser/LM probability than the MT output.\nWe show in Table 1 the percentage of reference sentences which were assigned\na higher or equal probability to the respective SMT output by each of the different\nmodels on the three different test sets. All results are reported using simple length nor-\nmalisation, where the log probability is divided by the sentence length, as we found\nthis to have a positive impact. The CM2 parser outperforms the n-gram LMs. When\nusing length normalisation, the parser performs better than chance, while the LMs\ncontinue to perform worse than 50%. While there is a slight increase in both unnor-\nmalised and normalised language model scores by using higher order LMs, the parser\nstill achieves accuracy results between 1.5 (MT05) and 2.24 (MT04) times better than\nthe corresponding best LM6 scores.\nTo see to what extent these ﬁndings hold for SMT sentences of different degrees\nof ﬂuency, where ﬂuency is approximated by BLEU, the standard evaluation metric\nfor SMT systems ( Papineni et al. 2002 ), we bucketed the reference sentences by the\nsentence-level BLEU score of the corresponding SMT output.\n1 We would expect that\nSMT output which has a higher BLEU score is harder to differentiate from the human-\nproduced references (cf. Kulesza and Shieber 2004 ). Results are displayed in Fig. 1b,\nc and d. The correlation between BLEU range and model accuracy is measured by\napplying linear regression. Unsurprisingly, as shown by the linear regression lines,\nmodel accuracy appears to decrease as BLEU increases for all three test sets; the bet-\nter the SMT output, the harder it is for both models to differentiate between good and\nbad English. Note, however, that the relative classiﬁcation accuracy remains the same\nbetween the models.\nTo take a deeper look into which sentences the models were having problems with,\nwe bucketed the SMT translations by their length, and examined the classiﬁcation accu-\n1 Though there are reported issues with the use of BLEU (cf. Callison-Burch et al. 2006 ) ,am a n u a le v a l u -\nation of the SMT output conﬁrms a correlation between the sentence-level BLEU scores and ﬂuency of the\ntranslations. Furthermore, as this is the established metric on which SMT systems are optimised towards\nand evaluated against, we believe it to be a sufﬁcient proxy for ﬂuency.\n123\n322 S. Carter, C. Monz\n 0\n 20\n 40\n 60\n 80\n 100\n 120\n 140\n 160\n 180\n 200\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100\nNum of Sentences\nBleu Range\nMT04\nMT05\nMT06\n(a)\n 0\n 50\n 100\n 150\n 200\n 250\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100-104\n105-109\n110-114\n115-119\n120-124\n125-129\n130-134\n135-139\nNum of Sentences\nSentence Length\nMT04\nMT05\nMT06\n(e)\n0 %\n20 %\n40 %\n60 %\n80 %\n100 %\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100\nModel Accuracy\nBleu Range\nLM4\nLM5\nLM6\nCM2\n(b) MT04\n0 %\n20 %\n40 %\n60 %\n80 %\n100 %\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100-104\n105-109\n110-114\nModel Accuracy\nSentence Length\nLM4\nLM5\nLM6\nCM2\n(f) MT04\n0 %\n10 %\n20 %\n30 %\n40 %\n50 %\n60 %\n70 %\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100\nModel Accuracy\nBleu Range\nLM4\nLM5\nLM6\nCM2\n(c) MT05\n0 %\n20 %\n40 %\n60 %\n80 %\n100 %\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100-104\nModel Accuracy\nSentence Length\nLM4\nLM5\nLM6\nCM2\n(g) MT05\n0 %\n10 %\n20 %\n30 %\n40 %\n50 %\n60 %\n70 %\n80 %\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100\nModel Accuracy\nBleu Range\nLM4\nLM5\nLM6\nCM2\n(d) MT06\n0 %\n20 %\n40 %\n60 %\n80 %\n100 %\n0-4\n5-9\n10-14\n15-19\n20-24\n25-29\n30-34\n35-39\n40-44\n45-49\n50-54\n55-59\n60-64\n65-69\n70-74\n75-79\n80-84\n85-89\n90-94\n95-99\n100-104\n105-109\n110-114\n115-119\n120-124\n125-129\n130-134\n135-139\nModel Accuracy\nSentence Length\nLM4\nLM5\nLM6\nCM2\n(h) MT06\nFig. 1 Breakdown of model performance in discriminating between SMT output and human-produced\nreferences using length-normalised probabilities of a 4-, 5- and 6-gram LM and state-of-the-art parser. a\nshows the number of sentences per BLEU bin for each of the three test sets. e shows the number of SMT\nsentences in each length bin. b, c and d give a breakdown of classiﬁcation accuracy by BLEU score of\nSMT sentence for MT04, MT05 and MT06 test sets. f, g and h show the classiﬁcation accuracy of the\nlength-normalised models by length of the SMT sentences for the three test sets. b, c, d and f, g, h have\nlinear regression lines drawn for both models showing the underlying trends. All BLEU scores in these\nﬁgures are sentence-level BLEU\n123\nSyntactic discriminative language model rerankers 323\nTable 2 Average Pearson correlation coefﬁcient between the two different models and BLEU rankings for\neach of the MT04, MT05 and MT06 test sets\nModel MT04 MT05 MT06\nLM4 −0.045 0 .066 −0.027\nCM2 −0.027 0 .036 −0.046\nracy for each bucket for each model. The results are displayed in Fig. 1f, g and h. Even\nfor short reference sentences in the range of 1–4 words, the parser still outperforms\nthe n-gram LMs. Moreover, as the regression lines show, the parser performs better\nas sentence length increases, in contrast to the LMs, whose performance decreases\nwith longer translations. The parser is able to exploit deep syntactic structures that\ncapture long-range information within the sentence to assign it a parse whose proba-\nbility better reﬂects the ﬂuency of the translation, whereas the n-gram LMs, limited\nto a ﬁxed history, are unable to utilise this information.\nThese results are interesting, as parsers perform worse on standard Labeled Recall\nand Labeled Precision measures as sentence length increases ( McDonald 2007). This\ndemonstrates that these measures—the evaluation metrics which parsers are tradition-\nally developed and optimized towards—are not necessarily indicative of a parser’s\nability to differentiate between SMT output and human-produced translations.\n3.4 Correlating parser scores with translation quality\nConsidering a parser’s ability to better discriminate between SMT output and human\ntranslations, we propose to use parsers for reranking. As a ﬁrst step we examine the\ncorrelation between the probabilities assigned to a parser in an n-best list and smoothed\nBLEU ( Lin and Och 2004 ). We use the same SMT system and models as before. For\neach source sentence we output an n-best list of translation candidates. We score each\nsentence in the n-best list according to smoothed BLEU, and score each sentence with\nboth the n-gram language model and parser. We convert these scores to ranks, tied\nwhere necessary, and compute the Pearson correlation coefﬁcient between the BLEU\nand model rankings. The correlation coefﬁcients are then averaged over the entire\ntest set. Results are reported in Table 2. There is no correlation between the rankings\nassigned to the n-best lists by either n-gram or parser models and BLEU.\n3.5 Analysis\nWhile the parser is able to discriminate between SMT output and human-produced sen-\ntences, the results reported in Sect. 3.4 highlight the difﬁculty in using the probabilities\nfor discriminating between sentences of varying degrees of ﬂuency and grammatical-\nity. A look to the literature offers some potential explanations. In analysing why the\ninclusion of scores from a parser—similar to the used in this article—during decoding\ndid not help improve BLEU scores ( Post and Gildea 2008 ), the authors argue that the\nuse of a single probability to represent the quality of a single parse is too coarse a met-\n123\n324 S. Carter, C. Monz\nFig. 2 We show the parse over an SMT translation of the MT04 source segment 139. In the table we show\nthe same translation, along with the four human-produced reference translations. In the two ﬁnal columns\nwe print the length-normalised log probability assigned to each sentence by the four-gram LM (LM4) and\nCollins Model 2 (CM2) parser. The parser assigns the SMT output a higher normalised log probability\nthan the four reference translations, while the n-gram LM assigns the sentence the lowest normalised log\nprobability. We highlight in bold the lowest scores output by both models\nric for the successful exploitation of parse tree information. These results corroborate\nthe earlier experiments of Och et al. (2004), which demonstrate the lack of utility of a\ngenerative parser for reranking. The results from our averaged correlation experiments\nin Sect. 3.4 support the literature in showing that the direct use of a single probability\nto rank an n-best list does not lead to improvements in BLEU.\nTo give an example of why this is the case, we show in Fig. 2 a parse by Collins\nModel 2 over a 1-best sentence output from the MT04 test set. Here, the length-norma-\nlised log probability assigned by the parser to this disﬂuent sentence was higher than\nthose assigned to all four reference sentences. The parser incorrectly tags ‘believers’\nas a verb instead of a noun. The parser does this to obtain a good structure; the cost of\ncorrectly tagging ‘believers’ as a noun is too prohibitive. A quick glance at the WSJ\ntraining corpus hints at a reason why: out of 21K instances of TO, its nearest rightmost\nsibling is a VP 13,001 times, and an NP 8,460 times, making the former more likely. In\nassuming the input is a well-formed ﬂuent English sentence, assigning the most likely\nparse leads to good high-level structures that mask disﬂuency at the local level. In\ncomparison, the n-gram language model, which makes only local decisions, correctly\nassigns the sentence a lower length-normalised probability than its references.\n123\nSyntactic discriminative language model rerankers 325\nLooking at the example parse, one can formulate a number of different features\nthat could be used to distinguish between a ﬂuent and disﬂuent sentence. A lexica-\nlised parent-head-based feature, similar to that used in Collins et al. (2005), could\ndetermine that a VP headed by ‘believers’ is unlikely. Furthermore, a shallow POS\nn-gram sequence extracted from the parse could show that the trigram VB NNS JJ is\nimprobable.\nThere are many features, deep or shallow, that could be extracted and learnt from\nthe parse that lead to potential gains in a reranking setting. Our solution is to exam-\nine the potential beneﬁts of multiple syntactic features exploited in a discriminative\nlanguage model framework. Such a framework would allow us to conduct a thorough\ninvestigation of the different types of syntactic information extractable from a full\nparse tree in a computationally practical manner.\n4 Syntactic discriminative language models (DLMs)\nDLMs consist of a function φ(·) that maps a sentence onto a feature space and weight\nvector w (Roark et al. 2007 ). For training, negative and positive examples are supplied\nto the DLM for learning the weight vector. The weight vector and feature function is\nthen used to assign a non-probabilistic score to an unseen sentence.\nA beneﬁt of using discriminative techniques over generative models is that stan-\ndard generative LMs are trained exclusively on well-formed English. Given the large\nfeature space they operate in, the accurate assignment of probabilities to unseen events\nis a difﬁcult problem, and has been a major area of research for the past sixty years\n(for a detailed overview on language modelling and smoothing techniques, see ( Chen\nand Goodman 1998 ). Discriminative models are trained with positive and negative\nexamples, and therefore learn to assign negative weights to harmful features, without\nhaving to infer this from positive data only.\nDifferent parameter estimation methods to estimate the weight vector w for a DLM\nhave been previously examined in the Automatic Speech Recognition (ASR) domain\n(Roark et al. 2004b, 2007). These include optimising the log-likelihood under a log-lin-\near model, a batch algorithm which requires processing all the data before outputting\na weight vector as an answer, and approximating a 0/1 loss through the perceptron\nupdate rule, and an online algorithm which examines and updates the parameter vec-\ntor sequentially. The reader is referred to ( Roark et al. 2004b ; Emami et al. 2007 )\nfor a discussion on the beneﬁts of the log-linear model and the perceptron. Given\nthat this article examines the use of a syntactic feature space, which is larger than an\nalready large n-gram feature space, and that perceptrons perform feature selection as\na consequence of its learning procedure, we opt to use the perceptron algorithm.\n4.1 Perceptron\nThe perceptron, proposed by Rosenblatt (1958), is an online error minimisation learner\nthat, assuming linearly separable data, can theoretically converge to a solution that\nperfectly classiﬁes the data ( Freund and Schapire 1999 ). The perceptron has been suc-\ncessfully applied to parse reranking ( Collins and Duffy 2002 ), document reranking\n123\n326 S. Carter, C. Monz\nFig. 3 The standard perceptron\nalgorithm\nfor IR ( Crammer and Singer 2001 ; Elsas et al. 2008 ; Chen et al. 2009 ), ASR reranking\n(Roark et al. 2004b ; Collins et al. 2005 ; Singh-Miller and Collins 2007 ), and ﬁnally\nto SMT translation reranking ( Shen et al. 2004 ; Li and Khudanpur 2008 ), where Chi-\nnese–English translation systems were significantly improved.\n4.1.1 Algorithm\nThe standard perceptron algorithm is shown in Fig. 3. The algorithm takes as input\na set of n-best lists X, and an oracle function ORACLE(xi ) that determines the best\ntranslation (oracle best) for each of the n-best lists xi according to the BLEU met-\nric. As DLMs make comparisons at the sentence level, we use sentence-level BLEU\nwith additive smoothing ( Lin and Och 2004 ). While there are discrepancies between\nsentence- and corpus-level BLEU, we ﬁnd sentence-level BLEU sufﬁcient for reran-\nking SMT. T deﬁnes the number of iterations and N deﬁnes the size of the test set,\nwhich in our case is the number of n-best lists. The algorithm iterates over the n-best\nlists in a sequential manner (lines 2 and 3). If the selected hypothesis and oracle\nbest sentence match, the algorithm continues to the next n-best list. Otherwise, the\nweight vector is updated (line 7). Finally, it returns a weight vector as its solution\n(line 11).\nTo use the weight vector returned by the perceptron algorithm, each sentence z in\nan n-best list is scored as in (1):\nS(z) = βφ\n0(z) + w · φ(z) (1)\nThe SMT model score for each translation hypothesis φ0(z) is weighted by β. Roark\net al. (2007) argue that while it is possible to include φ0(z) as a feature of the percep-\ntron model, this may lead to under-training, so we adhere to the convention of using\na ﬁxed value for β.\nTo score an n-best list xi , we use the weight vector returned by the perceptron to\nassign a score to each sentence and select the best one, as in ( 2):\nz∗ = argmaxz∈GEN(xi)S(z) (2)\n123\nSyntactic discriminative language model rerankers 327\n4.1.2 Variants\nA shortcoming of the perceptron is that it can be unstable if the training data is not line-\narly separable. A number of solutions have been proposed in the literature. One solution\nis to use an averaged perceptron ( Freund and Schapire 1999 ), where the parameter vec-\ntor w output by the algorithm is averaged over each instance wavg = /Sigma1T\nt=1/Sigma1N\ni=1\nwi\nt\nN·T .\nAnother solution is the pocket perceptron ( Gallant 1999 ; Collins and Duffy 2002 ),\nwhere the weight vector returned is the one that correctly classiﬁes the most training\ninstances in a row, keeping an optimal model in its ‘pocket’. A third solution, called\nthe committee or voting perceptron, keeps a cache of optimal models, sorted by their\nsuccess counts ( Roark et al. 2004a ; Elsas et al. 2008 ). The cache sizes differentiate\nthe voting and committee perceptron, with the voting perceptron using the best cached\nmodel, and the committee perceptron utilising the top-n cached models. As previous\npublished work on using perceptrons for reranking SMT output utilised the average\nperceptron ( Li and Khudanpur 2008 ), we also use this model.\n4.2 Features\nIn examining different syntactic features, we distinguish between deep features—those\nextractable from a syntactic annotation layer that goes beyond pre-terminals—and\nshallow features which require only POS tags. In Sect. 4.2.1, we outline the different\ntoolkits that are used to extract features. In Sect. 4.2.2, we detail the features used that\ncan only be extracted from a full parse tree. In Sect. 4.2.3 we explain the features that\ncan be extracted from a POS tagger. In Table 3, we list the features we examine in this\narticle, and provide references for those feature types that have been explored, albeit\nfor different tasks, in either an SMT or ASR setting.\n4.2.1 Annotation layers\nIn this section, we outline the different toolkits and resulting output annotation layers\nfrom which we extract our syntactic features. Some of the features examined can only\nbe extracted from a full parse tree, while others can be extracted from either parsers\nor taggers.\nIt is interesting to see whether it is beneﬁcial to use features extracted from full parse\ntrees as opposed to those extracted from a POS tagger or other shallow representation.\nUsing parsers allows us to extract global features that relay deep structural informa-\ntion. Unfortunately, they are slow and memory-intensive, and may fail to return a parse\nfor long sentences, as they have O(n\n3) complexity in relation to sentence length. On\nthe other hand, POS taggers, while outputting no deep syntactic information, are more\nefﬁcient and robust, as they always output a complete POS sequence\nCRF tagger (CRF) We used Xuan-Hieu Phan’s implementation of a Conditional\nRandom Field tagger as our state-of-the-art POS tagger. 2\n2 Available at: http://crftagger.sourceforge.net.\n123\n328 S. Carter, C. Monz\nTable 3 Syntactic feature types examined in this article\nFeature type Field Task Citation Helpful\nSEQ-B & SEQ-C ASR Reranking Collins et al. (2005)Y e s\nCFG ASR Reranking Collins et al. (2005)Y e s\nHEAD ASR Reranking Collins et al. (2005)Y e s\nT-DEPTH SMT Difﬁcult-to-translate\nphrase localisation\nMohit and Hwa (2007)Y e s\nUNIT-P – – – –\nNT-C SMT Reranking/decoder\nfeatures\nOch et al. (2003);\nChiang et al. (2009)\nNo/yes\nNO-C SMT – – –\nPOS ASR / SMT Reranking/FLM\nfeatures\nCollins et al. (2005);\nBirch et al. (2007)\nYes\nVERBAGR – – – –\nPOSNUM – – – –\nNOPOS SMT Reranking/decoder\nfeatures\nOch et al. (2003);\nChiang et al. (2009)\nNo\nDuring experimentation different feature combinations are examined. Where a feature has been previously\nused, we list the ﬁeld, task and citation, and whether or not it proved useful\nSimple tagger (S-POS) We also use a simple maximum likelihood POS tagger,\nwhich assigns to each word the most likely tag according to a training set, regard-\nless of any context. The simple model does not use any smoothing, meaning that\nout-of-vocabulary items are simply assigned ⟨UNK⟩ as their tag.\nThe use of a simple POS tagger is motivated by analysis conducted in Sect. 3.5,\nwhere a manual evaluation of parse trees indicated that parsers provide good struc-\ntures over disﬂuent English by incorrectly tagging words. Assigning to words their\nmost likely POS tags according to unigram estimates should allow a discriminative\nlanguage model to better identify and penalise reordering mistakes.\n4.2.2 Deep features\nSequential rules (POS, SEQ-B and SEQ-C) From the full parse tree, we extract three\ndifferent layers. Fig. 4b shows the three annotation layers we extract from the parse\ntree shown in Fig. 4a. In Fig. 4b (POS), the sequence comprises the POS tags for each\nword. Fig. 4b (SEQ-B) captures chunk-based sequences by associating with each word\nthe ﬁrst non-POS ancestor node. For each word, it is also indicated whether it starts\nor continues a shallow chunk (\nb for the former and c for the latter). The sequence in\nFig. 4b (SEQ-C) is similar, but includes the POS tag of each word.\nFrom the POS, SEQ-B and SEQ-C layers, as well as from the S-POS and CRF-POS\noutput, we extract the following features, where wi is a word at index i, and t is a tag\nspeciﬁc to the layer we are extracting from:\n(ti−2ti−1ti ), (ti−1,ti ), (ti ), (ti wi )\n123\nSyntactic discriminative language model rerankers 329\n(a) (b)\nFig. 4 In a we show an example parse tree, and in b we show the POS, SEQ-B and SEQ-C sequences\nextracted from a\nContext free grammar rules (CFG) Each feature is a basic CFG rule used in the\nparse of a sentence. CFGs form the building blocks of most parsers, and except for\nrules from pre-terminals to leaf nodes, are entirely non-lexical. Thus these features\ncapture information about categorial reordering decisions, such as Noun Phrase (NP)\nbefore Verb Phrase (VP), or vice versa. While simple, this feature type can capture\nlong-range reordering mistakes.\nSyntactic head features (HEAD) We model head-parent relationships, such as NP\nheaded by NN (syntactic), represented by NP/NN, or NP headed by car, NP/car (lex-\nical). These features are extracted for each non-terminal (NT) in the tree. A head\ndenotes the most important syntactic child of a phrase category. Heads are extracted\nusing the hand crafted rules deﬁned in Appendix A of ( Collins 1999 ).\nIn addition to the simple head-parent relationships, we also model more complex\nhead-to-head dependencies within the parse tree. Given a parent NT P in a tree and\nits NT children (C ··· C\nk ), we model the relationship between P, the head child of\nPC h, and each sibling node of C h. We denote the relative position between Ch and\nthe sibling Ck with an integer, 1 if adjacent, 2 if not, positive if Ck is to the right,\nnegative if to the left of Ch. Finally we note the lexical or POS head of C h and Ck .T h e\nﬁnal feature is of the form: P,HC,Ck ,{+,−},lex /POS,lex /POS. Examples from\nFig. 4a include:\nVP,MD,VP,2,could,be\nVP,MD,VP,2,could,VBN\nVP,MD,VP,2,MD,be\nVP,MD,VP,2,MD,VBN\nTree depth(T-DEPTH) This feature measures the maximum height of the tree. The\nintuition is that a deep complex structure is indicative of a particularly disﬂuent and\nungrammatical sentence. Comparing the baseline SMT translations with the reference\n123\n330 S. Carter, C. Monz\nsentences for MT04, we see that the SMT output has on average a far higher tree depth;\nthe summed difference is between 20 and 56 levels higher for the SMT translations.\nWe normalise this feature by sentence length.\nUnit productions (UNIT-P) This feature takes the sum of all unit productions in the\nassociated parse tree. Comparing all the baseline SMT translations with the reference\nsentences for MT04, we see that the SMT output has a total difference of between\n7.5 and 20 more unit productions. It appears that overgeneration of unit productions\nis indicative of a disﬂuent sentence, and thus we include it as a feature.\nNT count (NT-C) This feature counts the number of non-terminal types in a sen-\ntence, normalised by sentence length. The aim is to capture the over/underproduction\nof certain feature types, a common problem in SMT output (e.g, a dropped verb).\nNode count (NO-C) This feature counts the number of nodes in a parse, normalised\nby sentence length. Despite being quite a simple feature, we note a general trend for\nparses of SMT sentences to contain more nodes than human-produced translations;\nthe SMT output for MT04 has a total difference of between 71 and 205 more nodes.\n4.2.3 Shallow features\nPOS n-grams We explore the use of POS n-gram features, from unigram to trigram\nfeatures.\nVerb agreement (VERBAGR) The verb agreement feature captures agreement\nbetween verb tenses that should match. We extract this feature by starting with each\ncoordinating conjunction and comma in a sentence, and examine a window of 5 words\non either side for verbs. If there are multiple verbs in this window, we return the nearest\none either side. This feature is extracted only if we ﬁnd a verb both to the left and right\nwithin the context length. For example, given the sentence “George/NNP was/VBD\nshouting/VBG and/CC screaming/VBG”, the verb agreement feature would be:\nVBG CC VBG\nThis feature can discriminate between the correct form “shouting and screaming ”\nand the incorrect “shouting and screamed”. Note this is not a trigram POS feature, as\nthe verbs do not have to be adjacent to the comma or coordinating conjunction.\nNT length(POSNUM) It is also possible to extract features from the POS layers that\ncapture frequency-based information. In particular, we wish to model the frequency\nof POS types for a given translation length. Features are of the form:\nlength (x)/num (POS, x)\nThe length of a sentence is represented by length (x), and the frequency with which\na speciﬁc POS tag occurs in the hypothesis translation x is num (POS, x). These fea-\ntures tie the number of POS tags to the length of a sentence, and thus model the\nunder/overproduction of certain POS types for speciﬁc sentence lengths. Here, we\nexamine ﬁve such types: verbs, nouns, adverbs, adjectives and determiners.\n123\nSyntactic discriminative language model rerankers 331\nPOS absence (NOPOS) A similar feature is one that models a lack of certain POS\ntypes, regardless of sentence length. Here again we model a lack of either verbs, nouns,\nadverbs, adjectives or determiners.\n5 Experiments\nIn this section, we evaluate the impact of different syntactic features used by a dis-\ncriminative language model on the MT04, MT05 and MT06 test sets. Note that we\nlooked at the baseline output of the MT04 test set in motivating features proposed in\nthis article. However, we did not examine the MT05 and MT06 test sets, which remain\nunseen. The SMT system and settings remain the same as those described in Sect. 3.2.\n5.1 Parameter optimisation\nThe SMT system is optimised on the MT02 and MT03 data sets. Since the parameters\nof the perceptron reranker also require optimisation, the development set was split into\nK folds. MERT was run on the union of the K-1 folds to optimise the parameters. The\nresulting setting was used to translate the remaining fold and to generate the n-best\nlists used for learning the optimal parameter settings of the perceptron reranker. The\nn-best lists contain the top-1000 most likely and distinct translation candidates, as it\nis possible that different alignments can lead to sentences which are lexically identi-\ncal but have different derivations. Untranslated source words were not removed from\ntranslations. Note that the Moses baseline we compare against was still trained on all\nthe development data in one go.\nTo optimise the β value in Eq. ( 1), we performed a grid search, with increments of\n0.1 examined between 0 and 1, and increments of 1 at 2\nx thereafter, on the MT0203\nset.\nAs we parse SMT output, all sentences were tokenised and lowercased in accor-\ndance with the output of the SMT system prior to training the parser. The simple\nunigram tagger was trained analogously, also on sections 02-21 of the PTB. A sen-\ntence was assigned the ⟨NOPARSE⟩ feature if the parser failed to generate a parse\nfor it. In such a situation, excluding the ⟨NOPARSE⟩ feature, a syntactic reranker can\nonly take into account lexical features when assigning a score to such a sentence. Note\nthat the ﬁrst sentence of each n-best list was always assigned a parse. The tagging\naccuracy of the parser and two POS taggers are as follows: CRF 97%, CM2 94.4%\nand S-POS 86.8%.\n5.2 Results\nHaving detailed the different annotation layers and syntactic features we intend to\nexplore, we now present experimental results. Table 4 presents Moses baseline 1-best\nresults on MT04, MT05 and MT06 test sets. In addition to the Moses baseline, we\npresent results using the averaged n-gram reranking model using unigram, bigram\nand trigram lexical features, as used by Li and Khudanpur (2008). Finally, we also\n123\n332 S. Carter, C. Monz\nTable 4 Moses baseline,\nn-gram-reranked and oracle\nresults on MT04, MT05 and\nMT06\nMT04 MT05 MT06\nMoses 48 .97 53 .92 38 .40\n+ DLM n-gram 49 .57 54 .42 39 .08\nOracle 61 .09 66 .34 50 .11\nTable 5 Results on MT\ndevelopment and test sets using\nsyntactic features from full parse\ntrees\nWe highlight in bold the largest\nscores\nMT0203 MT04 MT05 MT06\nMoses 51 .27 48 .97 53 .92 38 .40\n+ DLM n-gram 59 .87 49 .57 54 .42 39 .08\n+ DLM n-gram + POS 59 .70 49 .47 54.48 39.07\n+ DLM n-gram + SEQ-B 58 .52 49 .09 54 .11 39 .47\n+ DLM n-gram + SEQ-C 60 .37 49 .46 54 .19 39 .07\n+ DLM n-gram + CFG 59 .89 49 .53 54 .44 39 .58\n+ DLM n-gram + HEAD 61.53 49.44 54 .09 33 .45\n+ DLM n-gram + MAX-D 58 .79 49 .42 54 .08 39 .61\n+ DLM n-gram + UNIT-P 59 .51 49.81 54.39 39.76\n+ DLM n-gram + NT-C 58 .82 49 .51 54 .20 39 .68\n+ DLM n-gram + NO-C 53 .52 47 .14 52 .68 36 .92\npresent oracle results in the last row of Table 4, demonstrating the large room left for\nimprovement.\n5.2.1 Deep features\nIn Table 5 we present the results of using our perceptron rerankers with features\nextracted from full parse trees. The use of features from full parse trees did not help\nat all for MT04, apart from the UNIT-P model, which gave improvements of 0.34\nBLEU. For MT05, the CFG and POS feature sets show only small improvements.\nNote for MT05 the UNIT-P model no longer gives improvements above the n-gram\nonly model. For the MT06 test set, all syntactic models apart from HEAD achieve\nimprovements. These improvements against the lexical-only reranker do not hold for\nMT04 and MT05. Robustness is a problem; given unseen test data, we do not know\nwhether the inclusion of syntactic features from full parse trees will improve or harm\nthe translation quality of the system.\n5.2.2 POS layers compared\nIn comparing the effect of different syntactic toolkits, we conduct experiments with\nfeatures extracted from the POS taggers. The results are displayed in Table 6.\nThe CRF DLM outperforms the n-gram only DLM model on all three test sets. The\nS-POS DLM yields gains over the DLM n-gram model on all three of the test sets\nalso. Even though our S-POS tagger uses no back-off model or context, for two of the\n123\nSyntactic discriminative language model rerankers 333\nTable 6 BLEU scores and improvements when using features from our two POS taggers and POS\nannotations from the full tree parser\nMT04 MT05 MT06\nDLM n-gram 49 .57 54 .42 39 .08\nDLM n-gram + POS 49 .47 54 .48 39 .07\nImprovement −0.10 0 .06 −0.01\nDLM n-gram + CRF 49.74 54.51 39 .45\nImprovement 0.17 0.09 0 .37\nDLM n-gram + S-POS 49 .59 54.60 39 .48\nImprovement 0 .02 0.18 0 .40\nPOS features extracted from a simple unigram, maximum likelihood tagger give the largest improvements\non two of the three sets\nWe highlight in bold the best scores\nTable 7 Model results using POS tag frequency (vn, dn and allnum), lack of POS type (noall) and verb\nagreement (verbagr) features\nMT04 MT05 MT06\nMoses 48.97 53.92 38.40\n+ DLM n-gram 49.57 54.42 39.08\n++ S-POS+V+DNUM 49.65 † 54.60‡ 39.67‡\n++ S-POS+ALLNUM 49.65 54.60 39.67‡\n++ S-POS+NOALL 49.70‡ 54.46 39.69‡\n++ S-POS+VERBAGRE 49.44 54.56 39.55 ‡\nWe conduct signiﬁcance tests between the syntactic models and the DLM n-gram model. Significance at\n‡ p < 0.01. Significance at † p < 0.05\nWe highlight in bold the best scores\nthree test sets, it provides larger gains than the CRF tagger. Because the S-POS tagger\nresults in higher scores than the CRF tagger for two of the three test sets, we only use\nthe simple POS annotation layer for the following experiments.\n5.2.3 Shallow features using simple POS tagger\nTable 7 summarizes the results of using the POSNUM, NOPOS and VERBAGR\nfeatures. As for the POSNUM and NOPOS features, we look at speciﬁc POS cat-\negories, with POS replaced by the respective type V (verb), N (noun), D (deter-\nminer), RB (adverb), JJ (adjective) and ALL (all of the previous ﬁve types). For\nMT04, the best-performing model is S-POS +noall, with a significant improvement\nat p < 0.01 over the DLM n-gram model of 0.13 corpus-level BLEU points.\n3 For\nMT05, the best-performing model is S-POS +V+DNUM with a significant improve-\nment of 0.18 BLEU points at p < 0.01. The S-POS +ALLNUM model gives the same\n3 Statistical significance is calculated using the paired bootstrap resampling method ( Koehn 2004).\n123\n334 S. Carter, C. Monz\nFig. 5 Number of active\nfeatures (all features with\nnon-zero weights) in each model\nHEAD\nSEQ-C\nSEQ-B\nPOS\nCFG\nUNIT-P\nNT-C\nMAX-D\nN-GRAM\n 40000  60000  80000  100000  120000  140000  160000\nabsolute BLEU improvement for MT05, but is insignificant. For MT06, we have a\nlarger improvement of 0.41 BLEU, again at p < 0.01, using S-POS +NOALL. The\nS-POS+V+DNUM model is not the best-performing model on MT04 or MT06, but\nconsistently gives significant improvements.\n5.2.4 Deep + shallow feature combinations\nFinally, we investigate whether a combination of deep and shallow features leads to\nimprovements. As it is infeasible to try all feature combinations together, we pick the\nbest-performing deep feature type UNIT-P, and combine this with features extracted\nfrom our S-POS tagger. The combination of deep and shallow feature types does not\nlead to improvements over those presented in Table 7, so we conclude that the deep\nfeatures examined are redundant.\n6 Discussion\nAn explanation for the underperformance of features derived from full parse trees\nis overtraining. Examining the performance of the syntactic models on the MT0203\ndevelopment set, SEQ-C and HEAD models perform considerably better than the n-\ngram-only model and other syntactic models. This performance does not carry through\nto the test sets, indicating overtraining. Looking at the number of active (non-zero-\nweighted) features contained in the syntactic models in Fig. 5, we see that adding\nsyntactic features to our model increases the model size, and also that models SEQ-C\nand HEAD have the most features. We posit that these models are learning features\nthat explain the development data well, but do not generalise to unseen data.\nOverﬁtting is exacerbated by the lack of parses from which to extract parses. This\nis because we do not apply any sentence-level thresholding, meaning we are unable\nto parse every sentence in all the n-best lists, although every n-best list contained at\nleast one parse. For the MT0203 training set, only 87.3% of the sentences had a parse.\nThis means that the large feature spaces demonstrated by Fig. 5 were coming from a\nreduced example set. For the test sets, only between 80.7 and 82.6% of the sentences\nhad a parse, thus limiting the ability of the syntactic features to help improve trans-\nlation quality. The combination of a large feature space, over fewer training samples,\nleads to poor performance when using sparse features extracted from parsers.\n123\nSyntactic discriminative language model rerankers 335\nTable 8 N-gram precision rates and relative improvements on MT test sets above the Moses baseline and\nn-gram reranker\nTest set System n-gram precision (%)\n1 234\nMT04 Moses 81.46 57.80 41.17 29.70\n+n-gram 81.86 58.36 41.72 30.28\n++syntax 81.76 58.48 41 .92 30 .43\nImprovement (%) 0.4/ −0.1 1.2/0.2 1.8/0.5 2.5/0.5\nMT05 Moses 83.18 62.34 46.66 34.93\n+n-gram 83.31 62.74 47.20 35.54\n++syntax 83.28 62.96 47 .43 35 .74\nImprovement (%) 0.1/ −0.04 1/0.3 1.7/0.5 2.3/0.6\nMT06 Moses 74.17 47.45 31.27 21.05\n+n-gram 74.43 47.84 31.75 21.50\n++syntax 74.31 47.92 31 .87 21 .58\nImprovement (%) 0.2/ −0.2 1/0.2 1.9/0.4 2.5/0.4\nWe highlight in bold the highest precision scores, and show percentage improvements of our syntactic\nmodel above the Moses baseline and lexical-only reranker. For bigram, trigram and four-gram precision,\nsyntactic rerankers achieve the highest scores\nTable 8 presents the individual n-gram precision rates for our best syntactic models\nin comparison to the n-gram only DLM. There is a degradation in relative unigram\nprecision on all three sets, but we see an increase in bigram, trigram and four-gram\nprecision, indicating that our syntactic features resolve some word reordering prob-\nlems.\nTo see how the S-POS features help, Table 9 presents the different POS sequences\nassigned by the three different syntactic tools to the translation: he reiterated “ full\nsupport of the islamic republic for islamic government interim ” in afghanistan.T h i s\nsentence is chosen by the perceptron reranker using POS features from the CM2 parser.\nThe bigram “government interim” is tagged as “NN NN” by CM2 and the CRF tagger.\nThis feature has a positive weight, and thus is not penalised. Only S-POS tags “interim”\nas an adjective. In the last row of Table 9 we show the translation chosen by the per-\nceptron reranker using features from the S-POS tagger. This leads to an improvement\nof 17.34 sentence-level BLEU points, and is an example of the significant gains we\nmake using our S-POS tagger.\n7 Conclusion\nWe have examined the ability of a state-of-the-art parser to discriminate between ﬂu-\nent and disﬂuent English, ﬁnding that it outperforms a standard four-gram language\nmodel. Encouraged by this, we have proposed to use a syntactic discriminative lan-\nguage model to rerank translation hypothesis. The main contributions of this article\nare the extension of lexical discriminative rerankers with syntactic features, and the\n123\n336 S. Carter, C. Monz\nTable 9 POS assignments made by the three syntactic tools for the sentence he reiterated “full support of\nthe islamic republic for islamic government interim” in afghanistan\nSystem POS sequence BLEU\nCM2 PRP/he VBD/reiterated ”/” JJ/full NN/support IN/of DT/the\nNNP/islamic NNP/republic IN/for JJ/islamic NN/government\nNN/interim IN/in ”/” NNP/afghanistan./.\n50.30\nCRF he/PRP reiterated/VBD ”/” full/JJ support/NN of/IN the/DT\nislamic/JJ republic/NN for/IN islamic/JJ government/NN\ninterim/NN ”/” in/IN afghanistan/JJ./.\n50.30\nS-POS PRP/he VBD/reiterated ”/” JJ/full NN/support IN/of DT/the\nNNP/islamic NNP/republic IN/for NNP/islamic NN/government\nJJ/interim ”/” IN/in NNP/afghanistan./.\n50.30\nS-POS PRP/he VBD/reiterated ”/” JJ/full NN/support IN/of DT/the\nNNP/islamic NNP/republic IN/for NNP/islamic JJ/interim\nNN/government IN/in NNP/afghanistan./. ”/”\n67.64\nWe present sentence-level BLEU scores for both translations\nstudy of the use of a simple, non-context-aware POS tagger that overcomes problems\nencountered when using standard syntactic toolkits. Extensive experiments using our\nsyntactic models demonstrate significant improvements in BLEU over non-reranked\noutput and lexical-only reranked models. Furthermore, we have conducted experi-\nments examining the utility of deep and shallow syntactic annotation layers, and the\ndifferent features extractable from them.\nWe show that deep features, which are used with the intention of making more gen-\neralisable models, do not help within the perceptron reranking setting as they overﬁt\nthe training data, leading to problems with robustness of results over different test sets.\nAs far as future work is concerned, we believe an examination of the use of partial\nparsers may lead to a successful bridge between the beneﬁts of deep features from a\nfull parser and the coverage of data points from POS and other shallow tools.\nAcknowledgements The authors would like to thank Valentin Jijkoun, Sophia Katrenko and the anony-\nmous reviewers for their insightful comments and helpful discussions. This work has been funded in part\nby the European Commission through the CoSyne project FP7-ICT-4-248531.\nOpen Access This article is distributed under the terms of the Creative Commons Attribution Noncom-\nmercial License which permits any noncommercial use, distribution, and reproduction in any medium,\nprovided the original author(s) and source are credited.\nReferences\nArun A, Koehn P (2007) Online learning methods for discriminative training of phrase based statistical\nmachine translation. In: Machine translation summit XI: proceedings, Copenhagen, pp 15–20\nBikel DM (2002) Design of a multi-lingual, parallel-processing statistical parsing engine. In: HLT 2002:\nhuman language technology conference, proceedings of the second international conference on human\nlanguage technology research, San Diego, pp 178–182\nBilmes JA, Kirchhoff K (2003) Factored language models and generalized parallel backoff. In: HLT-NAACL\n2003: conference combining human language technology conference series and the North American\nchapter of the Association for Computational Linguistics conference series, Edmonton, pp 4–6\n123\nSyntactic discriminative language model rerankers 337\nBirch A, Osborne M, Koehn P (2007) CCG supertags in factored statistical machine translation. In: Pro-\nceedings of the second workshop on statistical machine translation (WMT 2007), Prague, pp 9–16\nBlunsom P, Cohn T, Osborne M (2008) A discriminative latent variable model for statistical machine transla-\ntion. In: ACL-08: HLT, 46th annual meeting of the Association for Computational Linguistics: human\nlanguage technologies, proceedings of the conference, Columbus, pp 200–208\nBrown PF, Pietra VJ, de Souza PV, Lai JC, Mercer RL (1992) Class-based n-gram models of natural\nlanguage. Comput Linguist 18(4):467–479\nCallison-Burch C, Osborne M, Koehn P (2006) Re-evaluating the role of BLEU in machine translation\nresearch. In: EACL-2006: 11th conference of the European chapter of the Association for Computa-\ntional Linguistics, Proceedings of the conference, Trento, pp 249–256\nCarter S, Monz C (2009) Parsing statistical machine translation output. In: Proceedings of the language &\ntechnology conference (LTC 2009), Pozna´n, pp 270–274\nCarter S, Monz C (2010) Discriminative syntactic reranking for statistical machine translation. In: AMTA\n2010: proceedings of the ninth conference of the Association for Machine Translation in the Americas,\nDenver, pp 3–12\nChang PC, Toutanova K (2007) A discriminative syntactic word order model for machine translation. In:\nproceedings of the 45th annual meeting of the Association for Computational Linguistics (ACL 2007),\nPrague, pp 9–16\nChen SF, Goodman J (1998) An empirical study of smoothing methods for language modelling. Tech. Rep.\nTR-10-98, University of Harvard, Cambridge\nChen X, Wang H, Lin X (2009) Learning to rank with a novel kernel perceptron method. In: Proceedings\nof the 18th ACM conference on information and knowledge management (CIKM 2009), Hong Kong,\npp 505–512\nChiang D (2005) A hierarchical phrase-based model for statistical machine translation. In: 43rd annual\nmeeting of the Association for Computational Linguistics (ACL 2005), Ann Arbor, pp 263–270\nChiang D (2007) Hierarchical phrase-based translation. Comput Linguist 33(2):201–228\nChiang D, Marton Y , Resnik P (2008) Online large-margin training of syntactic and structural translation\nfeatures. In: EMNLP 2008: 2008 conference on empirical methods in natural language processing,\nProceedings of the conference, Honolulu, pp 224–233\nChiang D, Wang W, Knight K (2009) 11,001 new features for statistical machine translations. In: Human\nlanguage technologies: the 2009 annual conference of the North American chapter of the Association\nfor Computational Linguistics, proceedings of the conference, Boulder, pp 218–226\nCollins M (1997) Three generative, lexicalized models for statistical parsing. In: Cohen PR, Wahlster W\n(eds) 35th annual meeting of the Association for Computational Linguistics and 8th conference of the\nEuropean chapter of the Association for Computational Linguistics, proceedings of the conference,\nMadrid, pp. 16–23\nCollins M (1999) Head-driven statistical models for natural language parsing. PhD thesis, University of\nPennsylvania, Philadelphia, Pennsylvania\nCollins M, Duffy N (2002) New ranking algorithms for parsing and tagging: kernels over discrete structures,\nand the voted perceptron. In: 40th annual meeting of the Association for Computational Linguistics,\nproceedings of the conference, Philadelphia, pp 263–270\nCollins M, Roark B, Saraclar M (2005) Discriminative syntactic language modeling for speech recognition.\nIn: 43rd annual meeting of the Association for Computational Linguistics (ACL 2005), Ann Arbor,\npp 507–514\nCrammer K, Singer Y (2001) Pranking with ranking. In: Proceedings of the twenty-ﬁfth annual conference\non advances in neural information processing systems (NIPS 2001), Vancouver, pp 641–647\nElsas JL, Carvalho VR, Carbonell JG (2008) Fast learning of document ranking functions with the commit-\ntee perceptron. In: Proceedings of the international conference on web search and web data mining\n(WSDM 2008), Stanford, pp 55–64\nEmami A, Papineni K, Sorensen J (2007) Large-scale distributed language modeling. In: Proceedings of\nthe international conference on acoustics, speech and signal processing (ICASSP 2007), Honolulu,\npp 37–40\nFreund Y, Schapire RE (1999) Large margin classiﬁcation using the perceptron algorithm. Mach Learn\n37(3):277–296\nGallant SI (1999) Perceptron based learning algorithms. IEEE Trans Neural Netw 1(2):179–191\n123\n338 S. Carter, C. Monz\nHasan S, Bender O, Ney H (2006) Reranking translation hypotheses using structural properties. In: EACL-\n2006: 11th conference of the European chapter of the Association for Computational Linguistics,\nproceedings of the conference, Trento, pp 41–48\nKoehn P (2004) Statistical significance tests for machine translation evaluation. In: Proceedings of the 2004\nconference on empirical methods in natural language processing, Barcelona, pp 388–395\nKoehn P, Hoang H (2007) Factored translation models. In: Proceedings of the 2007 joint conference on\nempirical methods in natural language processing and computational natural language learning (EMN-\nLP-CONLL 2007), Prague, pp 868–876\nKoehn P, Och FJ, Marcu D (2003) Statistical phrase-based translation. In: HLT-NAACL 2003: conference\ncombining human language technology conference series and the North American chapter of the\nAssociation for Computational Linguistics conference series, Edmonton, pp 48–54\nKoehn P, Hoang H, Birch A, Callison-Burch C, Federico M, Bertoldi N, Cowan B, Shen W, Moran C, Zens\nR, Dyer C, Bojar O, Constantin A, Herbst E (2007) Moses: open source toolkit for statistical machine\ntranslation. In: ACL 2007, proceedings of the interactive poster and demonstration sessions, Prague,\npp 177–180\nKulesza A, Shieber, S (2004) A learning approach to improving sentence-level MT evaluation. In: TMI-2004:\nproceedings of the tenth conference on theoretical and methodological issues in machine translation,\nBaltimore, pp 75–84\nLi Z, Khudanpur S (2008) Large-scale discriminative n-gram language models for statistical machine\ntranslation. In: AMTA-2008: MT at work: proceedings of the eighth conference of the Association for\nMachine Translation in the Americas, Waikiki, pp 133–142\nLiang P, Bouchard-Côté A, Klein D, Taskar B (2006) An end-to-end discriminative approach to machine\ntranslation. In: COLING ACL 2006, 21st international conference on computational linguistics and\n44th annual meeting of the Association for Computational Linguistics, proceedings of the conference,\nSydney, pp 761–768\nLin CY , Och FJ (2004) Orange: a method for evaluating automatic evaluation metrics for machine trans-\nlation. In: 20th international conference on computational linguistics, proceedings, vol I, Geneva,\npp 501–507\nMarcus M, Kim G, Marcinkiewicz MA, Macintyre R, Bies A, Ferguson M, Katz K, Schasberger B (1994)\nThe Penn Treebank: annotating predicate argument structure. In: Human language technology, pro-\nceedings of a workshop, Plainsboro, pp 114–119\nMcDonald R (2007) Characterizing the errors of data-driven dependency parsing models. In: EMNLP-\nCoNLL 2007: proceedings of the 2007 joint conference on empirical methods in natural language\nprocessing and computational natural language learning, Prague, pp 121–131\nMohit B, Hwa R (2007) Localization of difﬁcult-to-translate phrases. In: Proceedings of the second work-\nshop on statistical machine translation (WMT 2007), Prague, pp 248–255\nOch FJ (2003) Minimum error rate training in statistical machine translation. In: 41st annual meeting of\nthe Association for Computational Linguistics, proceedings of the conference, Sapporo, pp 160–167\nOch FJ, Ney H (2000) Improved statistical alignment models. In: 38th annual meeting of the Association\nfor Computational Linguistics, proceedings of the conference, Hong Kong, pp 440–447\nOch FJ, Gildea D, Khudanpur S, Sarkar A, Yamada K, Fraser A, Kumar S, Shen L, Smith D, Eng K, Jain\nV , Jin Z, Radev D (2003) Syntax for statistical machine translation. Tech. Rep. IRCS-00-07, Johns\nHopkins 2003 Summer Workshop, Baltimore\nOch FJ, Gildea D, Khudanpur S, Sarkar A, Yamada K, Fraser A, Kumar S, Shen L, Smith D, Eng K, Jain V ,\nJin Z, Radev D (2004) A smorgasbord of features for statistical machine translation. In: HLT-NAACL\n2004: human language technology conference of the North American chapter of the Association for\nComputational Linguistics, proceedings of the main conference, Boston, pp 161–168\nPapineni K, Roukos S, Ward T, Zhu WJ (2002) Bleu: a method for automatic evaluation of machine trans-\nlation. In: 40th annual meeting of the Association for Computational Linguistics, proceedings of the\nconference, Philadelphia, pp 311–318\nPost M, Gildea D (2008) Parsers as language models for statistical machine translation. In: AMTA-2008:\nMT at work: proceedings of the Eighth conference of the Association for Machine Translation in the\nAmericas, Waikiki, pp 172–181\nRoark B, Saraclar M, Collins M (2004a) Corrective language modeling for large vocabulary ASR with the\nperceptron algorithms. In: Proceedings of the international conference on acoustics, speech and signal\nprocessing (ICASSP 2004), Montreal, pp 749–752\n123\nSyntactic discriminative language model rerankers 339\nRoark B, Saraclar M, Collins M, Johnson M (2004b) Discriminative language modeling with conditional\nrandom ﬁelds and the perceptron algorithm. In: ACL-04, 42nd annual meeting of the Association for\nComputational Linguistics, proceedings of the conference, Barcelona, pp 47–54\nRoark B, Saraclar M, Collins M (2007) Discriminative n-gram language modeling. Comput Speech Lang\n21(2):373–392\nRosenblatt F (1958) The perceptron: a probabilistic model for information storage and organization in the\nbrain. Neurocomput Found Res 65(6):386–408\nShen L, Sarkar A, Och FJ (2004) Discriminative reranking for machine translation. In: HLT-NAACL 2004:\nhuman language technology conference of the North American chapter of the Association for Com-\nputational Linguistics, proceedings of the main Conference, Boston, pp 177–184\nSingh-Miller N, Collins C (2007) Trigger-based language modeling using a loss-sensitive perceptron algo-\nrithm. In: Proceedings of the international conference on acoustics, speech and signal processing\n(ICASSP 2007), Honolulu, pp 25–28\nStolcke A (2002) SRILM—an extensible language modeling toolkit. In: Proceedings of the international\nconference on spoken language processing (ICSLP 2002), Denver, pp 901–904\nTillmann C, Zhang T (2006) A discriminative global training algorithm for statistical MT. In: COLING ACL\n2006, 21st international conference on computational linguistics and 44th annual meeting of the Asso-\nciation for Computational Linguistics, proceedings of the conference, Sydney, pp 721–728\nWatanabe T, Suzuki J, Tsukada J, Isozaki H (2007) Online large-margin training for statistical machine\ntranslation. In: Proceedings of the 2007 joint conference on empirical methods in natural language\nprocessing and computational natural language learning (EMNLP-CONLL 2007), Prague, pp 764–773\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8843977451324463
    },
    {
      "name": "NIST",
      "score": 0.7835946083068848
    },
    {
      "name": "Machine translation",
      "score": 0.7831676602363586
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7687466144561768
    },
    {
      "name": "Discriminative model",
      "score": 0.7626692652702332
    },
    {
      "name": "Natural language processing",
      "score": 0.7610749006271362
    },
    {
      "name": "Parsing",
      "score": 0.5908041000366211
    },
    {
      "name": "Syntax",
      "score": 0.5702866911888123
    },
    {
      "name": "BLEU",
      "score": 0.4858779311180115
    },
    {
      "name": "Language model",
      "score": 0.4757221043109894
    },
    {
      "name": "Machine translation software usability",
      "score": 0.4689728021621704
    },
    {
      "name": "Evaluation of machine translation",
      "score": 0.4621359407901764
    },
    {
      "name": "Perceptron",
      "score": 0.45435774326324463
    },
    {
      "name": "Translation (biology)",
      "score": 0.4397633373737335
    },
    {
      "name": "Computational linguistics",
      "score": 0.4165685772895813
    },
    {
      "name": "Example-based machine translation",
      "score": 0.30806663632392883
    },
    {
      "name": "Artificial neural network",
      "score": 0.19244375824928284
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ],
  "cited_by": 11
}