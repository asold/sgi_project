{
    "title": "Efficiently Updating Domain Knowledge in Large Language Models: Techniques for Knowledge Injection without Comprehensive Retraining",
    "url": "https://openalex.org/W4399383377",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5099032456",
            "name": "Emily Czekalski",
            "affiliations": [
                "Vantage Power (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A1968497102",
            "name": "David R. Watson",
            "affiliations": [
                "Vantage Power (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4392135560",
        "https://openalex.org/W4391555991",
        "https://openalex.org/W4387806231",
        "https://openalex.org/W4385573338",
        "https://openalex.org/W4390891744",
        "https://openalex.org/W4398217726",
        "https://openalex.org/W4382322306",
        "https://openalex.org/W4399009725",
        "https://openalex.org/W4386409617",
        "https://openalex.org/W4407028609",
        "https://openalex.org/W4380993239",
        "https://openalex.org/W4286910674",
        "https://openalex.org/W4392593764",
        "https://openalex.org/W4390529182",
        "https://openalex.org/W4396621042"
    ],
    "abstract": "<title>Abstract</title> Recent advancements in natural language processing have highlighted the critical importance of efficiently updating pre-trained models with domain-specific knowledge. Traditional methods requiring comprehensive retraining are resource-intensive and impractical for many applications. The proposed techniques for knowledge injection, including the integration of adapter layers, retrieval-augmented generation (RAG), and knowledge distillation, offer a novel and significant solution to this challenge by enabling efficient updates without extensive retraining. Adapter layers allow for specialized fine-tuning, preserving the model's original capabilities while incorporating new information. RAG enhances the contextual relevance of generated responses by dynamically retrieving pertinent information from a domain-specific knowledge base. Knowledge distillation transfers specialized knowledge from smaller models to the larger pre-trained model, augmenting its performance in new domains. Experimental results demonstrated substantial improvements in accuracy, precision, recall, and F1-score, along with enhanced contextual relevance and coherence. The findings demonstrate the potential of the proposed methods to maintain the relevance and accuracy of language models in dynamic, information-rich environments, making them particularly useful in fields requiring timely and accurate information.",
    "full_text": null
}