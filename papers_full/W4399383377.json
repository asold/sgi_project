{
  "title": "Efficiently Updating Domain Knowledge in Large Language Models: Techniques for Knowledge Injection without Comprehensive Retraining",
  "url": "https://openalex.org/W4399383377",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5099032456",
      "name": "Emily Czekalski",
      "affiliations": [
        "Vantage Power (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1968497102",
      "name": "David R. Watson",
      "affiliations": [
        "Vantage Power (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392135560",
    "https://openalex.org/W4391555991",
    "https://openalex.org/W4387806231",
    "https://openalex.org/W4385573338",
    "https://openalex.org/W4390891744",
    "https://openalex.org/W4398217726",
    "https://openalex.org/W4382322306",
    "https://openalex.org/W4399009725",
    "https://openalex.org/W4386409617",
    "https://openalex.org/W4407028609",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W4286910674",
    "https://openalex.org/W4392593764",
    "https://openalex.org/W4390529182",
    "https://openalex.org/W4396621042"
  ],
  "abstract": "<title>Abstract</title> Recent advancements in natural language processing have highlighted the critical importance of efficiently updating pre-trained models with domain-specific knowledge. Traditional methods requiring comprehensive retraining are resource-intensive and impractical for many applications. The proposed techniques for knowledge injection, including the integration of adapter layers, retrieval-augmented generation (RAG), and knowledge distillation, offer a novel and significant solution to this challenge by enabling efficient updates without extensive retraining. Adapter layers allow for specialized fine-tuning, preserving the model's original capabilities while incorporating new information. RAG enhances the contextual relevance of generated responses by dynamically retrieving pertinent information from a domain-specific knowledge base. Knowledge distillation transfers specialized knowledge from smaller models to the larger pre-trained model, augmenting its performance in new domains. Experimental results demonstrated substantial improvements in accuracy, precision, recall, and F1-score, along with enhanced contextual relevance and coherence. The findings demonstrate the potential of the proposed methods to maintain the relevance and accuracy of language models in dynamic, information-rich environments, making them particularly useful in fields requiring timely and accurate information.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8107250928878784
    },
    {
      "name": "Retraining",
      "score": 0.7890462875366211
    },
    {
      "name": "Adapter (computing)",
      "score": 0.679140031337738
    },
    {
      "name": "Relevance (law)",
      "score": 0.6218932867050171
    },
    {
      "name": "Knowledge base",
      "score": 0.5470860004425049
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5362725853919983
    },
    {
      "name": "Domain knowledge",
      "score": 0.501248836517334
    },
    {
      "name": "Language model",
      "score": 0.4676603376865387
    },
    {
      "name": "Machine learning",
      "score": 0.4471731185913086
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.43667906522750854
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}