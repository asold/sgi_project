{
  "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models",
  "url": "https://openalex.org/W3095273266",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222769955",
      "name": "Liang, Kevin J",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4306579533",
      "name": "Hao, Weituo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288513539",
      "name": "Shen, Dinghan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2392674395",
      "name": "Zhou Yu-fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2049887450",
      "name": "Chen, Weizhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2256462353",
      "name": "Chen, Changyou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2619478227",
      "name": "Carin Lawrence",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3092206109",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3034537217",
    "https://openalex.org/W2978426779",
    "https://openalex.org/W2946411231",
    "https://openalex.org/W2978832950",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W3034619943",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W3209458476",
    "https://openalex.org/W2156163116",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1600737329",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2986300872",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W2945232141",
    "https://openalex.org/W3021805648",
    "https://openalex.org/W2996501936",
    "https://openalex.org/W3089659770",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3104240813",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3092806700",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2921861056",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2911588830",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W3127787589"
  ],
  "abstract": "Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (both memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-specific data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher's behavior on the linear interpolation of example pairs as well. We prove from a theoretical perspective that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach.",
  "full_text": "Published as a conference paper at ICLR 2021\nMIXKD: T OWARDS EFFICIENT DISTILLATION OF\nLARGE -SCALE LANGUAGE MODELS\nKevin J Liang1,2∗, Weituo Hao1∗, Dinghan Shen3, Yufan Zhou4, Weizhu Chen3,\nChangyou Chen4, Lawrence Carin1\n1Duke University 2Facebook AI 3Microsoft Dynamics 365 AI\n4State University of New York at Buffalo\n{kevin.liang, weituo.hao}@duke.edu\nABSTRACT\nLarge-scale language models have recently demonstrated impressive empirical\nperformance. Nevertheless, the improved results are attained at the price of bigger\nmodels, more power consumption, and slower inference, which hinder their appli-\ncability to low-resource (both memory and computation) platforms. Knowledge\ndistillation (KD) has been demonstrated as an effective framework for compress-\ning such big models. However, large-scale neural network systems are prone to\nmemorize training instances, and thus tend to make inconsistent predictions when\nthe data distribution is altered slightly. Moreover, the student model has few op-\nportunities to request useful information from the teacher model when there is\nlimited task-speciﬁc data available. To address these issues, we propose MixKD,\na data-agnostic distillation framework that leverages mixup, a simple yet efﬁcient\ndata augmentation approach, to endow the resulting model with stronger gener-\nalization ability. Concretely, in addition to the original training examples, the\nstudent model is encouraged to mimic the teacher’s behavior on the linear inter-\npolation of example pairs as well. We prove from a theoretical perspective that\nunder reasonable conditions MixKD gives rise to a smaller gap between the gen-\neralization error and the empirical error. To verify its effectiveness, we conduct\nexperiments on the GLUE benchmark, where MixKD consistently leads to signif-\nicant gains over the standard KD training, and outperforms several competitive\nbaselines. Experiments under a limited-data setting and ablation studies further\ndemonstrate the advantages of the proposed approach.\n1 I NTRODUCTION\nRecent language models (LM) pre-trained on large-scale unlabeled text corpora in a self-supervised\nmanner have signiﬁcantly advanced the state of the art across a wide variety of natural language\nprocessing (NLP) tasks (Devlin et al., 2018; Liu et al., 2019c; Yang et al., 2019; Joshi et al., 2020;\nSun et al., 2019b; Clark et al., 2020; Lewis et al., 2019; Bao et al., 2020). After the LM pre-\ntraining stage, the resulting parameters can be ﬁne-tuned to different downstream tasks. While these\nmodels have yielded impressive results, they typically have millions, if not billions, of parameters,\nand thus can be very expensive from storage and computational standpoints. Additionally, during\ndeployment, such large models can require a lot of time to process even a single sample. In settings\nwhere computation may be limited ( e.g. mobile, edge devices), such characteristics may preclude\nsuch powerful models from deployment entirely.\nOne promising strategy to compress and accelerate large-scale language models is knowledge distil-\nlation (Zhao et al., 2019; Tang et al., 2019; Sun et al., 2020). The key idea is to train a smaller model\n(a “student”) to mimic the behavior of the larger, stronger-performing, but perhaps less practical\nmodel (the “teacher”), thus achieving similar performance with a faster, lighter-weight model. A\nsimple but powerful method of achieving this is to use the output probability logits produced by the\nteacher model as soft labels for training the student (Hinton et al., 2015). With higher entropy than\none-hot labels, these soft labels contain more information for the student model to learn from.\n∗Equal contribution\n1\narXiv:2011.00593v2  [cs.CL]  17 Mar 2021\nPublished as a conference paper at ICLR 2021\nPrevious efforts on distilling large-scale LMs mainly focus on designing better training objectives,\nsuch as matching intermediate representations (Sun et al., 2019a; Mukherjee & Awadallah, 2019),\nlearning multiple tasks together (Liu et al., 2019a), or leveraging the distillation objective during the\npre-training stage (Jiao et al., 2019; Sanh et al., 2019). However, much less effort has been made\nto enrich task-speciﬁc data, a potentially vital component of the knowledge distillation procedure.\nIn particular, tasks with fewer data samples provide less opportunity for the student model to learn\nfrom the teacher. Even with a well-designed training objective, the student model is still prone to\noverﬁtting, despite effectively mimicking the teacher network on the available data.\nIn response to these limitations, we propose improving the value of knowledge distillation by us-\ning data augmentation to generate additional samples from the available task-speciﬁc data. These\naugmented samples are further processed by the teacher network to produce additional soft labels,\nproviding the student model more data to learn from a large-scale LM. Intuitively, this is akin to a\nstudent learning more from a teacher by asking more questions to further probe the teacher’s an-\nswers and thoughts. In particular, we demonstrate that mixup (Zhang et al., 2018) can signiﬁcantly\nimprove knowledge distillation’s effectiveness, and we show with a theoretical framework why this\nis the case. We call our framework MixKD.\nWe conduct experiments on 6 GLUE datasets (Wang et al., 2019) across a variety of task types,\ndemonstrating that MixKD signiﬁcantly outperforms knowledge distillation (Hinton et al., 2015) and\nother previous methods that compress large-scale language models. In particular, we show that our\nmethod is especially effective when the number of available task data samples is small, substantially\nimproving the potency of knowledge distillation. We also visualize representations learned with and\nwithout MixKD to show the value of interpolated distillation samples, perform a series of ablation\nand hyperparameter sensitivity studies, and demonstrate the superiority of MixKD over other BERT\ndata augmentation strategies.\n2 R ELATED WORK\n2.1 M ODEL COMPRESSION\nCompressing large-scale language models, such as BERT, has attracted signiﬁcant attention recently.\nKnowledge distillation has been demonstrated as an effective approach, which can be leveraged\nduring both the pre-training and task-speciﬁc ﬁne-tuning stages. Prior research efforts mainly focus\non improving the training objectives to beneﬁt the distillation process. Speciﬁcally, Turc et al. (2019)\nadvocate that task-speciﬁc knowledge distillation can be improved by ﬁrst pre-training the student\nmodel. It is shown by Clark et al. (2019) that a multi-task BERT model can be learned by distilling\nfrom multiple single-task teachers. Liu et al. (2019b) propose learning a stronger student model\nby distilling knowledge from an ensemble of BERT models. Patient knowledge distillation (PKD),\nintroduced by Sun et al. (2019a), encourages the student model to mimic the teacher’s intermediate\nlayers in addition to output logits. DistilBERT (Sanh et al., 2019) reduces the depth of BERT\nmodel by a factor of 2 via knowledge distillation during the pre-training stage. In this work, we\nevaluate MixKD on the case of task-speciﬁc knowledge distillation. Notably, it can be extended\nto the pre-training stage as well, which we leave for future work. Moreover, our method can be\nﬂexibly integrated with different KD training objectives (described above) to obtain even better\nresults. However, we utilize the BERT-base model as the testbed in this paper without loss of\ngenerality.\n2.2 D ATA AUGMENTATION IN NLP\nData augmentation (DA) has been studied extensively in computer vision as a powerful technique to\nincorporate prior knowledge of invariances and improve the robustness of learned models (Simard\net al., 1998; 2003; Krizhevsky et al., 2012). Recently, it has also been applied and shown effective\non natural language data. Many approaches can be categorized as label-preserving transformations,\nwhich essentially produce neighbors around a training example that maintain its original label. For\nexample, EDA (Wei & Zou, 2019) propose using various rule-based operations such as synonym\nreplacement, word insertion, swap or deletion to obtain augmented samples. Back-translation (Yu\net al., 2018; Xie et al., 2019) is another popular approach belonging to this type, which relies on\npre-trained translation models. Additionally, methods based on paraphrase generation have also\nbeen leveraged from the data augmentation perspective (Kumar et al., 2019). On the other hand,\nlabel-altering techniques like mixup (Zhang et al., 2018) have also been proposed for language (Guo\net al., 2019; Chen et al., 2020), producing interpolated inputs and labels for the models predict. The\n2\nPublished as a conference paper at ICLR 2021\nproposed MixKD framework leverages the ability of mixup to facilitate the student learning more\ninformation from the teacher. It is worth noting that MixKD can be combined with arbitrary label-\npreserving DA modules. Back-translation is employed as a special case here, and we believe other\nadvanced label-preserving transformations developed in the future can beneﬁt the MixKD approach\nas well.\n2.3 M IXUP\nMixup (Zhang et al., 2018) is a popular data augmentation strategy to increase model generalizability\nand robustness by training on convex combinations of pairs of inputs and labels(xi,yi) and (xj,yj):\nx′= λxi + (1−λ)xj (1)\ny′= λyi + (1−λ)yj (2)\nwith λ ∈[0,1] and (x′,y′) being the resulting virtual training example. This concept of interpo-\nlating samples was later generalized with Manifold mixup (Verma et al., 2019a) and also found to\nbe effective in semi-supervised learning settings (Verma et al., 2019b;c; Berthelot et al., 2019b;a).\nOther strategies include mixing together samples resulting from chaining together other augmen-\ntation techniques (Hendrycks et al., 2020), or replacing linear interpolation with the cutting and\npasting of patches (Yun et al., 2019).\n3 M ETHODOLOGY\n3.1 P RELIMINARIES\nIn NLP, an input sample i is often represented as a vector of tokens wi = {wi,1,wi,2,...,w i,T},\nwith each token wi,t ∈RV a one-hot vector often representing words (but also possibly subwords,\npunctuation, or special tokens) and V being the vocabulary size. These discrete tokens are then\nmapped to word embeddingsxi = {xi,1,xi,2,...,x i,T}, which serve as input to the machine learning\nmodel f. For supervised classiﬁcation problems, a one-hot label yi ∈RC indicates the ground-truth\nclass of xi out of C possible classes. The parameters θ of f are optimized with some form of\nstochastic gradient descent so that the output of the model f(xi) ∈RC is as close to yi as possible,\nwith cross-entropy as the most common loss function:\nLMLE = −1\nn\nn∑\ni\nyi ·log(f(xi)) (3)\nwhere nis the number of samples, and ·is the dot product.\n3.2 K NOWLEDGE DISTILLATION FOR BERT\nConsider two models f and g parameterized by θT and θS, respectively, with |θT|≫| θS|. Given\nenough training data and sufﬁcient optimization, f is likely to yield better accuracy than g, due to\nhigher modeling capacity, but may be too bulky or slow for certain applications. Being smaller in\nsize, gis more likely to satisfy operational constraints, but its weaker performance can be seen as a\ndisadvantage. To improve g, we can use the output predictionf(xi) on input xi as extra supervision\nfor gto learn from, seeking to matchg(xi) with f(xi). Given these roles, we refer togas the student\nmodel and f as the teacher model.\nWhile there are a number of recent large-scale language models driving the state of the art, we fo-\ncus here on BERT (Devlin et al., 2018) models. Following Sun et al. (2019a), we use the notation\nBERTk to indicate a BERT model with k Transformer (Vaswani et al., 2017) layers. While pow-\nerful, BERT models also tend to be quite large; for example, the default bert-base-uncased\n(BERT12) has ∼110M parameters. Reducing the number of layers (e.g. using BERT3) makes such\nmodels signiﬁcantly more portable and efﬁcient, but at the expense of accuracy. With a knowledge\ndistillation set-up, however, we aim to reduce this loss in performance.\n3.3 M IXUP DATA AUGMENTATION FOR KNOWLEDGE DISTILLATION\nWhile knowledge distillation can be a powerful technique, if the size of the available data is small,\nthen the student has only limited opportunities to learn from the teacher. This may make it much\nharder for knowledge distillation to close the gap between student and teacher model performance.\nTo correct this, we propose using data augmentation for knowledge distillation. While data aug-\nmentation (Yu et al., 2018; Xie et al., 2019; Yun et al., 2019; Kumar et al., 2019; Hendrycks et al.,\n3\nPublished as a conference paper at ICLR 2021\n2020; Shen et al., 2020; Qu et al., 2020) is a commonly used technique across machine learning\nfor increasing training samples, robustness, and overall performance, a limited modeling capacity\nconstrains the representations the student is capable of learning on its own. Instead, we propose\nusing the augmented samples to further query the teacher model, whose large size often allows it to\nlearn more powerful features.\nWhile many different data augmentation strategies have been proposed for NLP, we focus on\nmixup (Zhang et al., 2018) for generating additional samples to learn from the teacher. Mixup’s\nvicinal risk minimization tends to result in smoother decision boundaries and better generalization,\nwhile also being cheaper to compute than methods such as backtranslation (Yu et al., 2018; Xie\net al., 2019). Mixup was initially proposed for continuous data, where interpolations between data\npoints remain in-domain; its efﬁcacy was demonstrated primarily on image data, but examples in\nspeech recognition and tabular data were also shown to demonstrate generality.\nDirectly applying mixup to NLP is not quite as straightforward as it is for images, as language\ncommonly consists of sentences of variable length, each comprised of discrete word tokens. Since\nperforming mixup directly on the word tokens doesn’t result in valid language inputs, we instead\nperform mixup on the word embeddings at each time step xi,t (Guo et al., 2019). This can be\ninterpreted as a special case of Manifold mixup Verma et al. (2019a), where the mixing layer is set\nto the embedding layer. In other words, mixup samples are generated as:\nx′\ni,t = λxi,t + (1−λ)xj,t ∀t (4)\ny′\ni = λyi + (1−λ)yj (5)\nwith λ ∈[0,1]; random sampling of λfrom a Uniform or Beta distribution are common choices.\nNote that we index the augmented sample with i regardless of the value of λ. Sentence length\nvariability can be mitigated by grouping mixup pairs by length. Alternatively, padding is a common\ntechnique for setting a consistent input length across samples; thus, if x(i) contains more word\ntokens than x(j), then the extra word embeddings are mixed up with zero paddings. We ﬁnd this\napproach to be effective, while also being much simpler to implement.\nWe query the teacher model with the generated mixup samplex′\ni, producing output predictionf(x′\ni).\nThe student is encouraged to imitate this prediction on the same input, by minimizing the objective:\nLTMKD = d(f(x′\ni),g(x′\ni)) (6)\nwhere d(·,·) is a distance metric for distillation, with temperature-adjusted cross-entropy and mean\nsquare error (MSE) being common choices.\nSince we have the mixup samples already generated (with an easy-to-generate interpolated pseu-\ndolabel y′\ni), we can also train the student model on these augmented data samples in the usual way,\nwith a cross-entropy objective:\nLSM = −1\nn\nn∑\ni\ny′\ni ·log(f(x′\ni)) (7)\nOur ﬁnal objective for MixKD is a sum of the original data cross-entropy loss, student cross-entropy\nloss on the mixup samples, and knowledge distillation from the teacher on the mixup samples:\nL= LMLE + αSMLSM + αTMKDLTMKD (8)\nwhere αSM and αTMKD are hyperparameters weighting the loss terms.\n3.4 T HEORETICAL ANALYSIS\nWe develop a theoretical foundation for the proposed framework. We wish to prove that by adopting\ndata augmentation for knowledge distillation, one can achieve i) a smaller gap between generaliza-\ntion error and empirical error, and ii) better generalization.\nTo this end, assume the original training data {xi}n\ni=1 are sampled i.i.d. from the true data distri-\nbution p(x), and the augmented data distribution by mixup is denoted as q(x) (apparently pand q\nare dependent). Let f be the teacher function, and g ∈G be the learnable student function. Denote\nthe loss function to learn g as l(·,·)1. The population risk w.r.t. p(x) is deﬁned as R(f,g,p ) =\n1This is essentially the same asL in equation 8. We use a different notationl(f(x), g(x)) to explicitly spell\nout the two data-wise arguments f(x) and g(x).\n4\nPublished as a conference paper at ICLR 2021\nEx∼p(x) [l(f(x),g(x))], and the empirical risk as Remp(f,g, {xi}n\ni=1) = 1\nn\n∑n\ni=1 l(f(xi),g(xi)).\nA classic statement for generalization is the following: with at least 1 −δprobability, we have\nR(f,gp,p) −Remp(f,gp,{xi}n\ni=1) ≤ϵ, (9)\nwhere ϵ >0, and we have used gp to indicate that the function is learned based on p(x). Note\ndifferent training data would correspond to a different error ϵin equation 9. We use ϵp to denote\nthe minimum value over all ϵ’s satisfying equation 9. Similarly, we can replace p with q, and\n{xi}n\ni=1 with {xi}a\ni=1 ∪{x′\ni}b\ni=1 in equation 9 in the data-augmentation case. In this case, the\nstudent function is learned based on both the training data and augmented data, which we denote as\ng∗. Similarly, we also have a corresponding minimum error, which we denote as ϵ∗. Consequently,\nour goal of better generalization corresponds to proving R(f,g∗,p) ≤R(f,gp,p), and the goal of\na smaller gap corresponds to proving ϵ∗ ≤ϵp. In our theoretical results, we will give conditions\nwhen these goals are achievable. First, we consider the following three cases about the joint data\nX ≜ {xi}a\ni=1 ∪{x′\ni}b\ni=1 and the function class G:\n• Case 1: There exists a distribution ˜psuch that X are i.i.d. samples from it2; Gis a ﬁnite set.\n• Case 2: There exists ˜psuch that X are i.i.d. samples from it; Gis an inﬁnite set.\n• Case 3: There does not exist a distribution ˜psuch that X are i.i.d. samples from it.\nOur theoretical results are summarized in Theorems 1-3, which state that with enough augmented\ndata, our method can achieve smaller generalization errors. Proofs are given in the Appendix.\nTheorem 1 Assume the loss function l(·,·) is upper bounded by M >0. Under Case 1, there exists\na constant c> 0 such that if\nb≥M2 log(|G|/δ)\nc −a\nthen\nϵ∗≤ϵp\nwhere ϵ∗ and ϵp denote the minimal generalization gaps one can achieve with or without aug-\nmented data, with at least 1 −δ probability. If further assuming a better empirical risk with\ndata augmentation (which is usually the case in practice), i.e., Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤\nRemp(f,gp,{xi}n\ni=1), we have\nR(f,g∗,p) ≤R(f,gp,p)\nTheorem 2 Assume the loss function l(·,·) is upper bounded by M >0 and Lipschitz continuous.\nFix the probability parameter δ. Under Case 2, there exists a constant c> 0 such that if\nb≥M2 log(1/δ)\nc −a\nthen\nϵ∗≤ϵp\nwhere ϵ∗and ϵp denote the minimal generalization gaps one can achieve with or without augmented\ndata, with at least 1 −δprobability. If further assuming a better empirical risk with data augmen-\ntation, i.e., Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤Remp(f,gp,{xi}n\ni=1), we have\nR(f,g∗,p) ≤R(f,gp,p)\nA more interesting setting is Case 3. Our result is based on Baxter (2000), which studies learning\nfrom different and possibly correlated distributions.\nTheorem 3 Assume the loss function l(·,·) is upper bounded. Under Case 3, there exists constants\nc1,c2,c3 >0 such that if\nb≥alog(4/δ)\nc1a−c2\nand a≥c3\n2We make such an assumption because xi and x′\ni are dependent, thus existence of ˜p is unknown.\n5\nPublished as a conference paper at ICLR 2021\nModel SST-2 MRPC QQP MNLI-m QNLI RTE\nBERT12 92.20 90.53/86.52 88.21/91.25 84.12 91.32 77.98\nDistilBERT6 91.3 87.5/82.4 —-/88.5 82.2 89.2 59.9\nBERT6-FT 90.94 88.54/83.82 87.16/90.43 81.28 88.25 66.43\nBERT6-TMKD 91.63 88.93/83.82 86.60/90.27 81.49 88.71 65.34\nBERT6-SM+TMKD 91.17 89.30/84.31 87.19/90.56 82.02 88.63 65.34\nBERT6-FT+BT 91.74 89.60/84.80 87.06/90.39 82.10 87.68 67.51\nBERT6-TMKD+BT 91.86 89.52/84.56 87.15/90.59 82.17 88.38 69.98\nBERT6-SM+TMKD+BT 92.09 89.22/84.07 87.57/90.78 82.53 88.82 67.87\nBERT3-FT 87.16 81.68/71.08 84.99/88.65 75.55 83.98 58.48\nBERT3-TMKD 88.76 81.62/71.08 83.27/87.80 75.73 84.26 58.48\nBERT3-SM+TMKD 88.99 81.73/71.08 84.47/88.37 75.52 84.24 59.57\nBERT3-FT+BT 88.88 83.36/74.26 85.31/88.81 76.88 83.67 59.21\nBERT3-TMKD+BT 89.79 84.46/75.74 85.17/89.00 77.19 84.68 62.82\nBERT3-SM+TMKD+BT 90.37 84.14/75.25 85.56/89.09 77.52 84.83 60.65\nTable 1: GLUE dev set results. We report the results of our BERT12 teacher model, the 6-layer\nDistilBERT, and 3- and 6-layer MixKD student models with various ablations. DistilBERT results\ntaken from Sanh et al. (2019). For MRPC and QQP, we report F1/Accuracy.\nthen\nϵ∗≤ϵp\nwhere ϵ∗and ϵp denote the minimal generalization gaps one can achieve with or without augmented\ndata, with at least 1 −δprobability. If further assuming a better empirical risk with data augmen-\ntation, i.e., Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤Remp(f,gp,{xi}n\ni=1), we have\nR(f,g∗,p) ≤R(f,gp,p)\nRemark 4 For Theorem 3 to hold, based on Baxter (2000), it is enough to ensure {xi,x′\ni}and\n{xj,x′\nj}to be independent for i ̸= j. We achieve this by constructing x′\ni with xi and an extra\nrandom sample from the training data. Since all (xi,xj) and the extra random samples are inde-\npendent, the resulting concatenation will also be independent.\n4 E XPERIMENTS\nWe demonstrate the effectiveness of MixKD on a number of GLUE (Wang et al., 2019) dataset tasks:\nStanford Sentiment Treebank (SST-2) (Socher et al., 2013), Microsoft Research Paraphrase Corpus\n(MRPC) (Dolan & Brockett, 2005), Quora Question Pairs (QQP) 3, Multi-Genre Natural Language\nInference (MNLI) (Williams et al., 2018), Question Natural Language Inference (QNLI) (Rajpurkar\net al., 2016), and Recognizing Textual Entailment (RTE) (Dagan et al., 2005; Haim et al., 2006;\nGiampiccolo et al., 2007; Bentivogli et al., 2009). Note that MNLI contains both an in-domain\n(MNLI-m) and cross-domain (MNLI-mm) evaluation set. These datasets span sentiment analysis,\nparaphrase similarity matching, and natural language inference types of tasks. We use the Hugging\nFace Transformers4 implementation of BERT for our experiments.\n4.1 G LUE DATASET EVALUATION\nWe ﬁrst analyze the contributions of each component of our method, evaluating on the dev set\nof the GLUE datasets. For the teacher model, we ﬁne-tune a separate 12 Transformer-layer\nbert-base-uncased (BERT12) for each task. We use the smaller BERT3 and BERT6 as\nthe student model. We ﬁnd that initializing the embeddings and Transformer layers of the student\nmodel from the ﬁrst klayers of the teacher model provides a signiﬁcant boost to ﬁnal performance.\nWe use MSE as the knowledge distillation distance metric d(·,·). We generate one mixup sample\nfor each original sample in each minibatch (mixup ratio of 1), with λ ∼Beta(0.4,0.4). We set\nhyperparameters weighting the components in the loss term in equation 8 as αSM = αTMKD = 1.\n3data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n4https://huggingface.co/transformers/\n6\nPublished as a conference paper at ICLR 2021\nModel SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE\nBERT 12 93.5 88.9/84.8 71.2/89.2 84.6 83.4 90.5 66.4\nBERT 6 -FT 90.7 85.9/80.2 69.2/88.2 80.4 79.7 86.7 63.6\nBERT 6 -KD 91.5 86.2/80.6 70.1/88.8 80.2 79.8 88.3 64.7\nBERT 6 -PKD 92.0 85.0/79.9 70.7 /88.9 81.5 81.0 89.0 65.5\nBERT 6 -MixKD 92.5 86.4/81.9 70.5/ 89.1 82.2 81.2 88.2 68.3\nBERT 3 -FT 86.4 80.5/72.6 65.8/86.9 74.8 74.3 84.3 55.2\nBERT 3 -KD 86.9 79.5/71.1 67.3/87.6 75.4 74.8 84.0 56.2\nBERT 3 -PKD 87.5 80.7/72.5 68.1/87.8 76.7 76.3 84.7 58.2\nBERT 3 -MixKD 89.5 83.3 /75.2 67.2/87.4 77.2 76.8 84.4 62.0\nTable 3: GLUE test server results. We show results for the full variants of the 3- and 6-layerMixKD\nstudent models (SM+TMKD+BT). Knowledge distillation (KD) and Patient Knowledge Distillation\n(PKD) results are from Sun et al. (2019a).\nAs a baseline, we ﬁne-tune the student model on the task dataset without any distillation or aug-\nmentation, which we denote as BERTk-FT. We compare this againstMixKD, with both knowledge\ndistillation on the teacher’s predictions (LTMKD) and mixup for the student ( LSM), which we call\nBERTk-SM+TMKD. We also evaluate an ablated version without the student mixup loss (BERTk-\nTMKD) to highlight the knowledge distillation component speciﬁcally. We note that our method\ncan also easily be combined with other forms of data augmentation. For example, backtranslation\n(translating an input sequence to the data space of another language and then translating back to the\noriginal language) tends to generate varied but semantically similar sequences; these sentences also\ntend to be of higher quality than masking or word-dropping approaches. We show that our method\nhas an additive effect with other techniques by also testing our method with the dataset augmented\nwith German backtranslation, using thefairseq (Ott et al., 2019) neural machine translation code-\nbase to generate these additional samples. We also compare all of the aforementioned variants with\nbacktranslation samples augmenting the data; we denote these variants with an additional +BT.\nModel Inference Speed (samples/second) # of ParametersBERT12Teacher 115 109,483,778BERT6Student 252 66,956,546BERT3Student 397 45,692,930\nTable 2: Computation cost comparison of teacher\nand student models on SST-2 with batch size of\n16 on a Nvidia TITAN X GPU.\nWe report the model accuracy (and F1 score,\nfor MRPC and QQP) in Table 1. We also\nshow the performance of the full-scale teacher\nmodel (BERT12) and DistilBERT (Sanh et al.,\n2019), which performs basic knowledge distil-\nlation during BERT pre-training to a 6-layer\nmodel. For our method, we observe that a com-\nbination of data augmentation and knowledge\ndistillation leads to signiﬁcant gains in performance, with the best variant often being the combi-\nnation of teacher mixup knowledge distillation, student mixup, and backtranslation. In the case of\nSST-2, for example, BERT6-SM+TMKD+BT is able to capture 99.88% of the performance of the\nteacher model, closing 91.27% of the gap between the ﬁne-tuned student model and the teacher,\ndespite using far fewer parameters and having a much faster inference speed (Table 2).\nAfter analyzing the contributions of the components of our model on the dev set, we ﬁnd the\nSM+TMKD+BT variant to have the best performance overall and thus focus on this variant. We\nsubmit this version of MixKD to the GLUE test server, reporting its results in comparison with\nﬁne-tuning (FT), vanilla knowledge distillation (KD) (Hinton et al., 2015), and patient knowledge\ndistillation (PKD) (Sun et al., 2019a) in Table 3. Once again, we observe that our model outperforms\nthe baseline methods on most tasks.\n4.2 L IMITED -DATA SETTINGS\nOne of the primary motivations for using data augmentation for knowledge distillation is to give the\nstudent more opportunities to query the teacher model. For datasets with a large enough number of\nsamples relative to the task’s complexity, the original dataset may provide enough chances to learn\nfrom the teacher, reducing the relative value of data augmentation.\nAs such, we also evaluate MixKD with a BERT3 student on downsampled versions of QQP, MNLI\n(matched and mismatched), and QNLI in Figure 1. We randomly select10% and 1% of the data from\n7\nPublished as a conference paper at ICLR 2021\nQQP MNLI MNLI-mm QNLI\nDataset\n60\n65\n70\n75\n80\n85\n90Accuracy (%)\nFT\nFT+MixKD (w/o BT)\nFT (w/ BT)\nFT+MixKD (w/ BT)\nQQP MNLI MNLI-mm QNLI\nDataset\n50\n55\n60\n65\n70\n75\n80Accuracy (%)\nFT\nFT+MixKD (w/o BT)\nFT (w/ BT)\nFT+MixKD (w/ BT)\nFigure 1: Results of limited data case, where both the teacher and student models are learned with\nonly 10% (left) or 1% of the training data (right).\nthese datasets to train both the teacher and student models, using the same subset for all experiments\nfor fair comparison. In this data limited setting, we observe substantial gains from MixKD over\nthe ﬁne-tuned model for QQP ( +2.0%, +3.0%), MNLI-m (+3.9%, +3.4%), MNLI-mm (+4.4%,\n+3.3%), and QNLI (+2.4%, +4.1%) for 10% and 1% of the training data.\n4.3 E MBEDDINGS VISUALIZATION\n(a)\n (b)\nFigure 2: Latent space of randomly sampled\ntraining data and their mixup neighbours encoded\nby student model (a) learned by standard ﬁne-\ntuning (b) learned with MixKD.\nWe perform a qualitative examination of the ef-\nfect of the proposed MixKD by visualizing the\nlatent space between positive and negative sam-\nples as encoded by the student model with t-\nSNE plots (Maaten & Hinton, 2008). In Fig-\nure 2, we show the shift of the transformer\nfeatures at the [CLS] token position, with and\nwithout mixup data augmentation from the\nteacher. We randomly select a batch of 100\nsentences from the SST-2 dataset, of which 50\nare positive sentiment (blue square) and 50 are\nnegative sentiment (red circle). The intermedi-\nate mixup neighbours are indicated by triangles\nwith color determined by the closeness to the\npositive group or negative group. From Fig-\nure 2(a) to Figure 2(b), MixKD forces the lin-\nearly interpolated samples to be aligned with the manifold formed by the real training data and leads\nthe student model to explore meaningful regions of the feature space effectively.\n4.4 H YPERPARAMETER SENSITIVITY & FURTHER ANALYSIS\nLoss Hyperparameters Our ﬁnal objective in equation 8 has hyperparameters αSM and αTMKD,\nwhich control the weight of the student model’s cross-entropy loss for the mixup samples and the\nknowledge distillation loss with the teacher’s predictions on the mixup samples, respectively. We\ndemonstrate that the model is fairly stable over a wide range by sweeping both αSM and αTMKD\nover the range {0.1,0.5,1.0,2.0,10.0}. We do this for a BERT3 student and BERT12 teacher,\nwith SST-2 as the task; we show the results of this sensitivity study, both with and without German\nbacktranslation, in Figure 3. Given the overall consistency, we observe that our method is stable\nover a wide range of settings.\nMixup RatioWe also investigate the effect of the mixup ratio: the number of mixup samples gen-\nerated for each sample in a minibatch. We run a smaller sweep of αSM and αTMKD over the range\n{0.5,1.0,2.0}for mixup ratios of 2 and 3 for a BERT3 student SST-2, with and without German\nbacktranslation, in Figure 3. We conclude that the mixup ratio does not have a strong effect on over-\nall performance. Given that higher mixup ratio requires more computation (due to more samples\nover which to compute the forward and backward pass), we ﬁnd a mixup ratio of 1 to be enough.\n8\nPublished as a conference paper at ICLR 2021\nFigure 3: Hyperparameter sensitivity analysis regarding theMixKD approach, with different choices\nof αTMKD,αSM and the ratio of mixup samples (w.r.t.the original training data).\nMethods MNLI SST-2\nBERT6 81.3 90.9\nBERT6 + TinyBERT DA module81.5 91.3\nBERT6 +MixKD 82.5 92.1\nTable 4: We compare our approach with the data\naugmentation module proposed by TinyBert (Jiao\net al., 2019).\nComparing with TinyBERT’s DA module\nTinyBERT (Jiao et al., 2019) also utilizes\ndata augmentation for knowledge distillation.\nSpeciﬁcally, they adopt a conditional BERT\ncontextual augmentation (Wu et al., 2019) strat-\negy. To further verify the effectiveness of our\napproach, we use TinyBERT’s released code-\nbase5 to generate augmented samples and make\na direct comparison with MixKD. As shown in Table 4, our approach exhibits much stronger results\nfor distilling a 6-layer BERT model (on both MNLI and SST-2 datasets). Notably, TinyBERT’s data\naugmentation module is much less efﬁcient than mixup’s simple operation, generating 20 times the\noriginal data as augmented samples, thus leading to massive computation overhead.\n5 C ONCLUSIONS\nWe introduce MixKD, a method that uses data augmentation to signiﬁcantly increase the value of\nknowledge distillation for compressing large-scale language models. Intuitively, MixKD allows the\nstudent model additional queries to the teacher model, granting it more opportunities to absorb the\nlatter’s richer representations. We analyze MixKD from a theoretical standpoint, proving that our\napproach results in a smaller gap between generalization error and empirical error, as well as better\ngeneralization, under appropriate conditions. Our approach’s success on a variety of GLUE tasks\ndemonstrates its broad applicability, with a thorough set of experiments for validation. We also\nbelieve that the MixKD framework can further reduce the gap between student and teacher models\nwith the incorporation of more recent mixup and knowledge distillation techniques (Lee et al., 2020;\nWang et al., 2020; Mirzadeh et al., 2019), and we leave this to future work.\nACKNOWLEDGMENTS\nCC is partly supported by the Verizon Media FREP program.\nREFERENCES\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao\nPiao, Jianfeng Gao, Ming Zhou, et al. UniLMv2: Pseudo-masked Language Models for Uniﬁed\nLanguage Model Pre-training. arXiv preprint arXiv:2002.12804, 2020.\nJ. Baxter. A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research, 2000.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The Fifth PASCAL Recognizing\nTextual Entailment Challenge. TAC, 2009.\nDavid Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and\nColin Raffel. ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Aug-\nmentation Anchoring. International Conference on Learning Representations, 2019a.\n5https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/\nTinyBERT\n9\nPublished as a conference paper at ICLR 2021\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A\nRaffel. MixMatch: A Holistic Approach to Semi-Supervised Learning. Neural Information\nProcessing Systems, 2019b.\nJiaao Chen, Zichao Yang, and Diyi Yang. MixText: Linguistically-Informed Interpolation of Hidden\nSpace for Semi-Supervised Text Classiﬁcation. Association for Computational Linguistics, July\n2020.\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V Le.\nBAM! Born-again Multi-task Networks for Natural Language Understanding. arXiv preprint\narXiv:1907.04829, 2019.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training\nText Encoders as Discriminators Rather than Generators.arXiv preprint arXiv:2003.10555, 2020.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL Recognising Textual Entailment\nChallenge. Machine Learning Challenges Workshop, 2005.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding.arXiv preprint arXiv:1810.04805, 2018.\nWilliam B Dolan and Chris Brockett. Automatically Constructing a Corpus of Sentential Para-\nphrases. International Workshop on Paraphrasing, 2005.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The Third PASCAL Recog-\nnizing Textual Entailment Challenge. ACL-PASCAL Workshop on Textual Entailment and Para-\nphrasing, 2007.\nHongyu Guo, Yongyi Mao, and Richong Zhang. Augmenting Data with Mixup for Sentence Clas-\nsiﬁcation: An Empirical Study. arXiv preprint arXiv:1905.08941, 2019.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. The Second PASCAL Recognising Textual Entailment Challenge.PASCAL Challenges\nWorkshop on Recognising Textual Entailment, 2006.\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. AugMix: A Simple Method to Improve Robustness and Uncertainty under Data Shift.\nInternational Conference on Learning Representations, 2020.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network.arXiv\npreprint arXiv:1503.02531, 2015.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. TinyBERT: Distilling BERT for Natural Language Understanding. arXiv preprint\narXiv:1909.10351, 2019.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Span-\nBERT: Improving Pre-training by Representing and Predicting Spans. Association for Computa-\ntional Linguistics, 2020.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classiﬁcation with Deep Con-\nvolutional Neural Networks. Neural Information Processing Systems, 2012.\nAshutosh Kumar, Satwik Bhattamishra, Manik Bhandari, and Partha Talukdar. Submodular\nOptimization-based Diverse Paraphrasing and Its Effectiveness in Data Augmentation. North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, 2019.\nSaehyung Lee, Hyungyu Lee, and Sungroh Yoon. Adversarial Vertex Mixup: Toward Better Adver-\nsarially Robust Generalization. Computer Vision and Pattern Recognition, 2020.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising Sequence-to-sequence Pre-\ntraining for Natural Language Generation, Translation, and Comprehension. arXiv preprint\narXiv:1910.13461, 2019.\n10\nPublished as a conference paper at ICLR 2021\nLinqing Liu, Huan Wang, Jimmy Lin, Richard Socher, and Caiming Xiong. Attentive Student Meets\nMulti-task Teacher: Improved Knowledge Distillation for Pretrained Models. arXiv preprint\narXiv:1911.03588, 2019a.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task Deep Neural Networks\nfor Natural Language Understanding. arXiv preprint arXiv:1901.11504, 2019b.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pre-\ntraining Approach. arXiv preprint arXiv:1907.11692, 2019c.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. Journal of Machine\nLearning Research, 2008.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Has-\nsan Ghasemzadeh. Improved Knowledge Distillation via Teacher Assistant. arXiv preprint\narXiv:1902.03393, 2019.\nMehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.\n2018.\nSubhabrata Mukherjee and Ahmed Hassan Awadallah. Distilling Transformers into Simple Neural\nNetworks with Unlabeled Transfer Data. arXiv preprint arXiv:1910.01769, 2019.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies:\nDemonstrations, 2019.\nYanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Jiawei Han, and Weizhu Chen. CoDA:\nContrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Under-\nstanding. arXiv preprint arXiv:2010.08670, 2020.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. Empirical Methods in Natural Language Processing, 2016.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, A Distilled Version\nof BERT: Smaller, Faster, Cheaper and Lighter. arXiv preprint arXiv:1910.01108, 2019.\nDinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. A Simple but Tough-to-\nBeat Data Augmentation Approach for Natural Language Understanding and Generation. arXiv\npreprint arXiv:2009.13818, 2020.\nPatrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation Invariance\nin Pattern Recognition–Tangent Distance and Tangent Propagation. Neural Networks: Tricks of\nthe Trade, 1998.\nPY Simard, D Steinkraus, and JC Platt. Best Practices for Convolutional Neural Networks Applied\nto Visual Document Analysis. International Conference on Document Analysis and Recognition,\n2003.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive Deep Models for Semantic Compositionality over a Sentiment\nTreebank. Empirical Methods in Natural Language Processing, 2013.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient Knowledge Distillation for BERT Model\nCompression. arXiv preprint arXiv:1908.09355, 2019a.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\nHao Tian, and Hua Wu. ERNIE: Enhanced Representation through Knowledge Integration.arXiv\npreprint arXiv:1904.09223, 2019b.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mo-\nbileBERT: A Compact Task-agnostic BERT for Resource-limited Devices. arXiv preprint\narXiv:2004.02984, 2020.\n11\nPublished as a conference paper at ICLR 2021\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling Task-\nspeciﬁc Knowledge from BERT into Simple Neural Networks. arXiv preprint arXiv:1903.12136,\n2019.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read Students Learn Better:\nOn the Importance of Pre-training Compact Models. arXiv preprint arXiv:1908.08962, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is All You Need. Neural Information Process-\ning Systems, 2017.\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-\nPaz, and Yoshua Bengio. Manifold Mixup: Better Representations by Interpolating Hidden States.\nInternational Conference on Machine Learning, 2019a.\nVikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation Con-\nsistency Training for Semi-supervised Learning. International Joint Conference on Artiﬁcial In-\ntelligence, 2019b.\nVikas Verma, Meng Qu, Alex Lamb, Yoshua Bengio, Juho Kannala, and Jian Tang. GraphMix:\nImproved Training of GNNs for Semi-Supervised Learning. arXiv preprint arXiv:1909.11715,\n2019c.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGLUE: A Multi-task Benchmark and Analysis Platform for Natural Language Understanding.\nInternational Conference on Learning Representations, 2019.\nDongdong Wang, Yandong Li, Liqiang Wang, and Boqing Gong. Neural Networks Are More Pro-\nductive Teachers Than Human Raters: Active Mixup for Data-Efﬁcient Knowledge Distillation\nfrom a Blackbox Model. Computer Vision and Pattern Recognition, 2020.\nJason Wei and Kai Zou. EDA: Easy Data Augmentation Techniques for Boosting Performance on\nText Classiﬁcation Tasks. arXiv preprint arXiv:1901.11196, 2019.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. A Broad-coverage Challenge Corpus\nfor Sentence Understanding through Inference. North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 2018.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. Conditional BERT Contex-\ntual Augmentation. International Conference on Computational Science, 2019.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised Data\nAugmentation for Consistency Training. arXiv preprint arXiv:1904.12848, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXLNet: Generalized Autoregressive Pretraining for Language Understanding. Neural Informa-\ntion Processing Systems, 2019.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\nand Quoc V Le. QANet: Combining Local Convolution with Global Self-attention for Reading\nComprehension. arXiv preprint arXiv:1804.09541, 2018.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutMix: Regularization Strategy to Train Strong Classiﬁers with Localizable Features. Interna-\ntional Conference on Computer Vision, 2019.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond Empiri-\ncal Risk Minimization. International Conference on Learning Representations, 2018.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou. Extreme Language Model Compres-\nsion with Optimal Subwords and Shared Projections. arXiv preprint arXiv:1909.11687, 2019.\n12\nPublished as a conference paper at ICLR 2021\nA P ROOFS\nProof [Proof of Theorem 1] First of all, {xi}a\ni=1 ∪{x′\ni}b\ni=1 can be regarded as drawn from distribu-\ntion r(x) =ap(x) +bq(x)\na+ b .\nGiven Gis ﬁnite, we have the following theorem\nTheorem 5 (Mohri et al., 2018) Let lbe a bounded loss function, hypothesis set Gis ﬁnite. Then\nfor any δ >0, with probability at least 1 −δ, the following inequality holds for all g∈G:\nR(f,g,p ) −Remp(f,g, {xi}n\ni=1) ≤M\n√\nlog(|G|/δ)\n2n\nThus we have in our case:\nR(f,gp,p) −Remp(f,gp,{xi}n\ni=1) ≤ϵp ≤M\n√\nlog(|G|/δ)\n2n\nand\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1)\n=R(f,g∗,r) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) +\n∫\nl(f(x),g∗(x))(p(x) −r(x))dx\n=R(f,g∗,r) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) + b\na+ b\n∫\nl(f(x),g∗(x))(p(x) −q(x))dx\n≤R(f,g∗,r) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) +\n∫\nl(f(x),g∗(x))(p(x) −q(x))dx\n≤M\n√\nlog(|G|/δ)\n2(a+ b) + △ (10)\nwhere △=\n∫\nl(f(x),g∗(x))(p(x) −q(x))dx. If\nb≥M2 log(|G|/δ)\n2(ϵp −△)2 −a\nthen\n2(a+ b) ≥M2 log(|G|/δ)\n(ϵp −△)2\n(ϵp −△)2 ≥M2 log(|G|/δ)\n2(a+ b)\nϵp ≥M\n√\nlog(|G|/δ)\n2(a+ b) + △\nSubstitute into equation 10, we have\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵp\nRecall the deﬁnition of ϵ∗, which is the minimum value of all possible ϵsatisfying\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵ\nwe know that ϵ∗≤ϵp. Let c= 2(ϵp −△)2, we can conclude the theorem.\nProof [Proof of Theorem 2] First of all, {xi}a\ni=1 ∪{x′\ni}b\ni=1 can be regarded as drawn from distribu-\ntion r(x) =ap(x) +bq(x)\na+ b .\n13\nPublished as a conference paper at ICLR 2021\nTheorem 6 (Mohri et al., 2018) Let l be a non-negative loss function upper bounded by M >0,\nand for any ﬁxed y, l(y,y′) is L-Lipschitz for some L> 0, then with probability at least 1 −δ,\nR(f,g,p ) −Remp(f,g, {xi}n\ni=1) ≤2LRp(G) +M\n√\nlog(1/δ)\n2n\nThus we have\nR(f,g,p ) −Remp(f,g, {xi}n\ni=1) ≤ϵp ≤2LRp(G) +M\n√\nlog(1/δ)\n2n\nwhere Rp(G) are Rademacher complexity over all samples of size nsamples from p(x).\nWe also have\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1)\n=R(f,g∗,r) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) +\n∫\nl(f(x),g∗(x))(p(x) −r(x))dx\n=R(f,g∗,r) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) + b\na+ b\n∫\nl(f(x),g∗(x))(p(x) −q(x))dx\n≤R(f,g∗,r) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) +\n∫\nl(f(x),g∗(x))(p(x) −q(x))dx\n≤2LRr(G) +M\n√\nlog(1/δ)\n2(a+ b) + △ (11)\nwhere △=\n∫\nl(f(x),g∗(x))(p(x)−q(x))dx. Rr(G) are Rademacher complexity over all samples\nof size (a+ b) samples from r(x) =ap(x) +bq(x)\na+ b .\nIf\nb≥ M2 log(1/δ)\n2(ϵp −△− 2LRr(G))2 −a\nthen:\n2(a+ b) ≥ M2 log(1/δ)\n(ϵp −△− 2LRr(G))2\nϵp −△− 2LRr(G) ≥M\n√\nlog(1/δ)\n2(a+ b)\nϵp ≥M\n√\nlog(1/δ)\n2(a+ b) + △+ 2LRr(G)\nSubstitute into equation 11, we have:\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵp\nRecall the deﬁnition of ϵ∗, which is the minimum value of all possible ϵsatisfying\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵ\nwe know that ϵ∗≤ϵp. Let c= 2(ϵp −△− 2LRr(G))2, we can conclude the theorem.\nProof [Proof of Theorem 3] Similar to previous theorems, we write\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1)\n=R(f,g∗,ap+ bq\na+ b ) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) +\n∫\nl(f(x),g∗(x))(p(x) −ap(x) +bq(x)\na+ b )dx\n=R(f,g∗,ap+ bq\na+ b ) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) + b\na+ b\n∫\nl(f(x),g∗(x))(p(x) −q(x))dx\n≤R(f,g∗,ap+ bq\na+ b ) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) +△ (12)\n14\nPublished as a conference paper at ICLR 2021\nwhere △ =\n∫\nl(f(x),g∗(x))(p(x) − q(x))dx. For notation consistency, we write\nR(f,g∗,ap+ bq\na+ b ) =\n∫\nl(f(x) −g(x))ap(x) +bq(x)\na+ b dx. However, {xi}a\ni=1 ∪{x′\ni}b\ni=1 are not\ndrawn from the same distribution (which is r(x) =ap(x) +bq(x)\na+ b in previous cases).\nLet γ = ⌊a+ b\na ⌋, we split {xi}a\ni=1 ∪{x′\ni}b\ni=1 into γ parts that don’t overlap with each other. The\nﬁrst part is {xi}a\ni=1, all the other parts has at least aelements from {x′\ni}b\ni=1.\nLet\nλ=\n√\n64\nb log(4/δ) +64\na log C(G)\nwhere C(G) is space capacity deﬁned in Deﬁnition 4 in Baxter (2000), which depends on ϵ∗and G.\nBy Theorem 4 in Baxter (2000),\n[\nR(f,g∗,ap+ bq\na+ b ) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1)\n]2\n≤max{64\nγa log(4C(Gγ)\nδ ),16\na }\nBy Theorem 5 in Baxter (2000),\n64\nγa log(4C(Gγ)\nδ ) = 64\nγa(log(4\nδ) + log(C(Gγ))) ≤64\nγa(log(4\nδ) +γlog(C(G))) ≤λ2\nThe last inequality comes from b≤γa, which is because of γ = ⌊a+ b\na ⌋. Then we have\n[\nR(f,g∗,ap+ bq\na+ b ) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1)\n]2\n≤max{64\nγa log(4C(Gγ)\nδ ),16\na }≤ max{λ2,16\na }\nIf\nb≥ 64 log(4/δ)\n(ϵp −△)2 −64 logC(G)/a\nThen\nλ2 ≤64\na log C(G) + 64 log(4\nδ)(ϵp −△)2 −64 logC(G)/a\n64 log(4/δ)\nλ2 ≤(ϵp −△)2 (13)\nIf 16\n(ϵp −△)2 ≤a\nthen\n16\na ≤(ϵp −△)2 (14)\nCombine equation 13 and equation 14, we have\nR(f,g∗,ap+ bq\na+ b ) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵp −△\nSubstitute into equation 12, we have:\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵp\nRecall the deﬁnition of ϵ∗, which is the minimum value of all possible ϵsatisfying\nR(f,g∗,p) −Remp(f,g∗,{xi}a\ni=1 ∪{x′\ni}b\ni=1) ≤ϵ\nwe know that ϵ∗≤ϵp.\n15\nPublished as a conference paper at ICLR 2021\nB V ARIANCE ANALYSIS\nFor the purpose of getting a sense of variance, we run experiments with additional random seeds on\nMRPC and RTE, which are relatively smaller datasets, and MNLI and QNLI, which are relatively\nlarger datasets. Mean and standard deviation on the dev set of these GLUE datasets are reported in\nTable 5. We observe the variance of the same model’s performance to be small, especially on the\nrelatively larger datasets.\nModel MRPC MNLI-m QNLI RTE\nBERT6-TMKD+BT 89.79±0.27/85.04±0.48 82.05 ±0.11 88.42 ±0.06 69.37±0.50\nBERT6-SM+TMKD+BT 89.64±0.38/84.43±0.36 82.41±0.12 88.76±0.15 68.02 ±0.11\nBERT3-TMKD+BT 84.79±0.33/75.82±0.48 77.16 ±0.03 84.60 ±0.07 62.47±0.36\nBERT3-SM+TMKD+BT 84.53±0.39/75.85±0.60 77.42±0.11 84.88±0.06 60.83 ±0.18\nTable 5: Mean and variance reported for BERT6-TMKD+BT,BERT6-SM+TMKD+BT,BERT3-\nTMKD+BT and BERT3-SM+TMKD+BT.\n16",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8008013963699341
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6999155879020691
    },
    {
      "name": "Generalization",
      "score": 0.6466373801231384
    },
    {
      "name": "Inference",
      "score": 0.6355224847793579
    },
    {
      "name": "Task (project management)",
      "score": 0.5665169954299927
    },
    {
      "name": "Machine learning",
      "score": 0.5422220826148987
    },
    {
      "name": "Interpolation (computer graphics)",
      "score": 0.4972849190235138
    },
    {
      "name": "Distillation",
      "score": 0.4850734770298004
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4828782379627228
    },
    {
      "name": "Scale (ratio)",
      "score": 0.47532135248184204
    },
    {
      "name": "Artificial neural network",
      "score": 0.4430854618549347
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.4318923354148865
    },
    {
      "name": "Computation",
      "score": 0.42086759209632874
    },
    {
      "name": "Memorization",
      "score": 0.4181325137615204
    },
    {
      "name": "Algorithm",
      "score": 0.24921965599060059
    },
    {
      "name": "Mathematics",
      "score": 0.09685677289962769
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Motion (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I63190737",
      "name": "University at Buffalo, State University of New York",
      "country": "US"
    }
  ],
  "cited_by": 30
}