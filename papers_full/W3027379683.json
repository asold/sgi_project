{
    "title": "Analyzing Information Leakage of Updates to Natural Language Models",
    "url": "https://openalex.org/W3027379683",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2117185244",
            "name": "Santiago Zanella B√©guelin",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2318088954",
            "name": "Lukas Wutschitz",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2232393541",
            "name": "Shruti Tople",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2256286525",
            "name": "Victor R√ºhle",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2270255228",
            "name": "Andrew Paverd",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A131194035",
            "name": "OLGA OHRIMENKO",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": "https://openalex.org/A2137841054",
            "name": "Boris K√∂pf",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2027911405",
            "name": "Marc Brockschmidt",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6657138077",
        "https://openalex.org/W2051267297",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1493526108",
        "https://openalex.org/W2343954916",
        "https://openalex.org/W2952604841",
        "https://openalex.org/W2795435272",
        "https://openalex.org/W3035556513",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3035644192",
        "https://openalex.org/W2951368041",
        "https://openalex.org/W4310895557",
        "https://openalex.org/W2951152347",
        "https://openalex.org/W2963378725",
        "https://openalex.org/W2033165262",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2784621220",
        "https://openalex.org/W2788277448",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2971124187",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W3214586949",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W1992926795",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W1502953220",
        "https://openalex.org/W3048684575",
        "https://openalex.org/W1985511977",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W1473189865",
        "https://openalex.org/W2946930197"
    ],
    "abstract": "To continuously improve quality and reflect changes in data, machine learning\\napplications have to regularly retrain and update their core models. We show\\nthat a differential analysis of language model snapshots before and after an\\nupdate can reveal a surprising amount of detailed information about changes in\\nthe training data. We propose two new metrics---\\\\emph{differential score} and\\n\\\\emph{differential rank}---for analyzing the leakage due to updates of natural\\nlanguage models. We perform leakage analysis using these metrics across models\\ntrained on several different datasets using different methods and\\nconfigurations. We discuss the privacy implications of our findings, propose\\nmitigation strategies and evaluate their effect.\\n",
    "full_text": "Analyzing Information Leakage of Updates\nto Natural Language Models‚àó\nSantiago Zanella-B√©guelin\nsantiago@microsoft.com\nMicrosoft\nLukas Wutschitz\nluwutsch@microsoft.com\nMicrosoft\nShruti Tople\nshruti.tople@microsoft.com\nMicrosoft\nVictor R√ºhle\nvirueh@microsoft.com\nMicrosoft\nAndrew Paverd\nandrew.paverd@microsoft.com\nMicrosoft\nOlga Ohrimenko‚Ä†\noohrimenko@unimelb.edu.au\nUniversity of Melbourne\nBoris K√∂pf\nboris.koepf@microsoft.com\nMicrosoft\nMarc Brockschmidt\nmabrocks@microsoft.com\nMicrosoft\nABSTRACT\nTo continuously improve quality and reflect changes in data, ma-\nchine learning applications have to regularly retrain and update\ntheir core models. We show that a differential analysis of language\nmodel snapshots before and after an update can reveal a surpris-\ning amount of detailed information about changes in the training\ndata. We propose two new metrics‚Äîdifferential score and differential\nrank‚Äîfor analyzing the leakage due to updates of natural language\nmodels. We perform leakage analysis using these metrics across\nmodels trained on several different datasets using different meth-\nods and configurations. We discuss the privacy implications of our\nfindings, propose mitigation strategies and evaluate their effect.\nACM Reference format:\nSantiago Zanella-B√©guelin, Lukas Wutschitz, Shruti Tople, Victor R√ºhle,\nAndrew Paverd, Olga Ohrimenko, Boris K√∂pf, and Marc Brockschmidt. 2021.\nAnalyzing Information Leakage of Updates to Natural Language Models. In\nProceedings of 2020 ACM SIGSAC Conference on Computer and Communica-\ntions Security, Virtual Event, USA, November 9‚Äì13, 2020 (CCS ‚Äô20), 13 pages.\nhttps://doi.org/10.1145/3372297.3417880\n1 INTRODUCTION\nOver the last few years, deep learning has made sufficient progress\nto be integrated into intelligent, user-facing systems, which means\nthat machine learning models are now part of the software develop-\nment lifecycle. As part of this cycle, models are regularly updated\nto accommodate three different scenarios:\n‚Ä¢ data update , to improve performance when new and more\ndata becomes available;\n‚Ä¢ data specialization, to fine-tune a model on a specific dataset,\nor to handle distributional shift as usage patterns change;\nor\n‚Ä¢ data deletion, to respect requests for removal of users‚Äô data.\n‚àó¬©2020 Copyright held by the author(s). This is the author‚Äôs version of the work. It\nis posted here for your personal use. Not for redistribution. The definitive version\nwas published in CCS ‚Äô20: Proceedings of the 2020 ACM SIGSAC Conference on\nComputer and Communications Security October 2020, pp 363‚Äì375, https://doi.org/10.\n1145/3372297.3417880.\n‚Ä†Work done in part while at Microsoft.\nMotivated by these scenarios, we study privacy implications for\ntext data that is added (or removed) during retraining of genera-\ntive natural language models (LMs). Specifically, we consider an\nadversary with access to multiple snapshots of a model and wishes\nto learn information about differences in the data used to train\nthem. This threat model is motivated by the combination of three\nfactors: (1) the current trend to fine-tune pretrained public high-\ncapacity LMs to smaller private datasets; (2) the established ability\nof such LMs to memorize out-of-distribution training samples [6];\nand (3) the widespread deployment of LMs to end-user systems\n(e.g., predictive keyboards on smartphones), allowing adversaries\nto analyze them in detail. For the informed reader, we discuss the\nrelationship between this threat model and other attacks against\nprivacy and defenses like differential privacy later on in Section 2.2.\nWe show that data that is added or removed between model\nupdates can be extracted in this threat model, having severe impli-\ncations for deploying machine learning models trained on private\ndata. Some of the implications are counter-intuitive: for example,\nhonoring a request to remove a user‚Äôs data (as per GDPR) from the\ntraining corpus can mean that their data becomes exposed by re-\nleasing an updated model trained without it. Similarly, fine-tuning\na public snapshot of a high-capacity model (e.g., BERT [9] or GPT-\n2 [23]) with data from a single organization exposes this additional\ndata to anyone with access to both the fine-tuned model and the\noriginal public model (e.g., employees of this organization).\nIn order to extract information about the difference in the data\nused to train two language models, we develop a novel notion of\ndifferential score. The differential score of a token sequence captures\nthe difference between the probabilities assigned to it by the two\nmodels. The intuition is that sequences with higher differential\nscores are likely to have been added during model updates. We\ndevise an algorithm based on beam search to efficiently identify\nsuch token sequences, even if the individual models assign low\nprobability to them. This allows us to recover information about\nthe difference between the datasets used for training without any\nbackground knowledge of their contents or distribution.\nWhen given some background knowledge, the advantage of\nhaving access to two model snapshots becomes crisper. For ex-\nample, we train a recurrent neural network (RNN) on 20M to-\nkens of general Reddit comments, and update it by retraining it\narXiv:1912.07942v4  [cs.LG]  5 Aug 2021\non these comments plus 25K tokens from 940 messages of the\ntalk.politics.mideast newsgroup. When prompted with the\nword ‚ÄúTurkey‚Äù, our algorithm produces ‚ÄúTurkey searched an Amer-\nican plane‚Äù as the 2nd most likely result, although this phrase occurs\nonly 6 times in newsgroup messages and none in Reddit comments\n(i.e., < 0.000002% of the training data). An equivalent search using\nonly the updated network does not produce this sentence among\nthe top 10,000 results; it would take the longer prompt ‚ÄúTurkey\nsearched an‚Äù for this phrase to surface to the top 100 results.\nWe use differential score to experimentally study the effect of\nupdates in the three scenarios mentioned above. As a proxy for the\nupdate dataset, we use synthetically generated sentences (or ca-\nnaries) and real-world sentences from newsgroup messages. Using\nboth canaries and real-world data, we analyze the effect on attacks\nrecovering information from the update dataset of (1) different train-\ning types for updates, ranging from retraining a model from scratch\nwith an updated dataset to fine-tuning as is common for modern\nhigh-capacity language models; (2) the proportion of private and\npublic data used for the update; and (3) an adversary‚Äôs background\nknowledge. For robustness, we consider datasets of different sizes\non both RNNs as well as modern transformer architectures.\nSummary of Contributions. We present the first systematic study\nof the privacy implications of releasing snapshots of language mod-\nels trained on overlapping data. Our results validate that model\nupdates pose a substantial risk to content added to or removed from\ntraining data in terms of information leakage. Our key findings are:\n‚Ä¢By comparing two models, an adversary can extract specific\nsentences or fragments of discourse from the difference between\nthe data used to train them. This does not require any information\nabout the training data or the model architecture and is possible\neven when the change to the data is as small as 0.0001% of the orig-\ninal dataset. Smaller changes become exposed when given partial\nknowledge about the data.\n‚Ä¢We show that analyzing two model snapshots reveals sub-\nstantially more about the data that was added or removed than\nconsidering only a single snapshot at a time, as in [6].\n‚Ä¢Adding or removing additional non-sensitive training data\nbetween model updates is not a reliable mitigation.\n‚Ä¢Training with differential privacy mitigates the attack, but\nincurs substantial computational cost and reduces the utility of the\ntrained models.\n‚Ä¢Restricting access to the model and only outputting a subset\nof prediction results is a promising mitigation as it reduces the\neffectiveness of our attack without reducing utility of the model.\nThese findings apply to models fine-tuned on a smaller dataset, as\nwell as models retrained on the union of original and new data.\nStructure of the Paper. We provide background on language mod-\nels and describe our adversary model and attack scenarios in the\nnext section. We define the notion of differential score and describe\nhow to efficiently approximate it in Section 3. In Section 4 we de-\nscribe our experiments to analyze the effect of different factors on\nleakage. In Section 5 we investigate the source of leakage in model\nupdates, e.g., by comparing with leakage from access to only a\nsingle model. Finally, we consider mitigation strategies in Section 6,\nbefore describing related work and concluding.\n2 PRELIMINARIES\n2.1 Generative Language Models\nWe consider machine learning models capable of generating natural\nlanguage. These models are used in a variety of applications, includ-\ning automatic caption generation, language translation, and next-\nword prediction. Generative language models usually operate on a\nfixed set of known tokensùëá (often referred to as the model‚Äôsvocabu-\nlary) and are autoregressive, modeling the probability ùëù(ùë°1 ...ùë° ùëõ)of\na sequence of tokens ùë°1 ...ùë° ùëõ ‚ààùëáùëõ as the product of the per-token\nprobabilities conditional on their prefix ùëù(ùë°ùëñ |ùë°1 ...ùë° ùëñ‚àí1), i.e.,\nùëù(ùë°1 ...ùë° ùëõ)=\n√ñ\n1‚â§ùëñ‚â§ùëõ\nùëù(ùë°ùëñ |ùë°1 ...ùë° ùëñ‚àí1).\nTraining an autoregressive generative language model ùëÄ requires\nlearning a function (which we also refer to as ùëÄ) that maps token\nsequences of arbitrary length to a probability distribution over the\nvocabulary ùëá, modeling the likelihood of each token to appear next.\nWe use ùëÄ(ùë°<ùëñ)to denote the probability distribution over tokens\ncomputed by model ùëÄ after reading the sequence ùë°1 ...ùë° ùëñ‚àí1 ‚ààùëá‚àó,\nand ùëÄ(ùë°<ùëñ)(ùë°ùëñ)to denote the probability of a specific token ùë°ùëñ.\nGiven such a model ùëÄ, a simple predictive screen keyboard can\nbe implemented by feeding ùëÄthe words typed so far (e.g., from the\nstart of the current sentence) and displaying the, say, three most\nlikely tokens as one-tap options to the user.\nA variety of different architectures exist for the generation of\nnatural language using machine learning models. The most promi-\nnent are Recurrent Neural Networks (RNNs) using Long Short-Term\nMemory [17] cells (or variants thereof) and the more recent Trans-\nformers [23, 30]. These architectures differ substantially in how\nthey implement the modeling of the per-token probability distribu-\ntion, but as our experiments show, they behave nearly identically\nfor the purposes of our analysis.\nGiven a model architecture, a dataset ùê∑ ‚äÜùëá‚àóis required as\ntraining data to obtain a concrete model. We write ùëÄùê∑ to em-\nphasize that a model was trained on a dataset ùê∑. Throughout the\npaper, we use the standard measure ofperplexity perpùëÄ(ùë°1 ...ùë° ùëõ)=\nùëùùëÄ(ùë°1 ...ùë° ùëõ)\n‚àí1\nùëõ of a model ùëÄ on test data ùë°1 ...ùë° ùëõ, using the prob-\nability ùëùùëÄ(ùë°1 ...ùë° ùëõ)assigned to the sequence by model ùëÄ. Unlike\nthe more familiar accuracy, which only captures the correctness of\nthe most probable choice, this metric captures models being ‚Äúalmost\nright. ‚Äù Intuitively, perplexity can be thought as how ‚Äúsurprised‚Äù a\nmodel is by a next-word choice, and hence, lower perplexity values\nindicate a better match between data and model.\n2.2 Adversary Model and Goals\nLanguage models are regularly updated for a variety of reasons,\neither by adding and/or removing data from the training set. We use\nthe term model update to refer to any update in the parameters of\nthe model caused by training on different data. This is distinct from\nan update to the model architecture, which changes the number or\nuse of parameters. Each update creates a new version of the model,\nwhich we refer to as a snapshot.\nWe consider an adversary that has concurrent query access\nto two snapshots, ùëÄùê∑ and ùëÄùê∑‚Ä≤, of a language model trained on\ndatasets ùê∑ and ùê∑‚Ä≤respectively, where ùê∑ ‚ää ùê∑‚Ä≤. We write ùëÄ, ùëÄ‚Ä≤as\nshorthand for ùëÄùê∑, ùëÄùê∑‚Ä≤. The adversary can query the snapshots\nwith any sequence ùë† ‚ààùëá‚àóand observe the corresponding probabil-\nity distributions ùëÄ(ùë†)and ùëÄ‚Ä≤(ùë†). The adversary‚Äôs goal is to infer\ninformation about training data points in ùê∑‚Ä≤\\ùê∑, the difference\nbetween ùê∑ and ùê∑‚Ä≤. In the best case, an adversary would recover\nexact training points. We refer to an adversary who has access to\ntwo snapshots of the model as a snapshot attacker .\nRelationship to other attacks on training data. Snapshot attacks\nare reconstruction attacks [24] against the updated model, as the\ngoal is to recover data points in the dataset used for the update,\ngiven the original model as auxiliary information.\nThe goal of membership inference attacks [25, 26] is weaker in\nthat they only aim to determine whether a given point was present\nin the dataset used to train a model. However, the differential score\nof a phrase (which we use for reconstruction) can also serve as a\nsignal for inferring membership in the update dataset. We leave an\nevaluation of this approach to future work.\nFinally, model inversion attacks [12, 13] repurpose a model to\nwork backwards, inferring unknown attributes of individuals given\nknown attributes and a target prediction. Individuals need not\nbe present in the training data, and results are aggregate statistics\nrather than information about specific training points. See Section 7\nfor a more in-depth discussion of related attacks.\nRelationship to differential privacy. Differential privacy [11] guar-\nantees that a model does not leak significant information about any\nspecific training point. A differentially private model also guaran-\ntees group privacy, with a bound on the contribution of a group\nof training points that degrades linearly with the group size. A\ndifferentially private model that provides meaningful protection\nfor a group of |ùê∑‚Ä≤\\ùê∑|training points would hence protect against\nsnapshot attacks on ùëÄùê∑,ùëÄùê∑‚Ä≤. However, this also implies that ùëÄùê∑‚Ä≤\ncannot be significantly more useful (e.g. more accurate) than ùëÄùê∑.\nOur experiments in Section 6 confirm this intuition, and show that\na large privacy budget is needed for the updated model to gain in\nutility, so that in practice differential privacy provides an empirical\nmitigation rather than a strong formal guarantee.\n2.3 Analysis Scenarios\nTo guide our analysis, we focus on three concrete scenarios in which\nan adversary can gain concurrent access to two (or more) snapshots\nof a language model.\nData Updates. Many applications require language models that\nreflect recent patterns in language use. For example, a predictive\nkeyboard on a mobile device requires regular updates to suggest\nterms that have become more common recently (e.g., following\nnews trends or internet memes). To achieve this, vendors often\nregularly retrain an (otherwise unchanged) model on an updated\ndataset, for example by simply adding more recent data to the\ntraining dataset. In such cases, an adversary can easily gain access\nto two snapshots ùëÄùê∑ and ùëÄùê∑‚Ä≤ with ùê∑ ‚ää ùê∑‚Ä≤and may be interested\nin learning details about the update ùê∑‚Ä≤\\ùê∑. We show that we can\nextract entire sentences from this difference by comparing ùëÄùê∑\nand ùëÄùê∑‚Ä≤, revealing not only aggregate user behavior, but specific\nconversations.\nData Specialization. Some applications with little task-specific\ndata build on top of generic, pretrained high-capacity language\nmodels such as GPT-2 [23]. In such settings, training starts from\nthe pretrained model, but then uses a significantly smaller pri-\nvate dataset. As an example, an organization could simply use a\npublicly available off-the-shelf language model to create an email\nauthoring autocompletion system. However, by additionally train-\ning the model with some historical email data, it can be adapted to\norganization-specific terms, acronyms and concepts. In such a sce-\nnario, if an adversary can gain access to the specialized model ùëÄ‚Ä≤,\nthey can easily also obtain the (publicly available) model ùëÄ used\nas a basis. We show that by treating these as different snapshots\nof the same model, the adversary can extract parts of the private\ndataset used for specialization.\nUser Data Deletion. Art. 17 of GDPR [29] Right to erasure (‚Äúright\nto be forgotten‚Äù) gives data owners the right to request erasure of\ntheir personal data from a party who has collected and processed\nit. Language models trained on emails, text messages, or other\nuser-generated content may contain personal information that a\nuser can request to delete. The data collector would be required to\ndelete the user‚Äôs data and retrain any models in which it had been\nused. In many cases, these models may have already been released\neither to the public or to other users via services provided by the\ndata collector (e.g., text prediction and auto-correct services in text\neditors and mobile keyboards).\nThis scenario falls into our adversary setting, albeit in reverse\nchronological order. Here the dataset ùê∑‚Ä≤contains the data that will\nbe deleted, whilstùê∑does not (i.e., the differenceùê∑‚Ä≤\\ùê∑represents the\nuser‚Äôs data). With access to ùëÄùê∑ and ùëÄùê∑‚Ä≤, the attacker can attempt\nto infer the user‚Äôs data. Even if the retrained model overwrites the\nold model, it may not be possible to erase all instances of the old\nmodel simultaneously. For example, some users may be slow to\ndownload the new version or the old model may have been copied\nby other parties.\nNaturally, this scenario can be extended to other settings where\ndata is deleted between model updates. This scenario raises an\ninteresting question on whether deletion of data is in the user‚Äôs\nbest interest or if it makes their data more susceptible to leakage.\n3 NEW METRICS\nWe introduce two metrics called differential rank and differential\nscore to analyze data exposure between two snapshots of a genera-\ntive language model.\n3.1 Differential Score and Differential Rank\nWe aim to identify token sequences whose probability differs most\nbetween models ùëÄ and ùëÄ‚Ä≤. Intuitively, such sequences are most\nlikely to be related to the differences between their corresponding\ntraining datasets ùê∑ and ùê∑‚Ä≤.\nTo capture this notion formally, we define thedifferential score\n(DS)of token sequences, which is simply the sum of the differences\nof (contextualized) per-token probabilities. We also define arelative\nvariant fDS based on the relative change in probabilities, which we\nfound to be more robust w.r.t. the noise introduced by different\nrandom initializations of the models ùëÄ and ùëÄ‚Ä≤.\nDefinition 3.1. Given two language models ùëÄ,ùëÄ ‚Ä≤and a token\nsequence ùë°1 ...ùë° ùëõ ‚ààùëá‚àó, we define the differential score of a token\nas the increase in its probability and the relative differential score\nas the relative increase in its probability. We lift these concepts to\ntoken sequences by defining\nDSùëÄ‚Ä≤\nùëÄ (ùë°1 ...ùë° ùëõ)=\nùëõ‚àëÔ∏Å\nùëñ=1\nùëÄ‚Ä≤(ùë°<ùëñ)(ùë°ùëñ)‚àíùëÄ(ùë°<ùëñ)(ùë°ùëñ),\nfDSùëÄ‚Ä≤\nùëÄ (ùë°1 ...ùë° ùëõ)=\nùëõ‚àëÔ∏Å\nùëñ=1\nùëÄ‚Ä≤(ùë°<ùëñ)(ùë°ùëñ)‚àíùëÄ(ùë°<ùëñ)(ùë°ùëñ)\nùëÄ(ùë°<ùëñ)(ùë°ùëñ) .\nThe differential score of a token sequence is best interpreted\nrelative to that of other token sequences. This motivates ranking\nsequences according to their differential score.\nDefinition 3.2. We define the differential rank DR(ùë†)of ùë† ‚ààùëá‚àóas\nthe number of token sequences of length |ùë†|with differential score\nhigher than ùë†.\nDR(ùë†)=\n\f\f\f\nn\nùë†‚Ä≤‚ààùëá|ùë†|\n\f\f\fDSùëÄ‚Ä≤\nùëÄ (ùë†‚Ä≤)> DSùëÄ‚Ä≤\nùëÄ (ùë†)\no\f\f\f .\nThe lower the differential rank of a sequence, the more the se-\nquence is exposed by a model update, with the most exposed se-\nquence having rank 0.\n3.2 Approximating Differential Rank\nComputing the differential rankDR(ùë†)of a sequenceùë†of length |ùë†|=\nùëõrequires searching a space of size|ùëá|ùëõ. To avoid exponential blow-\nup, we rely on Algorithm 1, which approximates the differential\nrank based on beam search .\nAt iteration ùëñ, the algorithm maintains a set ùëÜ of ùëò (called the\nbeam width ) candidate sequences of length ùëñ together with their\ndifferential scores. The algorithm iterates over allùëò¬∑|ùëá|single-token\nextensions of these sequences, computes their differential scores,\nand keeps the ùëò highest-scoring sequences of length ùëñ+1 for the\nnext step. Eventually, the search completes and returns the setùëÜ.\nAlgorithm 1 Beam search for Differential Rank\nIn: ùëÄ,ùëÄ ‚Ä≤=models, ùëá=tokens, ùëò=beam width, ùëõ=length\nOut: ùëÜ=set of (ùëõ-gram, DS) pairs\n1: ùëÜ ‚Üê{(ùúñ,0)} ‚ä≤ Initialize with empty sequence ùúñ\n2: for ùëñ = 1 ...ùëõ do\n3: ùëÜ‚Ä≤‚Üê{(ùë†‚ó¶ùë°,ùëü +DSùëÄ‚Ä≤\nùëÄ (ùë†)(ùë°))|( ùë†,ùëü)‚àà ùëÜ,ùë° ‚ààùëá}\n4: ùëÜ ‚Üêtake(k,S‚Ä≤) ‚ä≤ Take top ùëò items from ùëÜ‚Ä≤\n5: return ùëÜ = {(ùë†1,ùëü1),..., (ùë†ùëò,ùëüùëò)}such that ùëü1 ‚â•¬∑¬∑¬∑‚â• ùëüùëò\nAlgorithm 1 returns a set of token sequences ùë† and their differ-\nential score ùëü. With this we can approximate the differential rank\nDR(ùë†)by the number of token sequencesin ùëÜwith differential score\nhigher than ùë†. For large enough beam widths this yields the true\nrank of ùë†. For smaller widths, the result is a lower bound on DR(ùë†),\nas a search may miss sequences with higher differential score.\nProposition 3.3. If Algorithm 1 returns a set\nùëÜ = {(ùë†1,ùëü1),..., (ùë†ùëò,ùëüùëò)}with ùëü1 ‚â•¬∑¬∑¬∑‚â• ùëüùëò,\nthen DSùëÄ‚Ä≤\nùëÄ (ùë†ùëñ)= ùëüùëñ and DR(ùë†ùëñ)‚â• ùëñ‚àí1.\nOptimizing for Speed. The beam width ùëò governs the trade-off\nbetween computational cost and the precision of the approxima-\ntion. In experiments, we found that shrinking the beam width as\nthe search progresses speeds up the search considerably without\ncompromising on the quality of results. Typically, we use a beam\nwidth |ùëá|, which we halve at each iteration. That is, we consider\n|ùëá|/2 candidate phrases of length two, |ùëá|/4 sequences of length\nthree, and so on.\nOptimizing for Diversity. Since the sequences returned by vanilla\nbeam search typically share a common prefix, we rely on group\nbeam search as a technique for increasing diversity: we split the\ninitial |ùëá|one-token sequences into multiple groups according to\ntheir differential score, and run parallel beam searches extending\neach of the groups independently. See [31] for more sophisticated\ntechniques for increasing diversity.\n4 LEAKAGE ANALYSIS\nWe use our new metrics to perform leakage analyses for various\ndatasets across various model update scenarios. We first describe\nour benchmark datasets with their model configurations and the\nmodel training scenarios we consider. Then, we discuss research\nquestions relevant to the analysis scenarios described in Section 2.3.\nWe then show experiments investigating these questions in detail,\nfirst using synthetically generated canaries as a proxy for updates\nwhere we can precisely control the differences between the datasets\nused to create model snapshots, and then in a realistic setting, in\nwhich we use a set of standard real-world datasets.\n4.1 Datasets and Models\nWe consider three datasets of different size and complexity, matched\nwith standard model architectures whose capacity we adapted to\nthe data size and implemented in TensorFlow.1\nConcretely, we use the Penn Treebank [ 20] (PTB) dataset as\na representative of low-data scenarios, as the standard training\ndataset has only around 900,000 tokens and a vocabulary size of\n10,000. As the corresponding model, we use a two-layer recurrent\nneural network using LSTM cells with 200-dimensional embeddings\nand hidden states and no additional regularization (this corresponds\nto the small configuration of Zaremba et al. [33]).\nSecond, we use a dataset of Reddit comments with 20 million\ntokens overall, of which we split off 5% as validation set. We use a\nvocabulary size of 10,000. We rely on two different model configu-\nrations for this dataset, which allows us to understand the impact\nof model size on information leakage using DR as a metric.\n(1) a one-layer RNN using an LSTM cell with 512-dimensional\nhidden states and 160-dimensional embeddings. We em-\nploy dropout on inputs and outputs with a keep rate of\n0.9 as regularizer. These parameters were chosen in line\nwith a neural language model suitable for next-word rec-\nommendations on resource-constrained mobile devices.\n(2) a model based on the Transformer architecture [30] (more\nconcretely, using the BERT [9] codebase) with four layers\nof six attention heads, each with a hidden dimension of 192.\n1Source code and tools available at: https://github.com/microsoft/language-privacy\nFinally, we use the Wikitext-103 dataset [22] with 103 million\ntraining tokens as a representative of a big data regime, using a\nvocabulary size of 20,000. As the model, we employ a two-layer\nRNN with 512-dimensional LSTM cells and token embedding size\n512 and dropout on inputs and outputs with a keep rate of 0.9 as\nregularizer. We combined this large dataset with this (relatively\nlow-capacity) model to test if our results still hold on datasets that\nclearly require more model capacity than is available.\nAll models and their training are following standard best prac-\ntices for generative language models and represent common (sim-\nple) baselines used in experiments on the used datasets. This can\nbe seen in the perplexity of the trained models on the held-out test\ndata, shown in Table 1, which is in line with common test results.\n4.2 Implementing Model Updates\nUpdated models can be created using different techniques, with\ndifferent applicability to the usage and analysis scenarios discussed\nin Section 2.3.\nRetraining. Given an updated dataset ùê∑‚Ä≤, a fresh model snapshot\nùëÄ‚Ä≤can be obtained by simply training a fresh model from scratch,\nwhich we refer to as retraining. This also involves a fresh (random)\ninitialization of the model parameters, and in practice, retraining\nrepeatedly on the same dataset will yield slightly different models.\nData deletion requires updating a model to eliminate the influence of\nsome training data points at the request of the data owner. This can\nbe done by retraining a model after pruning the data or, equivalently,\nusing techniques with lower computational cost [5, 14].\nContinued Training. In this approach, a fresh model snapshotùëÄ‚Ä≤\nis obtained by taking an existing model ùëÄ and continuing training\nit on additional data. This is the core of the data specialization\nscenario and sometimes also used in data update scenarios to avoid\nthe computational cost of training on a large dataset from scratch.\n4.3 Research Questions\nWith the training techniques outlined for different model update\nscenarios, we consider four research questions in our experiments.\nRQ0: Can an attacker learn private information from model up-\ndates? Here we address the basic question of whether private data\nused to update a model can be leaked in our adversarial setting and\nhow. We first answer this question by using differential score to\nfind information about private sequences used in a model update.\nWe then investigate the influence of other parameters of the system\non the differential score in more detail.\nRQ1: How does masking private data with additional non-sensitive\ndata (ùê∑extra) affect leakage? This is particularly important for the\nuser deletion scenario, for which we need to answer if it is possible\nto safely remove data of a single user, or if such dataset changes\nneed to be hidden among other substantial changes. Concretely, we\nanalyze whether including a large enough additional dataset ùê∑extra\nin an update can prevent leakage of information about the rest of\nthe data used. ùê∑extra can be any dataset which is either available\npublicly or is non-sensitive from the point of view of the model\nprovider or users.\nRQ2: How do retraining and continued training differ with respect\nto information leakage? In the continued training approach, the\nparameters of a previously trained model ùëÄùê∑ are updated based\nonly on new data ùê∑‚Ä≤\\ùê∑. In contrast, in the retraining strategy pa-\nrameters are updated using all data in ùê∑‚Ä≤. The most recent updates\nto model parameters depend only on new data in the continuing\ntraining case, whereas they depend on the whole training data ùê∑‚Ä≤\nwhen retraining a model from scratch. We analyze the effect of this\nseemingly more pronounced dependence.\nRQ3: How is leakage affected by an adversary‚Äôs background knowl-\nedge? Prior attacks on language models assume that the adversary\nhas background knowledge about the context in which a secret\nappears. We analyze the effect of such knowledge for inferring\nprivate data from model updates.\n4.4 Results with Canaries\nWe create a number of canary phrases‚Äîgrammatically correct\nphrases that do not appear in the original dataset‚Äîthat serve as a\nproxy for private data that the adversary is trying to extract. We\nconsider different word frequency characteristics to control the\ninfluence on the used vocabulary. Specifically, we fix the length\nof the canary phrase to 5, choose a valid phrase structure (e.g.,\nSubject, Verb, Adverb, Compound Object), and instantiate each\nplaceholder with a token in a dataset vocabulary. We create ca-\nnaries in which frequencies of tokens are all low (all tokens are\nfrom the least frequent quintile of words), mixed (one token from\neach quintile), increasing from low to high , and decreasing from\nhigh to low . For example, the mixed phrase across all the datasets\nis ‚ÄúNASA used deadly carbon devices‚Äù, and the all low phrase for\nPTB is ‚Äúnurses nervously trusted incompetent graduates‚Äù. As the\nvocabularies differ between the different datasets, the canaries are\nin general dataset-dependent. We vary the amount of private data ,\nùê∂, by inserting a canary phrase ùë† a number of times proportional\nto the number of tokens in the training corpus:\n(1) For PTB, we consider ùëò ‚àà {10,50,100}canary insertions\n(corresponding to 1 canary token in 18K training tokens, 1 in 3.6K,\nand 1 in 1.8K).\n(2) For the Reddit dataset, we useùëò ‚àà{5,50,500}(corresponding\nto 1 in 1M, 1 in 100K, 1 in 10K).\n(3) For the Wikitext-103 data, we use ùëò ‚àà{20,100}(correspond-\ning to 1 in 1M, 1 in 200K).\nWe train the model ùëÄ on ùê∑ and the model ùëÄ‚Ä≤on ùê∑ with ùëò\ncopies of the canary ùë†. We then compute the differential rank of the\ncanaries for different values of ùëò.\nRQ0: Can an attacker learn private information from model up-\ndates? We use our differential score based beam search (Algorithm 1)\nto extract canary phrases that correspond to the change in train-\ning data between ùëÄ and ùëÄ‚Ä≤. The results of varying the number\nof inserted canaries are summarized in Table 1. We highlight the\nfollowing findings:\n‚Ä¢For most combinations of ùëò and types of canaries, we\nsuccessfully recover the canary. This is indicated by the cells\nwith white background, where the canary phrase has themaximum\ndifferential score among all token sequences found by our beam\nsearch, i.e., it ranks first.\nTable 1: Differential score ( DS) for different datasets, model architectures, canaries, and insertion frequencies. White cells\nrepresent a differential rank ( DR) of 0 (as approximated by beam search), and gray cells represent DR > 1000.\nDataset Penn Treebank Reddit Wikitext-103\nModel Type (Perplexity) RNN (120.90) RNN (79.63) Transformer (69.29) RNN (48.59)\nCanary Token Freq. 1:18K 1:3.6K 1:1.8K 1:1M 1:100K 1:10K 1:1M 1:100K 1:10K 1:1M 1:200K\nAll Low 3.40 3.94 3.97 2.83 3.91 3.96 3.22 3.97 3.99 1.39 3.81\nLow to High 3.52 3.85 3.97 0.42 3.66 3.98 0.25 3.66 3.97 0.07 3.21\nMixed 3.02 3.61 3.90 0.23 3.04 3.92 0.39 3.25 3.96 0.25 3.02\nHigh to Low 1.96 2.83 3.46 0.74 1.59 2.89 0.18 1.87 3.10 0.08 1.22\nTable 2: Differential Score ( DSùëÄ‚Ä≤\nùëÄ ) of the mixed frequency canary phrase for the Reddit (RNN) model using different update\ntechniques. Model ùëÄ is trained on ùê∑orig. For the Retraining column, ùëÄ‚Ä≤is trained on ùê∑orig ‚à™ùê∑extra ‚à™ùê∂ starting from random\ninitial parameters. For the Cont‚Äôd Training 1column, ùëÄ‚Ä≤is trained on ùê∑extra ‚à™ùê∂ starting from ùëÄ. For the Cont‚Äôd Training 2\ncolumn, we first train a model ÀúùëÄ on ùê∑extra ‚à™ùê∂ starting from ùëÄ, and then train model ùëÄ‚Ä≤from ÀúùëÄ using additional public data\nùê∑‚Ä≤\nextra. A white cell background means that the differential rank DR (as approximated by our beam search) of the phrase is 0,\ngray cell background means that DR is >1000.\nRetraining Continued Training 1 Continued Training 2\n|ùê∑extra |/|ùê∑orig | 0% 20% 50% 100% 20% 50% 100% 100%\n1:1M 0.23 0.224 0.223 0.229 0.52 0.34 0.46 0.01\n1:100K 3.04 3.032 3.031 3.038 3.56 3.25 3.27 0.26\n‚Ä¢The signal for extraction is strong even when the inserted\ncanaries account for only 0.0001% of the tokens in the dataset.\nThis is visible in the first row of Table 1 where differential scores\napproach 4 ‚Äî close to the upper bound of 5 for 5-token canaries.\n‚Ä¢Private phrases that occur more often in the training data are\nmore exposed via a model update, as expected. This is visible in\nthe monotonic growth of the differential score of canaries with the\nnumber of insertions.\n‚Ä¢Phrases composed of rare words are more easily extracted,\nas seen in the high differential score of canaries constructed from\nlow-frequency tokens. In contrast, canaries with descending token\nfrequencies tolerate much higher number of insertions before being\nexposed. This is expected, as our beam search is biased towards\nfinding high-scoring prefixes.\n‚Ä¢Access to two model snapshots reveals substantially more\nthan access to a single snapshot. For comparison, we successfully\nextract a 5-token canary inserted 1 in 200k times (i.e. inserting one\ntoken every 1M tokens) from two snapshots of an LSTM-based\ngenerative model without additional knowledge. In contrast, [ 6,\nSection 6.1] reports failing to extract the middle token of a 5-token\ncanary inserted 1 in 100k times from a similar LSTM-based model\nwhen given the first and last two words.\nRQ1: Effect of amount of public vs. private data. In Table 2 we\nvary the amount of public data by partitioning the dataset ùê∑ into\nùê∑orig ‚äéùê∑extra such that the latter is 20%, 50%, or 100% of the size of\nùê∑orig (the 0% column is identical to Table 1). The retraining column\nshows that DSùëÄ‚Ä≤\nùëÄ does not change significantly across the different\ndataset splits. That is, canaries can be extracted from the trained\nmodel even when they are contained in a substantially larger dataset\nextension. Hence, the amount of public data in the update does not\nsignificantly affect the leakage of the private data.\nRQ2: Effect of training type. We train a model ùëÄ on a dataset\nùê∑orig to convergence, and then continue training ùëÄ using ùê∑extra\nand the canaries ùê∂, obtaining ùëÄ‚Ä≤. We compare the differential rank\nof the canaries on the models obtained using continued training\nwith that on the models retrained from scratch (shown in the middle\ncolumn of Table 2). We observe that in all cases the differential score\nis higher for continued training than for retraining. As expected,\nthe differential score of the canary phrase decreases as additional\nextra data is used for fine-tuning.\nRQ3: Effect of background knowledge. We evaluate the differen-\ntial score of suffixes of a canary phrase ùë† assuming knowledge of\na prefix. For ùëñ = 1,...,ùëõ we take the prefix ùë°1 ...ùë° ùëñ‚àí1 of the canary\nphrase and compute the differential score ùëü of the token ùë°ùëñ condi-\ntional on having read the prefix, i.e., ùëÄ‚Ä≤(ùë°<ùëñ)(ùë°ùëñ)‚àíùëÄ(ùë°<ùëñ)(ùë°ùëñ). The\nrelationship between ùëñand ùëü indicates how much knowledge about\nùë† is required to expose the remainder of the canary phrase.\nFigure 2 depicts the result of this analysis for canaries with high-\nto-low and all-low token frequencies on the Reddit dataset. Our\nresults show that, while the differential score of the first token\nwithout context is close to 0, the score of subsequent tokens quickly\ngrows for all-low canaries, even with a low number of canary\ninsertions. In contrast, more context is required before the score of\nhigh-to-low canaries increases, as the model is less influenced by\nthe small number of additional occurrences of frequent tokens.\nThis suggests that, even in cases where we fail to extract the\ncanary without additional knowledge, an adversary can use the\ndifferential rank to complete a partially known phrase, or confirm\nthat a phrase was used to update the model.\n0\n0.2\n0.4\n0.6\n0.8\n1\nDiÔ¨Äerential score\n0 1 2 3 4\nPreÔ¨Åx Length\nHL-5 HL-50 HL-500 LL-5 LL-50 LL-500\nFigure 1: Line graph showing that the differential score of\ncanaries with all-low token frequencies rises faster with re-\nspect to prefix length compared to canaries with high-to-low\ntoken frequencies.\nFigure 2: Differential score of tokens in canaries given a pre-\nfix for the Reddit dataset. LL- ùëò denotes ùëò canary insertions\nwith all-low token frequencies (solid lines), and HL- ùëò de-\nnotes high-to-low token frequencies (dashed lines).\n4.5 Results with Real-world Data\nWe simulate real-world scenarios by sourcing training data from\nreal-world conversations on specific topics, and using it as a proxy\nfor private data included in the training data used in model updates.\nThe adversary‚Äôs goal is to extract specific phrases occurring in the\nproxy dataset, or phrases that do not occur literally but nonetheless\nreveal the topic of conversations.\nWe mimic the data distribution shift by choosing conversations\non topics that are not dominant in the original dataset, so that\nwe can better judge whether phrases extracted using differential\nscore are on-topic and thus represent meaningful leakage of private\ninformation. Specifically, we compare models trained only on data\nfrom the Reddit dataset against models trained on data from the\nReddit dataset plus messages from one of two newsgroups from\nthe 20 Newsgroups dataset [19]:\na) rec.sport.hockey, containing around 184K tokens,‚âà1% of the\noriginal training data; and\nb) talk.politics.mideast, containing around 430K tokens,‚âà2%\nof the original training data.\nWe train a model ùëÄ on the entire Reddit dataset and retrain\nùëÄ‚Ä≤from scratch on the same dataset plus all messages from one\nof the two newsgroups. For both model architectures (RNNs and\nTransformer) described in Section 4.1 and each newsgroup, we\ncompute the sequences with highest relative differential score. Since\nthe sequences returned by vanilla beam search typically share a\ncommon prefix, we run a group beam search (see Section 3.2) to\nget a more diverse sample.\nRQ0: Can an attacker learn private information from model up-\ndates? Tables 3 and 7 (in the Appendix) display the highest-scoring\nsequences of length 4 in each group of a fDS-based 5-group beam\nsearch.\nThe exposed sentences are on-topic w.r.t. the newsgroup in-\ncluded, e.g., the hockey theme dominates the top ranked sequences\nin Table 3. This suggests that, information about the private data\nused for the update is leaked. It is noteworthy that these results are\nobtained assuming a weak adversary that does not require either\nbackground knowledge about the dataset distribution or about the\ninformation it tries to extract. In contrast, concurrent work on up-\ndates of image classification models [24] requires knowledge about\nthe data distribution to train shadow models, while prior work on\nsingle language models [6] requires a known prefix for extraction\nof a secret.\nGiven some background knowledge in the form of a long enough\nprefix of a phrase occurring in the private data, we show that the\ncomplete phrase can be extracted by a beam search directed by\ndifferential score (see Table 5).\nRQ1: Effect of amount of public vs. private data. We consider\npartitions of the Reddit dataset ùê∑ into ùê∑orig and ùê∑extra of different\nrelative sizes. For each partition, we train a model ùëÄ on ùê∑orig and\na model ùëÄ‚Ä≤on ùê∑orig ‚à™ùê∑extra ‚à™ùëÅ, where ùëÅ are all messages from\ntalk.politics.mideast. We observe the following:\n‚Ä¢For all phrases, the proportion of public data ranging from 5%\nto 100% used in the update does not significantly affect their relative\ndifferential scores, which confirms our findings for canaries.\n‚Ä¢The top two phrases resemble canaries in that they occur\nliterally multiple times in the update dataset, which explains their\nhigh scores. An exception is Little resistance was offered ,\nwhich appears 12 times in the dataset but still has low score. Other\nphrases do not occur literally in newsgroup messages, but digest\nrecurrent discussions or contain ùëõ-grams that do occur.\nRQ2: Effect of training type. We train a model ùëÄ on ùê∑orig to\nconvergence, and then continue training ùëÄ using ùê∑extra ‚à™ùëÅ to\nproduce a model ùëÄ‚Ä≤. To understand the effect of the training type\non information leakage, we sample a set of representative phrases\nand compare their relative differential scores w.r.t.ùëÄand ùëÄ‚Ä≤against\ntheir scores w.r.t.ùëÄ and a model trained on ùê∑‚à™ùëÅ from scratch.\nThe results are shown in Table 4, together with the perplexity\ndecrease after the model update. Retrained models correspond to the\ndata update and data deletion scenarios and their perplexity drop is\ngreater the more data is used during retraining. Continued training\ncorresponds to the data specialization scenario. The perplexity drop\nin the updated model is greater the larger is the proportion of\nnewsgroup data used in the update, for which the initial model is\nnot specialized.\nThe last two rows in Table 4 correspond to phrases found by\ngroup beam search in the continued training scenario, but that\nhave too low a score to be found whenùëÄ‚Ä≤is retrained from scratch\ninstead. The converse, i.e., phrases that have low score when contin-\nuing training and high score when retraining, seems to occur rarely\nand less consistently (e.g., Saudi troops surrounded village).\nFor phrases that occur literally in the dataset, the results are in\nline with those for canaries (see Table 2), with scores decreasing as\nmore data is used during the fine-tuning stage. For other phrases,\nthe results are not as clear-cut. While fine-tuning a model exclu-\nsively on private data yields scores that are significantly higher\nthan when retraining a model from scratch, this effect vanishes\nas more additional data is used; in some cases continued training\nyields scores lower than when retraining a model on the same data.\nTable 3: Top ranked phrases in group beam search for a model updated with rec.sport.hockey. For the layperson: Los Angeles\nKings, Minnesota North Stars, and Toronto Maple Leaf are National Hockey League teams; Norm Green was the owner of the\nNorth Stars; an ice hockey game consists of three periods with overtime to break ties. Capitalization added for emphasis.\nRNN TransformerPhrase fDS Phrase fDS\nAngeles Kings prize pools 56.42 Minnesota North Stars playoff 96.81\nNational Hockey League champions 53.68 Arsenal Maple Leaf fans 71.88\nNorm ‚Äôs advocate is 39.66 Overtime no scoring chance 54.77\nIntention you lecture me 21.59 Period 2 power play 47,85\nCovering yourself basically means 21.41 Penalty shot playoff results 42.63\nTable 4: Relative differential score of phrases found by beam search when retraining from scratch and continuing training from\na previous model. The results are for RNN models trained on partitions of the Reddit dataset with ùëÅ = talk.politics.mideast.\nCells for which continued training yields a higher score than retraining appear in bold font. Capitalization added for emphasis.\nRetraining Continued TrainingPhrase (# of occurrences in ùëÅ)\n|ùê∑extra |/|ùê∑orig | 0% 5% 10% 20% 100% 0% 5% 10% 20% 100%\nPerplexity decrease 0.79 1.17 2.45 3.82 11.82 73.97 18.45 10.29 6.08 8.28\nCenter for Policy Research (93) 99.77 101.38 97.11 98.65 91.53 276.98 198.69 150.56 122.25 117.54\nTroops surrounded village after (12) 44.50 44.50 44.50 44.41 44.54 173.95 47.38 19.48 7.81 35.56\nPartition of northern Israel (0) 27.61 16.81 38.48 26.10 38.76 68.98 16.48 12.47 22.93 18.82\nWest Bank peace talks (0) 25.68 25.64 25.69 25.71 25.75 71.54 24.38 28.60 16.91 4.62\nSpiritual and political leaders (0) 25.23 25.98 17.04 24.21 23.47 126.92 14.91 10.00 3.44 11.05\nSaudi troops surrounded village (0) 24.31 24.31 24.31 24.31 24.30 5.05 44.58 4.29 7.29 63.84\nArab governments invaded Turkey (0) 22.59 22.62 22.80 22.78 22.80 24.01 15.58 7.08 18.12 11.90\nLittle resistance was offered (12) 22.24 22.09 25.12 22.34 25.59 215.16 25.02 2.00 3.30 5.64\nBuffer zone aimed at protecting (0) 4.00 4.47 5.30 5.25 5.69 57.29 69.76 18.92 14.50 22.25\nCapital letters racial discrimination (0) 3.76 3.32 3.40 3.60 3.84 94.60 52.74 39.11 11.22 3.45\nRQ3: Effect of background knowledge. An adversary wishing to\nextract information about the dataset used to update a language\nmodel may direct a search using as prompt a known prefix from\nthe dataset. We study how long this prefix needs to be to recover\nthe rest of phrase.\nWe consider a RNN model ùëÄ trained on the full Reddit dataset\nand a model ùëÄ‚Ä≤trained on the union of the full Reddit dataset\nand all messages of the talk.politics.mideast newsgroup. We\nsample 4 phrases in newsgroup messages beginning with the name\nof a Middle Eastern country and containing only tokens in the\nmodel vocabulary. We believe it is feasible for the adversary to\nguess these prefixes from the description of the newsgroup or the\ngeopolitical context. For each phraseùë†and ùëñ = 0,..., |ùë†|‚àí1 we run a\nfDS-based beam search for phrases of the same length with constant\nbeam width 10,000 and 100 groups starting from ùë†1 ...ùë† ùëñ. Table 5\nshows the rank of ùë† among the search results (or ‚àûif absent).\nWe observe a correlation between the score of a phrase and\nthe minimum prefix sufficient to recover it. However, a dip in the\nscore of two consecutive tokens is much more consequential: a\ncommon word like the, which has a similar distribution in the\noriginal and private datasets, contributes little to the score of a\nphrase and is unlikely to be picked up as a candidate extension in a\nbeam search. Recovering from this requires additional heuristics or\na more expensive search, using wider beams or looking more than\none token ahead to better approximate the true rank of a phrase.\n5 CHARACTERIZING THE SOURCE OF\nLEAKAGE\nPrior work has primarily studied information leakage when an\nattacker has only access to a single model snapshot. Here, we first\nanalyze how much our analysis gains from having access to two\nmodel snapshots, and then consider the influence of common causes\nof leakage in the single-model case. The central ones are overfit-\nting [32] to the training data, and unintended memorization [6] of\ndata items that is independent of the distribution to be learned.\nRQ4: How important is access to a second model snapshot? We\nwant to analyze how much leakage of sensitive information is\nincreased when having access to two model snapshots ùëÄùê∑, ùëÄùê∑‚Ä≤\nin contrast to having only access to a single model ùëÄùê∑‚Ä≤. This is\na challenging analysis in a realistic setting, due to the size of the\ndata and the lack of an easily computable metric for information\nleakage. Concretely, we want to show that the data we can extract\nusing the differential analysis of ùëÄùê∑ and ùëÄùê∑‚Ä≤ is (a) more likely to\nbe part of ùê∑‚Ä≤than of ùê∑, (b) not very common in ùê∑‚Ä≤, and (c) that\n(a) and (b) are more true for the results of the differential analysis\nthan for the analysis of ùëÄùê∑‚Ä≤ alone.\nWe quantify how likely a given sentence is to be a part of a dataset\nusing a simpler, well-understood model of natural language data,\nnamely an ùëõ-gram model. ùëõ-gram models define the probability of\na token ùë°ùëõ+1 appearing after a sequence of tokens ùë°1 ...ùë° ùëõ as the\nTable 5: Results of beam searches for different prefix lengths. A rank of 0 means that the search recovers the complete phrase.\nDue to the heuristic nature of the search the rank reported may be lower than the true rank of ùë†. Conversely, a beam search\nmay not encounter ùë† at all despite having lower rank than most phrases encountered. For instance, this occurs for Turkey\nsearched an American plane, where all but 7 search results with no prompt have higher rank (lower score).\nPrefix length ùëñ\nPhrase ùë† # of occurrences fDS (ùë†) 0 1 2 3 4 5\nTurkey searched an American plane 6 82.96 ‚àû 1 1 0 0 ‚Äì\nIsrael allows freedom of religion 3 24.44 ‚àû ‚àû 788 55 0 ‚Äì\nIraq with an elected government 2 23.75 ‚àû ‚àû ‚àû 4 0 ‚Äì\nIsrael sealed off the occupied lands 2 6.48 ‚àû ‚àû ‚àû ‚àû 3442 2\nnumber of times ùë°1 ...ùë° ùëõùë°ùëõ+1 appeared in the dataset divided by the\nnumber of times ùë°1 ...ùë° ùëõ appeared.\nIn our experiments, we use the perplexity of 3-gram models\ntrained on ùê∑ (resp. ùëÅ) to capture how likely a given extracted\nsentence is part of the dataset ùê∑ (resp. ùëÅ). We compare these per-\nplexity values for sequences extracted using group beam search\nfrom the models ùëÄùê∑ (resp. ùëÄùê∑‚Ä≤) and for sequences extracted using\nour differential rank-based search, following the setup of Section\n4.5. Concretely, we used the entire Reddit comment data as dataset\nùê∑, and the messages ùëÅ from talk.politics.mideast as data up-\ndate. We are concerned with information an attacker can gain about\nthe contents of ùëÅ.\nFigure 3a shows the results of our analysis when we train ùëÄùê∑‚Ä≤\non ùê∑‚Ä≤= ùê∑‚à™ùëÅ from scratch. Points above the main diagonal are\ncloser in distribution to the (private) data update ùëÅ than to the\nbase data ùê∑. This shows that our attack extracts sequences using\ndifferential score (represented by red crosses) that are more likely to\nbe part of ùëÅ than of ùê∑, and that these sequences differ substantially\nfrom the sequences obtained by a single-model analysis. In fact,\nthe sequences obtained by single-model analysis for ùëÄùê∑ and ùëÄùê∑‚Ä≤\nshow little significant difference. Note that the perplexity values\nperp3-gram(ùê∑)are very high for some of the extracted sentences, as\nthey use combinations of tokens that never appear in the original\ntraining dataset ùê∑. Similarly, Figure 3b shows the results of this\nanalysis on the scenario in which we obtain ùëÄùê∑‚Ä≤ by specializing\nthe model ùëÄùê∑ by continuing training on the dataset ùëÅ. While our\ndifferential analysis again captures sequences more likely to be part\nof the updated data ùëÅ than of the original data ùê∑, the single-model\nanalysis now also shows some of this effect.\nRQ5: Is leakage due to overfitting or intended memorization? All\nmodels are trained using an early-stopping criterion that halts\ntraining when the model does not improve on a separate validation\nset. This effectively rules out overfitting to the training data. Addi-\ntionally, model training employs regularization strategies such as\ndropout to further encourage the trained models to generalize to\nunseen data.\nWe refer to the model‚Äôs ability to reproduce verbatim fragments\nof the training data as memorization and call it intended if this is\nnecessary to serve its purpose of generating natural language (e.g.,\na model needs to memorize the token pair ‚ÄúUnited States‚Äù, as it is\nan extremely common combination) and unintended otherwise.\nIn the experimental results in Table 4, we have included the num-\nber of times that the phrases with the highest differential scores\nappear in the update dataset. Since some of these phrases do not\nappear verbatim, we also measure how close these phrases are to\nphrases in the original and update datasets. Table 6 shows the Lev-\nenshtein distance of extracted phrases from Table 4 to their nearest\nneighbor in either dataset. Generally, we find closer matches in the\nupdate dataset. While ‚ÄúCenter for Policy Research‚Äù is a clear case of\nintended memorization, as the name appears many times in email\nsignatures, other phrases appear rarely or never, indicating that our\nanalysis extracts phrases that need not be memorized to serve its\npurpose. This is further supported by the results in Table 5, where\nextraction of complete sentences such as ‚ÄúIsrael allows freedom of\nreligion‚Äù occurring as few as three times in the dataset is possible.\nOverall, this indicates that intended memorization is unlikely to\nexplain our results.\nUnintended memorization may occur for infrequent phrases.\nHowever, it cannot alone explain our results, as shown by our\nsuccess in recovering canaries when using a low-capacity model in\na large-data regime (cf. Wikitext-103 column in Table 1), for which\nthe effect of unintended memorization is less pronounced, and\nevidenced by the large context needed to recover canaries from a\nsingle-model analysis [6]. The most likely explanation remains that\na differential analysis of two model snapshots amplifies otherwise\nimperceptible differences in the data used to train them, which\nwould be hard to suppress without hurting a model‚Äôs performance.\n6 MITIGATIONS\nIn this section, we discuss and analyze three strategies to miti-\ngate information leakage in model updates: (1) Differential Privacy,\n(2) continued training with public data, and (3) truncating the out-\nput of the updated model.\n6.1 Mitigation: Differential Privacy\nDifferential privacy (DP) [11] provides strong guarantees on the\namount of information leaked by a released output. Given a compu-\ntation over records it guarantees a bound on the effect that any input\nrecord can have on the output. Formally, ùêπ is a (ùúñ,ùõø)-differentially-\nprivate computation if for any datasets ùê∑ and ùê∑‚Ä≤that differ in one\nrecord and for any subset ùëÇ of ùêπ‚Äôs range we have\nPr(ùêπ(ùê∑)‚àà ùëÇ)‚â§ exp(ùúñ)¬∑ Pr(ùêπ(ùê∑‚Ä≤)‚àà ùëÇ)+ùõø.\nDifferential privacy is a natural candidate for defending against\nmembership-like inferences about data. The exact application of\ndifferential privacy for protecting the information in the model\n100\n1000\n10000\n100000\n1√ó106\n1√ó107\n1√ó108\n1000 10000 100000 1√ó106\nùëùùëíùëüùëù3‚àígram(ùê∑)(ùë†)\nùëùùëíùëüùëù3‚àígram(ùëÅ)(ùë†)\nExtracted fromùëÄ\nExtracted fromùëÄ‚Ä≤\nExtracted from(ùëÄ,ùëÄ‚Ä≤)\n(a) Re-training from scratch\n100\n1000\n10000\n100000\n1√ó106\n1√ó107\n1√ó108\n1000 10000 100000 1√ó106\nùëùùëíùëüùëù3‚àígram(ùê∑)(ùë†)\nùëùùëíùëüùëù3‚àígram(ùëÅ)(ùë†)\nExtracted fromùëÄ\nExtracted fromùëÄ‚Ä≤\nExtracted from(ùëÄ,ùëÄ‚Ä≤)\n(b) Continued training\nFigure 3: Sensitivity of extracted content. +depict sentences extracted from ùëÄ, √ófrom ùëÄ‚Ä≤, and ‚àófrom (ùëÄ,ùëÄ ‚Ä≤)using Differential\nScore. Vertical axis depicts the perplexity w.r.t data ùê∑, horizontal axis depicts perplexity w.r.t data update ùëÅ. Points above the\ndiagonal are closer in distribution to the (private) data update ùëÅ than to the base data ùê∑.\nTable 6: Quantifying near matches of extracted phrases from RNN models trained on the base Reddit dataset and updated with\ntalk.politics.mideast. For each extracted phrase, we compare the Levenshtein distance to its nearest neighbor in the base\nand update datasets respectively. The updated dataset contains closer matches for all phrases except west bank peace talks\nand capital letters racial discrimination, for which there are equally close matches in both datasets.\nExtracted phrase talk.politics.mideast Reddit\ncenter for policy research center for policy research 0 center for instant research 1\ntroops surrounded village after troops surrounded village after 0 from the village after 2\npartition of northern israel shelling of northern israel 1 annexation of northern greece 2\nwest bank peace talks . no peace talks 2 : stated peace talks 2\nspiritual and political leaders spiritual and political evolutions 1 , and like leaders 2\nsaudi troops surrounded village our troops surrounded village 1 \" hometown \" village 3\narab governments invaded turkey arab governments are not 2 ! or wrap turkey 3\nlittle resistance was offered little resistance was offered 0 , i was offered 2\nbuffer zone aimed at protecting \" aimed at protecting 2 ‚Äôs aimed at a 3\ncapital letters racial discrimination % of racial discrimination 2 allegory for racial discrimination 2\nupdate depends on what one wishes to protect w.r.t. the new data:\nindividual sentences in the new data or all information present in\nthe update. For the former, sequence-level privacy can suffice while\nfor the latter group DP can serve as a mitigation technique where\nthe size of the group is proportional to the number of sequences\nin the update. Recall that an ùúñ-DP algorithm ùêπ is ùëòùúñ-differentially\nprivate for groups of size ùëò [11].\nDifferential privacy can be achieved in gradient-based optimiza-\ntion computations [1, 4, 28] by clipping the gradient of every record\nin a batch according to some bound ùêø, then adding noise propor-\ntional to ùêøto the sum of the clipped gradients, averaging over the\nbatch size and using this noisy average gradient update during\nbackpropagation.\nWe evaluate the extent to which DP mitigates attacks considered\nin this paper by training models on the Penn Treebank (PTB) dataset\nwith canaries with sequence-level differential privacy. We train DP\nmodels using the TensorFlow Privacy library [ 2] for two sets of\n(ùúñ,ùõø)parameters, (5,1 √ó10‚àí5)and (111,1 √ó10‚àí5), for two datasets:\nPTB and PTB with 50 insertions of the all-low-frequency canary.\nWe rely on [2] to train models with differentially private stochastic\ngradient descent using a Gaussian noise mechanism and to compute\nthe overall privacy loss of the training phase. As expected, the\nperformance of models trained with DP degrades, in our case from\n‚âà23% accuracy in predicting the next token on the validation dataset\nto 11.89% and 13.34% for ùúñ values of 5 and 111, respectively.\nWhile the beam search with the parameters of Section 4.4 no\nlonger returns the canary phrase for the DP-trained models, we note\nthat the models have degraded so far that they are essentially only\npredicting the most common words from each class (e.g., ‚Äúis‚Äù when\na verb is required) and thus, the result is unsurprising. We note that\nthe guarantees of sequence-level DP formally do not apply for the\ncase where canary phrases are inserted as multiple sequences, and\nthat ùúñ values for our models are high. However, the ùúñ-analysis is\nan upper bound and similar observations about the effectiveness of\ntraining with DP with high ùúñ were reported by Carlini et al. [6].\nWe further investigate the effect of DP training on the differential\nrank of a canary phrase that was inserted 50 times. Instead of using\nour beam search method to approximate the differential rank, we\nfully explore the space of subsequences of length two, and find\nthat the DR for the two-token prefix of our canary phrase dropped\nfrom 0 to 9,458,399 and 849,685 for the models with ùúñ = 5 and\nùúñ = 111 respectively. In addition, we compare the differential score\nof the whole phrase and observe that it drops from 3.94 for the\noriginal model to 4.5 √ó10‚àí4 and 2.1 √ó10‚àí3 for models with ùúñ = 5\nand ùúñ = 111, respectively. Though our experiment results validate\nthat DP can mitigate the particular attack method considered in\nthis paper for canary phrases, the model degradation is significant.\nIn addition, the computational overhead of per-sequence gradient\nclipping required by [ 2] is substantial, making it unsuitable for\ntraining high-capacity neural language models on large datasets.\n6.2 Mitigation: Two-stage Continued Training\nWe also consider a possible mitigation strategy where we perform\ncontinued training in two stages. For this, we split the dataset into\nthree equal parts ùê∑orig, ùê∑extra and ùê∑‚Ä≤\nextra. We proceed as in the\ncontinued training setting in RQ2, but add a final step in which\nwe train on another dataset after training on the canaries. This\nresembles a setting where an attacker does not have access to two\nconsecutive snapshots. The rightmost column of Table 2, shows\nthat the differential score of the canary phrase drops substantially\nafter the second training stage. Thus, two or multi-stage continued\ntraining, where only the last trained model is released, might be a\npath toward mitigating leakage of private data.\n6.3 Mitigation: Truncating Output\nFinally, we analyze the effect of truncating the output of the updated\nmodel for each query. Specifically, the adversary still has full access\nto the original model ùëÄ but only receives the top ùëò tokens from\nthe updated model ùëÄ‚Ä≤. This is a slight weakening of our adversary\nmodel, but is realizable for some applications. For example, in the\nData Specialization scenario, the adversary may have full access to\nthe public base model, but can only access the specialized model via\nan API that truncates the results for each query. In theData Update\nscenario, even if models are deployed to client devices, it may be\npossible to enforce this by running the model in a Trusted Execution\nEnvironment (TEE), such as Intel SGX [18] or ARM TrustZone [3]\non the client device.\nTo evaluate the impact of this mitigation, we repeat the experi-\nment described in Section 5 and plot only the sentences extracted\nusing differential score (i.e., the ‚ÄòSnapshot attack‚Äô) for different val-\nues of ùëò. To facilitate comparison, we use the same beam width as\nin Figures 3a and 3b. As shown in Figure 4, decreasing the value\nof ùëò brings the extracted sequences closer to the main diagonal,\nwhere they have similar likelihood of being drawn from either\ndataset. Similarly to Figures 3a and 3b, we also observe a difference\nbetween re-training from scratch and continued training; for the\nsame value of ùëò, the sentences extracted after continued training\nare more likely to be private than those extracted after the model\nis re-trained from scratch. Additionally, if the adversary only has\naccess to the top ùëò outputs of the original model ùëÄ, this would\nfurther reduce the leakage. In applications where this mitigation is\nrealizable, returning only the topùëòoutputs can thus reduce leakage\nwithout decreasing the utility of the provided outputs.\n7 RELATED WORK\nSeveral works have shown that machine learning models can leak\ninformation about training data and proposed defenses for them.\nMembership inference attacks. Shokri et al. [26] show that one\ncan identify whether a record belongs to the training dataset of a\nclassification model given black-box access to the model and shadow\nmodels trained on data from a similar distribution. Salem et al. [25]\ndemonstrate that similar attacks are effective under weaker adver-\nsary models. It would be interesting to study how membership infer-\nence based on differential score compares to other techniques [8].\nSong and Shmatikov [27] also study sequence-to-sequence lan-\nguage models and show how a user can check if their data has\nbeen used for training. In their setting, an auditor needs an aux-\niliary dataset to train shadow models with the same algorithm as\nthe target model and queries the target model for predictions on\na sample of the user‚Äôs data. The auxiliary dataset does not need\nto be drawn from the same distribution as the original training\ndata (unlike [26]) and the auditor only observes a list of several\ntop-ranked tokens. In contrast, our approach requires no auxiliary\ndataset, but assumes access to the probability distributions over\nall tokens from two different model snapshots. From this, we are\nable to recover full sequences from the differences in training data\nrather than binary information about data presence. Like them,\nwe find that sequences with infrequent tokens provide a stronger\nsignal to the adversary/auditor.\nReconstruction attacks. These attacks abuse a model to recover\nspecific training points [24]. The attacks we present are a form of\nreconstruction attacks against an updated model: we recover data\npoints in the dataset used for the update given the original model\nas auxiliary information.\nCarlini et al. [6] is closest to our work, as it also considers infor-\nmation leakage of language models. The authors assess the risk of\n(unintended) memorization of rare sequences in the training data.\nThey show that canaries inserted into training data can be retrieved\nfrom a character-level language model. The key differences to our\napproach are that 1) we consider a different attack scenario where\nan adversary has access to two snapshots of a model , and 2) our\ncanaries follow the distribution of the data whereas Carlini et al.\n[6] add a random sequence of numbers in a fixed context into a\ndataset of financial news articles (e.g., ‚ÄúThe random number is ... ‚Äù),\nwhere such phrases are rare. We instead are able to extract canaries\nwithout any context , even when the canary token frequency in the\ntraining dataset is as low as one in a million.\nSalem et al. [24] consider reconstruction of training data that\nwas used to update a model. While their goal is similar to ours,\ntheir adversarial model and setup differ: 1) similar to Song and\nShmatikov [27] and Shokri et al. [26], their attacker uses shadow\nmodels trained on auxiliary data drawn from the same distribution\n1000\n10000\n100000\n1√ó106\n1√ó107\n1√ó108\n1√ó109\n1√ó1010\n100000 1√ó106 1√ó107\nùëùùëíùëüùëù3‚àígram(ùê∑)(ùë†)\nùëùùëíùëüùëù3‚àígram(ùëÅ)(ùë†)\nùëò=5\nùëò=20\nùëò=100\n(a) Re-training from scratch\n1000\n10000\n100000\n1√ó106\n1√ó107\n1√ó108\n1√ó109\n1√ó1010\n100000 1√ó106 1√ó107\nùëùùëíùëüùëù3‚àígram(ùê∑)(ùë†)\nùëùùëíùëüùëù3‚àígram(ùëÅ)(ùë†)\nùëò=5\nùëò=20\nùëò=100\n(b) Continued training\nFigure 4: Sentences extracted from (ùëÄ,ùëÄ ‚Ä≤)using Differential Score when the adversary only receives the top ùëò tokens from\nthe updated model ùëÄ‚Ä≤for each query. The axes have the same meaning as in Figures 3a and 3b.\nas the target training dataset, while in our setting the attacker has\nno prior knowledge of this distribution and does not need auxiliary\ndata; 2) the updated model is obtained by fine-tuning the target\nmodel with additional data rather than re-training it from scratch\non the changed dataset; 3) the focus is on classification models and\nnot on (generative) language models.\nInformation leakage from updates has also been considered for\nsearchable encryption: an attacker who has control over data in an\nupdate to an encrypted database can learn information about its\ncontent and previous encrypted searches on it [7].\nModel inversion attacks. Fredrikson et al. [12, 13] repurpose a\nmodel to work backwards, inferring unknown attributes of individ-\nuals given known attributes and a target prediction. Individuals\nneed not be present in the training data, and results are aggregate\nstatistics rather than information about specific training points.\nDifferential Privacy. In terms of defenses, McMahan et al. [21]\nstudy how to train LSTM models with DP guarantees at a user-level.\nThey investigate utility and privacy trade-offs of the trained mod-\nels depending on a range of parameters (e.g., clipping bound and\nbatch size). Carlini et al. [6] show that DP protects against leakage\nof canaries in character-level models, while Song and Shmatikov\n[27] show that an audit as described above fails when training\nlanguage models with user-level DP using the techniques of [21].\nPan-privacy [10], on the other hand, studies the problem of main-\ntaining differential privacy when an attacker observes snapshots of\nthe internal state of a DP algorithm between updates.\nDeletion of Data. Techniques to update models to delete training\ndata points can be broadly classified into exact and approximate\ndeletion. Ginart et al. [14] define exact deletion of a training point\nfrom a model as a stochastic operation returning the same distri-\nbution as re-training from scratch without that point, and develop\ndeletion algorithms forùëò-means clustering with low amortized cost.\nBourtoule et al. [5] propose an exact deletion methodology that\naggregates models trained on disjoint data shards, trading storage\nfor computation such that only shards that contain deleted points\nneed to be retrained. Exact deletion is equivalent to retraining\nfrom scratch, hence, publishing model snapshots before and after\ndeletion matches our adversarial model and our results apply.\nContemporary approximate deletion methods [15, 16] yield mod-\nels that are only statistically indistinguishable from a model re-\ntrained from scratch. These methods stochastically update model\nparameters based on estimates of the influence of the data to be\ndeleted and achieve relaxations of differential privacy. It would be\ninteresting to study how susceptible to snapshot attacks are models\nobtained by approximate deletion.\n8 CONCLUSION\nWe presented a first systematic study of the privacy implications\nof releasing snapshots of a language model trained on overlapping\ndata. Our results show that updates pose a threat which needs to be\nconsidered in the lifecycle of machine learning applications. We en-\ncourage the research community to work towards quantifying and\nreducing unintended information leakage caused by model updates,\nand hope to make practitioners aware of the privacy implications\nof deploying and updating high-capacity language models.\nACKNOWLEDGMENTS\nWe thank Doug Orr and Nicolas Papernot for helpful discussions\nand the anonymous reviewers for their valuable comments.\nREFERENCES\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,\nKunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In\n23rd ACM SIGSAC Conference on Computer and Communications Security, CCS\n2016. ACM, 308‚Äì318.\n[2] Galen Andrew, Steve Chien, and Nicolas Papernot. 2020. TensorFlow Privacy.\nhttps://github.com/tensorflow/privacy. (2020).\n[3] Arm. 2020. TrustZone Technology. (2020). https://developer.arm.com/\nip-products/security-ip/trustzone\n[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014. Private Empirical Risk\nMinimization: Efficient Algorithms and Tight Error Bounds. In 55th IEEE Annual\nSymposium on Foundations of Computer Science, FOCS 2014 . IEEE Computer\nSociety, 464‚Äì473.\n[5] Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui\nJia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine\nUnlearning. In 42nd IEEE Symposium on Security and Privacy, S&P 2021 . IEEE\nComputer Society. To appear.\n[6] Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej Kos, and Dawn Song. 2019.\nThe Secret Sharer: Evaluating and Testing Unintended Memorization in Neural\nNetworks. In 28th USENIX Security Symposium . USENIX Association, 267‚Äì284.\n[7] David Cash, Paul Grubbs, Jason Perry, and Thomas Ristenpart. 2015. Leakage-\nAbuse Attacks Against Searchable Encryption. In 22nd ACM SIGSAC Conference\non Computer and Communications Security, CCS 2015 . ACM, 668‚Äì679.\n[8] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert,\nand Yang Zhang. 2020. When Machine Unlearning Jeopardizes Privacy. (2020).\narXiv:cs.CR/2005.02205\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nIn 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, NAACL-HLT 2019 , Vol. 1.\nAssociation for Computational Linguistics, 380‚Äì385.\n[10] Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N. Rothblum, and Sergey\nYekhanin. 2010. Pan-Private Streaming Algorithms. In Innovations in Computer\nScience, ICS 2010 . Tsinghua University Press, 66‚Äì80.\n[11] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differen-\ntial Privacy. Foundations and Trends in Theoretical Computer Science 9, 3-4 (2014),\n211‚Äì407.\n[12] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion\nAttacks that Exploit Confidence Information and Basic Countermeasures. In\n22nd ACM SIGSAC Conference on Computer and Communications Security, CCS\n2015. ACM, 1322‚Äì1333.\n[13] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon M. Lin, David Page, and\nThomas Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case\nStudy of Personalized Warfarin Dosing. In 23rd USENIX Security Symposium .\nUSENIX Association, 17‚Äì32.\n[14] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. 2019. Making\nAI Forget You: Data Deletion in Machine Learning. In Advances in Neural Infor-\nmation Processing Systems 32, NeurIPS 2019 . Curran Associates, Inc., 3518‚Äì3531.\n[15] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal Sunshine\nof the Spotless Net: Selective Forgetting in Deep Networks. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, CVPR 2020 . IEEE, 9301‚Äì9309.\n[16] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens van der Maaten. 2020.\nCertified Data Removal from Machine Learning Models. In 37th International\nConference on Machine Learning, ICML 2020 . PMLR. To appear.\n[17] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long Short-Term Memory.\nNeural Computation 9, 8 (1997), 1735‚Äì1780.\n[18] Intel. 2020. Software Guard Extensions (SGX). (2020). https://software.intel.com/\nen-us/sgx\n[19] Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. In 12th International\nMachine Learning Conference on Machine Learning, ICML 1995 . Morgan Kaufmann,\n331‚Äì339.\n[20] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Build-\ning a Large Annotated Corpus of English: The Penn Treebank. Computational\nLinguistics 19, 2 (1993), 313‚Äì330.\n[21] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learn-\ning Differentially Private Recurrent Language Models. In 6th International Con-\nference on Learning Representations, ICLR 2018 . OpenReview.net.\n[22] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017.\nPointer Sentinel Mixture Models. In 5th International Conference on Learning\nRepresentations, ICLR 2017 . OpenReview.net.\n[23] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners . Technical\nReport. OpenAI.\n[24] Ahmed Salem, Apratim Bhattacharyya, Michael Backes, Mario Fritz, and Yang\nZhang. 2019. Updates-Leak: Data Set Inference and Reconstruction Attacks in\nOnline Learning. (2019). arXiv:cs.CR/1904.01067\n[25] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and\nMichael Backes. 2019. ML-Leaks: Model and Data Independent Membership\nInference Attacks and Defenses on Machine Learning Models. In 26th Annual\nNetwork and Distributed System Security Symposium, NDSS 2019 . The Internet\nSociety.\n[26] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.\nMembership Inference Attacks Against Machine Learning Models. In 38th IEEE\nSymposium on Security and Privacy, S&P 2017 . IEEE Computer Society, 3‚Äì18.\n[27] Congzheng Song and Vitaly Shmatikov. 2019. Auditing Data Provenance in\nText-Generation Models. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, KDD 2019 . ACM, 196‚Äì206.\n[28] S. Song, K. Chaudhuri, and A. D. Sarwate. 2013. Stochastic Gradient Descent\nwith Differentially Private Updates. In 1st IEEE Global Conference on Signal and\nInformation Processing, GlobalSIP 2013 . IEEE Computer Society, 245‚Äì248.\n[29] European Union. 2016. Regulation (EU) 2016/679 of the European Parliament\nand of the Council of 27 April 2016 on the protection of natural persons with\nregard to the processing of personal data and on the free movement of such data,\nand repealing Directive 95/46/EC (General Data Protection Regulation). (2016).\n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Advances in Neural Information Processing Systems 30, NIPS 2017 .\nCurran Associates, Inc., 5998‚Äì6008.\n[31] Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun,\nStefan Lee, David J. Crandall, and Dhruv Batra. 2018. Diverse Beam Search for\nImproved Description of Complex Scenes. In 32nd AAAI Conference on Artificial\nIntelligence, AAAI 2018 . AAAI Press, 7371‚Äì7379.\n[32] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy\nRisk in Machine Learning: Analyzing the Connection to Overfitting. In 31st IEEE\nComputer Security Foundations Symposium, CSF 2018 . IEEE Computer Society,\n268‚Äì282.\n[33] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent Neural\nNetwork Regularization. (2014). arXiv:cs.NE/1409.2329\nA RESULTS FOR TALK.POLITICS.MIDEAST\nTable 7 (deferred from Section 4.5) shows the highest-scoring se-\nquences of length 4 in a group beam search with 5 groups for the\ntalk.politics.mideast dataset for RNN and Transformer archi-\ntectures.\nTable 7: Top ranked phrases in a group beam search for a model updated with talk.politics.mideast. Center for Policy\nResearch is a prolific newsgroup poster; many of the posts around the time the 20 Newsgroups dataset [19] was collected\ndiscuss tensions between Turkey and Armenia.\nRNN TransformerPhrase fDS Phrase fDS\nTurkey searched first aid 31.32 Center for Policy Research 200.27\nDoll flies lay scattered 22.79 Escaped of course ... 95.18\nArab governments invaded Turkey 20.20 Holocaust %UNK% museum museum 88.20\nLawsuit offers crime rates 18.35 Troops surrounded village after 79.35\nSanity boosters health care 11.17 Turkey searched neither Arab 37.69"
}