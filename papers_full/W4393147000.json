{
  "title": "TaskLAMA: Probing the Complex Task Understanding of Language Models",
  "url": "https://openalex.org/W4393147000",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100742301",
      "name": "Quan Yuan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5061716911",
      "name": "Mehran Kazemi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100319817",
      "name": "Xin Xu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5078489709",
      "name": "Isaac Noble",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5081931413",
      "name": "Vaiva Imbrasaitė",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5111238931",
      "name": "Deepak Ramachandran",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6676187458",
    "https://openalex.org/W6677420194",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W168322017",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W6769030065",
    "https://openalex.org/W4313303751",
    "https://openalex.org/W6689029123",
    "https://openalex.org/W3105979796",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W6724682677",
    "https://openalex.org/W6713646853",
    "https://openalex.org/W6797156022",
    "https://openalex.org/W6688454842",
    "https://openalex.org/W6607999579",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W2404427926",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W3166699508",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1915711566",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2979461259",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3171446839",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W4285105798",
    "https://openalex.org/W4378510404",
    "https://openalex.org/W4379087254",
    "https://openalex.org/W2209694002",
    "https://openalex.org/W4380715528",
    "https://openalex.org/W4212865381",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W4386966676",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W4385572734",
    "https://openalex.org/W3154984731",
    "https://openalex.org/W2116938310",
    "https://openalex.org/W4385567216",
    "https://openalex.org/W3103543556"
  ],
  "abstract": "Structured Complex Task Decomposition (SCTD) is the problem of breaking down a complex real-world task (such as planning a wedding) into a directed acyclic graph over individual steps that contribute to achieving the task, with edges specifying temporal dependencies between steps. SCTD is an important component of assistive planning tools, and a challenge for commonsense reasoning systems. We probe how accurately SCTD can be done with the knowledge extracted from pre-trained Large Language Models (LLMs). We introduce a new high-quality human-annotated dataset for this problem and novel metrics to fairly assess performance of LLMs against several baselines. Our experiments reveal that LLMs are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15% to 280% over the best baseline. We also propose a number of approaches to further improve their performance, with a relative improvement of 7% to 37%. However, we find that LLMs still struggle to predict pairwise temporal dependencies, which reveals a gap in their understanding of complex tasks.",
  "full_text": "TaskLAMA: Probing the Complex Task Understanding of Language Models\nQuan Yuan\u0003, Mehran Kazemi\u0003, Xin Xu\u0003, Isaac Noble, Vaiva Imbrasaite, Deepak Ramachandran\nGoogle Research\nfyquan, mehrankazemi, xxujasmine, isaacn, vimbrasaite, ramachandrandg@google.com\nAbstract\nStructured Complex Task Decomposition (SCTD) is the\nproblem of breaking down a complex real-world task (such\nas planning a wedding) into a directed acyclic graph over in-\ndividual steps that contribute to achieving the task, with edges\nspecifying temporal dependencies between them. SCTD is an\nimportant component of assistive planning tools, and a chal-\nlenge for commonsense reasoning systems. We probe how\naccurately SCTD can be done with the knowledge extracted\nfrom pre-trained Large Language Models (LLMs). We in-\ntroduce a new high-quality human-annotated dataset for this\nproblem and novel metrics to fairly assess performance of\nLLMs against several baselines. Our experiments reveal that\nLLMs are able to decompose complex tasks into individ-\nual steps effectively, with a relative improvement of 15% to\n280% over the best baseline. We also propose a number of ap-\nproaches to further improve their performance, with a relative\nimprovement of 7% to 37% over the base model. However,\nwe ﬁnd that LLMs still struggle to predict pairwise temporal\ndependencies, which reveals a gap in their understanding of\ncomplex tasks.\nIntroduction\nIn their daily lives, people are involved in executing multi-\nple tasks of different temporal granularity in order to achieve\ntheir varied goals. There is abundant evidence that con-\nsciously decomposing a complex task into smaller sub-tasks\nleads to more efﬁcient and reliable execution. For example,\nKokkalis et al. (2013) show that people tend to achieve tasks\nbetter and faster when given a concrete plan with actionable\nsteps. Cheng et al. (2015) show that breaking a macro-task\ninto micro-tasks for workers results in more accurate over-\nall quality and allows for easier recovery from interruption.\nChilton et al. (2013) show that breaking down a task into\nsub-tasks is beneﬁcial for taxonomy creation. Teevan, Iqbal,\nand V on Veh (2016) show that breaking a task into sub-tasks\ncan be beneﬁcial for collaborative writing (e.g., writing a de-\nscription of a shared project). Allen and Peikoff (2001) argue\nin their book that “there is an inverse relationship between\nthings on your mind and those things getting done”.\nGiven a complex task, the goal of structured complex task\ndecomposition (SCTD) is to automatically ﬁnd a complete\n\u0003Contributed equally.\nCopyright c\r 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nTask: How to make mayonnaiseTake a blender\nAdd 2 eggs to \nthe blender\nMix a tablespoon \nof mustard\nAdd salt for taste\nSprinkle some \npepper\nAdd vegetable oil\nBlend the ingredients \non high speed\nContinue blending until \nthe mixture turns to \nmayonnaise\nTransfer the egg \nmayonnaise into a bowlTask Graph\nContext: Egg-based mayonnaise\nFigure 1: An example of a task graph for a complex task\nfrom the TaskLAMA dataset. Nine steps are outlined. The\nfour steps with horizontal texts can be done in any order.\nset of necessary steps for achieving the task, and specify\ntemporal dependencies between these steps (i.e. which steps\nshould be done before which). The output can be described\nas a directed acyclic graph, which we term Task Graph,\nwhere the nodes represent the steps and the edges represent\ntemporal dependencies. Any ordering of the nodes that re-\nspects the graph edges is a possible ordering of the steps\nto accomplish the task. Figure 1 demonstrates an example\ntask graph. The importance of this problem has led to an\nextensive classic and modern literature for developing solu-\ntions based on the latest AI technologies available (Newell,\nSimon et al. 1972; Kokkalis et al. 2013; Awadallah et al.\n2014; Zhang et al. 2021). These works were also motivated\nby the feature of SCTD being a quintessential or (informally\nspeaking) AI-Complete problem, involving many features of\nhuman-level reasoning such as logical/defeasible reasoning,\nuncertainty and high context-dependence. The existing solu-\ntions in the literature are typically based on crowd-sourcing\n(Kokkalis et al. 2013; Zhou et al. 2022b), based on simi-\nlarity or co-occurrence of user search queries (Awadallah\net al. 2014; Mehrotra and Yilmaz 2017; Zhang et al. 2015),\nor based on summarizing the content of relevant web-pages\nfound through web search (Zhang et al. 2021).\nIn this paper, we probe the extent to which such knowl-\nedge can be extracted from large language models (LLMs).\nPrevious work has shown that LLMs contain a large amount\nof different types of knowledge (Petroni et al. 2019; Sung\net al. 2021; West et al. 2021) and can do various types of\nreasoning (Wei et al. 2022; Nye et al. 2021; Kazemi et al.\n2023a). Our work extends this line of work by probing\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19468\nLLMs for their SCTD knowledge and reasoning abilities,\nand demonstrating their current strengths and limitations.\nWe create a high-quality human-annotated dataset for the\nSCTD problem. Following the current convention of nam-\ning language model probes as xLAMA, we name the dataset\nTaskLAMA. Speciﬁcally, we gather a set of 1612 tasks and\nask human annotators to 1) write their assumptions to pro-\nvide context for the task, 2) write the required steps for those\ntasks under the provided context, and 3) specify the tempo-\nral dependencies between the steps. This gives us a total of\n12118 steps and 11105 temporal dependencies.\nWe identify a potential problem with the metrics used in\nprevious work to measure the quality of the generated nodes:\none can arbitrarily improve the metric by simply adding du-\nplicate sub-steps. To solve this issue, we propose robust met-\nrics and report results with them providing the ﬁrst fair com-\nparison of methods for this problem. We also develop novel\nmetrics for measuring the quality of the generated temporal\ndependencies. These metrics are potentially generally appli-\ncable in other settings where annotated/labeled graphs pro-\nduced by generative models must be compared.\nWe compare the performance of LLMs on TaskLAMA\nagainst various baselines. Our results reveal that the steps\ngenerated by an off-the-shelf LLM have higher quality than\nthe baselines, offering 15% \u0000280% relative improvement\ncompared to the best baseline in terms of different metrics;\nthey also understand the context and can adapt the generated\nsteps based on the context in which the complex task is to be\ndone. We then propose a number of approaches to improve\nthe performance of off-the-shelf LLMs even further, using\nthe specialized structure of the SCTD problem. The combi-\nnation of these solutions result in 7% \u000037% improvement\nover the base model, depending on the metric we use. We\nalso measure the quality of the temporal dependencies pro-\nduced by LLMs and observe that while LLMs are good at\ngenerating good sequences of steps, their ability in predict-\ning pairwise temporal dependency remains unsatisfactory.\nA summary of our main contributions follow: 1- we cre-\nate TaskLAMA, a high-quality probe speciﬁcally focused on\ncomplex real-world task understanding, 2- we develop met-\nrics for measuring model performance for SCTD, 3- we pro-\npose various LLM-based approaches for SCTD and compare\nagainst a number of baselines that do not leverage LLMs,\n4- we conduct a comprehensive set of experiments showing\nthat LLMs perform well at decomposing a complex task into\na sequence of steps, but their understanding of temporal de-\npendencies between these steps remains unsatisfactory.\nRelated Work\nWe categorize the related work as follows:\nCrowd-Sourced SCTD: One line of work uses crowd-\nsourcing for obtaining the steps and their temporal order for\ncomplex tasks. Kokkalis et al. (2013) develop a framework\nwhere users can ﬁnd information about various tasks: if a\nsimilar task (measured using natural language processing\ntools) already exists in their database, the information about\nthat task is shown to the user; otherwise, the task is sent for\ncrowd-sourcing. Zhou et al. (2022b) combine the informa-\ntion from the WikiHow website to produce hierarchical task\ntrees. While crowd-sourcing may lead to high-precision task\ngraphs, it is costly and may suffer from low-recall as new\ntask graphs cannot be built on-the-ﬂy for novel tasks.\nQuery-based SCTD: Another commonly used approach\nfor SCTD is by leveraging user search queries. Awadallah\net al. (2014) create sessions from the search queries of a\ncommercial search engine and propose steps for complex\ntasks by ﬁnding the queries that frequently co-occurred with\nthe complex task in different sessions. Mehrotra and Yilmaz\n(2017) propose a hierarchical clustering approach for search\nqueries where the queries higher in the hierarchy correspond\nto tasks and their children represent steps to those tasks.\nZhang et al. (2015) map queries to demands using external\nknowledge and mine frequent demand patterns. In this work,\nwe compare against a number of query-based approaches.\nSummarization-based SCTD: Zhang et al. (2021) pro-\nposed an approach to SCTD where for a given complex task,\nﬁrst a web search is done to identify relevant web pages, and\nthen a language model is trained to summarize the contents\nof those web-pages into task graphs. In this work, we take a\ndifferent approach by measuring how much of the informa-\ntion can be directly obtained from the LLM itself.\nHierarchical Task Networks (HTNs):HTNs provide an\napproach to automated planning where the action dependen-\ncies can be given in the form of a hierarchically structured\nnetwork (Erol 1995; Humphreys 2019; Guan et al. 2023).\nThey are typically used for planning problems where the\nworld is observable, and the set of actions, pre-conditions,\nand effects can be explicitly deﬁned. In our setting, how-\never, plans should be constructed for many different tasks\n(many of which only observed at the test time) and with a\nvery partial view of the world.\nLLM Knowledge Probing: Previous work has shown\nthat LLMs contain a large amount of different types of\nknowledge. This includes factual (Petroni et al. 2019; Jiang\net al. 2020), commonsense (Zhou et al. 2020; Davison, Feld-\nman, and Rush 2019; Yin et al. 2022), biomedical (Sung\net al. 2021), numerical (Lin et al. 2020), scale (Zhang et al.\n2020), and many other types of knowledge. Most related\nto our work, it has been shown that LLMs perform well\nin breaking a simple goal into speciﬁc low-level actions a\nrobot needs to take to achieve the goal (Huang et al. 2022)\n(e.g., providing the low-level steps for a goal such as throw\naway garbage); they also perform well in simple, advice-\nseeking scripts (Sakaguchi et al. 2021; Madaan et al. 2022;\nBrahman et al. 2023) (e.g., go out with friends, live some-\nwhere warmer, etc.). Our work is in the same vein with these\nworks, but extends them by measuring the amount of infor-\nmation one can extract from LLMs for complex tasks re-\nquiring multiple (potentially complex) steps to be completed\n(see Table 1 for a sample of such tasks).\nThe TaskLAMA Probe\nTo measure the performance of LLMs for task graph gener-\nation, we create a dataset of task graphs for 1630 complex\ntasks. Following the LAnguage Model Analysis (LAMA)\nnaming convention, we call our dataset TaskLAMA. Cru-\ncially, TaskLAMA is created directly by human annotators as\nopposed to being extracted from public websites (as, e.g., in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19469\nComplex Task Assumption / Context\nBuild a curved retaining wall Using concrete\nStart a property management company In Florida\nWrite a grant proposal For non-proﬁt\nCook lobster tails at home Grilled\nInstall a light switch –\nGet a real estate license In Texas\nRecover deleted photos From iPhone\nPlan a wedding In Italy\nBecome a travel agent Online agent\nTable 1: Sample complex tasks and extra assumptions (con-\ntext) from TaskLAMA (– means no extra assumption).\n(Wang et al. 2022; Zhang, Lyu, and Callison-Burch 2020)) to\nallow for evaluating LLMs on previously unobserved data.\nBefore describing the dataset creation process, we start with\ndeﬁning our notation. We represent a graph G= (V; E) as a\ntuple with V= fv1; v2; : : : ; vngrepresenting the nodes and\nE \u001aV2 represent edges. Throughout this paper, we work\nwith directed graphs where, for an edge(vi; vj), the order of\nthe nodes is important.\nA task graph is deﬁned as follows:\nDeﬁnition 1 (Task Graph). A task graph for a complex task\nT is a graph G = (V; E) where each node vi 2 Vrep-\nresents a step required for accomplishing T and each edge\n(vi; vj) 2E represents a temporal dependence between vi\nand vj, indicating that vi should be done before vj.\nThe problem we study in this paper is the following:\nGiven a complex task T (and sometimes an extra context\nabout the task) as input, generate a task graph Gas output.\nTaskLAMA provides a probe for this problem with (Ti; Gi)\npairs. The creation of TaskLAMA involves four main com-\nponents: 1- selecting a setfT1; : : : ;T\u001cgof complex tasks, 2-\ngathering steps Viinvolved in the execution of these tasks, 3-\ngathering temporal dependencies Ei between the steps, and\n4- splitting the dataset into train, validation, and test sets. In\nwhat follows, we explain each component in detail 1.\nSelecting a Set of Complex Tasks\nWe obtain a varied and representative set of complex tasks\nperformed by humans from the following two sources.\nThe MSComplexTasks dataset (Zhang et al. 2021):\nThere are 711 distinct tasks in this dataset coming from the\nlogs of Wunderlist, a popular task management application.\nThese tasks are selected from a bigger pool of tasks using a\nnumber of ﬁlters, most notably ﬁltering simple tasks.\nPopular How Tosearch queries: From the logs of a com-\nmercial search engine, we extracted popular search queries\nthat start with How To. To ensure anonymity, we removed\nany query issued by fewer than 1000 unique users. We then\ndeduplicated these tasks and applied a number of other ﬁlters\nto remove tasks involving sensitive and harmful topics, tasks\nrequiring medical advice, or tasks that did not deem complex\n(e.g., tasks that did not involve multiple steps). We labeled\n1The full dataset can be downloaded from https://storage.\ngoogleapis.com/gresearch/tasklama/tasklama.zip\nthe remaining tasks based on topic and sampled from each\ntopic to avoid over-presence or under-presence from certain\ntopics. At the end, we obtained 901 tasks.\nGathering Steps for the Tasks\nComplex tasks can be typically accomplished in multiple\ndifferent ways, with a potentially different set of steps in-\nvolved each time depending on the context (e.g., Make a\nburger can have different steps depending on whether we do\nit On a charcoal grill or In an air fryer). To gather the steps\nVfor a task T while taking the context into account, we in-\nstructed annotators to do the following: for each task, write\ndown the assumptions they are making and then the set of\nsteps for the task under those assumptions. Some examples\nof tasks and assumptions are presented in Table 1.\nThe annotators were allowed to search and learn about the\ntask, but were required to write the steps in their own words\nand based on their understanding. The annotators were also\ninstructed to make sure that each step starts with a verb, cor-\nresponds to exactly one action, is meaningful as a standalone\nsentence/does not contain anaphora (e.g., avoid put it on the\ngrill), is actionable as opposed to general advice, and is ap-\nplicable in the context of the assumptions made. If the an-\nnotator was unfamiliar with the task, they were instructed to\nskip it; the task was then sent to another annotator.\nWe trained the annotators over three rounds of pilot study,\neach time having them annotate a small number of tasks\nand then explaining to them the mistakes they made. Some\nof the dominant mistakes in the initial rounds included di-\nrectly copying search results, failing to follow one of the\nrules mentioned above, and/or misunderstanding how to pro-\nvide assumptions. These issues were mostly resolved over\nthe three rounds of training. In the ﬁnal round, the workers\nspent an average of 892 seconds on each task. Through the\nabove process, we gathered a total of 12118 steps for our\n1612 tasks, with an average of 7:5 steps per task.\nGathering Temporal Dependencies\nOnce we gathered the assumptions and steps for each task, a\nseparate set of annotators were asked to specify the order de-\npendencies Efor the steps of each task. The annotators were\ninstructed to ﬁrst draw the graph on a piece of paper and then\nsubmit the edges one by one. Similar to the previous case,\nwe trained the workers over three rounds of pilot studies.\nThe mistakes in the initial rounds ranged from producing a\nlinear sequence instead of a graph, only providing a partial\ngraph (i.e. only a subset of the edges), and misunderstand-\ning the concept of temporal dependence. These issues were\nmostly resolved over the three rounds of training. In the ﬁ-\nnal round, the workers spent an average of 1138 seconds on\neach task. Through this process, we gathered a total 11105\ntemporal dependencies for our 1612 tasks, with an average\nof 6:9 dependencies per task.\nDataset Splitting\nWe split the data into train, validation, and test sets in such\na way that the tasks are conceptually different in the three\nsets. Towards this goal, we ﬁrst grouped the 1612 tasks into\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19470\n621 clusters based on their textual similarity and then ran-\ndomly split the clusters into train, validation, and test sets.\nThis splitting strategy ensures some amount of difference in\nthe tasks in each set. Following this splitting strategy, we\nended up with 965 examples in the training set, 169 in the\nvalidation set, and 478 in the test set.\nMethod\nTo generate task graphs for a given complex task, a model\nneeds to 1) generate the steps, and 2) decide the order de-\npendency between the steps.\nGenerating the Steps\nTo generate the steps for a task, we experiment with the fol-\nlowing strategies:\nIn-Context Learning (ICL): In ICL (Brown et al. 2020),\none provides a few demonstrations each containing an input\nand the expected output, followed by the query for which\nthe model has to generate the output. The model learns the\nrelation between the input and the output in context, and uses\nthat to generate an output for the provided query.\nMultiple Sequences (MultSeq): We notice that when we\ngenerate multiple sequences of steps for a task using the\nICL approach, the sequences sometimes have complemen-\ntary steps between them. To leverage this intuition, we gen-\nerate k sequences of steps using the ICL approach, setting\nthe decoding temperature to 0.5 to allow for diverse gener-\nations. Then we deduplicate the steps and combine the re-\nmaining steps to obtain the ﬁnal set of steps. The deduplica-\ntion procedure is explained in the Appendix.\nSample and Filter (S&F): We notice that when we gen-\nerate multiple sequences of steps for a task using the ICL\napproach, some of the sequences have higher quality than\nthe others. To leverage this, we ﬁrst train a separate model\nthat scores the sequences generated by the ICL approach.\nThen, we generate multiple sequences of steps and select the\none with the highest score according to the trained model. To\ntrain a model that can score the generated sequences, we ﬁrst\ngenerate 16 sequences per task for the tasks in our training\nset, then we evaluate each of the generated sequences with\nrespect to the golden sequence and obtain a single number\nindicating how good that sequence is. This gives us a dataset\nof (Task, Sequence of Steps, Score). We then train a model\nthat given a task and a sequence of steps predicts the score.\nSoft-Prompt Tuning (SPT): In the case of ICL, the in-\ncontext demonstrations we provide as input get mapped to\nthe corresponding token embeddings that are then fed into\nthe LLM. Recently, it has been shown that instead of using a\nﬁxed set of token embeddings as the in-context demonstra-\ntions, one can learn those embeddings based on training data\nto enable better in-context examples. This technique is typi-\ncally referred to as soft-prompt tuning (Lester, Al-Rfou, and\nConstant 2021). We learn the prompt embedding based on\nour training data and decide the size of the prompt based on\nperformance on our validation set.\nMultSeq + S&F: We generate k0 sequences of steps us-\ning the ICL approach, then select and combine the top k\nsequences ranked by the S&F model.\nMultSeq + SPT: We combine k sequences from the SPT\nmodel instead of the ICL model.\nS&F + SPT: We use the S&F model to score the se-\nquences of steps generated by SPT and then select the best.\nMultSeq + S&F + SPT: This is similar to S&F + SPT\nexcept that we combine the steps from the top k sequences.\nGenerating the Order Dependencies\nWhile task graphs are directed acyclic graphs (e.g., see Fig-\nure 1), when using an LLM to generate the steps for a task\nwe get a sequence of steps. We compare a few approaches\nthat can turn the sequence of steps generated by the LLM\ninto a task graph.\nLinear: We use the linear order of steps produced by the\nLLM as the ﬁnal task graph.\nICL: We provide multiple examples as demonstrations\neach containing a task, two of its steps, and the label indicat-\ning whether the ﬁrst step should be done before the second\none. We then provide a new task and two of its steps and ask\nfor the label.\nICL with Chain-of-Thought: Chain-of-thought (CoT)\nprompting (Wei et al. 2022) is a technique where besides\nproviding the input and the label, the demonstrations also\nprovide a rationale for the label. We test a version of ICL\nwith CoT where the rationale for the demonstrating exam-\nples are written manually.\nSPT: We soft-prompt tune the LLM on the training data\nto learn to predict the label given a task and two of its steps.\nLLM Scoring: Given the initial linear order produced by\nthe LLM, we generate m sequences by randomly swapping\nthe order of two steps and use the LLM to score the se-\nquences2. We then sort the sequences descendingly based\non their LLM score and select the highest scoring sequences.\nThen, for two steps vi and vj, if we see vi before vj in some\nsequences and vj before vi in the other sequences, we as-\nsume vi and vj can be done in any order; otherwise, if vi\nalways appears before vj (or vice versa), we assume vi has\nto be done before vj (or vice versa). We turn the sequences\ninto a graph following the above strategy.\nIn the case of cycles, i.e. if a model predicts that A should\nbe done before B, B should be done before C, and C should\nbe done before A, we remove the dependencies assuming\nthat each of the steps can be done before the other one so\nthey can be done in any order.\nMetrics\nFor evaluation, we need two sets of metrics: one for measur-\ning the quality of the steps (nodes) and one for measuring\nthe quality of the temporal dependencies (edges). We dis-\ncuss each of these separately.\nNode Metrics: Let VG = fv1; : : : ; vngbe the steps in the\ngolden graph, UM = fu1; : : : ; umgbe the steps in the gen-\nerated graph, and S be a pairwise similarity function of the\nsteps (we use the cosine similarity of the universal sentence\nencodings (Cer et al. 2018) of the steps). Previous work has\n2Note that LLMs can be used both to generate an output and\nalso to score a provided output.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19471\n- Rouge1 Rouge2 RougeL Hungarian Relaxed Hung.\nModel F1 F2 F1 F2 F1 F2 F1 F2 F1 F2\nRepeat Task 12.2 10.3 1.7 1.5 12.1 10.2 34.6 33.0 41.3 37.7\nRepeat Sim 11.6 10.2 1.5 1.3 10.8 9.5 33. 33.4 40.1 38.4\nCo-occur 16.4 15.0 2.7 2.5 13.6 12.5 34.8 34.7 41.7 40.0\nHierarchical 14.6 12.6 2.3 2.0 12.7 11.0 34.3 34.1 41.2 39.3\nICL 33.1 31.7 10.0 9.5 24.0 23.0 40.1 40.3 48.1 47.3\nMultSeq 32.5 37.2 10.5 12.1 22.9 26.3 35.3 42.8 47.5 50.4\nS&F 36.6 34.3 11.3 10.6 25.6 24.1 42.7 41.9 50.2 49.1\nSPT 38.7 36.3 13.2 12.3 26.7 25.1 43.6 42.4 51.5 50.3\nMultSeq + S&F 37.6 38.3 12.1 12.3 25.2 25.7 41.7 44.4 50.5 51.1\nMultSeq + SPT 36.8 41.3 13.4 15.0 25.3 28.4 39.6 46.2 51.3 53.2\nS&F + SPT 39.0 38.2 13.4 13.1 26.7 26.2 43.8 43.2 51.5 50.4\nMultSeq + S&F + SPT 38.7 40.0 13.7 14.1 25.8 26.7 42.5 44.5 51.6 51.9\nTable 2: The performance of different models for task step generation measured in terms of multiple metrics. Bold indicates\nwinner. ICL stands for In-Context Learning, MultSeq for Multiple Sequences, S&F for Sample and Filter, and SPT for Soft\nPrompt Tuning.\nproposed to compute precision as P\ni\nmaxj S(ui;vj)\njUMj and re-\ncall as P\nj\nmaxi S(uj;vi)\njVGj (Zhang et al. 2021). We ﬁnd, how-\never, that with these metrics, one can arbitrarily increase the\nprecision without sacriﬁcing recall. Consider the case where\nVG = fv1; v2g, UM1 = fu1; u2g, and UM2 = fu1; u0\n1; u2g\nand let S(u1; v1) = 0:6 and zero for other pairs, and u0\n1 be\na near-duplicate of u1. Ideally, the output of M1 should be\npreferred to the output of M2 because they provide the same\ninformation but M1 has no duplicates. However, using the\nabove formulae, M1 will have a precision of (0:6+0:0)=2 =\n0:3 and a recall of (0:6+0:0)=2 = 0:3, whereas M2 will\nhave a precision of (0:6+0:6+0:0)=3 = 0:4 and a recall of\n(0:6+0:0)=2 = 0:3. We observed this issue in our experiments\ntoo: combining the steps from multiple sequences (without\ndeduplication) increased both precision and recall.\nThe problem described above is due to the one-to-many\nmapping formulation of precision and recall. To solve this is-\nsue, we use a Hungarian matching (Kuhn 1955) of the steps\nthat enforces a one-to-one mapping. We note, however, that\nin some cases, one step in the golden graph (e.g., add salt\nand pepper) may correspond to more than one steps in the\ngenerated graph (e.g., add salt and add pepper ) and vice\nversa, in which case a one-to-one mapping may be too re-\nstrictive. To account for this, we also report a relaxed version\nof Hungarian matching that allows a one-to-two mapping3.\nOnce the precision and recall are computed using (re-\nlaxed) Hungarian matching, we compute the F1 score and\nreport it. We note that some of the generated steps that do\nnot appear in the golden steps may still be good steps (e.g.\nbecause they provide more detail that is not in the golden\ngraph). For this reason, recall might be a more informative\nmetric than precision. To account for this, we also report an\nF2 score where we weigh recall twice as much as precision.\nFollowing previous work (Zhang et al. 2021; Madaan\net al. 2022), we also concatenate the steps for each task and\n3We could also report more relaxed versions such as one-to-\nthree, but we observe that the cases where a single step corresponds\nto more than two steps are rare.\ncreate a single document, and then report Rouge (F1 and F2)\nscores for these documents.\nEdge Metrics To compare the generated edges with those\nof the ground truth graph, we ﬁrst match the nodes from the\ngenerated graph to the nodes from the golden graph using\nHungarian matching. If a node in one graph does not have a\nmatch in the other graph, then we connect it to a dummy sin-\ngleton node. Then, for each pair of matched nodes (vi; uj),\nlet Pi and Ci be the parents and children of vi respectively,\nand Pj; Cj be the parents and children of uj respectively.\nWe measure the amount of overlap between Pi and Pj as\nwell as the amount of overlap betweenCiand Cj, both com-\nputed in terms of Rouge score. Then we report two metrics:\n1- In-Degree: the average overlap between Pi and Pj over\nall matched pairs of steps, 2-Out-Degree: the average over-\nlap between Ci and Cj over all matched pairs of steps. In-\ntuitively, by measuring the overlap between Pi and Pj, we\nmeasure the amount of overlap between the (immediate) pre-\nconditions of the two matched nodes. Moreover, by measur-\ning the overlap between Ci and Cj, we measure the amount\nof overlap between the steps that become executable (imme-\ndiately) after we execute the matched nodes.\nFurthermore, we also report another metric, which we\nterm step proximity, computed as the average overlap be-\ntween Pi [Ci and Pj [Cj. Note that this metric does not\nevaluate the order of the temporal dependencies; instead,\nit evaluates whether the steps that should be done in close\nproximity to each other are indeed placed close to each other\nin the generated graph.\nTask Decomposition Results\nWe compare against a number of baselines outlined below.\nRepeat Task: This baseline repeats the task m times; m\nis a hyperparameter that we tune on the validation set.\nRepeat Similar: This baseline works similarly as the pre-\nvious baseline, but for the i-th step, we randomly select one\nof the (non-stop word) tokens in the step and replace it with a\nsemantically similar token. The similarity is computed based\non GloVe embeddings (Pennington, Socher, and Manning\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19472\nRouge1 F1 Rouge1 F2 Rouge2 F1 Rouge2 F2 RougeL F1 RougeL F2 Hungarian\nF1\nHungarian\nF2\nRelaxed\nHung. F1\nRelaxed\nHung. F2\n0\n10\n20\n30\n40\n50F1/F2-Score\nWithout Context\nWith Context\nFigure 2: The model performance with and without the context provided as input. When the context is provided to the model,\nit performs better on all metrics.\n2014) of the tokens. The rationale for this substitution is that\nthose words are likely to appear in the steps. For example,\nfor the taskmake a smoothie, some of the most similar words\nto smoothie include yogurt, juice, strawberry and granola\nwhich are likely to appear in the steps of the task.\nSearch Query Co-occurrence: We cluster a large set of\nqueries from a commercial search engine by placing near-\nduplicate queries into the same cluster. Then, inspired by\nAwadallah et al. (2014), for a given taskT we ﬁnd the cluster\nCt that is most similar to T and we rank the other clusters\nbased on the co-occurrence of their queries with those of Ct\nand pick the top k clusters. We select a representative query\nfrom each of these cluster to serve as a step for the task T.\nThe hyperparameter k is tuned on the validation set.\nSearch Query Hierarchy: We perform a second level of\nclustering on top of the clustering from the previous base-\nline, where we aim to put similar-intent queries into the same\ncluster. Then, inspired by Mehrotra and Yilmaz (2017), for a\ngiven task T we ﬁnd the clusters CL1 and CL2 from our level\none and level two clustering (note thatCL1 is a child of CL2)\nwhere T should belong, and then obtain steps by selecting a\nquery from each of the top k sibling clusters of CL1. Here, k\nis a hyperparameter that is tuned on the validation set.\nResults: According to the results in Table 2, even the ICL\napproach signiﬁcantly outperforms the other baselines on all\nmetrics. For example, we observe a relative improvement of\n280% over the best baseline in terms of Rouge2 F2-Score,\nand 15% in terms of Hungarian matching F1-score. This es-\ntablishes LLMs as a powerful source for extracting infor-\nmation about task steps. Our solutions further improve upon\nthe ICL results. In particular, we observe that both S&F and\nSPT result in improvements compared to ICL across various\nmetrics, with SPT providing more improvement compared\nto S&F. The MultSeq method brings improvement mostly\nfor the F2-scores, due to having higher recall, showing that\nthe steps generated in multiple LLM calls could be com-\nplementary as the combination of them improves recall, and\nhence the F2-score. The approaches that mix two solutions\nperform better than the individual approaches in isolation\nin many cases. Among these approaches, MultSeq + SPT\nworks best in terms of the F2-score andS&F+SPT performs\nbest in terms of F1-score. The combination of all three solu-\ntions also works well, but is often dominated by one of the\napproaches that combines two solutions.\nTask: Clean a mattress Context: Using baking soda\nModel Output w/o Context: Vacuum the mattress, wash the mattress \ncover, wash the mattress pad, dampen a cloth with vinegar, wipe the \nmattress with the cloth, allow to dry, remove any stains.\nModel Output w/ Context: Vacuum the mattress, mix baking soda \nand water in a bowl, apply the mixture to the mattress, allow the \nbaking soda to dry, vacuum the mattress again.\nFigure 3: An example of model outputs for a task with and\nwithout the context provided as input to the model.\nContext Understanding Results\nThe steps for completing a complex task can be different de-\npending on the context. For example,recover deleted photos\nhas different steps depending on the device. We measure the\nability of LLMs in providing contextualized steps for a task.\nRecall that in our dataset, the steps for a task are writ-\nten under certain assumptions that provide the context. To\nmeasure how well LLMs can providing contextualized steps\nfor a task, we compare their performance with and without\nthe assumption/context being provided to them. We test both\nsettings with ICL: in one case we only provide the task and\nthe steps and in the other we provide the task, assumptions/-\ncontext, and the steps.\nAccording to Figure 2, when we provide extra context,\nthe model performs better across all metrics. This shows that\nLLMs are able to adjust the steps based on the context for a\ntask. In Figure 3, we provide an example model output with\nand without the context provided to the model. We can see\nthat when no context is provided to the model, the model\neither provides general steps or selects a speciﬁc approach\n(using vinegar), but when the context is provided the model\nprovides steps that are more speciﬁc to the context.\nTemporal Dependency Results\nWe next verify how well LLMs can predict the temporal de-\npendencies between the steps of the tasks (i.e. the edges of\nthe task graphs). Initially, we compare the ICL, ICL+CoT,\nand SPT approaches on the golden nodes and edges (note\nthat the other two edge prediction approaches are only ap-\nplicable to generated graphs). In this case, for each pair of\nnodes in each of the golden task graphs in the test we ask\nthe model to predict if one step should be done before the\nother one and report the accuracy. The results are reported\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19473\n- - In-Degree Out-Degree Step Proximity\nModel #Seqs Rouge1 Rouge2 RougeL Rouge1 Rouge2 RougeL Rouge1 Rouge2 RougeL\nLinear Order 1 18.3 8.8 17.7 17.3 7.5 16.7 20.2 5.9 18.2\nICL 1 13.6 8.0 13.3 13.6 7.6 13.3 15.2 4.0 13.8\nICL with CoT 1 14.0 6.4 13.5 13.7 5.9 13.2 18.2 4.8 16.2\nSPT 1 18.0 8.9 17.3 17.4 8.2 16.7 19.9 5.6 17.8\nSPT + Linear 1 18.1 8.9 17.4 17.1 8.1 16.4 20.0 5.7 17.9\nLLM Scoring 1 18.4 8.8 17.7 17.2 7.5 16.6 20.2 5.9 18.2\nICL 2 10.3 4.4 10.0 10.1 4.2 9.8 13.6 3.5 12.3\nICL with CoT 2 10.8 3.8 10.3 10.5 3.9 10.0 14.4 3.7 12.8\nSPT 2 13.1 5.8 12.7 12.8 5.5 12.3 15.1 4.2 13.5\nTable 3: The performance of different models for edge evaluation on the generated graphs (#seqs=1 corresponds to the SPT\nmodel and #seqs=2 to the SPT+MultSeq model). The winner is in bold. ICL stands for In-Context Learning, CoT for Chain-of-\nThought, and SPT for Soft Prompt Tuning.\nModel Accuracy\nMajority Class 53.8\nICL 47.5\nICL with CoT 49.6\nSPT 78.6\nTable 4: The performance of different models for edge eval-\nuation on the golden graphs. The Majority Class baseline\nalways predicts no dependency.\nin Table 4. According to the results, both ICL and ICL+CoT\nperform quite poorly, but the performance improves mas-\nsively after soft-prompt tuning, thus showing that temporal\ndependency understanding does not surface out of the box\nfrom LLMs but requires tuning on some data.\nWe next evaluate the performance of the approaches on\nthe generated graphs. To this end, we conduct two exper-\niments in one case we ﬁx the generated steps to those of\nthe SPT model (i.e. there is only one sequence of steps)\nand in the second we ﬁx the generated steps to those of the\nSPT+MultSeq model (i.e. there are multiple sequences of\nsteps). We then use the aforementioned approaches to decide\nthe links. The results are reported in Table 3. According to\nthe results, in the case where we use only one sequence, we\nobserve that the linear order produced by the LLM is quite\na strong baseline: it outperforms the other models for step\nproximity (except for LLM Scoring where the two models\nare on-par) and only slightly underperforms in terms of in-\ndegree and out-degree metrics. We also tried a version of the\nSPT where we combine it with the linear order of the LLM\nby making the following assumption: if the LLM produced\nstep A before step B, then we assume either A should be\ndone before B, or A and B can be done in any order (i.e.\nwe rule out the possibility that B should be done before A).\nEven in this case, we observe that the linear model alone\nstill gives a better performance. Our results show that while\nLLMs are good at generating the sequence of steps for a task\nin the right order, they are not particularly good at individu-\nally deciding which step of a task should be done before the\nother or if two steps can be done in any order.\nTask: Adopt a child\nVisit the official \nwebsite of the child \nwelfare department Generated Task Graph\nFill the application form\nAttend the \ninterview\nComplete \nthe home \nstudy\nSubmit the home study \nreport to the child \nwelfare department\nVisit the child welfare \ndepartment for the \nfinal approval\nSubmit the required \ndocuments\nFigure 4: An example of an LLM-generated task graph.\nQualitative Analysis\nIn Figure 4, we demonstrate an LLM-generated task graph.\nWe can see that many of the steps and the temporal depen-\ndencies are sensible; however, there may also be some prob-\nlems present. For example, one probably needs to complete\nthe home study before attending the interview. More quali-\ntative examples are provided in the Appendix.\nConclusion\nWe studied the power of large language models (LLMs)\nfor structured complex task decomposition (SCTD), i.e. the\nproblem of decomposing a complex real-world task into\nmultiple steps and determining the temporal dependencies\nbetween the steps. We created a probe named TaskLAMA,\ndeveloped metrics for measuring the performance, tested\nvarious task decomposition and temporal dependency pre-\ndiction models and compared against baselines. Our results\nindicate that LLMs are strong in task decomposition. For\npredicting temporal dependencies, however, while they are\nable to produce a good sequence of steps with the right order\nof steps, their ability in predicting pairwise temporal depen-\ndencies still lags behind. Future work can ﬁnd ways of im-\nproving the temporal dependency understanding of LLMs,\nor develop new approaches to improve LLM-based SCTD,\ne.g. by generating the entire task graph at once (see Sak-\naguchi et al. (2021); Madaan et al. (2022)) or by breaking\nthe complex tasks into simpler tasks and then solving those\nsimpler task (see Zhou et al. (2022a); Kazemi et al. (2023b)).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19474\nAcknowledgements\nWe thank Sid Mittal, Deepti Bhatia, Amr Ahmed, and Tania\nBedrax-Weiss for their valuable feedback.\nReferences\nAllen, D.; and Peikoff, A. 2001. Getting Things Done: The\nArt of Stress-Free Productivity.\nAwadallah, A. H.; White, R. W.; Pantel, P.; Dumais, S. T.;\nand Wang, Y .-M. 2014. Supporting complex search tasks.\nIn Proceedings of the 23rd ACM international conference\non conference on information and knowledge management ,\n829–838.\nBrahman, F.; Bhagavatula, C.; Pyatkin, V .; Hwang, J. D.; Li,\nX. L.; Arai, H. J.; Sanyal, S.; Sakaguchi, K.; Ren, X.; and\nChoi, Y . 2023. PlaSma: Making Small Language Models\nBetter Procedural Knowledge Models for (Counterfactual)\nPlanning. arXiv preprint arXiv:2305.19472.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCer, D.; Yang, Y .; Kong, S.-y.; Hua, N.; Limtiaco, N.; John,\nR. S.; Constant, N.; Guajardo-Cespedes, M.; Yuan, S.; Tar,\nC.; et al. 2018. Universal sentence encoder. arXiv preprint\narXiv:1803.11175.\nCheng, J.; Teevan, J.; Iqbal, S. T.; and Bernstein, M. S. 2015.\nBreak it down: A comparison of macro-and microtasks. In\nProceedings of the 33rd Annual ACM Conference on Human\nFactors in Computing Systems, 4061–4064.\nChilton, L. B.; Little, G.; Edge, D.; Weld, D. S.; and Landay,\nJ. A. 2013. Cascade: Crowdsourcing taxonomy creation. In\nProceedings of the SIGCHI Conference on Human Factors\nin Computing Systems, 1999–2008.\nDavison, J.; Feldman, J.; and Rush, A. M. 2019. Com-\nmonsense knowledge mining from pretrained models. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 1173–1178.\nErol, K. 1995. Hierarchical task network planning: formal-\nization, analysis, and implementation. University of Mary-\nland, College Park.\nGuan, L.; Valmeekam, K.; Sreedharan, S.; and Kambham-\npati, S. 2023. Leveraging Pre-trained Large Language Mod-\nels to Construct and Utilize World Models for Model-based\nTask Planning. arXiv preprint arXiv:2305.14909.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In International Con-\nference on Machine Learning, 9118–9147. PMLR.\nHumphreys, T. 2019. Exploring HTN planners through ex-\nample. Game AI Pro, 360: 103–22.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How\ncan we know what language models know? Transactions of\nthe Association for Computational Linguistics, 8: 423–438.\nKazemi, M.; Yuan, Q.; Bhatia, D.; Kim, N.; Xu, X.; Imbra-\nsaite, V .; and Ramachandran, D. 2023a. BoardgameQA: A\nDataset for Natural Language Reasoning with Contradictory\nInformation. arXiv preprint arXiv:2306.07934.\nKazemi, S. M.; Kim, N.; Bhatia, D.; Xu, X.; and Ramachan-\ndran, D. 2023b. Lambada: Backward chaining for automated\nreasoning in natural language. In ACL.\nKokkalis, N.; K ¨ohn, T.; Huebner, J.; Lee, M.; Schulze, F.;\nand Klemmer, S. R. 2013. Taskgenies: Automatically pro-\nviding action plans helps people complete tasks. ACM\nTransactions on Computer-Human Interaction (TOCHI) ,\n20(5): 1–25.\nKuhn, H. W. 1955. The Hungarian method for the assign-\nment problem. Naval research logistics quarterly, 2(1-2):\n83–97.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The\npower of scale for parameter-efﬁcient prompt tuning. arXiv\npreprint arXiv:2104.08691.\nLin, B. Y .; Lee, S.; Khanna, R.; and Ren, X. 2020. Birds have\nfour legs?! numersense: Probing numerical commonsense\nknowledge of pre-trained language models. arXiv preprint\narXiv:2005.00683.\nMadaan, A.; Zhou, S.; Alon, U.; Yang, Y .; and Neubig, G.\n2022. Language models of code are few-shot commonsense\nlearners. arXiv preprint arXiv:2210.07128.\nMehrotra, R.; and Yilmaz, E. 2017. Extracting hierarchies\nof search tasks & subtasks via a bayesian nonparametric ap-\nproach. In Proceedings of the 40th international ACM SIGIR\nconference on research and development in information re-\ntrieval, 285–294.\nNewell, A.; Simon, H. A.; et al. 1972. Human problem solv-\ning, volume 104. Prentice-hall Englewood Cliffs, NJ.\nNye, M.; Andreassen, A. J.; Gur-Ari, G.; Michalewski, H.;\nAustin, J.; Bieber, D.; Dohan, D.; Lewkowycz, A.; Bosma,\nM.; Luan, D.; et al. 2021. Show your work: Scratchpads\nfor intermediate computation with language models. arXiv\npreprint arXiv:2112.00114.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nthe 2014 conference on empirical methods in natural lan-\nguage processing (EMNLP), 1532–1543.\nPetroni, F.; Rockt¨aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;\nMiller, A. H.; and Riedel, S. 2019. Language models as\nknowledge bases? arXiv preprint arXiv:1909.01066.\nSakaguchi, K.; Bhagavatula, C.; Bras, R. L.; Tandon, N.;\nClark, P.; and Choi, Y . 2021. proscript: Partially ordered\nscripts generation via pre-trained language models. arXiv\npreprint arXiv:2104.08251.\nSung, M.; Lee, J.; Yi, S.; Jeon, M.; Kim, S.; and Kang, J.\n2021. Can Language Models be Biomedical Knowledge\nBases? arXiv preprint arXiv:2109.07154.\nTeevan, J.; Iqbal, S. T.; and V on Veh, C. 2016. Supporting\ncollaborative writing with microtasks. In Proceedings of the\n2016 CHI conference on human factors in computing sys-\ntems, 2657–2668.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19475\nWang, Z.; Zhang, H.; Fang, T.; Song, Y .; Wong, G. Y .;\nand See, S. 2022. SubeventWriter: Iterative Sub-event\nSequence Generation with Coherence Controller. arXiv\npreprint arXiv:2210.06694.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWest, P.; Bhagavatula, C.; Hessel, J.; Hwang, J. D.; Jiang, L.;\nBras, R. L.; Lu, X.; Welleck, S.; and Choi, Y . 2021. Sym-\nbolic knowledge distillation: from general language models\nto commonsense models. arXiv preprint arXiv:2110.07178.\nYin, D.; Bansal, H.; Monajatipoor, M.; Li, L. H.; and\nChang, K.-W. 2022. GeoMLAMA: Geo-Diverse Common-\nsense Probing on Multilingual Pre-Trained Language Mod-\nels. arXiv preprint arXiv:2205.12247.\nZhang, L.; Lyu, Q.; and Callison-Burch, C. 2020. Reason-\ning about goals, steps, and temporal ordering with WikiHow.\narXiv preprint arXiv:2009.07690.\nZhang, X.; Ramachandran, D.; Tenney, I.; Elazar, Y .; and\nRoth, D. 2020. Do language embeddings capture scales?\narXiv preprint arXiv:2010.05345.\nZhang, Y .; Jauhar, S. K.; Kiseleva, J.; White, R.; and Roth,\nD. 2021. Learning to decompose and organize complex\ntasks. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2726–2735.\nZhang, Y .; Zhang, M.; Liu, Y .; Tat-Seng, C.; Zhang, Y .; and\nMa, S. 2015. Task-based recommendation on a web-scale.\nIn 2015 IEEE International Conference on Big Data (Big\nData), 827–836. IEEE.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q.; et al.\n2022a. Least-to-most prompting enables complex reasoning\nin large language models. arXiv preprint arXiv:2205.10625.\nZhou, S.; Zhang, L.; Yang, Y .; Lyu, Q.; Yin, P.; Callison-\nBurch, C.; and Neubig, G. 2022b. Show me more details:\nDiscovering hierarchies of procedures from semi-structured\nweb data. arXiv preprint arXiv:2203.07264.\nZhou, X.; Zhang, Y .; Cui, L.; and Huang, D. 2020. Evalu-\nating commonsense in pre-trained language models. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 34, 9733–9740.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19476",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5702084302902222
    },
    {
      "name": "Task (project management)",
      "score": 0.5652218461036682
    },
    {
      "name": "Natural language processing",
      "score": 0.4028726816177368
    },
    {
      "name": "Cognitive science",
      "score": 0.39407047629356384
    },
    {
      "name": "Psychology",
      "score": 0.2531861662864685
    },
    {
      "name": "Systems engineering",
      "score": 0.10823383927345276
    },
    {
      "name": "Engineering",
      "score": 0.10414192080497742
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 7
}