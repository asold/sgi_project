{
  "title": "Transformer-based Network for RGB-D Saliency Detection",
  "url": "https://openalex.org/W3217425316",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1852389517",
      "name": "Wang Yue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097564441",
      "name": "Jia Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989596241",
      "name": "Zhang Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743659395",
      "name": "Li, Yuke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3037591689",
      "name": "Elder, James",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227143521",
      "name": "Lu, Huchuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1993713494",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3035357085",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W20683899",
    "https://openalex.org/W2909381593",
    "https://openalex.org/W2963529609",
    "https://openalex.org/W2887522866",
    "https://openalex.org/W2948300571",
    "https://openalex.org/W3106587394",
    "https://openalex.org/W1976409045",
    "https://openalex.org/W2963868681",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2798857366",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2100470808",
    "https://openalex.org/W2957414648",
    "https://openalex.org/W3010616503",
    "https://openalex.org/W2962741298",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W3104979525",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3034320133",
    "https://openalex.org/W3108812909",
    "https://openalex.org/W2739012628",
    "https://openalex.org/W3108608656",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W2026014384",
    "https://openalex.org/W3035687312",
    "https://openalex.org/W2337762808",
    "https://openalex.org/W1966025376",
    "https://openalex.org/W2804743778",
    "https://openalex.org/W2766315367",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3108421143",
    "https://openalex.org/W3002301267",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2765838470",
    "https://openalex.org/W2801872748",
    "https://openalex.org/W3099871687",
    "https://openalex.org/W3108822985"
  ],
  "abstract": "RGB-D saliency detection integrates information from both RGB images and depth maps to improve prediction of salient regions under challenging conditions. The key to RGB-D saliency detection is to fully mine and fuse information at multiple scales across the two modalities. Previous approaches tend to apply the multi-scale and multi-modal fusion separately via local operations, which fails to capture long-range dependencies. Here we propose a transformer-based network to address this issue. Our proposed architecture is composed of two modules: a transformer-based within-modality feature enhancement module (TWFEM) and a transformer-based feature fusion module (TFFM). TFFM conducts a sufficient feature fusion by integrating features from multiple scales and two modalities over all positions simultaneously. TWFEM enhances feature on each scale by selecting and integrating complementary information from other scales within the same modality before TFFM. We show that transformer is a uniform operation which presents great efficacy in both feature fusion and feature enhancement, and simplifies the model design. Extensive experimental results on six benchmark datasets demonstrate that our proposed network performs favorably against state-of-the-art RGB-D saliency detection methods.",
  "full_text": "Transformer-based Network for RGB-D Saliency Detection\nYue Wang1, Xu Jia1, Lu Zhang1, Yuke Li2, James Elder3, Huchuan Lu1\n1Dalian University of Technology\n2UC Berkeley\n3York University\nAbstract\nRGB-D saliency detection integrates information from both\nRGB images and depth maps to improve prediction of salient\nregions under challenging conditions. The key to RGB-D\nsaliency detection is to fully mine and fuse information\nat multiple scales across the two modalities. Previous ap-\nproaches tend to apply the multi-scale and multi-modal fu-\nsion separately via local operations, which fails to capture\nlong-range dependencies. Here we propose a transformer-\nbased network to address this issue. Our proposed archi-\ntecture is composed of two modules: a transformer-based\nwithin-modality feature enhancement module (TWFEM) and\na transformer-based feature fusion module (TFFM). TFFM\nconducts a sufﬁcient feature fusion by integrating features\nfrom multiple scales and two modalities over all positions\nsimultaneously. TWFEM enhances feature on each scale by\nselecting and integrating complementary information from\nother scales within the same modality before TFFM. We show\nthat transformer is a uniform operation which presents great\nefﬁcacy in both feature fusion and feature enhancement, and\nsimpliﬁes the model design. Extensive experimental results\non six benchmark datasets demonstrate that our proposed\nnetwork performs favorably against state-of-the-art RGB-D\nsaliency detection methods.\nIntroduction\nRGB-D saliency detection aims at discovering and segment-\ning the most salient objects accurately in complex scenes by\nexploiting both RGB image and depth data. It can serve as\nan early vision step for visual tracking (Lee and Kim 2018),\nobject detection (Xu et al. 2015), content-aware image edit-\ning (Wang, Shen, and Ling 2018), and image retrieval (He\net al. 2012).\nRGB images capture both color cues and texture details of\nobjects that may be sufﬁcient to distinguish foreground ob-\njects from background in some simple scenes. However, it\nmay not work for highly cluttered scenes or scenes in which\nforeground and background share similar colors and tex-\ntures. Depth maps, on the other hand, would beneﬁt the dis-\ncovering and segmenting process for these complex scenes.\nIt records the distance of different objects in a scene to the\ncamera, which provides an additional cue to capture the spa-\ntial structure and 3D layout. Therefore, information pro-\nvided by RGB images and depth maps are complementary\nto each other. Effective fusion of information from such two\nmodalities can be critical for accurate saliency detection.\nInspired by the success of deep learning methods, RGB-D\nsaliency detection apply CNN models (Simonyan and Zis-\nserman 2015; He et al. 2016; Huang et al. 2017) to ef-\nfectively extract multi-scale features from RGB image and\ndepth map. High-level features from deep layers represent\ncoarse-scale and semantic information in an image, while\nlow-level features from shallow layers capture ﬁne-scale de-\ntails for precisely localizing boundaries of objects. Due to\nthis complementarity, effectively fusion information across\nscales is also a key to successful RGB-D saliency detection.\nIn addition, CNN-based structures also contribute to the fu-\nsion of multi-scale multi-modal features. However, as a local\noperation, the receptive ﬁeld of CNNs is limited by its kernel\nsize and number of layers. It makes long-range dependency\nmodeling difﬁcult for feature fusion in saliency detection.\nThere have been several methods (Li, Liu, and Ling 2020;\nGongyang et al. 2020; Piao et al. 2019; Chen and Li 2019,\n2018) working on the fusion of multi-modal features across\nmultiple scales with improved RGB-D saliency detection\nperformance. However, most of them conduct feature fu-\nsion in a progressive manner by CNN-based local opera-\ntions, that is, for each time, features from two modalities\nonly fuse with each other within the same scale and at a\nlocal spatial neighborhood. This could cause insufﬁcient in-\ntegration of two modalities because the potential long-range\ndependencies would be restricted. In addition, these prior\napproaches tend to employ different techniques for different\nmodules, e.g, channel-wise concatenation or spatial atten-\ntion for fusion across modalities; U-Net-like progressive in-\ntegration for fusion across scales and recurrent attention for\nfeature enhancement. Such diversity within modules leads\nto a potentially unwieldy complexity in the design of model\narchitecture. This motivates us to design a uniform operation\napplied to multi-modal multi-scale fusion and even feature\nenhancement, which would simplify the model design and\nbeneﬁt performance boost of RGB-D saliency detection.\nInspired by transformer (Vaswani et al. 2017), a non-local\nmodel with self attention and cross attention layer to cap-\nture long-range dependencies in an image, we propose a\nnovel transformer-based network for RGB-D saliency de-\ntection to address the aforementioned limitations. It con-\ntains a Transformer-based Feature Fusion Module (TFFM)\narXiv:2112.00582v1  [cs.CV]  1 Dec 2021\nPinit\nRGB \nDepth GT Conv\nf3r\nf4r\nf5r\nf3d\nf4d\nf5d\nConv\nPot\nfinit\nfot\nL i n i t \nL f i n a l \nfmsr\n+ \nTWFEM TFFM Backbone \nf3e4e5er\nf3e4e5ed\nf5er\nf4er\nf3er\nf3ed\nf4ed\nf5ed\nfmsd\n×2 \n×4 \n+ \n+ \n×4 \n×2 \nn3×c\nn4×c\nn5×c n5×c\nn4×c\nn3×c n3×c\n(n3 + n4 + n5) × c  \nn5×c\nn4×c\nn3×c\nn5×c\nn4×c\nn3×c n3×c\nn3×c\n(n3 + n4 + n5) × c  \nn3×c\nFigure 1: The structure of our overall method. Here, the ×2,×4 in ﬁgure represent the UP×2(·),UP×4(·) in TWFEM.\nto fuse multi-scale multi-modal features globally, and a\nTransformer-based Within-modality Feature Enhancement\nModule (TWFEM) to enhance features across scales within\nthe same modality. Both of these modules are composed\nof only transformer decoders. And instead of only consid-\nering long-range dependencies among all positions within\none feature, our proposed structure takes advantage of trans-\nformer’s non-local essence to simultaneously work on fea-\ntures among different modalities and multiple scales. For\nTFFM, in the self-attention layer of transformer, a guidance\nfor global fusion is generated by reﬁning an initial fused fea-\nture with its self-interaction between each two positions. In\nthe cross-attention layer, features from different modalities\nand scales are taken as embeddings of a sequence. With such\nguidance, each position is equipped with a ﬁnal fused fea-\nture that combines multi-modal and multi-scale information\nat all positions in the image, which is further fed to a clas-\nsiﬁer to predict the saliency probability map. In addition,\nTWFEM is employed within each modality to generate en-\nhanced features fed to TFFM. It allows feature on each scale\nto look through features from all other scales at all positions\nto select and integrate complementary information for en-\nhancement while remains its original resolution. It also sim-\nply produces the initial fused feature for TFFM. In this way,\nwe are able to have a uniform design based on transform-\ners for both feature enhancement and feature fusion, which\nsimplify the model design and boost saliency detection per-\nformance.\nMain contributions of this work are three-folded.\n- We introduce a novel transformer-based framework for\nRGB-D saliency detection which simultaneously and glob-\nally integrates features across modalities and scales.\n- Our RGB-D saliency detector uses only transformers as\na uniform operations for both feature fusion and feature en-\nhancement, which shows the potential of transformer in this\ntask and simpliﬁes the model design.\n- Extensive experiments over six benchmark datasets\nshow that the proposed transformer-based network generally\nperforms favorably against state-of-the-art RGB-D saliency\ndetection methods.\nRelated Work\nRGB-D Saliency Detection\nEarly RGB-D saliency detection methods (Peng et al. 2014;\nCheng et al. 2014; Zhu et al. 2017a; Cong et al. 2016; Zhu\net al. 2017b; Zhu and Li 2017) focuse on hand-crafted low-\nlevel features and thus struggled to handle complex scenes.\nMore recent deep learning approaches extract high-level rep-\nresentations (Qu et al. 2017; Han et al. 2017) and obtain\nmulti-scale features from different levels (Chen, Li, and Su\n2019) to improve the performance. A common CNN ar-\nchitecture for RGB-D saliency detection involves a two-\nstream network to generate multi-scale features from two\nmodalities, followed by several separate fusion processes\nvia local operations. One approach (Li, Liu, and Ling 2020;\nGongyang et al. 2020; Zhang et al. 2020b) ﬁrst fuses multi-\nmodal features separately within each scale and then com-\nbines the fused features across all scales. A second approach\n(Chen and Li 2018; Li et al. 2020; Pang et al. 2020) progres-\nsively merges the multi-modal features from coarse to ﬁne\nscales. Generally these approaches rely on convolutional op-\nerations which allow only information from adjacent spatial\npositions to contribute to feature fusion at one time. To com-\npensate for this limitation, some methods employ extra fea-\nture enhancement module. They design different structures\nfor multi-scale feature fusion, multi-modal feature fusion\nand feature enhancement separately, which complicates the\nnetwork and still fail to simultaneously consider long-term\ndependencies among all scales, modalities and positions. For\nexample, Piao et al. (2019) use a recurrent module to en-\nhance the fused features, while Liu, Zhang, and Han (2020)\nuse attention to enhance features across modalities before\nfusion. Exceptions to the two-stream network approach do\nexist. Piao et al. (2020) use only RGB image as input to the\nsingle-stream network, and Zhao et al. (2020) use a single-\nstream network that combines the RGB and depth data di-\nrectly from the input. But they also use a progressive ap-\nproach for multi-scale fusion with local operations.\nTransformers\nThe transformer model is ﬁrst proposed by Vaswani et al.\n(2017) for the machine translation task. It has an encoder-\ndecoder structure involving two kinds of attention. The\n+ \nQ\n+ \nV\nK V\nQ\n+ \nK\nAdd & Norm (RL)\nAttention (A)\nAttention (A)\nFFN\nAdd & Norm (RL)\nAdd & Norm (RL)\nx ynx×c ny×c\nosanx×c\nocanx×c\noonx×c\nPositional\nEncoding for x\nPositional\nEncoding for y\nSA \nLayer\nCA \nLayer\nFF \nLayer\nFigure 2: The structure of the transformer decoder module.\ntransformer encoder contains a self-attention layer, while\nthe transformer decoder has both self-attention and cross-\nattention layers. The transformer model is adopted by Carion\net al. (2020) to exploit non-local relationships for end-to-end\nobject detection. A pixel-level feature extracted by a CNN-\nbased backbone is reﬁned by the self-attention layer in the\ntransformer encoder module. The transformer decoder mod-\nule then generates object-level feature from the reﬁned en-\ncoder feature over all positions in an image. Zhu et al. (2020)\nand Zhang et al. (2020a) show that the transformer model is\ncapable of using multi-scale pixel-level features simultane-\nously to achieve better performance for object detection and\nsegmentation tasks. The transformer model is also used to\ntransform multi-modal features for image caption and video\nretrieval (Li et al. 2019; Gabeur et al. 2020).\nIn this paper, we apply the structure of transformer to\nachieve simultaneous multi-modal and multi-scale feature\nfusion over all spatial positions for RGB- D saliency detec-\ntion We also show that transformer offers a unifying archi-\ntecture which can also be applied for within-modality fea-\nture enhancement before fusion.\nMethod\nIn this section, we introduce the overall structure of our\ntransformer-based network for RGB-D saliency detection.\nBoth our transformer-based feature fusion module (TFFM)\nand transformer-based within-modality feature enhance-\nment module (TWFEM) are based upon a common trans-\nformer decoder template. We review this template ﬁrst, and\nthen go into the speciﬁcs of the TFFM and TWFEM. The\noverall architecture is shown in Fig.1.\nTransformer Decoder Module\nThe original transformer model (Vaswani et al. 2017) con-\nsists of a transformer encoder and a transformer decoder.\nBoth of the TWFRM and TFFM only apply the structure\nof transformer decoder module. Here, we ﬁrst introduce the\nattention module, the core of the transformer decoder mod-\nule. Given a query Q ∈RnQ×c, a key K ∈RnP ×c, and\na value V ∈RnP ×c, the dot-product attention A is com-\nputed to output a weighted sum of V, with weights given by\na compatibility function of Qwith K(Shen et al. 2021):\nA(Q,K,V ) =ρ(QKT )V (1)\nHere nQ is the number of positions in featureQ. K,V share\nthe same number of positions nP . c is the common fea-\nture dimensionality for Q,K,V . ρ(·) represents the softmax\nfunction along each row (see (Shen et al. 2021; Vaswani\net al. 2017) for details). The output of this attention mod-\nule has the same size RnQ×c as Q.\nIn this paper, we follow Vaswani et al. (2017) and apply\nthe same structure of transformer decoder module. Trans-\nformer decoder requires two input features: x∈Rnx×c and\ny ∈Rny×c. It is able to reﬁne xby selecting and integrat-\ning relevant information fromybased on the global relation-\nship between xand yover all positions. It is implemented by\nthree sub-layers: a self-attention (SA) layer, a cross-attention\n(CA) layer and a feedforward (FF) layer. Both self-attention\nand cross-attention layers contain an attention module (A)\nand a Residual connection and Layer normalization block\n(RL). Feedforward layer contains a feedforward network\n(FFN) with two linear transformations and a ReLU activa-\ntion in between, and a RL block.\nosa = SA(x) =RL(A(˜x,˜x,x))\noca = CA(osa,y) =RL(A(˜osa,˜y,y))\noo = FF(oca) =RL(FFN (oca))\n(2)\nosa,oca,oo ∈Rnx×c are the outputs of self-attention, cross-\nattention, feedforward layers and have the same size with\ninput x. For each attention module, Qand K are added by\ntheir corresponding positional encodings as (Carion et al.\n2020) and represented as ˜x,˜osa,˜y. It help to consider the\nspatial information between any two positions in Qand K\nfor learning the attention weight, which is essential espe-\ncially when Q and K have different number of positions.\nHere, the self-attention layer uses xas Q,K,V in attention\nmodule, which reﬁnes x based on its self-relationship be-\ntween any two positions to achieve a better queryosa for the\ncross-attention layer. The cross-attention layer usesosa as Q\nand yas K,V in attention module, which looks through all\npositions of yto extract useful information for osa to get its\nfurther reﬁned feature oca. The ﬁnal output oo is then calcu-\nlated from oca by feedforward layer. We can name the above\nprocesses as one transformer decoder block ( TD) and have\noo = TD(x,y). The meanings of xand y are speciﬁed in\nTFFM and TWFEM. And we show the overall structure of\ntransformer decoder in Fig.2.\nTo save the memory and computational costs, we employ\nan efﬁcient attention implementation (Shen et al. 2021) to\nreplace the original attention module (A) in transformer de-\ncoder module. Instead of multiplying Qand K ﬁrst to form\nan nQ ×nP matrix, Efﬁcient attention (EA) ﬁrst multiples\nKT and V to form a c×cmatrix:\nEA(Q,K,V ) =ρq(Q)(ρk(K)T V) (3)\nwhere ρq(·) and ρk(·) are the softmax function along each\nrow of Qand each column of Krespectively. Sincecis nor-\nmally much smaller than the number of positionsn= h×w,\nh,w are the height and width for feature, it efﬁciently de-\ncreases the computational costs and still aggregates value\nfeatures from all positions in a global way. Besides, the ef-\nﬁcient attention we employ also has the function of multi-\nhead as (Vaswani et al. 2017).\nfot-1\nf3er f4er f5er\nSelf Attentionfsat\nf3ed f4ed f5ed\nf3e4e5er f3e4e5ed\nf3e4e5erd\nCross Attentionfcat\nFeedforwardfot\nt-th Fusion BlockTFt(·) \nf4\nf3 f5e\nSelf Attentionf4sa\nf35e\nCross Attentionf4ca\nFeedforwardf4e\nTE(·) \n(a) (b) \nn3×c n4×c n5×c n3×c n4×c n5×c\nn3×c n3×c n3×c n3×c\n(n3 + n4 + n5) × c (n3 + n4 + n5) × c\n(n3 + n4 + n5 + n3 + n4 + n5) × c n3×c n5×c (n3 + n5) × c\nn4×c n4×c n4×c n4×c\nFigure 3: (a) The structure of our t-th transformer-based feature fusion block TFt(·) (Eq. 5). (b) The structure of our\ntransformer-based feature enhancement block TE(·), we use the process of generating the enhanced f4e (Eq. 9) as example.\nTransformer-based Feature Fusion Module\nWe apply a common two-stream network as existing RGB-\nD saliency detection methods (Piao et al. 2019; Zhang et al.\n2020b) where each stream uses VGG16 (Simonyan and\nZisserman 2015) as backbone to extract three-scale fea-\ntures for one modality. Therefore, how to automatically fuse\nmulti-modal multi-scale features becomes the key to RGB-\nD saliency detection. Different from most of the previous at-\ntempts that separately process multi-modal fusion and multi-\nscale fusion via local operations, we propose a transformer-\nbased feature fusion module (TFFM). It manages to simul-\ntaneously fuse multi-scale multi-modal features in a global\nway, where the fused feature at each positions is calculated\nby a adaptively weighted combination of information from\nall features at all positions. Following (Carion et al. 2020),\nour TFFM also contains several fusion blocks which manage\nto reﬁne the fused feature gradually.\nWe apply the structure of transformer decoder for each\nfusion block. As mentioned above, the transformer decoder\nmodule requires two inputs, xand y, it selects useful infor-\nmation from yto reﬁne x. For our global feature fusion, in-\nput yshould contains information from all scales and modal-\nities. We obtain and denote features of three scales from\nRGB and depth streams asfr\nie ∈Rni×c and fd\nie ∈Rni×c re-\nspectively (see TWFEM), wherei∈{3,4,5}, ni = hi ×wi.\nNote that these multi-scale features have different ni,hi,wi\nbut share the the same dimensionality c. We do not need to\nupsample all these features to the same size to fuse features\nwith different resolutions together. Instead, we directly con-\ncatenate {f3e,f4e,f5e}r for RGB stream to get fr\n3e4e5e ∈\nRn345×c, where n345 = n3 + n4 + n5, and {f3e,f4e,f5e}d\nfor depth stream to get fd\n3e4e5e ∈Rn345×c. This practice is\nmore suitable for our TFFM for two reasons. Firstly, in each\nfusion block, we need to have access to all multi-scale multi-\nmodal features. Upsampling these features to a higher res-\nolution when using them repeatedly increases the memory\nand computational costs, while our processing would save\nthese resources by remaining features to their original sizes.\nSecondly, objects in a scene have large variations in scales.\nBy maintaining multi-scale features to their original resolu-\ntions, our method manage to cover objects over all scales\nsince features in lower resolution will still help predict ob-\njects with larger sizes, while features in higher resolution\nwill continue detecting small objects and boundries. After\nthat, we concatenate two-stream featuresfr\n3e4e5e and fd\n3e4e5e\ntogether as frd\n3e4e5e ∈R(n345×2)×c. frd\n3e4e5e is used as y to\nprovide abundant information from all scales and modali-\nties.\nInput xfor each fusion block is a guidance of global fea-\nture fusion and have the same size with the output fused fea-\nture. Carion et al. (2020) use a stack of transformer decoder\nblocks to generate object-level feature from pixel-level fea-\nture, its xshould have the same size with the output object-\nlevel feature. However, since there is no object-level prior\ninformation, xfor its ﬁrst block has to be set as a zero tensor\nso that the object-level feature has to be generated from zero.\nWhile for our RGB-D saliency detection, an initial fused\nfeature finit ∈Rn3×c with a higher resolution can be eas-\nily obtained (see TWFEM) and used as xin the ﬁrst fusion\nblock to provide a better guidance for feature fusion. With\nthe required finit and frd\n3e4e5e, the overall process of the ﬁrst\ntransformer-based fusion block is as follows:\nfsa = SA(finit)\nfca = CA(fsa,frd\n3e4e5e)\nfo = FF(fca)\n(4)\nWith finit as query, we ﬁrst apply a self-attention layer to re-\nﬁne itself for achieving a better guidancefsa for fusion. And\nthen in the cross-attention layer, fsa on each position is re-\nﬁned by looking through all positions in frd\n3e4e5e to generate\na better fused feature fca. Therefore, for each positions in\nfca, it contains information which is adaptively selected and\nfused from all positions in multi-scale multi-modal features.\nThe feedforward layer is applied to get the output fused fea-\nture fo ∈Rn3×c which has the same size as finit. Note that\nthe positional encoding offrd\n3e4e5e is obtained by concatenat-\ning positional encodings of all multi-scale multi-modal fea-\ntures in sequence. Even though features in frd\n3e4e5e have dif-\nferent resolutions, this transformer-based block would still\nconsider the spatial information during fusion.\nIn the following fusion blocks, input xis replaced by the\noutput fo from a previous block. We name this transformer-\nbased fusion block as TF and the process of t-th fusion\nblock can be written as:\nft\no = TFt(ft−1\no ,frd\n3e4e5e) (5)\nwhere t∈[1,...,T ], T denotes the number of the straight-\nforward fusion blocks in our TFFM. ft\no is the output of t-th\nblock. f0\no = finit is the input xfor the ﬁrst block, TFt(·)\nrepresents the process in t-th fusion block.\nThe proposed TFFM manages to fuse multi-scale multi-\nmodal simultaneously in a global way. Meanwhile, by us-\ning several fusion blocks, our method is able to generate\nthe ﬁnal fused feature gradually with information from all\nscales, modalities and positions. The architecture of one\ntransformer-based feature fusion block is shown in Fig.3 (a).\nWith ft\no ∈Rn3×c output from each block, it is ﬁrst re-\nshaped back to form Rc×h3×w3 . Then it is used to obtain\nsaliency prediction Pt\no by one convolutional layer as a clas-\nsiﬁer. For better supervision on all the fusion processes in all\nT fusion blocks, ft\no from all blocks are used to produce T\nsaliency maps. The overall prediction loss is calculated by:\nLfinal =\nT∑\nt=1\nLbce(Pt\no,S) (6)\nwhere Lbce stands for the binary cross-entropy loss, Sis the\nsaliency ground-truth. And the convolutional layer is shared\namong all T blocks for predicting the saliency maps from all\nT fused features.\nTransformer-based Within-modality Feature\nEnhancement Module\nTo further improve the performance, a TWFEM is proposed\nto generate the enhanced multi-scale features{f3e,f4e,f5e}\nfor each modality separately and a simple initial fused fea-\nture finit before TFFM. Firstly, feature on each scale are\nenhanced by further extracting and integrating complemen-\ntary information from other scales within the same modality\nwhile remains its original resolution. It is also implemented\nas stacks of transformer decoders which indicates the ﬂexi-\nbility of transformer decoder and uniformity of the proposed\nframework. Secondly, finit is computed by simply fusing\nenhanced multi-scale features of two modalities. It is used\nin TFFM as a better guidance for global feature fusion.\nWe ﬁrst show how to generate {f3e,f4e,f5e}for each\nmodality separately. For each stream, we can directly ex-\ntract three-scale features from its backbone network sepa-\nrately and apply one convolution layer for each feature to\nmake them have the same dimensionality c, which are de-\nnoted as fr\ni ∈Rc×hi×wi and fd\ni ∈Rc×hi×wi for features\nfrom RGB stream and depth stream, where i ∈ {3,4,5}.\nThis is a simple way to extract the feature on each scale\nfor each modality, while these features can be further en-\nhanced by automatically selecting and integrating comple-\nmentary information from other scales since features with\ndifferent scales contain different levels of information. Fea-\ntures from coarser scales contain more semantic informa-\ntion, while features from ﬁner scales contain more detailed\ninformation. Therefore, the structure of transformer decoder\nis suitable for feature enhancement across scales. It allows\nthe enhanced features to contain richer information from dif-\nferent scales while remain their original resolutions.\nWe reshape all features to form fi ∈Rni×c and then en-\nhance features within the same modality from coarse scale\nto ﬁne scale in a progressive way. It also uses the structure\nof transformer decoder. For eachfi, we enhance it by setting\nitself as input x, the concatenation of features from its ﬁner\nscales (fj where j > i,j∈[3,4,5]) and the enhanced fea-\ntures from its coarser scales ( fle where l < i,l∈[3,4,5])\nas input y. It means for each stream, we ﬁrst enhance the\ncoarsest-scale feature f5 by f3,f4 from the same modality\nwhile remaining f5’s original resolution. The enhancement\nprocess consists of a self-attention layer to reﬁnef5 by itself,\na cross-attention layer to extract and integrate complemen-\ntary information from f3, f4, and a feedforward layer:\nf5sa = SA(f5)\nf5ca = CA(f5sa,f34)\nf5e = FF(f5ca)\n(7)\nwhere f34 ∈ R(n3+n4)×c is formed by concatenating f3\nand f4. f5e ∈Rn5×c is the enhanced feature of f5. Here,\nwe also select and integrate useful information in a global\nway where features from all spatial positions in f3 and f4\ncan be contributed to each position in f5e. We name it as a\ntransformer-based enhancement block ( TE) and the above\nprocess for enhancing f5 can be represented as:\nf5e = TE(f5,f34) (8)\nSimilarly, we can progressively compute enhanced fea-\ntures for the other two scales. For f4e, its complementary\ninformation is extracted from f3 and the enhanced f5e for\nintegration, and for f3e, it is from the enhanced f4e and f5e:\nf4e = TE(f4,f35e) (9)\nf3e = TE(f3,f4e5e) (10)\nwhere f35e ∈R(n3+n5)×c is the concatenation of f3 and\nf5e. f4e5e ∈ R(n4+n5)×c is the concatenation of f4e and\nf5e. The structure of our TE(·) is shown in Fig.3 (b).\nThen, with the enhanced feature fie ∈ Rni×c, i ∈\n{3,4,5}from both two modalities, we start to generate the\nsimple initial fused featurefinit by a simple way to ﬁrst fuse\nmulti-scale features by addition for each modality, then fuse\nmulti-modal features:\nfms = f3e + UP×2(f4e) +UP×4(f5e)\nfinit = fr\nms + fd\nms\n(11)\nwhere UP×2(·), UP×4(·) stand for ×2, ×4 upsample func-\ntion, which ﬁrst reshape fie to form Rc×hi×wi, then apply\nthe corresponding bilinear upsampling and reshape it back to\nform Rn3×c as f3e. For getting a better guidance for TFFM,\nwe reshape finit back to form Rc×h3×w3 and apply one con-\nvolutional layer on it as a classiﬁer to get an initial saliency\nprediction map Pinit. Pinit can be supervised by the saliency\nground-truth S:\nLinit = Lbce(Pinit,S) (12)\nExperiments\nDatasets and Evaluation Metrics\nWe evaluate our proposed method on six widely used\nRGB-D saliency datasets including NJUD (Ju et al. 2014),\nNLPR (Peng et al. 2014), STERE (Niu et al. 2012),\nDES (Cheng et al. 2014), SIP (Fan et al. 2019), and DUT-\nD (Piao et al. 2019). Following Piao et al. (2019), we use the\nselected 800 images from DUT-D, 1485 images from NJUD\nand 700 images from NLPR for training our network. We\nTable 1: Results on different datasets. We highlight the best three results in each column inbold, bold and underline.\nDataset Metric CPFP DMRA ICNet DANet CMWNet S2MA A2dele PGAR SSF HDFNet Ours\nDES\nMAE 0.038 0.030 0.027 0.028 0.022 0.021 0.029 0.026 0.026 0.020 0.018\nFm 0.829 0.867 0.889 0.891 0.900 0.906 0.868 0.880 0.883 0.919 0.921\nSm 0.872 0.899 0.920 0.905 0.934 0.941 0.883 0.913 0.903 0.932 0.932\nEm 0.927 0.944 0.959 0.961 0.967 0.974 0.919 0.939 0.946 0.973 0.975\nDUT-D\nMAE 0.100 0.048 0.072 0.047 0.056 0.044 0.042 0.035 0.034 0.040 0.030\nFm 0.735 0.883 0.830 0.884 0.866 0.885 0.891 0.914 0.914 0.892 0.923\nSm 0.749 0.887 0.852 0.889 0.887 0.902 0.884 0.919 0.914 0.905 0.924\nEm 0.815 0.930 0.901 0.929 0.922 0.935 0.929 0.950 0.951 0.938 0.954\nNJUD\nMAE 0.053 0.051 0.052 0.046 0.046 0.053 0.051 0.042 0.043 0.037 0.036\nFm 0.837 0.872 0.868 0.877 0.880 0.865 0.873 0.893 0.885 0.894 0.894\nSm 0.878 0.885 0.894 0.897 0.903 0.894 0.868 0.909 0.898 0.911 0.913\nEm 0.900 0.920 0.913 0.926 0.923 0.916 0.916 0.935 0.934 0.934 0.932\nNLPR\nMAE 0.038 0.031 0.028 0.031 0.029 0.030 0.028 0.024 0.026 0.027 0.024\nFm 0.818 0.855 0.870 0.865 0.859 0.853 0.878 0.885 0.875 0.878 0.895\nSm 0.884 0.898 0.922 0.908 0.917 0.915 0.895 0.930 0.913 0.916 0.924\nEm 0.920 0.942 0.945 0.945 0.940 0.942 0.945 0.955 0.951 0.948 0.960\nSIP\nMAE 0.064 0.088 0.069 0.054 0.062 0.057 0.070 0.055 0.057 0.050 0.049\nFm 0.819 0.815 0.836 0.864 0.851 0.854 0.829 0.854 0.850 0.863 0.866\nSm 0.850 0.800 0.854 0.878 0.867 0.872 0.826 0.876 0.867 0.878 0.885\nEm 0.899 0.858 0.900 0.917 0.909 0.913 0.889 0.912 0.913 0.920 0.921\nSTERE\nMAE 0.051 0.050 0.045 0.047 0.043 0.051 0.044 0.041 0.044 0.039 0.042\nFm 0.830 0.869 0.865 0.858 0.869 0.855 0.877 0.880 0.862 0.879 0.880\nSm 0.879 0.874 0.902 0.892 0.905 0.890 0.876 0.907 0.889 0.906 0.901\nEm 0.907 0.926 0.926 0.926 0.930 0.926 0.928 0.937 0.927 0.937 0.934\nModel Size (MB)291.9 238.8 312.2 106.8 342.9 346.8 60.1 64.9 131.8 176.9 129.9\nImage GT Depth Ours HDFNet SSF PGAR A2dele CDNet S2MA CMWNet DANet ICNet DMRA CPFP \nFigure 4: Visual comparison between our method and the state-of-the-art methods.\nthen evaluate our model on the remaining images in these\nthree datasets and the other three datasets.\nWe adopt four widely used evaluation metrics for quanti-\ntative evaluation including F-measure ( Fm) (Achanta et al.\n2009), mean absolute error (MAE) (Borji et al. 2015), S-\nmeasure (Sm) (Fan et al. 2017) and E-measure ( Em) (Fan\net al. 2018). In this paper, we report the mean F-measure\nvalue with the adaptive threshold asFm. For MAE, the lower\nvalue means the method is better, while for all other metrics,\nthe higher value means the method is better.\nImplementation Details\nWe apply PyTorch toolbox for implementation using one\nGeForce RTX 2080Ti GPU with 11 GB memory. The pa-\nrameters of two-stream backbone networks are both initial-\nized by VGG16 (Simonyan and Zisserman 2015). The input\nRGB images and depth images are all resized to 256 ×256.\nWe train our method for 150k iterations by ADAM opti-\nmizer (Kingma and Ba 2015). The initial learning rate, batch\nsize, number of channel cand number of blocks T in TFFM\nare set to 1e-4, 6, 128 and 4. During evaluation, the output\nprediction map of the last transformer-based fusion block is\nused as the ﬁnal saliency prediction map.\nComparison with state-of-the-art methods\nTo verify the effectiveness of our proposed transformer-\nbased network for RGB-D saliency detection, we com-\npare our performance and model size with ten state-of-art\nRGB-D saliency detection methods including: DANet (Zhao\net al. 2020), SSF (Zhang et al. 2020b), S2MA (Liu,\nZhang, and Han 2020), ICNET (Li, Liu, and Ling 2020),\nCMWNet (Gongyang et al. 2020), DMRA (Piao et al.\n2019), CPFP (Zhao et al. 2019), A2dele (Piao et al. 2020),\nPGAR (Chen and Fu 2020), and HDFNet (Pang et al. 2020)\nin Table 1. For a fair comparison, we report the results\nand model sizes with VGG16 as backbone for DANet and\nHDFNet.\nIt demonstrates that our proposed transformer-based net-\nwork generally performs favorably against all the listed\nstate-of-the-art methods. It beneﬁts from our TFFM which\nrequires a global and adaptive response of information from\nall scales, all modalities and all spatial positions for our ﬁ-\nnal fused feature at each position, and our TWFEM which\nenhances feature on each scale with complementary infor-\nTable 2: Ablation study on our proposed transformer-based network. We highlight the best result in each column inbold.\nIndex model STERE NLPR DUT-DMSMMF TWFEM TFFMMAE Fm Sm Em MAE Fm Sm Em MAE Fm Sm Em\n1 ✓ 0.054 0.846 0.865 0.9150.031 0.863 0.901 0.9450.045 0.889 0.891 0.9322 ✓ ✓ 0.045 0.861 0.888 0.9250.028 0.877 0.913 0.9500.037 0.901 0.908 0.9423 ✓ ✓ 0.043 0.876 0.894 0.9310.027 0.888 0.916 0.9530.035 0.912 0.914 0.9494 ✓ w/o prog ✓ 0.042 0.873 0.901 0.930 0.025 0.887 0.922 0.9550.032 0.913 0.921 0.9485 ✓ ✓ ✓ 0.042 0.880 0.901 0.934 0.024 0.895 0.924 0.960 0.030 0.923 0.924 0.954\nTable 3: Comparison of setting different T for the entangled architecture. We highlight the best result in each column inbold.\nSTERE NLPR DUT-D\nmodel MAE Fm Sm Em MAE Fm Sm Em MAE Fm Sm Em\nT = 0 0.045 0.861 0.888 0.925 0.028 0.877 0.913 0.950 0.037 0.901 0.908 0.942\nT = 2 0.041 0.876 0.900 0.933 0.025 0.884 0.920 0.952 0.031 0.917 0.920 0.951\nT = 4 0.042 0.880 0.901 0.934 0.024 0.895 0.924 0.960 0.030 0.923 0.924 0.954\nT = 5 0.040 0.880 0.905 0.934 0.025 0.891 0.921 0.956 0.029 0.923 0.926 0.956\nmation from other scales within the same modality.\nMoreover, since we only use the transformer-based struc-\nture with the efﬁcient attention for both feature fusion and\nenhancement instead of designing diverse and complicated\nstructures, the model size of our network is smaller than\nalmost all the other two-steam methods except for PGAR.\nIt is because PGAR uses a much lighter network as back-\nbone than VGG16 for depth stream which only contains\nfour convolution layers. Our model size is also larger than\nA2dele and DANet since they apply the one-stream network.\nA2dele only has RGB image as input and DANet combines\nRGB images and depth maps together as one input of one-\nstream network. We also present some qualitative examples\nin Fig.4. It shows that our method not only precisely predicts\nthe whole saliency objects ( e.g. row 1), but also accurately\ndetects the boundaries of saliency regions (e.g. row 4).\nAblation Study\nEffectiveness of each transformer-based module. To\ndemonstrate the impact of each part in our overall net-\nwork, we conduct ablation studies by evaluating ﬁve subset-\nmodels and present results on three RGB-D datasets. As\nshown in Table 2, model1 which only uses MSMMF means\na simple multi-scale multi-modal fusion method using Eq.11\nin which fie is replaced by fi since we do not have the\nenhanced features. And the following models add TWFEM\nand TFFM separately to see the impact of each transformer-\nbased module. Model 5 is our overall performance by using\nboth TWFEM and TFFM. For model 4, we also use both\nTWFEM and TFFM, but in its TWFEM, we do not use the\nprogressive way to get enhanced features f4e and f3e as in\nEq.9, Eq.10. Instead, we directly use f3,f5 to get f4e and\nf4,f5 to get f3e. We show the impact of our progressive en-\nhanced way by the comparison between model 4 and model\n5. We also include some visual examples in Fig.5.\nIt indicates that for model 1 which only fuses multi-scale\nand multi-modal features in a simple way for saliency pre-\ndiction may perform well on some simple situations. How-\never, without a global and adaptive fusion, it is not suit-\nImage GTDepth model 1\n model 2 model 3 model 4\n model 5\nFigure 5: Visual examples for ablation study.\nable for complex scenes which causes poor performance on\ndatasets like STERE. Our TFFM can signiﬁcantly improve\nthe performance since it allows this global and adaptive fu-\nsion, which also improves the ability of generalization for\ncomplex situations. Our TWFEM enhances features with\ncomplimentary information across scales which not only\ngains improvements for model 2 with only a simple fusion,\nbut also provides features with more abundant information\nfor TFFM. We also show that the progressively enhanced\nfeature from coarse to ﬁne scale in our TWFEM can provide\nfurther improvements for saliency prediction.\nEffectiveness of T. We also discuss the chosen num-\nber of blocks in our TFFM by conducting the experiments\nto set T = 0,2,4,5 for the fusion blocks in our overall\ntransformer-based network (by setting T = 0 is equal to\nmodel 2 in Table 2). We present the comparisons on three\ndatasets in Table 3. Since the initial fused feature from our\nTWFEM provides useful information related to saliency pre-\ndiction for our TFFM as the initial guidance. By setting dif-\nferent T may not have a signiﬁcant inﬂuence on the ﬁnal\nresult predicted by the output feature from the last block,\nespecially when T is large enough. However, using a small\nnumber of blocks may still affect the performance on some\ndatasets such as the NLPR dataset. Considering both the per-\nformance and the efﬁciency of our model, we choose T = 4\nfor our transformer-based network.\nConclusion\nIn this paper, we propose a transformer-based network for\nRGB-D saliency detection, which uses transformer as a\nuniform operation for both feature enhancement and fea-\nture fusion. It consists of two stages: Firstly, a transformer-\nbased single-modal feature enhancement module (TWFEM)\nenhances feature on each scale by selecting and integrat-\ning complementary information from other scales within\nthe same modality. It also generates a simple initial fused\nfeature. Secondly, our transformer-based fusion module\n(TFFM) uses the initial fused feature as a guidance to si-\nmultaneously fuse multi-scale, multi-modal features. It gen-\nerates a ﬁnal fused feature that at each position, calculates\nby an adaptive combination of information from all scales\nand both modalities which captures the long-range depen-\ndencies. Evaluation results on six RGB-D datasets demon-\nstrate the effectiveness of our method by performing favor-\nably against state-of-the-art RGB-D saliency methods.\nReferences\nAchanta, R.; Hemami, S.; Estrada, F.; and Susstrunk, S.\n2009. Frequency-tuned Salient Region Detection. In 2009\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 1597–1604. IEEE.\nBorji, A.; Cheng, M.-M.; Jiang, H.; and Li, J. 2015. Salient\nObject Detection: A Benchmark. IEEE Transactions on Im-\nage Processing, 24(12): 5706–5722.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end Object Detection\nwith Transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChen, H.; and Li, Y . 2018. Progressively Complementarity-\naware Fusion Network for RGB-D Salient Object Detection.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 3051–3060.\nChen, H.; and Li, Y . 2019. Three-stream Attention-aware\nNetwork for RGB-D Salient Object Detection. IEEE Trans-\nactions on Image Processing, 28(6): 2825–2835.\nChen, H.; Li, Y .; and Su, D. 2019. Multi-modal Fusion Net-\nwork with Multi-scale Multi-path and Cross-modal Interac-\ntions for RGB-D Salient Object Detection. Pattern Recog-\nnition, 86: 376–385.\nChen, S.; and Fu, Y . 2020. Progressively Guided Alter-\nnate Reﬁnement Network for RGB-D Salient Object Detec-\ntion. In European Conference on Computer Vision, 520–\n538. Springer.\nCheng, Y .; Fu, H.; Wei, X.; Xiao, J.; and Cao, X. 2014.\nDepth Enhanced Saliency Detection Method. In Proceed-\nings of international conference on internet multimedia\ncomputing and service, 23–27.\nCong, R.; Lei, J.; Zhang, C.; Huang, Q.; Cao, X.; and Hou,\nC. 2016. Saliency Detection for Stereoscopic Images Based\non Depth Conﬁdence Analysis and Multiple Cues Fusion.\nIEEE Signal Processing Letters, 23(6): 819–823.\nFan, D.-P.; Cheng, M.-M.; Liu, Y .; Li, T.; and Borji, A.\n2017. Structure-measure: A New Way to Evaluate Fore-\nground Maps. In Proceedings of the IEEE International\nConference on Computer Vision, 4548–4557.\nFan, D.-P.; Gong, C.; Cao, Y .; Ren, B.; Cheng, M.-\nM.; and Borji, A. 2018. Enhanced-alignment Measure\nfor Binary Foreground Map Evaluation. arXiv preprint\narXiv:1805.10421.\nFan, D.-P.; Lin, Z.; Zhao, J.-X.; Liu, Y .; Zhang, Z.; Hou,\nQ.; Zhu, M.; and Cheng, M.-M. 2019. Rethinking RGB-D\nSalient Object Detection: Models, Datasets, and Large-scale\nBenchmarks. arXiv preprint arXiv:1907.06781.\nGabeur, V .; Sun, C.; Alahari, K.; and Schmid, C. 2020.\nMulti-modal Transformer for Video Retrieval. In Euro-\npean Conference on Computer Vision (ECCV), volume 5.\nSpringer.\nGongyang, L.; Zhi, L.; Linwei, Y .; Yang, W.; and Haibin, L.\n2020. Cross-modal Weighting Network for RGB-D Salient\nObject Detection. In European Conference on Computer\nVision. Springer.\nHan, J.; Chen, H.; Liu, N.; Yan, C.; and Li, X. 2017. CNNs-\nbased RGB-D Saliency Detection via Cross-view Transfer\nand Multiview Fusion. IEEE transactions on cybernetics,\n48(11): 3171–3183.\nHe, J.; Feng, J.; Liu, X.; Cheng, T.; Lin, T.-H.; Chung, H.;\nand Chang, S.-F. 2012. Mobile Product Search with Bag of\nHash Bits and Boundary Reranking. In 2012 IEEE Confer-\nence on Computer Vision and Pattern Recognition, 3005–\n3012. IEEE.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. InProceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\n770–778.\nHuang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger,\nK. Q. 2017. Densely Connected Convolutional Networks.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 4700–4708.\nJu, R.; Ge, L.; Geng, W.; Ren, T.; and Wu, G. 2014. Depth\nSaliency based on Anisotropic Center-surround Difference.\nIn 2014 IEEE International Conference on Image Process-\ning (ICIP), 1115–1119. IEEE.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In ICLR.\nLee, H.; and Kim, D. 2018. Salient Region-based Online\nObject Tracking. In 2018 IEEE Winter Conference on Ap-\nplications of Computer Vision (WACV), 1170–1177. IEEE.\nLi, C.; Cong, R.; Piao, Y .; Xu, Q.; and Loy, C. C. 2020.\nRGB-D Salient Object Detection with Cross-modality Mod-\nulation and Selection. InEuropean Conference on Computer\nVision, 225–241. Springer.\nLi, G.; Liu, Z.; and Ling, H. 2020. ICNet: Information Con-\nversion Network for RGB-D based Salient Object Detection.\nIEEE Transactions on Image Processing, 29: 4873–4884.\nLi, G.; Zhu, L.; Liu, P.; and Yang, Y . 2019. Entangled\nTransformer for Image Captioning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n8928–8937.\nLiu, N.; Zhang, N.; and Han, J. 2020. Learning Selective\nSelf-mutual Attention for RGB-D Saliency Detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 13756–13765.\nNiu, Y .; Geng, Y .; Li, X.; and Liu, F. 2012. Leveraging Stere-\nopsis for Saliency Analysis. In 2012 IEEE Conference on\nComputer Vision and Pattern Recognition, 454–461. IEEE.\nPang, Y .; Zhang, L.; Zhao, X.; and Lu, H. 2020. Hierarchi-\ncal Dynamic Filtering Network for RGB-D Salient Object\nDetection. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XXV 16, 235–252. Springer.\nPeng, H.; Li, B.; Xiong, W.; Hu, W.; and Ji, R. 2014.\nRGBD Salient Object Detection: A Benchmark and Algo-\nrithms. In European Conference on Computer Vision, 92–\n109. Springer.\nPiao, Y .; Ji, W.; Li, J.; Zhang, M.; and Lu, H. 2019.\nDepth-Induced Multi-Scale Recurrent Attention Network\nfor Saliency Detection. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, 7254–7263.\nPiao, Y .; Rong, Z.; Zhang, M.; Ren, W.; and Lu, H. 2020.\nA2dele: Adaptive and Attentive Depth Distiller for Efﬁ-\ncient RGB-D Salient Object Detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9060–9069.\nQu, L.; He, S.; Zhang, J.; Tian, J.; Tang, Y .; and Yang, Q.\n2017. RGBD Salient Object Detection via Deep Fusion.\nIEEE Transactions on Image Processing, 26(5): 2274–2285.\nShen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2021. Ef-\nﬁcient Attention: Attention with Linear Complexities. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, 3531–3539.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Con-\nvolutional Networks for Large-scale Image Recognition. In\nICLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is All You Need. In Advances in Neural Information\nProcessing Systems, 5998–6008.\nWang, W.; Shen, J.; and Ling, H. 2018. A Deep Network So-\nlution for Attention and Aesthetics Aware Photo Cropping.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 41(7): 1531–1544.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y . 2015. Show, Attend and\nTell: Neural Image Caption Generation with Visual Atten-\ntion. In International Conference on Machine Learning,\n2048–2057.\nZhang, D.; Zhang, H.; Tang, J.; Wang, M.; Hua, X.; and Sun,\nQ. 2020a. Feature Pyramid Transformer. In European Con-\nference on Computer Vision, 323–339. Springer.\nZhang, M.; Ren, W.; Piao, Y .; Rong, Z.; and Lu, H. 2020b.\nSelect, Supplement and Focus for RGB-D Saliency Detec-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 3472–3481.\nZhao, J.-X.; Cao, Y .; Fan, D.-P.; Cheng, M.-M.; Li, X.-Y .;\nand Zhang, L. 2019. Contrast Prior and Fluid Pyramid In-\ntegration for RGBD Salient Object Detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 3927–3936.\nZhao, X.; Zhang, L.; Pang, Y .; Lu, H.; and Zhang, L. 2020.\nA Single Stream Network for Robust and Real-time RGB-D\nSalient Object Detection. In European Conference on Com-\nputer Vision, 646–662. Springer.\nZhu, C.; and Li, G. 2017. A Three-pathway Psychobiolog-\nical Framework of Salient Object Detection using Stereo-\nscopic Technology. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision Workshops, 3008–\n3014.\nZhu, C.; Li, G.; Guo, X.; Wang, W.; and Wang, R. 2017a. A\nMultilayer Backpropagation Saliency Detection Algorithm\nbased on Depth Mining. In International Conference on\nComputer Analysis of Images and Patterns, 14–23. Springer.\nZhu, C.; Li, G.; Wang, W.; and Wang, R. 2017b. An Inno-\nvative Salient Object Detection using Center-dark Channel\nPrior. In Proceedings of the IEEE International Conference\non Computer Vision Workshops, 1509–1515.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. arXiv preprint arXiv:2010.04159.",
  "topic": "RGB color model",
  "concepts": [
    {
      "name": "RGB color model",
      "score": 0.8080068230628967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.664547324180603
    },
    {
      "name": "Transformer",
      "score": 0.6459384560585022
    },
    {
      "name": "Computer science",
      "score": 0.644940197467804
    },
    {
      "name": "Salient",
      "score": 0.5691947340965271
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5234181880950928
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.5127502679824829
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.507715106010437
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5045806169509888
    },
    {
      "name": "Fusion",
      "score": 0.45218437910079956
    },
    {
      "name": "Computer vision",
      "score": 0.3685692548751831
    },
    {
      "name": "Engineering",
      "score": 0.1815129816532135
    },
    {
      "name": "Voltage",
      "score": 0.1566670536994934
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "cited_by": 6
}